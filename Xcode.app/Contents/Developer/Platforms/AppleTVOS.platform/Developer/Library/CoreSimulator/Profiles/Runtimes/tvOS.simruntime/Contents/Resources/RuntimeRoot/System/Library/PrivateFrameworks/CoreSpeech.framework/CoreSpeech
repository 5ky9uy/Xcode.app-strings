mcpl
supo
333?
?ffffff
NSt3__114basic_ifstreamIcNS_11char_traitsIcEEEE
NSt3__113basic_filebufIcNS_11char_traitsIcEEEE
@mcpl
NSt3__118basic_stringstreamIcNS_11char_traitsIcEENS_9allocatorIcEEEE
NSt3__115basic_stringbufIcNS_11char_traitsIcEENS_9allocatorIcEEEE
NSt3__119basic_ostringstreamIcNS_11char_traitsIcEENS_9allocatorIcEEEE
@xfua2vpelppa
Affffff
triggerStartSampleCount
triggerEndSampleCount
triggerEndSeconds
triggerScore
isContinuous
activeChannel
CSContinuousVoiceTrigger Queue
v8@?0
-[CSContinuousVoiceTrigger startDetectTwoShot:]
-[CSContinuousVoiceTrigger startDetectTwoShot:]_block_invoke
-[CSContinuousVoiceTrigger speechManagerDidStartForwarding:successfully:error:]
-[CSContinuousVoiceTrigger speechManagerDidStopForwarding:forReason:]
best_score
-[CSContinuousVoiceTrigger _shotAnalyzerNDAPI:hasResultAvailable:forChannel:]
-[CSContinuousVoiceTrigger _shotAnalyzerNDAPI:hasResultAvailable:forChannel:]_block_invoke
-[CSContinuousVoiceTrigger _keywordAnalyzerNDAPI:hasResultAvailable:forChannel:]
best_start
best_end
hash
TQ,R
superclass
T#,R
description
T@"NSString",R,C
debugDescription
speechManager
T@"CSSpeechManager",W,N,V_speechManager
queue
T@"NSObject<OS_dispatch_queue>",&,N,V_queue
currentAsset
T@"CSAsset",&,N,V_currentAsset
keywordAnalyzer
T@"CSKeywordAnalyzerNDAPI",&,N,V_keywordAnalyzer
keywordThreshold
Tf,N,V_keywordThreshold
mode
Tq,N,V_mode
analyzedSampleCount
TQ,N,V_analyzedSampleCount
TQ,N,V_triggerEndSampleCount
twoShotDecisionWaitSamples
TQ,N,V_twoShotDecisionWaitSamples
twoShotThreshold
Tf,N,V_twoShotThreshold
delegate
T@"<CSVoiceTriggerDelegate>",W,N,V_delegate
-[CSAudioSampleRateConverter _createSampleRateConverterWithInASBD:outASBD:]
/BuildRoot/Library/Caches/com.apple.xbs/Sources/CoreSpeech_Sim/CoreSpeech-96/CoreSpeech/CSAudioSampleRateConverter.m
<Unknown File>
Too many buffers
i32@?0^I8^{AudioBufferList=I[1{AudioBuffer=II^v}]}16^^{AudioStreamPacketDescription}24
-[CSAudioSampleRateConverter convertSampleRateOfBuffer:]
+[CSUtils(LanguageCode) getSiriLanguageWithFallback:]
-[CSSpeakerModel _createDirectoryIfNotExist:]
xx_XX
model
audio
self ENDSWITH '.wav'
modelPath
T@"NSString",R,N
utteranceDirectory
enrollmentUtterance
T@"NSArray",R,N
isValid
TB,R,N,V_isValid
needsRetrain
TB,R,N
firstPassDetectedChannel
firstPassScore
firstPassBestStartSampleCount
firstPassBestEndSampleCount
firstPassFireSampleCount
VoiceTrigger First Pass Queue
-[CSVoiceTriggerFirstPass initWithManager:asset:]_block_invoke
RUNNING
STOPPED
v12@?0B8
-[CSVoiceTriggerFirstPass setAsset:]
-[CSVoiceTriggerFirstPass _setAsset:]_block_invoke
v16@?0Q8
-[CSVoiceTriggerFirstPass _transitVoiceTriggerStatus:]_block_invoke
-[CSVoiceTriggerFirstPass speechManagerDidStartForwarding:successfully:error:]
-[CSVoiceTriggerFirstPass speechManagerDidStopForwarding:forReason:]
-[CSVoiceTriggerFirstPass _keywordAnalyzerNDAPI:hasResultAvailable:forChannel:]
samples_at_fire
voiceTriggerStartPolicy
T@"CSVoiceTriggerEnabledPolicyNonAOP",&,N,V_voiceTriggerStartPolicy
voiceTriggerEnabled
TB,N,V_voiceTriggerEnabled
keywordAnalyzersNDAPI
T@"NSMutableArray",&,N,V_keywordAnalyzersNDAPI
hasTriggerPending
TB,N,V_hasTriggerPending
firstPassThreshold
Tf,N,V_firstPassThreshold
T@"<CSVoiceTriggerFirstPassDelegate>",W,N,V_delegate
speechManagerStartSampleCount
CSSpeechManager Asset Query Queue
-[CSSpeechManager _getVoiceTriggerAsset]
-[CSSpeechManager registerSpeechController:]
-[CSSpeechManager _notifyEvent:]
-[CSSpeechManager _createRecorderWithContextIfNeeded:error:]
-[CSSpeechManager _prepareRecorderWithSettings:error:]
-[CSSpeechManager _prepareListenWithSettings:error:]
-[CSSpeechManager prewarmAudioSession]
-[CSSpeechManager recordRoute]
-[CSSpeechManager recordSettings]
-[CSSpeechManager setClientContext:error:]
-[CSSpeechManager setClientContext:error:]_block_invoke
-[CSSpeechManager prepareRecordingForClient:error:]
-[CSSpeechManager prepareRecordingForClient:error:]_block_invoke
-[CSSpeechManager _startRecordingWithSettings:error:]
-[CSSpeechManager _startListening:]
-[CSSpeechManager _setRecordMode:error:]
-[CSSpeechManager _setCurrentContext:error:]
-[CSSpeechManager _releaseClientAudioSession:]
-[CSSpeechManager _releaseAudioSessionForListening:error:]
-[CSSpeechManager startRecordingWithSetting:event:error:]
-[CSSpeechManager startRecordingWithSetting:event:error:]_block_invoke
-[CSSpeechManager _startRecordingForClient:error:]
-[CSSpeechManager _startRecordingForClient:error:]_block_invoke
-[CSSpeechManager stopRecordingWithEvent:]_block_invoke_2
-[CSSpeechManager mediaserverdDidRestart]
-[CSSpeechManager didTransitFrom:to:by:]_block_invoke
-[CSSpeechManager didIgnoreEvent:from:]
-[CSSpeechManager _createListenPollingTimer]
-[CSSpeechManager _createListenPollingTimer]_block_invoke
-[CSSpeechManager _startListenPolling]
-[CSSpeechManager _stopListenPolling]
-[CSSpeechManager audioRecorderBufferAvailable:buffer:atTime:]_block_invoke
-[CSSpeechManager audioRecorderDidStartRecording:successfully:error:]_block_invoke
-[CSSpeechManager audioRecorderDidStopRecording:forReason:]_block_invoke
-[CSSpeechManager audioRecorderRecordHardwareConfigurationDidChange:toConfiguration:]_block_invoke
-[CSSpeechManager audioRecorderBeginRecordInterruption:]_block_invoke
-[CSSpeechManager audioRecorderBeginRecordInterruption:withContext:]_block_invoke
-[CSSpeechManager audioRecorderEndRecordInterruption:]_block_invoke
-[CSSpeechManager CSAssetManagerDidDownloadNewAsset:]
-[CSSpeechManager CSLanguageCodeUpdateMonitor:didReceiveLanguageCodeChanged:]
-[CSSpeechManager _reinitializeVoiceTriggerWithAsset:]
Init
Stop
FirstPass
SecondPass
RecordPending
RecordWithVTRunning
RecordWithVTStopped
PollingListening
StoppingWithVTRunning
StoppingWithVTStopped
unknown(%tu)
ClientPrepare
AudioRecorderRelease
VoiceTriggerRunning
VoiceTriggerStopped
FirstPassTriggered
SecondPassTriggered
SecondPassRejected
SelfTriggerDetected
RecordPendingTimeout
ClientStartRecording
ClientStopRecording
ClientReleaseRecordSession
RecordingDidStop
ListeningSucceed
MediaserverdRestarted
kDidStartFailed
assetQueryQueue
T@"NSObject<OS_dispatch_queue>",&,N,V_assetQueryQueue
stateMachine
T@"CSStateMachine",&,N,V_stateMachine
audioBuffer
T@"CSAudioCircularBuffer",&,N,V_audioBuffer
currentVoiceTriggerAsset
T@"CSAsset",&,N,V_currentVoiceTriggerAsset
voiceTriggerFirstPass
T@"CSVoiceTriggerFirstPass",&,N,V_voiceTriggerFirstPass
voiceTriggerSecondPass
T@"CSVoiceTriggerSecondPass",&,N,V_voiceTriggerSecondPass
clientController
T@"<CSSpeechManagerDelegate>",W,N,V_clientController
voiceTriggerEventNotifier
T@"CSVoiceTriggerEventNotifier",&,N,V_voiceTriggerEventNotifier
voiceTriggerFileLogger
T@"CSVoiceTriggerFileLogger",&,N,V_voiceTriggerFileLogger
selfTriggerDetector
T@"CSSelfTriggerDetector",&,N,V_selfTriggerDetector
continuousVoiceTrigger
T@"CSContinuousVoiceTrigger",&,N,V_continuousVoiceTrigger
keywordDetector
T@"CSKeywordDetector",&,N,V_keywordDetector
myriad
T@"CSMyriadPHash",&,N,V_myriad
voiceTriggerFidesClient
T@"CSVoiceTriggerFidesClient",&,N,V_voiceTriggerFidesClient
activeAudioProcessors
T@"NSHashTable",&,N,V_activeAudioProcessors
lastForwardedSampleCount
TQ,N,V_lastForwardedSampleCount
secondPassStartSampleCount
TQ,N,V_secondPassStartSampleCount
clientStartSampleCount
TQ,N,V_clientStartSampleCount
recordingPendingTimeout
Tq,N,V_recordingPendingTimeout
lastVoiceTriggerEventInfo
T@"NSDictionary",&,N,V_lastVoiceTriggerEventInfo
listenPollingTimer
T@"NSObject<OS_dispatch_source>",&,N,V_listenPollingTimer
audioRecorder
T@"CSAudioRecorder",&,N,V_audioRecorder
com.apple.MobileAsset.VoiceTriggerAssets
com.apple.MobileAsset.VoiceTriggerAssetsWatch
com.apple.MobileAsset.VoiceTriggerAssetsMarsh
com.apple.MobileAsset.SpeechEndpointAssets
en-US
-[CSAssetManager init]
Serial CSAssetManager queue
-[CSAssetManager assetOfType:language:]
-[CSAssetManager installedAssetOfType:language:]
-[CSAssetManager _installedAssetOfType:withPredicate:]
-[CSAssetManager _assetQueryForAssetType:withPredicate:localOnly:]
-[CSAssetManager _runAssetQuery:completion:]
-[CSAssetManager _runAssetQuery:completion:]_block_invoke
-[CSAssetManager _runAssetQuery:completion:]_block_invoke_2
v24@?0@"NSArray"8@"NSError"16
-[CSAssetManager _fetchRemoteMetaData]
-[CSAssetManager _fetchRemoteAssetOfType:withPredicate:]
-[CSAssetManager _updateFromRemoteToLocalAssets:forAssetType:]
-[CSAssetManager _defaultDownloadOptions]
-[CSAssetManager _downloadAsset:withComplete:]
-[CSAssetManager _downloadAsset:withComplete:]_block_invoke
v16@?0d8
-[CSAssetManager _downloadAsset:withComplete:]_block_invoke_2
v16@?0@"NSError"8
-[CSAssetManager _startDownloadingAsset:progress:completion:]
v24@?0@"NSDictionary"8@"NSError"16
-[CSAssetManager _startDownloadingAsset:progress:completion:]_block_invoke
-[CSAssetManager CSLanguageCodeUpdateMonitor:didReceiveLanguageCodeChanged:]
currentLanguageCode
+[CSAssetManager(Utils) predicateForAssetType:language:]
(%@ == %K)
(%@ IN %K)
((%K == nil) OR (%K != %@))
 && 
configFileNDAPI
threshold
config.txt
VTFirstPassConfigPathNDAPI
VTFirstPassThreshold
Tf,R,N
isTriggerEvent
totalSampleCount
-[CSVTUITrainingSession closeSessionWithStatus:successfully:]
-[CSVTUITrainingSession closeSessionWithStatus:successfully:complete:]_block_invoke
-[CSVTUITrainingSession suspendTraining]
-[CSVTUITrainingSession suspendTraining]_block_invoke
-[CSVTUITrainingSession resumeTraining]
-[CSVTUITrainingSession resumeTraining]_block_invoke
-[CSVTUITrainingSession setupPhraseSpotter]
-[CSVTUITrainingSession handleAudioInput:]_block_invoke_2
v16@?0@"NSDictionary"8
-[CSVTUITrainingSession handleAudioBufferForVTWithAudioInput:withDetectedBlock:]
-[CSVTUITrainingSession feedSpeechRecognitionTrailingSamplesWithCompletedBlock:]
-[CSVTUITrainingSession trimBeginingOfPCMBufferWithVoiceTriggerEventInfo:]
-[CSVTUITrainingSession audioSessionUnsupportedAudioRoute]
-[CSVTUITrainingSession didDetectBeginOfSpeech]
-[CSVTUITrainingSession didDetectEndOfSpeech:]
PHS explicit training utterance
[%ld] VTUISession Number:[%ld]
-[CSVTUITrainingSession startMasterTimerWithTimeout:]
-[CSVTUITrainingSession handleMasterTimeout:]
-[CSVTUITrainingSession stopMasterTimer]
-[CSVTUITrainingSession speechRecognitionTask:didHypothesizeTranscription:]
-[CSSpeechController initializeRecordSessionWithContext:]
-[CSSpeechController prepareRecordWithSettings:error:]
-[CSSpeechController setCurrentContext:error:]
-[CSSpeechController preheat]
-[CSSpeechController prewarmAudioSession]
-[CSSpeechController resetAudioSession]
-[CSSpeechController reset]
-[CSSpeechController releaseAudioSession]
-[CSSpeechController releaseAudioSession:]
-[CSSpeechController startRecordingWithSettings:error:]
-[CSSpeechController startRecordingWithSettings:error:]_block_invoke
yyyy_MM_dd-HHmmss.SSS
%@/CS-%@.wav
-[CSSpeechController stopRecording]
-[CSSpeechController recordRoute]
-[CSSpeechController speechManagerLPCMRecordBufferAvailable:chunk:]_block_invoke
-[CSSpeechController speechManagerRecordBufferAvailable:buffer:]_block_invoke
-[CSSpeechController speechManagerDidStartForwarding:successfully:error:]
-[CSSpeechController speechManagerDidStartForwarding:successfully:error:]_block_invoke
-[CSSpeechController speechManagerDidStopForwarding:forReason:]
-[CSSpeechController speechManagerDidStopForwarding:forReason:]_block_invoke
-[CSSpeechController speechManagerRecordHardwareConfigurationDidChange:toConfiguration:]
-[CSSpeechController speechManagerRecordHardwareConfigurationDidChange:toConfiguration:]_block_invoke
-[CSSpeechController speechManagerBeginRecordInterruption:]
-[CSSpeechController speechManagerBeginRecordInterruption:]_block_invoke
-[CSSpeechController speechManagerBeginRecordInterruption:withContext:]
-[CSSpeechController speechManagerBeginRecordInterruption:withContext:]_block_invoke
-[CSSpeechController speechManagerEndRecordInterruption:]
-[CSSpeechController speechManagerEndRecordInterruption:]_block_invoke
-[CSSpeechController audioConverterDidConvertPackets:packets:timestamp:]
-[CSSpeechController setAlertSoundFromURL:forType:]
-[CSSpeechController playAlertSoundForType:]
-[CSSpeechController playRecordStartingAlertAndResetEndpointer]
-[CSSpeechController setMeteringEnabled:]
-[CSSpeechController outputReferenceChannel]
-[CSSpeechController voiceTriggerInfo]
-[CSSpeechController voiceTriggerDidDetectTwoShotAtTime:]_block_invoke
-[CSSpeechController keywordDetectorDidDetectKeyword]_block_invoke
%c%c%c%c
none
endpointerProxy
T@"CSEndpointerProxy",&,N,V_endpointerProxy
avvcContext
T@"NSDictionary",&,N,V_avvcContext
isOpus
TB,N,V_isOpus
isActivated
TB,N,V_isActivated
isNarrowBand
TB,N,V_isNarrowBand
audioFileWriter
T@"CSAudioFileWriter",&,N,V_audioFileWriter
TQ,N,V_activeChannel
twoShotNotificationEnabled
TB,N,V_twoShotNotificationEnabled
T@"<CSSpeechControllerDelegate>",W,N,V_delegate
duckOthersOption
TB,N
endpointAnalyzer
T@"<CSEndpointAnalyzer>",R,N
com.apple.voicetrigger
com.apple.voicetrigger.notbackedup
VoiceTrigger Enabled
VoiceTrigger CoreSpeech Enabled
Enable Two Shot Notification
com.apple.demo-settings
StoreDemoMode
File Logging Level
Library
Logs/CrashReporter/VoiceTrigger/audio/
/Logs/CrashReporter/Assistant/
SpeechLogs
-[CSPreferences assistantAudioFileLogDirectory]
VoiceTrigger/SAT
Caches/VoiceTrigger/SATUpdate
Caches/VoiceTrigger/SATUpload
-[CSPreferences getUserVoiceProfileUploadPathWithEnrolledLanguageList:]
json
-[CSPreferences notifyUserVoiceProfileUploadComplete]
-[CSPreferences getUserVoiceProfileUpdateDirectory]
-[CSPreferences notifyUserVoiceProfileUpdateReady]
Enable VoiceTrigger Upon VoiceProfile Sync For Language
enrollment_completed
enrollment_migrated
-[CSPreferences _markSATEnrollmentWithMarker:forLanguage:]
initialState
Tq,N,V_initialState
transitions
T@"NSMutableDictionary",&,N,V_transitions
T@"<CSStateMachineDelegate>",W,N,V_delegate
currentState
Tq,R,N,V_currentState
Serial CSEventMonitor queue
/BuildRoot/Library/Caches/com.apple.xbs/Sources/CoreSpeech_Sim/CoreSpeech-96/CoreSpeech/CSEventMonitor.m
Subclasses need to overwrite this method
-[CSAudioFileLog _closeAudioFile]
-[CSAudioFileLog startRecording]_block_invoke
-input.wav
-[CSAudioFileLog appendAudioData:]_block_invoke
-[CSAudioFileLog stopRecording]_block_invoke
VoiceTrigger audio logging queue
-[CSVoiceTriggerFileLogger _audioLogDirectory]
/tmp
yyyyMMdd-HHmmss
%@%@%@
.json
-[CSVoiceTriggerFileLogger _writeDictionary:toPath:]
-triggered
.wav
-almost
-rejected
T@"CSAudioCircularBuffer",W,N,V_audioBuffer
extraSamplesAtStart
2ndChanceThreshold
loggingThreshold
configFileRecognizer
useKeywordSpotting
recognizerThresholdOffset
recognizerScoreScaleFactor
recognizerToken
config_marsh.txt
recognizer.json
hey_Siri
VTSecondPassExtraSamplesAtStart
TQ,R,N
VTSecondPassConfigPathNDAPI
VTSecondPassThreshold
VTSecondPass2ndChanceThreshold
VTSecondPassLoggingThreshold
VTSecondPassConfigPathRecognizer
VTSecondPassUserKeywordSpotting
VTSecondPassRecognizerThresholdOffset
VTSecondPassRecognizerScoreScaleFactor
VTSecondPassRecognizerToken
B8@?0
v12@?0i8
-[CSSpeechEndpointAssetMetaUpdateMonitor _startMonitoringWithQueue:]
-[CSSpeechEndpointAssetMetaUpdateMonitor _stopMonitoring]
-[CSSpeechEndpointAssetMetaUpdateMonitor _didReceiveNewSpeechEndpointAssetMetaData]
v16@?0@8
com.apple.MobileAsset.SpeechEndpointAssets.cached-metadata-updated
createFloatArray
sqrt
complex part zero vec
fft magnitudes array
normalized fft magnitudes
zeroCrossingVAD
HammingWindow
myriad_audio_analysis
windowed array for signal estimation
-[CSMyriadPHash pHash:length:]
last energy value set
current energy value set
-[CSMyriadPHash _generateMyriadInfo:score:]
/private/var/tmp/siriBC
com.apple.siri.myriad.apayload
signalEstimate
Ts,N,V_signalEstimate
com.apple.fides.borealis.record-creation
com.apple.fides.borealis
-[CSVoiceTriggerFidesClient _logDESRecordWithType:result:]
type
languageCode
triggerSampleCount
PHSMigrationStatus
csChunk
-[CSVoiceTriggerFidesClient _logDESRecordWithType:result:]_block_invoke
v24@?0@"NSUUID"8@"NSError"16
-[CSVoiceTriggerFidesClient voiceTriggerDidDetectKeyword:]_block_invoke
-[CSVoiceTriggerFidesClient voiceTriggerDidDetectNearMiss:]_block_invoke
near-miss
trigger
speaker-reject
unknown
numChannels
numSamples
sampleByteDepth
startSampleCount
hostTime
Testing [%@] against regex.
regex.json
Cannot parse to JSON
Cannot find the file
trailing_garbage
leading_garbage
regex
com.apple.VoiceTriggerUI.TrainingSessionQueue
com.apple.VoiceTriggerUI.TrainingManager
-[CSVTUITrainingManager setLocaleIdentifier:]
-[CSVTUITrainingManager createKeywordDetector]
-[CSVTUITrainingManager cleanupWithCompletion:]
-[CSVTUITrainingManager cleanupWithCompletion:]_block_invoke
-[CSVTUITrainingManager cleanupWithCompletion:]_block_invoke_2
-[CSVTUITrainingManager trainUtterance:shouldUseASR:completion:]
-[CSVTUITrainingManager trainUtterance:shouldUseASR:completion:]_block_invoke_2
-[CSVTUITrainingManager cancelTrainingForID:]
-[CSVTUITrainingManager closeSessionBeforeStartWithStatus:successfully:withCompletion:]
-[CSVTUITrainingManager _shouldShowHeadsetDisconnectionMessage]
-[CSVTUITrainingManager _startAudioSession]
-[CSVTUITrainingManager setSuspendAudio:]
-[CSVTUITrainingManager setSuspendAudio:]_block_invoke
-[CSVTUITrainingManager VTUITrainingSessionStopListen]
Tf,V_rms
T@"<CSVTUITrainingManagerDelegate>",W,N,V_delegate
speechRecognizerAvailable
TB,R,V_speechRecognizerAvailable
audioSource
suspendAudio
InternalBuild
-[CSKeywordAnalyzerNDAPI initWithConfigPath:resourcePath:]
-[CSKeywordAnalyzerNDAPI _setStartAnalyzeTime:]
activePhraseId
TI,N,V_activePhraseId
T@"<CSKeywordAnalyzerNDAPIScoreDelegate>",W,N,V_delegate
com.apple.corespeech
Framework
Logs/CrashReporter/CoreSpeech/
Logs/CrashReporter/CoreSpeech/audio/
%@/%@%@%@
_CSGetOrCreateAudioLogDirectory
totalAudioRecorded
Td,N,V_totalAudioRecorded
featuresAtEndpoint
T@"NSArray",&,N,V_featuresAtEndpoint
endpointerType
Tq,N,V_endpointerType
serverFeatureLatencyDistribution
T@"NSDictionary",&,N,V_serverFeatureLatencyDistribution
additionalMetrics
T@"NSDictionary",&,N,V_additionalMetrics
Languages
Footprint
Premium
com.apple.coreaudio.BorealisToggled
-[CSVoiceTriggerEnabledMonitor _startMonitoringWithQueue:]
-[CSVoiceTriggerEnabledMonitor _stopMonitoring]
-[CSVoiceTriggerEnabledMonitor _checkVoiceTriggerEnabled]
waitTimeSinceVT
keyword_detector.json
keywordDetectorConfigPathRecognizer
keywordDetectorThreshold
keywordDetectorWaitTimeSinceVT
isMaximized
-[CSVoiceTriggerSpeakerTrainer trainUtterance:languageCode:]
samples_fed
Dictionary to JSON conversion failed : %@
Borealis Input
com.apple.VoiceTriggerUI.AVVCSessionQueue
-[CSVTUIAudioSessionAVVC voiceController]
-[CSVTUIAudioSessionAVVC prepareRecord]
-[CSVTUIAudioSessionAVVC startRecording]
-[CSVTUIAudioSessionAVVC stopRecording]
-[CSVTUIAudioSessionAVVC _hasInputAudioRoute]
-[CSVTUIAudioSessionAVVC _hasCorrectInputAudioRoute]
-[CSVTUIAudioSessionAVVC _hasCorrectOutputAudioRoute]
T@"<CSVTUIAudioSessionDelegate>",W,N,V_delegate
beepLocation
statsComputed
beepPower
signalPower
originalPower
absMaxVal
above95pcOfMax
jbl_begin.bin
-[CSBeepCanceller init]
-[CSBeepCanceller willBeep]
-[CSBeepCanceller reset]
T@"<CSBeepCancellerDelegate>",W,N,V_delegate
metrics
T@"NSDictionary",R,N
allocator<T>::allocate(size_t n) 'n' exceeds maximum supported size
triggerFireSampleCount
triggerStartSeconds
triggerFireSeconds
effectiveThreshold
recognizerScore
recognizerScaleFactor
earlyDetectFiredMachTime
triggerStartMachTime
triggerEndMachTime
triggerFireMachTime
hardwareSamplerate
configVersion
VoiceTrigger Second Pass Queue
-[CSVoiceTriggerSecondPass setBypassForSeconds:]_block_invoke
-[CSVoiceTriggerSecondPass voiceTriggerFirstPass:didDetectKeyword:]
com.apple.voicetrigger.EarlyDetect
-[CSVoiceTriggerSecondPass voiceTriggerFirstPass:didDetectKeyword:]_block_invoke
-[CSVoiceTriggerSecondPass speechManagerLPCMRecordBufferAvailable:chunk:]_block_invoke
-[CSVoiceTriggerSecondPass speechManagerDidStartForwarding:successfully:error:]
-[CSVoiceTriggerSecondPass speechManagerDidStopForwarding:forReason:]
-[CSVoiceTriggerSecondPass _analyzeForKeywordDetection:result:forChannel:]
-[CSVoiceTriggerSecondPass keywordAnalyzerQuasar:hasResultAvailable:forChannel:]
keywordAnalyzerNDAPI
T@"CSKeywordAnalyzerNDAPI",&,N,V_keywordAnalyzerNDAPI
keywordAnalyzerQuasar
T@"CSKeywordAnalyzerQuasar",&,N,V_keywordAnalyzerQuasar
speakerDetector
T@"CSSpeakerDetectorNDAPI",&,N,V_speakerDetector
speakerModel
T@"CSSpeakerModel",&,N,V_speakerModel
secondPassTimeout
TQ,N,V_secondPassTimeout
numAnalyzedSamples
TQ,N,V_numAnalyzedSamples
keywordLoggingThreshold
Tf,N,V_keywordLoggingThreshold
lastScore
Tf,N,V_lastScore
TQ,N,V_extraSamplesAtStart
useSAT
TB,N,V_useSAT
nearMissDelayTimeout
TQ,N,V_nearMissDelayTimeout
nearMissCandidateDetectedSamples
TQ,N,V_nearMissCandidateDetectedSamples
hasPendingNearMiss
TB,N,V_hasPendingNearMiss
lastAnalyzerResult
T@"NSDictionary",&,N,V_lastAnalyzerResult
numBypassSamples
Tq,N,V_numBypassSamples
Tf,N,V_recognizerScore
isRunningRecognizer
TB,N,V_isRunningRecognizer
recognizerResultPending
TB,N,V_recognizerResultPending
Tf,N,V_recognizerScoreScaleFactor
TQ,N,V_earlyDetectFiredMachTime
VoiceTriggerEventNotifier queue
-[CSVoiceTriggerEventNotifier _notifyTriggerEvent:]
-[CSVoiceTriggerEventNotifier _notifyTriggerEvent:]_block_invoke
com.apple.coreaudio.borealisTrigger
com.apple.voicetrigger.nearMiss
-[CSVoiceTriggerEventNotifier _notifyTwoShotDetectionAt:]
isContinuousRunningMode
TB,N,V_isContinuousRunningMode
CSInitialContinousZeros
CSMaxContinousZeros
CSMidSegmentContinousZeros
start
+[CSUtils(AudioFile) readAudioChunksFrom:block:]
-[CSAudioCircularBuffer initWithNumChannels:recordingDuration:samplingRate:]
-[CSAudioCircularBuffer copySamplesFromHostTime:]
-[CSAudioCircularBuffer copySamplesFrom:to:]
-[CSAudioCircularBuffer copyBufferWithNumSamplesCopiedIn:]
-[CSAudioCircularBuffer reset]
-[CSAudioCircularBuffer saveRecordingBufferFrom:to:toURL:]
bufferLength
TQ,N,V_bufferLength
copySamples
  mNumChannels: 
  mRecordingDurationInSecs: 
  mSampleRate: 
  mBytesPerSample: 
  mBufferLengthInSamples: 
  mNextWritePos: 
  mSamplesCount: 
  mMemoryPool(
): [
    chan-
: sz=
: mem-sz: 
-[CSVoiceTriggerAssetMetaUpdateMonitor _startMonitoringWithQueue:]
-[CSVoiceTriggerAssetMetaUpdateMonitor _stopMonitoring]
-[CSVoiceTriggerAssetMetaUpdateMonitor _didReceiveNewVoiceTriggerAssetMetaData]
com.apple.MobileAsset.VoiceTriggerAssets.cached-metadata-updated
com.apple.MobileAsset.VoiceTriggerAssetsWatch.cached-metadata-updated
com.apple.MobileAsset.VoiceTriggerAssetsMarsh.cached-metadata-updated
speakerDetectorNDAPI
configPath
retrainTriggerThreshold
speakerDetectorNDAPIConfigPath
speakerDetectorThreshold
speakerDetectorRetrainTriggerThreshold
{wordCount: %ld, trailingSilDuration: %ld, eosLikelihood: %f, pauseCounts: %@, silencePosterior: %f, taskName: %@, processedAudioDurationInMilliseconds: %ld}
wordCount
Tq,N,V_wordCount
trailingSilenceDuration
Tq,N,V_trailingSilenceDuration
eosLikelihood
Td,N,V_eosLikelihood
pauseCounts
T@"NSArray",C,N,V_pauseCounts
silencePosterior
Td,N,V_silencePosterior
processedAudioDurationInMilliseconds
Tq,N,V_processedAudioDurationInMilliseconds
taskName
T@"NSString",C,N,V_taskName
com.apple.cs.%@.apQueue
-[CSVAD2EndpointAnalyzer preheat]
-[CSVAD2EndpointAnalyzer resetForNewRequestWithSampleRate:]
-[CSVAD2EndpointAnalyzer reset]
-[CSVAD2EndpointAnalyzer _resetWithSampleRate:]
-[CSVAD2EndpointAnalyzer handleVoiceTriggerWithActivationInfo:]_block_invoke
-[CSVAD2EndpointAnalyzer processAudioSamplesAsynchronously:]
-[CSVAD2EndpointAnalyzer processAudioSamplesAsynchronously:]_block_invoke
-[CSVAD2EndpointAnalyzer _processAudioSamples:]
endpointStyle
Tq,N
delay
Td,N
startWaitTime
automaticEndpointingSuspensionEndTime
minimumDurationForEndpointer
lastEndOfVoiceActivityTime
Td,R,N
lastStartOfVoiceActivityTime
bypassSamples
endpointMode
interspeechWaitTime
endWaitTime
saveSamplesSeenInReset
T@"<CSEndpointAnalyzerDelegate>",W,N
canProcessCurrentRequest
TQ,N
endpointerModelVersion
sampleRate
Td,N,V_sampleRate
frameRate
TI,N,V_frameRate
detectedOneShotStartpoint
TB,N,V_detectedOneShotStartpoint
detectedRecurrentStartpoint
TB,N,V_detectedRecurrentStartpoint
communicatedStartPointDetection
TB,N,V_communicatedStartPointDetection
detectedOneShotEndpoint
TB,N,V_detectedOneShotEndpoint
detectedRecurrentEndpoint
TB,N,V_detectedRecurrentEndpoint
communicatedEndpointDetection
TB,N,V_communicatedEndpointDetection
samplesSeen
Td,N,V_samplesSeen
numSamplesProcessed
Td,N,V_numSamplesProcessed
lastOneShotStartpoint
Td,N,V_lastOneShotStartpoint
lastOneShotEndpoint
Td,N,V_lastOneShotEndpoint
lastRecurrentStartpoint
Td,N,V_lastRecurrentStartpoint
lastRecurrentEndpoint
Td,N,V_lastRecurrentEndpoint
floatSampleBuffer
T@"NSMutableData",&,N,V_floatSampleBuffer
topLevelParameterDict
T@"NSDictionary",&,N,V_topLevelParameterDict
modelDictPath
T@"NSString",&,N,V_modelDictPath
isConfigured
TB,N,V_isConfigured
previousSamplesSeen
Td,N,V_previousSamplesSeen
apQueue
T@"NSObject<OS_dispatch_queue>",&,N,V_apQueue
recordingDidStop
TB,N,V_recordingDidStop
vtEndInSampleCount
TQ,N,V_vtEndInSampleCount
Tq,N,V_endpointStyle
Td,N,V_delay
Td,N,V_startWaitTime
Td,N,V_automaticEndpointingSuspensionEndTime
Td,N,V_minimumDurationForEndpointer
Td,N,V_bypassSamples
Tq,N,V_endpointMode
Td,N,V_interspeechWaitTime
Td,N,V_endWaitTime
TB,N,V_saveSamplesSeenInReset
T@"<CSEndpointAnalyzerDelegate>",W,N,V_delegate
-[CSVAD2EndpointAnalyzer(private) _configureWithSampleRate:andFrameRate:]
kAUEndpointVAD2Property_EDLStartWaitTimeSec
kAUEndpointVAD2Property_EDLInterspeechWaitTimeSec
kAUEndpointVAD2Property_EDLSpeechStartAdjustSec
kAUEndpointVAD2Property_EDLSpeechEndAdjustSec
kAUEndpointVAD2Property_EDLWindowLengthSeconds
kAUEndpointVAD2Property_EDLSpeechFraction
kAUEndpointVAD2Property_EDLNonspeechFraction
kAUEndpointVAD2Property_IsRealtimeOperationMode
kAUEndpointVAD2Property_DecoderLatencySeconds
kAudioUnitProperty_MaximumFramesPerSlice
-[CSVAD2EndpointAnalyzer(private) _detectVoiceActivityInSamples:numSamples:]
Library/Audio/Tunings/Generic/AU/
EndpointerModelPathForStyle
aufx-epv2-bluetooth8khz-appl.plist
aufx-epv2-appl.plist
EndpointerSpeechBeginListener
EndpointerSpeechEndListener
RecurrentVADSpeechBeginListener
RecurrentVADSpeechEndListener
-[CSAudioChunk subChunkFrom:numSamples:forChannel:]
data
T@"NSData",R,N,V_data
TQ,R,N,V_numChannels
TQ,R,N,V_numSamples
TQ,R,N,V_sampleByteDepth
TQ,R,N,V_startSampleCount
TQ,R,N,V_hostTime
best_phrase
early_warning
is_rescoring
sampleFed
TQ,N,V_sampleFed
bestPhrase
TQ,N,V_bestPhrase
bestStart
TQ,N,V_bestStart
bestEnd
TQ,N,V_bestEnd
bestScore
Tf,N,V_bestScore
earlyWarning
TB,N,V_earlyWarning
isRescoring
TB,N,V_isRescoring
dictionary
-[CSVTUITrainingSessionWithPayload _firedVoiceTriggerTimeout]
-[CSVTUITrainingSessionWithPayload _firedEndPointTimeout]
-[CSVTUITrainingSessionWithPayload audioSessionDidStartRecording:error:]
-[CSVTUITrainingSessionWithPayload audioSessionDidStopRecording:]
-[CSVTUITrainingSessionWithPayload didDetectBeginOfSpeech]
-[CSVTUITrainingSessionWithPayload didDetectEndOfSpeech:]
-[CSVTUITrainingSessionWithPayload speechRecognitionTask:didHypothesizeTranscription:]_block_invoke
-[CSVTUITrainingSessionWithPayload speechRecognitionTask:didHypothesizeTranscription:]_block_invoke_2
-[CSVTUITrainingSessionWithPayload speechRecognitionTask:didFinishRecognition:]_block_invoke
-[CSVTUITrainingSessionWithPayload speechRecognitionTask:didFinishRecognition:]_block_invoke_2
-[CSVTUITrainingSessionWithPayload speechRecognitionTask:didFinishSuccessfully:]_block_invoke
-[CSVTUITrainingSessionWithPayload matchRecognitionResult:withMatchedBlock:withNonMatchedBlock:]
SearchOrMessaging
Warmup
Median
ExtraDelayMs
EndpointerDecisionLagMs
ClientLagThresholdMsKey
ClampedSFLatencyMsForClientLag
UseDefaultServerFeaturesOnClientLag
-[CSHybridEndpointAnalyzer init]
com.apple.cs.%@.stateserialqueue
com.apple.cs.%@.sepfQueue
com.apple.cs.%@.hybridClassifierfQueue
com.apple.cs.%@.silencePosteriorGeneratorQueue
-[CSHybridEndpointAnalyzer processAudioSamplesAsynchronously:]
-[CSHybridEndpointAnalyzer processAudioSamplesAsynchronously:]_block_invoke
-[CSHybridEndpointAnalyzer updateEndpointerThreshold:]
-[CSHybridEndpointAnalyzer updateEndpointerDelayedTrigger:]
-[CSHybridEndpointAnalyzer processServerEndpointFeatures:]
-[CSHybridEndpointAnalyzer shouldAcceptEagerResultForDuration:resultsCompletionHandler:]_block_invoke
-[CSHybridEndpointAnalyzer clientSilenceFeaturesAvailable:]
-[CSHybridEndpointAnalyzer clientSilenceFeaturesAvailable:]_block_invoke
-[CSHybridEndpointAnalyzer clientSilenceFeaturesAvailable:]_block_invoke_2
-[CSHybridEndpointAnalyzer serverFeaturesLatencyDistributionDictionary]_block_invoke
q24@?0@8@16
-[CSHybridEndpointAnalyzer handleVoiceTriggerWithActivationInfo:]_block_invoke
-[CSHybridEndpointAnalyzer recordingStoppedForReason:]
-[CSHybridEndpointAnalyzer resetForNewRequestWithSampleRate:]
-[CSHybridEndpointAnalyzer resetForNewRequestWithSampleRate:]_block_invoke
/BuildRoot/Library/Caches/com.apple.xbs/Sources/CoreSpeech_Sim/CoreSpeech-96/CoreSpeech/CSHybridEndpointAnalyzer.m
CSHybridEndpointAnalyzer reset called
-[CSHybridEndpointAnalyzer CSLanguageCodeUpdateMonitor:didReceiveLanguageCodeChanged:]
-[CSHybridEndpointAnalyzer CSAssetManagerDidDownloadNewAsset:]
-[CSHybridEndpointAnalyzer _updateAssetWithLanguage:]_block_invoke_2
-[CSHybridEndpointAnalyzer _updateAssetWithLanguage:]_block_invoke
cs_hep_marsh.json
cs_hep.json
-[CSHybridEndpointAnalyzer _getCSHybridEndpointerConfigForAsset:]
TQ,N,V_numSamplesProcessed
didAddAudio
TB,N,V_didAddAudio
caesuraSPG
T@"EARCaesuraSilencePosteriorGenerator",&,N,V_caesuraSPG
clientSilenceFeaturesAtEndpoint
T@"EARClientSilenceFeatures",&,N,V_clientSilenceFeaturesAtEndpoint
TB,N,V_canProcessCurrentRequest
hybridClassifier
T@"_EAREndpointer",&,N,V_hybridClassifier
T@"NSString",&,N,V_endpointerModelVersion
serverFeaturesQueue
T@"NSObject<OS_dispatch_queue>",&,N,V_serverFeaturesQueue
lastKnownServerEPFeatures
T@"CSServerEndpointFeatures",&,N,V_lastKnownServerEPFeatures
serverFeatureLatencies
T@"NSMutableArray",&,N,V_serverFeatureLatencies
serverFeaturesWarmupLatency
Td,N,V_serverFeaturesWarmupLatency
lastServerFeatureTimestamp
T@"NSDate",&,N,V_lastServerFeatureTimestamp
didReceiveServerFeatures
TB,N,V_didReceiveServerFeatures
clientLagThresholdMs
Td,N,V_clientLagThresholdMs
clampedSFLatencyMsForClientLag
Td,N,V_clampedSFLatencyMsForClientLag
useDefaultServerFeaturesOnClientLag
TB,N,V_useDefaultServerFeaturesOnClientLag
hybridClassifierQueue
T@"NSObject<OS_dispatch_queue>",&,N,V_hybridClassifierQueue
lastReportedEndpointTimeMs
Td,N,V_lastReportedEndpointTimeMs
stateSerialQueue
T@"NSObject<OS_dispatch_queue>",&,N,V_stateSerialQueue
didCommunicateEndpoint
TB,N,V_didCommunicateEndpoint
currentRequestSampleRate
TQ,N,V_currentRequestSampleRate
vtExtraAudioAtStartInMs
Td,N,V_vtExtraAudioAtStartInMs
hepAudioOriginInMs
Td,N,V_hepAudioOriginInMs
firstAudioPacketTimestamp
T@"NSDate",&,N,V_firstAudioPacketTimestamp
didTimestampFirstAudioPacket
TB,N,V_didTimestampFirstAudioPacket
silencePosteriorGeneratorQueue
T@"NSObject<OS_dispatch_queue>",&,N,V_silencePosteriorGeneratorQueue
+[CSUtils(Time) hostTimeFromSampleCount:anchorHostTime:anchorSampleCount:]
+[CSUtils(Time) sampleCountFromHostTime:anchorHostTime:anchorSampleCount:]
-[CSEndpointerProxy resetForNewRequestWithSampleRate:]
-[CSEndpointerProxy resetForVoiceTriggerTwoShotWithSampleRate:]
-[CSEndpointerProxy endpointer:didDetectStartpointAtTime:]
-[CSEndpointerProxy endpointer:didDetectHardEndpointAtTime:withMetrics:]
-[CSEndpointerProxy _shouldEnterTwoShotAtEndPointTime:]
-[CSEndpointerProxy endpointerModelVersion]
hybridEndpointer
T@"<CSEndpointAnalyzerImpl>",&,N,V_hybridEndpointer
vad2Endpointer
T@"<CSEndpointAnalyzerImpl>",&,N,V_vad2Endpointer
activeEndpointer
T@"<CSEndpointAnalyzerImpl>",W,N,V_activeEndpointer
didEnterTwoshot
TB,N,V_didEnterTwoshot
endpointerDelegate
T@"<CSEndpointAnalyzerDelegate>",W,N,V_endpointerDelegate
-[CSFirstUnlockMonitor _stopMonitoring]
-[CSSpringboardStartMonitor _startMonitoringWithQueue:]
-[CSSpringboardStartMonitor _stopMonitoring]
-[CSSpringboardStartMonitor _checkSpringBoardStarted]
com.apple.springboard.finishedstartup
com.apple.transcribe.Transcriber
-[CSKeywordAnalyzerQuasar initWithConfigPath:triggerTokens:useKeywordSpotting:]
-[CSKeywordAnalyzerQuasar reset]
-[CSKeywordAnalyzerQuasar runRecognition]
-[CSKeywordAnalyzerQuasar runRecognition]_block_invoke
-[CSKeywordAnalyzerQuasar endAudio]
-[CSKeywordAnalyzerQuasar endAudio]_block_invoke
-[CSKeywordAnalyzerQuasar _recognizeWavData:length:]
-[CSKeywordAnalyzerQuasar speechRecognizer:didRecognizePartialResult:]_block_invoke
-[CSKeywordAnalyzerQuasar speechRecognizer:didFinishRecognitionWithError:]_block_invoke
-[CSKeywordAnalyzerQuasar _getConfidence:]_block_invoke
v32@?0@"_EARSpeechRecognitionToken"8Q16^B24
triggerConfidence
Td,R,N,V_triggerConfidence
T@"<CSKeywordAnalyzerQuasarScoreDelegate>",W,N,V_delegate
corespeech.json
hybridendpointer.json
hybridendpointer_marsh.json
/BuildRoot/Library/Caches/com.apple.xbs/Sources/CoreSpeech_Sim/CoreSpeech-96/CoreSpeech/CSAsset.m
ERR: Unknown assetType: %lu
+[CSAsset fallBackAssetResourcePath]
defaultFallback
-[CSAsset initWithResourcePath:configFile:configVersion:]
-[CSAsset _decodeJson:]
-[CSAsset getNumberForKey:category:default:]
-[CSAsset getStringForKey:category:default:]
configVersion:%@ resourcePath:%@ path:%@
path
T@"NSString",R,N,V_path
resourcePath
T@"NSString",R,N,V_resourcePath
hashFromResourcePath
T@"NSString",R,N,V_configVersion
-[CSAudioConverter _convertBufferedLPCM:allowPartial:timestamp:]
-[CSAudioConverter _convertBufferedLPCM:allowPartial:timestamp:]_block_invoke
/BuildRoot/Library/Caches/com.apple.xbs/Sources/CoreSpeech_Sim/CoreSpeech-96/CoreSpeech/CSAudioConverter.m
Cannot produce ASPD for PCM
-[CSAudioConverter reset]
-[CSAudioConverter _configureAudioConverter:]
T@"<CSAudioConverterDelegate>",W,V_delegate
CreateAudioConverter
-[CSLanguageCodeUpdateMonitor _startMonitoringWithQueue:]
-[CSLanguageCodeUpdateMonitor _stopMonitoring]
-[CSLanguageCodeUpdateMonitor _didReceiveLanguageCodeUpdate]
Serial CSPolicy queue
CSSelfVoiceTriggerDetector Queue
-[CSSelfTriggerDetector speechManagerDidStartForwarding:successfully:error:]
-[CSSelfTriggerDetector speechManagerDidStopForwarding:forReason:]
-[CSSelfTriggerDetector keywordAnalyzerNDAPI:hasResultAvailable:forChannel:]
com.apple.siri.corespeech.selftrigger
outputAudioChannel
TQ,N,V_outputAudioChannel
-[CSSpeakerDetectorNDAPI _initializeSAT:]
-[CSSpeakerDetectorNDAPI processSuperVector:withResult:]
T@"<CSSpeakerDetectorNDAPIDelegate>",W,N,V_delegate
-[CSVTUIKeywordDetector initWithLanguageCode:]
-[CSAudioRecorder _voiceControllerWithContext:error:]
-[CSAudioRecorder _beepCanceller]
-[CSAudioRecorder prepareRecordWithSettings:error:]
-[CSAudioRecorder setCurrentContext:error:]
-[CSAudioRecorder prewarmAudioSession]
-[CSAudioRecorder releaseAudioSession:]
-[CSAudioRecorder _resetZeroFilter]
-[CSAudioRecorder startRecordingWithSettings:error:]
-[CSAudioRecorder startRecording:]
-[CSAudioRecorder startRecording]
-[CSAudioRecorder stopRecording]
-[CSAudioRecorder _recordingSampleRate]
-[CSAudioRecorder playAlertSoundForType:]
-[CSAudioRecorder voiceControllerDidStartRecording:successfully:error:]
-[CSAudioRecorder voiceControllerDidStopRecording:forReason:]
ZeroFilterMetrics
BeepCancellerMetrics
-[CSAudioRecorder voiceControllerRecordHardwareConfigurationDidChange:toConfiguration:]
-[CSAudioRecorder voiceControllerBeginRecordInterruption:]
-[CSAudioRecorder voiceControllerBeginRecordInterruption:withContext:]
-[CSAudioRecorder voiceControllerEndRecordInterruption:]
-[CSAudioRecorder voiceControllerMediaServicesWereLost:]
-[CSAudioRecorder voiceControllerMediaServicesWereReset:]
-[CSAudioRecorder _deinterleaveBufferIfNeeded:]
-[CSAudioRecorder _createDeInterleaverIfNeeded]
-[CSAudioRecorder _createSampleRateConverterIfNeeded]
T@"<CSAudioRecorderDelegate>",W,N,V_delegate
twoShotDecisionWaitTime
CVTConfigPathNDAPI
CVTThreshold
CVTTwoShotThreshold
CVTTwoShotDecisionWaitTime
CSKeywordDetector Queue
-[CSKeywordDetector startDetectKeyword:]
-[CSKeywordDetector startDetectKeyword:]_block_invoke
-[CSKeywordDetector speechManagerDidStartForwarding:successfully:error:]
-[CSKeywordDetector speechManagerDidStopForwarding:forReason:]
-[CSKeywordDetector keywordAnalyzerQuasar:hasResultAvailable:forChannel:]
T@"CSKeywordAnalyzerQuasar",&,N,V_keywordAnalyzer
decisionWaitSampleCount
TQ,N,V_decisionWaitSampleCount
((?:[a-z]|[0-9])*)\.asset
+[CSUtils(ResourcePathHash) assetHashInResourcePath:]
nohash
-[CSAudioFileWriter initWithURL:inputFormat:outputFormat:]
-[CSAudioFileWriter addSamples:len:]
init
_setAsset:
_reset
reset
setActivePhraseId:
CVTConfigPathNDAPI
resourcePath
alloc
initWithConfigPath:resourcePath:
setDelegate:
setActiveChannel:
CVTThreshold
inputRecordingSampleRate
CVTTwoShotDecisionWaitTime
CVTTwoShotThreshold
objectForKeyedSubscript:
doubleValue
numSamples
channelForProcessedInput
subChunkFrom:numSamples:forChannel:
processAudioChunk:
_shotAnalyzerNDAPI:hasResultAvailable:forChannel:
_keywordAnalyzerNDAPI:hasResultAvailable:forChannel:
floatValue
supportCSTwoShotDecision
voiceTriggerDidDetectTwoShotAtTime:
respondsToSelector:
numberWithFloat:
numberWithBool:
numberWithUnsignedInteger:
dictionaryWithObjects:forKeys:count:
voiceTriggerDidDetectKeyword:
isEqual:
class
self
performSelector:
performSelector:withObject:
performSelector:withObject:withObject:
isProxy
isKindOfClass:
isMemberOfClass:
conformsToProtocol:
retain
release
autorelease
retainCount
zone
hash
superclass
description
debugDescription
keywordAnalyzerNDAPI:hasResultAvailable:forChannel:
speechManagerRecordBufferAvailable:buffer:
speechManagerLPCMRecordBufferAvailable:chunk:
speechManagerDidStartForwarding:successfully:error:
speechManagerDidStopForwarding:forReason:
speechManagerRecordingContext
speechManagerRecordHardwareConfigurationDidChange:toConfiguration:
speechManagerBeginRecordInterruption:
speechManagerBeginRecordInterruption:withContext:
speechManagerEndRecordInterruption:
initWithManager:asset:
setAsset:
startDetectTwoShot:
.cxx_destruct
delegate
speechManager
setSpeechManager:
queue
setQueue:
currentAsset
setCurrentAsset:
keywordAnalyzer
setKeywordAnalyzer:
keywordThreshold
setKeywordThreshold:
mode
setMode:
analyzedSampleCount
setAnalyzedSampleCount:
triggerEndSampleCount
setTriggerEndSampleCount:
twoShotDecisionWaitSamples
setTwoShotDecisionWaitSamples:
twoShotThreshold
setTwoShotThreshold:
_keywordThreshold
_twoShotThreshold
_delegate
_speechManager
_queue
_currentAsset
_keywordAnalyzer
_mode
_analyzedSampleCount
_triggerEndSampleCount
_twoShotDecisionWaitSamples
lpcmNarrowBandASBD
lpcmASBD
initWithInASBD:outASBD:
_createSampleRateConverterWithInASBD:outASBD:
dealloc
length
dataWithLength:
mutableBytes
stringWithUTF8String:
currentHandler
handleFailureInMethod:object:file:lineNumber:description:
bytes
setLength:
upsampler
downsampler
convertSampleRateOfBuffer:
_sampleRateConverter
_outBufferScaleFactor
_inASBD
_outASBD
getSiriLanguageWithFallback:
initWithLength:
convertToFloatLPCMBufFromShortLPCMBuf:
convertToShortLPCMBufFromFloatLPCMBuf:
modelDirectory
_createDirectoryIfNotExist:
utteranceDirectory
defaultManager
fileExistsAtPath:isDirectory:
removeItemAtPath:error:
createDirectoryAtPath:withIntermediateDirectories:attributes:error:
localizedDescription
sharedPreferences
CSSATBasePath
stringByAppendingPathComponent:
_satPath
hashFromResourcePath
array
contentsOfDirectoryAtPath:error:
predicateWithFormat:
filteredArrayUsingPredicate:
caseInsensitiveCompare:
sortedArrayUsingSelector:
countByEnumeratingWithState:objects:count:
addObject:
count
modelPath
fileExistsAtPath:
_isDirectoryEmpty:
initWithAsset:languageCode:
enrollmentUtterance
needsRetrain
isValid
_asset
_languageCode
_modelPath
_utteranceDirectory
_isValid
_transitVoiceTriggerStatus:
setCallback:
isEnabled
VTFirstPassThreshold
removeAllObjects
VTFirstPassConfigPathNDAPI
inputRecordingProcessingChannelsBitset
iterateBitset:block:
_startVoiceTrigger
_stopVoiceTrigger
startRecordingWithSetting:event:error:
stopRecordingWithEvent:
voiceTriggerFirstPass:didDetectKeyword:
voiceTriggerStartPolicy
setVoiceTriggerStartPolicy:
voiceTriggerEnabled
setVoiceTriggerEnabled:
keywordAnalyzersNDAPI
setKeywordAnalyzersNDAPI:
hasTriggerPending
setHasTriggerPending:
firstPassThreshold
setFirstPassThreshold:
_voiceTriggerEnabled
_hasTriggerPending
_firstPassThreshold
_voiceTriggerStartPolicy
_keywordAnalyzersNDAPI
setDelayInterstitialSounds:level:
inputRecordingSampleByteDepth
inputRecordingFramesPerPacket
inputRecordingSampleRateNarrowBand
inputRecordingBytesPerFrame
inputRecordingBytesPerPacket
inputRecordingNumberOfChannels
inputRecordingDurationInSecs
inputRecordingSampleBitDepth
inputRecordingEncoderAudioQuality
inputRecordingSampleRateConverterAlgorithm
inputRecordingBufferDuration
audioConverterBitrate
channelForOutputReference
zeroFilterWindowSizeInMs
zeroFilterApproxAbsSpeechThreshold
csAudioProcessingQueuePriority
UTF8String
weakObjectsHashTable
_createListenPollingTimer
_setupStateMachine
_setupCircularBuffer
defaultCenter
mediaserverdDidRestart
addObserver:selector:name:object:
_setupVoiceTrigger
sharedInstance
addObserver:
removeObserver:
supportContinuousVoiceTrigger
shouldRunVTOnCS
defaultFallBackAssetForVoiceTrigger
_getVoiceTriggerAsset
isEqualToString:
initWithAudioBuffer:
registerObserver:
supportSelfTriggerSuppression
supportKeywordDetector
initWithNumChannels:recordingDuration:samplingRate:
initWithInitialState:
addTransitionFrom:to:for:
currentState
_notifyEvent:
_eventName:
performTransitionForEvent:
audioRecorder
initWithContext:error:
setAudioRecorder:
isRecording
setCurrentContext:error:
_setRecordMode:error:
prepareRecordWithSettings:error:
prewarmAudioSession
recordRoute
recordSettings
isNarrowBand
_createRecorderWithContextIfNeeded:error:
_prepareRecorderWithSettings:error:
startRecordingWithSettings:error:
releaseClientAudioSession:
_releaseAudioSessionForListening:error:
_releaseClientAudioSession:
releaseAudioSession:
sampleCount
_startRecordingWithSettings:error:
_reinitializeVoiceTriggerIfNeeded
_startRecordingForClient:error:
unsignedIntegerValue
supportOpportunisticZLL
unsignedLongLongValue
sampleCountFromHostTime:
stopRecording
_stateName:
_stopFirstPassVoiceTrigger
_stopSelfTriggerDetector
_stopSecondPassVoiceTrigger
_stopForwardingToClient
_stopContinuousVoiceTrigger
_stopKeywordDetector
_startFirstPassVoiceTrigger
_startSelfTriggerDetector
_startSecondPassVoiceTrigger
_startForwardingToClient
_startContinuousVoiceTrigger
_startKeywordDetector
_startListenPolling
voiceTriggerRecordContext
lpcmRecordSettings
_prepareListenWithSettings:error:
_startListening:
_stopListenPolling
removeObject:
containsObject:
setIsContinuousRunningMode:
_getClientRecordContext
isRecordContextVoiceTrigger:
voiceTriggerInfo
startDetectKeyword:
addSamples:numSamples:atHostTime:
notifyEvent:
bufferLength
copySamplesFrom:to:
hostTimeFromSampleCount:
_reinitializeVoiceTriggerWithAsset:
isEqualAsset:
numberWithInteger:
stringWithFormat:
audioRecorderBufferAvailable:buffer:atTime:
audioRecorderBufferAvailable:buffer:
audioRecorderDidStartRecording:successfully:error:
audioRecorderDidStopRecording:forReason:
audioRecorderRecordHardwareConfigurationDidChange:toConfiguration:
audioRecorderBeginRecordInterruption:
audioRecorderBeginRecordInterruption:withContext:
audioRecorderEndRecordInterruption:
voiceTriggerDetectedOnAOP:
didTransitFrom:to:by:
didIgnoreEvent:from:
voiceTriggerDidDetectNearMiss:
voiceTriggerDidDetectSpeakerReject:
keywordDetectorDidDetectKeyword
CSAssetManagerDidDownloadNewAsset:
CSLanguageCodeUpdateMonitor:didReceiveLanguageCodeChanged:
initWithVoiceTriggerFirstPass:voicetriggerSecondPass:voicetriggerEventNotifier:audioRecorder:
registerSpeechController:
getCurrentState
isClientRecording
setClientContext:error:
prepareRecordingForClient:error:
_setCurrentContext:error:
releaseClientAudioSession
assetQueryQueue
setAssetQueryQueue:
stateMachine
setStateMachine:
audioBuffer
setAudioBuffer:
currentVoiceTriggerAsset
setCurrentVoiceTriggerAsset:
voiceTriggerFirstPass
setVoiceTriggerFirstPass:
voiceTriggerSecondPass
setVoiceTriggerSecondPass:
clientController
setClientController:
voiceTriggerEventNotifier
setVoiceTriggerEventNotifier:
voiceTriggerFileLogger
setVoiceTriggerFileLogger:
selfTriggerDetector
setSelfTriggerDetector:
continuousVoiceTrigger
setContinuousVoiceTrigger:
keywordDetector
setKeywordDetector:
myriad
setMyriad:
voiceTriggerFidesClient
setVoiceTriggerFidesClient:
activeAudioProcessors
setActiveAudioProcessors:
lastForwardedSampleCount
setLastForwardedSampleCount:
secondPassStartSampleCount
setSecondPassStartSampleCount:
clientStartSampleCount
setClientStartSampleCount:
recordingPendingTimeout
setRecordingPendingTimeout:
lastVoiceTriggerEventInfo
setLastVoiceTriggerEventInfo:
listenPollingTimer
setListenPollingTimer:
_audioRecorder
_assetQueryQueue
_stateMachine
_audioBuffer
_currentVoiceTriggerAsset
_voiceTriggerFirstPass
_voiceTriggerSecondPass
_clientController
_voiceTriggerEventNotifier
_voiceTriggerFileLogger
_selfTriggerDetector
_continuousVoiceTrigger
_keywordDetector
_myriad
_voiceTriggerFidesClient
_activeAudioProcessors
_lastForwardedSampleCount
_secondPassStartSampleCount
_clientStartSampleCount
_recordingPendingTimeout
_lastVoiceTriggerEventInfo
_listenPollingTimer
lowercaseString
hasPrefix:
substringFromIndex:
regularExpressionWithPattern:options:error:
mutableCopy
replaceMatchesInString:options:range:withTemplate:
_stringByStrippingLeadingNoise:
_stringByStrippingTrailingNoise:
rangeOfString:options:
numberOfMatchesInString:options:range:
stringValue
_firstMatchesForRegularExpression:
firstMatchInString:options:range:
numberOfRanges
rangeAtIndex:
substringWithRange:
_stringByFixingNamePattern:
_stringByStrippingNoiseLeadingNoise:TrailingNoise:
_hasSubstring:
_matchesRegularExpression:
_caseInsensitiveHasMatchInEnumeration:
_firstMatchesForRegularExpressions:
getVoiceTriggerAssetTypeString
getEndpointAssetTypeString
dictionary
_fetchRemoteMetaData
assetOfType:language:
installedAssetOfType:language:
_isReadyToUse
predicateForAssetType:language:
installedAssetOfType:withPredicate:
_fetchRemoteAssetOfType:withPredicate:
_installedAssetOfType:withPredicate:
getCSAssetOfType:
_assetQueryForAssetType:withPredicate:localOnly:
runQueryAndReturnError:
predicate
_findLatestInstalledAsset:
state
isLatestCompareTo:
initWithAssetType:
setPredicate:
setQueriesLocalAssetInformationOnly:
startQuery:
isSpringboardStarted
isFirstUnlocked
predicateForfetchRemoteMetadataForAssetType:
_runAssetQuery:completion:
_updateFromRemoteToLocalAssets:forAssetType:
isInstalled
isDownloading
cancelDownloadAndReturnError:
path
purgeAndReturnError:
_downloadAsset:withComplete:
_startDownloadingAsset:progress:completion:
objectForKey:
setProgressHandler:
requiredDiskSpaceIsAvailable:error:
_defaultDownloadOptions
beginDownloadWithOptions:
resumeDownload:
adjustDownloadOptions:completion:
setObject:forKeyedSubscript:
sharedManager
assetForCurrentLanguageOfType:
CSVoiceTriggerAssetMetaUpdateMonitor:didReceiveNewVoiceTriggerAssetMetaData:
CSSpeechEndpointAssetMetaUpdateMonitor:didReceiveNewSpeechEndpointAssetMetaData:
installedAssetForCurrentLanguageOfType:
currentLanguageCode
addObserver:forAssetType:
removeObserver:forAssetType:
_csAssetsDictionary
_enablePolicy
_currentLanguageCode
_observers
getVoiceTriggerAssetCurrentCompatibilityVersion
getEndpointAssetCurrentCompatibilityVersion
supportPremiumAssets
componentsJoinedByString:
predicateWithFormat:argumentArray:
getStringForKey:category:default:
getNumberForKey:category:default:
setupPhraseSpotter
startMasterTimerWithTimeout:
resultAlreadyReported
stopMasterTimer
closeSessionWithStatus:successfully:complete:
closeSessionWithCompletion:
updateMeterAndForward
pushAudioInputIntoPCMBuffer:
requestTriggeredUtterance:
sharedTrainer
trainUtterance:languageCode:
setupSpeechRecognitionTaskWithVoiceTriggerEventInfo:
trimBeginingOfPCMBufferWithVoiceTriggerEventInfo:
computeRequiredTrailingSamples
feedSpeechRecognitionWithPCMBuffer
closeSessionWithStatus:successfully:
handleAudioBufferForVTWithAudioInput:withDetectedBlock:
finishSpeechRecognitionTask
feedSpeechRecognitionTrailingSamplesWithCompletedBlock:
triggeredUtterance:
updateMeters
averagePower
CSVTUITrainingSessionRMSAvailable:
analyze:
boolValue
numSamplesInPCMBuffer
objectAtIndex:
appendAudioPCMBuffer:
removeObjectAtIndex:
frameLength
initWithCommonFormat:sampleRate:channels:interleaved:
initWithPCMFormat:frameCapacity:
mutableAudioBufferList
setFrameLength:
replaceObjectAtIndex:withObject:
removeObjectsInRange:
createAVAudioPCMBufferWithNSData:
handleAudioInput:
sharedGrammars
getLMEforLocale:
setContextualStrings:
setTaskHint:
setObject:forKey:
_setVoiceTriggerEventInfo:
recognitionTaskWithRequest:delegate:
endAudio
finish
handleMasterTimeout:
scheduledTimerWithTimeInterval:target:selector:userInfo:repeats:
invalidate
formattedString
whitespaceAndNewlineCharacterSet
stringByTrimmingCharactersInSet:
speechRecognitionDidDetectSpeech:
speechRecognitionTask:didHypothesizeTranscription:
speechRecognitionTask:didFinishRecognition:
speechRecognitionTaskFinishedReadingAudio:
speechRecognitionTaskWasCancelled:
speechRecognitionTask:didFinishSuccessfully:
audioSessionDidStartRecording:error:
audioSessionDidStopRecording:
audioSessionRecordBufferAvailable:
audioSessionUnsupportedAudioRoute
audioSessionErrorDidOccur:
didDetectBeginOfSpeech
didDetectEndOfSpeech:
initWithUtteranceId:sessionNumber:Locale:audioSession:keywordDetector:speechRecognizer:speechRecognitionRequest:sessionDelegate:sessionDispatchQueue:completion:
startTraining
suspendTraining
resumeTraining
_status
_utteranceId
_sessionNumber
_locale
_audioSession
_speechRecognizer
_speechRecognitionRequest
_speechRecognitionTask
_masterTimer
_pcmBufArray
_resultReported
_sessionProcess
_sessionSuspended
_ASRErrorOccured
_sessionDelegate
_trainingCompletion
_numRequiredTrailingSamples
_numTrailingSamples
initWithManager:
getFixedHighPrioritySerialQueueWithLabel:
twoShotNotificationEnabled
_contextToString:
_getRecordSettings
unsignedIntValue
_setupDownsamplerIfNeeded
_setupAudioConverter:
duckOthersOption
setDuckOthersOption:
_isVoiceTriggered
_currentAudioRecorderSampleRate
resetForNewRequestWithSampleRate:
fileLoggingIsEnabled
assistantAudioFileLogDirectory
setDateFormat:
stringFromDate:
URLWithString:
lpcmNonInterleavedASBD
lpcmInterleavedASBD
initWithURL:inputFormat:outputFormat:
dataForChannel:
sampleByteDepth
startSampleCount
hostTime
initWithData:numChannels:numSamples:sampleByteDepth:startSampleCount:hostTime:
processAudioSamplesAsynchronously:
addSamples:timestamp:
speechControllerLPCMRecordBufferAvailable:buffer:
data
addSamples:len:
channels
packetDescriptionCount
bytesDataSize
initWithCapacity:
packetDescriptions
initWithBytes:length:
timeStamp
speechControllerRecordBufferAvailable:buffers:recordedAt:
speechControllerDidStartRecording:successfully:error:
recordingStoppedForReason:
flush
speechControllerDidStopRecording:forReason:
close
speechControllerRecordHardwareConfigurationDidChange:toConfiguration:
speechControllerBeginRecordInterruption:
speechControllerBeginRecordInterruption:withContext:
speechControllerEndRecordInterruption:
narrowBandOpusConverter
opusConverter
setAlertSoundFromURL:forType:
playAlertSoundForType:
alertStartTime
playRecordStartingAlertAndResetEndpointer
setMeteringEnabled:
peakPowerForChannel:
averagePowerForChannel:
passThruVoiceTriggerInfo
resetForVoiceTriggerTwoShotWithSampleRate:
speechControllerDidDetectVoiceTriggerTwoShot:atTime:
speechControllerRequestsOperation:forReason:
metrics
setEndpointerDelegate:
processServerEndpointFeatures:
numberWithInt:
numberWithUnsignedInt:
lastEndOfVoiceActivityTime
endpointerModelVersion
updateEndpointerThreshold:
updateEndpointerDelayedTrigger:
shouldAcceptEagerResultForDuration:resultsCompletionHandler:
sharedController
audioConverterDidConvertPackets:packets:timestamp:
initializeRecordSessionWithContext:
preheat
resetAudioSession
releaseAudioSession
getLPCMAudioStreamBasicDescription
setSynchronousCallbackEnabled:
setRecordBufferDuration:
getRecordBufferDuration
startRecording:
isVoiceTriggered
peakPowerForOutputReference
averagePowerForOutputReference
outputReferenceChannel
endpointAnalyzer
setEndpointAnalyzerDelegate:
resetEndpointer
endpointerProxy
setEndpointerProxy:
avvcContext
setAvvcContext:
isOpus
setIsOpus:
isActivated
setIsActivated:
setIsNarrowBand:
audioFileWriter
setAudioFileWriter:
activeChannel
setTwoShotNotificationEnabled:
_opusAudioConverter
_narrowBandOpusConverter
_audioConverter
_downsampler
_requestedRecordSettings
_lastVoiceTriggerInfo
_isOpus
_isActivated
_isNarrowBand
_twoShotNotificationEnabled
_endpointerProxy
_avvcContext
_audioFileWriter
_activeChannel
_storeModeEnabled
setFileLoggingLevel:
fileLoggingLevel
intValue
baseDir
assistantLogDirectory
arrayWithObjects:count:
enumeratorAtURL:includingPropertiesForKeys:options:errorHandler:
getResourceValue:forKey:error:
lastPathComponent
getUserVoiceProfileUploadPathWithEnrolledLanguageList:
_CSSATUploadPath
_getEnrolledLanguageList
enumeratorAtPath:
_isDirectory:
pathExtension
copyItemAtPath:toPath:error:
_CSSATUpdatePath
_markSATEnrollmentSuccessForLanguageCode:
_markSATEnrollmentMigratedForLanguageCode:
_markSATEnrollmentWithMarker:forLanguage:
createFileAtPath:contents:attributes:
initialize
voiceTriggerInCoreSpeech
setFileLoggingIsEnabled:
voiceTriggerAudioLogDirectory
getUserVoiceProfileFileList
getUserVoiceProfileUploadPath
notifyUserVoiceProfileUploadComplete
getUserVoiceProfileUpdateDirectory
notifyUserVoiceProfileUpdateReady
integerValue
initialState
setInitialState:
transitions
setTransitions:
_currentState
_initialState
_transitions
_stopMonitoring
_startMonitoringWithQueue:
enumerateObservers:
CSEventMonitorDidReceiveEvent:
enumerateObserversInQueue:
notifyObserver:
utteranceFileASBD
_closeAudioFile
fileURLWithPath:isDirectory:
startRecording
appendAudioData:
_audioFile
_asbd
_url
_audioLength
date
_audioLogDirectory
_timeStampString
dataWithJSONObject:options:error:
writeToFile:atomically:
_metaFilenameWithPrefix:
stringByReplacingOccurrencesOfString:withString:
saveRecordingBufferFrom:to:toURL:
_writeDictionary:toPath:
VTSecondPassExtraSamplesAtStart
VTSecondPassConfigPathNDAPI
VTSecondPassThreshold
VTSecondPass2ndChanceThreshold
VTSecondPassLoggingThreshold
VTSecondPassConfigPathRecognizer
VTSecondPassUserKeywordSpotting
VTSecondPassRecognizerThresholdOffset
VTSecondPassRecognizerScoreScaleFactor
VTSecondPassRecognizerToken
_addVoiceTriggerEnabledConditions
_subscribeEventMonitors
subscribeEventMonitor:
addConditions:
_didReceiveNewSpeechEndpointAssetMetaData
_notifyObserver:
_notifyToken
opusRecordSettings
alertMuteSettings
pHash:length:
signalEstimate
dataWithCapacity:
appendBytes:length:
_generateMyriadInfo:score:
setSignalEstimate:
_signalEstimate
initWithBundleIdentifier:
isPermitted
shouldMakeRecordWithFrequency:
_lastTriggerDataWithResult:
_fidesRecordInfo
addEntriesFromDictionary:
UUIDString
saveRecordWithData:recordInfo:completion:
_logDESRecordWithType:result:
numChannels
numberWithUnsignedLongLong:
matchWithString:TrailingStr:LeadingStr:Pattern:
createGrammars
bundleForClass:
bundlePath
dataWithContentsOfFile:
JSONObjectWithData:options:error:
_getTrailingPatternsWithGrammars:withLocale:
_getLeadingPatternsWithGrammars:withLocale:
_getRegexPatternsWithGrammars:withUtt:withLocale:
_getLMEWithGrammar:withLocale:
URLSession:didBecomeInvalidWithError:
URLSession:didReceiveChallenge:completionHandler:
URLSessionDidFinishEventsForBackgroundURLSession:
getTrailingPatternsForUtt:Locale:
getLeadingPatternsForUtt:Locale:
getRegexPatternsForUtt:Locale:
_grammar
initWithLocaleIdentifier:withAudioSession:
setLocaleIdentifier:
createKeywordDetector
initWithLanguageCode:
localeWithLocaleIdentifier:
initWithLocale:
_stopAudioSession
destroySpeakerTrainer
_destroyAudioSession
_setupAudioSession
closeSessionBeforeStartWithStatus:successfully:withCompletion:
_createAudioAnalyzer
_shouldShowHeadsetDisconnectionMessage
_startAudioSession
createSpeechRecognizer
sharedtrainingSessionQueue
_audioSource
audioSource
prepareRecord
hasCorrectAudioRoute
VTUITrainingManagerFeedLevel:
resetEndPointer
VTUITrainingManagerStopListening
trainingManagerWithLocaleID:
CSVTUITrainingSessionStopListen
endpointer:didDetectStartpointAtTime:
endpointer:didDetectHardEndpointAtTime:withMetrics:
_beginOfSpeechDetected
_endOfSpeechDetected
cleanupWithCompletion:
trainUtterance:shouldUseASR:completion:
cancelTrainingForID:
suspendAudio
setSuspendAudio:
startRMS
stopRMS
shouldPerformRMS
didDetectForceEndPoint
VTUITrainingSessionStopListen
setRms:
speechRecognizerAvailable
_performRMS
_audioAnalyzer
_trainingSessions
_currentTrainingSession
_suspendAudio
_cleanupCompletion
_speechRecognizerAvailable
_rms
rootQueueWithFixedPriority:
hasRemoteCoreSpeech
getFixedPrioritySerialQueueWithLabel:fixedPriority:
_resetStartAnalyzeTime
resetBest
_setStartAnalyzeTime:
analyzeWavData:numSamples:
getAnalyzedResultForPhraseId:
sampleFed
getAnalyzedResult
initWithResult:
bestStart
setBestStart:
bestEnd
setBestEnd:
getSuperVectorWithEndPoint:
activePhraseId
_novDetector
_startAnalyzeSampleCount
_isStartSampleCountMarked
_lastSampleFed
_sampleFedWrapAroundOffset
_activePhraseId
initWithTotalAudioRecorded:featuresAtEndpoint:endpointerType:serverFeatureLatencyDistribution:additionalMetrics:
totalAudioRecorded
setTotalAudioRecorded:
featuresAtEndpoint
setFeaturesAtEndpoint:
endpointerType
setEndpointerType:
serverFeatureLatencyDistribution
setServerFeatureLatencyDistribution:
additionalMetrics
setAdditionalMetrics:
_totalAudioRecorded
_featuresAtEndpoint
_endpointerType
_serverFeatureLatencyDistribution
_additionalMetrics
localURL
string
_compatibilityVersion
appendString:
_version
appendFormat:
_footprint
assetForAssetType:resourcePath:configVersion:
attributes
isPremium
lpcmInt16ASBD
lpcmInt16NarrowBandASBD
opusASBD
opusNarrowBandASBD
aiffFileASBD
_checkVoiceTriggerEnabled
_didReceiveVoiceTriggerSettingChanged:
_notifyObserver:withEnabled:
CSVoiceTriggerEnabledMonitor:didReceiveEnabled:
_didReceiveVoiceTriggerSettingChangedInQueue:
_isVoiceTriggerEnabled
keywordDetectorThreshold
keywordDetectorConfigPathRecognizer
keywordDetectorWaitTimeSinceVT
initWithAsset:languageCode:speakerModel:
subdataWithRange:
analyzeWavForEnrollment:numSamples:
addLastTriggerToProfile
_saveUtterance:meta:to:
stringByAppendingString:
voiceController
setStopOnEndpointEnabled:
setRecordEndpointMode:
setRecordDelegate:
setPlaybackDelegate:
numberWithDouble:
playbackRoute
_hasInputAudioRoute
_hasCorrectInputAudioRoute
_hasCorrectOutputAudioRoute
convertStopReason:
hasAudioRoute
voiceControllerDidStartRecording:successfully:
voiceControllerDidStartRecording:successfully:error:
voiceControllerDidStopRecording:forReason:
voiceControllerDidDetectStartpoint:
voiceControllerDidDetectEndpoint:ofType:
voiceControllerDidDetectEndpoint:ofType:atTime:
voiceControllerEncoderErrorDidOccur:error:
voiceControllerLPCMRecordBufferAvailable:buffer:
voiceControllerRecordHardwareConfigurationDidChange:toConfiguration:
voiceControllerBeginRecordInterruption:
voiceControllerBeginRecordInterruption:withContext:
voiceControllerEndRecordInterruption:
voiceControllerMediaServicesWereLost:
voiceControllerMediaServicesWereReset:
voiceControllerRecordBufferAvailable:buffer:
voiceControllerPlaybackBufferAvailable:buffer:
voiceControllerDidStartPlaying:successfully:
voiceControllerDidStopPlaying:forReason:
voiceControllerDecoderErrorDidOccur:error:
voiceControllerPlaybackHardwareConfigurationDidChange:toConfiguration:
voiceControllerBeginPlaybackInterruption:
voiceControllerEndPlaybackInterruption:
_voiceController
dataWithBytes:length:
beepCancellerDidCancelSamples:buffer:timestamp:
cancelBeepFromSamples:timestamp:
willBeep
.cxx_construct
_beepCanceller
_beepFloatVec
_floatBuffer
_shortBuffer
triggerConfidence
initWithConfigPath:triggerTokens:useKeywordSpotting:
_notifySecondPassReject
runRecognition
configVersion
processSuperVector:withResult:
_analyzeForKeywordDetection:result:forChannel:
speakerDetector:didDetectSpeaker:
speakerDetector:didDetectSpeakerReject:
keywordAnalyzerQuasar:hasResultAvailable:forChannel:
setBypassForSeconds:
keywordAnalyzerNDAPI
setKeywordAnalyzerNDAPI:
keywordAnalyzerQuasar
setKeywordAnalyzerQuasar:
speakerDetector
setSpeakerDetector:
speakerModel
setSpeakerModel:
secondPassTimeout
setSecondPassTimeout:
numAnalyzedSamples
setNumAnalyzedSamples:
keywordLoggingThreshold
setKeywordLoggingThreshold:
lastScore
setLastScore:
extraSamplesAtStart
setExtraSamplesAtStart:
useSAT
setUseSAT:
nearMissDelayTimeout
setNearMissDelayTimeout:
nearMissCandidateDetectedSamples
setNearMissCandidateDetectedSamples:
hasPendingNearMiss
setHasPendingNearMiss:
lastAnalyzerResult
setLastAnalyzerResult:
numBypassSamples
setNumBypassSamples:
recognizerScore
setRecognizerScore:
isRunningRecognizer
setIsRunningRecognizer:
recognizerResultPending
setRecognizerResultPending:
recognizerScoreScaleFactor
setRecognizerScoreScaleFactor:
earlyDetectFiredMachTime
setEarlyDetectFiredMachTime:
_useSAT
_hasPendingNearMiss
_isRunningRecognizer
_recognizerResultPending
_keywordLoggingThreshold
_lastScore
_recognizerScore
_recognizerScoreScaleFactor
_keywordAnalyzerNDAPI
_keywordAnalyzerQuasar
_speakerDetector
_speakerModel
_secondPassTimeout
_numAnalyzedSamples
_extraSamplesAtStart
_nearMissDelayTimeout
_nearMissCandidateDetectedSamples
_lastAnalyzerResult
_numBypassSamples
_earlyDetectFiredMachTime
_notifyTriggerEvent:
_notifyNearMissEvent:
_notifySpeakerReject:
_notifyTwoShotDetectionAt:
_notifyKeywordDetect
unregisterObserver:
isContinuousRunningMode
_isContinuousRunningMode
numberWithUnsignedLong:
initWithZeroWindowSize:approxAbsSpeechThreshold:numHostTicksPerAudioSample:
filterZerosInAudioPacket:atBufferHostTime:filteredPacket:
endAudioAndFetchAnyTrailingZerosPacket:
_audioZeroFilterImpl
readAudioChunksFrom:block:
_addAssetManagerEnabledConditions
sampleCountFromHostTime:anchorHostTime:anchorSampleCount:
hostTimeFromSampleCount:anchorHostTime:anchorSampleCount:
stringWithCString:encoding:
createAudioCircularBufferWithDefaultSettings
addSamples:numSamples:
copySamplesFromHostTime:
copyBufferWithNumSamplesCopiedIn:
setBufferLength:
_csAudioCircularBufferImpl
_anchorSampleCount
_anchorHostTime
_bufferLength
_asssetMetaUpdatedKey
_didReceiveNewVoiceTriggerAssetMetaData
speakerDetectorNDAPIConfigPath
speakerDetectorThreshold
speakerDetectorRetrainTriggerThreshold
initWithWordCount:trailingSilenceDuration:eosLikelihood:pauseCounts:silencePosterior:taskName:processedAudioDurationInMilliseconds:
initWithWordCount:trailingSilenceFrames:endOfSilenceLikelihood:pauseCounts:silencePosterior:taskName:
wordCount
setWordCount:
trailingSilenceDuration
setTrailingSilenceDuration:
eosLikelihood
setEosLikelihood:
pauseCounts
setPauseCounts:
silencePosterior
setSilencePosterior:
processedAudioDurationInMilliseconds
setProcessedAudioDurationInMilliseconds:
taskName
setTaskName:
_wordCount
_trailingSilenceDuration
_eosLikelihood
_pauseCounts
_silencePosterior
_processedAudioDurationInMilliseconds
_taskName
_configureWithASBD:andFrameRate:
_resetWithSampleRate:
_configureWithSampleRate:andFrameRate:
_processAudioSamples:
_detectVoiceActivityInSamples:numSamples:
replaceBytesInRange:withBytes:length:
_getEndpointMetricsForAudioTimestamp:
endpointStyle
setEndpointStyle:
delay
setDelay:
startWaitTime
setStartWaitTime:
automaticEndpointingSuspensionEndTime
setAutomaticEndpointingSuspensionEndTime:
minimumDurationForEndpointer
setMinimumDurationForEndpointer:
lastStartOfVoiceActivityTime
bypassSamples
setBypassSamples:
endpointMode
setEndpointMode:
interspeechWaitTime
setInterspeechWaitTime:
endWaitTime
setEndWaitTime:
saveSamplesSeenInReset
setSaveSamplesSeenInReset:
canProcessCurrentRequest
handleVoiceTriggerWithActivationInfo:
sampleRate
setSampleRate:
frameRate
setFrameRate:
detectedOneShotStartpoint
setDetectedOneShotStartpoint:
detectedRecurrentStartpoint
setDetectedRecurrentStartpoint:
communicatedStartPointDetection
setCommunicatedStartPointDetection:
detectedOneShotEndpoint
setDetectedOneShotEndpoint:
detectedRecurrentEndpoint
setDetectedRecurrentEndpoint:
communicatedEndpointDetection
setCommunicatedEndpointDetection:
samplesSeen
setSamplesSeen:
numSamplesProcessed
setNumSamplesProcessed:
lastOneShotStartpoint
setLastOneShotStartpoint:
lastOneShotEndpoint
setLastOneShotEndpoint:
lastRecurrentStartpoint
setLastRecurrentStartpoint:
lastRecurrentEndpoint
setLastRecurrentEndpoint:
floatSampleBuffer
setFloatSampleBuffer:
topLevelParameterDict
setTopLevelParameterDict:
modelDictPath
setModelDictPath:
isConfigured
setIsConfigured:
previousSamplesSeen
setPreviousSamplesSeen:
apQueue
setApQueue:
recordingDidStop
setRecordingDidStop:
vtEndInSampleCount
setVtEndInSampleCount:
_audioUnitEPVAD2
_saveSamplesSeenInReset
_detectedOneShotStartpoint
_detectedRecurrentStartpoint
_communicatedStartPointDetection
_detectedOneShotEndpoint
_detectedRecurrentEndpoint
_communicatedEndpointDetection
_isConfigured
_recordingDidStop
_frameRate
_endpointStyle
_endpointMode
_interspeechWaitTime
_startWaitTime
_endWaitTime
_automaticEndpointingSuspensionEndTime
_minimumDurationForEndpointer
_bypassSamples
_delay
_sampleRate
_samplesSeen
_numSamplesProcessed
_lastOneShotStartpoint
_lastOneShotEndpoint
_lastRecurrentStartpoint
_lastRecurrentEndpoint
_floatSampleBuffer
_topLevelParameterDict
_modelDictPath
_previousSamplesSeen
_apQueue
_vtEndInSampleCount
copy
dictionaryWithContentsOfFile:
appendData:
_data
_numChannels
_numSamples
_sampleByteDepth
_startSampleCount
_hostTime
bestPhrase
bestScore
earlyWarning
setSampleFed:
setBestPhrase:
setBestScore:
setEarlyWarning:
isRescoring
setIsRescoring:
_earlyWarning
_isRescoring
_bestScore
_sampleFed
_bestPhrase
_bestStart
_bestEnd
isAvailable
_firedVoiceTriggerTimeout
shouldHandleSession
shouldMatchPayload
_firedEndPointTimeout
_registerVoiceTriggerTimeout
_reportStopListening
_registerEndPointTimeout
matchRecognitionResult:withMatchedBlock:withNonMatchedBlock:
bestTranscription
_detectBOS
_ASRResultReceived
_reportedStopListening
initWithConfiguration:modelVersion:
addAudio:numSamples:
updateEndpointerThresholdWithValue:
updateEndpointerDelayedTriggerSwitch:
timeIntervalSinceDate:
silenceFramesCountMs
silenceProbability
silenceDurationMs
initWithWordCount:trailingSilenceDuration:endOfSentenceLikelihood:pauseCounts:silencePosterior:clientSilenceFramesCountMs:clientSilenceProbability:silencePosteriorNF:serverFeaturesLatency:eagerResultEndTime:
acceptEagerResultWithFeatures:featuresToLog:
processedAudioMs
defaultServerEndpointFeatures
endOfSentenceLikelihood
initWithWordCount:trailingSilenceDuration:endOfSentenceLikelihood:pauseCounts:silencePosterior:clientSilenceFramesCountMs:clientSilenceProbability:silencePosteriorNF:serverFeaturesLatency:
didEndpointWithFeatures:audioTimestamp:featuresToLog:endpointPosterior:extraDelayMs:
getFrameDurationMs
serverFeaturesLatencyDistributionDictionary
dictionaryWithDictionary:
sortUsingComparator:
objectAtIndexedSubscript:
initWithConfigFile:samplingRate:queue:
initWithSilenceFramesCountMs:silenceProbability:silenceDurationMs:silencePosterior:processedAudioMs:
isHybridEndpointerAvailableForCurrentPlatform
requestSupportedWithSamplingRate:
_getCSHybridEndpointerConfigForAsset:
_updateAssetWithLanguage:
_updateAssetWithCurrentLanguage
clientSilenceFeaturesAvailable:
silenceDurationEstimateAvailable:numEstimates:clientProcessedAudioMs:
setCanProcessCurrentRequest:
didAddAudio
setDidAddAudio:
caesuraSPG
setCaesuraSPG:
clientSilenceFeaturesAtEndpoint
setClientSilenceFeaturesAtEndpoint:
hybridClassifier
setHybridClassifier:
setEndpointerModelVersion:
serverFeaturesQueue
setServerFeaturesQueue:
lastKnownServerEPFeatures
setLastKnownServerEPFeatures:
serverFeatureLatencies
setServerFeatureLatencies:
serverFeaturesWarmupLatency
setServerFeaturesWarmupLatency:
lastServerFeatureTimestamp
setLastServerFeatureTimestamp:
didReceiveServerFeatures
setDidReceiveServerFeatures:
clientLagThresholdMs
setClientLagThresholdMs:
clampedSFLatencyMsForClientLag
setClampedSFLatencyMsForClientLag:
useDefaultServerFeaturesOnClientLag
setUseDefaultServerFeaturesOnClientLag:
hybridClassifierQueue
setHybridClassifierQueue:
lastReportedEndpointTimeMs
setLastReportedEndpointTimeMs:
stateSerialQueue
setStateSerialQueue:
didCommunicateEndpoint
setDidCommunicateEndpoint:
currentRequestSampleRate
setCurrentRequestSampleRate:
vtExtraAudioAtStartInMs
setVtExtraAudioAtStartInMs:
hepAudioOriginInMs
setHepAudioOriginInMs:
firstAudioPacketTimestamp
setFirstAudioPacketTimestamp:
didTimestampFirstAudioPacket
setDidTimestampFirstAudioPacket:
silencePosteriorGeneratorQueue
setSilencePosteriorGeneratorQueue:
_canProcessCurrentRequest
_didAddAudio
_didReceiveServerFeatures
_useDefaultServerFeaturesOnClientLag
_didCommunicateEndpoint
_didTimestampFirstAudioPacket
_caesuraSPG
_clientSilenceFeaturesAtEndpoint
_hybridClassifier
_endpointerModelVersion
_serverFeaturesQueue
_lastKnownServerEPFeatures
_serverFeatureLatencies
_serverFeaturesWarmupLatency
_lastServerFeatureTimestamp
_clientLagThresholdMs
_clampedSFLatencyMsForClientLag
_hybridClassifierQueue
_lastReportedEndpointTimeMs
_stateSerialQueue
_currentRequestSampleRate
_vtExtraAudioAtStartInMs
_hepAudioOriginInMs
_firstAudioPacketTimestamp
_silencePosteriorGeneratorQueue
getHostClockFrequency
secondsToHostTime:
_shouldEnterTwoShotAtEndPointTime:
_shouldUseVAD2ForTwoShot
endpointerDelegate
hybridEndpointer
setHybridEndpointer:
vad2Endpointer
setVad2Endpointer:
activeEndpointer
setActiveEndpointer:
didEnterTwoshot
setDidEnterTwoshot:
_didEnterTwoshot
_endpointerDelegate
_hybridEndpointer
_vad2Endpointer
_activeEndpointer
_checkFirstUnlocked
_notifyObserver:withUnlocked:
CSFirstUnlockMonitor:didReceiveFirstUnlock:
_didReceiveFirstUnlockInQueue:
_didReceiveFirstUnlock:
_firstUnlocked
_checkSpringBoardStarted
_didReceiveSpringboardStarted:
_notifyObserver:withStarted:
CSSpringboardStartMonitor:didReceiveStarted:
_didReceiveSpringboardStartedInQueue:
_isSpringBoardStarted
componentsSeparatedByString:
processInfo
systemUptime
initWithConfiguration:
runRecognitionWithResultStream:
_recognizeWavData:length:
addAudioSamples:count:
tokens
_getConfidence:
firstObject
addObjectsFromArray:
lastObject
confidence
tokenName
enumerateObjectsUsingBlock:
speechRecognizer:didRecognizePartialResult:
speechRecognizer:didFinishRecognitionWithError:
speechRecognizer:didRecognizeFinalResults:
speechRecognizer:didRecognizeFinalResults:tokenSausage:nBestChoices:
speechRecognizer:didRecognizeFinalResultPackage:
speechRecognizer:didProcessAudioDuration:
speechRecognizer:didRecognizeRawEagerRecognitionCandidate:
speechRecognizer:didProduceEndpointFeaturesWithWordCount:trailingSilenceDuration:eosLikelihood:pauseCounts:silencePosterior:processedAudioDurationInMilliseconds:
_previousUtteranceTokens
_triggerTokenList
_recognizer
_recognizerBuffer
_useKeywordSpotting
_triggerConfidence
hybridEndpointerAssetFilename
initWithResourcePath:configFile:configVersion:
fallBackAssetResourcePath
_decodeJson:
assetHashInResourcePath:
getBoolForKey:category:default:
_decodedInfo
_path
_resourcePath
_configVersion
_configureAudioConverter:
_convertBufferedLPCM:allowPartial:timestamp:
_opusConverter
_bufferedLPCM
_recordBasePacketsPerSecond
_opusOutASBD
_convertPacketCount
_convertAudioCapacity
_lastTimestamp
_notifyObserver:withLanguageCode:
_didReceiveLanguageCodeUpdate
_checkAllConditionsEnabled
notifyCallback:
_monitors
_conditions
_callback
outputAudioChannel
setOutputAudioChannel:
_outputAudioChannel
_initializeSAT:
_computeSATScore:
_initializeNDAPI:resourcePath:
_threshold
_spkModel
_retrainTriggerThreshold
_sampleLengthFrom:To:
_lastKeywordScore
_voiceControllerWithContext:error:
_createDeInterleaverIfNeeded
_createSampleRateConverterIfNeeded
_recordingSampleRate
_shouldRunZeroFilter
_resetZeroFilter
sharedAnalytics
logEventWithType:context:
_processAudioChain:atTime:
_processAudioChainWithZeroFiltering:atTime:
enableVoiceTriggerOnAOP:
updateVoiceTriggerAOPModel:
voiceTriggerOccuredNotification:
_samplingRateConvertIfNeeded:
_deinterleaveBufferIfNeeded:
_zeroFilter
_deinterleaver
_interleavedABL
_pNonInterleavedABL
_needSampleRateConversion
decisionWaitSampleCount
setDecisionWaitSampleCount:
_decisionWaitSampleCount
isWriting
fFile
inASBD
outASBD
%s Setting two shot decision mode triggerEndSampleCount = %{public}tu (%{public}.3f), twoShotThreshold = %{public}.3f, twoShotDecisionWaitSamples = %{public}tu (%{public}.3f)
%s Shot: best score = %{public}f for channel = %{public}tu
%s analyzedSampleCount: %{public}tu, checkTime: %{public}tu
%s Entering two shot at %{public}.2f with [score: %{public}.3f > threshold: %{public}.3f]
%s Not entering two shot: [score: %{public}.3f < threshold: %{public}.3f]
%s NDAPI continuous voicetrigger best score = %{public}f for channel = %{public}tu
%s Cannot create SampleRateConverter using AudioConverterNew : %{public}d
%s Cannot set Quality property to audioConverter
%s Cannot set Complexity property to audioConverter
%s Audio resampling done : %tu
%s AudioConverter is sad: 0x%{public}xd
%s Siri language is nil, falling back to %@
%s same name of file exists, this will be removed
%s Creating Directory : %{public}@
%s Creating Directory failed : %{public}@
%s VoiceTrigger start policy changed : %{public}@
%s Initializing first pass Corealis for channel : %{public}tu
%s %tu first pass Corealis were created
%s _voiceTriggerEnabled = %{public}@
%s NDAPI first pass best score = %{public}f for channel = %{public}tu
%s We have a trigger pending, ignore score = %{public}f for channel = %{public}tu
%s CSVoiceTriggerAsset found: %{public}@
%s Cannot find voicetrigger asset from asset manager, let's fallback to asset in the framework
%s speechController = %{public}p
%s event : %{public}@
%s context = %{public}@
%s Creating new CSAudioRecorder with context : %{public}@
%s Cannot create audio recorder : %{public}@
%s It cannot change context because it is recording
%s Cannot change context : %{public}@
%s recordingSettings = %{public}@
%s AVVC already recording, change recording mode to recording mode here.
%s AVVC already recording, This shouldn't be called
%s AVVC Prepare recording failed : %{public}@
%s recordingSettings : %{public}@
%s AVVC already recording, nothing to prepare
%s Not supported in this platform
%s context : %{public}@
%s Cannot prepare since audio recorder was not initialized
%s recordingSettings from CS : %{public}@
%s settings : %{public}@
%s startRecording failed : %{public}@
%s mode : %{public}zd
%s Activate Context = %{public}@
%s setCurrentContext failed : %{public}@
%s sessionOptions : %{public}tu
%s startRecordingBy %{public}@
%s _lastForwardedSampleCounts = %{public}tu, audioBufferSampleCount = %{public}tu
%s Requested startHostTime = %{public}llu, _clientStartSampleCount = %{public}tu
%s Cannot use opportunisticZLL since _clientStartSampleCount is newer than audioBuffer sample count
%s Opportunistic ZLL will be used starting from : %{public}tu
%s Opportunistic ZLL will not be used : %{public}tu
%s Generating fake didStartRecording delegate
%s Generating fake didStopRecording delegate
%s Mediaserverd recovered from crash
%s from:%{public}@ to:%{public}@ by:%{public}@
%s Ignore event(%{public}@) from(%{public}@) since we don't have transition
%s Trying to startListening
%s _createRecorderWithContextIfNeeded failed, it will try again %{public}f seconds later
%s _prepareListenWithSettings failed, it will try again %{public}f seconds later
%s startListening succeed
%s startListening failed, it will try again %{public}f seconds later
%s ignore because lastForwardedSampleCount:%{public}lu, theMostRecentSampleCount:%{public}lu
%s Buffer underrun!!!!, lastForwardedSampleTime:%{public}lu, oldestSampleTimeInBuffer:%{public}lu
%s %{public}@, sucessfully:%{public}@ error:%{public}@
%s forward to speechManagerDidStartForwarding
%s ignore audioRecorderDidStartRecording
%s forReason : %{public}zd
%s forward to speechManagerDidStopForwarding
%s ignore audioRecorderDidStopRecording
%s toConfiguration: %{public}zd
%s context: %{public}@
%s new VoiceTrigger asset downloaded
%s Language Code Changed : %{public}@
%s new asset available, change to new model
%s given asset is same as existing, don't need to reinitialize VoiceTrigger
%s init-_currentLanguageCode: %{public}@
%s Not able to fetch remote meta now, registering for callback
%s CSAssetManager cannot query for nil language
%s Error running asset-query for assetType:%{public}lu, query: %{public}@, predicate: %{public}@, error: %{public}@
%s ::: found %{public}lu assets for assetType=%{public}lu, matching query: %{public}@
%s ::: %{public}s
%s ::: predicate: %{public}@
%s ::: %{public}s; query: %{public}@
%s Error running asset query: error %{public}@
%s ::: Request Fetching RemoteMetaData
%s ::: Request fetching remote asset
%s ::: Fetching remote asset
%s ::: Purging installed asset : %{public}@
%s ::: Request downloading remote asset
%s ::: Start downloading asset
%s ::: download progress: %{public}3.0f%%
%s ::: Error downloading; %{public}@
%s ::: download completed successfully.
%s Attempting to download asset %{public}@
%s Failure resuming paused voice asset %{public}@
%s Asset doesn't need downloading, invoking completion
%s _currentLanguageCode changed: %{public}@
%s ERR: Unknown AssetType: %{public}lu
%s Called with status : %{public}d
%s returning status to UI : %{public}d
%s Already reported status or no callback
%s %{public}s Called
%s Will suspend training
%s Will resume training
%s %{public}s called
%s Decide to delay ending ASR: [%{public}ld] samples
%s Triggered! Event info: %{public}@
%{public}9lld %{public}9lld %{public}9lld
%s analyzing.... score so far: %{public}5.3f
%s feeding tailing: [%{public}ld] samples
%s correctSampleSize:    [%{public}ld]
%s accumSampleSize:      [%{public}ld]
%s startBufferIndex:     [%{public}ld]
%s startBufferSampleSize:[%{public}ld]
%s samplesToBeDeleted:   [%{public}ld]
%s Total Number of buffs:[%{public}ld]
%s Adjusting the start buffer
%s Adjusting the array elements
%s Unsupported
%s Begin of speech detected
%s End of speech detected
%s %{public}s CALLED
%s Master Timeout Fired
%s recognized text = %{public}@
%s Context : %{public}@
%s Resetting CoreSpeech frameworks
%s Ask start recording from: %{public}tu
%s SpeechController to receive data from channel %{public}tu
%s Logging audio file into : %{public}@
%s %{public}@
%s SpeechManager still forwarding audio after didStopForwarding, we shouldn't have this
%s AVVCAudioBuffer contains %{public}d packet descriptions, size %{public}d, channels %{public}d. Ignoring
%s packetCount %{public}d
%s Bad packet length %{public}d. Skipping rest of record buffer.
%s SpeechController is trying to forward encoded audio after didStopForwarding, we shouldn't have this
%s Not available
%s Two shot is detected at time %{public}.3f, should notify? [%{public}@]
%s Requesting QuickStop operation upon detecting keyword
%s Couldn't create speech log directory at path %{public}@ %{public}@
%s Cannot delete existing SATUpload Diretory : %{public}@
%s Cannot create SAT Upload Directory : %{public}@
%s Cannot create directory(%{public}@)
%s Cannot copy file from %{public}@ to %{public}@ : %{public}@
%s PHS update directory already exists, remove before we move forward
%s Failed to delete PHS update directory
%s Failed to create PHS update directory
%s We need SAT directory, deleting the file with same name first
%s Failed to get device hash list %{public}@
%s Processing sync data from device hash: %{public}@
%s Error to copy profile from %{public}@ to %{public}@, error: %{public}@
%s Enabling VoiceTrigger Upon VoiceProfile sync for language: %@
%s VoiceTrigger does not exist for this platform, not setting VoiceTriggerEnabled
%s Sucessfully migrated language %{public}@
%s Migrated language %{public}@ but failed to mark SAT enrollment
%s Sucessfully marked as migrated for language : %{public}@
%s Failed to mark migrated for language : %{public}@
%s Failed to remove update path [%{public}@] upon migration completion, error: %{public}@
%s Coudn't mark SAT enrollment %{public}@ at path %{public}@
%s Marked SAT enrollment %{public}@ at path %{public}@
%s We can't mark SAT {public}%@ when there is no audio directory
%s Failure disposing audio file %{public}d
%s Audio file already configured, closing first
%s Creating audio file at URL %{public}@
%s Failed creating audio file at url %{public}@ %{public}d
%s Error setting input format %{public}d
%s No audio file to append data
%s Failed writing audio file %{public}d
%s Closing file at URL %{public}@, audio size: %{public}u
%s Couldn't create voice trigger audio logging directory at path %{public}@ %{public}@
%s Error writing out event info meta: %{public}@
%s Start monitoring : speech endpoint asset meta update
%s Stop monitoring : speech endpoint asset meta update
%s New speech endpoint asset is available
%s could not allocate %{public}d bytes for %{public}@
%s begin zerocross vad, lentotal = %{public}d
%s ran out of buffer, no voice activity
%s sigsum = %{public}f sigNorm= %{public}d
%s vad offset = %{public}d, lentotal = %{public}d
%s vad could not find a start offset %{public}d > %{public}d - %{public}d
%s BTLE AudioPayload ringBuffer startpoint: %{public}lld samplesAvail: %{public}lu
%s BTLE raw audio size = %{public}ld
%s BTLE padded %{public}ld samples to fill out buffer
%s Advert data: %{public}@
%s advert data write failed
%s Posted siri audio hash notification
%s Skipping DES record creation
%s Failed to create DES record: %{public}@
%s Created DES record with identifier: %{public}@
%s Fides trigger (trigger): %{public}@
%s Fides trigger (near-miss): %{public}@
%s Locale: [%{public}@]
%s No locale set when creating phrase spotter.
%s Creation of Keyword Detector failed.
%s %{public}s async called
%s Called before completion called
%s BEGIN num:%{public}ld use:%{public}d
%s AudioSession setup failed
%s Has wrong audio routing, ask user to unplug headset
%s Start Audio Session failed
%s _sessionNumber [%{public}ld]
%s %{public}s Canceling Training
%s AudioSession StartRecording Failed
%s Setting suspendAudio:[%{public}d]
%s Resume training
%s Suspend training
%s Stop Listening
%s NDAPI initialization failed
%s set StartAnalyzeSampleCount = %{public}lld
%s Couldn't create CoreSpeech log directory at path %{public}@ %{public}@
%s Start monitring : VoiceTrigger setting switch
%s Cannot start monitoring VoiceTrigger setting switch because it was already started
%s Stop monitring : VoiceTrigger setting switch
%s VoiceTrigger enabled = %{public}@
%s Creating new AVVC
%s Error initializing voice controller with context %{public}@ %{public}@
%s Trying to set record buffer duration to %{public}lf
%s Failed setting record buffer duration. Duration is %{public}lf
%s AVVC startRecordingWithSettings failed.
%s AVVC Stop Recording
%s audioInput:[%{public}@]
%s No Reocrd Route detected
%s audioOutput:[%{public}@]
%s BeepCanceller asset file loading from : %{public}@
%s beepVector Size = %{public}lu
%s Cannot initialize beep canceller
%s It will beep now
%s Reset beep cancellation
%s numBypassSamples set to: %{public}tu
%s Sending early detect notification upon first pass trigger
%s Received first pass triggered in channel: %{public}tu with trigger start: %{public}tu
%s Second pass set to analyze %{public}tu samples (%{public}.2fs) from %{public}tu to %{public}tu
%s Stop feeding audio to recognizer per sampleCount: %{public}tu > %{public}tu
%s Set to analyze %{public}.3f more audio until recognizer comes back
%s Notify second pass reject at: %{public}tu with best score up to: %{public}.3f
%s NDAPI second pass best score = %{public}f for channel = %{public}tu with analyzed samples: %{public}tu
%s Trigger detected with %{public}tu analyzed samples in NDAPI
%s Detected near miss!!!, %{public}@
%s Waiting for logging near miss until timeout %{public}tu samples
%s Detected near miss candidate at %{public}tu, let's wait %{public}tu samples to log
%s recognizerScore updated from %{public}.3f to: %{public}.3f
%s EventNotifier received VoiceTrigger event
%s Notifying VoiceTrigger Trigger!!!!
%s Reporting VoiceTrigger two shot detection at time : %{public}lf
%s Error reading audio file: %{public}d, skipping...
%s numChannels: %{public}lu, recordingDuration: %{public}f, sampleRate: %{public}f
%s Cannot copy samples since this is empty
%s Could NOT copyFrom: %{public}lu to: %{public}lu, retSampleCount: %{public}lu
%s copyBuffer: oldestSample: %{public}lu latestSample: %{public}lu, numSamplesCopied: %{public}lu
%s CSAudioCircularBuffer.reset
%s saveRecordingBufferFrom: %{public}lu to: %{public}lu toURL: %{public}@
%s csrb: %{public}@
%s Invalid request: (%{public}lu, %{public}lu): noting to write to file
%s Invalid request: reqStartSample=%{public}lu, reqEndSample=%{public}lu, oldestSampleInBuffer: %{public}lu, latestSampleInBuffer=%{public}lu
%s Start monitoring : VoiceTrigger Asset meta update
%s Stop monitoring : VoiceTrigger Asset meta update
%s New VoiceTrigger asset metadata is available
%s VAD2 preheat...
%s CSVAD2EndpointAnalyzer: resetForNewRequestWithSampleRate
%s ERR: Deprecated VAD2 reset called
%s %{public}@ Resetting with style %{public}zd, _samplesSeen: %{public}f, newSampleRate: %{public}tu, _sampleRate: %{public}f
%s _audioUnitEPVAD2=%{public}p, auNeedsReset: %{public}zd
%s Failed to reset EPVAD2: %{public}d
%s vtEndInSecs: %{public}f, _vtEndInSampleCount: %{public}lu, voiceTriggerInfo: %{public}@,
%s VAD2::RecordingDidStop: Ignoring processAudioSamplesAsynchronously, not queueing
%s VAD2::RecordingDidStop: Ignoring processAudioSamplesAsynchronously from async
%s Already communicated endpoint...returning
%s VAD2::RecordingDidStop: Ignoring _processAudioSamples
%s Empty samplesBuffer!
%s Received audio buffer with 8 frames of zeroes
%s Not configured
%s done: %{public}zd, _detectedOneShotStartpoint: %{public}zd, _communicatedEndpointDetection: %{public}zd, _startWaitTime: %{public}f_samplesSeen: %{public}f, _delay: %{public}f, _sampleRate: %{public}f(_startWaitTime + _delay) * _sampleRate): %{public}f, (_samplesSeen / _sampleRate): %{public}f, _automaticEndpointingSuspensionEndTime: %{public}f
%s No startpoint detected after %{public}f, timing out, _samplesSeen: %f, _samplesSeen(ms): %f, _lastOneShotEndpoint: %f
%s Ignoring recurrent endpoint at %{public}f becuase it's too early (< %{public}f)
%s Fell back to recurrent endpoint (%{public}f) because one-shot is too early (%{public}f < %{public}f)
%s Fell back to recurrent endpoint, _samplesSeen: %f, _samplesSeen(ms): %f, _lastOneShotEndpoint: %f
%s Reporting one-shot ep:  _samplesSeen: %f, _samplesSeen(ms): %f, _lastOneShotEndpoint: %f
%s sampleRate: %{public}lf frameRate: %{public}d
%s Skipping re-initialization of EPVAD2; no audio consumed yet
%s EPVAD2 reset with existing parameters
%s Could not find endpointer audio unit component
%s AU instantiation error: %{public}d
%s No model available for mode: %{public}d
%s Error reading plist endpoint model at %{public}@
%s Could not set kAUEndpointVADProperty_ViterbiModelData: %{public}d
%s Could not set %{public}@ to %{public}@: %{public}d
%s Could not initialize audio unit: %{public}d
%s CSVAD2Endpointer is configured
%s Unexpected block size of %{public}u, not %{public}u. Skipping this block of audio.
%s Could not process audio via endpointer: %{public}d
%s tuningLibraryPath: %{public}@
%s VAD2-epModelPath: %{public}@
%s Could not read kAUEndpointVAD2Property_LatestEndpointerEventTimeSeconds: %{public}d
%s Found one shot startpoint at %{public}.3f seconds
%s Found one shot endpoint at %{public}.3f seconds
%s Could not read kAUEndpointVAD2Property_LatestRecurrentVADEventTimeSeconds: %{public}d
%s Found recurrent startpoint at %{public}.3f seconds
%s Found recurrent endpoint at %{public}.3f seconds
%s Cannot generate subChunk since channel(%{public}tu) is larger than number of channels(%{public}tu)
%s Cannot generate subChunk if it reuqest more than it has : %{public}tu %{public}tu %{public}tu
%s Fired VoiceTrigger Timeout
%s Stop right now since ASR has issue
%s EOS Timeout Fired
%s AudioSession Started
%s AudioSession Stopped
%s unknown endpoint type
%s Non Final: [%{public}@]
%s NON Final Matching
%s Final: [%{public}@]
%s Final Matching
%s Final Not Matching
%s SPEECH RECOGNITION TASK FINISH UNSUCCESSFULLY
%s %{public}@ %{public}@ %{public}ld %{public}@
%s Recog Result: [%{public}ld]
%s CSEndpointAsset: %{public}@, path: %{public}@
%s _endpointerModelVersion = %{public}@
%s HEP::RecordingDidStop: Ignoring processAudioSamplesAsynchronously: Not queueing
%s HEP::RecordingDidStop: Ignoring processAudioSamplesAsynchronously
%s addAudio first sample offset: %{public}lu
%s Updated endpointer threshold: %{public}f
%s Updated endpointer delayed trigger: %{public}d
%s EARSPG: CSServerEndpointFeatures: %{public}@
%s Accepting RC: RCTime < 0: Server's processedAudioDuration(%{public}f) > _lastReportedEndpointTimeMs(%{public}f): sfLatency: %{public}f, rcTimeMs: %{public}f
%s Rejecting RC: SFLatency < 0: Server's processedAudioDuration(%{public}f): _lastReportedEndpointTimeMs(%{public}f): sfLatency: %{public}f, rcTimeMs: %{public}f
%s rcEpFeatures: %{public}@ shouldAccept: %{public}d
%s HEP::RecordingDidStop: Ignoring silenceScoreEstimateAvailable, Not queuing
%s silposnf=%f, clientProcessedAudioMs: %{public}f, effectiveClientProcessedAudioMs: %{public}lu
%s HEP::RecordingDidStop: Ignoring silenceScoreEstimateAvailable
%s Already communicated end-pt: Not Invoking hybridClassifier for silposnf=%{public}f
%s ClientLag: serverProcessedAudioMs(%{public}ld) > effectiveClientProcessedAudioMs(%{public}f)
%s ClientLag: Not invoking HybridClassifier: sfLatency > clientLagThreshold: %{public}f > %{public}f
%s ClientLag: Using DefaultServerFeatures with disconnected-state sfLatency: %{public}f
%s ClientLag: Using ServerFeatures with ClampedSFLatency: %{public}f
%s ClientLag: Not Invoking HybridClassifier as serverProcessedAudioMs > effectiveClientProcessedAudioMs
%s EARSPG: Invoking HybridClassifier with parameters: %{public}@ at effectiveClientProcessedAudioMs: %{public}lu,clientSilencePosterior: %{public}f, endpointPosterior: %{public}f, result: %{public}d
%s ServerFeaturesLatencyDistribution: %{public}@ additionalMetrics: %{public}@
%s Already communicated end-pt: Not scheduling work for hybridClassifierQueue for silposnf=%{public}f
%s unsorted-serverFeatureLatencies: %{public}@
%s triggerEndSeconds: %{public}f, _vtEndInSampleCount: %{public}lu, _vtExtraAudioAtStartInMs: %{public}lu,  _hepAudioOriginInMs: %{public}f, voiceTriggerInfo: %{public}@,
%s CSHybridEndpointAnalyzer recordingStoppedForReason: %{public}lu
%s CSHybridEndpointer resetForNewRequestWithSampleRate: %{public}lu
%s CSEndpointAsset exists: %{public}@
%s No asset for CSHybridEndpointer for currentLanguage: %{public}@. Fallback to VAD2
%s Created EARCaesuraSilencePosteriorGenerator: %{public}@
%s Created HybridClassifier(%{public}@); canProcessCurrentRequest after reset: %{public}zd,for sampleRate: %{public}lu, lang=%{public}@, version=%{public}@
%s csHepConfig: %{public}@
%s _clientHepConfig: %{public}f, _clampedSFLatencyForClientLag: %{public}f, _useDefaultServerFeaturesOnClientLag: %{public}zd
%s language changed to: %{public}@: CSHybridEndpointer new asset: %{public}@
%s new hybrid endpoint asset downloaded, CSHybridEndpointer asset : %{public}@
%s current asset changed to : %{public}@
%s %{public}@ doesnt exist
%s Could not read: %{public}@
%s Could not decode contents of: %{public}@: err: %{public}@
%s Delta is larger than anchorHostTime
%s Delta is larger than anchorSampleCount
%s CSHybridEndpointer canProcessCurrentRequest
%s CSHybridEndpointer can-NOT-ProcessCurrentRequest, fallback  to VAD2
%s _activeEndpointer=%{public}@
%s 2-shot: DONT reset any endpointers
%s endpointer: %{public}@: didDetectStartpointAtTime: %{public}f
%s EP_PROXY::RecordingDidStop: Ignoring startPoint-reporting
%s EP_PROXY::RecordingDidStop: Ignoring didDetectHardpoint-reporting
%s EP_PROXY::RecordingDidStop: Ignoring VAD2 2-shot reporting
%s CSEndpointerProxy didDetectHardEndpoint using for 1-2 shot at Time: %{public}f
%s EP_PROXY::RecordingDidStop: Ignoring didDetectHardEndpointAtTime:
%s %{public}@: Endpointer didDetectHardEndpointAtTime:withMetrics: %{public}f, CallingDelegate: %{public}@
%s CSEndpointerProxy: didDetectHardEndpoint: ep-time: %{public}f, triggerEnd: %{public}f, vad2EndWaitTime: %{public}f, delta: %{public}f, legacyTwoShotThreshold: %{public}f, enterTwoShot: %{public}zd
%s WARN: endpointerModelVersion called when CSHybridEndpointer is not available
%s Stop monitoring : First unlock
%s Start monitoring : Springboard start
%s Cannot start monitoring Springboard start because it was already started
%s Stop monitoring : Springboard start
%s SpringBoard started = %{public}@
%s Transcriber trigger token list: %{public}@
%s Initializing Quasar with config: %{public}@
%s Speech model loading took %{public}.3fms
%s Failed initialization in _EARSpeechRecognizer initWithConfiguration
%s runRecognition failed
%s endAudio failed
%s recognizeWavData failed
%s Partial result confidence: %{public}f
%s ERROR: %{public}s
%s Final result confidence: %{public}f
%s EAR Token[%{public}lu]: %{public}s (%{public}f)
%s Fallback asset resource path : %{public}@
%s Cannot find corespeech asset from resourcePath : %{public}@
%s Configuration file is not exists : %{public}@
%s Cannot read configuration file : %{public}@
%s Cannot decode configuration json file : %{public}@
%s Configuration json file is not expected format
%s Cannot access to %{public}@ %{public}@ using default value
%s There is not audio buffer to convert. Skip this.
%s Got asked for %{public}u packets, have %{public}u
%s [%{public}02u of %{public}02u] Opus packet with %u bytes
%s %{public}d bytesConsumed from opus coverter, remains %{public}d bytes
%s Resetting AudioConverter buffer
%s createAudioConverter : initial frames per buffer = dur %{public}.2f * sr %{public}.2f = %{public}u
%s _configureAudioConverter: encoded audio needs minimum of %{public}u bytes per output buffer
%s _configureAudioConverter: AudioConverterGetProperty(kAudioConverterPropertyMinimumOutputBufferSize) returned status %{public}d
%s _configureAudioConverter: final framesPerBuffer: %{public}u
%s _configureAudioConverter: _convertPacketCount: %{public}u
%s _configureAudioConverter: AudioConverterGetProperty(MaximumOutputPacketSize): returned status %{public}d
%s createAudioConverter: outputSizePerPacket: %{public}u
%s _configureAudioConverter: _convertAudioCapacity %{public}u bytes
%s Cannot create AudioConverter using AudioConverterNew : %{public}u
%s Cannot set encoder bit rate : %{public}u
%s Start monitoring : Siri language code
%s Stop monitoring : Siri language code
%s Siri language changed to : %{public}@
%s Ignore notifying change of language code, since it is nil
%s Output NDAPI second pass best score = %{public}f for channel = %{public}tu
%s Notifying self trigger detected
%s SAT successfully initialized : %{public}@
%s SAT Score = %{public}f, threshold = %{public}f
%s Cannot create CSVTUIKeywordDetector since there is no asset available
%s Cannot create CSVTUIKeywordDetector since we cannot initialize NDAPI
%s AVVC initialization failed
%s Successfully create AVVC : %{public}p
%s Trying to set record buffer duration : %{public}lf
%s Creating beep canceller...
%s Calling AVVC prepareRecordWithSettings : %{public}@
%s Creating SampleRateConverter
%s Ignore this call since there is no context on RemoteCoreSpeech
%s Calling AVVC setCurrentContext : %{public}@
%s Calling AVVC prewarmAudioSession
%s Calling AVVC releaseAudioSession : %{public}tu
%s zeroFilterWinSz: %{public}tu, numHostTicksPerAudioSample: %{public}f
%s _vtEndInSampleCount:%{public}ld, _numSamplesProcessed: %{public}ld, vtInfo: %{public}@
%s Resetting ZeroFilter
%s Calling AVVC startRecordingWithSettings : %{public}@
%s Calling AVVC startRecording
%s Calling AVVC stopRecording
%s AVVC sampling rate = %f
%s AVVC doesn't return sampleRate, assume it is default sample rate
%s Calling AVVC playAlertSoundsForType : %{public}zd
%s successfully : %{public}d, error : %{public}@
%s Zero Filter Metrics: %@
%s Beep Canceller Metrics : %@
%s toConfiguration : %{public}d
%s withContext : %{public}@
%s Failed to deinterleave the data: %{public}d
%s Cannot create de-interleaver using AudioConverterNew: %{public}d
%s Created de-interleaver
%s Created narrowBandToWidBandConverter
%s Setting decisionWaitSampleCount at %{public}tu (%{public}.3f) given vtEndSampleCount at %{public}tu (%{public}.3f)
%s Keyword detected at %{public}tu with %{public}.3f confidence
%s Keyword NOT detected at %tu with %{public}.3f confidence
%s Failed to create regular expression : %{public}@
%s ::: Error creating output file %{public}@, err: %{public}d
%s ::: Error writing to output wave file. : %{public}ld
CSContinuousVoiceTrigger
CSKeywordAnalyzerNDAPIScoreDelegate
NSObject
CSSpeechManagerDelegate
CSAudioSampleRateConverter
LanguageCode
LPCMTypeConversion
CSSpeakerModel
CSVoiceTriggerFirstPass
CSCoreSpeechServices
CSConfig
CSSpeechManager
CSAudioRecorderDelegate
CSStateMachineDelegate
CSVoiceTriggerDelegate
CSAssetManagerDelegate
CSLanguageCodeUpdateMonitorDelegate
CSVTUIEditDistance
CSAssetManager
CSVoiceTriggerAssetMetaUpdateMonitorDelegate
CSSpeechEndpointAssetMetaUpdateMonitorDelegate
Utils
VoiceTriggerFirstPass
CSVTUITrainingSession
SFSpeechRecognitionTaskDelegate
CSVTUIAudioSessionDelegate
CSVTUIEndPointDelegate
CSSpeechController
CSAudioConverterDelegate
CSPreferences
CSStateMachine
Meter
CSEventMonitor
CSAudioFileLog
Alert
CSVoiceTriggerFileLogger
Metrics
VoiceTriggerSecondPass
VoiceTriggerPassThru
CSVoiceTriggerEnabledPolicyNonAOP
CSSpeechEndpointAssetMetaUpdateMonitor
VoiceTriggerRecord
CSMyriadPHash
CSVoiceTriggerFidesClient
FidesRecordInfoHelper
CSVTUIRegularExpressionMatcher
CSVTUIASRGrammars
NSURLSessionDelegate
CSVTUITrainingManager
CSVTUITrainingSessionDelegate
CSEndpointAnalyzerDelegate
CSUtils
CSKeywordAnalyzerNDAPI
CSEndpointerMetrics
CSAsset
AudioStreamBasicDescription
CSVoiceTriggerEnabledMonitor
KeywordDetector
CSVoiceTriggerSpeakerTrainer
CSVTUIAudioSessionAVVC
CSVTUIAudioSession
AVVoiceControllerRecordDelegate
AVVoiceControllerPlaybackDelegate
CSBeepCanceller
CSVoiceTriggerSecondPass
CSSpeakerDetectorNDAPIDelegate
CSKeywordAnalyzerQuasarScoreDelegate
CSVoiceTriggerFirstPassDelegate
CSVoiceTriggerEventNotifier
CSAudioZeroFilter
AudioFile
CSAssetManagerEnablePolicy
CSAudioCircularBuffer
CSVoiceTriggerAssetMetaUpdateMonitor
SpeakerDetectorNDAPI
CSServerEndpointFeatures
CSVAD2EndpointAnalyzer
CSEndpointAnalyzerImpl
CSEndpointAnalyzer
private
CSAudioChunk
CSNovDetectorResult
CSNovDetector
Bitset
CSVTUITrainingSessionWithPayload
CSHybridEndpointAnalyzer
EARCaesuraSilencePosteriorGeneratorDelegate
Time
CSEndpointerProxy
CSFirstUnlockMonitor
CSSpringboardStartMonitor
CSKeywordAnalyzerQuasar
_EARSpeechRecognitionResultStream
CSAudioConverter
CSLanguageCodeUpdateMonitor
CSPolicy
CSEventMonitorDelegate
CSSelfTriggerDetector
RecordContext
CSSpeakerDetectorNDAPI
CSVTUIKeywordDetector
CSAudioRecorder
CSBeepCancellerDelegate
ContinuousVoiceTrigger
DuckOption
EndpointAnalyzer
CSKeywordDetector
ResourcePathHash
CSAudioFileWriter
B24@0:8@16
#16@0:8
@16@0:8
@24@0:8:16
@32@0:8:16@24
@40@0:8:16@24@32
B16@0:8
B24@0:8#16
B24@0:8:16
Vv16@0:8
Q16@0:8
^{_NSZone=}16@0:8
B24@0:8@"Protocol"16
@"NSString"16@0:8
v40@0:8@16@24Q32
v40@0:8@"CSKeywordAnalyzerNDAPI"16@"NSDictionary"24Q32
v32@0:8@16@24
v36@0:8@16B24@28
v32@0:8@16q24
v24@0:8@16
v32@0:8@"CSSpeechManager"16@"AVVCAudioBuffer"24
v32@0:8@"CSSpeechManager"16@"CSAudioChunk"24
v36@0:8@"CSSpeechManager"16B24@"NSError"28
v32@0:8@"CSSpeechManager"16q24
@"NSDictionary"16@0:8
v24@0:8@"CSSpeechManager"16
v32@0:8@"CSSpeechManager"16@"NSDictionary"24
@32@0:8@16@24
v16@0:8
f16@0:8
v20@0:8f16
q16@0:8
v24@0:8q16
v24@0:8Q16
@"<CSVoiceTriggerDelegate>"
@"CSSpeechManager"
@"NSObject<OS_dispatch_queue>"
@"CSAsset"
@"CSKeywordAnalyzerNDAPI"
@96@0:8{AudioStreamBasicDescription=dIIIIIIII}16{AudioStreamBasicDescription=dIIIIIIII}56
^{OpaqueAudioConverter=}96@0:8{AudioStreamBasicDescription=dIIIIIIII}16{AudioStreamBasicDescription=dIIIIIIII}56
@24@0:8@16
^{OpaqueAudioConverter=}
{AudioStreamBasicDescription="mSampleRate"d"mFormatID"I"mFormatFlags"I"mBytesPerPacket"I"mFramesPerPacket"I"mBytesPerFrame"I"mChannelsPerFrame"I"mBitsPerChannel"I"mReserved"I}
@"NSString"
v20@0:8B16
@"<CSVoiceTriggerFirstPassDelegate>"
@"CSVoiceTriggerEnabledPolicyNonAOP"
@"NSMutableArray"
I16@0:8
d16@0:8
S16@0:8
i16@0:8
v40@0:8@"CSAudioRecorder"16@"NSData"24Q32
v32@0:8@"CSAudioRecorder"16@"AVVCAudioBuffer"24
v36@0:8@"CSAudioRecorder"16B24@"NSError"28
v32@0:8@"CSAudioRecorder"16q24
v24@0:8@"CSAudioRecorder"16
v32@0:8@"CSAudioRecorder"16@"NSDictionary"24
v24@0:8@"NSDictionary"16
v40@0:8q16q24q32
v32@0:8q16q24
v24@0:8d16
v24@0:8@"CSAssetManager"16
v32@0:8@16@"NSString"24
@48@0:8@16@24@32@40
B32@0:8@16^@24
B24@0:8^@16
B32@0:8q16^@24
B32@0:8Q16^@24
B40@0:8@16Q24^@32
v32@0:8@16^@24
Q24@0:8Q16
@24@0:8q16
@24@0:8Q16
@"CSAudioRecorder"
@"CSStateMachine"
@"CSAudioCircularBuffer"
@"CSVoiceTriggerFirstPass"
@"CSVoiceTriggerSecondPass"
@"<CSSpeechManagerDelegate>"
@"CSVoiceTriggerEventNotifier"
@"CSVoiceTriggerFileLogger"
@"CSSelfTriggerDetector"
@"CSContinuousVoiceTrigger"
@"CSKeywordDetector"
@"CSMyriadPHash"
@"CSVoiceTriggerFidesClient"
@"NSHashTable"
@"NSDictionary"
@"NSObject<OS_dispatch_source>"
v28@0:8@16B24
@32@0:8Q16@24
@36@0:8Q16@24B32
v32@0:8@16@?24
v32@0:8Q16@24
v32@0:8@16Q24
v40@0:8@16@?24@?32
@"CSPolicy"
@"NSMutableDictionary"
v24@0:8@"SFSpeechRecognitionTask"16
v32@0:8@"SFSpeechRecognitionTask"16@"SFTranscription"24
v32@0:8@"SFSpeechRecognitionTask"16@"SFSpeechRecognitionResult"24
v28@0:8@"SFSpeechRecognitionTask"16B24
v28@0:8B16@20
v28@0:8B16@"NSError"20
v24@0:8@"NSData"16
v24@0:8@"NSError"16
@96@0:8q16q24@32@40@48@56@64@72@80@?88
v24@0:8@?16
v24@0:8i16B20
v32@0:8i16B20@?24
@"CSVTUIKeywordDetector"
@"<CSVTUIAudioSession>"
@"SFSpeechRecognizer"
@"SFSpeechAudioBufferRecognitionRequest"
@"SFSpeechRecognitionTask"
@"NSTimer"
@"<CSVTUITrainingSessionDelegate>"
v40@0:8@"CSAudioConverter"16@"NSArray"24Q32
{AudioStreamBasicDescription=dIIIIIIII}16@0:8
B24@0:8d16
B20@0:8B16
B32@0:8@16q24
B24@0:8q16
f24@0:8Q16
v32@0:8d16@?24
@"CSAudioConverter"
@"CSAudioSampleRateConverter"
@"<CSSpeechControllerDelegate>"
@"CSEndpointerProxy"
@"CSAudioFileWriter"
@24@0:8^@16
B32@0:8@16@24
@"<CSStateMachineDelegate>"
^{OpaqueExtAudioFile=}
@"NSURL"
S28@0:8^f16i24
@28@0:8Q16f24
s16@0:8
v20@0:8s16
v32@0:8q16@24
q48@0:8@16@24@32@40
v40@0:8@16@24@?32
v32@0:8@"NSURLSession"16@"NSError"24
v40@0:8@"NSURLSession"16@"NSURLAuthenticationChallenge"24@?<v@?q@"NSURLCredential">32
v24@0:8@"NSURLSession"16
@32@0:8q16@24
@40@0:8@16q24@32
v32@0:8@16d24
v40@0:8@16d24@32
v32@0:8@"<CSEndpointAnalyzer>"16d24
v40@0:8@"<CSEndpointAnalyzer>"16d24@"CSEndpointerMetrics"32
@24@0:8@?16
q36@0:8q16B24@?28
@"CSVAD2EndpointAnalyzer"
@"CSVTUITrainingSession"
@"<CSVTUITrainingManagerDelegate>"
@20@0:8i16
@28@0:8@16i24
v20@0:8I16
@"CSNovDetector"
@"<CSKeywordAnalyzerNDAPIScoreDelegate>"
@56@0:8d16@24q32@40@48
@"NSArray"
B40@0:8@16@24@32
v24@0:8@"<CSVTUIAudioSessionDelegate>"16
v24@0:8@"<Endpointer>"16
v28@0:8@16i24
v36@0:8@16i24d28
v28@0:8@"AVVoiceController"16B24
v36@0:8@"AVVoiceController"16B24@"NSError"28
v32@0:8@"AVVoiceController"16q24
v24@0:8@"AVVoiceController"16
v28@0:8@"AVVoiceController"16i24
v36@0:8@"AVVoiceController"16i24d28
v32@0:8@"AVVoiceController"16@"NSError"24
v32@0:8@"AVVoiceController"16@"AVVCAudioBuffer"24
v32@0:8@"AVVoiceController"16@"NSDictionary"24
q24@0:8q16
@"AVVoiceController"
@"<CSVTUIAudioSessionDelegate>"
{unique_ptr<BatchBeepCanceller, std::__1::default_delete<BatchBeepCanceller> >="__ptr_"{__compressed_pair<BatchBeepCanceller *, std::__1::default_delete<BatchBeepCanceller> >="__first_"^{BatchBeepCanceller}}}
{vector<float, std::__1::allocator<float> >="__begin_"^f"__end_"^f"__end_cap_"{__compressed_pair<float *, std::__1::allocator<float> >="__first_"^f}}
{vector<short, std::__1::allocator<short> >="__begin_"^s"__end_"^s"__end_cap_"{__compressed_pair<short *, std::__1::allocator<short> >="__first_"^s}}
@"<CSBeepCancellerDelegate>"
v32@0:8@"CSSpeakerDetectorNDAPI"16@"NSDictionary"24
v40@0:8@"CSKeywordAnalyzerQuasar"16@"NSDictionary"24Q32
v32@0:8@"CSVoiceTriggerFirstPass"16@"NSDictionary"24
@"CSKeywordAnalyzerQuasar"
@"CSSpeakerDetectorNDAPI"
@"CSSpeakerModel"
@36@0:8Q16S24d28
Q40@0:8@16Q24^@32
Q24@0:8^@16
{unique_ptr<CSAudioZeroFilterImpl<unsigned short>, std::__1::default_delete<CSAudioZeroFilterImpl<unsigned short> > >="__ptr_"{__compressed_pair<CSAudioZeroFilterImpl<unsigned short> *, std::__1::default_delete<CSAudioZeroFilterImpl<unsigned short> > >="__first_"^{CSAudioZeroFilterImpl<unsigned short>}}}
B32@0:8@16@?24
@32@0:8Q16f24f28
v32@0:8r^v16Q24
v40@0:8r^v16Q24Q32
@32@0:8Q16Q24
@24@0:8^Q16
v40@0:8Q16Q24@32
{unique_ptr<corespeech::CSAudioCircularBufferImpl<unsigned short>, std::__1::default_delete<corespeech::CSAudioCircularBufferImpl<unsigned short> > >="__ptr_"{__compressed_pair<corespeech::CSAudioCircularBufferImpl<unsigned short> *, std::__1::default_delete<corespeech::CSAudioCircularBufferImpl<unsigned short> > >="__first_"^{CSAudioCircularBufferImpl<unsigned short>}}}
r*16@0:8
@72@0:8q16q24d32@40d48@56q64
@64@0:8q16q24d32@40d48@56
v24@0:8@"CSAudioChunk"16
@"<CSEndpointAnalyzerDelegate>"16@0:8
v24@0:8@"<CSEndpointAnalyzerDelegate>"16
v24@0:8@"CSServerEndpointFeatures"16
v32@0:8d16@?<v@?B@"NSArray">24
^{OpaqueAudioComponentInstance=}
@"<CSEndpointAnalyzerDelegate>"
@"NSMutableData"
@24@0:8d16
B28@0:8^{AudioStreamBasicDescription=dIIIIIIII}16I24
v28@0:8d16I24
v28@0:8^f16I24
@64@0:8@16Q24Q32Q40Q48Q56
@40@0:8Q16Q24Q32
@"NSData"
@20@0:8I16
v32@0:8Q16@?24
v36@0:8^f16Q24f32
v24@0:8@"EARClientSilenceFeatures"16
@"EARCaesuraSilencePosteriorGenerator"
@"EARClientSilenceFeatures"
@"_EAREndpointer"
@"CSServerEndpointFeatures"
@"NSDate"
Q20@0:8f16
Q40@0:8Q16Q24Q32
@"<CSEndpointAnalyzerImpl>"
v48@0:8@16@24@32@40
v72@0:8@16q24q32d40@48d56q64
v32@0:8@"_EARSpeechRecognizer"16@"_EARSpeechRecognitionResult"24
v32@0:8@"_EARSpeechRecognizer"16@"NSError"24
v32@0:8@"_EARSpeechRecognizer"16@"NSArray"24
v48@0:8@"_EARSpeechRecognizer"16@"NSArray"24@"NSArray"32@"NSArray"40
v32@0:8@"_EARSpeechRecognizer"16@"_EARSpeechRecognitionResultPackage"24
v32@0:8@"_EARSpeechRecognizer"16d24
v32@0:8@"_EARSpeechRecognizer"16@"_EARSpeechRecognition"24
v72@0:8@"_EARSpeechRecognizer"16q24q32d40@"NSArray"48d56q64
@36@0:8@16@24B32
v28@0:8r^s16i24
d24@0:8@16
@"_EARSpeechRecognizer"
@"_EARSpeechRecognitionAudioBuffer"
@"<CSKeywordAnalyzerQuasarScoreDelegate>"
@40@0:8Q16@24@32
@40@0:8@16@24@32
B36@0:8@16@24B32
v36@0:8@16B24Q28
v24@0:8^{OpaqueAudioConverter=}16
@"<CSAudioConverterDelegate>"
f24@0:8@16
@32@0:8@16Q24
@"<CSSpeakerDetectorNDAPIDelegate>"
Q24@0:8I16I20
v40@0:8@"CSBeepCanceller"16@"NSData"24Q32
@32@0:8@16^@24
@"CSAudioZeroFilter"
@"CSBeepCanceller"
{AudioBufferList="mNumberBuffers"I"mBuffers"[1{AudioBuffer="mNumberChannels"I"mDataByteSize"I"mData"^v}]}
^{AudioBufferList=I[1{AudioBuffer=II^v}]}
@"<CSAudioRecorderDelegate>"
@104@0:8@16{AudioStreamBasicDescription=dIIIIIIII}24{AudioStreamBasicDescription=dIIIIIIII}64
s32@0:8r^v16q24
