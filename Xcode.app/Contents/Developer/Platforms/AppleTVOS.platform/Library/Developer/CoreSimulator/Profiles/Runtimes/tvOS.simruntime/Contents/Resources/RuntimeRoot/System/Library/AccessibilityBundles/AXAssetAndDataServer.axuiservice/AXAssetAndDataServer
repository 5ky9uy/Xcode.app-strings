softlink:r:path:/System/Library/PrivateFrameworks/HearingUtilities.framework/HearingUtilities
v8@?0
AXCustomPronunciations
AXVoiceOverActivities
Reconciled iCloud pronunciations: adding: %{private}@
Did not find a change, not updating
ICLOUD[%p]: Waiting 5s to publish, deleting %@
Created an empty length archive, not synching
ICLOUD[%p]: Publishing data to iCloud %{private}@
Custom pronunciations settings updated
refresh-voices
It's been too soon since we last updated our available voices. We need to wait longer. %f
/Library/Caches/com.apple.xbs/Sources/AccessibilityFrameworks_Sim/AccessibilityFrameworks-2933.26/Source/AXAssetAndDataServer/AXAssetAndDataServer.m
-[AXAssetAndDataServer _updateCachedTTSVoiceAssets]_block_invoke
We just refreshed voices, why are they missing?
-[AXAssetAndDataServer _handleExtantVoices]
Why is name nil for %@ %@
VoiceId
Language
Name
en-US
com.apple.speech.synthesis.voice
@"NSArray"8@?0
@"NSString"40@?0@"TTSVoiceAsset"8@"NSString"16q24@"NSString"32
@"NSString"32@?0@"TTSVoiceAsset"8@"NSString"16q24
B24@?0@"TTSVoiceAsset"8Q16
LastSpeechAssetUpdateCheck
com.apple.accessibility.api
com.apple.private.kernel.jetsam
com.apple.CoreRoutine.preferences
com.apple.TVServices.DisplayManager
com.apple.accessibility.voiceover
com.apple.appletv.pbs.display-manager-service-access
com.apple.accessibility.axctl
delete
options
audioLevel
Adding
Removing
v32@?0@8Q16^B24
v16@?0@"NSArray"8
got transcription results: %@
AXSpeechPronunciationClient
results
v24@?0@"NSDictionary"8@"NSError"16
Fail: %@, %d
Error
error
Starting pronunciation dictation: %@, %@, %@
q24@?0@"TTSVoiceAsset"8@"TTSVoiceAsset"16
v28@?0d8B16@"NSError"20
v16@?0@"MAProgressNotification"8
v16@?0q8
AXAssetAndDataServer
AXUIService
NSObject
AFAssistantUIService
AFSpeechDelegate
AFDictationDelegate
init
sharedInstance
mobileAssetWorkQueue
initWithTargetSerialQueue:
setAutomaticallyCancelPendingBlockUponSchedulingNewBlock:
_monitoriCloudVoiceOverData
_icloudDataChanged:
afterDelay:processBlock:
defaultStore
dataForKey:
initForReadingFromData:error:
arrayWithObjects:count:
setWithArray:
decodeObjectOfClasses:forKey:
count
_iCloudReconcileDataStore:
_iCloudReconcileActivitiesStore:
synchronize
syncPronunciationsWithCloudKit
_reconcilePronunciations
_reconcileActivities
voiceOverActivities
countByEnumeratingWithState:objects:count:
indexOfObject:
objectAtIndexedSubscript:
isIdenticalTo:
setVoiceOverActivities:
ignoreLogging
identifier
customPronunciationSubstitutions
containsObject:
_syncToWatch
setCustomPronunciationSubstitutions:
copy
archivedDataWithRootObject:requiringSecureCoding:error:
length
setData:forKey:
_saveCustomPronunciationsToExternalsAndDelete:
_customPronunciationsSettingsChanged
registerUpdateBlock:forRetrieveSelector:withListener:
_saveActivitiesToExternalsAndDelete:
defaultCenter
addObserver:selector:name:object:
currentLocale
languageCode
letterFeedbackEnabled
phoneticFeedbackEnabled
wordFeedbackEnabled
quickTypeWordFeedbackEnabled
speakCorrectionsEnabled
isEqualToString:
userDidSelectVoiceForLanguage:source:
availableVoicesForLanguageCode:queryingMobileAssets:
isDefault
speechVoiceIdentifierForLanguage:source:exists:
voiceForIdentifier:
footprint
isNeuralSiriVoiceIdentifier:
setSpeechVoiceIdentifier:forLanguage:source:
language
informSiriAboutVoiceUsageForIdentifier:forLanguage:add:
dictionary
numberWithBool:
setObject:forKeyedSubscript:
refreshAllAvailableVoices
allAvailableVoices
setCurrentVoices:
availableVoices:
currentVoices
_handleExtantVoices
objectForKeyedSubscript:
_updateDefaultVoiceIfNecessaryForLanguage:source:
mobileAssetDownloadQueue
_downloadAssetsForSelectedVoices
attributes
firstObject
stringByReplacingOccurrencesOfString:withString:
_combinedVocalizerAssets:
boolValue
intValue
_languageForAsset:
setIsCombinedVoice:
setNonCombinedVoiceId:
objectForKey:
allInstalledAssetsForVoiceId:
setIdentifier:
setLanguage:
setNonLocalizedNameWithoutQuality:
setQuality:
setGender:
setIsInstalled:
unsignedIntegerValue
setAssetSize:
setCanBeDownloaded:
addObject:
_vocalizerAssets
gender
languages
name
fileSize
numberWithLongLong:
isInstalled
state
supportsAlex
nameForVoiceIdentifier:
hasPrefix:
canBeDownloaded
isCombinedFootprint
supportsSiri
array
neural
_normalizedSiriName:
axSafelyAddObject:
_handleSiriStyleVoices:assetRetrieval:voiceIdentifier:voiceName:
axFilterObjectsUsingBlock:
setExtantVoices:
lowercaseString
migratePrefToNeuralVoiceIfNecessaryForVoice:
isSiriVoiceIdentifier:
startDownloadingVoice:
standardUserDefaults
date
dateByAddingTimeInterval:
setObject:forKey:
timeIntervalSinceReferenceDate
extantVoices
_checkForMacinTalkAssetUpdates
_checkForVocalizerAssetUpdates
_updateCachedTTSVoiceAssets
setWithObjects:
_checkForAssets:
valueForKey:
secureUnarchiveData:withExpectedClass:otherAllowedClasses:
_startPronunciationSession:
_cancelPronunciationSession
_stopPronunciationSession
_pronunciationAudioLevel
numberWithFloat:
dictionaryWithObjects:forKeys:count:
selectedSpeechVoiceIdentifiers
installedAssetsForLanguage:voiceType:
axSafelyAddObjectsFromArray:
setAutoDownloadedVoiceAssets:
informSiriAboutVoiceListWithLanguage:add:voiceId:
setSiriAutoUpdateListInitialized:
systemLanguageID
isVocalizerVoiceIdentifier:
numberWithLong:
type
mutableCopy
removeObject:
removeObjectsInArray:
numberWithUnsignedInteger:
enumerateObjectsUsingBlock:
getAutoDownloadedVoiceAssets:
_handleFailureForSpeechPronunciation:
averagePower
stopSpeechWithOptions:
cancelSpeech
phonemeSuggestions
clientMessengerWithIdentifier:
backgroundAccessQueue
sendAsynchronousMessage:withIdentifier:targetAccessQueue:completion:
endSession
description
setDelegate:
setTranscriptionMode:
orthography
setOrthography:
initWithActivationEvent:
setIsEyesFree:
setEndpointerOperationMode:
startDictationWithLanguageCode:options:speechOptions:
initWithType:
setDoNotBlockBeforeFirstUnlock:
returnTypes:
queryMetaDataSync
results
assetsForLanguage:voiceType:
masteredVersion
sortedArrayUsingComparator:
setAllowsCellularAccess:
totalWritten
totalExpected
_alexAssets
_beginDownloadIfNecessaryForAssets:
_vocalizerAssetsForLanguage:
_beginDownloadIfNecessaryForSiriAssets:
voiceAssetForVoiceId:
downloadAsset:progressHandler:
selectedIdsForTesting
isCompactVocalizerVoiceIdentifier:
isAssetManagedBySiriForVoiceId:
_handleSiriAssetDownload:
_handleMobileAssetDownload:
mobileAssetForVoiceId:
_voiceFromInternalVoiceListWithIdentifier:
isCombinedVocalizerVoiceIdentifier:
compare:options:
cleanUpExtraInstalledAssetsIfNecessary:
inUnitTestMode
replaceTestAsset:withAsset:
spaceCheck:
_handleAssetProgress:asset:installedAsset:
attachProgressCallBack:
_mobileAssetDownloadOptions
_updateAsset:existingAsset:
startDownload:then:
assetType
assetIsDownloading:
purgeAsset:
alexLocalAssetURL
getLocalFileUrl
isEqual:
_addToDownloadQueue:
purge:
_purgeExistingAsset:inFavorOfAsset:
setAlexLocalAssetURL:
defaultManager
path
setAttributes:ofItemAtPath:error:
possibleRequiredEntitlementsForProcessingMessageWithIdentifier:
class
self
performSelector:
performSelector:withObject:
performSelector:withObject:withObject:
isProxy
isKindOfClass:
isMemberOfClass:
conformsToProtocol:
respondsToSelector:
retain
release
autorelease
retainCount
zone
hash
superclass
debugDescription
TQ,R
T#,R
T@"NSString",R,C
processMessage:withIdentifier:fromClientWithIdentifier:error:
messageWithIdentifierShouldBeProcessedAsynchronously:
processMessageAsynchronously:withIdentifier:fromClientWithIdentifier:completion:
accessQueueForProcessingMessageWithIdentifier:
messageWithIdentifierRequiresWritingBlock:
processInitializationMessage:
serviceWasFullyInitialized
connectionWillBeInterruptedForClientWithIdentifier:
requiredEntitlementForProcessingMessageWithIdentifier:
assistantConnectionRequestWillStart:
assistantConnectionDismissAssistant:
assistantConnection:dismissAssistantWithReason:
assistantConnectionRequestFinished:
assistantConnection:receivedCommand:completion:
assistantConnection:requestFailedWithError:requestClass:
assistantConnection:openURL:completion:
assistantConnection:openApplicationWithBundleID:URL:completion:
assistantConnection:shouldSpeak:
assistantConnection:didChangeAudioSessionID:
assistantConnectionAudioSessionDidBeginInterruption:
assistantConnectionAudioSessionDidBeginInterruption:userInfo:
assistantConnectionAudioSessionDidEndInterruption:shouldResume:
assistantConnectionAudioSessionDidEndInterruption:shouldResume:userInfo:
assistantConnectionWillStartAcousticIDRequest:
assistantConnectionDidDetectMusic:
assistantConnection:didFinishAcousticIDRequestWithSuccess:
assistantConnection:setUserActivtiyInfoAndMakeCurrent:webpageURL:
assistantConnectionInvalidateCurrentUserActivity:
assistantConnection:wantsToCacheImage:
assistantConnection:extensionRequestWillStartForApplication:
assistantConnection:extensionRequestFinishedForApplication:error:
assistantConnection:startUIRequestWithText:completion:
assistantConnection:startUIRequestWithInfo:completion:
assistantConnection:willStartAudioPlaybackRequest:
assistantConnection:didStartAudioPlaybackRequest:
assistantConnection:didStopAudioPlaybackRequest:error:
assistantConnection:didHandleQuickStopWithAction:
assistantConnection:willProcessStartPlayback:intent:completion:
assistantConnection:willProcessStartPlayback:
assistantConnection:startPlaybackDidFail:
assistantConnection:audioSessionWillBecomeActive:
assistantConnection:audioSessionDidBecomeActive:
assistantConnection:willProcessAppLaunchWithBundleIdentifier:
assistantConnection:appLaunchFailedWithBundleIdentifier:
assistantConnectionSpeechRecordingWillBegin:
assistantConnection:speechRecordingWillBeginWithInputAudioPowerXPCWrapper:
assistantConnection:speechRecordingDidBeginOnAVRecordRoute:
assistantConnection:speechRecordingDidBeginOnAVRecordRoute:audioSessionID:
assistantConnection:speechRecordingDidChangeAVRecordRoute:
assistantConnectionSpeechRecordingDidDetectStartpoint:
assistantConnection:speechRecordingPerformTwoShotPromptWithType:completion:
assistantConnectionSpeechRecordingDidEnd:
assistantConnectionSpeechRecordingDidCancel:
assistantConnection:speechRecordingDidFail:
assistantConnectionDidChangeAudioRecordingPower:
assistantConnection:speechRecognitionDidFail:
assistantConnection:speechRecognized:
assistantConnection:speechRecognizedPartialResult:
assistantConnection:recognizedAdditionalSpeechInterpretation:refId:
assistantConnection:recognitionUpdateWithPhrases:utterances:refId:
assistantConnection:recognitionUpdateWillBeginForTask:
dictationConnectionSpeechRecordingWillBegin:
dictationConnectionSpeechRecordingDidBegin:
dictationConnection:speechRecordingDidBeginWithOptions:
dictationConnection:didBeginLocalRecognitionWithModelInfo:
dictationConnectionSpeechRecordingDidEnd:
dictationConnectionSpeechRecordingDidCancel:
dictationConnection:speechRecordingDidFail:
dictationConnection:speechRecognitionDidFail:
dictationConnection:didDetectLanguage:confidenceScores:
dictationConnection:didDetectLanguage:confidenceScores:isConfident:
dictationConnection:didRecognizeMultilingualSpeech:
dictationConnection:languageDetectorFailedWithError:
dictationConnection:didRecognizePhrases:languageModel:correctionIdentifier:
dictationConnection:didRecognizePhrases:languageModel:correctionIdentifier:replacingPreviousPhrasesCount:
dictationConnection:didRecognizeTokens:languageModel:
dictationConnection:didRecognizePartialResult:
dictationConnection:didProcessAudioDuration:
dictationConnectionSpeechRecognitionDidSucceed:
dictationConnection:didRecognizeTranscriptionObjects:languageModel:
dictationConnnectionDidChangeAvailability:
dictationConnection:didFinishWritingAudioFile:error:
dictationConnection:didReceiveSearchResults:recognizedText:stable:final:
dictationConnection:didRecognizePackage:
_isVocalizerAsset:
_purgeExistingSiriAsset:inFavorOfAsset:
.cxx_destruct
_lastTTSVoiceAssetUpdate
_pronunciationPusher
_lastActivitiesICloudRetrieval
_activitiesPusher
_isUpdatingCachedTTSVoices
_dictationConnection
_isRecording
Checking pronunciations
%{public}@
Got pronunciations: %{private}@, count: %d
Exception decoding data: %@
Checking activities
Got activities: count: %d
did not find %@ in %@
%@ not identical to %@
Reconciled iCloud activities: adding: %{private}@
Did not find a change, not updating
Skipping publishing to iCloud because we just retrieved this data
Created an empty length archive, not synching
ICLOUD[%p]: Publishing data to iCloud %{private}@
Not updating default spoken content voice because no speech features are enabled.
Not updating default spoken content voice because switch control is not enabled.
Not updating default voice because the voice doesn't match our system language.
Switching voice from %@ to %@ for source %ld because default voice has changed, and the user hasn't selected one for language %@
Updated TextToSpeech's available voices cache with voices: %@
Vocalizer assets example[0]: %{public}@
Victoria macinTalkAsset: %{public}@
local macinTalkAsset: %{public}@
remote macinTalkAsset: %{public}@
New siri names %@
Original old siri assets %@
Filtered out old siri assets so we're left with %@
Siri voices not supported
Set extant voices: %{public}@
Retrieved siri-style assets: %{public}@
Siri style: skipping non premium: %@
Siri style: skipping neural: %@
Processed Siri-style assets: %{public}@
made siri voice %@ with asset %@
Updating spoken content settings because the selected voice was a Siri voice, but a neural voice matching the same name and language: %@ was available
Downloading %@ because a non-neural variant was previously selected, but a neural variant is now available
Removing %@ from active siri list, because neural voice was available and non-neural version wasn't in use outside of Speech Features
Force checking speech assets: %d date: %{public}@
No previous LastSpeechAssetUpdateCheck was found. Setting date earlier: %{public}@.
Has speech asset data: %d
Not checking voice assets because it's too soon: %{public}@ - only %f seconds 
Checking on assets now
Asset is installed and is in selected voices: %@
Inform siri about %@
%@ lang: %@ gender: %@ type: %@ name: %@ voice to active siri voice list
Starting with voices: %@
Removing siri auto download voice: %@
Setting Siri auto-downloaded voice[%@]: %@
MAAssetQuery error fetching results %{public}@ %{public}@ %{public}@
Asset progress: %{public}@ - %f
Checking assets: %{public}@
checking assets[%{public}@] %{public}@
Handling download for %@ %@
Finished downloading asset: %@, %@
Updating siri voice list usage after downloading a selected voice ID that was missing
could not parse voice identifier %{public}@
could not find asset for voice identifier %{public}@ but the voice was installed. This is expected if it is a built-in voice.
could not find asset for voice identifier %{public}@
selected voice %{public}@ is already downloading.
selected voice %{public}@ is required by the OS.
selected voice %{public}@ is installed but not in the catalog.
selected voice %{public}@ has unknown state.
selected voice %{public}@ is already installed.
selected voice %{public}@ %@ is not present. Beginning download.
Asset not present, but no space to download voice %{public}@: %{public}@
asset installed with os
Adding asset to queue: %{public}@
No siri asset installed
Installed asset %{public}@, latest asset: %{public}@
Detected new Asset, downloading: %{public}@
Enough disk space available - start %{public}@
Updating Installed Asset: %{public}@
Error: %{public}@
Asset state: %@
Error: Not enough disk space
No newer asset than %{public}@ - [installed asset %{public}@]
Purging old asset: %{public}@
No macintalk or siri asset installed
No newer asset than %{public}@ - %@ [installed asset %{public}@]
Purged old asset: %{public}@, %{public}@
Trying to update unknown asset (expecting Macintalk, Combined, or Siri): %{public}@
Could not remove file protection from: %{public}@
Could not purge old asset: %{public}@
@16@0:8
@24@0:8Q16
B24@0:8@16
#16@0:8
@24@0:8:16
@32@0:8:16@24
@40@0:8:16@24@32
B16@0:8
B24@0:8#16
B24@0:8:16
Vv16@0:8
Q16@0:8
^{_NSZone=}16@0:8
B24@0:8@"Protocol"16
@"NSString"16@0:8
@48@0:8@16Q24@32^@40
B24@0:8Q16
v48@0:8@16Q24@32@?40
v24@0:8@16
v16@0:8
@"NSDictionary"48@0:8@"NSDictionary"16Q24@"NSString"32^@40
v48@0:8@"NSDictionary"16Q24@"NSString"32@?<v@?@"NSDictionary"@"NSError">40
@"AXAccessQueue"24@0:8Q16
v24@0:8@"NSDictionary"16
v24@0:8@"NSString"16
@"NSString"24@0:8Q16
@"NSSet"24@0:8Q16
v32@0:8@16q24
v40@0:8@16@24@?32
v40@0:8@16@24@32
v48@0:8@16@24@32@?40
v28@0:8@16B24
v28@0:8@16I24
v32@0:8@16@24
v36@0:8@16B24@28
v32@0:8@16Q24
v48@0:8@16q24@32@?40
v24@0:8@"AFConnection"16
v32@0:8@"AFConnection"16q24
v40@0:8@"AFConnection"16@"AceObject<SAAceCommand>"24@?<v@?@"AceObject<SAAceCommand>">32
v40@0:8@"AFConnection"16@"NSError"24@"NSString"32
v40@0:8@"AFConnection"16@"NSURL"24@?<v@?B>32
v48@0:8@"AFConnection"16@"NSString"24@"NSURL"32@?<v@?B>40
v28@0:8@"AFConnection"16B24
v28@0:8@"AFConnection"16I24
v32@0:8@"AFConnection"16@"NSDictionary"24
v36@0:8@"AFConnection"16B24@"NSDictionary"28
v40@0:8@"AFConnection"16@"NSDictionary"24@"NSURL"32
v32@0:8@"AFConnection"16@"INImage"24
v32@0:8@"AFConnection"16@"NSString"24
v40@0:8@"AFConnection"16@"NSString"24@"NSError"32
v40@0:8@"AFConnection"16@"NSString"24@?<v@?B>32
v40@0:8@"AFConnection"16@"AFRequestInfo"24@?<v@?B>32
v32@0:8@"AFConnection"16@"AFAudioPlaybackRequest"24
v40@0:8@"AFConnection"16@"AFAudioPlaybackRequest"24@"NSError"32
v32@0:8@"AFConnection"16Q24
v48@0:8@"AFConnection"16q24@"INIntent"32@?<v@?BB>40
v36@0:8@16@24I32
v40@0:8@16q24@?32
v48@0:8@16@24@32@40
v32@0:8@"AFConnection"16@"AFXPCWrapper"24
v36@0:8@"AFConnection"16@"NSString"24I32
v40@0:8@"AFConnection"16q24@?<v@?dd@"NSError">32
v32@0:8@"AFConnection"16@"NSError"24
v32@0:8@"AFConnection"16@"SASSpeechRecognized"24
v32@0:8@"AFConnection"16@"SASSpeechPartialResult"24
v40@0:8@"AFConnection"16@"AFSpeechInterpretation"24@"NSString"32
v48@0:8@"AFConnection"16@"NSArray"24@"NSArray"32@"NSString"40
v44@0:8@16@24@32B40
v56@0:8@16@24@32@40Q48
v32@0:8@16d24
v48@0:8@16@24@32B40B44
v24@0:8@"AFDictationConnection"16
v32@0:8@"AFDictationConnection"16@"AFDictationOptions"24
v32@0:8@"AFDictationConnection"16@"NSString"24
v32@0:8@"AFDictationConnection"16@"NSError"24
v40@0:8@"AFDictationConnection"16@"NSString"24@"NSDictionary"32
v44@0:8@"AFDictationConnection"16@"NSString"24@"NSDictionary"32B40
v32@0:8@"AFDictationConnection"16@"SASMultilingualSpeechRecognized"24
v48@0:8@"AFDictationConnection"16@"NSArray"24@"NSString"32@40
v56@0:8@"AFDictationConnection"16@"NSArray"24@"NSString"32@40Q48
v40@0:8@"AFDictationConnection"16@"NSArray"24@"NSString"32
v32@0:8@"AFDictationConnection"16@"SASSpeechPartialResult"24
v32@0:8@"AFDictationConnection"16d24
v40@0:8@"AFDictationConnection"16@"NSFileHandle"24@"NSError"32
v48@0:8@"AFDictationConnection"16@"NSArray"24@"NSString"32B40B44
v32@0:8@"AFDictationConnection"16@"AFSpeechPackage"24
@24@0:8@16
v48@0:8@16@?24@?32@?40
v20@0:8B16
v36@0:8@16@24B32
f16@0:8
@20@0:8B16
@"AXDispatchTimer"
@"AFDictationConnection"
softlink:r:path:/System/Library/PrivateFrameworks/HearingUtilities.framework/HearingUtilities
v8@?0
AXCustomPronunciations
AXVoiceOverActivities
Reconciled iCloud pronunciations: adding: %{private}@
Did not find a change, not updating
ICLOUD[%p]: Waiting 5s to publish, deleting %@
Created an empty length archive, not synching
ICLOUD[%p]: Publishing data to iCloud %{private}@
Custom pronunciations settings updated
refresh-voices
It's been too soon since we last updated our available voices. We need to wait longer. %f
/Library/Caches/com.apple.xbs/Sources/AccessibilityFrameworks_Sim/AccessibilityFrameworks-2933.26/Source/AXAssetAndDataServer/AXAssetAndDataServer.m
-[AXAssetAndDataServer _updateCachedTTSVoiceAssets]_block_invoke
We just refreshed voices, why are they missing?
-[AXAssetAndDataServer _handleExtantVoices]
Why is name nil for %@ %@
VoiceId
Language
Name
en-US
com.apple.speech.synthesis.voice
@"NSArray"8@?0
@"NSString"40@?0@"TTSVoiceAsset"8@"NSString"16q24@"NSString"32
@"NSString"32@?0@"TTSVoiceAsset"8@"NSString"16q24
B24@?0@"TTSVoiceAsset"8Q16
LastSpeechAssetUpdateCheck
com.apple.accessibility.api
com.apple.private.kernel.jetsam
com.apple.CoreRoutine.preferences
com.apple.TVServices.DisplayManager
com.apple.accessibility.voiceover
com.apple.appletv.pbs.display-manager-service-access
com.apple.accessibility.axctl
delete
options
audioLevel
Adding
Removing
v32@?0@8Q16^B24
v16@?0@"NSArray"8
got transcription results: %@
AXSpeechPronunciationClient
results
v24@?0@"NSDictionary"8@"NSError"16
Fail: %@, %d
Error
error
Starting pronunciation dictation: %@, %@, %@
q24@?0@"TTSVoiceAsset"8@"TTSVoiceAsset"16
v28@?0d8B16@"NSError"20
v16@?0@"MAProgressNotification"8
v16@?0q8
AXAssetAndDataServer
AXUIService
NSObject
AFAssistantUIService
AFSpeechDelegate
AFDictationDelegate
init
sharedInstance
mobileAssetWorkQueue
initWithTargetSerialQueue:
setAutomaticallyCancelPendingBlockUponSchedulingNewBlock:
_monitoriCloudVoiceOverData
_icloudDataChanged:
afterDelay:processBlock:
defaultStore
dataForKey:
initForReadingFromData:error:
arrayWithObjects:count:
setWithArray:
decodeObjectOfClasses:forKey:
count
_iCloudReconcileDataStore:
_iCloudReconcileActivitiesStore:
synchronize
syncPronunciationsWithCloudKit
_reconcilePronunciations
_reconcileActivities
voiceOverActivities
countByEnumeratingWithState:objects:count:
indexOfObject:
objectAtIndexedSubscript:
isIdenticalTo:
setVoiceOverActivities:
ignoreLogging
identifier
customPronunciationSubstitutions
containsObject:
_syncToWatch
setCustomPronunciationSubstitutions:
copy
archivedDataWithRootObject:requiringSecureCoding:error:
length
setData:forKey:
_saveCustomPronunciationsToExternalsAndDelete:
_customPronunciationsSettingsChanged
registerUpdateBlock:forRetrieveSelector:withListener:
_saveActivitiesToExternalsAndDelete:
defaultCenter
addObserver:selector:name:object:
currentLocale
languageCode
letterFeedbackEnabled
phoneticFeedbackEnabled
wordFeedbackEnabled
quickTypeWordFeedbackEnabled
speakCorrectionsEnabled
isEqualToString:
userDidSelectVoiceForLanguage:source:
availableVoicesForLanguageCode:queryingMobileAssets:
isDefault
speechVoiceIdentifierForLanguage:source:exists:
voiceForIdentifier:
footprint
isNeuralSiriVoiceIdentifier:
setSpeechVoiceIdentifier:forLanguage:source:
language
informSiriAboutVoiceUsageForIdentifier:forLanguage:add:
dictionary
numberWithBool:
setObject:forKeyedSubscript:
refreshAllAvailableVoices
allAvailableVoices
setCurrentVoices:
availableVoices:
currentVoices
_handleExtantVoices
objectForKeyedSubscript:
_updateDefaultVoiceIfNecessaryForLanguage:source:
mobileAssetDownloadQueue
_downloadAssetsForSelectedVoices
attributes
firstObject
stringByReplacingOccurrencesOfString:withString:
_combinedVocalizerAssets:
boolValue
intValue
_languageForAsset:
setIsCombinedVoice:
setNonCombinedVoiceId:
objectForKey:
allInstalledAssetsForVoiceId:
setIdentifier:
setLanguage:
setNonLocalizedNameWithoutQuality:
setQuality:
setGender:
setIsInstalled:
unsignedIntegerValue
setAssetSize:
setCanBeDownloaded:
addObject:
_vocalizerAssets
gender
languages
name
fileSize
numberWithLongLong:
isInstalled
state
supportsAlex
nameForVoiceIdentifier:
hasPrefix:
canBeDownloaded
isCombinedFootprint
supportsSiri
array
neural
_normalizedSiriName:
axSafelyAddObject:
_handleSiriStyleVoices:assetRetrieval:voiceIdentifier:voiceName:
axFilterObjectsUsingBlock:
setExtantVoices:
lowercaseString
migratePrefToNeuralVoiceIfNecessaryForVoice:
isSiriVoiceIdentifier:
startDownloadingVoice:
standardUserDefaults
date
dateByAddingTimeInterval:
setObject:forKey:
timeIntervalSinceReferenceDate
extantVoices
_checkForMacinTalkAssetUpdates
_checkForVocalizerAssetUpdates
_updateCachedTTSVoiceAssets
setWithObjects:
_checkForAssets:
valueForKey:
secureUnarchiveData:withExpectedClass:otherAllowedClasses:
_startPronunciationSession:
_cancelPronunciationSession
_stopPronunciationSession
_pronunciationAudioLevel
numberWithFloat:
dictionaryWithObjects:forKeys:count:
selectedSpeechVoiceIdentifiers
installedAssetsForLanguage:voiceType:
axSafelyAddObjectsFromArray:
setAutoDownloadedVoiceAssets:
informSiriAboutVoiceListWithLanguage:add:voiceId:
setSiriAutoUpdateListInitialized:
systemLanguageID
isVocalizerVoiceIdentifier:
numberWithLong:
type
mutableCopy
removeObject:
removeObjectsInArray:
numberWithUnsignedInteger:
enumerateObjectsUsingBlock:
getAutoDownloadedVoiceAssets:
_handleFailureForSpeechPronunciation:
averagePower
stopSpeechWithOptions:
cancelSpeech
phonemeSuggestions
clientMessengerWithIdentifier:
backgroundAccessQueue
sendAsynchronousMessage:withIdentifier:targetAccessQueue:completion:
endSession
description
setDelegate:
setTranscriptionMode:
orthography
setOrthography:
initWithActivationEvent:
setIsEyesFree:
setEndpointerOperationMode:
startDictationWithLanguageCode:options:speechOptions:
initWithType:
setDoNotBlockBeforeFirstUnlock:
returnTypes:
queryMetaDataSync
results
assetsForLanguage:voiceType:
masteredVersion
sortedArrayUsingComparator:
setAllowsCellularAccess:
totalWritten
totalExpected
_alexAssets
_beginDownloadIfNecessaryForAssets:
_vocalizerAssetsForLanguage:
_beginDownloadIfNecessaryForSiriAssets:
voiceAssetForVoiceId:
downloadAsset:progressHandler:
selectedIdsForTesting
isCompactVocalizerVoiceIdentifier:
isAssetManagedBySiriForVoiceId:
_handleSiriAssetDownload:
_handleMobileAssetDownload:
mobileAssetForVoiceId:
_voiceFromInternalVoiceListWithIdentifier:
isCombinedVocalizerVoiceIdentifier:
compare:options:
cleanUpExtraInstalledAssetsIfNecessary:
inUnitTestMode
replaceTestAsset:withAsset:
spaceCheck:
_handleAssetProgress:asset:installedAsset:
attachProgressCallBack:
_mobileAssetDownloadOptions
_updateAsset:existingAsset:
startDownload:then:
assetType
assetIsDownloading:
purgeAsset:
alexLocalAssetURL
getLocalFileUrl
isEqual:
_addToDownloadQueue:
purge:
_purgeExistingAsset:inFavorOfAsset:
setAlexLocalAssetURL:
defaultManager
path
setAttributes:ofItemAtPath:error:
possibleRequiredEntitlementsForProcessingMessageWithIdentifier:
class
self
performSelector:
performSelector:withObject:
performSelector:withObject:withObject:
isProxy
isKindOfClass:
isMemberOfClass:
conformsToProtocol:
respondsToSelector:
retain
release
autorelease
retainCount
zone
hash
superclass
debugDescription
TQ,R
T#,R
T@"NSString",R,C
processMessage:withIdentifier:fromClientWithIdentifier:error:
messageWithIdentifierShouldBeProcessedAsynchronously:
processMessageAsynchronously:withIdentifier:fromClientWithIdentifier:completion:
accessQueueForProcessingMessageWithIdentifier:
messageWithIdentifierRequiresWritingBlock:
processInitializationMessage:
serviceWasFullyInitialized
connectionWillBeInterruptedForClientWithIdentifier:
requiredEntitlementForProcessingMessageWithIdentifier:
assistantConnectionRequestWillStart:
assistantConnectionDismissAssistant:
assistantConnection:dismissAssistantWithReason:
assistantConnectionRequestFinished:
assistantConnection:receivedCommand:completion:
assistantConnection:requestFailedWithError:requestClass:
assistantConnection:openURL:completion:
assistantConnection:openApplicationWithBundleID:URL:completion:
assistantConnection:shouldSpeak:
assistantConnection:didChangeAudioSessionID:
assistantConnectionAudioSessionDidBeginInterruption:
assistantConnectionAudioSessionDidBeginInterruption:userInfo:
assistantConnectionAudioSessionDidEndInterruption:shouldResume:
assistantConnectionAudioSessionDidEndInterruption:shouldResume:userInfo:
assistantConnectionWillStartAcousticIDRequest:
assistantConnectionDidDetectMusic:
assistantConnection:didFinishAcousticIDRequestWithSuccess:
assistantConnection:setUserActivtiyInfoAndMakeCurrent:webpageURL:
assistantConnectionInvalidateCurrentUserActivity:
assistantConnection:wantsToCacheImage:
assistantConnection:extensionRequestWillStartForApplication:
assistantConnection:extensionRequestFinishedForApplication:error:
assistantConnection:startUIRequestWithText:completion:
assistantConnection:startUIRequestWithInfo:completion:
assistantConnection:willStartAudioPlaybackRequest:
assistantConnection:didStartAudioPlaybackRequest:
assistantConnection:didStopAudioPlaybackRequest:error:
assistantConnection:didHandleQuickStopWithAction:
assistantConnection:willProcessStartPlayback:intent:completion:
assistantConnection:willProcessStartPlayback:
assistantConnection:startPlaybackDidFail:
assistantConnection:audioSessionWillBecomeActive:
assistantConnection:audioSessionDidBecomeActive:
assistantConnection:willProcessAppLaunchWithBundleIdentifier:
assistantConnection:appLaunchFailedWithBundleIdentifier:
assistantConnectionSpeechRecordingWillBegin:
assistantConnection:speechRecordingWillBeginWithInputAudioPowerXPCWrapper:
assistantConnection:speechRecordingDidBeginOnAVRecordRoute:
assistantConnection:speechRecordingDidBeginOnAVRecordRoute:audioSessionID:
assistantConnection:speechRecordingDidChangeAVRecordRoute:
assistantConnectionSpeechRecordingDidDetectStartpoint:
assistantConnection:speechRecordingPerformTwoShotPromptWithType:completion:
assistantConnectionSpeechRecordingDidEnd:
assistantConnectionSpeechRecordingDidCancel:
assistantConnection:speechRecordingDidFail:
assistantConnectionDidChangeAudioRecordingPower:
assistantConnection:speechRecognitionDidFail:
assistantConnection:speechRecognized:
assistantConnection:speechRecognizedPartialResult:
assistantConnection:recognizedAdditionalSpeechInterpretation:refId:
assistantConnection:recognitionUpdateWithPhrases:utterances:refId:
assistantConnection:recognitionUpdateWillBeginForTask:
dictationConnectionSpeechRecordingWillBegin:
dictationConnectionSpeechRecordingDidBegin:
dictationConnection:speechRecordingDidBeginWithOptions:
dictationConnection:didBeginLocalRecognitionWithModelInfo:
dictationConnectionSpeechRecordingDidEnd:
dictationConnectionSpeechRecordingDidCancel:
dictationConnection:speechRecordingDidFail:
dictationConnection:speechRecognitionDidFail:
dictationConnection:didDetectLanguage:confidenceScores:
dictationConnection:didDetectLanguage:confidenceScores:isConfident:
dictationConnection:didRecognizeMultilingualSpeech:
dictationConnection:languageDetectorFailedWithError:
dictationConnection:didRecognizePhrases:languageModel:correctionIdentifier:
dictationConnection:didRecognizePhrases:languageModel:correctionIdentifier:replacingPreviousPhrasesCount:
dictationConnection:didRecognizeTokens:languageModel:
dictationConnection:didRecognizePartialResult:
dictationConnection:didProcessAudioDuration:
dictationConnectionSpeechRecognitionDidSucceed:
dictationConnection:didRecognizeTranscriptionObjects:languageModel:
dictationConnnectionDidChangeAvailability:
dictationConnection:didFinishWritingAudioFile:error:
dictationConnection:didReceiveSearchResults:recognizedText:stable:final:
dictationConnection:didRecognizePackage:
_isVocalizerAsset:
_purgeExistingSiriAsset:inFavorOfAsset:
.cxx_destruct
_lastTTSVoiceAssetUpdate
_pronunciationPusher
_lastActivitiesICloudRetrieval
_activitiesPusher
_isUpdatingCachedTTSVoices
_dictationConnection
_isRecording
Checking pronunciations
Got pronunciations: %{private}@, count: %d
Exception decoding data: %@
Checking activities
Got activities: count: %d
did not find %@ in %@
%@ not identical to %@
Reconciled iCloud activities: adding: %{private}@
Did not find a change, not updating
%{public}@
Skipping publishing to iCloud because we just retrieved this data
Created an empty length archive, not synching
ICLOUD[%p]: Publishing data to iCloud %{private}@
Not updating default spoken content voice because no speech features are enabled.
Not updating default spoken content voice because switch control is not enabled.
Not updating default voice because the voice doesn't match our system language.
Switching voice from %@ to %@ for source %ld because default voice has changed, and the user hasn't selected one for language %@
Updated TextToSpeech's available voices cache with voices: %@
Vocalizer assets example[0]: %{public}@
Victoria macinTalkAsset: %{public}@
local macinTalkAsset: %{public}@
remote macinTalkAsset: %{public}@
New siri names %@
Original old siri assets %@
Filtered out old siri assets so we're left with %@
Siri voices not supported
Set extant voices: %{public}@
Retrieved siri-style assets: %{public}@
Siri style: skipping non premium: %@
Siri style: skipping neural: %@
Processed Siri-style assets: %{public}@
made siri voice %@ with asset %@
Updating spoken content settings because the selected voice was a Siri voice, but a neural voice matching the same name and language: %@ was available
Downloading %@ because a non-neural variant was previously selected, but a neural variant is now available
Removing %@ from active siri list, because neural voice was available and non-neural version wasn't in use outside of Speech Features
Force checking speech assets: %d date: %{public}@
No previous LastSpeechAssetUpdateCheck was found. Setting date earlier: %{public}@.
Has speech asset data: %d
Not checking voice assets because it's too soon: %{public}@ - only %f seconds 
Checking on assets now
Asset is installed and is in selected voices: %@
Inform siri about %@
%@ lang: %@ gender: %@ type: %@ name: %@ voice to active siri voice list
Starting with voices: %@
Removing siri auto download voice: %@
Setting Siri auto-downloaded voice[%@]: %@
MAAssetQuery error fetching results %{public}@ %{public}@ %{public}@
Asset progress: %{public}@ - %f
Checking assets: %{public}@
checking assets[%{public}@] %{public}@
Handling download for %@ %@
Finished downloading asset: %@, %@
Updating siri voice list usage after downloading a selected voice ID that was missing
could not parse voice identifier %{public}@
could not find asset for voice identifier %{public}@ but the voice was installed. This is expected if it is a built-in voice.
could not find asset for voice identifier %{public}@
selected voice %{public}@ is already downloading.
selected voice %{public}@ is required by the OS.
selected voice %{public}@ is installed but not in the catalog.
selected voice %{public}@ has unknown state.
selected voice %{public}@ is already installed.
selected voice %{public}@ %@ is not present. Beginning download.
Asset not present, but no space to download voice %{public}@: %{public}@
asset installed with os
Adding asset to queue: %{public}@
No siri asset installed
Installed asset %{public}@, latest asset: %{public}@
Detected new Asset, downloading: %{public}@
Enough disk space available - start %{public}@
Updating Installed Asset: %{public}@
Error: %{public}@
Asset state: %@
Error: Not enough disk space
No newer asset than %{public}@ - [installed asset %{public}@]
Purging old asset: %{public}@
No macintalk or siri asset installed
No newer asset than %{public}@ - %@ [installed asset %{public}@]
Purged old asset: %{public}@, %{public}@
Trying to update unknown asset (expecting Macintalk, Combined, or Siri): %{public}@
Could not remove file protection from: %{public}@
Could not purge old asset: %{public}@
@16@0:8
@24@0:8Q16
B24@0:8@16
#16@0:8
@24@0:8:16
@32@0:8:16@24
@40@0:8:16@24@32
B16@0:8
B24@0:8#16
B24@0:8:16
Vv16@0:8
Q16@0:8
^{_NSZone=}16@0:8
B24@0:8@"Protocol"16
@"NSString"16@0:8
@48@0:8@16Q24@32^@40
B24@0:8Q16
v48@0:8@16Q24@32@?40
v24@0:8@16
v16@0:8
@"NSDictionary"48@0:8@"NSDictionary"16Q24@"NSString"32^@40
v48@0:8@"NSDictionary"16Q24@"NSString"32@?<v@?@"NSDictionary"@"NSError">40
@"AXAccessQueue"24@0:8Q16
v24@0:8@"NSDictionary"16
v24@0:8@"NSString"16
@"NSString"24@0:8Q16
@"NSSet"24@0:8Q16
v32@0:8@16q24
v40@0:8@16@24@?32
v40@0:8@16@24@32
v48@0:8@16@24@32@?40
v28@0:8@16B24
v28@0:8@16I24
v32@0:8@16@24
v36@0:8@16B24@28
v32@0:8@16Q24
v48@0:8@16q24@32@?40
v24@0:8@"AFConnection"16
v32@0:8@"AFConnection"16q24
v40@0:8@"AFConnection"16@"AceObject<SAAceCommand>"24@?<v@?@"AceObject<SAAceCommand>">32
v40@0:8@"AFConnection"16@"NSError"24@"NSString"32
v40@0:8@"AFConnection"16@"NSURL"24@?<v@?B>32
v48@0:8@"AFConnection"16@"NSString"24@"NSURL"32@?<v@?B>40
v28@0:8@"AFConnection"16B24
v28@0:8@"AFConnection"16I24
v32@0:8@"AFConnection"16@"NSDictionary"24
v36@0:8@"AFConnection"16B24@"NSDictionary"28
v40@0:8@"AFConnection"16@"NSDictionary"24@"NSURL"32
v32@0:8@"AFConnection"16@"INImage"24
v32@0:8@"AFConnection"16@"NSString"24
v40@0:8@"AFConnection"16@"NSString"24@"NSError"32
v40@0:8@"AFConnection"16@"NSString"24@?<v@?B>32
v40@0:8@"AFConnection"16@"AFRequestInfo"24@?<v@?B>32
v32@0:8@"AFConnection"16@"AFAudioPlaybackRequest"24
v40@0:8@"AFConnection"16@"AFAudioPlaybackRequest"24@"NSError"32
v32@0:8@"AFConnection"16Q24
v48@0:8@"AFConnection"16q24@"INIntent"32@?<v@?BB>40
v36@0:8@16@24I32
v40@0:8@16q24@?32
v48@0:8@16@24@32@40
v32@0:8@"AFConnection"16@"AFXPCWrapper"24
v36@0:8@"AFConnection"16@"NSString"24I32
v40@0:8@"AFConnection"16q24@?<v@?dd@"NSError">32
v32@0:8@"AFConnection"16@"NSError"24
v32@0:8@"AFConnection"16@"SASSpeechRecognized"24
v32@0:8@"AFConnection"16@"SASSpeechPartialResult"24
v40@0:8@"AFConnection"16@"AFSpeechInterpretation"24@"NSString"32
v48@0:8@"AFConnection"16@"NSArray"24@"NSArray"32@"NSString"40
v44@0:8@16@24@32B40
v56@0:8@16@24@32@40Q48
v32@0:8@16d24
v48@0:8@16@24@32B40B44
v24@0:8@"AFDictationConnection"16
v32@0:8@"AFDictationConnection"16@"AFDictationOptions"24
v32@0:8@"AFDictationConnection"16@"NSString"24
v32@0:8@"AFDictationConnection"16@"NSError"24
v40@0:8@"AFDictationConnection"16@"NSString"24@"NSDictionary"32
v44@0:8@"AFDictationConnection"16@"NSString"24@"NSDictionary"32B40
v32@0:8@"AFDictationConnection"16@"SASMultilingualSpeechRecognized"24
v48@0:8@"AFDictationConnection"16@"NSArray"24@"NSString"32@40
v56@0:8@"AFDictationConnection"16@"NSArray"24@"NSString"32@40Q48
v40@0:8@"AFDictationConnection"16@"NSArray"24@"NSString"32
v32@0:8@"AFDictationConnection"16@"SASSpeechPartialResult"24
v32@0:8@"AFDictationConnection"16d24
v40@0:8@"AFDictationConnection"16@"NSFileHandle"24@"NSError"32
v48@0:8@"AFDictationConnection"16@"NSArray"24@"NSString"32B40B44
v32@0:8@"AFDictationConnection"16@"AFSpeechPackage"24
@24@0:8@16
v48@0:8@16@?24@?32@?40
v20@0:8B16
v36@0:8@16@24B32
f16@0:8
@20@0:8B16
@"AXDispatchTimer"
@"AFDictationConnection"
