INTERNAL: bwd
^9giP9giP9giP9giP9
?fff
r1?~
U?DH
y4#@y4#@y4#@y4#@
B6"9B6"9B6"9B6"9N
!rr{>
!rr{>
}76R
YVpA
YVpA
YVpA
<7p7M}
<7p7M}
<7p7M}
:I~D
:I~D
:I~D
:I~D
?G/d?
*?9s
)>e\
^9giP9
BNNS Dropout: Error layer_params is NULL
BNNS Dropout: Error rate can't be a negative number.
BNNS Dropout: Error Input must be BNNSDataTypeFloat32 type
BNNS Dropout: Error Output must be BNNSDataTypeFloat32 type
BNNS Dropout: Error memory allocation failed
BNNS Dropout: Error input descriptor is invalid
BNNS Dropout: Error output descriptor is invalid
BNNS Dropout: Input and output descriptors must have the same shape.
BNNS Dropout: Tensors with dimension greater than %u are not supported.
BNNS Dropout: Error filter is NULL
BNNS Dropout: Error wrong filter type, filter is not Dropout
BNNS Dropout: input pointer is NULL
BNNS Dropout: batch_size>1 and in_stride is 0
BNNS Dropout: output pointer is NULL
BNNS Dropout: batch_size>1 and out_stride is 0
BNNS BatchNorm: Error filter is NULL
BNNS Dropout: input_delta layout, shape and stride must match layer_params->i_desc
BNNS Dropout: output_delta layout, shape and stride must match layer_params->o_desc
Unsupported target %d
v16@?0Q8
v24@?0Q8^{pcg32_xsh_rr<16>=}16
Shouldn't call get_data_size with Indexded2 or Indexed4, switch to get_data_bits
BNNS Tensor Contraction: inputB pointer must not be NULL unless B is a weight or the operation is quadratic.
BNNS Tensor Contraction: After adding batch dimension, inputA tensor has too many indices
BNNS Tensor Contraction: After adding batch dimension, inputB tensor has too many indices
BNNS Tensor Contraction: After adding batch dimension, output tensor has too many indices
BNNS Tensor Contraction: inB_delta calculation is non-sensical for a contraction of a tensor with itself.
BNNS Tensor Contraction: inA_delta calculation requires inA to be non-NULL
BNNS Tensor Contraction: inA_delta calculation requires out_delta (and out_delta->data) to be non-NULL
inA_delta
BNNS Tensor Contraction: After adding batch dimension, out_delta tensor has too many indices
BNNS Tensor Contraction: After adding batch dimension, inputA_delta tensor has too many indices
BNNS Tensor Contraction: inA_delta calculation requires inB to be non-NULL
BNNS Tensor Contraction: inB_delta calculation requires inA to be non-NULL
BNNS Tensor Contraction: inB_delta calculation requires out_delta (and out_delta->data) to be non-NULL
inB_delta
BNNS Tensor Contraction: After adding batch dimension, inputB_delta tensor has too many indices
BNNS Tensor Contraction: beta must be 0.0 or 1.0 (beta=%.2f)
BNNS Tensor Contraction: inputA descriptor is illegal "%s"
BNNS Tensor Contraction: inputB descriptor is illegal "%s"
BNNS Tensor Contraction: ouput descriptor is illegal "%s"
BNNS Tensor Contraction: invalid op string "%s"
BNNS Tensor Contraction: both inputA and inputB cannot be weights "%s"
BNNS Tensor Contraction: Wildcard index '*' must appear on both sides of operation or neither: "%s"
BNNS Tensor Contraction: Wildcard index '*' must appear at most once per index set: "%s"
BNNS Tensor Contraction: Wildcard index '*' must appear consistently only at start or end of index sets: "%s"
BNNS Tensor Contraction: number of indices from operation (%td) does not match dimension of input A descriptor (%zu) "%s"
BNNS Tensor Contraction: number of indices from operation (%td) does not match dimension of input B descriptor (%zu) "%s"
BNNS Tensor Contraction: number of indices from operation (%td) does not match dimension of output descriptor (%zu) "%s"
BNNS Tensor Contraction: '%c', index %zu of inputA has size %zu and index %td of output has size %zu, but sizes are required to match
BNNS Tensor Contraction: '%c', index %d of inputA has size %zu and index %td of inputB has size %zu, but sizes are required to match
BNNS Tensor Contraction: index '%c' of input A does not match any index of inputB or output "%s"
BNNS Tensor Contraction: '%c', index %d of inputB has size %zu and index %td of output has size %zu, but sizes are required to match
BNNS Tensor Contraction: index '%c' of input B does not match any index of inputA or output "%s"
BNNS Tensor Contraction: index '%c' of output does not match any index of inputA or inputB "%s"
Data type of output not supported
Data type of inputB not supported
Data type of inputA not supported
%c_*ki, %c_*jk -> c_*ij
%c_*ki, %c_*kj -> c_*ij
%c_*ik, %c_*jk -> c_*ij
%c_*ik, %c_*kj -> c_*ij
BNNS Tensor Contraction: %s shape does not match inputA shape in dimension %ld (%zu vs %zu)
BNNS Tensor Contraction: out_delta shape does not match output shape in dimension %ld (%zu vs %zu)
BNNS Tensor Contraction: Wildcard index '*' can only appear as first or last index of "%s_%s"
BNNS Tensor Contraction: repeated indices "%s_%s"
v24@?0Q8^v16
v16@?0^v8
BNNS SoftMax: sum result shouldn't be 0, something is wrong
malloc activation_grad failed
apply_specialized_convolution_PKT_1: o_height less then 4 isn't supported
BNNS Copy Filter: in and out must not be NULL.
BNNS Copysum Filter: unsupported data type conversion
internal bwd copysum
BNNS Tensor Contraction: Input must not be NULL
BNNS Tensor Contraction: Output must not be NULL
BNNS Tensor Contraction: index '%c' appears multiple times on right-hand side, but with different sizes
BNNS Tensor Contraction: '%c', index %d of input has size %zu and index %td of output has size %zu, but sizes are required to match
BNNS Tensor Contraction: index '%c' appears multiple times on left-hand side, but with different sizes
BNNS Tensor Contraction: Unsupported data type conversion %sfrom %s to %s
 (in summation)
 (in copy)
BNNSTranspose: src and dest have a different number of dimensions.
BNNSTranspose: specified axes must be within range for supplied tensor (axes: %zu, %zu) dimension of tensor is %zu
internal_transpose
internal_copy
int8
int16
int32
uint8
uint16
uint32
fp16
fp32
indexed8
indexed4
indexed2
unknown
BNNS Convolution version2: Error minimum float w_pack weight packing value is 32
BNNS Dequantize: input and output can't be null
BNNS Dequantize: in place conversion not supported
BNNS Dequantize: only __fp16 output is supported
BNNS Dequantize: lut is null and needed for Indexed input type
BNNS Dequantize: input type not supported
BNNS batch norm forward: malloc for mean failed
BNNS batch norm forward: malloc for var failed
BNNS batch norm forward: malloc for isqrtvar failed
BNNS batch norm backward: malloc for dbeta failed
BNNS batch norm backward: malloc for dgamma failed
BNNS instance norm backward: malloc for dbeta failed
BNNS instance norm backward: malloc for dgamma failed
BNNS group norm backward: malloc for dbeta failed
BNNS group norm backward: malloc for dgamma failed
BNNS Fully Connected: Unexpected result, should be at least 1
BNNS Fully Connected: Unexpected result, at least 1 batch should remain
BNNS Fully Connected: Unexpected result, at least 1 output should remain
BNNS Fully Connected: Failed to fit in memory limit, using generic code
BNNS Fully Connected: weights conversion buffer isn't allocated.
BNNS Fully Connected Backward: output delta is NULL
BNNS Fully Connected Backward: only float32 delta are supported
BNNS Fully Connected Backward: output delta layout is BNNSDataLayoutImageCHW
BNNS Fully Connected Backward: input delta layout is BNNSDataLayoutImageCHW
BNNS Fully Connected Backward: bias delta layout is BNNSDataLayoutImageCHW
BNNS Fully Connected Backward: Fusion of compute with activation is unsupported for given activation function
BNNS Fully Connected Backward: failed to apply activation backward
BNNS Fully Connected Backward: failed to allocate temporary buffer for summation
BNNS Fully Connected Backward: user requested weights delta but didn't supply original inputs
BNNS Fully Connected Backward: only fp32 inputs/weights/outputs are supported for backward
BNNS Fully Connected Backward: weights sizes doesn't match weights delta sizes
BNNS Fully Connected Create: failed to allocated memory (%zu bytes))
BNNS Fully Connected: using generic code to convert data 1 element each time
BNNS Fully Connected: output data type not supported for conversion
BNNS Fully Connected: input must be a 1D array
BNNS Fully Connected: input descriptor is illegal
BNNS Fully Connected: output must be a 1D array
BNNS Fully Connected: output descriptor is illegal
BNNS Fully Connected: weights row size should match input vector size
BNNS Fully Connected: weights number of rows should match output vector size
BNNS Fully Connected: weights must be a 2D array
BNNS Fully Connected: weights descriptor is illegal
BNNS Fully Connected: Output data types supported in this version: Float32, Float16
BNNS Fully Connected: Bias data types supported in this version: Float32
BNNS Fully Connected: Float16 output isn't supported with bias or activation function
BNNS Fully Connected: Input/Weight data types supported in this version: Float32, Int8, Int16
BNNS Fully Connected: Data type of weights and input should match
BNNS Fully Connected: size computation wraparound -I %zu -O %zu
BNNS Fully Connected: bias size computation wraparound -O %zu bias data type %x
BNNS InTopK: invalid axis value %zu (input tensor has %zu dimensions)
BNNS InTopK: Unsupported input data type for this operation (supported data types are: fp32)
BNNS TopK: Unsupported best_indices data type for this operation (supported data types are: int32)
BNNS InTopK: input tensor with axis removed is not congruent with test_indices tensor
BNNS InTopK: input tensor with axis removed is not congruent with output tensor
BNNS TopK: input descriptor is illegal
BNNS TopK: best_values descriptor is illegal
BNNS TopK: best_indices descriptor is illegal
BNNS TopK: Unsupported input data type for this operation (supported data types are: fp32)
BNNS TopK: Unsupported best_values data type for this operation (supported data types are: fp32)
BNNS TopK: invalid axis value %zu (input tensor has %zu dimensions)
BNNS TopK: best_indices tensor is not consistent with input tensor
BNNS TopK: best_values tensor is not consistent with input tensor
BNNS TopK: best_indices tensor size[%zu]=%zu should be K=%zu
BNNS TopK: best_values tensor size[%zu]=%zu should be K=%zu
malloc
BNNS Optimizer: Error unsupported optimizer function
BNNS Optimizer: Error OptimizerAlgFields is NULL
BNNS Optimizer: Error 0 time_step is not valid for Adam optimizer. minimal time step value is 1
BNNS Optimizer: parameter array pointer is NULL
BNNS Optimizer: gradient array pointer is NULL
BNNS Optimizer: accumulator array pointer is NULL
BNNS Optimizer: parameter pointer number %zu is NULL
BNNS Optimizer: gradient pointer number %zu is NULL
BNNS Optimizer: accumulator pointer number %zu is NULL
BNNS Optimizer Error in parameter %zu: Parameter and Gradient descriptors must have the same sizes and strides
BNNS Optimizer Error in parameter %zu: Parameter and Accumulator descriptors must have the same sizes and strides
BNNS Optimizer Error in parameter %zu: Parameter data type is not BNNSDataTypeFloat32
BNNS Optimizer Error in gradient %zu: Gradient data type is not BNNSDataTypeFloat32
BNNS Optimizer Error in accumulator %zu: Accumulator data type is not BNNSDataTypeFloat32
BNNS Optimizer Error in parameter %zu: parameter descriptor must be contiguous such that stride[0]=1 and for every N>0 stride[N]=stride[N-1]*size[N-1]
BNNS Optimizer: Adam optimizer require accumulators pointer to be valid
BNNS Optimizer: accumulator1 pointer number %zu is NULL
BNNS Optimizer: accumulator2 pointer number %zu is NULL
BNNS Optimizer Error in parameter %zu: Parameter and Accumulator1 descriptors must have the same sizes and strides
BNNS Optimizer Error in parameter %zu: Parameter and Accumulator2 descriptors must have the same sizes and strides
BNNS Optimizer Error in accumulator1 %zu: Accumulator data type is not BNNSDataTypeFloat32
BNNS Optimizer Error in accumulator2 %zu: Accumulator data type is not BNNSDataTypeFloat32
BNNS Optimizer: RMSProp optimizer require accumulators pointer to be valid
BNNS Optimizer: accumulator_n pointer number %zu is NULL
BNNS Optimizer: accumulator_g pointer number %zu is NULL
BNNS Optimizer: momentum pointer number %zu is NULL
BNNS Optimizer Error in accumulator_n %zu: Accumulator data type is not BNNSDataTypeFloat32
BNNS Optimizer Error in accumulator_g %zu: Accumulator data type is not BNNSDataTypeFloat32
BNNS Optimizer Error in momentum %zu: Accumulator data type is not BNNSDataTypeFloat32
BNNS unsupported sgd variant
BNNS Optimizer: Adam beta1 and beta2 must be positive
BNNS Optimizer: Adam beta1 and beta2 must be strictly less than 1
BNNS Optimizer: RMSProp alpha must be in (0,1)
BNNS Sparse Fully Connected: input must be a 1D array
BNNS Sparse Fully Connected: input descriptor is illegal
BNNS Sparse Fully Connected: output must be a 1D array
BNNS Sparse Fully Connected: output descriptor is illegal
BNNS Sparse Fully Connected: invalid matrix layout
BNNS Sparse Fully Connected: invalid block row count: %u
BNNS Sparse Fully Connected: invalid block col count: %u
BNNS Sparse Fully Connected: input data types supported in this version: Float16
BNNS Sparse Fully Connected: output data types supported in this version: Float32, Float16
BNNS Sparse Fully Connected: weights data types supported in this version: Float16
BNNS Sparse Fully Connected: unsupported matrix block size %u x %u
BNNS Sparse Fully Connected: Allocation of the setup structure failed
BNNS Compare: in0 is NULL
BNNS Compare: in1 is NULL
BNNS Compare: out is NULL
BNNS Compare: Unsupported I/O tensor data types.
BNNS Compare: Mismatch in input tensor data types.
BNNS Compare: Invalid operation %d for data type %s.
BNNS Compare: I/O tensor number of dimension mismatch.
BNNS Compare: I/O tensor dimension %lu mismatch.
BNNS Compare: Partially overlapping input and output tensors are not supported.
BNNS Compare: I/O tensor layout mismatch.
BNNS Compare: Unsupported I/O tensor layout.
BNNS Compare: Unsupported operator.
BNNS Fully Connected Choose: inputs, weights, outputs and bias must be contigeous in memory (stride[0] <= 1)
BNNS Fully Connected Choose: bias and activation aren't supported when output isn't fp32
BNNS Fully Connected Init: missing data table for indexed data type
BNNS Fully Connected Direct Apply: failed to create layer context
BNNS Fully Connected Init: input descriptor CHW layout isn't set properly
BNNS Fully Connected Init: output descriptor CHW layout isn't set properly
BNNS Fully Connected Init: weights descriptor row major size[0] doesn't match input vector size
BNNS Fully Connected Init: weights descriptor row major size[1] doesn't match output vector size
BNNS Fully Connected Init: weights descriptor column major size[0] doesn't match output vector size
BNNS Fully Connected Init: weights descriptor column major size[1] doesn't match input vector size
BNNS Fully Connected Init: weights descriptor isn't set properly
BNNS Fully Connected Init BNNSNDArrayDescriptor: stride[%zu] is too small
BNNS Loss: Error unsupported loss function
BNNS Loss: Error Input width is 0
BNNS Loss: Error Input must be BNNSDataTypeFloat32 type
BNNS Loss Error: Input must be contiguous. stride0 must be 1 or 0
BNNS Loss Error: Input tensors must be contiguous.
BNNS Loss: Error unknown reduction function
BNNS Loss: Error output data type must be BNNSDataTypeFloat32
BNNS Loss: Error output size>1 is only allowed with reduction BNNSLossReductionNone.
BNNS Yolo loss: Error out width (size[0]) must be 1 as yolo loss is always reduced
BNNS Loss: Error anchors is NULL
BNNS Loss: Error number of grid rows is 0
BNNS Loss: Error number of grid columns is 0
BNNS Loss: Error number of anchor boxes is 0
BNNS Loss: Error anchor box size <= 5
BNNS Loss: Error object minimum iou is negative
BNNS Loss: Error no object maximum iou is negative
BNNS Loss: Error input descriptor width (%zu) different from expected width of grid_size*num_anchors*(5+num_+classes) (%zu)
BNNS Loss: memory allocation failed
BNNS Loss: Error filter is NULL
BNNS Loss: Error batch_size is 0
BNNS Loss: Error in is NULL
BNNS Loss Error: in_stride is smaller than i_desc.size[0] or Tensor size
BNNS Loss: Error labels is NULL
BNNS Loss Error: labels_stride is smaller than i_desc.size[0] or Tensor size
BNNS Loss: Error weights_size>0 but weights pointer is NULL
BNNS Loss Warning: weight_size==0 but weight pointer is not NULL. Weight pointer is ignored.
BNNS Loss Error: weight_size value must be 0,1 batch_size, or batch_size*input_width for softmax/sigmoid/categorical cross entropy, mse, mae, log, hinge, and huber
BNNS Loss: Error yolo weight_size value must be 0, use yolo specific weight factors during filter create
BNNS Loss: Error out is NULL
BNNS Loss: Error in_delta descriptor is ilegal
BNNS Loss: Error in_delta must be contiguous, such that stride[0] is 0 or 1
BNNS Yolo loss: Error weight size must be 0
BNNS Loss Backward: Error filter is NULL
BNNS Loss Backward: Error batch_size is 0
BNNS Loss Backward: Error in is NULL
BNNS Loss Backward: Error in_stride is smaller than i_desc.size[0]
BNNS Loss Backward: Error labels is NULL
BNNS Loss Backward: Error labels_stride is smaller than i_desc.size[0]
BNNS Loss Backward: Error weights_size>0 but weights pointer is NULL
BNNS Loss Backward Error: weight_size value must be 0,1 batch_size, or batch_size*input_width for softmax/sigmoid/categorical cross entropy, mse, mae, log, hinge, and huber
BNNS Loss Backward: Error in_delta descriptor is illegal
BNNS Loss Backward: Error in_delta must be contiguous, such that stride[0] is 0 or 1
BNNS Loss Backward: Error in_delta is NULL
BNNS Loss Backward: Error out_delta descriptor is illegal
BNNS Loss Backward: Error out_delta must be contiguous, such that stride[0] is 0 or 1
BNNS Loss Backward: Error out_delta is NULL
BNNS softmax cross entropy loss error: in pointer is NULL
BNNS softmax cross entropy loss error: in stride cannot be smaller than input width
BNNS softmax cross entropy loss: Error out pointer is NULL
BNNS softmax cross entropy loss error: batch_size is 0
BNNS softmax cross entropy loss: malloc failed
BNNS softmax cross entropy loss: Error weight size must be equal 0,1, batch size, or batch_size*tensor_size 
BNNS softmax cross entropy loss backward : alloc failed
BNNS Loss Warning: reduction BNNSLossReductionWeightedMean sum of weights is zero
BNNS Loss: Error reduction BNNSLossReductionNonZeroWeightMean all weights are zero
BNNS sigmoid cross entropy loss error: in pointer is NULL
BNNS sigmoid cross entropy loss error: in stride cannot be smaller than input width
BNNS sigmoid cross entropy loss: Error out pointer is NULL
BNNS sigmoid cross entropy loss error: batch_size is 0
BNNS sigmoid cross entropy loss error: must be 0,1 or batch_size*input_width
BNNS sigmoid cross entropy loss: malloc failed
BNNS MSE loss error: in stride cannot be smaller than input width
BNNS MSE loss: malloc failed
BNNS MSE loss error: must be 0,1 or batch_size*input_width
BNNS Huber loss error: in pointer is NULL
BNNS Huber loss error: in stride cannot be smaller than input width
BNNS Huber loss: Error out pointer is NULL
BNNS Huber loss error: batch_size is 0
BNNS Huber loss error: must be 0,1 or batch_size*input_width
BNNS Huber loss: malloc failed
BNNS Yolo loss: Error filter is NULL
BNNS Yolo loss: Error batch_size is 0
BNNS Yolo loss: Error in pointer is NULL
BNNS Yolo loss: Error input stride is smaller than input width
BNNS Yolo loss: Error labels (ground truth) is NULL
BNNS Yolo loss: Error ground truth labels stride is smaller than input width
BNNS Yolo loss: Error input delta stride is smaller than input width
BNNS Yolo loss: Error out pointer is NULL
BNNS Yolo loss: Error yolo redution type must be BNNSLossReductionSum
BNNS Yolo loss: Error input descriptor width does not match grid_rows*grid_columns*anchors*(5+classes)
BNNS Yolo loss: alloc failed
BNNS log loss error: in pointer is NULL
BNNS log loss error: in stride cannot be smaller than input width
BNNS log loss: Error out pointer is NULL
BNNS log loss error: batch_size is 0
BNNS log loss error: must be 0,1 or batch_size*input_width
BNNS cosine distance loss error: in pointer is NULL
BNNS cosine distance loss error: in stride cannot be smaller than input width
BNNS cosine distance loss: Error out pointer is NULL
BNNS cosine distance loss error: batch_size is 0
BNNS cosine distance loss: Error weight size must be equal to 0, 1, batch_size
BNNS Loss: Error in_delta descriptor is illegal
BNNS Hinge loss error: in pointer is NULL
BNNS Hinge loss error: in stride cannot be smaller than input width
BNNS Hinge loss: Error out pointer is NULL
BNNS Hinge loss error: batch_size is 0
BNNS Hinge loss error: must be 0,1 or batch_size*input_width
BNNS MAE loss error: in pointer is NULL
BNNS MAE loss error: in stride cannot be smaller than input width
BNNS MAE loss: Error out pointer is NULL
BNNS MAE loss error: batch_size is 0
BNNS MAE loss error: must be 0,1 or batch_size*input_width
BNNS categorical cross entropy loss error: in pointer is NULL
BNNS categorical cross entropy loss error: in stride cannot be smaller than input width
BNNS categorical cross entropy loss: Error out pointer is NULL
BNNS categorical cross entropy loss error: batch_size is 0
BNNS categorical cross entropy loss error: must be 0,1 or batch_size*input_width
BNNS softmax cross entropy loss backward error: in stride cannot be smaller than input width
BNNS softmax cross entropy loss backward error: out_delta.size[0] is not 1
BNNS softmax cross entropy loss backward : malloc failed
BNNS softmax cross entropy loss: Error weight size must be equal 0,1 or batch size
BNNS sigmoid cross entropy loss backward error: in stride cannot be smaller than input width
BNNS sigmoid cross entropy loss backward error: must be 0,1 or batch_size*input_width
BNNS MSE loss backward error: must be 0,1 or batch_size*input_width
BNNS Log loss backward error: in stride cannot be smaller than input width
BNNS Log loss backward error: must be 0,1 or batch_size*input_width
BNNS cosine distance loss backward error: in stride cannot be smaller than input width
BNNS cosine distance loss backward error: must be 0,1 or batch_size
BNNS Hinge loss backward error: in stride cannot be smaller than input width
BNNS Hinge loss backward error: must be 0,1 or batch_size*input_width
BNNS MAE loss backward error: in stride cannot be smaller than input width
BNNS MAE loss backward error: must be 0,1 or batch_size*input_width
BNNS Categorical CE loss backward error: in stride cannot be smaller than input width
BNNS Categorical CE loss backward error: must be 0,1 or batch_size*input_width
BNNS Create Depthwise Convolution: incompatible numbers of channels between images and convolution parameters
BNNS Create Depthwise Convolution: input must be BNNSDataLayoutImageCHW layout 
BNNS Create Depthwise Convolution: input descriptor is illegal
BNNS Create Depthwise Convolution: output must be BNNSDataLayoutImageCHW layout 
BNNS Create Depthwise Convolution: output descriptor is illegal
BNNS Create Depthwise Convolution: weights descriptor is illegal
Depthwise Convolution does not support asymmetric padding. left and right pad must match, up and down pad must match
BNNS Create Depthwise Convolution: input size computation wraparound input (%zu x %zu), output (%zu x %zu), stride (%zu x %zu), padding (%zu x %zu)
BNNS Create Depthwise Convolution: output(%zu x %zu x %zu) is larger than input(%zu x %zu x %zu) with padding(%zu x %zu).
 kernel (%zu x %zu) and stride (%zu x %zu)
BNNS Create Depthwise Convolution: only float32 is supported
BNNS Create Depthwise Convolution: channel multiplier mismatch
BNNS Create Depthwise Convolution: malloc failed
BNNS Create Depthwise Convolution: weights malloc failed 
BNNS DEPTHWISE CONV: input batch stride doesn't make sense (%zu < %zu x %zu)
BNNS DEPTHWISE CONV: output batch stride doesn't make sense (%zu < %zu x %zu)
BNNS LSTM Direct Apply Backward: BNNSNDArrayFlagBackpropAccumulate is only supported on layer_delta->input_descriptor.data_desc.
BNNS LSTM Direct Apply Backward: layer_params initialization failed
BNNS LSTM Direct Apply Backward: layer_delta initialization failed
BNNS LSTM APPLY BACKWARD: forward pass intermediate results weren't cached, recomputing forward pass
BNNS LSTM Direct Apply Backward: failed to allocate training cache buffer
BNNS LSTM Direct Apply Backward: failed to compute forward intermediate results
BNNS LSTM Direct Apply Backward: training cache capacity isn't sufficent
BNNS LSTM Direct Apply Backward: failed to allocate scratch buffer
LSTM input descriptor.data isn't set correctly (size don't match input_size/batch_size/seq_len)
LSTM input descriptor.hidden isn't set correctly (size don't match hidden_size/batch_size/num_directions/num_layers)
LSTM input descriptor.cell_state isn't set correctly (size don't match hidden_size/batch_size/num_directions/num_layers)
LSTM output descriptor.data isn't set correctly (size don't match hidden_size/num_directions/batch_size/num_layers)
LSTM output descriptor.hidden isn't set correctly (size don't match hidden_size/batch_size/num_directions/num_layers)
LSTM output descriptor.cell_state isn't set correctly (size don't match hidden_size/batch_size/num_directions/num_layers)
LSTM forget_gate isn't set correctly (size don't match hidden_size/batch_size/num_directions/num_layers)
LSTM input_gate isn't set correctly (size don't match hidden_size/batch_size/num_directions/num_layers)
LSTM candidate_gate isn't set correctly (size don't match hidden_size/batch_size/num_directions/num_layers)
LSTM output_gate isn't set correctly (size don't match hidden_size/batch_size/num_directions/num_layers)
BNNS LSTM: Data type not supported, fail
BNNS LOW MEM CONVOLUTION: Client needs to support weights ptr with low_mem
BNNS LOW MEM CONVOLUTION: malloc
BNNS LOW MEM CONVOLUTION: failed to create convolution context
BNNS LOW MEM CONVOLUTION: failed to divide work between convolution contexts to fit in memory
BNNS LOW MEM CONVOLUTION: failed to create single generic convolution
BNNS LOW MEM CONVOLUTION: malloc
BNNS LOW MEM CONVOLUTION: Winograd weights memory doesn't fit in memory
BNNS LOW MEM CONVOLUTION: unable to create Winograd that fit in memory
BNNS LOW MEM CONVOLUTION: failed to create winograd convolution
BNNS LOW MEM CONVOLUTION: incompatible BNNS_low_memory_context id
BNNS LOW MEM CONVOLUTION:: context type not supported
BNNS LOW MEM CONVOLUTION: apply convolution failed
BNNS Create Convolution: fp32 weights allocation failed
BNNS Create Convolution: fp16 weights allocation failed
BNNS Create Convolution: single descriptors allocation failed
BNNS Convolution Create: allocation failed
failed to upconvert or copy weights
failed to allocate none generic format
BNNS Normalization: Error layer_params is NULL
BNNS Normalization: Error beta descriptor is illegal
BNNS Normalization: Error gamma descriptor is illegal
BNNS Normalization: Error input descriptor is illegal
BNNS Normalization: Error input descriptor must be contiguous such that stride[0]=1 and for every N>0 stride[N]=stride[N-1]*size[N-1]
BNNS Normalization: Error output descriptor is illegal
BNNS Normalization: Error output descriptor must be contiguous such that stride[0]=1 and for every N>0 stride[N]=stride[N-1]*size[N-1]
BNNS Normalization: Error input size (%zu) different from output size (%zu)
BNNS Layer Normalization: Error gamma descriptor must match the shape of input up to the normalizaton axis
BNNS Layer Normalization: Error beta descriptor must match the shape of input up to the normalizaton axis
BNNS Normalization: Error gamma descriptor size[0] must be the same as number of input channels
BNNS Normalization: Error beta descriptor size[0] must be the same as number of input channels
BNNS Normalization: Error moving mean descriptor is illegal
BNNS Normalization: Error moving mean descriptor size[0] must be the same as number of input channels
BNNS Normalization: Error moving variance descriptor is illegal
BNNS Normalization: Error moving variance descriptor size[0] must be the same as number of input channels
BNNS Normalization: Error Input must be BNNSDataTypeFloat32 type
BNNS Normalization: Error Output must be BNNSDataTypeFloat32 type
BNNS Normalization: Error Input must be in BNNSDataLayoutImageCHW layout
BNNS Normalization: Error Output must be in BNNSDataLayoutImageCHW layout
BNNS Normalization: Error beta must be BNNSDataTypeFloat32 type
BNNS Normalization: Error gamma must be BNNSDataTypeFloat32 type
BNNS Normalization: Error moving mean must be BNNSDataTypeFloat32 type
BNNS Normalization: Error moving variance must be BNNSDataTypeFloat32 type
BNNS Normalization Warning: epsilon is zero, it may cause division by zero
BNNS Normalization: Error momentum must be between 0 and 1
BNNS Normalization: Error memory allocation failed
BNNS Normalization: Failed to init filter
BNNS Fused Convolution and Normalization: Error convolution_layer_params is NULL
BNNS Fused Convolution and Normalization: Error normalization_layer_params is NULL
BNNS Fused Convolution and Normalization: Error convolution output descriptor and normalization input descriptor must have the same sizes, strides and data types
BNNS Fused Convolution and Normalization: Error memory allocation failed
BNNS Fused Convolution and Normalization create filter failed: Convolution type error
BNNS Fused Convolution and Normalization create filter failed
BNNS Fused Fully Connected and Normalization: Error fully_layer_params is NULL
BNNS Fused Fully Connected and Normalization: Error batch_normalization_layer_params is NULL
BNNS Fused Fully Connected and Normalization: Error fully connected output descriptor and normalization input descriptor must have the same sizes, strides and data types
BNNS Fused Fully Connected and Normalization: Error memory allocation failed
BNNS Fused Fully Connected and Normalization create filter failed
BNNS Normalization: input pointer is NULL
BNNS Normalization: batch_size>1 and in_stride is 0
BNNS Normalization: output pointer is NULL
BNNS Normalization: batch_size>1 and out_stride is 0
BNNS Normalization: malloc failed
BNNS Normalization Backward: Error filter is NULL
BNNS Normalization Backward: BNNSNDArrayFlagBackpropAccumulate is no supported for beta_delta.
BNNS Normalization Backward: BNNSNDArrayFlagBackpropAccumulate is no supported for gamma_delta.
BNNS Normalization: Error normalization input delta and input must have the same sizes, strides and data types
BNNS Normalization: Error Normalization beta delta and beta must have the same sizes, strides and data types
BNNS Normalization: Error Normalization gamma delta and gamma must have the same sizes, strides and data types
BNNS Normalization: Error Normalization output delta and output must have the same sizes, strides and data types
BNNS Normalization Backward: Error Normalization beta delta pointer is NULL. backward computation of all active parameters must be done in a single function call. 
BNNS Normalization Backward: Error Normalization gamma delta pointer is NULL. backward computation of all active parameters must be done in a single function call. 
BNNS Normalization Backward: Error out delta descriptor pointer is NULL
BNNS Normalization Backward: Error out delta descriptor data pointer is NULL
BNNS Normalization Backward Warning: in delta, beta delta and gamma delta pointers are NULL, nothing to do
BNNS Normalization Backward: Error cannot compute input delta because input delta data pointer is NULL
BNNS Normalization Backward: Error cannot compute beta delta because beta delta data pointer is NULL
BNNS Normalization Backward: Error cannot compute gamma delta because gamma delta data pointer is NULL
BNNS Layer Normalization: Error beta_delta descriptor must match the shape of input up to the normalizaton axis
BNNS Layer Normalization: Error gamma_delta descriptor must match the shape of input up to the normalizaton axis
BNNS Normalization Backward: Error beta delta descriptor must be contiguous such that   stride[0]=1 and for every N>0 stride[N]=stride[N-1]*size[N-1], and size[0] must be equal to number of   input channels
BNNS Normalization Backward: Error gamma delta descriptor must be contiguous such that   stride[0]=1 and for every N>0 stride[N]=stride[N-1]*size[N-1], and size[0] must be equal to number of   input channels
BNNS Normalization Backward: Error output delta descriptor must be contiguous such that stride[0]=1 and for every N>0 stride[N]=stride[N-1]*size[N-1]
BNNS Normalization Backward: Error output delta size must be the same size of input
BNNS Normalization Backward: Error input delta descriptor must be contiguous such that stride[0]=1 and for every N>0 stride[N]=stride[N-1]*size[N-1]
BNNS Normalization Backward: Error input delta size must be the same size of input
BNNS Normalization Backward: Error input delta descriptor data type must be BNNSDataTypeFloat32
BNNS Normalization Backward: Error beta delta descriptor data type must be BNNSDataTypeFloat32
BNNS Normalization Backward: Error no internal storage of x_hat, make sure to run normalization forward with training flag enabled before running normalization backward
BNNS Normalization Backward: Error no internal storage of inverse variance, make sure to run normalization forward with training flag enabled before running normalization backward
BNNS Normalization Backward: Error cannot compute activation backward, output is NULL
BNNS Normalization Backward: Error Fusion of normalization with activation is unsupported for given activation function
BNNS CONVOLUTIONS VERSION2: only dense memory laout is supported
BNNS CONVOLUTIONS VERSION2: output bias or scale is not supported
BNNS CONVOLUTIONS VERSION2: integer engine does not support weight bias and scale
BNNS CONVOLUTIONS VERSION2: malloc failed
BNNS CONVOLUTIONS VERSION2: malloc failed
BNNS CONV: input batch stride doesn't make sense (%zu < %zu x %zu)
BNNS CONV: output batch stride doesn't make sense (%zu < %zu x %zu)
BNNS CONVOLUTIONS: malloc failed
BNNS Convolution: malloc failed
BNNS Padding: layer_params is NULL
BNNS Padding: Padding is not supported beyond 4-D tensors.
BNNS Padding: Undefined padding mode.
BNNS Padding: Unsupported data type: FP16.
BNNS Padding: Unsupported data layout.
BNNS Padding: input and output desciptors have differing numbers of dimensions.
BNNS Padding: Input dimension is too small for the padding size.
BNNS Padding: Input size + padding sizes doesn't match output size in dimension %zu.
BNNS Padding: input descriptor is illegal
BNNS Padding: output descriptor is illegal
BNNS Padding: Unsupported data type.
BNNS Padding: I/O data type mismatch.
BNNS Padding: memory allocation failed
BNNS Padding: filter is NULL
BNNS Padding: wrong filter type, filter is not Padding.
BNNS Padding: input pointer is NULL
BNNS Padding: batch_size > 1 and in_stride is 0
BNNS Padding: output pointer is NULL
BNNS Padding: batch_size > 1 and out_stride is 0
BNNS Padding: wrong filter type, filter is not padding
BNNS Padding: input descriptor is NULL
BNNS Padding: input data pointer is NULL
BNNS Padding: batch_size>1 and in_stride is 0
BNNS Padding: output delta descriptor is NULL
BNNS Padding: output delta data pointer is NULL
BNNS Padding: batch_size>1 and out_delta_stride is 0
x_ijf*, x_ijc* -> G_fc*
BNNS GetStateSize : filter is NULL
BNNS SetState: Error filter is NULL
BNNS SetState: Source filter state is NULL
BNNS SetState: Target filter is NOT Dropout filter
BNNS GetState: Error filter is NULL
BNNS GetState: Target filter state is NULL
BNNS GetState: Source filter is NOT Dropout filter
BNNS LayerNorm: Error Normalization axis must be 0, 1, or 2.
BNNS GroupNorm: Error The number of input channels must be divisible by the number of groups.
BNNS Normalization: Error Normalization axis must be 0, 1, or 2.
BNNS Normalization: Error The number of input channels must be divisible by the number of groups.
BNNS Fused Filter: Error number_of_fused_filters is not 2. currently supporting only 2 fused filters
BNNS Fused Filter: Error currently supporting first filter of BNNSConvolution,BNNSFullyConnected or BNNSTransposedConvolution type only
BNNS Fused Filter: Error currently supporting second filter of BNNSBatchNorm, BNNSInstanceNorm, BNNSLayerNorm, and BNNSGroupNorm types only
BNNS Fused Filter: Error convolution activation must be BNNSActivationFunctionIdentity
BNNS Fused Filter: Error fully connected activation must be BNNSActivationFunctionIdentity
BNNS Normalization: Error filter is NULL
BNNS Normalization: Error wrong filter type, filter is not normalization
BNNS BatchNorm: Error wrong filter type, filter is not batch norm
BNNS InstanceNorm: Error filter is NULL
BNNS InstanceNorm: Error wrong filter type, filter is not instance norm
BNNS LayerNorm: Error filter is NULL
BNNS LayerNorm: Error wrong filter type, filter is not layer norm
BNNS GroupNorm: Error filter is NULL
BNNS GroupNorm: Error wrong filter type, filter is not group norm
BNNS Fused Filter: Error wrong filter, this function should only be used for a filter created with BNNSFilterCreateFusedLayer
BNNS Arithmetic Filter: Error wrong filter, this function should only be used for a filter created with BNNSFilterCreateLayerArithmetic
BNNS Pooling: Error wrong filter type, filter is not pooling
Resize filter: backward pass requires in_delta and in_delta->data to be non-NULL.
BNNS Copysum filter: backward pass requires in_delta and in_delta->data to be non-NULL.
BNNS Padding: backward pass requires in_delta and in_delta->data to be non-NULL.
BNNS Normalization Backward Warning: Batch size is 0, nothing to do
BNNS Normalization Backward: Error wrong filter, this function should only be used for a filter created with BNNSFilterCreateLayerNormalization
BNNS BatchNorm Backward: Error wrong filter, this function should only be used for a filter created with BNNSFilterCreateLayerBatchNorm
BNNS Instance Norm Backward Warning: Batch size is 0, nothing to do
BNNS Instance Norm Backward: Error wrong filter, this function should only be used for a filter created with BNNSFilterCreateLayerInstanceNorm
BNNS Layer Norm Backward Warning: Batch size is 0, nothing to do
BNNS Layer Norm Backward: Error wrong filter, this function should only be used for a filter created with BNNSFilterCreateLayerLayerNorm
BNNS Group Norm Backward Warning: Batch size is 0, nothing to do
BNNS Group Norm Backward: Error wrong filter, this function should only be used for a filter created with BNNSFilterCreateLayerGroupNorm
BNNS Fused Filter Backward Warning: Batch size is 0, nothing to do
BNNS Fused Filter Backward: Error wrong filter, this function should only be used for a filter created with BNNSFilterCreateFusedLayer
BNNS Fused Filter Backward: Error Weight delta pointer is NULL. backward computation of all active parameters must be done in a single function call. 
BNNS Fused Filter Backward: Error bias delta pointer is NULL. backward computation of all active parameters must be done in a single function call. 
BNNS Fused Filter Backward: Error normalization beta delta pointer is NULL. backward computation of all active parameters must be done in a single function call. 
BNNS Fused Filter Backward: Error normalization gamma delta pointer is NULL. backward computation of all active parameters must be done in a single function call. 
Batchnorm filter apply must be called with BNNSBatchNormFilterApplyBatch
Instance Norm filter apply must be called with BNNSInstanceNormFilterApplyBatch
Layer Norm filter apply must be called with BNNSLayerNormFilterApplyBatch
Group Norm filter apply must be called with BNNSLayerNormFilterApplyBatch
Fused filter apply must be called with BNNSFusedFilterApplyBatch
malloc failed
BNNS Convolution Variant: input type %s not supported!!!
BNNS Convolution Variant: compute type %s not supported!!!
BNNS Apply Activation Backward: BNNSActivationFunctionIntegerLinearSaturate isn't supported
BNNS Apply Activation Backward: BNNSActivationFunctionIntegerLinearSaturatePerChannel isn't supported
BNNS Activation: number of memory descriptor is lower than expected
BNNS Activation: Failed to init filter
BNNS Activation Backward: At least one of in or out must be non-NULL
BNNS Activation Backward: input dimension (%zu) and input_delta dimension (%zu) must match
BNNS Activation Backward: input and input_delta shapes must match
BNNS Activation Backward: output dimension (%zu) and output_delta dimension (%zu) must match
BNNS Activation Backward: output and output_delta shapes must match
BNNS Activation Backward: weights_delta must have a single dimension and be of size %zu
BNNS Activation Backward: Softmax with CHW input requires output
BNNS Activation Backward: LogSoftmax with CHW input requires output
BNNS Activation Backward: Chosen activation function requires application over a region contiguous in memory (axis_flags=%x)
BNNS Activation: in-place activation layer is allowed only for output types with the same or smaller storage size
BNNS Activation: Apply failed
BNNS Activation Init: input descriptor is illegal
BNNS Activation Init: output descriptor is illegal
BNNS Activation Init: layout doesn't match
BNNS Activation Init: only 3D conversions are supported
BNNS Activation Init: dim %zu input size %zu != %zu output size
BNNS Activation Init: alpha can't be +/-inf or Nan for Gumbel or Gumbel Max
BNNS Activation Init: beta can't be +/-inf or Nan or zero or negative for Gumbel or Gumbel Max
BNNS Activation Init: BNNSActivationFunctionPReLUPerChannel is only valid with data layout BNNSDataLayoutImageCHW
BNNS Activation: Error NULL input/output pointer
BNNS Activation: Error number of input channels must be equal to number of output channels
BNNS Activation: Error dimensions of input tensor (%zu) and output tensor (%zu) must match
BNNS Activation: Error shape of input tensor and output tensor must match
BNNS Softmax: Non-contiguous softmax implementation only supports fp32->fp32
BNNS Multihead Attention Apply: If backprop_cache is non-NULL, backprop_cache_size must also be non-NULL
BNNS Multihead Attention Apply: If workspace is non-NULL, workspace_size must also be non-NULL
BNNS Multihead Attention Apply: Supplied workspace is too small (size %ld, but required %ld)
Multihead Attention Backward: Only support for a full-size backprop_cache has been implemented.
Multihead Attention Backward: If workspace is non-NULL, workspace_size must also be non-NULL
Multihead Attention Backward: Insufficient workspace supplied (%zu bytes). Require %zu bytes.
layer_params->query.target_desc
layer_params->key.target_desc
layer_params->query.weights
layer_params->key.weights
layer_params->value.weights
(d_model, d_key, num_heads)
(source_length, d_model)
layer_params->value.target_desc
(d_model, d_value, num_heads)
layer_params->output.target_desc
(target_length, d_model)
layer_params->output.weights
(num_heads*d_value, d_model)
BNNS Multihead Attention Create: layer_params->key_attn_bias.data and layer_params->value.attn_bias.data must both be NULL or must both be not NULL
layer_params->key_attn_bias
(d_key, num_heads)
layer_params->value_attn_bias
(d_value, num_heads)
layer_params->query.bias
layer_params->key.bias
layer_params->value.bias
layer_params->output.bias
(d_model)
BNNS Multihead Attention Create: Unsupported layout for query.weights
BNNS Multihead Attention Create: Unsupported layout for key.weights
BNNS Multihead Attention Create: Unsupported layout for value.weights
BNNS MULTIHEAD ATTENTION GetPointer: Layer was created with dropout=0.0, cannot return pointer to dropout value.
BNNS MULTIHEAD ATTENTION GetPointer: Unsupported target %d
BNNS Multihead Attention Create: Expected %s to have exactly %zu dimensions, but it has %zu
BNNS Multihead Attention Create: Expected %s to have shape %s = (
%s%zu
) but has actual shape (
%s: 
BNNS Permute Filter: layer_params is NULL
BNNS Permute Filter: illegal input descriptor
BNNS Permute Filter: illegal output descriptor
BNNS Permute Filter: input and output descriptor layout dimension does not match. input dimm = %zu output dimm = %zu
BNNS Permute Filter: permutation array index %zu has illegal value of %zu
BNNS Permute Filter: permutation array is missing axis %zu
BNNS Permute Filter: permutation array axis %zu appears %zu times, each axis should appear exactly once
0123456789abcdefghijklmnopqrstuvwxyz
BNNS POOLING: input batch stride doesn't make sense (%zu < %zu x %zu)
BNNS POOLING: dilation supported only in BNNSPoolingFunctionMax/UnMax
BNNS POOLING: dilation supported only for BNNSDataTypeFloat32 data type
Pooling layer filter running slow path: stride=%zu,%zu kernel=%zu,%zu
BNNS Pooling Backward: cannot run backward without output delta
BNNS Pooling Backward: only float32 output delta are supported
BNNS Pooling Backward: only float32 input delta are supported
BNNS Pooling Backward: only float32 bias delta are supported
BNNS Pooling Backward: Fusion of compute with activation is unsupported for given activation function
BNNS POOLING: layer parameters is NULL
BNNS POOLING: input must be a 3D array
BNNS POOLING: input descriptor is illegal
BNNS POOLING: output must be a 3D array
BNNS POOLING: output descriptor is illegal
BNNS POOLING: invalid kernel dimensions, should be greater than 0
BNNS POOLING: optimized code supports kernel width/height up to 16
BNNS POOLING: dilation only supported for BNNSPoolingFunctionMax/UnMax
BNNS POOLING: dilation only supported for BNNSDataTypeFloat32 input/output data types
BNNS_POOLING: input size computation wraparound input (%zu x %zu), output (%zu x %zu), stride (%zu x %zu), padding (left %zu right %zu up %zu down %zu)
BNNS_POOLING: input size computation wraparound input (%zu x %zu), output (%zu x %zu), stride (%zu x %zu), padding (%zu x %zu)
BNNS_POOLING: output(%zu x %zu) is larger than input(%zu x %zu) with padding(left %zu right %zu up %zu down %zu).
 stride (%zu x %zu)
BNNS_POOLING: output(%zu x %zu) is larger than input(%zu x %zu) with padding(%zu x %zu).
 stride (%zu x %zu)
BNNS_POOLING: supported input/output data types: float32 float16
BNNS POOLING: Internal Memory size %zu doesn't match expected %d
BNNS Pooling Backward: only float32 delta are supported
BNNS Pooling Backward: wrong pooling function called
BNNS Reduction Create: Only FP32 input data is supported for reduction function %u
BNNS Reduction Create: Only FP32 output data is supported for reduction function %u
BNNS Reduction Create: Only FP32 or bool input data is supported for reduction function %u
BNNS Reduction Create: Only FP32 or bool output data is supported for reduction function %u
BNNS Reduction Create: Invalid reduction function %u
BNNS Reduction Create: i_desc and o_desc must have the same number of dimensions (%zu vs %zu)
BNNS Reduction Create: i_desc and w_desc must have the same number of dimensions (%zu vs %zu)
BNNS Reduction Create: i_desc and o_desc dimensions must have the same size, or o_desc.size[d] must be 1 to indicate reduction (dimension %zu: %zu vs %zu)
BNNS Reduction Create: i_desc and w_desc dimensions must have the same size (dimension %zu: %zu vs %zu)
BNNS Reduction Create: ArgMin/ArgMax reduction is only supported on a single axis
BNNS Reduction Backward: input_delta must have the same number of dimensions as i_desc
BNNS Reduction Backward: input_delta must have the same shape as i_desc
BNNS Reduction Backward: output_delta must have the same number of dimensions as o_desc
BNNS Reduction Backward: output_delta must have the same shape as o_desc
BNNS Reduction Backward: weights_delta must have the same number of dimensions as w_desc
BNNS Reduction Backward: weights_delta must have the same shape as w_desc
BNNS Reduction Direct Apply: Size of the dimensions of the output can not be greater than the size of the input
BNNS Reduction Direct Apply: ArgMin/ArgMax reduction is only supported on a single axis
Backward usage is not supported for reduction type %u
BNNS Reduction Backward: Reduce function %u requires input to be supplied for backward computation
BNNS Reduction Backward: Reduce function %u requires output to be supplied for backward computation
BNNS Reduction Backward: Calculation of weights_delta requires input to be supplied
Size mismatch in dimension %zu: input %zu, output %zu
BNNS Reduce Apply Generic: Size mismatch in dimension %zu: input %zu, output %zu
BNNS Arithmetic Filter: layer_params is NULL
BNNS Arithmetic Filter: unsupported arithmetic function
BNNS Arithmetic Filter: memory allocation failed
BNNS Arithmetic Filter: Failed to init filter
BNNS Arithmetic Filter: filter is NULL
BNNS Arithmetic Filter: wrong filter type, this function should only be used for a filter created with BNNSFilterCreateLayerArithmetic
BNNS Arithmetic Filter: in pointer is NULL
BNNS Arithmetic Filter: batch_size>1 and in_stride is 0
BNNS Arithmetic Filter Internal: expected number of inputs call failed
BNNS Arithmetic Filter: wrong number_of_inputs, number_of_inputs=%zu, expecting %zu
BNNS Arithmetic Filter: in[%zu] is NULL
BNNS Arithmetic Filter: in_stride[%zu] is 0
BNNS Arithmetic Filter: output pointer is NULL
BNNS Arithmetic Filter: batch_size>1 and out_stride is 0
BNNS Arithmetic Filter Internal: unsupported arithmetic function
BNNS Arithmetic Filter: descriptors for input that is being processed in-place and output must exactly match
BNNS Arithmetic Filter: input is being processed in-place but input and output descriptor types do not match
BNNS Arithmetic Filter: output dimension smaller than input is not supported. batch size>1, input type is BNNSSample but out_type is not BNNSSample
BNNS Arithmetic Filter: input1 and input2 have the same data pointer, but descriptors are different
BNNS Arithmetic Filter: input1 and input2 have the same data pointer, but different BNNSDescriptorType
BNNS Arithmetic Filter: descriptors for input1 that is being processed in-place and output must exactly match
BNNS Arithmetic Filter: input1 is being processed in-place but input1 and output descriptor types do not match
BNNS Arithmetic Filter: descriptors for input2 that is being processed in-place and output must exactly match
BNNS Arithmetic Filter: input2 is being processed in-place but input2 and output descriptor types do not match
BNNS Arithmetic Filter: output dimension smaller than input is not supported. batch size>1, an input type is BNNSSample but out_type is not BNNSSample
BNNS Arithmetic Filter: out delta descriptor pointer is NULL
BNNS Arithmetic Filter: out delta descriptor data pointer is NULL
BNNS Arithmetic Filter: out_delta_stride is 0
BNNS Arithmetic Filter: out delta descriptor validation failed
BNNS Arithmetic Filter: in_delta is NULL
BNNS Arithmetic Filter: in_delta_stride is NULL
BNNS Arithmetic Filter: in is NULL, but it is required for backward compute
BNNS Arithmetic Filter: in_delta descriptor pointer %zu is NULL
BNNS Arithmetic Filter: cannot compute activation backward, output is NULL
BNNS Arithmetic Filter: Fusion of arithmetic with activation is unsupported for given activation function
BNNS Arithmetic Filter Backward Internal: unsupported arithmetic function
BNNS Arithmetic Filter Backward: failed to apply activation backward
BNNS Arithmetic Filter: input type is BNNSConstant, no gradient to compute
BNNS Arithmetic Filter: in_delta[0] is NULL
BNNS Arithmetic Filter: in_delta[0]->data is NULL
BNNS Arithmetic Filter: in_delta_stride[0] is 0
BNNS Arithmetic Filter: in delta descriptor validation failed
BNNS Arithmetic Filter: arithmetic input_delta and input descriptors must have the same sizes, strides and data types
BNNS Arithmetic Filter: arithmetic output_delta and output descriptors must have the same sizes, strides and data types
BNNS Arithmetic Filter: descriptors for out_delta that is being processed in-place and in_delta must exactly match
BNNS Arithmetic Filter: input delta is being processed in-place but input type and output type are different
BNNS Arithmetic Filter: input delta and output delta has the same data pointer (inplace), but different in_delta_stride
BNNS Arithmetic Filter: input delta and output delta has the same data pointer (inplace), but in_delta.flags indicates to accumulate result
BNNS Arithmetic Filter: backward compute for chosen arithmetic function does not allow inplace gradient. out delta data must not be the same as input delta data
BNNS Arithmetic Filter: inputs are both BNNSConstant, no gradient to compute
BNNS Arithmetic Filter: in_delta[0] is NULL but in1 type is not BNNSConstant
BNNS Arithmetic Filter: in_delta[0]->data is NULL but in1 type is not BNNSConstant
BNNS Arithmetic Filter: in1_type is BNNSSample but in_delta_stride[0] is 0
BNNS Arithmetic Filter: input1 delta data pointer is not NULL. cannot compute gradient for BNNSConstant
BNNS Arithmetic Filter: in1 delta descriptor validation failed
BNNS Arithmetic Filter: in_delta[1] is NULL but in2 type is not BNNSConstant
BNNS Arithmetic Filter: in_delta[1]->data is NULL but in2 type is not BNNSConstant
BNNS Arithmetic Filter: in2_type is BNNSSample, but in_delta_stride[1] is 0
BNNS Arithmetic Filter: input2 delta data pointer is not NULL. cannot compute gradient for BNNSConstant
BNNS Arithmetic Filter: in2 delta descriptor validation failed
BNNS Arithmetic Filter: input1 delta and input2 delta has the same data pointer, but descriptors are different
BNNS Arithmetic Filter: input1 delta and input2 delta has the same data pointer, but different data types
BNNS Arithmetic Filter: input1 delta and input2 delta has the same data pointer, but different in_delta_stride
BNNS Arithmetic Filter: forward input pointer points to the same array but input gradient pointers point to different arrays
BNNS Arithmetic Filter: descriptors for out_delta that is being processed in-place and in1_delta must exactly match
BNNS Arithmetic Filter: input1 delta is being processed in-place but input type and output type are different
BNNS Arithmetic Filter: input1 delta and output delta has the same data pointer (inplace), but different in_delta_stride
BNNS Arithmetic Filter: input1 delta and output delta has the same data pointer (inplace), but in1_delta.flags indicates to accumulate result
BNNS Arithmetic Filter: descriptors for out_delta that is being processed in-place and in2_delta must exactly match
BNNS Arithmetic Filter: input2 delta is being processed in-place but input2 type and output type are different
BNNS Arithmetic Filter: input2 delta and output delta has the same data pointer (inplace), but different in_delta_stride
BNNS Arithmetic Filter: input2 delta and output delta has the same data pointer (inplace), but in2_delta.flags indicates to accumulate result
BNNS Arithmetic Filter: backward compute for chosen arithmetic function does not allow inplace gradient. out delta data must not be the same as either of the input delta data
BNNS Arithmetic Filter: unsupported optimizer function
BNNS Arithmetic Filter: arithmetic_function_fields is NULL
BNNS Arithmetic Filter: malloc failed
BNNS Arithmetic Filter: input descriptor validation failed
BNNS Arithmetic Filter: input1 descriptor validation failed
BNNS Arithmetic Filter: unsupported out_type
BNNS Arithmetic Filter: unsupported in_type
BNNS Arithmetic Filter: in, out descriptor check failed
BNNS Arithmetic Filter: input2 descriptor validation failed
BNNS Arithmetic Filter: unsupported in1_type
BNNS Arithmetic Filter: unsupported in2_type
BNNS Arithmetic Filter: in1,in2,out descriptor check failed
BNNS Arithmetic: Activation creation failed
BNNS Arithmetic Filter: input and output data types do not match
BNNS Arithmetic Filter: input size[%zu]=%zu does not equal max input,output size of %zu or 1
BNNS Arithmetic Filter: output size[%zu]=%zu does not equal max input,output size of %zu
BNNS Arithmetic Filter: input1 and out data types do not match
BNNS Arithmetic Filter: input2 and out data types do not match
BNNS Arithmetic Filter: input1 size[%zu]=%zu does not equal max input1,input2,out size of %zu or 1
BNNS Arithmetic Filter: input2 size[%zu]=%zu does not equal max input1,input2,out size of %zu  or 1
BNNS Arithmetic Filter: output size[%zu]=%zu does not equal max input1,input2,out size of %zu
BNNS Arithmetic Filter:  illegal descriptor
BNNS Arithmetic Filter: only BNNSDataTypeFloat32 is supported
BNNS Arithmetic Filter: data layout unsupported
BNNS Arithmetic Filter: descriptor memory non contiguous
BNNS Arithmetic Filter Internal: descriptors must point to contiguous memory
BNNS Arithmetic Filter: in1 delta descriptor memory non contiguous
BNNS Arithmetic Filter: in2 delta descriptor memory non contiguous
BNNS Arithmetic Filter: output delta descriptor memory non contiguous
BNNS Arithmetic Filter: out delta descriptor memory non contiguous
BNNS Arithmetic Filter: in delta descriptor memory non contiguous
v8@?0
hw.physicalcpu
BNNSActivationFunctionIntegerLinearSaturate isn't supported in apply_bias_and_activation
BNNSActivationFunctionIntegerLinearSaturatePerChannel isn't supported in apply_bias_and_activation
allocation failed, size=%zu
allocation failed, size=%zu, align=%zu
invalid layer data type: %u
BNNS: layout not supported
BNNS: active dimension must be greater than 0
BNNS: dimension %zu stride %zu is lower then previous dimension actual size %zu * %zu (size*stride)
Unsupported layout: %d
BNNS Create Convolution Winograd: malloc failed
weight packing must be at least 32 and a power of 2
BNNS Convolution version2: Error minimum int32_t w_pack weight packing value is 32
BNNS Dequantize: shouldn't have reached fp16 convert path
Transposed convolution currently only supports BNNSDataTypeFloat32 data type
BNNS_TRANSPOSE_CONV: forward pass check (swapping input and output)  - input size computation wraparound input (%zu x %zu), output (%zu x %zu), stride (%zu x %zu), asymmetric padding (%zu,%zu,%zu,%zu)
BNNS_TRANSPOSE_CONV: forward pass check (swapping input and output)  - input size computation wraparound input (%zu x %zu), output (%zu x %zu), stride (%zu x %zu), padding (%zu x %zu)
BNNS_TRANSPOSE_CONV: forward pass check (swapping input and output) - output(%zu x %zu x %zu) is larger than input(%zu x %zu x %zu) with padding(%zu x %zu).
 kernel (%zu x %zu) and stride (%zu x %zu)
Filter is NULL
transposed convolution apply failed
BNNS Convolution Backward: BNNSNDArrayFlagBackpropAccumulate has not yet been implemented for weights_delta or bias_delta.
BNNS Convolution Backward: cannot run backward without output delta
BNNS Convolution Backward: only float32 delta are supported
BNNS Convolution Backward: internal error, incorrect wrapper
BNNS Convolution weight packing is not supported backward
BNNS Convolution Backward: BNNSFlagsUseClientPtr must be enabled during training
BNNS Convolution Backward: Fusion of compute with activation is unsupported for given activation function
BNNS Convolution Backward: could not create transposed convolution filter
transposed convolution input delta backward failed
BNNS Convolution Backward: could not create convolution filter
transposed convolution backward failed
BNNS Create Layer Convolution: padding descriptor creation failed
BNNS Create Layer Convolution: input channels must be divisible by groups
BNNS Create Layer Convolution: output channels must be divisible by groups
BNNS Create Layer Convolution: grouped convolution weight layout must be BNNSDataLayoutConvolutionWeightsOIHW
BNNS Convolution Create: incompatible numbers of channels between images and convolution parameters
BNNS Convolution Create: input must be a 3D array
BNNS Convolution Create: input descriptor is illegal
BNNS Conv: output must be a 3D array
BNNS Convolution Create: output descriptor is illegal
BNNS Convolution Create:  weights descriptor is illegal
BNNS CONV: not allowed to delay allocation to apply if weights ptr isn't maintained by Client
BNNS Convolution Create: input size computation wraparound input (%zu x %zu), output (%zu x %zu), stride (%zu x %zu), padding (%zu x %zu)
BNNS Convolution Create: output(%zu x %zu x %zu) is larger than input(%zu x %zu x %zu) with padding(%zu x %zu).
 kernel (%zu x %zu) and stride (%zu x %zu)
BNNS Convolution Create: failed to create dilated convolution
BNNS Convolution Create: failed to create packed weights convolution
BNNS Convolution Create: failed to create grouped convolution
BNNS CONV: input stack data type is not Float32 or Float16 or int8 or uint8
BNNS CONV: output stack data type is not Float32 or Float16 or int8 or uint8
BNNS CONV: int8/uint8 output isn't support without int8/uint8 inputs and input8 weights
BNNS CONV: int8 input stack only supported with int8 output stack and int8 weights
BNNS CONV: uint8 input stack only supported with uint8 output stack and int8 weights
BNNS CONV: int8 convolution can't have bias or activation other than BNNSActivationFunctionIntegerLinearSaturate
BNNS Convolution Create: failed to allocate memory
BNNS Grouped Convolution apply: groups should not be used if groups <= 1
BNNS Grouped Convolution apply: unsupported convolution filter type 
Transposed Convolution layer_params is NULL
Transposed Convolution does not support convolution groups
Transposed Convolution does not support BNNSInternalFlagsUseLowMemConvolutions
Transposed Convolution does not support weight packing
BNNS malloc failed
transposed convolution input manipulation failed
transposed convolution input manipulation failed - nothing to do, should not have allocated dst_input buffer
transposed convolution backward dy padding failed
BNNS Grouped Convolution: input descriptor does not represent contiguous memory
BNNS Grouped Convolution: in_stride [%zu] and input memory size [%zu] do not match
BNNS Grouped Convolution: output descriptor does not represent contiguous memory
BNNS Grouped Convolution: out_stride [%zu] and output memory size [%zu] do not match
BNNS Grouped Convolution: weight descriptor does not represent contiguous memory
BNNS Grouped Convolution: bias descriptor does not represent contiguous memory
Convolution backward: in_delta, weights_delta, bias_delta descriptor pointers are all NULL
BNNS Convolution Backward: out_delta descriptor memory layout is not contiguous
BNNS Convolution Backward: in_delta descriptor memory layout is not contiguous
BNNS Convolution Backward: weights_delta descriptor memory layout is not contiguous
BNNS Convolution Backward: bias_delta descriptor memory layout is not contiguous
BNNS Convolution Backward: out_stride and out_delta_stride do not match
BNNS Convolution Backward: out_delta_stride [%zu] does not match out_delta descriptor memory size [%zu]
BNNS Convolution Backward: in_stride and in_delta_stride do not match
BNNS Convolution Backward: in_delta_stride [%zu] does not match in_delta descriptor memory size [%zu]
Downsampling not supported for dimension > 2.
BNNS Resize: downsampling has no support for 3 or more dimensions.
BNNS Resize: downsampling has no support for 3 or higher dimension.
BNNS Resize: input_delta has %zu dimensions but must match input that has %zu
BNNS Resize: output_delta has %zu dimensions but must match output that has %zu
BNNS Resize: input_delta size[%zu]=%zu but must match input size[%zu]=%zu
BNNS Resize: output_delta size[%zu]=%zu but must match output size[%zu]=%zu
BNNS Resize: Unsupported data type
BNNS Resize: Input and output have different number of dimensions
BNNS Resize: resize must be in same direction for all direction (request %d downsample and %d upsample)
BNNS Resize: Linear interpolation requires resize in at most two dimensions, but %d dimensions are resized
BNNS Resize: Unsupported interpolation method %d
