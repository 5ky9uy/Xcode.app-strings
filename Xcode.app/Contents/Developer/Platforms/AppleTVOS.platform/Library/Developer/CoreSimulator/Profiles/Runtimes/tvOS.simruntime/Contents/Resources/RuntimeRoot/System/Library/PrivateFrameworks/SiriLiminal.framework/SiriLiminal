@(#)PROGRAM:SiriLiminal  PROJECT:CoreSpeech-
com.apple.SiriLiminal
Framework
v8@?0
::: Initializing SiriLiminal logging...
en_US_POSIX
yyyyMMdd-HHmmss
SLLogInitIfNeeded_block_invoke
gitrelno_unavailable
+[SLASRFeatureExtractor extractASRFaturesFrom:]
max:
min:
stddev:
average:
v32@?0@"AFSpeechUtterance"8Q16^B24
+[SLASRFeatureExtractor extractLRNNFaturesFrom:]
+[SLASRFeatureExtractor _getTokenConfidenceForPath:fromPhrases:]
v32@?0@"AFSpeechPhrase"8Q16^B24
+[SLASRFeatureExtractor _getTokenConfidenceForPath:fromPhrases:]_block_invoke
v32@?0@"AFSpeechToken"8Q16^B24
+[SLUtils decodeJsonFromFile:]
domainProb
assetVersion
SLInvocationType
SLVoiceTriggerEventInfo
SLAudioSourceOption
SLLanguageCode
Dictation
recognizerConfigs
checkerTimes
checkerConfig
checkerType
triggerEndSampleCount
triggerStartSampleCount
Missing keys in context
reason
-[SLProgressiveCheckerContext initWithContext:error:]
Missing config for Progressive checker %@
v32@?0@"NSNumber"8Q16^B24
-[SLProgressiveCheckerAnalyzer initWithConfig:withDelegate:error:]
EAR recognizer init failed for config: %@
-[SLProgressiveCheckerAnalyzer initWithConfig:withDelegate:error:]_block_invoke
v32@?0@"NSString"8Q16^B24
Progressive Checker
v32@?0@"_EARSyncSpeechRecognizer"8Q16^B24
-[SLProgressiveCheckerAnalyzer _addAudio:]
-[SLProgressiveCheckerAnalyzer _addAudio:]_block_invoke
-[SLProgressiveCheckerAnalyzer _endAudio]
-[SLProgressiveCheckerAnalyzer _endAudio]_block_invoke
com.apple.sl
Version
InputOpsMap
OutputMap
ModelFile
SupportedInputOrigins
threshold
modelIndex
name
shape
numAsrRecords
topPathScores
topPathNumTokens
latticeMaxScores
latticeMinScores
latticeMeanScores
latticeVarScores
trailingSilence
lrnnScores
lrnnProcessed
inputOrigin
acousticFTMScore
timeFromPrevQuery
speakerIdScore
airpodsConnectedState
boronActivityScore
acousticSpeechActivityScore
attSiriPrevOutputState
multiModalDecisionStage
eosLikelihood
nldaScore
mitigatorScore
mitigatorDecision
mitigatorThreshold
numTokensTopPath
trailingSilenceDuration
theshold
Missing config for Ures %@
-[SLUresMitigator initWithConfig:error:]
Unable to create model with error %@
-[SLUresMitigator dealloc]
-[SLUresMitigator _createInputOriginThresholdMap:]_block_invoke
v32@?0@"NSString"8@"NSDictionary"16^B24
-[SLUresMitigator _createInputOriginThresholdMap:]
-[SLUresMitigator _processInputFeats:]_block_invoke
Failed to create feature with error %@
-[SLUresMitigator _processInputFeats:]
Failed to get output with error %@
requestMitigated
requestNotMitigated
score
decision
decisionValue
-[SLUresMitigator _convertNSArrayToMultiArray:withShape:]
-[SLUresMitigator _convertMultiArrayToNSArray:withShape:]
%s ::: SL logging initialized (%s)
%s Received nil recog candidate, nothing to extract
%s Extracted LRNN Score: %f from Model Version: %{public}@
%s Constructing tokens for speech path %{public}@
%s Adding score %{public}ld for token %{public}@
%s ERR: metaData is nil, defaulting to NO for %{public}@
%s ERR: read metafile %{public}@ failed with %{public}@ - defaulting to NO
%s %{public}@
%s Created SLAcousticContext: %{public}@
%s Configured buffer size: %f samples, to be flushed after every %lu samples
%s Added checker to analyze %{public}lu samples with config file %{public}@
%s Failed to initialize SLProgressiveCheckerAnalyzer with error %{public}@
%s Initialized Progressive Checkers !
%s Unable to copy from circular buffer !
%s Flushed %lu samples to checker, samples since last flush %lu, total samples in buffer %lu
%s Calling endAudio after feeding %{public}lu samples to recognizer
%s Checker %lu fired, analyzed %{public}lu samples, token %{public}@,  confidence %{public}f
%s Flushed %lu samples to checker
%s All Checkers Finished, analyzed %{public}lu samples, token %{public}@, confidence %{public}f
%s %@
%s Invalid config for %{public}@
%s Threshold map: %{public}@
%s AttFeature: %{public}@ --> %{public}@
%s Value for feature: %{public}@ isn't set, abort model run
%s Failed to convert array to MLMultiArray, not using feature %{public}@ for inference
%s Using software configured threshold: %.3f
%s Failed to convert NSArray with shape %{public}@ to MLMultiArray with err %{public}@
%s Mismatch in output shape, expected: %{public}@ got: %{public}@
SLASRFeatures
SLLRNNFeatures
SLASRFeatureExtractor
SLUtils
SLBertClassifierResult
SLBertClassifier
SLProgressiveCheckerContext
SLProgressiveCheckerResult
SLProgressiveCheckerAnalyzer
SLUresMitigatorResult
SLUresMitigatorIpFeats
SLUresMitigator
localeWithLocaleIdentifier:
setLocale:
setDateFormat:
latticePathMaxScores
setLatticePathMaxScores:
latticePathMinScores
setLatticePathMinScores:
latticePathMeanScores
setLatticePathMeanScores:
latticePathVarScores
setLatticePathVarScores:
topLatticePathScores
setTopLatticePathScores:
topLatticePathTokenCount
setTopLatticePathTokenCount:
setSnr:
trailingSilence
setTrailingSilence:
.cxx_destruct
_snr
_trailingSilence
_latticePathMaxScores
_latticePathMinScores
_latticePathMeanScores
_latticePathVarScores
_topLatticePathScores
_topLatticePathTokenCount
T@"NSArray",&,N,V_latticePathMaxScores
T@"NSArray",&,N,V_latticePathMinScores
T@"NSArray",&,N,V_latticePathMeanScores
T@"NSArray",&,N,V_latticePathVarScores
T@"NSArray",&,N,V_topLatticePathScores
TQ,N,V_topLatticePathTokenCount
Tf,N,V_snr
Tf,N,V_trailingSilence
lrnnScore
setLrnnScore:
lrnnProcessed
setLrnnProcessed:
_lrnnProcessed
_lrnnScore
Tf,N,V_lrnnScore
TB,N,V_lrnnProcessed
recognition
utterances
count
initWithCapacity:
phrases
_getTokenConfidenceForPath:fromPhrases:
expressionForConstantValue:
arrayWithObject:
expressionForFunction:arguments:
expressionValueWithObject:context:
addObject:
doubleValue
numberWithDouble:
floatValue
_getLastTokenForPath:fromPhrases:
enumerateObjectsUsingBlock:
audioAnalytics
endTime
silenceStartTime
latticeMitigatorResult
score
threshold
version
interpretationIndices
lastObject
unsignedIntegerValue
interpretations
objectAtIndex:
tokens
firstObject
array
dictionaryRepresentation
objectAtIndexedSubscript:
addObjectsFromArray:
confidenceScore
numberWithInteger:
text
countByEnumeratingWithState:objects:count:
removeSpaceBefore
removeSpaceAfter
appendString:
extractASRFaturesFrom:
extractLRNNFaturesFrom:
getBestSpeechRecognitionTextFromPackage:
dataWithContentsOfFile:
JSONObjectWithData:options:error:
decodeJsonFromFile:
init
dictionary
addEntriesFromDictionary:
setObject:forKey:
initWithScore:assetVersion:extractedFeats:
domainProb
assetVersion
extractedFeats
_domainProb
_assetVersion
_extractedFeats
T@"NSArray",R,N,V_domainProb
T@"NSString",R,N,V_assetVersion
T@"NSDictionary",R,N,V_extractedFeats
initWithConfig:error:locale:
processInputText:
objectForKeyedSubscript:
stringWithFormat:
dictionaryWithObjects:forKeys:count:
errorWithDomain:code:userInfo:
initWithContext:error:
audioOption
vtei
invocationType
locale
_audioOption
_vtei
_invocationType
_locale
TQ,R,N,V_audioOption
T@"NSDictionary",R,N,V_vtei
TQ,R,N,V_invocationType
T@"NSString",R,N,V_locale
initWithScore:ofType:analyzedSamples:detailedResults:
resultType
analyzedSamples
detailedResult
_score
_resultType
_analyzedSamples
_detailedResult
TQ,R,N,V_resultType
TQ,R,N,V_analyzedSamples
Tf,R,N,V_score
T@"NSArray",R,N,V_detailedResult
stringByDeletingLastPathComponent
objectForKey:
unsignedIntValue
numberWithUnsignedInteger:
initWithNumChannels:recordingDuration:samplingRate:
stringByAppendingPathComponent:
initWithConfiguration:
localizedDescription
_startNewRequestWithContext:
_addAudio:
_endAudio
resetWithSamplingRate:language:taskType:userId:sessionId:deviceId:farField:audioSource:maxAudioBufferSizeSeconds:
reset
length
bytes
addSamples:numSamples:
sampleCount
bufferLength
copyBufferWithNumSamplesCopiedIn:
resultsWithAddedFloatAudio:numberOfSamples:taskName:
resultsWithAddedAudio:numberOfSamples:taskName:
resultsWithEndedAudio
removeObjectAtIndex:
confidence
tokenName
analyzer:hasPartialResult:
indexSetWithIndex:
removeObjectsAtIndexes:
copybufferFrom:to:
removeObject:
analyzer:hasFinalResult:
initWithConfig:withDelegate:error:
startNewRequestWithContext:
addAudio:
endAudio
_activeRecognizers
_context
_checkerEndSamples
_analyzedSamplesSoFar
_latestScore
_queue
_delegate
_checkerType
_circBuffer
_numSamplesAddedToBufferSinceLastFlush
_numSamplesInStride
initWithScore:decision:decisionLevel:detailedResults:extractedFeats:
didMitigate
decisionLevel
_didMitigate
_threshold
_decisionLevel
TB,R,N,V_didMitigate
Td,R,N,V_decisionLevel
Tf,R,N,V_threshold
T@"NSDictionary",R,N,V_detailedResult
speechPackage
setSpeechPackage:
inputOrigin
setInputOrigin:
acousticFTMScores
setAcousticFTMScores:
boronScore
setBoronScore:
speakerIDScore
setSpeakerIDScore:
didDetectSpeechActivity
setDidDetectSpeechActivity:
isAirpodsConnected
setIsAirpodsConnected:
timeSinceLastQuery
setTimeSinceLastQuery:
decisionStage
setDecisionStage:
prevStageOutput
setPrevStageOutput:
eosLikelihood
setEosLikelihood:
nldaMetaInfo
setNldaMetaInfo:
nldaScore
setNldaScore:
_didDetectSpeechActivity
_isAirpodsConnected
_speechPackage
_inputOrigin
_acousticFTMScores
_boronScore
_speakerIDScore
_timeSinceLastQuery
_decisionStage
_prevStageOutput
_eosLikelihood
_nldaMetaInfo
_nldaScore
T@"AFSpeechPackage",&,N,V_speechPackage
T@"NSNumber",&,N,V_inputOrigin
T@"NSNumber",&,N,V_acousticFTMScores
T@"NSNumber",&,N,V_boronScore
T@"NSNumber",&,N,V_speakerIDScore
TB,N,V_didDetectSpeechActivity
TB,N,V_isAirpodsConnected
Td,N,V_timeSinceLastQuery
TQ,N,V_decisionStage
T@"NSNumber",&,N,V_prevStageOutput
T@"NSNumber",&,N,V_eosLikelihood
T@"NSDictionary",&,N,V_nldaMetaInfo
T@"NSNumber",&,N,V_nldaScore
fileURLWithPath:
setComputeUnits:
modelWithContentsOfURL:configuration:error:
setUsesCPUOnly:
_createInputOriginThresholdMap:
dealloc
_processInputFeats:
enumerateKeysAndObjectsUsingBlock:
isEqualToString:
arrayWithObjects:count:
numberWithFloat:
numberWithBool:
valueForKey:
_convertNSArrayToMultiArray:withShape:
initWithDictionary:error:
predictionFromFeatures:options:error:
featureValueForName:
multiArrayValue
_convertMultiArrayToNSArray:withShape:
initWithShape:dataType:error:
setObject:forKeyedSubscript:
shape
getTranscriptionForSpeechPackage:
initWithConfig:error:
processInputFeats:completion:
_uresModel
_options
_inputOpsMap
_outputMap
_thresholdMap
_version
@16@0:8
v24@0:8@16
Q16@0:8
v24@0:8Q16
f16@0:8
v20@0:8f16
v16@0:8
@"NSArray"
B16@0:8
v20@0:8B16
@24@0:8@16
@32@0:8@16@24
@40@0:8@16@24@32
@"NSString"
@"NSDictionary"
@40@0:8@16^@24@32
@32@0:8@16^@24
@44@0:8f16Q20Q28@36
@40@0:8@16@24^@32
@"NSMutableArray"
@"SLProgressiveCheckerContext"
@"NSObject<OS_dispatch_queue>"
@"<SLProgressiveCheckerAnalyzerDelegate>"
@"CSAudioCircularBuffer"
@48@0:8f16B20d24@32@40
d16@0:8
v24@0:8d16
@"AFSpeechPackage"
@"NSNumber"
v32@0:8@16@?24
@"MLModel"
@"MLPredictionOptions"
@"NSMutableDictionary"
@(#)PROGRAM:SiriLiminal  PROJECT:CoreSpeech-
com.apple.SiriLiminal
Framework
v8@?0
::: Initializing SiriLiminal logging...
en_US_POSIX
yyyyMMdd-HHmmss
SLLogInitIfNeeded_block_invoke
gitrelno_unavailable
+[SLASRFeatureExtractor extractASRFaturesFrom:]
max:
min:
stddev:
average:
v32@?0@"AFSpeechUtterance"8Q16^B24
+[SLASRFeatureExtractor extractLRNNFaturesFrom:]
+[SLASRFeatureExtractor _getTokenConfidenceForPath:fromPhrases:]
v32@?0@"AFSpeechPhrase"8Q16^B24
+[SLASRFeatureExtractor _getTokenConfidenceForPath:fromPhrases:]_block_invoke
v32@?0@"AFSpeechToken"8Q16^B24
+[SLUtils decodeJsonFromFile:]
domainProb
assetVersion
SLInvocationType
SLVoiceTriggerEventInfo
SLAudioSourceOption
SLLanguageCode
Dictation
recognizerConfigs
checkerTimes
checkerConfig
checkerType
triggerEndSampleCount
triggerStartSampleCount
Missing keys in context
reason
-[SLProgressiveCheckerContext initWithContext:error:]
Missing config for Progressive checker %@
v32@?0@"NSNumber"8Q16^B24
-[SLProgressiveCheckerAnalyzer initWithConfig:withDelegate:error:]
EAR recognizer init failed for config: %@
-[SLProgressiveCheckerAnalyzer initWithConfig:withDelegate:error:]_block_invoke
v32@?0@"NSString"8Q16^B24
Progressive Checker
v32@?0@"_EARSyncSpeechRecognizer"8Q16^B24
-[SLProgressiveCheckerAnalyzer _addAudio:]
-[SLProgressiveCheckerAnalyzer _addAudio:]_block_invoke
-[SLProgressiveCheckerAnalyzer _endAudio]
-[SLProgressiveCheckerAnalyzer _endAudio]_block_invoke
com.apple.sl
Version
InputOpsMap
OutputMap
ModelFile
SupportedInputOrigins
threshold
modelIndex
name
shape
numAsrRecords
topPathScores
topPathNumTokens
latticeMaxScores
latticeMinScores
latticeMeanScores
latticeVarScores
trailingSilence
lrnnScores
lrnnProcessed
inputOrigin
acousticFTMScore
timeFromPrevQuery
speakerIdScore
airpodsConnectedState
boronActivityScore
acousticSpeechActivityScore
attSiriPrevOutputState
multiModalDecisionStage
eosLikelihood
nldaScore
mitigatorScore
mitigatorDecision
mitigatorThreshold
numTokensTopPath
trailingSilenceDuration
theshold
Missing config for Ures %@
-[SLUresMitigator initWithConfig:error:]
Unable to create model with error %@
-[SLUresMitigator dealloc]
-[SLUresMitigator _createInputOriginThresholdMap:]_block_invoke
v32@?0@"NSString"8@"NSDictionary"16^B24
-[SLUresMitigator _createInputOriginThresholdMap:]
-[SLUresMitigator _processInputFeats:]_block_invoke
Failed to create feature with error %@
-[SLUresMitigator _processInputFeats:]
Failed to get output with error %@
requestMitigated
requestNotMitigated
score
decision
decisionValue
-[SLUresMitigator _convertNSArrayToMultiArray:withShape:]
-[SLUresMitigator _convertMultiArrayToNSArray:withShape:]
%s ::: SL logging initialized (%s)
%s Received nil recog candidate, nothing to extract
%s Extracted LRNN Score: %f from Model Version: %{public}@
%s Constructing tokens for speech path %{public}@
%s Adding score %{public}ld for token %{public}@
%s ERR: metaData is nil, defaulting to NO for %{public}@
%s ERR: read metafile %{public}@ failed with %{public}@ - defaulting to NO
%s %{public}@
%s Created SLAcousticContext: %{public}@
%s Configured buffer size: %f samples, to be flushed after every %lu samples
%s Added checker to analyze %{public}lu samples with config file %{public}@
%s Failed to initialize SLProgressiveCheckerAnalyzer with error %{public}@
%s Initialized Progressive Checkers !
%s Unable to copy from circular buffer !
%s Flushed %lu samples to checker, samples since last flush %lu, total samples in buffer %lu
%s Calling endAudio after feeding %{public}lu samples to recognizer
%s Checker %lu fired, analyzed %{public}lu samples, token %{public}@,  confidence %{public}f
%s Flushed %lu samples to checker
%s All Checkers Finished, analyzed %{public}lu samples, token %{public}@, confidence %{public}f
%s %@
%s Invalid config for %{public}@
%s Threshold map: %{public}@
%s AttFeature: %{public}@ --> %{public}@
%s Value for feature: %{public}@ isn't set, abort model run
%s Failed to convert array to MLMultiArray, not using feature %{public}@ for inference
%s Using software configured threshold: %.3f
%s Failed to convert NSArray with shape %{public}@ to MLMultiArray with err %{public}@
%s Mismatch in output shape, expected: %{public}@ got: %{public}@
SLASRFeatures
SLLRNNFeatures
SLASRFeatureExtractor
SLUtils
SLBertClassifierResult
SLBertClassifier
SLProgressiveCheckerContext
SLProgressiveCheckerResult
SLProgressiveCheckerAnalyzer
SLUresMitigatorResult
SLUresMitigatorIpFeats
SLUresMitigator
localeWithLocaleIdentifier:
setLocale:
setDateFormat:
latticePathMaxScores
setLatticePathMaxScores:
latticePathMinScores
setLatticePathMinScores:
latticePathMeanScores
setLatticePathMeanScores:
latticePathVarScores
setLatticePathVarScores:
topLatticePathScores
setTopLatticePathScores:
topLatticePathTokenCount
setTopLatticePathTokenCount:
setSnr:
trailingSilence
setTrailingSilence:
.cxx_destruct
_snr
_trailingSilence
_latticePathMaxScores
_latticePathMinScores
_latticePathMeanScores
_latticePathVarScores
_topLatticePathScores
_topLatticePathTokenCount
T@"NSArray",&,N,V_latticePathMaxScores
T@"NSArray",&,N,V_latticePathMinScores
T@"NSArray",&,N,V_latticePathMeanScores
T@"NSArray",&,N,V_latticePathVarScores
T@"NSArray",&,N,V_topLatticePathScores
TQ,N,V_topLatticePathTokenCount
Tf,N,V_snr
Tf,N,V_trailingSilence
lrnnScore
setLrnnScore:
lrnnProcessed
setLrnnProcessed:
_lrnnProcessed
_lrnnScore
Tf,N,V_lrnnScore
TB,N,V_lrnnProcessed
recognition
utterances
count
initWithCapacity:
phrases
_getTokenConfidenceForPath:fromPhrases:
expressionForConstantValue:
arrayWithObject:
expressionForFunction:arguments:
expressionValueWithObject:context:
addObject:
doubleValue
numberWithDouble:
floatValue
_getLastTokenForPath:fromPhrases:
enumerateObjectsUsingBlock:
audioAnalytics
endTime
silenceStartTime
latticeMitigatorResult
score
threshold
version
interpretationIndices
lastObject
unsignedIntegerValue
interpretations
objectAtIndex:
tokens
firstObject
array
dictionaryRepresentation
objectAtIndexedSubscript:
addObjectsFromArray:
confidenceScore
numberWithInteger:
text
countByEnumeratingWithState:objects:count:
removeSpaceBefore
removeSpaceAfter
appendString:
extractASRFaturesFrom:
extractLRNNFaturesFrom:
getBestSpeechRecognitionTextFromPackage:
dataWithContentsOfFile:
JSONObjectWithData:options:error:
decodeJsonFromFile:
init
dictionary
addEntriesFromDictionary:
setObject:forKey:
initWithScore:assetVersion:extractedFeats:
domainProb
assetVersion
extractedFeats
_domainProb
_assetVersion
_extractedFeats
T@"NSArray",R,N,V_domainProb
T@"NSString",R,N,V_assetVersion
T@"NSDictionary",R,N,V_extractedFeats
initWithConfig:error:locale:
processInputText:
objectForKeyedSubscript:
stringWithFormat:
dictionaryWithObjects:forKeys:count:
errorWithDomain:code:userInfo:
initWithContext:error:
audioOption
vtei
invocationType
locale
_audioOption
_vtei
_invocationType
_locale
TQ,R,N,V_audioOption
T@"NSDictionary",R,N,V_vtei
TQ,R,N,V_invocationType
T@"NSString",R,N,V_locale
initWithScore:ofType:analyzedSamples:detailedResults:
resultType
analyzedSamples
detailedResult
_score
_resultType
_analyzedSamples
_detailedResult
TQ,R,N,V_resultType
TQ,R,N,V_analyzedSamples
Tf,R,N,V_score
T@"NSArray",R,N,V_detailedResult
stringByDeletingLastPathComponent
objectForKey:
unsignedIntValue
numberWithUnsignedInteger:
initWithNumChannels:recordingDuration:samplingRate:
stringByAppendingPathComponent:
initWithConfiguration:
localizedDescription
_startNewRequestWithContext:
_addAudio:
_endAudio
resetWithSamplingRate:language:taskType:userId:sessionId:deviceId:farField:audioSource:maxAudioBufferSizeSeconds:
reset
length
bytes
addSamples:numSamples:
sampleCount
bufferLength
copyBufferWithNumSamplesCopiedIn:
resultsWithAddedFloatAudio:numberOfSamples:taskName:
resultsWithAddedAudio:numberOfSamples:taskName:
resultsWithEndedAudio
removeObjectAtIndex:
confidence
tokenName
analyzer:hasPartialResult:
indexSetWithIndex:
removeObjectsAtIndexes:
copybufferFrom:to:
removeObject:
analyzer:hasFinalResult:
initWithConfig:withDelegate:error:
startNewRequestWithContext:
addAudio:
endAudio
_activeRecognizers
_context
_checkerEndSamples
_analyzedSamplesSoFar
_latestScore
_queue
_delegate
_checkerType
_circBuffer
_numSamplesAddedToBufferSinceLastFlush
_numSamplesInStride
initWithScore:decision:decisionLevel:detailedResults:extractedFeats:
didMitigate
decisionLevel
_didMitigate
_threshold
_decisionLevel
TB,R,N,V_didMitigate
Td,R,N,V_decisionLevel
Tf,R,N,V_threshold
T@"NSDictionary",R,N,V_detailedResult
speechPackage
setSpeechPackage:
inputOrigin
setInputOrigin:
acousticFTMScores
setAcousticFTMScores:
boronScore
setBoronScore:
speakerIDScore
setSpeakerIDScore:
didDetectSpeechActivity
setDidDetectSpeechActivity:
isAirpodsConnected
setIsAirpodsConnected:
timeSinceLastQuery
setTimeSinceLastQuery:
decisionStage
setDecisionStage:
prevStageOutput
setPrevStageOutput:
eosLikelihood
setEosLikelihood:
nldaMetaInfo
setNldaMetaInfo:
nldaScore
setNldaScore:
_didDetectSpeechActivity
_isAirpodsConnected
_speechPackage
_inputOrigin
_acousticFTMScores
_boronScore
_speakerIDScore
_timeSinceLastQuery
_decisionStage
_prevStageOutput
_eosLikelihood
_nldaMetaInfo
_nldaScore
T@"AFSpeechPackage",&,N,V_speechPackage
T@"NSNumber",&,N,V_inputOrigin
T@"NSNumber",&,N,V_acousticFTMScores
T@"NSNumber",&,N,V_boronScore
T@"NSNumber",&,N,V_speakerIDScore
TB,N,V_didDetectSpeechActivity
TB,N,V_isAirpodsConnected
Td,N,V_timeSinceLastQuery
TQ,N,V_decisionStage
T@"NSNumber",&,N,V_prevStageOutput
T@"NSNumber",&,N,V_eosLikelihood
T@"NSDictionary",&,N,V_nldaMetaInfo
T@"NSNumber",&,N,V_nldaScore
fileURLWithPath:
setComputeUnits:
modelWithContentsOfURL:configuration:error:
setUsesCPUOnly:
_createInputOriginThresholdMap:
dealloc
_processInputFeats:
enumerateKeysAndObjectsUsingBlock:
isEqualToString:
arrayWithObjects:count:
numberWithFloat:
numberWithBool:
valueForKey:
_convertNSArrayToMultiArray:withShape:
initWithDictionary:error:
predictionFromFeatures:options:error:
featureValueForName:
multiArrayValue
_convertMultiArrayToNSArray:withShape:
initWithShape:dataType:error:
setObject:forKeyedSubscript:
shape
getTranscriptionForSpeechPackage:
initWithConfig:error:
processInputFeats:completion:
_uresModel
_options
_inputOpsMap
_outputMap
_thresholdMap
_version
@16@0:8
v24@0:8@16
Q16@0:8
v24@0:8Q16
f16@0:8
v20@0:8f16
v16@0:8
@"NSArray"
B16@0:8
v20@0:8B16
@24@0:8@16
@32@0:8@16@24
@40@0:8@16@24@32
@"NSString"
@"NSDictionary"
@40@0:8@16^@24@32
@32@0:8@16^@24
@44@0:8f16Q20Q28@36
@40@0:8@16@24^@32
@"NSMutableArray"
@"SLProgressiveCheckerContext"
@"NSObject<OS_dispatch_queue>"
@"<SLProgressiveCheckerAnalyzerDelegate>"
@"CSAudioCircularBuffer"
@48@0:8f16B20d24@32@40
d16@0:8
v24@0:8d16
@"AFSpeechPackage"
@"NSNumber"
v32@0:8@16@?24
@"MLModel"
@"MLPredictionOptions"
@"NSMutableDictionary"
