@(#)PROGRAM:HomeAI  PROJECT:HomeAI-150.5.9
Mb@?
UsesIOSurface
HardwareAcceleratedTransfer
HighSpeedTransfer
HMIPixelBufferTransferMultiStageResize
a0cTa1cTa2cTa3cTa0hTa1hTa2hTa3hT
Motion
Person
Vehicle
sFXg@
+Sg@
?>Ip
?(-_U
=$@'
hp|i1
f?94{W
A`~q@
#>m?
?>Ip
iUK:
\.@;&b|^
hp|i1
?UUUUUU
?333333
ffffff
N6homeai3mod28ImageDescriptorBufferFloat32E
NSt3__118basic_stringstreamIcNS_11char_traitsIcEENS_9allocatorIcEEEE
NSt3__115basic_stringbufIcNS_11char_traitsIcEENS_9allocatorIcEEEE
HBN6homeai10clustering15GreedyClustererE
NSt3__120__shared_ptr_emplaceINS_6vectorIxNS_9allocatorIxEEEENS2_IS4_EEEE
NSt3__120__shared_ptr_emplaceIN6homeai10clustering15GreedyClusterer9cluster_tENS_9allocatorIS4_EEEE
NSt3__120__shared_ptr_emplaceINS_6vectorINS_5tupleIJxxfEEENS_9allocatorIS3_EEEENS4_IS6_EEEE
N6homeai3mod29ImageDescriptorBufferAbstractE
?ffffff
GNSt3__120__shared_ptr_emplaceIN6homeai10clustering15GreedyClustererENS_9allocatorIS3_EEEE
MbP?
]=dZ
>X@[;
Bhs<
m%*=
%pP=2.
333?
pAfff?{
Motion
Person
Vehicle
Face
Package
N2cv13BaseRowFilterE
N2cv16BaseColumnFilterE
N2cv10BaseFilterE
N2cv12FilterEngineE
N2cv18SymmRowSmallFilterIhiNS_21SymmRowSmallVec_8u32sEEE
N2cv9RowFilterIhiNS_21SymmRowSmallVec_8u32sEEE
N2cv18SymmRowSmallFilterIffNS_19SymmRowSmallVec_32fEEE
N2cv9RowFilterIffNS_19SymmRowSmallVec_32fEEE
N2cv9RowFilterIhiNS_12RowVec_8u32sEEE
N2cv9RowFilterIhfNS_8RowNoVecEEE
N2cv9RowFilterIhdNS_8RowNoVecEEE
N2cv9RowFilterItfNS_8RowNoVecEEE
N2cv9RowFilterItdNS_8RowNoVecEEE
N2cv9RowFilterIsfNS_13RowVec_16s32fEEE
N2cv9RowFilterIsdNS_8RowNoVecEEE
N2cv9RowFilterIffNS_10RowVec_32fEEE
N2cv9RowFilterIfdNS_8RowNoVecEEE
N2cv9RowFilterIddNS_8RowNoVecEEE
N2cv12ColumnFilterINS_13FixedPtCastExIihEENS_11ColumnNoVecEEE
N2cv12ColumnFilterINS_4CastIfhEENS_11ColumnNoVecEEE
N2cv12ColumnFilterINS_4CastIdhEENS_11ColumnNoVecEEE
N2cv12ColumnFilterINS_4CastIftEENS_11ColumnNoVecEEE
N2cv12ColumnFilterINS_4CastIdtEENS_11ColumnNoVecEEE
N2cv12ColumnFilterINS_4CastIfsEENS_11ColumnNoVecEEE
N2cv12ColumnFilterINS_4CastIdsEENS_11ColumnNoVecEEE
N2cv12ColumnFilterINS_4CastIffEENS_11ColumnNoVecEEE
N2cv12ColumnFilterINS_4CastIddEENS_11ColumnNoVecEEE
N2cv21SymmColumnSmallFilterINS_13FixedPtCastExIihEENS_19SymmColumnVec_32s8uEEE
N2cv16SymmColumnFilterINS_13FixedPtCastExIihEENS_19SymmColumnVec_32s8uEEE
N2cv12ColumnFilterINS_13FixedPtCastExIihEENS_19SymmColumnVec_32s8uEEE
N2cv21SymmColumnSmallFilterINS_4CastIisEENS_25SymmColumnSmallVec_32s16sEEE
N2cv16SymmColumnFilterINS_4CastIisEENS_25SymmColumnSmallVec_32s16sEEE
N2cv12ColumnFilterINS_4CastIisEENS_25SymmColumnSmallVec_32s16sEEE
N2cv21SymmColumnSmallFilterINS_4CastIffEENS_22SymmColumnSmallVec_32fEEE
N2cv16SymmColumnFilterINS_4CastIffEENS_22SymmColumnSmallVec_32fEEE
N2cv12ColumnFilterINS_4CastIffEENS_22SymmColumnSmallVec_32fEEE
N2cv16SymmColumnFilterINS_4CastIfhEENS_11ColumnNoVecEEE
N2cv16SymmColumnFilterINS_4CastIdhEENS_11ColumnNoVecEEE
N2cv16SymmColumnFilterINS_4CastIftEENS_11ColumnNoVecEEE
N2cv16SymmColumnFilterINS_4CastIdtEENS_11ColumnNoVecEEE
N2cv16SymmColumnFilterINS_4CastIisEENS_11ColumnNoVecEEE
N2cv12ColumnFilterINS_4CastIisEENS_11ColumnNoVecEEE
N2cv16SymmColumnFilterINS_4CastIfsEENS_20SymmColumnVec_32f16sEEE
N2cv12ColumnFilterINS_4CastIfsEENS_20SymmColumnVec_32f16sEEE
N2cv16SymmColumnFilterINS_4CastIdsEENS_11ColumnNoVecEEE
N2cv16SymmColumnFilterINS_4CastIffEENS_17SymmColumnVec_32fEEE
N2cv12ColumnFilterINS_4CastIffEENS_17SymmColumnVec_32fEEE
N2cv16SymmColumnFilterINS_4CastIddEENS_11ColumnNoVecEEE
N2cv8Filter2DIhNS_4CastIfhEENS_12FilterVec_8uEEE
N2cv8Filter2DIhNS_4CastIftEENS_11FilterNoVecEEE
N2cv8Filter2DIhNS_4CastIfsEENS_15FilterVec_8u16sEEE
N2cv8Filter2DIhNS_4CastIffEENS_11FilterNoVecEEE
N2cv8Filter2DIhNS_4CastIddEENS_11FilterNoVecEEE
N2cv8Filter2DItNS_4CastIftEENS_11FilterNoVecEEE
N2cv8Filter2DItNS_4CastIffEENS_11FilterNoVecEEE
N2cv8Filter2DItNS_4CastIddEENS_11FilterNoVecEEE
N2cv8Filter2DIsNS_4CastIfsEENS_11FilterNoVecEEE
N2cv8Filter2DIsNS_4CastIffEENS_11FilterNoVecEEE
N2cv8Filter2DIsNS_4CastIddEENS_11FilterNoVecEEE
N2cv8Filter2DIfNS_4CastIffEENS_13FilterVec_32fEEE
N2cv8Filter2DIdNS_4CastIddEENS_11FilterNoVecEEE
 !"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\]^_`abcdefghijklmnopqrstuvwxyz{|}~
ucwsifdr
?qxs?qxs
9D)5:
@`|-A
PF SdF
 [@W:[
D]eV
"x@
z4I5z
?~my
!Y?4
!I?u
!)?gX
?g^FT
?$"LT
?$"LT
N2cv15ThresholdRunnerE
N2cv6detail16LKTrackerInvokerE
N2cv9ColumnSumIihEE
N2cv9ColumnSumIitEE
N2cv9ColumnSumIisEE
N2cv6RowSumIhiEE
N2cv6RowSumIhdEE
N2cv6RowSumItiEE
N2cv6RowSumItdEE
N2cv6RowSumIsiEE
N2cv6RowSumIiiEE
N2cv6RowSumIsdEE
N2cv6RowSumIfdEE
N2cv6RowSumIddEE
N2cv9ColumnSumIdhEE
N2cv9ColumnSumIdtEE
N2cv9ColumnSumIdsEE
N2cv9ColumnSumIiiEE
N2cv9ColumnSumIifEE
N2cv9ColumnSumIdfEE
N2cv9ColumnSumIidEE
N2cv9ColumnSumIddEE
 !"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\]^_`abcdefghijklmnopqrstuvwxyz{|}~
<N2cv11_InputArrayE
N2cv12_OutputArrayE
N2cv9ExceptionE
14EmptyFuncTable
12GpuFuncTable
N2cv16ParallelLoopBodyE
N2cv16MorphologyRunnerE
N2cv14MorphRowFilterINS_5MinOpIhEENS_12MorphRowIVecINS_6VMin8uEEEEE
N2cv14MorphRowFilterINS_5MinOpItEENS_12MorphRowIVecINS_7VMin16uEEEEE
N2cv14MorphRowFilterINS_5MinOpIsEENS_12MorphRowIVecINS_7VMin16sEEEEE
N2cv14MorphRowFilterINS_5MinOpIfEENS_12MorphRowFVecINS_7VMin32fEEEEE
N2cv14MorphRowFilterINS_5MinOpIdEENS_13MorphRowNoVecEEE
N2cv14MorphRowFilterINS_5MaxOpIhEENS_12MorphRowIVecINS_6VMax8uEEEEE
N2cv14MorphRowFilterINS_5MaxOpItEENS_12MorphRowIVecINS_7VMax16uEEEEE
N2cv14MorphRowFilterINS_5MaxOpIsEENS_12MorphRowIVecINS_7VMax16sEEEEE
N2cv14MorphRowFilterINS_5MaxOpIfEENS_12MorphRowFVecINS_7VMax32fEEEEE
N2cv14MorphRowFilterINS_5MaxOpIdEENS_13MorphRowNoVecEEE
N2cv17MorphColumnFilterINS_5MinOpIhEENS_15MorphColumnIVecINS_6VMin8uEEEEE
N2cv17MorphColumnFilterINS_5MinOpItEENS_15MorphColumnIVecINS_7VMin16uEEEEE
N2cv17MorphColumnFilterINS_5MinOpIsEENS_15MorphColumnIVecINS_7VMin16sEEEEE
N2cv17MorphColumnFilterINS_5MinOpIfEENS_15MorphColumnFVecINS_7VMin32fEEEEE
N2cv17MorphColumnFilterINS_5MinOpIdEENS_16MorphColumnNoVecEEE
N2cv17MorphColumnFilterINS_5MaxOpIhEENS_15MorphColumnIVecINS_6VMax8uEEEEE
N2cv17MorphColumnFilterINS_5MaxOpItEENS_15MorphColumnIVecINS_7VMax16uEEEEE
N2cv17MorphColumnFilterINS_5MaxOpIsEENS_15MorphColumnIVecINS_7VMax16sEEEEE
N2cv17MorphColumnFilterINS_5MaxOpIfEENS_15MorphColumnFVecINS_7VMax32fEEEEE
N2cv17MorphColumnFilterINS_5MaxOpIdEENS_16MorphColumnNoVecEEE
N2cv11MorphFilterINS_5MinOpIhEENS_9MorphIVecINS_6VMin8uEEEEE
N2cv11MorphFilterINS_5MinOpItEENS_9MorphIVecINS_7VMin16uEEEEE
N2cv11MorphFilterINS_5MinOpIsEENS_9MorphIVecINS_7VMin16sEEEEE
N2cv11MorphFilterINS_5MinOpIfEENS_9MorphFVecINS_7VMin32fEEEEE
N2cv11MorphFilterINS_5MinOpIdEENS_10MorphNoVecEEE
N2cv11MorphFilterINS_5MaxOpIhEENS_9MorphIVecINS_6VMax8uEEEEE
N2cv11MorphFilterINS_5MaxOpItEENS_9MorphIVecINS_7VMax16uEEEEE
N2cv11MorphFilterINS_5MaxOpIsEENS_9MorphIVecINS_7VMax16sEEEEE
N2cv11MorphFilterINS_5MaxOpIfEENS_9MorphFVecINS_7VMax32fEEEEE
N2cv11MorphFilterINS_5MaxOpIdEENS_10MorphNoVecEEE
?N2cv5MatOpE
N2cv14MatOp_IdentityE
N2cv11MatOp_AddExE
N2cv9MatOp_BinE
N2cv9MatOp_CmpE
N2cv10MatOp_GEMME
N2cv12MatOp_InvertE
N2cv7MatOp_TE
N2cv11MatOp_SolveE
N2cv17MatOp_InitializerE
Error creating directory for person:%@
version
UUID
displayName
person.json
Error saving metadata to disk for person:%@
v8@?0
Error creating directory for person face crop:%@
faceBoundingBox
dateCreated
facecrop.json
Error saving metadata to disk for person face crop:%@
facecrop.jpeg
Error saving face crop image to disk for person face crop:%@
external.person.datasource.disk
HMIMutableCluster
Skipping flood fill total non zero pixels: %d
FloodFill generated more than 255 connected components.
Found %d components. Total non zero pixels: %d
image__Placeholder__0
ShotNet__ShotNet__ssd_predictions__block8_box__box_offset__0
ShotNet__ShotNet__ssd_predictions__block7_box__box_offset__0
ShotNet__ShotNet__ssd_predictions__block6_box__box_offset__0
ShotNet__ShotNet__ssd_predictions__block9_box__box_offset__0
ShotNet__ShotNet__ssd_predictions__block10_box__box_offset__0
ShotNet__ShotNet__ssd_predictions__block11_box__box_offset__0
ShotNet__ShotNet__ssd_predictions__block8_box__class_prob__0
ShotNet__ShotNet__ssd_predictions__block7_box__class_prob__0
ShotNet__ShotNet__ssd_predictions__block6_box__class_prob__0
ShotNet__ShotNet__ssd_predictions__block9_box__class_prob__0
ShotNet__ShotNet__ssd_predictions__block10_box__class_prob__0
ShotNet__ShotNet__ssd_predictions__block11_box__class_prob__0
ShotNet__ShotNet__ssd_predictions__block8_box__yaw_angle__0
ShotNet__ShotNet__ssd_predictions__block7_box__yaw_angle__0
ShotNet__ShotNet__ssd_predictions__block6_box__yaw_angle__0
ShotNet__ShotNet__ssd_predictions__block9_box__yaw_angle__0
ShotNet__ShotNet__ssd_predictions__block10_box__yaw_angle__0
ShotNet__ShotNet__ssd_predictions__block11_box__yaw_angle__0
ShotNet__ShotNet__ssd_predictions__block8_box__roll_angle__0
ShotNet__ShotNet__ssd_predictions__block7_box__roll_angle__0
ShotNet__ShotNet__ssd_predictions__block6_box__roll_angle__0
ShotNet__ShotNet__ssd_predictions__block9_box__roll_angle__0
ShotNet__ShotNet__ssd_predictions__block10_box__roll_angle__0
ShotNet__ShotNet__ssd_predictions__block11_box__roll_angle__0
SignificantActivityDetector
CoreML output nil or not of type MLFeatureTypeMultiArray
SignificantActivity
mlmodelc
significant.activity.detector
HMISystemResourceUsageLevelLow
HMISystemResourceUsageLevelMed
HMISystemResourceUsageLevelHigh
HMISystemResourceUsageLevelUnknown
HMISystemResourceUsageLevelUndefined
More than maximum supported components.
B16@?0@"NSValue"8
{CGRect={CGPoint=dd}{CGSize=dd}}
@"NSValue"16@?0@"NSValue"8
FaceFilteredState
HMIVAEF.fr
HMIVAEF.ya
HMIVAEF.ro
None
QualitySVMKnown
QualitySVMUnknown
QualityANFRScore
NoPredictions
HasFaceMask
Face Recognition
Face Yaw
Face Roll
%@@(%@,%@)
HMIVideoAnalyzerEventFace
%c%c%c%c
Type: %@, DTS: %@, PTS: %@, DUR: %@, NUM: %ld [%@]
Sync
%c%02d:%02d:%02d.%03d
%*lld/%d %@
%*lld %@
POSITIVE INFINITY
NEGATIVE INFINITY
   INDEFINITE    
  INVALID TIME   
%@ DTS %@ PTS %@ dur %@%@
PTS: %.2f
Bogus atomSize %llu, recovering by adjusting size.
en_US_POSIX
yyyy-MM-dd'T'HH:mm:ss
%.3f x %.3f
%.3f, %.3f %@
%.2f
%02x
q24@?0@"<HMIVideoEvent>"8@"<HMIVideoEvent>"16
B32@?0@"<HMIVideoEvent>"8Q16^B24
@16@?0@"<HMIVideoEvent>"8
Time: %@, Date: %@
lock
HMIVideoAnalyzerEventMotion
HMICoreAnalyticsVIPModelReportTime
Received nil data source
Fetching settings using data source: %@
Error fetching settings: %@
v24@?0@"HMIHomePersonManagerSettings"8@"NSError"16
handleUpdatedPerson: %@
handleUpdatedUnassociatedFaceCrop: %@
handleUpdatedPersonFaceCrop: %@
handleUpdatedFaceprint: %@
handleUpdatedSettings: %@
handleRemovedPersonWithUUID: %@
handleRemovedFaceCropWithUUID: %@
handleRemovedFaceprintWithUUID: %@
Successfully handled face misclassification
Error in handling face misclassification, error:%@
v24@?0@"NSDictionary"8@"NSError"16
Submitted face misclassification task, taskID:%u
B16@?0@"HMIFaceClassification"8
Not storing face crop and faceprint for HMIHomePersonManager.UUID:%@ from face event:%@
Storing unknown to Home face crop:%@ and faceprint:%@
Error storing unassociated face crop:%@, error:%@
Stored unassociated face crop:%@
v16@?0@"NSError"8
Error storing faceprint:%@, error:%@
Stored faceprint:%@
Reached face crop limit for sessionEntityUUID:%@ for HMIHomePersonManager.UUID:%@; not storing
Timer fired, but person data is not yet available, waiting...
Timer fired, updating home persons model
Not triggering daily VIP Model Core Analytics event, last event was sent less than 1 day ago
Triggering daily VIP Model Core Analytics event
Successfully ran persons model summary task
Failed to run persons model summary task, error:%@
Submitted persons model summary task, taskID:%u
Unrecognized timer: %@
UUID:%@ HomeUUID:%@
home.person.manager
Updating with settings: %@
Settings have disabled face classification, removing home persons model
Settings have enabled face classification, updating home persons model
Storing face crop:%@ failed with error:%@
Storing face crop:%@ completed successfully
store.facecrop.operation
Storing faceprint:%@ failed with error:%@
Storing faceprint:%@ completed successfully
store.faceprint.operation
JPEGRepresentation
Frame %lu @ %@
camera.video.frame
Logi Circle 2
%#{flags}
transferPixelBuffer
VTPixelTransferSessionCreate failed. Error %d
VTSessionSetProperty failed. Error %d
VTPixelTransferSessionTransferImage failed. Error %d
JPEGDataFromPixelBuffer
Using 720p params for dewarping
Using 1080p params for dewarping
300p skipping dewarp params
Error creating directory:%@ to save video frames
%@-%06lu.%@
Error saving video frame:%@ JPEG to disk
Saved video frame:%@ to disk
Invalid crop for affine transform
Error in pixelbuffer format for affine transform
Error generating pixelbuffer for affine transform
Error applying affine transform
vision.utilities
Registering for Thermal Level Notifications
v12@?0i8
/private/var
Cannot get available space, error: %@
Footprint: %@, Average: %@, Peak: %@
OutOfMemory
Reached high water mark.
memory.sampler
Registering for PBSSystemLoad Notifications
com.apple.appletv.system-load
PBSSystemLoad is now: %llu
HMIPBSSystemLoadMonitor
PrimaryUsagePage
PrimaryUsage
LocationID
Failed to read sample buffer, error: %@
Asset reader failed, ignoring
HMIVideoAssetReader
HMIHomeKitClient HomeKit Delegate Queue
B16@?0@"HMHome"8
B16@?0@"HMResidentDevice"8
personManager is nil for homeUUID: %@
B16@?0@"HMCameraProfile"8
v16@?0@"HMHomeManager"8
Error refreshing home data: %@
No homes were located
Found home: name: %@, primary: %s, UUID: %@
homekit.client
HMMutableHomeManagerConfiguration
Unable to find class %s
/System/Library/Frameworks/HomeKit.framework/HomeKit
/System/Library/Frameworks/HomeKit.framework/Contents/MacOS/HomeKit
HMHomeManager
Not supported error
General error
Espresso error
incorrect binserializer key
small sparsity error
feature extraction error
initialization error
no saved state to revert
nominal distance not changed
batch size violation
computation kill request was issued
too few IDs to build VIP model
video error
error with projection computation
missing positional parameter
inconsistent state error
warping error
OpenGL error
invalid format
out of bounds
singular point configuration error
division by zero
LAPACK error
platform endianess not supported
hash already in use
invalid ID
invalid data type
data inconsistency error
I/O error
unknown option
invalid option
missing option
delegate error
vImage related error
memory allocation error
invalid parameter
unexpected null pointer
internal error
not implemented error
CVML_status module %d error %lld
BinSerializer
Face3D
FaceDescriptor
FaceFrontalizer
FaceWarper
Geometry2D
Geometry3D
ImageGrouping
ImageQuality
LandmarkDetector
MomentProcessor
FaceboxAligner
ImageDescriptor
ImageClassifier
ImageProcessing
VIPIdentification
ImageRegistration
SimilarityMatrix
Clustering
HumanDetector
FaceRegionMap
ObjectDetector
ObjectTracker
SRCClassifier
Kmeans
SparseCoding
FaceID
BoostedClassifier
FaceSegmenter
ImageAnalyzer
FaceAttributes
FaceprintAndAttributes
FaceQuality
Generic
ImageTools
VideoTools
ImageWarper
ThirdParty
BinSerializerProcessor
AppleNetParser
FaceProcessorCLI
ImageClassifierCLI
MPCmdlineClientCLI
ClusteringCLI
ImageProcessorCLI
PhotosProcessorCLI
CVMLEngine
CVML Module %lld
HMISignpost
%@ (%@)
input
transformed_features
classProbability
FaceRecognizabilityFilterSVM
FaceRecognizabilityFilterSVMDataScaler
FaceAestheticQualityFilterSVM
FaceAestheticQualityFilterSVMDataScaler
facequality.filter.svm
Ignoring %@
You must override %@ in a subclass
fetchAllPersonsWithCompletion
@16@?0@"HMPerson"8
v24@?0@"NSSet"8@"NSError"16
fetchPersonsWithUUIDs:%@
fetchAllPersonFaceCropsWithCompletion
@16@?0@"HMPersonFaceCrop"8
fetchFaceCropsForPersonsWithUUIDs:%@
fetchAllFaceprintsWithCompletion
@16@?0@"HMFaceprint"8
fetchFaceprintsForFaceCropsWithUUIDs:%@
fetchSettingsWithCompletion
performCloudPullWithCompletion
addFaceprints:%@
@16@?0@"HMIFaceprint"8
removeFaceprintsWithUUIDs:%@
external.person.datasource.homekit
HMFaceprint
com.apple.cvml.%@
CVML module = %@
HMIFC.ck.pu
dataRepresentation
personUUID
Person UUID
Could not initialize from decoded personUUID: %@
HMIVideoAnalyzerFrameResult
v16@?0@"HMIVideoAnalyzerEvent"8
Frame
Events
Region of Interest
B16@?0@"HMIVideoAnalyzerEvent"8
@16@?0#8
hmi://in-memory
HMIMemoryAVAsset
 Fullfilled content request: %@
Fullfilled data request: %@
tracks
Invalid parameter for windowSize: %lu, windowSize must be > 0
v16@?0@"MovingAverageEntry"8
HKD://
analyzed-video-frames
face-classifications
Success
SystemResourceUsageHigh
InsufficentTime
AnyMotion
SkippedAnalysis
Medium
High
No HMIVideoAnalyzerEventClass exists for event type %@
DidAnalyze
DidNotAnalyze
DidFailAnalysis
Canceled
Bypassed
Expired
SessionEnded
InErrorState
Predict
@"NSArray"16@?0@"HMICameraVideoFrameResult"8
@16@?0@"HMICameraVideoFrameResult"8
@16@?0@"HMIVideoAnalyzerEvent"8
Unknown
local
remote-fragment
fragment/%lu
[%@] (%lu) repetitions: %lu/%lu/%lu predictions: %lu/%lu/%lu analyzers: %lu lastEvents: %@ shouldPredict: %@
camera.video.analyzer.history
Start analysis, elapsed time since submission: %fs
v24@?0@"HMIVideoAssetWriter"8@"NSData"16
v72@?0@"HMIVideoAssetWriter"8@"NSData"16{?={?=qiIq}{?=qiIq}}24
%@, Date: %@, Time: %@, BitRate: %ld
v16@?0@"HMICameraActivityZone"8
%@ Fragment:%lu
camera.video.analyzer.request
HMICameraVideoAnalyzerScheduler
HIGH
Normal
Warning
Critical
Undefined
analysisTime: %.2f (%.2f), total: %.2f (%.2f), level: %@, memory: %@, availableSystemMemory: %@, temp: %.2f, thermalLevel: %lu, analysisFPS: %.2f, analyzers: %lu, activeAnalyzers: %lu, maxAnalyzers: %lu
v16@?0@"HMICameraVideoAnalyzerRequest"8
[%@] [A:%d]
 %@ [%lu,%lu] %@
camera.video.analyzer.scheduler
HMICameraVideoAnalyzer
Disabling transcoding because too many transcoding analyzers were created.
%@Video fragment duration: %fs is greater than expected value: %fs
%@Video fragment sequence number: %lu is not equal to expected value: %lu
%@Video fragment has no frames
HMIHomePersonManager settings: %@
HMIHomePersonManager homeUUID: %@
HMIHomePersonManager is nil, face classification disabled
XPC connection was interrupted, retrying.
Unknown delegate name: %@
%@Failed to start reading of the asset: %@
v32@?0@"AVAsset"8@"HMICameraVideoResourceAttributes"16@"NSError"24
Failed to encode fragment.
Transcoded, bytes: %lu (%f), original bytes %lu 
End analysis: didAnalyze, significant events detected: %@
v16@?0@"HMICameraVideoFrameResult"8
End analysis, time spent: %fs, elapsed time since submission: %fs, predicted: %@
Stopping analysis due to high system resource usage
Stopping analysis due to cancelling
Stopping analysis due to entering full bypass mode
Stopping analysis due to analysis time past the maximum fragment analysis time: %fs
Finished early @ %@
Frame selector produced 0 frames
Analyzed frame:%lu, Events:%@
Failed saving activity zone data into file
[%@] <%lu> Object coordinate:%@
@"NSSet"16@?0@"HMIVideoAnalyzerEvent"8
outcome
cameraManufacturer
cameraModel
faceRecognitionEnabled
faceDetected
transcoded
activeAnalyzers
systemResourceUsageLevel
thermalLevel
resultCode
framesAnalyzedFragment
analysisFPS
timeSinceFragmentWasSubmitted
error
underlyingError
timeToAnalyzeFragment
personScore
personConfidence
petScore
petConfidence
vehicleScore
vehicleConfidence
Uploading analytics event %@
com.apple.HomeAI.VideoAnalyzerStats
@"NSMutableDictionary"8@?0
com.apple.HomeAI
homeai: %@
camera.video.analyzer
HMIMutableFloatArray
Face below face quality thresholds: SVM recognizability = %lf, Yaw = %lf, discarding
Face below ANFR face quality threshold: ANFR confidence = %lf, discarding
personsModelPredictions is empty
Face removed from unknown & uncertain bucket, hasFaceMask is YES
Face removed from unknown and uncertian bucket, SSD confidence = %f, entropy of laplacian = %f, Face yaw = %f, box size: %f
Added to unknown bucket yaw: %@
Added to uncertain bucket yaw: %@
@16@?0@"HMIPersonsModelPrediction"8
Face recognition set is empty
face.classifier.vip
Initializing HMICameraVideoFrameAnalyzerFactory
Resetting VideoFrameAnalyzer
Warm starting model...
warm_start_model
Failed to create pixel buffer when warm starting model
Failed to warm start model: %@
Warm start of model took: %f
Model is not bundled into framework
Analyze Frame
Failed handleMotionDetection with error %@
@24@?0#8@"NSString"16
camera.video.frame.analyzer.factory
detections
bounds
label
confidence
randomUniform
value
{CGAffineTransform=dddddd}
name
type
probability
attributes
@16@?0@8
B16@?0@"HMICIFilterAttribute"8
HMIRotate
CIAffineTransform
inputAngle
inputTransform
v16@?0@"HMICIFilterAttribute"8
filters
v16@?0@8
com.apple.HomeAIDESPlugin
DES record saving is not permitted.
There isn't enough available disk space (%ld MB) to save DES records.
Couldn't determine the amount of available space disk space, continuing.
Saving DES Record, recordInfo: %@, data: %@
Saved DES Record: %@, error: %@
v24@?0@"NSUUID"8@"NSError"16
createInputTensorWithError %@
HMIDESDataSet
bias_b
bias_g
bias_r
is_network_bgr
scale
Label:%d Confidence:%f X:%f Y:%f Width:%f Height:%f Yaw:%@}
q24@?0@"HMIObjectDetection"8@"HMIObjectDetection"16
@16@?0@"HMIObjectDetection"8
<%@: %p> timeStamp: %@, detections: [%@], regionOfInterest: %@
<%@: %p> %@
HMIVideoFrameAnalyzer
Analyzing %@
[Fragment:%lu] Failed to generate poster frame for: %lu
camera.video.poster.frame.generator
Precision
Recall
True Positive
False Negative
False Positive
Source
Fragments
precision
recall
threshold
class
opacity
v16@?0#8
/System/Library/CoreServices/SystemVersion.plist
HomeAIBundleVersion
Debug
Truth
image_id
B16@?0@"NSDictionary"8
Cannot find ground truth for %@
classification_classes
@16@?0@"NSString"8
@24@?0@"NSString"8@"HMIVideoAnalyzerMutableReportSession"16
v24@?0@"NSString"8@"HMIVideoAnalyzerMutableReportSession"16
$schema
https://vega.github.io/schema/vega-lite/v4.json
description
PR Curves
width
container
height
config
style
align
center
baseline
layer
mark
line
encoding
field
quantitative
domain
nominal
legend
HMIVideoAnalyzerMutableReport
HMIFC.ck.u
HMIFC.ck.dr
HMIFC.ck.dc
HMIFC.ck.fbb
PVFC:PVFC
PVFC_FB
PVFC_CB
Ignoring error detecting face in Photos face crop, error: %@
Data Representation
Date Created
Face Bounding Box
Could not create image source
No meta data exists on image
Could not initialize from decoded UUID: %@ dataRepresentation: %@ dateCreated: %@
HMIVideoRetimer
mediaType == kCMMediaType_Video
Dropping frame, lastSamplePTS > nextSamplePTS.
allocator<T>::allocate(size_t n) 'n' exceeds maximum supported size
Descriptor count = 
Descriptor length = 
 bytes
 = [
identifier
HMICamera
Identifier
Name
Manufacturer
Model
Descriptor vectors nil
Error
UpdatePersonsModelTask
RemovePersonsModelTask
HomePersonClusteringTask
TuriTrialUpdateTask
FaceMisclassificationTask
PersonsModelsSummaryTask
FeedbackTask
EmptyTask
taskType
faceCrop
sourceUUID
homeUUID
isExternal
cameraProfileUUID
clipUUID
TaskID: %u running...
options is empty/nil, defaulting to Home persons clustering task
Unknown task type: %@
v32@?0@"HMITask"8Q16^B24
HMITaskHomeUUIDKey is nil
HMIUpdatePersonsModelTaskIsExternal is nil
HMIPersonsModelTaskSourceUUID is nil
Creating HMPhotosPersonManager for homeUUID:%@ sourceUUID:%@
Failed to get HMPhotosPersonManager
Creating HMHomePersonManager for homeUUID:%@
Failed to get HMHomePersonManager
Failed to initialize data source
Disk-based Home Person Data Source not supported
Current device is not primary resident, skipping clustering
HomeUUID is nil
task.service.server
Initializing HMITaskServiceServer
HMITaskService not available on this platform.
task.service
Error fetching persons:%@
fetch.persons.operation
%@ - %f
sparse
@"HMICameraVideoFrame"16@?0@"HMICameraVideoFrameSelectorFrameScore"8
q24@?0@"HMICameraVideoFrame"8@"HMICameraVideoFrame"16
q24@?0@"HMICameraVideoFrameSelectorFrameScore"8@"HMICameraVideoFrameSelectorFrameScore"16
v16@?0@"HMICameraVideoFrame"8
camera.video.frame.selector
camera.video.frame.sampler
Skipping sentinel faceprint in existingAtCurrentVersion
v16@?0@"HMIFaceprint"8
Skipping sentinel faceprint in createdAtCurrentVersion
Vision
Faceprint Version: %ld.%ld
Warm starting faceprint model...
warm_start_faceprint_model
Failed to create pixel buffer when warm starting faceprint model
Failed to warm start faceprint model: %@
Warm start of faceprint model took: %f
CreateFaceprint
Cropping face %@ from pixel buffer with dimensions: %.1f x %.1f roll: %.02f degrees
Error pixel buffer type conversion.
Error in rotating the face.
Face was rotated by:%.02f degrees
Cropping face %@ from face crop with dimensions %.1f x %.1f
B16@?0@"HMIFaceprint"8
%lu faceprint(s) exist for face crop:%@ but are not the current version
Using existing faceprint for face crop:%@
Faceprinting face crop:%@
Skipping crop, encountered error faceprinting: %@
Face crop has a facemask, creating sentinel faceprint
Vision run-time version: %d.%02d.%02d (%d)
faceprinter
face.detector.vision
HMIFP.ck.u
HMIFP.ck.d
HMIFP.ck.mu
HMIFP.ck.fcu
modelUUID
faceCropUUID
Data
Model UUID
Face Crop UUID
Could not initialize from decoded UUID: %@ data: %@ modelUUID: %@ faceCropUUID: %@
<%@ %@>
HMPMS.fce
Face Classification Enabled
com.apple.homeai.model.loader
/var/mobile/Library/Caches/com.apple.HomeAI/model_dir
[Model loading] failed to create asset dir: %@ error: %@
[Model loading] unpackaging
[Model loading] failed to remove dir
[Model loading] failed to remove dir %@ err %@
[Model loading] failed to untar model
[Model loading] failed to untar model asset into %@ err %@
HMIModelLoader
clip
faceCrops
authTag
internal
wrappedKey
1.2.840.113635.100.6.27.38
HMIFeedback Queue
HMIFeedback
Trusting host: %@
Force Certificate Pinning
HFFeedbackService
Error setting trust policies: %lu
Invalid certificate: %@
Downloading Clip
Cannot find camera profile.
Cannot find home for camera profile.
Fetched Clip videoAssetContext: %@, error: %@
Cannot download clip asset.
Check network connectivity.
v24@?0@"HMCameraClipVideoAssetContext"8@"NSError"16
Fetching Clip, progress %lu%%
v16@?0Q8
v16@?0@"HMCameraClip"8
Cannot download clip for camera profile.
Ensure the clip belongs to the camera profile.
Face crops are not available.
Fetching Face Crops
@16@?0@"HMCameraClipSignificantEvent"8
Fetched Person UUIDs: %@
Fetched Face Crops: %@, error: %@
v24@?0@"HMCameraClip"8@"NSError"16
Fetching Clip
Clip doesn't have a video track.
%@%@
hkcvml-dev.apple.com
hkcvml.apple.com
https://%@/v2/clip-uuid/
feedback
%@.%@
Uploading payload data: %@, to URL %@
json
application/json
Content-Type
v32@?0@"NSData"8@"NSURLResponse"16@"NSError"24
Invalid key size.
Cannot generate initialization vector.
Failed to encrypt data.
Failed to encrypt face crop data.
Submitting clipUUID: %@, cameraProfileUUID: %@
Stripped Audio %@, error: %@
v24@?0@"NSURL"8@"NSError"16
Downloaded %@, error: %@
Failed to fetch pre-signed URL, error: %@
Failed to request service result, error: %@
Failed to decode service result, error: %@
Service result: %@
Deleting Temporary File %@
Deleted Temporary File %@, error: %@
v16@?0@"NSURL"8
HMIFeedbackSubmitClipOperation
HMCameraClipFetchVideoAssetContextOperation
HMIFC.pu
HMIFC.su
HMIFC.seu
HMIFC.fc
HMIFC.fp
HMIFC.c
HMIFC.f
Known
Uncertain
Source UUID
Session Entity UUID
Confidence
Familiarity
FaceCrop UUID
Faceprint UUID
Failed to generate persons model summmary, error:%@
persons.models.summary.task
Failed to fetch face crops with error: %@
@16@?0@"HMIPersonFaceCrop"8
Failed to fetch faceprints with error: %@
Error faceprinting face crops:%@
Person (%@) has no faceprints -- nothing to remove
Nearest face crop to be removed: %@
Failed to remove face crop with error: %@
Successfully removed face crop (%@) via user indicated misclassification
face.misclassification.task
default
metallib
Unable to create filter %@, error: %@.
hmi_thresholdFilter
hmi_addFilter
hmi_divideFilter
hmi_multiplyFilter
hmi_densityMapFilter
Failed to remove persons model, error:%@
Successfully removed persons model
SourceUUID:%@ HomeUUID:%@
remove.persons.model.task
%.4f
%.2f[%c]
HMIErrorDomain
Unexpected error
Failed to analyze
Failed to read fragment
Failed to verify fragment
Failed to generate poster frame
Failed to transcode fragment
Video analyzer in error state
Model failed to load
Fragment is invalid
No pixel buffer
Unknown error code %ld
HMIErrorCodeUnexpectedError
HMIErrorCodeFailedToAnalyze
HMIErrorCodeFailedToReadFragment
HMIErrorCodeFailedToVerifyFragment
HMIErrorCodeFailedToGeneratePosterFrame
HMIErrorcodeFailedToTranscodeFragment
HMIErrorCodeAnalyzerInErrorState
HMIErrorCodeModelFailedToLoad
HMIErrorCodeFragmentIsInvalid
HMIPrivateErrorCodeNilPixelBuffer
HMIPrivateErrorCodeEmptyURL
HMIPrivateErrorCodeAVAssetReaderInitializationFailed
HMIPrivateErrorCodeFailedToLoadProperty
HMIPrivateErrorCodeAssetLoadCancelled
HMIPrivateErrorCodeReadingStartFailed
HMIPrivateErrorCodeNoTrackOutput
HMIPrivateErrorCodeSampleBufferUnavailable
HMIPrivateErrorCodeNoVideoTrackFound
HMIPrivateErrorCodeMultipleVideoTracksFound
HMIPrivateErrorInvalidPresentationTime
HMIPrivateErrorAVAssetReaderNotStarted
HMIPrivateErrorCodeSequentialIntegrityViolated
HMIPrivateErrorCodeAnalysisServiceNoConfiguration
HMIPrivateErrorCodeLoadingCoreMLModelFailed
HMIPrivateErrorCodeCoreMLPredictionFailed
HMIPrivateErrorCodeCoreMLOutputIncorrect
HMIPrivateErrorCodeCropAndResizeFailed
HMIPrivateErrorCodePosterFrameGenerationFailed
HMIPrivateErrorCodeCodecNotAvailable
HMIPrivateErrorCodeEncodingFailed
HMIPrivateErrorCodeInvalidVideoFrameFormatToSave
HMIPrivateErrorCodeScalerError
HMIPrivateErrorCodeFaceprintingFailed
HMIPrivateErrorCodeUpdatePersonsModelTaskFailed
HMIPrivateErrorCodeExternalPersonSourceDiskError
HMIPrivateErrorCodeLoadPersonsModelsFailed
HMIPrivateErrorCodeUpdatePersonsModelFailed
HMIPrivateErrorCodeRemovePersonsModelFailed
HMIPrivateErrorCodePersonsModelPredictionFailed
HMIPrivateErrorCodeNilDataSource
HMIPrivateErrorCodeUnknownTask
HMIPrivateErrorCodeHomePersonClusteringTaskFailed
HMIPrivateErrorCodeRemovePersonsModelTaskFailed
HMIPrivateErrorCodeFaceMisclassificationTaskFailed
ERROR_%ld
%@: %@
Provided sequenceNumbers: %@, don't match fragment's sequenceNumbers: %@
v12@?0I8
B28@?0I8@"NSData"12@"NSData"20
video/mp4
Failed to read fragment data, err: %d
first
second
HMIVideoFragment
[%@]
Initialization Segment Data
Separable Segment Data
Sequence Numbers
Time Range
HMISessionEntityManager
WARNING: received multiple motion detections -- using first
WARNING: no motion detection to use for face tracking
Face event doesn't have a recognition, ignoring
B16@?0@"NSUUID"8
v24@?0@"NSUUID"8^B16
Transition Pr(detectionIdx: %lu, previousIndex: %lu, sessionUUID:%@) = %lf
Face embedding distance to cluster (detectionIdx: %lu, sessionUUID:%@) = %lf
Face embedding distance to previous face (detectionIdx: %lu, sessionUUID:%@) = %lf
Dynamic threshold (detectionIdx: %lu, sessionUUID:%@) = %lf
Adding new sessionEntityUUID: %@
@16@?0@"HMIFaceClassification"8
v32@?0@"HMIVideoAnalyzerEventPerson"8Q16^B24
HMITrialUpdateTask
v16@?0@"HMIPoint"8
HMIVideoAnnotator
HMIVideoAnalyzerResultFilter
HMIVideoAnalyzerResultActivityZoneFilter
HMIPSUP.ck.p
HMIPSUP.ck.s
Could not initialize from decoded sourceUUID: %@
HMIP.ck.u
HMIP.ck.n
HMIP.ck.pl
personLinks
@"HMIPersonSourceUUIDPair"16@?0@"HMPersonLink"8
Could not initialize from decoded UUID: %@
v16@?0@"<NSObject><NSCopying><NSSecureCoding>"8
HMIPersonsModel
data:;base64,%@
@24@?0@8@16
%.6f
data:;base64,
Invalid JSONObject class name: %@
Invalid JSONObject: %@
Cannot parse JSON data: %@
yyyy-MM-dd'T'HH:mm:ss.SSS'Z'
Error creating directory:%@ to save face classifications
faceprintUUID
%@-%03lu-%03u-%@-%@-%@
known
unknown
Error saving metadata to disk for face classification:%@
Error saving face crop image to disk for face classification:%@
Saved face classification:%@ to disk
face.utilities
frameDimensions.width > 0 && frameDimensions.height > 0
v32@?0@"HMISparseOpticalFlowFeatureVector"8Q16^B24
HMIPersonBlob (%@): {center = (%f, %f),  size = (%f, %f), source = %@}
detection
projection
q24@?0@"HMIFaceTrackerMatch"8@"HMIFaceTrackerMatch"16
q24@?0@"HMIFaceTrackerMotionRecord"8@"HMIFaceTrackerMotionRecord"16
HMIFaceTracker
Handling motion detection update for sessionPTS: %f
@"NSArray"16@?0@"HMISparseOpticalFlowMotionDetection"8
newPersonWithFaceEvents.count > 0 || personWithoutFaceEvents.count > 0 || personWithFilteredFaceEvents.count > 0
@"HMIPersonBlob"16@?0@"HMIVideoAnalyzerEventPerson"8
Initial faces consumed
Timeout reached -- resetting face tracker
@"HMIPersonBlob"16@?0@"HMIPersonBlob"8
v32@?0@"HMIPersonBlob"8Q16^B24
v16@?0@"HMIFaceTrackerMatch"8
v32@?0@"NSNumber"8@"NSNumber"16^B24
B16@?0@"HMIPersonBlob"8
v24@?0Q8^B16
v32@?0@"HMIFaceTrackerMatch"8Q16^B24
v32@?0@"NSUUID"8@"NSNumber"16^B24
v32@?0@"NSNumber"8Q16^B24
Failed to initialize HMICameraVideoResourceAttributes from fragment data, err: %d
camera.video.resource.attributes
creationDate
duration
firstFragmentSequenceNumber
fragmentCount
nominalFrameRate
timeRange
formatDescriptions
[Fragment:%lu] Creating asset with url: %@
[Fragment:%lu] Failed to initialize AVAssetReader
[Fragment:%lu] Start %0.2f, Duration %0.2f, FPS %0.2f
[Fragment:%lu] No fragment URL found on loading request %@
[Fragment:%lu] No fragment data found for URL %@
[Fragment:%lu] Finished handling load request
camera.video.asset.reader
com.apple.videotoolbox.videoencoder.h264.rtvc
Error in VTCompressionSessionEncodeFrameWithOutputHandler %d
Frame dropped.
v24@?0i8I12^{opaqueCMSampleBuffer=}16
Error Calling VTCompressionSessionEncodeFrameWithOutputHandler, error: %d
Attempting recovery.
Recovered successfuly.
Failed to encode sample after resetting encoder session.
Failed to recover, error %@
Video encoder is not running, ignoring %@
Failed to encode and recover from failure.
HMIVideoEncoder
Cannot set property, %d
addFaceCrops:%@
@16@?0@"HMIFaceCrop"8
addPersonFaceCrops:%@
addPersons:%@
@16@?0@"HMIPerson"8
fetchAllUnassociatedFaceCropsWithCompletion
@16@?0@"HMFaceCrop"8
removeFaceCropsWithUUIDs:%@
removePersonsWithUUIDs:%@
associateFaceCropsWithUUIDs:%@ toPersonWithUUID:%@
home.person.datasource.homekit
HMFaceCrop
HMPersonFaceCrop
HMMutablePerson
HMPMS.ifple
HMPMS.sfce
Importing From Photo Library Enabled
Sharing Face Classifications Enabled
detectionScore
faceFilteredState
homeFamiliarity
v16@?0@"HMIFaceClassification"8
q24@?0@"NSNumber"8@"NSNumber"16
externalFamiliarity
sessionEntityAssignment
com.apple.HomeAI.FaceEvent
faceprintsClustered
clusters
personsCreated
faceprintsAssociated
faceprintingDuration
clusteringDuration
totalDuration
errorCode
errorDescription
com.apple.HomeAI.FaceClustering
v24@?0@"NSString"8@"NSNumber"16
v16@?0@"HMIPersonsModelSummary"8
@avg.self
externalLibraries
homeIdentities
averageExternalIdentities
averageHomeFaceCrops
averageExternalFaceCrops
homeToExternal
externalToExternal
com.apple.HomeAI.PersonsModels
@"NSDictionary"8@?0
B16@?0@"NSNumber"8
HMIAnalytics
personsmodels
home
external
user_defined_person_links.bin
v32@?0@"NSUUID"8@"HMIPersonsModel"16^B24
v16@?0@"HMIPerson"8
Writing updated userDefinedPersonLinksByHome[%@] to disk
Error adding faceprints to model for personUUID: %@
v32@?0@"HMIPerson"8@"NSSet"16^B24
Stale Home VNPersonsModel with homeUUID: %@ sourceUUID: %@ detected, attempting to remove...
Failed to remove stale Home VNPersonsModel with homeUUID: %@ sourceUUID: %@, error getting model storage path
Stale model path no longer on disk, proceeding with building persons model...
Failed to remove stale Home VNPersonsModel with homeUUID: %@ sourceUUID: %@
Failed to persist VNPersonsModel for homeUUID: %@ sourceUUID: %@, error getting model storage path
Persisted VNPersonsModel for homeUUID: %@ sourceUUID: %@
Failed to persist VNPersonsModel for homeUUID: %@ sourceUUID: %@, path: %@
Did not remove VNPersonsModel for homeUUID: %@ sourceUUID: %@, no model found
Failed to remove VNPersonsModel for homeUUID: %@ sourceUUID: %@, error getting model storage path
Error removing user defined person links file: %@
Removed userDefinedPersonLinksByHome for homeUUID: %@
Removed VNPersonsModel for homeUUID: %@ sourceUUID:%@
Failed to remove VNPersonsModel for homeUUID: %@ sourceUUID: %@, path: %@
Home persons model not found for homeUUID: %@
Failed to predict using VNPersonsModel for home persons model
Unable to build equivalency map for homeUUID: %@, error: %@
Unable to get person model for homeUUID: %@ (self.personsModelsByHome is %@ nil)
Failed to predict using VNPersonsModel for homeUUID: %@ sourceUUID: %@
Failure to lookup equivalency cell for %@ (equivalencyCellForHome is%@ nil)
 not
v24@?0@"HMIPersonSourceUUIDPair"8^B16
v32@?0@"NSSet"8@"HMIPersonsModelPrediction"16^B24
Resetting HMIPersonsModelManager
%@.bin
Directory to load/store persons models (%@) does not exist
Failed to enumerate persons models at path (%@)
\A[A-F0-9]{8}-[A-F0-9]{4}-[A-F0-9]{4}-[A-F0-9]{4}-[A-F0-9]{12}\.bin\Z
B16@?0@"NSURL"8
Persons Model Storage Path:%@
Failed to enumerate homes at path: %@
Failed to parse Home UUID from path: %@
Failed to load External HMIPersonsModel at path: %@, error: %@
Loaded External HMIPersonsModel for homeUUID: %@ sourceUUID: %@
Directory to load/store home persons model (%@) contains multiples files: %@ there can only be one
No home model found for homeUUID: %@
Failed to load Home HMIPersonsModel at path: %@, error: %@
Loaded Home HMIPersonsModel for homeUUID: %@ sourceUUID: %@
Loaded %lu user defined equivalencies found for home: %@
No user defined equivalencies found for home: %@ (reason: %@)
Invalid file path in load model attempt: %@
Refusing to load %@ VNPersonsModel at path: %@
Failed to load VNPersonsModel at path: %@
@24@?0@"NSUUID"8@"HMIPersonsModel"16
v32@?0@"HMIPersonSourceUUIDPair"8@"NSSet"16^B24
persons.model.manager
HMIDESTrainer
is_training
checkpoint
Loss/total_loss
trainingCallback %lu, loss: %f
B32@?0Q8@"<ETDataProvider>"16@"<ETTaskContext>"24
Training was skipped because %@ is YES.
Training Started
Training Finished
v16@?0@"HMIDESLayerParameters"8
Unknown reason.
Skipped
{code: %@, message: "%@"}
{code: %@}
v24@?0@"HMIExternalPersonManagerSettings"8@"NSError"16
Timer fired, updating external persons model
external.person.manager
Settings have disabled face classification, removing external persons model
Settings have enabled face classification, updating external persons model
Failed to remove persons model, error:%@, retrying...
Submitted persons model remove task, taskID:%u, retryOnError:%@
remove.persons.model.operation
HMIVideoAnalyzerScheduler
v32@?0@"HMIVideoAnalyzer"8Q16^B24
Scheduler state: 
usage: %@
, mem: %llu MB
, max: %llu MB
, thermalLevel: %lu
v16@?0@"HMIVideoAnalyzer"8
SignificantActivity.mlmodelc
HOMEAI_SIGNIFICANT_ACTIVITY_DETECTOR
v16@?0@"<TRINamespaceUpdateProtocol>"8
Submitted task returned error: %@
compiledModel
personThresholdHigh
personThresholdMedium
petThresholdHigh
petThresholdMedium
vehicleThresholdHigh
vehicleThresholdMedium
faceThreshold
HMITuriTrialManager
Clustering successful
Clustering error
greedy_clustering
descriptorA.length == descriptorB.length
cluster.objects.count > 0
faceObservations.count > 0
faceObservations.firstObject.faceprint != nullptr
faceprintLength == faceObservation.faceprint.lengthInBytes / sizeof(float)
HMIVideoAnalyzerLegacy
Analyzing fragment: %@, configuration: %@
Motion was assumed because AnyMotion was turned on.
Fragment analysis was skipped.
v16@?0@"HMIVideoAnalyzerFrameResult"8
@16@?0@"HMICameraVideoPosterFrame"8
HMIVideoFrameSampler
HMIVideoFrameIntervalSampler
Sample buffer has an invalid PTS.
HMIVideoPackageAnalyzer
CILanczosScaleTransform
Detecting Package
Unable to resize frame to target size for package detection: %@
Candidate detector returned nil box list, skipping the rest of analysis
@"HMIVideoAnalyzerEventPackage"16@?0@"NSValue"8
laplacian
detectionConfidence
boxSize
HMIFaceQualityEntropyOfLaplacian
HMIFaceQualityDistanceToJunkCluster
d24@?0@"NSString"8d16
face.qualityscore.consolidated
HMIVideoAnalyzerFragmentResult
@"HMIVideoAnalyzerFrameResult"16@?0@"HMIVideoAnalyzerFrameResult"8
@"HMIVideoFrame"16@?0@"HMIVideoFrame"8
@"NSArray"16@?0@"HMIVideoAnalyzerFrameResult"8
Max Confidence Events
Frame Results
Thumbnails
Fragment
Configuration
HMICVFR.f
HMICVFR.roi
HMICVFR.ae
HMICVAR.rc
HMICVAR.fr
HMICVAR.cd
HMICVAR.d
HMICVAR.pf
HMICVAR.ttaf
HMICVAR.tsfws
HMICVAR.lsn
HMICVAR.vf
HMICVAR.af
HMICVAC.pfgi
HMICVAC.pfh
HMICVAC.mfad
HMICVAC.mfd
HMICVAC.st
HMICVAC.smisn
HMICVAC.tf
HMICVAC.fce
HMICVAC.csd
HMICVAC.c
HMICVAC.hu
HMICVASE.e
HMICVASE.vf
HMICVASE.as
HMICVF.et
HMICVF.sn
HMICVF.mf
HMICVF.az
HMICVF.d
HMICVPF.d
HMICVPF.to
HMICVPF.w
HMICVPF.h
HMIOD.l
HMIOD.c
HMIOD.b.x
HMIOD.b.y
HMIOD.b.w
HMIOD.b.h
HMIOD.y.a
HMIOD.r.o
HMIP.x
HMIP.y
HMIAZ.p
HMIAZ.i
HMICameraVideoFrame does not support NSSecureCoding in the simulator
 isInclusion:%d 
class-label
coordinates
overlap
inclusion
activityZone
%@-%@-%@-%@.json
%@/%@
Error creating activity zone result directory: %@
%@/activityzone-%@
Activity zone file path:%@
Error converting activity zone results to JSON: %@
Error writing activity zone results JSON to file: %@
motionScore %f
B16@?0@"HMICameraActivityZone"8
Activity zones coordinates:%@
Inclusion zone:%@ intersecting with:(%@) Object coordinate %@ insetThreshold %f
Exclusion zone:%@ intersecting with:(%@) Object coordinate %@ insetThreshold %f
Events after activity zone filtering:(%@) Object coordinate %@ insetPercentage %f
zoneType
filteringLevel
numDetectedObjects
com.apple.HomeAI.ActivityZone
noFiltering
resize_0
resize_10
resize_20
exclusion
resize_26
resize_36
camera.activity.zone
HMIVideoAnalyzerConfiguration
Thumbnail Interval
Thumbnail Height
Max Fragment Duration
Max Fragment Analysis Duration
Transcode
Min Frame Quality
Min Frame Scale
Camera
Recognize Faces
minFrameQuality > 0 && minFrameQuality <= 1
minFrameScale > 0 && minFrameScale <= 1
Event Triggers
Activity Zone Count
HMIVideoAnalyzerDynamicConfiguration
Unsupported aspect ratio: (%d, %d)
configuration
maxAnalysisFPS
preferenceOverrides
videoFragment
videoFrame
videoAnalyzerIdentifier
delegate
didAnalyzeFragment
didNotAnalyzeFragment
didFindSignificantEvent
result
significantEvents
HMIAnalysisServiceTypeUnknown
HMIAnalysisServiceTypeLocal
HMIAnalysisServiceTypeRemoteFragment
HMIAnalysisServiceTypeUndefined
v24@?0@"HMICameraVideoFragment"8@"HMICameraVideoAnalyzerResult"16
v24@?0@"HMICameraVideoFragment"8@"NSError"16
HMIAnalysisService
Remote analysis not supported in the simulator
Asset data is ignored if the video fragment is passed through the properties dictionary
Analyzer configuration property key (%@) is missing
v24@?0@"HMICameraVideoAnalyzerSignificantEvent"8@"HMICameraVideoFragment"16
camera.video.analysis.service
HMIVAEP.f
HMIVAEP.ibbe
Is Bounding Box & Confidence Estimated
Is Object Moving
Face
%@ %@
HMIVideoAnalyzerEventPerson
com.apple.homeai.pfl
PFLPrivatize Failed
Privatized Training Result
Encrypted Training Result
PFLPrivatizeCustomNorm
/System/Library/PrivateFrameworks/PrivateFederatedLearning.framework/PrivateFederatedLearning
/System/Library/PrivateFrameworks/PrivateFederatedLearning.framework/Contents/MacOS/PrivateFederatedLearning
com.apple.homed
syntheticEvents
personDetected
petDetected
vehicleDetected
skipSequentialMediaIntegrityCheck
analysisQOS
analysisServiceType
motionDetector
frameSelectorSampleRateMultiplier
frameSelectorTargetInterval
processingDevice
confidenceThresholdPersonHigh
confidenceThresholdPetHigh
confidenceThresholdVehicleHigh
confidenceThresholdFaceHigh
confidenceThresholdPersonMedium
confidenceThresholdPetMedium
confidenceThresholdVehicleMedium
confidenceThresholdFaceMedium
modelTimeout
uploadVideoAnalysisEvent
saveVideoFramesToDisk
assetReaderMaximizePowerEfficiency
assetReaderForceBGRA
analysisEnableTranscodeFragment
realTimeEncoding
maxConcurrentAnalysisRequests
espressoLowPriority
maxConcurrentAnalyzers
opticalFlowLowPriority
opticalFlowBackgroundProcessing
enableAnalyzerHistory
saveDESRecords
DESSkipTraining
DESSkipTrainingScalar
DESSkipPrivatize
personsModelStoragePath
personDataSourceDiskStoragePath
personsModelClassificationThresholdKnown
personsModelClassificationThresholdUnknown
saveFaceCropsToDisk
enableSavingActivityZoneInfo
annotateVideo
showROI
posterFrameHeight
useDevelopmentFeedbackService
enableAnyMotionAnalysis
useStreaming
user-interactive
user-initiated
unspecified
utility
background
Preference %@ is now %@, previously was %@
Only NSNumber and NSString properties are supported.
HMIPreference
home.person.datasource.disk
Error fetching face crops for person:%@, error:%@
fetch.personfacecrops.operation
CILabDeltaE
inputImage
inputImage2
Already started listening for the notification
HMINotifydObserver
HMIVideoAnalyzerEventVehicle
{CGRect={CGPoint=dd}{CGSize=dd}}44@?0i8{CGRect={CGPoint=dd}{CGSize=dd}}12
Unable to generate integralImage: %@
Unable to generate densityMap
Unable to generate floodImage. %@
Elapsed time for box extraction  %1.5f
com.apple.HomeAI.%@%@%@.%tu
Faceprints
Clusters
Persons
Associated
Faceprinting Duration
Clustering Duration
Total Duration
HMIHomePersonClusteringTask
Error performing cloud pull:%@
Fetching persons
@"NSUUID"16@?0@"HMIPerson"8
Fetched %lu persons (%lu unnamed)
Exiting early because task was canceled.
Skipping named person
B16@?0@"HMIPersonSourceUUIDPair"8
Fetching face crops for person: %@
Deleting unnamed person %@ (0 face crops)
Deleting unnamed person %@ (age = %f seconds)
Error fetching face crops:%@
@"NSUUID"16@?0@"HMIFaceCrop"8
Error fetching faceprints:%@
Storing %lu newly created faceprints
Error saving new faceprints:%@
Removing %lu faceprints from old versions
Error removing faceprints from old versions:%@
Number of faceprints to cluster: %lu
Clustering error:%@
Number of clusters: %lu
Cluster of size %lu beneath threshold of %d
Face prediction error:%@
Error adding new persons:%@
@16@?0@"NSNumber"8
Error associating face crops with person (%@): %@
v32@?0@"VNCluster"8@"NSUUID"16^B24
Error on dispatch_group_wait (associateFaceCrops)
Finished calls to associateFaceCropsWithUUIDs
Error associating face crops for %lu person%@: (
 ...
v16@?0@"HMIPersonFaceCrop"8
Error removing person with UUID:%@, error:%@
Succesfully removed person %@
Error fetching persons, error:%@
Fetched %lu persons
Error fetching facecrops for person:%@, error:%@
Fetched %lu face crops for person: %@
Ignoring error fetching faceprints for person:%@, error:%@
Error faceprinting face crops for person:%@, error:%@
Storing newly created faceprints: %@
Removing existing faceprints at other versions: %@
Failed to generate persons model, error:%@
Invalid configuration: isExternalLibrary is NO but self.dataSource does not conform to HMIHomePersonManagerDataSource protocol!
Successfully updated persons model
B16@?0@"HMIPerson"8
WARNING: Model has %lu named persons -- limit supported is %d
Error fetching faces to subsample for %@: %@
Fetched 0 training faces for %@, this would remove all face crops! Skipping face crop removal.
Expected subsampling to leave no more than %d, but got %lu faces selected. Enforcing limit.
Subsampling will retain %lu from a total of %lu faces for %@
@"NSUUID"16@?0@"VNFaceObservation"8
@"NSUUID"16@?0@"NSUUID"8
Deleting a total of %lu face crops after subsampling
Selected %lu persons for subsampling faces but did not choose any face crops to delete!
update.persons.model.task
Error fetching faceprints for face crop UUIDs:%@, error:%@
fetch.faceprints.operation
Storing faceprints:%@ failed with error:%@
Storing faceprints:%@ completed successfully
store.faceprints.operation
Removing faceprints:%@ failed with error:%@
Removing faceprints:%@ completed successfully
remove.faceprints.operation
Removing faceCrops:%@ failed with error:%@
Removing faceCrops:%@ completed successfully
remove.facecrops.operation
HmiAverageImage failed to render CIImage into CGImage.
v32@?0@"NSNumber"8@16^B24
HMICameraVideoFrameAnalyzerSignificantActivity
Face Classification is enabled, but homeUUID is nil, skipping face recognition
AnalyzerEvents: [%lu/%lu] %@
v24@?0@"HMIVideoAnalyzerEvent"8^B16
none
@"NSString"16@?0@"NSObject"8
high
medium
Unexpected class event %@
q24@?0@"HMIVideoAnalyzerEvent"8@"HMIVideoAnalyzerEvent"16
v32@?0@"HMIVideoAnalyzerEvent"8Q16^B24
Face classification skipped for stationary or low confidence face: %@, removing face event from person event
ClassifyFaceEvent
Face classification failed for face: %@, error: %@, removing face event from person event
Face: %@ didn't produce any classifications
camera.video.frame.analyzer.significant.activity
HMIDESBT.u
HMIDESBackgroundTask
Unable to archive task %@: %@
HMIVideoFrame
scale > 0 && scale <= 1
quality > 0 && quality <= 1
HMIVideoFrame failed to create compressed representation.
]1337;File=inline=1;preserveAspectRatio=1:%@
Data (JPEG)
CVPixelBuffer
Presentation Time Stamp
Size
Store
HMIVideoFrame does not support NSSecureCoding in the simulator.
[Model loading] Error opening encrypted file
[Model loading] Error reading mmaped encrypted file
[Model loading] Error reading data from compressed file
[Model loading] returning unsuccessfully post decompression due to invalid destination size
[Model loading] Error in out file.
/scratch.data
failed stat %s
[Model loading] failed to unpackage resources
[Model loading] unpackaged resources version
HMICompressionHelper
/Library/Spotlight/Backup/temp_%lu.dat
Error in opening temporary file.
preallocated temporary file failed. %d
Serious error in writing temporary file. %d
HMIVideoBuffer
HMIVideoAnalyzerEventPet
Bounding Box
P(%@|[%@])=%@
HMIVideoAnalyzerEvent
Events file "%@" does not exist.
Cannot read events from file "%@", error: %@
Package
Person
Vehicle
Motion
Cannot load events file, exception: %@
Event
event
motion
person
vehicle
face
HMIVideoDecoder
Cannot reorder frames.
Video decoder is not running, ignoring %@
Sample buffer has no samples, skipping.
Invalid DTS, expected > %@, got %@, skipping.
Format description is missing.
Cannot create decoder.
Cannot decode frame, err: %d.
Frame decode error %d
HMIVideoAssetWriter
HMIVideoAssetWriterOutput
This operation cannot be completed because the asset writer has already started.
UTType class is not available.
Cannot add video input.
Cannot add audio input.
Failed to start writing, assetWriter.status: %ld, assetWriter:.error: %@
Started writing at %@
didOutputSegmentData segmentType: %ld
B16@?0@"AVAssetSegmentTrackReport"8
HMICMSampleBufferIsAudio(sbuf)
Trying to recover by adjusting trim duration from %@ to the minimum trim duration: %@
Asset writer has failed fatally, ignoring %@
Failed to start asset writer.
Failed to write to asset writer, error %@
Underlying asset writer has failed.
Failed to restart asset writer.
Restarted asset writer.
Dropped  %@ because an input for the media type was not found.
Dropped  %@ because asset writer is waiting for a sync sample.
Dropped  %@ because of input error %@
Couldn't append sample buffer because, exception %@
Dropped  %@ because an input %@ is not ready for more media data.
Asset writer has failed fatally, ignoring flush.
Finished waiting for flushSegment, assetWriter.status: %ld, assetWriter.error: %@
Failed to flush segment.
B16@?0@"AVAssetWriterInput"8
Number of all face observations: %ld
v32@?0@"<NSObject><NSCopying><NSSecureCoding>"8Q16^B24
@"NSDictionary"24@?0@"NSUUID"8@"HMIPersonsModel"16
Invalid entry in userDefinedPersonLinks: %@
All links for %@ in userDefinedPersonLinks are invalid
v16@?0@"HMIPersonSourceUUIDPair"8
Skipping person who belongs to user defined equivalency cell: %@
Comparing persons (%@, %@)
Equivalency determined between pair: (%@, %@)!
Cannot add to matching equivalency cell because it already has entry from this source: %@
v32@?0@"NSUUID"8@"NSSet"16^B24
v32@?0@"NSUUID"8@"NSDictionary"16^B24
v24@?0@"VNFaceObservation"8^B16
HMIPersonsModelEquivalencyTable
Failed to update persons model, error:%@, retrying...
Failed to update persons model, error:%@
Submitted persons model update task, taskID:%u, retryOnError:%@
update.persons.model.operation
Time %@
Sparse Optical Flow
camera.video.sparse.optical.flow
HMIVideoFrameSelector
Reference: %@, Target: %@
Adding Candidate: %@
q24@?0@"HMIVideoFrameSelectorFrameCandidate"8@"HMIVideoFrameSelectorFrameCandidate"16
Candidates: %@
v16@?0@"HMIVideoFrameSelectorFrameCandidate"8
Selected: %@
Setting sampleRate: %f, referenceInterval: %@, targetInterval: %@
HMIFR.c
HMIFR.fc
HMIFR.fp
HMIFR.fqs
HMIFR.sea
TransitionMatrix
Joint
Face Classifications
Face Quality Score
HMIFaceRecognition
selector
arguments
Creating analyzer with identifier: %@, configuration: %@
HMIVideoAnalyzer does not support remote analysis.
HMIVideoAnalyzerServer
Recieved Message: %@
Unknown %@
Sent Message Reply: %@
dynamicConfiguration
Video fragment duration: %fs is greater than the max fragment duration value: %fs
Cancel is not yet implemented.
Audio format should not change.
Video format should not change.
self.outputVideoFormat
_encode
!self.encoder
self.inputVideoFormat
Processing: %@
v16@?0@"HMIVideoAnalyzerResultFilter"8
Bundling Fragment Result, timeRange: %@, frames: [%@], thumbnails [%@]
Frame analyzer result buffer should be empty. %@
Thumbnail buffer should be empty. %@
@16@?0@"HMIVideoFrameAnalyzerResult"8
Analyzer Failed: %@
Sending Result: %@
, %.10s
, time: %3ld
, buffer: [
] %5ld KB
, analysisFPS: %.2f
, delay: %4.1f
, pts: %4.1f
, frames: %3ld
, fragments: %3ld
, frameAnalysisTime: %5.2f
, transcode: %@
, encoding: %@ (%@)
, monitored: %@
, frameResults: %lu
, thumbnails: %lu
, recognizeFaces: %@
, activityZones: %lu
, triggers: %@
/tmp/com.apple.HomeAI/familiar-face-data
Error initializing with home UUID: %@ source UUID:%@
Error enumerating persons
Error loading person from JSON
Error enumerating face crops for person UUID:%@
Error loading face crop from JSON
Error loading face crop image
person.datasource.disk
Number of channels in input image must be 1 or 3
cvFloodFill
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-150.5.9/OpenCV/src/imgproc/floodfill.cpp
Connectivity must be 4, 0(=4) or 8
lo_diff and up_diff must be non-negative
Seed point is outside of image
mask must be 2 pixel wider and 2 pixel taller than filled image
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-150.5.9/OpenCV/src/core/datastructs.cpp
cvReleaseMemStorage
cvRestoreMemStoragePos
NULL storage pointer
cvMemStorageAlloc
Too large memory block is requested
storage->free_space % CV_STRUCT_ALIGN == 0
requested size is negative or too big
(size_t)ptr % CV_STRUCT_ALIGN == 0
cvCreateSeq
Specified element size doesn't match to the size of the specified element type (try to use 0 for element type)
cvSetSeqBlockSize
Storage block size is too small to fit the sequence elements
cvCvtSeqToArray
cvStartReadSeq
cvChangeSeqBlock
cvGetSeqReaderPos
cvSetSeqReaderPos
cvSeqPush
ptr + elem_size <= seq->block_max
block->start_index > 0
NULL sequence pointer
cvSeqPushMulti
number of removed elements is negative
cvSeqPopMulti
delta > 0
cvClearSeq
Invalid sequence header
cvSeqSlice
Bad sequence slice
Bad input sequence
cvSeqSort
Null compare function
cvCreateSet
cvSetAdd
count <= CV_SET_ELEM_IDX_MASK+1
cvCreateGraph
cvGraphAddVtx
cvFindGraphEdgeByPtr
ofs == 1 || start_vtx == edge->vtx[0]
graph pointer is NULL
cvGraphAddEdgeByPtr
vertex pointers coinside (or set to NULL)
edge->flags >= 0
Invalid graph pointer
cvCloneGraph
cvInitTreeNodeIterator
cvNextTreeNode
icvInitMemStorage
icvDestroyMemStorage
icvGoNextMemBlock
parent->bottom == block
icvGrowSeq
The sequence has NULL storage pointer
storage->free_space >= delta
block->count % seq->elem_size == 0 && block->count > 0
seq->first->start_index == 0
icvFreeSeqBlock
(in_front_of ? block : block->prev)->count == 0
seq->ptr == block->data
block->count > 0 && block->count % seq->elem_size == 0
Unknown/unsupported border type
borderInterpolate
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-150.5.9/OpenCV/src/imgproc/filter.cpp
columnBorderType != BORDER_WRAP
!rowFilter.empty() && !columnFilter.empty()
bufType == srcType
0 <= anchor.x && anchor.x < ksize.width && 0 <= anchor.y && anchor.y < ksize.height
roi.x >= 0 && roi.y >= 0 && roi.width >= 0 && roi.height >= 0 && roi.x + roi.width <= wholeSize.width && roi.y + roi.height <= wholeSize.height
start
srcRoi.x >= 0 && srcRoi.y >= 0 && srcRoi.width >= 0 && srcRoi.height >= 0 && srcRoi.x + srcRoi.width <= src.cols && srcRoi.y + srcRoi.height <= src.rows
wholeSize.width > 0 && wholeSize.height > 0
proceed
src && dst && count > 0
srcY >= startY
dstY <= roi.height
src.type() == srcType && dst.type() == dstType
apply
dstOfs.x >= 0 && dstOfs.y >= 0 && dstOfs.x + srcRoi.width <= dst.cols && dstOfs.y + srcRoi.height <= dst.rows
_kernel.channels() == 1
getKernelType
cn == CV_MAT_CN(bufType) && ddepth >= std::max(sdepth, CV_32S) && kernel.type() == ddepth
getLinearRowFilter
Unsupported combination of source format (=%d), and buffer format (=%d)
cn == CV_MAT_CN(bufType) && sdepth >= std::max(ddepth, CV_32S) && kernel.type() == sdepth
getLinearColumnFilter
Unsupported combination of buffer format (=%d), and destination format (=%d)
cn == CV_MAT_CN(_dstType)
createSeparableLinearFilter
ktype == CV_8U || ktype == CV_32S || ktype == CV_32F || ktype == CV_64F
preprocess2DKernel
cn == CV_MAT_CN(dstType) && ddepth >= sdepth
getLinearFilter
Unsupported combination of source format (=%d), and destination format (=%d)
createLinearFilter
(symmetryType & (KERNEL_SYMMETRICAL | KERNEL_ASYMMETRICAL)) != 0
SymmColumnVec_32s8u
SymmColumnSmallVec_32s16s
SymmColumnSmallVec_32f
SymmColumnVec_32f16s
SymmColumnVec_32f
anchor.inside(Rect(0, 0, ksize.width, ksize.height))
normalizeAnchor
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-150.5.9/OpenCV/src/imgproc/precomp.hpp
(symmetryType & (KERNEL_SYMMETRICAL | KERNEL_ASYMMETRICAL)) != 0 && this->ksize <= 5
SymmRowSmallFilter
kernel.type() == DataType<DT>::type && (kernel.rows == 1 || kernel.cols == 1)
RowFilter
kernel.type() == DataType<ST>::type && (kernel.rows == 1 || kernel.cols == 1)
ColumnFilter
this->ksize == 3
SymmColumnSmallFilter
SymmColumnFilter
_kernel.type() == DataType<KT>::type
Filter2D
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-150.5.9/OpenCV/src/core/alloc.cpp
Failed to allocate %lu bytes
OutOfMemoryError
borderType != BORDER_CONSTANT
pyrDown
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-150.5.9/OpenCV/src/imgproc/pyramids.cpp
!_src.empty()
pyrDown_
std::abs(dsize.width*2 - ssize.width) <= 2 && std::abs(dsize.height*2 - ssize.height) <= 2
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-150.5.9/OpenCV/src/core/opengl_interop_deprecated.cpp
GlBuffer
GlTexture
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-150.5.9/OpenCV/src/core/persistence.cpp
Invalid pointer to file storage
The node is neither a map nor an empty collection
cvGetFileNodeByName
Null element name
cvStartWriteStruct
The file storage is opened for reading
cvEndWriteStruct
cvWriteInt
cvWriteString
cvWriteRawData
Negative number of elements
Null data pointer
cvStartReadRawData
Null pointer to source file node or reader
The file node should be a numerical scalar or a sequence
cvReadRawDataSlice
Null pointer to reader or destination array
The readed sequence is a scalar, thus len must be 1
The sequence element is not a numerical scalar
The sequence slice does not fit an integer number of records
Null pointers to source file node or destination array
cvReadRawData
opencv-sequence
opencv-sequence-tree
opencv-graph
opencv-sparse-matrix
opencv-image
opencv-matrix
opencv-nd-matrix
Invalid type info
cvRegisterType
Some of required function pointers (is_instance, release, read or write) are NULL
Type name should start with a letter or _
Type name should contain only letters, digits, - and _
NULL double pointer
cvRead
The node does not represent a user object (unknown type?)
Unknown array type
The storage is not opened
icvPuts
An attempt to add element without a key to a map, or add element with key to sequence
icvXMLWriteTag
A single _ is a reserved tag name
Closing tag should not include any attributes
Key should start with a letter or _
Key name may only contain alphanumeric characters [a-zA-Z0-9], '-' and '_'
icvDecodeFormat
fmt_pairs != 0 && max_len > 0
Invalid data type specification
Too long data type specification
(align & (align-1)) == 0 && size < INT_MAX
%.8e
-.Inf
.Inf
%.16e
elements with keys can not be written to sequence
icvXMLWriteScalar
icvYMLWrite
The key is an empty
The key is too long
Key must start with a letter or _
Key names may only contain alphanumeric characters [a-zA-Z0-9], '-', '_' and ' '
icvReleaseSeq
flags
count
Some of essential sequence attributes are absent
icvReadSeq
The sequence flags are invalid
curve
closed
hole
untyped
header_dt
header_user_data
One of "header_dt" and "header_user_data" is there, while the other is not
rect
origin
Only one of "header_user_data", "rect" and "origin" tags may occur
color
data
The image data is not found in file storage
The number of stored elements does not match to "count"
Too complex format for the matrix
icvDecodeSimpleFormat
recursive
false
False
FALSE
icvWriteSeqTree
CV_IS_SEQ( seq )
sequences
icvWriteSeq
level
The size of element calculated from "dt" and the elem_size do not match
icvGetFormat
Size of sequence element (elem_size) is inconsistent with seq->flags
%d%c
The size of header calculated from "header_dt" is greater than header_size
icvWriteHeaderData
opencv-sequence-tree instance should contain a field "sequences" that should be a sequence
icvReadSeqTree
All the sequence tree nodes should contain "level" field
level == prev_level + 1
icvReleaseGraph
vertex_dt
edge_dt
vertex_count
edge_count
Some of essential graph attributes are absent
icvReadGraph
oriented
Graph edges should start with 2 integers and a float
%df%s
vertices
edges
No edges data
No vertices data
Some of stored vertex indices are out of range
Duplicated edge has occured
cvAlignPtr
(align & (align-1)) == 0
icvWriteGraph
CV_IS_GRAPH(graph)
2if%s
sizes
Some of essential matrix attributes are absent
icvReadSparseMat
Could not determine sparse matrix dimensionality
The matrix data is not found in file storage
Sparse matrix data is corrupted
icvWriteSparseMat
CV_IS_SPARSE_MAT(mat)
(reader).seq->elem_size == sizeof(idx)
k < dims
Some of essential image attributes are absent
icvReadImage
layout
interleaved
Only interleaved images can be read
The matrix size does not match to the number of stored elements
icvWriteImage
CV_IS_IMAGE(image)
Images with planar data layout are not supported
top-left
bottom-left
planar
rows
cols
icvReadMat
icvWriteMat
CV_IS_MAT_HDR_Z(mat)
icvReadMatND
Could not determine the matrix dimensionality
icvWriteMatND
CV_IS_MATND_HDR(mat)
cvMat
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-150.5.9/OpenCV/include/opencv2/core/types_c.h
(unsigned)CV_MAT_DEPTH(type) <= CV_64F
type == CV_32FC1 || type == CV_32FC2 || type == CV_64FC1 || type == CV_64FC2
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-150.5.9/OpenCV/src/core/dxt.cpp
This mode (using nonzero_rows with a single-column matrix) breaks the function's logic, so it is prohibited.
For fast convolution/correlation use 2-column matrix or single-row matrix instead
!inv
type == srcB.type() && srcA.size() == srcB.size()
mulSpectrums
(flags & DFT_NO_PERMUTE) == 0
(unsigned)k0 < (unsigned)n && (unsigned)k1 < (unsigned)n
factors[0] == factors[nf-1]
(unsigned)j < (unsigned)n2
(unsigned)j < (unsigned)n
RealDFT
tab_size == n
CCSIDFT
src != dst
DFTInit
nf < 34
elem_size == sizeof(Complex<float>)
channels() == CV_MAT_CN(dtype)
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-150.5.9/OpenCV/src/core/copy.cpp
mask.depth() == CV_8U && (mcn == 1 || mcn == cn)
size() == mask.size()
checkScalar(value, type(), _value.kind(), _InputArray::MAT )
setTo
mask.empty() || mask.type() == CV_8U
repeat
ny > 0 && nx > 0
maskarr == 0
cvCopy
src.depth() == dst.depth() && src.size == dst.size
(coi1 != 0 || src.channels() == 1) && (coi2 != 0 || dst.channels() == 1)
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-150.5.9/OpenCV/src/core/opengl_interop.cpp
The library is compiled without OpenGL support
throw_nogl
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-150.5.9/OpenCV/src/imgproc/thresh.cpp
Unknown threshold type
thresh_8u
thresh_16s
thresh_32f
img.depth() == CV_8U && winSize.width > 2 && winSize.height > 2
buildOpticalFlowPyramid
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-150.5.9/OpenCV/src/video/lkpyramid.cpp
maxLevel >= 0 && winSize.width > 2 && winSize.height > 2
calcOpticalFlowPyrLK
(npoints = prevPtsMat.checkVector(2, CV_32F, true)) >= 0
nextPtsMat.checkVector(2, CV_32F, true) == npoints
statusMat.isContinuous()
errMat.isContinuous()
levels1 >= 0
ofs.x >= winSize.width && ofs.y >= winSize.height && ofs.x + prevPyr[lvlStep1].cols + winSize.width <= fullSize.width && ofs.y + prevPyr[lvlStep1].rows + winSize.height <= fullSize.height
levels2 >= 0
ofs.x >= winSize.width && ofs.y >= winSize.height && ofs.x + nextPyr[lvlStep2].cols + winSize.width <= fullSize.width && ofs.y + nextPyr[lvlStep2].rows + winSize.height <= fullSize.height
prevPyr[level * lvlStep1].size() == nextPyr[level * lvlStep2].size()
prevPyr[level * lvlStep1].type() == nextPyr[level * lvlStep2].type()
depth == CV_8U
calcSharrDeriv
cvAlign
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-150.5.9/OpenCV/include/opencv2/core/internal.hpp
scn == 1
convertAndUnrollScalar
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-150.5.9/OpenCV/src/core/arithm.cpp
op == CMP_LT || op == CMP_LE || op == CMP_EQ || op == CMP_NE || op == CMP_GE || op == CMP_GT
compare
The operation is neither 'array op array' (where arrays have the same size and the same type), nor 'array op scalar', nor 'scalar op array'
The operation is neither 'array op array' (where arrays have the same size and type), nor 'array op scalar', nor 'scalar op array'
binary_op
(mask.type() == CV_8UC1 || mask.type() == CV_8SC1)
mask.size == src1.size
operator()
-256 <= ((b) - (a)) && ((b) - (a)) <= 512
-256 <= ((a) - (b)) && ((a) - (b)) <= 512
The operation is neither 'array op array' (where arrays have the same size and the same number of channels), nor 'array op scalar', nor 'scalar op array'
arithm_op
src2.type() == CV_64F && (src2.rows == 4 || src2.rows == 1)
When the input arrays in add/subtract/multiply/divide functions have different types, the output array type must be explicitly specified
-256 <= (a - b) && (a - b) <= 512
img.dims <= 2 && templ.dims <= 2 && corr.dims <= 2
crossCorr
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-150.5.9/OpenCV/src/imgproc/templmatch.cpp
depth == tdepth || tdepth == CV_32F
corrsize.height <= img.rows + templ.rows - 1 && corrsize.width <= img.cols + templ.cols - 1
ccn == 1 || delta == 0
the input arrays are too big
CV_MAT_CN(sumType) == CV_MAT_CN(srcType)
getRowSumFilter
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-150.5.9/OpenCV/src/imgproc/smooth.cpp
CV_MAT_CN(sumType) == CV_MAT_CN(dstType)
getColumnSumFilter
Unsupported combination of sum format (=%d), and destination format (=%d)
sumCount == ksize-1
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-150.5.9/OpenCV/src/imgproc/utils.cpp
top >= 0 && bottom >= 0 && left >= 0 && right >= 0
copyMakeBorder
value[0] == value[1] && value[0] == value[2] && value[0] == value[3]
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-150.5.9/OpenCV/src/imgproc/deriv.cpp
ktype == CV_32F || ktype == CV_64F
getScharrKernels
dx >= 0 && dy >= 0 && dx+dy == 1
getSobelKernels
The kernel size must be odd and not larger than 31
dx >= 0 && dy >= 0 && dx+dy > 0
ksize > order
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-150.5.9/OpenCV/src/core/convert.cpp
src && nsrcs > 0 && dst && ndsts > 0 && fromTo && npairs > 0
mixChannels
j < nsrcs && src[j].depth() == depth
i1 >= 0 && j < ndsts && dst[j].depth() == depth
type == B.type() && (type == CV_32FC1 || type == CV_64FC1 || type == CV_32FC2 || type == CV_64FC2)
gemm
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-150.5.9/OpenCV/src/core/matmul.cpp
a_size.width == len
a_size.height == len
C.type() == type && (((flags&GEMM_3_T) == 0 && C.rows == d_size.height && C.cols == d_size.width) || ((flags&GEMM_3_T) != 0 && C.rows == d_size.width && C.cols == d_size.height))
type == CV_64FC2
src1.type() == src2.type()
scaleAdd
src.channels() == 1
mulTransposed
delta.channels() == 1 && (delta.rows == src.rows || delta.rows == 1) && (delta.cols == src.cols || delta.cols == 1)
GEMM_TransposeBlock
MulTransposedR
delta_cols == 1
cn <= 4 && func != 0
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-150.5.9/OpenCV/src/core/stat.cpp
src.channels() == 1 && func != 0
countNonZero
mean
(cn == 1 && (mask.empty() || mask.type() == CV_8U)) || (cn >= 1 && mask.empty() && !minIdx && !maxIdx)
minMaxIdx
img.dims <= 2
src.type() == CV_8UC1
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-150.5.9/OpenCV/src/core/lapack.cpp
type == CV_32F || type == CV_64F
invert
m == n
method == DECOMP_LU || method == DECOMP_CHOLESKY
n == 1
type == _src2.type() && (type == CV_32F || type == CV_64F)
solve
(method != DECOMP_LU && method != DECOMP_CHOLESKY) || is_normal || src.rows == src.cols
src.rows == 1
The function can not solve under-determined linear systems
src.rows == src.cols
eigen
w.type() == u.type() && u.type() == vt.type() && u.data && vt.data && w.data
backSubst
u.cols >= nm && vt.rows >= nm && (w.size() == Size(nm, 1) || w.size() == Size(1, nm) || w.size() == Size(vt.rows, u.cols))
rhs.data == 0 || (rhs.type() == type && rhs.rows == m)
_SVDcompute
0 <= d && d <= CV_MAX_DIM && _sizes
create
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-150.5.9/OpenCV/src/core/matrix.cpp
step[dims-1] == (size_t)CV_ELEM_SIZE(flags)
m.dims >= 2
0 <= _rowRange.start && _rowRange.start <= _rowRange.end && _rowRange.end <= m.rows
0 <= _colRange.start && _colRange.start <= _colRange.end && _colRange.end <= m.cols
m.dims <= 2
0 <= roi.x && 0 <= roi.width && roi.x + roi.width <= m.cols && 0 <= roi.y && 0 <= roi.height && roi.y + roi.height <= m.rows
ranges
r == Range::all() || (0 <= r.start && r.start < r.end && r.end <= m.size[i])
dims <= 2
diag
img->dataOrder == IPL_DATA_ORDER_PIXEL
img->dataOrder == IPL_DATA_ORDER_PIXEL || img->roi->coi != 0
(int)nelems >= 0
reserve
resize
COI is not supported by the function
cvarrToMat
seq->total > 0 && CV_ELEM_SIZE(seq->flags) == seq->elem_size
dims <= 2 && step[0] > 0
locateROI
adjustROI
reshape
The matrix is not continuous, thus its number of rows can not be changed
Bad new number of rows
The total number of matrix elements is not divisible by the new number of rows
The total width is not divisible by the new number of channels
cn <= 4
scalarToRawData
i < 0
getMat
0 <= i && i < (int)vv.size()
This method is not implemented for oclMat yet
k == STD_VECTOR_MAT
0 <= i && i < (int)v.size()
getMatVector
This function in deprecated, do not use it
getGlBuffer
getGlTexture
k == GPU_MAT
getGpuMat
size
i < (int)vv.size()
total
empty
!fixedSize() || ((Mat*)obj)->size.operator()() == _sz
!fixedType() || ((Mat*)obj)->type() == mtype
!fixedSize() || ((gpu::GpuMat*)obj)->size() == _sz
!fixedType() || ((gpu::GpuMat*)obj)->type() == mtype
!fixedSize() || ((ogl::Buffer*)obj)->size() == _sz
!fixedType() || ((ogl::Buffer*)obj)->type() == mtype
!fixedSize() || ((Mat*)obj)->size.operator()() == Size(cols, rows)
!fixedSize() || ((gpu::GpuMat*)obj)->size() == Size(cols, rows)
!fixedSize() || ((ogl::Buffer*)obj)->size() == Size(cols, rows)
!fixedType() && !fixedSize()
CV_MAT_TYPE(mtype) == m.type()
m.dims == dims
m.size[j] == sizes[j]
mtype == type0 || (CV_MAT_CN(mtype) == 1 && ((1 << type0) & fixedDepthMask) != 0)
dims == 2 && ((sizes[0] == sz.height && sizes[1] == sz.width) || (allowTransposed && sizes[0] == sz.width && sizes[1] == sz.height))
dims == 2 && (sizes[0] == 1 || sizes[1] == 1 || sizes[0]*sizes[1] == 0)
!fixedSize() || len == vv.size()
mtype == type0 || (CV_MAT_CN(mtype) == CV_MAT_CN(type0) && ((1 << type0) & fixedDepthMask) != 0)
!fixedSize() || len == ((vector<uchar>*)v)->size() / esz
Vectors with element size %d are not supported. Please, modify OutputArray::create()
create() called for the missing output array
!fixedSize() || len == len0
v[j].empty()
i < (int)v.size()
!fixedType() || (CV_MAT_CN(mtype) == m.channels() && ((1 << CV_MAT_TYPE(flags)) & fixedDepthMask) != 0)
!fixedSize()
release
clear
k == MAT
getMatRef
setIdentity
src.dims <= 2 && esz <= (size_t)32
transpose
src.size() == dst.size() && (src.cols == 1 || src.rows == 1)
func != 0
m.dims <= 2 && m.rows == m.cols
completeSymm
src.dims <= 2
src.channels() == dst.channels()
_arrays && (_ptrs || _planes)
init
narrays <= 1000
arrays[i] != 0
A.size == arrays[i0]->size
A.step[d-1] == A.elemSize()
copyTo
convertTo
minMaxLoc
0 <= _dims && _dims <= CV_MAX_DIM
setSize
s >= 0
%s:%d: error: (%d) %s in function %s
%s:%d: error: (%d) %s
OpenCV Error: %s (%s) in %s, file %s, line %d
No Error
Backtrace
Unspecified error
Internal error
Insufficient memory
Bad argument
Iterations do not converge
Autotrace call
Incorrect size of input array
Null pointer
Division by zero occured
Image step is wrong
Inplace operation is not supported
Requested object was not found
Input image depth is not supported by function
Formats of input arguments do not match
Sizes of input arguments do not match
One of arguments' values is out of range
Unsupported format or combination of formats
Input COI is not supported
Bad number of channels
Bad flag (parameter or structure field)
Bad parameter of type CvPoint
Bad type of mask argument
Parsing error
The function/feature is not implemented
Memory block has been corrupted
Assertion failed
No GPU support
Gpu API call
No OpenGL support
OpenGL API call
Unknown %s code %d
status
module != 0 && module->name != 0 && module->version != 0
cvRegisterModule
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-150.5.9/OpenCV/src/core/system.cpp
cxcore
2.4.13.6
unknown function
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-150.5.9/OpenCV/include/opencv2/dynamicuda/dynamicuda.hpp
copy
copyWithMask
convert
mallocPitch
The library is compiled without CUDA support
qualityLevel > 0 && minDistance >= 0 && maxCorners >= 0
goodFeaturesToTrack
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-150.5.9/OpenCV/src/imgproc/featureselect.cpp
mask.empty() || (mask.type() == CV_8UC1 && mask.size() == image.size())
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-150.5.9/OpenCV/src/core/array.cpp
Non-positive width or height
cvCreateMatHeader
Invalid matrix type
cvInitMatHeader
Non-positive cols or rows
cvReleaseMat
Bad CvMat header
cvCloneMat
NULL matrix header pointer
cvInitMatNDHeader
invalid array data type
NULL <sizes> pointer
non-positive or too large number of dimensions
one of dimesion sizes is non-positive
The array is too big
cvCreateMatNDHeader
Bad CvMatND header
cvCloneMatND
src->dims <= CV_MAX_DIM
_dst.data == data0
Incorrect number of arrays
cvInitNArrayIterator
Some of required array pointers is NULL
Iterator pointer is NULL
COI set is not allowed here
Number of dimensions is the same for all arrays
Data type is not the same for all arrays
Number of channels is not the same for all arrays
Depth is not the same for all arrays
Mask should have 8uC1 or 8sC1 data type
Dimension sizes are the same for all arrays
cvNextNArraySlice
iterator != 0
cvCreateSparseMat
bad number of dimensions
cvReleaseSparseMat
Invalid sparse array header
cvCloneSparseMat
Invalid sparse matrix header
cvInitSparseMatIterator
NULL iterator pointer
Data is already allocated
cvCreateData
unrecognized or unsupported array type
cvReleaseData
Only continuous nD arrays are supported here
cvGetElemType
cvGetDims
Array should be CvMat or IplImage
cvGetSize
cvScalarToRawData
scalar && data
The number of channels must be 1, 2, 3 or 4
index is out of range
cvPtr2D
COI must be non-null in case of planar images
NULL pointer to indices
cvPtrND
NULL array pointer is passed
cvGetMat
The matrix has NULL data pointer
The image has NULL data pointer
Images with planar data layout should be used with COI selected
The image is interleaved and has over CV_CN_MAX channels
Pixel order should be used with coi == 0
Input array has NULL data pointer
Unrecognized or unsupported array type
cvCreateImage
null pointer to header
cvInitImageHeader
Bad input roi
Unsupported format
Bad input origin
Bad input align
cvReleaseImageHeader
cvReleaseImage
cvSetImageROI
rect.width >= 0 && rect.height >= 0 && rect.x < image->width && rect.y < image->height && rect.x + rect.width >= (int)(rect.width > 0) && rect.y + rect.height >= (int)(rect.height > 0)
cvSetImageCOI
cvGetImageCOI
Bad image header
cvCloneImage
cvGetMatND
icvGetNodePtr
CV_IS_SPARSE_MAT( mat )
One of indices is out of range
(newsize & (newsize - 1)) == 0
GRAY
BGRA
op == MORPH_ERODE || op == MORPH_DILATE
getMorphologyRowFilter
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-150.5.9/OpenCV/src/imgproc/morph.cpp
Unsupported data type (=%d)
getMorphologyColumnFilter
getMorphologyFilter
depth == CV_8U || depth == CV_16U || depth == CV_16S || depth == CV_32F || depth == CV_64F
createMorphologyFilter
shape == MORPH_RECT || shape == MORPH_CROSS || shape == MORPH_ELLIPSE
getStructuringElement
morphOp
((size_t)src[i] & 15) == 0
((size_t)_src[i] & 15) == 0
_kernel.type() == CV_8U
MorphFilter
src.type() == CV_8UC1 || src.type() == CV_32FC1
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-150.5.9/OpenCV/src/imgproc/corner.cpp
cornerEigenValsVecs
CV_MAT_CN(_type) == e.a.channels()
assign
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-150.5.9/OpenCV/src/core/matop.cpp
Unknown operation
Invalid matrix initializer type
%{public}@%{public}@
%{public}@%{public}s
Identifier = %@, Name = %@
HMISignpost
softlink:o:path:/System/Library/Frameworks/HomeKit.framework/HomeKit
softlink:o:path:/System/Library/Frameworks/HomeKit.framework/HomeKit
softlink:o:path:/System/Library/Frameworks/HomeKit.framework/HomeKit
softlink:o:path:/System/Library/Frameworks/HomeKit.framework/HomeKit
softlink:o:path:/System/Library/PrivateFrameworks/PrivateFederatedLearning.framework/PrivateFederatedLearning
HMIExternalPersonDataSourceDisk
HMIExternalPersonManagerDataSource
HMIPersonManagerDataSource
NSObject
HMIMutableCluster
HMFLogging
HMIFloodFillProcessorKernel
HMIInputFeatureProvider
MLFeatureProvider
HMISignificantActivityDetector
HMISystemResourceUsage
HMISystemResourceUsageMonitor
HMISystemResourceUsageMonitorProtocol
HMIBoundingBoxExtractorConfiguration
HMIBoundingBoxExtractor
HMIVideoAnalyzerEventFace
HMIDataReader
HMIVideoEventEntry
HMIVideoEvent
HMIVideoEventBuffer
HMIVideoTimelineEntry
HMIVideoTimeline
HMIVideoTimelineProfiler
HMITimeIntervalAverage
HMIVideoAnalyzerEventMotion
HMIHomePersonManager
HMFTimerDelegate
HMIStoreFaceCropOperation
HMIStoreFaceprintOperation
HMICameraVideoFrame
HMIVisionUtilities
HMIThermalMonitorService
HMIThermalMonitor
HMIMemorySampler
HMIPBSSystemLoadMonitor
HMIVideoAssetReader
HMIHomeKitClient
HMHomeManagerDelegateAdapter
HMHomeManagerDelegate
HMISignpost
HMIFaceQualityFilterModelInput
HMIFaceQualityFilterSVM
HMIVideoNode
HMIVideoProcessingNode
PFLBackgroundRunner
_DASExtensionRunner
HMIExternalPersonDataSourceHomeKit
HMI_CVML_Error
HMIPersonFaceCrop
NSCopying
NSSecureCoding
NSCoding
HMIVideoAnalyzerFrameResult
HMIMemoryAVAsset
AVAssetResourceLoaderDelegate
MovingAverageEntry
MovingAverage
HMICameraVideoAnalyzerResult
HMICameraVideoAnalyzerConfiguration
HMICameraVideoAnalyzerSignificantEvent
HMICameraVideoFrameResult
HMICameraVideoFragment
HMICameraVideoAnalyzerHistory
HMICameraVideoAnalyzerRequest
HMIVideoEncoderDelegate
HMIVideoRetimerDelegate
HMIVideoFrameSamplerDelegate
HMICameraVideoFrameSelectorDelegate
HMICameraVideoAnalyzerScheduler
HMISystemResourceUsageMonitorDelegate
HMICameraVideoAnalyzer
HMIDESMutableFloatArray
HMIFaceClassifierVIP
HMIFaceClassifier
HMICameraVideoFrameAnalyzerFactory
HMICIFilterAttributeValue
HMICIFilterAttribute
HMICIFilter
HMIDESDatasetSample
HMIDESDataset
HMINMSConfiguration
HMIObjectDetection
HMIObjectDetectionUtils
HMIVideoFrameAnalyzerResult
HMIVideoFrameAnalyzer
HMIVideoFrameAnalyzerDelegateAdapter
HMIVideoFrameAnalyzerDelegate
HMICameraVideoPosterFrame
HMICameraVideoPosterFrameGeneratorInput
HMICameraVideoPosterFrameGenerator
HMIVideoAnalyzerMutableReportComparison
HMIVideoAnalyzerMutableReportSession
HMIVideoAnalyzerMutableReport
HMIFaceCrop
HMIVideoRetimer
HMIVideoRetimerDelegateAdapter
HMICamera
HMITask
HMIHomeTask
HMIEmptyTask
HMITaskServiceServer
HMITaskService
HMIFetchPersonsOperation
HMFObject
HMICameraVideoFrameSelectorFrameScore
HMICameraVideoFrameSelector
HMICameraVideoFrameSamplerDelegate
HMICameraVideoFrameSampler
HMIUpdatedFaceprintsResult
HMIFaceprinter
HMIFaceDetectorVision
HMIFaceDetector
HMIFaceprint
HMIPoint
HMIHomePersonManagerSettings
NSMutableCopying
HMIMutableHomePersonManagerSettings
HMIModelLoader
HMIFeedbackSession
NSURLSessionDelegate
HMIFeedbackSubmitClipOperation
HMIFeedbackTask
HMIFeedback
HMIFaceClassification
HMIPersonsModelsSummaryTask
HMIFaceMisclassificationTask
HMIKernels
HMIRemovePersonsModelTask
HMIPersonManager
HMIConfidence
HMIError
HMIVideoFragment
HMISessionEntityManager
HMITuriTrialUpdateTask
HMIVideoAnnotator
HMISystemResourceUsageMonitorImpl
HMIVideoAnalyzerResultFilter
HMIVideoAnalyzerResultActivityZoneFilter
HMIPersonSourceUUIDPair
HMIPerson
HMIPersonsModelSummary
HMIPersonsModel
HMIJSONArchiver
HMIJSONUnarchiver
HMIFaceUtilities
HMIPersonBlob
HMIFaceTrackerMatch
HMIFaceTrackerMotionRecord
HMIFaceTracker
HMICameraVideoResourceAttributes
HMICameraVideoAssetReader
HMIVideoEncoder
HMIVideoEncoderDelegateAdapter
HMIHomePersonDataSourceHomeKit
HMIHomePersonManagerDataSource
HMIExternalPersonManagerSettings
HMIMutableExternalPersonManagerSettings
HMIAnalytics
HMIPersonsModelsSummary
HMIPersonsModelPrediction
HMIPersonsModelManager
HMIDESLayerParameters
HMIDESTrainingResult
HMIDESTrainer
HMIVideoAnalyzerResultOutcome
HMIExternalPersonManager
HMIRemovePersonsModelOperation
HMIVideoAnalyzerScheduler
HMITuriTrialManager
HMIGreedyClustering
HMIVideoAnalyzerLegacy
HMICameraVideoAnalyzerDelegate
HMIVideoFrameSampler
HMIVideoFrameIntervalSampler
HMIVideoFrameSamplerDelegateAdapter
HMIVideoPackageAnalyzer
HMIVideoPackageAnalyzerDelegateAdapter
HMIVideoPackageAnalyzerDelegate
HMIFaceQualityEntropyOfLaplacian
HMIFaceQualityDistanceToJunkCluster
HMIFaceQualityScoreConsolidated
HMIVideoAnalyzerFragmentResult
HMICameraActivityZone
HMIVideoAnalyzerConfiguration
HMIVideoAnalyzerDynamicConfiguration
HMIMotionDetection
HMICameraVideoAnalyzerDelegateAdapter
HMIAnalysisService
HMIVideoAnalyzerEventPerson
HMIDESRunner
HMIPreference
HMIHomePersonDataSourceDisk
HMIFetchPersonFaceCropsOperation
HMIDeltaEFilter
HMINotifydObserver
HMIVideoAnalyzerEventVehicle
HMIPackageCandidateDetectorConfiguration
HMIIntegralImageProcessorKernel
HMIPackageCandidateDetector
HMIClusteringTaskSummary
HMIHomePersonClusteringTask
HMIUpdatePersonsModelTask
HMIFetchFaceprintsForFaceCropsOperation
HMIStoreFaceprintsOperation
HMIRemoveFaceprintsOperation
HMIRemoveFaceCropsOperation
HMIAverageImage
HMIRegionOfInterestOperation
HMICameraVideoFrameAnalyzerSignificantActivity
HMICameraVideoFrameAnalyzer
HMIDESBackgroundTask
HMIVideoFrame
HMICompressionHelper
HMIVideoCommandBuffer
HMIVideoCommandBufferDelegateAdapter
HMIVideoCommandBufferDelegate
HMIVideoAnalyzerEventPet
HMIVideoAnalyzerEvent
HMIVideoAnalyzerEventPackage
HMIVideoDecoder
HMIVideoDecoderDelegateAdapter
HMIVideoDecoderDelegate
HMIVideoAssetWriter
AVAssetWriterDelegate
HMIVideoAssetWriterDelegateAdapter
HMIVideoAssetWriterDelegate
HMIPersonsModelEquivalencyTable
HMIUpdatePersonsModelOperation
HMISparseOpticalFlowFeatureVector
HMISparseOpticalFlowFrame
HMISparseOpticalFlowMotionDetection
HMISparseOpticalFlowMotionDetector
HMIMotionDetector
HMIVideoFrameSelectorFrameCandidate
HMIVideoFrameSelector
HMIVideoFrameSelectorDelegateAdapter
HMIVideoFrameSelectorDelegate
HMIFaceRecognition
HMIVideoAnalyzer
HMIVideoAnalyzerDelegatePrivate
HMIVideoAnalyzerDelegate
HMIVideoAnalyzerServer
HMIVideoAnalyzerDelegateAdapter
HMIPersonDataSourceDisk
workQueue
sourceURL
UUID
UUIDString
URLByAppendingPathComponent:
defaultManager
path
createDirectoryAtPath:withIntermediateDirectories:attributes:error:
stringWithFormat:
hmiPrivateErrorWithCode:description:underlyingError:
name
dictionaryWithObjects:forKeys:count:
serializeJSONObject:url:error:
countByEnumeratingWithState:objects:count:
personUUID
faceBoundingBox
dateCreated
dataRepresentation
writeToURL:atomically:
hmiPrivateErrorWithCode:description:
hmfErrorWithCode:
categoryWithName:
logCategory
isEqual:
class
self
performSelector:
performSelector:withObject:
performSelector:withObject:withObject:
isProxy
isKindOfClass:
isMemberOfClass:
conformsToProtocol:
respondsToSelector:
retain
release
autorelease
retainCount
zone
hash
superclass
description
debugDescription
TQ,R
T#,R
T@"NSString",R,C
fetchAllPersonsWithCompletion:
fetchPersonsWithUUIDs:completion:
fetchAllPersonFaceCropsWithCompletion:
fetchFaceCropsForPersonsWithUUIDs:completion:
fetchAllFaceprintsWithCompletion:
fetchFaceprintsForFaceCropsWithUUIDs:completion:
performCloudPullWithCompletion:
addFaceprints:completion:
removeFaceprintsWithUUIDs:completion:
fetchSettingsWithCompletion:
addPerson:completion:
addPersonFaceCrops:completion:
init
data
initWithData:
arrayWithObject:
count
copy
addObjectsFromArray:
scale:
add:
addObject:
floatArrayByScaling:
logIdentifier
initWithFaceprint:
faceprintUUIDs
addLinkedEntityUUIDs:
linkedEntityUUIDs
addFaceprints:
centroid
.cxx_destruct
_faceprintUUIDs
_linkedEntityUUIDs
_centroid
T@"HMIDESMutableFloatArray",R,V_centroid
TQ,R,N
T@"NSArray",R
T@"NSSet",R
firstObject
baseAddress
bytesPerRow
region
internal
exceptionWithName:reason:userInfo:
processWithInputs:arguments:output:error:
formatForInputAtIndex:
outputFormat
pixelBuffer
dealloc
inputName
arrayWithObjects:count:
setWithArray:
isEqualToString:
featureValueWithPixelBuffer:
featureValueForName:
featureNames
T@"NSSet",R,N
initWithPixelBuffer:inputName:
_pixelBuffer
_inputName
T^{__CVBuffer=},R,V_pixelBuffer
T@"NSString",R,V_inputName
objectAtIndexedSubscript:
doubleValue
sharedInstance
usesCPUOnly
setUsesCPUOnly:
boolPreferenceForKey:defaultValue:
setAllowBackgroundGPUCompute:
fileURLWithPath:
modelWithContentsOfURL:configuration:error:
hmiPrivateErrorWithCode:underlyingError:
arrayWithCapacity:
initWithName:
_runNeuralNetworkOnPixelBuffer:offsets:scores:yaws:rolls:error:
_postProcessOffsets:scores:yaws:rolls:outputPredictions:
initWithPixelBuffer:presentationTimeStamp:
printWithScale:
transferPixelBuffer:pixelFormat:options:error:
inputFeatureValueName
mlModel
predictionOptions
predictionFromFeatures:options:error:
offsetsFeatureValueNames
type
multiArrayValue
scoresFeatureValueNames
yawsFeatureValueNames
rollsFeatureValueNames
array
shape
unsignedLongValue
dataPointer
strides
useSoftmax
numberWithDouble:
initWithLabelIndex:confidence:unclampedBoundingBox:yaw:roll:
nmsConfiguration
nmsMultiClass:output:nmsConfiguration:
boundingBox
labelIndex
confidence
roll
initWithLabelIndex:confidence:boundingBox:yaw:roll:
bundleForClass:
pathForResource:ofType:
defaultAssetPath
initWithConfidenceThresholds:nmsConfiguration:assetPath:error:
predict:detectedObjects:error:
inputDimensions
_confidenceThresholds
_anchorSizes
_numberOfAnchors
_useSoftmax
_mlModel
_inputFeatureValueName
_offsetsFeatureValueNames
_scoresFeatureValueNames
_yawsFeatureValueNames
_rollsFeatureValueNames
_nmsConfiguration
_predictionOptions
_inputDimensions
T@"MLModel",R,V_mlModel
T@"NSString",R,V_inputFeatureValueName
T@"NSArray",R,V_offsetsFeatureValueNames
T@"NSArray",R,V_scoresFeatureValueNames
T@"NSArray",R,V_yawsFeatureValueNames
T@"NSArray",R,V_rollsFeatureValueNames
T@"HMINMSConfiguration",R,V_nmsConfiguration
TB,R,V_useSoftmax
T@"MLPredictionOptions",R,V_predictionOptions
T{CGSize=dd},R,V_inputDimensions
systemResourceUsageLevel
setSystemResourceUsageLevel:
_systemResourceUsageLevel
Tq,N,V_systemResourceUsageLevel
UTF8String
productInfo
productClass
initWithProductClass:workQueue:
systemResourceUsageMonitorImpl
setDelegate:
delegate
getCurrentSystemResourceUsage
start
T@"<HMISystemResourceUsageMonitorDelegate>",W
_systemResourceUsageMonitorImpl
_workQueue
T@"HMISystemResourceUsageMonitorImpl",R,V_systemResourceUsageMonitorImpl
T@"NSObject<OS_dispatch_queue>",R,V_workQueue
initWithOverlapThreshold:scalePreMerge:minBoxSizePreMerge:maxBoxSizePreMerge:maxBoxSizePostMerge:minBoxSizeFinal:maxBoxSizeFinal:
defaultConfiguration
overlapThreshold
minBoxSizePreMerge
maxBoxSizePreMerge
maxBoxSizePostMerge
minBoxSizeFinal
maxBoxSizeFinal
scalePreMerge
_overlapThreshold
_minBoxSizePreMerge
_maxBoxSizePreMerge
_maxBoxSizePostMerge
_minBoxSizeFinal
_maxBoxSizeFinal
_scalePreMerge
Tf,R,V_overlapThreshold
Tf,R,V_minBoxSizePreMerge
Tf,R,V_maxBoxSizePreMerge
Tf,R,V_maxBoxSizePostMerge
Tf,R,V_minBoxSizeFinal
Tf,R,V_maxBoxSizeFinal
Tf,R,V_scalePreMerge
initWithConfig:
config
extent
rawBoxesInFloodFillImage:
squaredBoxes:
scaledBoxes:scale:
validBoxes:minSize:maxSize:imageSize:
mergedBoxes:overlapThreshold:minSize:maxSize:imageSize:
numberWithInt:
null
contextWithOptions:
createPixelBufferWithSize:pixelFormat:useIOSurface:
render:toCVPixelBuffer:bounds:colorSpace:
valueWithRect:
rectValue
isValidBox:minSize:maxSize:imageSize:
na_filter:
valueWithBytes:objCType:
na_map:
squaredBox:
arrayWithArray:
removeObjectAtIndex:
arrayByAddingObjectsFromArray:
boxesInFloodFillImage:
_config
T@"HMIBoundingBoxExtractorConfiguration",R,V_config
initWithConfidence:boundingBox:yaw:roll:faceRecognition:userInfo:
initWithConfidence:boundingBox:hasMotionVectors:userInfo:
attributeDescriptions
faceRecognition
initWithName:value:
shortDescription
encodeWithCoder:
encodeObject:forKey:
initWithCoder:
decodeObjectOfClass:forKey:
userInfo
supportsSecureCoding
initWithConfidence:boundingBox:
initWithConfidence:boundingBox:faceRecognition:
_faceRecognition
_yaw
_roll
T@"NSNumber",R,V_yaw
T@"NSNumber",R,V_roll
T@"HMIFaceRecognition",R,V_faceRecognition
whitespaceCharacterSet
stringByTrimmingCharactersInSet:
length
bytes
subdataWithRange:
position
readUInt32
readUInt64
seek:
readData:
_data
_position
localeWithLocaleIdentifier:
setLocale:
setDateFormat:
dateFromString:
substringToIndex:
stringWithCapacity:
appendFormat:
time
T{?=qiIq},R
initWithValue:time:
value
_value
_time
T{?=qiIq},R,V_time
T@,R,V_value
removeAllObjects
hmf_removeFirstObject
indexOfObject:inSortedRange:options:usingComparator:
insertObject:atIndex:
hmf_objectsPassingTest:
indexesOfObjectsPassingTest:
objectsAtIndexes:
removeObjectsAtIndexes:
objectAtIndex:
componentsJoinedByString:
initWithMaxCapacity:
objectsInTimeRange:includeEnd:
extractObjectsInTimeRange:
neighborsOfObject:
_lock
_maxCapacity
initWithTime:date:
date
_date
T@"NSDate",R,V_date
lastObject
dateAtTime:
timeIntervalSinceDate:
addDate:atTime:
timeIntervalSinceDateAtTime:
_buffer
addValue:
startedAtTime:
endedAtTime:
averageTime
_timeline
_average
initWithWindowSize:
addNumber:
movingAverage
movingAverageForInterval:defaultValue:
valueForInterval:defaultValue:
performBlock:
initWithUUID:homeUUID:
setMaxConcurrentOperationCount:
initWithTimeInterval:options:
resume
dictionary
initWithTimeout:
_updateSettings:
finish
addExecutionBlock:
operationQueue
addOperation:
watchdogTimer
taskServiceClient
homeUUID
objectForKeyedSubscript:
submitTaskWithOptions:completionHandler:
dataSource
initWithDataSource:faceCrop:
error
setCompletionBlock:
hmiPrivateErrorWithCode:
initWithDataSource:faceprint:
classifications
familiarity
sourceUUID
na_firstObjectPassingTest:
unknownFacesSavedCounts
sessionEntityUUID
intValue
setObject:forKeyedSubscript:
faceCrop
faceprint
storeUnassociatedFaceCrop:completion:
storeFaceprint:completion:
isPersonDataAvailableViaHomeKit
initWithSourceUUID:homeUUID:external:
suspend
analyticsTimer
settings
isFaceClassificationEnabled
standardUserDefaults
doubleForKey:
timeIntervalSinceReferenceDate
setDouble:forKey:
cancelWithError:
timerDidFire:
setDataSource:
handleUpdatedPerson:
handleUpdatedUnassociatedFaceCrop:
handleUpdatedPersonFaceCrop:
handleUpdatedFaceprint:
handleUpdatedSettings:
handleRemovedPersonWithUUID:
handleRemovedFaceCropWithUUID:
handleRemovedFaceprintWithUUID:
handleMisclassifiedPersonForFaceCrop:
handleNewFaceEvents:
lock
_dataSource
_settings
_operationQueue
_watchdogTimer
_analyticsTimer
_unknownFacesSavedCounts
T@"NSOperationQueue",R,V_operationQueue
T@"HMFTimer",R,V_watchdogTimer
T@"HMFTimer",R,V_analyticsTimer
T@"HMFUnfairLock",R,N,V_lock
T@"NSMutableDictionary",R,V_unknownFacesSavedCounts
T@"HMIHomePersonManagerSettings",R,V_settings
T@"<HMIHomePersonManagerDataSource>",W,N,V_dataSource
setWithObject:
addFaceCrops:completion:
main
_faceCrop
T@"<HMIHomePersonManagerDataSource>",R,V_dataSource
T@"HMIFaceCrop",R,V_faceCrop
_faceprint
T@"HMIFaceprint",R,V_faceprint
initWithPixelBuffer:presentationTime:frameId:fragmentSequenceNumber:
jpegData
size
imageWithData:
render:toCVPixelBuffer:
numberWithUnsignedLong:
dictionaryWithDictionary:
setObject:forKey:
appendBytes:length:
presentationTime
frameId
initWithPixelBuffer:
initWithJPEGData:presentationTime:frameId:fragmentSequenceNumber:size:
JPEGRepresentationWithDownscaleFactor:outSize:
fragmentSequenceNumber
motionDetections
setMotionDetections:
sessionPresentationTime
setSessionPresentationTime:
_frameId
_fragmentSequenceNumber
_jpegData
_motionDetections
_size
_presentationTime
_sessionPresentationTime
T@"NSData",R,V_jpegData
T@"NSArray",&,V_motionDetections
T{?=qiIq},V_sessionPresentationTime
T{?=qiIq},R,V_presentationTime
T{CGSize=dd},R,V_size
TQ,R,V_frameId
TQ,R,V_fragmentSequenceNumber
point
transferPixelBuffer:crop:size:pixelFormat:options:error:
numberWithFloat:
createPixelBufferFromJPEGDataProvider:error:
applyPadding:withOriginalSize:padding:
globalSession
releaseCachedResources
cropPixelBuffer:crop:error:
resizePixelBuffer:size:error:
resizePixelBuffer:size:pixelFormat:options:error:
createJPEGDataFromPixelBuffer:scale:encodeQuality:error:
createPixelBufferFromJPEGPath:error:
createPixelBufferFromJPEGData:error:
createPixelBufferFromImageData:error:
dewarpPixelBuffer:crop:size:pixelFormat:options:cameraModel:error:
imposeMinSizeFor:withOriginalSize:minCrop:
maintainAspectRatio:originalSize:ratioThreshold:
saveVideoFrame:videoId:baseURL:error:
transferPixelBuffer:rotationAngle:crop:size:precision:error:
releaseCachedVisionResources
initWithService:
readValue
_service
_updateThermalLevel
services
objectForKey:
readValueFromSensor:value:
readMaxValue:
thermalLevel
_client
_thermalLevelNotificationToken
_notificationQueue
_thermalLevel
_services
T@"NSMutableDictionary",R,V_services
TQ,R,V_thermalLevel
unsignedLongLongValue
tick
numberWithUnsignedLongLong:
setZeroPadsFractionDigits:
setAllowedUnits:
setCountStyle:
setAllowsNonnumericFormatting:
stringFromByteCount:
highWaterMark
initWithName:reason:userInfo:
stop
setHighWaterMark:
average
_highWaterMark
_tick
T@"HMFTimer",R,V_tick
T@"MovingAverage",R,V_average
Tq,V_highWaterMark
_updatePBSSystemLoad
pbsSystemLoad
isIdle
_pbsSystemLoadNotificationToken
_pbsSystemLoad
TQ,R,V_pbsSystemLoad
initWithAsset:readVideoTrack:readAudioTrack:
assetReaderWithAsset:error:
_createOutputsForAsset:readVideo:readAudio:
tracksWithMediaType:
assetReaderTrackOutputWithTrack:outputSettings:
setAlwaysCopiesSampleData:
canAddOutput:
addOutput:
cancelReading
copyNextSampleBuffer
status
copyNextSampleBufferWithTrackIndexOutput:
startReading
_copyNextSampleBufferFromTrackOutput:
initWithAsset:
_asset
_assetReader
_trackSamples
_trackOutputs
initWithCachePolicy:
setName:
setup
homes
hmf_firstObjectWithUUID:
personManager
residentDevices
isCurrentDevice
na_any:
uuid
photosPersonManagerWithUUID:
accessories
cameraProfiles
isSetup
defaultPrivateConfiguration
setOptions:
cachePolicy
setCachePolicy:
setDiscretionary:
homeKitOperationQueue
setDelegateQueue:
setDidUpdateHomes:
initWithHomeMangerConfiguration:
setHomeManager:
homeManager
dateWithTimeIntervalSinceNow:
_refreshBeforeDate:completionHandler:
isPrimary
initWithNoCaching
homePersonManagerForHomeUUID:
homeForHMPersonManagerUUID:
homePersonManagersForCurrentDevice
photosPersonManagerForHomeUUID:sourceUUID:
isCurrentDevicePrimaryResident
cameraProfileWithUUID:
homeWithCameraProfileUUID:
_setup
_homes
_homeKitOperationQueue
_cachePolicy
_homeManager
T@"NSOperationQueue",R,V_homeKitOperationQueue
TB,R,GisSetup,V_setup
TQ,R,V_cachePolicy
T@"HMHomeManager",&,V_homeManager
T@"NSArray",R,V_homes
didUpdateHomes
homeManager:didUpdateAuthorizationStatus:
homeManagerDidUpdateHomes:
homeManagerDidUpdatePrimaryHome:
homeManager:didAddHome:
homeManager:didRemoveHome:
homeManager:didReceiveAddAccessoryRequest:
_didUpdateHomes
T@?,C,V_didUpdateHomes
initWithName:deferred:
begin
signpostLog
beginDate
hasBegun
shouldSignpost
signpostIdentifier
identifier
_name
_beginDate
_signpostIdentifier
_identifier
T@"NSDate",R,V_beginDate
TQ,R,V_signpostIdentifier
T@"NSUUID",R,C,V_identifier
T@"NSString",R,C,V_name
getUUIDBytes:
dataWithBytesNoCopy:length:freeWhenDone:
getBytes:range:
input
featureValueWithMultiArray:
initWithInput:inputName:
setInput:
_input
T@"MLMultiArray",&,N,V_input
modelWithContentsOfURL:error:
setObject:atIndexedSubscript:
dataScalerInputName
scalerModel
predictionFromFeatures:error:
dataScalerOutputName
svmInputName
svmOutputName
dictionaryValue
modelPathForResource:
defaultRecognizabilityModelPath
defaultRecognizabilityDataScalerPath
defaultAestheticQualityModelPath
defaultAestheticQualityDataScalerPath
initWithModelPath:dataScalerPath:error:
predict:output:error:
_scalerModel
T@"MLModel",R,V_scalerModel
_status
_error
Tq,R
T@"NSError",R
handleVideoSampleBuffer:
handleAudioSampleBuffer:
handleSampleBuffer:
flush
flushAsync
task
setTask:
_task
T@"HMIDESBackgroundTask",&,V_task
photosPersonManager
personFromHomePerson:
initWithUUID:dataRepresentation:dateCreated:faceBoundingBox:personUUID:
modelUUID
faceCropUUID
initWithUUID:data:modelUUID:faceCropUUID:
addOrUpdateFaceprints:completion:
initWithHMPhotosPersonManager:
setPhotosPersonManager:
_photosPersonManager
T@"HMPhotosPersonManager",&,V_photosPersonManager
stringWithUTF8String:
mainBundle
localizedStringForKey:value:table:
errorWithDomain:code:userInfo:
createNSErrorWithStatus:andMessage:
createNSExceptionWithStatus:andMessage:
initWithUUID:dataRepresentation:dateCreated:faceBoundingBox:
copyWithZone:
TB,R
_personUUID
T@"NSUUID",R,C,V_personUUID
initWithFrame:events:regionOfInterest:
na_each:
frame
redactedCopy
events
regionOfInterest
eventClasses
maxConfidenceEventForEventClass:
decodeObjectOfClasses:forKey:
decodeRectForKey:
encodeRect:forKey:
initWithFrame:events:
maxConfidenceEvents
_frame
_events
_regionOfInterest
T{CGRect={CGPoint=dd}{CGSize=dd}},R,V_regionOfInterest
T@"HMIVideoFrame",R,V_frame
T@"NSSet",R,V_events
URLWithString:
initWithURL:options:
resourceLoader
setDelegate:queue:
contentInformationRequest
setContentType:
setContentLength:
setByteRangeAccessSupported:
dataRequest
requestedOffset
requestedLength
respondWithData:
finishLoading
loadValuesAsynchronouslyForKeys:completionHandler:
resourceLoader:shouldWaitForLoadingOfRequestedResource:
resourceLoader:shouldWaitForRenewalOfRequestedResource:
resourceLoader:didCancelLoadingRequest:
resourceLoader:shouldWaitForResponseToAuthenticationChallenge:
resourceLoader:didCancelAuthenticationChallenge:
loadValuesSynchronously
initWithValue:
T@"NSNumber",R,V_value
windowSize
queue
setMovingAverage:
setQueue:
_movingAverage
_queue
_windowSize
T@"NSMutableArray",&,N,V_queue
TQ,R,N,V_windowSize
Td,V_movingAverage
integerValue
analyzerEvents
allObjects
na_flatMap:
_eventsFromAnalyzerEvents:
_annotationScoresFromAnalyzerEvents:
frameResults
confidenceThatEventOccurred:
aggregatedEvents
resultCode
lastSequenceNumber
analysisFPS
duration
creationDate
isEqualToArray:
posterFrames
videoFragment
timeToAnalyzeFragment
timeSinceFragmentWasSubmitted
numberWithInteger:
initWithEvents:posterFrames:frameResults:annotationScores:duration:creationDate:resultCode:lastSequenceNumber:
initWithPosterFrames:frameResults:duration:creationDate:resultCode:lastSequenceNumber:
aggregatedEventTypes
isEqual:excludeTime:
annotationScores
setDuration:
setCreationDate:
setResultCode:
setTimeToAnalyzeFragment:
setTimeSinceFragmentWasSubmitted:
setVideoFragment:
setAnalysisFPS:
_analysisFPS
_annotationScores
_posterFrames
_frameResults
_creationDate
_resultCode
_timeToAnalyzeFragment
_timeSinceFragmentWasSubmitted
_videoFragment
_lastSequenceNumber
_duration
T{?=qiIq},V_duration
T@"NSDate",&,V_creationDate
TQ,R,V_lastSequenceNumber
Tq,R,V_events
T@"NSDictionary",R,V_annotationScores
T@"NSArray",R,V_posterFrames
T@"NSArray",R,V_frameResults
Tq,V_resultCode
Td,V_timeToAnalyzeFragment
Td,V_timeSinceFragmentWasSubmitted
T@"HMICameraVideoFragment",&,V_videoFragment
Tf,V_analysisFPS
initWithIdentifier:name:manufacturer:model:
initWithPosterFrameGenerationInterval:posterFrameHeight:maxFragmentAnalysisDuration:maxFragmentDuration:transcodeFragment:camera:
getAnalysisServiceTypePreference
numberPreferenceForKey:defaultValue:withMap:
allocWithZone:
initWithPosterFrameGenerationInterval:posterFrameHeight:maxFragmentAnalysisDuration:maxFragmentDuration:transcodeFragment:
posterFrameGenerationInterval
posterFrameHeight
maxFragmentAnalysisDuration
maxFragmentDuration
transcodeFragment
setTranscodeFragment:
camera
serviceType
setServiceType:
startingMediaIntegritySequenceNumber
setStartingMediaIntegritySequenceNumber:
useScheduler
setUseScheduler:
inMediaAnalysis
setInMediaAnalysis:
faceClassificationEnabled
setFaceClassificationEnabled:
currentSessionDuration
setCurrentSessionDuration:
setHomeUUID:
_transcodeFragment
_useScheduler
_inMediaAnalysis
_faceClassificationEnabled
_posterFrameGenerationInterval
_posterFrameHeight
_maxFragmentAnalysisDuration
_maxFragmentDuration
_camera
_serviceType
_startingMediaIntegritySequenceNumber
_homeUUID
_currentSessionDuration
TQ,V_serviceType
TQ,V_startingMediaIntegritySequenceNumber
TB,V_useScheduler
TB,V_inMediaAnalysis
TB,V_faceClassificationEnabled
T{?=qiIq},V_currentSessionDuration
T@"NSUUID",&,V_homeUUID
TQ,R,V_posterFrameGenerationInterval
TQ,R,V_posterFrameHeight
Td,R,V_maxFragmentAnalysisDuration
Td,R,V_maxFragmentDuration
TB,V_transcodeFragment
T@"HMICamera",R,V_camera
confidenceThatEventOccurred:events:annotationScores:
initWithEvents:annotationScores:videoFrame:
videoFrame
_videoFrame
T@"HMICameraVideoFrame",R,V_videoFrame
_detectionsFromAnalyzerEvents:
_faceClassificationsFromAnalyzerEvents:
face
level
initWithFrame:events:annotationScores:detections:regionOfInterest:faceClassifications:
initWithFrame:regionOfInterest:analyzerEvents:
detections
faceClassifications
_detections
_faceClassifications
_analyzerEvents
T@"HMICameraVideoFrame",R,V_frame
T@"NSArray",R,V_detections
T@"NSSet",R,V_faceClassifications
T@"NSSet",R,V_analyzerEvents
initWithSequenceNumber:data:moovFragment:eventTypes:activityZones:
setUrl:
moovFragment
appendData:
sequenceNumber
stringByAppendingFormat:
absoluteString
initWithSequenceNumber:data:moovFragment:
initWithSequenceNumber:data:moovFragment:eventTypes:
initWithSequenceNumber:fragmentData:eventTypes:
initWithSequenceNumber:fragmentData:eventTypes:activityZones:url:
fragmentData
eventTypes
activityZones
_sequenceNumber
_moovFragment
_eventTypes
_activityZones
_url
T@"NSMutableData",R
T@"NSURL",&,N,V_url
TQ,R,V_sequenceNumber
T@"NSData",R,V_data
T@"NSData",R,N,V_moovFragment
Tq,R,V_eventTypes
T@"NSArray",R,V_activityZones
totalRequests
setTotalRequests:
flag
predictions
setPredictions:
totalPredictions
setTotalPredictions:
lastRequestResult
isEqualToSet:
repetitions
setRepetitions:
totalRepetitions
setTotalRepetitions:
setLastRequestResult:
setLastRequestSignificantEvents:
analyzer
scheduler
activeAnalyzerCount
maxPredictions
minRepetitions
lastRequestSignificantEvents
fragment
attributes
assetDuration
initWithMinRepetitions:maxPredictions:analyzer:
addRequest:result:significantEvents:
shouldPredictRequest:
predictedSignificantEventsForRequest:
predictedResultForRequest:
reset
_minRepetitions
_maxPredictions
_predictions
_repetitions
_totalPredictions
_totalRepetitions
_totalRequests
_lastRequestResult
_lastRequestSignificantEvents
_analyzer
Tq,V_predictions
Tq,V_repetitions
Tq,V_totalPredictions
Tq,V_totalRepetitions
Tq,V_totalRequests
T@"HMICameraVideoAnalyzerResult",&,V_lastRequestResult
T@"NSArray",&,V_lastRequestSignificantEvents
T@"HMICameraVideoAnalyzer",R,W,V_analyzer
Tq,R,V_minRepetitions
Tq,R,V_maxPredictions
significantEventsInternal
setFlag:
initWithAssetData:error:
loadAttributesFromVideoFragment:
analysisSubmissionTime
dimensions
initWithDimensions:codecType:realTime:error:
maxVideoEncoderFrameRate
setExpectedFrameRate:
setAverageBitRate:
setDataRateLimit:
initWithVideoFormat:audioFormat:
firstSequenceNumber
setNextSequenceNumber:
setAllowRecovery:
initWithFrameRate:
encoder
retimer
frameSampler
initWithGenerationFrequency:andPosterFrameHeight:
initWithInput:
startHandlingFrames
readerForVideoFragment:workQueue:logIdentifier:
maxAnalysisFPS
initWithResourceAttributes:sampleRate:activityZones:
preAnalyze:
handleMotionDetection:sessionPTS:frameDimensions:sessionIdentifier:
setAssetWriterDidOutputInitializationSegment:
mutableCopy
initWithInitializationSegment:separableSegment:
setAssetWriterDidOutputSeparableSegment:
assetWriter
flushWithNextSamplePTS:
initWithSampleBuffer:
localizedStringFromDate:dateStyle:timeStyle:
averageBitRate
points
drawPolygonWithNormalizedPoints:
drawTextHeaderBar:
draw:
baseDecodeTimeStamp
audioSamples
posterFrameGenerator
videoFrameResults
timeSinceAnalysisStart
timeSinceAnalysisSubmission
analysisStartTime
encoder:didEncodeSampleBuffer:
encoder:didFailWithError:
retimer:didRetimeSampleBuffer:
frameSampler:didSampleFrame:
frameSampler:didDropFrame:
selector:maySelectFrame:
selector:didDetectMotion:atSessionPTS:frameDimensions:
initWithVideoFragment:analyzer:maxAnalysisFPS:
addSignificantEvent:
significantEvents
markForPrediction
shouldSkipAnalysis
shouldFailAnalysis
loadAttributes
startAnalysis
startEncodingSessionForAsset:expectedFrameRate:error:
startPosterFrameGeneratorWithInterval:frameHeight:
startAssetReaderWithWorkQueue:logIdentifier:
startFrameSelector
finishEncoderSession
makeDidAnalyzeResult
makeDidNotAnalyzeResultWithResultCode:
cancel
frameSelector
assetReader
setVideoFrameResults:
phase
setPhase:
_analysisSubmissionTime
_analysisStartTime
_maxAnalysisFPS
_fragment
_attributes
_encoder
_retimer
_frameSampler
_audioSamples
_assetWriter
_posterFrameGenerator
_frameSelector
_videoFrameResults
_significantEventsInternal
_phase
_flag
T@"NSMutableArray",R,V_significantEventsInternal
Tq,V_phase
Tq,V_flag
T@"NSDate",R,V_analysisSubmissionTime
Td,R
T@"NSDate",R,V_analysisStartTime
Td,R,V_maxAnalysisFPS
Td,R,V_analysisFPS
T@"HMICameraVideoFragment",R,V_fragment
T@"HMICameraVideoResourceAttributes",R,V_attributes
T@"HMIVideoEncoder",R,V_encoder
T@"HMIVideoRetimer",R,V_retimer
T@"HMIVideoFrameSampler",R,V_frameSampler
T@"NSMutableArray",R,V_audioSamples
T@"HMIVideoAssetWriter",R,V_assetWriter
T@"HMICameraVideoPosterFrameGenerator",R,V_posterFrameGenerator
T@"HMICameraVideoFrameSelector",R,V_frameSelector
T@"HMICameraVideoAssetReader",R,V_assetReader
T@"HMICameraVideoAnalyzer",R,V_analyzer
T@"NSMutableArray",&,V_videoFrameResults
weakObjectsPointerArray
setSystemResourceUsageMonitorUsageLevel:
systemResourceUsageMonitorUsageLevel
numberPreferenceForKey:defaultValue:
maxConcurrentAnalyzers
maxConcurrentAnalyzersForSystemResourceUsageLevel:
analyzers
isActive
averageAnalysisTimeMovingAverage
averageTotalAnalysisTimeMovingAverage
averageTotalAnalysisTime
isAudioAccessory
isProductTypeJ42
updateAnalysisFPS:
internalAnalyzers
hmf_addObject:
assertOwner
addPointer:
compact
setCount:
_compactInternalAnalyzers
isPaused
logState
processPendingRequests
history
setInBypassMode:
_markPendingRequestsWithFlag:
averageAnalysisTime
pendingRequests
stringByPaddingToLength:withString:startingAtIndex:
_outcomeCountsAsString
mediaIntegritySequenceNumber
_flagCountsAsString
inBypassMode
inErrorState
analysisInProgress
appendString:
systemResourceUsageDidChangeTo:
inFullBypassMode
transcodingAnalyzerCount
requestDidEnd:outcome:
registerAnalyzer:
removeAllAnalyzers
setPaused:
systemResourceUsageMonitor
analysisFPSPreference
_paused
_maxConcurrentAnalyzers
_internalAnalyzers
_systemResourceUsageMonitor
_systemResourceUsageMonitorUsageLevel
_averageAnalysisTimeMovingAverage
_averageTotalAnalysisTimeMovingAverage
_analysisFPSPreference
T@"NSPointerArray",R,V_internalAnalyzers
T@"HMISystemResourceUsageMonitor",R,V_systemResourceUsageMonitor
Tq,V_systemResourceUsageMonitorUsageLevel
T@"MovingAverage",R,V_averageAnalysisTimeMovingAverage
T@"MovingAverage",R,V_averageTotalAnalysisTimeMovingAverage
Td,R,V_analysisFPSPreference
paused
TB,GisPaused,V_paused
TQ,R,V_maxConcurrentAnalyzers
qosMap
maxVideoEncoders
isProductTypeB238
_scheduleRequest:
sessionEnded
configuration
_handleError:request:
setLastRequestSubmissionTime:
internalPendingRequests
hmf_enqueue:
pendingRequestsCount
lastRequestSubmissionTime
timeIntervalSinceNow
setSessionEnded:
setRunRemotely:
_handleError:request:description:
skipSequentialMediaIntegrityCheck
nominalFrameRate
_handleDidNotAnalyzeRequest:resultCode:
_shouldContinueAnalyzingRequest:resultCode:
hmf_maybeDequeue
_analyzeRequest:
setAnalysisInProgress:
_checkRequest:
_predictRequest:
_analyzeRequestRemotely:retryOnConnectionInterruption:
_analyzeRequestLocally:
setMediaIntegritySequenceNumber:
homePersonManager
remoteAnalysisService
preferenceOverrides
analyzer:didFindSignificantEvent:inFragment:
code
_handleError:request:underlyingError:
_handleDidAnalyzeRequest:withResult:
_handleDidNotAnalyzeRequest:withResult:error:
requestAnalysisForAssetData:withProperties:andCompletionHandler:
warmStartModel
warmStart
numberWithUnsignedInteger:
_willAnalyzeRequest:
_handleError:request:description:underlyingError:
_analyzeRequestFramesLocally:
startReading:
analyzer:willAnalyzeRequest:
_finishEncodingSessionForRequest:withResult:
_sendAnalyticsEventForRequest:outcome:result:error:
shouldUploadVideoAnalysisEvent
sendEventForFaceEvent:homePersonManagerUUID:camera:
_requestDidEnd:outcome:
_notifyDidAnalyzeRequest:withResult:
_handleDidNotAnalyzeRequest:resultCode:error:
_notifyDidNotAnalyzeRequest:withResult:
hmiErrorWithCode:description:underlyingError:
_enterErrorState
_notifyDidFailAnalysisForRequest:withError:
analyzer:didAnalyzeFragment:withResult:
analyzer:didAnalyzeRequest:withResult:
analyzer:didNotAnalyzeFragment:withResult:
analyzer:didNotAnalyzeRequest:withResult:
analyzer:didFailAnalysisForFragment:withError:
analyzer:didFailAnalysisForRequest:withError:
setInErrorState:
readNextFrame:error:
willHandleFramesFromVideoResource:
handleVideoFrame:error:
isFinishedEarly
_analyzeRequestFrames:
_handleDidAnalyzeRequest:
frames
_analyzeVideoFrame:request:result:error:
analyze:targetEventTypes:enableFaceClassification:sessionIdentifier:homeUUID:error:
saveToJsonActivityZones:motionDetection:videoFragmentUrl:frameId:UUID:detectionID:zoneID:
saveActivityZoneresultsInJSON:result:videoFrame:motionDetection:
filterEvents:withActivityZones:motionDetection:insetPercentageInclusion:insetPercentageExclusion:
generateAndSubmitStats:filteredEvents:motionDetection:activityZones:
_analyzeFrame:request:error:
filterDetectedObjects:request:result:
shouldSaveVideoFramesToDisk
saveFaceClassifications:videoId:fragmentId:frameId:baseURL:error:
manufacturer
model
numberWithBool:
eventConfidenceThresholdsHigh
eventConfidenceThresholdsMedium
string
bundleWithIdentifier:
infoDictionary
queryVersionInformation
classHierarchyMap
initWithConfiguration:identifier:
analyzeFragment:
clearPendingFragments
setHomePersonManager:
externalPersonManagers
setExternalPersonManagers:
streamAnalyzer
currentRequest
setCurrentRequest:
setConfiguration:
setRemoteAnalysisService:
setSaveVideoFramesToDisk:
_flagCounts
_outcomeCounts
_skipSequentialMediaIntegrityCheck
_analysisInProgress
_inErrorState
_inBypassMode
_sessionEnded
_uploadVideoAnalysisEvent
_saveVideoFramesToDisk
_delegate
_homePersonManager
_externalPersonManagers
_internalPendingRequests
_lastRequestSubmissionTime
_history
_streamAnalyzer
_currentRequest
_scheduler
_mediaIntegritySequenceNumber
_configuration
_remoteAnalysisService
T@"NSMutableArray",R,V_internalPendingRequests
T@"NSDate",&,V_lastRequestSubmissionTime
T@"HMICameraVideoAnalyzerHistory",R,V_history
T@"HMIVideoAnalyzer",R,V_streamAnalyzer
T@"HMICameraVideoAnalyzerRequest",&,V_currentRequest
T@"HMICameraVideoAnalyzerScheduler",R,V_scheduler
TQ,V_mediaIntegritySequenceNumber
TB,R,V_skipSequentialMediaIntegrityCheck
TB,V_analysisInProgress
TB,V_inErrorState
TB,V_inBypassMode
T@"HMICameraVideoAnalyzerConfiguration",&,V_configuration
T@"HMIAnalysisService",&,N,V_remoteAnalysisService
TB,V_sessionEnded
uploadVideoAnalysisEvent
TB,R,GshouldUploadVideoAnalysisEvent,V_uploadVideoAnalysisEvent
saveVideoFramesToDisk
TB,GshouldSaveVideoFramesToDisk,V_saveVideoFramesToDisk
TB,R,V_transcodeFragment
T@"<HMICameraVideoAnalyzerDelegate>",W,V_delegate
T@"HMIHomePersonManager",&,V_homePersonManager
T@"NSSet",&,V_externalPersonManagers
appendFloats:count:
unsignedIntegerValue
initWithFloats:count:
mutableFloats
mutableBytes
floats
subtract:
initWithDataTensor:
fillWithFloat:
appendArray:
l2Norm
floatArrayByAdding:
floatArrayBySubtracting:
mutableCopyWithZone:
T@"NSData",R,&,N
Tr^f,R,N
T^f,R,N
initWithShape:dataType:error:
faceObservationWithRequestRevision:unalignedBoundingBox:alignedBoundingBox:
faceprinter
createFacePixelBufferForFaceDetection:pixelBuffer:roll:error:
computeJunkScoreForPixelBuffer:
qualityPredictionFromSVMUsingFaceQualityFilterSVM:detectorConfidence:laplacian:yaw:boxSize:error:
createFaceprintForFacePixelBuffer:fastMode:error:
predictPersonFromFaceObservation:homeUUID:error:
faceAttributes
facemaskCategory
label
descriptorData
currentModelUUID
floatValue
classificationThresholdKnown
linkedEntityUUID
classificationThresholdUnknown
initWithUUID:sourceUUID:sessionEntityUUID:faceCrop:faceprint:confidence:familiarity:
initWithFaceCrop:faceprint:classifications:predictedLinkedEntityUUIDs:faceQualityScore:sessionEntityAssignment:
classifyFaceEvent:pixelBuffer:fastMode:homeUUID:error:
initWithError:
faceRecognizabilityFilter
faceAestheticQualityFilter
_faceprinter
_faceRecognizabilityFilter
_faceAestheticQualityFilter
_classificationThresholdKnown
_classificationThresholdUnknown
T@"HMIFaceprinter",R,V_faceprinter
T@"HMIFaceQualityFilterSVM",R,V_faceRecognizabilityFilter
T@"HMIFaceQualityFilterSVM",R,V_faceAestheticQualityFilter
Td,R,V_classificationThresholdKnown
Td,R,V_classificationThresholdUnknown
setFrameAnalyzer:
modelTimeoutPreference
hmf_zeroUUID
frameAnalyzer
kick
initWithThresholdWithLabels:metricWithLabels:thresholdDefault:metricDefault:
initWithMediumConfidenceThresholds:highConfidenceThresholds:nmsConfiguration:assetPath:error:
ensureFrameAnalyzerWithError:
personThresholdMedium
petThresholdMedium
vehicleThresholdMedium
personThresholdHigh
petThresholdHigh
vehicleThresholdHigh
faceThreshold
numberPreferenceForKey:
na_dictionaryByMappingValues:
defaultConfidenceThresholdsMedium
mergeDictionary:with:
defaultConfidenceThresholdsHigh
addEntriesFromDictionary:
T@"HMICameraVideoFrameAnalyzerFactory",R
getConfidenceThresholdPreferenceForKey:defaultConfidenceThreshold:
eventConfidenceThresholdsMediumFromTrial
eventConfidenceThresholdsHighFromTrial
eventConfidenceFaceThresholdFromTrial
_frameAnalyzer
T@"<HMICameraVideoFrameAnalyzer>",&,N,V_frameAnalyzer
containsObject:
vectorWithString:
CGAffineTransformValue
valueAtIndex:
initWithDictionary:forType:
initWithDictionary:
_type
T@"NSString",R,V_name
T@"NSString",R,V_type
T@"HMICIFilterAttributeValue",R,V_value
attributeForKey:
filterWithName:withInputParameters:
expectedAttributeForKey:
setValue:forKey:
outputImage
applyToImage:withProbability:
probability
_probability
Td,R,V_probability
T@"NSArray",R,V_attributes
imageData
initWithImageData:regionOfInterest:detections:
createRegionOfInterestPixelBufferWithError:
initWithData:type:shape:strides:
distantPast
initWithBundleIdentifier:
isPermitted
saveRecordWithData:recordInfo:completion:
saveDESRecordWithVideoFrame:recordInfo:
augmentWithOptions:
createImageTensorWithError:
createBoxesTensorWithError:
createClassesTensorWithError:
createWeightsTensorWithError:
_boxesTensorData
_weightsTensorData
_classesTensorData
_imageData
T@"NSData",R,V_imageData
samples
imageName
boxesName
weightsName
classesName
initWithSamples:imageName:boxesName:weightsName:classesName:
dataPointAtIndex:error:
numberOfDataPoints
_samples
_imageName
_boxesName
_weightsName
_classesName
T@"NSArray",R,V_samples
T@"NSString",R,V_imageName
T@"NSString",R,V_boxesName
T@"NSString",R,V_weightsName
T@"NSString",R,V_classesName
initWithCVPixelBuffer:imageParameters:error:
setMaxNumberOfElements:
thresholdWithLabels
thresholdDefault
metricWithLabels
metricDefault
thresholdForLabel:
metricForLabel:
_thresholdWithLabels
_metricWithLabels
_thresholdDefault
_metricDefault
T@"NSDictionary",R,V_thresholdWithLabels
T@"NSDictionary",R,V_metricWithLabels
T@"NSNumber",R,V_thresholdDefault
T@"NSNumber",R,V_metricDefault
_labelIndex
_confidence
_boundingBox
Ti,R,V_labelIndex
Td,R,V_confidence
T{CGRect={CGPoint=dd}{CGSize=dd}},R,N,V_boundingBox
sortedArrayUsingComparator:
nonMaximumSuppression:output:withThreshold:withMetric:
intersectionOverUnion:b:
intersectionOverMinArea:b:
convertObjectDetections:cropRect:originalImageSize:
presentationTimeStamp
recognizeFaces
sessionIdentifier
frameAnalyzer:didAnalyzeFrame:error:
handleSampleBuffer:motionDetections:
setRecognizeFaces:
_analysisTime
_recognizeFaces
_sessionIdentifier
T@"<HMIVideoFrameAnalyzerDelegate>",W,V_delegate
T@"NSUUID",R,V_sessionIdentifier
TB,V_recognizeFaces
frameAnalyzerDidAnalyzeFrame
setFrameAnalyzerDidAnalyzeFrame:
_frameAnalyzerDidAnalyzeFrame
T@?,C,V_frameAnalyzerDidAnalyzeFrame
initWithData:timeOffset:width:height:
timeOffset
width
height
_width
_height
_timeOffset
T{?=qiIq},R,V_timeOffset
TQ,R,V_width
TQ,R,V_height
generationFrequency
frameHeight
_frameHeight
_generationFrequency
T{?=qiIq},R,V_generationFrequency
TQ,R,V_frameHeight
posterFramesInternal
nextGenerationTime
setNextGenerationTime:
saveAsPosterFrame:error:
setPosterFramesInternal:
_posterFramesInternal
_nextGenerationTime
T@"NSMutableArray",&,V_posterFramesInternal
T@"HMICameraVideoPosterFrameGeneratorInput",R,V_input
T{?=qiIq},V_nextGenerationTime
precision
recall
truePositive
falseNegative
falsePositive
initWithPrecision:recall:truePositive:falseNegative:falsePositive:
_precision
_recall
_truePositive
_falseNegative
_falsePositive
Tf,R,V_precision
Tf,R,V_recall
Tq,R,V_truePositive
Tq,R,V_falseNegative
Tq,R,V_falsePositive
fragments
source
initWithSource:
appendFragmentResult:
setSource:
_source
_fragments
T@"NSString",&,V_source
T@"NSMutableArray",R,V_fragments
systemDeviceInformation
unarchivedObjectOfClass:fromData:error:
sessions
eventClassesArray
compareWithTruth:eventClass:threshold:
shortNameForEventClass:
archivedDataWithRootObject:requiringSecureCoding:error:
initWithContentsOfFile:
eventClassForShortName:
initWithValue:levelThresholds:
outcome
initWithFragment:events:frameResults:thumbnails:configuration:outcome:
dataWithJSONObject:options:error:
initWithData:encoding:
version
encodeInteger:forKey:
deviceInformation
decodeIntegerForKey:
T@"NSDictionary",R
initWithData:error:
appendFragmentResult:forKey:source:
appendFragmentResultsFromReport:
chartDataWithTruth:isBaseline:
truthReportFromLegacyFormat:
chartSpec
_version
_deviceInformation
_sessions
Tq,R,V_version
T@"NSDictionary",R,V_deviceInformation
T@"NSMutableDictionary",R,V_sessions
T@"NSData",R
T@"NSString",R
detectFacesInImageData:error:
unalignedBoundingBox
faceBoxFromPhotosFaceCropImageData:
newDictionaryPopulatedWithFaceCropDataFromImageData:
dataUsingEncoding:
JSONObjectWithData:options:error:
isEqualToData:
isEqualToDate:
faceCropFromPhotosFaceCropImageData:
_UUID
_dataRepresentation
_dateCreated
_faceBoundingBox
T@"NSUUID",R,C,V_UUID
T@"NSData",R,C,V_dataRepresentation
T@"NSDate",R,C,V_dateCreated
T{CGRect={CGPoint=dd}{CGSize=dd}},R,V_faceBoundingBox
_lastSample
T@"<HMIVideoRetimerDelegate>",W,V_delegate
retimerDidRetimeSampleBuffer
setRetimerDidRetimeSampleBuffer:
_retimerDidRetimeSampleBuffer
T@?,C,V_retimerDidRetimeSampleBuffer
_manufacturer
_model
T@"NSUUID",R,V_identifier
T@"NSString",R,V_manufacturer
T@"NSString",R,V_model
initWithTaskID:timeout:
initWithTaskID:
results
taskID
_taskID
Ti,R,V_taskID
initWithTaskID:homeUUID:timeout:
T@"NSUUID",R,V_homeUUID
initPrivate
nextTaskID
setNextTaskID:
buildUpdatePersonsModelTaskFromOptions:error:
buildRemovePersonsModelTaskFromOptions:error:
buildHomePersonClusteringTaskFromOptions:error:
buildTuriTrialUpdateTaskFromOptions:error:
getNextTaskID
buildFaceMisclassificationTaskFromOptions:error:
buildPersonsModelsSummaryTaskFromOptions:error:
buildSubmitFeedbackTaskFromOptions:error:
submitTask:completionHander:
operations
enumerateObjectsUsingBlock:
boolValue
stringPreferenceForKey:defaultValue:
initWithHMHomePersonManager:
initWithHomeUUID:sourceUUID:error:
initWithTaskID:homeUUID:sourceUUID:dataSource:externalLibrary:removeExcessFaceCrops:
initWithTaskID:homeUUID:sourceUUID:
initWithTaskID:dataSource:faceCrop:
initWithTaskID:homeUUID:dataSource:sourceUUID:personsModelManager:error:
initWithTaskID:homeUUID:
initWithTaskID:cameraProfileUUID:clipUUID:
cancelTask:
_nextTaskID
Ti,V_nextTaskID
taskService
allowedClasses
privateDescription
propertyDescription
T@"NSArray",R,C,N
initWithDataSource:
persons
_persons
T@"<HMIPersonManagerDataSource>",R,V_dataSource
T@"NSSet",R,V_persons
score
initWithFrame:score:
_score
Tf,R,V_score
hasPreferenceForKey:
initWithResourceAttributes:sampleRate:targetInterval:
valuePreferenceForKey:defaultValue:withMap:
alloc
initWithSize:
framesInternal
detector
appendFramePixelBuffer:atTime:
detectWithGlobalMotionScore:activityZones:
sortUsingComparator:
maxFrameCount
predictedFrames
removeObject:
sampler
appendFrame:error:
sampler:didFindSample:
sampler:didFindSample:target:
sampler:didDiscardFrame:
sampleRate
_sampler
_framesInternal
_maxFrameCount
_predictedFrames
_detector
_sampleRate
T@"HMICameraVideoFrameSampler",R,V_sampler
T{?=qiIq},R,V_sampleRate
T@"NSMutableArray",R,V_framesInternal
Tq,R,V_maxFrameCount
T@"NSMutableArray",R,V_predictedFrames
T@"<HMIMotionDetector>",R,V_detector
T@"<HMICameraVideoFrameSelectorDelegate>",W,V_delegate
setFrame:
sampleInterval
targetInterval
unmatchedSampleFrames
isMarkedAsFinished
_appendFrame:error:
setMarkedAsFinished:
_markedAsFinished
_unmatchedSampleFrames
_targetInterval
_sampleInterval
T{?=qiIq},R,V_targetInterval
T{?=qiIq},R,V_sampleInterval
T@"NSMutableArray",R,V_unmatchedSampleFrames
T@"HMICameraVideoFrame",&,V_frame
markedAsFinished
TB,GisMarkedAsFinished,V_markedAsFinished
T@"<HMICameraVideoFrameSamplerDelegate>",W,V_delegate
existingAtCurrentVersion
isSentinelFaceprint
createdAtCurrentVersion
initWithExistingAtCurrentVersion:createdAtCurrentVersion:existingAtOtherVersions:
allAtCurrentVersion
existingAtOtherVersions
_existingAtOtherVersions
_createdAtCurrentVersion
_existingAtCurrentVersion
T@"NSSet",R,V_existingAtOtherVersions
T@"NSSet",R,V_createdAtCurrentVersion
T@"NSSet",R,V_existingAtCurrentVersion
_minorVersionFromVisionVersion:
hmf_UUIDWithNamespace:data:
initWithCVPixelBuffer:options:
setInputFaceObservations:
setPrivateRevision:error:
setDetectionLevel:
setRevision:
performRequests:error:
setFaceprint:
createFacePixelBufferFromFaceCrop:error:
unionSet:
generateFaceprintForFaceCrop:error:
sentinelFaceprintWithUUID:modelUUID:faceCropUUID:
updatedFaceprintsForFaceCrops:withExistingFaceprints:error:
shouldUseCPUOnlyForVisionFaceDetection
initWithData:options:
detectFacesInPixelBuffer:error:
_modelUUID
_faceCropUUID
sentinelFaceprint
TB,R,GisSentinelFaceprint
T@"NSData",R,C,V_data
T@"NSUUID",R,C,V_modelUUID
T@"NSUUID",R,C,V_faceCropUUID
initWithPoint:
_point
T{CGPoint=dd},R,V_point
encodeBool:forKey:
decodeBoolForKey:
TB,GisFaceClassificationEnabled,V_faceClassificationEnabled
TB,D,GisFaceClassificationEnabled
fileExistsAtPath:
fileHandleForReadingAtPath:
assetDirectoryPath
unpackageModelAssets:intoDirectory:error:
removeItemAtPath:error:
unTarFileWithFd:toPath:
unpackageModelAssetsAtPath:error:
pendingUpdates
setPendingUpdates:
_pendingUpdates
T@"NSObject<OS_dispatch_queue>",&,N,V_queue
T@"NSMutableSet",&,N,V_pendingUpdates
defaultSessionConfiguration
sessionWithConfiguration:delegate:delegateQueue:
URLSession:didBecomeInvalidWithError:
URLSession:didReceiveChallenge:completionHandler:
URLSessionDidFinishEventsForBackgroundURLSession:
homeKitClient
session
feedbackServiceHost
_homeKitClient
_session
_feedbackServiceHost
T@"NSURLSession",R,V_session
T@"NSString",R,V_feedbackServiceHost
T@"HMIHomeKitClient",R,V_homeKitClient
protectionSpace
host
serverTrust
credentialForTrust:
authenticationMethod
feedbackSession
_temporaryFileURLWithUUID:extension:error:
clipManager
initWithClipManager:clip:
setClipDestinationFileURL:
hmiPrivateErrorWithCode:description:suggestion:underlyingError:
setFetchVideoAssetContextCompletionBlock:
setDownloadProgressHandler:
faceClassification
person
setFaceCrops:
fetchClipWithUUID:completion:
assetWithURL:
composition
addMutableTrackWithMediaType:preferredTrackID:
insertTimeRange:ofTrack:atTime:error:
initWithAsset:presetName:
setOutputFileType:
setOutputURL:
setShouldOptimizeForNetworkUse:
isInternalInstall
setTimeRange:
exportAsynchronouslyWithCompletionHandler:
feedbackServiceURL
base64EncodedDataWithOptions:
stringByAppendingPathComponent:
requestWithURL:
setHTTPMethod:
setValue:forHTTPHeaderField:
uploadTaskWithRequest:fromFile:completionHandler:
dataWithLength:
faceCrops
_base64StringFromData:
_attachEncryptedDataUsingKey:toPayload:error:
dataWithContentsOfURL:
setAssetData:
serviceResult
_createPayloadWithServiceResult:error:
_uploadPayloadData:uploadURL:completionHandler:
_stripAudioTrackFromAsset:completionHandler:
setServiceResult:
_downloadClipWithCameraProfileUUID:clipUUID:completionHandler:
_requestPreSignedURLWithClipUUID:completionHandler:
feedbackRequestURLForClipWithUUID:
dataTaskWithURL:completionHandler:
removeItemAtURL:error:
_removeTemporaryFiles
_submitClipWithCameraProfileUUID:clipUUID:completionHandler:
initWithFeedbackSession:cameraProfileUUID:clipUUID:
_attachFaceCrops:toPayload:error:
cameraProfileUUID
clipUUID
temporaryFileURLs
assetData
_feedbackSession
_cameraProfileUUID
_clipUUID
_temporaryFileURLs
_faceCrops
_assetData
_serviceResult
T@"HMIFeedbackSession",R,V_feedbackSession
T@"NSUUID",R,V_cameraProfileUUID
T@"NSUUID",R,V_clipUUID
T@"NSMutableArray",R,V_temporaryFileURLs
T@"NSSet",&,V_faceCrops
T@"NSData",&,V_assetData
T@"NSDictionary",&,V_serviceResult
_operation
submitFeedbackWithCameraProfileUUID:clipUUID:runRemotely:completionHandler:
submitFeedbackWithCameraProfileUUID:clipUUID:completionHandler:
initWithPersonUUID:sourceUUID:sessionEntityUUID:confidence:familiarity:
initWithUUID:sourceUUID:faceBoundingBox:
initWithUUID:sourceUUID:sessionEntityUUID:faceBoundingBox:facecrop:faceprint:confidence:
decodeDoubleForKey:
encodeDouble:forKey:
initWithUUID:name:personsModelIdentifier:faceBoundingBox:
personsModelIdentifier
_personsModelIdentifier
_sourceUUID
_sessionEntityUUID
_familiarity
T@"NSString",R,V_identifier
T@"NSString",R,V_personsModelIdentifier
T@"NSUUID",R
T@"NSUUID",R,V_personUUID
T@"NSUUID",R,V_sourceUUID
T@"NSUUID",R,V_sessionEntityUUID
Tq,R,V_familiarity
summaryForHomeUUID:error:
sendEventForPersonsModels:
removeNearestFaceprint:withFaceCrops:
hmf_isEmpty
faceDistanceFromDescriptor:toDescriptor:
removeFaceCropsWithUUIDs:completion:
T@"HMIPersonFaceCrop",R,V_faceCrop
URLForResource:withExtension:
metalLibraryData
kernelWithFunctionName:fromMetalLibraryData:error:
colorKernelWithFunctionName:
kernelWithFunctionName:
thresholdFilter
addFilter
divideFilter
multiplyFilter
densityMapFilter
removePersonsModelForHomeUUID:sourceUUID:error:
initWithUUID:
supportsFaceClassification
setSupportsFaceClassification:
setPersonDataAvailableViaHomeKit:
_supportsFaceClassification
_personDataAvailableViaHomeKit
T@"NSUUID",R,C,V_homeUUID
T@"<HMIPersonManagerDataSource>",W,N,V_dataSource
TB,V_supportsFaceClassification
personDataAvailableViaHomeKit
TB,GisPersonDataAvailableViaHomeKit,V_personDataAvailableViaHomeKit
levelThresholds
characterAtIndex:
_levelThresholds
T@"NSArray",R,V_levelThresholds
Td,R,V_value
_hmiErrorWithCode:description:reason:suggestion:underlyingError:
hmiErrorWithCode:underlyingError:
hmiErrorWithCode:
hmiErrorWithCode:description:
initWithData:timeRange:
initWithInitializationSegment:separableSegment:timeRange:
timeRange
sequenceNumbers
initWithInitializationSegment:separableSegment:timeRange:sequenceNumbers:
numberWithUnsignedInt:
_ensureAttributes
videoFormatDescription
audioFormatDescription
initializationSegment
separableSegment
isCombinableWithFragment:
canFragmentData:
dataWithData:
videoTrackTimeRange
decodeCMTimeRangeForKey:
encodeCMTimeRange:forKey:
isInitializationSegment:combinableWithInitializationSegment:
fragmentData:handler:
initWithInitializationSegment:separableSegment:sequenceNumbers:
audioTrackTimeRange
_attributesLoaded
_videoFormatDescription
_audioFormatDescription
_initializationSegment
_separableSegment
_sequenceNumbers
_baseDecodeTimeStamp
_videoTrackTimeRange
_audioTrackTimeRange
_timeRange
T@"NSData",R,C
Tr^{opaqueCMFormatDescription=},R,V_videoFormatDescription
Tr^{opaqueCMFormatDescription=},R,V_audioFormatDescription
T{?={?=qiIq}{?=qiIq}},R,V_videoTrackTimeRange
T{?={?=qiIq}{?=qiIq}},R,V_audioTrackTimeRange
T{?=qiIq},R,V_baseDecodeTimeStamp
T@"NSData",R,V_initializationSegment
T@"NSData",R,V_separableSegment
T{?={?=qiIq}{?=qiIq}},R,V_timeRange
T@"NSArray",R,V_sequenceNumbers
initWithFrameDimensions:
handleMotionDetections:atTime:
sessionEntities
allKeys
trackNewFaces:personWithoutFaceEvents:personWithFilteredFaceEvents:withMotionDetection:atTime:
getUUIDToNextFaceIndexWithPreviousFaceindex:
predictedLinkedEntityUUIDs
intersectsSet:
faceQualityScore
hasMotionVectors
initWithConfidence:boundingBox:hasMotionVectors:face:
handleMotionDetection:sessionPTS:
assignSessionEntitiesToPersonWithFaceEvents:personWithoutFaceEvents:personWithFilteredFaceEvents:videoFrame:
_faceTracker
_sessionUUIDToPreviousFaceIndex
_sessionUUIDToPreviousFaceprints
_sessionEntities
T@"NSMutableDictionary",R,V_sessionEntities
configure
loadModelFromTrialWithError:
_createFontWithSize:
dictionaryWithObjectsAndKeys:
initWithString:attributes:
initWithString:
drawRectWithCGRect:scale:
drawText:at:
_sbuf
_context
_colorSpace
_font
resourceUsageMonitor
_resourceUsageMonitor
T@"<HMISystemResourceUsageMonitorProtocol>",R,V_resourceUsageMonitor
T@"<HMISystemResourceUsageMonitorDelegate>",W,Vdelegate
initWith
applyWithFrameResult:
initWithActivityZones:
personManagerUUID
initWithPersonUUID:sourceUUID:
personSourceUUIDPairFromPersonLink:
T@"NSUUID",R,C,V_sourceUUID
initWithUUID:name:personLinks:
defaultFormatter
initWithName:value:options:formatter:
personLinks
initWithUUID:name:
_personLinks
T@"NSSet",R,C,V_personLinks
initWithSourceUUID:externalLibrary:faceCountsByPerson:
isExternalLibrary
faceCountsByPerson
_externalLibrary
_faceCountsByPerson
externalLibrary
TB,R,GisExternalLibrary,V_externalLibrary
T@"NSDictionary",R,V_faceCountsByPerson
visionPersonsModel
personUniqueIdentifiers
faceCountForPersonWithUniqueIdentifier:
initWithPersonsModel:homeUUID:sourceUUID:externalLibrary:
summary
_visionPersonsModel
T@"VNPersonsModel",R,V_visionPersonsModel
T@"HMIPersonsModelSummary",R
_addValueToContainer:forKey:
_containerIsArray
_valueForNumber:
stringFromDate:
base64EncodedStringWithOptions:
_JSONObjectWithObject:options:
initWithDictionary
_addClassToContainer:
object
numberWithLongLong:
decimalNumberWithString:
JSONObjectStringWithObject:pretty:options:
JSONObjectWithObject:options:
JSONObjectStringWithObject:
allowsKeyedCoding
initWithArray
encodeInt32:forKey:
encodeInt64:forKey:
objectJSON
objectPrettyJSON
options
_container
_options
Tq,V_options
hasPrefix:
hasSuffix:
_objectWithJSONObject:allowedClasses:
classMap
initWithJSONObject:
setClassMap:
objectWithJSONData:classMap:
objectWithJSONObject:classMap:
objectWithJSONObjectString:classMap:
decodeInt32ForKey:
decodeInt64ForKey:
container
_classMap
T@,R,V_container
T@"NSDictionary",&,V_classMap
timeZoneForSecondsFromGMT:
setTimeZone:
initWithLocaleIdentifier:
faceObservationWithRequestRevision:boundingBox:andAlignedBoundingBox:
initWithData:elementCount:elementType:lengthInBytes:labelsAndConfidence:requestRevision:
setFaceId:
anyObject
initWithFaceEvent:
URLByAppendingPathExtension:
writeToURL:atomically:encoding:error:
faceObservationsFromFaceprintsForClustering:
faceObservationFromFaceprint:
estimatePersonBoundingBoxFromFaceBoundingBox:
eventsWithPersonsAndFacesMergedFromEvents:
boundingBoxForFaceTracker
_computeBlobPropertiesWithBoundingBox:personBoundingBox:dx:dy:motionVectors:isDetection:
blobID
personBoundingBox
motionMean
frameDimensions
boxDistanceToPersonBlob:
sizeDistanceToPersonBlob:
isBijectiveToPersonBlob:
isMoving
setFaceBoundingBox:
setPersonBoundingBox:
personIndices
personIndex
personIouMax
origin
target
midpoint
motion
setMotionMean:
setPosition:
initWithPersonWithFaceEvent:motionVectors:personIndex:regionOfInterest:frameDimensions:
initWithPersonWithoutFaceEvent:personIndex:regionOfInterest:frameDimensions:
initWithPersonBlob:motionVectors:personIndex:regionOfInterest:frameDimensions:
similarityToPersonBlob:
adjustFaceBoundingBoxFromPersonBlob:
isLost
setPersonIndex:
setPersonIndices:
setPersonIouMax:
_personIouMax
_personIndex
_personIndices
_blobID
_frameDimensions
_motionMean
_personBoundingBox
T{CGSize=dd},R,V_frameDimensions
T{CGRect={CGPoint=dd}{CGSize=dd}},V_faceBoundingBox
T{CGRect={CGPoint=dd}{CGSize=dd}},V_personBoundingBox
T{CGPoint=dd},V_position
T{CGVector=dd},V_motionMean
T@"NSNumber",&,V_personIndex
T@"NSMutableSet",&,V_personIndices
Tf,V_personIouMax
T@"NSUUID",R,V_blobID
initWithProjectedFaceIndex:detectedFaceIndex:score:
projectedFaceIndex
detectedFaceIndex
_projectedFaceIndex
_detectedFaceIndex
TQ,R,V_projectedFaceIndex
TQ,R,V_detectedFaceIndex
Td,R,V_score
compare:
initWithMotionVectors:regionOfInterest:time:
motionVectors
_motionVectors
T@"NSArray",R,V_motionVectors
setPreviousTime:
motionRecordsQueue
previousPersons
setPreviousPersons:
setPreviousProjectedPersonIndices:
setPreviousFilteredPersonIndices:
previousTime
indexSetWithIndexesInRange:
containsIndex:
removeIndex:
removeObjectForKey:
enumerateKeysAndObjectsUsingBlock:
enumerateIndexesUsingBlock:
subarrayWithRange:
previousProjectedPersonIndices
previousFilteredPersonIndices
_previousPersons
_previousProjectedPersonIndices
_previousFilteredPersonIndices
_motionRecordsQueue
_previousTime
T@"NSArray",&,V_previousPersons
T@"NSArray",&,V_previousProjectedPersonIndices
T@"NSArray",&,V_previousFilteredPersonIndices
T@"NSMutableArray",R,V_motionRecordsQueue
T{?=qiIq},V_previousTime
initWithAssetDuration:creationDate:firstSequenceNumber:lastSequenceNumber:
initWithAssetDuration:creationDate:firstSequenceNumber:lastSequenceNumber:nominalFrameRate:dimensions:baseDecodeTimeStamp:
initWithAssetDuration:creationDate:
_firstSequenceNumber
_nominalFrameRate
_dimensions
_assetDuration
T{?=qiIq},R,V_assetDuration
T@"NSDate",R,V_creationDate
TQ,R,V_firstSequenceNumber
Td,R,V_nominalFrameRate
T{CGSize=dd},R,V_dimensions
initWithVideoFragment:workQueue:logIdentifier:
resourceLoaderWorkQueue
assetKeys
initWithAsset:error:
setAssetReader:
_propertiesLoadedForAsset:resultCallback:
statusOfValueForKey:error:
_didKeyValueLoadFailed:
trackKeys
_propertiesLoadedForTrack:fromAsset:resultCallback:
initWithTrack:outputSettings:
setMaximizePowerEfficiency:
_validateSequentialIntegrityOfFragmentsInAsset:
_sequenceNumberOfLastFragmentInAsset:
_sequenceNumberOfFirstFragmentInAsset:
dateValue
formatDescriptions
latentBaseDecodeTimeStampOfFirstTrackFragment
outputs
currentFrameId
setCurrentFrameId:
request
finishLoadingWithError:
requestsAllDataToEndOfResource
_logIdentifier
_currentFrameId
_resourceLoaderWorkQueue
TQ,V_currentFrameId
T@"AVAssetReader",&,N,V_assetReader
T@"NSObject<OS_dispatch_queue>",R,V_resourceLoaderWorkQueue
T@"HMICameraVideoFragment",R,V_videoFragment
T@"NSString",R,V_logIdentifier
_initSessionWithError:
setMaxKeyFrameIntervalDuration:
_invalidateSession
_encodeSampleBuffer:attemptRecovery:
setExpectedDuration:
maxKeyFrameIntervalDuration
dataRateLimit
expectedFrameRate
expectedDuration
numberOfDroppedFrames
_forceKeyFrameOnNextEncodedFrame
_codecType
_realTime
_maxKeyFrameIntervalDuration
_averageBitRate
_expectedFrameRate
_expectedDuration
_numberOfDroppedFrames
_dataRateLimit
T@"<HMIVideoEncoderDelegate>",W,V_delegate
Tq,N,V_averageBitRate
Tq,N,V_maxKeyFrameIntervalDuration
T{HMIVideoEncoderDataRate=qq},N,V_dataRateLimit
Tq,N,V_expectedFrameRate
Td,N,V_expectedDuration
TQ,R,V_numberOfDroppedFrames
encoderDidEncodeSampleBuffer
encoderDidFailWithError
setEncoderDidEncodeSampleBuffer:
setEncoderDidFailWithError:
_encoderDidEncodeSampleBuffer
_encoderDidFailWithError
T@?,C,V_encoderDidEncodeSampleBuffer
T@?,C,V_encoderDidFailWithError
addOrUpdateFaceCrops:completion:
addOrUpdatePersons:completion:
fetchAllUnassociatedFaceCropsWithCompletion:
removePersonsWithUUIDs:completion:
associateFaceCropsWithUUIDs:toPersonWithUUID:completion:
addPersons:completion:
T@"HMHomePersonManager",&,V_homePersonManager
isImportingFromPhotoLibraryEnabled
isSharingFaceClassificationsEnabled
setImportingFromPhotoLibraryEnabled:
setSharingFaceClassificationsEnabled:
_importingFromPhotoLibraryEnabled
_sharingFaceClassificationsEnabled
importingFromPhotoLibraryEnabled
TB,GisImportingFromPhotoLibraryEnabled,V_importingFromPhotoLibraryEnabled
sharingFaceClassificationsEnabled
TB,GisSharingFaceClassificationsEnabled,V_sharingFaceClassificationsEnabled
TB,D,GisImportingFromPhotoLibraryEnabled
TB,D,GisSharingFaceClassificationsEnabled
sessionEntityAssignment
numberOfFaceprintsClustered
numberOfClusters
numberOfPersonsCreated
numberOfUnknownFaceprintsAssociated
faceprintingDuration
clusteringDuration
totalDuration
modelSummaries
bucketForValue:usingBuckets:
valueForKeyPath:
homeToExternalEquivalencies
externalToExternalEquivalencies
sendEventForClusteringTask:
initWithModelSummaries:homeToExternalEquivalencies:externalToExternalEquivalencies:
_modelSummaries
_homeToExternalEquivalencies
_externalToExternalEquivalencies
T@"NSSet",R,V_modelSummaries
TQ,R,V_homeToExternalEquivalencies
TQ,R,V_externalToExternalEquivalencies
initWithSourceUUID:personUUID:confidence:linkedEntityUUID:
_linkedEntityUUID
T@"NSNumber",R,V_confidence
T@"NSUUID",R,V_linkedEntityUUID
initWithPersonsModels:userDefinedPersonLinks:error:
personsModelsByHome
setMaximumIdentities:
setMaximumTrainingFaceprintsPerIdentity:
initWithConfiguration:
userDefinedPersonLinksByHome
isEqualToDictionary:
persistUserDefinedPersonLinks:forHomeUUID:error:
addFaceObservations:toPersonWithUniqueIdentifier:error:
loadModelsWithError:
homePersonsModelForHomeWithUUID:
getModelStoragePathForModel:error:
persistModel:toPath:error:
buildEquivalencyMapForPersonsModels:userDefinedPersonLinks:error:
equivalencyTablesByHome
getUserDefinedPersonLinksStoragePathForHomeUUID:error:
predictPersonFromFaceObservation:limit:canceller:error:
predictedPersonUniqueIdentifier
initWithUUIDString:
equivalencyCellForPerson:
modelFromURL:options:error:
URLByDeletingLastPathComponent
setReadOnly:
writeToURL:options:error:
getRootModelStoragePathWithError:
pathWithComponents:
getModelStoragePathForHome:error:
URLByAppendingPathComponent:isDirectory:
absoluteURL
fileHandleForReadingFromURL:error:
readDataToEndOfFile
unarchivedObjectOfClasses:fromData:error:
contentsOfDirectoryAtURL:includingPropertiesForKeys:options:error:
regularExpressionWithPattern:options:error:
lastPathComponent
numberOfMatchesInString:options:range:
modelURLsFromPath:error:
loadPersonsModelFromURL:externalLibrary:homeUUID:error:
loadUserDefinedPersonLinksForHomeUUID:error:
URLByDeletingPathExtension
pathExtension
attributesOfItemAtPath:error:
fileSize
loadModelAtPath:error:
personToEquivalencyCell
T@"HMIPersonsModelManager",R
buildPersonsModelForHomeUUID:sourceUUID:externalLibrary:faceObservationsByPerson:error:
predictHomePersonFromFaceObservation:homeUUID:error:
equivalencyCellForPerson:homeUUID:error:
_userDefinedPersonLinksByHome
_personsModelsByHome
_equivalencyTablesByHome
T@"NSDictionary",R,V_userDefinedPersonLinksByHome
T@"NSDictionary",R,V_personsModelsByHome
T@"NSDictionary",R,V_equivalencyTablesByHome
initWithName:weights:biases:
weights
biases
_weights
_biases
T@"HMIDESMutableFloatArray",R,V_weights
T@"HMIDESMutableFloatArray",R,V_biases
initWithLayerParameters:losses:
layerParameters
losses
_layerParameters
_losses
T@"NSArray",R,V_layerParameters
T@"NSArray",R,V_losses
getParameterOfType:forLayerNamed:error:
networkPath
initWithTrainingNetworkPath:inferenceInputs:inferenceOutputs:trainingInputs:trainingOutputs:trainingControlVariableName:withInitializer:error:
initWithTrainingModelDefinition:forPlatform:error:
getParametersFromLayers:fromTask:error:
getTensorNamed:
doTrainingOnData:forNumberOfEpochs:withCallback:error:
initWithTrainingNetworkPath:data:error:
trainLayers:epochs:inferenceInputs:inferenceOutputs:trainingInputs:trainingOutputs:error:
_networkPath
T@"HMIDESDataset",R,V_data
T@"NSURL",R,V_networkPath
initWithCode:message:
message
success
skipped
T@"HMIVideoAnalyzerResultOutcome",R
isSkipped
isSuccess
_message
_code
TQ,R,V_code
T@"NSString",R,V_message
T@"HMIExternalPersonManagerSettings",R,V_settings
T@"<HMIExternalPersonManagerDataSource>",W,N,V_dataSource
removePersonsModelWithRetryOnError:
external
_external
TB,R,V_external
_getAnalyzers
_updateAnalyzer:withIndex:
_logState
monitored
delay
stringWithString:
stateDescription
_usageMonitor
_usageLevel
clientWithIdentifier:
updateLevels
submitUpdateModelTask
addUpdateHandlerForNamespaceName:usingBlock:
levelForFactor:withNamespaceName:
fileValue
registerForTrialUpdates
modelPath
_trialClient
_compiledModelArchivePath
_personThresholdHigh
_personThresholdMedium
_petThresholdHigh
_petThresholdMedium
_vehicleThresholdHigh
_vehicleThresholdMedium
_faceThreshold
_modelPath
Td,R,V_personThresholdHigh
Td,R,V_personThresholdMedium
Td,R,V_petThresholdHigh
Td,R,V_petThresholdMedium
Td,R,V_vehicleThresholdHigh
Td,R,V_vehicleThresholdMedium
Td,R,V_faceThreshold
T@"NSString",R,V_modelPath
lengthInBytes
faceId
initWithFaceThreshold:singleLinkThreshold:percentConnectionsThreshold:faceprintRevision:error:
addFaceObservations:toFaceDescriptorBuffer:error:
convertToClusters:
setObjects:
setClusterId:
objects
setTotalObjectCount:
setShouldUpdateRepresentative:
dataWithBytes:length:
centermostFaceprintInCluster:faceObservations:
getClustersWithFaces:error:
.cxx_construct
_greedyClusterer
initWithConfiguration:identifier:remote:
thumbnailInterval
thumbnailHeight
transcode
eventTriggers
legacyAnalyzer
_handleFragment:withResult:events:outcome:
analyzer:didFailWithError:
initWithJPEGData:size:presentationTimeStamp:
_makeFrameResult:withPresentationTimeStamp:
analyzer:didAnalyzeFrame:
analyzer:didAnalyzeFragment:
analyzeFragment:configuration:
setMonitored:
_group
_configurationBySequenceNumber
_pendingFragments
_failed
_cancelled
_sessionDuration
_legacyAnalyzer
T@"HMICameraVideoAnalyzer",R,V_legacyAnalyzer
T@"<HMIVideoFrameSamplerDelegate>",W,V_delegate
initWithInterval:
_interval
_firstPTS
_lastIndex
frameSamplerDidSampleFrame
frameSamplerDidDropFrame
setFrameSamplerDidSampleFrame:
setFrameSamplerDidDropFrame:
_frameSamplerDidSampleFrame
_frameSamplerDidDropFrame
T@?,C,V_frameSamplerDidSampleFrame
T@?,C,V_frameSamplerDidDropFrame
initWithDetectionFps:prerollLengthSeconds:
filterWithName:
imageWithCVPixelBuffer:
currentFrameIndex
setCurrentFrameIndex:
prerollLengthSeconds
detectionFps
averageImage
addToAverage:
candidateDetector
defaultConfig
initWithReferenceImage:config:
setCandidateDetector:
updateWithFrame:
detectedPackageCandidates
packageAnalyzer:didDetectPackages:error:
scaleFilter
setScaleFilter:
candidateConfig
setCandidateConfig:
_detectionFps
_currentFrameIndex
_prerollLengthSeconds
_averageImage
_scaleFilter
_candidateConfig
_candidateDetector
T@"HMIVideoFrameIntervalSampler",R,V_sampler
Ti,R,V_detectionFps
Td,R,V_prerollLengthSeconds
T@"HMIAverageImage",R,V_averageImage
Ti,V_currentFrameIndex
T@"CIFilter",&,V_scaleFilter
T@"HMIPackageCandidateDetectorConfiguration",&,V_candidateConfig
T@"HMIPackageCandidateDetector",&,V_candidateDetector
T@"<HMIVideoPackageAnalyzerDelegate>",W,V_delegate
packageAnalyzerDidDetectPackages
setPackageAnalyzerDidDetectPackages:
_packageAnalyzerDidDetectPackages
T@?,C,V_packageAnalyzerDidDetectPackages
_numBins
_maxLaplacianScore
_minLaplacianScore
_binWidth
_maxScore
_histogram
computeJunkScoreForFacePrint:
_maxDistanceScore
_junkCentroid
cutOffThresholds
maximumDifferences
computeScoreWithYaw:laplacian:detectorConfidence:boxSize:
_cutOffThresholds
_maximumDifferences
T@"NSDictionary",R,V_weights
T@"NSDictionary",R,V_cutOffThresholds
T@"NSDictionary",R,V_maximumDifferences
thumbnails
_thumbnails
_outcome
T@"HMIVideoFragment",R,V_fragment
T@"NSArray",R,V_thumbnails
T@"HMIVideoAnalyzerResultOutcome",R,V_outcome
T@"HMIVideoAnalyzerDynamicConfiguration",R,V_configuration
decodeCMTimeForKey:
encodeCMTime:forKey:
decodeIntForKey:
encodeInt:forKey:
initWithPoints:isInclusion:
isInclusion
overlapsWithElipseInsideRect:
activityZoneType
overlapsWithElipseInsideRect:withInsetPercentage:
checkIfObjectIsStaticWithBoundingBox:motionDetection:eventClass:
containsEvent:withInsetPercentage:
stringByDeletingPathExtension
jsonReperesentaionOfDetectedObject:motionDetection:eventClass:
na_all:
characterSetWithCharactersInString:
componentsSeparatedByCharactersInSet:
submitCoreAnalyticsEvent:filteringLevel:numberOfDetectedObjects:
activityZonesFromString:isInclusion:
initWithPoints:
containsVectorWithSource:destination:
_inclusion
_points
T@"NSArray",R,C,V_points
inclusion
TB,R,GisInclusion,V_inclusion
minFrameQuality
minFrameScale
setThumbnailInterval:
setThumbnailHeight:
setMaxFragmentAnalysisDuration:
setMaxFragmentDuration:
setTranscode:
setMinFrameQuality:
setMinFrameScale:
setCamera:
_transcode
_minFrameQuality
_minFrameScale
_thumbnailHeight
_thumbnailInterval
T{?=qiIq},V_thumbnailInterval
TQ,V_thumbnailHeight
Td,V_maxFragmentAnalysisDuration
T{?=qiIq},V_maxFragmentDuration
TB,V_transcode
T@"HMICamera",&,V_camera
Td,V_minFrameQuality
Td,V_minFrameScale
setActivityZones:
setEventTriggers:
_eventTriggers
Tq,V_eventTriggers
T@"NSArray",&,V_activityZones
raise:format:
initWithBoundingBox:
applyEventTypeAndCheckIfSubBoundingIsStatic:forMetric:eventType:confidence:
T{CGRect={CGPoint=dd}{CGSize=dd}},R,V_boundingBox
_finish
didAnalyzeFragment
didFailAnalysisForFragment
didFindSignificantEvent
didNotAnalyzeFragment
setDidAnalyzeFragment:
setDidFailAnalysisForFragment:
setDidNotAnalyzeFragment:
setDidFindSignificantEvent:
_didAnalyzeFragment
_didFailAnalysisForFragment
_didFindSignificantEvent
_didNotAnalyzeFragment
T@?,C,N,V_didAnalyzeFragment
T@?,C,N,V_didFailAnalysisForFragment
T@?,C,N,V_didFindSignificantEvent
T@?,C,N,V_didNotAnalyzeFragment
strongToWeakObjectsMapTable
nextRequestID
setNextRequestID:
runRemotely
setPreferenceOverrideFromDictionary:
getNextRequestID
requests
expectedClasses
requestAnalysisForPixelBuffer:withProperties:andCompletionHandler:
cancelRequest:
_runRemotely
_nextRequestID
_requests
Ti,V_nextRequestID
T@"NSMapTable",R,V_requests
TB,V_runRemotely
initWithConfidence:boundingBox:hasMotionVectors:
hasEstimatedBoundingBox
initWithConfidence:boundingBox:face:
_isBoundingBoxEstimated
_face
isBoundingBoxEstimated
TB,R,GhasEstimatedBoundingBox,V_isBoundingBoxEstimated
T{CGRect={CGPoint=dd}{CGSize=dd}},R
T@"HMIVideoAnalyzerEventFace",R,V_face
flattenTrainingResult:
encryptedDataWithPublicKey:inPlaceDataFloatNumbers:count:error:
packageTrainingResult:privatize:maxNorm:normBinCount:encryptionKey:error:
isProductTypeB520
isProductTypeJ105
preferenceCache
preferenceOverridesInternal
preferenceLoggedValues
initWithKey:options:domain:defaultValue:
systemPreferenceValueForKey:
logPreferenceForKey:value:
caseInsensitiveCompare:
pretendProductTypeIsUnknown
setPretendProductTypeIsUnknown:
T@"HMIPreference",R
addPreferenceOverrideFromDictionary:
removeAllPreferenceOverrides
numberPreferenceForKey:defaultValue:withParser:
valuePreferenceForKey:defaultValue:withParser:
preferenceCacheFlushTimer
_preferenceCacheFlushTimer
_preferenceCache
_preferenceLoggedValues
_preferenceOverridesInternal
T@"HMFTimer",R,V_preferenceCacheFlushTimer
T@"NSMutableDictionary",R,N,V_preferenceCache
T@"NSMutableDictionary",R,N,V_preferenceLoggedValues
T@"NSMutableDictionary",R,N,V_preferenceOverridesInternal
setNumberStyle:
numberFromString:
initWithDataSource:person:
personFaceCrops
_person
_personFaceCrops
T@"HMIPerson",R,V_person
T@"NSSet",R,V_personFaceCrops
inputImage
referenceImage
threshold
applyWithExtent:arguments:
initWithInputImage:referenceImage:threshold:
_inputImage
_referenceImage
_threshold
T@"CIImage",R,V_inputImage
T@"CIImage",R,V_referenceImage
Td,R,V_threshold
token
notificationName
publishValueForToken:
publishInitialValue
setToken:
callback
initWithNotificationName:andQueue:andCallback:
_token
_notificationName
_callback
T@"NSObject<OS_dispatch_queue>",R,N,V_queue
T@?,R,N,V_callback
Ti,N,V_token
Tr*,R,N,V_notificationName
initWithDeltaEThreshold:densityThresholdPreAverage:temporalDensityThreshold:densityMapScale:temporalAverageDecay:boundingBoxConfig:
deltaEThreshold
densityThresholdPreAverage
temporalDensityThreshold
densityMapScale
temporalAverageDecay
boundingBoxConfig
_deltaEThreshold
_densityThresholdPreAverage
_temporalDensityThreshold
_densityMapScale
_temporalAverageDecay
_boundingBoxConfig
Tf,R,V_deltaEThreshold
Tf,R,V_densityThresholdPreAverage
Tf,R,V_temporalDensityThreshold
Tf,R,V_densityMapScale
Tf,R,V_temporalAverageDecay
T@"HMIBoundingBoxExtractorConfiguration",R,V_boundingBoxConfig
metalCommandBuffer
device
initWithDevice:
setOffset:
metalTexture
encodeToCommandBuffer:sourceTexture:destinationTexture:
applyWithExtent:roiCallback:arguments:
initWithDecay:
applyWithExtent:inputs:arguments:error:
temporalAverageImage
boxExtractor
_temporalAverageImage
_boxExtractor
T@"HMIAverageImage",R,V_temporalAverageImage
T@"HMIBoundingBoxExtractor",R,V_boxExtractor
T@"HMIPackageCandidateDetectorConfiguration",R,V_config
setMaximumFractionDigits:
stringFromNumber:
URLForDirectory:inDomain:appropriateForURL:create:error:
setNumberOfFaceprintsClustered:
setNumberOfClusters:
setNumberOfPersonsCreated:
setNumberOfUnknownFaceprintsAssociated:
setFaceprintingDuration:
setClusteringDuration:
setTotalDuration:
setError:
_numberOfFaceprintsClustered
_numberOfClusters
_numberOfPersonsCreated
_numberOfUnknownFaceprintsAssociated
_faceprintingDuration
_clusteringDuration
_totalDuration
Tq,V_numberOfFaceprintsClustered
Tq,V_numberOfClusters
Tq,V_numberOfPersonsCreated
Tq,V_numberOfUnknownFaceprintsAssociated
Td,V_faceprintingDuration
Td,V_clusteringDuration
Td,V_totalDuration
T@"NSError",&,V_error
_stageZero_expireUnnamedPersons
isCancelled
personsModelManager
hmf_isEqualToUUID:
removePerson:
personCreatedDateFromFaceCrops:
_stageOne_fetchFaceCrops
_stageTwo_fetchFaceprints:
_stageThree_generateFaceprintsForFaceCrops:existingFaceprints:
_stageFour_clusterFaceprints:
_stageFive_addPersons:clusterMapping:faceprints:
_stageSix_associateFaceCropsWithClusterMapping:faceprints:
startTime
_clusterer
_faceClassifier
_personsModelManager
_summary
_startTime
T@"HMIClusteringTaskSummary",R,V_summary
T@"NSDate",R,V_startTime
T@"HMIPersonsModelManager",R,V_personsModelManager
waitUntilFinished
limitEnforcedSubsetFromPersons:
initWithDataSource:faceCropUUIDs:
faceprints
initWithDataSource:faceprints:
initWithDataSource:faceprintUUIDs:
shouldRemoveExcessFaceCrops
subsampleFacesForPersons:withFaceObservationsMap:dataSource:vnUUIDToFaceCropUUIDMap:
trainingFaceObservationsForPersonWithUniqueIdentifier:canceller:error:
na_setByRemovingObjectsFromSet:
_removeExcessFaceCrops
removeExcessFaceCrops
TB,R,GshouldRemoveExcessFaceCrops,V_removeExcessFaceCrops
faceCropUUIDs
_faceCropUUIDs
_faceprints
T@"NSSet",R,V_faceCropUUIDs
T@"NSSet",R,V_faceprints
T@"NSSet",R,V_faceprintUUIDs
blackImage
numImages
setNumImages:
currentAverage
decay
createCGImage:fromRect:format:colorSpace:deferred:
imageWithCGImage:options:
setCurrentAverage:
setDecay:
_decay
_currentAverage
_numImages
T@"CIImage",&,V_currentAverage
Tq,V_numImages
Tf,V_decay
initWithFrame:size:
cropRect
_cropRect
T{CGRect={CGPoint=dd}{CGSize=dd}},R,V_cropRect
T@"HMICameraVideoFrame",R,W,V_frame
T@"NSError",R,V_error
mapTableWithKeyOptions:valueOptions:
setCountLimit:
significantActivityDetector
regionOfInterestOperations
regionOfInterestOperationQueue
sessionEntityManagers
_analyzerEventsFromObjectDetections:
_simulatedEventForEventClass:
_targetEventsSetFromTargetEventTypes:enableFaceClassification:
_filterEvents:targetEventClasses:
_filterEvents:withMotionDetections:cropRectNormalized:
_eventsWithFaceClassificationAppliedFromEvents:videoFrame:sessionIdentifier:homeUUID:error:
eventsWithContentsOfFile:
compressedFrameWithScale:quality:error:
saveDESRecordVideoFrame:withResult:
lowercaseString
highConfidenceThresholds
mediumConfidenceThresholds
_rankForEventClass:
_createStationaryEventFromEvent:
confidenceLevel
faceClassifier
transaction
setTransaction:
_mediumConfidenceThresholds
_highConfidenceThresholds
_regionOfInterestOperationQueue
_regionOfInterestOperations
_significantActivityDetector
_transaction
_sessionEntityManagers
T@"NSDictionary",R,V_mediumConfidenceThresholds
T@"NSDictionary",R,V_highConfidenceThresholds
T@"NSOperationQueue",R,V_regionOfInterestOperationQueue
T@"NSMapTable",R,V_regionOfInterestOperations
T@"HMISignificantActivityDetector",R,V_significantActivityDetector
T@"HMIFaceClassifierVIP",R,V_faceClassifier
T@"HMFOSTransaction",&,N,V_transaction
T@"NSCache",R,V_sessionEntityManagers
taskIdentifier
taskRunnerClass
initWithURL:
activityForScheduling
T@"NSURL",R,V_url
pixelBufferFrameWithError:
printWithHeight:
_store
_presentationTimeStamp
T{?=qiIq},R,V_presentationTimeStamp
initWithContentsOfFile:options:error:
fileDescriptor
availableData
unpackageTarData:size:parentDir:
getDataOutWithSize:withFlag:fd:
uncompressedContentsForCompressedFile:outPath:
capacity
condition
isFull
wait
setSize:
sampleBufferDelay
buffer:willHandleSampleBuffer:
signal
unlock
bufferWillFlush:
bufferWillFinish:
fillRatio
isEmpty
handleBlock:
videoDuration
delegateQueue
_capacity
_condition
_sampleBufferDelay
_delegateQueue
_videoDuration
TQ,V_size
T@"NSCondition",R,V_condition
T@"HMITimeIntervalAverage",R,V_sampleBufferDelay
T@"<HMIVideoCommandBufferDelegate>",W,V_delegate
T@"NSObject<OS_dispatch_queue>",R,V_delegateQueue
T{?=qiIq},R,V_videoDuration
TQ,R,V_capacity
Tf,R
bufferWillHandleSampleBuffer
bufferWillFinish
bufferWillFlush
setBufferWillHandleSampleBuffer:
setBufferWillFlush:
setBufferWillFinish:
_bufferWillHandleSampleBuffer
_bufferWillFlush
_bufferWillFinish
T@?,C,V_bufferWillHandleSampleBuffer
T@?,C,V_bufferWillFlush
T@?,C,V_bufferWillFinish
dataWithContentsOfFile:options:error:
_hasMotionVectors
_userInfo
T@"HMIConfidence",R,V_confidence
T@"NSDictionary",R,V_userInfo
TB,R,V_hasMotionVectors
_drainBuffer:
_failWithDescription:
decoder:didDecodeSampleBuffer:
_invalidateSession:
handleSampleBuffer:outputFrame:
_createSessionWithFormatDescription:
decoder:didFailWithError:
_didDecodeSampleBuffer:
_reorderBufferDidBecomeFull
_lastSampleBufferPTS
_lastSampleBufferDTS
_semaphore
T@"<HMIVideoDecoderDelegate>",W,V_delegate
decoderDidDecodeSampleBuffer
decoderDidFailWithError
setDecoderDidDecodeSampleBuffer:
setDecoderDidFailWithError:
_decoderDidDecodeSampleBuffer
_decoderDidFailWithError
T@?,C,V_decoderDidDecodeSampleBuffer
T@?,C,V_decoderDidFailWithError
_checkNotStarted
typeWithIdentifier:
initWithContentType:
setOutputFileTypeProfile:
setPreferredOutputSegmentInterval:
setInitialSegmentStartTime:
setInitialMovieFragmentSequenceNumber:
setProducesCombinableFragments:
initWithMediaType:outputSettings:sourceFormatHint:
setExpectsMediaDataInRealTime:
setMediaTimeScale:
canAddInput:
addInput:
assetWriterInputWithMediaType:outputSettings:sourceFormatHint:
startWriting
assetWriter:didOutputInitializationSegment:
trackReports
mediaType
firstVideoSampleInformation
assetWriter:didOutputSeparableSegment:timeRange:
_appendSampleBuffer:
_createAssetWriterWithInitialSegmentStartTime:
startSessionAtSourceTime:
allowRecovery
isReadyForMoreMediaData
_removeTrimDurationAttachmentsFromAudioSampleBuffer:
allowRecoveryFromInsufficientAudioTrim
_ensureFirstAudioSampleBufferHasSufficientPrimingTrim:
appendSampleBuffer:
flushSegment
assetWriter:didFailWithError:
assetWriter:didOutputSegmentData:segmentType:segmentReport:
assetWriter:didOutputSegmentData:segmentType:
nextSequenceNumber
setAllowRecoveryFromInsufficientAudioTrim:
_skipInitializationSegment
_dropSamplesUntilSync
_dropTrimDurationAttachments
_outputQueue
_videoFormat
_audioFormat
_allowRecovery
_allowRecoveryFromInsufficientAudioTrim
_nextSequenceNumber
T@"<HMIVideoAssetWriterDelegate>",W,V_delegate
TB,V_allowRecovery
TQ,V_nextSequenceNumber
TB,V_allowRecoveryFromInsufficientAudioTrim
assetWriterDidOutputInitializationSegment
assetWriterDidOutputSeparableSegment
assetWriterDidFailWithError
setAssetWriterDidFailWithError:
_assetWriterDidOutputInitializationSegment
_assetWriterDidOutputSeparableSegment
_assetWriterDidFailWithError
T@?,C,V_assetWriterDidOutputInitializationSegment
T@?,C,V_assetWriterDidOutputSeparableSegment
T@?,C,V_assetWriterDidFailWithError
inputs
faceObservationsForPersonWithUniqueIdentifier:error:
setByAddingObject:
facesAreSamePersonFromSet:andSet:
setByAddingObjectsFromSet:
facesAreSamePersonFromSet:andSet:distanceThreshold:percentMatchingThreshold:
_personToEquivalencyCell
T@"NSDictionary",R,V_personToEquivalencyCell
updatePersonsModelWithRetryOnError:
initWithOrigin:motion:
distance
eventType
setEventType:
_eventType
_origin
_motion
T{CGPoint=dd},R,V_origin
T{CGPoint=dd},R
T{CGVector=dd},R,V_motion
Tq,V_eventType
initWithPixelBuffer:atTime:
classMotionScoreMap
classPaddingMap
scoreForSubBoundingBox:forMetric:eventType:confidence:
initWithBoundingBox:size:motionVectors:motionScore:
motionScore
_motionScore
Tf,R,V_motionScore
applyActivityZoneFilteringOnSourcePoint:destinationPoint:activityZones:
_computeOpticalFlow:with:globalMotionScore:activityZones:
_frames
T@"NSMutableArray",R,V_frames
sbuf
initWithSampleBuffer:score:detections:
T^{opaqueCMSampleBuffer=},R,V_sbuf
setSampleRate:
_handleReference:target:
_drainCandidatesThatExpiredBefore:
_ensureDetectorForPixelBuffer:
frameSelector:didSelectFrame:detections:
_candidates
_enabled
_referenceInterval
_expirationInterval
_reference
T@"<HMIVideoFrameSelectorDelegate>",W,V_delegate
frameSelectorDidSelectFrame
setFrameSelectorDidSelectFrame:
_frameSelectorDidSelectFrame
T@?,C,V_frameSelectorDidSelectFrame
initWithFaceCrop:faceprint:classifications:
initWithFaceCrop:faceprint:classifications:predictedLinkedEntityUUIDs:
_classifications
_faceQualityScore
_predictedLinkedEntityUUIDs
_sessionEntityAssignment
T@"NSSet",R,V_predictedLinkedEntityUUIDs
Tq,R,V_sessionEntityAssignment
T@"NSSet",R,V_classifications
Td,R,V_faceQualityScore
analyzerWithConfiguration:identifier:legacy:remote:error:
handleAssetData:withOptions:completionHandler:
analyzerWithOptions:error:
analyzerWithConfiguration:identifier:error:
handleAssetData:withOptions:errorHandler:
handleMessageWithOptions:completionHandler:
setEncode:
encode
T@"HMIVideoAnalyzerConfiguration",R,C,V_configuration
T@"NSDictionary",R,C,V_options
Td,N
TB,N
T@"<HMIVideoAnalyzerDelegate>",W,V_delegate
TQ,R,V_status
commandBuffer
_notifyDelegateDidFailWithError:
dynamicConfigurationBuffer
_prepareForInputVideoFormat:
_prepareForInputAudioFormat:
_ensureEncoder
handleSampleBuffer:errorHandler:
_handleDecodedSampleBuffer:
timeline
inputAudioFormat
inputVideoFormat
outputVideoFormat
_configureAssetWriter
setAssetWriter:
setEncoder:
_prepareForOutputVideoFormat:
decoder
_configureEncoder
dynamicConfigurationForTime:
frameThumbnailSampler
packageAnalyzer
frameAnalyzerResultBuffer
_notifyDelegateDidAnalyzeFrame:
thumbnailBuffer
setInitializationSegment:
_notifyDelegateDidAnalyzeFragment:
_produceResult:withArguments:
analyzer:didProduceResult:
startDate
setInputVideoFormat:
setInputAudioFormat:
setOutputVideoFormat:
currentPTS
setCurrentPTS:
currentDTS
setCurrentDTS:
_numDecodedSamples
_numDidAnalyzeFrames
_numDidAnalyzeFragments
_monitored
_encode
_inputVideoFormat
_inputAudioFormat
_outputVideoFormat
_commandBuffer
_decoder
_frameThumbnailSampler
_frameAnalyzerResultBuffer
_thumbnailBuffer
_dynamicConfigurationBuffer
_packageAnalyzer
_startDate
_currentPTS
_currentDTS
Tr^{opaqueCMFormatDescription=},V_inputVideoFormat
Tr^{opaqueCMFormatDescription=},V_inputAudioFormat
Tr^{opaqueCMFormatDescription=},V_outputVideoFormat
T@"HMIVideoCommandBuffer",R,V_commandBuffer
T@"HMIVideoDecoder",R,V_decoder
T@"HMIVideoFrameSampler",R,V_frameThumbnailSampler
T@"HMIVideoEncoder",&,V_encoder
T@"HMIVideoFrameSelector",R,V_frameSelector
T@"HMIVideoFrameAnalyzer",R,V_frameAnalyzer
T@"HMIVideoAssetWriter",&,V_assetWriter
T{?=qiIq},V_currentPTS
T{?=qiIq},V_currentDTS
T@"HMIVideoEventBuffer",R,V_frameAnalyzerResultBuffer
T@"HMIVideoEventBuffer",R,V_thumbnailBuffer
T@"NSData",&,V_initializationSegment
T@"HMIVideoEventBuffer",R,V_dynamicConfigurationBuffer
T@"HMIVideoPackageAnalyzer",R,V_packageAnalyzer
T@"HMIVideoAnalyzerScheduler",R,V_scheduler
T@"HMIVideoTimeline",R,V_timeline
T@"NSDate",R,V_startDate
analyzerDidAnalyzeFrame
analyzerDidAnalyzeFragment
analyzerDidFailWithError
setAnalyzerDidAnalyzeFrame:
setAnalyzerDidAnalyzeFragment:
setAnalyzerDidFailWithError:
_analyzerDidAnalyzeFrame
_analyzerDidAnalyzeFragment
_analyzerDidFailWithError
T@?,C,V_analyzerDidAnalyzeFrame
T@?,C,V_analyzerDidAnalyzeFragment
T@?,C,V_analyzerDidFailWithError
getStoragePath
getResourceValue:forKey:error:
dataWithContentsOfURL:options:error:
_sourceURL
T@"NSURL",R,V_sourceURL
@16@0:8
B24@0:8@16
#16@0:8
@24@0:8:16
@32@0:8:16@24
@40@0:8:16@24@32
B16@0:8
B24@0:8#16
B24@0:8:16
Vv16@0:8
Q16@0:8
^{_NSZone=}16@0:8
B24@0:8@"Protocol"16
@"NSString"16@0:8
v24@0:8@?16
v32@0:8@16@?24
v24@0:8@?<v@?@"NSSet"@"NSError">16
v32@0:8@"NSSet"16@?<v@?@"NSSet"@"NSError">24
v24@0:8@?<v@?@"NSError">16
v32@0:8@"NSSet"16@?<v@?@"NSError">24
v24@0:8@?<v@?@"HMIExternalPersonManagerSettings"@"NSError">16
@"HMFLogCategory"16@0:8
@24@0:8@16
v24@0:8@16
v16@0:8
@"NSMutableArray"
@"NSMutableSet"
@"HMIDESMutableFloatArray"
B48@0:8@16@24@32^@40
i20@0:8i16
i16@0:8
@"MLFeatureValue"24@0:8@"NSString"16
@"NSSet"16@0:8
@32@0:8^{__CVBuffer=}16@24
^{__CVBuffer=}16@0:8
^{__CVBuffer=}
@"NSString"
@48@0:8@16@24@32^@40
B40@0:8^{__CVBuffer=}16@24^@32
B64@0:8^{__CVBuffer=}16@24@32@40@48^@56
v56@0:8@16@24@32@40@48
{CGSize=dd}16@0:8
[91d]
[6[6{CGSize="width"d"height"d}]]
[6Q]
@"MLModel"
@"NSArray"
@"HMINMSConfiguration"
@"MLPredictionOptions"
{CGSize="width"d"height"d}
q16@0:8
v24@0:8q16
@"HMISystemResourceUsage"16@0:8
@"<HMISystemResourceUsageMonitorDelegate>"16@0:8
v24@0:8@"<HMISystemResourceUsageMonitorDelegate>"16
@"HMISystemResourceUsageMonitorImpl"
@"NSObject<OS_dispatch_queue>"
@44@0:8f16f20f24f28f32f36f40
f16@0:8
B72@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16f48f52{CGSize=dd}56
@48@0:8@16f24f28{CGSize=dd}32
{CGRect={CGPoint=dd}{CGSize=dd}}48@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16
@28@0:8@16f24
@52@0:8@16f24f28f32{CGSize=dd}36
@"HMIBoundingBoxExtractorConfiguration"
@56@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24
@64@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24@56
@88@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24@56@64@72@80
@"HMIFaceRecognition"
@"NSNumber"
I16@0:8
v24@0:8Q16
@24@0:8Q16
@"NSData"
{?=qiIq}16@0:8
@48@0:8@16{?=qiIq}24
{?="value"q"timescale"i"flags"I"epoch"q}
@24@0:8q16
@68@0:8{?={?=qiIq}{?=qiIq}}16B64
@64@0:8{?={?=qiIq}{?=qiIq}}16
@"HMFUnfairLock"
@48@0:8{?=qiIq}16@40
@"NSDate"
v48@0:8@16{?=qiIq}24
@40@0:8{?=qiIq}16
d40@0:8{?=qiIq}16
@"HMIVideoEventBuffer"
v40@0:8{?=qiIq}16
d16@0:8
@"HMIVideoTimeline"
@"HMITimeIntervalAverage"
v24@0:8d16
d32@0:8d16d24
@"MovingAverage"
v24@0:8@"HMFTimer"16
@32@0:8@16@24
@"<HMIHomePersonManagerDataSource>"
@"HMIHomePersonManagerSettings"
@"NSOperationQueue"
@"HMFTimer"
@"NSMutableDictionary"
@"HMIFaceCrop"
@"HMIFaceprint"
@24@0:8^{__CVBuffer=}16
@64@0:8^{__CVBuffer=}16{?=qiIq}24Q48Q56
@80@0:8@16{?=qiIq}24Q48Q56{CGSize=dd}64
@28@0:8f16^{CGSize=dd}20
^{__CVBuffer=}64@0:8^{__CVBuffer=}16{CGRect={CGPoint=dd}{CGSize=dd}}24^@56
^{__CVBuffer=}48@0:8^{__CVBuffer=}16{CGSize=dd}24^@40
^{__CVBuffer=}60@0:8^{__CVBuffer=}16{CGSize=dd}24I40q44^@52
^{__CVBuffer=}44@0:8^{__CVBuffer=}16I24q28^@36
^{__CVBuffer=}92@0:8^{__CVBuffer=}16{CGRect={CGPoint=dd}{CGSize=dd}}24{CGSize=dd}56I72q76^@84
@40@0:8^{__CVBuffer=}16f24f28^@32
^{__CVBuffer=}32@0:8^{CGDataProvider=}16^@24
^{__CVBuffer=}32@0:8@16^@24
^{__CVBuffer=}100@0:8^{__CVBuffer=}16{CGRect={CGPoint=dd}{CGSize=dd}}24{CGSize=dd}56I72q76@84^@92
^{__CVBuffer=}40@0:8{CGSize=dd}16I32B36
{CGRect={CGPoint=dd}{CGSize=dd}}80@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{CGSize=dd}48{CGSize=dd}64
{CGRect={CGPoint=dd}{CGSize=dd}}68@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{CGSize=dd}48i64
{CGRect={CGPoint=dd}{CGSize=dd}}68@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{CGSize=dd}48f64
^{__CVBuffer=}92@0:8^{__CVBuffer=}16f24{CGRect={CGPoint=dd}{CGSize=dd}}28{CGSize=dd}60Q76^@84
@24@0:8^{__IOHIDServiceClient=}16
^{__IOHIDServiceClient=}
B28@0:8i16^d20
B24@0:8^d16
^{__IOHIDEventSystemClient=}
@32@0:8@16B24B28
B32@0:8@16B24B28
^{opaqueCMSampleBuffer=}24@0:8@16
^{opaqueCMSampleBuffer=}16@0:8
^{opaqueCMSampleBuffer=}24@0:8^Q16
@"AVAsset"
@"AVAssetReader"
^{__CFArray=}
@"HMHomeManager"
v32@0:8@16Q24
v32@0:8@16@24
v32@0:8@"HMHomeManager"16Q24
v24@0:8@"HMHomeManager"16
v32@0:8@"HMHomeManager"16@"HMHome"24
v32@0:8@"HMHomeManager"16@"HMAddAccessoryRequest"24
@?16@0:8
@28@0:8@16B24
@"NSUUID"
@"MLMultiArray"
@40@0:8@16@24^@32
B40@0:8@16^d24^@32
@"NSError"
v24@0:8^{opaqueCMSampleBuffer=}16
@"HMIDESBackgroundTask"
@"HMPhotosPersonManager"
@32@0:8q16@24
@24@0:8^{_NSZone=}16
v24@0:8@"NSCoder"16
@24@0:8@"NSCoder"16
@80@0:8@16@24@32{CGRect={CGPoint=dd}{CGSize=dd}}40@72
@64@0:8@16@24{CGRect={CGPoint=dd}{CGSize=dd}}32
@24@0:8#16
{CGRect={CGPoint=dd}{CGSize=dd}}16@0:8
@"HMIVideoFrame"
@"NSSet"
{CGRect="origin"{CGPoint="x"d"y"d}"size"{CGSize="width"d"height"d}}
B32@0:8@16@24
B32@0:8@"AVAssetResourceLoader"16@"AVAssetResourceLoadingRequest"24
B32@0:8@"AVAssetResourceLoader"16@"AVAssetResourceRenewalRequest"24
v32@0:8@"AVAssetResourceLoader"16@"AVAssetResourceLoadingRequest"24
B32@0:8@"AVAssetResourceLoader"16@"NSURLAuthenticationChallenge"24
v32@0:8@"AVAssetResourceLoader"16@"NSURLAuthenticationChallenge"24
q24@0:8@16
@96@0:8q16@24@32@40{?=qiIq}48@72q80Q88
@80@0:8@16@24{?=qiIq}32@56q64Q72
q24@0:8q16
B28@0:8@16B24
v20@0:8f16
@"NSDictionary"
@"HMICameraVideoFragment"
@52@0:8Q16Q24d32d40B48
@60@0:8Q16Q24d32d40B48@52
v20@0:8B16
@"HMICamera"
@40@0:8q16@24@32
@"HMICameraVideoFrame"
@88@0:8@16q24@32@40{CGRect={CGPoint=dd}{CGSize=dd}}48@80
@40@0:8Q16@24@32
@56@0:8Q16@24@32q40@48
@48@0:8Q16@24@32q40
@40@0:8Q16@24q32
@56@0:8Q16@24q32@40@48
@"NSURL"
@40@0:8q16q24@32
v40@0:8@16@24@32
@"HMICameraVideoAnalyzerResult"
@"HMICameraVideoAnalyzer"
v32@0:8@16^{opaqueCMSampleBuffer=}24
v32@0:8@"HMIVideoEncoder"16^{opaqueCMSampleBuffer=}24
v32@0:8@"HMIVideoEncoder"16@"NSError"24
v32@0:8@"HMIVideoRetimer"16^{opaqueCMSampleBuffer=}24
v32@0:8@"HMIVideoFrameSampler"16^{opaqueCMSampleBuffer=}24
v72@0:8@16@24{?=qiIq}32{CGSize=dd}56
v32@0:8@"HMICameraVideoFrameSelector"16@"HMICameraVideoFrame"24
v72@0:8@"HMICameraVideoFrameSelector"16@"NSArray"24{?=qiIq}32{CGSize=dd}56
@40@0:8@16@24d32
B40@0:8@16q24^@32
B32@0:8Q16Q24
@"HMICameraVideoResourceAttributes"
@"HMIVideoEncoder"
@"HMIVideoRetimer"
@"HMIVideoFrameSampler"
@"HMIVideoAssetWriter"
@"HMICameraVideoPosterFrameGenerator"
@"HMICameraVideoFrameSelector"
@"HMICameraVideoAssetReader"
v32@0:8@16q24
@"NSPointerArray"
@"HMISystemResourceUsageMonitor"
q40@0:8q16q24@32
v28@0:8@16B24
v40@0:8@16q24@32
v32@0:8q16@24
v40@0:8q16@24@32
v48@0:8q16@24@32@40
B32@0:8@16^q24
v48@0:8@16@24@32@40
@40@0:8@16@24@32
B48@0:8@16@24^@32^@40
v48@0:8@16q24@32@40
[7i]
[3i]
@"<HMICameraVideoAnalyzerDelegate>"
@"HMIHomePersonManager"
@"HMICameraVideoAnalyzerHistory"
@"HMIVideoAnalyzer"
@"HMICameraVideoAnalyzerRequest"
@"HMICameraVideoAnalyzerScheduler"
@"HMICameraVideoAnalyzerConfiguration"
@"HMIAnalysisService"
@32@0:8r^f16Q24
v32@0:8r^f16Q24
r^f16@0:8
^f16@0:8
@20@0:8f16
@"NSMutableData"
@52@0:8@16^{__CVBuffer=}24B32@36^@44
@"HMIVideoAnalyzerEventFace"52@0:8@"HMIVideoAnalyzerEventFace"16^{__CVBuffer=}24B32@"NSUUID"36^@44
@24@0:8^@16
@64@0:8@16d24d32d40d48^@56
@"HMIFaceprinter"
@"HMIFaceQualityFilterSVM"
B24@0:8^@16
@60@0:8@16q24B32@36@44^@52
v72@0:8@16{?=qiIq}24{CGSize=dd}48@64
@32@0:8@16d24
@"<HMICameraVideoFrameAnalyzer>"
@"HMICIFilterAttributeValue"
^{__CVBuffer=}24@0:8^@16
@56@0:8@16@24@32@40@48
@32@0:8Q16^@24
@48@0:8@16@24@32@40
@76@0:8i16d20{CGRect={CGPoint=dd}{CGSize=dd}}28@60@68
f80@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{CGRect={CGPoint=dd}{CGSize=dd}}48
v48@0:8@16@24d32q40
@72@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24{CGSize=dd}56
B32@0:8^{opaqueCMSampleBuffer=}16@24
@"<HMIVideoFrameAnalyzerDelegate>"
v40@0:8@"HMIVideoFrameAnalyzer"16@"HMIVideoFrameAnalyzerResult"24@"NSError"32
@64@0:8@16{?=qiIq}24Q48Q56
@48@0:8{?=qiIq}16Q40
B32@0:8@16^@24
@"HMICameraVideoPosterFrameGeneratorInput"
@48@0:8f16f20q24q32q40
@32@0:8@16^@24
@36@0:8@16#24f32
{CGRect={CGPoint=dd}{CGSize=dd}}24@0:8@16
@72@0:8@16@24@32{CGRect={CGPoint=dd}{CGSize=dd}}40
^{opaqueCMSampleBuffer=}
@"<HMIVideoRetimerDelegate>"
@20@0:8i16
@28@0:8i16d20
@36@0:8i16@20d28
i32@0:8@16@?24
B20@0:8i16
v20@0:8i16
@"NSArray"16@0:8
@"<HMIPersonManagerDataSource>"
v32@0:8@"HMICameraVideoFrameSampler"16@"HMICameraVideoFrame"24
v40@0:8@"HMICameraVideoFrameSampler"16@"HMICameraVideoFrame"24@"HMICameraVideoFrame"32
@56@0:8@16{?=qiIq}24@48
@"<HMICameraVideoFrameSelectorDelegate>"
@"HMICameraVideoFrameSampler"
@"<HMIMotionDetector>"
@72@0:8@16{?=qiIq}24{?=qiIq}48
@"<HMICameraVideoFrameSamplerDelegate>"
q20@0:8i16
@36@0:8^{__CVBuffer=}16B24^@28
^{__CVBuffer=}48@0:8@16^{__CVBuffer=}24@32^@40
@32@0:8^{__CVBuffer=}16^@24
@"NSArray"32@0:8^{__CVBuffer=}16^@24
@"NSArray"32@0:8@"NSData"16^@24
@32@0:8{CGPoint=dd}16
{CGPoint=dd}16@0:8
{CGPoint="x"d"y"d}
B40@0:8@16@24^@32
v40@0:8@16@24@?32
v32@0:8@"NSURLSession"16@"NSError"24
v40@0:8@"NSURLSession"16@"NSURLAuthenticationChallenge"24@?<v@?q@"NSURLCredential">32
v24@0:8@"NSURLSession"16
@"HMIHomeKitClient"
@"NSURLSession"
@"HMIFeedbackSession"
@36@0:8i16@20@28
@"HMFOperation"
v44@0:8@16@24B32@?36
@56@0:8@16@24@32d40q48
@72@0:8@16@24@32@40@48d56q64
@96@0:8@16@24@32{CGRect={CGPoint=dd}{CGSize=dd}}40@72@80d88
@28@0:8i16@20
@"HMIPersonFaceCrop"
@32@0:8d16@24
@56@0:8Q16@24@32@40@48
@48@0:8q16@24@32@40
@72@0:8@16{?={?=qiIq}{?=qiIq}}24
@80@0:8@16@24{?={?=qiIq}{?=qiIq}}32
@88@0:8@16@24{?={?=qiIq}{?=qiIq}}32@80
{?={?=qiIq}{?=qiIq}}16@0:8
r^{opaqueCMFormatDescription=}16@0:8
r^{opaqueCMFormatDescription=}
{?="start"{?="value"q"timescale"i"flags"I"epoch"q}"duration"{?="value"q"timescale"i"flags"I"epoch"q}}
@32@0:8{CGSize=dd}16
@"HMIFaceTracker"
@24@0:8^{opaqueCMSampleBuffer=}16
r^{__CTFont=}24@0:8d16
v52@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16f48
v40@0:8@16{CGPoint=dd}24
^{CGContext=}
^{CGColorSpace=}
^{__CTFont=}
@"<HMISystemResourceUsageMonitorDelegate>"
@"<HMISystemResourceUsageMonitorProtocol>"
@36@0:8@16B24@28
@44@0:8@16@24@32B40
@"VNPersonsModel"
@32@0:8@16q24
@36@0:8@16B24q28
v24@0:8#16
v28@0:8i16@20
v32@0:8d16@24
v28@0:8B16@20
i24@0:8@16
d24@0:8@16
@32@0:8#16@24
B64@0:8@16@24Q32@40@48^@56
@84@0:8@16@24i32{CGRect={CGPoint=dd}{CGSize=dd}}36{CGSize=dd}68
@76@0:8@16i24{CGRect={CGPoint=dd}{CGSize=dd}}28{CGSize=dd}60
v108@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{CGRect={CGPoint=dd}{CGSize=dd}}48d80d88@96B104
v48@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16
v32@0:8{CGPoint=dd}16
{CGVector=dd}16@0:8
v32@0:8{CGVector=dd}16
{CGVector="dx"d"dy"d}
@40@0:8Q16Q24d32
@80@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24{?=qiIq}56
@72@0:8@16@24@32@40{?=qiIq}48
@64@0:8{?=qiIq}16@40Q48Q56
@112@0:8{?=qiIq}16@40Q48Q56d64{CGSize=dd}72{?=qiIq}88
Q24@0:8@16
B32@0:8^@16^@24
@40@0:8{?=ii}16I24B28^@32
B28@0:8^{opaqueCMSampleBuffer=}16B24
v32@0:8{HMIVideoEncoderDataRate=qq}16
{HMIVideoEncoderDataRate=qq}16@0:8
^{OpaqueVTCompressionSession=}
{?="width"i"height"i}
@"<HMIVideoEncoderDelegate>"
{HMIVideoEncoderDataRate="bytes"q"seconds"q}
v24@0:8@?<v@?@"HMIHomePersonManagerSettings"@"NSError">16
v40@0:8@"NSSet"16@"NSUUID"24@?<v@?@"NSError">32
@"HMHomePersonManager"
q32@0:8q16@24
@40@0:8@16Q24Q32
B52@0:8@16@24B32@36^@44
@44@0:8@16B24@28^@36
@72@0:8@16Q24@32@40@48@56^@64
@"HMIDESDataset"
@32@0:8Q16@24
@"<HMIExternalPersonManagerDataSource>"
@"HMIExternalPersonManagerSettings"
@36@0:8@16@24B32
@"TRIClient"
B40@0:8@16^{ImageDescriptorBufferFloat32=^^?{vector<long long, std::__1::allocator<long long>>=^q^q{__compressed_pair<long long *, std::__1::allocator<long long>>=^q}}{map<long long, int, std::__1::less<long long>, std::__1::allocator<std::__1::pair<const long long, int>>>={__tree<std::__1::__value_type<long long, int>, std::__1::__map_value_compare<long long, std::__1::__value_type<long long, int>, std::__1::less<long long>, true>, std::__1::allocator<std::__1::__value_type<long long, int>>>=^{__tree_end_node<std::__1::__tree_node_base<void *> *>}{__compressed_pair<std::__1::__tree_end_node<std::__1::__tree_node_base<void *> *>, std::__1::allocator<std::__1::__tree_node<std::__1::__value_type<long long, int>, void *>>>={__tree_end_node<std::__1::__tree_node_base<void *> *>=^{__tree_node_base<void *>}}}{__compressed_pair<unsigned long, std::__1::__map_value_compare<long long, std::__1::__value_type<long long, int>, std::__1::less<long long>, true>>=Q}}}^vQQQBQi^f}24^@32
f32@0:8@16@24
@56@0:8@16@24@32q40^@48
@24@0:8^{vector<std::__1::pair<long long, long long>, std::__1::allocator<std::__1::pair<long long, long long>>>=^{pair<long long, long long>}^{pair<long long, long long>}{__compressed_pair<std::__1::pair<long long, long long> *, std::__1::allocator<std::__1::pair<long long, long long>>>=^{pair<long long, long long>}}}16
{shared_ptr<homeai::clustering::GreedyClusterer>="__ptr_"^{GreedyClusterer}"__cntrl_"^{__shared_weak_count}}
v40@0:8@"HMICameraVideoAnalyzer"16@"HMICameraVideoFragment"24@"HMICameraVideoAnalyzerResult"32
v40@0:8@"HMICameraVideoAnalyzer"16@"HMICameraVideoFragment"24@"NSError"32
v40@0:8@"HMICameraVideoAnalyzer"16@"HMICameraVideoAnalyzerSignificantEvent"24@"HMICameraVideoFragment"32
@"NSObject<OS_dispatch_group>"
@"<HMIVideoFrameSamplerDelegate>"
@28@0:8f16d20
@"<HMIVideoPackageAnalyzerDelegate>"
@"HMIVideoFrameIntervalSampler"
@"HMIAverageImage"
@"CIFilter"
@"HMIPackageCandidateDetectorConfiguration"
@"HMIPackageCandidateDetector"
v40@0:8@"HMIVideoPackageAnalyzer"16@"NSArray"24@"NSError"32
f24@0:8^{__CVBuffer=}16
{vector<float, std::__1::allocator<float>>="__begin_"^f"__end_"^f"__end_cap_"{__compressed_pair<float *, std::__1::allocator<float>>="__value_"^f}}
f24@0:8@16
d48@0:8d16d24d32d40
@64@0:8@16@24@32@40@48@56
@"HMIVideoFragment"
@"HMIVideoAnalyzerResultOutcome"
@"HMIVideoAnalyzerDynamicConfiguration"
@48@0:8@16@24@32f40f44
B48@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16
B52@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16f48
B28@0:8@16f24
B72@0:8@16@24@32@40@48@56@64
B64@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16@48#56
B48@0:8{CGPoint=dd}16{CGPoint=dd}32
@48@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16
B68@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16q48q56f64
i40@0:8^{__CVBuffer=}16@24@?32
i40@0:8@16@24@?32
@"NSMapTable"
@60@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24B56
@68@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24B56@60
@"HMIVideoAnalyzerEventFace"
@60@0:8@16B24d28Q36@44^@52
Q24@0:8q16
@40@0:8@16@24@?32
@"HMIPerson"
@"CIImage"
@40@0:8r*16@24@?32
r*16@0:8
@44@0:8f16f20f24f28f32@36
@"HMIBoundingBoxExtractor"
@60@0:8i16@20@28@36@44^@52
@"HMIGreedyClustering"
@"<HMIFaceClassifier>"
@"HMIPersonsModelManager"
@"HMIClusteringTaskSummary"
@52@0:8i16@20@28@36B44B48
@40@0:8@16{CGSize=dd}24
@56@0:8@16@24@32@40^@48
@56@0:8@"NSDictionary"16@"NSDictionary"24@"HMINMSConfiguration"32@"NSString"40^@48
v24@0:8@"HMICameraVideoFrame"16
v72@0:8@"NSArray"16{?=qiIq}24{CGSize=dd}48@"NSUUID"64
@"HMICameraVideoFrameResult"60@0:8@"HMICameraVideoFrame"16q24B32@"NSUUID"36@"NSUUID"44^@52
@"NSDictionary"16@0:8
q24@0:8#16
@28@0:8q16B24
@"HMISignificantActivityDetector"
@"HMIFaceClassifierVIP"
@"HMFOSTransaction"
@"NSCache"
@64@0:8@16{CGSize=dd}24{?=qiIq}40
@48@0:8^{__CVBuffer=}16{?=qiIq}24
@40@0:8d16d24^@32
*40@0:8Q16Q24^i32
i40@0:8^v16Q24r*32
i32@0:8@16@24
@"NSCondition"
@"<HMIVideoCommandBufferDelegate>"
v32@0:8@"HMIVideoCommandBuffer"16^{opaqueCMSampleBuffer=}24
v24@0:8@"HMIVideoCommandBuffer"16
#24@0:8@16
@"HMIConfidence"
B24@0:8r^{opaqueCMFormatDescription=}16
^{OpaqueVTDecompressionSession=}
^{opaqueCMBufferQueue=}
@"NSObject<OS_dispatch_semaphore>"
@"<HMIVideoDecoderDelegate>"
v32@0:8@"HMIVideoDecoder"16^{opaqueCMSampleBuffer=}24
v32@0:8@"HMIVideoDecoder"16@"NSError"24
v48@0:8@16@24q32@40
v40@0:8@16@24q32
v48@0:8@"AVAssetWriter"16@"NSData"24q32@"AVAssetSegmentReport"40
v40@0:8@"AVAssetWriter"16@"NSData"24q32
@32@0:8r^{opaqueCMFormatDescription=}16r^{opaqueCMFormatDescription=}24
@"AVAssetWriter"
^{opaqueCMFormatDescription=}
@"<HMIVideoAssetWriterDelegate>"
v80@0:8@16@24{?={?=qiIq}{?=qiIq}}32
v32@0:8@"HMIVideoAssetWriter"16@"NSData"24
v80@0:8@"HMIVideoAssetWriter"16@"NSData"24{?={?=qiIq}{?=qiIq}}32
v32@0:8@"HMIVideoAssetWriter"16@"NSError"24
B48@0:8@16@24d32d40
@48@0:8{CGPoint=dd}16{CGVector=dd}32
@76@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{CGSize=dd}48@64f72
f68@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16q48q56f64
v48@0:8^{__CVBuffer=}16{?=qiIq}24
@32@0:8^f16@24
@"NSArray"32@0:8^f16@"NSArray"24
@48@0:8^{__CVBuffer=}16^{__CVBuffer=}24^f32@40
B56@0:8{CGPoint=dd}16{CGPoint=dd}32@48
@36@0:8^{opaqueCMSampleBuffer=}16f24@28
v24@0:8^{__CVBuffer=}16
v32@0:8^{opaqueCMSampleBuffer=}16^{opaqueCMSampleBuffer=}24
@"<HMIVideoFrameSelectorDelegate>"
v40@0:8@16^{opaqueCMSampleBuffer=}24@32
v40@0:8@"HMIVideoFrameSelector"16^{opaqueCMSampleBuffer=}24@"NSArray"32
@64@0:8@16@24@32@40d48q56
@48@0:8@16@24B32B36^@40
@"<HMIVideoAnalyzerDelegate>"
@"HMIVideoAnalyzerConfiguration"
v32@0:8@"HMIVideoAnalyzer"16@"HMIVideoAnalyzerFragmentResult"24
v32@0:8@"HMIVideoAnalyzer"16@"HMIVideoAnalyzerFrameResult"24
v32@0:8@"HMIVideoAnalyzer"16@"NSError"24
v32@0:8@"HMIVideoAnalyzer"16@"NSDictionary"24
v32@0:8^{opaqueCMSampleBuffer=}16@?24
v24@0:8r^{opaqueCMFormatDescription=}16
v32@0:8:16@24
@"HMIVideoCommandBuffer"
@"HMIVideoDecoder"
@"HMIVideoFrameSelector"
@"HMIVideoFrameAnalyzer"
@"HMIVideoPackageAnalyzer"
@"HMIVideoAnalyzerScheduler"
gepj
gepj
ARGB
f024
v024
v024
333333
333333
333333
333333
333333
333333
