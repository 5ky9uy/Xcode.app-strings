@(#)PROGRAM:HomeAI  PROJECT:HomeAI-130
Mb@?
UsesIOSurface
HardwareAcceleratedTransfer
HighSpeedTransfer
HMIPixelBufferTransferMultiStageResize
a0cTa1cTa2cTa3cTa0hTa1hTa2hTa3hT
Motion
Person
Vehicle
sFXg@
+Sg@
?>Ip
?(-_U
=$@'
hp|i1
f?94{W
A`~q@
#>m?
?>Ip
iUK:
\.@;&b|^
hp|i1
?333333
N6homeai3mod28ImageDescriptorBufferFloat32E
NSt3__118basic_stringstreamIcNS_11char_traitsIcEENS_9allocatorIcEEEE
NSt3__115basic_stringbufIcNS_11char_traitsIcEENS_9allocatorIcEEEE
HBN6homeai10clustering15GreedyClustererE
NSt3__120__shared_ptr_emplaceINS_6vectorIxNS_9allocatorIxEEEENS2_IS4_EEEE
NSt3__120__shared_ptr_emplaceIN6homeai10clustering15GreedyClusterer9cluster_tENS_9allocatorIS4_EEEE
NSt3__120__shared_ptr_emplaceINS_6vectorINS_5tupleIJxxfEEENS_9allocatorIS3_EEEENS4_IS6_EEEE
N6homeai3mod29ImageDescriptorBufferAbstractE
L@ffffff
GNSt3__120__shared_ptr_emplaceIN6homeai10clustering15GreedyClustererENS_9allocatorIS3_EEEE
MbP?
]=dZ
>X@[;
Bhs<
m%*=
%pP=2.
Motion
Person
Vehicle
Face
N2cv13BaseRowFilterE
N2cv16BaseColumnFilterE
N2cv10BaseFilterE
N2cv12FilterEngineE
N2cv18SymmRowSmallFilterIhiNS_21SymmRowSmallVec_8u32sEEE
N2cv9RowFilterIhiNS_21SymmRowSmallVec_8u32sEEE
N2cv18SymmRowSmallFilterIffNS_19SymmRowSmallVec_32fEEE
N2cv9RowFilterIffNS_19SymmRowSmallVec_32fEEE
N2cv9RowFilterIhiNS_12RowVec_8u32sEEE
N2cv9RowFilterIhfNS_8RowNoVecEEE
N2cv9RowFilterIhdNS_8RowNoVecEEE
N2cv9RowFilterItfNS_8RowNoVecEEE
N2cv9RowFilterItdNS_8RowNoVecEEE
N2cv9RowFilterIsfNS_13RowVec_16s32fEEE
N2cv9RowFilterIsdNS_8RowNoVecEEE
N2cv9RowFilterIffNS_10RowVec_32fEEE
N2cv9RowFilterIfdNS_8RowNoVecEEE
N2cv9RowFilterIddNS_8RowNoVecEEE
N2cv12ColumnFilterINS_13FixedPtCastExIihEENS_11ColumnNoVecEEE
N2cv12ColumnFilterINS_4CastIfhEENS_11ColumnNoVecEEE
N2cv12ColumnFilterINS_4CastIdhEENS_11ColumnNoVecEEE
N2cv12ColumnFilterINS_4CastIftEENS_11ColumnNoVecEEE
N2cv12ColumnFilterINS_4CastIdtEENS_11ColumnNoVecEEE
N2cv12ColumnFilterINS_4CastIfsEENS_11ColumnNoVecEEE
N2cv12ColumnFilterINS_4CastIdsEENS_11ColumnNoVecEEE
N2cv12ColumnFilterINS_4CastIffEENS_11ColumnNoVecEEE
N2cv12ColumnFilterINS_4CastIddEENS_11ColumnNoVecEEE
N2cv21SymmColumnSmallFilterINS_13FixedPtCastExIihEENS_19SymmColumnVec_32s8uEEE
N2cv16SymmColumnFilterINS_13FixedPtCastExIihEENS_19SymmColumnVec_32s8uEEE
N2cv12ColumnFilterINS_13FixedPtCastExIihEENS_19SymmColumnVec_32s8uEEE
N2cv21SymmColumnSmallFilterINS_4CastIisEENS_25SymmColumnSmallVec_32s16sEEE
N2cv16SymmColumnFilterINS_4CastIisEENS_25SymmColumnSmallVec_32s16sEEE
N2cv12ColumnFilterINS_4CastIisEENS_25SymmColumnSmallVec_32s16sEEE
N2cv21SymmColumnSmallFilterINS_4CastIffEENS_22SymmColumnSmallVec_32fEEE
N2cv16SymmColumnFilterINS_4CastIffEENS_22SymmColumnSmallVec_32fEEE
N2cv12ColumnFilterINS_4CastIffEENS_22SymmColumnSmallVec_32fEEE
N2cv16SymmColumnFilterINS_4CastIfhEENS_11ColumnNoVecEEE
N2cv16SymmColumnFilterINS_4CastIdhEENS_11ColumnNoVecEEE
N2cv16SymmColumnFilterINS_4CastIftEENS_11ColumnNoVecEEE
N2cv16SymmColumnFilterINS_4CastIdtEENS_11ColumnNoVecEEE
N2cv16SymmColumnFilterINS_4CastIisEENS_11ColumnNoVecEEE
N2cv12ColumnFilterINS_4CastIisEENS_11ColumnNoVecEEE
N2cv16SymmColumnFilterINS_4CastIfsEENS_20SymmColumnVec_32f16sEEE
N2cv12ColumnFilterINS_4CastIfsEENS_20SymmColumnVec_32f16sEEE
N2cv16SymmColumnFilterINS_4CastIdsEENS_11ColumnNoVecEEE
N2cv16SymmColumnFilterINS_4CastIffEENS_17SymmColumnVec_32fEEE
N2cv12ColumnFilterINS_4CastIffEENS_17SymmColumnVec_32fEEE
N2cv16SymmColumnFilterINS_4CastIddEENS_11ColumnNoVecEEE
N2cv8Filter2DIhNS_4CastIfhEENS_12FilterVec_8uEEE
N2cv8Filter2DIhNS_4CastIftEENS_11FilterNoVecEEE
N2cv8Filter2DIhNS_4CastIfsEENS_15FilterVec_8u16sEEE
N2cv8Filter2DIhNS_4CastIffEENS_11FilterNoVecEEE
N2cv8Filter2DIhNS_4CastIddEENS_11FilterNoVecEEE
N2cv8Filter2DItNS_4CastIftEENS_11FilterNoVecEEE
N2cv8Filter2DItNS_4CastIffEENS_11FilterNoVecEEE
N2cv8Filter2DItNS_4CastIddEENS_11FilterNoVecEEE
N2cv8Filter2DIsNS_4CastIfsEENS_11FilterNoVecEEE
N2cv8Filter2DIsNS_4CastIffEENS_11FilterNoVecEEE
N2cv8Filter2DIsNS_4CastIddEENS_11FilterNoVecEEE
N2cv8Filter2DIfNS_4CastIffEENS_13FilterVec_32fEEE
N2cv8Filter2DIdNS_4CastIddEENS_11FilterNoVecEEE
 !"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\]^_`abcdefghijklmnopqrstuvwxyz{|}~
ucwsifdr
?qxs?qxs
9D)5:
@`|-A
PF SdF
 [@W:[
D]eV
"x@
z4I5z
?~my
!Y?4
!I?u
!)?gX
?g^FT
?$"LT
?$"LT
N2cv15ThresholdRunnerE
N2cv6detail16LKTrackerInvokerE
N2cv9ColumnSumIihEE
N2cv9ColumnSumIitEE
N2cv9ColumnSumIisEE
N2cv6RowSumIhiEE
N2cv6RowSumIhdEE
N2cv6RowSumItiEE
N2cv6RowSumItdEE
N2cv6RowSumIsiEE
N2cv6RowSumIiiEE
N2cv6RowSumIsdEE
N2cv6RowSumIfdEE
N2cv6RowSumIddEE
N2cv9ColumnSumIdhEE
N2cv9ColumnSumIdtEE
N2cv9ColumnSumIdsEE
N2cv9ColumnSumIiiEE
N2cv9ColumnSumIifEE
N2cv9ColumnSumIdfEE
N2cv9ColumnSumIidEE
N2cv9ColumnSumIddEE
 !"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\]^_`abcdefghijklmnopqrstuvwxyz{|}~
<N2cv11_InputArrayE
N2cv12_OutputArrayE
N2cv9ExceptionE
14EmptyFuncTable
12GpuFuncTable
N2cv16ParallelLoopBodyE
N2cv16MorphologyRunnerE
N2cv14MorphRowFilterINS_5MinOpIhEENS_12MorphRowIVecINS_6VMin8uEEEEE
N2cv14MorphRowFilterINS_5MinOpItEENS_12MorphRowIVecINS_7VMin16uEEEEE
N2cv14MorphRowFilterINS_5MinOpIsEENS_12MorphRowIVecINS_7VMin16sEEEEE
N2cv14MorphRowFilterINS_5MinOpIfEENS_12MorphRowFVecINS_7VMin32fEEEEE
N2cv14MorphRowFilterINS_5MinOpIdEENS_13MorphRowNoVecEEE
N2cv14MorphRowFilterINS_5MaxOpIhEENS_12MorphRowIVecINS_6VMax8uEEEEE
N2cv14MorphRowFilterINS_5MaxOpItEENS_12MorphRowIVecINS_7VMax16uEEEEE
N2cv14MorphRowFilterINS_5MaxOpIsEENS_12MorphRowIVecINS_7VMax16sEEEEE
N2cv14MorphRowFilterINS_5MaxOpIfEENS_12MorphRowFVecINS_7VMax32fEEEEE
N2cv14MorphRowFilterINS_5MaxOpIdEENS_13MorphRowNoVecEEE
N2cv17MorphColumnFilterINS_5MinOpIhEENS_15MorphColumnIVecINS_6VMin8uEEEEE
N2cv17MorphColumnFilterINS_5MinOpItEENS_15MorphColumnIVecINS_7VMin16uEEEEE
N2cv17MorphColumnFilterINS_5MinOpIsEENS_15MorphColumnIVecINS_7VMin16sEEEEE
N2cv17MorphColumnFilterINS_5MinOpIfEENS_15MorphColumnFVecINS_7VMin32fEEEEE
N2cv17MorphColumnFilterINS_5MinOpIdEENS_16MorphColumnNoVecEEE
N2cv17MorphColumnFilterINS_5MaxOpIhEENS_15MorphColumnIVecINS_6VMax8uEEEEE
N2cv17MorphColumnFilterINS_5MaxOpItEENS_15MorphColumnIVecINS_7VMax16uEEEEE
N2cv17MorphColumnFilterINS_5MaxOpIsEENS_15MorphColumnIVecINS_7VMax16sEEEEE
N2cv17MorphColumnFilterINS_5MaxOpIfEENS_15MorphColumnFVecINS_7VMax32fEEEEE
N2cv17MorphColumnFilterINS_5MaxOpIdEENS_16MorphColumnNoVecEEE
N2cv11MorphFilterINS_5MinOpIhEENS_9MorphIVecINS_6VMin8uEEEEE
N2cv11MorphFilterINS_5MinOpItEENS_9MorphIVecINS_7VMin16uEEEEE
N2cv11MorphFilterINS_5MinOpIsEENS_9MorphIVecINS_7VMin16sEEEEE
N2cv11MorphFilterINS_5MinOpIfEENS_9MorphFVecINS_7VMin32fEEEEE
N2cv11MorphFilterINS_5MinOpIdEENS_10MorphNoVecEEE
N2cv11MorphFilterINS_5MaxOpIhEENS_9MorphIVecINS_6VMax8uEEEEE
N2cv11MorphFilterINS_5MaxOpItEENS_9MorphIVecINS_7VMax16uEEEEE
N2cv11MorphFilterINS_5MaxOpIsEENS_9MorphIVecINS_7VMax16sEEEEE
N2cv11MorphFilterINS_5MaxOpIfEENS_9MorphFVecINS_7VMax32fEEEEE
N2cv11MorphFilterINS_5MaxOpIdEENS_10MorphNoVecEEE
?N2cv5MatOpE
N2cv14MatOp_IdentityE
N2cv11MatOp_AddExE
N2cv9MatOp_BinE
N2cv9MatOp_CmpE
N2cv10MatOp_GEMME
N2cv12MatOp_InvertE
N2cv7MatOp_TE
N2cv11MatOp_SolveE
N2cv17MatOp_InitializerE
Error creating directory for person:%@
version
UUID
displayName
person.json
Error saving metadata to disk for person:%@
v8@?0
Error creating directory for person face crop:%@
faceBoundingBox
dateCreated
facecrop.json
Error saving metadata to disk for person face crop:%@
facecrop.jpeg
Error saving face crop image to disk for person face crop:%@
external.person.datasource.disk
hash
TQ,R
superclass
T#,R
description
T@"NSString",R,C
debugDescription
HMIMutableCluster
centroid
T@"HMIDESMutableFloatArray",R,V_centroid
TQ,R,N
faceprintUUIDs
T@"NSArray",R
linkedEntityUUIDs
T@"NSSet",R
featureNames
T@"NSSet",R,N
pixelBuffer
T^{__CVBuffer=},R,V_pixelBuffer
inputName
T@"NSString",R,V_inputName
image__Placeholder__0
ShotNet__ShotNet__ssd_predictions__block8_box__box_offset__0
ShotNet__ShotNet__ssd_predictions__block7_box__box_offset__0
ShotNet__ShotNet__ssd_predictions__block6_box__box_offset__0
ShotNet__ShotNet__ssd_predictions__block9_box__box_offset__0
ShotNet__ShotNet__ssd_predictions__block10_box__box_offset__0
ShotNet__ShotNet__ssd_predictions__block11_box__box_offset__0
ShotNet__ShotNet__ssd_predictions__block8_box__class_prob__0
ShotNet__ShotNet__ssd_predictions__block7_box__class_prob__0
ShotNet__ShotNet__ssd_predictions__block6_box__class_prob__0
ShotNet__ShotNet__ssd_predictions__block9_box__class_prob__0
ShotNet__ShotNet__ssd_predictions__block10_box__class_prob__0
ShotNet__ShotNet__ssd_predictions__block11_box__class_prob__0
ShotNet__ShotNet__ssd_predictions__block8_box__yaw_angle__0
ShotNet__ShotNet__ssd_predictions__block7_box__yaw_angle__0
ShotNet__ShotNet__ssd_predictions__block6_box__yaw_angle__0
ShotNet__ShotNet__ssd_predictions__block9_box__yaw_angle__0
ShotNet__ShotNet__ssd_predictions__block10_box__yaw_angle__0
ShotNet__ShotNet__ssd_predictions__block11_box__yaw_angle__0
ShotNet__ShotNet__ssd_predictions__block8_box__roll_angle__0
ShotNet__ShotNet__ssd_predictions__block7_box__roll_angle__0
ShotNet__ShotNet__ssd_predictions__block6_box__roll_angle__0
ShotNet__ShotNet__ssd_predictions__block9_box__roll_angle__0
ShotNet__ShotNet__ssd_predictions__block10_box__roll_angle__0
ShotNet__ShotNet__ssd_predictions__block11_box__roll_angle__0
SignificantActivityDetector
CoreML output nil or not of type MLFeatureTypeMultiArray
SignificantActivity
mlmodelc
significant.activity.detector
mlModel
T@"MLModel",R,V_mlModel
inputFeatureValueName
T@"NSString",R,V_inputFeatureValueName
offsetsFeatureValueNames
T@"NSArray",R,V_offsetsFeatureValueNames
scoresFeatureValueNames
T@"NSArray",R,V_scoresFeatureValueNames
yawsFeatureValueNames
T@"NSArray",R,V_yawsFeatureValueNames
rollsFeatureValueNames
T@"NSArray",R,V_rollsFeatureValueNames
nmsConfiguration
T@"HMINMSConfiguration",R,V_nmsConfiguration
useSoftmax
TB,R,V_useSoftmax
predictionOptions
T@"MLPredictionOptions",R,V_predictionOptions
inputDimensions
T{CGSize=dd},R,V_inputDimensions
HMISystemResourceUsageLevelLow
HMISystemResourceUsageLevelMed
HMISystemResourceUsageLevelHigh
HMISystemResourceUsageLevelUnknown
HMISystemResourceUsageLevelUndefined
systemResourceUsageLevel
Tq,N,V_systemResourceUsageLevel
delegate
T@"<HMISystemResourceUsageMonitorDelegate>",W
systemResourceUsageMonitorImpl
T@"HMISystemResourceUsageMonitorImpl",R,V_systemResourceUsageMonitorImpl
workQueue
T@"NSObject<OS_dispatch_queue>",R,V_workQueue
HMIVAEF.fr
HMIVAEF.fq
HMIVAEF.ya
HMIVAEF.ro
Confidence
Bounding Box
Face Recognition
Face Yaw
Face Roll
HMIVideoAnalyzerEventFace
faceQuality
T@"HMIFaceQuality",R,V_faceQuality
T@"NSNumber",R,V_yaw
roll
T@"NSNumber",R,V_roll
faceRecognition
T@"HMIFaceRecognition",R,V_faceRecognition
%c%c%c%c
Type: %@, DTS: %@, PTS: %@, DUR: %@, NUM: %ld [%@]
Sync
%c%02d:%02d:%02d.%03d
%*lld/%d %@
%*lld %@
POSITIVE INFINITY
NEGATIVE INFINITY
   INDEFINITE    
  INVALID TIME   
%@ DTS %@ PTS %@ dur %@%@
PTS: %.2f
Bogus atomSize %llu, recovering by adjusting size.
en_US_POSIX
yyyy-MM-dd'T'HH:mm:ss
%.2f x %.2f
%.2f, %.2f %@
%.2fs
%02x
time
T{?=qiIq},R
T{?=qiIq},R,V_time
value
T@,R,V_value
q24@?0@"<HMIVideoEvent>"8@"<HMIVideoEvent>"16
B32@?0@"<HMIVideoEvent>"8Q16^B24
@16@?0@"<HMIVideoEvent>"8
Time: %@, Date: %@
date
T@"NSDate",R,V_date
Invalid parameter not satisfying: %@
lock
HMIVideoAnalyzerEventMotion
HMICoreAnalyticsVIPModelReportTime
Received nil data source
Fetching settings using data source: %@
Error fetching settings: %@
v24@?0@"HMIHomePersonManagerSettings"8@"NSError"16
handleUpdatedPerson: %@
handleUpdatedUnassociatedFaceCrop: %@
handleUpdatedPersonFaceCrop: %@
handleUpdatedFaceprint: %@
handleUpdatedSettings: %@
handleRemovedPersonWithUUID: %@
handleRemovedFaceCropWithUUID: %@
handleRemovedFaceprintWithUUID: %@
Successfully handled face misclassification
Error in handling face misclassification, error:%@
v24@?0@"NSDictionary"8@"NSError"16
Submitted face misclassification task, taskID:%u
B16@?0@"HMIFaceClassification"8
Not storing face crop and faceprint for HMIHomePersonManager.UUID:%@ from face event:%@
Storing unknown to Home face crop:%@ and faceprint:%@
Error storing unassociated face crop:%@, error:%@
Stored unassociated face crop:%@
v16@?0@"NSError"8
Error storing faceprint:%@, error:%@
Stored faceprint:%@
Reached face crop limit for sessionEntityUUID:%@ for HMIHomePersonManager.UUID:%@; not storing
Timer fired, updating home persons model
Not triggering daily VIP Model Core Analytics event, last event was sent less than 1 day ago
Triggering daily VIP Model Core Analytics event
Successfully ran persons model summary task
Failed to run persons model summary task, error:%@
Submitted persons model summary task, taskID:%u
Unrecognized timer: %@
UUID:%@ HomeUUID:%@
home.person.manager
Updating with settings: %@
Settings have disabled face classification, removing home persons model
Settings have enabled face classification, updating home persons model
operationQueue
T@"NSOperationQueue",R,V_operationQueue
watchdogTimer
T@"HMFTimer",R,V_watchdogTimer
analyticsTimer
T@"HMFTimer",R,V_analyticsTimer
T@"HMFUnfairLock",R,N,V_lock
unknownFacesSavedCounts
T@"NSMutableDictionary",R,V_unknownFacesSavedCounts
settings
T@"HMIHomePersonManagerSettings",R,V_settings
dataSource
T@"<HMIHomePersonManagerDataSource>",W,N,V_dataSource
Storing face crop:%@ failed with error:%@
Storing face crop:%@ completed successfully
store.facecrop.operation
T@"<HMIHomePersonManagerDataSource>",R,V_dataSource
faceCrop
T@"HMIFaceCrop",R,V_faceCrop
Storing faceprint:%@ failed with error:%@
Storing faceprint:%@ completed successfully
store.faceprint.operation
faceprint
T@"HMIFaceprint",R,V_faceprint
JPEGRepresentation
Frame %lu @ %@
camera.video.frame
jpegData
T@"NSData",R,V_jpegData
motionDetections
T@"NSArray",&,V_motionDetections
sessionPresentationTime
T{?=qiIq},V_sessionPresentationTime
presentationTime
T{?=qiIq},R,V_presentationTime
size
T{CGSize=dd},R,V_size
frameId
TQ,R,V_frameId
fragmentSequenceNumber
TQ,R,V_fragmentSequenceNumber
Logi Circle 2
%#{flags}
transferPixelBuffer
VTPixelTransferSessionCreate failed. Error %d
VTSessionSetProperty failed. Error %d
VTPixelTransferSessionTransferImage failed. Error %d
JPEGDataFromPixelBuffer
Using 720p params for dewarping
Using 1080p params for dewarping
300p skipping dewarp params
Error creating directory:%@ to save video frames
%@-%06lu.%@
Error saving video frame:%@ JPEG to disk
Saved video frame:%@ to disk
Error in pixelbuffer format for rotation
Error generating pixelbuffer for rotaion
Error applying affine transform
vision.utilities
Registering for Thermal Level Notifications
v12@?0i8
services
T@"NSMutableDictionary",R,V_services
thermalLevel
TQ,R,V_thermalLevel
Footprint: %@, Average: %@, Peak: %@
OutOfMemory
Reached high water mark.
memory.sampler
tick
T@"HMFTimer",R,V_tick
average
T@"MovingAverage",R,V_average
highWaterMark
Tq,V_highWaterMark
PrimaryUsagePage
PrimaryUsage
LocationID
Failed to read sample buffer, error: %@
Asset reader failed, ignoring
Read: %@
HMIVideoAssetReader
HMIHomeKitClient HomeKit Delegate Queue
B16@?0@"HMCameraProfile"8
v16@?0@"HMHomeManager"8
Error refreshing home data: %@
No homes were located
Found home: name: %@, primary: %s, UUID: %@
homekit.client
homeKitOperationQueue
T@"NSOperationQueue",R,V_homeKitOperationQueue
setup
TB,R,GisSetup,V_setup
cachePolicy
TQ,R,V_cachePolicy
homes
T@"NSArray",R,V_homes
didUpdateHomes
T@?,C,V_didUpdateHomes
HMMutableHomeManagerConfiguration
Unable to find class %s
/System/Library/Frameworks/HomeKit.framework/HomeKit
HMHomeManager
Not supported error
General error
Espresso error
incorrect binserializer key
small sparsity error
feature extraction error
initialization error
no saved state to revert
nominal distance not changed
batch size violation
computation kill request was issued
too few IDs to build VIP model
video error
error with projection computation
missing positional parameter
inconsistent state error
warping error
OpenGL error
invalid format
out of bounds
singular point configuration error
division by zero
LAPACK error
platform endianess not supported
hash already in use
invalid ID
invalid data type
data inconsistency error
I/O error
unknown option
invalid option
missing option
delegate error
vImage related error
memory allocation error
invalid parameter
unexpected null pointer
internal error
not implemented error
CVML_status module %d error %lld
BinSerializer
Face3D
FaceDescriptor
FaceFrontalizer
FaceWarper
Geometry2D
Geometry3D
ImageGrouping
ImageQuality
LandmarkDetector
MomentProcessor
FaceboxAligner
ImageDescriptor
ImageClassifier
ImageProcessing
VIPIdentification
ImageRegistration
SimilarityMatrix
Clustering
HumanDetector
FaceRegionMap
ObjectDetector
ObjectTracker
SRCClassifier
Kmeans
SparseCoding
FaceID
BoostedClassifier
FaceSegmenter
ImageAnalyzer
FaceAttributes
FaceprintAndAttributes
FaceQuality
Generic
ImageTools
VideoTools
ImageWarper
ThirdParty
BinSerializerProcessor
AppleNetParser
FaceProcessorCLI
ImageClassifierCLI
MPCmdlineClientCLI
ClusteringCLI
ImageProcessorCLI
PhotosProcessorCLI
CVMLEngine
CVML Module %lld
input
T@"MLMultiArray",&,N,V_input
transformed_features
classLabel
FaceQualityFilterSVM
FaceQualityFilterSVMDataScaler
facequality.filter.svm
scalerModel
T@"MLModel",R,V_scalerModel
Tq,R
error
T@"NSError",R
Ignoring %@
You must override %@ in a subclass
task
T@"HMIDESBackgroundTask",&,V_task
fetchAllPersonsWithCompletion
@16@?0@"HMPerson"8
v24@?0@"NSSet"8@"NSError"16
fetchPersonsWithUUIDs:%@
fetchAllPersonFaceCropsWithCompletion
@16@?0@"HMPersonFaceCrop"8
fetchFaceCropsForPersonsWithUUIDs:%@
fetchAllFaceprintsWithCompletion
@16@?0@"HMFaceprint"8
fetchFaceprintsForFaceCropsWithUUIDs:%@
fetchSettingsWithCompletion
performCloudPullWithCompletion
addFaceprints:%@
@16@?0@"HMIFaceprint"8
removeFaceprintsWithUUIDs:%@
external.person.datasource.homekit
photosPersonManager
T@"HMPhotosPersonManager",&,V_photosPersonManager
HMFaceprint
com.apple.cvml.%@
CVML module = %@
HMIFC.ck.pu
dataRepresentation
personUUID
Person UUID
Could not initialize from decoded personUUID: %@
supportsSecureCoding
TB,R
T@"NSUUID",R,C,V_personUUID
HMIVideoAnalyzerFrameResult
{events: [%@], frame: %@}
B16@?0@"HMIVideoAnalyzerEvent"8
v16@?0@"HMIVideoAnalyzerEvent"8
@16@?0#8
frame
T@"HMIVideoFrame",R,V_frame
events
T@"NSSet",R,V_events
hmi://in-memory
HMIMemoryAVAsset
 Fullfilled content request: %@
Fullfilled data request: %@
tracks
T@"NSNumber",R,V_value
Invalid parameter for windowSize: %lu, windowSize must be > 0
v16@?0@"MovingAverageEntry"8
queue
T@"NSMutableArray",&,N,V_queue
windowSize
TQ,R,N,V_windowSize
movingAverage
Td,V_movingAverage
HKD://
analyzed-video-frames
face-classifications
Success
SystemResourceUsageHigh
InsufficentTime
AnyMotion
SkippedAnalysis
None
Medium
High
No HMIVideoAnalyzerEventClass exists for event type %@
DidAnalyze
DidNotAnalyze
DidFailAnalysis
Canceled
Bypassed
Expired
SessionEnded
InErrorState
Predict
@"NSArray"16@?0@"HMICameraVideoFrameResult"8
@16@?0@"HMICameraVideoFrameResult"8
@16@?0@"HMIVideoAnalyzerEvent"8
duration
T{?=qiIq},V_duration
creationDate
T@"NSDate",&,V_creationDate
lastSequenceNumber
TQ,R,V_lastSequenceNumber
Tq,R,V_events
annotationScores
T@"NSDictionary",R,V_annotationScores
posterFrames
T@"NSArray",R,V_posterFrames
frameResults
T@"NSArray",R,V_frameResults
resultCode
Tq,V_resultCode
timeToAnalyzeFragment
Td,V_timeToAnalyzeFragment
timeSinceFragmentWasSubmitted
Td,V_timeSinceFragmentWasSubmitted
videoFragment
T@"HMICameraVideoFragment",&,V_videoFragment
analysisFPS
Tf,V_analysisFPS
local
remote-fragment
serviceType
TQ,V_serviceType
startingMediaIntegritySequenceNumber
TQ,V_startingMediaIntegritySequenceNumber
useScheduler
TB,V_useScheduler
inMediaAnalysis
TB,V_inMediaAnalysis
faceClassificationEnabled
TB,V_faceClassificationEnabled
currentSessionDuration
T{?=qiIq},V_currentSessionDuration
posterFrameGenerationInterval
TQ,R,V_posterFrameGenerationInterval
posterFrameHeight
TQ,R,V_posterFrameHeight
maxFragmentAnalysisDuration
Td,R,V_maxFragmentAnalysisDuration
maxFragmentDuration
Td,R,V_maxFragmentDuration
transcodeFragment
TB,R,V_transcodeFragment
videoFrame
T@"HMICameraVideoFrame",R,V_videoFrame
T@"HMICameraVideoFrame",R,V_frame
detections
T@"NSArray",R,V_detections
regionOfInterest
T{CGRect={CGPoint=dd}{CGSize=dd}},R,V_regionOfInterest
faceClassifications
T@"NSSet",R,V_faceClassifications
analyzerEvents
T@"NSSet",R,V_analyzerEvents
fragment/%lu
fragmentData
T@"NSMutableData",R
T@"NSURL",&,N,V_url
sequenceNumber
TQ,R,V_sequenceNumber
T@"NSData",R,V_data
moovFragment
T@"NSData",R,N,V_moovFragment
eventTypes
Tq,R,V_eventTypes
activityZones
T@"NSArray",R,V_activityZones
[%@] (%lu) repetitions: %lu/%lu/%lu predictions: %lu/%lu/%lu analyzers: %lu lastEvents: %@ shouldPredict: %@
camera.video.analyzer.history
predictions
Tq,V_predictions
repetitions
Tq,V_repetitions
totalPredictions
Tq,V_totalPredictions
totalRepetitions
Tq,V_totalRepetitions
totalRequests
Tq,V_totalRequests
lastRequestResult
T@"HMICameraVideoAnalyzerResult",&,V_lastRequestResult
lastRequestSignificantEvents
T@"NSArray",&,V_lastRequestSignificantEvents
analyzer
T@"HMICameraVideoAnalyzer",R,W,V_analyzer
minRepetitions
Tq,R,V_minRepetitions
maxPredictions
Tq,R,V_maxPredictions
Start analysis, elapsed time since submission: %fs
v24@?0@"HMIVideoAssetWriter"8@"NSData"16
v72@?0@"HMIVideoAssetWriter"8@"NSData"16{?={?=qiIq}{?=qiIq}}24
%@, Date: %@, Time: %@, BitRate: %ld
v16@?0@"HMICameraActivityZone"8
%@ Fragment:%lu
camera.video.analyzer.request
significantEventsInternal
T@"NSMutableArray",R,V_significantEventsInternal
phase
Tq,V_phase
flag
Tq,V_flag
analysisSubmissionTime
T@"NSDate",R,V_analysisSubmissionTime
timeSinceAnalysisSubmission
Td,R
analysisStartTime
T@"NSDate",R,V_analysisStartTime
timeSinceAnalysisStart
maxAnalysisFPS
Td,R,V_maxAnalysisFPS
Td,R,V_analysisFPS
fragment
T@"HMICameraVideoFragment",R,V_fragment
attributes
T@"HMICameraVideoResourceAttributes",R,V_attributes
encoder
T@"HMIVideoEncoder",R,V_encoder
retimer
T@"HMIVideoRetimer",R,V_retimer
audioSamples
T@"NSMutableArray",R,V_audioSamples
assetWriter
T@"HMIVideoAssetWriter",R,V_assetWriter
posterFrameGenerator
T@"HMICameraVideoPosterFrameGenerator",R,V_posterFrameGenerator
frameSelector
T@"HMICameraVideoFrameSelector",R,V_frameSelector
assetReader
T@"HMICameraVideoAssetReader",R,V_assetReader
T@"HMICameraVideoAnalyzer",R,V_analyzer
videoFrameResults
T@"NSMutableArray",&,V_videoFrameResults
significantEvents
shouldSkipAnalysis
shouldFailAnalysis
HMICameraVideoAnalyzerScheduler
HIGH
Unknown
Normal
Warning
Critical
Undefined
analysisTime: %.2f (%.2f), total: %.2f (%.2f), level: %@, memory: %@, availableSystemMemory: %@, temp: %.2f, thermalLevel: %lu, analysisFPS: %.2f, analyzers: %lu, activeAnalyzers: %lu, maxAnalyzers: %lu
v16@?0@"HMICameraVideoAnalyzerRequest"8
[%@] [A:%d]
 %@ [%lu,%lu] %@
camera.video.analyzer.scheduler
internalAnalyzers
T@"NSPointerArray",R,V_internalAnalyzers
systemResourceUsageMonitor
T@"HMISystemResourceUsageMonitor",R,V_systemResourceUsageMonitor
systemResourceUsageMonitorUsageLevel
Tq,V_systemResourceUsageMonitorUsageLevel
averageAnalysisTime
averageAnalysisTimeMovingAverage
T@"MovingAverage",R,V_averageAnalysisTimeMovingAverage
averageTotalAnalysisTime
averageTotalAnalysisTimeMovingAverage
T@"MovingAverage",R,V_averageTotalAnalysisTimeMovingAverage
analysisFPSPreference
Td,R,V_analysisFPSPreference
paused
TB,GisPaused,V_paused
analyzers
maxConcurrentAnalyzers
TQ,R,V_maxConcurrentAnalyzers
activeAnalyzerCount
transcodingAnalyzerCount
HMICameraVideoAnalyzer
Disabling transcoding because too many transcoding analyzers were created.
%@Video fragment duration: %fs is greater than expected value: %fs
%@Video fragment sequence number: %lu is not equal to expected value: %lu
%@Video fragment has no frames
HMIHomePersonManager settings:%@
Face classification is enabled but home person manager is nil
HMIHomePersonManager is nil, face classification disabled
XPC connection was interrupted, retrying.
Unknown delegate name: %@
%@Failed to start reading of the asset: %@
v32@?0@"AVAsset"8@"HMICameraVideoResourceAttributes"16@"NSError"24
Failed to encode fragment.
Transcoded, bytes: %lu (%f), original bytes %lu 
End analysis: didAnalyze, significant events detected: %@
v16@?0@"HMICameraVideoFrameResult"8
End analysis, time spent: %fs, elapsed time since submission: %fs, predicted: %@
Stopping analysis due to high system resource usage
Stopping analysis due to cancelling
Stopping analysis due to entering full bypass mode
Stopping analysis due to analysis time past the maximum fragment analysis time: %fs
Finished early @ %@
Frame selector produced 0 frames
Analyzed frame:%lu, Events:%@
Failed saving activity zone data into file
[%@] <%lu> Object coordinate:%@
@"NSSet"16@?0@"HMIVideoAnalyzerEvent"8
outcome
sessionId
framesAnalyzedFragment
underlyingError
personScore
personConfidence
petScore
petConfidence
vehicleScore
vehicleConfidence
Uploading analytics event %@
com.apple.HomeKit.VideoAnalyzerStats
@"NSMutableDictionary"8@?0
com.apple.HomeAI
homeai: %@
camera.video.analyzer
internalPendingRequests
T@"NSMutableArray",R,V_internalPendingRequests
lastRequestSubmissionTime
T@"NSDate",&,V_lastRequestSubmissionTime
history
T@"HMICameraVideoAnalyzerHistory",R,V_history
streamAnalyzer
T@"HMIVideoAnalyzer",R,V_streamAnalyzer
currentRequest
T@"HMICameraVideoAnalyzerRequest",&,V_currentRequest
scheduler
T@"HMICameraVideoAnalyzerScheduler",R,V_scheduler
mediaIntegritySequenceNumber
TQ,V_mediaIntegritySequenceNumber
skipSequentialMediaIntegrityCheck
TB,R,V_skipSequentialMediaIntegrityCheck
analysisInProgress
TB,V_analysisInProgress
inErrorState
TB,V_inErrorState
inBypassMode
TB,V_inBypassMode
configuration
T@"HMICameraVideoAnalyzerConfiguration",&,V_configuration
remoteAnalysisService
T@"HMIAnalysisService",&,N,V_remoteAnalysisService
sessionEnded
TB,V_sessionEnded
uploadVideoAnalysisEvent
TB,R,GshouldUploadVideoAnalysisEvent,V_uploadVideoAnalysisEvent
saveVideoFramesToDisk
TB,GshouldSaveVideoFramesToDisk,V_saveVideoFramesToDisk
pendingRequests
isActive
T@"<HMICameraVideoAnalyzerDelegate>",W,V_delegate
identifier
T@"NSUUID",R,C,V_identifier
homePersonManager
T@"HMIHomePersonManager",&,V_homePersonManager
externalPersonManagers
T@"NSSet",&,V_externalPersonManagers
HMIMutableFloatArray
T@"NSData",R,&,N
floats
Tr^f,R,N
mutableFloats
T^f,R,N
Face below HMIFaceQualityEntropyOfLaplacian threshold: score = %f, discarding
personsModelPredictions is empty
Unable to initialize featureVector for SVM model prediction on face quality : %@
SVM model prediction for face quality failed : %@
Face removed from unknown and uncertian bucket, SSD confidence = %f, entropy of laplacian = %f, Face yaw = %f, box size: %f
Added to unknown bucket yaw: %@
Added to uncertain bucket yaw: %@
@16@?0@"HMIPersonsModelPrediction"8
Face recognition set is empty
face.classifier.vip
faceprinter
T@"HMIFaceprinter",R,V_faceprinter
faceQualityFilter
T@"HMIFaceQualityFilterSVM",R,V_faceQualityFilter
classificationThresholdKnown
Td,R,V_classificationThresholdKnown
classificationThresholdUnknown
Td,R,V_classificationThresholdUnknown
thresholdWithLabels
T@"NSDictionary",R,V_thresholdWithLabels
thresholdDefault
T@"NSNumber",R,V_thresholdDefault
Initializing HMICameraVideoFrameAnalyzerFactory
Resetting VideoFrameAnalyzer
Warm starting model...
warm_start_model
Failed to create pixel buffer when warm starting model
Failed to warm start model: %@
Warm start of model took: %f
Model is not bundled into framework
Analyze Frame
Failed handleMotionDetection with error %@
camera.video.frame.analyzer.factory
sharedInstance
T@"HMICameraVideoFrameAnalyzerFactory",R
frameAnalyzer
T@"<HMICameraVideoFrameAnalyzer>",&,N,V_frameAnalyzer
bounds
label
confidence
randomUniform
{CGAffineTransform=dddddd}
name
type
T@"NSString",R,V_name
T@"NSString",R,V_type
T@"HMICIFilterAttributeValue",R,V_value
probability
@16@?0@8
B16@?0@"HMICIFilterAttribute"8
HMIRotate
CIAffineTransform
inputAngle
inputTransform
v16@?0@"HMICIFilterAttribute"8
Td,R,V_probability
T@"NSArray",R,V_attributes
filters
v16@?0@8
com.apple.HomeAIDESPlugin
DES record saving is not permitted.
Saving DES Record, recordInfo: %@, data: %@
Saved DES Record: %@, error: %@
v24@?0@"NSUUID"8@"NSError"16
imageData
T@"NSData",R,V_imageData
createInputTensorWithError %@
HMIDESDataSet
samples
T@"NSArray",R,V_samples
imageName
T@"NSString",R,V_imageName
boxesName
T@"NSString",R,V_boxesName
weightsName
T@"NSString",R,V_weightsName
classesName
T@"NSString",R,V_classesName
bias_b
bias_g
bias_r
is_network_bgr
scale
Label:%d Confidence:%f X:%f Y:%f Width:%f Height:%f Yaw:%@}
labelIndex
Ti,R,V_labelIndex
Td,R,V_confidence
boundingBox
T{CGRect={CGPoint=dd}{CGSize=dd}},R,N,V_boundingBox
q24@?0@"HMIObjectDetection"8@"HMIObjectDetection"16
@16@?0@"HMIObjectDetection"8
<%@: %p> timeStamp: %@, detections: [%@]
<%@: %p> %@
HMIVideoFrameAnalyzer
Analyzing %@
T@"<HMIVideoFrameAnalyzerDelegate>",W,V_delegate
sessionIdentifier
T@"NSUUID",R,V_sessionIdentifier
recognizeFaces
TB,V_recognizeFaces
frameAnalyzerDidAnalyzeFrame
T@?,C,V_frameAnalyzerDidAnalyzeFrame
timeOffset
T{?=qiIq},R,V_timeOffset
width
TQ,R,V_width
height
TQ,R,V_height
generationFrequency
T{?=qiIq},R,V_generationFrequency
frameHeight
TQ,R,V_frameHeight
[Fragment:%lu] Failed to generate poster frame for: %lu
camera.video.poster.frame.generator
posterFramesInternal
T@"NSMutableArray",&,V_posterFramesInternal
T@"HMICameraVideoPosterFrameGeneratorInput",R,V_input
nextGenerationTime
T{?=qiIq},V_nextGenerationTime
HMIFC.ck.u
HMIFC.ck.dr
HMIFC.ck.dc
HMIFC.ck.fbb
PVFC:PVFC
PVFC_FB
PVFC_CB
Ignoring error detecting face in Photos face crop, error: %@
Data Representation
Date Created
Face Bounding Box
Could not create image source
No meta data exists on image
Could not initialize from decoded UUID: %@ dataRepresentation: %@ dateCreated: %@
T@"NSUUID",R,C,V_UUID
T@"NSData",R,C,V_dataRepresentation
T@"NSDate",R,C,V_dateCreated
T{CGRect={CGPoint=dd}{CGSize=dd}},R,V_faceBoundingBox
HMIVideoRetimer
mediaType == kCMMediaType_Video
T@"<HMIVideoRetimerDelegate>",W,V_delegate
retimerDidRetimeSampleBuffer
T@?,C,V_retimerDidRetimeSampleBuffer
allocator<T>::allocate(size_t n) 'n' exceeds maximum supported size
Descriptor count = 
Descriptor length = 
 bytes
 = [
manufacturer
model
HMICamera
Identifier
Name
Manufacturer
Model
T@"NSUUID",R,V_identifier
T@"NSString",R,V_manufacturer
T@"NSString",R,V_model
Descriptor vectors nil
Error
UpdatePersonsModelTask
RemovePersonsModelTask
HomePersonClusteringTask
TuriTrialUpdateTask
FaceMisclassificationTask
PersonsModelsSummaryTask
FeedbackTask
EmptyTask
taskType
sourceUUID
homeUUID
isExternal
cameraProfileUUID
clipUUID
taskID
Ti,R,V_taskID
results
T@"NSDictionary",R
TaskID: %u running...
options is empty/nil, defaulting to Home persons clustering task
Unknown task type: %@
v32@?0@"HMITask"8Q16^B24
HMIUpdatePersonsModelTaskHomeUUID is nil
HMIUpdatePersonsModelTaskIsExternal is nil
HMIPersonsModelTaskSourceUUID is nil
Creating HMPhotosPersonManager for homeUUID:%@ sourceUUID:%@
Failed to get HMPhotosPersonManager
Creating HMHomePersonManager for homeUUID:%@
Failed to get HMHomePersonManager
Failed to initialize data source
Disk-based Home Person Data Source not supported
Current device is not primary resident, skipping clustering
task.service.server
nextTaskID
Ti,V_nextTaskID
Initializing HMITaskServiceServer
HMITaskService not available on this platform.
task.service
%@ - %f
score
Tf,R,V_score
sparse
@"HMICameraVideoFrame"16@?0@"HMICameraVideoFrameSelectorFrameScore"8
q24@?0@"HMICameraVideoFrame"8@"HMICameraVideoFrame"16
q24@?0@"HMICameraVideoFrameSelectorFrameScore"8@"HMICameraVideoFrameSelectorFrameScore"16
v16@?0@"HMICameraVideoFrame"8
camera.video.frame.selector
sampler
T@"HMICameraVideoFrameSampler",R,V_sampler
sampleRate
T{?=qiIq},R,V_sampleRate
framesInternal
T@"NSMutableArray",R,V_framesInternal
maxFrameCount
Tq,R,V_maxFrameCount
predictedFrames
T@"NSMutableArray",R,V_predictedFrames
detector
T@"<HMIMotionDetector>",R,V_detector
T@"<HMICameraVideoFrameSelectorDelegate>",W,V_delegate
frames
camera.video.frame.sampler
targetInterval
T{?=qiIq},R,V_targetInterval
sampleInterval
T{?=qiIq},R,V_sampleInterval
unmatchedSampleFrames
T@"NSMutableArray",R,V_unmatchedSampleFrames
T@"HMICameraVideoFrame",&,V_frame
markedAsFinished
TB,GisMarkedAsFinished,V_markedAsFinished
T@"<HMICameraVideoFrameSamplerDelegate>",W,V_delegate
allAtCurrentVersion
existingAtOtherVersions
T@"NSSet",R,V_existingAtOtherVersions
createdAtCurrentVersion
T@"NSSet",R,V_createdAtCurrentVersion
existingAtCurrentVersion
T@"NSSet",R,V_existingAtCurrentVersion
Vision
Faceprint Version: %ld.%ld
Warm starting faceprint model...
warm_start_faceprint_model
Failed to create pixel buffer when warm starting faceprint model
Failed to warm start faceprint model: %@
Warm start of faceprint model took: %f
CreateFaceprint
Cropping face %@ from pixel buffer with dimensions: %.1f x %.1f roll: %.02f degrees
Error pixel buffer type conversion.
Error in rotating the face.
Face was rotated by:%.02f degrees
Cropping face %@ from face crop with dimensions %.1f x %.1f
B16@?0@"HMIFaceprint"8
%lu faceprint(s) exist for face crop:%@ but are not the current version
Using existing faceprint for face crop:%@
Faceprinting face crop:%@
Vision run-time version: %d.%02d.%02d (%d)
face.detector.vision
HMIFP.ck.u
HMIFP.ck.d
HMIFP.ck.mu
HMIFP.ck.fcu
modelUUID
faceCropUUID
Data
Model UUID
Face Crop UUID
Could not initialize from decoded UUID: %@ data: %@ modelUUID: %@ faceCropUUID: %@
T@"NSData",R,C,V_data
T@"NSUUID",R,C,V_modelUUID
T@"NSUUID",R,C,V_faceCropUUID
<%@ %@>
point
T{CGPoint=dd},R,V_point
HMPMS.fce
Face Classification Enabled
TB,GisFaceClassificationEnabled,V_faceClassificationEnabled
TB,D,GisFaceClassificationEnabled
com.apple.homeai.model.loader
/var/mobile/Library/Caches/com.apple.HomeAI/model_dir
[Model loading] failed to create asset dir: %@ error: %@
[Model loading] unpackaging
[Model loading] failed to remove dir
[Model loading] failed to remove dir %@ err %@
[Model loading] failed to untar model
[Model loading] failed to untar model asset into %@ err %@
HMIModelLoader
T@"NSObject<OS_dispatch_queue>",&,N,V_queue
pendingUpdates
T@"NSMutableSet",&,N,V_pendingUpdates
clip
faceCrops
authTag
internal
wrappedKey
1.2.840.113635.100.6.27.38
HMIFeedback Queue
HMIFeedback
session
T@"NSURLSession",R,V_session
feedbackServiceHost
T@"NSString",R,V_feedbackServiceHost
homeKitClient
T@"HMIHomeKitClient",R,V_homeKitClient
Trusting host: %@
Force Certificate Pinning
HFFeedbackService
Error setting trust policies: %lu
Invalid certificate: %@
Downloading Clip
Cannot find camera profile.
Cannot find home for camera profile.
Fetched Clip videoAssetContext: %@, error: %@
Cannot download clip asset.
Check network connectivity.
v24@?0@"HMCameraClipVideoAssetContext"8@"NSError"16
Fetching Clip, progress %lu%%
v16@?0Q8
v16@?0@"HMCameraClip"8
Cannot download clip for camera profile.
Ensure the clip belongs to the camera profile.
Fetching Face Crops
@16@?0@"HMCameraClipSignificantEvent"8
v24@?0@"HMCameraClip"8@"NSError"16
Fetching Clip
Clip doesn't have a video track.
%@%@
hkcvml-dev.apple.com
https://%@/v2/clip-uuid/
%@.%@
Uploading payload data: %@, to URL %@
json
application/json
Content-Type
v32@?0@"NSData"8@"NSURLResponse"16@"NSError"24
Invalid key size.
Cannot generate initialization vector.
Failed to encrypt data.
Failed to encrypt face crop data.
Stripped Audio %@, error: %@
Result: %@
v24@?0@"NSURL"8@"NSError"16
Downloaded %@, error: %@
Deleting Temporary File %@
Deleted Temporary File %@, error: %@
v16@?0@"NSURL"8
HMIFeedbackSubmitClipOperation
feedbackSession
T@"HMIFeedbackSession",R,V_feedbackSession
T@"NSUUID",R,V_cameraProfileUUID
T@"NSUUID",R,V_clipUUID
temporaryFileURLs
T@"NSMutableArray",R,V_temporaryFileURLs
T@"NSSet",&,V_faceCrops
assetData
T@"NSData",&,V_assetData
HMCameraClipFetchVideoAssetContextOperation
HMIFC.pu
HMIFC.su
HMIFC.seu
HMIFC.fc
HMIFC.fp
HMIFC.c
HMIFC.f
Known
Uncertain
Source UUID
Session Entity UUID
Familiarity
FaceCrop UUID
Faceprint UUID
T@"NSString",R,V_identifier
personsModelIdentifier
T@"NSString",R,V_personsModelIdentifier
T@"NSUUID",R
T@"NSUUID",R,V_personUUID
T@"NSUUID",R,V_sourceUUID
sessionEntityUUID
T@"NSUUID",R,V_sessionEntityUUID
familiarity
Tq,R,V_familiarity
persons.models.summary.task
Failed to fetch face crops with error: %@
@16@?0@"HMIPersonFaceCrop"8
Failed to fetch faceprints with error: %@
Error faceprinting face crops:%@
Person (%@) has no faceprints -- nothing to remove
Nearest face crop to be removed: %@
Failed to remove face crop with error: %@
Successfully removed face crop (%@) via user indicated misclassification
face.misclassification.task
T@"HMIPersonFaceCrop",R,V_faceCrop
Failed to remove persons model, error:%@
Successfully removed persons model
remove.persons.model.task
T@"NSUUID",R,C,V_homeUUID
T@"<HMIPersonManagerDataSource>",W,N,V_dataSource
supportsFaceClassification
TB,V_supportsFaceClassification
%.4f
levelThresholds
T@"NSArray",R,V_levelThresholds
Td,R,V_value
HMIErrorDomain
Unexpected error
Failed to analyze
Failed to read fragment
Failed to verify fragment
Failed to generate poster frame
Failed to transcode fragment
Video analyzer in error state
Model failed to load
Fragment is invalid
No pixel buffer
Unknown error code %ld
HMIErrorCodeUnexpectedError
HMIErrorCodeFailedToAnalyze
HMIErrorCodeFailedToReadFragment
HMIErrorCodeFailedToVerifyFragment
HMIErrorCodeFailedToGeneratePosterFrame
HMIErrorcodeFailedToTranscodeFragment
HMIErrorCodeAnalyzerInErrorState
HMIErrorCodeModelFailedToLoad
HMIErrorCodeFragmentIsInvalid
HMIPrivateErrorCodeNilPixelBuffer
HMIPrivateErrorCodeEmptyURL
HMIPrivateErrorCodeAVAssetReaderInitializationFailed
HMIPrivateErrorCodeFailedToLoadProperty
HMIPrivateErrorCodeAssetLoadCancelled
HMIPrivateErrorCodeReadingStartFailed
HMIPrivateErrorCodeNoTrackOutput
HMIPrivateErrorCodeSampleBufferUnavailable
HMIPrivateErrorCodeNoVideoTrackFound
HMIPrivateErrorCodeMultipleVideoTracksFound
HMIPrivateErrorInvalidPresentationTime
HMIPrivateErrorAVAssetReaderNotStarted
HMIPrivateErrorCodeSequentialIntegrityViolated
HMIPrivateErrorCodeAnalysisServiceNoConfiguration
HMIPrivateErrorCodeLoadingCoreMLModelFailed
HMIPrivateErrorCodeCoreMLPredictionFailed
HMIPrivateErrorCodeCoreMLOutputIncorrect
HMIPrivateErrorCodeCropAndResizeFailed
HMIPrivateErrorCodePosterFrameGenerationFailed
HMIPrivateErrorCodeCodecNotAvailable
HMIPrivateErrorCodeEncodingFailed
HMIPrivateErrorCodeInvalidVideoFrameFormatToSave
HMIPrivateErrorCodeScalerError
HMIPrivateErrorCodeFaceprintingFailed
HMIPrivateErrorCodeUpdatePersonsModelTaskFailed
HMIPrivateErrorCodeExternalPersonSourceDiskError
HMIPrivateErrorCodeLoadPersonsModelsFailed
HMIPrivateErrorCodeUpdatePersonsModelFailed
HMIPrivateErrorCodeRemovePersonsModelFailed
HMIPrivateErrorCodePersonsModelPredictionFailed
HMIPrivateErrorCodeNilDataSource
HMIPrivateErrorCodeUnknownTask
HMIPrivateErrorCodeHomePersonClusteringTaskFailed
HMIPrivateErrorCodeRemovePersonsModelTaskFailed
HMIPrivateErrorCodeFaceMisclassificationTaskFailed
ERROR_%ld
%@: %@
Provided sequenceNumbers: %@, don't match fragment's sequenceNumbers: %@
v12@?0I8
B28@?0I8@"NSData"12@"NSData"20
video/mp4
Failed to read fragment data, err: %d
HMIVideoFragment
[%@]
Initialization Segment Data
Separable Segment Data
Sequence Numbers
Time Range
T@"NSData",R,C
videoFormatDescription
Tr^{opaqueCMFormatDescription=},R,V_videoFormatDescription
audioFormatDescription
Tr^{opaqueCMFormatDescription=},R,V_audioFormatDescription
videoTrackTimeRange
T{?={?=qiIq}{?=qiIq}},R,V_videoTrackTimeRange
audioTrackTimeRange
T{?={?=qiIq}{?=qiIq}},R,V_audioTrackTimeRange
initializationSegment
T@"NSData",R,V_initializationSegment
separableSegment
T@"NSData",R,V_separableSegment
timeRange
T{?={?=qiIq}{?=qiIq}},R,V_timeRange
sequenceNumbers
T@"NSArray",R,V_sequenceNumbers
HMISessionEntityManager
WARNING: received multiple motion detections -- using first
WARNING: no motion detection to use for face tracking
B16@?0@"NSUUID"8
v24@?0@"NSUUID"8^B16
v16@?0@"HMIFaceprint"8
Transition Pr(detectionIdx: %lu, previousIndex: %lu, sessionUUID:%@) = %lf
Face embedding distance to cluster (detectionIdx: %lu, sessionUUID:%@) = %lf
Face embedding distance to previous face (detectionIdx: %lu, sessionUUID:%@) = %lf
Dynamic threshold (detectionIdx: %lu, sessionUUID:%@) = %lf
Adding new sessionEntityUUID: %@
@16@?0@"HMIFaceClassification"8
v32@?0@"HMIVideoAnalyzerEventPerson"8Q16^B24
sessionEntities
T@"NSMutableDictionary",R,V_sessionEntities
HMITrialUpdateTask
v16@?0@"HMIPoint"8
HMIVideoAnnotator
resourceUsageMonitor
T@"<HMISystemResourceUsageMonitorProtocol>",R,V_resourceUsageMonitor
T@"<HMISystemResourceUsageMonitorDelegate>",W,Vdelegate
HMIVideoAnalyzerResultFilter
HMIVideoAnalyzerResultActivityZoneFilter
HMIP.ck.u
HMIP.ck.n
Could not initialize from decoded UUID: %@
T@"NSString",R,C,V_name
externalLibrary
TB,R,GisExternalLibrary,V_externalLibrary
faceCountsByPerson
T@"NSDictionary",R,V_faceCountsByPerson
v16@?0@"<NSObject><NSCopying><NSSecureCoding>"8
HMIPersonsModel
visionPersonsModel
T@"VNPersonsModel",R,V_visionPersonsModel
summary
T@"HMIPersonsModelSummary",R
data:;base64,%@
@24@?0@8@16
%.6f
options
Tq,V_options
objectJSON
T@"NSString",R
objectPrettyJSON
data:;base64,
Invalid JSONObject class name: %@
Invalid JSONObject: %@
Cannot parse JSON data: %@
container
T@,R,V_container
classMap
T@"NSDictionary",&,V_classMap
yyyy-MM-dd'T'HH:mm:ss.SSS'Z'
Error creating directory:%@ to save face classifications
faceprintUUID
%@-%03lu-%03u-%@-%@-%@
known
unknown
Error saving metadata to disk for face classification:%@
Error saving face crop image to disk for face classification:%@
Saved face classification:%@ to disk
face.utilities
frameDimensions.width > 0 && frameDimensions.height > 0
v32@?0@"HMISparseOpticalFlowFeatureVector"8Q16^B24
HMIPersonBlob (%@): {center = (%f, %f),  size = (%f, %f), source = %@}
detection
projection
frameDimensions
T{CGSize=dd},R,V_frameDimensions
T{CGRect={CGPoint=dd}{CGSize=dd}},V_faceBoundingBox
personBoundingBox
T{CGRect={CGPoint=dd}{CGSize=dd}},V_personBoundingBox
position
T{CGPoint=dd},V_position
motionMean
T{CGVector=dd},V_motionMean
personIndex
T@"NSNumber",&,V_personIndex
personIndices
T@"NSMutableSet",&,V_personIndices
personIouMax
Tf,V_personIouMax
blobID
T@"NSUUID",R,V_blobID
projectedFaceIndex
TQ,R,V_projectedFaceIndex
detectedFaceIndex
TQ,R,V_detectedFaceIndex
Td,R,V_score
q24@?0@"HMIFaceTrackerMatch"8@"HMIFaceTrackerMatch"16
motionVectors
T@"NSArray",R,V_motionVectors
q24@?0@"HMIFaceTrackerMotionRecord"8@"HMIFaceTrackerMotionRecord"16
HMIFaceTracker
Handling motion detection update for sessionPTS: %f
@"NSArray"16@?0@"HMISparseOpticalFlowMotionDetection"8
newPersonWithFaceEvents.count > 0 || personWithoutFaceEvents.count > 0
@"HMIPersonBlob"16@?0@"HMIVideoAnalyzerEventPerson"8
Initial faces consumed
Timeout reached -- resetting face tracker
@"HMIPersonBlob"16@?0@"HMIPersonBlob"8
v32@?0@"HMIPersonBlob"8Q16^B24
v16@?0@"HMIFaceTrackerMatch"8
B16@?0@"HMIPersonBlob"8
v24@?0Q8^B16
v32@?0@"HMIFaceTrackerMatch"8Q16^B24
v32@?0@"NSNumber"8@"NSNumber"16^B24
v32@?0@"NSUUID"8@"NSNumber"16^B24
v32@?0@"NSNumber"8Q16^B24
previousPersons
T@"NSArray",&,V_previousPersons
previousPersonIndices
T@"NSArray",&,V_previousPersonIndices
motionRecordsQueue
T@"NSMutableArray",R,V_motionRecordsQueue
previousTime
T{?=qiIq},V_previousTime
Failed to initialize HMICameraVideoResourceAttributes from fragment data, err: %d
camera.video.resource.attributes
assetDuration
T{?=qiIq},R,V_assetDuration
T@"NSDate",R,V_creationDate
firstSequenceNumber
TQ,R,V_firstSequenceNumber
nominalFrameRate
Td,R,V_nominalFrameRate
dimensions
T{CGSize=dd},R,V_dimensions
baseDecodeTimeStamp
T{?=qiIq},R,V_baseDecodeTimeStamp
firstFragmentSequenceNumber
fragmentCount
formatDescriptions
[Fragment:%lu] Creating asset with url: %@
[Fragment:%lu] Failed to initialize AVAssetReader
[Fragment:%lu] Start %0.2f, Duration %0.2f, FPS %0.2f
[Fragment:%lu] No fragment URL found on loading request %@
[Fragment:%lu] No fragment data found for URL %@
[Fragment:%lu] Finished handling load request
camera.video.asset.reader
currentFrameId
TQ,V_currentFrameId
T@"AVAssetReader",&,N,V_assetReader
resourceLoaderWorkQueue
T@"NSObject<OS_dispatch_queue>",R,V_resourceLoaderWorkQueue
T@"HMICameraVideoFragment",R,V_videoFragment
logIdentifier
T@"NSString",R,V_logIdentifier
com.apple.videotoolbox.videoencoder.h264.rtvc
Error in VTCompressionSessionEncodeFrameWithOutputHandler %d
Frame dropped.
Encoded:  %@
v24@?0i8I12^{opaqueCMSampleBuffer=}16
Error Calling VTCompressionSessionEncodeFrameWithOutputHandler, error: %d
Attempting recovery.
Recovered successfuly.
Failed to encode sample after resetting encoder session.
Failed to recover, error %@
Video encoder is not running, ignoring %@
Failed to encode and recover from failure.
HMIVideoEncoder
T@"<HMIVideoEncoderDelegate>",W,V_delegate
averageBitRate
maxKeyFrameIntervalDuration
numberOfDroppedFrames
TQ,R,V_numberOfDroppedFrames
encoderDidEncodeSampleBuffer
T@?,C,V_encoderDidEncodeSampleBuffer
encoderDidFailWithError
T@?,C,V_encoderDidFailWithError
addFaceCrops:%@
@16@?0@"HMIFaceCrop"8
addPersonFaceCrops:%@
addPersons:%@
@16@?0@"HMIPerson"8
fetchAllUnassociatedFaceCropsWithCompletion
@16@?0@"HMFaceCrop"8
removeFaceCropsWithUUIDs:%@
removePersonsWithUUIDs:%@
associateFaceCropsWithUUIDs:%@ toPersonWithUUID:%@
home.person.datasource.homekit
T@"HMHomePersonManager",&,V_homePersonManager
HMFaceCrop
HMPersonFaceCrop
HMMutablePerson
HMPMS.ifple
HMPMS.sfce
Importing From Photo Library Enabled
Sharing Face Classifications Enabled
shortDescription
privateDescription
propertyDescription
attributeDescriptions
T@"NSArray",R,C,N
importingFromPhotoLibraryEnabled
TB,GisImportingFromPhotoLibraryEnabled,V_importingFromPhotoLibraryEnabled
sharingFaceClassificationsEnabled
TB,GisSharingFaceClassificationsEnabled,V_sharingFaceClassificationsEnabled
TB,D,GisImportingFromPhotoLibraryEnabled
TB,D,GisSharingFaceClassificationsEnabled
detectionScore
laplacianScore
homeFamiliarity
v16@?0@"HMIFaceClassification"8
q24@?0@"NSNumber"8@"NSNumber"16
externalFamiliarity
com.apple.HomeAI.FaceEvent
faceprintsClustered
clusters
personsCreated
faceprintsAssociated
faceprintingDuration
clusteringDuration
totalDuration
errorCode
errorDescription
com.apple.HomeAI.FaceClustering
v24@?0@"NSString"8@"NSNumber"16
v16@?0@"HMIPersonsModelSummary"8
@avg.self
externalLibraries
homeIdentities
averageExternalIdentities
averageHomeFaceCrops
averageExternalFaceCrops
homeToExternal
externalToExternal
com.apple.HomeAI.PersonsModels
@"NSDictionary"8@?0
HMIAnalytics
home
external
T@"NSUUID",R,C,V_sourceUUID
modelSummaries
T@"NSSet",R,V_modelSummaries
homeToExternalEquivalencies
TQ,R,V_homeToExternalEquivalencies
externalToExternalEquivalencies
TQ,R,V_externalToExternalEquivalencies
T@"NSNumber",R,V_confidence
linkedEntityUUID
T@"NSUUID",R,V_linkedEntityUUID
Number of all face observations: %ld
v32@?0@"<NSObject><NSCopying><NSSecureCoding>"8Q16^B24
@"NSDictionary"24@?0@"NSUUID"8@"HMIPersonsModel"16
Comparing persons (%@, %@)
Equivalency determined between pair: (%@, %@)!
v32@?0@"HMIPersonSourceUUIDPair"8@"NSSet"16^B24
v24@?0@"HMIPersonSourceUUIDPair"8^B16
v32@?0@"NSUUID"8@"NSSet"16^B24
v32@?0@"NSUUID"8@"NSDictionary"16^B24
v24@?0@"VNFaceObservation"8^B16
Error adding faceprints to model for personUUID: %@
v32@?0@8@16^B24
Stale Home VNPersonsModel with sourceUUID: %@ detected, attempting to remove...
Failed to remove stale Home VNPersonsModel with sourceUUID: %@, error getting model storage path
Failed to remove stale Home VNPersonsModel with sourceUUID: %@
Failed to persist VNPersonsModel for sourceUUID: %@, error getting model storage path
Persisted VNPersonsModel for sourceUUID: %@
Failed to persist VNPersonsModel for sourceUUID: %@, path: %@
Did not remove VNPersonsModel for sourceUUID: %@, no model found
Failed to remove VNPersonsModel for sourceUUID: %@, error getting model storage path
Removed VNPersonsModel for sourceUUID:%@
Failed to remove VNPersonsModel for sourceUUID: %@, path: %@
Home persons model not found
Failed to predict using VNPersonsModel for home persons model
Unable to build equivalency map: %@
Failed to predict using VNPersonsModel for sourceUUID: %@
Failure to lookup equivalency cell for %@ (self.personToEquivalencyCell is%@ nil)
 not
v32@?0@"NSUUID"8@"HMIPersonsModel"16^B24
v32@?0@"NSSet"8@"HMIPersonsModelPrediction"16^B24
%@.bin
Directory to load/store persons models (%@) does not exist
Failed to enumerate external persons models at path (%@)
Persons Model Storage Path:%@
Loaded External HMIPersonsModel for sourceUUID: %@
Directory to load/store home persons model (%@) contains multiples files: %@ there can only be one
No home model found
Loaded Home HMIPersonsModel for sourceUUID: %@
Invalid file path in load model attempt: %@
Failed to load VNPersonsModel at path: %@
@24@?0@"NSUUID"8@"HMIPersonsModel"16
Failed to generate persons models summary, error: %@
persons.model.manager
T@"HMIPersonsModelManager",R
homePersonsModel
T@"HMIPersonsModel",R,V_homePersonsModel
personsModels
T@"NSDictionary",&,V_personsModels
personToEquivalencyCell
T@"NSDictionary",R,V_personToEquivalencyCell
T@"HMIPersonsModelsSummary",R
weights
T@"HMIDESMutableFloatArray",R,V_weights
biases
T@"HMIDESMutableFloatArray",R,V_biases
layerParameters
T@"NSArray",R,V_layerParameters
losses
T@"NSArray",R,V_losses
HMIDESTrainer
is_training
checkpoint
Loss/total_loss
trainingCallback %lu, loss: %f
B32@?0Q8@"<ETDataProvider>"16@"<ETTaskContext>"24
Training was skipped because %@ is YES.
Training Started
Training Finished
v16@?0@"HMIDESLayerParameters"8
T@"HMIDESDataset",R,V_data
networkPath
T@"NSURL",R,V_networkPath
Unknown reason.
Skipped
{code: %@, message: "%@"}
{code: %@}
success
T@"HMIVideoAnalyzerResultOutcome",R
skipped
code
TQ,R,V_code
isSkipped
isSuccess
message
T@"NSString",R,V_message
v24@?0@"HMIExternalPersonManagerSettings"8@"NSError"16
Timer fired, updating external persons model
external.person.manager
Settings have disabled face classification, removing external persons model
Settings have enabled face classification, updating external persons model
T@"HMIExternalPersonManagerSettings",R,V_settings
T@"<HMIExternalPersonManagerDataSource>",W,N,V_dataSource
Failed to remove persons model, error:%@, retrying...
Submitted persons model remove task, taskID:%u, retryOnError:%@
%@ (%@)
remove.persons.model.operation
T@"NSUUID",R,V_homeUUID
TB,R,V_external
HMIVideoAnalyzerScheduler
v32@?0@"HMIVideoAnalyzer"8Q16^B24
Scheduler state: 
usage: %@
, mem: %@
, max: %@
v16@?0@"HMIVideoAnalyzer"8
SignificantActivity.mlmodelc
HOMEAI_SIGNIFICANT_ACTIVITY_DETECTOR
v16@?0@"<TRINamespaceUpdateProtocol>"8
Submitted task returned error: %@
compiledModel
personThresholdHigh
personThresholdMedium
petThresholdHigh
petThresholdMedium
vehicleThresholdHigh
vehicleThresholdMedium
faceThreshold
HMITuriTrialManager
Td,R,V_personThresholdHigh
Td,R,V_personThresholdMedium
Td,R,V_petThresholdHigh
Td,R,V_petThresholdMedium
Td,R,V_vehicleThresholdHigh
Td,R,V_vehicleThresholdMedium
Td,R,V_faceThreshold
modelPath
T@"NSString",R,V_modelPath
Clustering successful
Clustering error
greedy_clustering
descriptorA.length == descriptorB.length
cluster.objects.count > 0
faceObservations.count > 0
faceObservations.firstObject.faceprint != nil
faceprintLength == faceObservation.faceprint.lengthInBytes / sizeof(float)
HMIVideoAnalyzerLegacy
Analyzing fragment: %@, configuration: %@
Motion was assumed because AnyMotion was turned on.
Fragment analysis was skipped.
v16@?0@"HMIVideoAnalyzerFrameResult"8
@16@?0@"HMICameraVideoPosterFrame"8
Analyzed fragment result %@, with configuration: %@
legacyAnalyzer
T@"HMICameraVideoAnalyzer",R,V_legacyAnalyzer
HMIVideoFrameSampler
T@"<HMIVideoFrameSamplerDelegate>",W,V_delegate
HMIVideoFrameIntervalSampler
frameSamplerDidSamplerFrame
T@?,C,V_frameSamplerDidSamplerFrame
HMIVideoPackageAnalyzer
Detecting Package
T@"<HMIVideoPackageAnalyzerDelegate>",W,V_delegate
packageAnalyzerDidDetectPackage
T@?,C,V_packageAnalyzerDidDetectPackage
HMIFaceQuality
entropyOfLaplacianScore
T@"NSNumber",R,V_entropyOfLaplacianScore
HMIFaceQualityEntropyOfLaplacian
HMIFaceQualityDistanceToJunkCluster
HMIVideoAnalyzerFragmentResult
@"NSArray"16@?0@"HMIVideoAnalyzerFrameResult"8
{maxConfidenceEvents: [%@] frameResults: [%@], thumbnails: %@, fragment: %@}
T@"HMIVideoFragment",R,V_fragment
thumbnails
T@"NSArray",R,V_thumbnails
T@"HMIVideoAnalyzerResultOutcome",R,V_outcome
T@"HMIVideoAnalyzerDynamicConfiguration",R,V_configuration
HMICVFR.f
HMICVFR.roi
HMICVFR.ae
HMICVAR.rc
HMICVAR.fr
HMICVAR.cd
HMICVAR.d
HMICVAR.pf
HMICVAR.ttaf
HMICVAR.tsfws
HMICVAR.lsn
HMICVAR.vf
HMICVAR.af
HMICVAC.pfgi
HMICVAC.pfh
HMICVAC.mfad
HMICVAC.mfd
HMICVAC.st
HMICVAC.smisn
HMICVAC.tf
HMICVAC.fce
HMICVAC.csd
HMICVASE.e
HMICVASE.vf
HMICVASE.as
HMICVF.et
HMICVF.sn
HMICVF.mf
HMICVF.az
HMICVF.d
HMICVPF.d
HMICVPF.to
HMICVPF.w
HMICVPF.h
HMIOD.l
HMIOD.c
HMIOD.b.x
HMIOD.b.y
HMIOD.b.w
HMIOD.b.h
HMIOD.y.a
HMIOD.r.o
HMIP.x
HMIP.y
HMIAZ.p
HMIAZ.i
HMICameraVideoFrame does not support NSSecureCoding in the simulator
 isInclusion:%d 
class-label
coordinates
overlap
inclusion
activityZone
%@-%@-%@-%@.json
%@/%@
Error creating activity zone result directory: %@
%@/activityzone-%@
Activity zone file path:%@
Error converting activity zone results to JSON: %@
Error writing activity zone results JSON to file: %@
B16@?0@"HMICameraActivityZone"8
Activity zones coordinates:%@
Inclusion zone:%@ intersecting with:(%@) Object coordinate %@
Exclusion zone:%@ intersecting with:(%@) Object coordinate %@
Events after activity zone filtering:(%@) Object coordinate %@
zoneType
filteringLevel
numDetectedObjects
com.apple.HomeAI.ActivityZone
noFiltering
resize_0
resize_10
resize_20
exclusion
resize_26
resize_36
camera.activity.zone
points
T@"NSArray",R,C,V_points
TB,R,GisInclusion,V_inclusion
HMIVideoAnalyzerConfiguration
Thumbnail Interval
Thumbnail Height
Max Fragment Duration
Max Fragment Analysis Duration
Transcode
Min Frame Quality
Min Frame Scale
Camera
Recognize Faces
minFrameQuality > 0 && minFrameQuality <= 1
minFrameScale > 0 && minFrameScale <= 1
thumbnailInterval
T{?=qiIq},V_thumbnailInterval
thumbnailHeight
TQ,V_thumbnailHeight
Td,V_maxFragmentAnalysisDuration
T{?=qiIq},V_maxFragmentDuration
transcode
TB,V_transcode
camera
T@"HMICamera",&,V_camera
minFrameQuality
Td,V_minFrameQuality
minFrameScale
Td,V_minFrameScale
Event Triggers
Detect Packages
Activity Zone Count
HMIVideoAnalyzerDynamicConfiguration
detectPackages
TB,V_detectPackages
eventTriggers
Tq,V_eventTriggers
T@"NSArray",&,V_activityZones
T{CGRect={CGPoint=dd}{CGSize=dd}},R,V_boundingBox
Unsupported aspect ratio: (%d, %d)
preferenceOverrides
videoAnalyzerIdentifier
didAnalyzeFragment
didNotAnalyzeFragment
didFindSignificantEvent
result
HMIAnalysisServiceTypeUnknown
HMIAnalysisServiceTypeLocal
HMIAnalysisServiceTypeRemoteFragment
HMIAnalysisServiceTypeUndefined
v24@?0@"HMICameraVideoFragment"8@"HMICameraVideoAnalyzerResult"16
v24@?0@"HMICameraVideoFragment"8@"NSError"16
T@?,C,N,V_didAnalyzeFragment
didFailAnalysisForFragment
T@?,C,N,V_didFailAnalysisForFragment
T@?,C,N,V_didFindSignificantEvent
T@?,C,N,V_didNotAnalyzeFragment
HMIAnalysisService
Remote analysis not supported in the simulator
Asset data is ignored if the video fragment is passed through the properties dictionary
Analyzer configuration property key (%@) is missing
v24@?0@"HMICameraVideoAnalyzerSignificantEvent"8@"HMICameraVideoFragment"16
camera.video.analysis.service
nextRequestID
Ti,V_nextRequestID
requests
T@"NSMapTable",R,V_requests
runRemotely
TB,V_runRemotely
HMIVAEP.f
Face
HMIVideoAnalyzerEventPerson
face
T@"HMIVideoAnalyzerEventFace",R,V_face
com.apple.homeai.pfl
PFLPrivatize Failed
Privatized Training Result
Encrypted Training Result
PFLPrivatizeCustomNorm
/System/Library/PrivateFrameworks/PrivateFederatedLearning.framework/PrivateFederatedLearning
com.apple.homed
syntheticEvents
personDetected
petDetected
vehicleDetected
analysisQOS
analysisServiceType
motionDetector
frameSelectorSampleRateMultiplier
frameSelectorTargetInterval
processingDevice
confidenceThresholdPersonHigh
confidenceThresholdPetHigh
confidenceThresholdVehicleHigh
confidenceThresholdPersonMedium
confidenceThresholdPetMedium
confidenceThresholdVehicleMedium
modelTimeout
assetReaderMaximizePowerEfficiency
assetReaderForceBGRA
analysisEnableTranscodeFragment
realTimeEncoding
maxConcurrentAnalysisRequests
espressoLowPriority
opticalFlowLowPriority
opticalFlowBackgroundProcessing
enableAnalyzerHistory
saveDESRecords
DESSkipTraining
DESSkipTrainingScalar
DESSkipPrivatize
confidenceThresholdFace
personsModelStoragePath
personDataSourceDiskStoragePath
personsModelClassificationThresholdKnown
personsModelClassificationThresholdUnknown
saveFaceCropsToDisk
enableSavingActivityZoneInfo
annotateVideo
showROI
useDevelopmentFeedbackService
user-interactive
user-initiated
unspecified
default
utility
background
Preference %@ is now %@, previously was %@
Only NSNumber and NSString properties are supported.
HMIPreference
T@"HMIPreference",R
qosMap
isProductTypeJ42
isProductTypeJ105
isProductTypeB238
isInternalInstall
preferenceCacheFlushTimer
T@"HMFTimer",R,V_preferenceCacheFlushTimer
preferenceCache
T@"NSMutableDictionary",R,N,V_preferenceCache
preferenceLoggedValues
T@"NSMutableDictionary",R,N,V_preferenceLoggedValues
preferenceOverridesInternal
T@"NSMutableDictionary",R,N,V_preferenceOverridesInternal
usesCPUOnly
home.person.datasource.disk
Already started listening for the notification
HMINotifydObserver
T@"NSObject<OS_dispatch_queue>",R,N,V_queue
callback
T@?,R,N,V_callback
token
Ti,N,V_token
notificationName
Tr*,R,N,V_notificationName
HMIVideoAnalyzerEventVehicle
com.apple.HomeAI.%@%@%@.%tu
Faceprints
Clusters
Persons
Associated
Faceprinting Duration
Clustering Duration
Total Duration
numberOfFaceprintsClustered
Tq,V_numberOfFaceprintsClustered
numberOfClusters
Tq,V_numberOfClusters
numberOfPersonsCreated
Tq,V_numberOfPersonsCreated
numberOfUnknownFaceprintsAssociated
Tq,V_numberOfUnknownFaceprintsAssociated
Td,V_faceprintingDuration
Td,V_clusteringDuration
Td,V_totalDuration
T@"NSError",&,V_error
HMIHomePersonClusteringTask
Error performing cloud pull:%@
Error fetching face crops:%@
@"NSUUID"16@?0@"HMIFaceCrop"8
Error fetching faceprints:%@
Storing newly created faceprints: %@
Error saving new faceprints:%@
Number of faceprints to cluster: %lu
Clustering error:%@
Number of clusters: %lu
Cluster of size %lu beneath threshold of %d
Face prediction error:%@
Error adding new persons:%@
@16@?0@"NSNumber"8
Error associating face crops with person (%@): %@
v32@?0@"VNCluster"8@"NSUUID"16^B24
Error associating face crops for %lu person%@: (
 ...
T@"HMIClusteringTaskSummary",R,V_summary
startTime
T@"NSDate",R,V_startTime
Fetching persons
Error fetching persons, error:%@
Fetched %lu persons
Exiting early because task was canceled.
Fetching face crops for person: %@
Error fetching facecrops for person:%@, error:%@
Fetched %lu face crops for person: %@
Ignoring error fetching faceprints for person:%@, error:%@
Error faceprinting face crops for person:%@, error:%@
Removing existing faceprints at other versions: %@
Failed to generate persons model, error:%@
Successfully updated persons model
update.persons.model.task
T@"<HMIPersonManagerDataSource>",R,V_dataSource
Error fetching persons:%@
fetch.persons.operation
persons
T@"NSSet",R,V_persons
Error fetching face crops for person:%@, error:%@
fetch.personfacecrops.operation
person
T@"HMIPerson",R,V_person
personFaceCrops
T@"NSSet",R,V_personFaceCrops
Error fetching faceprints for face crop UUIDs:%@, error:%@
fetch.faceprints.operation
faceCropUUIDs
T@"NSSet",R,V_faceCropUUIDs
faceprints
T@"NSSet",R,V_faceprints
Storing faceprints:%@ failed with error:%@
Storing faceprints:%@ completed successfully
store.faceprints.operation
Removing faceprints:%@ failed with error:%@
Removing faceprints:%@ completed successfully
remove.faceprints.operation
T@"NSSet",R,V_faceprintUUIDs
cropRect
T{CGRect={CGPoint=dd}{CGSize=dd}},R,V_cropRect
T@"HMICameraVideoFrame",R,W,V_frame
T@"NSError",R,V_error
HMICameraVideoFrameAnalyzerSignificantActivity
none
@"NSString"16@?0@"NSObject"8
high
medium
q24@?0@"HMIVideoAnalyzerEvent"8@"HMIVideoAnalyzerEvent"16
ClassifyFaceEvent
Face classification failed for face: %@, error: %@, removing face event from person event
Face: %@ didn't produce any classifications, removing face event from person event
v24@?0@"HMIVideoAnalyzerEvent"8^B16
camera.video.frame.analyzer.significant.activity
classHierarchyMap
mediumConfidenceThresholds
T@"NSDictionary",R,V_mediumConfidenceThresholds
highConfidenceThresholds
T@"NSDictionary",R,V_highConfidenceThresholds
regionOfInterestOperationQueue
T@"NSOperationQueue",R,V_regionOfInterestOperationQueue
regionOfInterestOperations
T@"NSMapTable",R,V_regionOfInterestOperations
significantActivityDetector
T@"HMISignificantActivityDetector",R,V_significantActivityDetector
faceClassifier
T@"HMIFaceClassifierVIP",R,V_faceClassifier
transaction
T@"HMFOSTransaction",&,N,V_transaction
sessionEntityManagers
T@"NSCache",R,V_sessionEntityManagers
HMIDESBT.u
HMIDESBackgroundTask
Unable to archive task %@: %@
T@"NSURL",R,V_url
HMIVideoFrame
scale > 0 && scale <= 1
quality > 0 && quality <= 1
HMIVideoFrame failed to create compressed representation.
]1337;File=inline=1;preserveAspectRatio=1:%@
Data (JPEG)
CVPixelBuffer
Presentation Time Stamp
Size
Store
HMIVideoFrame does not support NSSecureCoding in the simulator.
presentationTimeStamp
T{?=qiIq},R,V_presentationTimeStamp
[Model loading] Error opening encrypted file
[Model loading] Error reading mmaped encrypted file
[Model loading] Error reading data from compressed file
[Model loading] returning unsuccessfully post decompression due to invalid destination size
[Model loading] Error in out file.
/scratch.data
failed stat %s
[Model loading] failed to unpackage resources
[Model loading] unpackaged resources version
HMICompressionHelper
/Library/Spotlight/Backup/temp_%lu.dat
Error in opening temporary file.
preallocated temporary file failed. %d
Serious error in writing temporary file. %d
HMIVideoBuffer
T@"<HMIVideoCommandBufferDelegate>",W,V_delegate
videoSampleCount
videoDelay
videoDuration
bufferWillHandleSampleBuffer
T@?,C,V_bufferWillHandleSampleBuffer
bufferWillFlush
T@?,C,V_bufferWillFlush
bufferWillFinish
T@?,C,V_bufferWillFinish
HMIVideoAnalyzerEventPet
HMIVideoAnalyzerEvent
Events file "%@" does not exist.
Cannot read events from file "%@", error: %@
Person
Vehicle
Motion
Cannot load events file, exception: %@
eventClasses
T@"HMIConfidence",R,V_confidence
confidenceLevel
HMIVideoDecoder
Cannot reorder frames.
Decoded:  %@
Video decoder is not running, ignoring %@
Sample buffer has no samples, skipping.
Invalid DTS, expected > %@, got %@, skipping.
Format description is missing.
Cannot create decoder.
Cannot decode frame, err: %d.
T@"<HMIVideoDecoderDelegate>",W,V_delegate
decoderDidDecodeSampleBuffer
T@?,C,V_decoderDidDecodeSampleBuffer
decoderDidFailWithError
T@?,C,V_decoderDidFailWithError
Frame decode error %d
HMIVideoAssetWriter
HMIVideoAssetWriterOutput
This operation cannot be completed because the asset writer has already started.
UTType class is not available.
Cannot add video input.
Cannot add audio input.
Failed to start writing, assetWriter.status: %ld, assetWriter:.error: %@
Started writing at %@
didOutputSegmentData segmentType: %ld
B16@?0@"AVAssetSegmentTrackReport"8
Asset writer has failed fatally, ignoring %@
Failed to start asset writer.
Underlying asset writer has failed.
Failed to restart asset writer.
Restarted asset writer.
Dropped  %@ because an input for the media type was not found.
Dropped  %@ because asset writer is waiting for a sync sample.
Wrote    %@
Dropped  %@ because of input error %@
Dropped  %@ because an input %@ is not ready for more media data.
Finished waiting for flushSegment, assetWriter.status: %ld, assetWriter.error: %@
T@"<HMIVideoAssetWriterDelegate>",W,V_delegate
allowRecovery
TB,V_allowRecovery
nextSequenceNumber
TQ,V_nextSequenceNumber
assetWriterDidOutputInitializationSegment
T@?,C,V_assetWriterDidOutputInitializationSegment
assetWriterDidOutputSeparableSegment
T@?,C,V_assetWriterDidOutputSeparableSegment
assetWriterDidFailWithError
T@?,C,V_assetWriterDidFailWithError
B16@?0@"AVAssetWriterInput"8
Failed to update persons model, error:%@, retrying...
Failed to update persons model, error:%@
Submitted persons model update task, taskID:%u, retryOnError:%@
update.persons.model.operation
origin
T{CGPoint=dd},R,V_origin
target
T{CGPoint=dd},R
midpoint
motion
T{CGVector=dd},R,V_motion
eventType
Tq,V_eventType
Time %@
Sparse Optical Flow
camera.video.sparse.optical.flow
T@"NSMutableArray",R,V_frames
sbuf
T^{opaqueCMSampleBuffer=},R,V_sbuf
HMIVideoFrameSelector
Reference: %@, Target: %@
Adding Candidate: %@
q24@?0@"HMIVideoFrameSelectorFrameCandidate"8@"HMIVideoFrameSelectorFrameCandidate"16
Candidates: %@
v16@?0@"HMIVideoFrameSelectorFrameCandidate"8
Selected: %@
Setting sampleRate: %f, referenceInterval: %@, targetInterval: %@
T@"<HMIVideoFrameSelectorDelegate>",W,V_delegate
frameSelectorDidSelectFrame
T@?,C,V_frameSelectorDidSelectFrame
HMIFR.c
HMIFR.fc
HMIFR.fp
Face Classifications
HMIFaceRecognition
predictedLinkedEntityUUIDs
T@"NSSet",R,V_predictedLinkedEntityUUIDs
classifications
T@"NSSet",R,V_classifications
selector
arguments
Creating analyzer with identifier: %@, configuration: %@
HMIVideoAnalyzer does not support remote analysis.
T@"HMIVideoAnalyzerConfiguration",R,C,V_configuration
T@"NSDictionary",R,C,V_options
stateDescription
delay
Td,N
monitored
TB,N
T@"<HMIVideoAnalyzerDelegate>",W,V_delegate
TQ,R,V_status
HMIVideoAnalyzerServer
Recieved Message: %@
Unknown %@
Sent Message Reply: %@
dynamicConfiguration
Video fragment duration: %fs is greater than the max fragment duration value: %fs
Handling: %@
Cancel is not yet implemented.
>> -[HMIVideoAnalyzerServer flush]
v16@?0@"HMIVideoAnalyzerResultFilter"8
Bundling Fragment Result, timeRange: %@, frames: [%@], thumbnails [%@]
Frame analyzer result buffer should be empty. %@
Thumbnail buffer should be empty. %@
@16@?0@"HMIVideoFrameAnalyzerResult"8
Analyzer Failed: %@
Sending Result: %@
 %.2f
: buffer: %4.1fs %3ldf
, current: %4.1fs %3ldf
, results: [%ld, %ld]
, frame: %@
, delay: %@
, enc: %@
, res: %lu
, thb: %lu
analyzerDidAnalyzeFrame
T@?,C,V_analyzerDidAnalyzeFrame
analyzerDidAnalyzeFragment
T@?,C,V_analyzerDidAnalyzeFragment
analyzerDidFailWithError
T@?,C,V_analyzerDidFailWithError
/tmp/com.apple.HomeAI/familiar-face-data
Error initializing with home UUID: %@ source UUID:%@
Error enumerating persons
Error loading person from JSON
Error enumerating face crops for person UUID:%@
Error loading face crop from JSON
Error loading face crop image
person.datasource.disk
sourceURL
T@"NSURL",R,V_sourceURL
Tq,N,V_classLabel
v24@?0@"MLModel"8@"NSError"16
T@"MLModel",R,N,V_model
T^{__CVBuffer=},N,V_image__Placeholder__0
T@"MLMultiArray",&,N,V_ShotNet__ShotNet__ssd_predictions__block8_box__yaw_angle__0
T@"MLMultiArray",&,N,V_ShotNet__ShotNet__ssd_predictions__block8_box__roll_angle__0
T@"MLMultiArray",&,N,V_ShotNet__ShotNet__ssd_predictions__block8_box__box_offset__0
T@"MLMultiArray",&,N,V_ShotNet__ShotNet__ssd_predictions__block8_box__class_prob__0
T@"MLMultiArray",&,N,V_ShotNet__ShotNet__ssd_predictions__block7_box__yaw_angle__0
T@"MLMultiArray",&,N,V_ShotNet__ShotNet__ssd_predictions__block7_box__roll_angle__0
T@"MLMultiArray",&,N,V_ShotNet__ShotNet__ssd_predictions__block7_box__box_offset__0
T@"MLMultiArray",&,N,V_ShotNet__ShotNet__ssd_predictions__block7_box__class_prob__0
T@"MLMultiArray",&,N,V_ShotNet__ShotNet__ssd_predictions__block6_box__yaw_angle__0
T@"MLMultiArray",&,N,V_ShotNet__ShotNet__ssd_predictions__block6_box__roll_angle__0
T@"MLMultiArray",&,N,V_ShotNet__ShotNet__ssd_predictions__block6_box__box_offset__0
T@"MLMultiArray",&,N,V_ShotNet__ShotNet__ssd_predictions__block6_box__class_prob__0
T@"MLMultiArray",&,N,V_ShotNet__ShotNet__ssd_predictions__block9_box__yaw_angle__0
T@"MLMultiArray",&,N,V_ShotNet__ShotNet__ssd_predictions__block9_box__roll_angle__0
T@"MLMultiArray",&,N,V_ShotNet__ShotNet__ssd_predictions__block9_box__box_offset__0
T@"MLMultiArray",&,N,V_ShotNet__ShotNet__ssd_predictions__block9_box__class_prob__0
T@"MLMultiArray",&,N,V_ShotNet__ShotNet__ssd_predictions__block10_box__yaw_angle__0
T@"MLMultiArray",&,N,V_ShotNet__ShotNet__ssd_predictions__block10_box__roll_angle__0
T@"MLMultiArray",&,N,V_ShotNet__ShotNet__ssd_predictions__block10_box__box_offset__0
T@"MLMultiArray",&,N,V_ShotNet__ShotNet__ssd_predictions__block10_box__class_prob__0
T@"MLMultiArray",&,N,V_ShotNet__ShotNet__ssd_predictions__block11_box__yaw_angle__0
T@"MLMultiArray",&,N,V_ShotNet__ShotNet__ssd_predictions__block11_box__roll_angle__0
T@"MLMultiArray",&,N,V_ShotNet__ShotNet__ssd_predictions__block11_box__box_offset__0
T@"MLMultiArray",&,N,V_ShotNet__ShotNet__ssd_predictions__block11_box__class_prob__0
T@"MLMultiArray",&,N,V_transformed_features
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-130/OpenCV/src/core/datastructs.cpp
cvReleaseMemStorage
cvRestoreMemStoragePos
NULL storage pointer
cvMemStorageAlloc
Too large memory block is requested
storage->free_space % CV_STRUCT_ALIGN == 0
requested size is negative or too big
(size_t)ptr % CV_STRUCT_ALIGN == 0
cvCreateSeq
Specified element size doesn't match to the size of the specified element type (try to use 0 for element type)
cvSetSeqBlockSize
Storage block size is too small to fit the sequence elements
cvCvtSeqToArray
cvStartReadSeq
cvChangeSeqBlock
cvGetSeqReaderPos
cvSetSeqReaderPos
cvSeqPush
ptr + elem_size <= seq->block_max
block->start_index > 0
NULL sequence pointer
cvSeqPushMulti
number of removed elements is negative
cvSeqPopMulti
delta > 0
cvClearSeq
Invalid sequence header
cvSeqSlice
Bad sequence slice
Bad input sequence
cvSeqSort
Null compare function
cvCreateSet
cvSetAdd
count <= CV_SET_ELEM_IDX_MASK+1
cvCreateGraph
cvGraphAddVtx
cvFindGraphEdgeByPtr
ofs == 1 || start_vtx == edge->vtx[0]
graph pointer is NULL
cvGraphAddEdgeByPtr
vertex pointers coinside (or set to NULL)
edge->flags >= 0
Invalid graph pointer
cvCloneGraph
cvInitTreeNodeIterator
cvNextTreeNode
icvInitMemStorage
icvDestroyMemStorage
icvGoNextMemBlock
parent->bottom == block
icvGrowSeq
The sequence has NULL storage pointer
storage->free_space >= delta
block->count % seq->elem_size == 0 && block->count > 0
seq->first->start_index == 0
icvFreeSeqBlock
(in_front_of ? block : block->prev)->count == 0
seq->ptr == block->data
block->count > 0 && block->count % seq->elem_size == 0
Unknown/unsupported border type
borderInterpolate
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-130/OpenCV/src/imgproc/filter.cpp
columnBorderType != BORDER_WRAP
!rowFilter.empty() && !columnFilter.empty()
bufType == srcType
0 <= anchor.x && anchor.x < ksize.width && 0 <= anchor.y && anchor.y < ksize.height
roi.x >= 0 && roi.y >= 0 && roi.width >= 0 && roi.height >= 0 && roi.x + roi.width <= wholeSize.width && roi.y + roi.height <= wholeSize.height
start
srcRoi.x >= 0 && srcRoi.y >= 0 && srcRoi.width >= 0 && srcRoi.height >= 0 && srcRoi.x + srcRoi.width <= src.cols && srcRoi.y + srcRoi.height <= src.rows
wholeSize.width > 0 && wholeSize.height > 0
proceed
src && dst && count > 0
srcY >= startY
dstY <= roi.height
src.type() == srcType && dst.type() == dstType
apply
dstOfs.x >= 0 && dstOfs.y >= 0 && dstOfs.x + srcRoi.width <= dst.cols && dstOfs.y + srcRoi.height <= dst.rows
_kernel.channels() == 1
getKernelType
cn == CV_MAT_CN(bufType) && ddepth >= std::max(sdepth, CV_32S) && kernel.type() == ddepth
getLinearRowFilter
Unsupported combination of source format (=%d), and buffer format (=%d)
cn == CV_MAT_CN(bufType) && sdepth >= std::max(ddepth, CV_32S) && kernel.type() == sdepth
getLinearColumnFilter
Unsupported combination of buffer format (=%d), and destination format (=%d)
cn == CV_MAT_CN(_dstType)
createSeparableLinearFilter
ktype == CV_8U || ktype == CV_32S || ktype == CV_32F || ktype == CV_64F
preprocess2DKernel
cn == CV_MAT_CN(dstType) && ddepth >= sdepth
getLinearFilter
Unsupported combination of source format (=%d), and destination format (=%d)
createLinearFilter
(symmetryType & (KERNEL_SYMMETRICAL | KERNEL_ASYMMETRICAL)) != 0
SymmColumnVec_32s8u
SymmColumnSmallVec_32s16s
SymmColumnSmallVec_32f
SymmColumnVec_32f16s
SymmColumnVec_32f
anchor.inside(Rect(0, 0, ksize.width, ksize.height))
normalizeAnchor
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-130/OpenCV/src/imgproc/precomp.hpp
(symmetryType & (KERNEL_SYMMETRICAL | KERNEL_ASYMMETRICAL)) != 0 && this->ksize <= 5
SymmRowSmallFilter
kernel.type() == DataType<DT>::type && (kernel.rows == 1 || kernel.cols == 1)
RowFilter
kernel.type() == DataType<ST>::type && (kernel.rows == 1 || kernel.cols == 1)
ColumnFilter
this->ksize == 3
SymmColumnSmallFilter
SymmColumnFilter
_kernel.type() == DataType<KT>::type
Filter2D
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-130/OpenCV/src/core/alloc.cpp
Failed to allocate %lu bytes
OutOfMemoryError
borderType != BORDER_CONSTANT
pyrDown
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-130/OpenCV/src/imgproc/pyramids.cpp
!_src.empty()
pyrDown_
std::abs(dsize.width*2 - ssize.width) <= 2 && std::abs(dsize.height*2 - ssize.height) <= 2
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-130/OpenCV/src/core/opengl_interop_deprecated.cpp
GlBuffer
GlTexture
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-130/OpenCV/src/core/persistence.cpp
Invalid pointer to file storage
The node is neither a map nor an empty collection
cvGetFileNodeByName
Null element name
cvStartWriteStruct
The file storage is opened for reading
cvEndWriteStruct
cvWriteInt
cvWriteString
cvWriteRawData
Negative number of elements
Null data pointer
cvStartReadRawData
Null pointer to source file node or reader
The file node should be a numerical scalar or a sequence
cvReadRawDataSlice
Null pointer to reader or destination array
The readed sequence is a scalar, thus len must be 1
The sequence element is not a numerical scalar
The sequence slice does not fit an integer number of records
Null pointers to source file node or destination array
cvReadRawData
opencv-sequence
opencv-sequence-tree
opencv-graph
opencv-sparse-matrix
opencv-image
opencv-matrix
opencv-nd-matrix
Invalid type info
cvRegisterType
Some of required function pointers (is_instance, release, read or write) are NULL
Type name should start with a letter or _
Type name should contain only letters, digits, - and _
NULL double pointer
cvRead
The node does not represent a user object (unknown type?)
Unknown array type
The storage is not opened
icvPuts
An attempt to add element without a key to a map, or add element with key to sequence
icvXMLWriteTag
A single _ is a reserved tag name
Closing tag should not include any attributes
Key should start with a letter or _
Key name may only contain alphanumeric characters [a-zA-Z0-9], '-' and '_'
icvDecodeFormat
fmt_pairs != 0 && max_len > 0
Invalid data type specification
Too long data type specification
(align & (align-1)) == 0 && size < INT_MAX
%.8e
-.Inf
.Inf
%.16e
elements with keys can not be written to sequence
icvXMLWriteScalar
icvYMLWrite
The key is an empty
The key is too long
Key must start with a letter or _
Key names may only contain alphanumeric characters [a-zA-Z0-9], '-', '_' and ' '
icvReleaseSeq
flags
count
Some of essential sequence attributes are absent
icvReadSeq
The sequence flags are invalid
curve
closed
hole
untyped
header_dt
header_user_data
One of "header_dt" and "header_user_data" is there, while the other is not
rect
Only one of "header_user_data", "rect" and "origin" tags may occur
color
data
The image data is not found in file storage
The number of stored elements does not match to "count"
Too complex format for the matrix
icvDecodeSimpleFormat
recursive
false
False
FALSE
icvWriteSeqTree
CV_IS_SEQ( seq )
sequences
icvWriteSeq
level
The size of element calculated from "dt" and the elem_size do not match
icvGetFormat
Size of sequence element (elem_size) is inconsistent with seq->flags
%d%c
The size of header calculated from "header_dt" is greater than header_size
icvWriteHeaderData
opencv-sequence-tree instance should contain a field "sequences" that should be a sequence
icvReadSeqTree
All the sequence tree nodes should contain "level" field
level == prev_level + 1
icvReleaseGraph
vertex_dt
edge_dt
vertex_count
edge_count
Some of essential graph attributes are absent
icvReadGraph
oriented
Graph edges should start with 2 integers and a float
%df%s
vertices
edges
No edges data
No vertices data
Some of stored vertex indices are out of range
Duplicated edge has occured
cvAlignPtr
(align & (align-1)) == 0
icvWriteGraph
CV_IS_GRAPH(graph)
2if%s
sizes
Some of essential matrix attributes are absent
icvReadSparseMat
Could not determine sparse matrix dimensionality
The matrix data is not found in file storage
Sparse matrix data is corrupted
icvWriteSparseMat
CV_IS_SPARSE_MAT(mat)
(reader).seq->elem_size == sizeof(idx)
k < dims
Some of essential image attributes are absent
icvReadImage
layout
interleaved
Only interleaved images can be read
The matrix size does not match to the number of stored elements
icvWriteImage
CV_IS_IMAGE(image)
Images with planar data layout are not supported
top-left
bottom-left
planar
rows
cols
icvReadMat
icvWriteMat
CV_IS_MAT_HDR_Z(mat)
icvReadMatND
Could not determine the matrix dimensionality
icvWriteMatND
CV_IS_MATND_HDR(mat)
type == CV_32FC1 || type == CV_32FC2 || type == CV_64FC1 || type == CV_64FC2
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-130/OpenCV/src/core/dxt.cpp
This mode (using nonzero_rows with a single-column matrix) breaks the function's logic, so it is prohibited.
For fast convolution/correlation use 2-column matrix or single-row matrix instead
!inv
type == srcB.type() && srcA.size() == srcB.size()
mulSpectrums
(flags & DFT_NO_PERMUTE) == 0
(unsigned)k0 < (unsigned)n && (unsigned)k1 < (unsigned)n
factors[0] == factors[nf-1]
(unsigned)j < (unsigned)n2
(unsigned)j < (unsigned)n
RealDFT
tab_size == n
CCSIDFT
src != dst
DFTInit
nf < 34
elem_size == sizeof(Complex<float>)
channels() == CV_MAT_CN(dtype)
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-130/OpenCV/src/core/copy.cpp
mask.depth() == CV_8U && (mcn == 1 || mcn == cn)
size() == mask.size()
checkScalar(value, type(), _value.kind(), _InputArray::MAT )
setTo
mask.empty() || mask.type() == CV_8U
repeat
ny > 0 && nx > 0
maskarr == 0
cvCopy
src.depth() == dst.depth() && src.size == dst.size
(coi1 != 0 || src.channels() == 1) && (coi2 != 0 || dst.channels() == 1)
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-130/OpenCV/src/core/opengl_interop.cpp
The library is compiled without OpenGL support
throw_nogl
threshold
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-130/OpenCV/src/imgproc/thresh.cpp
Unknown threshold type
thresh_8u
thresh_16s
thresh_32f
img.depth() == CV_8U && winSize.width > 2 && winSize.height > 2
buildOpticalFlowPyramid
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-130/OpenCV/src/video/lkpyramid.cpp
maxLevel >= 0 && winSize.width > 2 && winSize.height > 2
calcOpticalFlowPyrLK
(npoints = prevPtsMat.checkVector(2, CV_32F, true)) >= 0
nextPtsMat.checkVector(2, CV_32F, true) == npoints
statusMat.isContinuous()
errMat.isContinuous()
levels1 >= 0
ofs.x >= winSize.width && ofs.y >= winSize.height && ofs.x + prevPyr[lvlStep1].cols + winSize.width <= fullSize.width && ofs.y + prevPyr[lvlStep1].rows + winSize.height <= fullSize.height
levels2 >= 0
ofs.x >= winSize.width && ofs.y >= winSize.height && ofs.x + nextPyr[lvlStep2].cols + winSize.width <= fullSize.width && ofs.y + nextPyr[lvlStep2].rows + winSize.height <= fullSize.height
prevPyr[level * lvlStep1].size() == nextPyr[level * lvlStep2].size()
prevPyr[level * lvlStep1].type() == nextPyr[level * lvlStep2].type()
depth == CV_8U
calcSharrDeriv
cvAlign
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-130/OpenCV/include/opencv2/core/internal.hpp
scn == 1
convertAndUnrollScalar
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-130/OpenCV/src/core/arithm.cpp
op == CMP_LT || op == CMP_LE || op == CMP_EQ || op == CMP_NE || op == CMP_GE || op == CMP_GT
compare
The operation is neither 'array op array' (where arrays have the same size and the same type), nor 'array op scalar', nor 'scalar op array'
The operation is neither 'array op array' (where arrays have the same size and type), nor 'array op scalar', nor 'scalar op array'
binary_op
(mask.type() == CV_8UC1 || mask.type() == CV_8SC1)
mask.size == src1.size
operator()
-256 <= ((b) - (a)) && ((b) - (a)) <= 512
-256 <= ((a) - (b)) && ((a) - (b)) <= 512
The operation is neither 'array op array' (where arrays have the same size and the same number of channels), nor 'array op scalar', nor 'scalar op array'
arithm_op
src2.type() == CV_64F && (src2.rows == 4 || src2.rows == 1)
When the input arrays in add/subtract/multiply/divide functions have different types, the output array type must be explicitly specified
-256 <= (a - b) && (a - b) <= 512
img.dims <= 2 && templ.dims <= 2 && corr.dims <= 2
crossCorr
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-130/OpenCV/src/imgproc/templmatch.cpp
depth == tdepth || tdepth == CV_32F
corrsize.height <= img.rows + templ.rows - 1 && corrsize.width <= img.cols + templ.cols - 1
ccn == 1 || delta == 0
the input arrays are too big
CV_MAT_CN(sumType) == CV_MAT_CN(srcType)
getRowSumFilter
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-130/OpenCV/src/imgproc/smooth.cpp
CV_MAT_CN(sumType) == CV_MAT_CN(dstType)
getColumnSumFilter
Unsupported combination of sum format (=%d), and destination format (=%d)
sumCount == ksize-1
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-130/OpenCV/src/imgproc/utils.cpp
top >= 0 && bottom >= 0 && left >= 0 && right >= 0
copyMakeBorder
value[0] == value[1] && value[0] == value[2] && value[0] == value[3]
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-130/OpenCV/src/imgproc/deriv.cpp
ktype == CV_32F || ktype == CV_64F
getScharrKernels
dx >= 0 && dy >= 0 && dx+dy == 1
getSobelKernels
The kernel size must be odd and not larger than 31
dx >= 0 && dy >= 0 && dx+dy > 0
ksize > order
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-130/OpenCV/src/core/convert.cpp
src && nsrcs > 0 && dst && ndsts > 0 && fromTo && npairs > 0
mixChannels
j < nsrcs && src[j].depth() == depth
i1 >= 0 && j < ndsts && dst[j].depth() == depth
type == B.type() && (type == CV_32FC1 || type == CV_64FC1 || type == CV_32FC2 || type == CV_64FC2)
gemm
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-130/OpenCV/src/core/matmul.cpp
a_size.width == len
a_size.height == len
C.type() == type && (((flags&GEMM_3_T) == 0 && C.rows == d_size.height && C.cols == d_size.width) || ((flags&GEMM_3_T) != 0 && C.rows == d_size.width && C.cols == d_size.height))
type == CV_64FC2
src1.type() == src2.type()
scaleAdd
src.channels() == 1
mulTransposed
delta.channels() == 1 && (delta.rows == src.rows || delta.rows == 1) && (delta.cols == src.cols || delta.cols == 1)
GEMM_TransposeBlock
MulTransposedR
delta_cols == 1
cn <= 4 && func != 0
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-130/OpenCV/src/core/stat.cpp
src.channels() == 1 && func != 0
countNonZero
mean
(cn == 1 && (mask.empty() || mask.type() == CV_8U)) || (cn >= 1 && mask.empty() && !minIdx && !maxIdx)
minMaxIdx
img.dims <= 2
src.type() == CV_8UC1
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-130/OpenCV/src/core/lapack.cpp
type == CV_32F || type == CV_64F
invert
m == n
method == DECOMP_LU || method == DECOMP_CHOLESKY
n == 1
type == _src2.type() && (type == CV_32F || type == CV_64F)
solve
(method != DECOMP_LU && method != DECOMP_CHOLESKY) || is_normal || src.rows == src.cols
src.rows == 1
The function can not solve under-determined linear systems
src.rows == src.cols
eigen
w.type() == u.type() && u.type() == vt.type() && u.data && vt.data && w.data
backSubst
u.cols >= nm && vt.rows >= nm && (w.size() == Size(nm, 1) || w.size() == Size(1, nm) || w.size() == Size(vt.rows, u.cols))
rhs.data == 0 || (rhs.type() == type && rhs.rows == m)
_SVDcompute
0 <= d && d <= CV_MAX_DIM && _sizes
create
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-130/OpenCV/src/core/matrix.cpp
step[dims-1] == (size_t)CV_ELEM_SIZE(flags)
m.dims >= 2
0 <= _rowRange.start && _rowRange.start <= _rowRange.end && _rowRange.end <= m.rows
0 <= _colRange.start && _colRange.start <= _colRange.end && _colRange.end <= m.cols
m.dims <= 2
0 <= roi.x && 0 <= roi.width && roi.x + roi.width <= m.cols && 0 <= roi.y && 0 <= roi.height && roi.y + roi.height <= m.rows
ranges
r == Range::all() || (0 <= r.start && r.start < r.end && r.end <= m.size[i])
dims <= 2
diag
img->dataOrder == IPL_DATA_ORDER_PIXEL
img->dataOrder == IPL_DATA_ORDER_PIXEL || img->roi->coi != 0
(int)nelems >= 0
reserve
resize
COI is not supported by the function
cvarrToMat
seq->total > 0 && CV_ELEM_SIZE(seq->flags) == seq->elem_size
dims <= 2 && step[0] > 0
locateROI
adjustROI
reshape
The matrix is not continuous, thus its number of rows can not be changed
Bad new number of rows
The total number of matrix elements is not divisible by the new number of rows
The total width is not divisible by the new number of channels
cn <= 4
scalarToRawData
i < 0
getMat
0 <= i && i < (int)vv.size()
This method is not implemented for oclMat yet
k == STD_VECTOR_MAT
0 <= i && i < (int)v.size()
getMatVector
This function in deprecated, do not use it
getGlBuffer
getGlTexture
k == GPU_MAT
getGpuMat
i < (int)vv.size()
total
empty
!fixedSize() || ((Mat*)obj)->size.operator()() == _sz
!fixedType() || ((Mat*)obj)->type() == mtype
!fixedSize() || ((gpu::GpuMat*)obj)->size() == _sz
!fixedType() || ((gpu::GpuMat*)obj)->type() == mtype
!fixedSize() || ((ogl::Buffer*)obj)->size() == _sz
!fixedType() || ((ogl::Buffer*)obj)->type() == mtype
!fixedSize() || ((Mat*)obj)->size.operator()() == Size(cols, rows)
!fixedSize() || ((gpu::GpuMat*)obj)->size() == Size(cols, rows)
!fixedSize() || ((ogl::Buffer*)obj)->size() == Size(cols, rows)
!fixedType() && !fixedSize()
CV_MAT_TYPE(mtype) == m.type()
m.dims == dims
m.size[j] == sizes[j]
mtype == type0 || (CV_MAT_CN(mtype) == 1 && ((1 << type0) & fixedDepthMask) != 0)
dims == 2 && ((sizes[0] == sz.height && sizes[1] == sz.width) || (allowTransposed && sizes[0] == sz.width && sizes[1] == sz.height))
dims == 2 && (sizes[0] == 1 || sizes[1] == 1 || sizes[0]*sizes[1] == 0)
!fixedSize() || len == vv.size()
mtype == type0 || (CV_MAT_CN(mtype) == CV_MAT_CN(type0) && ((1 << type0) & fixedDepthMask) != 0)
!fixedSize() || len == ((vector<uchar>*)v)->size() / esz
Vectors with element size %d are not supported. Please, modify OutputArray::create()
create() called for the missing output array
!fixedSize() || len == len0
v[j].empty()
i < (int)v.size()
!fixedType() || (CV_MAT_CN(mtype) == m.channels() && ((1 << CV_MAT_TYPE(flags)) & fixedDepthMask) != 0)
!fixedSize()
release
clear
k == MAT
getMatRef
setIdentity
src.dims <= 2 && esz <= (size_t)32
transpose
src.size() == dst.size() && (src.cols == 1 || src.rows == 1)
func != 0
m.dims <= 2 && m.rows == m.cols
completeSymm
src.dims <= 2
src.channels() == dst.channels()
_arrays && (_ptrs || _planes)
init
narrays <= 1000
arrays[i] != 0
A.size == arrays[i0]->size
A.step[d-1] == A.elemSize()
copyTo
convertTo
minMaxLoc
0 <= _dims && _dims <= CV_MAX_DIM
setSize
s >= 0
%s:%d: error: (%d) %s in function %s
%s:%d: error: (%d) %s
OpenCV Error: %s (%s) in %s, file %s, line %d
No Error
Backtrace
Unspecified error
Internal error
Insufficient memory
Bad argument
Iterations do not converge
Autotrace call
Incorrect size of input array
Null pointer
Division by zero occured
Image step is wrong
Inplace operation is not supported
Requested object was not found
Input image depth is not supported by function
Formats of input arguments do not match
Sizes of input arguments do not match
One of arguments' values is out of range
Unsupported format or combination of formats
Input COI is not supported
Bad number of channels
Bad flag (parameter or structure field)
Bad parameter of type CvPoint
Bad type of mask argument
Parsing error
The function/feature is not implemented
Memory block has been corrupted
Assertion failed
No GPU support
Gpu API call
No OpenGL support
OpenGL API call
Unknown %s code %d
status
module != 0 && module->name != 0 && module->version != 0
cvRegisterModule
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-130/OpenCV/src/core/system.cpp
cxcore
2.4.13.6
unknown function
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-130/OpenCV/include/opencv2/dynamicuda/dynamicuda.hpp
copy
copyWithMask
convert
mallocPitch
The library is compiled without CUDA support
qualityLevel > 0 && minDistance >= 0 && maxCorners >= 0
goodFeaturesToTrack
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-130/OpenCV/src/imgproc/featureselect.cpp
mask.empty() || (mask.type() == CV_8UC1 && mask.size() == image.size())
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-130/OpenCV/src/core/array.cpp
Non-positive width or height
cvCreateMatHeader
Invalid matrix type
cvInitMatHeader
Non-positive cols or rows
cvReleaseMat
Bad CvMat header
cvCloneMat
NULL matrix header pointer
cvInitMatNDHeader
invalid array data type
NULL <sizes> pointer
non-positive or too large number of dimensions
one of dimesion sizes is non-positive
The array is too big
cvCreateMatNDHeader
Bad CvMatND header
cvCloneMatND
src->dims <= CV_MAX_DIM
_dst.data == data0
Incorrect number of arrays
cvInitNArrayIterator
Some of required array pointers is NULL
Iterator pointer is NULL
COI set is not allowed here
Number of dimensions is the same for all arrays
Data type is not the same for all arrays
Number of channels is not the same for all arrays
Depth is not the same for all arrays
Mask should have 8uC1 or 8sC1 data type
Dimension sizes are the same for all arrays
cvNextNArraySlice
iterator != 0
cvCreateSparseMat
bad number of dimensions
cvReleaseSparseMat
Invalid sparse array header
cvCloneSparseMat
Invalid sparse matrix header
cvInitSparseMatIterator
NULL iterator pointer
Data is already allocated
cvCreateData
unrecognized or unsupported array type
cvReleaseData
Only continuous nD arrays are supported here
cvGetElemType
cvGetDims
Array should be CvMat or IplImage
cvGetSize
index is out of range
cvPtr2D
COI must be non-null in case of planar images
NULL pointer to indices
cvPtrND
NULL array pointer is passed
cvGetMat
The matrix has NULL data pointer
The image has NULL data pointer
Images with planar data layout should be used with COI selected
The image is interleaved and has over CV_CN_MAX channels
Pixel order should be used with coi == 0
Input array has NULL data pointer
Unrecognized or unsupported array type
cvCreateImage
null pointer to header
cvInitImageHeader
Bad input roi
Unsupported format
Bad input origin
Bad input align
cvReleaseImageHeader
cvReleaseImage
cvSetImageROI
rect.width >= 0 && rect.height >= 0 && rect.x < image->width && rect.y < image->height && rect.x + rect.width >= (int)(rect.width > 0) && rect.y + rect.height >= (int)(rect.height > 0)
cvSetImageCOI
cvGetImageCOI
Bad image header
cvCloneImage
cvGetMatND
icvGetNodePtr
CV_IS_SPARSE_MAT( mat )
One of indices is out of range
(newsize & (newsize - 1)) == 0
GRAY
BGRA
op == MORPH_ERODE || op == MORPH_DILATE
getMorphologyRowFilter
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-130/OpenCV/src/imgproc/morph.cpp
Unsupported data type (=%d)
getMorphologyColumnFilter
getMorphologyFilter
depth == CV_8U || depth == CV_16U || depth == CV_16S || depth == CV_32F || depth == CV_64F
createMorphologyFilter
shape == MORPH_RECT || shape == MORPH_CROSS || shape == MORPH_ELLIPSE
getStructuringElement
morphOp
((size_t)src[i] & 15) == 0
((size_t)_src[i] & 15) == 0
_kernel.type() == CV_8U
MorphFilter
src.type() == CV_8UC1 || src.type() == CV_32FC1
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-130/OpenCV/src/imgproc/corner.cpp
cornerEigenValsVecs
CV_MAT_CN(_type) == e.a.channels()
assign
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-130/OpenCV/src/core/matop.cpp
Unknown operation
Invalid matrix initializer type
%{public}@%{public}@
%{public}@%{public}s
Could not load FaceQualityFilterSVM.mlmodelc in the bundle resource
Could not load SignificantActivity.mlmodelc in the bundle resource
Could not load FaceQualityFilterSVMDataScaler.mlmodelc in the bundle resource
softlink:o:path:/System/Library/Frameworks/HomeKit.framework/HomeKit
softlink:o:path:/System/Library/Frameworks/HomeKit.framework/HomeKit
softlink:o:path:/System/Library/Frameworks/HomeKit.framework/HomeKit
softlink:o:path:/System/Library/Frameworks/HomeKit.framework/HomeKit
softlink:o:path:/System/Library/PrivateFrameworks/PrivateFederatedLearning.framework/PrivateFederatedLearning
HMIExternalPersonDataSourceDisk
HMIExternalPersonManagerDataSource
HMIPersonManagerDataSource
NSObject
HMIMutableCluster
HMFLogging
HMIInputFeatureProvider
MLFeatureProvider
HMISignificantActivityDetector
HMISystemResourceUsage
HMISystemResourceUsageMonitor
HMISystemResourceUsageMonitorProtocol
HMIVideoAnalyzerEventFace
HMIDataReader
HMIVideoEventEntry
HMIVideoEvent
HMIVideoEventBuffer
HMIVideoTimelineEntry
HMIVideoTimeline
HMIVideoTimelineProfiler
HMITimeIntervalAverage
HMIVideoAnalyzerEventMotion
HMIHomePersonManager
HMFTimerDelegate
HMIStoreFaceCropOperation
HMIStoreFaceprintOperation
HMICameraVideoFrame
HMIVisionUtilities
HMIThermalMonitorService
HMIThermalMonitor
HMIMemorySampler
HMIVideoAssetReader
HMIHomeKitClient
HMHomeManagerDelegateAdapter
HMHomeManagerDelegate
HMIFaceQualityFilterModelInput
HMIFaceQualityFilterSVM
HMIVideoNode
HMIVideoProcessingNode
PFLBackgroundRunner
_DASExtensionRunner
HMIExternalPersonDataSourceHomeKit
HMI_CVML_Error
HMIPersonFaceCrop
NSCopying
NSSecureCoding
NSCoding
HMIVideoAnalyzerFrameResult
HMIMemoryAVAsset
AVAssetResourceLoaderDelegate
MovingAverageEntry
MovingAverage
HMICameraVideoAnalyzerResult
HMICameraVideoAnalyzerConfiguration
HMICameraVideoAnalyzerSignificantEvent
HMICameraVideoFrameResult
HMICameraVideoFragment
HMICameraVideoAnalyzerHistory
HMICameraVideoAnalyzerRequest
HMIVideoEncoderDelegate
HMIVideoRetimerDelegate
HMICameraVideoFrameSelectorDelegate
HMICameraVideoAnalyzerScheduler
HMISystemResourceUsageMonitorDelegate
HMICameraVideoAnalyzer
HMIDESMutableFloatArray
HMIFaceClassifierVIP
HMIFaceClassifier
HMINMSConfiguration
HMICameraVideoFrameAnalyzerFactory
HMICIFilterAttributeValue
HMICIFilterAttribute
HMICIFilter
HMIDESDatasetSample
HMIDESDataset
HMIObjectDetection
HMIObjectDetectionUtils
HMIVideoFrameAnalyzerResult
HMIVideoFrameAnalyzer
HMIVideoFrameAnalyzerDelegateAdapter
HMIVideoFrameAnalyzerDelegate
HMICameraVideoPosterFrame
HMICameraVideoPosterFrameGeneratorInput
HMICameraVideoPosterFrameGenerator
HMIFaceCrop
HMIVideoRetimer
HMIVideoRetimerDelegateAdapter
HMICamera
HMITask
HMIEmptyTask
HMITaskServiceServer
HMITaskService
HMICameraVideoFrameSelectorFrameScore
HMICameraVideoFrameSelector
HMICameraVideoFrameSamplerDelegate
HMICameraVideoFrameSampler
HMIUpdatedFaceprintsResult
HMIFaceprinter
HMIFaceDetectorVision
HMIFaceDetector
HMIFaceprint
HMIPoint
HMIHomePersonManagerSettings
NSMutableCopying
HMIMutableHomePersonManagerSettings
HMIModelLoader
HMIFeedbackSession
NSURLSessionDelegate
HMIFeedbackSubmitClipOperation
HMIFeedbackTask
HMIFeedback
HMIFaceClassification
HMIPersonsModelsSummaryTask
HMIFaceMisclassificationTask
HMIRemovePersonsModelTask
HMIPersonManager
HMIConfidence
HMIError
HMIVideoFragment
HMISessionEntityManager
HMITuriTrialUpdateTask
HMIVideoAnnotator
HMISystemResourceUsageMonitorImpl
HMIVideoAnalyzerResultFilter
HMIVideoAnalyzerResultActivityZoneFilter
HMIPerson
HMIPersonsModelSummary
HMIPersonsModel
HMIJSONArchiver
HMIJSONUnarchiver
HMIFaceUtilities
HMIPersonBlob
HMIFaceTrackerMatch
HMIFaceTrackerMotionRecord
HMIFaceTracker
HMICameraVideoResourceAttributes
HMICameraVideoAssetReader
HMIVideoEncoder
HMIVideoEncoderDelegateAdapter
HMIHomePersonDataSourceHomeKit
HMIHomePersonManagerDataSource
HMIExternalPersonManagerSettings
HMFObject
HMIMutableExternalPersonManagerSettings
HMIAnalytics
HMIPersonSourceUUIDPair
HMIPersonsModelsSummary
HMIPersonsModelPrediction
HMIPersonsModelManager
HMIDESLayerParameters
HMIDESTrainingResult
HMIDESTrainer
HMIVideoAnalyzerResultOutcome
HMIExternalPersonManager
HMIRemovePersonsModelOperation
HMIVideoAnalyzerScheduler
HMITuriTrialManager
HMIGreedyClustering
HMIVideoAnalyzerLegacy
HMICameraVideoAnalyzerDelegate
HMIVideoFrameSampler
HMIVideoFrameIntervalSampler
HMIVideoFrameSamplerDelegateAdapter
HMIVideoFrameSamplerDelegate
HMIVideoPackageAnalyzer
HMIVideoPackageAnalyzerDelegateAdapter
HMIVideoPackageAnalyzerDelegate
HMIFaceQuality
HMIFaceQualityEntropyOfLaplacian
HMIFaceQualityDistanceToJunkCluster
HMIVideoAnalyzerFragmentResult
HMICameraActivityZone
HMIVideoAnalyzerConfiguration
HMIVideoAnalyzerDynamicConfiguration
HMIMotionDetection
HMICameraVideoAnalyzerDelegateAdapter
HMIAnalysisService
HMIVideoAnalyzerEventPerson
HMIDESRunner
HMIPreference
HMIHomePersonDataSourceDisk
HMINotifydObserver
HMIVideoAnalyzerEventVehicle
HMIClusteringTaskSummary
HMIHomePersonClusteringTask
HMIUpdatePersonsModelTask
HMIFetchPersonsOperation
HMIFetchPersonFaceCropsOperation
HMIFetchFaceprintsForFaceCropsOperation
HMIStoreFaceprintsOperation
HMIRemoveFaceprintsOperation
HMIRegionOfInterestOperation
HMICameraVideoFrameAnalyzerSignificantActivity
HMICameraVideoFrameAnalyzer
HMIDESBackgroundTask
HMIVideoFrame
HMICompressionHelper
HMIVideoCommandBuffer
HMIVideoCommandBufferDelegateAdapter
HMIVideoCommandBufferDelegate
HMIVideoAnalyzerEventPet
HMIVideoAnalyzerEvent
HMIVideoDecoder
HMIVideoDecoderDelegateAdapter
HMIVideoDecoderDelegate
HMIVideoAssetWriter
AVAssetWriterDelegate
HMIVideoAssetWriterDelegateAdapter
HMIVideoAssetWriterDelegate
HMIUpdatePersonsModelOperation
HMISparseOpticalFlowFeatureVector
HMISparseOpticalFlowFrame
HMISparseOpticalFlowMotionDetection
HMISparseOpticalFlowMotionDetector
HMIMotionDetector
HMIVideoFrameSelectorFrameCandidate
HMIVideoFrameSelector
HMIVideoFrameSelectorDelegateAdapter
HMIVideoFrameSelectorDelegate
HMIFaceRecognition
HMIVideoAnalyzer
HMIVideoAnalyzerDelegatePrivate
HMIVideoAnalyzerDelegate
HMIVideoAnalyzerServer
HMIVideoAnalyzerDelegateAdapter
HMIPersonDataSourceDisk
FaceQualityFilterSVMInput
FaceQualityFilterSVMOutput
FaceQualityFilterSVM
SignificantActivityInput
SignificantActivityOutput
SignificantActivity
FaceQualityFilterSVMDataScalerInput
FaceQualityFilterSVMDataScalerOutput
FaceQualityFilterSVMDataScaler
workQueue
sourceURL
UUID
UUIDString
URLByAppendingPathComponent:
defaultManager
path
createDirectoryAtPath:withIntermediateDirectories:attributes:error:
stringWithFormat:
hmiPrivateErrorWithCode:description:underlyingError:
name
dictionaryWithObjects:forKeys:count:
serializeJSONObject:url:error:
countByEnumeratingWithState:objects:count:
personUUID
faceBoundingBox
dateCreated
dataRepresentation
writeToURL:atomically:
hmiPrivateErrorWithCode:description:
hmfErrorWithCode:
categoryWithName:
logCategory
isEqual:
class
self
performSelector:
performSelector:withObject:
performSelector:withObject:withObject:
isProxy
isKindOfClass:
isMemberOfClass:
conformsToProtocol:
respondsToSelector:
retain
release
autorelease
retainCount
zone
hash
superclass
description
debugDescription
fetchAllPersonsWithCompletion:
fetchPersonsWithUUIDs:completion:
fetchAllPersonFaceCropsWithCompletion:
fetchFaceCropsForPersonsWithUUIDs:completion:
fetchAllFaceprintsWithCompletion:
fetchFaceprintsForFaceCropsWithUUIDs:completion:
performCloudPullWithCompletion:
addFaceprints:completion:
removeFaceprintsWithUUIDs:completion:
fetchSettingsWithCompletion:
addPerson:completion:
addPersonFaceCrops:completion:
init
data
initWithData:
arrayWithObject:
count
copy
addObjectsFromArray:
scale:
add:
addObject:
floatArrayByScaling:
logIdentifier
initWithFaceprint:
faceprintUUIDs
addLinkedEntityUUIDs:
linkedEntityUUIDs
addFaceprints:
centroid
.cxx_destruct
_faceprintUUIDs
_linkedEntityUUIDs
_centroid
pixelBuffer
dealloc
inputName
arrayWithObjects:count:
setWithArray:
isEqualToString:
featureValueWithPixelBuffer:
featureValueForName:
featureNames
initWithPixelBuffer:inputName:
_pixelBuffer
_inputName
objectAtIndexedSubscript:
doubleValue
sharedInstance
usesCPUOnly
setUsesCPUOnly:
boolPreferenceForKey:defaultValue:
setAllowBackgroundGPUCompute:
fileURLWithPath:
modelWithContentsOfURL:configuration:error:
hmiPrivateErrorWithCode:underlyingError:
arrayWithCapacity:
initWithName:
_runNeuralNetworkOnPixelBuffer:offsets:scores:yaws:rolls:error:
invalidate
_postProcessOffsets:scores:yaws:rolls:outputPredictions:
initWithPixelBuffer:presentationTimeStamp:
printWithScale:
transferPixelBuffer:pixelFormat:options:error:
inputFeatureValueName
mlModel
predictionOptions
predictionFromFeatures:options:error:
offsetsFeatureValueNames
type
multiArrayValue
scoresFeatureValueNames
yawsFeatureValueNames
rollsFeatureValueNames
array
shape
unsignedLongValue
dataPointer
strides
useSoftmax
numberWithDouble:
initWithLabelIndex:confidence:unclampedBoundingBox:yaw:roll:
nmsConfiguration
nmsMultiClass:output:nmsConfiguration:
boundingBox
labelIndex
confidence
roll
initWithLabelIndex:confidence:boundingBox:yaw:roll:
bundleForClass:
pathForResource:ofType:
defaultAssetPath
initWithConfidenceThresholds:nmsConfiguration:assetPath:error:
predict:detectedObjects:error:
inputDimensions
_confidenceThresholds
_anchorSizes
_numberOfAnchors
_useSoftmax
_mlModel
_inputFeatureValueName
_offsetsFeatureValueNames
_scoresFeatureValueNames
_yawsFeatureValueNames
_rollsFeatureValueNames
_nmsConfiguration
_predictionOptions
_inputDimensions
systemResourceUsageLevel
setSystemResourceUsageLevel:
_systemResourceUsageLevel
UTF8String
productInfo
productClass
initWithProductClass:workQueue:
systemResourceUsageMonitorImpl
setDelegate:
delegate
getCurrentSystemResourceUsage
start
_systemResourceUsageMonitorImpl
_workQueue
initWithConfidence:boundingBox:yaw:roll:faceQuality:faceRecognition:
initWithConfidence:boundingBox:
value
initWithName:value:
faceRecognition
faceQuality
encodeWithCoder:
encodeObject:forKey:
initWithCoder:
decodeObjectOfClass:forKey:
supportsSecureCoding
initWithConfidence:boundingBox:faceRecognition:
initWithConfidence:boundingBox:yaw:roll:
initWithConfidence:boundingBox:yaw:roll:faceRecognition:
attributeDescriptions
_faceRecognition
_faceQuality
_yaw
_roll
whitespaceCharacterSet
stringByTrimmingCharactersInSet:
length
bytes
subdataWithRange:
position
readUInt32
readUInt64
seek:
readData:
_data
_position
internal
localeWithLocaleIdentifier:
setLocale:
setDateFormat:
dateFromString:
substringToIndex:
stringWithCapacity:
appendFormat:
time
initWithValue:time:
_value
_time
removeAllObjects
hmf_removeFirstObject
indexOfObject:inSortedRange:options:usingComparator:
insertObject:atIndex:
hmf_objectsPassingTest:
indexesOfObjectsPassingTest:
objectsAtIndexes:
removeObjectsAtIndexes:
objectAtIndex:
na_map:
componentsJoinedByString:
initWithMaxCapacity:
objectsInTimeRange:includeEnd:
extractObjectsInTimeRange:
neighborsOfObject:
_lock
_maxCapacity
initWithTime:date:
date
_date
lastObject
dateAtTime:
timeIntervalSinceDate:
addDate:atTime:
timeIntervalSinceDateAtTime:
_buffer
addValue:
startedAtTime:
endedAtTime:
averageTime
_timeline
_average
initWithWindowSize:
addNumber:
movingAverage
movingAverageForInterval:defaultValue:
valueForInterval:defaultValue:
exceptionWithName:reason:userInfo:
performBlock:
initWithUUID:homeUUID:
setMaxConcurrentOperationCount:
initWithTimeInterval:options:
resume
dictionary
initWithTimeout:
_updateSettings:
finish
addExecutionBlock:
operationQueue
addOperation:
watchdogTimer
taskServiceClient
objectForKeyedSubscript:
submitTaskWithOptions:completionHandler:
dataSource
initWithDataSource:faceCrop:
error
setCompletionBlock:
hmiPrivateErrorWithCode:
initWithDataSource:faceprint:
classifications
familiarity
sourceUUID
na_firstObjectPassingTest:
unknownFacesSavedCounts
sessionEntityUUID
intValue
numberWithInt:
setObject:forKeyedSubscript:
faceCrop
faceprint
storeUnassociatedFaceCrop:completion:
storeFaceprint:completion:
homeUUID
initWithSourceUUID:homeUUID:external:
suspend
analyticsTimer
settings
isFaceClassificationEnabled
standardUserDefaults
doubleForKey:
timeIntervalSinceReferenceDate
setDouble:forKey:
cancelWithError:
timerDidFire:
setDataSource:
handleUpdatedPerson:
handleUpdatedUnassociatedFaceCrop:
handleUpdatedPersonFaceCrop:
handleUpdatedFaceprint:
handleUpdatedSettings:
handleRemovedPersonWithUUID:
handleRemovedFaceCropWithUUID:
handleRemovedFaceprintWithUUID:
handleMisclassifiedPersonForFaceCrop:
handleNewFaceEvents:
lock
_dataSource
_settings
_operationQueue
_watchdogTimer
_analyticsTimer
_unknownFacesSavedCounts
setWithObject:
addFaceCrops:completion:
main
_faceCrop
_faceprint
initWithPixelBuffer:presentationTime:frameId:fragmentSequenceNumber:
jpegData
size
imageWithData:
extent
createPixelBufferWithSize:pixelFormat:useIOSurface:
contextWithOptions:
render:toCVPixelBuffer:
numberWithUnsignedLong:
dictionaryWithDictionary:
setObject:forKey:
appendBytes:length:
presentationTime
frameId
initWithPixelBuffer:
initWithJPEGData:presentationTime:frameId:fragmentSequenceNumber:size:
JPEGRepresentationWithDownscaleFactor:outSize:
fragmentSequenceNumber
motionDetections
setMotionDetections:
sessionPresentationTime
setSessionPresentationTime:
_frameId
_fragmentSequenceNumber
_jpegData
_motionDetections
_size
_presentationTime
_sessionPresentationTime
point
transferPixelBuffer:crop:size:pixelFormat:options:error:
numberWithFloat:
createPixelBufferFromJPEGDataProvider:error:
applyPadding:withOriginalSize:padding:
cropPixelBuffer:crop:error:
resizePixelBuffer:size:error:
resizePixelBuffer:size:pixelFormat:options:error:
createJPEGDataFromPixelBuffer:scale:encodeQuality:error:
createPixelBufferFromJPEGPath:error:
createPixelBufferFromJPEGData:error:
createPixelBufferFromImageData:error:
dewarpPixelBuffer:crop:size:pixelFormat:options:cameraModel:error:
imposeMinSizeFor:withOriginalSize:minCrop:
maintainAspectRatio:originalSize:ratioThreshold:
saveVideoFrame:videoId:baseURL:error:
transferPixelBuffer:rotationAngle:crop:size:error:
initWithService:
readValue
_service
_updateThermalLevel
services
objectForKey:
readValueFromSensor:value:
readMaxValue:
thermalLevel
_client
_thermalLevelNotificationToken
_notificationQueue
_thermalLevel
_services
tick
numberWithUnsignedLongLong:
setZeroPadsFractionDigits:
setAllowedUnits:
setCountStyle:
setAllowsNonnumericFormatting:
stringFromByteCount:
highWaterMark
initWithName:reason:userInfo:
stop
setHighWaterMark:
average
_highWaterMark
_tick
initWithAsset:readVideoTrack:readAudioTrack:
assetReaderWithAsset:error:
_createOutputsForAsset:readVideo:readAudio:
tracksWithMediaType:
assetReaderTrackOutputWithTrack:outputSettings:
setAlwaysCopiesSampleData:
canAddOutput:
addOutput:
cancelReading
copyNextSampleBuffer
status
copyNextSampleBufferWithTrackIndexOutput:
startReading
_copyNextSampleBufferFromTrackOutput:
initWithAsset:
_asset
_assetReader
_trackSamples
_trackOutputs
initWithCachePolicy:
setName:
setup
homes
hmf_firstObjectWithUUID:
personManager
residentDevices
isCurrentDevice
photosPersonManagerWithUUID:
accessories
cameraProfiles
uuid
isSetup
defaultPrivateConfiguration
setOptions:
cachePolicy
setCachePolicy:
setDiscretionary:
homeKitOperationQueue
setDelegateQueue:
setDidUpdateHomes:
initWithHomeMangerConfiguration:
dateWithTimeIntervalSinceNow:
_refreshBeforeDate:completionHandler:
isPrimary
initWithNoCaching
homePersonManagerForHomeUUID:
homePersonManagerForCurrentDevice
photosPersonManagerForHomeUUID:sourceUUID:
isCurrentDevicePrimaryResident
cameraProfileWithUUID:
homeWithCameraProfileUUID:
_setup
_homes
_homeKitOperationQueue
_cachePolicy
didUpdateHomes
homeManager:didUpdateAuthorizationStatus:
homeManagerDidUpdateHomes:
homeManagerDidUpdatePrimaryHome:
homeManager:didAddHome:
homeManager:didRemoveHome:
homeManager:didReceiveAddAccessoryRequest:
_didUpdateHomes
input
featureValueWithMultiArray:
initWithInput:inputName:
setInput:
_input
modelWithContentsOfURL:error:
setObject:atIndexedSubscript:
dataScalerInputName
scalerModel
predictionFromFeatures:error:
dataScalerOutputName
svmInputName
svmOutputName
int64Value
defaultModelPath
defaultDataScalerPath
initWithModelPath:dataScalerPath:error:
predict:output:error:
_scalerModel
_status
_error
handleVideoSampleBuffer:
handleAudioSampleBuffer:
handleSampleBuffer:
flush
flushAsync
task
setTask:
_task
photosPersonManager
initWithUUID:name:
initWithUUID:dataRepresentation:dateCreated:faceBoundingBox:personUUID:
modelUUID
faceCropUUID
initWithUUID:data:modelUUID:faceCropUUID:
addOrUpdateFaceprints:completion:
initWithHMPhotosPersonManager:
setPhotosPersonManager:
_photosPersonManager
stringWithUTF8String:
mainBundle
localizedStringForKey:value:table:
errorWithDomain:code:userInfo:
createNSErrorWithStatus:andMessage:
createNSExceptionWithStatus:andMessage:
initWithUUID:dataRepresentation:dateCreated:faceBoundingBox:
arrayByAddingObjectsFromArray:
copyWithZone:
_personUUID
events
allObjects
frame
na_filter:
na_each:
eventClasses
maxConfidenceEventForEventClass:
decodeObjectOfClasses:forKey:
initWithFrame:events:
maxConfidenceEvents
_frame
_events
URLWithString:
initWithURL:options:
resourceLoader
setDelegate:queue:
contentInformationRequest
setContentType:
setContentLength:
setByteRangeAccessSupported:
dataRequest
requestedOffset
requestedLength
dataWithBytesNoCopy:length:freeWhenDone:
respondWithData:
finishLoading
loadValuesAsynchronouslyForKeys:completionHandler:
resourceLoader:shouldWaitForLoadingOfRequestedResource:
resourceLoader:shouldWaitForRenewalOfRequestedResource:
resourceLoader:didCancelLoadingRequest:
resourceLoader:shouldWaitForResponseToAuthenticationChallenge:
resourceLoader:didCancelAuthenticationChallenge:
loadValuesSynchronously
initWithValue:
windowSize
queue
setMovingAverage:
setQueue:
_movingAverage
_queue
_windowSize
integerValue
analyzerEvents
na_flatMap:
_eventsFromAnalyzerEvents:
_annotationScoresFromAnalyzerEvents:
frameResults
confidenceThatEventOccurred:
aggregatedEvents
resultCode
lastSequenceNumber
analysisFPS
duration
creationDate
isEqualToArray:
posterFrames
videoFragment
timeToAnalyzeFragment
timeSinceFragmentWasSubmitted
numberWithInteger:
initWithEvents:posterFrames:frameResults:annotationScores:duration:creationDate:resultCode:lastSequenceNumber:
initWithPosterFrames:frameResults:duration:creationDate:resultCode:lastSequenceNumber:
aggregatedEventTypes
isEqual:excludeTime:
annotationScores
setDuration:
setCreationDate:
setResultCode:
setTimeToAnalyzeFragment:
setTimeSinceFragmentWasSubmitted:
setVideoFragment:
setAnalysisFPS:
_analysisFPS
_annotationScores
_posterFrames
_frameResults
_creationDate
_resultCode
_timeToAnalyzeFragment
_timeSinceFragmentWasSubmitted
_videoFragment
_lastSequenceNumber
_duration
initWithPosterFrameGenerationInterval:posterFrameHeight:maxFragmentAnalysisDuration:maxFragmentDuration:transcodeFragment:
getAnalysisServiceTypePreference
numberPreferenceForKey:defaultValue:withMap:
allocWithZone:
initWithPosterFrameGenerationInterval:posterFrameHeight:maxFragmentAnalysisDuration:maxFragmentDuration:
posterFrameGenerationInterval
posterFrameHeight
maxFragmentAnalysisDuration
maxFragmentDuration
transcodeFragment
serviceType
setServiceType:
startingMediaIntegritySequenceNumber
setStartingMediaIntegritySequenceNumber:
useScheduler
setUseScheduler:
inMediaAnalysis
setInMediaAnalysis:
faceClassificationEnabled
setFaceClassificationEnabled:
currentSessionDuration
setCurrentSessionDuration:
_transcodeFragment
_useScheduler
_inMediaAnalysis
_faceClassificationEnabled
_posterFrameGenerationInterval
_posterFrameHeight
_maxFragmentAnalysisDuration
_maxFragmentDuration
_serviceType
_startingMediaIntegritySequenceNumber
_currentSessionDuration
confidenceThatEventOccurred:events:annotationScores:
initWithEvents:annotationScores:videoFrame:
videoFrame
_videoFrame
_detectionsFromAnalyzerEvents:
_faceClassificationsFromAnalyzerEvents:
face
level
initWithFrame:events:annotationScores:detections:regionOfInterest:faceClassifications:
initWithFrame:regionOfInterest:analyzerEvents:
detections
regionOfInterest
faceClassifications
_detections
_faceClassifications
_analyzerEvents
_regionOfInterest
initWithSequenceNumber:data:moovFragment:eventTypes:activityZones:
setUrl:
moovFragment
appendData:
sequenceNumber
stringByAppendingFormat:
absoluteString
initWithSequenceNumber:data:moovFragment:
initWithSequenceNumber:data:moovFragment:eventTypes:
initWithSequenceNumber:fragmentData:eventTypes:
initWithSequenceNumber:fragmentData:eventTypes:activityZones:url:
fragmentData
eventTypes
activityZones
_sequenceNumber
_moovFragment
_eventTypes
_activityZones
_url
totalRequests
setTotalRequests:
flag
predictions
setPredictions:
totalPredictions
setTotalPredictions:
lastRequestResult
isEqualToSet:
repetitions
setRepetitions:
totalRepetitions
setTotalRepetitions:
setLastRequestResult:
setLastRequestSignificantEvents:
analyzer
scheduler
activeAnalyzerCount
maxPredictions
minRepetitions
identifier
lastRequestSignificantEvents
fragment
attributes
assetDuration
initWithMinRepetitions:maxPredictions:analyzer:
addRequest:result:significantEvents:
shouldPredictRequest:
predictedSignificantEventsForRequest:
predictedResultForRequest:
reset
_minRepetitions
_maxPredictions
_predictions
_repetitions
_totalPredictions
_totalRepetitions
_totalRequests
_lastRequestResult
_lastRequestSignificantEvents
_analyzer
significantEventsInternal
setFlag:
initWithAssetData:error:
loadAttributesFromVideoFragment:
analysisSubmissionTime
firstObject
dimensions
initWithDimensions:codecType:realTime:error:
initWithVideoFormat:audioFormat:
firstSequenceNumber
setNextSequenceNumber:
setAllowRecovery:
encoder
retimer
initWithGenerationFrequency:andPosterFrameHeight:
initWithInput:
startHandlingFrames
readerForVideoFragment:workQueue:logIdentifier:
maxAnalysisFPS
initWithResourceAttributes:sampleRate:
preAnalyze:
handleMotionDetection:sessionPTS:frameDimensions:sessionIdentifier:
setAssetWriterDidOutputInitializationSegment:
mutableCopy
initWithInitializationSegment:separableSegment:
setAssetWriterDidOutputSeparableSegment:
assetWriter
initWithSampleBuffer:
localizedStringFromDate:dateStyle:timeStyle:
averageBitRate
points
drawPolygonWithNormalizedPoints:
drawTextHeaderBar:
draw:
baseDecodeTimeStamp
audioSamples
posterFrameGenerator
videoFrameResults
timeSinceAnalysisStart
timeSinceAnalysisSubmission
analysisStartTime
encoder:didEncodeSampleBuffer:
encoder:didFailWithError:
retimer:didRetimeSampleBuffer:
selector:maySelectFrame:
selector:didDetectMotion:atSessionPTS:frameDimensions:
initWithVideoFragment:analyzer:maxAnalysisFPS:
addSignificantEvent:
significantEvents
markForPrediction
shouldSkipAnalysis
shouldFailAnalysis
loadAttributes
startAnalysis
startEncodingSessionForAsset:error:
startPosterFrameGeneratorWithInterval:frameHeight:
startAssetReaderWithWorkQueue:logIdentifier:
startFrameSelector
finishEncoderSession
makeDidAnalyzeResult
makeDidNotAnalyzeResultWithResultCode:
cancel
frameSelector
assetReader
setVideoFrameResults:
phase
setPhase:
_analysisSubmissionTime
_analysisStartTime
_maxAnalysisFPS
_fragment
_attributes
_encoder
_retimer
_audioSamples
_assetWriter
_posterFrameGenerator
_frameSelector
_videoFrameResults
_significantEventsInternal
_phase
_flag
weakObjectsPointerArray
setSystemResourceUsageMonitorUsageLevel:
systemResourceUsageMonitorUsageLevel
numberPreferenceForKey:defaultValue:
isProductTypeJ105
currentState
isPictureInPictureActive
isProductTypeB238
inFullBypassMode
numberWithUnsignedInteger:
analyzers
isActive
averageAnalysisTimeMovingAverage
averageTotalAnalysisTimeMovingAverage
averageTotalAnalysisTime
isProductTypeJ42
updateAnalysisFPS:
internalAnalyzers
hmf_addObject:
assertOwner
addPointer:
compact
setCount:
_compactInternalAnalyzers
isPaused
logState
processPendingRequests
history
maxConcurrentAnalyzers
setInBypassMode:
_markPendingRequestsWithFlag:
averageAnalysisTime
pendingRequests
stringByPaddingToLength:withString:startingAtIndex:
_outcomeCountsAsString
mediaIntegritySequenceNumber
_flagCountsAsString
inBypassMode
inErrorState
analysisInProgress
appendString:
systemResourceUsageDidChangeTo:
transcodingAnalyzerCount
requestDidEnd:outcome:
registerAnalyzer:
removeAllAnalyzers
setPaused:
systemResourceUsageMonitor
analysisFPSPreference
_paused
_maxConcurrentAnalyzers
_internalAnalyzers
_systemResourceUsageMonitor
_systemResourceUsageMonitorUsageLevel
_averageAnalysisTimeMovingAverage
_averageTotalAnalysisTimeMovingAverage
_analysisFPSPreference
qosMap
_scheduleRequest:
sessionEnded
configuration
_handleError:request:
setLastRequestSubmissionTime:
internalPendingRequests
hmf_enqueue:
pendingRequestsCount
lastRequestSubmissionTime
timeIntervalSinceNow
setSessionEnded:
setRunRemotely:
_handleError:request:description:
skipSequentialMediaIntegrityCheck
nominalFrameRate
_handleDidNotAnalyzeRequest:resultCode:
_shouldContinueAnalyzingRequest:resultCode:
hmf_maybeDequeue
_analyzeRequest:
setAnalysisInProgress:
_checkRequest:
_predictRequest:
_analyzeRequestRemotely:retryOnConnectionInterruption:
_analyzeRequestLocally:
setMediaIntegritySequenceNumber:
homePersonManager
remoteAnalysisService
preferenceOverrides
analyzer:didFindSignificantEvent:inFragment:
code
_handleError:request:underlyingError:
_handleDidAnalyzeRequest:withResult:
_handleDidNotAnalyzeRequest:withResult:error:
requestAnalysisForAssetData:withProperties:andCompletionHandler:
warmStartModel
warmStart
_willAnalyzeRequest:
_handleError:request:description:underlyingError:
_analyzeRequestFramesLocally:
startReading:
analyzer:willAnalyzeRequest:
_finishEncodingSessionForRequest:withResult:
_sendAnalyticsEventForRequest:outcome:result:error:
shouldUploadVideoAnalysisEvent
sendEventForFaceEvent:homePersonManagerUUID:
_requestDidEnd:outcome:
_notifyDidAnalyzeRequest:withResult:
_handleDidNotAnalyzeRequest:resultCode:error:
_notifyDidNotAnalyzeRequest:withResult:
hmiErrorWithCode:description:underlyingError:
_enterErrorState
_notifyDidFailAnalysisForRequest:withError:
analyzer:didAnalyzeFragment:withResult:
analyzer:didAnalyzeRequest:withResult:
analyzer:didNotAnalyzeFragment:withResult:
analyzer:didNotAnalyzeRequest:withResult:
analyzer:didFailAnalysisForFragment:withError:
analyzer:didFailAnalysisForRequest:withError:
setInErrorState:
readNextFrame:error:
willHandleFramesFromVideoResource:
handleVideoFrame:error:
isFinishedEarly
_analyzeRequestFrames:
_handleDidAnalyzeRequest:
frames
_analyzeVideoFrame:request:result:error:
analyze:targetEventTypes:enableFaceClassification:sessionIdentifier:error:
saveToJsonActivityZones:videoFragmentUrl:frameId:UUID:detectionID:zoneID:
saveActivityZoneresultsInJSON:result:videoFrame:
filterEvents:withActivityZones:insetPercentageInclusion:insetPercentageExclusion:
generateAndSubmitStats:filteredEvents:activityZones:
_analyzeFrame:request:error:
filterDetectedObjects:request:result:
shouldSaveVideoFramesToDisk
saveFaceClassifications:videoId:fragmentId:frameId:baseURL:error:
userInfo
eventConfidenceThresholdsHigh
eventConfidenceThresholdsMedium
string
bundleWithIdentifier:
infoDictionary
queryVersionInformation
classHierarchyMap
initWithConfiguration:identifier:
analyzeFragment:
clearPendingFragments
setHomePersonManager:
externalPersonManagers
setExternalPersonManagers:
streamAnalyzer
currentRequest
setCurrentRequest:
setConfiguration:
setRemoteAnalysisService:
setSaveVideoFramesToDisk:
_flagCounts
_outcomeCounts
_skipSequentialMediaIntegrityCheck
_analysisInProgress
_inErrorState
_inBypassMode
_sessionEnded
_uploadVideoAnalysisEvent
_saveVideoFramesToDisk
_delegate
_identifier
_homePersonManager
_externalPersonManagers
_internalPendingRequests
_lastRequestSubmissionTime
_history
_streamAnalyzer
_currentRequest
_scheduler
_mediaIntegritySequenceNumber
_configuration
_remoteAnalysisService
appendFloats:count:
unsignedIntegerValue
initWithFloats:count:
mutableFloats
mutableBytes
floats
subtract:
initWithDataTensor:
fillWithFloat:
appendArray:
l2Norm
floatArrayByAdding:
floatArrayBySubtracting:
mutableCopyWithZone:
faceObservationWithRequestRevision:unalignedBoundingBox:alignedBoundingBox:
faceprinter
createFacePixelBufferForFaceDetection:pixelBuffer:roll:error:
computeJunkScoreForPixelBuffer:
initWithEntropyOfLaplacianScore:
createFaceprintForFacePixelBuffer:fastMode:error:
predictPersonFromFaceObservation:error:
descriptorData
currentModelUUID
floatValue
classificationThresholdKnown
linkedEntityUUID
initWithShape:dataType:error:
classificationThresholdUnknown
initWithUUID:sourceUUID:sessionEntityUUID:faceCrop:faceprint:confidence:familiarity:
initWithFaceCrop:faceprint:classifications:predictedLinkedEntityUUIDs:
classifyFaceEvent:pixelBuffer:fastMode:error:
initWithError:
faceQualityFilter
_faceprinter
_faceQualityFilter
_classificationThresholdKnown
_classificationThresholdUnknown
thresholdWithLabels
thresholdDefault
initWithThresholdWithLabels:thresholdDefault:
thresholdForLabel:
_thresholdWithLabels
_thresholdDefault
setFrameAnalyzer:
modelTimeoutPreference
frameAnalyzer
kick
eventConfidenceFaceThreshold
initWithMediumConfidenceThresholds:highConfidenceThresholds:faceThreshold:nmsConfiguration:assetPath:error:
ensureFrameAnalyzerWithError:
personThresholdMedium
petThresholdMedium
vehicleThresholdMedium
personThresholdHigh
petThresholdHigh
vehicleThresholdHigh
faceThreshold
getConfidenceThresholdPreferenceForKey:defaultConfidenceThreshold:
eventConfidenceThresholdsMediumFromTrial
eventConfidenceThresholdsHighFromTrial
eventConfidenceFaceThresholdFromTrial
_frameAnalyzer
containsObject:
vectorWithString:
CGAffineTransformValue
valueWithBytes:objCType:
valueAtIndex:
initWithDictionary:forType:
initWithDictionary:
_name
_type
attributeForKey:
filterWithName:withInputParameters:
expectedAttributeForKey:
setValue:forKey:
outputImage
applyToImage:withProbability:
probability
_probability
imageData
initWithImageData:regionOfInterest:detections:
arrayWithArray:
initWithData:type:shape:strides:
distantPast
initWithBundleIdentifier:
isPermitted
saveRecordWithData:recordInfo:completion:
saveDESRecordWithVideoFrame:recordInfo:
augmentWithOptions:
createImageTensorWithError:
createBoxesTensorWithError:
createClassesTensorWithError:
createWeightsTensorWithError:
_boxesTensorData
_weightsTensorData
_classesTensorData
_imageData
samples
imageName
boxesName
weightsName
classesName
initWithSamples:imageName:boxesName:weightsName:classesName:
dataPointAtIndex:error:
numberOfDataPoints
_samples
_imageName
_boxesName
_weightsName
_classesName
initWithCVPixelBuffer:imageParameters:error:
setMaxNumberOfElements:
_labelIndex
_confidence
_boundingBox
sortedArrayUsingComparator:
nonMaximumSuppression:output:withThreshold:
intersectionOverUnion:b:
convertObjectDetections:cropRect:originalImageSize:
presentationTimeStamp
recognizeFaces
sessionIdentifier
frameAnalyzer:didAnalyzeFrame:error:
handleSampleBuffer:motionDetections:
setRecognizeFaces:
_analysisTime
_recognizeFaces
_sessionIdentifier
frameAnalyzerDidAnalyzeFrame
setFrameAnalyzerDidAnalyzeFrame:
_frameAnalyzerDidAnalyzeFrame
initWithData:timeOffset:width:height:
timeOffset
width
height
_width
_height
_timeOffset
generationFrequency
frameHeight
_frameHeight
_generationFrequency
posterFramesInternal
nextGenerationTime
setNextGenerationTime:
saveAsPosterFrame:error:
setPosterFramesInternal:
_posterFramesInternal
_nextGenerationTime
detectFacesInImageData:error:
unalignedBoundingBox
faceBoxFromPhotosFaceCropImageData:
newDictionaryPopulatedWithFaceCropDataFromImageData:
dataUsingEncoding:
JSONObjectWithData:options:error:
isEqualToData:
isEqualToDate:
encodeRect:forKey:
decodeRectForKey:
faceCropFromPhotosFaceCropImageData:
_UUID
_dataRepresentation
_dateCreated
_faceBoundingBox
_lastSample
retimerDidRetimeSampleBuffer
setRetimerDidRetimeSampleBuffer:
_retimerDidRetimeSampleBuffer
manufacturer
model
initWithIdentifier:name:manufacturer:model:
_manufacturer
_model
initWithTaskID:timeout:
initWithTaskID:
results
taskID
_taskID
initPrivate
nextTaskID
setNextTaskID:
buildUpdatePersonsModelTaskFromOptions:error:
buildRemovePersonsModelTaskFromOptions:error:
buildHomePersonClusteringTaskFromOptions:error:
buildTuriTrialUpdateTaskFromOptions:error:
getNextTaskID
buildFaceMisclassificationTaskFromOptions:error:
buildPersonsModelsSummaryTaskFromOptions:error:
buildSubmitFeedbackTaskFromOptions:error:
submitTask:completionHander:
operations
enumerateObjectsUsingBlock:
boolValue
stringPreferenceForKey:defaultValue:
initWithHMHomePersonManager:
initWithHomeUUID:sourceUUID:error:
initWithTaskID:sourceUUID:dataSource:externalLibrary:
initWithTaskID:sourceUUID:
initWithTaskID:dataSource:faceCrop:
initWithTaskID:dataSource:error:
initWithTaskID:cameraProfileUUID:clipUUID:
cancelTask:
_nextTaskID
taskService
allowedClasses
score
initWithFrame:score:
_score
hasPreferenceForKey:
initWithResourceAttributes:sampleRate:targetInterval:
valuePreferenceForKey:defaultValue:withMap:
alloc
initWithSize:
framesInternal
detector
appendFramePixelBuffer:atTime:
detectWithGlobalMotionScore:
sortUsingComparator:
maxFrameCount
predictedFrames
removeObject:
sampler
appendFrame:error:
sampler:didFindSample:
sampler:didFindSample:target:
sampler:didDiscardFrame:
sampleRate
_sampler
_framesInternal
_maxFrameCount
_predictedFrames
_detector
_sampleRate
setFrame:
sampleInterval
targetInterval
unmatchedSampleFrames
isMarkedAsFinished
_appendFrame:error:
setMarkedAsFinished:
_markedAsFinished
_unmatchedSampleFrames
_targetInterval
_sampleInterval
existingAtCurrentVersion
unionSet:
createdAtCurrentVersion
initWithExistingAtCurrentVersion:createdAtCurrentVersion:existingAtOtherVersions:
allAtCurrentVersion
existingAtOtherVersions
_existingAtOtherVersions
_createdAtCurrentVersion
_existingAtCurrentVersion
_minorVersionFromVisionVersion:
hmf_zeroUUID
hmf_UUIDWithNamespace:data:
initWithCVPixelBuffer:options:
setInputFaceObservations:
setPrivateRevision:error:
setDetectionLevel:
performRequests:error:
createFacePixelBufferFromFaceCrop:error:
generateFaceprintForFaceCrop:error:
updatedFaceprintsForFaceCrops:withExistingFaceprints:error:
initWithData:options:
detectFacesInPixelBuffer:error:
_modelUUID
_faceCropUUID
initWithPoint:
_point
encodeBool:forKey:
decodeBoolForKey:
fileExistsAtPath:
fileHandleForReadingAtPath:
assetDirectoryPath
unpackageModelAssets:intoDirectory:error:
removeItemAtPath:error:
unTarFileWithFd:toPath:
unpackageModelAssetsAtPath:error:
pendingUpdates
setPendingUpdates:
_pendingUpdates
defaultSessionConfiguration
sessionWithConfiguration:delegate:delegateQueue:
URLSession:didBecomeInvalidWithError:
URLSession:didReceiveChallenge:completionHandler:
URLSessionDidFinishEventsForBackgroundURLSession:
homeKitClient
session
feedbackServiceHost
_homeKitClient
_session
_feedbackServiceHost
protectionSpace
host
serverTrust
credentialForTrust:
authenticationMethod
feedbackSession
_temporaryFileURLWithUUID:extension:
clipManager
initWithClipManager:clip:
setClipDestinationFileURL:
hmiPrivateErrorWithCode:description:suggestion:underlyingError:
setFetchVideoAssetContextCompletionBlock:
setDownloadProgressHandler:
faceClassification
person
fetchClipWithUUID:completion:
assetWithURL:
composition
addMutableTrackWithMediaType:preferredTrackID:
insertTimeRange:ofTrack:atTime:error:
initWithAsset:presetName:
setOutputFileType:
setOutputURL:
setShouldOptimizeForNetworkUse:
isInternalInstall
setTimeRange:
exportAsynchronouslyWithCompletionHandler:
feedbackServiceURL
base64EncodedDataWithOptions:
initWithData:encoding:
stringByAppendingPathComponent:
requestWithURL:
setHTTPMethod:
setValue:forHTTPHeaderField:
uploadTaskWithRequest:fromFile:completionHandler:
dataWithLength:
faceCrops
_base64StringFromData:
_attachEncryptedDataUsingKey:toPayload:error:
dataWithJSONObject:options:error:
feedbackRequestURLForClipWithUUID:
dataWithContentsOfURL:
_createPayloadWithServiceResult:error:
_uploadPayloadData:uploadURL:completionHandler:
dataTaskWithURL:completionHandler:
_stripAudioTrackFromAsset:completionHandler:
_downloadClipWithCameraProfileUUID:clipUUID:completionHandler:
removeItemAtURL:error:
_removeTemporaryFiles
_submitClipWithCameraProfileUUID:clipUUID:completionHandler:
initWithFeedbackSession:cameraProfileUUID:clipUUID:
_attachFaceCrops:toPayload:error:
cameraProfileUUID
clipUUID
temporaryFileURLs
setFaceCrops:
assetData
setAssetData:
_feedbackSession
_cameraProfileUUID
_clipUUID
_temporaryFileURLs
_faceCrops
_assetData
_operation
submitFeedbackWithCameraProfileUUID:clipUUID:runRemotely:completionHandler:
submitFeedbackWithCameraProfileUUID:clipUUID:completionHandler:
initWithPersonUUID:sourceUUID:sessionEntityUUID:confidence:familiarity:
initWithUUID:sourceUUID:faceBoundingBox:
initWithUUID:sourceUUID:sessionEntityUUID:faceBoundingBox:facecrop:faceprint:confidence:
decodeDoubleForKey:
decodeIntegerForKey:
encodeDouble:forKey:
encodeInteger:forKey:
initWithUUID:name:personsModelIdentifier:faceBoundingBox:
personsModelIdentifier
_personsModelIdentifier
_sourceUUID
_sessionEntityUUID
_familiarity
summary
sendEventForPersonsModels:
removeNearestFaceprint:withFaceCrops:
hmf_isEmpty
faceDistanceFromDescriptor:toDescriptor:
removeFaceCropsWithUUIDs:completion:
removePersonsModelForSourceUUID:error:
initWithUUID:
supportsFaceClassification
setSupportsFaceClassification:
_supportsFaceClassification
_homeUUID
levelThresholds
initWithValue:levelThresholds:
_levelThresholds
_hmiErrorWithCode:description:reason:suggestion:underlyingError:
hmiErrorWithCode:underlyingError:
hmiErrorWithCode:
hmiErrorWithCode:description:
initWithData:timeRange:
initWithInitializationSegment:separableSegment:timeRange:
numberWithUnsignedInt:
initWithInitializationSegment:separableSegment:timeRange:sequenceNumbers:
_ensureAttributes
videoFormatDescription
audioFormatDescription
initializationSegment
separableSegment
isCombinableWithFragment:
canFragmentData:
dataWithData:
timeRange
videoTrackTimeRange
sequenceNumbers
decodeCMTimeRangeForKey:
encodeCMTimeRange:forKey:
isInitializationSegment:combinableWithInitializationSegment:
fragmentData:handler:
initWithInitializationSegment:separableSegment:sequenceNumbers:
audioTrackTimeRange
_attributesLoaded
_videoFormatDescription
_audioFormatDescription
_initializationSegment
_separableSegment
_sequenceNumbers
_videoTrackTimeRange
_audioTrackTimeRange
_timeRange
initWithFrameDimensions:
handleMotionDetections:atTime:
sessionEntities
allKeys
trackNewFaces:personWithoutFaceEvents:withMotionDetection:atTime:
getUUIDToNextFaceIndexWithPreviousFaceindex:
predictedLinkedEntityUUIDs
intersectsSet:
removeObjectAtIndex:
initWithConfidence:boundingBox:face:
handleMotionDetection:sessionPTS:
assignSessionEntitiesToPersonWithFaceEvents:personWithoutFaceEvents:videoFrame:
_faceTracker
_sessionUUIDToPreviousFaceIndex
_sessionUUIDToPreviousFaceprints
_sessionEntities
configure
loadModelFromTrialWithError:
_createFontWithSize:
dictionaryWithObjectsAndKeys:
initWithString:attributes:
_sbuf
_context
_colorSpace
_font
resourceUsageMonitor
_resourceUsageMonitor
initWith
applyWithFrameResult:
initWithActivityZones:
initWithSourceUUID:externalLibrary:faceCountsByPerson:
isExternalLibrary
faceCountsByPerson
_externalLibrary
_faceCountsByPerson
visionPersonsModel
personUniqueIdentifiers
faceCountForPersonWithUniqueIdentifier:
initWithPersonsModel:sourceUUID:externalLibrary:
_visionPersonsModel
_addValueToContainer:forKey:
_containerIsArray
null
_valueForNumber:
stringFromDate:
base64EncodedStringWithOptions:
_JSONObjectWithObject:options:
na_dictionaryByMappingValues:
initWithDictionary
_addClassToContainer:
object
numberWithLongLong:
numberWithBool:
decimalNumberWithString:
JSONObjectStringWithObject:pretty:options:
JSONObjectWithObject:options:
JSONObjectStringWithObject:
allowsKeyedCoding
initWithArray
encodeInt32:forKey:
encodeInt64:forKey:
objectJSON
objectPrettyJSON
options
_container
_options
hasPrefix:
hasSuffix:
_objectWithJSONObject:allowedClasses:
classMap
initWithJSONObject:
setClassMap:
objectWithJSONData:classMap:
objectWithJSONObject:classMap:
objectWithJSONObjectString:classMap:
decodeInt32ForKey:
decodeInt64ForKey:
container
_classMap
timeZoneForSecondsFromGMT:
setTimeZone:
initWithLocaleIdentifier:
faceObservationWithRequestRevision:boundingBox:andAlignedBoundingBox:
initWithData:elementCount:elementType:lengthInBytes:labelsAndConfidence:requestRevision:
setFaceprint:
setFaceId:
anyObject
estimatePersonBoundingBoxFromFaceBoundingBox:
URLByAppendingPathExtension:
writeToURL:atomically:encoding:error:
faceObservationsFromFaceprintsForClustering:
faceObservationFromFaceprint:
eventsWithPersonsAndFacesMergedFromEvents:
_computeBlobPropertiesWithBoundingBox:personBoundingBox:dx:dy:motionVectors:isDetection:
blobID
personBoundingBox
motionMean
frameDimensions
boxDistanceToPersonBlob:
sizeDistanceToPersonBlob:
isBijectiveToPersonBlob:
isMoving
setFaceBoundingBox:
setPersonBoundingBox:
personIndices
personIndex
personIouMax
origin
target
midpoint
motion
setMotionMean:
setPosition:
initWithPersonWithFaceEvent:motionVectors:personIndex:regionOfInterest:frameDimensions:
initWithPersonWithoutFaceEvent:personIndex:regionOfInterest:frameDimensions:
initWithPersonBlob:motionVectors:personIndex:regionOfInterest:frameDimensions:
similarityToPersonBlob:
adjustFaceBoundingBoxFromPersonBlob:
isLost
setPersonIndex:
setPersonIndices:
setPersonIouMax:
_personIouMax
_personIndex
_personIndices
_blobID
_frameDimensions
_motionMean
_personBoundingBox
initWithProjectedFaceIndex:detectedFaceIndex:score:
projectedFaceIndex
detectedFaceIndex
_projectedFaceIndex
_detectedFaceIndex
compare:
initWithMotionVectors:regionOfInterest:time:
motionVectors
_motionVectors
setPreviousTime:
motionRecordsQueue
previousPersons
setPreviousPersons:
setPreviousPersonIndices:
previousTime
indexSetWithIndexesInRange:
containsIndex:
removeIndex:
removeObjectForKey:
enumerateIndexesUsingBlock:
enumerateKeysAndObjectsUsingBlock:
previousPersonIndices
_previousPersons
_previousPersonIndices
_motionRecordsQueue
_previousTime
initWithAssetDuration:creationDate:firstSequenceNumber:lastSequenceNumber:
initWithAssetDuration:creationDate:firstSequenceNumber:lastSequenceNumber:nominalFrameRate:dimensions:baseDecodeTimeStamp:
initWithAssetDuration:creationDate:
_firstSequenceNumber
_nominalFrameRate
_dimensions
_assetDuration
_baseDecodeTimeStamp
initWithVideoFragment:workQueue:logIdentifier:
resourceLoaderWorkQueue
initWithAsset:error:
setAssetReader:
assetKeys
_propertiesLoadedForAsset:resultCallback:
fragments
statusOfValueForKey:error:
_didKeyValueLoadFailed:
trackKeys
_propertiesLoadedForTrack:fromAsset:resultCallback:
initWithTrack:outputSettings:
setMaximizePowerEfficiency:
_validateSequentialIntegrityOfFragmentsInAsset:
_sequenceNumberOfLastFragmentInAsset:
_sequenceNumberOfFirstFragmentInAsset:
dateValue
formatDescriptions
latentBaseDecodeTimeStampOfFirstTrackFragment
outputs
currentFrameId
setCurrentFrameId:
request
finishLoadingWithError:
requestsAllDataToEndOfResource
_logIdentifier
_currentFrameId
_resourceLoaderWorkQueue
_initSessionWithError:
setMaxKeyFrameIntervalDuration:
setAverageBitRate:
_invalidateSession
_encodeSampleBuffer:attemptRecovery:
maxKeyFrameIntervalDuration
numberOfDroppedFrames
_forceKeyFrameOnNextEncodedFrame
_codecType
_realTime
_maxKeyFrameIntervalDuration
_averageBitRate
_numberOfDroppedFrames
encoderDidEncodeSampleBuffer
encoderDidFailWithError
setEncoderDidEncodeSampleBuffer:
setEncoderDidFailWithError:
_encoderDidEncodeSampleBuffer
_encoderDidFailWithError
addOrUpdateFaceCrops:completion:
addOrUpdatePersons:completion:
fetchAllUnassociatedFaceCropsWithCompletion:
removePersonsWithUUIDs:completion:
associateFaceCropsWithUUIDs:toPersonWithUUID:completion:
addPersons:completion:
isImportingFromPhotoLibraryEnabled
isSharingFaceClassificationsEnabled
setImportingFromPhotoLibraryEnabled:
setSharingFaceClassificationsEnabled:
shortDescription
privateDescription
propertyDescription
_importingFromPhotoLibraryEnabled
_sharingFaceClassificationsEnabled
entropyOfLaplacianScore
numberOfFaceprintsClustered
numberOfClusters
numberOfPersonsCreated
numberOfUnknownFaceprintsAssociated
faceprintingDuration
clusteringDuration
totalDuration
modelSummaries
valueForKeyPath:
homeToExternalEquivalencies
externalToExternalEquivalencies
sendEventForClusteringTask:
initWithPersonUUID:sourceUUID:
initWithModelSummaries:homeToExternalEquivalencies:externalToExternalEquivalencies:
_modelSummaries
_homeToExternalEquivalencies
_externalToExternalEquivalencies
initWithSourceUUID:personUUID:confidence:linkedEntityUUID:
_linkedEntityUUID
initWithUUIDString:
faceObservationsForPersonWithUniqueIdentifier:error:
facesAreSamePersonFromSet:andSet:
setByAddingObjectsFromSet:
setByAddingObject:
facesAreSamePersonFromSet:andSet:distanceThreshold:percentMatchingThreshold:
setMaximumIdentities:
initWithConfiguration:
addFaceObservations:toPersonWithUniqueIdentifier:error:
loadModelsWithError:
homePersonsModel
getModelStoragePathForModel:error:
setPersonsModels:
persistModel:toPath:error:
personsModels
buildEquivalencyMapForPersonsModels:error:
predictPersonFromFaceObservation:limit:canceller:error:
predictedPersonUniqueIdentifier
personToEquivalencyCell
modelFromURL:options:error:
setReadOnly:
writeToURL:options:error:
getRootModelStoragePathWithError:
pathWithComponents:
URLForDirectory:inDomain:appropriateForURL:create:error:
URLByAppendingPathComponent:isDirectory:
absoluteURL
contentsOfDirectoryAtURL:includingPropertiesForKeys:options:error:
modelURLsFromPath:error:
loadPersonsModelFromURL:externalLibrary:error:
URLByDeletingPathExtension
lastPathComponent
pathExtension
loadModelAtPath:error:
buildPersonsModelForSourceUUID:externalLibrary:faceObservationsByPerson:error:
predictHomePersonFromFaceObservation:error:
_homePersonsModel
_personsModels
_personToEquivalencyCell
initWithName:weights:biases:
weights
biases
_weights
_biases
initWithLayerParameters:losses:
layerParameters
losses
_layerParameters
_losses
getParameterOfType:forLayerNamed:error:
networkPath
initWithTrainingNetworkPath:inferenceInputs:inferenceOutputs:trainingInputs:trainingOutputs:trainingControlVariableName:withInitializer:error:
initWithTrainingModelDefinition:forPlatform:error:
getParametersFromLayers:fromTask:error:
getTensorNamed:
doTrainingOnData:forNumberOfEpochs:withCallback:error:
initWithTrainingNetworkPath:data:error:
trainLayers:epochs:inferenceInputs:inferenceOutputs:trainingInputs:trainingOutputs:error:
_networkPath
initWithCode:message:
message
success
skipped
isSkipped
isSuccess
_message
_code
removePersonsModelWithRetryOnError:
external
_external
_getAnalyzers
_updateAnalyzer:withIndex:
_logState
monitored
delay
stringWithString:
stateDescription
_usageMonitor
_usageLevel
clientWithIdentifier:
updateLevels
submitUpdateModelTask
addUpdateHandlerForNamespaceName:usingBlock:
levelForFactor:withNamespaceName:
fileValue
registerForTrialUpdates
modelPath
_trialClient
_compiledModelArchivePath
_personThresholdHigh
_personThresholdMedium
_petThresholdHigh
_petThresholdMedium
_vehicleThresholdHigh
_vehicleThresholdMedium
_faceThreshold
_modelPath
lengthInBytes
faceId
initWithFaceThreshold:singleLinkThreshold:percentConnectionsThreshold:faceprintRevision:error:
addFaceObservations:toFaceDescriptorBuffer:error:
convertToClusters:
setObjects:
setClusterId:
objects
setTotalObjectCount:
setShouldUpdateRepresentative:
dataWithBytes:length:
centermostFaceprintInCluster:faceObservations:
getClustersWithFaces:error:
.cxx_construct
_greedyClusterer
initWithConfiguration:identifier:remote:
thumbnailInterval
thumbnailHeight
transcode
eventTriggers
legacyAnalyzer
_handleFragment:withResult:events:outcome:
analyzer:didFailWithError:
initWithJPEGData:size:presentationTimeStamp:
_makeFrameResult:withPresentationTimeStamp:
analyzer:didAnalyzeFrame:
initWithFragment:events:frameResults:thumbnails:configuration:outcome:
analyzer:didAnalyzeFragment:
analyzeFragment:configuration:
setMonitored:
_group
_configurationBySequenceNumber
_pendingFragments
_failed
_cancelled
_sessionDuration
_legacyAnalyzer
frameSampler:didSampleFrame:
initWithInterval:
_interval
_nextTime
frameSamplerDidSamplerFrame
setFrameSamplerDidSamplerFrame:
_frameSamplerDidSamplerFrame
packageAnalyzerDidDetectPackage
packageAnalyzer:didDetectPackage:error:
setPackageAnalyzerDidDetectPackage:
_packageAnalyzerDidDetectPackage
_entropyOfLaplacianScore
_numBins
_maxLaplacianScore
_minLaplacianScore
_binWidth
_maxScore
_histogram
computeJunkScoreForFacePrint:
_maxDistanceScore
_junkCentroid
thumbnails
outcome
_thumbnails
_outcome
decodeCMTimeForKey:
encodeCMTime:forKey:
isEqualToDictionary:
decodeIntForKey:
encodeInt:forKey:
initWithPoints:isInclusion:
isInclusion
overlapsWithElipseInsideRect:
overlapsWithElipseInsideRect:withInsetPercentage:
containsCornersOfRect:withInsetPercentage:
stringByDeletingPathExtension
jsonReperesentaionOfDetectedObject:
na_all:
submitCoreAnalyticsEvent:filteringLevel:numberOfDetectedObjects:
initWithPoints:
_inclusion
_points
minFrameQuality
minFrameScale
camera
setThumbnailInterval:
setThumbnailHeight:
setMaxFragmentAnalysisDuration:
setMaxFragmentDuration:
setTranscode:
setMinFrameQuality:
setMinFrameScale:
setCamera:
_transcode
_minFrameQuality
_minFrameScale
_thumbnailHeight
_camera
_thumbnailInterval
detectPackages
setActivityZones:
setEventTriggers:
setDetectPackages:
_detectPackages
_eventTriggers
raise:format:
initWithBoundingBox:
applyEventTypeAndCheckIfSubBoundingIsStatic:forMetric:eventType:confidence:
_finish
didAnalyzeFragment
didFailAnalysisForFragment
didFindSignificantEvent
didNotAnalyzeFragment
setDidAnalyzeFragment:
setDidFailAnalysisForFragment:
setDidNotAnalyzeFragment:
setDidFindSignificantEvent:
_didAnalyzeFragment
_didFailAnalysisForFragment
_didFindSignificantEvent
_didNotAnalyzeFragment
strongToWeakObjectsMapTable
nextRequestID
setNextRequestID:
runRemotely
setPreferenceOverrideFromDictionary:
getNextRequestID
requests
expectedClasses
requestAnalysisForPixelBuffer:withProperties:andCompletionHandler:
cancelRequest:
_runRemotely
_nextRequestID
_requests
_face
flattenTrainingResult:
encryptedDataWithPublicKey:inPlaceDataFloatNumbers:count:error:
packageTrainingResult:privatize:maxNorm:normBinCount:encryptionKey:error:
preferenceCache
preferenceOverridesInternal
addEntriesFromDictionary:
preferenceLoggedValues
initWithKey:options:domain:defaultValue:
systemPreferenceValueForKey:
logPreferenceForKey:value:
caseInsensitiveCompare:
addPreferenceOverrideFromDictionary:
removeAllPreferenceOverrides
numberPreferenceForKey:defaultValue:withParser:
valuePreferenceForKey:defaultValue:withParser:
preferenceCacheFlushTimer
_preferenceCacheFlushTimer
_preferenceCache
_preferenceLoggedValues
_preferenceOverridesInternal
setNumberStyle:
numberFromString:
token
notificationName
publishValueForToken:
publishInitialValue
setToken:
callback
initWithNotificationName:andQueue:andCallback:
_token
_notificationName
_callback
setNumberOfFaceprintsClustered:
setNumberOfClusters:
setNumberOfPersonsCreated:
setNumberOfUnknownFaceprintsAssociated:
setFaceprintingDuration:
setClusteringDuration:
setTotalDuration:
setError:
_numberOfFaceprintsClustered
_numberOfClusters
_numberOfPersonsCreated
_numberOfUnknownFaceprintsAssociated
_faceprintingDuration
_clusteringDuration
_totalDuration
_stageZero_fetchFaceCrops
_stageOne_fetchFaceprints:
_stageTwo_generateFaceprintsForFaceCrops:existingFaceprints:
_stageThree_clusterFaceprints:
_stageFour_addPersons:clusterMapping:faceprints:
_stageFive_associateFaceCropsWithClusterMapping:faceprints:
subarrayWithRange:
startTime
_clusterer
_faceClassifier
_summary
_startTime
initWithDataSource:
addOperations:waitUntilFinished:
persons
isCancelled
initWithDataSource:person:
personFaceCrops
initWithDataSource:faceCropUUIDs:
faceprints
initWithDataSource:faceprints:
initWithDataSource:faceprintUUIDs:
_persons
_person
_personFaceCrops
faceCropUUIDs
_faceCropUUIDs
_faceprints
initWithFrame:size:
cropRect
_cropRect
mapTableWithKeyOptions:valueOptions:
setCountLimit:
significantActivityDetector
regionOfInterestOperations
regionOfInterestOperationQueue
sessionEntityManagers
waitUntilFinished
_analyzerEventsFromObjectDetections:
_simulatedEventForEventClass:
_targetEventsSetFromTargetEventTypes:enableFaceClassification:
_filterEvents:targetEventClasses:
_filterEvents:withMotionDetections:cropRectNormalized:
_eventsWithFaceClassificationAppliedFromEvents:videoFrame:sessionIdentifier:error:
eventsWithContentsOfFile:
compressedFrameWithScale:quality:error:
saveDESRecordVideoFrame:withResult:
lowercaseString
highConfidenceThresholds
_levelThresholdsForEventType:
mediumConfidenceThresholds
_rankForEventClass:
faceClassifier
transaction
setTransaction:
_mediumConfidenceThresholds
_highConfidenceThresholds
_regionOfInterestOperationQueue
_regionOfInterestOperations
_significantActivityDetector
_transaction
_sessionEntityManagers
archivedDataWithRootObject:requiringSecureCoding:error:
taskIdentifier
taskRunnerClass
initWithURL:
activityForScheduling
pixelBufferFrameWithError:
printWithHeight:
_store
_presentationTimeStamp
initWithContentsOfFile:options:error:
fileDescriptor
availableData
unpackageTarData:size:parentDir:
getDataOutWithSize:withFlag:fd:
uncompressedContentsForCompressedFile:outPath:
buffer:willHandleSampleBuffer:
bufferWillFlush:
bufferWillFinish:
initWithMaxDuration:
handleBlock:
videoSampleCount
videoDuration
videoDelay
_semaphore
_count
_maxDuration
_delay
bufferWillHandleSampleBuffer
bufferWillFinish
bufferWillFlush
setBufferWillHandleSampleBuffer:
setBufferWillFlush:
setBufferWillFinish:
_bufferWillHandleSampleBuffer
_bufferWillFlush
_bufferWillFinish
dataWithContentsOfFile:options:error:
confidenceLevel
_drainBuffer:
_failWithDescription:
decoder:didDecodeSampleBuffer:
_invalidateSession:
handleSampleBuffer:outputFrame:
_createSessionWithFormatDescription:
decoder:didFailWithError:
_didDecodeSampleBuffer:
_reorderBufferDidBecomeFull
_lastSampleBufferPTS
_lastSampleBufferDTS
decoderDidDecodeSampleBuffer
decoderDidFailWithError
setDecoderDidDecodeSampleBuffer:
setDecoderDidFailWithError:
_decoderDidDecodeSampleBuffer
_decoderDidFailWithError
_checkNotStarted
typeWithIdentifier:
initWithContentType:
setOutputFileTypeProfile:
setPreferredOutputSegmentInterval:
setInitialSegmentStartTime:
setInitialMovieFragmentSequenceNumber:
setProducesCombinableFragments:
initWithMediaType:outputSettings:sourceFormatHint:
setExpectsMediaDataInRealTime:
setMediaTimeScale:
canAddInput:
addInput:
assetWriterInputWithMediaType:outputSettings:sourceFormatHint:
startWriting
assetWriter:didOutputInitializationSegment:
trackReports
mediaType
firstVideoSampleInformation
assetWriter:didOutputSeparableSegment:timeRange:
_appendSampleBuffer:
_createAssetWriterWithInitialSegmentStartTime:
startSessionAtSourceTime:
allowRecovery
isReadyForMoreMediaData
appendSampleBuffer:
flushSegment
assetWriter:didFailWithError:
assetWriter:didOutputSegmentData:segmentType:segmentReport:
assetWriter:didOutputSegmentData:segmentType:
nextSequenceNumber
_skipInitializationSegment
_dropSamplesUntilSync
_outputQueue
_videoFormat
_audioFormat
_allowRecovery
_nextSequenceNumber
assetWriterDidOutputInitializationSegment
assetWriterDidOutputSeparableSegment
assetWriterDidFailWithError
setAssetWriterDidFailWithError:
_assetWriterDidOutputInitializationSegment
_assetWriterDidOutputSeparableSegment
_assetWriterDidFailWithError
inputs
updatePersonsModelWithRetryOnError:
initWithOrigin:motion:
distance
eventType
setEventType:
_eventType
_origin
_motion
initWithPixelBuffer:atTime:
classMotionScoreMap
classPaddingMap
scoreForSubBoundingBox:forMetric:eventType:confidence:
initWithBoundingBox:size:motionVectors:
_computeOpticalFlow:with:globalMotionScore:
_frames
sbuf
initWithSampleBuffer:score:detections:
setSampleRate:
_handleReference:target:
_drainCandidatesThatExpiredBefore:
_ensureDetectorForPixelBuffer:
frameSelector:didSelectFrame:detections:
_candidates
_enabled
_referenceInterval
_expirationInterval
_reference
frameSelectorDidSelectFrame
setFrameSelectorDidSelectFrame:
_frameSelectorDidSelectFrame
initWithFaceCrop:faceprint:classifications:
_classifications
_predictedLinkedEntityUUIDs
analyzerWithConfiguration:identifier:legacy:remote:error:
handleAssetData:withOptions:completionHandler:
analyzerWithOptions:error:
analyzerWithConfiguration:identifier:error:
handleAssetData:withOptions:errorHandler:
handleMessageWithOptions:completionHandler:
_notifyDelegateDidFailWithError:
_ensureAssetWriterForFragment:
handleSampleBuffer:errorHandler:
_handleDecodedSampleBuffer:
_ensureEncoderForSampleBuffer:
dynamicConfigurationForTime:
_notifyDelegateDidAnalyzeFrame:
_notifyDelegateDidAnalyzeFragment:
_produceResult:withArguments:
analyzer:didProduceResult:
_commandBuffer
_decoder
_frameThumbnailSampler
_encode
_currentPTS
_currentDTS
_frameAnalyzerResultBuffer
_thumbnailBuffer
_dynamicConfigurationBuffer
_packageAnalyzer
_numDecodedSamples
_numDidAnalyzeFrames
_numDidAnalyzeFragments
_monitored
analyzerDidAnalyzeFrame
analyzerDidAnalyzeFragment
analyzerDidFailWithError
setAnalyzerDidAnalyzeFrame:
setAnalyzerDidAnalyzeFragment:
setAnalyzerDidFailWithError:
_analyzerDidAnalyzeFrame
_analyzerDidAnalyzeFragment
_analyzerDidFailWithError
getStoragePath
getResourceValue:forKey:error:
dataWithContentsOfURL:options:error:
_sourceURL
featureValueWithInt64:
initWithClassLabel:
classLabel
setClassLabel:
_classLabel
URLOfModelInThisBundle
initWithContentsOfURL:error:
initWithContentsOfURL:configuration:error:
initWithMLModel:
loadContentsOfURL:configuration:completionHandler:
initWithFeatureProviderArray:
predictionsFromBatch:options:error:
featuresAtIndex:
loadWithConfiguration:completionHandler:
initWithConfiguration:error:
predictionFromInput:error:
predictionsFromInputs:options:error:
featureValueWithCGImage:pixelsWide:pixelsHigh:pixelFormatType:options:error:
imageBufferValue
initWithImage__Placeholder__0:
featureValueWithImageAtURL:pixelsWide:pixelsHigh:pixelFormatType:options:error:
initWithImage__Placeholder__0FromCGImage:error:
initWithImage__Placeholder__0AtURL:error:
setImage__Placeholder__0WithCGImage:error:
setImage__Placeholder__0WithURL:error:
image__Placeholder__0
setImage__Placeholder__0:
_image__Placeholder__0
initWithShotNet__ShotNet__ssd_predictions__block8_box__yaw_angle__0:ShotNet__ShotNet__ssd_predictions__block8_box__roll_angle__0:ShotNet__ShotNet__ssd_predictions__block8_box__box_offset__0:ShotNet__ShotNet__ssd_predictions__block8_box__class_prob__0:ShotNet__ShotNet__ssd_predictions__block7_box__yaw_angle__0:ShotNet__ShotNet__ssd_predictions__block7_box__roll_angle__0:ShotNet__ShotNet__ssd_predictions__block7_box__box_offset__0:ShotNet__ShotNet__ssd_predictions__block7_box__class_prob__0:ShotNet__ShotNet__ssd_predictions__block6_box__yaw_angle__0:ShotNet__ShotNet__ssd_predictions__block6_box__roll_angle__0:ShotNet__ShotNet__ssd_predictions__block6_box__box_offset__0:ShotNet__ShotNet__ssd_predictions__block6_box__class_prob__0:ShotNet__ShotNet__ssd_predictions__block9_box__yaw_angle__0:ShotNet__ShotNet__ssd_predictions__block9_box__roll_angle__0:ShotNet__ShotNet__ssd_predictions__block9_box__box_offset__0:ShotNet__ShotNet__ssd_predictions__block9_box__class_prob__0:ShotNet__ShotNet__ssd_predictions__block10_box__yaw_angle__0:ShotNet__ShotNet__ssd_predictions__block10_box__roll_angle__0:ShotNet__ShotNet__ssd_predictions__block10_box__box_offset__0:ShotNet__ShotNet__ssd_predictions__block10_box__class_prob__0:ShotNet__ShotNet__ssd_predictions__block11_box__yaw_angle__0:ShotNet__ShotNet__ssd_predictions__block11_box__roll_angle__0:ShotNet__ShotNet__ssd_predictions__block11_box__box_offset__0:ShotNet__ShotNet__ssd_predictions__block11_box__class_prob__0:
ShotNet__ShotNet__ssd_predictions__block8_box__yaw_angle__0
setShotNet__ShotNet__ssd_predictions__block8_box__yaw_angle__0:
ShotNet__ShotNet__ssd_predictions__block8_box__roll_angle__0
setShotNet__ShotNet__ssd_predictions__block8_box__roll_angle__0:
ShotNet__ShotNet__ssd_predictions__block8_box__box_offset__0
setShotNet__ShotNet__ssd_predictions__block8_box__box_offset__0:
ShotNet__ShotNet__ssd_predictions__block8_box__class_prob__0
setShotNet__ShotNet__ssd_predictions__block8_box__class_prob__0:
ShotNet__ShotNet__ssd_predictions__block7_box__yaw_angle__0
setShotNet__ShotNet__ssd_predictions__block7_box__yaw_angle__0:
ShotNet__ShotNet__ssd_predictions__block7_box__roll_angle__0
setShotNet__ShotNet__ssd_predictions__block7_box__roll_angle__0:
ShotNet__ShotNet__ssd_predictions__block7_box__box_offset__0
setShotNet__ShotNet__ssd_predictions__block7_box__box_offset__0:
ShotNet__ShotNet__ssd_predictions__block7_box__class_prob__0
setShotNet__ShotNet__ssd_predictions__block7_box__class_prob__0:
ShotNet__ShotNet__ssd_predictions__block6_box__yaw_angle__0
setShotNet__ShotNet__ssd_predictions__block6_box__yaw_angle__0:
ShotNet__ShotNet__ssd_predictions__block6_box__roll_angle__0
setShotNet__ShotNet__ssd_predictions__block6_box__roll_angle__0:
ShotNet__ShotNet__ssd_predictions__block6_box__box_offset__0
setShotNet__ShotNet__ssd_predictions__block6_box__box_offset__0:
ShotNet__ShotNet__ssd_predictions__block6_box__class_prob__0
setShotNet__ShotNet__ssd_predictions__block6_box__class_prob__0:
ShotNet__ShotNet__ssd_predictions__block9_box__yaw_angle__0
setShotNet__ShotNet__ssd_predictions__block9_box__yaw_angle__0:
ShotNet__ShotNet__ssd_predictions__block9_box__roll_angle__0
setShotNet__ShotNet__ssd_predictions__block9_box__roll_angle__0:
ShotNet__ShotNet__ssd_predictions__block9_box__box_offset__0
setShotNet__ShotNet__ssd_predictions__block9_box__box_offset__0:
ShotNet__ShotNet__ssd_predictions__block9_box__class_prob__0
setShotNet__ShotNet__ssd_predictions__block9_box__class_prob__0:
ShotNet__ShotNet__ssd_predictions__block10_box__yaw_angle__0
setShotNet__ShotNet__ssd_predictions__block10_box__yaw_angle__0:
ShotNet__ShotNet__ssd_predictions__block10_box__roll_angle__0
setShotNet__ShotNet__ssd_predictions__block10_box__roll_angle__0:
ShotNet__ShotNet__ssd_predictions__block10_box__box_offset__0
setShotNet__ShotNet__ssd_predictions__block10_box__box_offset__0:
ShotNet__ShotNet__ssd_predictions__block10_box__class_prob__0
setShotNet__ShotNet__ssd_predictions__block10_box__class_prob__0:
ShotNet__ShotNet__ssd_predictions__block11_box__yaw_angle__0
setShotNet__ShotNet__ssd_predictions__block11_box__yaw_angle__0:
ShotNet__ShotNet__ssd_predictions__block11_box__roll_angle__0
setShotNet__ShotNet__ssd_predictions__block11_box__roll_angle__0:
ShotNet__ShotNet__ssd_predictions__block11_box__box_offset__0
setShotNet__ShotNet__ssd_predictions__block11_box__box_offset__0:
ShotNet__ShotNet__ssd_predictions__block11_box__class_prob__0
setShotNet__ShotNet__ssd_predictions__block11_box__class_prob__0:
_ShotNet__ShotNet__ssd_predictions__block8_box__yaw_angle__0
_ShotNet__ShotNet__ssd_predictions__block8_box__roll_angle__0
_ShotNet__ShotNet__ssd_predictions__block8_box__box_offset__0
_ShotNet__ShotNet__ssd_predictions__block8_box__class_prob__0
_ShotNet__ShotNet__ssd_predictions__block7_box__yaw_angle__0
_ShotNet__ShotNet__ssd_predictions__block7_box__roll_angle__0
_ShotNet__ShotNet__ssd_predictions__block7_box__box_offset__0
_ShotNet__ShotNet__ssd_predictions__block7_box__class_prob__0
_ShotNet__ShotNet__ssd_predictions__block6_box__yaw_angle__0
_ShotNet__ShotNet__ssd_predictions__block6_box__roll_angle__0
_ShotNet__ShotNet__ssd_predictions__block6_box__box_offset__0
_ShotNet__ShotNet__ssd_predictions__block6_box__class_prob__0
_ShotNet__ShotNet__ssd_predictions__block9_box__yaw_angle__0
_ShotNet__ShotNet__ssd_predictions__block9_box__roll_angle__0
_ShotNet__ShotNet__ssd_predictions__block9_box__box_offset__0
_ShotNet__ShotNet__ssd_predictions__block9_box__class_prob__0
_ShotNet__ShotNet__ssd_predictions__block10_box__yaw_angle__0
_ShotNet__ShotNet__ssd_predictions__block10_box__roll_angle__0
_ShotNet__ShotNet__ssd_predictions__block10_box__box_offset__0
_ShotNet__ShotNet__ssd_predictions__block10_box__class_prob__0
_ShotNet__ShotNet__ssd_predictions__block11_box__yaw_angle__0
_ShotNet__ShotNet__ssd_predictions__block11_box__roll_angle__0
_ShotNet__ShotNet__ssd_predictions__block11_box__box_offset__0
_ShotNet__ShotNet__ssd_predictions__block11_box__class_prob__0
predictionFromImage__Placeholder__0:error:
initWithTransformed_features:
transformed_features
setTransformed_features:
_transformed_features
@16@0:8
B24@0:8@16
#16@0:8
@24@0:8:16
@32@0:8:16@24
@40@0:8:16@24@32
B16@0:8
B24@0:8#16
B24@0:8:16
Vv16@0:8
Q16@0:8
^{_NSZone=}16@0:8
B24@0:8@"Protocol"16
@"NSString"16@0:8
v24@0:8@?16
v32@0:8@16@?24
v24@0:8@?<v@?@"NSSet"@"NSError">16
v32@0:8@"NSSet"16@?<v@?@"NSSet"@"NSError">24
v24@0:8@?<v@?@"NSError">16
v32@0:8@"NSSet"16@?<v@?@"NSError">24
v24@0:8@?<v@?@"HMIExternalPersonManagerSettings"@"NSError">16
@"HMFLogCategory"16@0:8
@24@0:8@16
v24@0:8@16
v16@0:8
@"NSMutableArray"
@"NSMutableSet"
@"HMIDESMutableFloatArray"
@"MLFeatureValue"24@0:8@"NSString"16
@"NSSet"16@0:8
@32@0:8^{__CVBuffer=}16@24
^{__CVBuffer=}16@0:8
^{__CVBuffer=}
@"NSString"
@48@0:8@16@24@32^@40
B40@0:8^{__CVBuffer=}16@24^@32
B64@0:8^{__CVBuffer=}16@24@32@40@48^@56
v56@0:8@16@24@32@40@48
{CGSize=dd}16@0:8
[91d]
[6[6{CGSize="width"d"height"d}]]
[6Q]
@"MLModel"
@"NSArray"
@"HMINMSConfiguration"
@"MLPredictionOptions"
{CGSize="width"d"height"d}
q16@0:8
v24@0:8q16
@"HMISystemResourceUsage"16@0:8
@"<HMISystemResourceUsageMonitorDelegate>"16@0:8
v24@0:8@"<HMISystemResourceUsageMonitorDelegate>"16
@"HMISystemResourceUsageMonitorImpl"
@"NSObject<OS_dispatch_queue>"
@56@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24
@64@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24@56
@72@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24@56@64
@80@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24@56@64@72
@88@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24@56@64@72@80
@"HMIFaceRecognition"
@"HMIFaceQuality"
@"NSNumber"
I16@0:8
v24@0:8Q16
@24@0:8Q16
@"NSData"
{?=qiIq}16@0:8
@48@0:8@16{?=qiIq}24
{?="value"q"timescale"i"flags"I"epoch"q}
@24@0:8q16
@68@0:8{?={?=qiIq}{?=qiIq}}16B64
@64@0:8{?={?=qiIq}{?=qiIq}}16
@"HMFUnfairLock"
@48@0:8{?=qiIq}16@40
@"NSDate"
v48@0:8@16{?=qiIq}24
@40@0:8{?=qiIq}16
d40@0:8{?=qiIq}16
@"HMIVideoEventBuffer"
v40@0:8{?=qiIq}16
d16@0:8
@"HMIVideoTimeline"
@"HMITimeIntervalAverage"
v24@0:8d16
d32@0:8d16d24
@"MovingAverage"
v24@0:8@"HMFTimer"16
@32@0:8@16@24
@"<HMIHomePersonManagerDataSource>"
@"HMIHomePersonManagerSettings"
@"NSOperationQueue"
@"HMFTimer"
@"NSMutableDictionary"
@"HMIFaceCrop"
@"HMIFaceprint"
@24@0:8^{__CVBuffer=}16
@64@0:8^{__CVBuffer=}16{?=qiIq}24Q48Q56
@80@0:8@16{?=qiIq}24Q48Q56{CGSize=dd}64
@28@0:8f16^{CGSize=dd}20
^{__CVBuffer=}64@0:8^{__CVBuffer=}16{CGRect={CGPoint=dd}{CGSize=dd}}24^@56
^{__CVBuffer=}48@0:8^{__CVBuffer=}16{CGSize=dd}24^@40
^{__CVBuffer=}60@0:8^{__CVBuffer=}16{CGSize=dd}24I40q44^@52
^{__CVBuffer=}44@0:8^{__CVBuffer=}16I24q28^@36
^{__CVBuffer=}92@0:8^{__CVBuffer=}16{CGRect={CGPoint=dd}{CGSize=dd}}24{CGSize=dd}56I72q76^@84
@40@0:8^{__CVBuffer=}16f24f28^@32
^{__CVBuffer=}32@0:8^{CGDataProvider=}16^@24
^{__CVBuffer=}32@0:8@16^@24
^{__CVBuffer=}100@0:8^{__CVBuffer=}16{CGRect={CGPoint=dd}{CGSize=dd}}24{CGSize=dd}56I72q76@84^@92
^{__CVBuffer=}40@0:8{CGSize=dd}16I32B36
{CGRect={CGPoint=dd}{CGSize=dd}}80@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{CGSize=dd}48{CGSize=dd}64
{CGRect={CGPoint=dd}{CGSize=dd}}68@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{CGSize=dd}48i64
{CGRect={CGPoint=dd}{CGSize=dd}}68@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{CGSize=dd}48f64
B48@0:8@16@24@32^@40
^{__CVBuffer=}84@0:8^{__CVBuffer=}16f24{CGRect={CGPoint=dd}{CGSize=dd}}28{CGSize=dd}60^@76
@24@0:8^{__IOHIDServiceClient=}16
^{__IOHIDServiceClient=}
B28@0:8i16^d20
B24@0:8^d16
^{__IOHIDEventSystemClient=}
@32@0:8@16B24B28
B32@0:8@16B24B28
^{opaqueCMSampleBuffer=}24@0:8@16
^{opaqueCMSampleBuffer=}16@0:8
^{opaqueCMSampleBuffer=}24@0:8^Q16
@"AVAsset"
@"AVAssetReader"
^{__CFArray=}
v32@0:8@16Q24
v32@0:8@16@24
v32@0:8@"HMHomeManager"16Q24
v24@0:8@"HMHomeManager"16
v32@0:8@"HMHomeManager"16@"HMHome"24
v32@0:8@"HMHomeManager"16@"HMAddAccessoryRequest"24
@?16@0:8
@"MLMultiArray"
@40@0:8@16@24^@32
B40@0:8@16^q24^@32
@"NSError"
v24@0:8^{opaqueCMSampleBuffer=}16
@"HMIDESBackgroundTask"
@"HMPhotosPersonManager"
@32@0:8q16@24
@24@0:8^{_NSZone=}16
v24@0:8@"NSCoder"16
@24@0:8@"NSCoder"16
@80@0:8@16@24@32{CGRect={CGPoint=dd}{CGSize=dd}}40@72
@"NSUUID"
@24@0:8#16
@"HMIVideoFrame"
@"NSSet"
B32@0:8@16@24
B32@0:8@"AVAssetResourceLoader"16@"AVAssetResourceLoadingRequest"24
B32@0:8@"AVAssetResourceLoader"16@"AVAssetResourceRenewalRequest"24
v32@0:8@"AVAssetResourceLoader"16@"AVAssetResourceLoadingRequest"24
B32@0:8@"AVAssetResourceLoader"16@"NSURLAuthenticationChallenge"24
v32@0:8@"AVAssetResourceLoader"16@"NSURLAuthenticationChallenge"24
q24@0:8@16
@96@0:8q16@24@32@40{?=qiIq}48@72q80Q88
@80@0:8@16@24{?=qiIq}32@56q64Q72
q24@0:8q16
B28@0:8@16B24
f16@0:8
v20@0:8f16
@"NSDictionary"
@"HMICameraVideoFragment"
@48@0:8Q16Q24d32d40
@52@0:8Q16Q24d32d40B48
v20@0:8B16
@40@0:8q16@24@32
@"HMICameraVideoFrame"
@88@0:8@16q24@32@40{CGRect={CGPoint=dd}{CGSize=dd}}48@80
{CGRect={CGPoint=dd}{CGSize=dd}}16@0:8
{CGRect="origin"{CGPoint="x"d"y"d}"size"{CGSize="width"d"height"d}}
@40@0:8Q16@24@32
@56@0:8Q16@24@32q40@48
@48@0:8Q16@24@32q40
@40@0:8Q16@24q32
@56@0:8Q16@24q32@40@48
@"NSURL"
@40@0:8q16q24@32
v40@0:8@16@24@32
@"HMICameraVideoAnalyzerResult"
@"HMICameraVideoAnalyzer"
v32@0:8@16^{opaqueCMSampleBuffer=}24
v32@0:8@"HMIVideoEncoder"16^{opaqueCMSampleBuffer=}24
v32@0:8@"HMIVideoEncoder"16@"NSError"24
v32@0:8@"HMIVideoRetimer"16^{opaqueCMSampleBuffer=}24
v72@0:8@16@24{?=qiIq}32{CGSize=dd}56
v32@0:8@"HMICameraVideoFrameSelector"16@"HMICameraVideoFrame"24
v72@0:8@"HMICameraVideoFrameSelector"16@"NSArray"24{?=qiIq}32{CGSize=dd}56
@40@0:8@16@24d32
B32@0:8@16^@24
B32@0:8Q16Q24
@"HMICameraVideoResourceAttributes"
@"HMIVideoEncoder"
@"HMIVideoRetimer"
@"HMIVideoAssetWriter"
@"HMICameraVideoPosterFrameGenerator"
@"HMICameraVideoFrameSelector"
@"HMICameraVideoAssetReader"
v32@0:8@16q24
@"NSPointerArray"
@"HMISystemResourceUsageMonitor"
q40@0:8q16q24@32
v28@0:8@16B24
v40@0:8@16q24@32
v32@0:8q16@24
v40@0:8q16@24@32
v48@0:8q16@24@32@40
B32@0:8@16^q24
@40@0:8@16@24@32
B48@0:8@16@24^@32^@40
v48@0:8@16q24@32@40
[7i]
[3i]
@"<HMICameraVideoAnalyzerDelegate>"
@"HMIHomePersonManager"
@"HMICameraVideoAnalyzerHistory"
@"HMIVideoAnalyzer"
@"HMICameraVideoAnalyzerRequest"
@"HMICameraVideoAnalyzerScheduler"
@"HMICameraVideoAnalyzerConfiguration"
@"HMIAnalysisService"
@32@0:8r^f16Q24
v32@0:8r^f16Q24
r^f16@0:8
^f16@0:8
@20@0:8f16
@"NSMutableData"
@44@0:8@16^{__CVBuffer=}24B32^@36
@"HMIVideoAnalyzerEventFace"44@0:8@"HMIVideoAnalyzerEventFace"16^{__CVBuffer=}24B32^@36
@24@0:8^@16
@"HMIFaceprinter"
@"HMIFaceQualityFilterSVM"
B24@0:8^@16
@52@0:8@16q24B32@36^@44
v72@0:8@16{?=qiIq}24{CGSize=dd}48@64
@32@0:8@16d24
@"<HMICameraVideoFrameAnalyzer>"
@"HMICIFilterAttributeValue"
@56@0:8@16@24@32@40@48
@32@0:8Q16^@24
@76@0:8i16d20{CGRect={CGPoint=dd}{CGSize=dd}}28@60@68
i16@0:8
f80@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{CGRect={CGPoint=dd}{CGSize=dd}}48
v40@0:8@16@24d32
@72@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24{CGSize=dd}56
B32@0:8^{opaqueCMSampleBuffer=}16@24
@"<HMIVideoFrameAnalyzerDelegate>"
v40@0:8@"HMIVideoFrameAnalyzer"16@"HMIVideoFrameAnalyzerResult"24@"NSError"32
@64@0:8@16{?=qiIq}24Q48Q56
@48@0:8{?=qiIq}16Q40
@"HMICameraVideoPosterFrameGeneratorInput"
{CGRect={CGPoint=dd}{CGSize=dd}}24@0:8@16
@72@0:8@16@24@32{CGRect={CGPoint=dd}{CGSize=dd}}40
^{opaqueCMSampleBuffer=}
@"<HMIVideoRetimerDelegate>"
@48@0:8@16@24@32@40
@20@0:8i16
@28@0:8i16d20
i32@0:8@16@?24
B20@0:8i16
@32@0:8@16^@24
v20@0:8i16
@28@0:8@16f24
v32@0:8@"HMICameraVideoFrameSampler"16@"HMICameraVideoFrame"24
v40@0:8@"HMICameraVideoFrameSampler"16@"HMICameraVideoFrame"24@"HMICameraVideoFrame"32
@"<HMICameraVideoFrameSelectorDelegate>"
@"HMICameraVideoFrameSampler"
@"<HMIMotionDetector>"
@72@0:8@16{?=qiIq}24{?=qiIq}48
@"<HMICameraVideoFrameSamplerDelegate>"
q20@0:8i16
@36@0:8^{__CVBuffer=}16B24^@28
^{__CVBuffer=}48@0:8@16^{__CVBuffer=}24@32^@40
@32@0:8^{__CVBuffer=}16^@24
@"NSArray"32@0:8^{__CVBuffer=}16^@24
@"NSArray"32@0:8@"NSData"16^@24
@32@0:8{CGPoint=dd}16
{CGPoint=dd}16@0:8
{CGPoint="x"d"y"d}
B40@0:8@16@24^@32
v40@0:8@16@24@?32
v32@0:8@"NSURLSession"16@"NSError"24
v40@0:8@"NSURLSession"16@"NSURLAuthenticationChallenge"24@?<v@?q@"NSURLCredential">32
v24@0:8@"NSURLSession"16
@"HMIHomeKitClient"
@"NSURLSession"
@"HMIFeedbackSession"
@36@0:8i16@20@28
@"HMFOperation"
v44@0:8@16@24B32@?36
@56@0:8@16@24@32d40q48
@72@0:8@16@24@32@40@48d56q64
@96@0:8@16@24@32{CGRect={CGPoint=dd}{CGSize=dd}}40@72@80d88
@64@0:8@16@24{CGRect={CGPoint=dd}{CGSize=dd}}32
@"HMIPersonFaceCrop"
@28@0:8i16@20
@"<HMIPersonManagerDataSource>"
@32@0:8d16@24
@56@0:8Q16@24@32@40@48
@48@0:8q16@24@32@40
@72@0:8@16{?={?=qiIq}{?=qiIq}}24
@80@0:8@16@24{?={?=qiIq}{?=qiIq}}32
@88@0:8@16@24{?={?=qiIq}{?=qiIq}}32@80
{?={?=qiIq}{?=qiIq}}16@0:8
r^{opaqueCMFormatDescription=}16@0:8
r^{opaqueCMFormatDescription=}
{?="start"{?="value"q"timescale"i"flags"I"epoch"q}"duration"{?="value"q"timescale"i"flags"I"epoch"q}}
@32@0:8{CGSize=dd}16
@"HMIFaceTracker"
@24@0:8^{opaqueCMSampleBuffer=}16
r^{__CTFont=}24@0:8d16
^{CGContext=}
^{CGColorSpace=}
^{__CTFont=}
@"<HMISystemResourceUsageMonitorDelegate>"
@"<HMISystemResourceUsageMonitorProtocol>"
@36@0:8@16B24@28
@36@0:8@16@24B32
@"VNPersonsModel"
@32@0:8@16q24
@36@0:8@16B24q28
v24@0:8#16
v28@0:8i16@20
v32@0:8d16@24
v28@0:8B16@20
i24@0:8@16
d24@0:8@16
@32@0:8#16@24
{CGRect={CGPoint=dd}{CGSize=dd}}48@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16
B64@0:8@16@24Q32@40@48^@56
@84@0:8@16@24i32{CGRect={CGPoint=dd}{CGSize=dd}}36{CGSize=dd}68
@76@0:8@16i24{CGRect={CGPoint=dd}{CGSize=dd}}28{CGSize=dd}60
v108@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{CGRect={CGPoint=dd}{CGSize=dd}}48d80d88@96B104
v48@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16
v32@0:8{CGPoint=dd}16
{CGVector=dd}16@0:8
v32@0:8{CGVector=dd}16
{CGVector="dx"d"dy"d}
@40@0:8Q16Q24d32
@80@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24{?=qiIq}56
@64@0:8@16@24@32{?=qiIq}40
@64@0:8{?=qiIq}16@40Q48Q56
@112@0:8{?=qiIq}16@40Q48Q56d64{CGSize=dd}72{?=qiIq}88
Q24@0:8@16
B32@0:8^@16^@24
@40@0:8{?=ii}16I24B28^@32
B28@0:8^{opaqueCMSampleBuffer=}16B24
^{OpaqueVTCompressionSession=}
{?="width"i"height"i}
@"<HMIVideoEncoderDelegate>"
v24@0:8@?<v@?@"HMIHomePersonManagerSettings"@"NSError">16
v40@0:8@"NSSet"16@"NSUUID"24@?<v@?@"NSError">32
@"HMHomePersonManager"
@"NSArray"16@0:8
@40@0:8@16Q24Q32
B48@0:8@16@24d32d40
B44@0:8@16B24@28^@36
@36@0:8@16B24^@28
@"HMIPersonsModel"
@72@0:8@16Q24@32@40@48@56^@64
@"HMIDESDataset"
@32@0:8Q16@24
@"<HMIExternalPersonManagerDataSource>"
@"HMIExternalPersonManagerSettings"
@"TRIClient"
B40@0:8@16^{ImageDescriptorBufferFloat32=^^?{vector<long long, std::__1::allocator<long long> >=^q^q{__compressed_pair<long long *, std::__1::allocator<long long> >=^q}}{map<long long, int, std::__1::less<long long>, std::__1::allocator<std::__1::pair<const long long, int> > >={__tree<std::__1::__value_type<long long, int>, std::__1::__map_value_compare<long long, std::__1::__value_type<long long, int>, std::__1::less<long long>, true>, std::__1::allocator<std::__1::__value_type<long long, int> > >=^{__tree_end_node<std::__1::__tree_node_base<void *> *>}{__compressed_pair<std::__1::__tree_end_node<std::__1::__tree_node_base<void *> *>, std::__1::allocator<std::__1::__tree_node<std::__1::__value_type<long long, int>, void *> > >={__tree_end_node<std::__1::__tree_node_base<void *> *>=^{__tree_node_base<void *>}}}{__compressed_pair<unsigned long, std::__1::__map_value_compare<long long, std::__1::__value_type<long long, int>, std::__1::less<long long>, true> >=Q}}}^vQQQBQi^f}24^@32
f32@0:8@16@24
@56@0:8@16@24@32q40^@48
@24@0:8^{vector<std::__1::pair<long long, long long>, std::__1::allocator<std::__1::pair<long long, long long> > >=^{pair<long long, long long>}^{pair<long long, long long>}{__compressed_pair<std::__1::pair<long long, long long> *, std::__1::allocator<std::__1::pair<long long, long long> > >=^{pair<long long, long long>}}}16
{shared_ptr<homeai::clustering::GreedyClusterer>="__ptr_"^{GreedyClusterer}"__cntrl_"^{__shared_weak_count}}
v40@0:8@"HMICameraVideoAnalyzer"16@"HMICameraVideoFragment"24@"HMICameraVideoAnalyzerResult"32
v40@0:8@"HMICameraVideoAnalyzer"16@"HMICameraVideoFragment"24@"NSError"32
v40@0:8@"HMICameraVideoAnalyzer"16@"HMICameraVideoAnalyzerSignificantEvent"24@"HMICameraVideoFragment"32
v48@0:8@16@24@32@40
@"NSObject<OS_dispatch_group>"
@"<HMIVideoFrameSamplerDelegate>"
v32@0:8@"HMIVideoFrameSampler"16^{opaqueCMSampleBuffer=}24
@"<HMIVideoPackageAnalyzerDelegate>"
v40@0:8@"HMIVideoPackageAnalyzer"16@24@"NSError"32
f24@0:8^{__CVBuffer=}16
{vector<float, std::__1::allocator<float> >="__begin_"^f"__end_"^f"__end_cap_"{__compressed_pair<float *, std::__1::allocator<float> >="__value_"^f}}
f24@0:8@16
@64@0:8@16@24@32@40@48@56
@"HMIVideoFragment"
@"HMIVideoAnalyzerResultOutcome"
@"HMIVideoAnalyzerDynamicConfiguration"
@40@0:8@16@24f32f36
@28@0:8@16B24
B48@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16
B52@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16f48
B64@0:8@16@24@32@40@48@56
@"HMICamera"
@48@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16
B68@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16q48q56f64
i40@0:8^{__CVBuffer=}16@24@?32
i40@0:8@16@24@?32
@"NSMapTable"
@"HMIVideoAnalyzerEventFace"
@60@0:8@16B24d28Q36@44^@52
@40@0:8@16@24@?32
@40@0:8r*16@24@?32
r*16@0:8
@36@0:8i16@20^@28
@"HMIGreedyClustering"
@"<HMIFaceClassifier>"
@"HMIClusteringTaskSummary"
@40@0:8i16@20@28B36
@"HMIPerson"
@40@0:8@16{CGSize=dd}24
@64@0:8@16@24d32@40@48^@56
@64@0:8@"NSDictionary"16@"NSDictionary"24d32@"HMINMSConfiguration"40@"NSString"48^@56
v24@0:8@"HMICameraVideoFrame"16
v72@0:8@"NSArray"16{?=qiIq}24{CGSize=dd}48@"NSUUID"64
@"HMICameraVideoFrameResult"52@0:8@"HMICameraVideoFrame"16q24B32@"NSUUID"36^@44
@"NSDictionary"16@0:8
q24@0:8#16
@28@0:8q16B24
@"HMISignificantActivityDetector"
@"HMIFaceClassifierVIP"
@"HMFOSTransaction"
@"NSCache"
@64@0:8@16{CGSize=dd}24{?=qiIq}40
@48@0:8^{__CVBuffer=}16{?=qiIq}24
@40@0:8d16d24^@32
*40@0:8Q16Q24^i32
i40@0:8^v16Q24r*32
i32@0:8@16@24
@"NSObject<OS_dispatch_semaphore>"
@"<HMIVideoCommandBufferDelegate>"
v32@0:8@"HMIVideoCommandBuffer"16^{opaqueCMSampleBuffer=}24
v24@0:8@"HMIVideoCommandBuffer"16
@"HMIConfidence"
B24@0:8r^{opaqueCMFormatDescription=}16
^{OpaqueVTDecompressionSession=}
^{opaqueCMBufferQueue=}
@"<HMIVideoDecoderDelegate>"
v32@0:8@"HMIVideoDecoder"16^{opaqueCMSampleBuffer=}24
v32@0:8@"HMIVideoDecoder"16@"NSError"24
v48@0:8@16@24q32@40
v40@0:8@16@24q32
v48@0:8@"AVAssetWriter"16@"NSData"24q32@"AVAssetSegmentReport"40
v40@0:8@"AVAssetWriter"16@"NSData"24q32
@32@0:8r^{opaqueCMFormatDescription=}16r^{opaqueCMFormatDescription=}24
@"AVAssetWriter"
^{opaqueCMFormatDescription=}
@"<HMIVideoAssetWriterDelegate>"
v80@0:8@16@24{?={?=qiIq}{?=qiIq}}32
v32@0:8@"HMIVideoAssetWriter"16@"NSData"24
v80@0:8@"HMIVideoAssetWriter"16@"NSData"24{?={?=qiIq}{?=qiIq}}32
v32@0:8@"HMIVideoAssetWriter"16@"NSError"24
@48@0:8{CGPoint=dd}16{CGVector=dd}32
@72@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{CGSize=dd}48@64
f68@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16q48q56f64
v48@0:8^{__CVBuffer=}16{?=qiIq}24
@24@0:8^f16
@"NSArray"24@0:8^f16
@40@0:8^{__CVBuffer=}16^{__CVBuffer=}24^f32
@36@0:8^{opaqueCMSampleBuffer=}16f24@28
v24@0:8^{__CVBuffer=}16
v32@0:8^{opaqueCMSampleBuffer=}16^{opaqueCMSampleBuffer=}24
@"<HMIVideoFrameSelectorDelegate>"
v40@0:8@16^{opaqueCMSampleBuffer=}24@32
v40@0:8@"HMIVideoFrameSelector"16^{opaqueCMSampleBuffer=}24@"NSArray"32
@48@0:8@16@24B32B36^@40
@"<HMIVideoAnalyzerDelegate>"
@"HMIVideoAnalyzerConfiguration"
v32@0:8@"HMIVideoAnalyzer"16@"HMIVideoAnalyzerFragmentResult"24
v32@0:8@"HMIVideoAnalyzer"16@"HMIVideoAnalyzerFrameResult"24
v32@0:8@"HMIVideoAnalyzer"16@"NSError"24
v32@0:8@"HMIVideoAnalyzer"16@"NSDictionary"24
v32@0:8^{opaqueCMSampleBuffer=}16@?24
v32@0:8:16@24
@"HMIVideoCommandBuffer"
@"HMIVideoDecoder"
@"HMIVideoFrameSampler"
@"HMIVideoFrameSelector"
@"HMIVideoFrameAnalyzer"
@"HMIVideoPackageAnalyzer"
@"HMIVideoAnalyzerScheduler"
@32@0:8^{CGImage=}16^@24
B32@0:8^{CGImage=}16^@24
@208@0:8@16@24@32@40@48@56@64@72@80@88@96@104@112@120@128@136@144@152@160@168@176@184@192@200
gepj
gepj
ARGB
f024
v024
v024
333333
333333
333333
333333
