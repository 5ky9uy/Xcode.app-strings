v8@?0
wakeGestureTimestamp
TQ,N,V_wakeGestureTimestamp
dismissalTimestamp
TQ,N,V_dismissalTimestamp
delegate
T@"<CSGestureMonitorDelegate>",W,N,V_delegate
hash
TQ,R
superclass
T#,R
description
T@"NSString",R,C
debugDescription
CMWakeGestureManager
Unable to find class %s
/System/Library/PrivateFrameworks/CoreMotion.framework/CoreMotion
meta_version.json
enrollment_version.json
meta_version
trainingType
explicit
implicit
handheld
near-field
far-field
utteranceWav
productVersion
productType
triggerSource
audioInputSource
otherSourceProfileMatch
containsPayload
grainedDate
+[CSUtteranceMetadataManager saveUtteranceMetadataForUtterance:isExplicitEnrollment:isHandheldEnrollment:triggerSource:audioInput:otherBiometricResult:containsPayload:]
+[CSUtteranceMetadataManager _getBaseMetaDictionaryForUtterancePath:]
+[CSUtteranceMetadataManager _writeMetaDict:forUtterancePath:]
.wav
.json
+[CSUtteranceMetadataManager upgradeMetaFilesIfNecessaaryAtSATRoot:]
+[CSUtteranceMetadataManager _saveMetaVersionFileAtPath:]
audio
+[CSUtteranceMetadataManager _upgradeLocaleDirectoryIfNecessary:]
enrollment_completed
json
+[CSUtteranceMetadataManager _audioDirectoryNeedsUpgrade:]
+[CSUtteranceMetadataManager _upgradeUtteranceMeta:]
com.apple.siri.SiriDebug.SpeakerVoiceGradingTrigger
com.apple.siri.SiriDebug.RemoteNearMissGradingTrigger
com.apple.siri.SiriDebug
+[CSSiriDebugConnection launchSiriDebugAppWithMessage:]_block_invoke
v24@?0@"AFSiriResponse"8@"NSError"16
CSMediaPlayingMonitor queue
-[CSMediaPlayingMonitor initializeMediaPlayingState]_block_invoke
v16@?0@8
v12@?0I8
-[CSMediaPlayingMonitor _startMonitoringWithQueue:]
-[CSMediaPlayingMonitor _stopMonitoring]
-[CSMediaPlayingMonitor _notePossiblePlayPausedStateChange:]
PLAYING
NOT PLAYING
Audio/Video
Alarm
CSVolumeMonitor queue
-[CSVolumeMonitor _startMonitoringWithQueue:]
-[CSVolumeMonitor fetchVolumeFromAVSystemControllerForAudioCategory:]_block_invoke
-[CSVolumeMonitor systemControllerDied:]
-[CSVolumeMonitor startObservingSystemVolumes]
-[CSVolumeMonitor _startObservingSystemControllerLifecycle]
CSTimerMonitor queue
-[CSTimerMonitor _startMonitoringWithQueue:]
-[CSTimerMonitor _stopMonitoring]
type
TQ,N,V_type
deviceId
T@"NSString",&,N,V_deviceId
CSAlarmMonitor queue
-[CSAlarmMonitor _startMonitoringWithQueue:]
-[CSAlarmMonitor _stopMonitoring]
-[CSBiometricMatchMonitor init]
-[CSBiometricMatchMonitor getLastBiometricMatchEvent:atTime:]
-[CSBiometricMatchMonitor device:matchEventOccurred:]
biometricDevice
T@"BKDevice",&,N,V_biometricDevice
T@"<CSBiometricMatchMonitorDelegate>",W,N,V_delegate
BKDevice
/System/Library/PrivateFrameworks/BiometricKit.framework/BiometricKit
BKDeviceManager
-[CSAudioZeroCounter getZeroStatisticsFromBuffer:entireSamples:]
-[CSAudioZeroCounter stopReportZeroStatistics]
smartSiriVolume
noiseLevelChannelBitset
LKFSChannelBitset
energyBufferSize
noiseLowerPercentile
noiseUpperPercentile
LKFSLowerPercentile
LKFSUpperPercentile
noiseTimeConstant
noiseMicSensitivityOffset
LKFSTimeConstant
LKFSMicSensitivityOffset
noiseTTSMappingInputRangeLow
noiseTTSMappingInputRangeHigh
noiseTTSMappingOutputRangeLow
noiseTTSMappingOutputRangeHigh
LKFSTTSMappingInputRangeLow
LKFSTTSMappingInputRangeHigh
LKFSTTSMappingOutputRangeLow
LKFSTTSMappingOutputRangeHigh
userOffsetInputRangeLow
userOffsetInputRangeHigh
userOffsetOutputRangeLow
userOffsetOutputRangeHigh
TTSVolumeLowerLimitDB
TTSVolumeUpperLimitDB
noiseWeight
SSVNoiseLevelChannelBitset
TQ,R,N
SSVLKFSChannelBitset
SSVEnergyBufferSize
TI,R,N
SSVNoiseLowerPercentile
SSVNoiseUpperPercentile
SSVLKFSLowerPercentile
SSVLKFSUpperPercentile
SSVNoiseTimeConstant
Tf,R,N
SSVNoiseMicSensitivityOffset
SSVLKFSTimeConstant
SSVLKFSMicSensitivityOffset
SSVNoiseTTSMappingInputRangeLow
SSVNoiseTTSMappingInputRangeHigh
SSVNoiseTTSMappingOutputRangeLow
SSVNoiseTTSMappingOutputRangeHigh
SSVLKFSTTSMappingInputRangeLow
SSVLKFSTTSMappingInputRangeHigh
SSVLKFSTTSMappingOutputRangeLow
SSVLKFSTTSMappingOutputRangeHigh
SSVUserOffsetInputRangeLow
SSVUserOffsetInputRangeHigh
SSVUserOffsetOutputRangeLow
SSVUserOffsetOutputRangeHigh
SSVTTSVolumeLowerLimitDB
SSVTTSVolumeUpperLimitDB
SSVNoiseWeight
SSVParameterDirectionary
T@"NSDictionary",R,N
PCM-
OPUS_
-synced
com.apple.CoreSpeech.AudioLogging
+[CSAudioFileManager generateDeviceAudioLogging:numChannels:speechId:]_block_invoke
+[CSAudioFileManager _readDataFromFileHandle:toFileHandle:]
%@%@.wav
%@/%@%@.wav
+[CSAudioFileManager _createAudioFileWriterWithLoggingDir:inputFormat:outputFormat:]
en_US_POSIX
yyyy_MM_dd-HHmmss.SSS
^%@*
#SpkrRejected#
FinalScores
SpIdScoreThreshold
buildVersion
speakerDetected
-detected.json
-rejected.json
-[NSDictionary(SpIdMetadataLogging) logSpeakerIdMetadataAtFilepath:additionalMetadata:]
/BuildRoot/Library/Caches/com.apple.xbs/Sources/CoreSpeech_Sim/CoreSpeech-140.13.114/CoreSpeech/NSDictionary+SpIdMetadataLogging.m
<Unknown File>
Error creating uttMetaJsonData: %@
v12@?0i8
-[CSVoiceTriggerAssetDownloadMonitor _startMonitoringWithQueue:]
-[CSVoiceTriggerAssetDownloadMonitor _stopMonitoring]
-[CSVoiceTriggerAssetDownloadMonitor _didInstalledNewVoiceTriggerAsset]
com.apple.MobileAsset.VoiceTriggerAssets.new-asset-installed
com.apple.MobileAsset.VoiceTriggerAssetsWatch.new-asset-installed
com.apple.MobileAsset.VoiceTriggerAssetsMarsh.new-asset-installed
com.apple.MobileAsset.VoiceTriggerAssetsMac.new-asset-installed
+[CSUtils(Directory) removeLogFilesInDirectory:matchingPattern:beforeDays:]_block_invoke
v24@?0@"NSArray"8^@16
+[CSUtils(Directory) clearLogFilesInDirectory:matchingPattern:exceedNumber:]_block_invoke_2
+[CSUtils(Directory) clearLogFilesInDirectory:matchingPattern:exceedNumber:]_block_invoke
Unable to get %@ for file at %@: %@
q24@?0@"NSURL"8@"NSURL"16
B24@?0@"NSURL"8@"NSDictionary"16
+[CSUtils(Directory) _contentsOfDirectoryAtURL:matchingPattern:includingPropertiesForKeys:error:]
triggerStartSampleCount
noiseLevelDB
musicLevelDB
musicPlaybackVolumeDB
alarmVolume
finalTTSVolume
isMediaPlaying
isAlarmPlaying
isTimerPlaying
isLKFSProcessPaused
removeVoiceTriggerSamples
-[CSSmartSiriVolume initWithSamplingRate:asset:]
-[CSSmartSiriVolume startSmartSiriVolume]_block_invoke
RUNNING
PAUSED
v12@?0B8
-[CSSmartSiriVolume initializeMediaPlayingState]_block_invoke
playing
NOT playing
-[CSSmartSiriVolume initializeMediaPlayingState]
-[CSSmartSiriVolume initializeAlarmState]_block_invoke
firing
NOT firing
-[CSSmartSiriVolume initializeTimerState]_block_invoke
-[CSSmartSiriVolume _setAsset:]
-[CSSmartSiriVolume _processAudioChunk:soundType:]
v16@?0Q8
-[CSSmartSiriVolume estimateSoundLevelbySoundType:]_block_invoke
-[CSSmartSiriVolume _pauseSSVProcessing]_block_invoke
-[CSSmartSiriVolume _resumeSSVProcessing]_block_invoke
-[CSSmartSiriVolume voiceTriggerDidDetectKeyword:]
-[CSSmartSiriVolume voiceTriggerDidDetectKeyword:]_block_invoke
-[CSSmartSiriVolume estimatedTTSVolumeForNoiseLevelAndLKFS:LKFS:]_block_invoke
-[CSSmartSiriVolume _combineResultsWithOptimalFromNoise:andOptimalFromLkfs:withUserOffset:]
-[CSSmartSiriVolume CSMediaPlayingMonitor:didReceiveMediaPlayingChanged:]_block_invoke
-[CSSmartSiriVolume CSAlarmMonitor:didReceiveAlarmChanged:]_block_invoke
-[CSSmartSiriVolume CSTimerMonitor:didReceiveTimerChanged:]_block_invoke
-[CSSmartSiriVolume _setStartAnalyzeTime:]
T@"<CSSmartSiriVolumeDelegate>",W,N,V_delegate
allocator<T>::allocate(size_t n) 'n' exceeds maximum supported size
com.apple.assistant
Siri Global
 - %@
CSKeychainValueForAccountAndKey
CSAudioFileReader Queue
-[CSAudioFileReader initWithURL:]
-[CSAudioFileReader prepareRecording:]
-[CSAudioFileReader startRecording]
-[CSAudioFileReader _readAudioBufferAndFeed]
-[CSAudioFileReader stopRecording]
T@"<CSAudioFileReaderDelegate>",W,N,V_delegate
-[CSAudioSampleRateConverter _createSampleRateConverterWithInASBD:outASBD:]
/BuildRoot/Library/Caches/com.apple.xbs/Sources/CoreSpeech_Sim/CoreSpeech-140.13.114/CoreSpeech/CSAudioSampleRateConverter.m
Too many buffers
i32@?0^I8^{AudioBufferList=I[1{AudioBuffer=II^v}]}16^^{AudioStreamPacketDescription}24
-[CSAudioSampleRateConverter convertSampleRateOfBuffer:]
-[NSString(XPCObject) _cs_initWithXPCObject:]
sysConfigFilepath
T@"NSString",R,N,V_sysConfigFilepath
spIdType
satScoreThreshold
+[CSUtils(LanguageCode) getSiriLanguageWithFallback:]
com.apple.VoiceTriggerUI.TrainingSessionQueue
com.apple.VoiceTriggerUI.TrainingManager
-[CSVTUITrainingManager setLocaleIdentifier:]
-[CSVTUITrainingManager createKeywordDetector]
-[CSVTUITrainingManager prepareWithCompletion:]_block_invoke
-[CSVTUITrainingManager cleanupWithCompletion:]
-[CSVTUITrainingManager cleanupWithCompletion:]_block_invoke
-[CSVTUITrainingManager trainUtterance:shouldUseASR:completion:]
-[CSVTUITrainingManager trainUtterance:shouldUseASR:completion:]_block_invoke_2
-[CSVTUITrainingManager trainUtterance:shouldUseASR:completion:]_block_invoke
-[CSVTUITrainingManager cancelTrainingForID:]
-[CSVTUITrainingManager closeSessionBeforeStartWithStatus:successfully:withCompletion:]
-[CSVTUITrainingManager _shouldShowHeadsetDisconnectionMessage]
-[CSVTUITrainingManager _startAudioSession]
-[CSVTUITrainingManager setSuspendAudio:]
-[CSVTUITrainingManager setSuspendAudio:]_block_invoke
-[CSVTUITrainingManager CSVTUITrainingSessionStopListen]
-[CSVTUITrainingManager CSVTUITrainingSession:hasTrainUtterance:languageCode:payload:]
Tf,V_rms
T@"<CSVTUITrainingManagerDelegate>",W,N,V_delegate
speechRecognizerAvailable
TB,R,V_speechRecognizerAvailable
audioSource
suspendAudio
self ENDSWITH '.wav'
-[CSSpeakerModel discard]
modelPath
T@"NSString",R,N
utteranceDirectory
tiModelPath
tiUtteranceDirectory
tdtiModelPath
tdtiUtteranceDirectory
enrollmentUtterance
T@"NSArray",R,N
isValid
TB,R,N
needsRetrain
lastSpeakerIdInfo
audioSessionState
TQ,N,GgetAudioSessionState,V_audioSessionState
com.apple.corespeech.corespeechservices
com.apple.corespeech.xpc
+[CSCoreSpeechServices installedVoiceTriggerAssetForLanguageCode:completion:]_block_invoke
reason
CoreSpeechXPC service invalidated
+[CSCoreSpeechServices fetchRemoteVoiceTriggerAssetForLanguageCode:completion:]_block_invoke
+[CSCoreSpeechServices voiceTriggerRTModelForVersion:minorVersion:downloadedModels:preinstalledModels:completion:]_block_invoke
+[CSCoreSpeechServices voiceTriggerRTModelForVersion:minorVersion:downloadedModels:preinstalledModels:completion:]
+[CSCoreSpeechServices voiceTriggerJarvisLanguageList:jarvisSelectedLanguage:completion:]_block_invoke
+[CSCoreSpeechServices voiceTriggerJarvisLanguageList:jarvisSelectedLanguage:completion:]
+[CSCoreSpeechServices requestUpdatedSATAudio]_block_invoke
+[CSCoreSpeechServices getFirstPassRunningMode]_block_invoke
v16@?0q8
fileUrl
T@"NSURL",&,N,V_fileUrl
aesKey
T@"NSData",&,N,V_aesKey
readBuffer
T@"NSData",&,N,V_readBuffer
sampleByteDepth
TQ,N,V_sampleByteDepth
speechManagerStartSampleCount
speechManagerSetRecordModeToRecordingDelay
speechManagerDeviceId
speechManagerDuckOthers
triggerEndMachTime
CSSpeechManager Asset Query Queue
-[CSSpeechManager startManager]
-[CSSpeechManager startManager]_block_invoke
-[CSSpeechManager _reset]
-[CSSpeechManager _getVoiceTriggerAssetForMac:]_block_invoke
v24@?0@"CSAsset"8@"NSError"16
v16@?0@"CSAsset"8
-[CSSpeechManager registerSpeechController:]
-[CSSpeechManager _notifyEvent:]
-[CSSpeechManager _createRecorderWithContextIfNeeded:error:]
-[CSSpeechManager _prepareRecorderWithSettings:error:]
B8@?0
-[CSSpeechManager _prepareRecorderWithSettings:error:]_block_invoke
v20@?0B8@"NSError"12
-[CSSpeechManager _prepareListenWithSettings:error:]
-[CSSpeechManager prewarmAudioSession]
-[CSSpeechManager recordRoute]
-[CSSpeechManager recordDeviceInfo]
-[CSSpeechManager recordSettings]
-[CSSpeechManager setClientContext:error:]
-[CSSpeechManager setClientContext:error:]_block_invoke
Mediaserverd is recovering from crash
-[CSSpeechManager prepareRecordingForClient:error:]
-[CSSpeechManager prepareRecordingForClient:error:]_block_invoke
Cannot prepare since audio recorder was not initialized
-[CSSpeechManager _startRecordingWithSettings:error:]
-[CSSpeechManager _startListening:]
-[CSSpeechManager _startListeningWithSettings:error:]
-[CSSpeechManager _setRecordMode:error:]
-[CSSpeechManager _setRecordMode:withDelay:error:]
-[CSSpeechManager _scheduleSetRecordModeToRecordingWithDelay:forReason:validator:completion:]_block_invoke
-[CSSpeechManager _scheduleSetRecordModeToRecordingWithDelay:forReason:validator:completion:]
-[CSSpeechManager _cancelPendingSetRecordModeToRecordingForReason:]
-[CSSpeechManager _performPendingSetRecordModeToRecordingForReason:]
-[CSSpeechManager _setCurrentContext:error:]
-[CSSpeechManager _releaseClientAudioSession:]
-[CSSpeechManager _releaseAudioSessionForListening:error:]
-[CSSpeechManager _handleBluetoothDeviceTriggerEvent:settings:error:]
-[CSSpeechManager _handleVoiceTriggerSwitchAOP2APEvent:settings:error:]
-[CSSpeechManager _handleJarvisFirstPassTriggerEvent:settings:error:]
-[CSSpeechManager startRecordingAsyncWithSetting:event:completion:]
-[CSSpeechManager startRecordingWithSetting:event:error:]
-[CSSpeechManager _startRecordingWithSettings:event:error:]
-[CSSpeechManager _startRecordingWithSettings:event:error:]_block_invoke_2
-[CSSpeechManager _startRecordingWithSettings:event:error:]_block_invoke
Fail to start recording under PollingListening state.
Fail to start recording when awaiting to stop.
-[CSSpeechManager _startListeningForBluetoothDeviceVoiceTrigger:settings:error:]
deviceID is required for bluetooth voice trigger listening
-[CSSpeechManager _startRecordingForAOPFirstPassTriggerWithSettings:error:]
-[CSSpeechManager _startRecordingForClient:error:]
-[CSSpeechManager _startRecordingForClient:error:]_block_invoke
-[CSSpeechManager stopRecordingWithEvent:]_block_invoke
-[CSSpeechManager _stopRecordingWithEvent:]_block_invoke
-[CSSpeechManager audioRecorderLostMediaserverd:]
-[CSSpeechManager mediaserverdDidRestart]
-[CSSpeechManager didTransitFrom:to:by:]_block_invoke
-[CSSpeechManager didIgnoreEvent:from:]
-[CSSpeechManager _createListenPollingTimer]
-[CSSpeechManager _createListenPollingTimer]_block_invoke
-[CSSpeechManager _startListenPolling]
-[CSSpeechManager _stopListenPolling]
-[CSSpeechManager _destroyAudioRecorderIfNeeded]
-[CSSpeechManager _startForwardingToClient]
-[CSSpeechManager _stopForwardingToClient]
-[CSSpeechManager audioRecorderBufferAvailable:buffer:atTime:]_block_invoke
-[CSSpeechManager audioRecorderDidStartRecording:successfully:error:]_block_invoke
-[CSSpeechManager audioRecorderDidStopRecording:forReason:]_block_invoke
-[CSSpeechManager audioRecorderRecordHardwareConfigurationDidChange:toConfiguration:]_block_invoke
-[CSSpeechManager audioRecorderDidFinishAlertPlayback:ofType:error:]_block_invoke
-[CSSpeechManager audioRecorderBeginRecordInterruption:]_block_invoke
-[CSSpeechManager audioRecorderBeginRecordInterruption:withContext:]_block_invoke
-[CSSpeechManager audioRecorderEndRecordInterruption:]_block_invoke
-[CSSpeechManager audioRecorder:willSetAudioSessionActive:]_block_invoke
-[CSSpeechManager audioRecorder:didSetAudioSessionActive:]_block_invoke
-[CSSpeechManager audioRecorderDisconnected:]_block_invoke
-[CSSpeechManager CSLanguageCodeUpdateMonitor:didReceiveLanguageCodeChanged:]
Init
Stop
FirstPassAP
FirstPassJarvis
SecondPassAP
SecondPassAOP
RecordPendingAP
RecordPendingAOP
RecordWithVTRunningAP
Record
PollingListeningWithVTRunningAP
PollingListening
Listeninng
StoppingWithVTRunningAP
Stopping
MediaserverdRecoveringWithVTRunningAP
MediaserverdRecovering
unknown(%tu)
ClientPrepare
AudioRecorderRelease
VoiceTriggerRunning
VoiceTriggerStopped
JarvisVoiceTriggerFirstPassTriggered
JarvisVoiceTriggerFirstPassRejected
JarvisVoiceTriggerExternalTriggered
VoiceTriggerSwitchAP2AOP
VoiceTriggerSwitchAOP2AP
APFirstPassTriggered
AOPFirstPassTriggered
SecondPassTriggered
SecondPassRejected
SelfTriggerDetected
RecordPendingTimeout
ClientStartRecording
ClientStopRecording
ClientReleaseRecordSession
RecordingDidStop
ListeningSucceed
MediaserverdCrashed
MediaserverdRestarted
kDidStartFailed
RecorderDisconnected
SiriEnabled
SiriDisabled
-[CSSpeechManager getEstimatedTTSVolume]
-[CSSpeechManager CSSiriEnabledMonitor:didReceiveEnabled:]_block_invoke
-[CSSpeechManager _createClearLoggingFileTimer]
-[CSSpeechManager _createClearLoggingFileTimer]_block_invoke
-[CSSpeechManager _startClearLoggingFilesTimer]
assetQueryQueue
T@"NSObject<OS_dispatch_queue>",&,N,V_assetQueryQueue
stateMachine
T@"CSStateMachine",&,N,V_stateMachine
audioBuffer
T@"CSAudioCircularBuffer",&,N,V_audioBuffer
clientController
T@"<CSSpeechManagerDelegate>",W,N,V_clientController
secondPassStartSampleCount
TQ,N,V_secondPassStartSampleCount
lastVoiceTriggerEventInfo
T@"NSDictionary",&,N,V_lastVoiceTriggerEventInfo
T@"CSSmartSiriVolume",&,N,V_smartSiriVolume
activeAudioProcessors
T@"NSHashTable",&,N,V_activeAudioProcessors
continuousAudioProcessors
T@"NSHashTable",&,N,V_continuousAudioProcessors
lastForwardedSampleCount
TQ,N,V_lastForwardedSampleCount
clientStartSampleCount
TQ,N,V_clientStartSampleCount
listenPollingTimer
T@"NSObject<OS_dispatch_source>",&,N,V_listenPollingTimer
clearLoggingFileTimer
T@"NSObject<OS_dispatch_source>",&,N,V_clearLoggingFileTimer
listenPollingTimerCount
Tq,N,V_listenPollingTimerCount
clearLoggingFileTimerCount
Tq,N,V_clearLoggingFileTimerCount
pendingSetRecordModeToRecordingToken
T@"NSUUID",&,N,V_pendingSetRecordModeToRecordingToken
pendingSetRecordModeToRecordingCompletion
T@?,C,N,V_pendingSetRecordModeToRecordingCompletion
isSiriEnabled
TB,N,V_isSiriEnabled
deviceRoleIsStereo
TB,N,V_deviceRoleIsStereo
isAudioSessionActive
TB,N,V_isAudioSessionActive
audioSessionActivationDelay
Td,N,V_audioSessionActivationDelay
shouldChangeContextAfterDidStop
TB,N,V_shouldChangeContextAfterDidStop
pendingContext
T@"NSDictionary",&,N,V_pendingContext
audioRecorder
T@"CSAudioRecorder",&,N,V_audioRecorder
queue
T@"NSObject<OS_dispatch_queue>",&,N,V_queue
triggerEndSeconds
extraSamplesAtStart
triggerStartMachTime
satTriggered
+[CSSpIdImplicitTraining sharedInstance]
com.apple.corespeech.spid.implicittraining
-[CSSpIdImplicitTraining processImplicitTrainingUtterance:forVoiceProfileId:withRecordDeviceInfo:withRecordCtxt:withVoiceTriggerCtxt:withOtherCtxt:withCompletion:]
unknown
-[CSSpIdImplicitTraining processImplicitTrainingUtterance:forVoiceProfileId:withRecordDeviceInfo:withRecordCtxt:withVoiceTriggerCtxt:withOtherCtxt:withCompletion:]_block_invoke
ERR: Audio path is nil - Bailing out
ERR: Failed to read file: %@
v44@?0{AudioBufferList=I[1{AudioBuffer=II^v}]}8B32@"NSError"36
-[CSSpIdImplicitTraining processAudioChunk:]
-[CSSpIdImplicitTraining processImplicitTrainingUtteranceWithVoiceTriggerEventInfo:]_block_invoke
v32@?0@"NSString"8Q16^B24
-[CSSpIdImplicitTraining recordingStoppedForReason:]_block_invoke
unknownSource
unknownInput
-[CSSpIdImplicitTraining _getNumberOfWavFilesAt:]
-[CSSpIdImplicitTraining _getBiometricMatchResult]
MATCH
MIS-MATCH
-[CSSpIdImplicitTraining _isUtteranceHandheld]
lastCachedUtterance
T@"NSURL",&,N,V_lastCachedUtterance
uttLogger
T@"<CSAudioFileWriter>",&,N,V_uttLogger
recordingContext
T@"NSDictionary",&,N,V_recordingContext
voiceTriggerEventInfo
T@"NSDictionary",&,N,V_voiceTriggerEventInfo
biometricMonitor
T@"CSBiometricMatchMonitor",&,N,V_biometricMonitor
gestureMonitor
T@"CSGestureMonitor",&,N,V_gestureMonitor
Serial CSAssetManager queue
en-US
-[CSAssetManager initWithDaemonMode:]
-[CSAssetManager initWithDaemonMode:]_block_invoke
-[CSAssetManager _fetchRemoteMetaData]
-[CSAssetManager CSLanguageCodeUpdateMonitor:didReceiveLanguageCodeChanged:]
currentLanguageCode
deque
triggerEndSampleCount
isTriggerEvent
totalSampleCount
triggerScore
-[CSVTUITrainingSession closeSessionWithStatus:successfully:]
-[CSVTUITrainingSession closeSessionWithStatus:successfully:complete:]_block_invoke
-[CSVTUITrainingSession suspendTraining]
-[CSVTUITrainingSession suspendTraining]_block_invoke
-[CSVTUITrainingSession resumeTraining]
-[CSVTUITrainingSession resumeTraining]_block_invoke
-[CSVTUITrainingSession setupPhraseSpotter]
-[CSVTUITrainingSession handleAudioInput:]_block_invoke_2
v16@?0@"NSDictionary"8
-[CSVTUITrainingSession handleAudioBufferForVTWithAudioInput:withDetectedBlock:]
-[CSVTUITrainingSession feedSpeechRecognitionTrailingSamplesWithCompletedBlock:]
-[CSVTUITrainingSession trimBeginingOfPCMBufferWithVoiceTriggerEventInfo:]
-[CSVTUITrainingSession audioSessionUnsupportedAudioRoute]
-[CSVTUITrainingSession didDetectBeginOfSpeech]
-[CSVTUITrainingSession didDetectEndOfSpeech:]
PHS explicit training utterance
[%ld] VTUISession Number:[%ld]
-[CSVTUITrainingSession startMasterTimerWithTimeout:]
-[CSVTUITrainingSession handleMasterTimeout:]
-[CSVTUITrainingSession stopMasterTimer]
-[CSVTUITrainingSession speechRecognitionTask:didHypothesizeTranscription:]
activeChannel
TQ,N,V_activeChannel
T@"<CSKeywordAnalyzerNDEAPIScoreDelegate>",W,N,V_delegate
twoShotAudibleFeedbackDelay
CSSpeechRecordSettingsKey_AudioSessionActiveDelay
CSSpeechRecordSettingsKey_AudioSessionActiveReason
com.apple.corespeech.twoShotAudibleFeedback
-[CSSpeechController initializeRecordSessionWithContext:]
-[CSSpeechController prepareRecordWithSettings:error:]
-[CSSpeechController setCurrentContext:error:]
-[CSSpeechController preheat]
-[CSSpeechController prewarmAudioSession]
-[CSSpeechController resetAudioSession]
-[CSSpeechController reset]
-[CSSpeechController releaseAudioSession]
-[CSSpeechController releaseAudioSession:]
-[CSSpeechController startRecordingWithSettings:error:]
-[CSSpeechController startRecordingWithSettings:error:]_block_invoke
v16@?0@"NSError"8
-[CSSpeechController stopRecording]
-[CSSpeechController recordRoute]
-[CSSpeechController recordDeviceInfo]
-[CSSpeechController speechManagerLPCMRecordBufferAvailable:chunk:]_block_invoke
-[CSSpeechController speechManagerRecordBufferAvailable:buffer:]_block_invoke
-[CSSpeechController speechManagerDidStartForwarding:successfully:error:]
-[CSSpeechController speechManagerDidStartForwarding:successfully:error:]_block_invoke
-[CSSpeechController speechManagerDidStopForwarding:forReason:]
-[CSSpeechController speechManagerDidStopForwarding:forReason:]_block_invoke
-[CSSpeechController speechManagerRecordHardwareConfigurationDidChange:toConfiguration:]
-[CSSpeechController speechManagerRecordHardwareConfigurationDidChange:toConfiguration:]_block_invoke
-[CSSpeechController speechManagerDetectedSystemVolumeChange:withVolume:forReason:]
-[CSSpeechController speechManagerDetectedSystemVolumeChange:withVolume:forReason:]_block_invoke
-[CSSpeechController speechManagerDidFinishAlertPlayback:ofType:error:]
-[CSSpeechController speechManagerDidFinishAlertPlayback:ofType:error:]_block_invoke
-[CSSpeechController speechManagerBeginRecordInterruption:]
-[CSSpeechController speechManagerBeginRecordInterruption:]_block_invoke
-[CSSpeechController speechManagerBeginRecordInterruption:withContext:]
-[CSSpeechController speechManagerBeginRecordInterruption:withContext:]_block_invoke
-[CSSpeechController speechManagerEndRecordInterruption:]
-[CSSpeechController speechManagerEndRecordInterruption:]_block_invoke
-[CSSpeechController speechManager:willSetAudioSessionActive:]
-[CSSpeechController speechManager:willSetAudioSessionActive:]_block_invoke
-[CSSpeechController speechManager:didSetAudioSessionActive:]
-[CSSpeechController speechManager:didSetAudioSessionActive:]_block_invoke
-[CSSpeechController audioConverterDidConvertPackets:packets:durationInSec:timestamp:]
-[CSSpeechController setAlertSoundFromURL:forType:]
-[CSSpeechController playAlertSoundForType:]
-[CSSpeechController playRecordStartingAlertAndResetEndpointer]
-[CSSpeechController setMeteringEnabled:]
-[CSSpeechController outputReferenceChannel]
-[CSSpeechController voiceTriggerInfo]
-[CSSpeechController voiceTriggerDidDetectTwoShotAtTime:]_block_invoke
-[CSSpeechController keywordDetectorDidDetectKeyword]_block_invoke
Accounts
Speech Identifier
%c%c%c%c
none
-[CSSpeechController beginWaitingForMyriad]
-[CSSpeechController endWaitingForMyriadWithDecision:]
-[CSSpeechController CSMediaPlayingMonitor:didReceiveMediaPlayingChanged:]_block_invoke
-[CSSpeechController CSAlarmMonitor:didReceiveAlarmChanged:]_block_invoke
-[CSSpeechController CSTimerMonitor:didReceiveTimerChanged:]_block_invoke
-[CSSpeechController _setSoundPlayingState]
 NOT
-[CSSpeechController speakerRecognizer:hasSpeakerIdInfo:]
-[CSSpeechController speakerRecognizerFinishedProcessing:withFinalSpeakerIdInfo:]
endpointerProxy
T@"CSEndpointerProxy",&,N,V_endpointerProxy
speechManager
T@"CSSpeechManager",W,N,V_speechManager
avvcContext
T@"NSDictionary",&,N,V_avvcContext
isOpus
TB,N,V_isOpus
isActivated
TB,N,V_isActivated
isNarrowBand
TB,N,V_isNarrowBand
audioFileWriter
T@"CSPlainAudioFileWriter",&,N,V_audioFileWriter
spIdFactory
T@"CSSpeakerIdRecognizerFactory",&,N,V_spIdFactory
spIdRecognizer
T@"<CSSpIdSpeakerRecognizer>",&,N,V_spIdRecognizer
voiceTriggerImplicitTraining
T@"CSSpIdImplicitTraining",&,N,V_voiceTriggerImplicitTraining
spIdUserScores
T@"NSDictionary",&,N,V_spIdUserScores
twoShotNotificationEnabled
TB,N,V_twoShotNotificationEnabled
TB,N,V_isMediaPlaying
TB,N,V_isAlarmPlaying
TB,N,V_isTimerPlaying
isSoundPlaying
TB,N,V_isSoundPlaying
myriadPreventingTwoShotFeedback
TB,N,V_myriadPreventingTwoShotFeedback
T@"<CSSpeechControllerDelegate>",W,N,V_delegate
duckOthersOption
TB,N
endpointAnalyzer
T@"<CSEndpointAnalyzer>",R,N
-[CSAsset(RTModel) RTModelWithFallbackLanguage:]
%02x
-[CSVTUITrainingSessionWithPayload _firedVoiceTriggerTimeout]
-[CSVTUITrainingSessionWithPayload _firedEndPointTimeout]
-[CSVTUITrainingSessionWithPayload handleAudioInput:]_block_invoke
-[CSVTUITrainingSessionWithPayload audioSessionDidStartRecording:error:]
-[CSVTUITrainingSessionWithPayload audioSessionDidStopRecording:]
-[CSVTUITrainingSessionWithPayload didDetectBeginOfSpeech]
-[CSVTUITrainingSessionWithPayload didDetectEndOfSpeech:]
-[CSVTUITrainingSessionWithPayload closeSessionWithStatus:successfully:]
-[CSVTUITrainingSessionWithPayload speechRecognitionTask:didHypothesizeTranscription:]_block_invoke
-[CSVTUITrainingSessionWithPayload speechRecognitionTask:didFinishRecognition:]_block_invoke
-[CSVTUITrainingSessionWithPayload speechRecognitionTask:didFinishSuccessfully:]_block_invoke
-[CSVTUITrainingSessionWithPayload matchRecognitionResult:withMatchedBlock:withNonMatchedBlock:]
com.apple.voicetrigger
com.apple.voicetrigger.notbackedup
kCSPreferencesJarvisTriggerModeDidChangeDarwinNotification
[profileId: %@, language: %@, product: %@, version: %@]
profileId
T@"NSString",&,N,V_profileId
languageCode
T@"NSString",&,N,V_languageCode
productCategory
T@"NSString",&,N,V_productCategory
version
T@"NSNumber",&,N,V_version
VoiceTrigger Enabled
VoiceTrigger CoreSpeech Enabled
Enable Two Shot Notification
com.apple.demo-settings
StoreDemoMode
File Logging Level
Library
Logs/CrashReporter/VoiceTrigger/audio/
/Logs/CrashReporter/Assistant/
SpeechLogs
-[CSPreferences assistantAudioFileLogDirectory]
Second Pass Audio Logging Enabled
Jarvis Audio Logging Enabled
Jarvis Trigger Mode
VoiceTrigger/SAT
Caches/VoiceTrigger/SATUpdate
Caches/VoiceTrigger/SATUpdateNewerZone
Caches/VoiceTrigger/SATUpload
-[CSPreferences getUserVoiceProfileUploadPathWithEnrolledLanguageList:]
-[CSPreferences notifyUserVoiceProfileUploadComplete]
-[CSPreferences _getUserVoiceProfileUpdateDirectoryWithUpdatePath:]
-[CSPreferences notifyUserVoiceProfileUpdateReady]
-[CSPreferences notifyUserVoiceProfileUpdateReady]_block_invoke
Enable VoiceTrigger Upon VoiceProfile Sync For Language
enrollment_migrated
-[CSPreferences _markSATEnrollmentWithMarker:forLanguage:]
-[CSPreferences markSATEnrollmentSuccessForLanguageCode:]
Unknown device %@ - Bailing out
-[CSPreferences uploadUserVoiceProfile:completion:]
Cannot delete existing SATUpload Diretory : %@
Cannot create SAT Upload Directory : %@
audiocache
Failed to copy to SATUpload Diretory : %@
Failed to upload %@ with error %@
-[CSPreferences notifyUserVoiceProfileUploadComplete:]
-[CSPreferences notifyUserVoiceProfileDownloadReadyForUser:getData:completion:]
Missing downloadTriggerBlock - Bailing out
Failed to get contents of updatePath - %@
Remote VoiceTrigger Delay
Remote VoiceTrigger Endpoint Timeout
VoiceTrigger/interstitial
Myriad File Logging Enabled
-[CSPreferences enableAudioInjection:]
Audio Injection Enabled
-[CSPreferences setAudioInjectionFilePath:]
Audio Injection File Path
-[CSPreferences audioInjectionFilePath]
-[CSPreferences audioInjectionFilePath]_block_invoke
v32@?0@8Q16^B24
SpeakerId Enabled
SmartSiriVolume SoftVolume Enabled
Audio Session Activation Delay
Max Number Logging Files
Enable SiriActivation HomePod
CSSampleCountHostTimeConverter
anchorSampleCount
TQ,N,V_anchorSampleCount
anchorHostTime
TQ,N,V_anchorHostTime
initialState
Tq,N,V_initialState
transitions
T@"NSMutableDictionary",&,N,V_transitions
T@"<CSStateMachineDelegate>",W,N,V_delegate
currentState
Tq,R,N,V_currentState
-[CSPlainAudioFileWriter initWithURL:inputFormat:outputFormat:]
-[CSPlainAudioFileWriter addSamples:numSamples:]
fileURL
T@"NSURL",R,N,V_fileURL
Serial CSEventMonitor queue
/BuildRoot/Library/Caches/com.apple.xbs/Sources/CoreSpeech_Sim/CoreSpeech-140.13.114/CoreSpeech/CSEventMonitor.m
Subclasses need to overwrite this method
-[CSAudioFileLog _closeAudioFile]
-[CSAudioFileLog startRecording]_block_invoke
-input.wav
-[CSAudioFileLog appendAudioData:]_block_invoke
-[CSAudioFileLog stopRecording]_block_invoke
-[CSSpeechManager(Alert) setAlertSoundFromURL:forType:]
-[CSSpeechManager(Alert) playAlertSoundForType:]
-[CSSpeechManager(Alert) playRecordStartingAlertAndResetEndpointer]
[%@]
[%llu]
[%f]
BuiltInAOPVoiceTrigger
TestSetAPMode
TestSetAOPMode
JarvisVoiceTrigger
Unknown
UUID
T@"NSString",R,N,V_UUID
activationInfo
T@"NSDictionary",&,N,V_activationInfo
hosttime
TQ,N,V_hosttime
vadScore
Tf,N,V_vadScore
localizedDescription
com.apple.MobileAsset.VoiceTriggerAssets
com.apple.MobileAsset.VoiceTriggerAssetsWatch
com.apple.MobileAsset.VoiceTriggerAssetsMarsh
com.apple.MobileAsset.VoiceTriggerAssetsMac
com.apple.MobileAsset.SpeechEndpointAssets
Serial CSAssetController queue
-[CSAssetController assetOfType:language:]
-[CSAssetController assetOfType:language:completion:]
-[CSAssetController installedAssetOfType:language:]
-[CSAssetController installedAssetOfType:language:completion:]
v24@?0@"ASAsset"8@"NSError"16
-[CSAssetController _installedAssetOfType:withPredicate:]
-[CSAssetController _installedAssetOfType:withPredicate:completion:]_block_invoke
v24@?0@"NSArray"8@"NSError"16
-[CSAssetController _assetQueryForAssetType:withPredicate:localOnly:]
-[CSAssetController _runAssetQuery:completion:]
-[CSAssetController _runAssetQuery:completion:]_block_invoke
-[CSAssetController fetchRemoteMetaOfType:]
-[CSAssetController _fetchRemoteAssetOfType:withPredicate:localOnly:completion:]
-[CSAssetController _updateFromRemoteToLocalAssets:forAssetType:completion:]
-[CSAssetController _defaultDownloadOptions]
-[CSAssetController _downloadAsset:withComplete:]
v16@?0d8
-[CSAssetController _downloadAsset:withComplete:]_block_invoke
-[CSAssetController _startDownloadingAsset:progress:completion:]
v24@?0@"NSDictionary"8@"NSError"16
-[CSAssetController _startDownloadingAsset:progress:completion:]_block_invoke
+[CSAssetController(Utils) predicateForAssetType:language:]
(%@ == %K)
(%@ IN %K)
((%K == nil) OR (%K != %@))
 && 
VoiceProfileCompatabiltyVersion
VoiceProfileProductType
VoiceProfileSWVersion
VoiceProfileCategoryType
VoiceProfileIdentifier
Unknown InvocationStyle: %lu
tdti
Unknown CSSpIdType: %ld
+[CSUtils(SpeakerId) spIdTypeForString:]
EnrollmentRunMode
DetectionRunMode
RetrainingMode
/BuildRoot/Library/Caches/com.apple.xbs/Sources/CoreSpeech_Sim/CoreSpeech-140.13.114/CoreSpeech/CSUtils+SpeakerId.m
Unknown RunMode: %ld
config_td_spid.txt
config_ti_spid.txt
config_tdti_spid.txt
xx_XX
+[CSUtils(SpeakerId) spIdSATDirForLocale:]
spid
+[CSUtils(SpeakerId) spIdSATAudioDirForLocale:spidType:]
model
+[CSUtils(SpeakerId) spIdSATModelDirForLocale:spidType:]
spid-imported
+[CSUtils(SpeakerId) createDirectoryIfDoesNotExist:]
Logs/CoreSpeech/spid/
grading
+[CSUtils(SpeakerId) spIdAudioLogsCountLimitReached]
SiriDebugVT
SpeakerIDToGradeData
VoiceProfiles
trained_users.json
VoiceProfileCache
AudioFileOpenURL Failed : %@
EARTests
AudioFileOpenURL failed: %@
ExtAudioFileWrapAudioFileID failed: %@
Error reading audio-file: %d
EOF. Num bytes read: %lu
+[CSUtils(SpeakerId) isSpidAssetsAvailable]
+[CSUtils(SpeakerId) getVoiceProfileVersionFromVersionFilePath:]
+[CSUtils(SpeakerId) getVoiceProfileProductCategoryFromVersionFilePath:]
+[CSUtils(SpeakerId) migrateVoiceProfileToVersion:forLanguageCode:]
td-sr-model
+[CSUtils(SpeakerId) updateVoiceProfileVersionFileForLanguageCode:]
kCSDeviceCategory_Unknown
kCSDeviceCategory_iOS_NonAop
kCSDeviceCategory_iOS_Aop
kCSDeviceCategory_macOS
iPad3,4
iPad3,5
iPad3,6
iPad4,1
iPad4,2
iPad4,3
iPad4,4
iPad4,5
iPad4,6
iPad4,7
iPad4,8
iPad4,9
iPad5,1
iPad5,2
iPad5,3
iPad5,4
iPad6,7
iPad6,8
iPad6,11
iPad6,12
iPhone5,1
iPhone5,2
iPhone5,3
iPhone5,4
iPhone6,1
iPhone6,2
iPhone7,1
iPhone7,2
iPad
iPhone
+[CSUtils(SpeakerId) deviceCategoryForDeviceProductType:]
+[CSUtils(SpeakerId) isCurrentDeviceCompatibleWithNewerVoiceProfileAt:]
+[CSUtils(SpeakerId) isCurrentDeviceCompatibleWithVoiceProfileAt:]
pathExtension='json'
T@"<CSAudioDecoderDelegate>",W,V_delegate
CSAudioRouteChangeMonitor queue
-[CSAudioRouteChangeMonitor jarvisAudioRouteDidChange:]_block_invoke
-[CSAudioRouteChangeMonitor _stopMonitoring]
-[CSAudioRouteChangeMonitor _notifyJarvisConnectionState:]
CSSmartSiriVolumeEnablePolicy queue
-[CSSmartSiriVolumeEnablePolicy _addSmartSiriVolumeEnabledConditions]_block_invoke
VoiceTrigger Asset Change Monitor
T@"<CSVoiceTriggerAssetChangeDelegate>",W,N,V_delegate
com.apple.corespeech.voicetriggerassetchange
-[CSSpeechEndpointAssetMetaUpdateMonitor _startMonitoringWithQueue:]
-[CSSpeechEndpointAssetMetaUpdateMonitor _stopMonitoring]
-[CSSpeechEndpointAssetMetaUpdateMonitor _didReceiveNewSpeechEndpointAssetMetaData]
com.apple.MobileAsset.SpeechEndpointAssets.cached-metadata-updated
+[CSVTUIRegularExpressionMatcher matchWithString:TrailingStr:LeadingStr:Pattern:]
-[CSEncryptedAudioFileWriter endAudio]
writeBuffer
T@"NSMutableData",&,N,V_writeBuffer
-[CSDispatchGroup leave]
regex.json
Cannot parse to JSON
Cannot find the file
trailing_garbage
leading_garbage
regex
CSP2P_CommandType_Key
CSP2P_CommandDict_Key
corespeech
com.apple.siridebug.request.generic
com.apple.siridebug.command.remote.heysiri
com.apple.siridebug.command.parallel.recording
com.apple.siridebug.command.transfer.voiceprofile
com.apple.siridebug.command.fetch.parallelrecording
com.apple.siridebug.command.transfer.parallelrecording
com.apple.siridebug.command.fetch.voicegradingdata
com.apple.siridebug.command.transfer.voicegradingdata
com.apple.siridebug.command.delete.voiceprofile
CSP2P_RemoteHeySiriEnable_Key
CSP2P_RemoteHeySiriStatus_Key
CSP2P_RemoteRecordingStart_Key
CSP2P_RemoteRecordingStatus_Key
CSP2P_VoiceProfileData_Key
CSP2P_VoiceProfileFileName_Key
CSP2P_VoiceProfileSpeakerName_Key
CSP2P_VoiceProfileLocale_Key
CSP2P_VoiceProfileDataType_Key
CSP2P_VoiceProfileSegment_Key
CSP2P_VoiceProfileTotalSegments_Key
CSP2P_VoiceProfileStatus_Key
CSP2P_VoiceProfileProfileId_Key
CSP2P_VoiceProfileRecordedData_Key
CSP2P_VoiceProfileRemoteFileName_Key
CSP2P_VoiceDataToBeGraded_Key
CSP2P_VoiceFileNameToBeGraded_Key
CSP2P_GradingDataTransferStatus_Key
CSP2P_PeerIdentifier_Key
CSP2P_VoiceProfilePeerName_Key
CSP2P_IsDataCompressed_Key
CSP2P_UncompressedDataSize_Key
com.apple.corespeech.p2psvc
-[CSP2PService processRemoteCommandWithPayload:fromPeer:withReply:]_block_invoke
CoreSpeech
-[CSP2PService sendInfo1ToNearbyPeer]_block_invoke
-[CSP2PService sendInfo2ToNearbyPeer]_block_invoke
-[CSP2PService _processRemoteHeySiriCommandWithRequest:fromSenderID:withReply:]
-[CSP2PService _processParallelRecordingCommandWithRequest:fromSenderID:withReply:]
-[CSP2PService _compressFilesInDirectory:matchingPredicate:compressedFileAvailable:]
/BuildRoot/Library/Caches/com.apple.xbs/Sources/CoreSpeech_Sim/CoreSpeech-140.13.114/CoreSpeech/CSP2PService.m
Invalid parameter not satisfying: %@
peerId
-[CSP2PService _sendParallelRecordingsToPeerId:voiceProfileRequestInfo:withReply:]
-[CSP2PService _sendParallelRecordingsToPeerId:voiceProfileRequestInfo:withReply:]_block_invoke
v52@?0@"NSString"8@"NSData"16Q24Q32B40@"NSError"44
-almost
-[CSP2PService _sendData2ToPeerId:]_block_invoke_2
-detected.wav
-[CSP2PService _sendData1ToPeerId:]_block_invoke_2
-rejected.wav
-[CSP2PService _sendData1ToPeerId:]_block_invoke
-[CSP2PService _sendData1File:withFileName:toPeerId:withCompressedFlag:withUncompressedDataSize:withRetainFileFlag:]
fileData
fileName
-[CSP2PService _sendData1File:withFileName:toPeerId:withCompressedFlag:withUncompressedDataSize:withRetainFileFlag:]_block_invoke
-[CSP2PService _receiveParallelRecordingFromPeerId:recordingInfo:withReply:]
remote
%@_%@
Ignoring sync of existing file %@ from %@
-[CSP2PService _receiveData1FromPeerId:requestInfo:withReply:]
%@.%@
-[CSP2PService _receiveVoiceProfileFromPeerId:voiceProfileInfo:withReply:]
CoreSpeechCache
-[CSP2PService _receiveVoiceProfileFromPeerId:voiceProfileInfo:withReply:]_block_invoke
-[CSP2PService _processVoiceProfileDeleteCommandWithRequest:fromSenderID:withReply:]_block_invoke
-[CSP2PService _processDataFetchCommandWithRequest:fromSenderID:withReply:]
lastCommunicatedPeer
T@"NSString",&,N,V_lastCommunicatedPeer
adCompanionServiceProvider
T@"<CSADCompanionServiceProvider>",W,N,V_adCompanionServiceProvider
com.apple.corespeech
InternalBuild
CSSafeSetOutErrorWithNSError
PTQ+ABwag03BwO/CKvIK/A
yyyyMMdd
-[CSKeywordAnalyzerNDAPI initWithConfigPath:resourcePath:]
-[CSKeywordAnalyzerNDAPI _setStartAnalyzeTime:]
samples_at_fire
threshold_normal
-[CSKeywordAnalyzerNDAPI getThreshold]
threshold_logging
-[CSKeywordAnalyzerNDAPI getLoggingThreshold]
activePhraseId
TI,N,V_activePhraseId
T@"<CSKeywordAnalyzerNDAPIScoreDelegate>",W,N,V_delegate
Framework
Logs/CrashReporter/CoreSpeech/
Logs/CrashReporter/CoreSpeech/audio/
%@/%@%@%@
_CSGetOrCreateAudioLogDirectory
/tmp
totalAudioRecorded
Td,N,V_totalAudioRecorded
featuresAtEndpoint
T@"NSArray",&,N,V_featuresAtEndpoint
endpointerType
Tq,N,V_endpointerType
serverFeatureLatencyDistribution
T@"NSDictionary",&,N,V_serverFeatureLatencyDistribution
additionalMetrics
T@"NSDictionary",&,N,V_additionalMetrics
CSSACInfoMonitor queue
-[CSSACInfoMonitor _startMonitoringWithQueue:]
-[CSSACInfoMonitor _stopMonitoring]
-[CSSACInfoMonitor isDeviceRoleStereo]
CSRemoteControlClient
-[CSRemoteControlClient dealloc]
T@"<CSRemoteControlClientDelegate>",W,N,V_delegate
Languages
Footprint
Premium
RTModelData
RTModelHash
RTModelLocale
RTModelDigest
RTModelSignature
RTModelCertificate
RT Model for 
 from asset 
CorealisRTModel
CorealisRTModelVersion
dataSize(%d), hash(%@), locale(%@), digest(%@), cert(%@), signature(%@)
supportsSecureCoding
TB,R
modelData
T@"NSData",R,N,V_modelData
modelLocale
T@"NSString",R,N,V_modelLocale
modelHash
T@"NSString",R,N,V_modelHash
digest
T@"NSData",R,N,V_digest
signature
T@"NSData",R,N,V_signature
certificate
T@"NSData",R,N,V_certificate
com.apple.coreaudio.BorealisToggled
-[CSVoiceTriggerEnabledMonitor _startMonitoringWithQueue:]_block_invoke
-[CSVoiceTriggerEnabledMonitor _startMonitoringWithQueue:]
-[CSVoiceTriggerEnabledMonitor _stopMonitoring]
-[CSVoiceTriggerEnabledMonitor _checkVoiceTriggerEnabled]
CSRemoteRecordClient Queue
T@"<CSRemoteRecordClientDelegate>",W,N,V_delegate
Borealis Input
com.apple.VoiceTriggerUI.RemoteRecordSessionQueue
-[CSVTUIAudioSessionRemote _audioRecorder]
-[CSVTUIAudioSessionRemote prepareRecord]
T@"<CSVTUIAudioSessionDelegate>",W,N,V_delegate
-[CSSiriEnabledMonitor _startMonitoringWithQueue:]
Enabled
Disabled
-[CSSiriEnabledMonitor _stopMonitoring]
_AssistantPrefsChangedNotification
beepLocation
statsComputed
beepPower
signalPower
originalPower
absMaxVal
above95pcOfMax
totalInputSamples
totalOutputSamples
jbl_begin.bin
-[CSBeepCanceller init]
-[CSBeepCanceller willBeep]
-[CSBeepCanceller reset]
T@"<CSBeepCancellerDelegate>",W,N,V_delegate
metrics
[UniqueUttTag: %@, InvocationStyle:(%lu)%@, Asset: %@, vtEventInfo: %@]
%@_%@_%@.wav
%@_%@_%@.json
invocationStyle
TQ,N,V_invocationStyle
asset
T@"CSAsset",&,N,V_asset
vtEventInfo
T@"NSDictionary",&,N,V_vtEventInfo
uniqueUttTag
T@"NSString",&,N,V_uniqueUttTag
locale
T@"NSString",R,N,V_locale
Failed to create rootless dir at path: %@, status: %d, errno: %d, err: %s
CoreSpeechRootless
-[NSFileManager(Rootless) convertToRootlessDirectoryAtPath:error:]
Failed to convert path: %@ to rootless, status: %d, errno: %d, err: %s
CSActivationEventNotifier
-[CSActivationEventNotifier notifyActivationEvent:completion:]_block_invoke
-[CSActivationEventNotifier notifyActivationEvent:deviceId:activationInfo:completion:]_block_invoke
-[CSActivationEventNotifier setDelegate:for:]_block_invoke
-[CSActivationEventNotifier _didReceiveAOPFirstPassTrigger:completion:]
-[CSActivationEventNotifier _didReceiveAOPFirstPassTrigger:completion:]_block_invoke
-[CSActivationEventNotifier receiveTestNotificationAPMode]
-[CSActivationEventNotifier receiveTestNotificationAOPMode]
notifyToken
Ti,N,V_notifyToken
delegates
T@"NSMapTable",&,N,V_delegates
pendingActivationEvent
T@"CSActivationEvent",&,N,V_pendingActivationEvent
pendingCompletion
T@?,C,N,V_pendingCompletion
CSInitialContinousZeros
CSMaxContinousZeros
CSMidSegmentContinousZeros
start
+[CSUtils(AudioFile) readAudioChunksFrom:block:]
-[CSAssetManagerEnablePolicy _addAssetManagerEnabledConditions]_block_invoke
-[CSAudioCircularBuffer initWithNumChannels:recordingDuration:samplingRate:]
-[CSAudioCircularBuffer copySamplesFromHostTime:]
-[CSAudioCircularBuffer copySamplesFrom:to:]
-[CSAudioCircularBuffer copyBufferWithNumSamplesCopiedIn:]
-[CSAudioCircularBuffer reset]
-[CSAudioCircularBuffer saveRecordingBufferFrom:to:toURL:]
bufferLength
TQ,N,V_bufferLength
copySamples
  mNumChannels: 
  mRecordingDurationInSecs: 
  mSampleRate: 
  mBytesPerSample: 
  mBufferLengthInSamples: 
  mNextWritePos: 
  mSamplesCount: 
  mMemoryPool(
): [
    chan-
: sz=
: mem-sz: 
-[CSVoiceTriggerAssetMetaUpdateMonitor _startMonitoringWithQueue:]
-[CSVoiceTriggerAssetMetaUpdateMonitor _stopMonitoring]
-[CSVoiceTriggerAssetMetaUpdateMonitor _didReceiveNewVoiceTriggerAssetMetaData]
com.apple.MobileAsset.VoiceTriggerAssets.cached-metadata-updated
com.apple.MobileAsset.VoiceTriggerAssetsWatch.cached-metadata-updated
com.apple.MobileAsset.VoiceTriggerAssetsMarsh.cached-metadata-updated
com.apple.MobileAsset.VoiceTriggerAssetsMac.cached-metadata-updated
{wordCount: %ld, trailingSilDuration: %ld, eosLikelihood: %f, pauseCounts: %@, silencePosterior: %f, taskName: %@, processedAudioDurationInMilliseconds: %ld}
wordCount
Tq,N,V_wordCount
trailingSilenceDuration
Tq,N,V_trailingSilenceDuration
eosLikelihood
Td,N,V_eosLikelihood
pauseCounts
T@"NSArray",C,N,V_pauseCounts
silencePosterior
Td,N,V_silencePosterior
processedAudioDurationInMilliseconds
Tq,N,V_processedAudioDurationInMilliseconds
taskName
T@"NSString",C,N,V_taskName
com.apple.VoiceTriggerUI.AVVCSessionQueue
-[CSVTUIAudioSessionAVVC voiceController]
-[CSVTUIAudioSessionAVVC prepareRecord]
-[CSVTUIAudioSessionAVVC startRecording]
-[CSVTUIAudioSessionAVVC stopRecording]
-[CSVTUIAudioSessionAVVC _hasInputAudioRoute]
-[CSVTUIAudioSessionAVVC _hasCorrectInputAudioRoute]
-[CSVTUIAudioSessionAVVC _hasCorrectOutputAudioRoute]
VoiceTriggerEventInfo
-[CSVoiceTriggerSpeakerTrainer notifyImplicitTrainingUtteranceAvailable:forVoiceProfileId:withRecordDeviceInfo:withRecordCtxt:withVoiceTriggerCtxt:withOtherCtxt:withCompletion:]
com.apple.cs.%@.apQueue
-[CSVAD2EndpointAnalyzer preheat]
-[CSVAD2EndpointAnalyzer resetForNewRequestWithSampleRate:recordContext:]
-[CSVAD2EndpointAnalyzer reset]
-[CSVAD2EndpointAnalyzer _resetWithSampleRate:]
-[CSVAD2EndpointAnalyzer handleVoiceTriggerWithActivationInfo:]_block_invoke
-[CSVAD2EndpointAnalyzer processAudioSamplesAsynchronously:]
-[CSVAD2EndpointAnalyzer processAudioSamplesAsynchronously:]_block_invoke
-[CSVAD2EndpointAnalyzer _processAudioSamples:]
endpointStyle
Tq,N
delay
Td,N
startWaitTime
automaticEndpointingSuspensionEndTime
minimumDurationForEndpointer
lastEndOfVoiceActivityTime
Td,R,N
lastStartOfVoiceActivityTime
bypassSamples
endpointMode
interspeechWaitTime
endWaitTime
saveSamplesSeenInReset
T@"<CSEndpointAnalyzerDelegate>",W,N
canProcessCurrentRequest
TQ,N
endpointerModelVersion
elapsedTimeWithNoSpeech
sampleRate
Td,N,V_sampleRate
frameRate
TI,N,V_frameRate
detectedOneShotStartpoint
TB,N,V_detectedOneShotStartpoint
detectedRecurrentStartpoint
TB,N,V_detectedRecurrentStartpoint
communicatedStartPointDetection
TB,N,V_communicatedStartPointDetection
detectedOneShotEndpoint
TB,N,V_detectedOneShotEndpoint
detectedRecurrentEndpoint
TB,N,V_detectedRecurrentEndpoint
communicatedEndpointDetection
TB,N,V_communicatedEndpointDetection
samplesSeen
Td,N,V_samplesSeen
numSamplesProcessed
Td,N,V_numSamplesProcessed
lastOneShotStartpoint
Td,N,V_lastOneShotStartpoint
lastOneShotEndpoint
Td,N,V_lastOneShotEndpoint
lastRecurrentStartpoint
Td,N,V_lastRecurrentStartpoint
lastRecurrentEndpoint
Td,N,V_lastRecurrentEndpoint
floatSampleBuffer
T@"NSMutableData",&,N,V_floatSampleBuffer
topLevelParameterDict
T@"NSDictionary",&,N,V_topLevelParameterDict
modelDictPath
T@"NSString",&,N,V_modelDictPath
isConfigured
TB,N,V_isConfigured
previousSamplesSeen
Td,N,V_previousSamplesSeen
apQueue
T@"NSObject<OS_dispatch_queue>",&,N,V_apQueue
recordingDidStop
TB,N,V_recordingDidStop
vtEndInSampleCount
TQ,N,V_vtEndInSampleCount
Tq,N,V_endpointStyle
Td,N,V_delay
Td,N,V_startWaitTime
Td,N,V_automaticEndpointingSuspensionEndTime
Td,N,V_minimumDurationForEndpointer
Td,N,V_bypassSamples
Tq,N,V_endpointMode
Td,N,V_interspeechWaitTime
Td,N,V_endWaitTime
TB,N,V_saveSamplesSeenInReset
T@"<CSEndpointAnalyzerDelegate>",W,N,V_delegate
-[CSVAD2EndpointAnalyzer(private) _configureWithSampleRate:andFrameRate:]
kAUEndpointVAD2Property_EDLStartWaitTimeSec
kAUEndpointVAD2Property_EDLInterspeechWaitTimeSec
kAUEndpointVAD2Property_EDLSpeechStartAdjustSec
kAUEndpointVAD2Property_EDLSpeechEndAdjustSec
kAUEndpointVAD2Property_EDLWindowLengthSeconds
kAUEndpointVAD2Property_EDLSpeechFraction
kAUEndpointVAD2Property_EDLNonspeechFraction
kAUEndpointVAD2Property_IsRealtimeOperationMode
kAUEndpointVAD2Property_DecoderLatencySeconds
kAudioUnitProperty_MaximumFramesPerSlice
-[CSVAD2EndpointAnalyzer(private) _detectVoiceActivityInSamples:numSamples:]
Library/Audio/Tunings/Generic/AU/
EndpointerModelPathForStyle
aufx-epv2-bluetooth8khz-appl.plist
aufx-epv2-appl.plist
EndpointerSpeechBeginListener
EndpointerSpeechEndListener
RecurrentVADSpeechBeginListener
RecurrentVADSpeechEndListener
-[NSArray(XPCObject) _cs_initWithXPCObject:]
-[NSArray(XPCObject) _cs_initWithXPCObject:]_block_invoke
B24@?0Q8@"NSObject<OS_xpc_object>"16
-[NSArray(XPCObject) _cs_xpcObject]_block_invoke
-[CSAudioChunk subChunkFrom:numSamples:forChannel:]
-[CSAudioChunk subChunkFrom:numSamples:]
-[CSAudioChunk skipSamplesAtStartSuchThatNumSamplesReceivedSoFar:reachesACountOf:completionHandler:]
-[CSAudioChunk splitAudioChunkSuchThatNumSamplesReceivedSoFar:reachesACountOf:completionHandler:]
data
T@"NSData",R,N,V_data
numChannels
TQ,R,N,V_numChannels
numSamples
TQ,R,N,V_numSamples
TQ,R,N,V_sampleByteDepth
startSampleCount
TQ,R,N,V_startSampleCount
hostTime
TQ,R,N,V_hostTime
samples_fed
best_phrase
best_start
best_end
best_score
early_warning
is_rescoring
sampleFed
TQ,N,V_sampleFed
bestPhrase
TQ,N,V_bestPhrase
bestStart
TQ,N,V_bestStart
bestEnd
TQ,N,V_bestEnd
bestScore
Tf,N,V_bestScore
earlyWarning
TB,N,V_earlyWarning
isRescoring
TB,N,V_isRescoring
dictionary
SearchOrMessaging
Warmup
Median
ExtraDelayMs
EndpointerDecisionLagMs
ClientLagThresholdMsKey
ClampedSFLatencyMsForClientLag
UseDefaultServerFeaturesOnClientLag
com.apple.cs.%@.stateserialqueue
com.apple.cs.%@.sepfQueue
-[CSHybridEndpointAnalyzer init]_block_invoke_2
-[CSHybridEndpointAnalyzer init]_block_invoke
com.apple.cs.%@.hybridClassifierfQueue
com.apple.cs.%@.silencePosteriorGeneratorQueue
-[CSHybridEndpointAnalyzer processAudioSamplesAsynchronously:]
-[CSHybridEndpointAnalyzer processAudioSamplesAsynchronously:]_block_invoke
-[CSHybridEndpointAnalyzer updateEndpointerThreshold:]
-[CSHybridEndpointAnalyzer updateEndpointerDelayedTrigger:]
-[CSHybridEndpointAnalyzer processServerEndpointFeatures:]
-[CSHybridEndpointAnalyzer shouldAcceptEagerResultForDuration:resultsCompletionHandler:]_block_invoke_2
-[CSHybridEndpointAnalyzer shouldAcceptEagerResultForDuration:resultsCompletionHandler:]_block_invoke
-[CSHybridEndpointAnalyzer clientSilenceFeaturesAvailable:]
-[CSHybridEndpointAnalyzer clientSilenceFeaturesAvailable:]_block_invoke_2
-[CSHybridEndpointAnalyzer clientSilenceFeaturesAvailable:]_block_invoke
-[CSHybridEndpointAnalyzer serverFeaturesLatencyDistributionDictionary]_block_invoke
q24@?0@8@16
-[CSHybridEndpointAnalyzer handleVoiceTriggerWithActivationInfo:]_block_invoke
-[CSHybridEndpointAnalyzer recordingStoppedForReason:]
-[CSHybridEndpointAnalyzer resetForNewRequestWithSampleRate:recordContext:]
-[CSHybridEndpointAnalyzer resetForNewRequestWithSampleRate:recordContext:]_block_invoke
-[CSHybridEndpointAnalyzer _readClientLagParametersFromHEPAsset:]_block_invoke
/BuildRoot/Library/Caches/com.apple.xbs/Sources/CoreSpeech_Sim/CoreSpeech-140.13.114/CoreSpeech/CSHybridEndpointAnalyzer.m
CSHybridEndpointAnalyzer reset called
-[CSHybridEndpointAnalyzer CSLanguageCodeUpdateMonitor:didReceiveLanguageCodeChanged:]
-[CSHybridEndpointAnalyzer CSAssetManagerDidDownloadNewAsset:]
-[CSHybridEndpointAnalyzer _updateAssetWithLanguage:]_block_invoke
cs_hep_marsh.json
cs_hep.json
-[CSHybridEndpointAnalyzer _getCSHybridEndpointerConfigForAsset:]
currentAsset
T@"CSAsset",&,N,V_currentAsset
TQ,N,V_numSamplesProcessed
didAddAudio
TB,N,V_didAddAudio
caesuraSPG
T@"EARCaesuraSilencePosteriorGenerator",&,N,V_caesuraSPG
clientSilenceFeaturesAtEndpoint
T@"EARClientSilenceFeatures",&,N,V_clientSilenceFeaturesAtEndpoint
TB,N,V_canProcessCurrentRequest
hybridClassifier
T@"_EAREndpointer",&,N,V_hybridClassifier
T@"NSString",&,N,V_endpointerModelVersion
serverFeaturesQueue
T@"NSObject<OS_dispatch_queue>",&,N,V_serverFeaturesQueue
lastKnownServerEPFeatures
T@"CSServerEndpointFeatures",&,N,V_lastKnownServerEPFeatures
serverFeatureLatencies
T@"NSMutableArray",&,N,V_serverFeatureLatencies
serverFeaturesWarmupLatency
Td,N,V_serverFeaturesWarmupLatency
lastServerFeatureTimestamp
T@"NSDate",&,N,V_lastServerFeatureTimestamp
didReceiveServerFeatures
TB,N,V_didReceiveServerFeatures
clientLagThresholdMs
Td,N,V_clientLagThresholdMs
clampedSFLatencyMsForClientLag
Td,N,V_clampedSFLatencyMsForClientLag
useDefaultServerFeaturesOnClientLag
TB,N,V_useDefaultServerFeaturesOnClientLag
hybridClassifierQueue
T@"NSObject<OS_dispatch_queue>",&,N,V_hybridClassifierQueue
lastReportedEndpointTimeMs
Td,N,V_lastReportedEndpointTimeMs
stateSerialQueue
T@"NSObject<OS_dispatch_queue>",&,N,V_stateSerialQueue
didCommunicateEndpoint
TB,N,V_didCommunicateEndpoint
currentRequestSampleRate
TQ,N,V_currentRequestSampleRate
vtExtraAudioAtStartInMs
Td,N,V_vtExtraAudioAtStartInMs
hepAudioOriginInMs
Td,N,V_hepAudioOriginInMs
recordContext
T@"NSDictionary",&,N,V_recordContext
firstAudioPacketTimestamp
T@"NSDate",&,N,V_firstAudioPacketTimestamp
didTimestampFirstAudioPacket
TB,N,V_didTimestampFirstAudioPacket
silencePosteriorGeneratorQueue
T@"NSObject<OS_dispatch_queue>",&,N,V_silencePosteriorGeneratorQueue
didDetectSpeech
TB,N,V_didDetectSpeech
Td,N,V_elapsedTimeWithNoSpeech
+[CSUtils(Time) hostTimeFromSampleCount:anchorHostTime:anchorSampleCount:]
+[CSUtils(Time) sampleCountFromHostTime:anchorHostTime:anchorSampleCount:]
+[CSUtils(Time) macHostTimeFromBridgeHostTime:]
+[CSVoiceTriggerEnrollmentDataManager saveRawUtteranceAndMetadata:to:isExplicitEnrollment:]
+[CSVoiceTriggerEnrollmentDataManager saveUtteranceAndMetadata:atDirectory:isExplicitEnrollment:]
+[CSVoiceTriggerEnrollmentDataManager saveUtterance:utteranceAudioPath:numSamplesToWrite:isExplicitEnrollment:]
+[CSVoiceTriggerEnrollmentDataManager saveMetadata:isExplicitEnrollment:]
+[CSVoiceTriggerEnrollmentDataManager _getBaseMetaDictionaryForUtterancePath:]
+[CSVoiceTriggerEnrollmentDataManager writeMetaDict:atMetaPath:]
-[CSEndpointerProxy _setupVAD2Endpointer]
-[CSEndpointerProxy resetForNewRequestWithSampleRate:recordContext:]
-[CSEndpointerProxy resetForVoiceTriggerTwoShotWithSampleRate:]
-[CSEndpointerProxy endpointer:didDetectStartpointAtTime:]
-[CSEndpointerProxy endpointer:didDetectHardEndpointAtTime:withMetrics:]
-[CSEndpointerProxy _shouldEnterTwoShotAtEndPointTime:]
-[CSEndpointerProxy endpointerModelVersion]
hybridEndpointer
T@"<CSEndpointAnalyzerImpl>",&,N,V_hybridEndpointer
vad2Endpointer
T@"<CSEndpointAnalyzerImpl>",&,N,V_vad2Endpointer
activeEndpointer
T@"<CSEndpointAnalyzerImpl>",W,N,V_activeEndpointer
didEnterTwoshot
TB,N,V_didEnterTwoshot
vad2EndpointStyle
Tq,N,V_vad2EndpointStyle
vad2EndpointtMode
Tq,N,V_vad2EndpointtMode
vad2StartWaitTime
Td,N,V_vad2StartWaitTime
vad2EndWaitTime
Td,N,V_vad2EndWaitTime
vad2InterspeechWaitTime
Td,N,V_vad2InterspeechWaitTime
vad2Delay
Td,N,V_vad2Delay
vad2AutomaticEndpointingSuspensionEndTime
Td,N,V_vad2AutomaticEndpointingSuspensionEndTime
vad2MinimumDurationForEndpointer
Td,N,V_vad2MinimumDurationForEndpointer
vad2SaveSamplesSeenInReset
TB,N,V_vad2SaveSamplesSeenInReset
endpointerDelegate
T@"<CSEndpointAnalyzerDelegate>",W,N,V_endpointerDelegate
-[NSNumber(XPCObject) _cs_initWithXPCObject:]
-[NSNumber(XPCObject) _cs_xpcObject]
-[CSFirstUnlockMonitor _stopMonitoring]
userName
sysConfigRoot
satModelDir
satAudioDir
-[CSSpringboardStartMonitor _startMonitoringWithQueue:]
-[CSSpringboardStartMonitor _stopMonitoring]
-[CSSpringboardStartMonitor _checkSpringBoardStarted]
com.apple.springboard.finishedstartup
CSSPGEndpointAnalyzer
hybridendpointer.json
-[CSSPGEndpointAnalyzer reset]_block_invoke
-[CSSPGEndpointAnalyzer dealloc]
-[CSSPGEndpointAnalyzer stop]_block_invoke
-[CSSPGEndpointAnalyzer clientSilenceFeaturesAvailable:]
endpointThreshold
Tf,N,V_endpointThreshold
hasReported
TB,N,V_hasReported
T@"<CSSPGEndpointAnalyzerDelegate>",W,N,V_delegate
com.apple.transcribe.Transcriber
-[CSKeywordAnalyzerQuasar initWithConfigPath:triggerTokens:useKeywordSpotting:]
-[CSKeywordAnalyzerQuasar reset]
-[CSKeywordAnalyzerQuasar runRecognition]
-[CSKeywordAnalyzerQuasar runRecognition]_block_invoke
-[CSKeywordAnalyzerQuasar endAudio]
-[CSKeywordAnalyzerQuasar endAudio]_block_invoke
-[CSKeywordAnalyzerQuasar _recognizeWavData:length:]
-[CSKeywordAnalyzerQuasar speechRecognizer:didRecognizePartialResult:]_block_invoke
-[CSKeywordAnalyzerQuasar speechRecognizer:didFinishRecognitionWithError:]_block_invoke
-[CSKeywordAnalyzerQuasar _getConfidence:]_block_invoke
v32@?0@"_EARSpeechRecognitionToken"8Q16^B24
triggerConfidence
Td,R,N,V_triggerConfidence
T@"<CSKeywordAnalyzerQuasarScoreDelegate>",W,N,V_delegate
-[CSNetworkAvailabilityMonitor _startMonitoringWithQueue:]
-[CSNetworkAvailabilityMonitor _stopMonitoring]
-[CSNetworkAvailabilityMonitor _availabilityChanged]
%@ {route = %@, isRemoteDevice = %d, remoteDeviceUID = %@, remoteDeviceProductIdentifier = %@}
route
isRemoteDevice
remoteDeviceUID
remoteDeviceProductIdentifier
T@"NSString",R,C,N,V_route
TB,R,N,V_isRemoteDevice
T@"NSUUID",R,C,N,V_remoteDeviceUID
T@"NSString",R,C,N,V_remoteDeviceProductIdentifier
corespeech.json
hybridendpointer_marsh.json
/BuildRoot/Library/Caches/com.apple.xbs/Sources/CoreSpeech_Sim/CoreSpeech-140.13.114/CoreSpeech/CSAsset.m
ERR: Unknown assetType: %lu
/System/Library/PrivateFrameworks/CoreSpeech.framework
+[CSAsset fallBackAssetResourcePath]
-[CSAsset initWithResourcePath:configFile:configVersion:]
-[CSAsset _decodeJson:]
-[CSAsset getNumberForKey:category:default:]
-[CSAsset getStringForKey:category:default:]
configVersion:%@ resourcePath:%@ path:%@
path
T@"NSString",R,N,V_path
resourcePath
T@"NSString",R,N,V_resourcePath
hashFromResourcePath
configVersion
T@"NSString",R,N,V_configVersion
-[CSAudioConverter _convertBufferedLPCM:allowPartial:timestamp:]
-[CSAudioConverter _convertBufferedLPCM:allowPartial:timestamp:]_block_invoke
/BuildRoot/Library/Caches/com.apple.xbs/Sources/CoreSpeech_Sim/CoreSpeech-140.13.114/CoreSpeech/CSAudioConverter.m
Cannot produce ASPD for PCM
-[CSAudioConverter reset]
-[CSAudioConverter _configureAudioConverter:]
T@"<CSAudioConverterDelegate>",W,V_delegate
CreateAudioConverter
-[CSLanguageCodeUpdateMonitor _startMonitoringWithQueue:]
-[CSLanguageCodeUpdateMonitor _stopMonitoring]
-[CSLanguageCodeUpdateMonitor _didReceiveLanguageCodeUpdate]
Serial CSPolicy queue
voic
carplay
raisetospeak
auto
-[NSDictionary(XPCObject) _cs_initWithXPCObject:]
-[NSDictionary(XPCObject) _cs_initWithXPCObject:]_block_invoke
B24@?0r*8@"NSObject<OS_xpc_object>"16
-[NSDictionary(XPCObject) _cs_xpcObject]_block_invoke
v32@?0@8@16^B24
-[CSSpeakerDetectorNDAPI _initializeSAT:]
-[CSSpeakerDetectorNDAPI processSuperVector:withResult:]
T@"<CSSpeakerDetectorNDAPIDelegate>",W,N,V_delegate
isMaximized
-[CSVTUIKeywordDetector initWithLanguageCode:]
config.txt
-[CSAudioRecorder willDestroy]
-[CSAudioRecorder _destroyVoiceController]
-[CSAudioRecorder _voiceControllerWithContext:error:]
-[CSAudioRecorder _beepCanceller]
-[CSAudioRecorder prepareRecordWithSettings:error:]
-[CSAudioRecorder setCurrentContext:error:]
-[CSAudioRecorder prewarmAudioSession]
-[CSAudioRecorder releaseAudioSession:]
-[CSAudioRecorder setDuckOthersOption:]
-[CSAudioRecorder enableMiniDucking:]
Enable
Disable
-[CSAudioRecorder _resetZeroFilter]
-[CSAudioRecorder _startRecordingForAudioInjection]
-[CSAudioRecorder startRecordingWithSettings:error:]
-[CSAudioRecorder startRecording:]
context
-[CSAudioRecorder startRecording]
-[CSAudioRecorder stopRecording]
-[CSAudioRecorder setRecordMode:error:]
-[CSAudioRecorder _recordingSampleRate]
Builtin Microphone
isBluetoothConnected
Audio Recording
-[CSAudioRecorder voiceControllerRecordBufferAvailable:buffer:]
-[CSAudioRecorder playRecordStartingAlertAndResetEndpointer]
-[CSAudioRecorder playAlertSoundForType:]
ZeroFilterMetrics
-[CSAudioRecorder _audioRecorderDidStopRecordingForReason:]
BeepCancellerMetrics
-[CSAudioRecorder voiceControllerDidStartRecording:successfully:error:]
-[CSAudioRecorder voiceControllerDidStopRecording:forReason:]
-[CSAudioRecorder voiceControllerRecordHardwareConfigurationDidChange:toConfiguration:]
-[CSAudioRecorder voiceControllerDidFinishAlertPlayback:ofType:error:]
-[CSAudioRecorder voiceControllerBeginRecordInterruption:]
-[CSAudioRecorder voiceControllerBeginRecordInterruption:withContext:]
-[CSAudioRecorder voiceControllerEndRecordInterruption:]
-[CSAudioRecorder voiceControllerWillSetAudioSessionActive:willActivate:]
-[CSAudioRecorder voiceControllerDidSetAudioSessionActive:isActivated:]
-[CSAudioRecorder voiceControllerMediaServicesWereLost:]
-[CSAudioRecorder voiceControllerMediaServicesWereReset:]
-[CSAudioRecorder _deinterleaveBufferIfNeeded:]
-[CSAudioRecorder _createDeInterleaverIfNeeded]
-[CSAudioRecorder _createSampleRateConverterIfNeeded]
-[CSAudioRecorder _createAudioPowerMeterIfNeeded]
T@"<CSAudioRecorderDelegate>",W,N,V_delegate
kCCParamError
kCCBufferTooSmall
kCCMemoryFailure
kCCAlignmentError
kCCDecodeError
kCCUnimplemented
kCCOverflow
kCCRNGFailure
kCCUnspecifiedError
kCCCallSequenceError
kCCKeySizeError
Unexpected: %ld
randomBytesWithLength:%lu failed. err=%d
CSNSDataEncryptDecrypt
-[NSData(Encryption) saveEncryptedDataUsingAESKey:atFilepath:]_block_invoke
ivtag
v40@?0@"NSData"8@"NSData"16@"NSData"24@"NSError"32
+[NSData(Encryption) decryptedDataUsingAESKey:atFilepath:error:]
aesKey is nil
/BuildRoot/Library/Caches/com.apple.xbs/Sources/CoreSpeech_Sim/CoreSpeech-140.13.114/CoreSpeech/NSData+Encryption.m
AESKey(%lu) != %lu
CoreSpeechNSDataEncryption
CoreSpeechNSDataDecryption
-[NSData(XPCObject) _cs_initWithXPCObject:]
%@-%@
-[CSOSTransaction initWithDescription:]
-[CSOSTransaction dealloc]
nohash
((?:[a-z]|[0-9])*)\.asset
+[CSUtils(ResourcePathHash) assetHashInResourcePath:]
alloc
init
sharedInstance
startObserving
.cxx_destruct
wakeGestureTimestamp
setWakeGestureTimestamp:
dismissalTimestamp
setDismissalTimestamp:
delegate
setDelegate:
_wakeGestureTimestamp
_dismissalTimestamp
_delegate
isWakeGestureAvailable
sharedManager
startWakeGestureUpdates
gestureMonitorDidReceiveWakeGesture:
gestureMonitorDidReceiveSleepGesture:
isEqual:
class
self
performSelector:
performSelector:withObject:
performSelector:withObject:withObject:
isProxy
isKindOfClass:
isMemberOfClass:
conformsToProtocol:
respondsToSelector:
retain
release
autorelease
retainCount
zone
hash
superclass
description
debugDescription
wakeGestureManager:didUpdateWakeGesture:
wakeGestureManager:didUpdateWakeGesture:detectedAt:
wakeGestureManager:didUpdateWakeGesture:orientation:
_gestureManager
_getBaseMetaDictionaryForUtterancePath:
dictionaryWithDictionary:
setObject:forKeyedSubscript:
numberWithUnsignedInteger:
timeStampWithSaltGrain
numberWithBool:
_writeMetaDict:forUtterancePath:
deviceProductType
deviceProductVersion
dictionaryWithObjects:forKeys:count:
dataWithJSONObject:options:error:
localizedDescription
stringByReplacingOccurrencesOfString:withString:
writeToFile:atomically:
URLByAppendingPathComponent:
_saveMetaVersionFileAtPath:
defaultManager
arrayWithObjects:count:
contentsOfDirectoryAtURL:includingPropertiesForKeys:options:error:
countByEnumeratingWithState:objects:count:
getResourceValue:forKey:error:
boolValue
_upgradeLocaleDirectoryIfNecessary:
writeToURL:atomically:
_audioDirectoryNeedsUpgrade:
absoluteString
lastPathComponent
isEqualToString:
path
pathExtension
fileExistsAtPath:
_upgradeUtteranceMeta:
dataWithContentsOfURL:
JSONObjectWithData:options:error:
objectForKeyedSubscript:
unsignedIntegerValue
dictionary
saveUtteranceMetadataForUtterance:isExplicitEnrollment:isHandheldEnrollment:triggerSource:audioInput:otherBiometricResult:containsPayload:
saveMetaVersionFileAtSATAudioDirectory:
upgradeMetaFilesIfNecessaaryAtSATRoot:
initWithAppBundleIdentifier:
initWithTaskDeliverer:
initWithMessage:makeAppFrontmost:
handleSiriRequest:deliveryHandler:completionHandler:
launchSiriDebugAppWithMessage:
_notifyObserver:mediaIsPlayingState:
enumerateObserversInQueue:
_stopMonitoring
defaultCenter
_notePossiblePlayPausedStateChange:
addObserver:selector:name:object:
removeObserver:name:object:
userInfo
objectForKey:
notifyObserver:
CSMediaPlayingMonitor:didReceiveMediaPlayingChanged:
initializeMediaPlayingState
_startMonitoringWithQueue:
mediaPlayingState
_mediaIsPlaying
_queue
fetchVolumeFromAVSystemControllerForAudioCategory:
_startObservingSystemControllerLifecycle
startObservingSystemVolumes
removeObserver:
dealloc
musicVolume
alarmVolume
systemVolumeDidChange:
systemControllerDied:
_musicVolumeLevel
_alarmVolumeLevel
initializeTimerState
timerState
_timerFiringState
initWithRecordType:deviceId:
copy
_createAVVCContextWithType:deviceId:
avvcActivationMode:
numberWithInteger:
setObject:forKey:
contextForBuiltInVoiceTrigger
initWithAVVCContext:
avvcContext
type
setType:
deviceId
setDeviceId:
_avvcContext
_type
_deviceId
initializeAlarmState
alarmState
_alarmFiringState
assetManagerEnabledPolicy
availableDevices
firstObject
deviceWithDescriptor:error:
lastMatchEventWithError:
result
timeStamp
biometricMatchMonitorDidReceiveMatchAttempt:atTime:
device:matchEventOccurred:
getLastBiometricMatchEvent:atTime:
biometricDevice
setBiometricDevice:
_biometricDevice
zeroFilterWindowSizeInMs
shouldDeinterleaveAudioOnCS
bytes
initWithToken:sampleRate:numChannels:
getZeroStatisticsFromBuffer:entireSamples:
stopReportZeroStatistics
_methodToken
_continuousZeroCounter
_zeroCounterWinSz
_numChannels
_analyzeStep
_sampleRate
_shouldDeinterleaveAudio
getNumberForKey:category:default:
numberWithUnsignedInt:
unsignedIntValue
numberWithFloat:
floatValue
SSVNoiseLevelChannelBitset
SSVLKFSChannelBitset
SSVEnergyBufferSize
SSVNoiseLowerPercentile
SSVNoiseUpperPercentile
SSVLKFSLowerPercentile
SSVLKFSUpperPercentile
SSVNoiseTimeConstant
SSVNoiseMicSensitivityOffset
SSVLKFSTimeConstant
SSVLKFSMicSensitivityOffset
SSVNoiseTTSMappingInputRangeLow
SSVNoiseTTSMappingInputRangeHigh
SSVNoiseTTSMappingOutputRangeLow
SSVNoiseTTSMappingOutputRangeHigh
SSVLKFSTTSMappingInputRangeLow
SSVLKFSTTSMappingInputRangeHigh
SSVLKFSTTSMappingOutputRangeLow
SSVLKFSTTSMappingOutputRangeHigh
SSVUserOffsetInputRangeLow
SSVUserOffsetInputRangeHigh
SSVUserOffsetOutputRangeLow
SSVUserOffsetOutputRangeHigh
SSVTTSVolumeLowerLimitDB
SSVTTSVolumeUpperLimitDB
SSVNoiseWeight
SSVParameterDirectionary
_sharedAudioLoggingQueue
fileURL
URLByDeletingLastPathComponent
sharedPreferences
assistantAudioFileLogDirectory
containsString:
removeItemAtURL:error:
seekToEndOfFile
seekToFileOffset:
readDataOfLength:
length
writeData:
fileLoggingIsEnabled
_createAudioFileWriterWithLoggingDir:inputFormat:outputFormat:
_createTempAudioFileWriterWithInputFormat:outputFormat:
_getDateLabel
stringWithFormat:
stringByAppendingPathComponent:
fileURLWithPath:
initWithURL:inputFormat:outputFormat:
maxNumLoggingFiles
pruneNumberOfLogFilesTo:
URLWithString:
localeWithLocaleIdentifier:
setLocale:
setDateFormat:
stringFromDate:
removeLogFilesInDirectory:matchingPattern:beforeDays:
arrayWithObjects:
clearLogFilesInDirectory:matchingPattern:exceedNumber:
generateDeviceAudioLogging:numChannels:speechId:
_readDataFromFileHandle:toFileHandle:
createAudioFileWriterFromWithInputFormat:outputFormat:
removeLogFilesOlderThanNDays:
deviceBuildVersion
addEntriesFromDictionary:
stringWithUTF8String:
currentHandler
handleFailureInMethod:object:file:lineNumber:description:
logSpeakerIdMetadataAtFilepath:additionalMetadata:
_notificationKey
_didInstalledNewVoiceTriggerAsset
_notifyObserver:
enumerateObservers:
CSVoiceTriggerAssetDownloadMonitor:didInstallNewAsset:
_notifyToken
dateWithTimeIntervalSinceNow:
distantFuture
compare:
URLsInDirectory:matchingPattern:completion:
count
objectAtIndex:
_sortedURLsInDirectory:matchingPattern:completion:
_contentsOfDirectoryAtURL:matchingPattern:includingPropertiesForKeys:error:
sortedArrayUsingComparator:
regularExpressionWithPattern:options:error:
numberOfMatchesInString:options:range:
predicateWithBlock:
filteredArrayUsingPredicate:
UTF8String
_setDefaultParameters
_setAsset:
_convertDB2Mag:
getNumElementInBitset:
_reset
fetchInitSystemVolumes
_resumeSSVProcessing
_pauseSSVProcessing
setCallback:
isEnabled
addObserver:
_getMusicVolumeDB:
_resetStartAnalyzeTime
inputRecordingSampleByteDepth
convertToFloatLPCMBufFromShortLPCMBuf:
dataForChannel:
iterateBitset:block:
_prepareSoundLevelBufferFromSamples:soundType:
startSampleCount
_setStartAnalyzeTime:
numSamples
subChunkFrom:numSamples:
_processAudioChunk:soundType:
_estimatedTTSVolume:lowerLimit:upperLimit:TTSmappingInputRangeLow:TTSmappingInputRangeHigh:TTSmappingOutputRangeLow:TTSmappingOutputRangeHigh:
_combineResultsWithOptimalFromNoise:andOptimalFromLkfs:withUserOffset:
_scaleInputWithInRangeOutRange:minIn:maxIn:minOut:maxOut:
sharedAnalytics
logEventWithType:context:
smartSiriVolumeSoftVolumeEnabled
CSSmartSiriVolumeDidReceiveAlarmChanged:
CSSmartSiriVolumeDidReceiveTimerChanged:
CSSmartSiriVolumeDidReceiveMusicVolumeChanged:
CSAlarmMonitor:didReceiveAlarmChanged:
CSTimerMonitor:didReceiveTimerChanged:
speechManagerRecordBufferAvailable:buffer:
speechManagerLPCMRecordBufferAvailable:chunk:
speechManagerDidStartForwarding:successfully:error:
speechManagerDidStopForwarding:forReason:
speechManagerRecordingContext
speechManagerRecordHardwareConfigurationDidChange:toConfiguration:
speechManagerDetectedSystemVolumeChange:withVolume:forReason:
speechManagerDidFinishAlertPlayback:ofType:error:
speechManagerBeginRecordInterruption:
speechManagerBeginRecordInterruption:withContext:
speechManagerEndRecordInterruption:
speechManager:willSetAudioSessionActive:
speechManager:didSetAudioSessionActive:
voiceTriggerDidDetectKeyword:
voiceTriggerDidDetectNearMiss:
voiceTriggerDidDetectSpeakerReject:
voiceTriggerDidDetectTwoShotAtTime:
keywordDetectorDidDetectKeyword
voiceTriggerGotSuperVector:
initWithSamplingRate:asset:
startSmartSiriVolume
setAsset:
prepareSoundLevelBufferFromSamples:soundType:firedVoiceTriggerEvent:triggerStartTimeSampleOffset:triggerEndTimeSampleOffset:
estimateSoundLevelbySoundType:
reset
estimatedTTSVolumeForNoiseLevelAndLKFS:LKFS:
CSVolumeMonitor:didReceiveMusicVolumeChanged:
CSVolumeMonitor:didReceiveAlarmVolumeChanged:
.cxx_construct
_smartSiriVolumeNoiseLevel
_smartSiriVolumeLKFS
_floatBuffer
_defaults
_ssvEnablePolicy
_startAnalyzeSampleCount
_samplesFed
_processedSampleCount
_isStartSampleCountMarked
_shouldPauseSSVProcess
_shouldPauseLKFSProcess
_alarmSoundIsFiring
_timerSoundIsFiring
_currentAsset
_musicVolumeDB
_alarmVolume
_noiseLevelChannelBitset
_LKFSChannelBitset
_energyBufferSize
_noiseLowerPercentile
_noiseUpperPercentile
_LKFSLowerPercentile
_LKFSUpperPercentile
_noiseTimeConstant
_noiseMicSensitivityOffset
_LKFSTimeConstant
_LKFSMicSensitivityOffset
_noiseTTSMappingInputRangeLow
_noiseTTSMappingInputRangeHigh
_noiseTTSMappingOutputRangeLow
_noiseTTSMappingOutputRangeHigh
_LKFSTTSMappingInputRangeLow
_LKFSTTSMappingInputRangeHigh
_LKFSTTSMappingOutputRangeLow
_LKFSTTSMappingOutputRangeHigh
_userOffsetInputRangeLow
_userOffsetInputRangeHigh
_userOffsetOutputRangeLow
_userOffsetOutputRangeHigh
_TTSVolumeLowerLimitDB
_TTSVolumeUpperLimitDB
_noiseWeight
stringByAppendingFormat:
inputRecordingSampleRate
_readAudioBufferAndFeed
audioFileReaderDidStartRecording:successfully:error:
dataWithLength:
audioFileReaderBufferAvailable:buffer:atTime:
audioFileReaderDidStopRecording:forReason:
close
initWithURL:
setRecordBufferDuration:
prepareRecording:
startRecording
stopRecording
readSamplesFromChannelIdx:
_fFile
_audioFeedTimer
_bufferDuration
_outASBD
lpcmNarrowBandASBD
lpcmASBD
initWithInASBD:outASBD:
_createSampleRateConverterWithInASBD:outASBD:
mutableBytes
setLength:
upsampler
downsampler
convertSampleRateOfBuffer:
_sampleRateConverter
_outBufferScaleFactor
_inASBD
initWithUTF8String:
_cs_initWithXPCObject:
_cs_xpcObject
initWithCSSpIdType:delegate:
spIdType
satScoreThreshold
processAudioData:
endProcessing
updateModelWithBestScoreUser:
rejectUtterance
logUtteranceUnderDirectory:withScores:withWinner:
sysConfigFilepath
_sysConfigFilepath
getSiriLanguageWithFallback:
initWithLocaleIdentifier:withAudioSession:
setLocaleIdentifier:
createKeywordDetector
initWithLanguageCode:
initWithLocale:
prepareRecord
isRecording
releaseAudioSession
_stopAudioSession
destroySpeakerTrainer
_destroyAudioSession
_setupAudioSession
closeSessionBeforeStartWithStatus:successfully:withCompletion:
_createAudioAnalyzer
_shouldShowHeadsetDisconnectionMessage
_startAudioSession
createSpeechRecognizer
sharedtrainingSessionQueue
initWithUtteranceId:sessionNumber:Locale:audioSession:keywordDetector:speechRecognizer:speechRecognitionRequest:sessionDelegate:sessionDispatchQueue:completion:
addObject:
startTraining
suspendTraining
closeSessionWithStatus:successfully:complete:
_audioSource
audioSource
setEndpointStyle:
setStartWaitTime:
setEndWaitTime:
setInterspeechWaitTime:
preheat
resetForNewRequestWithSampleRate:recordContext:
hasCorrectAudioRoute
resumeTraining
VTUITrainingManagerFeedLevel:
VTUITrainingManagerStopListening
sharedTrainer
trainUtterance:languageCode:payload:
audioSessionDidStartRecording:error:
audioSessionDidStopRecording:
initWithData:numChannels:numSamples:sampleByteDepth:startSampleCount:hostTime:
processAudioSamplesAsynchronously:
audioSessionRecordBufferAvailable:
audioSessionErrorDidOccur:
audioSessionUnsupportedAudioRoute
didDetectBeginOfSpeech
didDetectEndOfSpeech:
trainingManagerWithLocaleID:
CSVTUITrainingSessionRMSAvailable:
CSVTUITrainingSessionStopListen
CSVTUITrainingSession:hasTrainUtterance:languageCode:payload:
endpointer:didDetectStartpointAtTime:
endpointer:didDetectHardEndpointAtTime:withMetrics:
prepareWithCompletion:
_beginOfSpeechDetected
_endOfSpeechDetected
cleanupWithCompletion:
trainUtterance:shouldUseASR:completion:
cancelTrainingForID:
suspendAudio
setSuspendAudio:
startRMS
stopRMS
shouldPerformRMS
didDetectForceEndPoint
setRms:
speechRecognizerAvailable
_performRMS
_locale
_audioSession
_audioAnalyzer
_keywordDetector
_trainingSessions
_currentTrainingSession
_sessionNumber
_suspendAudio
_cleanupCompletion
_speechRecognizer
_speechRecognizerAvailable
_rms
initWithLength:
convertToShortLPCMBufFromFloatLPCMBuf:
spIdSATModelDirForLocale:spidType:
createDirectoryIfDoesNotExist:
spIdSATAudioDirForLocale:spidType:
spIdSATModelDirForLocale:profileId:spidType:
utteranceDirectory
array
contentsOfDirectoryAtPath:error:
predicateWithFormat:
caseInsensitiveCompare:
sortedArrayUsingSelector:
modelPath
_isDirectoryEmpty:
removeItemAtPath:error:
initWithSpeakerModelFileName:languageCode:
tiModelPath
tdtiModelPath
tdtiUtteranceDirectory
tiUtteranceDirectory
enrollmentUtterance
needsRetrain
discard
isValid
_modelFileName
_languageCode
_modelPath
_utteranceDirectory
_tdtiModelPath
_tdtiUtteranceDirectory
_tiModelPath
_tiUtteranceDirectory
initWithContext:delegate:
processAudioChunk:
recordingStoppedForReason:
processMyriadDecision:
lastSpeakerIdInfo
getAudioSessionState
notifyAduioSessionStateChange:
setAudioSessionState:
_audioSessionState
initWithMachServiceName:options:
setRemoteObjectInterface:
initWithServiceName:
getCoreSpeechXPCConnection
errorWithDomain:code:userInfo:
setInvalidationHandler:
resume
remoteObjectProxy
installedVoiceTriggerAssetForLanguageCode:completion:
fetchRemoteVoiceTriggerAssetForLanguageCode:completion:
voiceTriggerRTModelForVersion:minorVersion:downloadedModels:preinstalledModels:completion:
voiceTriggerJarvisLanguageList:jarvisSelectedLanguage:completion:
getCoreSpeechServiceConnection
requestUpdatedSATAudio:
getFirstPassRunningMode:
requestUpdatedSATAudio
getFirstPassRunningMode
inputRecordingFramesPerPacket
inputRecordingSampleRateNarrowBand
inputRecordingBytesPerFrame
inputRecordingBytesPerPacket
inputRecordingNumberOfChannels
inputRecordingDurationInSecs
inputRecordingSampleBitDepth
EncryptionAudioSampleByteDepth
inputRecordingEncoderAudioQuality
inputRecordingSampleRateConverterAlgorithm
inputRecordingBufferDuration
audioConverterBitrate
channelForOutputReference
channelForProcessedInput
zeroFilterApproxAbsSpeechThreshold
csAudioProcessingQueuePriority
daysBeforeRemovingLogFiles
decryptedDataUsingAESKey:atFilepath:error:
initWithFileUrl:aesKey:sampleByteDepth:
readAudioChunksWithCallback:
fileUrl
setFileUrl:
aesKey
setAesKey:
readBuffer
setReadBuffer:
sampleByteDepth
setSampleByteDepth:
_fileUrl
_aesKey
_readBuffer
_sampleByteDepth
weakObjectsHashTable
_setupStateMachine
_setupCircularBuffer
_createListenPollingTimer
_createClearLoggingFileTimer
audioSessionActivationDelay
getSpeechManagerStateMachineWithType:
_notifyEvent:
supportJarvisVoiceTrigger
sharedNotifier
setDelegate:for:
mediaserverdDidRestart
_startClearLoggingFilesTimer
willDestroy
assetForCurrentLanguageOfType:completion:
supportContinuousVoiceTrigger
shouldRunVTOnCS
supportExternalTrigger
_getVoiceTriggerAssetForMac:
supportCircularBuffer
_createCircularBuffer
initWithNumChannels:recordingDuration:samplingRate:
getSpeechManagerStateMachine
currentState
_eventName:
performTransitionForEvent:
audioRecorder
initWithContext:error:
setAudioRecorder:
setCurrentContext:error:
supportSessionActivateDelay
isDeviceRoleStereo
_getClientRecordContext
isRecordContextVoiceTrigger:
isRecordingContextHDVC:
isRecordContextJarvisVoiceTrigger:
doubleValue
unsignedLongLongValue
hostTimeToTimeInterval:
_scheduleSetRecordModeToRecordingWithDelay:forReason:validator:completion:
_setRecordMode:error:
prepareRecordWithSettings:error:
prepareListenWithSettings:error:
prewarmAudioSession
recordRoute
recordDeviceInfo
recordSettings
isNarrowBand
_createRecorderWithContextIfNeeded:error:
_enableMiniDucking:
_prepareRecorderWithSettings:error:
startRecordingWithSettings:error:
startListening:
startListeningWithSettings:error:
_cancelPendingSetRecordModeToRecordingForReason:
setRecordMode:error:
_setRecordMode:withDelay:error:
releaseClientAudioSession:
_releaseAudioSessionForListening:error:
_releaseClientAudioSession:
releaseAudioSession:
enableMiniDucking:
_isBluetoothDeviceTriggerEvent:
sampleCount
_startListeningForBluetoothDeviceVoiceTrigger:settings:error:
_stateName:
_startRecordingWithSettings:event:error:
_handleAOPFirstPassTriggerEvent:settings:error:
_handleVoiceTriggerSwitchAOP2APEvent:settings:error:
_handleBluetoothDeviceTriggerEvent:settings:error:
_handleJarvisFirstPassTriggerEvent:settings:error:
_startRecordingWithSettings:error:
_startRecordingForClient:error:
notifyEvent:
jarvisVoiceTriggerRecordContext:
opusRecordSettings
_prepareListenWithSettings:error:
_startListening:
voiceTriggerRecordContext
lpcmRecordSettings
supportOpportunisticZLL
sampleCountFromHostTime:
_stopRecordingWithEvent:
handleLostServerConnection
handleServerDidRestart
_stopForwardingToClient
_startForwardingToClient
_startListenPolling
_destroyAudioRecorderIfNeeded
_stopListenPolling
removeObject:
containsObject:
addSamples:numSamples:atHostTime:
processSampleCount:hostTime:
bufferLength
copySamplesFrom:to:
_setCurrentContext:error:
_performPendingSetRecordModeToRecordingForReason:
supportSmartVolume
audioRecorderBufferAvailable:buffer:atTime:
audioRecorderBufferAvailable:buffer:
audioRecorderDidStartRecording:successfully:error:
audioRecorderDidStopRecording:forReason:
audioRecorderRecordHardwareConfigurationDidChange:toConfiguration:
audioRecorderDidFinishAlertPlayback:ofType:error:
audioRecorderBeginRecordInterruption:
audioRecorderBeginRecordInterruption:withContext:
audioRecorderEndRecordInterruption:
audioRecorder:willSetAudioSessionActive:
audioRecorder:didSetAudioSessionActive:
voiceTriggerDetectedOnAOP:
audioRecorderDisconnected:
audioRecorderLostMediaserverd:
didTransitFrom:to:by:
didIgnoreEvent:from:
CSSiriEnabledMonitor:didReceiveEnabled:
CSAudioServerCrashMonitorDidReceiveServerCrash:
CSAudioServerCrashMonitorDidReceiveServerRestart:
activationEventNotifier:event:completion:
CSAudioRouteChangeMonitor:didReceiveAudioRouteChangeEvent:
CSLanguageCodeUpdateMonitor:didReceiveLanguageCodeChanged:
initWithVoiceTriggerFirstPass:firstPassType:voicetriggerSecondPass:voicetriggerEventNotifier:audioRecorder:stateMachineType:
startManager
_getVoiceTriggerAsset:
_destroyCircularBuffer
registerSpeechController:
getCurrentState
isClientRecording
setClientContext:error:
prepareRecordingForClient:error:
_startListeningWithSettings:error:
releaseClientAudioSession
startRecordingAsyncWithSetting:event:completion:
startRecordingWithSetting:event:error:
_startRecordingForAOPFirstPassTriggerWithSettings:error:
stopRecordingWithEvent:
_startForwardingToSmartSiriVolume
_reinitializeSmartSiriVolumeWithAsset:
getEstimatedTTSVolume
queue
setQueue:
assetQueryQueue
setAssetQueryQueue:
stateMachine
setStateMachine:
audioBuffer
setAudioBuffer:
clientController
setClientController:
secondPassStartSampleCount
setSecondPassStartSampleCount:
lastVoiceTriggerEventInfo
setLastVoiceTriggerEventInfo:
smartSiriVolume
setSmartSiriVolume:
activeAudioProcessors
setActiveAudioProcessors:
continuousAudioProcessors
setContinuousAudioProcessors:
lastForwardedSampleCount
setLastForwardedSampleCount:
clientStartSampleCount
setClientStartSampleCount:
listenPollingTimer
setListenPollingTimer:
clearLoggingFileTimer
setClearLoggingFileTimer:
listenPollingTimerCount
setListenPollingTimerCount:
clearLoggingFileTimerCount
setClearLoggingFileTimerCount:
pendingSetRecordModeToRecordingToken
setPendingSetRecordModeToRecordingToken:
pendingSetRecordModeToRecordingCompletion
setPendingSetRecordModeToRecordingCompletion:
isSiriEnabled
setIsSiriEnabled:
deviceRoleIsStereo
setDeviceRoleIsStereo:
isAudioSessionActive
setIsAudioSessionActive:
setAudioSessionActivationDelay:
shouldChangeContextAfterDidStop
setShouldChangeContextAfterDidStop:
pendingContext
setPendingContext:
_isSiriEnabled
_deviceRoleIsStereo
_isAudioSessionActive
_shouldChangeContextAfterDidStop
_audioRecorder
_assetQueryQueue
_stateMachine
_audioBuffer
_clientController
_secondPassStartSampleCount
_lastVoiceTriggerEventInfo
_smartSiriVolume
_activeAudioProcessors
_continuousAudioProcessors
_lastForwardedSampleCount
_clientStartSampleCount
_listenPollingTimer
_clearLoggingFileTimer
_listenPollingTimerCount
_clearLoggingFileTimerCount
_pendingSetRecordModeToRecordingToken
_pendingSetRecordModeToRecordingCompletion
_audioSessionActivationDelay
_pendingContext
_notifyObserver:withClamshellState:
CSClamshellStateMonitor:didReceiveClamshellStateChange:
isClamshellClosed
_didReceiveClamshellStateChangeNotification:
currentLanguageCode
spIdSATAudioDirForLocale:profileId:spidType:
_getBiometricMatchResult
_isUtteranceHandheld
route
UUID
UUIDString
stringByAppendingString:
_getNumberOfWavFilesAt:
initWithFilepath:
utteranceFileASBD
endAudio
dataWithBytes:length:
addSamples:numSamples:
streamAudioFromFileUrl:audioStreamBasicDescriptor:samplesPerStreamChunk:audioDataAvailableHandler:
data
hostTime
spIdSATImplicitAudioCacheDirForLocale:profileId:
enumerateObjectsUsingBlock:
hostTimeToSeconds:
processImplicitTrainingUtterance:forVoiceProfileId:withRecordDeviceInfo:withRecordCtxt:withVoiceTriggerCtxt:withOtherCtxt:withCompletion:
setRecordingContext:
processImplicitTrainingUtteranceWithVoiceTriggerEventInfo:
lastCachedUtterance
setLastCachedUtterance:
uttLogger
setUttLogger:
recordingContext
voiceTriggerEventInfo
setVoiceTriggerEventInfo:
biometricMonitor
setBiometricMonitor:
gestureMonitor
setGestureMonitor:
_triggerEndMachTime
_triggerStartMachTime
_extraSamplesAtBegining
_numSamplesAdded
_discardedPaylodSamples
_implicitRecordingInProgress
_lastCachedUtterance
_uttLogger
_recordingContext
_voiceTriggerEventInfo
_biometricMonitor
_gestureMonitor
lowercaseString
hasPrefix:
substringFromIndex:
mutableCopy
replaceMatchesInString:options:range:withTemplate:
_stringByStrippingLeadingNoise:
_stringByStrippingTrailingNoise:
rangeOfString:options:
stringValue
_firstMatchesForRegularExpression:
firstMatchInString:options:range:
numberOfRanges
rangeAtIndex:
substringWithRange:
_stringByFixingNamePattern:
_stringByStrippingNoiseLeadingNoise:TrailingNoise:
_hasSubstring:
_matchesRegularExpression:
_caseInsensitiveHasMatchInEnumeration:
_firstMatchesForRegularExpressions:
initWithDaemonMode:
sharedController
addObserver:forAssetType:
_fetchRemoteMetaData
_canFetchRemoteAsset:
assetOfType:language:
installedAssetOfType:language:
assetOfType:language:completion:
installedAssetOfType:language:completion:
fetchRemoteMetaOfType:
supportHybridEndpointer
CSAssetManagerDidDownloadNewAsset:
CSVoiceTriggerAssetMetaUpdateMonitor:didReceiveNewVoiceTriggerAssetMetaData:
CSSpeechEndpointAssetMetaUpdateMonitor:didReceiveNewSpeechEndpointAssetMetaData:
CSAssetController:didDownloadNewAssetForType:
setDaemonRunningMode:
assetForCurrentLanguageOfType:
installedAssetForCurrentLanguageOfType:
installedAssetForCurrentLanguageOfType:completion:
removeObserver:forAssetType:
_enablePolicy
_currentLanguageCode
_observers
_daemonRunningMode
setupPhraseSpotter
startMasterTimerWithTimeout:
resultAlreadyReported
stopMasterTimer
closeSessionWithCompletion:
removeAllObjects
updateMeterAndForward
pushAudioInputIntoPCMBuffer:
requestTriggeredUtterance:
setupSpeechRecognitionTaskWithVoiceTriggerEventInfo:
trimBeginingOfPCMBufferWithVoiceTriggerEventInfo:
computeRequiredTrailingSamples
feedSpeechRecognitionWithPCMBuffer
closeSessionWithStatus:successfully:
handleAudioBufferForVTWithAudioInput:withDetectedBlock:
finishSpeechRecognitionTask
feedSpeechRecognitionTrailingSamplesWithCompletedBlock:
triggeredUtterance:
updateMeters
averagePower
analyze:
numSamplesInPCMBuffer
appendAudioPCMBuffer:
removeObjectAtIndex:
frameLength
initWithCommonFormat:sampleRate:channels:interleaved:
initWithPCMFormat:frameCapacity:
mutableAudioBufferList
setFrameLength:
replaceObjectAtIndex:withObject:
removeObjectsInRange:
createAVAudioPCMBufferWithNSData:
handleAudioInput:
_registerEndPointTimeout
sharedGrammars
getLMEforLocale:
setContextualStrings:
setTaskHint:
_setVoiceTriggerEventInfo:
recognitionTaskWithRequest:delegate:
finish
handleMasterTimeout:
scheduledTimerWithTimeInterval:target:selector:userInfo:repeats:
invalidate
formattedString
whitespaceAndNewlineCharacterSet
stringByTrimmingCharactersInSet:
speechRecognitionDidDetectSpeech:
speechRecognitionTask:didHypothesizeTranscription:
speechRecognitionTask:didFinishRecognition:
speechRecognitionTaskFinishedReadingAudio:
speechRecognitionTaskWasCancelled:
speechRecognitionTask:didFinishSuccessfully:
_status
_utteranceId
_speechRecognitionRequest
_speechRecognitionTask
_masterTimer
_pcmBufArray
_resultReported
_sessionProcess
_sessionSuspended
_ASRErrorOccured
_sessionDelegate
_trainingCompletion
_numRequiredTrailingSamples
_numTrailingSamples
initWithBlob:
activeChannel
setActiveChannel:
_currentBlob
_activeChannel
initWithManager:
startController
getFixedHighPrioritySerialQueueWithLabel:
twoShotNotificationEnabled
_currentAudioRecorderSampleRate
supportPhatic
_initializeMediaPlayingState
_initializeAlarmState
_initializeTimerState
_setSoundPlayingState
_contextToString:
_getRecordSettings
supportPacketDecoding
integerValue
_private_PacketDecodingUsed
_private_PacketEncodingUsed
_setupDownsamplerIfNeeded
_setupAudioConverter:
numberWithInt:
duckOthersOption
setDuckOthersOption:
speakerIdEnabled
_isVoiceTriggered
voiceTriggerInfo
initWithSpIdInvocationStyle:asset:locale:vtEventInfo:
_isSpeakerIdTrainingTriggered
speakerIdRecognizerWithContext:delegate:
_shouldSetStartSampleCount
_isJarvisVoiceTriggered
_isAutoPrompted
_setupSpeakerId
elapsedTimeWithNoSpeech
_canPlayPhaticDuringMediaPlayback
speechControllerDidDetectVoiceTriggerTwoShot:atTime:
shouldDelayPhaticForMyriadDecision
_shouldSchedulePhaticAtStartRecording
_scheduledPhaticDelay
speechControllerRequestsOperation:forReason:completion:
_phaticPlaybackReason
lpcmNonInterleavedASBD
lpcmInterleavedASBD
isRecordContextRaiseToSpeak:
isRecordContextAutoPrompt:
isRecordContextSpeakerIdTrainingTrigger:
isRecordingContextBTDT:
isRecordContextJarvisButtonPress:
addSamples:timestamp:
speechControllerLPCMRecordBufferAvailable:buffer:
channels
packetDescriptionCount
bytesDataSize
initWithCapacity:
packetDescriptions
initWithBytes:length:
speechControllerRecordBufferAvailable:buffers:recordedAt:
speechControllerRecordBufferAvailable:buffers:durationInSec:recordedAt:
speechControllerDidStartRecording:successfully:error:
flush
speechControllerDidStopRecording:forReason:
_deviceAudioLogging
speechControllerRecordHardwareConfigurationDidChange:toConfiguration:
speechControllerDidUpdateSmartSiriVolume:forReason:
speechControllerDidFinishAlertPlayback:ofType:error:
speechControllerBeginRecordInterruption:
speechControllerBeginRecordInterruption:withContext:
speechControllerEndRecordInterruption:
speechController:willSetAudioSessionActive:
speechController:didSetAudioSessionActive:
narrowBandOpusConverter
opusConverter
setAlertSoundFromURL:forType:
playAlertSoundForType:
alertStartTime
playRecordStartingAlertAndResetEndpointer
setMeteringEnabled:
peakPowerForChannel:
averagePowerForChannel:
passThruVoiceTriggerInfo
resetForVoiceTriggerTwoShotWithSampleRate:
endPointAnalyzerType
speechControllerRequestsOperation:forReason:
metrics
setEndpointerDelegate:
processServerEndpointFeatures:
allKeys
initWithData:encoding:
_getSpeechIdentifier
speexRecordSettings
_isJarvisButtonPress
lastEndOfVoiceActivityTime
endpointerModelVersion
updateEndpointerThreshold:
updateEndpointerDelayedTrigger:
shouldAcceptEagerResultForDuration:resultsCompletionHandler:
isSmartSiriVolumeAvailable
audioConverterDidConvertPackets:packets:durationInSec:timestamp:
speakerRecognizer:hasSpeakerIdInfo:
speakerRecognizerFinishedProcessing:withFinalSpeakerIdInfo:
initializeRecordSessionWithContext:
resetAudioSession
getLPCMAudioStreamBasicDescription
setSynchronousCallbackEnabled:
getRecordBufferDuration
startRecording:
isVoiceTriggered
PacketDecodingUsed
isJarvisVoiceTriggered
isRTSTriggered
peakPowerForOutputReference
averagePowerForOutputReference
outputReferenceChannel
endpointAnalyzer
setEndpointAnalyzerDelegate:
resetEndpointer
getSmartSiriVolume
beginWaitingForMyriad
endWaitingForMyriadWithDecision:
endpointerProxy
setEndpointerProxy:
speechManager
setSpeechManager:
setAvvcContext:
isOpus
setIsOpus:
isActivated
setIsActivated:
setIsNarrowBand:
audioFileWriter
setAudioFileWriter:
spIdFactory
setSpIdFactory:
spIdRecognizer
setSpIdRecognizer:
voiceTriggerImplicitTraining
setVoiceTriggerImplicitTraining:
spIdUserScores
setSpIdUserScores:
setTwoShotNotificationEnabled:
isMediaPlaying
setIsMediaPlaying:
isAlarmPlaying
setIsAlarmPlaying:
isTimerPlaying
setIsTimerPlaying:
isSoundPlaying
setIsSoundPlaying:
myriadPreventingTwoShotFeedback
setMyriadPreventingTwoShotFeedback:
_opusAudioConverter
_narrowBandOpusConverter
_audioConverter
_downsampler
_requestedRecordSettings
_lastVoiceTriggerInfo
_audibleFeedbackQueue
_twoShotAudibleFeedbackDecisionGroup
_isOpus
_isActivated
_isNarrowBand
_twoShotNotificationEnabled
_isMediaPlaying
_isAlarmPlaying
_isTimerPlaying
_isSoundPlaying
_myriadPreventingTwoShotFeedback
_endpointerProxy
_speechManager
_audioFileWriter
_spIdFactory
_spIdRecognizer
_voiceTriggerImplicitTraining
_spIdUserScores
stringWithCapacity:
appendFormat:
RTModelWithFallbackLanguage:
_sha1:
_sha256:
isAvailable
_firedVoiceTriggerTimeout
shouldHandleSession
shouldMatchPayload
_firedEndPointTimeout
_registerVoiceTriggerTimeout
_reportStopListening
_registerForceEndPointTimeout
matchRecognitionResult:withMatchedBlock:withNonMatchedBlock:
bestTranscription
getTrailingPatternsForUtt:Locale:
getLeadingPatternsForUtt:Locale:
getRegexPatternsForUtt:Locale:
matchWithString:TrailingStr:LeadingStr:Pattern:
_detectBOS
_ASRResultReceived
_reportedStopListening
_utteranceStored
_numSamplesFed
initWithProfileId:languageCode:productCategory:version:
profileId
setProfileId:
languageCode
setLanguageCode:
productCategory
setProductCategory:
version
setVersion:
_profileId
_productCategory
_version
_storeModeEnabled
setFileLoggingLevel:
fileLoggingLevel
intValue
baseDir
assistantLogDirectory
createDirectoryAtPath:withIntermediateDirectories:attributes:error:
CSSATBasePath
enumeratorAtURL:includingPropertiesForKeys:options:errorHandler:
fileExistsAtPath:isDirectory:
_CSSATDownloadPathForNewerZone
_getUserVoiceProfileUpdateDirectoryWithUpdatePath:
_CSSATDownloadPath
isCurrentDeviceCompatibleWithVoiceProfileAt:
spIdSATDirForLocale:profileId:spidType:
copyItemAtPath:toPath:error:
_markSATEnrollmentSuccessForLanguageCode:
_markSATEnrollmentMigratedForLanguageCode:
updateVoiceProfileVersionFileForLanguageCode:
_markSATEnrollmentWithMarker:forLanguage:
spIdSATDirForLocale:
createFileAtPath:contents:attributes:
deviceCategoryForDeviceProductType:
_CSSATUploadPath
_getEnrolledLanguageList
enumeratorAtPath:
getCurrentVoiceProfileVersionForLanguageCode:
_isDirectory:
deviceCategoryStringRepresentationForCategoryType:
getUserVoiceProfileUpdateDirectoryForNewerZone
isCurrentDeviceCompatibleWithNewerVoiceProfileAt:
interstitialRelativeDirForLevel:
voiceTriggerEnabled
voiceTriggerInCoreSpeech
setFileLoggingIsEnabled:
voiceTriggerAudioLogDirectory
secondPassAudioLoggingEnabled
jarvisAudioLoggingEnabled
setJarvisTriggerMode:
getJarvisTriggerMode
getUserVoiceProfileFileList
getUserVoiceProfileUploadPathWithEnrolledLanguageList:
notifyUserVoiceProfileUploadComplete
getUserVoiceProfileUpdateDirectory
notifyUserVoiceProfileUpdateReady
markSATEnrollmentSuccessForLanguageCode:
uploadUserVoiceProfile:completion:
notifyUserVoiceProfileUploadComplete:
notifyUserVoiceProfileDownloadReadyForUser:getData:completion:
remoteVoiceTriggerDelayTime
remoteVoiceTriggerEndpointTimeoutWithDefault:
interstitialAbsoluteDirForLevel:
myriadFileLoggingEnabled
enableAudioInjection:
audioInjectionEnabled
setAudioInjectionFilePath:
audioInjectionFilePath
useSiriActivationSPIForHomePod
hostTimeFromSampleCount:anchorHostTime:anchorSampleCount:
sampleCountFromHostTime:anchorHostTime:anchorSampleCount:
hostTimeFromSampleCount:
anchorSampleCount
setAnchorSampleCount:
anchorHostTime
setAnchorHostTime:
_anchorSampleCount
_anchorHostTime
initWithInitialState:
addTransitionFrom:to:for:
initialState
setInitialState:
transitions
setTransitions:
_currentState
_initialState
_transitions
isWriting
fFile
inASBD
outASBD
_fileURL
CSEventMonitorDidReceiveEvent:
_closeAudioFile
fileURLWithPath:isDirectory:
appendAudioData:
_audioFile
_asbd
_url
_audioLength
initWithCSspIdType:withSysConfigFile:sysConfigRoot:delegate:
initWithType:deviceId:activationInfo:vadScore:hosttime:
_activationTypeString
builtInMicVoiceTriggerEvent:hostTime:
jarvisVoiceTriggerEvent:activationInfo:hostTime:
initWithType:deviceId:activationInfo:hosttime:
activationInfo
setActivationInfo:
hosttime
setHosttime:
vadScore
setVadScore:
_vadScore
_UUID
_activationInfo
_hosttime
isScreenLocked
getVoiceTriggerAssetTypeString
getEndpointAssetTypeString
_isReadyToUse
predicateForAssetType:language:
installedAssetOfType:withPredicate:
_fetchRemoteAssetOfType:withPredicate:localOnly:completion:
installedAssetOfType:withPredicate:completion:
_installedAssetOfType:withPredicate:
getCSAssetOfType:
_installedAssetOfType:withPredicate:completion:
_assetQueryForAssetType:withPredicate:localOnly:
runQueryAndReturnError:
predicate
_findLatestInstalledAsset:
stopQuery
startQuery:
state
isLatestCompareTo:
initWithAssetType:
setPredicate:
setQueriesLocalAssetInformationOnly:
isSpringboardStarted
isFirstUnlocked
predicateForfetchRemoteMetadataForAssetType:
_runAssetQuery:completion:
_updateFromRemoteToLocalAssets:forAssetType:completion:
isInstalled
isDownloading
cancelDownloadAndReturnError:
purgeAndReturnError:
_downloadAsset:withComplete:
_startDownloadingAsset:progress:completion:
setProgressHandler:
requiredDiskSpaceIsAvailable:error:
_defaultDownloadOptions
beginDownloadWithOptions:
resumeDownload:
adjustDownloadOptions:completion:
_csAssetsDictionary
getVoiceTriggerAssetCurrentCompatibilityVersion
getEndpointAssetCurrentCompatibilityVersion
supportPremiumAssets
componentsJoinedByString:
predicateWithFormat:argumentArray:
spIdSATDirForLocale:profileId:
stringForCSSpIdType:
URLsForDirectory:inDomains:
lastObject
spIdAudioLogsDir
spIdAudioLogsDir2
spIdSiriDebugVTDataDirectory
spIdSiriDebugVoiceProfileStoreRootDirectory
spIdSiriDebugVoiceProfileStoreRootDirectoryForLocale:
dataWithCapacity:
satConfigFileNameForCSSpIdType:
resourcePath
getProfileVersionFilePathForLanguageCode:
getVoiceProfileVersionFromVersionFilePath:
dataWithContentsOfFile:
getVoiceProfileProductCategoryFromVersionFilePath:
moveItemAtPath:toPath:error:
setValue:forKey:
writeToFile:options:error:
setWithObjects:
stringForInvocationStyle:
spIdTypeForString:
stringForCSSATRunMode:
spIdVoiceProfileImportRootDir
spIdAudioLogsCountLimitReached
spIdDataRootDirectory
spIdSiriDebugTrainedUsersFilePathForLocale:
spIdSiriDebugVoiceProfileRootDirectoryForProfile:locale:
spidAudioTrainUtterancesDir
isSpidAssetsAvailable
getCurrentVoiceProfileProductCategoryForLanguageCode:
checkIfMigrationNecessaryForCompatibilityVersion:forLanguageCode:
migrateVoiceProfileToVersion:forLanguageCode:
opusASBD
lpcmInt16ASBD
objectAtIndexedSubscript:
audioDecoderDidDecodePackets:buffer:timestamp:
opusDecoder
addPackets:timestamp:
_decoder
_fetchJarvisConnectionState
_notifyJarvisConnectionState:
getJarvisConnected:
jarvisConnected
preferredExternalRouteDidChange:
jarvisAudioRouteDidChange:
_isJarvisConnected
_addSmartSiriVolumeEnabledConditions
_subscribeEventMonitors
subscribeEventMonitor:
addConditions:
hasRemoteBuiltInMic
assetChangeMonitorDidDetectAssetChange:
sharedMonitor
startMonitoring
notifyVoiceTriggerAssetChanged
_didReceiveNewSpeechEndpointAssetMetaData
alertMuteBehaviorDict
alertMuteSettings
appendBytes:length:
generateIfNecessaryVoiceTriggerProfilesAESKey
saveEncryptedDataUsingAESKey:atFilepath:
initWithFileUrl:sampleByteDepth:
writeBuffer
setWriteBuffer:
_writeBuffer
enter
leave
waitWithTimeout:
_dispatchGroup
_dispatchGroupCounter
createGrammars
bundleForClass:
bundlePath
_getTrailingPatternsWithGrammars:withLocale:
_getLeadingPatternsWithGrammars:withLocale:
_getRegexPatternsWithGrammars:withUtt:withLocale:
_getLMEWithGrammar:withLocale:
URLSession:didBecomeInvalidWithError:
URLSession:didReceiveChallenge:completionHandler:
URLSessionDidFinishEventsForBackgroundURLSession:
_grammar
_processRemoteHeySiriCommandWithRequest:fromSenderID:withReply:
_processParallelRecordingCommandWithRequest:fromSenderID:withReply:
_sendParallelRecordingsToPeerId:voiceProfileRequestInfo:withReply:
_receiveParallelRecordingFromPeerId:recordingInfo:withReply:
_receiveVoiceProfileFromPeerId:voiceProfileInfo:withReply:
_processDataFetchCommandWithRequest:fromSenderID:withReply:
_processVoiceProfileDeleteCommandWithRequest:fromSenderID:withReply:
_receiveData1FromPeerId:requestInfo:withReply:
_sendData1ToPeerId:
_sendData2ToPeerId:
numberWithUnsignedLong:
sendMessageWithPayload:toPeer:withReply:
_compressFilesInDirectory:matchingPredicate:compressedFileAvailable:
_sendData1File:withFileName:toPeerId:withCompressedFlag:withUncompressedDataSize:withRetainFileFlag:
URLByDeletingPathExtension
stringByDeletingPathExtension
stringByAppendingPathExtension:
temporaryDirectory
importSpIdProfileFromDir:forLocale:completion:
deleteSpeakerIdProfileWithIdentifier:forLocale:completion:
processRemoteCommandWithPayload:fromPeer:withReply:
sendInfo1ToNearbyPeer
sendInfo2ToNearbyPeer
adCompanionServiceProvider
setAdCompanionServiceProvider:
lastCommunicatedPeer
setLastCommunicatedPeer:
_adCompanionServiceProvider
_lastCommunicatedPeer
supportRaiseToSpeak
supportTTS
rootQueueWithFixedPriority:
supportKeywordDetector
supportSelfTriggerSuppression
supportCSTwoShotDecision
supportImplicitTraining
supportSAT
supportPremiumModel
hasRemoteCoreSpeech
getFixedPrioritySerialQueueWithLabel:fixedPriority:
systemUpTime
deviceUserAssignedName
initWithConfigPath:resourcePath:
resetBest
analyzeWavData:numSamples:
getAnalyzedResultForPhraseId:
sampleFed
getAnalyzedResult
keywordAnalyzerNDAPI:hasResultAvailable:forChannel:
initWithResult:
bestStart
setBestStart:
bestEnd
setBestEnd:
getSuperVectorWithEndPoint:
getOptionValue:
getThreshold
getLoggingThreshold
activePhraseId
setActivePhraseId:
_novDetector
_lastSampleFed
_sampleFedWrapAroundOffset
_activePhraseId
date
initWithTotalAudioRecorded:featuresAtEndpoint:endpointerType:serverFeatureLatencyDistribution:additionalMetrics:
totalAudioRecorded
setTotalAudioRecorded:
featuresAtEndpoint
setFeaturesAtEndpoint:
endpointerType
setEndpointerType:
serverFeatureLatencyDistribution
setServerFeatureLatencyDistribution:
additionalMetrics
setAdditionalMetrics:
_totalAudioRecorded
_featuresAtEndpoint
_endpointerType
_serverFeatureLatencyDistribution
_additionalMetrics
_isDeviceRoleStereo
waitingForConnection:error:
isConnected
localURL
string
_compatibilityVersion
appendString:
_footprint
assetForAssetType:resourcePath:configVersion:
attributes
isPremium
containsValueForKey:
decodeObjectForKey:
encodeObject:forKey:
base64EncodedStringWithOptions:
substringToIndex:
supportsSecureCoding
encodeWithCoder:
initWithCoder:
initWithData:hash:locale:digest:signature:certificate:
initWithHash:locale:
initWithData:hash:locale:
builtInRTModelDictionary
modelData
modelLocale
modelHash
digest
signature
certificate
_modelData
_modelLocale
_modelHash
_digest
_signature
_certificate
lpcmInt16NarrowBandASBD
opusNarrowBandASBD
aiffFileASBD
_checkVoiceTriggerEnabled
_didReceiveVoiceTriggerSettingChanged:
_notifyObserver:withEnabled:
CSVoiceTriggerEnabledMonitor:didReceiveEnabled:
_didReceiveVoiceTriggerSettingChangedInQueue:
_isVoiceTriggerEnabled
startRecordingWithOptions:error:
stopRecording:
didPlayEndpointBeep
hasPendingTwoShotBeep
retrainSpIdProfilesForLocale:completion:
convertStopReason:
resetEndPointer
hasAudioRoute
_didReceiveSiriSettingChanged:
dataWithContentsOfFile:options:error:
beepCancellerDidCancelSamples:buffer:timestamp:
numberWithUnsignedLongLong:
cancelBeepFromSamples:timestamp:
willBeep
_beepCanceller
_beepFloatVec
_shortBuffer
_numTotalInputSamples
_numTotalOutputSamples
utteranceAudioFilepathForSpIdType:
utteranceMetadataFilePathForSpIdType:
uniqueUttTag
setUniqueUttTag:
asset
locale
vtEventInfo
setVtEventInfo:
invocationStyle
setInvocationStyle:
_uniqueUttTag
_asset
_vtEventInfo
_invocationStyle
fileSystemRepresentation
makeRootlessDirectoryAtPath:error:
convertToRootlessDirectoryAtPath:error:
interfaceWithProtocol:
setWithArray:
setClasses:forSelector:argumentIndex:ofReply:
strongToWeakObjectsMapTable
_startMonitoring
_isVoiceTriggerEvent:
notifyActivationEvent:completion:
_hasPendingActivationForType:
secondsToHostTime:
receiveTestNotificationAPMode
receiveTestNotificationAOPMode
start
stop
notifyActivationEvent:deviceId:activationInfo:completion:
_didReceiveAOPFirstPassTrigger:completion:
_setupTestNotification
notifyToken
setNotifyToken:
delegates
setDelegates:
pendingActivationEvent
setPendingActivationEvent:
pendingCompletion
setPendingCompletion:
_delegates
_pendingActivationEvent
_pendingCompletion
initWithZeroWindowSize:approxAbsSpeechThreshold:numHostTicksPerAudioSample:
filterZerosInAudioPacket:atBufferHostTime:filteredPacket:
endAudioAndFetchAnyTrailingZerosPacket:
_audioZeroFilterImpl
readAudioChunksFrom:block:
_addAssetManagerEnabledConditions
getTestResponse:
setDelayInterstitialSounds:level:completion:
getTriggerCount:
clearTriggerCount:
stringWithCString:encoding:
createAudioCircularBufferWithDefaultSettings
copySamplesFromHostTime:
copyBufferWithNumSamplesCopiedIn:
saveRecordingBufferFrom:to:toURL:
setBufferLength:
_csAudioCircularBufferImpl
_bufferLength
_asssetMetaUpdatedKey
_didReceiveNewVoiceTriggerAssetMetaData
getSpeechManagerStateMachineForMac
getSpeechManagerStateMachineAOPBridgeOS
getSpeechManagerStateMachineDefault
initWithWordCount:trailingSilenceDuration:eosLikelihood:pauseCounts:silencePosterior:taskName:processedAudioDurationInMilliseconds:
initWithWordCount:trailingSilenceFrames:endOfSilenceLikelihood:pauseCounts:silencePosterior:taskName:
wordCount
setWordCount:
trailingSilenceDuration
setTrailingSilenceDuration:
eosLikelihood
setEosLikelihood:
pauseCounts
setPauseCounts:
silencePosterior
setSilencePosterior:
processedAudioDurationInMilliseconds
setProcessedAudioDurationInMilliseconds:
taskName
setTaskName:
_wordCount
_trailingSilenceDuration
_eosLikelihood
_pauseCounts
_silencePosterior
_processedAudioDurationInMilliseconds
_taskName
voiceController
setStopOnEndpointEnabled:
setRecordEndpointMode:
setRecordDelegate:
setPlaybackDelegate:
numberWithDouble:
playbackRoute
_hasInputAudioRoute
_hasCorrectInputAudioRoute
_hasCorrectOutputAudioRoute
voiceControllerDidStartRecording:successfully:
voiceControllerDidStartRecording:successfully:error:
voiceControllerDidStopRecording:forReason:
voiceControllerDidDetectStartpoint:
voiceControllerDidDetectEndpoint:ofType:
voiceControllerDidDetectEndpoint:ofType:atTime:
voiceControllerEncoderErrorDidOccur:error:
voiceControllerDidFinishAlertPlayback:ofType:error:
voiceControllerRecordHardwareConfigurationDidChange:toConfiguration:
voiceControllerBeginRecordInterruption:
voiceControllerBeginRecordInterruption:withContext:
voiceControllerEndRecordInterruption:
voiceControllerMediaServicesWereLost:
voiceControllerMediaServicesWereReset:
voiceControllerWillSetAudioSessionActive:willActivate:
voiceControllerDidSetAudioSessionActive:isActivated:
voiceControllerRecordBufferAvailable:buffer:
voiceControllerLPCMRecordBufferAvailable:buffer:
voiceControllerPlaybackBufferAvailable:buffer:
voiceControllerDidStartPlaying:successfully:
voiceControllerDidStopPlaying:forReason:
voiceControllerDecoderErrorDidOccur:error:
voiceControllerPlaybackHardwareConfigurationDidChange:toConfiguration:
voiceControllerBeginPlaybackInterruption:
voiceControllerEndPlaybackInterruption:
_voiceController
trainUtterance:languageCode:
notifyImplicitTrainingUtteranceAvailable:forVoiceProfileId:withRecordDeviceInfo:withRecordCtxt:withVoiceTriggerCtxt:withOtherCtxt:withCompletion:
_configureWithASBD:andFrameRate:
_resetWithSampleRate:
_configureWithSampleRate:andFrameRate:
subChunkFrom:numSamples:forChannel:
_processAudioSamples:
_detectVoiceActivityInSamples:numSamples:
replaceBytesInRange:withBytes:length:
_getEndpointMetricsForAudioTimestamp:
endpointStyle
delay
setDelay:
startWaitTime
automaticEndpointingSuspensionEndTime
setAutomaticEndpointingSuspensionEndTime:
minimumDurationForEndpointer
setMinimumDurationForEndpointer:
lastStartOfVoiceActivityTime
bypassSamples
setBypassSamples:
endpointMode
setEndpointMode:
interspeechWaitTime
endWaitTime
saveSamplesSeenInReset
setSaveSamplesSeenInReset:
canProcessCurrentRequest
handleVoiceTriggerWithActivationInfo:
sampleRate
setSampleRate:
frameRate
setFrameRate:
detectedOneShotStartpoint
setDetectedOneShotStartpoint:
detectedRecurrentStartpoint
setDetectedRecurrentStartpoint:
communicatedStartPointDetection
setCommunicatedStartPointDetection:
detectedOneShotEndpoint
setDetectedOneShotEndpoint:
detectedRecurrentEndpoint
setDetectedRecurrentEndpoint:
communicatedEndpointDetection
setCommunicatedEndpointDetection:
samplesSeen
setSamplesSeen:
numSamplesProcessed
setNumSamplesProcessed:
lastOneShotStartpoint
setLastOneShotStartpoint:
lastOneShotEndpoint
setLastOneShotEndpoint:
lastRecurrentStartpoint
setLastRecurrentStartpoint:
lastRecurrentEndpoint
setLastRecurrentEndpoint:
floatSampleBuffer
setFloatSampleBuffer:
topLevelParameterDict
setTopLevelParameterDict:
modelDictPath
setModelDictPath:
isConfigured
setIsConfigured:
previousSamplesSeen
setPreviousSamplesSeen:
apQueue
setApQueue:
recordingDidStop
setRecordingDidStop:
vtEndInSampleCount
setVtEndInSampleCount:
_audioUnitEPVAD2
_saveSamplesSeenInReset
_detectedOneShotStartpoint
_detectedRecurrentStartpoint
_communicatedStartPointDetection
_detectedOneShotEndpoint
_detectedRecurrentEndpoint
_communicatedEndpointDetection
_isConfigured
_recordingDidStop
_frameRate
_endpointStyle
_endpointMode
_interspeechWaitTime
_startWaitTime
_endWaitTime
_automaticEndpointingSuspensionEndTime
_minimumDurationForEndpointer
_bypassSamples
_delay
_samplesSeen
_numSamplesProcessed
_lastOneShotStartpoint
_lastOneShotEndpoint
_lastRecurrentStartpoint
_lastRecurrentEndpoint
_floatSampleBuffer
_topLevelParameterDict
_modelDictPath
_previousSamplesSeen
_apQueue
_vtEndInSampleCount
dictionaryWithContentsOfFile:
initWithArray:
subdataWithRange:
appendData:
skipSamplesAtStartSuchThatNumSamplesReceivedSoFar:reachesACountOf:completionHandler:
splitAudioChunkSuchThatNumSamplesReceivedSoFar:reachesACountOf:completionHandler:
numChannels
_data
_numSamples
_startSampleCount
_hostTime
bestPhrase
bestScore
earlyWarning
setSampleFed:
setBestPhrase:
setBestScore:
setEarlyWarning:
isRescoring
setIsRescoring:
_earlyWarning
_isRescoring
_bestScore
_sampleFed
_bestPhrase
_bestStart
_bestEnd
_readClientLagParametersFromHEPAsset:
addAudio:numSamples:
updateEndpointerThresholdWithValue:
updateEndpointerDelayedTriggerSwitch:
timeIntervalSinceDate:
silenceFramesCountMs
silenceProbability
silenceDurationMs
initWithWordCount:trailingSilenceDuration:endOfSentenceLikelihood:pauseCounts:silencePosterior:clientSilenceFramesCountMs:clientSilenceProbability:silencePosteriorNF:serverFeaturesLatency:eagerResultEndTime:
acceptEagerResultWithFeatures:featuresToLog:
processedAudioMs
defaultServerEndpointFeatures
endOfSentenceLikelihood
initWithWordCount:trailingSilenceDuration:endOfSentenceLikelihood:pauseCounts:silencePosterior:clientSilenceFramesCountMs:clientSilenceProbability:silencePosteriorNF:serverFeaturesLatency:
didEndpointWithFeatures:audioTimestamp:featuresToLog:endpointPosterior:extraDelayMs:
getFrameDurationMs
serverFeaturesLatencyDistributionDictionary
sortUsingComparator:
initWithConfigFile:samplingRate:queue:
initWithSilenceFramesCountMs:silenceProbability:silenceDurationMs:silencePosterior:processedAudioMs:
initWithConfiguration:modelVersion:
requestSupportedWithSamplingRate:
_getCSHybridEndpointerConfigForAsset:
_updateAssetWithLanguage:
_updateAssetWithCurrentLanguage
clientSilenceFeaturesAvailable:
silenceDurationEstimateAvailable:numEstimates:clientProcessedAudioMs:
setCanProcessCurrentRequest:
currentAsset
setCurrentAsset:
didAddAudio
setDidAddAudio:
caesuraSPG
setCaesuraSPG:
clientSilenceFeaturesAtEndpoint
setClientSilenceFeaturesAtEndpoint:
hybridClassifier
setHybridClassifier:
setEndpointerModelVersion:
serverFeaturesQueue
setServerFeaturesQueue:
lastKnownServerEPFeatures
setLastKnownServerEPFeatures:
serverFeatureLatencies
setServerFeatureLatencies:
serverFeaturesWarmupLatency
setServerFeaturesWarmupLatency:
lastServerFeatureTimestamp
setLastServerFeatureTimestamp:
didReceiveServerFeatures
setDidReceiveServerFeatures:
clientLagThresholdMs
setClientLagThresholdMs:
clampedSFLatencyMsForClientLag
setClampedSFLatencyMsForClientLag:
useDefaultServerFeaturesOnClientLag
setUseDefaultServerFeaturesOnClientLag:
hybridClassifierQueue
setHybridClassifierQueue:
lastReportedEndpointTimeMs
setLastReportedEndpointTimeMs:
stateSerialQueue
setStateSerialQueue:
didCommunicateEndpoint
setDidCommunicateEndpoint:
currentRequestSampleRate
setCurrentRequestSampleRate:
vtExtraAudioAtStartInMs
setVtExtraAudioAtStartInMs:
hepAudioOriginInMs
setHepAudioOriginInMs:
recordContext
setRecordContext:
firstAudioPacketTimestamp
setFirstAudioPacketTimestamp:
didTimestampFirstAudioPacket
setDidTimestampFirstAudioPacket:
silencePosteriorGeneratorQueue
setSilencePosteriorGeneratorQueue:
didDetectSpeech
setDidDetectSpeech:
setElapsedTimeWithNoSpeech:
_canProcessCurrentRequest
_didAddAudio
_didReceiveServerFeatures
_useDefaultServerFeaturesOnClientLag
_didCommunicateEndpoint
_didTimestampFirstAudioPacket
_didDetectSpeech
_caesuraSPG
_clientSilenceFeaturesAtEndpoint
_hybridClassifier
_endpointerModelVersion
_serverFeaturesQueue
_lastKnownServerEPFeatures
_serverFeatureLatencies
_serverFeaturesWarmupLatency
_lastServerFeatureTimestamp
_clientLagThresholdMs
_clampedSFLatencyMsForClientLag
_hybridClassifierQueue
_lastReportedEndpointTimeMs
_stateSerialQueue
_currentRequestSampleRate
_vtExtraAudioAtStartInMs
_hepAudioOriginInMs
_recordContext
_firstAudioPacketTimestamp
_silencePosteriorGeneratorQueue
_elapsedTimeWithNoSpeech
getHostClockFrequency
macHostTimeFromBridgeHostTime:
saveMetadata:isExplicitEnrollment:
saveUtterance:utteranceAudioPath:numSamplesToWrite:isExplicitEnrollment:
writeMetaDict:atMetaPath:
saveRawUtteranceAndMetadata:to:isExplicitEnrollment:
saveUtteranceAndMetadata:atDirectory:isExplicitEnrollment:
_setupVAD2Endpointer
_shouldEnterTwoShotAtEndPointTime:
_shouldUseVAD2ForTwoShot
endpointerDelegate
hybridEndpointer
setHybridEndpointer:
vad2Endpointer
setVad2Endpointer:
activeEndpointer
setActiveEndpointer:
didEnterTwoshot
setDidEnterTwoshot:
vad2EndpointStyle
setVad2EndpointStyle:
vad2EndpointtMode
setVad2EndpointtMode:
vad2StartWaitTime
setVad2StartWaitTime:
vad2EndWaitTime
setVad2EndWaitTime:
vad2InterspeechWaitTime
setVad2InterspeechWaitTime:
vad2Delay
setVad2Delay:
vad2AutomaticEndpointingSuspensionEndTime
setVad2AutomaticEndpointingSuspensionEndTime:
vad2MinimumDurationForEndpointer
setVad2MinimumDurationForEndpointer:
vad2SaveSamplesSeenInReset
setVad2SaveSamplesSeenInReset:
_didEnterTwoshot
_vad2SaveSamplesSeenInReset
_endpointerDelegate
_hybridEndpointer
_vad2Endpointer
_activeEndpointer
_vad2EndpointStyle
_vad2EndpointtMode
_vad2StartWaitTime
_vad2EndWaitTime
_vad2InterspeechWaitTime
_vad2Delay
_vad2AutomaticEndpointingSuspensionEndTime
_vad2MinimumDurationForEndpointer
initWithBool:
initWithDouble:
initWithLongLong:
initWithUnsignedLongLong:
objCType
longLongValue
_checkFirstUnlocked
_notifyObserver:withUnlocked:
CSFirstUnlockMonitor:didReceiveFirstUnlock:
_didReceiveFirstUnlockInQueue:
_didReceiveFirstUnlock:
_firstUnlocked
_scaleDecayConstants:
_savePeaks:averagePower:maxSample:
_linearToDB:
_ampToDB:
initWithSampleRate:
process:stride:inFrameToProcess:
getPeakPowerDB
getAveragePowerDB
_averagePowerI
_instantaneousMode
_peak
_maxPeak
_decay
_peakDecay
_averagePowerPeak
_peakHoldCount
_previousBlockSize
_decay1
_peakDecay1
initWithCSspIdType:userName:assetResourcePath:satDirectory:assetHash:
userName
sysConfigFile
sysConfigRoot
satModelDir
satAudioDir
analyzeSpeakerVector:numElements:
updateSAT
_checkSpringBoardStarted
_didReceiveSpringboardStarted:
_notifyObserver:withStarted:
CSSpringboardStartMonitor:didReceiveStarted:
_didReceiveSpringboardStartedInQueue:
_isSpringBoardStarted
fallBackAssetResourcePath
initWithConfigFile:
spgEndpointAnalyzerDidDetectEndpoint:
initWithEndpointThreshold:
endpointThreshold
setEndpointThreshold:
hasReported
setHasReported:
_hasReported
_endpointThreshold
componentsSeparatedByString:
processInfo
systemUptime
initWithConfiguration:
runRecognitionWithResultStream:
_recognizeWavData:length:
addAudioSamples:count:
tokens
_getConfidence:
keywordAnalyzerQuasar:hasResultAvailable:forChannel:
addObjectsFromArray:
confidence
tokenName
speechRecognizer:didRecognizePartialResult:
speechRecognizer:didFinishRecognitionWithError:
speechRecognizer:didRecognizeFinalResults:
speechRecognizer:didRecognizeFinalResults:tokenSausage:nBestChoices:
speechRecognizer:didRecognizeFinalResultPackage:
speechRecognizer:didProcessAudioDuration:
speechRecognizer:didRecognizeRawEagerRecognitionCandidate:
speechRecognizer:didProduceEndpointFeaturesWithWordCount:trailingSilenceDuration:eosLikelihood:pauseCounts:silencePosterior:processedAudioDurationInMilliseconds:
speechRecognizer:didRecognizePartialResultNbest:
initWithConfigPath:triggerTokens:useKeywordSpotting:
runRecognition
triggerConfidence
_previousUtteranceTokens
_triggerTokenList
_recognizer
_recognizerBuffer
_useKeywordSpotting
_triggerConfidence
_availabilityChanged
_didReceivedNetworkAvailabilityChangedNotification:
_notifyObserver:withNetworkAvailability:
CSNetworkAvailabilityMonitor:didReceiveNetworkAvailabilityChanged:
initWithFormat:
decodeObjectOfClass:forKey:
initWithRoute:isRemoteDevice:remoteDeviceUID:remoteDeviceProductIdentifier:
isRemoteDevice
remoteDeviceUID
remoteProductIdentifier
copyWithZone:
initWithAVVCRecordDeviceInfo:
remoteDeviceProductIdentifier
_isRemoteDevice
_route
_remoteDeviceUID
_remoteDeviceProductIdentifier
hybridEndpointerAssetFilename
initWithResourcePath:configFile:configVersion:
_decodeJson:
assetHashInResourcePath:
defaultFallBackAssetForSmartSiriVolume
getBoolForKey:category:default:
getStringForKey:category:default:
containsKey:category:
containsCategory:
hashFromResourcePath
isEqualAsset:
configVersion
_decodedInfo
_path
_resourcePath
_configVersion
_configureAudioConverter:
_convertBufferedLPCM:allowPartial:timestamp:
_opusConverter
_bufferedLPCM
_recordBasePacketsPerSecond
_opusOutASBD
_convertPacketCount
_convertAudioCapacity
_lastTimestamp
_outPacketSizeInSec
_notifyObserver:withLanguageCode:
_didReceiveLanguageCodeUpdate
_checkAllConditionsEnabled
notifyCallback:
_monitors
_conditions
_callback
recordContextString:
initWithDictionary:
enumerateKeysAndObjectsUsingBlock:
speakerDetectorThreshold
maxSpeakerVectorsToPersist
_initializeSAT:
_computeSATScore:
speakerDetector:didDetectSpeaker:
speakerDetector:didDetectSpeakerReject:
addLastTriggerToProfileWithSuperVector:
initWithAsset:speakerModel:
_initializeNDAPI:resourcePath:
processSuperVector:withResult:
analyzeWavForEnrollment:numSamples:
addLastTriggerToProfile
getSATVectorCount
getMaxSpeakerVectorsToPersist
_threshold
_maxSpeakerVectorsToPersist
_spkModel
CVTThreshold
VTSecondPassCategoryForFirstPassSource:
VTSecondPassPreTriggerAudioTimeFrom:
_sampleLengthFrom:To:
_keywordAnalyzer
_lastKeywordScore
_keywordThreshold
_extraSamplesAtStart
_voiceControllerWithContext:error:
_recordingSampleRate
_destroyVoiceController
_audioRecorderDidStartRecordingSuccessfully:error:
_shouldUseRemoteRecordForContext:
_createDeInterleaverIfNeeded
_createSampleRateConverterIfNeeded
_createAudioPowerMeterIfNeeded
_shouldInjectAudio
_needResetAudioInjectionIndex:
_shouldRunZeroFilter
_resetZeroFilter
_startRecordingForAudioInjection
_holdAudioRecordingTransaction
currentRecordDeviceInfo
initWithDescription:
streamDescription
_deinterleaveBufferIfNeeded:
_samplingRateConvertIfNeeded:
_processAudioChainWithZeroFiltering:atTime:
_processAudioChain:atTime:
playAlertSoundForType:overrideMode:
_releaseAudioRecordingTransaction
_audioRecorderDidStopRecordingForReason:
_zeroFilter
_deinterleaver
_interleavedABL
_pNonInterleavedABL
_needSampleRateConversion
_remoteRecordClient
_powerMeter
_shouldUsePowerMeter
_latestContext
_shouldUseRemoteRecord
_opusDecoder
_audioFileReader
_audioFilePathIndex
_waitingForDidStart
_recordingTransaction
dataWithPropertyList:format:options:error:
encryptedDataWithAESGCMKey:completion:
propertyListWithData:options:format:error:
decryptedDataWithAESGCMKey:ivData:tagData:error:
randomBytesWithLength:error:
errorMessageForCCErrorCode:
getVoiceTriggerProfilesAESKey
generateIfNecessaryAESKeyWithKeySizeInBits:applicationTag:keyLabel:shouldGenerateIfNecessary:
generateAESKeyWithKeySizeInBits:
storeAESKeyInKeychain:applicationTag:keyLabel:
getAESKeyFromKeychainWithApplicationTag:keyLabel:
deleteAESKeyWithApplicationTag:keyLabel:
getKeychainAttributesForAESKeyWithApplicationTag:keyLabel:
setTriggerMode:
getTriggerMode
_transaction
_description
isSiriRestrictedOnLockScreen
CSGestureMonitor
CSGestureMonitorPhone
CMWakeGestureDelegate
NSObject
CSUtteranceMetadataManager
CSSiriDebugConnection
CSMediaPlayingMonitor
CSVolumeMonitor
CSTimerMonitor
CSAudioRecordContext
CSAlarmMonitor
CSAssetManagerEnablePolicyFactory
CSBiometricMatchMonitor
BKDeviceDelegate
CSAudioZeroCounter
SmartSiriVolume
CSAudioFileManager
SpIdMetadataLogging
CSVoiceTriggerAssetDownloadMonitor
Directory
CSSmartSiriVolume
CSMediaPlayingMonitorDelegate
CSAlarmMonitorDelegate
CSTimerMonitorDelegate
CSSpeechManagerDelegate
CSVoiceTriggerDelegate
CSAudioFileReader
CSAudioSampleRateConverter
XPCObject
CSSpIdProcessor
LanguageCode
CSVTUITrainingManager
CSVTUITrainingSessionDelegate
CSVTUIAudioSessionDelegate
CSEndpointAnalyzerDelegate
LPCMTypeConversion
CSSpeakerModel
CSSpIdVTSpeakerRecognizer
CSSpIdSpeakerRecognizer
CSAudioSessionMonitor
CSCoreSpeechServices
CSConfig
CSEncryptedAudioFileReader
CSSpeechManager
CSAudioRecorderDelegate
CSStateMachineDelegate
CSSiriEnabledMonitorDelegate
CSAudioServerCrashMonitorGibraltarDelegate
CSSmartSiriVolumeDelegate
CSActivationEventNotifierDelegate
CSAudioRouteChangeMonitorDelegate
CSVoiceTriggerAssetDownloadMonitorDelegate
CSLanguageCodeUpdateMonitorDelegate
CSClamshellStateMonitor
CSSpIdImplicitTraining
CSVTUIEditDistance
CSAssetManager
CSVoiceTriggerAssetMetaUpdateMonitorDelegate
CSSpeechEndpointAssetMetaUpdateMonitorDelegate
CSAssetControllerDelegate
CSVTUITrainingSession
SFSpeechRecognitionTaskDelegate
CSVTUIEndPointDelegate
CSKeywordAnalyzerNDEAPI
CSSpeechController
CSAudioConverterDelegate
CSSpIdSpeakerRecognizerDelegate
RTModel
CSVTUITrainingSessionWithPayload
CSSpIdTrainingParallelRecorder
CSVoiceProfileContext
CSPreferences
CSAudioTimeConverter
CSStateMachine
Meter
CSPlainAudioFileWriter
CSAudioFileWriter
CSEventMonitor
CSAudioFileLog
Alert
Metrics
CSSpIdSpeakerVectorGenerator
CSActivationEvent
CSSpIdTIOnlySpeakerRecognizer
CSScreenLockMonitor
CSAssetController
CSEventMonitorDelegate
Utils
SpeakerId
CSAudioDecoder
CSAudioRouteChangeMonitor
VoiceTriggerPassThru
CSSmartSiriVolumeEnablePolicy
AudioHardware
CSVoiceTriggerAssetChangeMonitor
CSSpeechEndpointAssetMetaUpdateMonitor
VoiceTriggerRecord
CSVTUIRegularExpressionMatcher
CSEncryptedAudioFileWriter
CSDispatchGroup
CSVTUIASRGrammars
NSURLSessionDelegate
CSP2PService
CSUtils
CSKeywordAnalyzerNDAPI
CSEndpointerMetrics
CSSACInfoMonitor
CSRemoteControlClient
CSAsset
CSVoiceTriggerRTModel
NSSecureCoding
NSCoding
AudioStreamBasicDescription
CSVoiceTriggerEnabledMonitor
CSRemoteRecordClient
CSSpIdProfilesManager
CSVTUIAudioSessionRemote
CSVTUIAudioSession
CSSiriEnabledMonitor
CSBeepCanceller
CSSpIdContext
Rootless
CoreSpeechXPCProtocol
CSActivationEventNotifier
CSAudioZeroFilter
AudioFile
CSAssetManagerEnablePolicy
CSCoreSpeechServiceListenerDelegate
CSAudioCircularBuffer
CSVoiceTriggerAssetMetaUpdateMonitor
CSSpeakerIdRecognizerFactory
CSAssetManagerEnablePolicyMac
CSSpeechManagerStateMachineFactory
CSServerEndpointFeatures
CSVTUIAudioSessionAVVC
AVVoiceControllerRecordDelegate
AVVoiceControllerPlaybackDelegate
CSVoiceTriggerSpeakerTrainer
CSVAD2EndpointAnalyzer
CSEndpointAnalyzerImpl
CSEndpointAnalyzer
private
CSAudioChunk
CSNovDetectorResult
CSNovDetector
Bitset
CSHybridEndpointAnalyzer
CSAssetManagerDelegate
EARCaesuraSilencePosteriorGeneratorDelegate
Time
CSVoiceTriggerEnrollmentDataManager
CSEndpointerProxy
CSFirstUnlockMonitor
CSAudioPowerMeter
CSSpIdSATAnalyzer
CSSpringboardStartMonitor
CSSPGEndpointAnalyzer
CSKeywordAnalyzerQuasar
_EARSpeechRecognitionResultStream
CSNetworkAvailabilityMonitor
CSAudioRecordDeviceInfo
NSCopying
CSAudioConverter
CSLanguageCodeUpdateMonitor
CSPolicy
RecordContext
CSSpeakerDetectorNDAPI
CSVTUIKeywordDetector
CSAudioRecorder
CSBeepCancellerDelegate
CSAudioDecoderDelegate
CSAudioFileReaderDelegate
Encryption
CSAESKeyManager
CSJarvisTriggerModeMonitor
CSOSTransaction
DuckOption
ResourcePathHash
CSSRFUserSettingMonitor
@16@0:8
v16@0:8
Q16@0:8
v24@0:8Q16
v24@0:8@16
@"<CSGestureMonitorDelegate>"
B24@0:8@16
#16@0:8
@24@0:8:16
@32@0:8:16@24
@40@0:8:16@24@32
B16@0:8
B24@0:8#16
B24@0:8:16
Vv16@0:8
^{_NSZone=}16@0:8
B24@0:8@"Protocol"16
@"NSString"16@0:8
v32@0:8@16q24
v40@0:8@16q24Q32
v36@0:8@16q24i32
v32@0:8@"CMWakeGestureManager"16q24
v40@0:8@"CMWakeGestureManager"16q24Q32
v36@0:8@"CMWakeGestureManager"16q24i32
@"CMWakeGestureManager"
v60@0:8@16B24B28@32@40Q48B56
@24@0:8@16
v32@0:8@16@24
q16@0:8
@"NSObject<OS_dispatch_queue>"
f16@0:8
@32@0:8Q16@24
q24@0:8Q16
@"NSDictionary"
@"NSString"
v32@0:8@"BKDevice"16@"BKMatchEvent"24
B32@0:8^B16^Q24
@"<CSBiometricMatchMonitorDelegate>"
@"BKDevice"
@32@0:8@16f24I28
v28@0:8@16I24
I16@0:8
v36@0:8@16I24@28
@96@0:8{AudioStreamBasicDescription=dIIIIIIII}16{AudioStreamBasicDescription=dIIIIIIII}56
@104@0:8@16{AudioStreamBasicDescription=dIIIIIIII}24{AudioStreamBasicDescription=dIIIIIIII}64
v20@0:8f16
r*16@0:8
v36@0:8@16@24f32
v40@0:8@16@24Q32
v40@0:8@16@24@?32
@48@0:8@16@24@32^@40
v32@0:8@"CSMediaPlayingMonitor"16q24
v32@0:8@"CSAlarmMonitor"16q24
v32@0:8@"CSTimerMonitor"16q24
v36@0:8@16B24@28
v36@0:8@16f24Q28
v40@0:8@16q24@32
v28@0:8@16B24
v32@0:8@"CSSpeechManager"16@"AVVCAudioBuffer"24
v32@0:8@"CSSpeechManager"16@"CSAudioChunk"24
v36@0:8@"CSSpeechManager"16B24@"NSError"28
v32@0:8@"CSSpeechManager"16q24
@"NSDictionary"16@0:8
v36@0:8@"CSSpeechManager"16f24Q28
v40@0:8@"CSSpeechManager"16q24@"NSError"32
v24@0:8@"CSSpeechManager"16
v32@0:8@"CSSpeechManager"16@"NSDictionary"24
v28@0:8@"CSSpeechManager"16B24
v24@0:8d16
v24@0:8@"NSDictionary"16
v24@0:8@"NSData"16
@28@0:8f16@20
v28@0:8I16q20
v52@0:8@16q24B32Q36Q44
f24@0:8q16
f24@0:8f16f20
f36@0:8f16f20f24f28f32
f44@0:8f16f20f24f28f32f36f40
f28@0:8f16f20f24
v28@0:8@16f24
f20@0:8f16
{unique_ptr<SmartSiriVolume, std::__1::default_delete<SmartSiriVolume> >="__ptr_"{__compressed_pair<SmartSiriVolume *, std::__1::default_delete<SmartSiriVolume> >="__value_"^{SmartSiriVolume}}}
{vector<float, std::__1::allocator<float> >="__begin_"^f"__end_"^f"__end_cap_"{__compressed_pair<float *, std::__1::allocator<float> >="__value_"^f}}
@"NSUserDefaults"
@"CSSmartSiriVolumeEnablePolicy"
@"CSAsset"
@"<CSSmartSiriVolumeDelegate>"
B24@0:8d16
@20@0:8I16
^{OpaqueExtAudioFile=}
@"NSObject<OS_dispatch_source>"
{AudioStreamBasicDescription="mSampleRate"d"mFormatID"I"mFormatFlags"I"mBytesPerPacket"I"mFramesPerPacket"I"mBytesPerFrame"I"mChannelsPerFrame"I"mBitsPerChannel"I"mReserved"I}
@"<CSAudioFileReaderDelegate>"
^{OpaqueAudioConverter=}96@0:8{AudioStreamBasicDescription=dIIIIIIII}16{AudioStreamBasicDescription=dIIIIIIII}56
^{OpaqueAudioConverter=}
v40@0:8@16@24@32
B44@0:8@16@24@32B40
B44@0:8@"CSVTUITrainingSession"16@"NSData"24@"NSString"32B40
v28@0:8B16@20
v24@0:8q16
v28@0:8B16@"NSError"20
v24@0:8@"NSError"16
v32@0:8@16d24
v40@0:8@16d24@32
v32@0:8@"<CSEndpointAnalyzer>"16d24
v40@0:8@"<CSEndpointAnalyzer>"16d24@"CSEndpointerMetrics"32
@32@0:8@16@24
v24@0:8@?16
@24@0:8@?16
q36@0:8q16B24@?28
B24@0:8q16
v32@0:8i16B20@?24
v20@0:8B16
@"<CSVTUIAudioSession>"
@"CSVAD2EndpointAnalyzer"
@"CSVTUIKeywordDetector"
@"NSMutableArray"
@"CSVTUITrainingSession"
@"SFSpeechRecognizer"
@"<CSVTUITrainingManagerDelegate>"
@32@0:8@"CSSpIdContext"16@"<CSSpIdSpeakerRecognizerDelegate>"24
v24@0:8@"CSAudioChunk"16
v32@0:8@16@?24
v56@0:8Q16Q24@32@40@?48
d16@0:8
S16@0:8
i16@0:8
@40@0:8@16@24Q32
B24@0:8@?16
@"NSURL"
@"NSData"
v40@0:8@"CSAudioRecorder"16@"NSData"24Q32
v32@0:8@"CSAudioRecorder"16@"AVVCAudioBuffer"24
v36@0:8@"CSAudioRecorder"16B24@"NSError"28
v32@0:8@"CSAudioRecorder"16q24
v40@0:8@"CSAudioRecorder"16q24@"NSError"32
v24@0:8@"CSAudioRecorder"16
v32@0:8@"CSAudioRecorder"16@"NSDictionary"24
v28@0:8@"CSAudioRecorder"16B24
v40@0:8q16q24q32
v32@0:8q16q24
v28@0:8@"CSSiriEnabledMonitor"16B24
v24@0:8@"CSAudioServerCrashMonitorGibraltar"16
v40@0:8@"CSActivationEventNotifier"16@"CSActivationEvent"24@?<v@?B@"NSError">32
v32@0:8@"CSAudioRouteChangeMonitor"16q24
v32@0:8@16@"NSString"24
@64@0:8@16Q24@32@40@48Q56
B32@0:8@16^@24
B24@0:8^@16
B32@0:8q16^@24
B40@0:8q16d24^@32
v48@0:8d16@24@?32@?40
B32@0:8Q16^@24
B40@0:8Q16@24^@32
v40@0:8@16Q24@?32
B40@0:8@16Q24^@32
v32@0:8@16^@24
@24@0:8q16
@24@0:8Q16
B24@0:8Q16
@?16@0:8
@"CSAudioRecorder"
@"CSStateMachine"
@"CSAudioCircularBuffer"
@"<CSSpeechManagerDelegate>"
@"CSSmartSiriVolume"
@"NSHashTable"
@"NSUUID"
v72@0:8@16@24@32@40@48@56@?64
Q24@0:8@16
@"<CSAudioFileWriter>"
@"CSBiometricMatchMonitor"
@"CSGestureMonitor"
v32@0:8@16Q24
v32@0:8@"CSAssetController"16Q24
@20@0:8B16
v32@0:8Q16@?24
v40@0:8Q16@24@?32
@"CSPolicy"
@"NSMutableDictionary"
v24@0:8@"SFSpeechRecognitionTask"16
v32@0:8@"SFSpeechRecognitionTask"16@"SFTranscription"24
v32@0:8@"SFSpeechRecognitionTask"16@"SFSpeechRecognitionResult"24
v28@0:8@"SFSpeechRecognitionTask"16B24
@96@0:8q16q24@32@40@48@56@64@72@80@?88
v24@0:8i16B20
@"SFSpeechAudioBufferRecognitionRequest"
@"SFSpeechRecognitionTask"
@"NSTimer"
@"<CSVTUITrainingSessionDelegate>"
@"NSMutableData"
@"<CSKeywordAnalyzerNDEAPIScoreDelegate>"
v44@0:8@16@24f32Q36
v44@0:8@"CSAudioConverter"16@"NSArray"24f32Q36
v32@0:8@"<CSSpIdSpeakerRecognizer>"16@"NSDictionary"24
{AudioStreamBasicDescription=dIIIIIIII}16@0:8
B20@0:8B16
B32@0:8@16q24
f24@0:8Q16
v32@0:8d16@?24
@"CSAudioConverter"
@"CSAudioSampleRateConverter"
@"CSAudioZeroCounter"
@"NSObject<OS_dispatch_group>"
@"<CSSpeechControllerDelegate>"
@"CSEndpointerProxy"
@"CSSpeechManager"
@"CSPlainAudioFileWriter"
@"CSSpeakerIdRecognizerFactory"
@"<CSSpIdSpeakerRecognizer>"
@"CSSpIdImplicitTraining"
v40@0:8@16@?24@?32
@48@0:8@16@24@32@40
@"NSNumber"
@24@0:8^@16
B32@0:8@16@24
v32@0:8@?16@?24
d24@0:8d16
v32@0:8Q16Q24
Q24@0:8Q16
@"<CSStateMachineDelegate>"
B32@0:8r^v16Q24
B32@0:8r^v16q24
@48@0:8Q16@24@32@40
@32@0:8@16Q24
@52@0:8Q16@24@32f40Q44
@48@0:8Q16@24@32Q40
@36@0:8Q16@24B32
v44@0:8Q16@24B32@?36
v80@0:8@16{AudioStreamBasicDescription=dIIIIIIII}24Q64@?72
B32@0:8Q16@24
@"<CSAudioDecoderDelegate>"
@"<CSVoiceTriggerAssetChangeDelegate>"
q48@0:8@16@24@32@40
v32@0:8@"NSURLSession"16@"NSError"24
v40@0:8@"NSURLSession"16@"NSURLAuthenticationChallenge"24@?<v@?q@"NSURLCredential">32
v24@0:8@"NSURLSession"16
@32@0:8q16@24
@40@0:8@16q24@32
v56@0:8@16@24@32B40Q44B52
@"<CSADCompanionServiceProvider>"
@20@0:8i16
@28@0:8@16i24
v20@0:8I16
@"CSNovDetector"
@"<CSKeywordAnalyzerNDAPIScoreDelegate>"
@56@0:8d16@24q32@40@48
@"NSArray"
B32@0:8d16^@24
@"<CSRemoteControlClientDelegate>"
v24@0:8@"NSCoder"16
@24@0:8@"NSCoder"16
@64@0:8@16@24@32@40@48@56
@40@0:8@16@24@32
@"<CSRemoteRecordClientDelegate>"
v24@0:8@"<CSVTUIAudioSessionDelegate>"16
v24@0:8@"<Endpointer>"16
q24@0:8q16
@"<CSVTUIAudioSessionDelegate>"
{unique_ptr<BatchBeepCanceller, std::__1::default_delete<BatchBeepCanceller> >="__ptr_"{__compressed_pair<BatchBeepCanceller *, std::__1::default_delete<BatchBeepCanceller> >="__value_"^{BatchBeepCanceller}}}
{vector<short, std::__1::allocator<short> >="__begin_"^s"__end_"^s"__end_cap_"{__compressed_pair<short *, std::__1::allocator<short> >="__value_"^s}}
@"<CSBeepCancellerDelegate>"
v32@0:8@"NSString"16@?<v@?@"NSString"@"NSString"@"NSError">24
v56@0:8Q16Q24@"NSArray"32@"NSArray"40@?<v@?@"CSVoiceTriggerRTModel"@"CSVoiceTriggerRTModel"@"NSError">48
v40@0:8@"NSArray"16@"NSString"24@?<v@?@"NSString"@"NSError">32
v48@0:8Q16@24@32@?40
v20@0:8i16
@"NSMapTable"
@"CSActivationEvent"
@36@0:8Q16S24d28
Q40@0:8@16Q24^@32
Q24@0:8^@16
{unique_ptr<CSAudioZeroFilterImpl<unsigned short>, std::__1::default_delete<CSAudioZeroFilterImpl<unsigned short> > >="__ptr_"{__compressed_pair<CSAudioZeroFilterImpl<unsigned short> *, std::__1::default_delete<CSAudioZeroFilterImpl<unsigned short> > >="__value_"^{CSAudioZeroFilterImpl<unsigned short>}}}
B32@0:8@16@?24
Vv24@0:8@?16
Vv40@0:8@16q24@?32
Vv24@0:8@?<v@?@"NSString">16
Vv40@0:8@"NSArray"16q24@?<v@?@"NSError">32
Vv24@0:8@?<v@?Q>16
Vv24@0:8@?<v@?>16
Vv24@0:8@?<v@?B>16
Vv24@0:8@?<v@?q>16
@32@0:8Q16f24f28
v32@0:8r^v16Q24
v40@0:8r^v16Q24Q32
@32@0:8Q16Q24
@24@0:8^Q16
v40@0:8Q16Q24@32
{unique_ptr<corespeech::CSAudioCircularBufferImpl<unsigned short>, std::__1::default_delete<corespeech::CSAudioCircularBufferImpl<unsigned short> > >="__ptr_"{__compressed_pair<corespeech::CSAudioCircularBufferImpl<unsigned short> *, std::__1::default_delete<corespeech::CSAudioCircularBufferImpl<unsigned short> > >="__value_"^{CSAudioCircularBufferImpl<unsigned short>}}}
@72@0:8q16q24d32@40d48@56q64
@64@0:8q16q24d32@40d48@56
v28@0:8@16i24
v36@0:8@16i24d28
v36@0:8@16i24@28
v28@0:8@"AVVoiceController"16B24
v36@0:8@"AVVoiceController"16B24@"NSError"28
v32@0:8@"AVVoiceController"16q24
v24@0:8@"AVVoiceController"16
v28@0:8@"AVVoiceController"16i24
v36@0:8@"AVVoiceController"16i24d28
v32@0:8@"AVVoiceController"16@"NSError"24
v36@0:8@"AVVoiceController"16i24@"NSError"28
v32@0:8@"AVVoiceController"16@"NSDictionary"24
v32@0:8@"AVVoiceController"16@"AVVCAudioBuffer"24
@"AVVoiceController"
B36@0:8@16@24B32
v32@0:8Q16@24
v32@0:8Q16@"NSDictionary"24
@"<CSEndpointAnalyzerDelegate>"16@0:8
v24@0:8@"<CSEndpointAnalyzerDelegate>"16
v24@0:8@"CSServerEndpointFeatures"16
v32@0:8d16@?<v@?B@"NSArray">24
^{OpaqueAudioComponentInstance=}
@"<CSEndpointAnalyzerDelegate>"
@24@0:8d16
B28@0:8^{AudioStreamBasicDescription=dIIIIIIII}16I24
v28@0:8d16I24
v28@0:8^f16I24
@64@0:8@16Q24Q32Q40Q48Q56
@40@0:8Q16Q24Q32
v40@0:8Q16Q24@?32
I24@0:8Q16
v24@0:8@"CSAssetManager"16
v36@0:8^f16Q24f32
v24@0:8@"EARClientSilenceFeatures"16
@"EARCaesuraSilencePosteriorGenerator"
@"EARClientSilenceFeatures"
@"_EAREndpointer"
@"CSServerEndpointFeatures"
@"NSDate"
Q20@0:8f16
d24@0:8Q16
Q40@0:8Q16Q24Q32
v36@0:8@16@24B32
B44@0:8@16@24Q32B40
B28@0:8@16B24
@"<CSEndpointAnalyzerImpl>"
@20@0:8f16
v32@0:8r^s16i24i28
v28@0:8i16i20i24
@56@0:8Q16@24@32@40@48
f32@0:8@16Q24
@"<CSSPGEndpointAnalyzerDelegate>"
v48@0:8@16@24@32@40
v72@0:8@16q24q32d40@48d56q64
v32@0:8@"_EARSpeechRecognizer"16@"_EARSpeechRecognitionResult"24
v32@0:8@"_EARSpeechRecognizer"16@"NSError"24
v32@0:8@"_EARSpeechRecognizer"16@"NSArray"24
v48@0:8@"_EARSpeechRecognizer"16@"NSArray"24@"NSArray"32@"NSArray"40
v32@0:8@"_EARSpeechRecognizer"16@"_EARSpeechRecognitionResultPackage"24
v32@0:8@"_EARSpeechRecognizer"16d24
v32@0:8@"_EARSpeechRecognizer"16@"_EARSpeechRecognition"24
v72@0:8@"_EARSpeechRecognizer"16q24q32d40@"NSArray"48d56q64
@36@0:8@16@24B32
v28@0:8r^s16i24
d24@0:8@16
@"_EARSpeechRecognizer"
@"_EARSpeechRecognitionAudioBuffer"
@"<CSKeywordAnalyzerQuasarScoreDelegate>"
@24@0:8^{_NSZone=}16
@44@0:8@16B24@28@36
@40@0:8Q16@24@32
v36@0:8@16B24Q28
v24@0:8^{OpaqueAudioConverter=}16
@"<CSAudioConverterDelegate>"
f24@0:8@16
@"CSSpeakerModel"
@"<CSSpeakerDetectorNDAPIDelegate>"
Q24@0:8I16I20
@"CSKeywordAnalyzerNDAPI"
v40@0:8@"CSBeepCanceller"16@"NSData"24Q32
v40@0:8@"CSAudioDecoder"16@"NSData"24Q32
v40@0:8@"CSAudioFileReader"16@"NSData"24Q32
v36@0:8@"CSAudioFileReader"16B24@"NSError"28
v32@0:8@"CSAudioFileReader"16q24
@32@0:8@16^@24
@"CSAudioZeroFilter"
@"CSBeepCanceller"
{AudioBufferList="mNumberBuffers"I"mBuffers"[1{AudioBuffer="mNumberChannels"I"mDataByteSize"I"mData"^v}]}
^{AudioBufferList=I[1{AudioBuffer=II^v}]}
@"CSRemoteRecordClient"
@"CSAudioPowerMeter"
@"CSAudioDecoder"
@"CSAudioFileReader"
@"CSOSTransaction"
@"<CSAudioRecorderDelegate>"
@32@0:8Q16^@24
@40@0:8@16@24^@32
@44@0:8Q16@24@32B40
B40@0:8@16@24@32
@"NSObject<OS_os_transaction>"
pbhw
pbtb
pbiu
otua
ciov
bhev
eltb
siar
emoh
emoh
cvpc
pbiu
N@fff?
@(knN33
mcpl
supo
?ffffff
@mcpl
NSt3__118basic_stringstreamIcNS_11char_traitsIcEENS_9allocatorIcEEEE
NSt3__115basic_stringbufIcNS_11char_traitsIcEENS_9allocatorIcEEEE
NSt3__119basic_ostringstreamIcNS_11char_traitsIcEENS_9allocatorIcEEEE
xfua2vpelppa
?ffffff
%s ERR: uttPath is nil -  Bailing out
%s ERR: uttPath is nil - Bailing out
%s ERR: uttMeta is nil - Bailing out
%s ::: Error creating json Metadata: %{public}@
%s Error reading contents of SAT root: %{public}@: err: %{public}@
%s Error determining if file is dir-entry: url=%{public}@, err=%{public}@
%s ERROR creating meta-version json-data from dict: ERR: %{public}@
%s Error reading contents of audioDir: %{public}@, err: %{public}@
%s Missing meta-file: Creating new Meta file for audio file: %{public}@
%s ERR: Unexpected. metaVersionFileData is empty while the file exists at: %{public}@
%s Json-Err reading metaVersionFile: %{public}@: err: %{public}@
%s ERR: uttMetaURL is nil
%s ERR: Unexpected. metaData is nil while the uttMetaPath exists at: %{public}@
%s error reading meta-file: %{public}@
%s %@ task delivered.
%s %@ completed with response %@ and error %@.
%s Get initial state from MediaRemote: media is on playing state %{public}ld.
%s Start monitoring MediaRemote: media playback
%s Stop monitoring MediaRemote: media playback
%s MediaRemote reported the now playing app playback state changed to %s (state %d)
%s Celestial is not available on this platform.
%s notification = %{public}@
%s MobileTimer is not available on this platform.
%s Failed to create biometricdevice with error %@
%s BiometricMatchEvent: result = %u, timeStamp = %llu
%s BiometricMatchEvents unavailable with error %@
%s ERR: Biometric device is nil - Bailing out
%s BiometricMatchEventOccurred: result = %u, timeStamp = %llu
%s In %@: Continuous digital zero detected, lasting %{public}u samples per channel
%s In %@: Continuous digital zero in this audio chunk detected, lasting %{public}u samples per channel
%s Plan removing the temp file %{public}@
%s Failed to remove temp file %{public}@ reason: %{public}@
%s Start copying %{public}u bytes of data to crashreporter
%s Failed to read data from %{public}@
%s Finished copying data to crashreporter.
%s Logging audio file into : %{public}@
%s SpkrId:: Error creating uttMetaJsonData: %@
%s SpkrId:: Failed to create UttMeta...
%s Start monitoring : VoiceTrigger Asset Download
%s Stop monitoring : VoiceTrigger Asset Download
%s New VoiceTrigger is now installed
%s CS logging files under %{public}@ created before %{public}@ will be removed.
%s Couldn't get creation date: %{public}@
%s Could not remove %{public}@: %{public}@
%s CS logging files number %{public}lu with pattern %{public}@ under %{public}@ exceeding limit, only keep the latest %{public}lu ones
%s Regular expression is nil!
%s SmartSiriVolume: deleted %{public}u elements in energy buffer.
%s SmartSiriVolume: number of elements to delete exceeds energy buffer size, ignore.
%s SmartSiriVolume init value for noise estimation %{public}f
%s SmartSiriVolume init value for LKFS estimation %{public}f
%s SmartSiriVolume enable policy changed : %{public}@
%s SmartSiriVolume received MediaRemote initial state as %{public}@
%s SmartSiriVolume haven't got MediaRemote callback yet, let's assume media is playing.
%s SmartSiriVolume received alarm initial state as %{public}@
%s SmartSiriVolume received timer initial state as %{public}@
%s asset is nil, use default parameters(this should not happen).
%s SmartSiriVolume configure: %{public}@
%s SmartSiriVolume heartbeat = %{public}lld
%s SmartSiriVolume: estimated noise level %{public}f
%s SmartSiriVolume: estimated LKFS %{public}f
%s SmartSiriVolume: pause SSV calculation.
%s SmartSiriVolume: resume SSV calculation.
%s SmartSiriVolume received VT event!
%s SmartSiriVolume remove samples from VT utterances by %{public}llu, with startAnalyzeSampleCount = %{public}llu, samplesFed = %{public}llu, triggerStartSampleCount = %{public}llu
%s SmartSiriVolume trying to delete too many VT samples, set triggerDurationToDelete to be limited max: %{public}llu
%s SmartSiriVolume got empty VT event!
%s SmartSiriVolume dismiss alarm firing as VoiceTrigger detected.
%s SmartSiriVolume dismiss timer firing as VoiceTrigger detected.
%s SmartSiriVolume: final estimated TTS volume in dB %{public}f
%s SmartSiriVolume: adjust TTS volume since alarm/timer is firing.
%s SmartSiriVolume: TTS volume in dB from noise %{public}f, from LKFS %{public}f, with user offset %{public}f
%s SmartSiriVolume: soft volume algorithm in use
%s SmartSiriVolume: pause LKFS calculation according to MediaRemote notification.
%s SmartSiriVolume: resume LKFS calculation according to MediaRemote notification.
%s SmartSiriVolume received unknown media playing state, let's assume media is playing.
%s SmartSiriVolume received unknown alarm state, let's reset alarm state.
%s SmartSiriVolume: alarm firing status = %@ according to MobileTimer notification.
%s SmartSiriVolume received unknown timer state, let's reset timer state.
%s SmartSiriVolume: timer firing status = %@ according to MobileTimer notification.
%s SmartSiriVolume: set StartAnalyzeSampleCount = %{public}lld
%s Couldn't find keychain value %@ for account %@ %{public}d
%s ::: Error reading file %@, err: %d
%s CSAudioFileReader requires prepare recording settings to feed audio
%s CSAudioFileReader only support LinearPCM to feed
%s Setting ExtAudioFileSetProperty failed : %d
%s Starting audio file feed timer, bufferDuration = %f sampleRate = %f, bytesPerFrame = %d, channelsPerFrame = %d
%s ::: Error reading data from audio file : %d
%s Reach to EOF, chunkSize = %d
%s Stopping audio file feed timer
%s Cannot create SampleRateConverter using AudioConverterNew : %{public}d
%s Cannot set Quality property to audioConverter
%s Cannot set Complexity property to audioConverter
%s Audio resampling done : %lu
%s AudioConverter is sad: 0x%{public}xd
%s xpc object string return nil
%s xpc object should be XPC_TYPE_STRING
%s Siri language is nil, falling back to %@
%s Locale: [%{public}@]
%s No locale set when creating phrase spotter.
%s Creation of Keyword Detector failed.
%s %s async called
%s %{public}s Called
%s %{public}s async called
%s Called before completion called
%s BEGIN num:%{public}ld use:%{public}d
%s AudioSession setup failed
%s Has wrong audio routing, ask user to unplug headset
%s Start Audio Session failed
%s _sessionNumber [%{public}ld]
%s %{public}s Canceling Training
%s Called with status : %{public}d
%s %{public}s called
%s AudioSession StartRecording Failed
%s Setting suspendAudio:[%{public}d]
%s Resume training
%s Suspend training
%s Stop Listening
%s ERR: Failed to save explicit utterance
%s Cannot remove model directory(%@) : %@
%s Cannot remove utterance directory(%@) : %@
%s getCoreSpeechXPCConnection Invalidated
%s Asking current VoiceTrigger aset for %{public}d.%{public}d
%s Asking keyword language given Jarvis language list %{public}@, jarvis-selected language: %{public}@
%s CSCoreSpeechServices Invalidated
%s Request updated SAT audio succeed.
%s Request updated SAT audio failed.
%s Siri is enabled, let start!!
%s Siri is not enabled yet we are keep waiting
%s Cannot get a VoiceTrigger asset : %{public}@
%s CSVoiceTriggerAsset found: %{public}@
%s speechController = %{public}p
%s event : %{public}@
%s context = %{public}@
%s Creating new CSAudioRecorder with context : %{public}@
%s Cannot create audio recorder : %{public}@
%s It cannot change context because it is recording
%s Cannot change context : %{public}@
%s recordingSettings = %{public}@, %{public}d
%s AVVC already recording.
%s setRecordModeToRecordingDelay = %{public}f
%s timeIntervalSinceLastTriggerEnd = %{public}f
%s Failed SetRecordModeToRecording with %{public}f seconds delay from prepareRecorder due to error %{public}@.
%s Finished SetRecordModeToRecording with %{public}f seconds delay from prepareRecorder.
%s Cancelled SetRecordModeToRecording with %{public}f seconds delay from prepareRecorder.
%s Scheduled SetRecordModeToRecording with %{public}f seconds delay in prepareRecorder.
%s AVVC already recording, change record mode to recording here.
%s AVVC already recording, nothing to prepare
%s AVVC Prepare recording failed : %{public}@
%s recordingSettings : %{public}@
%s AVVC Prepare Listening failed : %{public}@
%s context : %{public}@
%s Cannot set context since mediaserverd is recovering from crash
%s Cannot prepare since mediaserverd is recovering from crash
%s Cannot prepare since audio recorder was not initialized
%s recordingSettings from CS : %{public}@
%s settings : %{public}@
%s startRecording failed : %{public}@
%s startListening failed : %{public}@
%s startListening with settings %{public}@ failed : %{public}@
%s mode : %{public}ld
%s Creating fake session activation notification for recording mode
%s setRecordMode failed : %{public}@
%s mode : %{public}ld, delay : %{public}.3f
%s Not supported in this platform
%s Delayed SetRecordModeToRecording: Consumed token %{public}@ with %{public}f seconds delay for reason %{public}@.
%s Delayed SetRecordModeToRecording: Setting record mode to recording for reason %{public}@.
%s Delayed SetRecordModeToRecording: Failed to set record mode to recording for reason %{public}@ due to error %@.
%s Delayed SetRecordModeToRecording: Successfully set record mode to recording for reason %{public}@.
%s Delayed SetRecordModeToRecording: Ignored set record mode to recording for reason %{public}@ because the validator rejected.
%s Delayed SetRecordModeToRecording: Ignored set record mode to recording for reason %{public}@ because the scheduled token %{public}@ does not match the current token %{public}@.
%s Delayed SetRecordModeToRecording: Scheduled new token %{public}@ with %{public}f seconds delay for reason %{public}@.
%s Delayed SetRecordModeToRecording: Cancelled token %{public}@ for reason %{public}@.
%s Delayed SetRecordModeToRecording: Consumed token %{public}@ in advance for reason %{public}@.
%s Activate Context = %{public}@
%s setCurrentContext failed : %{public}@
%s sessionOptions : %{public}tu
%s Expect bluetooth device first pass trigger event
%s destroy AVVC if it has failed startListening for bluetooth device VoiceTrigger
%s Cannot handle bluetooth device trigger event in current state : %{public}@
%s Expect switch AOP to AP event
%s Cannot handle switch AOP to AP event in current state : %{public}@
%s Expect Jarvis firstpass trigger event
%s startRecordingBy %{public}@, currentState: %{public}@, settings: %{public}@
%s start recording since mediaserverd is recovering from crash
%s _lastForwardedSampleCount = %{public}tu, audioBufferSampleCount = %{public}tu
%s _lastForwardedSampleCounts = %{public}tu, audioBufferSampleCount = %{public}tu
%s Failed SetRecordModeToRecording with %{public}f seconds delay from startRecording due to error %{public}@.
%s Finished SetRecordModeToRecording with %{public}f seconds delay from startRecording.
%s Cancelled SetRecordModeToRecording with %{public}f seconds delay from startRecording.
%s Scheduled SetRecordModeToRecording with %{public}f seconds delay in startRecording.
%s Try start recording under PollingListening state.
%s Try start recording under Stopping state.
%s deviceID is required for bluetooth voice trigger listening
%s Ignore unknown event %{public}@
%s Failed to set current context %{public}@.
%s Failed to set prepare recorder %{public}@.
%s Failed to start recording %{public}@.
%s Requested startHostTime = %{public}llu, _clientStartSampleCount = %{public}tu
%s Cannot use opportunisticZLL since _clientStartSampleCount is newer than audioBuffer sample count
%s Opportunistic ZLL will be used starting from : %{public}tu
%s Opportunistic ZLL will not be used : %{public}tu
%s Generating fake didStartRecording delegate
%s stopRecording by %{public}@, currentState: %{public}@
%s Generating fake didStopRecording delegate
%s AudioRecorder lost mediaserverd connection
%s Mediaserverd recovered from crash
%s from:%{public}@ to:%{public}@ by:%{public}@
%s We do nothing for transition between %{public}@ and %{public}@
%s Ignore event(%{public}@) from(%{public}@) since we don't have transition
%s Trying to startListening
%s _createRecorderWithContextIfNeeded failed, it will try again %{public}f seconds later
%s _prepareListenWithSettings failed, it will try again %{public}f seconds later
%s startListening failed, it will try again %{public}f seconds later
%s Listen polling is already started, ignore startListenPolling request.
%s No listen polling timer is on, ignore stopListenPolling request.
%s Still recording, let's do not destroy.
%s ClientSpeechController is nil
%s ignore because lastForwardedSampleCount:%{public}lu, theMostRecentSampleCount:%{public}lu
%s Buffer underrun!!!!, lastForwardedSampleTime:%{public}lu, oldestSampleTimeInBuffer:%{public}lu
%s %{public}@, sucessfully:%{public}@ error:%{public}@
%s forward to speechManagerDidStartForwarding
%s listen succeed under %{public}@, going to stop recording
%s startListening succeed
%s didStart failed, it will try again %{public}f seconds later
%s ignore audioRecorderDidStartRecording
%s forReason : %{public}ld
%s forward to speechManagerDidStopForwarding
%s ignore audioRecorderDidStopRecording
%s Changing context to %{public}@ after receiving didStop
%s We need to change context once we received didStop, however, pending context is nil
%s toConfiguration: %{public}ld
%s type: %{public}d, error: %{public}@
%s context: %{public}@
%s active : %{public}d
%s Ignore session active notification
%s Language Code Changed : %{public}@
%s SmartSiriVolume: final estimated TTS volume %{public}f with music volume %{public}f
%s Wrongly called: SmartSiriVolume is not supported on this device type.
%s We stop listen polling since we anyway going to stop
%s Trying to start clear logging files
%s Clear logging file timer is already started, ignore startClearLoggingFilesTimer request.
%s SpkrId:: No Implicit training on Horseman - Bailing out
%s VoiceTriggerEventInfo is nil - Bailing out
%s Received implicit utterance for %{public}@ from %{public}@ with context %d
%s %@
%s ERR: SAT did not trigger!!! - Bailing out
%s Filled up tdti and ti buckets - Bailing out
%s Setting payloadstartSample %lu for trigger duration of %@secs
%s Failed to read file: %@
%s EOF: utteranceLength: %lums, tdtiLength: %lums tiLength: %lums tdtiDiscardedLength: %lums tiDiscardedLength: %lums
%s Saved tdti implicit utterance %{public}@
%s Discard too short TI utterance tiLength: %lums tiDiscardedLength: %lums
%s ERR: Failed to delete file %{public}@ with erorr %{public}@
%s Saved ti implicit utterance %{public}@
%s SpkrId:: Invalid chunk! chunk=%lu
%s SpkrId:: Current Locale is nil - Bailing out
%s SpkrId:: AudioCache path is nil - Bailing out
%s SpkrId:: Exceeded maximum number of audio cache files (%ld) - Bailing out
%s SpkrId:: VoiceTrigger info unavailable for Hey Siri invocation - using whole utterance
%s Implicit recording not in progress, returning
%s ERR: AudioCache path is nil - Bailing out
%s SpkrId:: Implicit utterance(%@) with samples %lu (%1.2fs), discarded samples %lu (%1.2fs), extra samples %lu, trigger duration %1.2fs
%s ERR: fetching contents of %{public}@ failed with error %{public}@
%s Biometric match happened in last %f secs
%s Biometric match result: %@ happened in last %f secs
%s No biometric information available
%s Tagging as handheld as user interacted in last %f secs
%s Tagging as farfield as last user interaction %f secs back
%s Tagging as FarField as user dismissed
%s init-_currentLanguageCode: %{public}@
%s assetManagerEnabledPolicy is already enabled, fetching remote meta data now
%s Not able to fetch remote meta now, registering for callback
%s Asset Manager Policy has been enabled, try to fetch remote meta now
%s Cannot fetch VoiceTrigger asset meta data
%s _currentLanguageCode changed: %{public}@
%s returning status to UI : %{public}d
%s Already reported status or no callback
%s Will suspend training
%s Will resume training
%s Decide to delay ending ASR: [%{public}ld] samples
%s Triggered! Event info: %{public}@
%{public}9lld %{public}9lld %{public}9lld
%s analyzing.... score so far: %{public}5.3f
%s feeding tailing: [%{public}ld] samples
%s correctSampleSize:    [%{public}ld]
%s accumSampleSize:      [%{public}ld]
%s startBufferIndex:     [%{public}ld]
%s startBufferSampleSize:[%{public}ld]
%s samplesToBeDeleted:   [%{public}ld]
%s Total Number of buffs:[%{public}ld]
%s Adjusting the start buffer
%s Adjusting the array elements
%s Unsupported
%s Begin of speech detected
%s End of speech detected with endpoint type: %{public}ld
%s %{public}s CALLED
%s Master Timeout Fired
%s recognized text = %{public}@
%s Context : %{public}@
%s Resetting CoreSpeech frameworks
%s Ask start recording from: %{public}tu
%s Voice trigger to use the current voice triggered channel: %{public}tu
%s Auto prompt to use the last voice triggered channel: %{public}tu
%s SpeechController to receive data from default channel
%s SpeechController to receive data from channel %{public}tu
%s Start recording invoked too late (%{public}.3f seconds), override scheduledCheckTime: %{public}llu to currentTime: %{public}llu
%s Scheduled audible feedback decision after %{public}.3fseconds (vtEndMachTime: %{public}llu currentMachTime: %{public}llu)
%s Two shot audible feedback decision not needed since we already stopped recording
%s Two shot audible feedback decision (%{public}.3fs later than the scheduled time), elapsedTimeWithNoSpeech: %{public}.3f
%s Two shot audible feedback is needed, should notify? [%{public}@]
%s Two shot audible feedback decision timed out while waiting for Myriad decision
%s Phatic not needed since we already stopped recording
%s Phatic decision elapsedTimeWithNoSpeech: %{public}.3f
%s Notifying scheduled phatic playback...
%s Failed to playback phatic, error: %{public}@
%s Ask delay audio session active by %{public}f seconds
%s SpeechManager still forwarding audio after didStopForwarding, we shouldn't have this
%s Mute endpointing as SpeakerId parallel recording triggered
%s AVVCAudioBuffer contains %{public}d packet descriptions, size %{public}d, channels %{public}d. Ignoring
%s packetCount %{public}d
%s Bad packet length %{public}d. Skipping rest of record buffer.
%s SmartSiriVolume update reason: %lu
%s SpeechController is trying to forward encoded audio after didStopForwarding, we shouldn't have this
%s Not available
%s Endpointer is not hybrid endpoint, we should rely on CVT for two shot beep
%s Not playing two shot feedback since this is horseman, Phatic will be played
%s Not playing two shot feedback since this is hearst or javis, Phatic will be played
%s Cannot play Phatic since music is being played now
%s Two shot is detected at time %{public}.3f, should notify? [%{public}@]
%s Requesting QuickStop operation upon detecting keyword
%s Received Myriad started
%s Received Myriad finished with decision: %tu
%s Received unknown media playing state, ignoring
%s Received unknown alarm playing state, ignoring
%s Received unknown timer playing state, ignoring
%s Detected sound is%{public}@ playing: media(%d) alarm(%d) timer(%d)
%s SpkrId:: SpeakerIdInfo from incorrect SpeakerRecognizer: expected: %@, spkrRecognizer: %@
%s CS doesn't have ndblobbuilder!
%s Fired VoiceTrigger Timeout
%s Stop right now since ASR has issue
%s EOS Timeout Fired
%s Force endpoint fired
%s Discarding surplus audio of %{public}lu samples
%s AudioSession Started
%s AudioSession Stopped
%s unknown endpoint type
%s Called with status : %{public}d and success : %{public}d utteranceStored : %{public}d
%s ERR: Failed to store utterance, overiding status
%s Non Final: [%{public}@]
%s NON Final Matching
%s Final: [%{public}@]
%s Final Matching
%s Final Not Matching
%s SPEECH RECOGNITION TASK FINISH UNSUCCESSFULLY
%s %{public}@ %{public}@ %{public}ld %{public}@
%s Recog Result: [%{public}ld]
%s Couldn't create speech log directory at path %{public}@ %{public}@
%s Called Obsolete API, returning nil
%s Called Obsolete API, returning
%s PHS update directory already exists, remove before we move forward
%s Failed to delete PHS update directory
%s Failed to create PHS update directory
%s Failed to get device hash list %{public}@
%s Processing sync data from device hash: %{public}@
%s Error to copy profile from %{public}@ to %{public}@, error: %{public}@
%s Copied %@ to %@
%s Skipping marking enrollment success for language %{public}@ with error %{public}@
%s language: %{public}@, enableVTAfterSyncLanguage: %{public}@, currSiriLanguage: %{public}@
%s Enabling VoiceTrigger Upon VoiceProfile sync for language: %{public}@ and enrolled language: %{public}@
%s VoiceTrigger does not exist for this platform, not setting VoiceTriggerEnabled
%s Not enabling VoiceTrigger Upon VoiceProfile sync for language: %{public}@
%s Sucessfully migrated language %{public}@
%s Migrated language %{public}@ but failed to mark SAT enrollment
%s Sucessfully marked as migrated for language : %{public}@
%s Failed to mark migrated for language : %{public}@
%s Failed to remove update path [%{public}@] upon migration completion, error: %{public}@
%s Coudn't mark SAT enrollment %{public}@ at path %{public}@
%s Marked SAT enrollment %{public}@ at path %{public}@
%s We can't mark SAT {public}%@ when there is no audio directory
%s ERR: Failed in marking Enrollment as Successful
%s Skipping uploading legacy version (%lu) of voice profile, current version %d
%s Cannot create %{public}@ with error %{public}@ - Skipping language
%s Cannot create directory(%{public}@)
%s Skipping audiocache file %@
%s Cannot copy file %{public}@ to %{public}@ with error %{public}@
%s Upload of voice profile at %{public}@ completed with error %{public}@
%s Upload of Voice Profiles completed with error %@
%s Cannot delete existing SATUpload Diretory : %{public}@
%s %{public}@
%s SAT download path is nil - Bailing out
%s Download for %{public}@ failed with %{public}@
%s ERR - Received profile with mismatch product category - Skipping
%s enableAudioInection: is only available on internal builds
%s setAudioInjectionFilePath: is only available on internal builds
%s kCSAudioInjectionFilePathKey is not array type
%s kCSAudioInjectionFilePathKey array size = %d
%s kCSAudioInjectionFilePathKey doesn't have NSString as an array entry
%s ::: Error creating output file %{public}@, err: %{public}d
%s ::: Error writing to output wave file. : %{public}ld
%s Failure disposing audio file %{public}d
%s Audio file already configured, closing first
%s Creating audio file at URL %{public}@
%s Failed creating audio file at url %{public}@ %{public}d
%s Error setting input format %{public}d
%s No audio file to append data
%s Failed writing audio file %{public}d
%s Closing file at URL %{public}@, audio size: %{public}u
%s Cannot setAlertSoundFromURL since mediaserverd is recovering
%s Cannot playAlertSoundForType since mediaserverd is recovering
%s Cannot playRecordStartingAlertAndResetEndpointer since mediaserverd is recovering
%s CSAssetController cannot query for nil language
%s Error running asset-query for assetType:%{public}lu, query: %{public}@, predicate: %{public}@, error: %{public}@
%s ::: found %{public}lu assets for assetType=%{public}lu, matching query: %{public}@
%s ::: %{public}s
%s ::: predicate: %{public}@
%s ::: %{public}s; query: %{public}@
%s Error running asset query: error %{public}@
%s ::: Request Fetching RemoteMetaData : assetType : %{public}d
%s ::: Request fetching remote asset
%s ::: Fetching remote asset
%s ::: Purging installed asset : %{public}@
%s ::: Request downloading remote asset
%s ::: Start downloading asset
%s ::: download progress: %{public}3.0f%%
%s ::: Error downloading; %{public}@
%s ::: download completed successfully.
%s Attempting to download asset %{public}@
%s Failure resuming paused voice asset %{public}@
%s Asset doesn't need downloading, invoking completion
%s ERR: Unknown AssetType: %{public}lu
%s SpkrId:: Unknown CSSpIdType string: %@
%s SpkrId:: ERR: spIdRootDirForLocale called with locale=nil
%s SpkrId:: Incorrect usage of API - Bailing out
%s SpkrId:: path is nil - Bailing out
%s SpkrId:: Direntry with same name exists, this will be removed: %@
%s SpkrId:: Creating Directory : %@
%s SpkrId:: Creating Directory failed : %@
%s SpkrId:: Exceeded privacy limit for grading utterances : %ld (%d)
%s SpkrId:: SpeakerId Assets missing at %@
%s SpkrId:: Could not read existing %@ file: err: %@
%s SpkrId:: ERR: Could not read existing %@ file: err: %@
%s SpkrId:: ERR: %@ is a directory
%s SpkrId:: ERR: Could not find version file - %@
%s ERR: Failed to delete %{public}@ with error %{public}@
%s Successfully moved %{public}@ to %{public}@
%s ERR: Failed to move %{public}@ to %{public}@ with error %{public}@
%s Skipping moving of file %{public}@
%s Coudn't fetch the list of files at path: %{public}@ %{public}@
%s Migrating Voice Profile for %{public}@ from %lu to %lu not supported
%s ERR: Unknown device-category for device: %{public}@, languageCode: %{public}@
%s Could not read existing %@ file: err: %@
%s ERR: error creating updatedVoiceProfileJsonData from: %@, err: %@
%s ERR: error removing voice profile version file at: %@, err: %@
%s ERR: Error writing voice profile version file at: %@, err:%@
%s Unknown Device category for deviceProduceType: %@
%s ERR: Unknown device. returning false: %{public}@
%s ERR: satLanguagePath is nil. returning false
%s Voice Profile Mismatch - CurrentDeviceCategory %@ VoiceProfileCategory %@
%s Malformed audio-dir URL for string <%{public}@>:url
%s ERR: reading contents of audioDir: %{public}@
%s No jsonFiles found in %{public}@: jsonFiles.count=%{public}lu
%s Unexpected: empty JSON data for file: %{public}@
%s Error reading metaDict at path: %{public}@
%s metaProductType: %{public}@
%s vtprofile: currDevice=[%{public}@:%{public}@] ; vpDirDevice=[%{public}@:%{public}@]
%s VoiceProfile MATCH
%s VoiceProfile MIS-MATCH
%s Could not find productType in VT-Meta file, trying next one
%s No compatible VT profile found for CurrDevice: %{public}@
%s Received Jarvis route change notification
%s Stop monitoring : AudioRouteChangeMonitor
%s Notifying Jarvis Connection State : %{public}d
%s SmartSiriVolume cannot be resumed since Siri is speaking
%s Start monitoring : speech endpoint asset meta update
%s Stop monitoring : speech endpoint asset meta update
%s New speech endpoint asset is available
%s Testing [%@] against regex.
%s Failed to write to: %{public}@. err: %{public}@
%s unbalanced dispatch_group_enter and leave : ignore we are ignore dispatch_group_leave
%s Non internal build, Ignoring command %@ from peerId %@ - Bailing out!
%s Received Malformed command %@ from peerId %@ - Bailing out!
%s Command %@ received from peerId %@
%s Unknown Command: (%@) - Ignoring
%s Triggering sync with peer - %@
%s Triggering nearmiss sync with peer - %@
%s CSP2P_RemoteHeySiriCmd: ENABLE HeySiri: Not Implemented Yet: 
%s CSP2P_RemoteHeySiriCmd: DISABLE HeySiri: Not Implemented Yet: 
%s CSP2P_ParallelRecordingCmd: received malformed command -                         CSP2P_VoiceProfileProfileId_Key - %@                                              CSP2P_VoiceProfileSpeakerName_Key - %@ 
%s CSP2P_ParallelRecordingCmd: received command when speakerId is disabled - Bailing out
%s CSP2P_ParallelRecordingCmd: SpId Assets are not available - Bailing out
%s CSP2P_ParallelRecordingCmd: START RECORDING
%s CSP2P_ParallelRecordingCmd: Failed to set the recording context
%s CSP2P_ParallelRecordingCmd: Failed to prepare recording
%s CSP2P_ParallelRecordingCmd: Failed to start recording
%s CSP2P_ParallelRecordingCmd: STOP RECORDING
%s Cannot read contents of directory: %@, err: %@
%s Could not determine if [%@] is a directory or not. Err=%@
%s Found dir: %@. Skipping compression
%s _compressFilesInDirectory: Malloc failed for file %@ (%lu) - Discarding
%s _compressFilesInDirectory: Compression failed for file %@ (%lu) - Sending Uncompressed
%s _compressFilesInDirectory: File %@ compressed from %ld to %ld 
%s CSP2P_VoiceProfileParallelRecordingsFetchCmd: Cannot send VoiceProfile when _adCompanionServiceProvider is nil - returning
%s CSP2P_VoiceProfileParallelRecordingsFetchCmd: Message incomplete - Bailing out %@
%s CSP2P_VoiceProfileParallelRecordingsFetchCmd: File %@ isCompressed: %d, compressedSize: %ld, err: %@ 
%s CSP2P_VoiceProfileParallelRecordingsFetchCmd: Failed VoiceProfileTransfer: %@, error %@
%s Failed to remove the file %@
%s transfering file %@ isCompressed: %d, compressedSize: %ld, err:%@
%s Failed moving file from %@ to %@ with error %@
%s %@ is nil - Bailing out
%s Failed in transporting Voice file %@ with reponse: %@, error %@
%s Failed to remove the file %@ with error %@
%s Failed to move the file %@ to %@ with error %@
%s CSP2P_VoiceProfileParallelRecordingTransferCmd: received malformed command - %@ %@ %@
%s CSP2P_VoiceProfileParallelRecordingTransferCmd: unknown IDS peer with passed Identifier %@, %@ %@
%s CSP2P_VoiceProfileParallelRecordingTransferCmd: received malformed command - %@
%s CSP2P_VoiceProfileParallelRecordingTransferCmd: Creating directory failed with error %@
%s Syncing parallel recorded audio file - %@ from %@
%s Uncompressed file %@ sent by peer %@
%s ERR: Failed to allocate buffer of size %zu, bailing out
%s Writing to file(%@) failed!. Err=%@
%s received malformed command - %@ %@ %@
%s unknown IDS peer with passed Identifier %@, %@ %@
%s received malformed command - %@
%s Ignoring sync of existing file %@ from %@
%s Syncing audio file - %@ from %@
%s CSP2P_VoiceProfileTransferCmd: received malformed command - %@ %@ %@
%s CSP2P_VoiceProfileTransferCmd: received malformed command: CSP2P_VoiceProfileData_Key: %@CSP2P_VoiceProfileFileName_Key: %@CSP2P_VoiceProfileSpeakerName_Key: %@CSP2P_VoiceProfileLocale_Key: %@CSP2P_VoiceProfileDataType_Key: %@CSP2P_VoiceProfileTotalSegments_Key: %@CSP2P_VoiceProfileSegment_Key: %@
%s CSP2P_VoiceProfileTransferCmd: received command when speakerId is disabled - Bailing out
%s CSP2P_VoiceProfileTransferCmd: SpId Assets are not available - Bailing out
%s CSP2P_VoiceProfileTransferCmd: Received VoiceProfile Segment (%@/%@) from peerId %@
%s CSP2P_VoiceProfileTransferCmd: Failed to delete the directory %@ with error %@
%s CSP2P_VoiceProfileTransferCmd: received VoiceProfileSegment %@, expected %d
%s CSP2P_VoiceProfileTransferCmd: Creating directory failed with error %@
%s CSP2P_VoiceProfileTransferCmd: Writing to file failed!!!
%s Trigger Voice training with import Dir %@
%s Import finished with result: %d, err: %@
%s Received request to delete VoiceProfile %@ from peerId %@
%s Delete finished with result: %d, err: %@
%s Cannot send data across when _adCompanionServiceProvider is nil - returning
%s ERR: %{public}@
%s NDAPI initialization failed
%s set StartAnalyzeSampleCount = %{public}lld
%s NDAPI config doesn't contain threshold_normal
%s NDAPI config doesn't contain threshold_logging
%s Couldn't create CoreSpeech log directory at path %{public}@ %{public}@
%s Start monitoring : SACInfo
%s Stop monitoring : SACInfo
%s Device is in stereo mode : %{public}@
%s Dealloc of CSRemoteControlClient, it should close connection
%s VoiceTrigger is already %{public}@, received duplicated notification!
%s Start monitring : VoiceTrigger setting switch
%s Cannot start monitoring VoiceTrigger setting switch because it was already started
%s Stop monitring : VoiceTrigger setting switch
%s VoiceTrigger enabled = %{public}@
%s Creating new CSAudioRecorder with context : %@
%s AudioRecorder creation failed : %@
%s Cannot prepare since audio recorder does not exist
%s AudioRecorder is already recording, do not prepare anymore
%s Cannot prepareRecordWithSettings : %@
%s Start monitoring : Siri setting switch, Siri is %{public}@
%s Stop monitoring : Siri setting switch
%s Siri Enabled = %{public}@
%s BeepCanceller asset file loading from : %{public}@
%s Could not read beep file: %@
%s beepVector Size = %{public}lu
%s Cannot initialize beep canceller
%s Beep canceller initialized with maxNumSamples = %{public}d
%s It will beep now
%s Reset beep cancellation
%s Vault already exists and is NOT secure. Attempting to secure: %{public}@
%s Success setting up DataVault at: %{public}@
%s Received Activation Event : %{public}@
%s Returning error for already existing pending activation event : %{public}@
%s No delegate registered : Postpone activation event handling until we have delegate registered
%s Pending Timeout fired for %{public}@ returning error for timeout
%s There is no pending activation event to timeout
%s Cannot handle activation event : %{public}@
%s Found pending activation : %{public}@, handle pending activation immediately
%s AOP First Pass trigger detected
%s Error reading audio file: %{public}d, skipping...
%s AssetManager cannot be turned on since springBoard is not started
%s AssetManager cannot be turned on since isFirstUnlocked is NO
%s AssetManager cannot be turned on since network is not available
%s numChannels: %{public}lu, recordingDuration: %{public}f, sampleRate: %{public}f
%s Cannot copy samples since this is empty
%s Could NOT copyFrom: %{public}lu to: %{public}lu, retSampleCount: %{public}lu
%s copyBuffer: oldestSample: %{public}lu latestSample: %{public}lu, numSamplesCopied: %{public}lu
%s CSAudioCircularBuffer.reset
%s saveRecordingBufferFrom: %{public}lu to: %{public}lu toURL: %{public}@
%s csrb: %{public}@
%s Invalid request: (%{public}lu, %{public}lu): noting to write to file
%s Invalid request: reqStartSample=%{public}lu, reqEndSample=%{public}lu, oldestSampleInBuffer: %{public}lu, latestSampleInBuffer=%{public}lu
%s Start monitoring : VoiceTrigger Asset meta update
%s Stop monitoring : VoiceTrigger Asset meta update
%s New VoiceTrigger asset metadata is available
%s Error initializing voice controller with context %@ %@
%s Trying to set record buffer duration to %lf
%s Failed setting record buffer duration. Duration is %lf
%s AVVC startRecordingWithSettings failed.
%s AVVC Stop Recording
%s audioInput:[%@]
%s No Reocrd Route detected
%s audioOutput:[%@]
%s ERR: FilePath is nil - Bailing out
%s ERR: Training utterance doesnt exist at %@ - Bailing out
%s ERR: Training utterance is marked as directory at %@ - Bailing out
%s VAD2 preheat...
%s CSVAD2EndpointAnalyzer: resetForNewRequestWithSampleRate
%s ERR: Deprecated VAD2 reset called
%s %{public}@ Resetting with style %{public}ld, _samplesSeen: %{public}f, newSampleRate: %{public}tu, _sampleRate: %{public}f
%s _audioUnitEPVAD2=%{public}p, auNeedsReset: %{public}d
%s Failed to reset EPVAD2: %{public}d
%s vtEndInSecs: %{public}f, _vtEndInSampleCount: %{public}lu, voiceTriggerInfo: %{public}@,
%s VAD2::RecordingDidStop: Ignoring processAudioSamplesAsynchronously, not queueing
%s VAD2::RecordingDidStop: Ignoring processAudioSamplesAsynchronously from async
%s Already communicated endpoint...returning
%s VAD2::RecordingDidStop: Ignoring _processAudioSamples
%s Empty samplesBuffer!
%s Received audio buffer with 8 frames of zeroes
%s Not configured
%s done: %{public}d, _detectedOneShotStartpoint: %{public}d, _communicatedEndpointDetection: %{public}d, _startWaitTime: %{public}f_samplesSeen: %{public}f, _delay: %{public}f, _sampleRate: %{public}f(_startWaitTime + _delay) * _sampleRate): %{public}f, (_samplesSeen / _sampleRate): %{public}f, _automaticEndpointingSuspensionEndTime: %{public}f
%s No startpoint detected after %{public}f, timing out, _samplesSeen: %f, _samplesSeen(ms): %f, _lastOneShotEndpoint: %f
%s Ignoring recurrent endpoint at %{public}f becuase it's too early (< %{public}f)
%s Fell back to recurrent endpoint (%{public}f) because one-shot is too early (%{public}f < %{public}f)
%s Fell back to recurrent endpoint, _samplesSeen: %f, _samplesSeen(ms): %f, _lastOneShotEndpoint: %f
%s Reporting one-shot ep:  _samplesSeen: %f, _samplesSeen(ms): %f, _lastOneShotEndpoint: %f
%s sampleRate: %{public}lf frameRate: %{public}d
%s Skipping re-initialization of EPVAD2; no audio consumed yet
%s EPVAD2 reset with existing parameters
%s Could not find endpointer audio unit component
%s AU instantiation error: %{public}d
%s No model available for mode: %{public}d
%s Error reading plist endpoint model at %{public}@
%s Could not set kAUEndpointVADProperty_ViterbiModelData: %{public}d
%s Could not set %{public}@ to %{public}@: %{public}d
%s Could not initialize audio unit: %{public}d
%s CSVAD2Endpointer is configured
%s Unexpected block size of %{public}u, not %{public}u. Skipping this block of audio.
%s Could not process audio via endpointer: %{public}d
%s tuningLibraryPath: %{public}@
%s VAD2-epModelPath: %{public}@
%s Could not read kAUEndpointVAD2Property_LatestEndpointerEventTimeSeconds: %{public}d
%s Found one shot startpoint at %{public}.3f seconds
%s Found one shot endpoint at %{public}.3f seconds
%s Could not read kAUEndpointVAD2Property_LatestRecurrentVADEventTimeSeconds: %{public}d
%s Found recurrent startpoint at %{public}.3f seconds
%s Found recurrent endpoint at %{public}.3f seconds
%s xpc object should be XPC_TYPE_ARRAY
%s xpcObject value is NULL
%s Cannot decode non-plist types of XPC object
%s Cannot encode non-plist types into XPC object : %{public}@
%s Cannot generate subChunk since channel(%{public}tu) is larger than number of channels(%{public}tu)
%s Cannot generate subChunk if it reuqest more than it has : %{public}tu %{public}tu %{public}tu
%s SpkrId:: totalNumSamplesReceived(%lu) > maxSamplesToSkip(%lu), Not Skipping any samples
%s SpkrId:: Processing ended at: numSamplesProcessed=%lu, totalSampleCountToReach=%lu
%s Failed to get HEP asset: %{public}@
%s HEP Asset: %{public}@, path: %{public}@
%s HEP::RecordingDidStop: Ignoring processAudioSamplesAsynchronously: Not queueing
%s HEP::RecordingDidStop: Ignoring processAudioSamplesAsynchronously
%s addAudio first sample offset: %{public}lu
%s Updated endpointer threshold: %{public}f
%s Updated endpointer delayed trigger: %{public}d
%s EARSPG: CSServerEndpointFeatures: %{public}@
%s Accepting RC: RCTime < 0: Server's processedAudioDuration(%{public}f) > _lastReportedEndpointTimeMs(%{public}f): sfLatency: %{public}f, rcTimeMs: %{public}f
%s Rejecting RC: SFLatency < 0: Server's processedAudioDuration(%{public}f): _lastReportedEndpointTimeMs(%{public}f): sfLatency: %{public}f, rcTimeMs: %{public}f
%s rcEpFeatures: %{public}@ shouldAccept: %{public}d
%s HEP::RecordingDidStop: Ignoring silenceScoreEstimateAvailable, Not queuing
%s silposnf=%f, clientProcessedAudioMs: %{public}f, effectiveClientProcessedAudioMs: %{public}lu
%s Detected speech start at %{public}f of effectiveClientProcessedAudioMs
%s HEP::RecordingDidStop: Ignoring silenceScoreEstimateAvailable
%s Already communicated end-pt: Not Invoking hybridClassifier for silposnf=%{public}f
%s ClientLag: serverProcessedAudioMs(%{public}ld) > effectiveClientProcessedAudioMs(%{public}f)
%s ClientLag: Not invoking HybridClassifier: sfLatency > clientLagThreshold: %{public}f > %{public}f
%s ClientLag: Using DefaultServerFeatures with disconnected-state sfLatency: %{public}f
%s ClientLag: Using ServerFeatures with ClampedSFLatency: %{public}f
%s ClientLag: Not Invoking HybridClassifier as serverProcessedAudioMs > effectiveClientProcessedAudioMs
%s EARSPG: Invoking HybridClassifier with parameters: %{public}@ at effectiveClientProcessedAudioMs: %{public}lu,clientSilencePosterior: %{public}f, endpointPosterior: %{public}f, result: %{public}d
%s ServerFeaturesLatencyDistribution: %{public}@ additionalMetrics: %{public}@
%s Already communicated end-pt: Not scheduling work for hybridClassifierQueue for silposnf=%{public}f
%s unsorted-serverFeatureLatencies: %{public}@
%s triggerEndSeconds: %{public}f, _vtEndInSampleCount: %{public}lu, _vtExtraAudioAtStartInMs: %{public}lu,  _hepAudioOriginInMs: %{public}f, voiceTriggerInfo: %{public}@,
%s CSHybridEndpointAnalyzer recordingStoppedForReason: %{public}lu
%s CSHybridEndpointer resetForNewRequestWithSampleRate: %{public}lu
%s CSEndpointAsset exists: %{public}@
%s No asset for CSHybridEndpointer for currentLanguage: %{public}@. Fallback to VAD2
%s Created EARCaesuraSilencePosteriorGenerator: %{public}@
%s Created HybridClassifier(%{public}@); canProcessCurrentRequest after reset: %{public}d,for sampleRate: %{public}lu, lang=%{public}@, version=%{public}@
%s csHepConfig: %{public}@
%s _clientHepConfig: %{public}f, _clampedSFLatencyForClientLag: %{public}f, _useDefaultServerFeaturesOnClientLag: %{public}d
%s language changed to: %{public}@: CSHybridEndpointer new asset: %{public}@
%s new hybrid endpoint asset downloaded, CSHybridEndpointer asset : %{public}@
%s current asset changed to : %{public}@
%s %{public}@ doesnt exist
%s Could not read: %{public}@
%s Could not decode contents of: %{public}@: err: %{public}@
%s Delta is larger than anchorHostTime
%s Delta is larger than anchorSampleCount
%s Not supported on this platform
%s Saving utterance and meta as %{public}@ training.
%s Failed to write utterance into %{public}@
%s Saving %{public}@ at %{public}@ as %{public}@ training.
%s numSamplesToWrite %{public}lu
%s Failed to get CSAudioFileWriter:
%s Failed to addSamples to CSAudioFileWriter: %{public}@
%s Failed to endAudio on CSAudioFileWriter: %{public}@
%s ERR: called with nil metaPath
%s ERR: called with nil uttPath
%s ERR: called with nil uttMeta
%s Cannot create json file : %{public}@
%s Creating new VAD2-EP
%s CSHybridEndpointer canProcessCurrentRequest
%s CSHybridEndpointer can-NOT-ProcessCurrentRequest, fallback  to VAD2
%s _activeEndpointer=%{public}@
%s shouldUseCVT2ShotDecision: %d, isWatchRTSTriggered=%d
%s CVT-2shot: Resetting VAD2 ep
%s CVT-2shot: NOT resetting vad2 ep
%s CVT-2shot: Ignoring non-VAD2 ep
%s endpointer: %{public}@: didDetectStartpointAtTime: %{public}f
%s EP_PROXY::RecordingDidStop: Ignoring startPoint-reporting
%s EP_PROXY::RecordingDidStop: Ignoring didDetectHardpoint-reporting
%s EP_PROXY::RecordingDidStop: Ignoring VAD2 2-shot reporting
%s CSEndpointerProxy didDetectHardEndpoint using for 1-2 shot at Time: %{public}f
%s EP_PROXY::RecordingDidStop: Ignoring didDetectHardEndpointAtTime:
%s %{public}@: Endpointer didDetectHardEndpointAtTime:withMetrics: %{public}f, CallingDelegate: %{public}@
%s CSEndpointerProxy: didDetectHardEndpoint: ep-time: %{public}f, triggerEnd: %{public}f, vad2EndWaitTime: %{public}f, delta: %{public}f, legacyTwoShotThreshold: %{public}f, enterTwoShot: %{public}d
%s WARN: endpointerModelVersion called when CSHybridEndpointer is not available
%s Cannot create NSNumber if xpcObject is NULL
%s XPC object type should be BOOL, DOUBLE, INT64, or UINT64
%s Cannot create xpcObject if objcType is NULL
%s Cannot create xpcObject since there is no matching type
%s Stop monitoring : First unlock
%s Start monitoring : Springboard start
%s Cannot start monitoring Springboard start because it was already started
%s Stop monitoring : Springboard start
%s SpringBoard started = %{public}@
%s Failed to initialize caesuraSPG, stopping monitoring
%s Start monitoring : EARCaesuraSilencePosteriorGenerator: %{public}@
%s deallocating EARCaesuraSilencePosteriorGenerator: %{public}@
%s Stopped monitoring : EARCaesuraSilencePosteriorGenerator
%s EARClientSilenceFeatures heartbeat = %{public}lld,                   silScoreEstimate = %{public}f
%s Transcriber trigger token list: %{public}@
%s Initializing Quasar with config: %{public}@
%s Speech model loading took %{public}.3fms
%s Failed initialization in _EARSpeechRecognizer initWithConfiguration
%s runRecognition failed
%s endAudio failed
%s recognizeWavData failed
%s Partial result confidence: %{public}f
%s ERROR: %{public}s
%s Final result confidence: %{public}f
%s EAR Token[%{public}lu]: %s (%{public}f)
%s network state notify key : %s
%s Start monitoring : network availability
%s Stop monitoring : network availability
%s Network availability changed
%s Fallback asset resource path : %{public}@
%s Cannot find corespeech asset from resourcePath : %{public}@
%s Configuration file is not exists : %{public}@
%s Cannot read configuration file : %{public}@
%s Cannot decode configuration json file : %{public}@
%s Configuration json file is not expected format
%s Cannot access to %{public}@ %{public}@ using default value
%s There is not audio buffer to convert. Skip this.
%s Got asked for %{public}u packets, have %{public}u
%s [%{public}02u of %{public}02u %{public}fs] Opus packet with %u bytes
%s %{public}d bytesConsumed from opus coverter, remains %{public}d bytes
%s Resetting AudioConverter buffer
%s createAudioConverter : initial frames per buffer = dur %{public}.2f * sr %{public}.2f = %{public}u
%s _configureAudioConverter: encoded audio needs minimum of %{public}u bytes per output buffer
%s _configureAudioConverter: AudioConverterGetProperty(kAudioConverterPropertyMinimumOutputBufferSize) returned status %{public}d
%s _configureAudioConverter: final framesPerBuffer: %{public}u
%s _configureAudioConverter: _convertPacketCount: %{public}u
%s _configureAudioConverter: AudioConverterGetProperty(MaximumOutputPacketSize): returned status %{public}d
%s createAudioConverter: outputSizePerPacket: %{public}u
%s _configureAudioConverter: _convertAudioCapacity %{public}u bytes
%s Cannot create AudioConverter using AudioConverterNew : %{public}u
%s Cannot set encoder bit rate : %{public}u
%s Start monitoring : Siri language code
%s Stop monitoring : Siri language code
%s Siri language changed to : %{public}@
%s Ignore notifying change of language code, since it is nil
%s xpc object should be XPC_TYPE_DICTIONARY
%s xpcObject key or value is NULL
%s Cannot encode key into xpcobject since the key is not NSString class type
%s SAT successfully initialized : %{public}@
%s SAT Score = %{public}f, threshold = %{public}f
%s Cannot create CSVTUIKeywordDetector since there is no asset available
%s Cannot create CSVTUIKeywordDetector since we cannot initialize NDAPI
%s AVVC initialization failed
%s Successfully create AVVC : %{public}p
%s Trying to set record buffer duration : %{public}lf
%s Failed setting record buffer duration. Duration is %{public}lf
%s Creating beep canceller...
%s Calling AVVC prepareRecordWithSettings : %{public}@
%s Creating SampleRateConverter
%s Calling AVVC setCurrentContext : %{public}@
%s Calling AVVC prewarmAudioSession
%s Calling AVVC releaseAudioSession : %{public}tu
%s Should not call setDuckOthersOptions with NO in B238
%s %{public}@ miniDucking now
%s zeroFilterWinSz: %{public}tu, numHostTicksPerAudioSample: %{public}f
%s _vtEndInSampleCount:%{public}ld, _numSamplesProcessed: %{public}ld, vtInfo: %{public}@
%s ::: CSAudioRecord will inject audio file instead of recording
%s Resetting AudioFilePathIndex
%s Increase AudioFilePathIndex = %d
%s AudioFilePathIndex is out-of-boundary _audioFilePathIndex:%d injectAudioFilePaths:%d
%s AudioFilePathIndex:%d accessing:%@
%s Unable to find injectAudioFilePath = %@
%s Resetting ZeroFilter
%s Calling AVVC startRecordingWithSettings : %{public}@
%s Calling AVVC startRecording
%s Calling AVVC stopRecording
%s no operation : setRecordMode is not implemented in AVVC
%s Sampling rate = %{public}f
%s AVVC sampling rate = %{public}f
%s AVVC doesn't return sampleRate, assume it is default sample rate
%s Cannot handle audio buffer : unexpected format(%{public}u)
%s Calling AVVC playAlertSoundForType to play alert
%s Calling AVVC playAlertSoundsForType : %{public}ld
%s Zero Filter Metrics: %@
%s Beep Canceller Metrics : %@
%s successfully : %{public}d, error : %{public}@
%s toConfiguration : %{public}d
%s type : %{public}d, error : %{public}@
%s withContext : %{public}@
%s activate : %{public}d
%s AVVC lost mediaserverd connection
%s AVVC informed mediaserverd reset, no further action required
%s Failed to deinterleave the data: %{public}d
%s Cannot create de-interleaver using AudioConverterNew: %{public}d
%s Created de-interleaver
%s Created narrowBandToWidBandConverter
%s Creating Audio Power Meter with record route %{public}@
%s We don't need Audio Power Meter with record route %{public}@
%s Could not encrypt data. Err=%{public}@
%s Failed to create ivtag-plist from dict: %{public}@, err=%{public}@
%s Failed to write ivData=%{public}@ at filepath=%{public}@, err=%{public}@
%s Failed to write encryptedData to file at: %{public}@, err=%{public}@
%s Failed to delete ivtag file when saving encryptedFile failed!: err=%{public}@
%s Could not read ivtag file: %{public}@, err: %{public}@
%s Could not create ivtagDict from ivtag-plist. Err=%@
%s Could not read encryptedData from file: %@, err: %@
%s Cannot create NSData with size 0
%s xpc object should be XPC_TYPE_DATA
%s Creating OS Transaction for %{public}@
%s Release OS Transaction for %{public}@
%s Failed to create regular expression : %{public}@
