Audio/Video
Alarm
v8@?0
CSVolumeMonitor queue
-[CSVolumeMonitor _startMonitoringWithQueue:]
-[CSVolumeMonitor fetchVolumeFromAVSystemControllerForAudioCategory:]_block_invoke
-[CSVolumeMonitor systemControllerDied:]
-[CSVolumeMonitor startObservingSystemVolumes]
-[CSVolumeMonitor _startObservingSystemControllerLifecycle]
CSTimerMonitor queue
-[CSTimerMonitor _startMonitoringWithQueue:]
-[CSTimerMonitor _stopMonitoring]
CSAlarmMonitor queue
-[CSAlarmMonitor _startMonitoringWithQueue:]
-[CSAlarmMonitor _stopMonitoring]
CSMediaPlayingMonitor queue
-[CSMediaPlayingMonitor initializeMediaPlayingState]_block_invoke
v16@?0@8
v12@?0I8
-[CSMediaPlayingMonitor _startMonitoringWithQueue:]
-[CSMediaPlayingMonitor _stopMonitoring]
-[CSMediaPlayingMonitor _notePossiblePlayPausedStateChange:]
PLAYING
NOT PLAYING
-[CSAudioZeroCounter getZeroStatisticsFromBuffer:entireSamples:]
-[CSAudioZeroCounter stopReportZeroStatistics]
smartSiriVolume
noiseLevelChannelBitset
LKFSChannelBitset
energyBufferSize
noiseLowerPercentile
noiseUpperPercentile
LKFSLowerPercentile
LKFSUpperPercentile
noiseTimeConstant
noiseMicSensitivityOffset
LKFSTimeConstant
LKFSMicSensitivityOffset
noiseTTSMappingInputRangeLow
noiseTTSMappingInputRangeHigh
noiseTTSMappingOutputRangeLow
noiseTTSMappingOutputRangeHigh
LKFSTTSMappingInputRangeLow
LKFSTTSMappingInputRangeHigh
LKFSTTSMappingOutputRangeLow
LKFSTTSMappingOutputRangeHigh
userOffsetInputRangeLow
userOffsetInputRangeHigh
userOffsetOutputRangeLow
userOffsetOutputRangeHigh
TTSVolumeLowerLimitDB
TTSVolumeUpperLimitDB
SSVNoiseLevelChannelBitset
TQ,R,N
SSVLKFSChannelBitset
SSVEnergyBufferSize
TI,R,N
SSVNoiseLowerPercentile
SSVNoiseUpperPercentile
SSVLKFSLowerPercentile
SSVLKFSUpperPercentile
SSVNoiseTimeConstant
Tf,R,N
SSVNoiseMicSensitivityOffset
SSVLKFSTimeConstant
SSVLKFSMicSensitivityOffset
SSVNoiseTTSMappingInputRangeLow
SSVNoiseTTSMappingInputRangeHigh
SSVNoiseTTSMappingOutputRangeLow
SSVNoiseTTSMappingOutputRangeHigh
SSVLKFSTTSMappingInputRangeLow
SSVLKFSTTSMappingInputRangeHigh
SSVLKFSTTSMappingOutputRangeLow
SSVLKFSTTSMappingOutputRangeHigh
SSVUserOffsetInputRangeLow
SSVUserOffsetInputRangeHigh
SSVUserOffsetOutputRangeLow
SSVUserOffsetOutputRangeHigh
SSVTTSVolumeLowerLimitDB
SSVTTSVolumeUpperLimitDB
SSVParameterDirectionary
T@"NSDictionary",R,N
com.apple.CoreSpeech.AudioLogging
+[CSAudioFileManager generateDeviceAudioLogging:numChannels:speechId:]_block_invoke
+[CSAudioFileManager _readDataFromFileHandle:toFileHandle:]
%@%@.wav
%@/%@%@.wav
+[CSAudioFileManager _createAudioFileWriterWithLoggingDir:inputFormat:outputFormat:]
en_US_POSIX
yyyy_MM_dd-HHmmss.SSS
^%@*
+[CSUtils(Directory) removeLogFilesInDirectory:matchingPattern:beforeDays:]_block_invoke
v24@?0@"NSArray"8^@16
+[CSUtils(Directory) clearLogFilesInDirectory:matchingPattern:exceedNumber:]_block_invoke_2
+[CSUtils(Directory) clearLogFilesInDirectory:matchingPattern:exceedNumber:]_block_invoke
Unable to get %@ for file at %@: %@
q24@?0@"NSURL"8@"NSURL"16
B24@?0@"NSURL"8@"NSDictionary"16
triggerStartSampleCount
removeVoiceTriggerSamples
-[CSSmartSiriVolume initWithSamplingRate:asset:]
-[CSSmartSiriVolume initializeMediaPlayingState]_block_invoke
NOT playing
playing
-[CSSmartSiriVolume initializeMediaPlayingState]
-[CSSmartSiriVolume initializeAlarmState]_block_invoke
firing
NOT firing
-[CSSmartSiriVolume initializeTimerState]_block_invoke
-[CSSmartSiriVolume _setAsset:]
-[CSSmartSiriVolume _processAudioChunk:soundType:]
v16@?0Q8
-[CSSmartSiriVolume estimateSoundLevelbySoundType:]_block_invoke
-[CSSmartSiriVolume pauseSSVProcessing]_block_invoke
-[CSSmartSiriVolume resumeSSVProcessing]_block_invoke
-[CSSmartSiriVolume voiceTriggerDidDetectKeyword:]
-[CSSmartSiriVolume voiceTriggerDidDetectKeyword:]_block_invoke
-[CSSmartSiriVolume estimatedTTSVolumeForNoiseLevelAndLKFS:LKFS:]_block_invoke
-[CSSmartSiriVolume _combineResultsWithOptimalFromNoise:andOptimalFromLkfs:withUserOffset:]
-[CSSmartSiriVolume CSMediaPlayingMonitor:didReceiveMediaPlayingChanged:]_block_invoke
-[CSSmartSiriVolume CSAlarmMonitor:didReceiveAlarmChanged:]_block_invoke
-[CSSmartSiriVolume CSTimerMonitor:didReceiveTimerChanged:]_block_invoke
-[CSSmartSiriVolume _setStartAnalyzeTime:]
hash
TQ,R
superclass
T#,R
description
T@"NSString",R,C
debugDescription
allocator<T>::allocate(size_t n) 'n' exceeds maximum supported size
com.apple.assistant
Siri Global
 - %@
CSKeychainValueForAccountAndKey
-[CSAudioFileReader initWithURL:]
-[CSAudioFileReader loadAllSamples]
numChannels
TI,R,N,V_numChannels
-[CSAudioSampleRateConverter _createSampleRateConverterWithInASBD:outASBD:]
/BuildRoot/Library/Caches/com.apple.xbs/Sources/CoreSpeech_Sim/CoreSpeech-140.12.2.4/CoreSpeech/CSAudioSampleRateConverter.m
<Unknown File>
Too many buffers
i32@?0^I8^{AudioBufferList=I[1{AudioBuffer=II^v}]}16^^{AudioStreamPacketDescription}24
-[CSAudioSampleRateConverter convertSampleRateOfBuffer:]
-[NSString(XPCObject) _cs_initWithXPCObject:]
+[CSUtils(LanguageCode) getSiriLanguageWithFallback:]
-[CSSpeakerModel _createDirectoryIfNotExist:]
xx_XX
model
audio
self ENDSWITH '.wav'
-[CSSpeakerModel discard]
modelPath
T@"NSString",R,N
utteranceDirectory
enrollmentUtterance
T@"NSArray",R,N
isValid
TB,R,N
needsRetrain
com.apple.corespeech.corespeechservices
+[CSCoreSpeechServices setDelayInterstitialSounds:level:]
+[CSCoreSpeechServices setDelayInterstitialSounds:level:]_block_invoke
v16@?0@"NSError"8
+[CSCoreSpeechServices getTriggerCount]_block_invoke
+[CSCoreSpeechServices clearTriggerCount]_block_invoke
speechManagerStartSampleCount
speechManagerSetRecordModeToRecordingDelay
triggerEndMachTime
CSSpeechManager Asset Query Queue
-[CSSpeechManager startManager]
-[CSSpeechManager startManager]_block_invoke
-[CSSpeechManager _reset]
-[CSSpeechManager _getSmartSiriVolumeAsset]
en-US
-[CSSpeechManager registerSpeechController:]
-[CSSpeechManager _notifyEvent:]
-[CSSpeechManager _createRecorderWithContextIfNeeded:error:]
-[CSSpeechManager _prepareRecorderWithSettings:error:]
B8@?0
-[CSSpeechManager _prepareRecorderWithSettings:error:]_block_invoke
v20@?0B8@"NSError"12
-[CSSpeechManager _prepareListenWithSettings:error:]
-[CSSpeechManager prewarmAudioSession]
-[CSSpeechManager recordRoute]
-[CSSpeechManager recordSettings]
-[CSSpeechManager setClientContext:error:]
-[CSSpeechManager setClientContext:error:]_block_invoke
reason
Mediaserverd is recovering from crash
-[CSSpeechManager prepareRecordingForClient:error:]
-[CSSpeechManager prepareRecordingForClient:error:]_block_invoke
Cannot prepare since audio recorder was not initialized
-[CSSpeechManager _startRecordingWithSettings:error:]
-[CSSpeechManager _startListening:]
-[CSSpeechManager _setRecordMode:error:]
-[CSSpeechManager _scheduleSetRecordModeToRecordingWithDelay:forReason:validator:completion:]_block_invoke
-[CSSpeechManager _scheduleSetRecordModeToRecordingWithDelay:forReason:validator:completion:]
-[CSSpeechManager _cancelPendingSetRecordModeToRecordingForReason:]
-[CSSpeechManager _performPendingSetRecordModeToRecordingForReason:]
-[CSSpeechManager _setCurrentContext:error:]
-[CSSpeechManager _releaseClientAudioSession:]
-[CSSpeechManager _releaseAudioSessionForListening:error:]
-[CSSpeechManager startRecordingWithSetting:event:error:]
-[CSSpeechManager startRecordingWithSetting:event:error:]_block_invoke
Fail to start recording under PollingListening state.
Fail to start recording when awaiting to stop.
-[CSSpeechManager _startRecordingForClient:error:]
-[CSSpeechManager _startRecordingForClient:error:]_block_invoke
-[CSSpeechManager stopRecordingWithEvent:]_block_invoke_2
-[CSSpeechManager audioRecorderLostMediaserverd:]
-[CSSpeechManager mediaserverdDidRestart]
-[CSSpeechManager didTransitFrom:to:by:]_block_invoke
-[CSSpeechManager didIgnoreEvent:from:]
-[CSSpeechManager _createListenPollingTimer]
-[CSSpeechManager _createListenPollingTimer]_block_invoke
-[CSSpeechManager _startListenPolling]
-[CSSpeechManager _stopListenPolling]
-[CSSpeechManager _destroyAudioRecorderIfNeeded]
-[CSSpeechManager _startForwardingToClient]
-[CSSpeechManager _stopForwardingToClient]
-[CSSpeechManager audioRecorderBufferAvailable:buffer:atTime:]_block_invoke
-[CSSpeechManager audioRecorderDidStartRecording:successfully:error:]_block_invoke
-[CSSpeechManager audioRecorderDidStopRecording:forReason:]_block_invoke
-[CSSpeechManager audioRecorderRecordHardwareConfigurationDidChange:toConfiguration:]_block_invoke
-[CSSpeechManager audioRecorderBeginRecordInterruption:]_block_invoke
-[CSSpeechManager audioRecorderBeginRecordInterruption:withContext:]_block_invoke
-[CSSpeechManager audioRecorderEndRecordInterruption:]_block_invoke
-[CSSpeechManager audioRecorder:willSetAudioSessionActive:]_block_invoke
-[CSSpeechManager audioRecorder:didSetAudioSessionActive:]_block_invoke
-[CSSpeechManager audioRecorderDisconnected:]_block_invoke
-[CSSpeechManager CSLanguageCodeUpdateMonitor:didReceiveLanguageCodeChanged:]
Init
Stop
FirstPass
SecondPass
RecordPending
RecordWithVTRunning
RecordWithVTStopped
PollingListeningWithVTRunning
PollingListeningWithVTStopped
Listeninng
StoppingWithVTRunning
StoppingWithVTStopped
MediaserverdRecoveringWithVTRunning
MediaserverdRecoveringWithVTStopped
unknown(%tu)
ClientPrepare
AudioRecorderRelease
VoiceTriggerRunning
VoiceTriggerStopped
FirstPassTriggered
SecondPassTriggered
SecondPassRejected
SelfTriggerDetected
RecordPendingTimeout
ClientStartRecording
ClientStopRecording
ClientReleaseRecordSession
RecordingDidStop
ListeningSucceed
MediaserverdCrashed
MediaserverdRestarted
kDidStartFailed
RecorderDisconnected
SiriEnabled
SiriDisabled
-[CSSpeechManager getEstimatedTTSVolume]
-[CSSpeechManager CSSiriEnabledMonitor:didReceiveEnabled:]_block_invoke
-[CSSpeechManager _createClearLoggingFileTimer]
-[CSSpeechManager _createClearLoggingFileTimer]_block_invoke
-[CSSpeechManager _startClearLoggingFilesTimer]
assetQueryQueue
T@"NSObject<OS_dispatch_queue>",&,N,V_assetQueryQueue
stateMachine
T@"CSStateMachine",&,N,V_stateMachine
audioBuffer
T@"CSAudioCircularBuffer",&,N,V_audioBuffer
clientController
T@"<CSSpeechManagerDelegate>",W,N,V_clientController
secondPassStartSampleCount
TQ,N,V_secondPassStartSampleCount
lastVoiceTriggerEventInfo
T@"NSDictionary",&,N,V_lastVoiceTriggerEventInfo
T@"CSSmartSiriVolume",&,N,V_smartSiriVolume
activeAudioProcessors
T@"NSHashTable",&,N,V_activeAudioProcessors
continuousAudioProcessors
T@"NSHashTable",&,N,V_continuousAudioProcessors
lastForwardedSampleCount
TQ,N,V_lastForwardedSampleCount
clientStartSampleCount
TQ,N,V_clientStartSampleCount
listenPollingTimer
T@"NSObject<OS_dispatch_source>",&,N,V_listenPollingTimer
clearLoggingFileTimer
T@"NSObject<OS_dispatch_source>",&,N,V_clearLoggingFileTimer
listenPollingTimerCount
Tq,N,V_listenPollingTimerCount
clearLoggingFileTimerCount
Tq,N,V_clearLoggingFileTimerCount
pendingSetRecordModeToRecordingToken
T@"NSUUID",&,N,V_pendingSetRecordModeToRecordingToken
pendingSetRecordModeToRecordingCompletion
T@?,C,N,V_pendingSetRecordModeToRecordingCompletion
isSiriEnabled
TB,N,V_isSiriEnabled
audioRecorder
T@"CSAudioRecorder",&,N,V_audioRecorder
queue
T@"NSObject<OS_dispatch_queue>",&,N,V_queue
com.apple.MobileAsset.VoiceTriggerAssets
com.apple.MobileAsset.VoiceTriggerAssetsWatch
com.apple.MobileAsset.VoiceTriggerAssetsMarsh
com.apple.MobileAsset.VoiceTriggerAssetsMac
com.apple.MobileAsset.SpeechEndpointAssets
-[CSAssetManager init]
Serial CSAssetManager queue
v12@?0B8
-[CSAssetManager assetOfType:language:]
-[CSAssetManager installedAssetOfType:language:]
-[CSAssetManager _installedAssetOfType:withPredicate:]
-[CSAssetManager _assetQueryForAssetType:withPredicate:localOnly:]
-[CSAssetManager _runAssetQuery:completion:]
-[CSAssetManager _runAssetQuery:completion:]_block_invoke
v24@?0@"NSArray"8@"NSError"16
-[CSAssetManager _fetchRemoteMetaData]
-[CSAssetManager _fetchRemoteAssetOfType:withPredicate:]
-[CSAssetManager _updateFromRemoteToLocalAssets:forAssetType:]
-[CSAssetManager _defaultDownloadOptions]
-[CSAssetManager _downloadAsset:withComplete:]
v16@?0d8
-[CSAssetManager _downloadAsset:withComplete:]_block_invoke
-[CSAssetManager _startDownloadingAsset:progress:completion:]
v24@?0@"NSDictionary"8@"NSError"16
-[CSAssetManager _startDownloadingAsset:progress:completion:]_block_invoke
-[CSAssetManager CSLanguageCodeUpdateMonitor:didReceiveLanguageCodeChanged:]
currentLanguageCode
+[CSAssetManager(Utils) predicateForAssetType:language:]
(%@ == %K)
(%@ IN %K)
((%K == nil) OR (%K != %@))
 && 
deque
activeChannel
twoShotAudibleFeedbackDelay
CSSpeechRecordSettingsKey_AudioSessionActiveDelay
com.apple.corespeech.twoShotAudibleFeedback
-[CSSpeechController initializeRecordSessionWithContext:]
-[CSSpeechController prepareRecordWithSettings:error:]
-[CSSpeechController setCurrentContext:error:]
-[CSSpeechController preheat]
-[CSSpeechController prewarmAudioSession]
-[CSSpeechController resetAudioSession]
-[CSSpeechController reset]
-[CSSpeechController releaseAudioSession]
-[CSSpeechController releaseAudioSession:]
-[CSSpeechController startRecordingWithSettings:error:]
-[CSSpeechController startRecordingWithSettings:error:]_block_invoke
-[CSSpeechController stopRecording]
-[CSSpeechController recordRoute]
-[CSSpeechController speechManagerLPCMRecordBufferAvailable:chunk:]_block_invoke
-[CSSpeechController speechManagerRecordBufferAvailable:buffer:]_block_invoke
-[CSSpeechController speechManagerDidStartForwarding:successfully:error:]
-[CSSpeechController speechManagerDidStartForwarding:successfully:error:]_block_invoke
-[CSSpeechController speechManagerDidStopForwarding:forReason:]
-[CSSpeechController speechManagerDidStopForwarding:forReason:]_block_invoke
-[CSSpeechController speechManagerRecordHardwareConfigurationDidChange:toConfiguration:]
-[CSSpeechController speechManagerRecordHardwareConfigurationDidChange:toConfiguration:]_block_invoke
-[CSSpeechController speechManagerDetectedSystemVolumeChange:withVolume:forReason:]
-[CSSpeechController speechManagerDetectedSystemVolumeChange:withVolume:forReason:]_block_invoke
-[CSSpeechController speechManagerBeginRecordInterruption:]
-[CSSpeechController speechManagerBeginRecordInterruption:]_block_invoke
-[CSSpeechController speechManagerBeginRecordInterruption:withContext:]
-[CSSpeechController speechManagerBeginRecordInterruption:withContext:]_block_invoke
-[CSSpeechController speechManagerEndRecordInterruption:]
-[CSSpeechController speechManagerEndRecordInterruption:]_block_invoke
-[CSSpeechController speechManager:willSetAudioSessionActive:]
-[CSSpeechController speechManager:willSetAudioSessionActive:]_block_invoke
-[CSSpeechController speechManager:didSetAudioSessionActive:]
-[CSSpeechController speechManager:didSetAudioSessionActive:]_block_invoke
-[CSSpeechController audioConverterDidConvertPackets:packets:timestamp:]
-[CSSpeechController setAlertSoundFromURL:forType:]
-[CSSpeechController playAlertSoundForType:]
-[CSSpeechController playRecordStartingAlertAndResetEndpointer]
-[CSSpeechController setMeteringEnabled:]
-[CSSpeechController outputReferenceChannel]
-[CSSpeechController voiceTriggerInfo]
-[CSSpeechController voiceTriggerDidDetectTwoShotAtTime:]_block_invoke
-[CSSpeechController keywordDetectorDidDetectKeyword]_block_invoke
Accounts
Speech Identifier
%c%c%c%c
none
-[CSSpeechController beginWaitingForMyriad]
-[CSSpeechController endWaitingForMyriadWithDecision:]
-[CSSpeechController CSMediaPlayingMonitor:didReceiveMediaPlayingChanged:]_block_invoke
-[CSSpeechController CSAlarmMonitor:didReceiveAlarmChanged:]_block_invoke
-[CSSpeechController CSTimerMonitor:didReceiveTimerChanged:]_block_invoke
-[CSSpeechController _setSoundPlayingState]
 NOT
endpointerProxy
T@"CSEndpointerProxy",&,N,V_endpointerProxy
speechManager
T@"CSSpeechManager",W,N,V_speechManager
avvcContext
T@"NSDictionary",&,N,V_avvcContext
isOpus
TB,N,V_isOpus
isActivated
TB,N,V_isActivated
isNarrowBand
TB,N,V_isNarrowBand
audioFileWriter
T@"CSAudioFileWriter",&,N,V_audioFileWriter
TQ,N,V_activeChannel
twoShotNotificationEnabled
TB,N,V_twoShotNotificationEnabled
isMediaPlaying
TB,N,V_isMediaPlaying
isAlarmPlaying
TB,N,V_isAlarmPlaying
isTimerPlaying
TB,N,V_isTimerPlaying
isSoundPlaying
TB,N,V_isSoundPlaying
myriadPreventingTwoShotFeedback
TB,N,V_myriadPreventingTwoShotFeedback
delegate
T@"<CSSpeechControllerDelegate>",W,N,V_delegate
duckOthersOption
TB,N
endpointAnalyzer
T@"<CSEndpointAnalyzer>",R,N
com.apple.voicetrigger
com.apple.voicetrigger.notbackedup
kDeviceCategory01
kDeviceCategory02
productType
VoiceTrigger Enabled
VoiceTrigger CoreSpeech Enabled
Enable Two Shot Notification
com.apple.demo-settings
StoreDemoMode
File Logging Level
Library
Logs/CrashReporter/VoiceTrigger/audio/
/Logs/CrashReporter/Assistant/
SpeechLogs
-[CSPreferences assistantAudioFileLogDirectory]
VoiceTrigger/SAT
Caches/VoiceTrigger/SATUpdate
Caches/VoiceTrigger/SATUpload
-[CSPreferences getUserVoiceProfileUploadPathWithEnrolledLanguageList:]
json
-[CSPreferences notifyUserVoiceProfileUploadComplete]
-[CSPreferences getUserVoiceProfileUpdateDirectory]
-[CSPreferences notifyUserVoiceProfileUpdateReady]
Enable VoiceTrigger Upon VoiceProfile Sync For Language
enrollment_completed
enrollment_migrated
-[CSPreferences _markSATEnrollmentWithMarker:forLanguage:]
-[CSPreferences isCurrentDeviceCompatibleWithVoiceProfileAt:]
pathExtension='json'
iPad6,3
iPad6,4
iPad7,1
iPad7,2
iPad7,3
iPad7,4
iPhone8,1
iPhone8,2
iPhone8,4
iPhone9,1
iPhone9,2
iPhone9,3
iPhone9,4
iPhone10,1
iPhone10,2
iPhone10,3
iPhone10,4
iPhone10,5
iPhone10,6
iPhone10,7
iPhone10,8
iPad3,4
iPad3,5
iPad3,6
iPad4,1
iPad4,2
iPad4,3
iPad4,4
iPad4,5
iPad4,6
iPad4,7
iPad4,8
iPad4,9
iPad5,1
iPad5,2
iPad5,3
iPad5,4
iPad6,7
iPad6,8
iPad6,11
iPad6,12
iPad7,5
iPad7,6
iPhone5,1
iPhone5,2
iPhone5,3
iPhone5,4
iPhone6,1
iPhone6,2
iPhone7,1
iPhone7,2
Remote VoiceTrigger Delay
Remote VoiceTrigger Endpoint Timeout
VoiceTrigger/interstitial
Myriad File Logging Enabled
initialState
Tq,N,V_initialState
transitions
T@"NSMutableDictionary",&,N,V_transitions
T@"<CSStateMachineDelegate>",W,N,V_delegate
currentState
Tq,R,N,V_currentState
Serial CSEventMonitor queue
/BuildRoot/Library/Caches/com.apple.xbs/Sources/CoreSpeech_Sim/CoreSpeech-140.12.2.4/CoreSpeech/CSEventMonitor.m
Subclasses need to overwrite this method
-[CSAudioFileLog _closeAudioFile]
-[CSAudioFileLog startRecording]_block_invoke
-input.wav
-[CSAudioFileLog appendAudioData:]_block_invoke
-[CSAudioFileLog stopRecording]_block_invoke
-[CSSpeechManager(Alert) setAlertSoundFromURL:forType:]
-[CSSpeechManager(Alert) playAlertSoundForType:]
-[CSSpeechManager(Alert) playRecordStartingAlertAndResetEndpointer]
v12@?0i8
-[CSSpeechEndpointAssetMetaUpdateMonitor _startMonitoringWithQueue:]
-[CSSpeechEndpointAssetMetaUpdateMonitor _stopMonitoring]
-[CSSpeechEndpointAssetMetaUpdateMonitor _didReceiveNewSpeechEndpointAssetMetaData]
com.apple.MobileAsset.SpeechEndpointAssets.cached-metadata-updated
Testing [%@] against regex.
regex.json
Cannot parse to JSON
Cannot find the file
trailing_garbage
leading_garbage
regex
com.apple.corespeech
InternalBuild
-[CSKeywordAnalyzerNDAPI initWithConfigPath:resourcePath:]
-[CSKeywordAnalyzerNDAPI _setStartAnalyzeTime:]
samples_at_fire
activePhraseId
TI,N,V_activePhraseId
T@"<CSKeywordAnalyzerNDAPIScoreDelegate>",W,N,V_delegate
Framework
Logs/CrashReporter/CoreSpeech/
Logs/CrashReporter/CoreSpeech/audio/
%@/%@%@%@
_CSGetOrCreateAudioLogDirectory
/tmp
totalAudioRecorded
Td,N,V_totalAudioRecorded
featuresAtEndpoint
T@"NSArray",&,N,V_featuresAtEndpoint
endpointerType
Tq,N,V_endpointerType
serverFeatureLatencyDistribution
T@"NSDictionary",&,N,V_serverFeatureLatencyDistribution
additionalMetrics
T@"NSDictionary",&,N,V_additionalMetrics
CSRemoteControlClient
-[CSRemoteControlClient dealloc]
T@"<CSRemoteControlClientDelegate>",W,N,V_delegate
Languages
Footprint
Premium
com.apple.coreaudio.BorealisToggled
-[CSVoiceTriggerEnabledMonitor _startMonitoringWithQueue:]_block_invoke
-[CSVoiceTriggerEnabledMonitor _startMonitoringWithQueue:]
-[CSVoiceTriggerEnabledMonitor _stopMonitoring]
-[CSVoiceTriggerEnabledMonitor _checkVoiceTriggerEnabled]
CSRemoteRecordClient Queue
T@"<CSRemoteRecordClientDelegate>",W,N,V_delegate
Borealis Input
com.apple.VoiceTriggerUI.RemoteRecordSessionQueue
-[CSVTUIAudioSessionRemote _audioRecorder]
-[CSVTUIAudioSessionRemote prepareRecord]
T@"<CSVTUIAudioSessionDelegate>",W,N,V_delegate
-[CSSiriEnabledMonitor _startMonitoringWithQueue:]
Enabled
Disabled
-[CSSiriEnabledMonitor _stopMonitoring]
_AssistantPrefsChangedNotification
beepLocation
statsComputed
beepPower
signalPower
originalPower
absMaxVal
above95pcOfMax
totalInputSamples
totalOutputSamples
jbl_begin.bin
-[CSBeepCanceller init]
-[CSBeepCanceller willBeep]
-[CSBeepCanceller reset]
T@"<CSBeepCancellerDelegate>",W,N,V_delegate
metrics
CSInitialContinousZeros
CSMaxContinousZeros
CSMidSegmentContinousZeros
start
+[CSUtils(AudioFile) readAudioChunksFrom:block:]
-[CSAudioCircularBuffer initWithNumChannels:recordingDuration:samplingRate:]
-[CSAudioCircularBuffer copySamplesFromHostTime:]
-[CSAudioCircularBuffer copySamplesFrom:to:]
-[CSAudioCircularBuffer copyBufferWithNumSamplesCopiedIn:]
-[CSAudioCircularBuffer reset]
-[CSAudioCircularBuffer saveRecordingBufferFrom:to:toURL:]
bufferLength
TQ,N,V_bufferLength
copySamples
  mNumChannels: 
  mRecordingDurationInSecs: 
  mSampleRate: 
  mBytesPerSample: 
  mBufferLengthInSamples: 
  mNextWritePos: 
  mSamplesCount: 
  mMemoryPool(
): [
    chan-
: sz=
: mem-sz: 
-[CSVoiceTriggerAssetMetaUpdateMonitor _startMonitoringWithQueue:]
-[CSVoiceTriggerAssetMetaUpdateMonitor _stopMonitoring]
-[CSVoiceTriggerAssetMetaUpdateMonitor _didReceiveNewVoiceTriggerAssetMetaData]
com.apple.MobileAsset.VoiceTriggerAssets.cached-metadata-updated
com.apple.MobileAsset.VoiceTriggerAssetsWatch.cached-metadata-updated
com.apple.MobileAsset.VoiceTriggerAssetsMarsh.cached-metadata-updated
com.apple.MobileAsset.VoiceTriggerAssetsMac.cached-metadata-updated
{wordCount: %ld, trailingSilDuration: %ld, eosLikelihood: %f, pauseCounts: %@, silencePosterior: %f, taskName: %@, processedAudioDurationInMilliseconds: %ld}
wordCount
Tq,N,V_wordCount
trailingSilenceDuration
Tq,N,V_trailingSilenceDuration
eosLikelihood
Td,N,V_eosLikelihood
pauseCounts
T@"NSArray",C,N,V_pauseCounts
silencePosterior
Td,N,V_silencePosterior
processedAudioDurationInMilliseconds
Tq,N,V_processedAudioDurationInMilliseconds
taskName
T@"NSString",C,N,V_taskName
com.apple.cs.%@.apQueue
-[CSVAD2EndpointAnalyzer preheat]
-[CSVAD2EndpointAnalyzer resetForNewRequestWithSampleRate:]
-[CSVAD2EndpointAnalyzer reset]
-[CSVAD2EndpointAnalyzer _resetWithSampleRate:]
triggerEndSeconds
-[CSVAD2EndpointAnalyzer handleVoiceTriggerWithActivationInfo:]_block_invoke
-[CSVAD2EndpointAnalyzer processAudioSamplesAsynchronously:]
-[CSVAD2EndpointAnalyzer processAudioSamplesAsynchronously:]_block_invoke
-[CSVAD2EndpointAnalyzer _processAudioSamples:]
endpointStyle
Tq,N
delay
Td,N
startWaitTime
automaticEndpointingSuspensionEndTime
minimumDurationForEndpointer
lastEndOfVoiceActivityTime
Td,R,N
lastStartOfVoiceActivityTime
bypassSamples
endpointMode
interspeechWaitTime
endWaitTime
saveSamplesSeenInReset
T@"<CSEndpointAnalyzerDelegate>",W,N
canProcessCurrentRequest
TQ,N
endpointerModelVersion
elapsedTimeWithNoSpeech
sampleRate
Td,N,V_sampleRate
frameRate
TI,N,V_frameRate
detectedOneShotStartpoint
TB,N,V_detectedOneShotStartpoint
detectedRecurrentStartpoint
TB,N,V_detectedRecurrentStartpoint
communicatedStartPointDetection
TB,N,V_communicatedStartPointDetection
detectedOneShotEndpoint
TB,N,V_detectedOneShotEndpoint
detectedRecurrentEndpoint
TB,N,V_detectedRecurrentEndpoint
communicatedEndpointDetection
TB,N,V_communicatedEndpointDetection
samplesSeen
Td,N,V_samplesSeen
numSamplesProcessed
Td,N,V_numSamplesProcessed
lastOneShotStartpoint
Td,N,V_lastOneShotStartpoint
lastOneShotEndpoint
Td,N,V_lastOneShotEndpoint
lastRecurrentStartpoint
Td,N,V_lastRecurrentStartpoint
lastRecurrentEndpoint
Td,N,V_lastRecurrentEndpoint
floatSampleBuffer
T@"NSMutableData",&,N,V_floatSampleBuffer
topLevelParameterDict
T@"NSDictionary",&,N,V_topLevelParameterDict
modelDictPath
T@"NSString",&,N,V_modelDictPath
isConfigured
TB,N,V_isConfigured
previousSamplesSeen
Td,N,V_previousSamplesSeen
apQueue
T@"NSObject<OS_dispatch_queue>",&,N,V_apQueue
recordingDidStop
TB,N,V_recordingDidStop
vtEndInSampleCount
TQ,N,V_vtEndInSampleCount
Tq,N,V_endpointStyle
Td,N,V_delay
Td,N,V_startWaitTime
Td,N,V_automaticEndpointingSuspensionEndTime
Td,N,V_minimumDurationForEndpointer
Td,N,V_bypassSamples
Tq,N,V_endpointMode
Td,N,V_interspeechWaitTime
Td,N,V_endWaitTime
TB,N,V_saveSamplesSeenInReset
T@"<CSEndpointAnalyzerDelegate>",W,N,V_delegate
-[CSVAD2EndpointAnalyzer(private) _configureWithSampleRate:andFrameRate:]
kAUEndpointVAD2Property_EDLStartWaitTimeSec
kAUEndpointVAD2Property_EDLInterspeechWaitTimeSec
kAUEndpointVAD2Property_EDLSpeechStartAdjustSec
kAUEndpointVAD2Property_EDLSpeechEndAdjustSec
kAUEndpointVAD2Property_EDLWindowLengthSeconds
kAUEndpointVAD2Property_EDLSpeechFraction
kAUEndpointVAD2Property_EDLNonspeechFraction
kAUEndpointVAD2Property_IsRealtimeOperationMode
kAUEndpointVAD2Property_DecoderLatencySeconds
kAudioUnitProperty_MaximumFramesPerSlice
-[CSVAD2EndpointAnalyzer(private) _detectVoiceActivityInSamples:numSamples:]
Library/Audio/Tunings/Generic/AU/
EndpointerModelPathForStyle
aufx-epv2-bluetooth8khz-appl.plist
aufx-epv2-appl.plist
EndpointerSpeechBeginListener
EndpointerSpeechEndListener
RecurrentVADSpeechBeginListener
RecurrentVADSpeechEndListener
-[NSArray(XPCObject) _cs_initWithXPCObject:]
-[NSArray(XPCObject) _cs_initWithXPCObject:]_block_invoke
B24@?0Q8@"NSObject<OS_xpc_object>"16
-[NSArray(XPCObject) _cs_xpcObject]_block_invoke
v32@?0@8Q16^B24
-[CSAudioChunk subChunkFrom:numSamples:forChannel:]
-[CSAudioChunk subChunkFrom:numSamples:]
data
T@"NSData",R,N,V_data
TQ,R,N,V_numChannels
numSamples
TQ,R,N,V_numSamples
sampleByteDepth
TQ,R,N,V_sampleByteDepth
startSampleCount
TQ,R,N,V_startSampleCount
hostTime
TQ,R,N,V_hostTime
samples_fed
best_phrase
best_start
best_end
best_score
early_warning
is_rescoring
sampleFed
TQ,N,V_sampleFed
bestPhrase
TQ,N,V_bestPhrase
bestStart
TQ,N,V_bestStart
bestEnd
TQ,N,V_bestEnd
bestScore
Tf,N,V_bestScore
earlyWarning
TB,N,V_earlyWarning
isRescoring
TB,N,V_isRescoring
dictionary
SearchOrMessaging
Warmup
Median
ExtraDelayMs
EndpointerDecisionLagMs
ClientLagThresholdMsKey
ClampedSFLatencyMsForClientLag
UseDefaultServerFeaturesOnClientLag
-[CSHybridEndpointAnalyzer init]
com.apple.cs.%@.stateserialqueue
com.apple.cs.%@.sepfQueue
com.apple.cs.%@.hybridClassifierfQueue
com.apple.cs.%@.silencePosteriorGeneratorQueue
-[CSHybridEndpointAnalyzer processAudioSamplesAsynchronously:]
-[CSHybridEndpointAnalyzer processAudioSamplesAsynchronously:]_block_invoke
-[CSHybridEndpointAnalyzer updateEndpointerThreshold:]
-[CSHybridEndpointAnalyzer updateEndpointerDelayedTrigger:]
-[CSHybridEndpointAnalyzer processServerEndpointFeatures:]
-[CSHybridEndpointAnalyzer shouldAcceptEagerResultForDuration:resultsCompletionHandler:]_block_invoke
-[CSHybridEndpointAnalyzer clientSilenceFeaturesAvailable:]
-[CSHybridEndpointAnalyzer clientSilenceFeaturesAvailable:]_block_invoke
-[CSHybridEndpointAnalyzer serverFeaturesLatencyDistributionDictionary]_block_invoke
q24@?0@8@16
extraSamplesAtStart
-[CSHybridEndpointAnalyzer handleVoiceTriggerWithActivationInfo:]_block_invoke
-[CSHybridEndpointAnalyzer recordingStoppedForReason:]
-[CSHybridEndpointAnalyzer resetForNewRequestWithSampleRate:]
-[CSHybridEndpointAnalyzer resetForNewRequestWithSampleRate:]_block_invoke
/BuildRoot/Library/Caches/com.apple.xbs/Sources/CoreSpeech_Sim/CoreSpeech-140.12.2.4/CoreSpeech/CSHybridEndpointAnalyzer.m
CSHybridEndpointAnalyzer reset called
-[CSHybridEndpointAnalyzer CSLanguageCodeUpdateMonitor:didReceiveLanguageCodeChanged:]
-[CSHybridEndpointAnalyzer CSAssetManagerDidDownloadNewAsset:]
-[CSHybridEndpointAnalyzer _updateAssetWithLanguage:]_block_invoke
cs_hep_marsh.json
cs_hep.json
-[CSHybridEndpointAnalyzer _getCSHybridEndpointerConfigForAsset:]
currentAsset
T@"CSAsset",&,N,V_currentAsset
TQ,N,V_numSamplesProcessed
didAddAudio
TB,N,V_didAddAudio
caesuraSPG
T@"EARCaesuraSilencePosteriorGenerator",&,N,V_caesuraSPG
clientSilenceFeaturesAtEndpoint
T@"EARClientSilenceFeatures",&,N,V_clientSilenceFeaturesAtEndpoint
TB,N,V_canProcessCurrentRequest
hybridClassifier
T@"_EAREndpointer",&,N,V_hybridClassifier
T@"NSString",&,N,V_endpointerModelVersion
serverFeaturesQueue
T@"NSObject<OS_dispatch_queue>",&,N,V_serverFeaturesQueue
lastKnownServerEPFeatures
T@"CSServerEndpointFeatures",&,N,V_lastKnownServerEPFeatures
serverFeatureLatencies
T@"NSMutableArray",&,N,V_serverFeatureLatencies
serverFeaturesWarmupLatency
Td,N,V_serverFeaturesWarmupLatency
lastServerFeatureTimestamp
T@"NSDate",&,N,V_lastServerFeatureTimestamp
didReceiveServerFeatures
TB,N,V_didReceiveServerFeatures
clientLagThresholdMs
Td,N,V_clientLagThresholdMs
clampedSFLatencyMsForClientLag
Td,N,V_clampedSFLatencyMsForClientLag
useDefaultServerFeaturesOnClientLag
TB,N,V_useDefaultServerFeaturesOnClientLag
hybridClassifierQueue
T@"NSObject<OS_dispatch_queue>",&,N,V_hybridClassifierQueue
lastReportedEndpointTimeMs
Td,N,V_lastReportedEndpointTimeMs
stateSerialQueue
T@"NSObject<OS_dispatch_queue>",&,N,V_stateSerialQueue
didCommunicateEndpoint
TB,N,V_didCommunicateEndpoint
currentRequestSampleRate
TQ,N,V_currentRequestSampleRate
vtExtraAudioAtStartInMs
Td,N,V_vtExtraAudioAtStartInMs
hepAudioOriginInMs
Td,N,V_hepAudioOriginInMs
firstAudioPacketTimestamp
T@"NSDate",&,N,V_firstAudioPacketTimestamp
didTimestampFirstAudioPacket
TB,N,V_didTimestampFirstAudioPacket
silencePosteriorGeneratorQueue
T@"NSObject<OS_dispatch_queue>",&,N,V_silencePosteriorGeneratorQueue
didDetectSpeech
TB,N,V_didDetectSpeech
Td,N,V_elapsedTimeWithNoSpeech
+[CSUtils(Time) hostTimeFromSampleCount:anchorHostTime:anchorSampleCount:]
+[CSUtils(Time) sampleCountFromHostTime:anchorHostTime:anchorSampleCount:]
-[CSEndpointerProxy resetForNewRequestWithSampleRate:]
-[CSEndpointerProxy resetForVoiceTriggerTwoShotWithSampleRate:]
-[CSEndpointerProxy endpointer:didDetectStartpointAtTime:]
-[CSEndpointerProxy endpointer:didDetectHardEndpointAtTime:withMetrics:]
-[CSEndpointerProxy _shouldEnterTwoShotAtEndPointTime:]
-[CSEndpointerProxy endpointerModelVersion]
hybridEndpointer
T@"<CSEndpointAnalyzerImpl>",&,N,V_hybridEndpointer
vad2Endpointer
T@"<CSEndpointAnalyzerImpl>",&,N,V_vad2Endpointer
activeEndpointer
T@"<CSEndpointAnalyzerImpl>",W,N,V_activeEndpointer
didEnterTwoshot
TB,N,V_didEnterTwoshot
endpointerDelegate
T@"<CSEndpointAnalyzerDelegate>",W,N,V_endpointerDelegate
-[NSNumber(XPCObject) _cs_initWithXPCObject:]
-[NSNumber(XPCObject) _cs_xpcObject]
-[CSFirstUnlockMonitor _stopMonitoring]
-[CSSpringboardStartMonitor _startMonitoringWithQueue:]
-[CSSpringboardStartMonitor _stopMonitoring]
-[CSSpringboardStartMonitor _checkSpringBoardStarted]
com.apple.springboard.finishedstartup
com.apple.transcribe.Transcriber
-[CSKeywordAnalyzerQuasar initWithConfigPath:triggerTokens:useKeywordSpotting:]
-[CSKeywordAnalyzerQuasar reset]
-[CSKeywordAnalyzerQuasar runRecognition]
-[CSKeywordAnalyzerQuasar runRecognition]_block_invoke
-[CSKeywordAnalyzerQuasar endAudio]
-[CSKeywordAnalyzerQuasar endAudio]_block_invoke
-[CSKeywordAnalyzerQuasar _recognizeWavData:length:]
-[CSKeywordAnalyzerQuasar speechRecognizer:didRecognizePartialResult:]_block_invoke
-[CSKeywordAnalyzerQuasar speechRecognizer:didFinishRecognitionWithError:]_block_invoke
-[CSKeywordAnalyzerQuasar _getConfidence:]_block_invoke
v32@?0@"_EARSpeechRecognitionToken"8Q16^B24
triggerConfidence
Td,R,N,V_triggerConfidence
T@"<CSKeywordAnalyzerQuasarScoreDelegate>",W,N,V_delegate
corespeech.json
hybridendpointer.json
hybridendpointer_marsh.json
/BuildRoot/Library/Caches/com.apple.xbs/Sources/CoreSpeech_Sim/CoreSpeech-140.12.2.4/CoreSpeech/CSAsset.m
ERR: Unknown assetType: %lu
+[CSAsset fallBackAssetResourcePath]
-[CSAsset initWithResourcePath:configFile:configVersion:]
-[CSAsset _decodeJson:]
-[CSAsset getNumberForKey:category:default:]
-[CSAsset getStringForKey:category:default:]
configVersion:%@ resourcePath:%@ path:%@
path
T@"NSString",R,N,V_path
resourcePath
T@"NSString",R,N,V_resourcePath
hashFromResourcePath
configVersion
T@"NSString",R,N,V_configVersion
-[CSAudioConverter _convertBufferedLPCM:allowPartial:timestamp:]
-[CSAudioConverter _convertBufferedLPCM:allowPartial:timestamp:]_block_invoke
/BuildRoot/Library/Caches/com.apple.xbs/Sources/CoreSpeech_Sim/CoreSpeech-140.12.2.4/CoreSpeech/CSAudioConverter.m
Cannot produce ASPD for PCM
-[CSAudioConverter reset]
-[CSAudioConverter _configureAudioConverter:]
T@"<CSAudioConverterDelegate>",W,V_delegate
CreateAudioConverter
-[CSLanguageCodeUpdateMonitor _startMonitoringWithQueue:]
-[CSLanguageCodeUpdateMonitor _stopMonitoring]
-[CSLanguageCodeUpdateMonitor _didReceiveLanguageCodeUpdate]
Serial CSPolicy queue
-[NSDictionary(XPCObject) _cs_initWithXPCObject:]
-[NSDictionary(XPCObject) _cs_initWithXPCObject:]_block_invoke
B24@?0r*8@"NSObject<OS_xpc_object>"16
-[NSDictionary(XPCObject) _cs_xpcObject]_block_invoke
v32@?0@8@16^B24
-[CSSpeakerDetectorNDAPI _initializeSAT:]
-[CSSpeakerDetectorNDAPI processSuperVector:withResult:]
T@"<CSSpeakerDetectorNDAPIDelegate>",W,N,V_delegate
-[CSAudioRecorder willDestroy]
-[CSAudioRecorder _destroyVoiceController]
-[CSAudioRecorder _voiceControllerWithContext:error:]
-[CSAudioRecorder _beepCanceller]
-[CSAudioRecorder prepareRecordWithSettings:error:]
-[CSAudioRecorder setCurrentContext:error:]
-[CSAudioRecorder prewarmAudioSession]
-[CSAudioRecorder releaseAudioSession:]
-[CSAudioRecorder _resetZeroFilter]
-[CSAudioRecorder startRecordingWithSettings:error:]
-[CSAudioRecorder startRecording:]
context
-[CSAudioRecorder startRecording]
-[CSAudioRecorder stopRecording]
-[CSAudioRecorder _recordingSampleRate]
Builtin Microphone
isBluetoothConnected
-[CSAudioRecorder playRecordStartingAlertAndResetEndpointer]
-[CSAudioRecorder playAlertSoundForType:]
ZeroFilterMetrics
-[CSAudioRecorder _audioRecorderDidStopRecordingForReason:]
BeepCancellerMetrics
-[CSAudioRecorder voiceControllerDidStartRecording:successfully:error:]
-[CSAudioRecorder voiceControllerDidStopRecording:forReason:]
-[CSAudioRecorder voiceControllerRecordHardwareConfigurationDidChange:toConfiguration:]
-[CSAudioRecorder voiceControllerBeginRecordInterruption:]
-[CSAudioRecorder voiceControllerBeginRecordInterruption:withContext:]
-[CSAudioRecorder voiceControllerEndRecordInterruption:]
-[CSAudioRecorder voiceControllerWillSetAudioSessionActive:willActivate:]
-[CSAudioRecorder voiceControllerDidSetAudioSessionActive:isActivated:]
-[CSAudioRecorder voiceControllerMediaServicesWereLost:]
-[CSAudioRecorder voiceControllerMediaServicesWereReset:]
-[CSAudioRecorder _deinterleaveBufferIfNeeded:]
-[CSAudioRecorder _createDeInterleaverIfNeeded]
-[CSAudioRecorder _createSampleRateConverterIfNeeded]
T@"<CSAudioRecorderDelegate>",W,N,V_delegate
-[NSData(XPCObject) _cs_initWithXPCObject:]
nohash
((?:[a-z]|[0-9])*)\.asset
+[CSUtils(ResourcePathHash) assetHashInResourcePath:]
-[CSAudioFileWriter initWithURL:inputFormat:outputFormat:]
-[CSAudioFileWriter addSamples:len:]
fileURL
T@"NSURL",R,N,V_fileURL
alloc
init
fetchVolumeFromAVSystemControllerForAudioCategory:
_startObservingSystemControllerLifecycle
startObservingSystemVolumes
defaultCenter
removeObserver:
dealloc
isEqualToString:
sharedInstance
_startMonitoringWithQueue:
musicVolume
alarmVolume
systemVolumeDidChange:
systemControllerDied:
.cxx_destruct
_queue
_musicVolumeLevel
_alarmVolumeLevel
initializeTimerState
_stopMonitoring
timerState
_timerFiringState
initializeAlarmState
alarmState
_alarmFiringState
_notifyObserver:mediaIsPlayingState:
enumerateObserversInQueue:
_notePossiblePlayPausedStateChange:
addObserver:selector:name:object:
removeObserver:name:object:
userInfo
objectForKey:
boolValue
notifyObserver:
CSMediaPlayingMonitor:didReceiveMediaPlayingChanged:
respondsToSelector:
initializeMediaPlayingState
mediaPlayingState
_mediaIsPlaying
zeroFilterWindowSizeInMs
bytes
initWithToken:sampleRate:numChannels:
getZeroStatisticsFromBuffer:entireSamples:
stopReportZeroStatistics
_methodToken
_continuousZeroCounter
_zeroCounterWinSz
_numChannels
_sampleRate
numberWithUnsignedInteger:
getNumberForKey:category:default:
unsignedIntegerValue
numberWithUnsignedInt:
unsignedIntValue
numberWithFloat:
floatValue
SSVNoiseLevelChannelBitset
SSVLKFSChannelBitset
SSVEnergyBufferSize
SSVNoiseLowerPercentile
SSVNoiseUpperPercentile
SSVLKFSLowerPercentile
SSVLKFSUpperPercentile
SSVNoiseTimeConstant
SSVNoiseMicSensitivityOffset
SSVLKFSTimeConstant
SSVLKFSMicSensitivityOffset
SSVNoiseTTSMappingInputRangeLow
SSVNoiseTTSMappingInputRangeHigh
SSVNoiseTTSMappingOutputRangeLow
SSVNoiseTTSMappingOutputRangeHigh
SSVLKFSTTSMappingInputRangeLow
SSVLKFSTTSMappingInputRangeHigh
SSVLKFSTTSMappingOutputRangeLow
SSVLKFSTTSMappingOutputRangeHigh
SSVUserOffsetInputRangeLow
SSVUserOffsetInputRangeHigh
SSVUserOffsetOutputRangeLow
SSVUserOffsetOutputRangeHigh
SSVTTSVolumeLowerLimitDB
SSVTTSVolumeUpperLimitDB
dictionaryWithObjects:forKeys:count:
SSVParameterDirectionary
_sharedAudioLoggingQueue
fileURL
URLByDeletingLastPathComponent
path
sharedPreferences
assistantAudioFileLogDirectory
containsString:
defaultManager
removeItemAtURL:error:
seekToEndOfFile
seekToFileOffset:
readDataOfLength:
length
writeData:
fileLoggingIsEnabled
_createAudioFileWriterWithLoggingDir:inputFormat:outputFormat:
_createTempAudioFileWriterWithInputFormat:outputFormat:
_getDateLabel
stringWithFormat:
stringByAppendingPathComponent:
fileURLWithPath:
initWithURL:inputFormat:outputFormat:
maxNumLoggingFiles
pruneNumberOfLogFilesTo:
URLWithString:
localeWithLocaleIdentifier:
setLocale:
setDateFormat:
stringFromDate:
removeLogFilesInDirectory:matchingPattern:beforeDays:
clearLogFilesInDirectory:matchingPattern:exceedNumber:
generateDeviceAudioLogging:numChannels:speechId:
_readDataFromFileHandle:toFileHandle:
createAudioFileWriterFromWithInputFormat:outputFormat:
removeLogFilesOlderThanNDays:
dateWithTimeIntervalSinceNow:
countByEnumeratingWithState:objects:count:
distantFuture
getResourceValue:forKey:error:
localizedDescription
compare:
_URLsInDirectory:matchingPattern:completion:
count
objectAtIndex:
_sortedURLsInDirectory:matchingPattern:completion:
_contentsOfDirectoryAtURL:matchingPattern:includingPropertiesForKeys:error:
arrayWithObjects:count:
sortedArrayUsingComparator:
regularExpressionWithPattern:options:error:
contentsOfDirectoryAtURL:includingPropertiesForKeys:options:error:
lastPathComponent
numberOfMatchesInString:options:range:
predicateWithBlock:
filteredArrayUsingPredicate:
class
description
UTF8String
_setDefaultParameters
_setAsset:
_convertDB2Mag:
getNumElementInBitset:
_reset
fetchInitSystemVolumes
addObserver:
_getMusicVolumeDB:
_resetStartAnalyzeTime
inputRecordingSampleByteDepth
convertToFloatLPCMBufFromShortLPCMBuf:
dataForChannel:
iterateBitset:block:
_prepareSoundLevelBufferFromSamples:soundType:
startSampleCount
_setStartAnalyzeTime:
numSamples
subChunkFrom:numSamples:
_processAudioChunk:soundType:
objectForKeyedSubscript:
_estimatedTTSVolume:lowerLimit:upperLimit:TTSmappingInputRangeLow:TTSmappingInputRangeHigh:TTSmappingOutputRangeLow:TTSmappingOutputRangeHigh:
_combineResultsWithOptimalFromNoise:andOptimalFromLkfs:withUserOffset:
_scaleInputWithInRangeOutRange:minIn:maxIn:minOut:maxOut:
isEqual:
self
performSelector:
performSelector:withObject:
performSelector:withObject:withObject:
isProxy
isKindOfClass:
isMemberOfClass:
conformsToProtocol:
retain
release
autorelease
retainCount
zone
hash
superclass
debugDescription
CSAlarmMonitor:didReceiveAlarmChanged:
CSTimerMonitor:didReceiveTimerChanged:
speechManagerRecordBufferAvailable:buffer:
speechManagerLPCMRecordBufferAvailable:chunk:
speechManagerDidStartForwarding:successfully:error:
speechManagerDidStopForwarding:forReason:
speechManagerRecordingContext
speechManagerRecordHardwareConfigurationDidChange:toConfiguration:
speechManagerDetectedSystemVolumeChange:withVolume:forReason:
speechManagerBeginRecordInterruption:
speechManagerBeginRecordInterruption:withContext:
speechManagerEndRecordInterruption:
speechManager:willSetAudioSessionActive:
speechManager:didSetAudioSessionActive:
voiceTriggerDidDetectKeyword:
voiceTriggerDidDetectNearMiss:
voiceTriggerDidDetectSpeakerReject:
voiceTriggerDidDetectTwoShotAtTime:
keywordDetectorDidDetectKeyword
initWithSamplingRate:asset:
startSmartSiriVolume
setAsset:
prepareSoundLevelBufferFromSamples:soundType:firedVoiceTriggerEvent:triggerStartTimeSampleOffset:triggerEndTimeSampleOffset:
estimateSoundLevelbySoundType:
pauseSSVProcessing
resumeSSVProcessing
reset
estimatedTTSVolumeForNoiseLevelAndLKFS:LKFS:
CSVolumeMonitor:didReceiveMusicVolumeChanged:
CSVolumeMonitor:didReceiveAlarmVolumeChanged:
.cxx_construct
_smartSiriVolumeNoiseLevel
_smartSiriVolumeLKFS
_floatBuffer
_defaults
_startAnalyzeSampleCount
_samplesFed
_processedSampleCount
_isStartSampleCountMarked
_shouldPauseSSVProcess
_shouldPauseLKFSProcess
_alarmSoundIsFiring
_timerSoundIsFiring
_currentAsset
_musicVolumeDB
_alarmVolume
_noiseLevelChannelBitset
_LKFSChannelBitset
_energyBufferSize
_noiseLowerPercentile
_noiseUpperPercentile
_LKFSLowerPercentile
_LKFSUpperPercentile
_noiseTimeConstant
_noiseMicSensitivityOffset
_LKFSTimeConstant
_LKFSMicSensitivityOffset
_noiseTTSMappingInputRangeLow
_noiseTTSMappingInputRangeHigh
_noiseTTSMappingOutputRangeLow
_noiseTTSMappingOutputRangeHigh
_LKFSTTSMappingInputRangeLow
_LKFSTTSMappingInputRangeHigh
_LKFSTTSMappingOutputRangeLow
_LKFSTTSMappingOutputRangeHigh
_userOffsetInputRangeLow
_userOffsetInputRangeHigh
_userOffsetOutputRangeLow
_userOffsetOutputRangeHigh
_TTSVolumeLowerLimitDB
_TTSVolumeUpperLimitDB
stringByAppendingFormat:
copy
close
dataWithBytes:length:
initWithURL:
loadAllSamples
readSamplesFromChannelIdx:
numChannels
_fFile
_deinterleavedData
_frameSize
_sampleByteDepth
lpcmNarrowBandASBD
lpcmASBD
initWithInASBD:outASBD:
_createSampleRateConverterWithInASBD:outASBD:
dataWithLength:
mutableBytes
stringWithUTF8String:
currentHandler
handleFailureInMethod:object:file:lineNumber:description:
setLength:
upsampler
downsampler
convertSampleRateOfBuffer:
_sampleRateConverter
_outBufferScaleFactor
_inASBD
_outASBD
initWithUTF8String:
_cs_initWithXPCObject:
_cs_xpcObject
getSiriLanguageWithFallback:
initWithLength:
convertToShortLPCMBufFromFloatLPCMBuf:
modelDirectory
_createDirectoryIfNotExist:
utteranceDirectory
fileExistsAtPath:isDirectory:
removeItemAtPath:error:
createDirectoryAtPath:withIntermediateDirectories:attributes:error:
CSSATBasePath
_satPath
array
contentsOfDirectoryAtPath:error:
predicateWithFormat:
caseInsensitiveCompare:
sortedArrayUsingSelector:
addObject:
modelPath
fileExistsAtPath:
_isDirectoryEmpty:
initWithSpeakerModelFileName:languageCode:
enrollmentUtterance
needsRetrain
discard
isValid
_modelFileName
_languageCode
_modelPath
_utteranceDirectory
initWithMachServiceName:options:
setRemoteObjectInterface:
getCoreSpeechServiceConnection
setInvalidationHandler:
resume
remoteObjectProxy
setDelayInterstitialSounds:level:completion:
getTriggerCount:
clearTriggerCount:
setDelayInterstitialSounds:level:
getTriggerCount
clearTriggerCount
inputRecordingFramesPerPacket
inputRecordingSampleRate
inputRecordingSampleRateNarrowBand
inputRecordingBytesPerFrame
inputRecordingBytesPerPacket
inputRecordingNumberOfChannels
inputRecordingDurationInSecs
inputRecordingSampleBitDepth
inputRecordingEncoderAudioQuality
inputRecordingSampleRateConverterAlgorithm
inputRecordingBufferDuration
audioConverterBitrate
channelForOutputReference
channelForProcessedInput
zeroFilterApproxAbsSpeechThreshold
csAudioProcessingQueuePriority
daysBeforeRemovingLogFiles
weakObjectsHashTable
_setupStateMachine
_setupCircularBuffer
_createListenPollingTimer
_createClearLoggingFileTimer
supportSmartVolume
setupSmartSiriVolume
supportAlwaysListening
_notifyEvent:
_startForwardingToSmartSiriVolume
mediaserverdDidRestart
_startClearLoggingFilesTimer
willDestroy
sharedManager
assetForCurrentLanguageOfType:
defaultFallBackAssetForSmartSiriVolume:
initWithNumChannels:recordingDuration:samplingRate:
_getSmartSiriVolumeAsset
initWithInitialState:
addTransitionFrom:to:for:
setDelegate:
currentState
_eventName:
performTransitionForEvent:
audioRecorder
initWithContext:error:
setAudioRecorder:
isRecording
setCurrentContext:error:
_getClientRecordContext
isRecordContextVoiceTrigger:
doubleValue
unsignedLongLongValue
hostTimeToTimeInterval:
_scheduleSetRecordModeToRecordingWithDelay:forReason:validator:completion:
_setRecordMode:error:
prepareRecordWithSettings:error:
prewarmAudioSession
recordRoute
recordSettings
isNarrowBand
errorWithDomain:code:userInfo:
_createRecorderWithContextIfNeeded:error:
_prepareRecorderWithSettings:error:
startRecordingWithSettings:error:
_cancelPendingSetRecordModeToRecordingForReason:
releaseClientAudioSession:
_releaseAudioSessionForListening:error:
_releaseClientAudioSession:
releaseAudioSession:
sampleCount
_startRecordingWithSettings:error:
_startRecordingForClient:error:
notifyEvent:
supportOpportunisticZLL
sampleCountFromHostTime:
stopRecording
_stateName:
_stopForwardingToClient
_startForwardingToClient
_startListenPolling
_destroyAudioRecorderIfNeeded
voiceTriggerRecordContext
lpcmRecordSettings
_prepareListenWithSettings:error:
_startListening:
_stopListenPolling
removeObject:
containsObject:
addSamples:numSamples:atHostTime:
initWithData:numChannels:numSamples:sampleByteDepth:startSampleCount:hostTime:
bufferLength
copySamplesFrom:to:
_performPendingSetRecordModeToRecordingForReason:
hostTimeFromSampleCount:
numberWithInteger:
audioRecorderBufferAvailable:buffer:atTime:
audioRecorderBufferAvailable:buffer:
audioRecorderDidStartRecording:successfully:error:
audioRecorderDidStopRecording:forReason:
audioRecorderRecordHardwareConfigurationDidChange:toConfiguration:
audioRecorderBeginRecordInterruption:
audioRecorderBeginRecordInterruption:withContext:
audioRecorderEndRecordInterruption:
audioRecorder:willSetAudioSessionActive:
audioRecorder:didSetAudioSessionActive:
voiceTriggerDetectedOnAOP:
audioRecorderDisconnected:
audioRecorderLostMediaserverd:
didTransitFrom:to:by:
didIgnoreEvent:from:
CSSiriEnabledMonitor:didReceiveEnabled:
CSAssetManagerDidDownloadNewAsset:
CSLanguageCodeUpdateMonitor:didReceiveLanguageCodeChanged:
initWithVoiceTriggerFirstPass:voicetriggerSecondPass:voicetriggerEventNotifier:audioRecorder:
startManager
registerSpeechController:
getCurrentState
isClientRecording
setClientContext:error:
prepareRecordingForClient:error:
_setCurrentContext:error:
releaseClientAudioSession
startRecordingWithSetting:event:error:
stopRecordingWithEvent:
_reinitializeSmartSiriVolumeWithAsset:
getEstimatedTTSVolume
queue
setQueue:
assetQueryQueue
setAssetQueryQueue:
stateMachine
setStateMachine:
audioBuffer
setAudioBuffer:
clientController
setClientController:
secondPassStartSampleCount
setSecondPassStartSampleCount:
lastVoiceTriggerEventInfo
setLastVoiceTriggerEventInfo:
smartSiriVolume
setSmartSiriVolume:
activeAudioProcessors
setActiveAudioProcessors:
continuousAudioProcessors
setContinuousAudioProcessors:
lastForwardedSampleCount
setLastForwardedSampleCount:
clientStartSampleCount
setClientStartSampleCount:
listenPollingTimer
setListenPollingTimer:
clearLoggingFileTimer
setClearLoggingFileTimer:
listenPollingTimerCount
setListenPollingTimerCount:
clearLoggingFileTimerCount
setClearLoggingFileTimerCount:
pendingSetRecordModeToRecordingToken
setPendingSetRecordModeToRecordingToken:
pendingSetRecordModeToRecordingCompletion
setPendingSetRecordModeToRecordingCompletion:
isSiriEnabled
setIsSiriEnabled:
_isSiriEnabled
_audioRecorder
_assetQueryQueue
_stateMachine
_audioBuffer
_clientController
_secondPassStartSampleCount
_lastVoiceTriggerEventInfo
_smartSiriVolume
_activeAudioProcessors
_continuousAudioProcessors
_lastForwardedSampleCount
_clientStartSampleCount
_listenPollingTimer
_clearLoggingFileTimer
_listenPollingTimerCount
_clearLoggingFileTimerCount
_pendingSetRecordModeToRecordingToken
_pendingSetRecordModeToRecordingCompletion
lowercaseString
hasPrefix:
substringFromIndex:
mutableCopy
replaceMatchesInString:options:range:withTemplate:
_stringByStrippingLeadingNoise:
_stringByStrippingTrailingNoise:
rangeOfString:options:
stringValue
_firstMatchesForRegularExpression:
firstMatchInString:options:range:
numberOfRanges
rangeAtIndex:
substringWithRange:
_stringByFixingNamePattern:
_stringByStrippingNoiseLeadingNoise:TrailingNoise:
_hasSubstring:
_matchesRegularExpression:
_caseInsensitiveHasMatchInEnumeration:
_firstMatchesForRegularExpressions:
getVoiceTriggerAssetTypeString
getEndpointAssetTypeString
dictionary
assetManagerEnabledPolicy
isEnabled
_fetchRemoteMetaData
setCallback:
assetOfType:language:
installedAssetOfType:language:
_isReadyToUse
predicateForAssetType:language:
installedAssetOfType:withPredicate:
_fetchRemoteAssetOfType:withPredicate:
_installedAssetOfType:withPredicate:
getCSAssetOfType:
_assetQueryForAssetType:withPredicate:localOnly:
runQueryAndReturnError:
predicate
_findLatestInstalledAsset:
state
isLatestCompareTo:
initWithAssetType:
setPredicate:
setQueriesLocalAssetInformationOnly:
startQuery:
isSpringboardStarted
isFirstUnlocked
supportHybridEndpointer
predicateForfetchRemoteMetadataForAssetType:
_runAssetQuery:completion:
_updateFromRemoteToLocalAssets:forAssetType:
isInstalled
isDownloading
cancelDownloadAndReturnError:
purgeAndReturnError:
_downloadAsset:withComplete:
numberWithBool:
_startDownloadingAsset:progress:completion:
setProgressHandler:
requiredDiskSpaceIsAvailable:error:
_defaultDownloadOptions
beginDownloadWithOptions:
resumeDownload:
adjustDownloadOptions:completion:
setObject:forKeyedSubscript:
CSVoiceTriggerAssetMetaUpdateMonitor:didReceiveNewVoiceTriggerAssetMetaData:
CSSpeechEndpointAssetMetaUpdateMonitor:didReceiveNewSpeechEndpointAssetMetaData:
installedAssetForCurrentLanguageOfType:
currentLanguageCode
addObserver:forAssetType:
removeObserver:forAssetType:
_csAssetsDictionary
_enablePolicy
_currentLanguageCode
_observers
getVoiceTriggerAssetCurrentCompatibilityVersion
getEndpointAssetCurrentCompatibilityVersion
supportPremiumAssets
componentsJoinedByString:
predicateWithFormat:argumentArray:
initWithManager:
startController
getFixedHighPrioritySerialQueueWithLabel:
twoShotNotificationEnabled
_currentAudioRecorderSampleRate
_initializeMediaPlayingState
_initializeAlarmState
_initializeTimerState
_setSoundPlayingState
_contextToString:
_getRecordSettings
_setupDownsamplerIfNeeded
_setupAudioConverter:
numberWithInt:
duckOthersOption
setDuckOthersOption:
_isVoiceTriggered
shouldRunVTOnCS
setObject:forKey:
_isAutoPrompted
elapsedTimeWithNoSpeech
speechControllerDidDetectVoiceTriggerTwoShot:atTime:
resetForNewRequestWithSampleRate:
setActiveChannel:
lpcmNonInterleavedASBD
lpcmInterleavedASBD
isRecordContextAutoPrompt:
sampleByteDepth
hostTime
processAudioSamplesAsynchronously:
addSamples:timestamp:
speechControllerLPCMRecordBufferAvailable:buffer:
data
addSamples:len:
channels
packetDescriptionCount
bytesDataSize
initWithCapacity:
packetDescriptions
initWithBytes:length:
timeStamp
speechControllerRecordBufferAvailable:buffers:recordedAt:
speechControllerDidStartRecording:successfully:error:
recordingStoppedForReason:
flush
speechControllerDidStopRecording:forReason:
_deviceAudioLogging
speechControllerRecordHardwareConfigurationDidChange:toConfiguration:
speechControllerDidUpdateSmartSiriVolume:forReason:
speechControllerBeginRecordInterruption:
speechControllerBeginRecordInterruption:withContext:
speechControllerEndRecordInterruption:
speechController:willSetAudioSessionActive:
speechController:didSetAudioSessionActive:
narrowBandOpusConverter
opusConverter
setAlertSoundFromURL:forType:
playAlertSoundForType:
alertStartTime
playRecordStartingAlertAndResetEndpointer
setMeteringEnabled:
updateMeters
peakPowerForChannel:
averagePowerForChannel:
passThruVoiceTriggerInfo
resetForVoiceTriggerTwoShotWithSampleRate:
speechControllerRequestsOperation:forReason:
metrics
setEndpointerDelegate:
processServerEndpointFeatures:
allKeys
initWithData:encoding:
_getSpeechIdentifier
lastEndOfVoiceActivityTime
endpointerModelVersion
updateEndpointerThreshold:
updateEndpointerDelayedTrigger:
shouldAcceptEagerResultForDuration:resultsCompletionHandler:
sharedController
audioConverterDidConvertPackets:packets:timestamp:
initializeRecordSessionWithContext:
preheat
resetAudioSession
releaseAudioSession
getLPCMAudioStreamBasicDescription
setSynchronousCallbackEnabled:
setRecordBufferDuration:
getRecordBufferDuration
startRecording:
isVoiceTriggered
peakPowerForOutputReference
averagePowerForOutputReference
outputReferenceChannel
voiceTriggerInfo
endpointAnalyzer
setEndpointAnalyzerDelegate:
resetEndpointer
getSmartSiriVolume
beginWaitingForMyriad
endWaitingForMyriadWithDecision:
delegate
endpointerProxy
setEndpointerProxy:
speechManager
setSpeechManager:
avvcContext
setAvvcContext:
isOpus
setIsOpus:
isActivated
setIsActivated:
setIsNarrowBand:
audioFileWriter
setAudioFileWriter:
activeChannel
setTwoShotNotificationEnabled:
isMediaPlaying
setIsMediaPlaying:
isAlarmPlaying
setIsAlarmPlaying:
isTimerPlaying
setIsTimerPlaying:
isSoundPlaying
setIsSoundPlaying:
myriadPreventingTwoShotFeedback
setMyriadPreventingTwoShotFeedback:
_opusAudioConverter
_narrowBandOpusConverter
_audioConverter
_downsampler
_requestedRecordSettings
_lastVoiceTriggerInfo
_twoShotAudibleFeedbackQueue
_twoShotAudibleFeedbackDecisionGroup
_isOpus
_isActivated
_isNarrowBand
_twoShotNotificationEnabled
_isMediaPlaying
_isAlarmPlaying
_isTimerPlaying
_isSoundPlaying
_myriadPreventingTwoShotFeedback
_delegate
_endpointerProxy
_speechManager
_avvcContext
_audioFileWriter
_activeChannel
_storeModeEnabled
setFileLoggingLevel:
fileLoggingLevel
intValue
baseDir
assistantLogDirectory
enumeratorAtURL:includingPropertiesForKeys:options:errorHandler:
getUserVoiceProfileUploadPathWithEnrolledLanguageList:
_CSSATUploadPath
_getEnrolledLanguageList
enumeratorAtPath:
_isDirectory:
pathExtension
copyItemAtPath:toPath:error:
_CSSATUpdatePath
isCurrentDeviceCompatibleWithVoiceProfileAt:
_markSATEnrollmentSuccessForLanguageCode:
_markSATEnrollmentMigratedForLanguageCode:
_markSATEnrollmentWithMarker:forLanguage:
createFileAtPath:contents:attributes:
deviceProductType
_deviceCategoryMap
dataWithContentsOfURL:
JSONObjectWithData:options:error:
interstitialRelativeDirForLevel:
voiceTriggerEnabled
voiceTriggerInCoreSpeech
setFileLoggingIsEnabled:
voiceTriggerAudioLogDirectory
getUserVoiceProfileFileList
getUserVoiceProfileUploadPath
notifyUserVoiceProfileUploadComplete
getUserVoiceProfileUpdateDirectory
notifyUserVoiceProfileUpdateReady
remoteVoiceTriggerDelayTime
remoteVoiceTriggerEndpointTimeoutWithDefault:
interstitialAbsoluteDirForLevel:
myriadFileLoggingEnabled
integerValue
initialState
setInitialState:
transitions
setTransitions:
_currentState
_initialState
_transitions
enumerateObservers:
CSEventMonitorDidReceiveEvent:
utteranceFileASBD
_closeAudioFile
fileURLWithPath:isDirectory:
startRecording
appendAudioData:
_audioFile
_asbd
_url
_audioLength
_addAssetManagerEnabledConditions
addConditions:
_didReceiveNewSpeechEndpointAssetMetaData
_notifyObserver:
_notifyToken
opusRecordSettings
alertMuteSettings
matchWithString:TrailingStr:LeadingStr:Pattern:
createGrammars
bundleForClass:
bundlePath
dataWithContentsOfFile:
_getTrailingPatternsWithGrammars:withLocale:
_getLeadingPatternsWithGrammars:withLocale:
_getRegexPatternsWithGrammars:withUtt:withLocale:
_getLMEWithGrammar:withLocale:
sharedGrammars
URLSession:didBecomeInvalidWithError:
URLSession:didReceiveChallenge:completionHandler:
URLSessionDidFinishEventsForBackgroundURLSession:
getTrailingPatternsForUtt:Locale:
getLeadingPatternsForUtt:Locale:
getRegexPatternsForUtt:Locale:
getLMEforLocale:
_grammar
hasRemoteCoreSpeech
rootQueueWithFixedPriority:
supportContinuousVoiceTrigger
supportKeywordDetector
supportSelfTriggerSuppression
supportCSTwoShotDecision
getFixedPrioritySerialQueueWithLabel:fixedPriority:
systemUpTime
initWithConfigPath:resourcePath:
resetBest
analyzeWavData:numSamples:
getAnalyzedResultForPhraseId:
sampleFed
getAnalyzedResult
keywordAnalyzerNDAPI:hasResultAvailable:forChannel:
initWithResult:
bestStart
setBestStart:
bestEnd
setBestEnd:
getSuperVectorWithEndPoint:
processAudioChunk:
activePhraseId
setActivePhraseId:
_novDetector
_lastSampleFed
_sampleFedWrapAroundOffset
_activePhraseId
stringByReplacingOccurrencesOfString:withString:
date
initWithTotalAudioRecorded:featuresAtEndpoint:endpointerType:serverFeatureLatencyDistribution:additionalMetrics:
totalAudioRecorded
setTotalAudioRecorded:
featuresAtEndpoint
setFeaturesAtEndpoint:
endpointerType
setEndpointerType:
serverFeatureLatencyDistribution
setServerFeatureLatencyDistribution:
additionalMetrics
setAdditionalMetrics:
_totalAudioRecorded
_featuresAtEndpoint
_endpointerType
_serverFeatureLatencyDistribution
_additionalMetrics
waitingForConnection:error:
isConnected
localURL
string
_compatibilityVersion
appendString:
_version
appendFormat:
_footprint
assetForAssetType:resourcePath:configVersion:
attributes
isPremium
lpcmInt16ASBD
lpcmInt16NarrowBandASBD
opusASBD
opusNarrowBandASBD
aiffFileASBD
_checkVoiceTriggerEnabled
_didReceiveVoiceTriggerSettingChanged:
_notifyObserver:withEnabled:
CSVoiceTriggerEnabledMonitor:didReceiveEnabled:
_didReceiveVoiceTriggerSettingChangedInQueue:
_isVoiceTriggerEnabled
startRecordingWithOptions:error:
stopRecording:
didPlayEndpointBeep
voiceTriggerEventInfo
hasPendingTwoShotBeep
audioSessionRecordBufferAvailable:
audioSessionDidStartRecording:error:
audioSessionDidStopRecording:
convertStopReason:
audioSessionErrorDidOccur:
prepareRecord
audioSource
resetEndPointer
hasAudioRoute
hasCorrectAudioRoute
averagePower
_didReceiveSiriSettingChanged:
beepCancellerDidCancelSamples:buffer:timestamp:
numberWithUnsignedLongLong:
cancelBeepFromSamples:timestamp:
willBeep
_beepCanceller
_beepFloatVec
_shortBuffer
_numTotalInputSamples
_numTotalOutputSamples
numberWithUnsignedLong:
initWithZeroWindowSize:approxAbsSpeechThreshold:numHostTicksPerAudioSample:
filterZerosInAudioPacket:atBufferHostTime:filteredPacket:
endAudioAndFetchAnyTrailingZerosPacket:
_audioZeroFilterImpl
readAudioChunksFrom:block:
_subscribeEventMonitors
subscribeEventMonitor:
getTestResponse:
interfaceWithProtocol:
setWithArray:
setClasses:forSelector:argumentIndex:ofReply:
sampleCountFromHostTime:anchorHostTime:anchorSampleCount:
hostTimeFromSampleCount:anchorHostTime:anchorSampleCount:
stringWithCString:encoding:
createAudioCircularBufferWithDefaultSettings
addSamples:numSamples:
copySamplesFromHostTime:
copyBufferWithNumSamplesCopiedIn:
saveRecordingBufferFrom:to:toURL:
setBufferLength:
_csAudioCircularBufferImpl
_anchorSampleCount
_anchorHostTime
_bufferLength
_asssetMetaUpdatedKey
_didReceiveNewVoiceTriggerAssetMetaData
initWithWordCount:trailingSilenceDuration:eosLikelihood:pauseCounts:silencePosterior:taskName:processedAudioDurationInMilliseconds:
initWithWordCount:trailingSilenceFrames:endOfSilenceLikelihood:pauseCounts:silencePosterior:taskName:
wordCount
setWordCount:
trailingSilenceDuration
setTrailingSilenceDuration:
eosLikelihood
setEosLikelihood:
pauseCounts
setPauseCounts:
silencePosterior
setSilencePosterior:
processedAudioDurationInMilliseconds
setProcessedAudioDurationInMilliseconds:
taskName
setTaskName:
_wordCount
_trailingSilenceDuration
_eosLikelihood
_pauseCounts
_silencePosterior
_processedAudioDurationInMilliseconds
_taskName
_configureWithASBD:andFrameRate:
_resetWithSampleRate:
_configureWithSampleRate:andFrameRate:
subChunkFrom:numSamples:forChannel:
_processAudioSamples:
_detectVoiceActivityInSamples:numSamples:
replaceBytesInRange:withBytes:length:
endpointer:didDetectStartpointAtTime:
_getEndpointMetricsForAudioTimestamp:
endpointer:didDetectHardEndpointAtTime:withMetrics:
endpointStyle
setEndpointStyle:
delay
setDelay:
startWaitTime
setStartWaitTime:
automaticEndpointingSuspensionEndTime
setAutomaticEndpointingSuspensionEndTime:
minimumDurationForEndpointer
setMinimumDurationForEndpointer:
lastStartOfVoiceActivityTime
bypassSamples
setBypassSamples:
endpointMode
setEndpointMode:
interspeechWaitTime
setInterspeechWaitTime:
endWaitTime
setEndWaitTime:
saveSamplesSeenInReset
setSaveSamplesSeenInReset:
canProcessCurrentRequest
handleVoiceTriggerWithActivationInfo:
sampleRate
setSampleRate:
frameRate
setFrameRate:
detectedOneShotStartpoint
setDetectedOneShotStartpoint:
detectedRecurrentStartpoint
setDetectedRecurrentStartpoint:
communicatedStartPointDetection
setCommunicatedStartPointDetection:
detectedOneShotEndpoint
setDetectedOneShotEndpoint:
detectedRecurrentEndpoint
setDetectedRecurrentEndpoint:
communicatedEndpointDetection
setCommunicatedEndpointDetection:
samplesSeen
setSamplesSeen:
numSamplesProcessed
setNumSamplesProcessed:
lastOneShotStartpoint
setLastOneShotStartpoint:
lastOneShotEndpoint
setLastOneShotEndpoint:
lastRecurrentStartpoint
setLastRecurrentStartpoint:
lastRecurrentEndpoint
setLastRecurrentEndpoint:
floatSampleBuffer
setFloatSampleBuffer:
topLevelParameterDict
setTopLevelParameterDict:
modelDictPath
setModelDictPath:
isConfigured
setIsConfigured:
previousSamplesSeen
setPreviousSamplesSeen:
apQueue
setApQueue:
recordingDidStop
setRecordingDidStop:
vtEndInSampleCount
setVtEndInSampleCount:
_audioUnitEPVAD2
_saveSamplesSeenInReset
_detectedOneShotStartpoint
_detectedRecurrentStartpoint
_communicatedStartPointDetection
_detectedOneShotEndpoint
_detectedRecurrentEndpoint
_communicatedEndpointDetection
_isConfigured
_recordingDidStop
_frameRate
_endpointStyle
_endpointMode
_interspeechWaitTime
_startWaitTime
_endWaitTime
_automaticEndpointingSuspensionEndTime
_minimumDurationForEndpointer
_bypassSamples
_delay
_samplesSeen
_numSamplesProcessed
_lastOneShotStartpoint
_lastOneShotEndpoint
_lastRecurrentStartpoint
_lastRecurrentEndpoint
_floatSampleBuffer
_topLevelParameterDict
_modelDictPath
_previousSamplesSeen
_apQueue
_vtEndInSampleCount
dictionaryWithContentsOfFile:
initWithArray:
enumerateObjectsUsingBlock:
subdataWithRange:
dataWithCapacity:
appendData:
_data
_numSamples
_startSampleCount
_hostTime
bestPhrase
bestScore
earlyWarning
setSampleFed:
setBestPhrase:
setBestScore:
setEarlyWarning:
isRescoring
setIsRescoring:
_earlyWarning
_isRescoring
_bestScore
_sampleFed
_bestPhrase
_bestStart
_bestEnd
initWithConfiguration:modelVersion:
addAudio:numSamples:
updateEndpointerThresholdWithValue:
updateEndpointerDelayedTriggerSwitch:
timeIntervalSinceDate:
numberWithDouble:
silenceFramesCountMs
silenceProbability
silenceDurationMs
initWithWordCount:trailingSilenceDuration:endOfSentenceLikelihood:pauseCounts:silencePosterior:clientSilenceFramesCountMs:clientSilenceProbability:silencePosteriorNF:serverFeaturesLatency:eagerResultEndTime:
acceptEagerResultWithFeatures:featuresToLog:
processedAudioMs
defaultServerEndpointFeatures
endOfSentenceLikelihood
initWithWordCount:trailingSilenceDuration:endOfSentenceLikelihood:pauseCounts:silencePosterior:clientSilenceFramesCountMs:clientSilenceProbability:silencePosteriorNF:serverFeaturesLatency:
didEndpointWithFeatures:audioTimestamp:featuresToLog:endpointPosterior:extraDelayMs:
getFrameDurationMs
serverFeaturesLatencyDistributionDictionary
dictionaryWithDictionary:
sortUsingComparator:
objectAtIndexedSubscript:
initWithConfigFile:samplingRate:queue:
initWithSilenceFramesCountMs:silenceProbability:silenceDurationMs:silencePosterior:processedAudioMs:
requestSupportedWithSamplingRate:
_getCSHybridEndpointerConfigForAsset:
_updateAssetWithLanguage:
_updateAssetWithCurrentLanguage
resourcePath
clientSilenceFeaturesAvailable:
silenceDurationEstimateAvailable:numEstimates:clientProcessedAudioMs:
setCanProcessCurrentRequest:
currentAsset
setCurrentAsset:
didAddAudio
setDidAddAudio:
caesuraSPG
setCaesuraSPG:
clientSilenceFeaturesAtEndpoint
setClientSilenceFeaturesAtEndpoint:
hybridClassifier
setHybridClassifier:
setEndpointerModelVersion:
serverFeaturesQueue
setServerFeaturesQueue:
lastKnownServerEPFeatures
setLastKnownServerEPFeatures:
serverFeatureLatencies
setServerFeatureLatencies:
serverFeaturesWarmupLatency
setServerFeaturesWarmupLatency:
lastServerFeatureTimestamp
setLastServerFeatureTimestamp:
didReceiveServerFeatures
setDidReceiveServerFeatures:
clientLagThresholdMs
setClientLagThresholdMs:
clampedSFLatencyMsForClientLag
setClampedSFLatencyMsForClientLag:
useDefaultServerFeaturesOnClientLag
setUseDefaultServerFeaturesOnClientLag:
hybridClassifierQueue
setHybridClassifierQueue:
lastReportedEndpointTimeMs
setLastReportedEndpointTimeMs:
stateSerialQueue
setStateSerialQueue:
didCommunicateEndpoint
setDidCommunicateEndpoint:
currentRequestSampleRate
setCurrentRequestSampleRate:
vtExtraAudioAtStartInMs
setVtExtraAudioAtStartInMs:
hepAudioOriginInMs
setHepAudioOriginInMs:
firstAudioPacketTimestamp
setFirstAudioPacketTimestamp:
didTimestampFirstAudioPacket
setDidTimestampFirstAudioPacket:
silencePosteriorGeneratorQueue
setSilencePosteriorGeneratorQueue:
didDetectSpeech
setDidDetectSpeech:
setElapsedTimeWithNoSpeech:
_canProcessCurrentRequest
_didAddAudio
_didReceiveServerFeatures
_useDefaultServerFeaturesOnClientLag
_didCommunicateEndpoint
_didTimestampFirstAudioPacket
_didDetectSpeech
_caesuraSPG
_clientSilenceFeaturesAtEndpoint
_hybridClassifier
_endpointerModelVersion
_serverFeaturesQueue
_lastKnownServerEPFeatures
_serverFeatureLatencies
_serverFeaturesWarmupLatency
_lastServerFeatureTimestamp
_clientLagThresholdMs
_clampedSFLatencyMsForClientLag
_hybridClassifierQueue
_lastReportedEndpointTimeMs
_stateSerialQueue
_currentRequestSampleRate
_vtExtraAudioAtStartInMs
_hepAudioOriginInMs
_firstAudioPacketTimestamp
_silencePosteriorGeneratorQueue
_elapsedTimeWithNoSpeech
getHostClockFrequency
secondsToHostTime:
hostTimeToSeconds:
_shouldEnterTwoShotAtEndPointTime:
_shouldUseVAD2ForTwoShot
endpointerDelegate
hybridEndpointer
setHybridEndpointer:
vad2Endpointer
setVad2Endpointer:
activeEndpointer
setActiveEndpointer:
didEnterTwoshot
setDidEnterTwoshot:
_didEnterTwoshot
_endpointerDelegate
_hybridEndpointer
_vad2Endpointer
_activeEndpointer
initWithBool:
initWithDouble:
initWithLongLong:
initWithUnsignedLongLong:
objCType
longLongValue
_checkFirstUnlocked
_notifyObserver:withUnlocked:
CSFirstUnlockMonitor:didReceiveFirstUnlock:
_didReceiveFirstUnlockInQueue:
_didReceiveFirstUnlock:
_firstUnlocked
_scaleDecayConstants:
_savePeaks:averagePower:maxSample:
_linearToDB:
_ampToDB:
initWithSampleRate:
process:stride:inFrameToProcess:
getPeakPowerDB
getAveragePowerDB
_averagePowerI
_instantaneousMode
_peak
_maxPeak
_decay
_peakDecay
_averagePowerPeak
_peakHoldCount
_previousBlockSize
_decay1
_peakDecay1
_checkSpringBoardStarted
_didReceiveSpringboardStarted:
_notifyObserver:withStarted:
CSSpringboardStartMonitor:didReceiveStarted:
_didReceiveSpringboardStartedInQueue:
_isSpringBoardStarted
componentsSeparatedByString:
processInfo
systemUptime
initWithConfiguration:
removeAllObjects
runRecognitionWithResultStream:
endAudio
_recognizeWavData:length:
addAudioSamples:count:
tokens
_getConfidence:
keywordAnalyzerQuasar:hasResultAvailable:forChannel:
firstObject
addObjectsFromArray:
lastObject
confidence
tokenName
speechRecognizer:didRecognizePartialResult:
speechRecognizer:didFinishRecognitionWithError:
speechRecognizer:didRecognizeFinalResults:
speechRecognizer:didRecognizeFinalResults:tokenSausage:nBestChoices:
speechRecognizer:didRecognizeFinalResultPackage:
speechRecognizer:didProcessAudioDuration:
speechRecognizer:didRecognizeRawEagerRecognitionCandidate:
speechRecognizer:didProduceEndpointFeaturesWithWordCount:trailingSilenceDuration:eosLikelihood:pauseCounts:silencePosterior:processedAudioDurationInMilliseconds:
initWithConfigPath:triggerTokens:useKeywordSpotting:
runRecognition
triggerConfidence
_previousUtteranceTokens
_triggerTokenList
_recognizer
_recognizerBuffer
_useKeywordSpotting
_triggerConfidence
hybridEndpointerAssetFilename
initWithResourcePath:configFile:configVersion:
_decodeJson:
assetHashInResourcePath:
fallBackAssetResourcePath
getBoolForKey:category:default:
getStringForKey:category:default:
hashFromResourcePath
isEqualAsset:
configVersion
_decodedInfo
_path
_resourcePath
_configVersion
_configureAudioConverter:
_convertBufferedLPCM:allowPartial:timestamp:
_opusConverter
_bufferedLPCM
_recordBasePacketsPerSecond
_opusOutASBD
_convertPacketCount
_convertAudioCapacity
_lastTimestamp
_notifyObserver:withLanguageCode:
_didReceiveLanguageCodeUpdate
_checkAllConditionsEnabled
notifyCallback:
_monitors
_conditions
_callback
initWithDictionary:
enumerateKeysAndObjectsUsingBlock:
speakerDetectorThreshold
_initializeSAT:
_computeSATScore:
speakerDetector:didDetectSpeaker:
speakerDetector:didDetectSpeakerReject:
initWithAsset:speakerModel:
_initializeNDAPI:resourcePath:
processSuperVector:withResult:
analyzeWavForEnrollment:numSamples:
addLastTriggerToProfile
_threshold
_spkModel
_voiceControllerWithContext:error:
_recordingSampleRate
_destroyVoiceController
setRecordDelegate:
setPlaybackDelegate:
_shouldUseRemoteRecordForContext:
_createDeInterleaverIfNeeded
_createSampleRateConverterIfNeeded
_shouldRunZeroFilter
_resetZeroFilter
playbackRoute
_deinterleaveBufferIfNeeded:
_samplingRateConvertIfNeeded:
_processAudioChainWithZeroFiltering:atTime:
_processAudioChain:atTime:
playAlertSoundForType:overrideMode:
sharedAnalytics
logEventWithType:context:
_audioRecorderDidStartRecordingSuccessfully:error:
_audioRecorderDidStopRecordingForReason:
voiceControllerDidStartRecording:successfully:
voiceControllerDidStartRecording:successfully:error:
voiceControllerDidStopRecording:forReason:
voiceControllerDidDetectStartpoint:
voiceControllerDidDetectEndpoint:ofType:
voiceControllerDidDetectEndpoint:ofType:atTime:
voiceControllerEncoderErrorDidOccur:error:
voiceControllerRecordHardwareConfigurationDidChange:toConfiguration:
voiceControllerBeginRecordInterruption:
voiceControllerBeginRecordInterruption:withContext:
voiceControllerEndRecordInterruption:
voiceControllerMediaServicesWereLost:
voiceControllerMediaServicesWereReset:
voiceControllerWillSetAudioSessionActive:willActivate:
voiceControllerDidSetAudioSessionActive:isActivated:
voiceControllerRecordBufferAvailable:buffer:
voiceControllerLPCMRecordBufferAvailable:buffer:
voiceControllerPlaybackBufferAvailable:buffer:
voiceControllerDidStartPlaying:successfully:
voiceControllerDidStopPlaying:forReason:
voiceControllerDecoderErrorDidOccur:error:
voiceControllerPlaybackHardwareConfigurationDidChange:toConfiguration:
voiceControllerBeginPlaybackInterruption:
voiceControllerEndPlaybackInterruption:
enableVoiceTriggerOnAOP:
updateVoiceTriggerAOPModel:
voiceTriggerOccuredNotification:
_voiceController
_zeroFilter
_deinterleaver
_interleavedABL
_pNonInterleavedABL
_needSampleRateConversion
_remoteRecordClient
_latestContext
_shouldUseRemoteRecord
isWriting
fFile
inASBD
outASBD
_fileURL
%s Celestial is not available on this platform.
%s notification = %{public}@
%s MobileTimer is not available on this platform.
%s Get initial state from MediaRemote: media is on playing state %{public}ld.
%s Start monitoring MediaRemote: media playback
%s Stop monitoring MediaRemote: media playback
%s MediaRemote reported the now playing app playback state changed to %s (state %d)
%s In %@: Continuous digital zero detected, lasting %{public}u samples per channel
%s In %@: Continuous digital zero in this audio chunk detected, lasting %{public}u samples per channel
%s Plan removing the temp file %{public}@
%s Failed to remove temp file %{public}@ reason: %{public}@
%s Start copying %{public}u bytes of data to crashreporter
%s Failed to read data from %{public}@
%s Finished copying data to crashreporter.
%s Logging audio file into : %{public}@
%s CS logging files under %{public}@ created before %{public}@ will be removed.
%s Couldn't get creation date: %{public}@
%s Could not remove %{public}@: %{public}@
%s CS logging files with pattern %{public}@ under %{public}@ exceeding limit, only keep the latest %{public}lu ones
%s SmartSiriVolume: deleted %{public}u elements in energy buffer.
%s SmartSiriVolume: number of elements to delete exceeds energy buffer size, ignore.
%s SmartSiriVolume init value for noise estimation %{public}f
%s SmartSiriVolume init value for LKFS estimation %{public}f
%s SmartSiriVolume received MediaRemote initial state as %{public}@
%s SmartSiriVolume haven't got MediaRemote callback yet, let's assume media is playing.
%s SmartSiriVolume received alarm initial state as %{public}@
%s SmartSiriVolume received timer initial state as %{public}@
%s asset is nil, use default parameters(this should not happen).
%s SmartSiriVolume configure: %{public}@
%s SmartSiriVolume heartbeat = %{public}lld
%s SmartSiriVolume: estimated noise level %{public}f
%s SmartSiriVolume: estimated LKFS %{public}f
%s SmartSiriVolume: pause SSV calculation.
%s SmartSiriVolume: resume SSV calculation.
%s SmartSiriVolume received VT event!
%s SmartSiriVolume remove samples from VT utterances by %{public}llu, with startAnalyzeSampleCount = %{public}llu, samplesFed = %{public}llu, triggerStartSampleCount = %{public}llu
%s SmartSiriVolume trying to delete too many VT samples, set triggerDurationToDelete to be limited max: %{public}llu
%s SmartSiriVolume got empty VT event!
%s SmartSiriVolume: final estimated TTS volume in dB %{public}f
%s SmartSiriVolume: adjust TTS volume since alarm/timer is firing.
%s SmartSiriVolume: TTS volume in dB from noise %{public}f, from LKFS %{public}f, with user offset %{public}f
%s SmartSiriVolume: pause LKFS calculation according to MediaRemote notification.
%s SmartSiriVolume: resume LKFS calculation according to MediaRemote notification.
%s SmartSiriVolume received unknown media playing state, let's assume media is playing.
%s SmartSiriVolume: alarm is firing according to MobileTimer notification.
%s SmartSiriVolume: alarm is not firing according to MobileTimer notification.
%s SmartSiriVolume received unknown alarm state, let's reset alarm state.
%s SmartSiriVolume: timer is firing according to MobileTimer notification.
%s SmartSiriVolume: timer is not firing according to MobileTimer notification.
%s SmartSiriVolume received unknown timer state, let's reset timer state.
%s SmartSiriVolume: set StartAnalyzeSampleCount = %{public}lld
%s Couldn't find keychain value %@ for account %@ %{public}d
%s ::: Error reading file %{public}@, err: %{public}d
%s ::: Error getting data format from audio file : %{public}d
%s ::: Error getting data length from audio file : %{public}d
%s ::: Error reading data from audio file : %{public}d
%s Cannot create SampleRateConverter using AudioConverterNew : %{public}d
%s Cannot set Quality property to audioConverter
%s Cannot set Complexity property to audioConverter
%s Audio resampling done : %tu
%s AudioConverter is sad: 0x%{public}xd
%s xpc object string return nil
%s xpc object should be XPC_TYPE_STRING
%s Siri language is nil, falling back to %@
%s Cannot create directory since directory is nil
%s same name of file exists, this will be removed
%s Creating Directory : %{public}@
%s Creating Directory failed : %{public}@
%s Cannot remove model directory(%@) : %@
%s Cannot remove utterance directory(%@) : %@
%s setting delay interstitial %{public}d sounds with level : %{public}d
%s CSCoreSpeechServices Invalidated
%s Cannot register interstitial sounds : %{public}@
%s Successfully register interstitial
%s XPC Connection is not exist?
%s Siri is enabled, let start!!
%s Siri is not enabled yet we are keep waiting
%s CSSmartSiriVolumeAsset found: %{public}@
%s Cannot find smartSiriVolume asset from asset manager, let's fallback to asset in the framework
%s speechController = %{public}p
%s event : %{public}@
%s context = %{public}@
%s Creating new CSAudioRecorder with context : %{public}@
%s Cannot create audio recorder : %{public}@
%s It cannot change context because it is recording
%s Cannot change context : %{public}@
%s recordingSettings = %{public}@
%s AVVC already recording.
%s setRecordModeToRecordingDelay = %{public}f
%s timeIntervalSinceLastTriggerEnd = %{public}f
%s Failed SetRecordModeToRecording with %{public}f seconds delay from prepareRecorder due to error %{public}@.
%s Finished SetRecordModeToRecording with %{public}f seconds delay from prepareRecorder.
%s Cancelled SetRecordModeToRecording with %{public}f seconds delay from prepareRecorder.
%s Scheduled SetRecordModeToRecording with %{public}f seconds delay in prepareRecorder.
%s AVVC already recording, change record mode to recording here.
%s AVVC already recording, This shouldn't be called
%s AVVC Prepare recording failed : %{public}@
%s recordingSettings : %{public}@
%s AVVC already recording, nothing to prepare
%s Not supported in this platform
%s context : %{public}@
%s Cannot set context since mediaserverd is recovering from crash
%s Cannot prepare since mediaserverd is recovering from crash
%s Cannot prepare since audio recorder was not initialized
%s recordingSettings from CS : %{public}@
%s settings : %{public}@
%s startRecording failed : %{public}@
%s mode : %{public}ld
%s Delayed SetRecordModeToRecording: Consumed token %{public}@ with %{public}f seconds delay for reason %{public}@.
%s Delayed SetRecordModeToRecording: Setting record mode to recording for reason %{public}@.
%s Delayed SetRecordModeToRecording: Failed to set record mode to recording for reason %{public}@ due to error %@.
%s Delayed SetRecordModeToRecording: Successfully set record mode to recording for reason %{public}@.
%s Delayed SetRecordModeToRecording: Ignored set record mode to recording for reason %{public}@ because the validator rejected.
%s Delayed SetRecordModeToRecording: Ignored set record mode to recording for reason %{public}@ because the scheduled token %{public}@ does not match the current token %{public}@.
%s Delayed SetRecordModeToRecording: Scheduled new token %{public}@ with %{public}f seconds delay for reason %{public}@.
%s Delayed SetRecordModeToRecording: Cancelled token %{public}@ for reason %{public}@.
%s Delayed SetRecordModeToRecording: Consumed token %{public}@ in advance for reason %{public}@.
%s Activate Context = %{public}@
%s setCurrentContext failed : %{public}@
%s sessionOptions : %{public}tu
%s startRecordingBy %{public}@
%s start recording since mediaserverd is recovering from crash
%s _lastForwardedSampleCounts = %{public}tu, audioBufferSampleCount = %{public}tu
%s Failed SetRecordModeToRecording with %{public}f seconds delay from startRecording due to error %{public}@.
%s Finished SetRecordModeToRecording with %{public}f seconds delay from startRecording.
%s Cancelled SetRecordModeToRecording with %{public}f seconds delay from startRecording.
%s Scheduled SetRecordModeToRecording with %{public}f seconds delay in startRecording.
%s Try start recording under PollingListening state.
%s Try start recording under Stopping state.
%s Requested startHostTime = %{public}llu, _clientStartSampleCount = %{public}tu
%s Cannot use opportunisticZLL since _clientStartSampleCount is newer than audioBuffer sample count
%s Opportunistic ZLL will be used starting from : %{public}tu
%s Opportunistic ZLL will not be used : %{public}tu
%s Generating fake didStartRecording delegate
%s Generating fake didStopRecording delegate
%s AudioRecorder lost mediaserverd connection
%s Mediaserverd recovered from crash
%s from:%{public}@ to:%{public}@ by:%{public}@
%s We do nothing for transition between %{public}@ and %{public}@
%s Ignore event(%{public}@) from(%{public}@) since we don't have transition
%s Trying to startListening
%s _createRecorderWithContextIfNeeded failed, it will try again %{public}f seconds later
%s _prepareListenWithSettings failed, it will try again %{public}f seconds later
%s startListening succeed
%s startListening failed, it will try again %{public}f seconds later
%s Listen polling is already started, ignore startListenPolling request.
%s No listen polling timer is on, ignore stopListenPolling request.
%s Still recording, let's do not destroy.
%s ClientSpeechController is nil
%s ignore because lastForwardedSampleCount:%{public}lu, theMostRecentSampleCount:%{public}lu
%s Buffer underrun!!!!, lastForwardedSampleTime:%{public}lu, oldestSampleTimeInBuffer:%{public}lu
%s %{public}@, sucessfully:%{public}@ error:%{public}@
%s forward to speechManagerDidStartForwarding
%s ignore audioRecorderDidStartRecording
%s forReason : %{public}ld
%s forward to speechManagerDidStopForwarding
%s ignore audioRecorderDidStopRecording
%s toConfiguration: %{public}ld
%s context: %{public}@
%s active : %{public}d
%s Language Code Changed : %{public}@
%s SmartSiriVolume: final estimated TTS volume %{public}f with music volume %{public}f
%s Wrongly called: SmartSiriVolume is not supported on this device type.
%s We stop listen polling since we anyway going to stop
%s Trying to start clear logging files
%s Clear logging file timer is already started, ignore startClearLoggingFilesTimer request.
%s init-_currentLanguageCode: %{public}@
%s Not able to fetch remote meta now, registering for callback
%s CSAssetManager cannot query for nil language
%s Error running asset-query for assetType:%{public}lu, query: %{public}@, predicate: %{public}@, error: %{public}@
%s ::: found %{public}lu assets for assetType=%{public}lu, matching query: %{public}@
%s ::: %{public}s
%s ::: predicate: %{public}@
%s ::: %{public}s; query: %{public}@
%s Error running asset query: error %{public}@
%s ::: Request Fetching RemoteMetaData
%s ::: Request fetching remote asset
%s ::: Fetching remote asset
%s ::: Purging installed asset : %{public}@
%s ::: Request downloading remote asset
%s ::: Start downloading asset
%s ::: download progress: %{public}3.0f%%
%s ::: Error downloading; %{public}@
%s ::: download completed successfully.
%s Attempting to download asset %{public}@
%s Failure resuming paused voice asset %{public}@
%s Asset doesn't need downloading, invoking completion
%s _currentLanguageCode changed: %{public}@
%s ERR: Unknown AssetType: %{public}lu
%s Context : %{public}@
%s Resetting CoreSpeech frameworks
%s Ask start recording from: %{public}tu
%s Voice trigger to use the current voice triggered channel: %{public}tu
%s Auto prompt to use the last voice triggered channel: %{public}tu
%s SpeechController to receive data from default channel
%s Start recording invoked too late, override scheduledCheckTime: %{public}llu to currentTime: %{public}llu
%s Scheduled audible feedback decision after %{public}.3fseconds (vtEndMachTime: %{public}llu currentMachTime: %{public}llu)
%s Two shot audible feedback decision timed out while waiting for Myriad decision
%s Two shot audible feedback decision not needed since we already stopped recording
%s Two shot audible feedback decision (%{public}.3fs later than the scheduled time), elapsedTimeWithNoSpeech: %{public}.3f
%s Two shot audible feedback is needed, should notify? [%{public}@]
%s Ask delay audio session active by %{public}f seconds
%s %{public}@
%s SpeechManager still forwarding audio after didStopForwarding, we shouldn't have this
%s AVVCAudioBuffer contains %{public}d packet descriptions, size %{public}d, channels %{public}d. Ignoring
%s packetCount %{public}d
%s Bad packet length %{public}d. Skipping rest of record buffer.
%s SpeechController is trying to forward encoded audio after didStopForwarding, we shouldn't have this
%s Not available
%s Two shot is detected at time %{public}.3f, should notify? [%{public}@]
%s Requesting QuickStop operation upon detecting keyword
%s Received Myriad started
%s Received Myriad finished with decision: %tu
%s Received unknown media playing state, ignoring
%s Received unknown alarm playing state, ignoring
%s Received unknown timer playing state, ignoring
%s Detected sound is%{public}@ playing
%s Couldn't create speech log directory at path %{public}@ %{public}@
%s Cannot delete existing SATUpload Diretory : %{public}@
%s Cannot create SAT Upload Directory : %{public}@
%s Cannot create directory(%{public}@)
%s Cannot copy file from %{public}@ to %{public}@ : %{public}@
%s PHS update directory already exists, remove before we move forward
%s Failed to delete PHS update directory
%s Failed to create PHS update directory
%s We need SAT directory, deleting the file with same name first
%s Failed to get device hash list %{public}@
%s Processing sync data from device hash: %{public}@
%s Error to copy profile from %{public}@ to %{public}@, error: %{public}@
%s language: %{public}@, enableVTAfterSyncLanguage: %{public}@, currSiriLanguage: %{public}@
%s Enabling VoiceTrigger Upon VoiceProfile sync for language: %{public}@
%s VoiceTrigger does not exist for this platform, not setting VoiceTriggerEnabled
%s Not enabling VoiceTrigger Upon VoiceProfile sync for language: %{public}@
%s Sucessfully migrated language %{public}@
%s Migrated language %{public}@ but failed to mark SAT enrollment
%s Sucessfully marked as migrated for language : %{public}@
%s Failed to mark migrated for language : %{public}@
%s Failed to remove update path [%{public}@] upon migration completion, error: %{public}@
%s Coudn't mark SAT enrollment %{public}@ at path %{public}@
%s Marked SAT enrollment %{public}@ at path %{public}@
%s We can't mark SAT {public}%@ when there is no audio directory
%s ERR: Unknown device. returning false: %{public}@
%s Malformed audio-dir URL for string <%{public}@>:url
%s ERR: reading contents of audioDir: %{public}@
%s No jsonFiles found in %{public}@: jsonFiles.count=%{public}lu
%s Unexpected: empty JSON data for file: %{public}@
%s Error reading metaDict at path: %{public}@
%s metaProductType: %{public}@
%s CurrentDevice{%{public}@:%{public}@} matched with: {%{public}@:%{public}@}
%s vtProfile{%{public}@:%{public}@} does NOT matcch with currDevice{%{public}@:%{public}@}
%s Could not find productType in VT-Meta file, trying next one
%s No compatible VT profile found for CurrDevice: %{public}@: %{public}@
%s Failure disposing audio file %{public}d
%s Audio file already configured, closing first
%s Creating audio file at URL %{public}@
%s Failed creating audio file at url %{public}@ %{public}d
%s Error setting input format %{public}d
%s No audio file to append data
%s Failed writing audio file %{public}d
%s Closing file at URL %{public}@, audio size: %{public}u
%s Cannot setAlertSoundFromURL since mediaserverd is recovering
%s Cannot playAlertSoundForType since mediaserverd is recovering
%s Cannot playRecordStartingAlertAndResetEndpointer since mediaserverd is recovering
%s Start monitoring : speech endpoint asset meta update
%s Stop monitoring : speech endpoint asset meta update
%s New speech endpoint asset is available
%s NDAPI initialization failed
%s set StartAnalyzeSampleCount = %{public}lld
%s Couldn't create CoreSpeech log directory at path %{public}@ %{public}@
%s Dealloc of CSRemoteControlClient, it should close connection
%s VoiceTrigger is already %{public}@, received duplicated notification!
%s Start monitring : VoiceTrigger setting switch
%s Cannot start monitoring VoiceTrigger setting switch because it was already started
%s Stop monitring : VoiceTrigger setting switch
%s VoiceTrigger enabled = %{public}@
%s Creating new CSAudioRecorder with context : %@
%s AudioRecorder creation failed : %@
%s Cannot prepare since audio recorder does not exist
%s AudioRecorder is already recording, do not prepare anymore
%s Cannot prepareRecordWithSettings : %@
%s Start monitoring : Siri setting switch, Siri is %{public}@
%s Stop monitoring : Siri setting switch
%s Siri Enabled = %{public}@
%s BeepCanceller asset file loading from : %{public}@
%s beepVector Size = %{public}lu
%s Cannot initialize beep canceller
%s Beep canceller initialized with maxNumSamples = %{public}d
%s It will beep now
%s Reset beep cancellation
%s Error reading audio file: %{public}d, skipping...
%s numChannels: %{public}lu, recordingDuration: %{public}f, sampleRate: %{public}f
%s Cannot copy samples since this is empty
%s Could NOT copyFrom: %{public}lu to: %{public}lu, retSampleCount: %{public}lu
%s copyBuffer: oldestSample: %{public}lu latestSample: %{public}lu, numSamplesCopied: %{public}lu
%s CSAudioCircularBuffer.reset
%s saveRecordingBufferFrom: %{public}lu to: %{public}lu toURL: %{public}@
%s csrb: %{public}@
%s Invalid request: (%{public}lu, %{public}lu): noting to write to file
%s Invalid request: reqStartSample=%{public}lu, reqEndSample=%{public}lu, oldestSampleInBuffer: %{public}lu, latestSampleInBuffer=%{public}lu
%s Start monitoring : VoiceTrigger Asset meta update
%s Stop monitoring : VoiceTrigger Asset meta update
%s New VoiceTrigger asset metadata is available
%s VAD2 preheat...
%s CSVAD2EndpointAnalyzer: resetForNewRequestWithSampleRate
%s ERR: Deprecated VAD2 reset called
%s %{public}@ Resetting with style %{public}ld, _samplesSeen: %{public}f, newSampleRate: %{public}tu, _sampleRate: %{public}f
%s _audioUnitEPVAD2=%{public}p, auNeedsReset: %{public}d
%s Failed to reset EPVAD2: %{public}d
%s vtEndInSecs: %{public}f, _vtEndInSampleCount: %{public}lu, voiceTriggerInfo: %{public}@,
%s VAD2::RecordingDidStop: Ignoring processAudioSamplesAsynchronously, not queueing
%s VAD2::RecordingDidStop: Ignoring processAudioSamplesAsynchronously from async
%s Already communicated endpoint...returning
%s VAD2::RecordingDidStop: Ignoring _processAudioSamples
%s Empty samplesBuffer!
%s Received audio buffer with 8 frames of zeroes
%s Not configured
%s done: %{public}d, _detectedOneShotStartpoint: %{public}d, _communicatedEndpointDetection: %{public}d, _startWaitTime: %{public}f_samplesSeen: %{public}f, _delay: %{public}f, _sampleRate: %{public}f(_startWaitTime + _delay) * _sampleRate): %{public}f, (_samplesSeen / _sampleRate): %{public}f, _automaticEndpointingSuspensionEndTime: %{public}f
%s No startpoint detected after %{public}f, timing out, _samplesSeen: %f, _samplesSeen(ms): %f, _lastOneShotEndpoint: %f
%s Ignoring recurrent endpoint at %{public}f becuase it's too early (< %{public}f)
%s Fell back to recurrent endpoint (%{public}f) because one-shot is too early (%{public}f < %{public}f)
%s Fell back to recurrent endpoint, _samplesSeen: %f, _samplesSeen(ms): %f, _lastOneShotEndpoint: %f
%s Reporting one-shot ep:  _samplesSeen: %f, _samplesSeen(ms): %f, _lastOneShotEndpoint: %f
%s sampleRate: %{public}lf frameRate: %{public}d
%s Skipping re-initialization of EPVAD2; no audio consumed yet
%s EPVAD2 reset with existing parameters
%s Could not find endpointer audio unit component
%s AU instantiation error: %{public}d
%s No model available for mode: %{public}d
%s Error reading plist endpoint model at %{public}@
%s Could not set kAUEndpointVADProperty_ViterbiModelData: %{public}d
%s Could not set %{public}@ to %{public}@: %{public}d
%s Could not initialize audio unit: %{public}d
%s CSVAD2Endpointer is configured
%s Unexpected block size of %{public}u, not %{public}u. Skipping this block of audio.
%s Could not process audio via endpointer: %{public}d
%s tuningLibraryPath: %{public}@
%s VAD2-epModelPath: %{public}@
%s Could not read kAUEndpointVAD2Property_LatestEndpointerEventTimeSeconds: %{public}d
%s Found one shot startpoint at %{public}.3f seconds
%s Found one shot endpoint at %{public}.3f seconds
%s Could not read kAUEndpointVAD2Property_LatestRecurrentVADEventTimeSeconds: %{public}d
%s Found recurrent startpoint at %{public}.3f seconds
%s Found recurrent endpoint at %{public}.3f seconds
%s xpc object should be XPC_TYPE_ARRAY
%s xpcObject value is NULL
%s Cannot decode non-plist types of XPC object
%s Cannot encode non-plist types into XPC object : %{public}@
%s Cannot generate subChunk since channel(%{public}tu) is larger than number of channels(%{public}tu)
%s Cannot generate subChunk if it reuqest more than it has : %{public}tu %{public}tu %{public}tu
%s CSEndpointAsset: %{public}@, path: %{public}@
%s _endpointerModelVersion = %{public}@
%s HEP::RecordingDidStop: Ignoring processAudioSamplesAsynchronously: Not queueing
%s HEP::RecordingDidStop: Ignoring processAudioSamplesAsynchronously
%s addAudio first sample offset: %{public}lu
%s Updated endpointer threshold: %{public}f
%s Updated endpointer delayed trigger: %{public}d
%s EARSPG: CSServerEndpointFeatures: %{public}@
%s Accepting RC: RCTime < 0: Server's processedAudioDuration(%{public}f) > _lastReportedEndpointTimeMs(%{public}f): sfLatency: %{public}f, rcTimeMs: %{public}f
%s Rejecting RC: SFLatency < 0: Server's processedAudioDuration(%{public}f): _lastReportedEndpointTimeMs(%{public}f): sfLatency: %{public}f, rcTimeMs: %{public}f
%s rcEpFeatures: %{public}@ shouldAccept: %{public}d
%s HEP::RecordingDidStop: Ignoring silenceScoreEstimateAvailable, Not queuing
%s silposnf=%f, clientProcessedAudioMs: %{public}f, effectiveClientProcessedAudioMs: %{public}lu
%s Detected speech start at %{public}f of effectiveClientProcessedAudioMs
%s HEP::RecordingDidStop: Ignoring silenceScoreEstimateAvailable
%s Already communicated end-pt: Not Invoking hybridClassifier for silposnf=%{public}f
%s ClientLag: serverProcessedAudioMs(%{public}ld) > effectiveClientProcessedAudioMs(%{public}f)
%s ClientLag: Not invoking HybridClassifier: sfLatency > clientLagThreshold: %{public}f > %{public}f
%s ClientLag: Using DefaultServerFeatures with disconnected-state sfLatency: %{public}f
%s ClientLag: Using ServerFeatures with ClampedSFLatency: %{public}f
%s ClientLag: Not Invoking HybridClassifier as serverProcessedAudioMs > effectiveClientProcessedAudioMs
%s EARSPG: Invoking HybridClassifier with parameters: %{public}@ at effectiveClientProcessedAudioMs: %{public}lu,clientSilencePosterior: %{public}f, endpointPosterior: %{public}f, result: %{public}d
%s ServerFeaturesLatencyDistribution: %{public}@ additionalMetrics: %{public}@
%s Already communicated end-pt: Not scheduling work for hybridClassifierQueue for silposnf=%{public}f
%s unsorted-serverFeatureLatencies: %{public}@
%s triggerEndSeconds: %{public}f, _vtEndInSampleCount: %{public}lu, _vtExtraAudioAtStartInMs: %{public}lu,  _hepAudioOriginInMs: %{public}f, voiceTriggerInfo: %{public}@,
%s CSHybridEndpointAnalyzer recordingStoppedForReason: %{public}lu
%s CSHybridEndpointer resetForNewRequestWithSampleRate: %{public}lu
%s CSEndpointAsset exists: %{public}@
%s No asset for CSHybridEndpointer for currentLanguage: %{public}@. Fallback to VAD2
%s Created EARCaesuraSilencePosteriorGenerator: %{public}@
%s Created HybridClassifier(%{public}@); canProcessCurrentRequest after reset: %{public}d,for sampleRate: %{public}lu, lang=%{public}@, version=%{public}@
%s csHepConfig: %{public}@
%s _clientHepConfig: %{public}f, _clampedSFLatencyForClientLag: %{public}f, _useDefaultServerFeaturesOnClientLag: %{public}d
%s language changed to: %{public}@: CSHybridEndpointer new asset: %{public}@
%s new hybrid endpoint asset downloaded, CSHybridEndpointer asset : %{public}@
%s current asset changed to : %{public}@
%s %{public}@ doesnt exist
%s Could not read: %{public}@
%s Could not decode contents of: %{public}@: err: %{public}@
%s Delta is larger than anchorHostTime
%s Delta is larger than anchorSampleCount
%s CSHybridEndpointer canProcessCurrentRequest
%s CSHybridEndpointer can-NOT-ProcessCurrentRequest, fallback  to VAD2
%s _activeEndpointer=%{public}@
%s Two-shot: DON'T reset any endpointers
%s endpointer: %{public}@: didDetectStartpointAtTime: %{public}f
%s EP_PROXY::RecordingDidStop: Ignoring startPoint-reporting
%s EP_PROXY::RecordingDidStop: Ignoring didDetectHardpoint-reporting
%s EP_PROXY::RecordingDidStop: Ignoring VAD2 2-shot reporting
%s CSEndpointerProxy didDetectHardEndpoint using for 1-2 shot at Time: %{public}f
%s EP_PROXY::RecordingDidStop: Ignoring didDetectHardEndpointAtTime:
%s %{public}@: Endpointer didDetectHardEndpointAtTime:withMetrics: %{public}f, CallingDelegate: %{public}@
%s CSEndpointerProxy: didDetectHardEndpoint: ep-time: %{public}f, triggerEnd: %{public}f, vad2EndWaitTime: %{public}f, delta: %{public}f, legacyTwoShotThreshold: %{public}f, enterTwoShot: %{public}d
%s WARN: endpointerModelVersion called when CSHybridEndpointer is not available
%s Cannot create NSNumber if xpcObject is NULL
%s XPC object type should be BOOL, DOUBLE, INT64, or UINT64
%s Cannot create xpcObject if objcType is NULL
%s Cannot create xpcObject since there is no matching type
%s Stop monitoring : First unlock
%s Start monitoring : Springboard start
%s Cannot start monitoring Springboard start because it was already started
%s Stop monitoring : Springboard start
%s SpringBoard started = %{public}@
%s Transcriber trigger token list: %{public}@
%s Initializing Quasar with config: %{public}@
%s Speech model loading took %{public}.3fms
%s Failed initialization in _EARSpeechRecognizer initWithConfiguration
%s runRecognition failed
%s endAudio failed
%s recognizeWavData failed
%s Partial result confidence: %{public}f
%s ERROR: %{public}s
%s Final result confidence: %{public}f
%s EAR Token[%{public}lu]: %s (%{public}f)
%s Fallback asset resource path : %{public}@
%s Cannot find corespeech asset from resourcePath : %{public}@
%s Configuration file is not exists : %{public}@
%s Cannot read configuration file : %{public}@
%s Cannot decode configuration json file : %{public}@
%s Configuration json file is not expected format
%s Cannot access to %{public}@ %{public}@ using default value
%s There is not audio buffer to convert. Skip this.
%s Got asked for %{public}u packets, have %{public}u
%s [%{public}02u of %{public}02u] Opus packet with %u bytes
%s %{public}d bytesConsumed from opus coverter, remains %{public}d bytes
%s Resetting AudioConverter buffer
%s createAudioConverter : initial frames per buffer = dur %{public}.2f * sr %{public}.2f = %{public}u
%s _configureAudioConverter: encoded audio needs minimum of %{public}u bytes per output buffer
%s _configureAudioConverter: AudioConverterGetProperty(kAudioConverterPropertyMinimumOutputBufferSize) returned status %{public}d
%s _configureAudioConverter: final framesPerBuffer: %{public}u
%s _configureAudioConverter: _convertPacketCount: %{public}u
%s _configureAudioConverter: AudioConverterGetProperty(MaximumOutputPacketSize): returned status %{public}d
%s createAudioConverter: outputSizePerPacket: %{public}u
%s _configureAudioConverter: _convertAudioCapacity %{public}u bytes
%s Cannot create AudioConverter using AudioConverterNew : %{public}u
%s Cannot set encoder bit rate : %{public}u
%s Start monitoring : Siri language code
%s Stop monitoring : Siri language code
%s Siri language changed to : %{public}@
%s Ignore notifying change of language code, since it is nil
%s xpc object should be XPC_TYPE_DICTIONARY
%s xpcObject key or value is NULL
%s Cannot encode key into xpcobject since the key is not NSString class type
%s SAT successfully initialized : %{public}@
%s SAT Score = %{public}f, threshold = %{public}f
%s AVVC initialization failed
%s Successfully create AVVC : %{public}p
%s Trying to set record buffer duration : %{public}lf
%s Failed setting record buffer duration. Duration is %{public}lf
%s Creating beep canceller...
%s Calling AVVC prepareRecordWithSettings : %{public}@
%s Creating SampleRateConverter
%s Calling AVVC setCurrentContext : %{public}@
%s Calling AVVC prewarmAudioSession
%s Calling AVVC releaseAudioSession : %{public}tu
%s zeroFilterWinSz: %{public}tu, numHostTicksPerAudioSample: %{public}f
%s _vtEndInSampleCount:%{public}ld, _numSamplesProcessed: %{public}ld, vtInfo: %{public}@
%s Resetting ZeroFilter
%s Calling AVVC startRecordingWithSettings : %{public}@
%s Calling AVVC startRecording
%s Calling AVVC stopRecording
%s Sampling rate = %{public}f
%s AVVC sampling rate = %{public}f
%s AVVC doesn't return sampleRate, assume it is default sample rate
%s Calling AVVC playAlertSoundForType to play alert
%s Calling AVVC playAlertSoundsForType : %{public}ld
%s Zero Filter Metrics: %@
%s Beep Canceller Metrics : %@
%s successfully : %{public}d, error : %{public}@
%s toConfiguration : %{public}d
%s withContext : %{public}@
%s activate : %{public}d
%s AVVC lost mediaserverd connection
%s AVVC informed mediaserverd reset, no further action required
%s Failed to deinterleave the data: %{public}d
%s Cannot create de-interleaver using AudioConverterNew: %{public}d
%s Created de-interleaver
%s Created narrowBandToWidBandConverter
%s Cannot create NSData with size 0
%s xpc object should be XPC_TYPE_DATA
%s Failed to create regular expression : %{public}@
%s ::: Error creating output file %{public}@, err: %{public}d
%s ::: Error writing to output wave file. : %{public}ld
CSVolumeMonitor
CSTimerMonitor
CSAlarmMonitor
CSMediaPlayingMonitor
CSAudioZeroCounter
SmartSiriVolume
CSAudioFileManager
Directory
CSSmartSiriVolume
CSMediaPlayingMonitorDelegate
NSObject
CSAlarmMonitorDelegate
CSTimerMonitorDelegate
CSSpeechManagerDelegate
CSVoiceTriggerDelegate
CSAudioFileReader
CSAudioSampleRateConverter
XPCObject
LanguageCode
LPCMTypeConversion
CSSpeakerModel
CSCoreSpeechServices
CSConfig
CSSpeechManager
CSAudioRecorderDelegate
CSStateMachineDelegate
CSSiriEnabledMonitorDelegate
CSVolumeMonitorDelegate
CSAssetManagerDelegate
CSLanguageCodeUpdateMonitorDelegate
CSVTUIEditDistance
CSAssetManager
CSVoiceTriggerAssetMetaUpdateMonitorDelegate
CSSpeechEndpointAssetMetaUpdateMonitorDelegate
Utils
CSSpeechController
CSAudioConverterDelegate
CSPreferences
CSStateMachine
Meter
CSEventMonitor
CSAudioFileLog
Alert
Metrics
CSAssetManagerEnablePolicyMac
VoiceTriggerPassThru
CSSpeechEndpointAssetMetaUpdateMonitor
VoiceTriggerRecord
CSVTUIRegularExpressionMatcher
CSVTUIASRGrammars
NSURLSessionDelegate
CSUtils
CSKeywordAnalyzerNDAPI
CSEndpointerMetrics
CSRemoteControlClient
CSAsset
AudioStreamBasicDescription
CSVoiceTriggerEnabledMonitor
CSRemoteRecordClient
CSVTUIAudioSessionRemote
CSVTUIAudioSession
CSSiriEnabledMonitor
CSBeepCanceller
CSAudioZeroFilter
AudioFile
CSAssetManagerEnablePolicy
CSCoreSpeechServiceListenerDelegate
CSAudioCircularBuffer
CSVoiceTriggerAssetMetaUpdateMonitor
CSServerEndpointFeatures
CSVAD2EndpointAnalyzer
CSEndpointAnalyzerImpl
CSEndpointAnalyzer
private
CSAudioChunk
CSNovDetectorResult
CSNovDetector
Bitset
CSHybridEndpointAnalyzer
EARCaesuraSilencePosteriorGeneratorDelegate
Time
CSEndpointerProxy
CSEndpointAnalyzerDelegate
CSFirstUnlockMonitor
CSAudioPowerMeter
CSSpringboardStartMonitor
CSKeywordAnalyzerQuasar
_EARSpeechRecognitionResultStream
CSAudioConverter
CSLanguageCodeUpdateMonitor
CSPolicy
CSEventMonitorDelegate
RecordContext
CSSpeakerDetectorNDAPI
CSAudioRecorder
AVVoiceControllerRecordDelegate
AVVoiceControllerPlaybackDelegate
CSBeepCancellerDelegate
DuckOption
ResourcePathHash
CSAudioFileWriter
@16@0:8
v24@0:8@16
v16@0:8
f16@0:8
@"NSObject<OS_dispatch_queue>"
q16@0:8
v32@0:8@16q24
@32@0:8@16f24I28
v28@0:8@16I24
@"NSString"
Q16@0:8
I16@0:8
v36@0:8@16I24@28
v32@0:8@16@24
@96@0:8{AudioStreamBasicDescription=dIIIIIIII}16{AudioStreamBasicDescription=dIIIIIIII}56
@104@0:8@16{AudioStreamBasicDescription=dIIIIIIII}24{AudioStreamBasicDescription=dIIIIIIII}64
v20@0:8f16
v24@0:8Q16
v36@0:8@16@24f32
v40@0:8@16@24Q32
v40@0:8@16@24@?32
@48@0:8@16@24@32^@40
B24@0:8@16
#16@0:8
@24@0:8:16
@32@0:8:16@24
@40@0:8:16@24@32
B16@0:8
B24@0:8#16
B24@0:8:16
Vv16@0:8
^{_NSZone=}16@0:8
B24@0:8@"Protocol"16
@"NSString"16@0:8
v32@0:8@"CSMediaPlayingMonitor"16q24
v32@0:8@"CSAlarmMonitor"16q24
v32@0:8@"CSTimerMonitor"16q24
v36@0:8@16B24@28
v36@0:8@16f24Q28
v28@0:8@16B24
v32@0:8@"CSSpeechManager"16@"AVVCAudioBuffer"24
v32@0:8@"CSSpeechManager"16@"CSAudioChunk"24
v36@0:8@"CSSpeechManager"16B24@"NSError"28
v32@0:8@"CSSpeechManager"16q24
@"NSDictionary"16@0:8
v36@0:8@"CSSpeechManager"16f24Q28
v24@0:8@"CSSpeechManager"16
v32@0:8@"CSSpeechManager"16@"NSDictionary"24
v28@0:8@"CSSpeechManager"16B24
v24@0:8d16
v24@0:8@"NSDictionary"16
@28@0:8f16@20
v28@0:8I16q20
v52@0:8@16q24B32Q36Q44
f24@0:8q16
f24@0:8f16f20
f36@0:8f16f20f24f28f32
f44@0:8f16f20f24f28f32f36f40
f28@0:8f16f20f24
v28@0:8@16f24
f20@0:8f16
{unique_ptr<SmartSiriVolume, std::__1::default_delete<SmartSiriVolume> >="__ptr_"{__compressed_pair<SmartSiriVolume *, std::__1::default_delete<SmartSiriVolume> >="__value_"^{SmartSiriVolume}}}
{vector<float, std::__1::allocator<float> >="__begin_"^f"__end_"^f"__end_cap_"{__compressed_pair<float *, std::__1::allocator<float> >="__value_"^f}}
@"NSUserDefaults"
@"CSAsset"
@24@0:8@16
@20@0:8I16
^{OpaqueExtAudioFile=}
^{OpaqueAudioConverter=}96@0:8{AudioStreamBasicDescription=dIIIIIIII}16{AudioStreamBasicDescription=dIIIIIIII}56
^{OpaqueAudioConverter=}
{AudioStreamBasicDescription="mSampleRate"d"mFormatID"I"mFormatFlags"I"mBytesPerPacket"I"mFramesPerPacket"I"mBytesPerFrame"I"mChannelsPerFrame"I"mBitsPerChannel"I"mReserved"I}
@32@0:8@16@24
d16@0:8
S16@0:8
i16@0:8
v40@0:8@"CSAudioRecorder"16@"NSData"24Q32
v32@0:8@"CSAudioRecorder"16@"AVVCAudioBuffer"24
v36@0:8@"CSAudioRecorder"16B24@"NSError"28
v32@0:8@"CSAudioRecorder"16q24
v24@0:8@"CSAudioRecorder"16
v32@0:8@"CSAudioRecorder"16@"NSDictionary"24
v28@0:8@"CSAudioRecorder"16B24
v40@0:8q16q24q32
v32@0:8q16q24
v28@0:8@"CSSiriEnabledMonitor"16B24
v28@0:8@"CSVolumeMonitor"16f24
v24@0:8@"CSAssetManager"16
v32@0:8@16@"NSString"24
@48@0:8@16@24@32@40
B32@0:8@16^@24
B24@0:8^@16
B32@0:8q16^@24
v48@0:8d16@24@?32@?40
B32@0:8Q16^@24
B40@0:8@16Q24^@32
v32@0:8@16^@24
Q24@0:8Q16
@24@0:8q16
@24@0:8Q16
v24@0:8q16
@?16@0:8
v24@0:8@?16
v20@0:8B16
@"CSAudioRecorder"
@"CSStateMachine"
@"CSAudioCircularBuffer"
@"<CSSpeechManagerDelegate>"
@"NSDictionary"
@"CSSmartSiriVolume"
@"NSHashTable"
@"NSObject<OS_dispatch_source>"
@"NSUUID"
@32@0:8Q16@24
@36@0:8Q16@24B32
v32@0:8@16@?24
v32@0:8Q16@24
v32@0:8@16Q24
v40@0:8@16@?24@?32
@"CSPolicy"
@"NSMutableDictionary"
v40@0:8@"CSAudioConverter"16@"NSArray"24Q32
{AudioStreamBasicDescription=dIIIIIIII}16@0:8
B24@0:8d16
B20@0:8B16
B32@0:8@16q24
B24@0:8q16
f24@0:8Q16
v32@0:8d16@?24
@"CSAudioConverter"
@"CSAudioSampleRateConverter"
@"CSAudioZeroCounter"
@"NSObject<OS_dispatch_group>"
@"<CSSpeechControllerDelegate>"
@"CSEndpointerProxy"
@"CSSpeechManager"
@"CSAudioFileWriter"
@24@0:8^@16
B32@0:8@16@24
d24@0:8d16
@"<CSStateMachineDelegate>"
@"NSURL"
q48@0:8@16@24@32@40
v32@0:8@"NSURLSession"16@"NSError"24
v40@0:8@"NSURLSession"16@"NSURLAuthenticationChallenge"24@?<v@?q@"NSURLCredential">32
v24@0:8@"NSURLSession"16
@32@0:8q16@24
@40@0:8@16q24@32
@20@0:8i16
@28@0:8@16i24
v20@0:8I16
@"CSNovDetector"
@"<CSKeywordAnalyzerNDAPIScoreDelegate>"
@56@0:8d16@24q32@40@48
@"NSArray"
B32@0:8d16^@24
@"<CSRemoteControlClientDelegate>"
@"<CSRemoteRecordClientDelegate>"
v24@0:8@"<CSVTUIAudioSessionDelegate>"16
v24@0:8@"<Endpointer>"16
q24@0:8q16
@"<CSVTUIAudioSessionDelegate>"
{unique_ptr<BatchBeepCanceller, std::__1::default_delete<BatchBeepCanceller> >="__ptr_"{__compressed_pair<BatchBeepCanceller *, std::__1::default_delete<BatchBeepCanceller> >="__value_"^{BatchBeepCanceller}}}
{vector<short, std::__1::allocator<short> >="__begin_"^s"__end_"^s"__end_cap_"{__compressed_pair<short *, std::__1::allocator<short> >="__value_"^s}}
@"<CSBeepCancellerDelegate>"
@36@0:8Q16S24d28
Q40@0:8@16Q24^@32
Q24@0:8^@16
{unique_ptr<CSAudioZeroFilterImpl<unsigned short>, std::__1::default_delete<CSAudioZeroFilterImpl<unsigned short> > >="__ptr_"{__compressed_pair<CSAudioZeroFilterImpl<unsigned short> *, std::__1::default_delete<CSAudioZeroFilterImpl<unsigned short> > >="__value_"^{CSAudioZeroFilterImpl<unsigned short>}}}
B32@0:8@16@?24
Vv24@0:8@?16
Vv40@0:8@16q24@?32
Vv24@0:8@?<v@?@"NSString">16
Vv40@0:8@"NSArray"16q24@?<v@?@"NSError">32
Vv24@0:8@?<v@?Q>16
Vv24@0:8@?<v@?>16
@32@0:8Q16f24f28
v32@0:8r^v16Q24
v40@0:8r^v16Q24Q32
@32@0:8Q16Q24
@24@0:8^Q16
v40@0:8Q16Q24@32
{unique_ptr<corespeech::CSAudioCircularBufferImpl<unsigned short>, std::__1::default_delete<corespeech::CSAudioCircularBufferImpl<unsigned short> > >="__ptr_"{__compressed_pair<corespeech::CSAudioCircularBufferImpl<unsigned short> *, std::__1::default_delete<corespeech::CSAudioCircularBufferImpl<unsigned short> > >="__value_"^{CSAudioCircularBufferImpl<unsigned short>}}}
r*16@0:8
@72@0:8q16q24d32@40d48@56q64
@64@0:8q16q24d32@40d48@56
v24@0:8@"CSAudioChunk"16
@"<CSEndpointAnalyzerDelegate>"16@0:8
v24@0:8@"<CSEndpointAnalyzerDelegate>"16
v24@0:8@"CSServerEndpointFeatures"16
v32@0:8d16@?<v@?B@"NSArray">24
^{OpaqueAudioComponentInstance=}
@"<CSEndpointAnalyzerDelegate>"
@"NSMutableData"
@24@0:8d16
B28@0:8^{AudioStreamBasicDescription=dIIIIIIII}16I24
v28@0:8d16I24
v28@0:8^f16I24
@64@0:8@16Q24Q32Q40Q48Q56
@40@0:8Q16Q24Q32
@"NSData"
I24@0:8Q16
v32@0:8Q16@?24
v36@0:8^f16Q24f32
v24@0:8@"EARClientSilenceFeatures"16
@"EARCaesuraSilencePosteriorGenerator"
@"EARClientSilenceFeatures"
@"_EAREndpointer"
@"CSServerEndpointFeatures"
@"NSMutableArray"
@"NSDate"
Q20@0:8f16
d24@0:8Q16
Q40@0:8Q16Q24Q32
v32@0:8@16d24
v40@0:8@16d24@32
v32@0:8@"<CSEndpointAnalyzer>"16d24
v40@0:8@"<CSEndpointAnalyzer>"16d24@"CSEndpointerMetrics"32
@"<CSEndpointAnalyzerImpl>"
@20@0:8f16
v20@0:8i16
v32@0:8r^s16i24i28
v28@0:8i16i20i24
v48@0:8@16@24@32@40
v72@0:8@16q24q32d40@48d56q64
v32@0:8@"_EARSpeechRecognizer"16@"_EARSpeechRecognitionResult"24
v32@0:8@"_EARSpeechRecognizer"16@"NSError"24
v32@0:8@"_EARSpeechRecognizer"16@"NSArray"24
v48@0:8@"_EARSpeechRecognizer"16@"NSArray"24@"NSArray"32@"NSArray"40
v32@0:8@"_EARSpeechRecognizer"16@"_EARSpeechRecognitionResultPackage"24
v32@0:8@"_EARSpeechRecognizer"16d24
v32@0:8@"_EARSpeechRecognizer"16@"_EARSpeechRecognition"24
v72@0:8@"_EARSpeechRecognizer"16q24q32d40@"NSArray"48d56q64
@36@0:8@16@24B32
v28@0:8r^s16i24
d24@0:8@16
@"_EARSpeechRecognizer"
@"_EARSpeechRecognitionAudioBuffer"
@"<CSKeywordAnalyzerQuasarScoreDelegate>"
@40@0:8Q16@24@32
@40@0:8@16@24@32
B36@0:8@16@24B32
v36@0:8@16B24Q28
v24@0:8^{OpaqueAudioConverter=}16
@"<CSAudioConverterDelegate>"
f24@0:8@16
@32@0:8@16Q24
@"CSSpeakerModel"
@"<CSSpeakerDetectorNDAPIDelegate>"
v28@0:8@16i24
v36@0:8@16i24d28
v28@0:8@"AVVoiceController"16B24
v36@0:8@"AVVoiceController"16B24@"NSError"28
v32@0:8@"AVVoiceController"16q24
v24@0:8@"AVVoiceController"16
v28@0:8@"AVVoiceController"16i24
v36@0:8@"AVVoiceController"16i24d28
v32@0:8@"AVVoiceController"16@"NSError"24
v32@0:8@"AVVoiceController"16@"NSDictionary"24
v32@0:8@"AVVoiceController"16@"AVVCAudioBuffer"24
v40@0:8@"CSBeepCanceller"16@"NSData"24Q32
@32@0:8@16^@24
v28@0:8B16@20
@"AVVoiceController"
@"CSAudioZeroFilter"
@"CSBeepCanceller"
{AudioBufferList="mNumberBuffers"I"mBuffers"[1{AudioBuffer="mNumberChannels"I"mDataByteSize"I"mData"^v}]}
^{AudioBufferList=I[1{AudioBuffer=II^v}]}
@"CSRemoteRecordClient"
@"<CSAudioRecorderDelegate>"
s32@0:8r^v16q24
fff?
mcpl
supo
?ffffff
NSt3__114basic_ifstreamIcNS_11char_traitsIcEEEE
NSt3__113basic_filebufIcNS_11char_traitsIcEEEE
@mcpl
NSt3__118basic_stringstreamIcNS_11char_traitsIcEENS_9allocatorIcEEEE
NSt3__115basic_stringbufIcNS_11char_traitsIcEENS_9allocatorIcEEEE
NSt3__119basic_ostringstreamIcNS_11char_traitsIcEENS_9allocatorIcEEEE
xfua2vpelppa
?ffffff
