?333?
N6vision3mod26ImageDescriptorBufferJointE
NSt3__118basic_stringstreamIcNS_11char_traitsIcEENS_9allocatorIcEEEE
NSt3__115basic_stringbufIcNS_11char_traitsIcEENS_9allocatorIcEEEE
:D7V
"nN%
AA)Z
>Y1\
?*Ral!
WXp?
>J6h
iQ~V?
iN^d
iN^d
N} y
?9(a
g|_\
&4I,)
|F"4
_Cp\
vj.7
ip[[
'Hlw
?VF#
uoEb
?\='
@ut\
?'/2
h?RD
"nN%
t><K
!sePmp
|zlK
c\qq
0Xr
9x&4
^Cp\
`!sePm
?aobHN
.5B?S
i3NCT
"nN%
|a2U0
%Tpx
"nN%
~NA~
lscz
AA)Z
r?jl@
<e5]O
! _B
?`vO
^Cp\F
@Y32
d:tz
N} y
?Uka
SrNl
-:Yj
tBFe@
@!"5
O7+]3n@
h8en
;a@O
?!=E
"LQ.
OVW
"nN%
0Xr
W zR&
/5B?S
|a2U0*
?EUwv
E|'f
i3NCT
1zn!
"nN%
#gaO;|
[@h=|
2uWv
 |(
?K[\
NSt3__120__shared_ptr_emplaceIN6vision3mod23ImageClassifierEspressoENS_9allocatorIS3_EEEE
N6vision3mod23ImageClassifierAbstractE
N6vision3mod29ImageDescriptorBufferAbstractE
NSt3__120__shared_ptr_pointerIPN6vision3mod28ImageDescriptorBufferFloat32ENS_14default_deleteIS3_EENS_9allocatorIS3_EEEE
NSt3__114default_deleteIN6vision3mod28ImageDescriptorBufferFloat32EEE
NSt3__120__shared_ptr_emplaceIN6vision3mod37FaceClassifier_BoostedPixelDifferenceENS_9allocatorIS3_EEEE
N6vision3mod5ERT2DIfEE
N6vision3mod16Transformation2DE
N6vision3mod23Transformation2DPrivateI19_Geometry2D_Affine_EE
N6vision3mod23Transformation2DPrivateI16_Geometry2D_RST_EE
NSt3__120__shared_ptr_pointerIPfNS_14default_deleteIA_fEENS_9allocatorIfEEEE
NSt3__114default_deleteIA_fEE
NSt3__120__shared_ptr_pointerIPN6vision3mod33ObjectDetector_DCNFaceDetector_v2ENS_14default_deleteIS3_EENS_9allocatorIS3_EEEE
NSt3__114default_deleteIN6vision3mod33ObjectDetector_DCNFaceDetector_v2EEE
Dfff?
?N6vision3mod23Transformation2DPrivateIA9_fEE
333333
?N6vision3mod23ImageClassifierEspressoE
NSt3__120__shared_ptr_emplaceIN6vision3mod23ImageClassifierEspresso9private_tENS_9allocatorIS4_EEEE
N6vision3mod32ImageDescriptorProcessorAbstractE
NSt3__120__shared_ptr_pointerIPKhZN4cvml4util31binserialized_table_of_contents9blob_infoC1ES2_RK26_BinSerializer_blobHeader_EUlS2_E_NS_9allocatorIS1_EEEE
ZN4cvml4util31binserialized_table_of_contents9blob_infoC1EPKhRK26_BinSerializer_blobHeader_EUlS4_E_
NSt3__120__shared_ptr_pointerIPN6vision3mod18LandmarkAttributesENS_14default_deleteIS3_EENS_9allocatorIS3_EEEE
NSt3__114default_deleteIN6vision3mod18LandmarkAttributesEEE
?xwwwww
?xwwwww
?N6vision3mod30ObjectDetector_DCNFaceDetectorE
N6vision3mod22ObjectDetectorAbstractE
NSt3__120__shared_ptr_emplaceIN8Espresso4blobIDv4_hLi2EEENS_9allocatorIS4_EEEE
N8Espresso4blobIDv4_hLi2EEE
NSt3__120__shared_ptr_emplaceIN6vision3mod30ObjectDetector_DCNFaceDetector4privENS_9allocatorIS4_EEEE
NSt3__120__shared_ptr_emplaceIN6vision3mod29ImageDescriptor_EspressoSceneENS_9allocatorIS3_EEEE
N6vision3mod29ImageDescriptor_EspressoSceneE
NSt3__120__shared_ptr_pointerIPN6vision3mod21ObjectTrackerAbstractENS_14default_deleteIS3_EENS_9allocatorIS3_EEEE
NSt3__114default_deleteIN6vision3mod21ObjectTrackerAbstractEEE
~|zxvuusqpnmkjiggfecba`_^]]\[ZYXWVUTTSRQQPONNMMLKKJIIHGGGFFEDDCCBBBAA@@??>>===<<;;:::999888776666555444333322211110000////.....----,,,,+++++*****)))))((((('''''''&&&&&&%%%%%%$$$$$$$########""""""!!!!!!!!!        
>333>
6>%I
s=X|E
+>>%I
NSt3__120__shared_ptr_pointerIPN6vision3mod30ObjectDetector_DCNFaceDetectorENS_14default_deleteIS3_EENS_9allocatorIS3_EEEE
NSt3__114default_deleteIN6vision3mod30ObjectDetector_DCNFaceDetectorEEE
333333
NSt3__120__shared_ptr_emplaceIN6vision3mod33ImageDescriptor_EspressoFaceColorENS_9allocatorIS3_EEEE
N6vision3mod33ImageDescriptor_EspressoFaceColorE
?ff&?
NSt3__110__function6__funcINS_6__bindIRFxP7__sFILEPKcRNS_3mapINS_12basic_stringIcNS_11char_traitsIcEENS_9allocatorIcEEEENS_6vectorIfNSB_IfEEEENS_4lessISD_EENSB_INS_4pairIKSD_SG_EEEEEEEJRKNS_12placeholders4__phILi1EEERKNSS_ILi2EEERKNSS_ILi3EEEEEENSB_IS12_EESP_EE
NSt3__110__function6__baseIFxP7__sFILEPKcRNS_3mapINS_12basic_stringIcNS_11char_traitsIcEENS_9allocatorIcEEEENS_6vectorIfNSA_IfEEEENS_4lessISC_EENSA_INS_4pairIKSC_SF_EEEEEEEEE
NSt3__16__bindIRFxP7__sFILEPKcRNS_3mapINS_12basic_stringIcNS_11char_traitsIcEENS_9allocatorIcEEEENS_6vectorIfNS9_IfEEEENS_4lessISB_EENS9_INS_4pairIKSB_SE_EEEEEEEJRKNS_12placeholders4__phILi1EEERKNSQ_ILi2EEERKNSQ_ILi3EEEEEE
NSt3__118__weak_result_typeIPFxP7__sFILEPKcRNS_3mapINS_12basic_stringIcNS_11char_traitsIcEENS_9allocatorIcEEEENS_6vectorIfNS9_IfEEEENS_4lessISB_EENS9_INS_4pairIKSB_SE_EEEEEEEEE
NSt3__110__function6__funcINS_6__bindIRFmmEJRKNS_12placeholders4__phILi1EEEEEENS_9allocatorISA_EES3_EE
NSt3__110__function6__baseIFmmEEE
NSt3__16__bindIRFmmEJRKNS_12placeholders4__phILi1EEEEEE
NSt3__118__weak_result_typeIPFmmEEE
NSt3__114unary_functionImmEE
NSt3__110__function6__funcINS_6__bindIRFxRKNS_3mapINS_12basic_stringIcNS_11char_traitsIcEENS_9allocatorIcEEEENS_6vectorIfNS7_IfEEEENS_4lessIS9_EENS7_INS_4pairIKS9_SC_EEEEEERKSC_RSC_EJRKNS_12placeholders4__phILi1EEERKNSS_ILi2EEERKNSS_ILi3EEEEEENS7_IS12_EESP_EE
NSt3__110__function6__baseIFxRKNS_3mapINS_12basic_stringIcNS_11char_traitsIcEENS_9allocatorIcEEEENS_6vectorIfNS6_IfEEEENS_4lessIS8_EENS6_INS_4pairIKS8_SB_EEEEEERKSB_RSB_EEE
NSt3__16__bindIRFxRKNS_3mapINS_12basic_stringIcNS_11char_traitsIcEENS_9allocatorIcEEEENS_6vectorIfNS5_IfEEEENS_4lessIS7_EENS5_INS_4pairIKS7_SA_EEEEEERKSA_RSA_EJRKNS_12placeholders4__phILi1EEERKNSQ_ILi2EEERKNSQ_ILi3EEEEEE
NSt3__118__weak_result_typeIPFxRKNS_3mapINS_12basic_stringIcNS_11char_traitsIcEENS_9allocatorIcEEEENS_6vectorIfNS5_IfEEEENS_4lessIS7_EENS5_INS_4pairIKS7_SA_EEEEEERKSA_RSA_EEE
NSt3__117bad_function_callE
NSt3__114basic_ifstreamIcNS_11char_traitsIcEEEE
NSt3__113basic_filebufIcNS_11char_traitsIcEEEE
NSt3__119basic_ostringstreamIcNS_11char_traitsIcEENS_9allocatorIcEEEE
NSt3__111__end_stateIcEE
NSt3__16__nodeIcEE
NSt3__120__shared_ptr_pointerIPNS_13__empty_stateIcEENS_14default_deleteIS2_EENS_9allocatorIS2_EEEE
NSt3__114default_deleteINS_13__empty_stateIcEEEE
NSt3__113__empty_stateIcEE
NSt3__116__owns_one_stateIcEE
NSt3__115__has_one_stateIcEE
NSt3__110__l_anchorIcEE
NSt3__110__r_anchorIcEE
NSt3__115__word_boundaryIcNS_12regex_traitsIcEEEE
NSt3__111__lookaheadIcNS_12regex_traitsIcEEEE
NSt3__123__match_any_but_newlineIcEE
NSt3__118__match_char_icaseIcNS_12regex_traitsIcEEEE
NSt3__120__match_char_collateIcNS_12regex_traitsIcEEEE
NSt3__112__match_charIcEE
NSt3__116__back_ref_icaseIcNS_12regex_traitsIcEEEE
NSt3__118__back_ref_collateIcNS_12regex_traitsIcEEEE
NSt3__110__back_refIcEE
NSt3__120__bracket_expressionIcNS_12regex_traitsIcEEEE
NSt3__128__begin_marked_subexpressionIcEE
NSt3__126__end_marked_subexpressionIcEE
NSt3__16__loopIcEE
NSt3__117__owns_two_statesIcEE
NSt3__117__repeat_one_loopIcEE
NSt3__111__alternateIcEE
NSt3__121__empty_non_own_stateIcEE
NSt3__111__match_anyIcEE
NSt3__120__shared_ptr_emplaceIN6vision3mod28ImageDescriptor_EspressoFaceENS_9allocatorIS3_EEEE
N6vision3mod28ImageDescriptor_EspressoFaceE
NSt3__120__shared_ptr_emplaceIN6vision3mod28ImageDescriptor_EspressoJunkENS_9allocatorIS3_EEEE
N6vision3mod28ImageDescriptor_EspressoJunkE
f024800L
N6vision3mod28ImageDescriptorBufferFloat32E
NSt3__120__shared_ptr_pointerIPN6vision3mod29ImageDescriptorBufferAbstractENS_14default_deleteIS3_EENS_9allocatorIS3_EEEE
NSt3__114default_deleteIN6vision3mod29ImageDescriptorBufferAbstractEEE
NSt3__120__shared_ptr_emplaceIN6vision3mod28ImageDescriptorBufferFloat32ENS_9allocatorIS3_EEEE
NSt3__120__shared_ptr_emplaceIN6vision3mod33ImageClassifier_HierarchicalModelENS_9allocatorIS3_EEEE
?333333
 @ff
N6vision3mod29GreedyClustererFacesWithTorsoE
N6vision3mod14FaceClusteringE
N6vision3mod24GreedyClustererWithTorsoE
NSt3__120__shared_ptr_pointerIPN4cvml4util12BackedBufferINS2_15RAMBackingStoreEEENS_14default_deleteIS5_EENS_9allocatorIS5_EEEE
NSt3__114default_deleteIN4cvml4util12BackedBufferINS2_15RAMBackingStoreEEEEE
NSt3__120__shared_ptr_pointerIPN4cvml4util15RAMBackingStoreENS_14default_deleteIS3_EENS_9allocatorIS3_EEEE
NSt3__114default_deleteIN4cvml4util15RAMBackingStoreEEE
NSt3__120__shared_ptr_pointerIPKN4cvml4util12BackedBufferINS2_15RAMBackingStoreEEENS_14default_deleteIS6_EENS_9allocatorIS6_EEEE
NSt3__114default_deleteIKN4cvml4util12BackedBufferINS2_15RAMBackingStoreEEEEE
NSt3__120__shared_ptr_emplaceINS_6vectorIhNS_9allocatorIhEEEENS2_IS4_EEEE
NSt3__120__shared_ptr_emplaceIN6vision3mod29GreedyClustererFacesWithTorsoENS_9allocatorIS3_EEEE
NSt3__120__shared_ptr_emplaceIN6vision3mod24GreedyClustererWithTorso9private_tENS_9allocatorIS4_EEEE
NSt3__120__shared_ptr_emplaceINS_6vectorImNS_9allocatorImEEEENS2_IS4_EEEE
NSt3__120__shared_ptr_emplaceINS_6vectorINS_4pairImmEENS_9allocatorIS3_EEEENS4_IS6_EEEE
N6vision3mod16LandmarkDetectorE
NSt3__120__shared_ptr_emplaceIN6vision3mod32ImageDescriptor_EspressoSmartCamENS_9allocatorIS3_EEEE
N6vision3mod32ImageDescriptor_EspressoSmartCamE
N6vision3mod32ImageDescriptorProcessorEspressoE
NSt3__120__shared_ptr_pointerIPfNS_14default_deleteIfEENS_9allocatorIfEEEE
NSt3__114default_deleteIfEE
NSt3__120__shared_ptr_emplaceIN6vision3mod32ImageDescriptorProcessorEspresso9private_tENS_9allocatorIS4_EEEE
NSt3__120__shared_ptr_emplaceIN8Espresso22dropout_augment_lowmemENS_9allocatorIS2_EEEE
NSt3__120__shared_ptr_pointerIPN6vision3mod13FaceRegionMapENS_14default_deleteIS3_EENS_9allocatorIS3_EEEE
NSt3__114default_deleteIN6vision3mod13FaceRegionMapEEE
N6vision3mod24ObjectTrackerCorrelationE
yA$H
ITCA
vA,2
q@,2
5mAv<
9Bh6
NSt3__120__shared_ptr_pointerIPN6vision3mod11FaceIDModelENS_14default_deleteIS3_EENS_9allocatorIS3_EEEE
NSt3__114default_deleteIN6vision3mod11FaceIDModelEEE
NSt3__120__shared_ptr_emplaceIN6vision3mod22ImageClassifierGlimmerENS_9allocatorIS3_EEEE
NSt3__110__function6__funcIZN6vision3mod14broadcastMinusIdLm16EEEvRKNS3_10CVMLMatrixIT_XT0_EEERKNS3_10CVMLVectorIS6_XT0_EEEiRS7_bEUlddE_NS_9allocatorISF_EEFdddEEE
NSt3__110__function6__baseIFdddEEE
ZN6vision3mod14broadcastMinusIdLm16EEEvRKNS0_10CVMLMatrixIT_XT0_EEERKNS0_10CVMLVectorIS3_XT0_EEEiRS4_bEUlddE_
NSt3__110__function6__funcIZN6vision3mod12broadcastAddIdLm16EEEvRKNS3_10CVMLMatrixIT_XT0_EEERKNS3_10CVMLVectorIS6_XT0_EEEiRS7_bEUlddE_NS_9allocatorISF_EEFdddEEE
ZN6vision3mod12broadcastAddIdLm16EEEvRKNS0_10CVMLMatrixIT_XT0_EEERKNS0_10CVMLVectorIS3_XT0_EEEiRS4_bEUlddE_
p}?33S?
?ffffff
N6vision3mod33ObjectDetector_DCNFaceDetector_v2E
NSt3__120__shared_ptr_emplaceIN6vision3mod33ObjectDetector_DCNFaceDetector_v24privENS_9allocatorIS4_EEEE
Ga=33
>N6vision3mod31ColorGaborImageDescriptorBufferE
N6vision3mod34ColorGaborImageDescriptorProcessorE
NSt3__120__shared_ptr_emplaceIN6vision3mod31ColorGaborImageDescriptorBufferENS_9allocatorIS3_EEEE
?2w-!
?O#-
?o/i
?`vO
?+0du
_{fI
tYLl>
?A+0du
++MJ
?S?o*Ra
?G=D
? <
?S\U
|a2U
?G8-x
??5^
?o*Ral!
tYLl
?gDio
?h?RD
?|DL
?pB!
FZ*o
?@M-[
#bJ$Q
?p%;6
FZ*oG
?+5{
?%]3
_{fI
?`vO
h"lx
?%]3
?U0*
?pB!
?Mg'
?a2U0*
o%;6
?,+MJA
?Ral!
?o*Ral!
?8J^
?Dio
?:#J{
?q $
$#ga
?{Ic
?scz
?o*Ral!
?U0*
?S?o*Ra
?Dio
8b->
?o/i
R?o*
?G8-x
?G8-x
?2ZGU
?Z*oG8-
?MJA
R?o*R
?`vO
_{fI
FZ*o
$#gaO
?{fI
?N6vision3mod12_GLOBAL__N_119BoxAlignerExceptionE
N6vision3mod20ObjectTrackerOptionsE
N6vision3mod21ObjectTrackerAbstractE
NSt3__120__shared_ptr_pointerIPN6vision3mod20ObjectTrackerOptionsENS_14default_deleteIS3_EENS_9allocatorIS3_EEEE
NSt3__114default_deleteIN6vision3mod20ObjectTrackerOptionsEEE
NSt3__120__shared_ptr_pointerIPN6vision3mod16LandmarkDetectorENS_14default_deleteIS3_EENS_9allocatorIS3_EEEE
NSt3__114default_deleteIN6vision3mod16LandmarkDetectorEEE
L>W[q>
N6vision3mod28ImageDescriptorAugmenterFlipE
N6vision3mod32ImageDescriptorAugmenterAbstractE
>N6vision3mod22ImageClassifierGlimmerE
NSt3__120__shared_ptr_emplaceIN6vision3mod22ImageClassifierGlimmer9private_tENS_9allocatorIS4_EEEE
NSt3__120__shared_ptr_pointerIP13vImage_BufferZ44+[ShotflowHelpers getCGImageAsVImageBuffer:]E3$_0NS_9allocatorIS1_EEEE
Z44+[ShotflowHelpers getCGImageAsVImageBuffer:]E3$_0
NSt3__120__shared_ptr_pointerIP7CGImagePFvS2_ENS_9allocatorIS1_EEEE
PFvP7CGImageE
NSt3__120__shared_ptr_pointerIPN6vision3mod15FaceFrontalizerENS_14default_deleteIS3_EENS_9allocatorIS3_EEEE
NSt3__114default_deleteIN6vision3mod15FaceFrontalizerEEE
NSt3__120__shared_ptr_pointerIPhNS_14default_deleteIA_hEENS_9allocatorIhEEEE
NSt3__114default_deleteIA_hEE
NSt3__120__shared_ptr_pointerIPN6vision3mod28ImageDescriptorAugmenterFlipENS_14default_deleteIS3_EENS_9allocatorIS3_EEEE
NSt3__114default_deleteIN6vision3mod28ImageDescriptorAugmenterFlipEEE
NSt3__120__shared_ptr_pointerIPN6vision3mod13ImageAnalyzerENS_14default_deleteIS3_EENS_9allocatorIS3_EEEE
NSt3__114default_deleteIN6vision3mod13ImageAnalyzerEEE
NSt3__120__shared_ptr_pointerIPN6vision3mod29GreedyClustererWithTorsoHacksENS_14default_deleteIS3_EENS_9allocatorIS3_EEEE
NSt3__114default_deleteIN6vision3mod29GreedyClustererWithTorsoHacksEEE
=33s?
B33XB
 BmQv9
333333
zd?333333
>UUUUUU
q=J?\
~|zxvuusqpnmkjiggfecba`_^]]\[ZYXWVUTTSRQQPONNMMLKKJIIHGGGFFEDDCCBBBAA@@??>>===<<;;:::999888776666555444333322211110000////.....----,,,,+++++*****)))))((((('''''''&&&&&&%%%%%%$$$$$$$########""""""!!!!!!!!!        X|E
+>>%I
N6vision3mod15ObjectTrackerExE
?N6vision3mod13CVMLCancellerE
N6vision3mod20GreedyClustererFacesE
N6vision3mod15GreedyClustererE
NSt3__120__shared_ptr_emplaceIN6vision3mod20GreedyClustererFacesENS_9allocatorIS3_EEEE
NSt3__120__shared_ptr_emplaceIN6vision3mod15GreedyClusterer9private_tENS_9allocatorIS4_EEEE
17VNNSDataStreambuf
24VNNSMutableDataStreambuf
?333?
&:/1
mVA=
\L;&
*RY;4|
KdC<
+/l?
rTD?
9FI?
W@+/l?
,A?P
?+/l?
+/l?@
q@4J?
9Q>`^
w=?k+K
F?]"!@R
?bD&>xzr?
uw?-
Fs?)
'I^?`
l?6a
>xq%
$@t=
;?`1S>
>M;{?
?;+n?
{f=~C
G]Z?
_=E3
wi@^ 
?k+H
7kV@
333?
?33s?
NSt3__120__shared_ptr_pointerIPfZN6vision3mod28readBinSerializedModelValuesEP7__sFILEPKcRKNS3_26BinSerializedModelFileInfoEbRNS3_11ModelValuesEE3$_0NS_9allocatorIfEEEE
ZN6vision3mod28readBinSerializedModelValuesEP7__sFILEPKcRKNS0_26BinSerializedModelFileInfoEbRNS0_11ModelValuesEE3$_0
NSt3__120__shared_ptr_pointerIPhZN6vision3mod28readBinSerializedModelValuesEP7__sFILEPKcRKNS3_26BinSerializedModelFileInfoEbRNS3_11ModelValuesEE3$_1NS_9allocatorIhEEEE
ZN6vision3mod28readBinSerializedModelValuesEP7__sFILEPKcRKNS0_26BinSerializedModelFileInfoEbRNS0_11ModelValuesEE3$_1
333333
NSt3__120__shared_ptr_emplaceIN6vision3mod19LandmarkDetectorDNNENS_9allocatorIS3_EEEE
Q9RI
Q9RI
7:RI
Q9RI
d*;4
>;_)K;
DX;B`e;
{r;l
k;B`e;
Q;_)K;
#;RI
7:RI
Q9RI
d*;4
7;_)K;
DX;B`e;
{r;o
{r;B`e;
DX;_)K;4
d*;RI
7:RI
Q9RI
7;_)K;
y;KY
3"<U
-<X94<
b<B`e<
4o<E
{r<!
ew<l
{r<E
h<B`e<e
#9<X94<
3"<RI
^;_)K;4
7:RI
Q9RI
k;KY
Q:RI
Q9RI
k:RI
DX;$
d*<4
?F<4
DX;4
Q:RI
>;B`e;'
#<X94<
z<KY
$=yX(=V
7=5^:=[
2D=]mE=
?F=]mE=
2D=\
<=5^:=
-2=2
+=yX(=
D<X94<
;B`e;
>;RI
{r<&
=R' =T
b=B`e=B>h=h
4o= Aq=
{r=F
{r= Aq=
(m=h
j=B>h=B`e=e
WJ=9
E6=|
%=R' =Qk
k:RI
7;B`e;
?$=V
b=B>h=D
s=Gry=
\~=n
\~=Gry=F
m=B>h=e
?$=v
;B`e;4
DX;'
3"<4
=R' =U
(=W[1=Z
B=_)K=a
S=d]\=
d=d]\=a
S=_)K=\
9=W[1=U
(=R' =P
j<<b
ew=I
{r<b
j<<>
ew<M
ew<>
=yX(=
A=;pN=
h=k+v=n
=k+v=
[=;pN=
5=yX(=
ew<b
7:RI
{r<M
7x=J{
/>:#
O/<N
=yX(=Y
U=B`e=
9#>/
$>0L&>
d*>1
'>0L&>/
Yu=B`e=
?F=Y
7=yX(=u
7;KY
=z6+=
/;=:
K=d]\=D
m=I.
V,>i
9>lx:>
<>6<=>
=>6<=>
;>lx:>l
u1>i
V,>
m=d]\=:
/;=z6+=
;_)K;
{r<)\
q,=6<==
Ga=F
s=J{
u >/
!N>`vO>
|P>`vO>
2D>7
~;>~
N=6<==
;_)K;o
<X94<
=z6+=6<==
ew=o
G!>ff&>
+>io0>F%5>
9>[B>>%uB>
qJ>_
QZ>?
+e>x
F>%uB>[B>>
9>F%5>io0>
+>ff&>
P=6<==z6+=u
/]<X94<r
Q9RI
{r<N
=yX(=
x=p_
/>aT
X>RI]>
Ga>f
n>W[q>}
=y>lxz>Zd{>
|>HP|>HP|>
|>Zd{>lxz>
s>W[q>
rh>f
Ga>RI]>P
/>o
/;=yX(=
y;RI
d*;KY
d*<b
ew=p_
2>Gr9>[
sW>v
8g>1
|>n4
3b>v
?>Gr9>
%>d;
Ga=:
d*<o
?F=d]\=F
8V>?
c>gDi>
t>5^z>$
>5^z>
4o>gDi>
s=d]\=
<;p
:_)K;RI
=yX(=
m=J{
P>>yX>
_>>yX>
kI>n
v>=yX(=
;_)K;
T<KY
L=B`e=I.
..>}
6>I.?>
h>iop>~
x>iop>
DX>r
G>I.?>}
c>]
=B`e=
;B`e;X9
c<)\
>B>(>W[1>lx:>
C>lx:>W[1>B>(>-!
;B`e;o
5=;pN=
>R' >z
!=>]
|P>]
g3>z
)>R' >b
xi=;pN=
{r;'
>R' >
m4>m
ZS>-
-r>HP|>
>HP|>
J*>R' >
?F<N
h=J{
H?>:#J>+
U>:#J>
m4>z
)>-!
=R' =
sW=k+v=(
>B>(>
g3>m
>>:#J>=
U>w-a>
l>w-a>=
U>:#J>m
g3>B>(>
=k+v=
:=R' =
#<B`
d*;u
Sc=n
%>W[1>
U>w-a>
\m>~
\m>w-a>+
!=>W[1>
Jj<1
d*;RI
=W[1=
..>lx:>]
ZS>]
F>lx:>
AO=W[1=
5<HP
7;RI
|P>-
:_)K;|
?$=\
Sc=o
/>>y
2>I.?>_
6Z>_
L>I.?>
%>>y
/>n4
Sc=\
;_)K;4
+=_)K=
(m=p
q,>#
q?_
q?h
q,>d;
(m=_)K=V
ew=r
\m>HP|>
c?r
>HP|>
%>>y
ew=a
2=tF
:B`e;
9=d]\=I
=d]\=Z
9=Qk
u`<RI
;B`e;
=R' =
d=&S
:p>7
R&?t
@=R' =
l=q=
~*>Gr9>p
H>>yX>
f%?=
+?-C,?
,?-C,?
h>>yX>p
H>Gr9>
{r<U
:0>[
_>iop>I
;.?{
;.?$
@"?$
>iop>
G!><
;X94<
fU=$
>ff&>
y'?u
c,?r
0?EG2?}
7?^K8?Y
8?^K8?
+5?}
3?EG2?
c,?u
y'?j
8V>9
5>ff&>
fU=|
<X94<B`
?n4 ?
,?I./?
1?I./?D
#?n4 ?
E6=tF
u >io0>
t>J{
$?B>(?
1?X94?
6?X94?
+?B>(?
@>io0>
@=B>h=
$>F%5>
sW>gDi>
V?E
%?gD)?
,?R'0?
S3?0L6?
>?R'@?
8E?a
B?R'@?_
9?0L6?
S3?R'0?
,?gD)?"
V?0
~{>gDi>
F>F%5>/
=B>h=
B<HP
41?4
=?io@?
H?q=J?
~K?(~L?6<M?H
M?6<M?(~L?
~K?q=J?U
B?io@?d
41?h
4o>v
WJ=F
s=Nb
->[B>>r
4!?"
1?]m5?
I<?Di??aTB?
O?2UP?
aQ?2UP?
E?aTB?Di??
8?]m5?
3b>r
O>[B>>V
=yX(=
N=Gry=*
0>%uB>
8g>5^z>
?n4 ?
$?gD)?h
F?GrI?
HN?2UP?
&R?F
V?p_W?
W?p_W?
&R?2UP?
K?GrI?}
-?gD)?
$?n4 ?
>5^z>
T>%uB>|
=Gry=
N=yX(=
Q<KY
S#>4
#?B>(?
41?]m5?c
A?o
X?GrY?
Z??W[?
[?h"\?h"\?
[??W[?
Z?GrY?~
\=?c
9?]m5?
,?B>(?
S#><
ZS=V
Y<RI
sW=n
qJ>RI]>
+?R'0?4
D?yXH?q
Q?tFT?}
Z?HP\?
_?R'`?
h`?R'`?$
]?HP\?h
V?tFT?n
K?yXH?
4?R'0?
p>RI]>
qJ>~
sW=2
<RI
~;>_
|@?o
D?yXH?
UO?A
R?]mU?~
c?=,d?Ttd?Ttd?=,d?
X?]mU?A
K?yXH?o
Ga>_
~;>
y'?D
1?0L6?
:?Di??
UO?X
.^?Nb`?aTb?
d?]me?
eg?+
Dh?+
f?]me?
d?aTb?Nb`?
UO?q
C?Di??
:?0L6?
V,>,
<B`e<
b='1
rh>[
*?I./?X94?
=?aTB?}
U?gDY?
I\?gDY?
F?aTB?d
9?X94?I./?u
rh>O
/>G
<B`e<
=5^:=B`e=q=
k>n4
;?io@?
E?GrI?
Q?]mU?
&b?O
(l?mVm??5n?
n??5n?mVm?
X?]mU?n
M?GrI?
E?io@?
=B`e=5^:=
<=B>h=
q?aT
:P?tFT?~
m?-!o?
Np?-!o?
X?tFT?
a!>r
=B>h=[
4o<z6
9#>X
\>W[q>
f%?
;?R'@?
g?U0j?
n?2Up?N
q?2Up?
l?U0j?
E?R'@?C
>W[q>?
(m=M
G!?=
,?EG2?
K?2UP?O
\?Nb`?}
?t?]mu?KYv?
v?KYv?]mu?
?t?<
c?Nb`?$
T?2UP?
7?EG2?$
G!?Zd
{r<V}
>0L&>l
;.?}
&R?+
^?aTb?
t?KYv?
w?>yx?
y?,ey?,ey?
y?>yx?
w?KYv?
e?aTb?
J9?}
9>0L&>tF
4o=\
B=Qk
2D= Aq=
'>lx:>
!N>x
8E?q=J?
X?HP\?
m?2Up?<
v?'1x?
ky?'1x?
r?2Up?
H`?HP\?~
O?q=J?
+5?{
!N>lx:>
= Aq=
t<RI
=]mE=
;>`vO>x
~K?2UP?
U?GrY?
a?]me?5
(l?-!o?N
?t?KYv?'1x?^
{?(~|?
|?(~|?
y?'1x?KYv?
?t?N
q?-!o?
(l?5
h?]me?
]?GrY?
U?2UP?
=y>x
d>`vO>
{r=]mE=
u<RI
ew<3
?F=F
+e>lxz>B>
eG?(~L?
aQ?4
j?mVm?
Np?S
s?]mu?
/|?q
w?]mu?S
Np?mVm?
aQ?(~L?
>lxz>
^)>=
?F=RI
)>6<=>
e>Zd{>9
R&?-C,?
$H?6<M?
V??W[?$
eg?c
j??5n?
s?KYv?>yx?
Wz?>yx?KYv?
4q??5n?c
_??W[?
&R?6<M?
2?-C,?
A ?:#
>Zd{>
4Q>6<=>
Yu=*
2?^K8?
R?p_W?
[?R'`?=,d?+
z?(~|?H
}?(~|?
g?=,d?R'`?
[?p_W?
=?^K8?\
d*>+
f>HP|>L7
W?h"\?
h`?Ttd?
Dh?6
>w?,ey?
C{?,ey?
Dh?Ttd?
h`?h"\?
!>?Y
>HP|>B
 <X9
f>HP|>L7
W?h"\?
h`?Ttd?
Dh?6
>w?,ey?
C{?,ey?
Dh?Ttd?
h`?h"\?
!>?Y
>HP|>B
 <X9
Yu=*
2?^K8?
R?p_W?
[?R'`?=,d?+
z?(~|?H
}?(~|?
g?=,d?R'`?
[?p_W?
=?^K8?\
d*>+
)>6<=>
e>Zd{>9
R&?-C,?
$H?6<M?
V??W[?$
eg?c
j??5n?
s?KYv?>yx?
Wz?>yx?KYv?
4q??5n?c
_??W[?
&R?6<M?
2?-C,?
A ?:#
>Zd{>
4Q>6<=>
ew<3
?F=F
+e>lxz>B>
eG?(~L?
aQ?4
j?mVm?
Np?S
s?]mu?
/|?q
w?]mu?S
Np?mVm?
aQ?(~L?
>lxz>
^)>=
?F=RI
=]mE=
;>`vO>x
~K?2UP?
U?GrY?
a?]me?5
(l?-!o?N
?t?KYv?'1x?^
{?(~|?
|?(~|?
y?'1x?KYv?
?t?N
q?-!o?
(l?5
h?]me?
]?GrY?
U?2UP?
=y>x
d>`vO>
{r=]mE=
u<RI
2D= Aq=
'>lx:>
!N>x
8E?q=J?
X?HP\?
m?2Up?<
v?'1x?
ky?'1x?
r?2Up?
H`?HP\?~
O?q=J?
+5?{
!N>lx:>
= Aq=
t<RI
{r<V}
>0L&>l
;.?}
&R?+
^?aTb?
t?KYv?
w?>yx?
y?,ey?,ey?
y?>yx?
w?KYv?
e?aTb?
J9?}
9>0L&>tF
4o=\
B=Qk
(m=M
G!?=
,?EG2?
K?2UP?O
\?Nb`?}
?t?]mu?KYv?
v?KYv?]mu?
?t?<
c?Nb`?$
T?2UP?
7?EG2?$
G!?Zd
4o<z6
9#>X
\>W[q>
f%?
;?R'@?
g?U0j?
n?2Up?N
q?2Up?
l?U0j?
E?R'@?C
>W[q>?
<=B>h=
q?aT
:P?tFT?~
m?-!o?
Np?-!o?
X?tFT?
a!>r
=B>h=[
=5^:=B`e=q=
k>n4
;?io@?
E?GrI?
Q?]mU?
&b?O
(l?mVm??5n?
n??5n?mVm?
X?]mU?n
M?GrI?
E?io@?
=B`e=5^:=
<B`e<
b='1
rh>[
*?I./?X94?
=?aTB?}
U?gDY?
I\?gDY?
F?aTB?d
9?X94?I./?u
rh>O
/>G
<B`e<
y'?D
1?0L6?
:?Di??
UO?X
.^?Nb`?aTb?
d?]me?
eg?+
Dh?+
f?]me?
d?aTb?Nb`?
UO?q
C?Di??
:?0L6?
V,>,
~;>_
|@?o
D?yXH?
UO?A
R?]mU?~
c?=,d?Ttd?Ttd?=,d?
X?]mU?A
K?yXH?o
Ga>_
~;>
Y<RI
sW=n
qJ>RI]>
+?R'0?4
D?yXH?q
Q?tFT?}
Z?HP\?
_?R'`?
h`?R'`?$
]?HP\?h
V?tFT?n
K?yXH?
4?R'0?
p>RI]>
qJ>~
sW=2
<RI
S#>4
#?B>(?
41?]m5?c
A?o
X?GrY?
Z??W[?
[?h"\?h"\?
[??W[?
Z?GrY?~
\=?c
9?]m5?
,?B>(?
S#><
ZS=V
=yX(=
N=Gry=*
0>%uB>
8g>5^z>
?n4 ?
$?gD)?h
F?GrI?
HN?2UP?
&R?F
V?p_W?
W?p_W?
&R?2UP?
K?GrI?}
-?gD)?
$?n4 ?
>5^z>
T>%uB>|
=Gry=
N=yX(=
Q<KY
WJ=F
s=Nb
->[B>>r
4!?"
1?]m5?
I<?Di??aTB?
O?2UP?
aQ?2UP?
E?aTB?Di??
8?]m5?
3b>r
O>[B>>V
41?4
=?io@?
H?q=J?
~K?(~L?6<M?H
M?6<M?(~L?
~K?q=J?U
B?io@?d
41?h
4o>v
@=B>h=
$>F%5>
sW>gDi>
V?E
%?gD)?
,?R'0?
S3?0L6?
>?R'@?
8E?a
B?R'@?_
9?0L6?
S3?R'0?
,?gD)?"
V?0
~{>gDi>
F>F%5>/
=B>h=
B<HP
u >io0>
t>J{
$?B>(?
1?X94?
6?X94?
+?B>(?
@>io0>
?n4 ?
,?I./?
1?I./?D
#?n4 ?
E6=tF
;X94<
fU=$
>ff&>
y'?u
c,?r
0?EG2?}
7?^K8?Y
8?^K8?
+5?}
3?EG2?
c,?u
y'?j
8V>9
5>ff&>
fU=|
<X94<B`
:0>[
_>iop>I
;.?{
;.?$
@"?$
>iop>
G!><
l=q=
~*>Gr9>p
H>>yX>
f%?=
+?-C,?
,?-C,?
h>>yX>p
H>Gr9>
{r<U
=R' =
d=&S
:p>7
R&?t
@=R' =
:B`e;
9=d]\=I
=d]\=Z
9=Qk
u`<RI
;B`e;
ew=r
\m>HP|>
c?r
>HP|>
%>>y
ew=a
2=tF
+=_)K=
(m=p
q,>#
q?_
q?h
q,>d;
(m=_)K=V
:_)K;|
?$=\
Sc=o
/>>y
2>I.?>_
6Z>_
L>I.?>
%>>y
/>n4
Sc=\
;_)K;4
|P>-
=W[1=
..>lx:>]
ZS>]
F>lx:>
AO=W[1=
5<HP
7;RI
d*;u
Sc=n
%>W[1>
U>w-a>
\m>~
\m>w-a>+
!=>W[1>
Jj<1
d*;RI
=R' =
sW=k+v=(
>B>(>
g3>m
>>:#J>=
U>w-a>
l>w-a>=
U>:#J>m
g3>B>(>
=k+v=
:=R' =
#<B`
h=J{
H?>:#J>+
U>:#J>
m4>z
)>-!
>R' >
m4>m
ZS>-
-r>HP|>
>HP|>
J*>R' >
?F<N
5=;pN=
>R' >z
!=>]
|P>]
g3>z
)>R' >b
xi=;pN=
{r;'
;B`e;X9
c<)\
>B>(>W[1>lx:>
C>lx:>W[1>B>(>-!
;B`e;o
T<KY
L=B`e=I.
..>}
6>I.?>
h>iop>~
x>iop>
DX>r
G>I.?>}
c>]
=B`e=
:_)K;RI
=yX(=
m=J{
P>>yX>
_>>yX>
kI>n
v>=yX(=
;_)K;
?F=d]\=F
8V>?
c>gDi>
t>5^z>$
>5^z>
4o>gDi>
s=d]\=
<;p
d*;KY
d*<b
ew=p_
2>Gr9>[
sW>v
8g>1
|>n4
3b>v
?>Gr9>
%>d;
Ga=:
d*<o
Q9RI
{r<N
=yX(=
x=p_
/>aT
X>RI]>
Ga>f
n>W[q>}
=y>lxz>Zd{>
|>HP|>HP|>
|>Zd{>lxz>
s>W[q>
rh>f
Ga>RI]>P
/>o
/;=yX(=
y;RI
<X94<
=z6+=6<==
ew=o
G!>ff&>
+>io0>F%5>
9>[B>>%uB>
qJ>_
QZ>?
+e>x
F>%uB>[B>>
9>F%5>io0>
+>ff&>
P=6<==z6+=u
/]<X94<r
;_)K;
{r<)\
q,=6<==
Ga=F
s=J{
u >/
!N>`vO>
|P>`vO>
2D>7
~;>~
N=6<==
;_)K;o
7;KY
=z6+=
/;=:
K=d]\=D
m=I.
V,>i
9>lx:>
<>6<=>
=>6<=>
;>lx:>l
u1>i
V,>
m=d]\=:
/;=z6+=
=yX(=Y
U=B`e=
9#>/
$>0L&>
d*>1
'>0L&>/
Yu=B`e=
?F=Y
7=yX(=u
{r<M
7x=J{
/>:#
O/<N
=yX(=
A=;pN=
h=k+v=n
=k+v=
[=;pN=
5=yX(=
ew<b
7:RI
j<<>
ew<M
ew<>
j<<b
ew=I
{r<b
DX;'
3"<4
=R' =U
(=W[1=Z
B=_)K=a
S=d]\=
d=d]\=a
S=_)K=\
9=W[1=U
(=R' =P
7;B`e;
?$=V
b=B>h=D
s=Gry=
\~=n
\~=Gry=F
m=B>h=e
?$=v
;B`e;4
{r<&
=R' =T
b=B`e=B>h=h
4o= Aq=
{r=F
{r= Aq=
(m=h
j=B>h=B`e=e
WJ=9
E6=|
%=R' =Qk
k:RI
>;B`e;'
#<X94<
z<KY
$=yX(=V
7=5^:=[
2D=]mE=
?F=]mE=
2D=\
<=5^:=
-2=2
+=yX(=
D<X94<
;B`e;
>;RI
DX;$
d*<4
?F<4
DX;4
Q:RI
Q9RI
k:RI
Q9RI
k;KY
Q:RI
Q9RI
7;_)K;
y;KY
3"<U
-<X94<
b<B`e<
4o<E
{r<!
ew<l
{r<E
h<B`e<e
#9<X94<
3"<RI
^;_)K;4
7:RI
Q9RI
d*;4
7;_)K;
DX;B`e;
{r;o
{r;B`e;
DX;_)K;4
d*;RI
7:RI
Q9RI
d*;4
>;_)K;
DX;B`e;
{r;l
k;B`e;
Q;_)K;
#;RI
7:RI
Q9RI
7:RI
Q9RI
NSt3__120__shared_ptr_pointerIPN6vision3mod18FaceBoxPoseAlignerIaEENS_14default_deleteIS4_EENS_9allocatorIS4_EEEE
NSt3__114default_deleteIN6vision3mod18FaceBoxPoseAlignerIaEEEE
CUUUUUU
G!B33
B333Cgf
NSt3__120__shared_ptr_emplaceI17espresso_buffer_tNS_9allocatorIS1_EEEE
NSt3__120__shared_ptr_emplaceIN6vision3mod22BoostedClassifierLightENS_9allocatorIS3_EEEE
NSt3__120__shared_ptr_emplaceIN6vision3mod44ImageDescriptor_PixelPairDifferenceProcessorENS_9allocatorIS3_EEEE
N6vision3mod44ImageDescriptor_PixelPairDifferenceProcessorE
@(#)PROGRAM:Vision  PROJECT:Vision-2.0.49
@(#)PROGRAM:TemporalRegistration-iOS  PROJECT:Vision-2.0.49
%s error %lld:%s in %s @ %s:%d
virtual std::vector<float> vision::mod::ImageDescriptorBufferJoint::computeDistancesFrom(const vision::mod::ImageDescriptorBufferAbstract &) const
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/ImageDescriptor/ImageDescriptor_BufferJoint.cpp
virtual float vision::mod::ImageDescriptorBufferJoint::computeDistanceFrom(const vision::mod::ImageDescriptorBufferAbstract &) const
virtual std::vector<float> vision::mod::ImageDescriptorBufferJoint::computeSelfDistances() const
CVML_status vision::mod::ImageDescriptorBufferJoint::computeDistanceBetweenDescriptors(const vision::mod::ImageDescriptorBufferJoint *, const vision::mod::ImageDescriptorBufferJoint *, float &) const
virtual vision::mod::ImageDescriptorBufferJoint *vision::mod::ImageDescriptorBufferJoint::getRepresentative(vision::mod::ImageDescriptorBufferAbstractRepresentativeMode, vision::mod::ImageDescriptorId) const
allocator<T>::allocate(size_t n) 'n' exceeds maximum supported size
void *vision::mod::ImageDescriptorBufferAbstract::getDataForKthDescriptor(int) const
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/ImageDescriptor/ImageDescriptor_BufferAbstract.h
Descriptor count = 
Descriptor length = 
 bytes
 = [
com.apple.cvml.%@
CVML module = %@
VNSequenceSummarizationObservation
VNVSO_FR
VNVSO_LSTMScores
One ore more properties (minimum/maximum clip duration, frame rate) id(are) not consistent with other(s)
begin
q24@?0@"NSDictionary"8@"NSDictionary"16
Coding version of deserialized VNSequenceSummarizationObservation object (%lu) is different than the one used in currently running software (%lu)
Deserialization of observation object has failed; the serialized data is likely corrupted. Error: %@
start
finish
Internal error occurred while creating clip generator class
%@:FrmRt=%@:@lstmScrs=%@
frameRate
T@"NSNumber",&,N,V_frameRate
lstmScores
T@"NSArray",R,N,V_lstmScores
v8@?0
CVML_debug_enable_cluster_log
en_US_POSIX
yyyy-MM-dd_HH-mm-ss-SSS
VNClusteringLog
%@_%@.log
Level %@ cluster map:
ClusterId: %lld
%@Faces: 
%lld
0 Lookup
1 Lookup
logFolderURL
T@"NSURL",R,V_logFolderURL
logFileURL
T@"NSURL",R,V_logFileURL
logEnabled
TB,R,V_logEnabled
fileNameBase
T@"NSString",R,V_fileNameBase
VNSuggestionLog
Input query - face IDs with flags (clusterIdsWithFlags):
faceId: %@
%@can be returned: %@
Suggested face IDs: %@:
ClusterId: %@   
%@Suggestions: 
%@, 
all sugestions for given input query (suggestionLists)
filtered suggestions to include only those that are part of input query (suggestedInputsLists)
Connected groups of suggestions face IDs (connectedSuggestedInputs):
Group %d suggestions: 
Final list of suggestions face IDs (results):
Cache file path is a required option
Invalid cache file path: %@
VNRequestOptionClusteringAlgorithm is a required option
VNRequestOptionClusteringAlgorithm must be set to either VNClusteringAlgorithm_Greedy or VNClusteringAlgorithm_GreedyWithTorso
Clustering threshold is a required option
Torso clustering threshold is a required option for algorithm type VNClusteringAlgorithm_GreedyWithTorso
RestoreClusteringState must have an NSData object
Clustering vector access method is a required option
Error initializing cluster state
v32@?0@"VNFaceObservation"8Q16^B24
Greedy clusterer failed with unknown error
Greedy clusterer failed with error: %s
Greedy clusterer failed with error: %llu
Internal error querying similar faces
getLevel1ClusteredIdsGroupedByLevel0ClustersForFaceId: m_ClusteringImpl_const (%llu) and/or faceId (%@) are/is not initialized
getLevel1ClusteredIdsGroupedByLevel0ClustersForFaceId: There is no level-1 cluster that contains faceId = %d
getLevel1ClusteredIdsGroupedByLevel0ClustersForFaceId: Level-1 cluster size is zero for a valid clusterID descriptor (%lld)
getLevel1ClusteredIdsGroupedByLevel0ClustersForFaceId: Level-0 cluster size is zero for a faceId (%d) from Level-1 cluster.
B24@?0@"NSNumber"8@"NSMutableOrderedSet"16
Parameter validation failed for getDistanceBetweenLevel1Clusters
v32@?0@8Q16^B24
  face[%d].representativeness = %.3f
  best rep = %d
Cache file path is a required parameter
RestoreClusteringState is a required parameter
Clustering request was canceled, error: %llu
Internal error in maximumFaceIdInModelAndReturnError
Clustering with greedy algorithm
adding faces (%lu): %s
Faces to add must be accompanied by grouping identifiers when performing clustering in torso mode.
Faces to add array must be the same size as that of the grouping identifiers array.
Clusters:
  %d: prevcount=%d, curcount=%d, shouldUpdateRep = %d
clusters:
  clusterId: %ld, %s
void vision::mod::ImageDescriptorBufferJoint::setAvailableFlagsForKthDescriptor(const int, bool, bool)
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/ImageDescriptor/ImageDescriptor_BufferJoint.h
void vision::mod::ImageDescriptorBufferJoint::setSideInfoForKthDescriptor(const int, vision::mod::DescriptorItemSideInfo &)
map::at:  key not found
value
epoch
timescale
faceId
Ti,VfaceId
faceRect
T{CGRect={CGPoint=dd}{CGSize=dd}},VfaceRect
framesSinceLast
Ti,VframesSinceLast
maxScore
Tf,VmaxScore
minScore
Tf,VminScore
numScores
Ti,VnumScores
swFaceId
Ti,VswFaceId
swCenter
T{CGPoint=dd},VswCenter
swSize
T{CGSize=dd},VswSize
swLastFrameSeen
Ti,VswLastFrameSeen
hwFaceId
Ti,VhwFaceId
hwCenter
T{CGPoint=dd},VhwCenter
hwSize
T{CGSize=dd},VhwSize
hwLastFrameSeen
Ti,VhwLastFrameSeen
    orientation = %d
Number of HW faces = %d - calculating rect
   hwFaceRect: (%.3f,%.3f,%.3f,%.3f), hasLeftEye = %d, hasRightEye = %d
Setting needSWFaceDetection = YES (v2)
   face %d = (%.3f,%.3f,%.3f,%.3f)
   fcrect  = (%.3f,%.3f,%.3f,%.3f)
   inserting prev face (hw%d,sw=%d) = (%.3f,%.3f,%.3f,%.3f) padding=(%.3f,%.3f)
Setting needSWFaceDetection = YES (v1)
Faces before filtering:
  face rect : (%.3f,%.3f,%.3f,%.3f)
q24@?0@"VNFaceObservation"8@"VNFaceObservation"16
Faces after filtering:
swFaceDetection forced = %d
Skipping face detection
v24@?0@"VNRequest"8@"NSError"16
calculateFaceFocus:
   adding rect: %.3f,%.3f,%.3f,%.3f
{CGRect={CGPoint=dd}{CGSize=dd}}
   focusScore = %d, %.3f
AdjustFaceIds: Examining '%s'
faceStat.id = %d
    rename found: %d mapped to %d
    new id: %d mapped to %d
    no id: assigning %d
    map found: %d maps to %d
       entry exists with same id: %d
%d faces so far unmatched:
    face %d
    %d overlaps with %d by %.3f %% : 
    matched!  mapping %d to %d
    not matched
      no match found for id %d - adding face
  prevConfig has %d entries
Found mapping!
   mapping not found for %d, mapping to itself
removing config entry: %d
Timestamp
  face ID = %d, timestamp = %.6f
FaceID
Rect
Width
Height
    inserting at index %d, count=%d
  extractFacesFromMetadata
extractFaceMetadata: invalid properties
AccumulatedFaceMetadata
  accumulatedFaceMetadata = %x
adding %d faces
Regions
regions exist
RegionList
  num regions = %d
    latestFaceTimestamp = %.6f
addFacesToImageStat: timestamp = %.6f, lastFaceIndex=%d
    imageTimestamp > latestFaceTimestamp
RollAngle
YawAngle
PitchAngle
LeftEyeX
LeftEyeY
LeftEyeWidth
LeftEyeHeight
LeftEyeBlinkLevel
RightEyeX
RightEyeY
RightEyeWidth
RightEyeHeight
RightEyeBlinkLevel
SmileLevel
      found face id %d, timestamp=%.6f, x=%.3f,y=%.3f,w=%.3f,h=%.3f
    adding face id %d, timestamp %.6f
    face id %d, timestamp %.6f - delta = %.6f, perhaps should use FaceCore
FaceInfoArray:
hwId = %d (lastSeen=%d, ctr=%.3f,%.3f size=%.3f,%.3f), swId = %d (lastSeen=%d, ctr=%.3f,%.3f size=%.3f,%.3f)
curConfig
T@"NSMutableDictionary",&,V_curConfig
faceIdMapping
T@"NSMutableDictionary",&,V_faceIdMapping
renameMapping
T@"NSMutableDictionary",&,V_renameMapping
faceIdCounter
Ti,V_faceIdCounter
faceInfoArray
T@"NSMutableArray",&,V_faceInfoArray
numFramesSinceFullFaceCore
Ti,V_numFramesSinceFullFaceCore
numFramesNoFaces
Ti,V_numFramesNoFaces
faceTimestampArray
T@"NSMutableArray",&,V_faceTimestampArray
latestImageTimestamp
Td,V_latestImageTimestamp
lastFaceIndex
Ti,V_lastFaceIndex
timeFaceDetectionDone
Td,VtimeFaceDetectionDone
timeBlinkDetectionDone
Td,VtimeBlinkDetectionDone
forceFaceDetectionEnable
TB,VforceFaceDetectionEnable
forceFaceDetailsEnable
TB,V_forceFaceDetailsEnable
latestFaceTimestamp
Td,VlatestFaceTimestamp
version
Ti,V_version
scaled Average Camera Travel Distance = %f
scaled Max Registration Error Integral = %f
scaled Mean peak registration error / Max peak registration error = %f
scaled Beginning vs. End AEMatrix difference vs. Average of Adjacent AE Matrix Differences = %f
scaled In-out ratio = %f
scaled Max inner distance = %f
scaled Average registration error skewness = %f
Sequence classified as NON-ACTION due to complete lack of local motion (%f, threshold %f)
Non-Linear SVM Action classifier called with:
Average Camera Travel Distance = %f
Max Registration Error Integral = %f
Mean peak registration error / Max peak registration error = %f
Beginning vs. End AEMatrix difference vs. Average of Adjacent AE Matrix Differences = %f
In-out ratio = %f
Max inner distance = %f
Average registration error skewness = %f
PREDICTION: --- %s --- (value = %f)
ACTION
NON-ACTION
testMaxInnerDistance
Tf,VtestMaxInnerDistance
testInOutRatio
Tf,VtestInOutRatio
testMaxPeakRegistrationError
Tf,VtestMaxPeakRegistrationError
testMeanPeakRegistrationError
Tf,VtestMeanPeakRegistrationError
testMinRegionOfInterestSize
Tf,VtestMinRegionOfInterestSize
testMaxRegistrationErrorSkewness
Tf,VtestMaxRegistrationErrorSkewness
testMaxRegistrationErrorIntegral
Tf,VtestMaxRegistrationErrorIntegral
testAverageCameraTravelDistance
Tf,VtestAverageCameraTravelDistance
testAverageRegistrationErrorSkewness
Tf,VtestAverageRegistrationErrorSkewness
testBeginningVsEndAEMatrixVsAverageAdjacentAEMatrix
Tf,VtestBeginningVsEndAEMatrixVsAverageAdjacentAEMatrix
svmParameters
T^{__SVMParameters=[7{__SVMScaleOffset=ff}]ddii^{BurstSupportVector}^{BurstSupportVector}},V_svmParameters
apple_scenes_onlyfc
std::map<std::string, float> vision::mod::ImageClassifierAbstract::classifyDescriptor(const vision::mod::ImageDescriptorBufferAbstract &, bool, bool)
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/ImageClassifier/ImageClassifier_Abstract.h
VNClusterOptionVectorMapReadOnlyFlag
Invalid Clusterer cache directory: %@
Invalid Clusterer type: %@
Failed to create clusterer; Error = %@
Suggestions request has been cancelled
Clustering request has been cancelled
Disgust
Neutral
Scream
Smile
Surprise
Suspicious
CVML_status vision::mod::ImageDescriptorBufferAbstract::initBufferWithData(void *, size_t, size_t, bool)
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/ImageDescriptor/ImageDescriptor_BufferAbstract.cpp
vision::mod::ImageDescriptorBufferAbstract::ImageDescriptorBufferAbstract(void *, size_t, size_t, bool)
vision::mod::ImageDescriptorBufferAbstract::ImageDescriptorBufferAbstract(const std::vector<ImageDescriptorId> &, void *, size_t, size_t, bool)
virtual CVML_status vision::mod::ImageDescriptorBufferAbstract::appendDescriptors(const vision::mod::ImageDescriptorBufferAbstract &)
virtual CVML_status vision::mod::ImageDescriptorBufferAbstract::deleteDescriptorAtIndex(const int, std::vector<int> *)
virtual CVML_status vision::mod::ImageDescriptorBufferAbstract::deleteDescriptorsAtIndexes(const std::vector<int> &, std::vector<int> *)
virtual CVML_status vision::mod::ImageDescriptorBufferAbstract::deleteDescriptorsWithIds(const std::vector<ImageDescriptorId> &, std::vector<ImageDescriptorId> *)
int vision::mod::ImageDescriptorBufferAbstract::getSelfDistanceIndexOnFlattenedList(int, int)
virtual float vision::mod::ImageDescriptorBufferAbstract::computeDistanceFrom(const vision::mod::ImageDescriptorBufferAbstract &) const
vision::mod::ImageDescriptorBufferAbstract &vision::mod::ImageDescriptorBufferAbstract::setDescriptorIdForKthDescriptor(int, vision::mod::ImageDescriptorId)
void vision::mod::ImageDescriptorBufferAbstract::resizeForDescriptorsCount(size_t, bool)
virtual vision::mod::ImageDescriptorBufferAbstract *vision::mod::ImageDescriptorBufferAbstract::createEmptyCopy() const
virtual vision::mod::ImageDescriptorBufferAbstract *vision::mod::ImageDescriptorBufferAbstract::getRepresentative(vision::mod::ImageDescriptorBufferAbstractRepresentativeMode, vision::mod::ImageDescriptorId) const
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/FaceWarper/FaceWarper_Warp.c
CVML_status FaceWarper_computeAnchorRST(const Geometry2D_cart2D *, const int *, int, const Geometry2D_point2D *, const Geometry2D_size2D *, Geometry2D_RST *)
v16@?0Q8
createBridgeSegment
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/RectangleDetector/QuadDetect/ContourUtilities.c
mPnts == nPnts
reverseContour
currCPtr != NULL
AnnealContour
sPtrPrev != sPtr
sPtr != sPtrNext
rPtr != NULL
annealSegments
sPrevPtr != sPtr
newCurr != NULL
mergeContourEnd
cPtr2->active
PCOORD_EQUAL(sPtr2->pary[sPtr2->nPnts-1], pc3)
(code == 1) || (code == 2)
mergeEndpointSearch4
code1 != -1
code2 != -1
VNSceneClassifierCreationOptionCustomHierarchy
VNSceneClassifierOptionMaximumLeafLabels
VNSceneClassifierOptionMaximumHierarchicalLabels
scene-descriptor
scene-classifier
scene-classifier-labels
scene-classifier-relationships
AN-out-1-q_descriptor
AN-out-1-q_classifier
custom hierarchy created for revision %lu cannot be used with a detector for revision %lu
VN_DEBUG_DUMP_SCENE_INTERMEDIATES
VN_scene_classifier_debug_intermediates/
%@ was initialized with request revision %lu but is being performed with request revision %lu
json
Could not compute image descriptor for image
Invalid data for input sceneprint descriptors
Invalid data for input image descriptor option
hash
TQ,R
superclass
T#,R
description
T@"NSString",R,C
debugDescription
classifierResourceTypesToNames
T@"NSDictionary",R,C
espressoModelImageprintClass
returnAllResultsOptionKey
T@"NSString",R
cla20k512_01f
Cannot load model for FaceClassifier_BoostedPixelDifference, model file path is nil
Cannot create FaceClassifier_BoostedPixelDifference with model path:'%s'
No image object passed to face prediction
Face Boosting Classifier initialization error
Invalid argument passeed in
VisionErrorDomain
The image is invalid
Invalid requestRevision requested
URL can't be nil
  calculateChecksumMD5ForFile: error reading %d bytes from file
Cannot create an MTLDevice
default
metallib
Cannot locate MLTLibrary
Cannot create an MTLLibrary
Cannot create an MTLCommandQueue
device
T@"<MTLDevice>",R,N,V_device
commandQueue
T@"<MTLCommandQueue>",R,N,V_commandQueue
library
T@"<MTLLibrary>",R,N,V_library
Geometry2D_cart2D vision::mod::ERT2D<float>::predict(const Geometry2D_cart2D &, const Geometry2D_cart2D &, const uint32_t) [F = float]
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/ERT/ERT2D.cpp
algParam
void vision::mod::ERT2D<float>::init(const vision::mod::ModelValues &, const char *, int32_t) [F = float]
nodeParam
leafParam
regLookup
ERT - p : %d
testFeature
p.n > 0
ERT - node params are unavailable
ERT - reg lookup is unavailable
ERT - maxCacheSize: %d
ERT - cacheIndexL : %d
ERT - cacheIndexR : %d
false
ERT - anchorIndex : %d
CVML_status vision::mod::ERT2D<float>::testFeature(int, int, int, const vision::mod::Transformation2D *, Geometry2D_cart2D &, bool &) [F = float]
CVML_status vision::mod::ERT2D<float>::testCascade(int, const vision::mod::Transformation2D *, Geometry2D_cart2D &) [F = float]
CVML_status vision::mod::ERT2D<float>::testCascade(int, const vision::mod::Transformation2D *, Geometry2D_cart2D &) [F = float]_block_invoke
com.apple.vis
request %@ was cancelled
request was cancelled
VNRequest
missing option %@
option %@ has an invalid value of %@
 - %@
argument %@ has an invalid value of %@
%@ requires the GPU for processing
processing with %@ is not supported
%@ cannot be performed with revision %lu, only %@
: %s
%@ (%@)
Internal Error Occurred. Code: %d; description: %s; reason: %s
"%@"
-%lu
v32@?0{_NSRange=QQ}8^B24
%@:%@:%@
Processing appendBurstSequenceFrame request
burst context is not available
burstFrameIdentifier
T@"NSString",C,N,V_burstFrameIdentifier
imageProperties
T@"NSDictionary",C,N,V_imageProperties
VNBarcodeSymbologyAztec
VNBarcodeSymbologyCode39
VNBarcodeSymbologyCode39Checksum
VNBarcodeSymbologyCode39FullASCII
VNBarcodeSymbologyCode39FullASCIIChecksum
VNBarcodeSymbologyCode93
VNBarcodeSymbologyCode93i
VNBarcodeSymbologyCode128
VNBarcodeSymbologyDataMatrix
VNBarcodeSymbologyEAN8
VNBarcodeSymbologyEAN13
VNBarcodeSymbologyI2of5
VNBarcodeSymbologyI2of5Checksum
VNBarcodeSymbologyITF14
VNBarcodeSymbologyPDF417
VNBarcodeSymbologyQR
VNBarcodeSymbologyUPCE
mn6rw9nrgs_opt.espresso
could not locate the face detection model file
B16@?0^@8
VN_facedetector_debug_intermediates/
_fd_image.vdump
_fd_image.png
_raw_bboxes.json
%@_face_%d
_raw_bbox_crop.png
imageURL
<binary-data>
rect
width
height
VN Face detector debug intermediates written to: %@
Failed to align a detected bounding box
B56@?0@"VNImageBuffer"8{CGRect={CGPoint=dd}{CGSize=dd}}16^@48
Wiping layers for face detector unsuccessful.
Exception thrown while trying to purge face detector layers.
unknown
noMapping
1DAffineMapping
1DLogisticMapping
1DPairwiseAffineMapping
postProcessorType
CVML_status vision::mod::readPostProcessorType(FILE *, const char *, std::string &)
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/ImageAnalyzer/ImageAnalyzer_PostProcessorMappings.cpp
CVML_status vision::mod::readParametersNoMapping(FILE *, const char *, vision::mod::ImageAnalyzer_PostProcessorParameters &)
CVML_status vision::mod::readParameters1DAffineMapping(FILE *, const char *, vision::mod::ImageAnalyzer_PostProcessorParameters &)
CVML_status vision::mod::compute1DAffineMapping(const vision::mod::ImageAnalyzer_PostProcessorParameters &, const std::vector<float> &, std::vector<float> &)
CVML_status vision::mod::readParameters1DLogisticMapping(FILE *, const char *, vision::mod::ImageAnalyzer_PostProcessorParameters &)
CVML_status vision::mod::compute1DLogisticMapping(const vision::mod::ImageAnalyzer_PostProcessorParameters &, const std::vector<float> &, std::vector<float> &)
CVML_status vision::mod::readParameters1DPairwiseAffineMapping(FILE *, const char *, vision::mod::ImageAnalyzer_PostProcessorParameters &)
CVML_status vision::mod::compute1DPairwiseAffineMapping(const vision::mod::ImageAnalyzer_PostProcessorParameters &, const std::vector<float> &, std::vector<float> &)
CVML_status vision::mod::loadFloat32Vector(FILE *, const std::string &, std::vector<float> &)
leftEyeOpen
TB,VleftEyeOpen
rightEyeOpen
TB,VrightEyeOpen
smiling
TB,Vsmiling
leftEyeBlinkScore
Tf,VleftEyeBlinkScore
rightEyeBlinkScore
Tf,VrightEyeBlinkScore
smileScore
Tf,VsmileScore
hasLeftEye
TB,VhasLeftEye
hasRightEye
TB,VhasRightEye
foundByFaceCore
TB,VfoundByFaceCore
normalizedFaceRect
T{CGRect={CGPoint=dd}{CGSize=dd}},VnormalizedFaceRect
focusScore
Tf,VfocusScore
faceScore
Tf,VfaceScore
leftEyeRect
T{CGRect={CGPoint=dd}{CGSize=dd}},VleftEyeRect
rightEyeRect
T{CGRect={CGPoint=dd}{CGSize=dd}},VrightEyeRect
FCRLeftEyeFeaturesOffset
Ti,VFCRLeftEyeFeaturesOffset
FCRRightEyeFeaturesOffset
Ti,VFCRRightEyeFeaturesOffset
FCRSmileFeaturesOffset
Ti,VFCRSmileFeaturesOffset
FCRBlinkFeaturesSize
Ti,VFCRBlinkFeaturesSize
FCRSmileFeaturesSize
Ti,VFCRSmileFeaturesSize
FCRSmileAndBlinkFeatures
T@"NSMutableArray",&,VFCRSmileAndBlinkFeatures
hwFaceRect
T{CGRect={CGPoint=dd}{CGSize=dd}},V_hwFaceRect
normalizedFocusScore
Tf,VnormalizedFocusScore
normalizedSigma
Tf,VnormalizedSigma
hasRollAngle
TB,VhasRollAngle
hasYawAngle
TB,VhasYawAngle
hasPitchAngle
TB,V_hasPitchAngle
rollAngle
Tf,VrollAngle
yawAngle
Tf,VyawAngle
pitchAngle
Tf,V_pitchAngle
timestamp
Td,Vtimestamp
isSyncedWithImage
TB,V_isSyncedWithImage
smallFace
TB,VsmallFace
Image_ImageROIGridStartX
Image_ImageROIGridStartY
Image_ImageROIGridEndX
Image_ImageROIGridEndY
Original ROI = %d,%d -> %d,%d
Smoothed ROI = %d,%d -> %d,%d
Sharpness ROI for %s updated to (%d,%d)->(%d,%d)
%s REGISTERED AGAINST %s
Registration result: tx = %d, ty = %d
----------REGISTRATION ERROR INTEGRAL 
Row interval: (%d->%d)
Column interval: (%d->%d)
sensedROI = (%d,%d)->(%d,%d)
referenceROI = (%d,%d)->(%d,%d)
Registration rejected due to ROI too large or too small.
Registration in favor of face detection ROI.
Registration rejected due to skewness, which can indicate a bad registration result.
Registration rejected due to insufficient local motion.
----------------------- facecore count = %d, numHWFaces = %d
Limited ROI = (%d,%d)->(%d,%d)
Computing sharpness over grid points (%d,%d)->(%d,%d)
After collapse avgHorzDiffY = %f, blurExtent = %f
Num HW faces = %d, facecore faces = %d
combined normalized focus score for face core detections = %f
Limited sharpness score = %f, with number of faces = %d
Thumbnail selection score computation for %s
Average facial focus score = %f
Initial score (no faces) = %f (isGarbage = %d)
Action selection score = %f
imageId
T@"NSString",&,V_imageId
orientation
Ti,Vorientation
faceStatArray
T@"NSMutableArray",&,V_faceStatArray
exclude
TB,Vexclude
AEStable
TB,VAEStable
AEAverage
Ti,VAEAverage
AETarget
Ti,VAETarget
AFStable
TB,VAFStable
temporalOrder
Ti,VtemporalOrder
avgHorzDiffY
Tf,VavgHorzDiffY
blurExtent
Tf,VblurExtent
imageScore
Tf,VimageScore
actionScore
Tf,VactionScore
timeReceived
Td,VtimeReceived
maxSkewness
Tf,VmaxSkewness
registrationErrorX
Tf,VregistrationErrorX
registrationErrorY
Tf,VregistrationErrorY
registrationErrorIntegral
Tf,VregistrationErrorIntegral
actionClusteringScore
Tf,VactionClusteringScore
hasRegistrationData
TB,VhasRegistrationData
facesRoiRect
T{CGRect={CGPoint=dd}{CGSize=dd}},VfacesRoiRect
numHWFaces
Ti,VnumHWFaces
emotionallyRejected
TB,VemotionallyRejected
doLimitedSharpnessAndBlur
TB,VdoLimitedSharpnessAndBlur
Tf,Vtx
Tf,Vty
isGarbage
TB,VisGarbage
roiSize
Tf,VroiSize
AEDelta
Ti,V_AEDelta
Registration error stats: mean=%f, stdDev=%f, skewness=%f, maxValue=%f
Insufficient peak error for ROI computation %f (threshold %f)
Peak rejection threshold = %f (mean = %f, std = %f)
Starting ROI construction at %d->%d
ERROR: unknown transformation
VNImageOptionImageOrientation
VNImageOptionProperties
VNImageOptionCameraPixelFocalLength
VNImageOptionCameraIntrinsics
VNImageOptionCIContext
VNImageOptionCameraOpticalCenter
VNRequestHandlerCleanupOption_AllPipelines
VNRequestHandlerCleanupOption_FacePipeline
VNRequestHandlerCleanupOption_ScenePipeline
VNRequestHandlerCleanupOption_SmartCamPipeline
VNRequestHandlerCleanupOption_JunkPipeline
VNCleanupLevel_Complete
VNCleanupLevel_Partial
VNRequestHandlerCleanupOption_ImageBuffers
VNCleanupPurgeAll
could not create an image buffer
imageSpecifier
com.apple.VNRequestHandler
modelContextObject
T@"NSObject",&,N,V_modelContextObject
burstAnalysisLoggingCallback
T@?,C,N,V_burstAnalysisLoggingCallback
VNPhotosRequestHandler
model request handler not available
VNSequenceRequestHandlerOption_LoggingCallback
CVML_status LandmarkDetector_getIntensityFeature(const vImage_Buffer &, const Geometry2D_point2D &, LandmarkDetector_intensitySamplingMode, bool, uint8_t, float &)
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/LandmarkDetector/LandmarkDetector_Features.cpp
CVML_status LandmarkDetector_getImagePreProcessedSamplingPixelPoint(const vImage_Buffer &, const Geometry2D_point2D &, bool, uint8_t, int, Geometry2D_point2D &)
float LandmarkDetector_getImagePreProcessedGaussianSmoothedIntensity(const vImage_Buffer &, const Geometry2D_point2D &)
failed to create a %lu x %lu pixel buffer of type '%c%c%c%c'
failed to warp image
failed to create GL context
failed to create image registration context
registration cannot be performed on images with different dimensions
CVML_status FastRegistration_computeSignatures(const vImage_Buffer *, _Bool, dispatch_queue_t, FastRegistration_Signatures *)
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/ImageRegistration/FastRegistration/FastRegistration_Core.c
CVML_status FastRegistration_register(const FastRegistration_Signatures *, const FastRegistration_Signatures *, float, dispatch_queue_t, float *, float *, float *, float *)
CVML_status FastRegistration_compareSignatures(const FastRegistration_Signatures *, const FastRegistration_Signatures *, float, float, float *, vImagePixelCount *, float *, vImagePixelCount *)
CVML_status FastRegistration_processProjections(float *, vImagePixelCount)
%@ cannot be performed with compatibility revision %lu and sceneprints generated by revision %lu
v32@?0@"NSString"8@16^B24
q24@?0@"VNClassificationObservation"8@"VNClassificationObservation"16
%@:%@
the custom hierarchy is for request revision %lu, not %lu
supportedImageSizeSet
T@"NSArray",R
knownSceneClassifications
T@"NSArray",R,N
sceneObservation
T@"VNSceneObservation",R,&,N,V_sceneObservation
maximumLeafObservations
TQ,N,V_maximumLeafObservations
maximumHierarchicalObservations
TQ,N,V_maximumHierarchicalObservations
customHierarchy
T@"VNSceneClassificationCustomHierarchy",R,C,N
static CVML_status vision::mod::image_quality::BlurMeasure::computeEdgeBasedBlurForImageRegionUsingBlurSignature(void *, Geometry2D_rect2D, float *, float, int *, int *)
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/ImageQuality/BlurMeasure/BlurMeasure.mm
CVML_status (anonymous namespace)::computeBlurStatsOnImageData(uint8_t *, int, int, int, float, float *, int *, int *, void **)
CVML_status (anonymous namespace)::applyInsetFactorToData(uint8_t **, int *, int *, int, int, float)
CVML_status (anonymous namespace)::computeBlurScoreOnImageSubblocks(uint8_t *, int, int, int, int, float, int, float *, int *, int *)
CVML_status (anonymous namespace)::applyInsetFactorToROI(Geometry2D_rect2D *, float)
confidence
mergesCount
smartDistance
B24@?0@"ShotflowFaceDetection"8@"NSDictionary"16
overlap_threshold
Tf,N,V_overlap_threshold
threshold
Tf,N
filterThreshold
Tf,N,V_filterThreshold
nmsThreshold
Tf,N,V_nmsThreshold
osfsThreshold
Tf,N,V_osfsThreshold
osfsSizeRatio
Tf,N,V_osfsSizeRatio
olmcsThreshold
Tf,N,V_olmcsThreshold
olmcsMergeCountDelta
Ti,N,V_olmcsMergeCountDelta
smartThreshold
Tf,N,V_smartThreshold
smartDistanceFactor
Tf,N,V_smartDistanceFactor
'%c%c%c%c'
0x%08X
tag %@ did not provide any data
tag %@ has a data overflow to %lu bytes
encountered unexpected length of %u, instead of %u
could not decode object of class %@
failed to write to data stream
unexpected end of data stream
Error while computing blur score: %s
virtual vision::mod::ImageClassifierAbstract &vision::mod::ImageClassifierEspresso::setDescriptorProcessor(const std::shared_ptr<ImageDescriptorProcessorAbstract> &)
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/ImageClassifier/ImageClassifierEspresso/ImageClassifier_Espresso.mm
virtual std::map<std::string, float> vision::mod::ImageClassifierEspresso::classifyDescriptorHandler(const vision::mod::ImageDescriptorBufferAbstract &)
v16@?0r^{shared_ptr<Espresso::abstract_batch>=^{abstract_batch}^{__shared_weak_count}}8
v24@?0{shared_ptr<Espresso::blob<float, 4> >=^{blob<float, 4>}^{__shared_weak_count}}8
Inconsistent platform
void vision::mod::ImageClassifierEspresso::private_t::loadClassifier(ImageClassifierEspresso::Options, vision::mod::ImageClassifierEspresso *, const char *, const char *, ImageClassifierEspresso::PLATFORM, ImageClassifierEspresso::COMPUTE_PATH)
virtual std::map<std::string, float> vision::mod::ImageClassifierEspresso::classifyImage_RGBA8888(const vImage_Buffer &)
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/ImageClassifier/ImageClassifierEspresso/ImageClassifier_Espresso.h
virtual std::map<std::string, float> vision::mod::ImageClassifierEspresso::classifyImage_BGRA8888(const vImage_Buffer &)
virtual std::map<std::string, float> vision::mod::ImageClassifierEspresso::classifyImage_Planar8(const vImage_Buffer &)
%@:Trk=%@
tracker type is undefined
tracking level is undefined
input observation is not available
%@ does not override %@
Tracker key has changed, restarting tracking sequesnce: current tracker key = %@; new tracker key = %@
Tracker level has changed, restarting tracking sequesnce: current tracking level = %@; new tracking level = %@
Internal error: internal type conversion failed
Internal error: unexpeted tracked object bounding box size
Bounding box input location has changed, restarting tracking sequesnce
currentTrackerBBox.x = %f; currentTrackerBBox.x = %f; currentTrackerBBox.width = %f; currentTrackerBBox.height = %f; 
newTrackerBBox.x = %f; newTrackerBBox.x = %f; newTrackerBBox.width = %f; newTrackerBBox.height = %f; 
currentBBoxArea = %f; overlappedBBoxArea = %f; overlappedBBoxArea / currentBBoxArea = %f
inputObservation
T@"VNDetectedObjectObservation",&,N,V_inputObservation
trackingLevel
TQ,N,V_trackingLevel
lastFrame
TB,N,GisLastFrame,V_lastFrame
Requested Metal Device is not supported: %@
metalDevice
T@"<MTLDevice>",R,V_metalDevice
wisdomParams
T@"NSDictionary",R,V_wisdomParams
useGPU
TB,R,V_useGPU
VNDetectorProcessOption_InputImageBuffers
VNDetectorProcessOption_ModelBackingStore
VNDetectorProcessOption_ImageCropAndScaleOption
VNDetectorProcessOption_Canceller
VNDetectorProcessOption_RequestRevision
unable to resolve which detector class to create for %@
com.apple.VN.processingQueue.%@
unable to create processing queue
:%@=%@
%@ does not implement %@
@"NSString"8@?0
Cannot create Metal Context for non-GPU targeting device
Cannot initialize Metal Context
requestRevision
TQ,R,N
synchronizationQueue
T@"NSObject<OS_dispatch_queue>",&,N,V_synchronizationQueue
configurationOptions
T@"NSDictionary",R,C,N,V_configurationOptions
metalContext
T@"VNMetalContext",R,N,V_metalContext
processingQueue
T@"NSObject<OS_dispatch_queue>",R,N,V_processingQueue
backingStore
TQ,R,N,V_backingStore
TQ,R,N,V_requestRevision
Espresso error
incorrect binserializer key
small sparsity error
feature extraction error
initialization error
no saved state to revert
nominal distance not changed
batch size violation
computation kill request was issued
too few IDs to build VIP model
video error
error with projection computation
missing positional parameter
inconsistent state error
warping error
OpenGL error
invalid format
out of bounds
singular point configuration error
division by zero
LAPACK error
platform endianess not supported
hash already in use
invalid ID
invalid data type
data inconsistency error
I/O error
unknown option
invalid option
missing option
delegate error
vImage related error
memory allocation error
invalid parameter
unexpected null pointer
internal error
not implemented error
CVML_status module %d error %lld
BinSerializer
Face3D
FaceDescriptor
FaceFrontalizer
FaceWarper
Geometry2D
Geometry3D
ImageGrouping
ImageQuality
LandmarkDetector
MomentProcessor
FaceboxAligner
ImageDescriptor
ImageClassifier
ImageProcessing
VIPIdentification
ImageRegistration
SimilarityMatrix
Clustering
HumanDetector
FaceRegionMap
ObjectDetector
ObjectTracker
SRCClassifier
Kmeans
SparseCoding
BoostedClassifier
FaceSegmenter
ImageAnalyzer
FaceAttributes
Generic
ImageTools
VideoTools
ImageWarper
ThirdParty
BinSerializerProcessor
AppleNetParser
FaceProcessorCLI
ImageClassifierCLI
MPCmdlineClientCLI
ClusteringCLI
ImageProcessorCLI
PhotosProcessorCLI
CVMLEngine
CVML Module %lld
v32@?0@"<NSCopying>"8@"VNImageAnalyzerCompoundRequestConfiguration"16^B24
VNFaceRegionMapVersion
userX
userY
userW
userH
alignX
alignY
alignW
alignH
rgnMapData
rgnMapW
rgnMapH
rgnMapRowBytes
pixelToRgnMap
unknown coding version
region map data has length of %lu instead of the expected %lu
Unknown
supportsSecureCoding
TB,R
regionLabels
T@"NSArray",C,V_regionLabels
CVML_status ImageProcessing_createCGImageFromVImageBuffer(const vImage_Buffer *, ImageProcessing_ImageType, CGImageRef *)
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/ImageProcessing/ImageProcessing_CoreGraphicsUtils.c
com.apple.VN.createGaborFilterBankGCDQueueName
com.apple.VN.extractGaborDescriptorGCDQueueName
com.apple.VN.gaborReadySyncQueueName
com.apple.VN.gaborDescriptorReadySyncQueueName
CVML_status Geometry3D_projectCart(const Geometry3D_cart3D *, const float *, const Geometry3D_pose *, const Geometry2D_cart2D *)
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/Geometry3D/Geometry3D_Projection.c
landmarkRefinerAndPupil_v2
Invalid landmark refiner model resource path
Could not read landmark refiner model data
unknown exception thrown
Unloading landmarks refiner model failed
Invalid image buffer size
Cannot create intermediate image buffer
Invalid parameters passed to landmark score computation
Could not compute landmark score, error code = %lld
Invalid parameters passed to blink score computation
faceAttributesPupilRefiner
T^v,R
model file is corrupt
void cvml::util::binserialized_table_of_contents::init(const void *const, size_t)
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/VisionKitFramework/VN/algorithm_util/binserialized_mapped_file_contents.h
Error %s when executing %s in file %s:%d
syslog_assert_failed
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/VisionKitFramework/VN/algorithm_util/common_defines.h
Model file info populated incorrectly
void cvml::util::binserialized_contents::init_model_values(const cvml::util::binserialized_table_of_contents &, const char *, const vision::mod::BinSerializedModelFileInfo &)
CVML_status FaceRegionMap_addForeheadLandmarks(std::vector<Geometry2D_point2D> &)
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/FaceRegionMap/FaceRegionMap_Core.cpp
VNImageBufferOption_DownscaleCGInterpolationQuality
VNImageBufferOption_UpscaleCGInterpolationQuality
VNImageBufferAugmentationApplePipeline
VNImageBufferAugmentationBlur
VNImageBufferAugmentationNoise
VNImageBufferAugmentationRotation
VNImageBufferAugmentationFlip
VNImageBufferAugmentationShear
VNImageBufferAugmentationExposure
VNImageBufferAugmentationOptionMaxRange
VNImageBufferAugmentationOptionMinRange
VNImageBufferAugmentationOptionNumberOfBuffers
VNImageBufferAugmentationOptionRandomSeed
ERROR while purging caches %s | %@
-[VNImageBufferManager purgeAllCaches]
this release call should not be used with anything but a referencing pixelbuffer %s
void CVPixelBufferReleaseReferencingPixelBufferCallback(void * _Nullable, const void * _Nullable)
Failed to create image for processing
Failed to create image for processing due to invalid requested buffer dimensions
Missing target buffer for crop operations
unable to create the interim YUV buffer
unable to create the Y plane wrapper buffer
Failed to transfer inBuffer to croppedBuffer. Error %d
VNImageBuffer - unhandled orientation: %d
Unable to crop image from source buffer
Unable to create CGImage for scaling
Unable to create a CGBitmapContext
Operation failed due to attempt to crop zero or near zero dimensioned area
Could not create buffer with format %@ (%ld)
invalid ROI size of %f x %f
invalid chunk size of %ld x %ld
invalid chunk increment of %lu x %lu
unable to create the cropped buffer - error: %d
CIDiscBlur
CIExposureAdjust
CIStraightenFilter
CIRandomGenerator
CIColorMonochrome
CIMultiplyBlendMode
The augmentationOptions do not conatain any of the VNImageBufferAugmentation keys
0x%x
CVML_status Geometry2D_buildCalibrationMatrix(Geometry2D_size2D *, float, float *)
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/Geometry2D/Geometry2D_Calibration.c
CVML_status Geometry2D_pixelToMetricHomo2D(const Geometry2D_homo2D *, const float *, Geometry2D_homo2D *)
CVML_status Geometry2D_metricToPixelHomo2D(const Geometry2D_homo2D *, const float *, Geometry2D_homo2D *)
meanShape
Processing Create Torsoprint request
inputFaceObservations
T@"NSArray",C,N
com.apple.espresso.mainqueue
vision::mod::ObjectDetector_DCNFaceDetector::ObjectDetector_DCNFaceDetector(const vision::mod::ObjectDetector_DCNFaceDetector::Options &)_block_invoke
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/ObjectDetector/DCNFaceDetector/ObjectDetector_DCNFaceDetector.mm
virtual std::vector<std::vector<DetectedObject> > vision::mod::ObjectDetector_DCNFaceDetector::detectObjectsInImages_Planar8(const std::vector<vImage_Buffer> &)
virtual std::vector<DetectedObject> vision::mod::ObjectDetector_DCNFaceDetector::detectObjectsInImage_Planar8(const vImage_Buffer &)
Unspecified error
face
model8c_.espresso.net
 modified on %@
 <dirty>
writeVersion
ToOutputStream:options:md5Context:error:
%@%lu%@
writeReadOnlyVersion
v24@?0Q8^B16
failed to obtain the data
VNRequestNameDetectFaceRectangles
VNRequestNameDetectFaceLandmarks
VNRequestNameDetectFaceExpressions
VNRequestNameDetectFace3DLandmarks
VNRequestNameDetectFacePose
VNRequestNameDetectHumanRectangles
VNRequestNameImageBlurriness
VNRequestNameExposureScore
VNRequestNameImageBrightness
VNRequestNameImageRegistration
VNRequestNameSceneClassification
VNRequestIdentifyJunk
VNRequestNameMLTransform
VNRequestNameSupportedBarcodeSymbologies
VNRequestNameDetectBarcodes
VNRequestNameCreateFaceprint
VNRequestNameCreateTorsoprint
VNRequestNameCreateFaceTorsoprint
VNRequestCreateFaceRegionMap
VNRequestNameDetectRectangles
VNRequestNameDetectHorizon
VNRequestNameAlignFaceRectangle
VNRequestNameCreateImageprint
VNRequestNameGroupImagesByTimeAndContent
VNRequestGetClusters
VNRequestOptionRepresentativeSample
VNRequestNameObjectTracking
VNRequestNameFaceTracking
VNRequestNameRectangleTracking
VNRequestNameClassifyImageAesthetics
VNRequestNameSequenceSummarization
VNRequestNameGenerateImageSaliency
VNRequestNameClassifyFaceAttributes
VNSegmentFaceLandmarksRequest
VNRequestBurstAnalysisResults
VNRequestOptionInputImage
VNRequestOptionInputFaces
VNRequestOptionAddObjectsToClustering
VNRequestOptionAddObjectGroupIdsToClustering
VNRequestOptionRemoveObjectsFromClustering
VNRequestOptionInputClusterIds
VNRequestOptionInputClusterFlags
VNRequestOptionForceFaceprintCreation
VNRequestOptionCacheFolderPath
VNRequestOptionGetClusteredFaceIds
VNRequestOptionGetClusteredFaceIdsForFaceId
VNRequestOptionGetDistanceBetweenClustersWithFaceIds
VNRequestOptionDumpIntermediateImages
VNRequestOptionRectangleMinimumAspectRatio
VNRequestOptionRectangleMaximumAspectRatio
VNRequestOptionRectangleQuadratureTolerance
VNRequestOptionRectangleMinimumSize
VNRequestOptionRectangleMinimumConfidence
VNRequestOptionRectangleMaximumNumber
VNRequestOptionInputThreshold
VNRequestOptionInputTorsoThreshold
VNRequestOptionInputTimestamp
VNRequestOptionInputImageprints
VNRequestOptionInputRegionOfInterest
VNRequestOptionPreferBackgroundProcessing
VNRequestOptionMetalContextPriority
VNRequestOptionProcessingDevice
VNRequestOptionMaximumIntermediateSideLength
VNRequestOptionClusteringAlgorithm
VNClusteringAlgorithm_Greedy
VNClusteringAlgorithm_GreedyWithTorso
VNClusteringAlgorithm_Agglomerative
VNClusteringAlgorithm_Hierarchical
VNRequestOptionSaveClusteringState
VNRequestOptionRestoreClusteringState
VNRequestOptionDetectionLevel
VNRequestOptionDetectionLevel_Accurate
VNRequestOptionDetectionLevel_Balanced
VNRequestOptionDetectionLevel_Fast
VNRequestOptionFaceRectangleDetectorEnableLowMemoryMode
VNRequestOptionRectangleDetectorRequiredVersion
VNRequestCreateSceneprint
VNRequestOptionInputSceneprints
VNRequestOptionInputSceneClassificationAllResults
VNRequestCreateSmartCamprint
VNRequestOptionInputSmartCamClassificationAllResults
VNRequestOptionSmartCamClassificationCenterTileOnly
VNRequestOptionBlurMethod
VNRequestAppendBurstSequenceFrame
VNRequestOptionInputImageProperties
VNRequestOptionBurstFrameIdentifier
VNRequestOptionBurstAllImageIdentifiers
VNRequestOptionBurstImageStats
VNRequestOptionBurstClusters
VNRequestNameDetectTextRectangles
VNRequestOptionReportCharacterBoxes
VNRequestOptionBarcodeSymbologies
VNRequestOptionTrackingLevel
VNRequestOptionTrackingLevelAccurate
VNRequestOptionTrackingLevelFast
VNRequestOptionTrackingInputObject
VNRequestOptionInputRectangle
VNRequestOptionDetectTextRectanglesRequiredVersion
VNRequestOptionTextRecognition
VNRequestOptionVNCoreMLModel
VNRequestOptionVNCoreMLModelParameters
VNTextRecognitionOptionNone
VNTextRecognitionOptionASCIICharacterSet
VNTextRecognitionOptionEnglishCharacterSet
VNTextRecognitionOptionDanishCharacterSet
VNTextRecognitionOptionDutchCharacterSet
VNTextRecognitionOptionFrenchCharacterSet
VNTextRecognitionOptionGermanCharacterSet
VNTextRecognitionOptionIcelandicCharacterSet
VNTextRecognitionOptionItalianCharacterSet
VNTextRecognitionOptionNorwegianCharacterSet
VNTextRecognitionOptionPortugueseCharacterSet
VNTextRecognitionOptionSpanishCharacterSet
VNTextRecognitionOptionSwedishCharacterSet
VNRequestOptionTextRecognitionAdditionalCharacters
VNRequestOptionMinimumLinePixelHeight
VNRequestWarningImageTooSmall
VNRequestWarningImageTooSmallForFaceObservations
VNRequestWarningImageMinimumLongDimension
VNRequestWarningImageMinimumShortDimension
VNRequestWarningBlinkDetectionFailure
Vision
VNImageAnalyzerCompoundRequest
VNGenerateOpticalFlowRequest
VNNOPRequest
v32@?0@"NSString"8#16^B24
%@(%lu):BG=%c:%@:Det=%lu
%@-%lu:MTL=%@:Det=%lu
unable to prepare %@
com.apple.VNRequestCancellationQueue
Encountered error cancelling request: %@
-[%@ %@] has not been implemented
the %@ cancellation handler has not been implemented
imageBuffer
All elements in the %@ array must be of class %@ (%@)
supportedRevisions
T@"NSIndexSet",R,C,N
defaultRevision
currentRevision
cancellationSemaphore
T@"NSObject<OS_dispatch_semaphore>",&,V_cancellationSemaphore
cancellationTriggered
TB,V_cancellationTriggered
modelFileBackingStore
TQ,N,V_modelFileBackingStore
dumpIntermediateImages
TB,N
preferredMetalContext
T@"<MTLDevice>",&,N
metalContextPriority
TQ,N,V_metalContextPriority
disallowsGPUUse
detectionLevel
TQ,N
processingDevice
T@"VNProcessingDevice",C,N
requestName
T@"NSString",R,C,N,V_requestName
options
T@"NSDictionary",R,C,N,V_options
preferBackgroundProcessing
usesCPUOnly
results
T@"NSArray",R,C,N,V_results
completionHandler
T@?,R,C,N,V_completionHandler
revision
TQ,N,V_revision
minimumDimension
TQ,R,N,V_minimumDimension
maximumDimension
TQ,R,N,V_maximumDimension
idealDimension
TQ,R,N,V_idealDimension
pixelsWideRange
T@"VNSizeRange",R,N,V_pixelsWideRange
pixelsHighRange
T@"VNSizeRange",R,N,V_pixelsHighRange
aspectRatioHandling
TQ,R,N,V_aspectRatioHandling
idealImageFormat
TI,R,N,V_idealImageFormat
idealOrientation
TI,R,N,V_idealOrientation
orientationAgnostic
TB,R,N,GisOrientationAgnostic,V_orientationAgnostic
The %@ required option was not found
The %@ option was expected to be a %@, but was instead a %@ (%@)
FTp_data
FTp_elementsCount
FTp_length
FTp_fp
FTp_tp
FTp_labelsAndConfidence
FTp_rev
FTp_VNFaceTorsoprint
FTp_algorithmVersion
Wrong type of print obeject
faceprint
T@"VNFaceprint",R,N,V_faceprint
torsoprint
T@"VNTorsoprint",R,N,V_torsoprint
validTorsoprint
TB,R,N,GisValidTorsoprint
apple_scenes
virtual CVML_status vision::mod::ImageDescriptorProcessorAbstract::computeDescriptorForImage(const vImage_Buffer &, ImageProcessing_ImageType, vision::mod::ImageDescriptorBufferAbstract &)
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/ImageDescriptor/ImageDescriptor_ProcessorAbstract.h
virtual CVML_status vision::mod::ImageDescriptorProcessorAbstract::computeDescriptorsForImages_BGRA8888(const std::vector<vImage_Buffer> &, __strong ImageProcessing_CancellationCheckBlock, vision::mod::ImageDescriptorBufferAbstract &)
virtual CVML_status vision::mod::ImageDescriptorProcessorAbstract::computeDescriptorsForImages(const std::vector<vImage_Buffer> &, ImageProcessing_ImageType, __strong ImageProcessing_CancellationCheckBlock, vision::mod::ImageDescriptorBufferAbstract &)
CVML_status vision::mod::ImageDescriptorAugmenterAbstract::augment(const std::vector<vImage_Buffer> &, ImageProcessing_ImageType)
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/ImageDescriptor/ImageDescriptor_AugmenterAbstract.h
std::vector<vImage_Buffer *> vision::mod::ImageDescriptorAugmenterAbstract::getAugmentedBatch(int)
std::vector<vImage_Buffer> vision::mod::ImageDescriptorAugmenterAbstract::getAugmentedImages() const
ConnectedComponents
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/RectangleDetector/QuadDetect/ConnectedComponents.c
pneighbor(*(Pcoord *)dequeFirst(dbLookup[mnid]), pix)
pneighbor(*(Pcoord *)dequeFirst(dbLookup[nid]), pix)
pneighbor(*(Pcoord *)dequeFirst(dbLookup[mid]), pix)
pneighbor(*(Pcoord *)dequeFirst(dbLookup[pid]), pix)
loc != -1
cidcnt < MAX_CONTOURS
eraseContourPixels
deque != NULL
Processing Segment Face Landmarks Request
Cannot calculate face attributes
VNFaceObservation object is expected to initialize Face Tracker
Object identifier is not initialized in the input face observation
Internal error: Face tracking failed
Internal error: Unable to create request to compute bbox alignment
Internal error: One or more face bounding boxes cannot be aligned
Internal error: Cannot create face classifier
Internal error: One or more face bounding boxes is not aligned
Processing DetectHumans request
CVML_status Geometry2D_cartToBari2D(const Geometry2D_cart2D *, const Geometry2D_cart2D *, Geometry2D_bari2D *)
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/Geometry2D/Geometry2D_Baricentric.c
CVML_status Geometry2D_bariToCart2D(const Geometry2D_bari2D *, const Geometry2D_cart2D *, Geometry2D_cart2D *)
model8c_.espresso
VNFaceDetector error aligning a detected bounding box
-[VNMomentProcessor initWithOptions:error:]
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/MomentProcessor/Moments/MomentProcessor.mm
error != nil
-[VNMomentProcessor processImagesFromDataProvider:error:]
-[VNMomentProcessor computeClusteringOfImageDescriptors:intoKGroups:error:]
-[VNMomentProcessor computeNaturalClusteringOfImageDescriptors:error:]
-[VNMomentProcessor computeClusteringTreeForImageDescriptors:assumeDescriptorsAreSorted:error:]
q24@?0@"VNMPImageDescriptor"8@"VNMPImageDescriptor"16
context
T@"VNMPContext",&,N,V_context
node
T^v,V_node
freeNodeOnDealloc
TB,V_freeNodeOnDealloc
pixelBuffer
cgImage
ciImage
imageData
image buffer is no longer available
%@ must be overridden
%@;opt=%@
T@"NSDictionary",R,C,N
object
%@:PB=%p
: %@
%@:CG=%p
%@:CI=%p
%@:URL=%@
%@:Data=%p
VNImageSaliencyObservation
OISW
OISH
SOBJ
BBOX
BBOY
BBSW
BBSH
NBBOX
NBBOY
NBBSW
NBBSH
salientObjects
boundingBox
T{CGRect={CGPoint=dd}{CGSize=dd}},R,N
narrowedBoundingBox
facerec
Processing Analyze Image Aesthetics request
VNRectangleTracking_BottomLeftTracker
VNRectangleTracking_BottomRightTracker
VNRectangleTracking_TopLeftTracker
VNRectangleTracking_TopRightTracker
No objects to track passed to the tracker
setTrackedObjects finished for %s
Internal error: wrong type of a corner tracker allocated
Internal error: initialization of internal object
Internal error: Setting input rectangles to one of the rectangle corners failed
v32@?0@"NSString"8@"VNObjectTracker"16^B24
Internal error: Tracker is not initialized
trackInFrame started for %s
Internal error: wrong type of a corner tracker object created
Internal error: tracking of one or more of the rectangle corners failed
trackInFrame finished for %s
v32@?0@8@16^B24
Internal error: Resetting tracker failed with error: %llu
VNRectangleObservation object is expected to initialize Rectangle Tracker
Internal error: Tracking of one of the corners failed, confidence = %f; threshold = %f
%@:%u%u%u
includeClusters
TB,N,V_includeClusters
includeAllImageIdentifiers
TB,N,V_includeAllImageIdentifiers
includeAllImageStats
TB,N,V_includeAllImageStats
void vision::mod::ImageAnalyzer::validateOptions(const vision::mod::ImageAnalyzer_Options &)
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/ImageAnalyzer/ImageAnalyzer.cpp
void vision::mod::ImageAnalyzer::initNetwork(const char *, const char *)
^ +| +$)
std::vector<std::pair<std::string, std::vector<bool> > > vision::mod::ImageAnalyzer::loadLabelsAndBooleanFlags(const char *)
CVML_status vision::mod::ImageAnalyzer::preProcessImage(const vImage_Buffer &, const Geometry2D_rect2D &)
size_t vision::mod::ImageAnalyzer::getExpectedNumberOfLables(vision::mod::ImageAnalyzer_AnalysisType)
vision::mod::ImageAnalyzer &vision::mod::ImageAnalyzer::analyze(uint32_t, const vImage_Buffer &, const Geometry2D_rect2D &)
std::map<std::string, ImageAnalyzer_LabelConfidenceAndBoundingBox> vision::mod::ImageAnalyzer::getSceneLabelsConfidencesAndBoundingBoxes(float, bool)
std::map<std::string, float> vision::mod::ImageAnalyzer::getSceneAestheticLabels(float)
float vision::mod::ImageAnalyzer::getSceneAestheticScore()
vision::mod::ImageAnalyzer_PostProcessor::ImageAnalyzer_PostProcessor(const char *, const char *)
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/ImageAnalyzer/ImageAnalyzer_PostProcessor.h
true
size_t vision::mod::ImageAnalyzer_PostProcessor::getOutputSize(size_t)
vision::mod::ImageAnalyzer_PostProcessor &vision::mod::ImageAnalyzer_PostProcessor::process(const std::vector<float> &, std::vector<float> &)
vision::mod::ImageAnalyzer_Tensor1D<float>::ImageAnalyzer_Tensor1D(const espresso_buffer_t &, bool) [T = float]
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/ImageAnalyzer/ImageAnalyzer_Types.h
const T vision::mod::ImageAnalyzer_Tensor1D<float>::operator[](int) const [T = float]
smartcam-descriptor
smartcam-classifier
smartcam-classifier-labels
smartcam-classifier-relationships
VN_DEBUG_DUMP_SMARTCAM_INTERMEDIATES
VN_smartcam_classifier_debug_intermediates/
std::shared_ptr<ImageDescriptorProcessorAbstract> vision::mod::ImageDescriptor_EspressoJunk_CurrentModel(const char *, int, vision::mod::ImageDescriptorProcessorEspresso::PLATFORM, vision::mod::ImageDescriptorProcessorEspresso::COMPUTE_PATH, vision::mod::ImageDescriptorProcessorEspresso::Options)
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/ImageDescriptor/ImageDescriptorEspresso/Models/Junk/ImageDescriptor_EspressoJunk.mm
model_junk_12_espresso
VNFaceExpressionDetectorProcessOption_InputFaceObservations
Could not read expressions model data
Could not create face expression module
VNFaceExpressionDetector face does not have landmark points
Corrupt face mark data
unexpected exception
data
elementsCount
length
labelsAndConfidence
VNSceneprint
algorithmVersion
VNDetectorProcessingQueueOption
failure with status %lld
 (%s)
encountered unknown exception
encountered an unexpected condition: %s
encountered an unexpected condition: %@
NSException
dividerScore
Tf,VdividerScore
trueLocalMaximum
Ti,VtrueLocalMaximum
leftImage
Ti,VleftImage
actionAmount
Tf,VactionAmount
noiseThreshold
Tf,VnoiseThreshold
highNoiseThreshold
Tf,VhighNoiseThreshold
ASCII
%@:Algo=[%lu]:%lu:%u%u%u:%@
Text detector object was not created
Processing detectTextRectangles request
invalid VNRequestOptionDetectTextRectanglesRequiredVersion algorithm value of %lu
algorithm
TQ,N,V_algorithm
minimumCharacterPixelHeight
TQ,N,V_minimumCharacterPixelHeight
detectDiacritics
TB,N,V_detectDiacritics
minimizeFalseDetections
TB,N,V_minimizeFalseDetections
textRecognition
T@"NSString",C,N,V_textRecognition
reportCharacterBoxes
TB,N,V_reportCharacterBoxes
rotationAngle
Tf,V_rotationAngle
Tf,V_yawAngle
Warning: %s
Error: %s
VN_DEBUGLOG_LEVEL
Planar8: 8 bit planar
RGBA8888: 8 bit RGBA interleaved
BGRA8888: 8 bit BGRA interleaved
ARGB8888: 8 bit ARGB interleaved
ABGR8888: 8 bit ABGR interleaved
PlanarF: float 32 bit planar
RGBAFFFF: float 32 bit RGBA interleaved
BGRAFFFF: float 32 bit BGRA interleaved
ARGBFFFF: float 32 bit ARGB interleaved
ABGRFFFF: float 32 bit ABGR interleaved
Multiple types
com.apple.vis.VNPersonsModel
cannot accept model version %lu
function unavailable
operation was cancelled
unknown model kind '%@'
readObjectForVersion
Tag:fromInputStream:intoObjectDictionary:md5Context:error:
readObjectForVersion%uTag:fromInputStream:intoObjectDictionary:md5Context:error:
cannot read model version %u
model data cannot be verified due to mismatched checksums
i12@?0I8
com.apple.vis.VNPersonsModelLoader
%@ read as %@
B28@?0I8@"NSObject"12^@20
<%@: %p> %lu identities
cannot create model with version %u
configuration
T@"VNPersonsModelConfiguration",R,C,N,V_configuration
TQ,R,N,V_version
lastModificationDate
T@"NSDate",R,C,N,V_lastModificationDate
maxIdentities
faceprintsPerIdentity
maximumAllowableIdentities
maximumFaceprintsPerIdentity
TQ,N,V_maximumFaceprintsPerIdentity
maximumIdentities
TQ,N,V_maximumIdentities
personUID
personUIDClass
 '%@' (%f)
faceObservation
T@"VNFaceObservation",R,N,V_faceObservation
predictedPersonUniqueIdentifier
T@"<NSObject><NSCopying><NSSecureCoding>",R,C,N,V_predictedPersonUniqueIdentifier
Tf,R,N,V_confidence
acceptableVersions
T@"NSIndexSet",C,N,V_acceptableVersions
readonly
TQ,N,V_version
readOnly
TB,N,V_readOnly
VNFaceLandmarkDetectorType
VNFaceDetectorType
VNFaceBoxAlignerType
VNFaceAttributesClassifierType
VNFaceExpressionDetectorType
VNFaceprintGeneratorDetectorType
VNFaceLandmarksSegmenterType
VNTorsoprintGeneratorDetectorType
VNHumanDetectorType
VNJunkIdentifierType
VNSceneClassifierType
VNSmartCamClassifierType
VNImageprintGeneratorType
VNSequenceSummarizationAnalyzerType
VNSmartCamCombinedAestheticsAndSaliencyDetectorType
VNImageAnalyzerMultiDetectorType
com.apple.VN.
looking for detector with key %s
found %s
found special case %s for configuration %s
creating new instance of %s with configuration %s for %s
failed to create %s with configuration %s - %s
created %s with configuration %s for %s
A %@ detector cannot be created with options %@
B32@?0@"<NSObject><NSCopying>"8@"VNDetector"16^B24
v32@?0@"<NSObject><NSCopying>"8@"VNDetector"16^B24
Error opening 
previous request results
%@ must override %@
VNFaceAttributeCategoryVersion
facrRev
FAC_label
FAC_LAC
VNFaceAttributesVersion
farRev
age_Cat
gender_Cat
eyes_Cat
smiling_Cat
facehair_Cat
haircolor_Cat
bald_Cat
glasses_Cat
age_baby
age_child
age_youngadult
age_senior
gender_male
gender_female
eyes_closed
eyes_open
smiling_not
facehair_beard
facehair_goatee
facehair_moustache
facehair_stubble
facehair_unsure
haircolor_black
haircolor_blonde
haircolor_brown
haircolor_gray
haircolor_red
haircolor_white
bald_not
bald
glasses_prescription
glasses_sunglasses
glasses_none
rev=%d
label=%@
allLabels=%@
label
T@"VNClassificationObservation",C,N,V_mostLikelyLabel
allLabelsWithConfidences
T@"NSArray",C,N,V_allLabelsWithConfidences
rev=%d
age=%@
gender=%@
eyes=%@
smiling=%@
facehair=%@
haircolor=%@
bald=%@
glasses=%@
ageCategory
T@"VNFaceAttributeCategory",C,N,V_ageCategory
genderCategory
T@"VNFaceAttributeCategory",C,N,V_genderCategory
eyesCategory
T@"VNFaceAttributeCategory",C,N,V_eyesCategory
smilingCategory
T@"VNFaceAttributeCategory",C,N,V_smilingCategory
faceHairCategory
T@"VNFaceAttributeCategory",C,N,V_faceHairCategory
hairColorCategory
T@"VNFaceAttributeCategory",C,N,V_hairColorCategory
baldCategory
T@"VNFaceAttributeCategory",C,N,V_baldCategory
glassesCategory
T@"VNFaceAttributeCategory",C,N,V_glassesCategory
sumSpatialPooling
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/HumanDetector/cv/CVCommon.cpp
pooledW = sumSpatialPoolingX(pooledRespY, Ncol, ptrPooledX, r)
convTriX
w0<=w1
human_3_model
loadModel
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/HumanDetector/humandetector/TemplateObjectDetectorApply.h
readFastDTreeDict
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/HumanDetector/ml/DecisionTreeApply.h
idx>=0 && idx<fastDTreeDict.size()
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/ImageDescriptor/ImageDescriptor_BufferFloat32.cpp
virtual std::vector<float> vision::mod::ImageDescriptorBufferFloat32::computeDistancesFrom(const vision::mod::ImageDescriptorBufferAbstract &) const
virtual float vision::mod::ImageDescriptorBufferFloat32::computeDistanceFrom(const vision::mod::ImageDescriptorBufferAbstract &) const
virtual std::vector<float> vision::mod::ImageDescriptorBufferFloat32::computeSelfDistances() const
CVML_status vision::mod::ImageDescriptorBufferFloat32::computeDistanceBetweenDescriptorAndDescriptors(const float *, const float *, size_t, float *) const
CVML_status vision::mod::ImageDescriptorBufferFloat32::computeDistanceBetweenDescriptors(const float *, const float *, float &) const
virtual vision::mod::ImageDescriptorBufferFloat32 *vision::mod::ImageDescriptorBufferFloat32::getRepresentative(vision::mod::ImageDescriptorBufferAbstractRepresentativeMode, vision::mod::ImageDescriptorId) const
void vision::mod::descriptorBufferPackScores(vision::mod::ImageDescriptorBufferFloat32 *, const std::vector<float> &)
std::shared_ptr<ImageDescriptorBufferFloat32> vision::mod::descriptorBufferUnpackedScores(const vision::mod::ImageDescriptorBufferFloat32 &, std::vector<float> &, size_t)
CVML_status Face3D_estimateCameraProjective(const Geometry2D_cart2D &, const Geometry3D_cart3D &, const float *, Geometry3D_pose &)
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/Face3D/Face3D_Core.cpp
CVML_status Face3D_estimateShapeProjective(const Geometry2D_cart2D &, const float *, const float *, int, const float *, const Geometry3D_pose &, float *)
CVML_status Face3D_updateShape(const float *, const float *, const float *, int, Geometry3D_cart3D &)
CVML_status Face3D_computeReprojectionError(const Geometry2D_cart2D &, const Geometry3D_cart3D &, const float *, const Geometry3D_pose &, float &)
objectType
bounds
T{CGRect={CGPoint=dd}{CGSize=dd}}
center
T{CGPoint=dd}
T{CGRect={CGPoint=dd}{CGSize=dd}},V_bounds
T{CGPoint=dd},V_center
sc_d
sc_ec
sc_l
sc_lac
VNSmartCamprint
sc_av
CVML_status ImageProcessing_getBytesPerPixelFromImageType(ImageProcessing_ImageType, size_t *)
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/ImageProcessing/ImageProcessing_Utils.c
CVML_status ImageProcessing_reallocVImageBuffer(vImage_Buffer *, vImagePixelCount, vImagePixelCount, size_t)
CVML_status ImageProcessing_copyVImageBufferData(const vImage_Buffer *, size_t, vImage_Buffer *)
CVML_status ImageProcessing_save(const char *, const vImage_Buffer *, ImageProcessing_ImageType)
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/Geometry2D/Geometry2D_Utils.c
CVML_status Geometry2D_Rect2DfromCoreImageToNatural(const Geometry2D_rect2D *, float, Geometry2D_rect2D *)
yyyy:MM:dd HH:mm:ss
VNFaceAttributesClassifierProcessOption_InputFaceObservations
049c-0300-112-5_chk-0200.espresso
Failed to scale and crop face rectangle
Unexpected zero size of one of the image buffer dimensions
Could not bind input buffer to Espresso network
softmax_age
softmax_gender
softmax_eyes
softmax_smile
softmax_facehair
softmax_haircolor
softmax_bald
softmax_glasses
Could not bind output face atribute score from Espresso network
Could not run Espresso network
currentVersion
WARNING: failed to compute ranking %@
CVML_status ImageProcessing_computeTilingParametersSimple(const vImage_Buffer *, size_t, int, vImagePixelCount, vImage_Buffer *, vImagePixelCount *, vImagePixelCount *)
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/ImageProcessing/ImageProcessing_Tiling.c
CVML_status ImageProcessing_computeTilingParameters(const vImage_Buffer *, size_t, int, int, vImagePixelCount, vImagePixelCount, vImage_Buffer *, vImagePixelCount *, vImagePixelCount *)
CVML_status ImageProcessing_tileImage(const vImage_Buffer *, size_t, vImagePixelCount, vImagePixelCount, vImagePixelCount, vImagePixelCount, vImage_Buffer ***, int *, int *)
CVML_status ImageProcessing_computeSegmentTiling(vImagePixelCount, vImagePixelCount, int, vImagePixelCount *, vImagePixelCount *)
CVML_status Geometry3D_POSIT(const Geometry2D_cart2D *, const Geometry3D_cart3D *, const float *, float *, float *)
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/Geometry3D/Geometry3D_POSIT.c
CVML_status Geometry3D_POSIT_getR1AndR2(__CLPK_real *, __CLPK_real *, __CLPK_integer, float *, float *)
The %d argument had an illegal value.
The %d-th diagonal element of the triangular factor of A is zero, so that A does not have full rank; the least squares solution could not be computed.
CVML_status Geometry2D_computeMinimumAlignedBoundingBox(const Geometry2D_cart2D *, Geometry2D_rect2D *)
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/Geometry2D/Geometry2D_Regions.c
CVML_status Geometry2D_isRectInsideRect(const Geometry2D_rect2D *, const Geometry2D_rect2D *, _Bool *)
CVML_status ctpl_setupTrackerWithReferenceFrame(s_tplTracker *, CVPixelBufferRef, float, float, float, float)
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/ObjectTracker/temporalEx/cTemplateTrackerFuncs.c
CVML_status ctpl_trackInFrame(s_tplTracker *, const CVPixelBufferRef)
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/ImageClassifier/ImageClassifier_HierarchicalModel.cpp
vision::mod::ImageClassifier_HierarchicalModel::ImageClassifier_HierarchicalModel(const char *, const char *, const std::vector<std::pair<std::string, bool> > &)
void vision::mod::ImageClassifier_HierarchicalModel::verifyClassificationMapCorrectness(const std::map<std::string, float> &)
void vision::mod::ImageClassfier_Graph::filterGraphForBasicNodes(const std::vector<std::pair<std::string, ImageClassfier_GraphNodeType> > &, bool)
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/ImageClassifier/ImageClassifier_IO.cpp
\s*([^\s]+)\s*->\s*([^\s]+)\s*
std::vector<std::pair<std::string, std::string> > ImageClassifier_loadRelations(const char *, const char *)
std::vector<std::string> ImageClassifier_readLinesFromFile(const char *, const char *)
bool ImageClassifier_stringToBool(const std::string &)
VNDetectorProcessOption_InputSceneprints
VNDetectorProcessOption_InputFrameNumber
video_sum_lstm.espresso
VNSceneprint input cannot be nil
Input frame number cannot be nil
lstm_2_c_out
lstm_2_c_in
lstm_2_h_out
lstm_2_h_in
input1
output1
Could not bind output aesthetics score from Espresso network
Espresso output results dimensions are incorrect
espresso-descriptor
espresso-classifier
espresso-classifier-labels
espresso-classifier-relationships
%@ must implement %@
could not locate the resource file "%@"
v32@?0@"NSString"8@"NSArray"16^B24
%@ must implement +classifierResourceTypesToNames
%@ with a %@ is not supported
Could not compute raw labels and confidence for image
Cannot calculate classification image descriptor
Cannot create image print
Cannot create observation object
Image buffer is not initialized
Image buffer supplied with 0 length dimension (width or height)
Could not compute image descriptor for image. Error: %s
T#,R,D
T@"NSString",R,D
resource key "%@" is not available
could not locate %@ in %@
symbology
barcodeDescriptor
ACBSBarcodeInfo
 (%@)
acbsBarcodeInfo
T@"NSDictionary",C,N,SsetACBSBarcodeInfo:,V_acbsBarcodeInfo
T@"NSString",R,C,N,V_symbology
T@"CIBarcodeDescriptor",R,N,V_barcodeDescriptor
payloadStringValue
T@"NSString",R,C,N
v32@?0@"NSString"8@"NSString"16^B24
Unable to obtain the supported barcode symbologies
%@:Symb=[%@]
VNDetectBarcodesLocateModeCenterOneVertical
VNDetectBarcodesLocateModeCenterOneVerticalThick
VNDetectBarcodesLocateModeCenterThreeVertical
VNDetectBarcodesLocateModeCenterThreeVerticalCrossed
VNDetectBarcodesLocateModeCenterOneHorizontal
VNDetectBarcodesLocateModeCenterOneHorizontalThick
VNDetectBarcodesLocateModeCenterThreeHorizontal
VNDetectBarcodesLocateModeCenterThreeHorizontalCrossed
VNDetectBarcodesLocateModeCenterOneEachDirection
VNDetectBarcodesLocateModeCenterThreeEachDirection
VNDetectBarcodesLocateModeCenterFiveEachDirection
VNDetectBarcodesLocateModeCenterThreeEachDirectionAndCoverageAndDiagonals
VNDetectBarcodesLocateModeCenterThreeVerticalAndCoverageAndDiagonals
VNDetectBarcodesLocateModeCenterThreeHorizontalAndCoverageAndDiagonals
VNDetectBarcodesLocateModeFastSearch
VNDetectBarcodesLocateModeRegularIntervalHorizontal
VNDetectBarcodesLocateModeRegularIntervalVertical
barcode detection requires at least one element in the symbologies property
%@ is not a supported barcode symbology
barcode type is not available
unknown barcode type of '%@'
_new%@DescriptorForACBSBarcodeInfo:
creation of a barcode descriptor for %@ is not supported
VNDetectBarcodesRequest
unable to create a barcode descriptor for %@
barcode location is not available
failed to analyze image
availableLocateModes
T@"NSArray",R,C,N
supportedSymbologies
locateMode
T@"NSString",C,N,V_locateMode
symbologies
CVML_status Geometry2D_normalizePoints(const Geometry2D_cart2D *, float *, Geometry2D_cart2D *, float *, float *, float *)
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/Geometry2D/Geometry2D_Normalization.c
VNObjectTrackerType
VNFaceTrackerType
VNRectangleTrackerType
v32@?0@"NSUUID"8@"VNTracker"16^B24
com.apple.VN.trackersCollectionManagementQueue
com.apple.VN.trackingProcessingQueue
A tracker cannot be created without specifying a unique tracker key
Cannot create a Tracker with unknown tracker type: %@
Internal error: Exceeded maximum allowed number of Trackers for a tracker type: %@
float vision::mod::GreedyClustererWithTorso::internalDistGreedy(float *, float *, int, vision::mod::DescriptorItemSideInfo *, vision::mod::DescriptorItemSideInfo *) const
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/Clustering/Clustering/GreedyWithTorso/GreedyWithTorsoClustering.cpp
std::shared_ptr<std::vector<size_t> > vision::mod::GreedyClustererWithTorso::addDescriptors(float *, std::vector<bool> &, int, int, size_t, const std::vector<float> &, const std::vector<int> &, const std::vector<DescriptorItemSideInfo> &)
std::shared_ptr<std::vector<size_t> > vision::mod::GreedyClustererWithTorso::addDescriptors(const vision::mod::ImageDescriptorBufferAbstract &, const std::vector<float> &, const std::vector<int> &)
  GreedyClustererWithTorso::serializeStatus - enter
  GreedyClustererWithTorso: Failed to open file - skipping serialization
  GreedyClustererWithTorso::serializeStatus - saving map file: %s
  GreedyClustererWithTorso: failed to save the file
virtual bool vision::mod::GreedyClustererWithTorso::serializeStatus(int) const
  GreedyClustererWithTorso::serializeStatus - done
  GreedyClustererWithTorso::deserializeStatus - failed to load memory map file
  GreedyClustererWithTorso::deserializeStatus - loaded a corrupt file as expected element size does not match
void vision::mod::GreedyClustererFacesWithTorso::getAverageDescriptorOfClusterContainingFace(vision::mod::ImageDescriptorId, std::map<ImageDescriptorId, std::vector<ImageDescriptorId> > &, float *) const
  GreedyClustererFacesWithTorso::serialize - enter
  GreedyClustererFacesWithTorso::serialize - cachefile: %s
  GreedyClustererFacesWithTorso::serialize - error creating new map file for serialization
  GreedyClustererFacesWithTorso::serialize - error calculating checksum for cluster data file
Invalid parameter passed to extract Clusterer model file names
  Clusterer - couldn't find sanity value
  Clusterer - versions mismatch (serialized: %d, current: %d
  Clusterer - data checksum mismatch for file: %s
  GreedyClustererFacesWithTorso: Failed to open '%s': errno=%d
  GreedyClustererFacesWithTorso: Opening '%s'
virtual void vision::mod::GreedyClustererFacesWithTorso::getIdsForCluster(vision::mod::ImageDescriptorId, std::vector<ImageDescriptorId> &, std::unordered_multimap<ImageDescriptorId, ImageDescriptorId> *) const
float dist_greedy(const float *, const float *, int, vision::mod::ClusteringJointDistanceTypes, vision::mod::GreedyClustering_Stage, const vision::mod::DescriptorItemSideInfo *, const vision::mod::DescriptorItemSideInfo *, bool, bool, bool)
BackedBuffer<BackingStore>::allocateElement -- could not allocate new element because grow failed
void vision::mod::GreedyClustererWithTorso::private_t::throwOnCancellation()
vision::mod::dist_greedy_status_t::dist_greedy_status_t(int, vision::mod::ImageDescriptorBufferFloat32DistanceMode)
BackedBuffer<BackingStore>::createByMappingDirectlyFromFile -- Invalid header detected for file '%s'
BackedBuffer<BackingStore>::createByMappingDirectlyFromFile -- The valid element list is corrupt
BackedBuffer<BackingStore>::isValidHeader -- cannot open source file '%s'
BackedBuffer<BackingStore>::isValidHeader -- corrupt header detected in file '%s'
BackedBuffer<BackingStore>::isValidHeader -- inconsistent header detected -- zero element size detected for file '%s'
BackedBuffer<BackingStore>::isValidHeader -- inconsistent header detected -- element count does not match max free element capacity for file '%s'
BackedBuffer<BackingStore>::isValidHeader -- inconsistent header detected -- free element count exceeds free element capacity for file '%s'
BackedBuffer<BackingStore>::isValidHeader -- inconsistent header detected -- could not validate file size for file '%s'
BackedBuffer<BackingStore>::isValidHeader -- inconsistent header detected -- expected file size does not match actual file size for file '%s'
-[VNMPImageData initWithVImage:externalImageId:andExifTimestampValue:error:]
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/MomentProcessor/Moments/MPImageData.m
ERROR: The input image does not seem to be 8888
-[VNMPImageData initWithCVPixelBufferImage:externalImageId:andExifTimestampValue:error:]
image
T^{vImage_Buffer=^vQQQ},R,V_image
imageCVPixelBuffer
T^{__CVBuffer=},R,V_imageCVPixelBuffer
imageFilePath
T@"NSString",&,V_imageFilePath
freeImageInDealloc
TB,V_freeImageInDealloc
externalImageId
T@"NSString",R,V_externalImageId
exifTimestamp
Tq,R,V_exifTimestamp
Failed to scale the input image
Failed to center square crop the input image
unable to determine preferred image size for detection
detectorPreferredImageSize
T@"VNSupportedImageSize",C,N,V_detectorPreferredImageSize
detectorWantsAnisotropicScaling
TB,N,V_detectorWantsAnisotropicScaling
detectorExecutionTimeInterval
Td,N,V_detectorExecutionTimeInterval
void vision::mod::LandmarkDetector::init(const vision::mod::ModelValues &, const char *)
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/LandmarkDetector/LandmarkDetector.cpp
void vision::mod::LandmarkDetector::preProcessImage(const vImage_Buffer &, const Geometry2D_rect2D &)
CVML_status vision::mod::LandmarkDetector::initMeanShapeWithLandmarks(const std::vector<Geometry2D_point2D> &, const vision::mod::LandmarkDetector_initializationMode)
CVML_status vision::mod::LandmarkDetector::initMeanShapeWithEyeTiltAngle(float)
std::vector<Geometry2D_point2D> vision::mod::LandmarkDetector::detectLandmarksCore(int, int)
%@:rm=%d,rle=%d,rre=%d,bd=%d,cs=%@
Processing DetectFaceLandmarks request
refineMouthRegion
TB,N,V_refineMouthRegion
refineLeftEyeRegion
TB,N,V_refineLeftEyeRegion
refineRightEyeRegion
TB,N,V_refineRightEyeRegion
performBlinkDetection
TB,N,V_performBlinkDetection
cascadeStepCount
T@"NSNumber",&,N,V_cascadeStepCount
smartcam
VNIp
ipType
MPImDesc
VNIpDescColGaborV
ipReqRev
state cannot be nil
Invalid format of VNImageprint serialized state
Failed to calculate MD5
Serialized and calculated MD5s don't match
Failed to deserialize requestRevision
Unexpected size of deserialized state of the object of type %@
Failed to initialize VNImageprint object
otherImageprint cannot be nil
unexpected error while calculating distance between faceprints
faceprints with invalid data supplied
could not compute faceprint distance
Unexpected size of serialized state of the object of type %@
%@-%p
serializedLength
descriptor
T@"VNMPImageDescriptor",&,N,V_descriptor
type
TQ,N,V_type
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/ImageDescriptor/ImageDescriptorEspresso/ImageDescriptor_Espresso.mm
vision::mod::ImageDescriptorProcessorEspresso::ImageDescriptorProcessorEspresso(vision::mod::ImageDescriptorProcessorEspresso::Options, const char *, const char *, vision::mod::ImageDescriptorProcessorEspresso::PLATFORM, vision::mod::ImageDescriptorProcessorEspresso::COMPUTE_PATH)
void vision::mod::ImageDescriptorProcessorEspresso::setNetworkBatchNumber(int)
CVML_status vision::mod::ImageDescriptorProcessorEspresso::private_t::compute_batch(vision::mod::ImageDescriptorProcessorEspresso *, const std::vector<vImage_Buffer> &, const int, float *, const Espresso::vimage2espresso_param &)
virtual CVML_status vision::mod::ImageDescriptorProcessorEspresso::computeDescriptorForImage_RGBA8888(const vImage_Buffer &, vision::mod::ImageDescriptorBufferAbstract &)
virtual CVML_status vision::mod::ImageDescriptorProcessorEspresso::computeDescriptorForImage_BGRA8888(const vImage_Buffer &, vision::mod::ImageDescriptorBufferAbstract &)
CVML_status vision::mod::ImageDescriptorProcessorEspresso::computeDescriptorForImage_XYZA8888(const vImage_Buffer &, vision::mod::ImageDescriptorBufferAbstract &, bool)
virtual CVML_status vision::mod::ImageDescriptorProcessorEspresso::computeDescriptorForImage_Planar8(const vImage_Buffer &, vision::mod::ImageDescriptorBufferAbstract &)
virtual CVML_status vision::mod::ImageDescriptorProcessorEspresso::computeDescriptorForAugmentedImage(const vImage_Buffer &, ImageProcessing_ImageType, vision::mod::ImageDescriptorAugmenterAbstract &, vision::mod::ImageDescriptorBufferAbstract &)
virtual CVML_status vision::mod::ImageDescriptorProcessorEspresso::computeDescriptorsForImages_RGBA8888(const std::vector<vImage_Buffer> &, __strong ImageProcessing_CancellationCheckBlock, vision::mod::ImageDescriptorBufferAbstract &)
CVML_status vision::mod::ImageDescriptorProcessorEspresso::computeDescriptorsForImages_XYZA8888(const std::vector<vImage_Buffer> &, vision::mod::ImageDescriptorBufferAbstract &, bool)
Inconsistent state
virtual CVML_status vision::mod::ImageDescriptorProcessorAbstract::computeDescriptorForAugmentedImage(const vImage_Buffer &, ImageProcessing_ImageType, vision::mod::ImageDescriptorAugmenterAbstract &, vision::mod::ImageDescriptorBufferAbstract &)
CVML_status vision::mod::ImageDescriptorAugmenterAbstract::augment(const vImage_Buffer &, ImageProcessing_ImageType)
CVML_status Geometry2D_estimateAffine(const Geometry2D_cart2D *, const Geometry2D_cart2D *, Geometry2D_Affine *)
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/Geometry2D/Geometry2D_Affine.c
CVML_status Geometry2D_mapAffine(const Geometry2D_cart2D *, const Geometry2D_Affine *, Geometry2D_cart2D *)
VNFaceGeometryEstimatorInitOption_ImageSize
VNFaceGeometryEstimatorInitOption_CameraFocalLength
VNFaceGeometryEstimatorProcessOption_EstimatePoseOnly
VNFaceGeometryEstimatorProcessOption_InputFaceObservation
eigenshape-current
_%llu
emp_data
emp_elementsCount
emp_length
emp_labelsAndConfidence
VNEspressoModelImageprint
emp_algorithmVersionCodingKey
emp_algorithmVersion
VNEspressoModelImageprintRequestRevision
Fauiled to unarchive 'print' object
Inconsistent intenal state
Invalid format of %@ serialized state
Memory allocation failure
Failed to initialize 'print' object
nil VNEspressoModelImageprint(s) supplied
VNEspressoModelImageprint(s) with different length supplied
VNEspressoModelImageprint(s) with different versions supplied
Unknown distance funtion requested
VNEspressoModelImageprint(s) with invalid data supplied
Unexpected error while calculating distance between VNEspressoModelImageprint(s)
currentCodingVersion
TI,R,D
codingTypesToCodingKeys
T@"NSDictionary",R,C,D
serializationMagicNumber
TQ,R,D
currentSerializationVersion
descriptorData
T@"NSData",&,V_descriptorData
elementCount
TQ,V_elementCount
lengthInBytes
TQ,V_lengthInBytes
distanceMode
Tq,V_distanceMode
T@"NSDictionary",C,V_labelsAndConfidence
T@"NSString",C,V_version
confidenceScoreType
TQ,R,N,V_confidenceScoreType
VNImageAestheticsObservation
OAES
PLHT
PREF
LCOL
PSYM
PPAT
PPERS
PPOST
NOISE
FAIL
PCOMP
INTREST
INTRUSIVE
CTILT
HCOL
LOWKEY
Score
wellChosenSubjectScore
aestheticScore
  %@=%@
v32@?0@"NSString"8@"NSNumber"16^B24
Tf,R,N,V_aestheticScore
wellFramedSubjectScore
Tf,R,N,V_wellFramedSubjectScore
Tf,R,N
wellChosenBackgroundScore
Tf,R,N,V_wellChosenBackgroundScore
tastefullyBlurredScore
Tf,R,N,V_tastefullyBlurredScore
sharplyFocusedSubjectScore
Tf,R,N,V_sharplyFocusedSubjectScore
wellTimedShotScore
Tf,R,N,V_wellTimedShotScore
pleasantLightingScore
Tf,R,N,V_pleasantLightingScore
pleasantReflectionsScore
Tf,R,N,V_pleasantReflectionsScore
harmoniousColorScore
Tf,R,N,V_harmoniousColorScore
livelyColorScore
Tf,R,N,V_livelyColorScore
pleasantSymmetryScore
Tf,R,N,V_pleasantSymmetryScore
pleasantPatternScore
Tf,R,N,V_pleasantPatternScore
immersivenessScore
Tf,R,N,V_immersivenessScore
pleasantPerspectiveScore
Tf,R,N,V_pleasantPerspectiveScore
pleasantPostProcessingScore
Tf,R,N,V_pleasantPostProcessingScore
noiseScore
Tf,R,N,V_noiseScore
failureScore
Tf,R,N,V_failureScore
pleasantCompositionScore
Tf,R,N,V_pleasantCompositionScore
interestingSubjectScore
Tf,R,N,V_interestingSubjectScore
intrusiveObjectPresenceScore
Tf,R,N,V_intrusiveObjectPresenceScore
pleasantCameraTiltScore
Tf,R,N,V_pleasantCameraTiltScore
lowKeyLightingScore
Tf,R,N,V_lowKeyLightingScore
VNFaceRegionMapGeneratorProcessOption_InputFaceObservations
faceRegionMap-current
[%g %g %g %g %g %g]
'%@' is not a valid CGAffineTransform encoding
3x3:|%g %g %g %g %g %g %g %g %g|
3x3:|
'%@' is not a valid matrix_float3x3 encoding
4x4:|%g %g %g %g %g %g %g %g %g %g %g %g %g %g %g %g|
4x4:|
'%@' is not a valid matrix_float4x4 encoding
VNObservation
uuid
 %@, revision %lu
uuid=%@:reqRev=%lu:conf=%f
T@"NSUUID",&,N,SsetUUID:,V_uuid
Tf,N,V_confidence
VNDetectedObjectObservation
 [%g %g %g %g]
T{CGRect={CGPoint=dd}{CGSize=dd}},N,V_boundingBox
identifier
T@"NSUUID",C,V_identifier
VNFaceprint
profile
platform
DEPRECATED_PROPERTY
Attempt to deserialize nil
Input data is not a VNFaceprint
CVMLFaceprint
facePrint
Input data is neither VNFaceprint nor CVMLFaceprint. NSKeyedUnarchiver error = %@
Serialized state data is an unsupported version (%lu)
Serialized state data length is invalid
Error computing serialized state payload data checksum
Serialized state payload data checksum mismatch
unsupported serialized state version %u
Error deserializing VNFaceprint
Attempt to write out nil faceprint.
the faceprint data of length %lu cannot be serialized as a version 1 representation
Checksum calculation failed
Cannot serialized VNFaceprint state for an unsupported version (%lu)
nil faceprint(s) supplied
faceprints with different length supplied
faceprints with different versions supplied
T@"NSData",&,N,V_faceprint
T@"NSString",C,N,V_key
TI,N,V_platform
TI,N,V_profile
faceprintInputPath
T@"NSString",C,N,V_faceprintInputPath
VNFaceObservation
alignedBBX
alignedBBY
alignedBBW
alignedBBH
landmarks
landmarks3D
pose
expressions
faceID
faceIDConfidence
faceTorsoprint
hasAlignedBBox
alignedRotAngle
roll
faceRegionMap
blinking
blinkScore
alignedMeanShape
landmarksScore
faceOrientationIndex
faceJunkinessIndex
faceAttributes
faceLmSegments
invalid pose data
 ID=%lu
 landmarks %g
T@"VNFaceLandmarks2D",&,N,V_landmarks
hasBBoxBeenAligned
TB,N,V_hasBBoxBeenAligned
alignedBoundingBox
T{_Geometry2D_rect2D_={_Geometry2D_point2D_=ff}{_Geometry2D_size2D_=ff}},N,V_alignedBoundingBox
alignedRotationAngle
Tf,N,V_alignedRotationAngle
landmarkPoints
T@"NSData",&,N,V_landmarkPoints
landmarkPoints3d
T@"NSData",&,N,V_landmarkPoints3d
poseData
T@"NSData",&,N,V_poseData
faceIdConfidence
Tf,N,V_faceIdConfidence
T@"NSData",&,N,V_alignedMeanShape
T@"VNTorsoprint",&,N
T@"NSNumber",&,N,V_roll
T@"NSNumber",&,N,V_yaw
landmarks3d
T@"VNFaceLandmarks3D",R,N
T@"VNFaceRegionMap",R,N,V_faceRegionMap
T@"VNFaceAttributes",R,N,V_faceAttributes
expressionsAndConfidence
T{?=[4]},R,N
nameConfidence
TQ,N,V_faceId
T@"VNFaceprint",&,N,V_faceprint
T@"VNFaceTorsoprint",R,N
faceLandmarkSegments
T@"VNFaceLandmarkSegments",R,N,V_faceLandmarkSegments
Tf,R
Tf,R,N,GfaceJunkinessIndex
Tf,R,N,GfaceOrientationIndex
referenceImageSignature
floatingImageSignature
%@ is not supported by %@
T@"VNImageRegistrationSignature",&,N,V_referenceImageSignature
T@"VNImageRegistrationSignature",&,N,V_floatingImageSignature
alignmentTransform
T{CGAffineTransform=dddddd},N
 [%g %g %g %g %g %g]
T{CGAffineTransform=dddddd},N,V_alignmentTransform
warpTransform
T{?=[3]},N,V_warpTransform
blur
exposure
blurScore
T@"NSNumber",&,N,V_blurScore
exposureScore
T@"NSNumber",&,N,V_exposureScore
VNImageprintObservation
VNImageprint
nil imageprint supplied
Failed creating a new VNImageprintObservation object
%@:ImgPrnt=%@
imageprint
T@"VNImageprint",&,N,V_imageprint
imageprintValid
TB,R,N,V_imageprintValid
imageprintVersion
T@"NSString",R,C,N,V_imageprintVersion
rawImageprintDescriptor
T@"NSData",R,N
 "%@"
T@"NSString",R,C,N,V_identifier
labels
T@"NSArray",R,C,N,V_labels
featureValue
 "%@" (%f)
T@"MLFeatureValue",R,C,N,V_featureValue
vnpbo_pbdict
T^{__CVBuffer=},R,N,V_pixelBuffer
topLeft
T{CGPoint=dd},R,N,V_topLeft
topRight
T{CGPoint=dd},R,N,V_topRight
bottomLeft
T{CGPoint=dd},R,N,V_bottomLeft
bottomRight
T{CGPoint=dd},R,N,V_bottomRight
transform
angle
T{CGAffineTransform=dddddd},N,V_transform
Td,N,V_angle
  clusterId = %lu;
  totalObjCount = %lu;
  objects = %@;
  shouldUpdateRep = %d;
  suggestedIdsForRep = %@;
  representativenessById = %@;
objects
T@"NSArray",&,N,V_objects
clusterId
TQ,N,V_clusterId
totalObjectCount
TQ,N,V_totalObjectCount
shouldUpdateRepresentative
TB,N,V_shouldUpdateRepresentative
suggestedIdsForRepresentative
T@"NSArray",&,N,V_suggestedIdsForRepresentative
representativenessById
T@"NSDictionary",&,N,V_representativenessById
clusters
suggestions
clusterState
clusteredFaceIDs
groupedClusteredFaceIDs
level0Distance
distancesByID
T@"NSArray",&,N,V_clusters
suggestionsForCluster
T@"NSArray",&,N,V_suggestionsForCluster
T@"NSData",&,N,V_clusterState
clusteredFaceIds
T@"NSSet",&,N,V_clusteredFaceIds
groupedClusteredFaceIdsForCluster
T@"NSArray",&,N,V_groupedClusteredFaceIdsForCluster
distance
T@"NSNumber",&,N,V_distance
distancesById
T@"NSDictionary",&,N,V_distancesById
VNSceneObservation
algo
descriptors
Undefined
Major component of encoded sceneprint is different than major component currently supported by software. The sceneprint object cannot be decoded.
Minor component of encoded sceneprint is different than minor component currently supported by software.
sceneprints
T@"NSArray",R,N,V_sceneprints
sceneprintVersion
T@"NSString",R,C,N,V_sceneprintVersion
VNSmartCamObservation
sc_algo
sc_descriptors
Major component of encoded smartCamprint is different than major component currently supported by software. The smartCamprint object cannot be decoded.
Minor component of encoded smartCamprint is different than minor component currently supported by software.
smartCamprints
T@"NSArray",C,N,V_smartCamprints
smartCamprintVersion
T@"NSString",R,C,N,V_smartCamprintVersion
allImages
bestImages
imageStats
coverImage
isAction
isPortrait
allImageIdentifiers
T@"NSArray",&,N,V_allImageIdentifiers
bestImageIdentifiers
T@"NSArray",&,N,V_bestImageIdentifiers
allImageStats
T@"NSDictionary",&,N,V_allImageStats
coverImageIdentifier
T@"NSString",&,N,V_coverImageIdentifier
TB,N,V_isAction
TB,N,V_isPortrait
characterBoxes
text
horizonDetector: props exist
horizonDetector: Orientation = %d
  Found makerNotes
    Found vector: %.3f,%.3f,%.3f
acc = (%.5f, %.5f, %.5f)
accelTilt = %.3f deg, accelPitch = %.3f deg, accMagnitudeDev %.3f
accelPitch = %.3f deg, accelMagnitudeDev = %.3f
MaxPitch = %.3f, MaxPixelTilt = %.3f, MinPixelTilt = %.3f, MaxAccelMagDev = %.3f, MaxAccelFFTDifff = %.3f
FFT detected angle = %.3f deg
Processing DetectFacePose request
%@:RoI=%g,%g,%g,%g:InFace[%@]
The region of interest [%g, %g, %g, %g] is not within the normalized bounds of [0 0 1 1]
B32@?0@"VNFaceObservation"8Q16^B24
regionOfInterest
T{CGRect={CGPoint=dd}{CGSize=dd}},N,V_regionOfInterest
CVML_status Geometry2D_cartToHomo2D(const Geometry2D_cart2D *, Geometry2D_homo2D *)
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/Geometry2D/Geometry2D_Homogeneous.c
CVML_status Geometry2D_estimateHomography(const Geometry2D_cart2D *, const Geometry2D_cart2D *, float *)
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/Geometry2D/Geometry2D_Homography.c
CVML_status Geometry2D_mapHomography(const Geometry2D_cart2D *, const float *, Geometry2D_cart2D *)
CVML_status Geometry2D_estimateHomographyMSS(const Geometry2D_cart2D *, const Geometry2D_cart2D *, float *, __CLPK_integer, float *)
CVML_status Geometry2D_estimateHomographyOD(const Geometry2D_cart2D *, const Geometry2D_cart2D *, float *, __CLPK_integer, float *)
pred
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/HumanDetector/ml/AdaBoostApply.cpp
weakModels.size()
CVML_status Geometry2D_estimateRST(const Geometry2D_cart2D *, const Geometry2D_cart2D *, Geometry2D_RST *)
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/Geometry2D/Geometry2D_RST.c
CVML_status Geometry2D_mapRST(const Geometry2D_cart2D *, const Geometry2D_RST *, Geometry2D_cart2D *)
Processing Analyze Image Saliency request
VNImageGrouperProcessOption_Threshold
VNImageGrouperProcessOption_AdjustDistancesWithTimestamp
Cannot create VNMomentProcessor object
Input imageprint array contains an item that is not an imageprint
vnpbo_width
vnpbo_height
vnpbo_bpr
vnpbo_pixelFormat
vnpbo_attribs
vnpbo_attach
vnpbo_bytes
%@_%zu
Processing Face Attributes request
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/BinSerializer/BinSerializer_Core.c
CVML_status BinSerializer_fseek(FILE *, const char *)
CVML_status BinSerializer_fgetBlobInfo(FILE *, const char *, uint64_t *, uint16_t *, double *, double *, uint64_t *)
CVML_status BinSerializer_freadInBytes(FILE *, const char *, _Bool, void **, size_t *)
CVML_status BinSerializer_freadInFloat(FILE *, const char *, _Bool, float **, size_t *)
_Bool LandmarkDetector_generateNormalizedFaceMesh63Landmarks(const Geometry2D_point2D *, const Geometry2D_size2D *, Geometry2D_cart2D *)
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/LandmarkDetector/LandmarkDetector_Mesh.c
initializeSpanList
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/RectangleDetector/QuadDetect/Spans.c
spl != NULL
addSpan
virtual CVML_status vision::mod::ImageDescriptorProcessorAbstract::computeDescriptorsForImages_Planar8(const std::vector<vImage_Buffer> &, ImageProcessing_CancellationCheckBlock, vision::mod::ImageDescriptorBufferAbstract &)
virtual CVML_status vision::mod::ImageDescriptorProcessorAbstract::computeDescriptorsForAugmentedImages(const std::vector<vImage_Buffer> &, ImageProcessing_ImageType, vision::mod::ImageDescriptorAugmenterAbstract &, ImageProcessing_CancellationCheckBlock, vision::mod::ImageDescriptorBufferAbstract &)
v32@?0@"<NSObject><NSCopying><NSSecureCoding>"8@"NSNumber"16^B24
A prediction for an unknown identity with serial number %@ and confidence %f was provided
q24@?0@"VNPersonsModelPrediction"8@"VNPersonsModelPrediction"16
serialNumberToIdentifier
maximumElementsPerID
faceIDModel
v32@?0@"NSNumber"8@"<NSObject><NSCopying><NSSecureCoding>"16^B24
Face ID model data deserialization failed with code %@
unable to serialize the face ID model (status = %@)
flsRev
flsWidth
flsHeight
flsChannels
flsData
rev=%d
w=%d
h=%d
chnl=%d
data=%@
Cannot create CVPixelBuffer object
espressoOutputBufferWidth
TQ,N,V_espressoOutputBufferWidth
espressoOutputBufferHeight
TQ,N,V_espressoOutputBufferHeight
espressoOutputBufferChannels
TQ,N,V_espressoOutputBufferChannels
espressoOutputBufferData
T@"NSData",&,N,V_espressoOutputBufferData
kmeans++
FaceIDModel_v1_d16
row index out of range
col index out of range
is_mean_computed and allocate cannot be true together
vector length < cols
vector length < rows
invalid axis value, axis must be either 0 or 1
invalid axis value: axis must be 0 or 1
matrix size too small for output
broadcast op: dimension mismatch
unknown axis value
dimensions of data points mismatch
output distance matrix too small
empty cumsum vector
output matrix size too small
matrix size mismatch
matrix vector size mismatch
vector size too small for output
hw.logicalcpu
%d %d %a %d
%d %d %a
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/HumanDetector/ml/DecisionTreeApply.cpp
DTreeDict.size()
CVML_status Geometry2D_cumulativeEuclideanDistanceCart2D(const Geometry2D_cart2D *, const Geometry2D_cart2D *, float *)
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/Geometry2D/Geometry2D_Distances.c
3.000 - Oct 20, 2016
burst_mode_logging
staccato_mode_logging
burst_max_pending_frames
burst_disable_analysis
burst_force_face_detection
burst_force_face_detail_detection
burst_dummy_analysis
burst_disable_facecore
burst_use_fixed_image
burst_fixed_image_filename
burst_dump_yuv
staccato_yuv_dump
burst_use_version
BurstSet_AlgorithmVersion
Image_FacesArray
Image_ISPFacesArray
Image_ImageScore
Image_Timestamp
Image_YUVData
ImageYUVWidth
ImageYUVHeight
ImageYData
ImageUVData
ImageYUVBytesPerRow
Image_TimeReceived
Image_TimeQueued
Image_TimeConverted
Image_TimeStartedAnalysis
Image_TimeStartedFaceDetection
Image_TimeDoneFaceDetection
Image_TimeDoneFaceBlinkDetection
Image_TimeDoneFaceFocusScore
Image_TimeDoneAnalysis
ImageFace_ID
ImageFaceX
ImageFaceY
ImageFaceW
ImageFaceH
ImageFaceFocusScore
ImageFaceLeftEyeOpen
ImageFaceRightEyeOpen
ImageFaceSmiling
ImageFaceLeftEyePosX
ImageFaceLeftEyePosY
ImageFaceRightEyePosX
ImageFaceRightEyePosY
ImageFaceTimestamp
ImageFaceRollAngle
ImageFaceYawAngle
ImageFacePitchAngle
ImageFaceLeftEyeBlinkScore
ImageFaceRightEyeBlinkScore
ImageFaceSmileScore
ImageFaceSmallFace
com.apple.camera
/var/mobile/Library/Caches/com.apple.camera
burstSets
com.apple.burstAnalyzer
dd-MM-yyyy'_'HH-mm-ss'_burstLog.txt'
kern.osversion
BURST ANALYSIS VERSION = %s (%s)
com.apple.staccato_dump
counter.bin
BurstDoc_AllImageStats
BurstDoc_AllImageIdentifiers
BurstDoc_BestImageIds
BurstDoc_LogFile
Computing action selection threshold
Mean non-zero actions = %f, std dev = %f
ACTION SELECTION THRESHOLD = %f
Examining image, id=%s, timestamp = %.6f, done=%d
Not processing frames, imageStat.timestamp = %.6f, latestFaceTimestamp = %.6f
LeftEyeFeaturesOffset
RightEyeFeaturesOffset
SmileFeaturesOffset
BlinkFeaturesSize
SmileFeaturesSize
foundByISP
burstimage_%06d.yuv
Image_FaceRectROI
Image_Width
Image_Height
Image_AEAverage
Image_AETarget
Image_AEStable
Image_AFStable
Image_Orientation
Image_AEMatrix
Error!  Done adding, but there are still frames left!
analysis is disabled
too many analysis frames pending
Adding image: %s
Image %d:%s has emotional score %d
Image %d:%s has been emotionally rejected.
Skipping projection computation because data isn't present
LOOKING FOR FALSE-POSITIVE FACES...
Analyzing %s...
REMOVING false-positive face with ID = %d
Keeping face with ID = %d
Collapsing %s
*_*_* GARBAGE DETECTOR FOR %s *_*_*
Travel = %f, maxSkewness = %f, avgSkewness = %f, blur = %f, avgBlur = %f, stdBlur = %f
hasFaces = %d
notBlurry = %d
veryBlurry = %d
potentiallyBlurry = %d
poorRegistration = %d
suspectRegistration = %d
******Image %s classified as garbage.
**** Image %s classified as garbage by association.
Score for %s:%d is %f 
with action score %f and center bias %f (isGarbage=%d)
NEW BEST
Cover photo PORTRAIT selection score for %d:%s = %f (unbiased = %f)
Cover photo ACTION selection score for %d:%s = %f
%s:   # faces = %d, avgH = %f
    face id=%d, rect=%.3f,%.3f,%.3f,%.3f, focus=%.3f, faceScore=%.3f, leftEyeOpen=%d, rightEyeOpen=%d
Performing emotional rejection of face images in cluster %d:
Items in next cluster:
Image %s is classified as garbage for portrait mode, no sharp faces.
Checking temporal order: %d vs. %d
Removing %d:%s
All items in one cluster.
BurstSet_TimeDoneCapturing
BurstSet_TimeDone
BurstSet_Setting_MaxNumPendingFrames
BurstSet_Setting_DisableAnalysis
BurstSet_Setting_DisableFaceCore
BurstSet_Setting_DummyAnalysisCount
BurstSet_Setting_EnableDumpYUV
BurstSet_IsAction
BurstSet_IsPortrait
BurstSet_CoverImage
  <CVMLBurst> Trying to write xml file to '%s'
loggingCallback
T@?,C,N,V_loggingCallback
faceAnalysisContext
T@"BurstImageFaceAnalysisContext",&,V_faceAnalysisContext
overrideImage
T@"VNImageBuffer",&,V_overrideImage
overrideProps
T@"NSDictionary",&,V_overrideProps
clusterArray
T@"NSMutableArray",&,V_clusterArray
Ti,V_temporalOrder
faceIDCounts
T@"NSCountedSet",&,V_faceIDCounts
T@"NSMutableArray",&,V_allImageIdentifiers
statsByImageIdentifier
T@"NSMutableDictionary",&,V_statsByImageIdentifier
clusterByImageIdentifier
T@"NSMutableDictionary",&,V_clusterByImageIdentifier
burstLogFileName
T@"NSString",&,V_burstLogFileName
actionClassifier
T@"BurstActionClassifier",&,V_actionClassifier
maxNumPendingFrames
Ti,V_maxNumPendingFrames
enableAnalysis
TB,V_enableAnalysis
dummyAnalysisCount
Ti,V_dummyAnalysisCount
enableFaceCore
TB,V_enableFaceCore
enableDumpYUV
TB,V_enableDumpYUV
burstCoverSelection
T@"NSString",&,V_burstCoverSelection
burstId
T@"NSString",&,V_burstId
bestImageIdentifiersArray
T@"NSArray",&,V_bestImageIdentifiersArray
versionString
T@"NSString",&,V_versionString
Images without faces = %d, threshold = %d, total # = %d
Classified as portrait mode. Affects cover photo selection.
all costs within valid region: 
mean = %f, std = %f
First average cost = %f
Second average cost = %f
--Invalidating two outliers from the start of the burst
--Invalidating one outlier from the start of the burst
Last average cost = %f
Second-to-last average cost = %f
--Invalidating two outliers from the end of the burst
--Invalidating one outlier from the end of the burst
Number of images too few after invalidation at the endpoints. Return one selection.
Result of three-way division: finalCost: %f, inOutRatio: %f
Classified as non-action.
Classified as action.
Between %d and %d: 
motion: %f
Action mean = %f, action std = %f, action threshold = %f
Local statistics for divider %03d
 with score %f:
 noise threshold = %f, high threshold = %f (mean %f, std %f)
Overall mean divider score = %f
clusterDividerArraySize = %d
Locally-maximal divider %d not considered due to being potential noise (%f vs %f,%f)
Locally-maximal divider %d not considered due to lack of any motion: %f
Locally-maximal divider %d not considered due to being potential noise (nearby peak).
local maxima size: %ld
divider %d
Re-running three-way division with minClusterSize = %d, maxClusterSize = %d
Strongest local maxima: %d and %d
Expanding main peak to include divider %d
Adding action-based cluster boundaries.
Cluster %d is too small for action-based cluster boundaries
Action statistics for cluster %d: mean %f std %f threshold %f
Adding ACTION DIVIDER at location %d
***Finding three way division:
firstValidImage = %d, lastValidImage = %d
NEW BEST: largestInnerDistance = %f, bestRatio = %f
Divider1 = %d, Divider2 = %d
RECURSING: (%d->%d) becomes (%d->%d)
Clustering costs: maxInner = %f, inOutRatio = %f
Threshold for dupes: %f
Distance between selections %d and %d: %f, %f
Selection score of %d is %f... isGarbage = %d
Choosing candidate %d from a series of dupes
Throwing away all dupes due to garbage classification
Keeping candidate %d
Tossing out the %s on %d
trash
reject
All images are garbage. Picking the middle selection = %s.
virtual CVML_status vision::mod::ImageDescriptorProcessorAbstract::computeDescriptorsForImages_RGBA8888(const std::vector<vImage_Buffer> &, ImageProcessing_CancellationCheckBlock, vision::mod::ImageDescriptorBufferAbstract &)
%@ must provide an implementation for %@
additional relationships must have at least one child identifier
%@: Revision %lu, %@
SCRR
SCRDL
could not decode additional relationships
%@_%lu(%@)
relationships
The classification identifier '%@' does not exist in the hierarchy
virtual std::vector<std::vector<DetectedObject> > vision::mod::ObjectDetector_DCNFaceDetector_v2::detectObjectsInImages_Planar8(const std::vector<vImage_Buffer> &)
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/ObjectDetector/DCNFaceDetector/ObjectDetector_DCNFaceDetector_v2.mm
virtual std::vector<DetectedObject> vision::mod::ObjectDetector_DCNFaceDetector_v2::detectObjectsInImage_Planar8(const vImage_Buffer &)
virtual std::vector<float> vision::mod::ColorGaborImageDescriptorBuffer::computeSelfDistances() const
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/ImageDescriptor/ImageDescriptorColorGabor/ColorGaborImageDescriptor.h
unable to locate resource bundle
unable to locate resource "%@" of type "%@" in %@
Invalid inputs specified to Espresso Plan Builder
Could not create Espresso context
Could not create Espresso plan
Could not create add network to Espresso plan
Could not build Espresso plan
Unsupported pixel format %lu
Error allocating %lu x %lu CVPixelBuffer with format %lu
espresso buffer image with dimensions %ld x %ld cannot be rendered into a pixel buffer with dimensions %ld x %ld
could not lock pixel buffer
Unsupported espresso buffer storage type (%lu)
espresso buffer image with row bytes size of %ld cannot be rendered into a pixel buffer with %lu bytes per row
Unknown Espresso buffer type
Could not enable Montreal feature in Espresso
Espresso network cannot be nil
Could not bind output buffer to Espresso network
Could not feed-forward buffer data becuase of compatibility of source and destination buffers
+[ImageProcessing_CoreImageUtils newCIImageFromVImage:withType:error:]
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/ImageProcessing/ImageProcessing_CoreImageUtils.m
Unable to create CIImage
+[ImageProcessing_CoreImageUtils newCGImageFromCIImage:error:]
Unable to create CGImage from CIImage
+[ImageProcessing_CoreImageUtils newVImageBufferFromCIImage:error:]
Unable to create vImage_Buffer from CIImage
ERTFaceBox::ERTNumCascadeStages
ERTFaceBox::ERTNumTrees
ERTFaceBox::ERTNumPredictions
ERTFaceBox::ERTNodesThresholds
ERTFaceBox::ERTNodesPredictions
ERTFaceBox::ERTNodesFeatureIDs
ERTFaceBox::ERTNodesLeafFlags
ERTFaceBox::ERTGlobalShift
ERTFaceBox::ERTDefaultPixelValue
ERTFaceBox::ERTDefaultFeatureValue
ERTFaceBox::ERTNumXYPairs
ERTFaceBox::ERTXYPairs
ERROR: ERTDefaultFeatureValue failed to load from ERT model file!
Error: 
 failed to load from ERT model file!
 unexpected size of value
Processing DetectFace3DLandmarks request
VNFaceLandmarkDetectorProcessOption_InputFaceObservations
VNFaceLandmarkDetectorProcessOption_RefineLeftEye
VNFaceLandmarkDetectorProcessOption_RefineRightEye
VNFaceLandmarkDetectorProcessOption_RefineMouth
VNFaceLandmarkDetectorProcessOption_BlinkDetection
VNFaceLandmarkDetectorProcessOption_CascadeStepCount
landmarks_v2
Invalid landmark model resource path
Could not read landmark model data
mouth
righteye
lefteye
memory allocation failure
Landmark Detector did not provide any data
unexpected exception thrown
VN_DEBUG_DUMP_FACE_LANDMARK_2D_INTERMEDIATES
VN_facelandmark2d_debug_intermediates/
_input_intermediate_image.vdump
_input_intermediate_image.png
_input_intermediate_meanShape.png
_output_intermediate_landmarks.png
_info.json
aligned
inputMeanShape
outputLandmarks
VN facelandmark2d debug intermediates written to: %@
Odd image dimensions are not supported
LKT:waitUntilCompleted
could not bind pixel buffer to texture
LKT::Pyramid
The number of scales specified is too large
LKT:ComputeFlow level %d
isValid
TB,R,N,V_isValid
needConversionBGRA2YUVA
TB,N,V_needConversionBGRA2YUVA
Ti,R,N,V_width
Ti,R,N,V_height
nscales
Ti,R,N,V_nscales
streamFrameCount
Ti,R,N,V_streamFrameCount
nwarpings
Ti,N,V_nwarpings
useNonLocalRegularization
TB,N,V_useNonLocalRegularization
nlreg_radius
Ti,N,V_nlreg_radius
nlreg_padding
Ti,N,V_nlreg_padding
nlreg_sigma_l
Tf,N,V_nlreg_sigma_l
nlreg_sigma_c
Tf,N,V_nlreg_sigma_c
nlreg_sigma_w
Tf,N,V_nlreg_sigma_w
lkt_bgra2yuva
lkt_zero_flow
lkt_downscale_2x
lkt_features
lkt_features_derivatives
lkt_solver_prepare_matrices
lkt_solver_box7_y
lkt_solver_box_x_and_Axb
lkt_nlreg_hegbf
junk
junk-descriptor-current
junk-classifier-current
junk-classifier-labels-current
Trying to run junk classifier when the classifier failed to initialize
VN_junk_classifier_debug_intermediates
.json
VN_DEBUG_DUMP_JUNK_INTERMEDIATES
Mapping beyond limit of 2
 VNRequestOptionRectangleDetectorRequiredVersion value is out of bounds: %d
 VNRequestOptionInputRegionOfInterest value malformed: %@
 PixelFocalLength value is out of bounds: %f
 VNRequestOptionRectangleMinimumAspectRatio value is out of bounds: %f
 VNRequestOptionRectangleMaximumAspectRatio value is out of bounds: %f
 VNRequestOptionRectangleMinimumAspectRatio value, %f is greater than VNRequestOptionRectangleMaximumAspectRatio value, %f
 VNRequestOptionRectangleQuadratureTolerance value is out of bounds: %f
 VNRequestOptionRectangleMinimumSize value is out of bounds: %f
 VNRequestOptionRectangleMinimumConfidence value is out of bounds: %f
 VNRequestOptionRectangleMaximumNumber value is out of bounds: %d
VNTorsoprintGeneratorProcessOption_InputFaceObservations
trimmed.pb.espresso
build/torso_input:0
build/single_part_torso_crops/perceptio/h_pool5_flat:0
Memory for torso bouding box is not allocated
Error in calculating torso bounding box dimensions
%@:maxLen=%lu:mthd=%lu
Processing VNImageBlurMetric request
maximumIntermediateSideLength
blurDeterminationMethod
TQ,N,V_maximumIntermediateSideLength
%s.svm
VNFaceDetectorInitOption_MinFaceSize
VNFaceDetectorInitOption_EnableLowMemoryMode
VN_DEBUG_DUMP_FACE_DETECT_INTERMEDIATES
pointCount
TQ,V_pointCount
{CGSize=dd}
points
Tr^,V_points
normalizedPoints
Tr^{CGPoint=dd},R
Can't use abstract base class. Must be VNVaceLandmark2D or 3D
pointsData
T@"NSData",&,V_pointsData
alignedBBox
T{_Geometry2D_rect2D_={_Geometry2D_point2D_=ff}{_Geometry2D_size2D_=ff}},V_alignedBBox
userFacingBBox
T{CGRect={CGPoint=dd}{CGSize=dd}},V_userFacingBBox
Tf,R,V_confidence
allPoints
T@"VNFaceLandmarkRegion2D",R
faceContour
leftEye
rightEye
leftEyebrow
rightEyebrow
nose
noseCrest
medianLine
outerLips
innerLips
leftPupil
rightPupil
T@"VNFaceLandmarkRegion3D",R
zero-dimensioned image (%ld x %ld)
array
%@ is nil
The %@ array has %lu items, which is less than the required count of %lu
The %@ array has %lu items, which is more than the maximum allowed of %lu
expectedAncestoralClass
All elements in the %@ array must be a Class object (%@)
All elements in the %@ array must be a VNRequest subclass (%@)
face observations
%@ %@
cluster IDs
face observation is nil
face observation must have a valid faceprint
%@ was given %@
CVMLObservation_CodingVersionCodingKey
CVMLObservation_ConfidenceCodingKey
CVMLFaceprint_CodingVersionCodingKey
CVMLFaceprint_FaceprintCodingKey
CVMLFaceprint_KeyCodingKey
CVMLFaceprint_ProfileCodingKey
CVMLFaceprint_PlatformCodingKey
CVMLImageprintObservation_ObjectCodingKey
CVMLImageprintObservation_VersionCodingKey
CVMLImageprintObservation_ImageprintTypeCodingKey
CVMLImageprintObservation_ImageprintDescriptorCodingKey
CVMLImageprintObservation_UUIDCodingKey
CVMLImageprintObservation_ImageprintTypeColorGabor
CVMLImageprintObservation_ImageprintDescriptorColorGaborVersion
MPImageDescriptor_externalImageId
MPImageDescriptor_exifTimestamp
MPImageDescriptor_quality
MPImageDescriptor_ColorGaborImageDescriptorBuffer_type
MPImageDescriptor_ColorGaborImageDescriptorBuffer_lengthInBytes
MPImageDescriptor_ColorGaborImageDescriptorBuffer_data
MPImageDescriptor_ColorGaborImageDescriptorBuffer_count
MPImageDescriptor_ColorGaborImageDescriptorBuffer_stride
T@"NSData",&,V_faceprint
T@"NSString",C,V_key
TI,V_platform
TI,V_profile
T@"NSString",C,V_faceprintInputPath
Creating NSKeyedUnarchiver object failed with error: %@
CVMLObservation
MPImageDescriptor
CVMLImageprintObservation
[CVMLObservation initWithData:forKey:] failed with error: %@
ERROR: state cannot be nil
ERROR: invalid image Id format
descriptorId
Tq,R,V_descriptorId
quality
Tf,R,V_quality
colorGaborDescriptor
T^v,R,V_colorGaborDescriptor
sceneClassifierDescriptor
T^v,R,V_sceneClassifierDescriptor
imageRegistrationDescriptor
T^v,R,V_imageRegistrationDescriptor
previousLeafId
Tq,V_previousLeafId
nextLeafId
Tq,V_nextLeafId
nextLeafDescriptorDistance
Tf,V_nextLeafDescriptorDistance
previousLeafDescriptorDistance
Tf,V_previousLeafDescriptorDistance
nextLeafTimestampDistance
Tq,V_nextLeafTimestampDistance
previousLeafTimestampDistance
Tq,V_previousLeafTimestampDistance
nextLeafTotalDistance
Tf,V_nextLeafTotalDistance
previousLeafTotalDistance
Tf,V_previousLeafTotalDistance
rawColorGaborDescriptor
T@"NSData",R,V_rawColorGaborDescriptor
T@"NSString",R,V_imageFilePath
Tp_data
Tp_elementsCount
Tp_length
Tp_labelsAndConfidence
Tp_VNTorsoprint
Tp_algorithmVersion
The model does not have a valid input feature of type image
%@:UUID=%@
model
T@"MLModel",&,V_model
modelType
Ti,V_modelType
inputImageKey
T@"NSString",&,V_inputImageKey
predictedFeatureKey
T@"NSString",&,V_predictedFeatureKey
predictedProbabilitiesKey
T@"NSString",R,V_predictedProbabilitiesKey
boundingBoxOutputDescription
T@"MLObjectBoundingBoxOutputDescription",R,V_boundingBoxOutputDescription
inputImageWidth
TQ,R,V_inputImageWidth
inputImageHeight
TQ,R,V_inputImageHeight
inputImageFormat
TI,R,V_inputImageFormat
No valid VNCoreMLModel found in passed in options
The outputs of the model are of unexpected types.
The confidence scores don't line up with the labls.
T@"VNCoreMLModel",R,V_model
tplTracker_FFT_3324
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/ObjectTracker/temporalEx/tplTrackerFFT.c
(outputIndex >= 0) && (outputIndex < 72)
tplTracker_IFFT_3324
B8@?0
_source_scaled.png
_source_scaled.vdump
VN Image Classifier debug intermediates written to: %@
scalingFactor
augmentationMode
numTiles
imageID
_tile_
.png
.vdump
debugID
MinConfidenceForClassificationRaw
hierarchicalLabelsAndConfidence
MinConfidenceForHierarchical
virtual CVML_status vision::mod::ImageDescriptorAugmenterFlip::augmentImage(const vImage_Buffer &, ImageProcessing_ImageType, const std::vector<vImage_Buffer *> &)
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/ImageDescriptor/ImageDescriptor_AugmenterFlip.h
CVML_status vision::mod::ImageDescriptorAugmenterFlip::flipLR(const vImage_Buffer *, ImageProcessing_ImageType, vImage_Buffer *)
CVML_status vision::mod::ImageDescriptorAugmenterFlip::flipUD(const vImage_Buffer *, ImageProcessing_ImageType, vImage_Buffer *)
virtual CVML_status vision::mod::ImageDescriptorAugmenterFlip::combine(const vision::mod::ImageDescriptorBufferAbstract &, vision::mod::ImageDescriptorBufferAbstract &)
virtual vision::mod::ImageDescriptorAugmenterAbstract::~ImageDescriptorAugmenterAbstract()
CVML_status vision::mod::ImageDescriptorAugmenterAbstract::clearAugmentedImages()
vision::mod::ImageClassifierAbstract &vision::mod::ImageClassifierAbstract::setMinConfidenceRatio(float)
vision::mod::ImageClassifierAbstract &vision::mod::ImageClassifierAbstract::setMinConfidence(float)
vision::mod::ImageClassifierAbstract &vision::mod::ImageClassifierAbstract::setMaxLabels(int)
std::map<std::string, float> vision::mod::ImageClassifierAbstract::classifyDescriptors(const vision::mod::ImageDescriptorBufferAbstract &, bool, bool)
Processing Create imageprint request
Attempt to create an image print request without a timestamp
Attempt to create an imageprint failed
timeStamp
T@"NSNumber",&,N,V_timeStamp
virtual vision::mod::ImageClassifierAbstract &vision::mod::ImageClassifierGlimmer::setDescriptorProcessor(const std::shared_ptr<ImageDescriptorProcessorAbstract> &)
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/ImageClassifier/ImageClassifierGlimmer/ImageClassifier_Glimmer.mm
void vision::mod::ImageClassifierGlimmer::private_t::loadData(void *, size_t, int)
void vision::mod::ImageClassifierGlimmer::private_t::loadClassifier(vision::mod::ImageClassifierGlimmer *, const char *)
void vision::mod::ImageClassifierGlimmer::private_t::loadClassifierBinserializer(vision::mod::ImageClassifierGlimmer *, const char *, const char *)
virtual std::map<std::string, float> vision::mod::ImageClassifierGlimmer::classifyImage_RGBA8888(const vImage_Buffer &)
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/ImageClassifier/ImageClassifierGlimmer/ImageClassifier_Glimmer.h
virtual std::map<std::string, float> vision::mod::ImageClassifierGlimmer::classifyImage_BGRA8888(const vImage_Buffer &)
virtual std::map<std::string, float> vision::mod::ImageClassifierGlimmer::classifyImage_Planar8(const vImage_Buffer &)
FaceRegionMap::Width
FaceRegionMap::Height
FaceRegionMap::Data
FaceRegionMap::Triangles
FaceRegionMap::NormalizedLandmarks
void vision::mod::FaceRegionMap::init(const vision::mod::ModelValues &)
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/FaceRegionMap/FaceRegionMap.cpp
Background
Left eye
Right eye
Left eyebrow
Right eyebrow
Root of nose
Nose
Chin
Lower left cheek
Lower right cheek
Between mouth and nose
Left cheek
Right cheek
Left temple
Right temple
Between eyebrows
Above left eye
Above right eye
Upper lip
Lower lip
Between lips
Forehead
Tip of nose
CVML_status vision::mod::FaceRegionMap::computeFaceRegionMap(const Geometry2D_rect2D, const std::vector<Geometry2D_point2D> &, vImage_Buffer &)
AdjacentContourHeal
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/RectangleDetector/QuadDetect/Contours.c
PCOORD_EQUAL(joinPoint, LAST_PCOORD(cPtrN))
PCOORD_EQUAL(joinPoint, FIRST_PCOORD(cPtrN))
healCenters
ady == 2
heal
(endpoint2 == 4)|| (endpoint2 == 8)
(endpoint2 == 8)|| (endpoint2 == 6)
(endpoint2 == 6)|| (endpoint2 == 2)
(endpoint2 == 2)|| (endpoint2 == 4)
testJoin
(orn >= 0) && (orn <= 8)
orn != 4
straightLineLSQ
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/RectangleDetector/QuadDetect/Leq.c
maxDev != -1.f
straightLineWLSQ
VNDetectedObjectObservation object is expected to initialize Object Tracker
Object identifier is not initialized in detected object observation
CVML_status FaceWarper_estimateEyesRST(const Geometry2D_cart2D *, const Geometry2D_point2D *, const Geometry2D_size2D *, _Bool, Geometry2D_RST *)
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/FaceWarper/FaceWarper_Mesh.c
CVML_status FaceWarper_estimateAnchorsRST(const Geometry2D_cart2D *, const int *, int, const Geometry2D_point2D *, const Geometry2D_size2D *, _Bool, Geometry2D_RST *)
allocSegments
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/RectangleDetector/QuadDetect/Segments.c
sdb->nSegments <= sdb->maxSegments
ThumbnailCluster - adding %s
[BurstThumbnailCluster initWithImageData] : metadata parsing error
[BurstThumbnailCluster initWithImageData] : no error
burstImages
T@"NSMutableArray",&,V_burstImages
imageProps
T@"NSMutableDictionary",&,V_imageProps
completionBlock
T@?,C,V_completionBlock
imagePixelBuffer
T^{__CVBuffer=},V_imagePixelBuffer
baseAddress
Tr^v,R,N
resourcePath
T@"NSString",&,N
void cvml::util::mapped_model_file::advise(int) const
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/VisionKitFramework/VN/algorithm_util/mapped_model_file.h
mmap MAP_FAILED
cvml::util::mapped_model_file::mapped_model_file(const char *, bool)
void cvml::util::mapped_model_file::open_file(const char *)
VNMPImageDescriptor_exifTimestamp
VNMPImageDescriptor_quality
VNMPImageDescriptor_ColorGaborImageDescriptorBuffer_type
VNMPImageDescriptor_ColorGaborImageDescriptorBuffer_lengthInBytes
VNMPImageDescriptor_ColorGaborImageDescriptorBuffer_data
VNMPImageDescriptor_ColorGaborImageDescriptorBuffer_count
VNMPImageDescriptor_ColorGaborImageDescriptorBuffer_stride
ERROR: Could not compute the image descriptor
ERROR: Could not compute the convnet-based image descriptor
ERROR: Could not compute image registration features
ERROR: Could not compute image quality
MPImageDescriptor cannot be serialized without being created
Failed to initialize VNMPImageDescriptor object
state parameter cannot be nil
Invalid state format
-[VNMPImageDescriptor computeDescriptorForImageData:context:error:]
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/MomentProcessor/Moments/MPImageDescriptor.mm
-[VNMPImageDescriptor computeQualityForImageData:andQualityCriteria:context:error:]
ERROR: image data property is not initialized
ERROR: all ranking criteria failed
T@"NSData",R
Tq,R,V_internalNonSerializedDescriptorId
Unexpected number of sceneprints calculated
video analyzer did provided %lu results
%@:%@:tmStmp=%@:frmRt=%@
supportedFrameRates
frameTimeStamp
T@"NSNumber",&,N,V_frameTimeStamp
T@"VNSequenceSummarizationObservation",&,N,V_inputObservation
Processing DetectFaceRectangles1 request
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/FaceFrontalizer/FaceFrontalizer.cpp
CVML_status vision::mod::FaceFrontalizer::frontalize_Planar8(const vImage_Buffer &, const Geometry2D_rect2D &, vImage_Buffer &)
CVML_status vision::mod::FaceFrontalizer::frontalize_RGBA8888(const vImage_Buffer &, const Geometry2D_rect2D &, vImage_Buffer &)
v24@?0@"NSString"8@"NSError"16
operation timeout
burst frame '%@' failed to be added
T@?,C,N
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/ImageProcessing/ImageProcessing_Smoothing.c
CVML_status ImageProcessing_smoothGaussian_createKernelForPlanarF(float, float, float **, int *)
CVML_status ImageProcessing_smoothGaussian_anisotropic_PlanarF(const vImage_Buffer *, const vImage_Buffer *, void **, float, float, float, Pixel_F, vImage_Flags)
RAMBackingStore::createFromContentsOfFile -- file does not exist '%s'
RAMBackingStore::createFromContentsOfFile -- Could not open file '%s'
RAMBackingStore::createFromContentsOfFile -- error seeking in provided file
RAMBackingStore::createFromContentsOfFile -- failed to allocate buffer
RAMBackingStore::createFromContentsOfFile -- failed to read contents of file '%s'
RAMBackingStore::createFromContentsOfFile -- failed to close file
RAMBackingStore::growStorage -- could not grow storage
%@%s
writeBackingStoreToFile-- Could not create file: '%s'
writeBackingStoreToFile-- error writing out data
writeBackingStoreToFile-- error closing file, file could be corrupt
writeBackingStoreToFile -- File exists at output path '%s', and we cannot safely overwrite this file
writeBackingStoreToFile-- File exists at output path '%s', and could not copy file to temporary directory
writeBackingStoreToFile-- Could not move file from temporary directory
writeBackingStoreToFile -- Non critical error -- Could not remove original file after rename
writeBackingStoreToFile-- Error closing file
writeBackingStoreToFile -- could not remove temporary file.
writeBackingStoreToFile-- Catastrophic error.  Original file damaged during save.
VNFaceprintGeneratorType
VNFaceprintGeneratorTypeEspressoCPU
VNFaceprintGeneratorProcessOption_InputFaceObservations
VNFaceprintGeneratorProcessingOption_CheckForJunkFaces
Descriptor type not specified!
Unsupported descriptor type
faceDescriptor-current
049c_chk-0300.espresso
Could not find face descriptor model resource!
Could not create new face observation instance
Faceprint input is not expected
Could not create memory efficient crop for face printing
Preprocessing of data for faceprinting failed
Poor quality face print candidate detected.  Not generating faceprint
Could not compute face descriptor due to internal error
VN_DEBUG_DUMP_FACEPRINTER_INTERMEDIATES
VN_faceprinter_debug_intermediates/
_pre_frontalized.vdump
_frontalized.vdump
_frontalized.png
_pre_frontalized.png
_bbox.json
_bbox_crop.png
VN Faceprinter debug intermediate written to: %@
TQ,R,N,V_length
useLowPriorityMode
TB,R,N,V_useLowPriorityMode
%@:%u
returnAllResults
TB,N,V_returnAllResults
VNImageAnalyzerMultiDetectorInitializationOptionModel
VNImageAnalyzerMultiDetectorInitializationOptionPreferLowerGPUMemoryUtilization
VNImageAnalyzerMultiDetectorProcessingOptionSkipInputImageScaling
VNImageAnalyzerMultiDetectorProcessingOptionCreateSceneprint
VNImageAnalyzerMultiDetectorProcessingOptionSceneprintRequestRevision
VNImageAnalyzerMultiDetectorProcessingOptionSceneprintIncludeLabelsAndConfidences
VNImageAnalyzerMultiDetectorProcessingOptionClassifyScene
VNImageAnalyzerMultiDetectorProcessingOptionSceneRequestRevision
VNImageAnalyzerMultiDetectorProcessingOptionSceneMaximumLeafLabels
VNImageAnalyzerMultiDetectorProcessingOptionSceneMaximumHierarchicalLabels
VNImageAnalyzerMultiDetectorProcessingOptionSceneMinimumConfidence
VNImageAnalyzerMultiDetectorProcessingOptionSceneClassificationCustomHierarchy
VNImageAnalyzerMultiDetectorProcessingOptionClassifyAesthetics
VNImageAnalyzerMultiDetectorProcessingOptionAestheticsRequestRevision
VNImageAnalyzerMultiDetectorProcessingOptionGenerateSaliencyHeatMap
VNImageAnalyzerMultiDetectorProcessingOptionSaliencyRequestRevision
failed to create saliency heat map image
scenenet_sc2.4_sa1.4_ae1.4_r9_opt_int8.espresso
scenenet_aesthetic_labels_basic-v8d
aesthetics/scores
inner/sceneprint
scenenet_labels_basic-v8d.csv
aesthetics/attributes
classification/labels
saliency/map
smartcam_sc2.0_sa1.0_r2_opt_fp16.espresso
inner/smartcamprint
smartcam_labels_basic-v3b.txt
failed to create image analyzer
scenenet_relationships-v8d.txt
smartcam_relationships-v3b.txt
a hierarchical model for detector model %lu is not supported
ImageAnalyzer_Tensor1D<float> vision::mod::ImageAnalyzer::getSceneRepresentation()
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/ImageAnalyzer/ImageAnalyzer.h
vision::mod::ImageAnalyzer_Tensor2D vision::mod::ImageAnalyzer::getSceneSaliency()
vision::mod::ImageAnalyzer_Tensor2D::ImageAnalyzer_Tensor2D(const espresso_buffer_t &)
vImage_Buffer vision::mod::ImageAnalyzer_Tensor2D::getVImageBufferFromTensor(ImageProcessing_ImageType &) const
CVML_status vision::mod::getImageTypeAndBytesPerPixelFromEspressoBuffer(const espresso_buffer_t &, ImageProcessing_ImageType &, size_t &)
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/Clustering/Clustering/GreedyWithTorso/GreedyWithTorsoClustering_hacks.cpp
ageClassifier_W
static std::shared_ptr<GreedyClustererWithTorsoHacks> vision::mod::GreedyClustererWithTorsoHacks::createGreedyClustererWithTorsoHacks(const std::string, bool, bool, float, float)
ageClassifier_b
float vision::mod::GreedyClustererWithTorsoHacks::thresholdScaleForDescriptor(float *, int)
CVML_status ctrTrackerComputation_trackNewFrame(CVPixelBufferRef, ctrTracker_context *, CGPoint *, _Bool *, _Bool *, float *)
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/ObjectTracker/correlationTracker/ctrTrackerTrack.c
CVML_status ctrTrackerComputation_updateHistory(CVPixelBufferRef, ctrTracker_context *, CGPoint *, _Bool *, float *)
initializeCannyEdgeContext
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/CannyEdge/cannyEdge.c
context->blockAddress
thresholdAndConnectCandidateEdges
context->edgeStackSize <= context->width*context->height
smartcam_onlyfc
Processing Create FaceRegionMap request
floatVectorSumProd
T^f,VfloatVectorSumProd
pulseVectorHeightCharBox
T*,VpulseVectorHeightCharBox
pulseVectorHeightCharBoxAdaptive
T*,VpulseVectorHeightCharBoxAdaptive
charBoxFlags
T^Q,VcharBoxFlags
charboxROIFullVectorRowStart
T^S,VcharboxROIFullVectorRowStart
charboxROIFullVectorHeight2
T^S,VcharboxROIFullVectorHeight2
allocationSize
TI,VallocationSize
mTop
Tf,VmTop
mBottom
Tf,VmBottom
bTop
Tf,VbTop
bBottom
Tf,VbBottom
posUL
Tf,VposUL
posLL
Tf,VposLL
posUR
Tf,VposUR
posLR
Tf,VposLR
medianHeightTop
TS,VmedianHeightTop
medianHeightBottom
TS,VmedianHeightBottom
loopBigBox
Ts,VloopBigBox
loopBigBoxPrev
Ts,VloopBigBoxPrev
filterWalkUpDownCount
TS,VfilterWalkUpDownCount
CCTextDetector_EnableDebug
CCTextDetector_DebugPathname
CCTextDetector_RequestRevision
creditCardSubsampleImage.png
votingImage.png
inverseVotingImage.png
/var/mobile/Media/DCIM/ccOutDebug/
textOutFirstPassImage.png
textOutSecondPassImage.png
adaptiveOutImage.png
selectedTextOutImageArray.png
uOutImage.png
textBoxRevised
textBoxRevisedNormalized
stubBox
stubBoxNormalized
{{%i,%i},{%i,%i}}
{{%2.4f,%2.4f},{%2.4f,%2.4f}}
textBox
textBoxMM
charBox
charBoxMM
charConfidence
{{1,1},{1,1}}
connectedComponents.png
stubBoxMM
CCTextDetector internal error
q24@?0@"VNTextObservation"8@"VNTextObservation"16
computeZCVectorHighProbability
TB,V_computeZCVectorHighProbability
midRow
Ti,V_midRow
minHeight
TI,V_minHeight
maxHeight
TI,V_maxHeight
startMaxFind
TI,V_startMaxFind
stopMaxFind
TI,V_stopMaxFind
mmHeightCard
Tf,V_mmHeightCard
mmWidthCard
Tf,V_mmWidthCard
pixelHeightCard
TI,V_pixelHeightCard
pixelWidthCard
TI,V_pixelWidthCard
minBoxWidth
TI,V_minBoxWidth
maxBoxWidth
TI,V_maxBoxWidth
startNormal
TI,V_startNormal
stopNormal
TI,V_stopNormal
startSensitized
TI,V_startSensitized
stopSensitized
TI,V_stopSensitized
charBoxContext
T@"CCCharBoxContext",&,V_charBoxContext
TC,V_ii
profileNormal
TC,V_profileNormal
debugMatlab
TB,V_debugMatlab
debugOut
TB,V_debugOut
debugFilename
T@"NSString",C,V_debugFilename
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/LandmarkDetector/LandmarkDetector_DNN.cpp
void vision::mod::LandmarkDetectorDNN::initBuffers()
CVML_status vision::mod::LandmarkDetectorDNN::preProcessImage(const vImage_Buffer &, ImageProcessing_ImageType, const Geometry2D_rect2D &, float &, float &, vision::mod::LandmarkDetectorDNN_ExceedingFaceBoundingBoxMode)
std::vector<Geometry2D_point2D> vision::mod::LandmarkDetectorDNN::detectLandmarks(const vImage_Buffer &, ImageProcessing_ImageType, const Geometry2D_rect2D &, vision::mod::LandmarkDetectorDNN_ExceedingFaceBoundingBoxMode)
BOOL faceWarperComputeAnchorTransform(NSData *__strong, CGAffineTransform *)
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/VisionKitFramework/VN/internal/VNFaceWarper.mm
VNObservationsCache
%@ did not provide a valid Espresso model file name
%@ did not provide a valid Espresso model input image dimensions blob name
could not obtain the dimensions of "%@"
espressoContext
T^v,R,N,V_espressoContext
espressoPlan
T^v,R,N,V_espressoPlan
espressoNetwork
T{?=^vi},R,N,V_espressoNetwork
networkRequiredInputImageWidth
TQ,R,N,V_networkRequiredInputImageWidth
networkRequiredInputImageHeight
TQ,R,N,V_networkRequiredInputImageHeight
%@:V%lu:AR%g-%g:QTol=%g:%g:%g:n=%lu
requiredVersion
TQ,N,SsetRequiredVersion:,V_requiredVersion
minimumAspectRatio
maximumAspectRatio
quadratureTolerance
minimumSize
minimumConfidence
maximumObservations
face model data is unavailable
%@:%u%u
%@ does not implement VNCreateSceneprintRequestRevision2
useCenterTileOnly
TB,N,V_useCenterTileOnly
The requests parameter must be an array of VNRequests
q24@?0@"VNRequest"8@"VNRequest"16
request classes
requests
Processing IdentifyJunk request
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/Clustering/Clustering/Greedy/GreedyClustering.cpp
std::shared_ptr<std::vector<size_t> > vision::mod::GreedyClusterer::addDescriptors(float *, int, int, size_t, const std::vector<float> &, const std::vector<int> &)
addDescriptors
elementSize == ELEMENT_SIZE
std::shared_ptr<std::vector<size_t> > vision::mod::GreedyClusterer::addDescriptors(const vision::mod::ImageDescriptorBufferAbstract &, const std::vector<float> &, const std::vector<int> &)
  GreedyClusterer::serializeStatus - enter
  GreedyClusterer: Failed to open file - skipping serialization
  GreedyClusterer::serializeStatus - saving map file: %s
  GreedyClusterer: failed to save the file
virtual bool vision::mod::GreedyClusterer::serializeStatus(int) const
  GreedyClusterer::serializeStatus - done
  GreedyClusterer::deserializeStatus - failed to load memory map file
  GreedyClusterer::deserializeStatus - loaded a corrupt file as expected element size does not match
CVML_status vision::mod::GreedyClustererFaces::updateInternal(const vision::mod::ImageDescriptorBufferAbstract &, const std::vector<float> &, const std::vector<int> &, std::vector<faceIdPair> &)
void vision::mod::GreedyClustererFaces::getAverageDescriptorOfClusterContainingFace(vision::mod::ImageDescriptorId, std::map<ImageDescriptorId, std::vector<ImageDescriptorId> > &, float *) const
  GreedyClustererFaces::serialize - enter
  GreedyClustererFaces::serialize - cachefile: %s
  GreedyClustererFaces::serialize - error creating new map file for serialization
  GreedyClustererFaces::serialize - error calculating checksum for cluster data file
  GreedyClustererFaces: Failed to open '%s': errno=%d
  GreedyClustererFaces: Opening '%s'
virtual void vision::mod::GreedyClustererFaces::getIdsForCluster(vision::mod::ImageDescriptorId, std::vector<ImageDescriptorId> &, std::unordered_multimap<ImageDescriptorId, ImageDescriptorId> *) const
void vision::mod::GreedyClusterer::private_t::throwOnCancellation()
IpObs%lu=%@
%@:%@:%f
Processing Group Images By Time and Content Request
inputImageprints
T@"NSArray",C,N,V_inputImageprints
clusteringDistanceThreshold
Tf,N,V_clusteringDistanceThreshold
%@:[%@]
Processing Align Face Rectangle request
Input faces not provided to face rectangle aligner
T@"NSArray",R,C,N,V_inputFaceObservations
T@"NSString",&,N,V_type
cachePath
T@"NSString",&,N,V_cachePath
state
T@"NSData",&,N,V_state
Tf,N,V_threshold
torsoThreshold
Tf,N,V_torsoThreshold
options parameter cannot be nil
%@:imageCropAndScaleOption=%lu:Model=%@
Failed to initialize VNCoreMLTransformer
The VNCoreMLTransform request failed
T@"VNCoreMLModel",R,N,V_model
imageCropAndScaleOption
TQ,N,V_imageCropAndScaleOption
debugMode
Ti,V_debugMode
timerMode
Ti,V_timerMode
clusterSplitDistanceType
Ti,V_clusterSplitDistanceType
qualityCriteriaList
T@"NSArray",&,V_qualityCriteriaList
useTimestampAdjustedDistances
TB,V_useTimestampAdjustedDistances
performClustersPostprocessing
TB,V_performClustersPostprocessing
performSceneClassification
TB,V_performSceneClassification
roiAreaThreshold
Tf,V_roiAreaThreshold
inliersRatioThreshold
Tf,V_inliersRatioThreshold
numberOfKeypointsToConsider
Ti,V_numberOfKeypointsToConsider
naturalClusteringDistanceThreshold
Tf,V_naturalClusteringDistanceThreshold
splitIntoMonotonicSpans
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/RectangleDetector/QuadDetect/SegmentUtilities.c
cPtr->nPnts > 1
!closedP
idx <= monoLength
mergeSegment
sPtr2 != NULL
T{CGRect={CGPoint=dd}{CGSize=dd}},N,V_box
defaultBox
T{CGRect={CGPoint=dd}{CGSize=dd}},N,V_defaultBox
scale
Ti,N,V_scale
Ti,N,V_mergesCount
Tf,N,V_rotationAngle
Tf,N,V_yawAngle
boxCenter
T{CGPoint=dd},R,N
distanceToDefaultBox
vision::mod::BoostedClassifierLight::BoostedClassifierLight(std::string)
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/Classifiers/BoostedClassifierLight/BoostedClassifierLight.cpp
THRESHOLDS
WEIGHTS
CVML_status vision::mod::BoostedClassifierLight::loadClassifierFromFile(std::string)
bool vision::mod::faceIsJunk(vision::mod::ImageDescriptorBufferAbstract &, size_t)
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/Clustering/Clustering/Greedy/GreedyClustering_hacks.cpp
/all_3lp_nodropout-symbol.espresso.bin
void vision::mod::readBinSerializedModelValues(const char *const, const char *, const vision::mod::BinSerializedModelFileInfo &, vision::mod::ModelValues &, bool)
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/CVML/CVML_BinSerializedModelReader.cpp
void vision::mod::readBinSerializedModelValues(FILE *, const char *, const vision::mod::BinSerializedModelFileInfo &, bool, vision::mod::ModelValues &)
void vision::mod::Face3D::init(const vision::mod::ModelValues &, const float *)
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/Face3D/Face3D.cpp
CVML_status vision::mod::Face3D::estimatePoseAndStructure(const std::vector<Geometry2D_point2D> &, Geometry3D_pose &, std::vector<Geometry3D_point3D> &, int)
CVML_status vision::mod::Face3D::estimatePose(const std::vector<Geometry2D_point2D> &, Geometry3D_pose &)
CVML_status Projections_computeShiftBruteForce(const float *, int, const Projections_meanStdTable *, const float *, int, const Projections_meanStdTable *, int, float, float *, float *, float *, float *)
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/ImageRegistration/Projections/Projections_Optimizer.c
CVML_status Projections_computeCost(int, float, float, const float *, int, const Projections_meanStdTable *, const float *, int, const Projections_meanStdTable *, int, float *)
convertYUV420ToRGBA8888: invalid dst size of %lu x %lu
convertYUV420ToRGBA8888: failed to allocate %lu bytes
public.png
com.microsoft.bmp
public.jpeg
convertYUV420ToRGBA8888: src must be YUV420 format!
Processing DetectFaceExpressions request
scorPdiffParameters
void vision::mod::LandmarkAttributes::init(const vision::mod::ModelValues &, bool)
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/LandmarkDetector/LandmarkDetector_Attributes.mm
exprParameters
blinkParametersApp
smileBlinkParametersGeo
lmarkQuality
exprParamsv1
pupilMeanStd
vision::mod::LandmarkAttributes::LandmarkAttributes(const char *, bool)
float vision::mod::LandmarkAttributes::computeFittingScoreIntensityDifference(const vImage_Buffer &, const Geometry2D_rect2D &, const std::vector<Geometry2D_point2D> &)
std::map<expressionAttributeType, float> vision::mod::LandmarkAttributes::computeExpressionAttributes(const Geometry2D_rect2D &, const std::vector<Geometry2D_point2D> &)
int vision::mod::LandmarkAttributes::computeBlinkFunction(const vImage_Buffer &, const Geometry2D_rect2D &, const std::vector<Geometry2D_point2D> &, vImage_Buffer &, vImage_Buffer &, std::vector<float> &, std::vector<float> &)
std::map<blinkType, float> vision::mod::LandmarkAttributes::computeBlinkAttributes(const Geometry2D_rect2D &, const std::vector<Geometry2D_point2D> &, std::vector<float> &)
Processing Create Faceprint request
Cannot create 'print' object
solo_landmarks_s9min6ugm8_opt.espresso
Espresso reosurces are not initialized
Internall error allocating Landmarks detector
Unexpected error allocating Landmarks detector
Internal error whilem processing Landmarks detector
ctrTracker_context *ctrTrackerInitialization_allocContext()
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/ObjectTracker/correlationTracker/ctrTrackerInitialization.c
CVML_status ctrTrackerInitialization_setUpTracker(CVPixelBufferRef, ctrTracker_context *, CGPoint *)
VNFaceBBoxAlignerProcessOption_InputFaceObservations
faceBoxPoseAligner-current
Could not read face box aligner model
Could not map face box aligner model
VN_facealigner_debug_intermediates/
_aligner_image.vdump
_aligner_image.png
_bboxes.json
_aligned_bbox_crop.png
_meanShape.png
VN aligner debug intermediates written to: %@
VNAlignBBox recieved a zero dimensioned image
Invalid face bounds supplied to face aligner
Could not create memory efficient crop for bbox alignment
Error aligning face bounds.  Bounds are likely out of bounds
VN_DEBUG_DUMP_FACE_ALIGNER_INTERMEDIATES
VNTrackingOption_ProcessingQueue
VNTrackingOption_TrackerKey
VNTrackingOption_TrackerType
VNTrackingOption_TrackingLevel
VNTrackingOption_InputBBox
VNTrackingOption_RequestRevision
Internal error: Tracker is busy with previous tracking requests. It needs to be reset to restart tracking sequence
Internal error: Conversion to Tracker coordinate system failed
Internal error: No frame to track objects was passed to the tracker
Internal error: Setting objects to track failed with error: %llu
Internal error: Tracking objects failed with error: %llu
Internal error: failed to initialize object IDs to rectangles dictionary
trackerObservationClass
isTracking
trackedFrameNumber
Tq,V_trackedFrameNumber
lastTrackedBBox
T{CGRect={CGPoint=dd}{CGSize=dd}},V_lastTrackedBBox
T@"NSUUID",R,V_key
level
T@"NSString",R,V_level
computeMag2d
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/HumanDetector/cv/gradient.cpp
Nrow>1 && Ncol>1
-[ShotflowNetwork initWithModelPath:espressoEngineID:espressoDeviceID:espressoStorageType:threshold:]
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/ObjectDetector/Shotflow/ShotflowNetwork.mm
logits_%ld
offsets_%ld
logits_roll_%ld
logits_yaw_%ld
-[ShotflowNetwork setInputShape:height:]
-[ShotflowNetwork runNetwork:inputIsBGR:]
-[ShotflowNetwork processVImage:inputIsBGR:]
-[ShotflowNetwork resizeAndProcessVImage:inputIsBGR:]
-[ShotflowNetwork processCIImage:]
preferredSmallSide
Processing VNImageBrightnessMetric request
XYPAIRS
CVML_status vision::mod::FaceClassifier_BoostedPixelDifference::loadModelFromFile(std::string)
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/Classifiers/FaceClassifier_BoostedPixelDifference/FaceClassifier_BoostedPixelDifference.cpp
VNSmartCamCombinedAestheticsAndSaliencyDetectorOptionClassifyAesthetics
VNSmartCamCombinedAestheticsAndSaliencyDetectorOptionGenerateSaliencyHeatMap
Could not bind output aesthetics scores
Could not bind output aesthetics attributes
Could not bind image to Espresso network
Could not run network
combined_classification_smartCamfanet_0-0011_saliency_bhutc68tnd_aesthetics_3eqm2xn28k.espresso
No aesthetic score data returned
Unexpected aesthetic score input types provided
No attribute data returned
Unexpected attribute scores input types provided
Error allocating VNImageAestheticsObservation
Unexpected espresso result
Failed to create observation
+N9mZUAHooNvMiQnjeTJ8g
   +-- %@
originalRequests
T@"NSArray",R,C,N,V_originalRequests
failed to allocate the vector result buffer (CVReturn=%ld)
unexpected optical flow estimation failure
optical flow cannot be performed on images with different dimensions
no request performer available
no image is available
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/ImageDescriptor/ImageDescriptorPixelPairDifference/ImageDescriptor_PixelPairDifference.cpp
CVML_status vision::mod::ImageDescriptor_PixelPairDifferenceProcessor::loadPixelPairXYList(std::vector<float>)
CVML_status vision::mod::ImageDescriptor_PixelPairDifferenceProcessor::computeDescriptorForImage_Planar8(const vImage_Buffer &, float *)
virtual CVML_status vision::mod::ImageDescriptor_PixelPairDifferenceProcessor::computeDescriptorForImage_RGBA8888(const vImage_Buffer &, vision::mod::ImageDescriptorBufferAbstract &)
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/ImageDescriptor/ImageDescriptorPixelPairDifference/ImageDescriptor_PixelPairDifference.hpp
virtual CVML_status vision::mod::ImageDescriptor_PixelPairDifferenceProcessor::computeDescriptorForImages_RGBA8888(const std::vector<vImage_Buffer> &, vision::mod::ImageDescriptorBufferAbstract &)
virtual CVML_status vision::mod::ImageDescriptor_PixelPairDifferenceProcessor::computeDescriptorForImages_Planar8(const std::vector<vImage_Buffer> &, vision::mod::ImageDescriptorBufferAbstract &)
computeACF
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/HumanDetector/cv/ChnsFeature.cpp
img.channels==3
expFeatSz[0]==featSz[0]
expFeatSz[1]==featSz[1]
expFeatSz[0]==histSz[0]
expFeatSz[1]==histSz[1]
nil buffer passed into initWithImageBuffer
Error while trying to allocate VNImageRegistrationSignature object
rowProjections
rowSum
rowSumSq
colProjections
colSum
colSumSq
inconsistent row data
inconsistent column data
The model has reached the maximum identity limit of %lu
identity serial numbers have been exhausted
delegate
T@"<VNPersonsModelDataDelegate>",W,N,V_delegate
T@"NSDate",R,N
VNFaceLandmarksSegmenterProcessOption_InputFaceObservations
Invalid parameter (size)
Unexpected request revision
Invalid parameter (numberOfSupportedFaceLandmarkSegments)
espresso_eyebrows.espresso
One of the dimensions of the input face image is zero
Input face aspect ratio > %f cannot be processed
input:0
Unexpected dimensions for input Espresso buffer
BiasAdd_16:0
Could not bind output buffer to Espresso
Could execute Espresso plan
VNImageprintGeneratorProcessOption_Timestamp
CVML_status ImageProcessing_isROIInsideImage(const Geometry2D_rect2D *, const vImage_Buffer *, _Bool *)
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/ImageProcessing/ImageProcessing_Crop.c
vImage_Buffer ImageProcessing_getBufferFromNaturalROI(const vImage_Buffer *, const Geometry2D_rect2D *, size_t)
vImage_Buffer ImageProcessing_getBufferFromCoreImageROI(const vImage_Buffer *, const Geometry2D_rect2D *, size_t)
CVML_status ImageProcessing_deepCopyBufferFromNaturalROI(const vImage_Buffer *, const Geometry2D_rect2D *, ImageProcessing_ImageType, vImage_Buffer *)
CVML_status ImageProcessing_deepCopyBufferFromCoreImageROI(const vImage_Buffer *, const Geometry2D_rect2D *, ImageProcessing_ImageType, vImage_Buffer *)
no valid initial image specifier was provided
projectionRows_planar8UtoF
projectionCols_planar8UtoF
CVML_status Projections_projectionRowsCols_planar8UtoF(const uint8_t *, int, int, size_t, float *, float *)
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/ImageRegistration/Projections/Projections_Core.c
CVML_status Projections_computeProjectionDerivative(const float *, int, float *)
fastSlidingAdaBoostPred
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/HumanDetector/humandetector/TemplateObjectDetectorApply.cpp
slidingSVMPred
svm.W.size() == svmSz[0]*svmSz[1]*svmSz[2]
Corr3d
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-2.0.49/HumanDetector/cv/CVCommon.h
featSz.size() == 3
templateSz.size() == 3
Template.size() == templateSz[0] * templateSz[1] * templateSz[2]
templateSz[0] <= featSz[0]
templateSz[1] <= featSz[1]
templateSz[2] == featSz[2]
rowcorr
lenResult==lenA-lenB+1
GeomTransform_constructor: unknown transform model (%d)
GeomTransform_minSupportPoints: unknown transform model (%d), reset to RIGID
GeomTransform_changeCoordinateSystem failed
GeomTransform_setModel: unknown new model (%d) use the old model (%d)
GeomTransform_estimate: unknown transform model (%d) use RIGID
GeomTransform_numTestsToDo: unknown transform model (%d) use RIGID
RigidTransform_estimate: not symmetric positive definite matrix
RigidTransform_estimate: the %ld-th argument is wrong in sposv_ call
AffineTransform_estimate: not symmetric positive definite matrix
AffineTransform_estimate: the %ld-th argument is wrong in sposv_ call
HomographyTransform_estimate:The %ld-th eigenvector failed to converge
HomographyTransform_estimate:The %ld-th eigenvector failed to converge
HomographyTransform_estimate: the %ld-th argument is wrong in ssyevx_ call
HomographyTransform_estimate: the %ld-th argument is wrong in ssyevx_ call
HomographyTransform_estimate: the 9-th parameter is too small pp[8]=%f 
IPDetector_constructor: Cannot allocate mFltImage 
IPDetector_constructor: Cannot allocate mTmpBuffer 
IPDetector_constructor: Cannot allocate mCornerVec 
IPDetector_constructor: Cannot allocate mBX, mBY 
IPDetector_response: box filter failed
v16@?0i8i12
histogram equalization queue
boxFilter_uint8_init: box filter failed when request minimum size err=%d
boxFilter_uint8: box filter failed err=%d
 invMatrix failed INFO1 = %ld
 invMatrix failed INFO2 = %ld
%s : -[EAGLContext setParameter:...] failed with error %d
gl_UtilsCreateContext
%s : calloc failed
ImageRegistrationCreateContext
%s : CFDictionaryCreateMutable failed
%s : CFArrayCreateMutable failed
%s : HistEqCreateContext failed
%s : RegistrationEngine_constructor failed
imageRegQueue
%s : dispatch_queue_create failed
%s : NULL input parameters
ImageRegistrationUnregisterOneImage
ImageRegister
%s : Need at least one non-reference image
%s : CFArrayGetCount(imageRegCtx->availExtraBuffers) != 0
%s : Could not locate scratch buffers
%s : CPU histogram equalization failed
HistogramEqualization
%s : invalid histogram equalization method
HistEqualizeCPU
%s : Unsupported image width,height
%s : Couldn't lock output buffer
%s : Couldn't lock input buffer
%s : CFArrayGetCount(imageRegCtx->availExtraBuffers) returned %d
GetUnusedExtraBuffer
GeomTransform_estimate failed
Pyramid_loadImage: incompatible size in pyramid (%lu!=%lu) or (%lu!=%lu)
v40@?0^Q8^Q16Q24Q32
WARNING: insufficient number of external corners provided (only %hu corners provided but minumum is %d)
Registration could not detect more that %d inlier corners at the highest resolution.
position
%s SHADER COMPILATION FAILED
VERTEX
FRAGMENT
<unknown>
Shader compile log:
<OOM with failed shader compile log>
PROGRAM LINK FAILED
Program link log:
<OOM with failed program link log>
%s : GL program failed to compile/link
HistEqCreateContext
source
textureScale
textureBias
attribute vec2 position;
varying vec2 texcoord;
void main()
  texcoord = 0.5 * (position + 1.0);
  gl_Position = vec4(position.x, position.y, 0.0, 1.0);
precision mediump float;
uniform sampler2D source;
uniform sampler2D lut;
uniform float textureScale;
uniform float textureBias;
varying highp vec2 texcoord;
void main()
  vec4 texColor = texture2D(source, texcoord);
  texColor = textureScale * texColor + textureBias;
  gl_FragColor.xyzw = vec4(texture2D(lut, vec2(texColor.x, 0.0)).x,
                           texture2D(lut, vec2(texColor.y, 0.0)).x,
                           texture2D(lut, vec2(texColor.z, 0.0)).x,
                           texture2D(lut, vec2(texColor.w, 0.0)).x);
__cpu_indicator_init
/BuildRoot/Library/Caches/com.apple.xbs/Sources/clang/clang-1000.2.21.14/src/projects/compiler-rt/lib/builtins/cpu_model.c
__cpu_model.__cpu_type < CPU_TYPE_MAX
__cpu_model.__cpu_subtype < CPU_SUBTYPE_MAX
stringWithUTF8String:
stringWithFormat:
mainBundle
localizedStringForKey:value:table:
dictionaryWithObjects:forKeys:count:
errorWithDomain:code:userInfo:
exceptionWithName:reason:userInfo:
createNSErrorWithStatus:andMessage:
createNSExceptionWithStatus:andMessage:
init
floatValue
count
errorWithCode:message:
getLSTMScoreValueFromIndex:
alloc
getLSTMScoreTimeStampFromIndex:
addObject:
objectForKeyedSubscript:
compare:
sortedArrayUsingComparator:
objectAtIndex:
allValues
objectAtIndexedSubscript:
allKeys
initWithScores:frameRate:
generateSequenceSummaryWithMinClipDuration:maxClipDuration:maxSequenceSummaryDuration:error:
.cxx_destruct
_lstmScores
_frameRate
supportedFrameRates
initWithLSTMScores:frameRate:requestRevision:
initWithRequestRevision:
copy
initWithCoder:
vn_decodeCodingVersionForKey:
class
decodeObjectOfClass:forKey:
reason
encodeWithCoder:
vn_encodeCodingVersion:forKey:
encodeObject:forKey:
copyWithZone:
setFrameRate:
setLstmScores:
frameRate
numberWithFloat:
arrayWithObjects:count:
observationsCacheKey
description
lstmScores
initWithFormat:
supportsSecureCoding
standardUserDefaults
boolForKey:
defaultManager
stringByDeletingLastPathComponent
fileExistsAtPath:isDirectory:
fileHandleForUpdatingAtPath:
writeToFile:atomically:encoding:error:
seekToEndOfFile
stringByAppendingString:
dataUsingEncoding:
writeData:
closeFile
length
stringWithString:
stringWithCapacity:
insertString:atIndex:
replaceCharactersInRange:withString:
localeWithLocaleIdentifier:
setLocale:
setDateFormat:
date
stringFromDate:
resetFileNameURLWithCurentDateTime
initWithOptions:logEnabled:logFileNameBase:
currentDateTime
URLByAppendingPathComponent:
path
appendString:toLogFile:
padStringWithSpaces:toSize:
appendFormat:
logString:
logClusterMap:level:
isLogEnabled
initWithOptions:logEnabled:
logClusterMapL0:
logClusterLookupMapL0:
logClusterMapL1:
logClusterLookupMapL1:
logFolderURL
logFileURL
logEnabled
fileNameBase
_logEnabled
_logFolderURL
_logFileURL
_fileNameBase
countByEnumeratingWithState:objects:count:
deleteCharactersInRange:
appendString:
logSuggestons:description:
UTF8String
logInputFaceIdsWithFlags:
logAllSuggestons:
logFilteredByInputQuerySuggestons:
logConnectedGroups:
logFinalSuggestionsList:
errorForInternalErrorWithLocalizedDescription:
isEqualToString:
numberWithInt:
isEqual:
isKindOfClass:
fileURLWithPath:
_parseOptions:error:
boolValue
bytes
initializeLogging
faceprint
faceId
faceTorsoprint
descriptorData
isValidTorsoprint
getUUIDBytes:
enumerateObjectsUsingBlock:
numberWithLongLong:
arrayWithObject:
setObjects:
setClusterId:
setObject:forKeyedSubscript:
dictionaryWithCapacity:
unsignedIntegerValue
objectForKey:
numberWithBool:
dictionary
containsObject:
setWithObject:
unionSet:
minusSet:
allObjects
mutableCopy
isEqualToSet:
dataWithBytes:length:
unsignedIntValue
orderedSetWithCapacity:
longLongValue
intValue
array
getLevel0ClusteredIdsForFaceId:error:
errorForInternalErrorWithLocalizedDescription:underlyingError:
minusOrderedSet:
firstObject
unsignedLongValue
setObject:forKey:
addFaceObservations:toFaceDescriptorBuffer:
numberWithUnsignedInteger:
setShouldUpdateRepresentative:
setSuggestedIdsForRepresentative:
setTotalObjectCount:
addFaceObservations:withGroupingIdentifiers:toFaceDescriptorBuffer:
getRepresentativenessForFaces:error:
clustererModelFileNamesFromState:storedInPath:error:
suggestionsForClusterIdsWithFlags:affinityThreshold:error:
getClusterState:
getClusteredIds:
getLevel1ClusteredIdsGroupedByLevel0ClustersForFaceId:error:
getDistanceBetweenLevel0ClustersWithFaceId:andFaceId:error:
getDistanceBetweenLevel1Clusters:error:
getClustersForClusterIds:options:error:
getDistances:to:error:
maximumFaceIdInModelAndReturnError:
cancelClustering:
initWithOptions:error:
setGreedyClustererFaces_const:
convertUpdatePairsToClusters:
.cxx_construct
_clusteringLogger
_suggestionsLogger
_cacheFolderPath
_thresholdN
_thresholdTorso
_type
_state
_vectorMapReadOnlyFlagN
m_ClusteringImpl_const
errorWithCode:message:underlyingError:
_cancellableUpdate:facesToMove:
clusterId
totalObjectCount
objects
shouldUpdateRepresentative
initWithUUIDBytes:
nonGroupedGroupID
getClustersWithOptions:error:
m_ClusteringImpl
lastPathComponent
initWithRect:withFaceId:
setFaceId:
faceRect
setFaceRect:
framesSinceLast
setFramesSinceLast:
computeAverage
initWithScore:
addScore:
computeStandardDeviation
maxScore
setMaxScore:
minScore
setMinScore:
numScores
setNumScores:
sumScores
sumSqScores
setSwFaceId:
setSwCenter:
setSwSize:
setSwLastFrameSeen:
setHwFaceId:
setHwCenter:
setHwSize:
setHwLastFrameSeen:
hwCenter
hwSize
swCenter
swSize
hwFaceId
hwFaceRect
swFaceId
swFaceRect
overlapWithHwRect:
overlapWithSwRect:
swLastFrameSeen
hwLastFrameSeen
setCurConfig:
setFaceIdMapping:
setRenameMapping:
setFaceIdCounter:
arrayWithCapacity:
setFaceInfoArray:
setNumFramesSinceFullFaceCore:
setNumFramesNoFaces:
setForceFaceDetectionEnable:
setForceFaceDetailsEnable:
setFaceTimestampArray:
setLatestFaceTimestamp:
setLatestImageTimestamp:
setLastFaceIndex:
setVersion:
orientation
version
faceStatArray
setHwFaceRect:
hasLeftEye
hasRightEye
padRoiRect:paddingX:paddingY:
isSyncedWithImage
temporalOrder
faceInfoArray
setFacesRoiRect:
setNumHWFaces:
boundingBox
subarrayWithRange:
calculateFaceCoreROI:imageStat:needSWFaceDetection:
initWithCVPixelBuffer:options:
forceFaceDetectionEnable
setDetectionLevel:
setRegionOfInterest:
performRequests:error:
results
observationWithRequestRevision:boundingBox:
forceFaceDetailsEnable
_filterFacesToProcess:imageSize:imageStat:
initWithCompletionHandler:
applyConfigurationOfRequest:
setInputFaceObservations:
facesRoiRect
setNormalizedFaceRect:
setFoundByFaceCore:
landmarks
leftEye
setHasLeftEye:
isBlinking
setLeftEyeOpen:
setLeftEyeBlinkScore:
setLeftEyeRect:
rightEye
setHasRightEye:
setRightEyeOpen:
setRightEyeBlinkScore:
setRightEyeRect:
expressionsAndScores
setSmiling:
leftEyeRect
rightEyeRect
value:withObjCType:
getValue:
setFocusScore:
burstImages
focusScore
setNormalizedFocusScore:
setNormalizedSigma:
imageId
faceIdCounter
renameMapping
removeObjectForKey:
faceIdMapping
curConfig
addEntriesFromDictionary:
removeObjectAtIndex:
arrayWithArray:
dictionaryWithDictionary:
unsignedLongLongValue
faceTimestampArray
doubleValue
numberWithDouble:
insertObject:atIndex:
addFaceToArray:
timestamp
lastFaceIndex
setTimestamp:
setHasRollAngle:
setRollAngle:
setHasYawAngle:
setYawAngle:
setHasPitchAngle:
setPitchAngle:
setSmileScore:
setIsSyncedWithImage:
initWithVersion:
findOverlappingFaceStat:imageStat:
findFacesInImage:imageStat:
calculateFaceFocusInImage:imageStat:
calcFaceScores:
adjustFaceIdsForImageStat:
extractFacesFromMetadata:
addFacesToImageStat:imageSize:
dumpFaceInfoArray
timeBlinkDetectionDone
setTimeBlinkDetectionDone:
timeFaceDetectionDone
setTimeFaceDetectionDone:
latestFaceTimestamp
numFramesSinceFullFaceCore
numFramesNoFaces
latestImageTimestamp
_forceFaceDetailsEnable
_faceIdCounter
_numFramesSinceFullFaceCore
_numFramesNoFaces
_lastFaceIndex
_version
_curConfig
_faceIdMapping
_renameMapping
_faceInfoArray
_faceTimestampArray
_latestImageTimestamp
points
pointCount
setSvmParameters:
svmParameters
computeKernelValueWithSupportVector:
scaleVector
predictResult
isBurstAction
testAverageCameraTravelDistance
setTestAverageCameraTravelDistance:
testMaxRegistrationErrorIntegral
setTestMaxRegistrationErrorIntegral:
testMaxPeakRegistrationError
setTestMaxPeakRegistrationError:
testMeanPeakRegistrationError
setTestMeanPeakRegistrationError:
testBeginningVsEndAEMatrixVsAverageAdjacentAEMatrix
setTestBeginningVsEndAEMatrixVsAverageAdjacentAEMatrix:
testInOutRatio
setTestInOutRatio:
testMaxInnerDistance
setTestMaxInnerDistance:
testAverageRegistrationErrorSkewness
setTestAverageRegistrationErrorSkewness:
testMinRegionOfInterestSize
setTestMinRegionOfInterestSize:
testMaxRegistrationErrorSkewness
setTestMaxRegistrationErrorSkewness:
hasBeenScaled
testVector
_svmParameters
checkInitInputs:cachePath:checkType:error:
localizedDescription
initWithType:cachePath:state:readOnly:threshold:error:
initWithType:cachePath:state:readOnly:threshold:torsoThreshold:error:
_createGreedyClustererWith:error:
_createGreedyClusterer:state:error:
_threshold
_torsoThreshold
_cacheDirectoryPath
_readOnly
computeDistanceToFaceprint:withDistanceFunction:error:
distanceBetweenFacesWithFaceprint:andFaceprint:error:
conformsToProtocol:
tryToPerformBlock:usingSignallingBlock:
representativenessForFaces:error:
distanceBetweenFacesWithFaceObservation:andFaceObservation:error:
allClusteredFaceIdsAndReturnError:
clusteredFaceIdsForClusterContainingFaceId:error:
l1ClusteredFaceIdsGroupedByL0ClustersForClustersContainingFaceIds:error:
distanceBetweenClustersWithFaceId:andFaceId:error:
distanceBetweenLevel1Clusters:error:
suggestionsForClustersWithFaceIds:affinityThreshold:canceller:error:
initWithType:cachePath:state:threshold:error:
initWithType:cachePath:state:threshold:torsoThreshold:error:
_clusterer
updateModelByAddingFaces:withGroupingIdentifiers:andRemovingFaces:canceller:error:
resetModelState:error:
saveAndReturnCurrentModelState:
updateModelByAddingFaces:andRemovingFaces:canceller:error:
updateModelByAddingFaces:canceller:error:
updateModelByAddingFaces:withGroupingIdentifiers:canceller:error:
updateModelByRemovingFaces:canceller:error:
configurationOptionKeysForDetectorKey
initWithRequestRevision:sceneprints:
initWithMinimumDimension:maximumDimension:idealDimension:
initWithIdealFormat:pixelsWideRange:pixelsHighRange:aspectRatioHandling:idealOrientation:orientationAgnostic:
completeInitializationAndReturnError:
configurationOptions
requestRevision
errorForInvalidOperationWithLocalizedDescription:
hierarchicalModelAndReturnError:
createHierarchicalModelForRequestRevision:error:
getRequiredRequestRevision:fromSupportedRevisionsForRequestClass:inOptions:error:
createDirectoryAtURL:withIntermediateDirectories:attributes:error:
calculateImageDescriptorsWithOptions:regionOfInterest:warningRecorder:error:
initDumpDebugIntermediates:debugInfo:
initImageDescriptorBuffer:descriptorBuffer:error:
metalContext
classifyImageHierarchicallyWithDescriptors:usingImageClassifier:hierarchicalClassifier:minimumClassificationConfidence:minimumClassificationConfidenceRatio:maximumLeafLabels:maximumHierarchicalLabels:outputDebugDictionary:options:metalContext:error:
validatedImageBufferFromOptions:error:
fileURL
stringByDeletingPathExtension
numberWithUnsignedLongLong:
stringValue
stringByAppendingPathExtension:
stringByAppendingPathComponent:
dataWithJSONObject:options:error:
writeToFile:atomically:
logInternalError:
blacklistedIdentifiers
identifier
sceneprints
lengthInBytes
distanceMode
createClassifierWithDescriptor:classifierAbsolutePath:computePlatform:computePath:labelsFilename:options:
createDescriprorProcessorWithModelPath:nBatch:computePlatform:computePath:options:
classifierResourceTypesToNamesForRevision:
espressoModelImageprintClass
returnAllResultsOptionKey
createObservationWithDescriptors:forRequestRevision:
supportedImageSizeSetForOptions:
self
performSelector:
performSelector:withObject:
performSelector:withObject:withObject:
isProxy
isMemberOfClass:
respondsToSelector:
retain
release
autorelease
retainCount
zone
hash
superclass
debugDescription
isSceneprinterCompatibleWithSceneprinterCreatedWithOptions:error:
processWithOptions:regionOfInterest:warningRecorder:error:
_sceneClassifierHierarchicalModel
pathForResource:ofType:
alignedBoundingBox
width
height
croppedBufferWithWidth:height:format:cropRect:options:error:
alignedRotationAngle
predictFaceOnImageCrop:faceObservation:error:
mPixelDifferenceFaceClassifier
augmentedCroppedBuffersWithWidth:height:format:cropRect:options:augmentationOptions:error:
setRevision:
supportedImageSizeSet
pixelsWideRange
idealDimension
pixelsHighRange
idealImageFormat
setUsesCPUOnly:
initWithURL:options:
newLibraryWithFile:error:
newCommandQueue
initWithDevice:error:
minimumLinearTextureAlignmentForPixelFormat:
bytesPerPixelForTextureFormat:
metalContextForDevice:error:
makeTextureCoherent:texture:
bindPixelBufferToMTL2DTexture:pixelFormat:plane:error:
bindPixelBufferToMTL2DTexture:pixelFormat:textureSize:plane:error:
textureBytesPerRow:format:
device
commandQueue
library
_device
_commandQueue
_library
dictionaryWithObjectsAndKeys:
errorForInvalidOption:named:localizedDescription:
stringByAppendingFormat:
errorForInvalidModelWithLocalizedDescription:underlyingError:
errorForUnsupportedRevision:ofRequestClass:
supportedRevisions
string
code
cStringUsingEncoding:
localizedFailureReason
errorForCancellationOfRequest:
errorForMemoryAllocationFailure
errorForUnimplementedFunctionWithLocalizedDescription:
errorForMissingOptionNamed:
errorForInvalidOption:named:
errorForInvalidArgument:named:
errorForInvalidModelWithLocalizedDescription:
errorForGPURequiredByRequest:
errorForUnsupportedProcessingDevice:
errorForUnsupportedRevision:ofRequest:
errorForEspressoReturnStatus:localizedDescription:
errorForCVReturnCode:localizedDescription:
errorForOSStatus:localizedDescription:
enumerateRangesUsingBlock:
initWithTargetedCVPixelBuffer:completionHandler:
initWithTargetedCGImage:completionHandler:
initWithTargetedCIImage:completionHandler:
initWithTargetedImageURL:completionHandler:
initWithTargetedImageData:completionHandler:
sequencedRequestPreviousObservationsKey
burstFrameIdentifier
imageProperties
requiredTargetedImageSpecifierReturningError:
imageBufferAndReturnError:
modelRequestHandlerAndReturnError:
modelContextObject
burstAnalysisLoggingCallback
setLoggingCallback:
setModelContextObject:
addBurstFrameWithIdentifier:fromImageBuffer:withProperties:error:
setResults:
warmUpRequestPerformer:error:
internalPerformInContext:error:
setBurstFrameIdentifier:
setImageProperties:
_burstFrameIdentifier
_imageProperties
UUID
UUIDString
targetsGPU
targetsCPU
targetsLabrador
validatedProcessingDeviceInOptions:error:
detectorWithConfigurationOptions:error:
processingQueue
espressoDeviceID
espressoEngine
espressoStorageType
metalDevice
espressoDeviceIDForMetalDevice:
getRequiredRequestRevision:inOptions:error:
bufferWithWidth:height:format:options:error:
shouldDumpDebugIntermediates
initWithObjectsAndKeys:
setBoundingBox:
setRoll:
setYaw:
processWithOptions:warningRecorder:error:
addObjectsFromArray:
processInChunksOfSize:overlapFraction:options:roi:handler:error:
setObject:atIndexedSubscript:
purgeIntermediates
supportsProcessingDevice:
_faceDetector
_faceBBoxAligner
leftEyeOpen
rightEyeOpen
smiling
foundByFaceCore
normalizedFaceRect
normalizedSigma
normalizedFocusScore
faceScore
leftEyeBlinkScore
rightEyeBlinkScore
smileScore
FCRLeftEyeFeaturesOffset
FCRRightEyeFeaturesOffset
FCRSmileFeaturesOffset
FCRBlinkFeaturesSize
FCRSmileFeaturesSize
allocWithZone:
initWithFaceStat:
setFaceScore:
setFCRLeftEyeFeaturesOffset:
setFCRRightEyeFeaturesOffset:
setFCRSmileFeaturesOffset:
setFCRBlinkFeaturesSize:
setFCRSmileFeaturesSize:
FCRSmileAndBlinkFeatures
setFCRSmileAndBlinkFeatures:
hasRollAngle
hasYawAngle
rollAngle
yawAngle
smallFace
setSmallFace:
hasPitchAngle
pitchAngle
_hasPitchAngle
_isSyncedWithImage
_pitchAngle
_hwFaceRect
setImageId:
setFaceStatArray:
setOrientation:
setMaxSkewness:
setHasRegistrationData:
dealloc
getSharpnessAndBlurLimits
AEAverage
setAEDelta:
maxSkewness
setRegistrationErrorX:
setRegistrationErrorY:
setRegistrationErrorIntegral:
registrationErrorIntegral
setActionClusteringScore:
updateROI:
computeImageColorHistogram:
computeImageSharpnessOnGrid:
computeBlurStatsOnGrid:
computeImageProjections:
getBytes:length:
computeFacialFocusScoreSum
allocateMeanStdPingPongBuffers::::
assignMeanStdBuffers:
initWithIdentifier:
computeSmoothedGridROI:nextStat:
flagAsGarbage
performRegistration:deltaCol:deltaRow:
canRegister
writeGridROI:
computeImageData:faceIDCounts:
collapseSharpnessGrid
computeRuleOfThreeDistance
computeSmilePercentage
computeImageDistance:
computeAEMatrixDifference:
setAEMatrix:
computeAEMatrix:
aeMatrix
computeScore:
compareImageStats:
compareImageOrder:
colorHistogram
exclude
setExclude:
AEStable
setAEStable:
setAEAverage:
AETarget
setAETarget:
AFStable
setAFStable:
setTemporalOrder:
avgHorzDiffY
setAvgHorzDiffY:
blurExtent
setBlurExtent:
imageScore
setImageScore:
actionScore
setActionScore:
timeReceived
setTimeReceived:
registrationErrorX
registrationErrorY
hasRegistrationData
actionClusteringScore
numHWFaces
emotionallyRejected
setEmotionallyRejected:
doLimitedSharpnessAndBlur
setDoLimitedSharpnessAndBlur:
setTx:
setTy:
isGarbage
setIsGarbage:
roiSize
setRoiSize:
AEDelta
numEntries
dissimilarity
projectionSignature
sharpnessGrid
gridWidth
gridHeight
gridROI
smoothedROI
_AEDelta
_imageId
_faceStatArray
initWithRequestPerformer:imageBuffer:observationsCache:
performRequests:inContext:error:
performRequests:onCVPixelBuffer:orientation:error:
_performRequests:onUnvettedImageBuffer:error:
performRequests:onCGImage:orientation:error:
initWithCGImage:options:
performRequests:onCIImage:orientation:error:
initWithCIImage:options:
performRequests:onImageURL:orientation:error:
performRequests:onImageData:orientation:error:
initWithData:options:
prepareForPerformingRequestsOfClass:error:
prepareForPerformingRequests:error:
cancelAllRequests
requestForcedCleanupWithOptions:
forcedCleanupWithOptions:
asyncProcessingDispatchQueue
sharedInstance
purgeAll
manager
forcedCleanup
purgeAllCaches
requestForcedCleanup
requestForcedCleanupWithOptions:completion:
performRequests:onCVPixelBuffer:error:
performRequests:onCGImage:error:
performRequests:onCIImage:error:
performRequests:onImageURL:error:
performRequests:onImageData:error:
performRequests:onImageSpecifier:error:
_requestPerformer
initWithCVPixelBuffer:orientation:options:
imageSpecifierWithCVPixelBuffer:orientation:options:error:
initWithImageSpecifier:
initWithCGImage:orientation:options:
imageSpecifierWithCGImage:orientation:options:error:
initWithCIImage:orientation:options:
imageSpecifierWithCIImage:orientation:options:error:
initWithURL:orientation:options:
imageSpecifierWithURL:orientation:options:error:
initWithData:orientation:options:
imageSpecifierWithData:orientation:options:error:
_options
_imageSpecifier
_observationsCache
setModelRequestHandler:
setBurstAnalysisLoggingCallback:
_burstAnalysisLoggingCallback
_modelContextObject
numberWithUnsignedInt:
_createN:CVPixelBuffers:withPixelFormat:width:height:error:
cachedFloatingImageBufferReturningError:
options
regionOfInterestPixelRectForWidth:height:
_createHomographicPixelBufferFromImageBuffer:cropRect:options:error:
getReferenceImageBuffer:registrationSignature:forRequestPerformingContext:options:error:
_calculateHomographicWarpTransform:ofFloatingImagePixelBuffer:ontoReferenceImagePixelBuffer:usingImageRegistrationContext:glContext:seededWithPreviousWarpTransform:error:
cachedFloatingImageRegistrationSignatureReturningError:
setReferenceImageSignature:
setFloatingImageSignature:
setWarpTransform:
wantsSequencedRequestObservationsRecording
initWithName:options:completionHandler:
initWithSceneObservation:completionHandler:
setValue:forRequestOption:
resolvedRevision
customHierarchy
detectorOfType:options:error:
sceneObservation
maximumLeafObservations
maximumHierarchicalObservations
compatibleRevisionForDependentRequestOfClass:beingPerformedByRevision:
resolvedRevisionForRevision:
validateImageBuffer:ofNonZeroWidth:andHeight:error:
cachedObservationsForRequest:beingPerformedOnBehalfOfRequest:
requestPerformerAndReturnError:
warnings
recordWarning:value:
enumerateKeysAndObjectsUsingBlock:
qosClass
regionOfInterest
processInSynchronizationQueueUsingQualityOfServiceClass:options:regionOfInterest:warningRecorder:error:
confidence
uuid
defaultRevision
knownSceneClassificationsForRevision:error:
getLabels
allSceneLabels
containsIndex:
_knownVNSceneClassifierLabelsForRevision:error:
_knownVNImageAnalyzerMultiDetectorLabelsForRevision:error:
initWithRequestRevision:identifier:confidence:
customHierarchyForSceneClassificationRequest:
customHierarchyWithAdditionalRelationships:error:
defineCustomHierarchy:error:
dependentRequestCompatability
knownSceneClassifications
initWithSceneObservation:
internalPerformRevision:inContext:error:
resultsSortingComparator
defineCustomHierarchyWithRelationships:error:
setSceneObservation:
setMaximumLeafObservations:
setMaximumHierarchicalObservations:
_sceneObservation
_customHierarchy
_maximumLeafObservations
_maximumHierarchicalObservations
revisionAvailability
processingDeviceNetworkWithModelPath:threshold:preferredDeviceID:engineID:storageType:
initWithNetwork:filterThreshold:
threshold
setThreshold:
sortDescriptorWithKey:ascending:
sortedArrayUsingDescriptors:
nmsThreshold
overlap:
osfsThreshold
osfsSizeRatio
isOverlappingSmallFace:withOverlapThreshold:withSizeRatio:
olmcsThreshold
olmcsMergeCountDelta
mergesCount
isOverlappingLowMergeDet:withOverlapThreshold:withMergeCountDelta:
defaultBox
scale
rotationAngle
initWithBox:defaultBox:confidence:scale:rotationAngle:yawAngle:mergesCount:
smartThreshold
smartDistanceFactor
boxCenter
filterThreshold
predicateWithBlock:
filteredArrayUsingPredicate:
resizeAndProcessVImage:inputIsBGR:
smartMergeBoxes:
nmsBoxes:
overlappingSmallFacesSuppression:
overlappingLowMergeCountSuppression:
filterBoxes:
initWithOptionsXloc:yloc:size:confidence:
setRotationAngle:
processingDeviceDetectorWithModelPath:networkThreshold:filterThreshold:preferredDeviceID:engineID:storageType:
mergeBoxes:
detect:inputIsBGR:
processBoxes:withHeight:andWidth:
overlap_threshold
setOverlap_threshold:
setFilterThreshold:
setNmsThreshold:
setOsfsThreshold:
setOsfsSizeRatio:
setOlmcsThreshold:
setOlmcsMergeCountDelta:
setSmartThreshold:
setSmartDistanceFactor:
_network
_overlap_threshold
_filterThreshold
_nmsThreshold
_osfsThreshold
_osfsSizeRatio
_olmcsThreshold
_olmcsMergeCountDelta
_smartThreshold
_smartDistanceFactor
write:maxLength:
archivedDataWithRootObject:requiringSecureCoding:error:
read:maxLength:
initWithLength:
mutableBytes
unarchivedObjectOfClass:fromData:error:
streamError
initWithSignatureData:
computeBlurSignatureForGrayscaleImage:error:
setSignatureData:
getSignatureData
_signatureData
computeBlurScore:onGrayscaleImage:andWithRegionOfInterestInImageCoordinates:andRegionOfInterestInsetFactor:error:
computeBlurScore:onGrayscaleImage:insetFactor:error:
computeBlurScore:usingBlurSignature:insetFactor:imageROI:error:
computeEdgeWidthBlurScore:onGrayscaleImage:error:
computeApproximateBlurScore:onGrayscaleImage:sampledPixelsCount:insetFactor:error:
computeApproximateBlurScore:onRGBAImage:sampledPixelsCount:insetFactor:error:
recordDefaultOptionsInDictionary:
_newTrackerOptionsForRequestRevision:error:
trackerWithOptions:error:
releaseTracker:
_trackingLevelOptionFromTrackingLevelEnum
trackerType
inputObservation
_trackingLevelEnumFromTrackingLevelOption:
initWithDetectedObjectObservation:completionHandler:
completionHandler
trackingLevel
isLastFrame
raise:format:
_resetTrackerIfNeeded:trackerProvider:options:error:
isTracking
setTrackedObjects:inFrame:error:
trackInFrame:error:
level
lastTrackedBBox
setTrackingLevel:
initWithDetectedObjectObservation:
newDefaultRequestInstance
setInputObservation:
setLastFrame:
_inputObservation
_trackingLevel
_lastFrame
setMetalDevice:
name
_initWisdomParams
mapMetalDeviceNameToWisdomParams
containsString:
initWithMetalDevice:
wisdomParams
useGPU
_useGPU
_metalDevice
_wisdomParams
hasPrefix:
substringFromIndex:
defaultDevice
detectorClassForConfigurationOptions:
initWithConfigurationOptions:
needsMetalContext
newMetalContextForConfigurationOptions:error:
initWithArray:
initWithCapacity:
detectorKeyComponentForDetectorConfigurationOptionKey:value:
keyForDetectorWithConfigurationOptions:
currentQueueIsSynchronizationQueue
detectorName
validateNonZeroImageWidth:height:componentNameProvidingBlock:error:
getArray:forKey:inOptions:withElementsOfClass:requiredMinimumCount:allowedMaximumCount:error:
validateImageBuffer:error:
getOptionalObject:ofClass:forKey:inOptions:error:
requiredObjectOfClass:forKey:inOptions:error:
recordDefaultConfigurationOptionsInDictionary:
fullyPopulateConfigurationOptions:
getOptionalCanceller:inOptions:error:
requiredCancellerInOptions:error:
backingStore
synchronizationQueue
setSynchronizationQueue:
_configurationOptions
_processingQueue
_requestRevision
_metalContext
_backingStore
_synchronizationQueue
uppercaseLetterCharacterSet
scannerWithString:
isAtEnd
scanUpToCharactersFromSet:intoString:
scanCharactersFromSet:intoString:
originalRequests
preferBackgroundProcessing
setDetectorConfigurationOption:value:
addOriginalRequest:withObservationClass:
detectorConfigurationOptions
observationClasses
_detectorConfigurationOptions
_originalRequests
_observationClasses
configurationForRequest:withObservationClass:
allConfigurations
_regionOfInterestConfigurations
initWithSubrequests:uniqueObservationClasses:
initWithDetectorType:configuration:
setPreferBackgroundProcessing:
detectionLevel
returnAllResults
_addCompoundRequestsToArray:forModel:withConfigurations:
recordWarningsInOriginalRequests
assignOriginalRequestsResultsFromObservations:obtainedInPerformingContext:
compoundRequestsForOriginalRequests:
_detectorType
failWithError:
decodeIntegerForKey:
initWithRequestRevision:regionMap:deallocateBuffer:userBBox:alignedBBox:valueToLabelMap:
encodeInteger:forKey:
dataWithBytesNoCopy:length:freeWhenDone:
numberWithUnsignedLong:
numberWithUnsignedChar:
regionNameAtNormalizedAlignedFaceCoordinate:
getRegionLabels
regionNameAtImageCoordinate:imageSize:
regionNameAtNormalizedFaceCoordinate:
regionLabels
setRegionLabels:
_regionMap
_userBBox
_internalAlignedBBox
_deallocateBuffer
_pixelValueToRegionLabelMap
_regionLabels
methodForSelector:
instancesRespondToSelector:
instanceMethodForSelector:
object:overridesSelector:
resetWithSignallingBlock:
releaseSignallingBlock
resetAndPerformBlock:usingSignallingBlock:
signalCancellation
wasSignalled
_signallingBlock
_lock
_signalled
load:
baseAddress
unload:
originalPixelBuffer
makeClippedRectAgainstImageExtentUsingOriginalRect:
createCroppedBufferWithMaxSideLengthOf:andCropBounds:andPixelFormat:andOptions:error:
setIsBlinking:
setBlinkScore:
_computeCentroidUsingPoints:indicies:numberOfIndicies:
faceAttributesPupilRefiner
createLumaPixelBufferFrom:scaleToSize:forFaceBBox:initializeVImage:initializeRect2D:initializeIgnoreCropAndScaleFlag:initializeLumaScaleFromOriginal:error:
computeLandmarksScoreOnImage:withFaceBoundingBox:andLandmarks:error:
detectBlinkOnFaceImage:faceObservation:lumaRec2DInImageCoordinates:landmarks:options:warningRecorder:error:
_faceAttributesPupilRefiner
mLandmarkRefinerModelFileHandle
modelFilesWereMemmapped
weakObjectsHashTable
lock
objectEnumerator
nextObject
purgeCachedRepresentations
unlock
removeObject:
contextWithMTLDevice:options:
contextWithOptions:
addImageBuffer:
removeBuffer:
sharedCIContextWithOptions:
mainCIContext
mainCIContextMetalDevice
lowPriorityCIContext
lowPriorityCIContextMetalDevice
activeImageBuffers
bufferTableLock
obtainImageSourceRefWithSubSampleFactor:andLowPriorityHint:
_obtainCreatedCGImageSourceRefAtAddress:forSubSampleFactor:protectedWithUnfairLock:operatingInLowPriority:
obtainImageSourceRef
initWithImageURL:
initWithImageData:
imageURL
imageData
exifOrientation
_getOrientationLock
_loadSubSample1Lock
_loadSubSample2Lock
_loadSubSample4Lock
_loadSubSample8Lock
_imageSourceSubsample1
_imageSourceSubsample2
_imageSourceSubsample4
_imageSourceSubsample8
_imageURL
_imageData
_orientation
_helpReadOrientationFromOptionsDictionary:
initWithOptions:
imageWithCGImage:
pixelBufferAttributes
imageByApplyingOrientation:
imageByCroppingToRect:
imageByApplyingTransform:highQualityDownsample:
extent
copyColorspaceForFormat:bitmapInfo:
render:toCVPixelBuffer:bounds:colorSpace:
_useCoreImageForFormat:
imageWithContentsOfURL:options:
imageWithData:options:
_cropCIImage:outBuffer:width:height:format:cropRect:performCrop:options:error:
_cropCVPixelBuffer:outBuffer:width:height:format:cropRect:performCrop:options:error:
_cropImageSourceManager:outBuffer:width:height:format:cropRect:performCrop:options:error:
calculateOrientationCorrectedImageDimensions
getCameraIntrinsicsAvailable:
imageWithCVImageBuffer:
imageWithContentsOfURL:
imageWithData:
_baseCIImage
render:toCVPixelBuffer:
firstObjectCommonWithArray:
_baseCVPixelBuffer
filterWithName:
imageByClampingToExtent
setValue:forKey:
outputImage
imageByApplyingTransform:
colorWithRed:green:blue:alpha:
imageByApplyingCGOrientation:
getPixelFocalLengthIfAvailable:
getCameraOpticalCenterIfAvailable:
createBufferWithMaxSideLengthOf:andPixelFormat:andOptions:error:
augmentedBuffersWithWidth:height:format:options:augmentationOptions:error:
_origPixelBuffer
_pixelBufferReps
_origCIImage
_passedInCIContext
_imageSourceManager
_origImageWidth
_origImageHeight
hasBBoxBeenAligned
getOptionalValidatedInputFaceObservations:clippedToRegionOfInterest:error:
_determineFacesToProcessFrom:outputFacesThatNeedNoProcessing:outputFacesThatNeedAlignment:outputFacesThatNeedTorsoprints:
initWithFaceObservations:
inputFaceObservations
dependencyProcessingOrdinality
shared
bundleForClass:
setIs_memory_tight:
is_memory_tight
setContext_metal:
setContext_cpu:
autoSetupNetBaseName:weights:scaleConfig:setupMode:computePath:autoAspectRatio:forceReset:useLowPriorityMode:gpuPriority:
initWithNetwork:
detect:face:sublandmark:doFaceRectFix:
newface
setForceMaxNScales:
wipeLayersMemory
autoResizeForAspectRatio:useLowPriorityMode:gpuPriority:
processBlobNoRotation:tex:doBGRA2RGBA:
getFacesFromNetworkResultOriginalWidth:originalHeight:
bounds
pathExtension
initWithNetworkAtPath:context:platform:computePath:
initWithConfiguration:
initWithConfiguration:dataSource:
setDelegate:
lastModificationDate
_lastModificationDate
localizedStringFromDate:dateStyle:timeStyle:
configuration
modelBuiltFromConfiguration:dataProvider:canceller:error:
maximumIdentities
faceIDModelMaximumElementsPerID
_writeVersion1ConfigurationToOutputStream:md5Context:error:
_writeVersion1InformationToOutputStream:md5Context:error:
faceModelPersonsCount
faceModelNumberOfFaceObservationsForPersonAtIndex:
faceModelFaceObservationAtIndex:forPersonAtIndex:
faceModelUniqueIdentifierOfPersonAtIndex:
upToDateFaceModelWithCanceller:error:
_writeReadOnlyVersion:toOutputStream:options:md5Context:error:
setMaximumIdentities:
setMaximumFaceprintsPerIdentity:
keyEnumerator
addFaceObservations:toPersonWithUniqueIdentifier:error:
versionNumbersEncodedInClass:withMethodNamePrefix:suffix:
addIndexes:
_version1ModelWithObjects:error:
newModelFromVersion:objects:error:
supportedWriteVersions
initWithIndex:
readOnly
_getModelWritingImplementation:selector:forVersion:readOnly:
enumerateIndexesWithOptions:usingBlock:
_getModelWritingImplementation:selector:version:forOptions:
open
writeToStream:options:error:
close
initToMemory
_writeToUnopenedStream:options:error:
propertyForKey:
initWithURL:append:
removeFaceObservations:fromPersonWithUniqueIdentifier:error:
removeAllFaceObservationsFromPersonWithUniqueIdentifier:error:
removePersonWithUniqueIdentifier:error:
_modelWasModified
personsModelDataWasModified:
writeVersion1ToOutputStream:options:md5Context:error:
writeReadOnlyVersion1ToOutputStream:options:md5Context:error:
dataWithOptions:error:
writeToURL:options:error:
_modelData
_faceModel_DO_NOT_ACCESS_DIRECTLY
appendBytes:length:
appendData:
setLength:
initWithIndexesInRange:
archivedDataWithRootObject:
enumerateObjectsAtIndexes:options:usingBlock:
strongToStrongObjectsMapTable
processingDevice
preferredMetalContext
integerValue
requestWithName:options:completionHandler:
defaultRequestInstanceWarmUpPerformer:error:
requestWithName:options:
copyStateOfRequest:
revision
setWarnings:
compatibleRevisionForDependentRequest:
metalContextPriority
cancellationTriggered
cachedObservationsForRequest:
hasCancellationHook
internalCancelInContext:error:
recordSequencedObservationsForRequest:
cacheObservationsForRequest:
sortedArrayWithOptions:usingComparator:
valueForWarning:
setCancellationTriggered:
null
deviceForMetalDevice:
usesCPUOnly
defaultCPUDevice
getOptionalArray:forKey:inOptions:withElementsOfClass:error:
addIndex:
lastIndex
_introspectionBuiltSupportedRevisions
_defaultRevisionForBuildVersion:
initialize
getRequiredObject:ofClass:forKey:inOptions:error:
getDoubleValue:forKey:inOptions:error:
getDoubleValue:forKey:inOptions:withDefaultValue:error:
getFloatValue:forKey:inOptions:error:
getFloatValue:forKey:inOptions:withDefaultValue:error:
getOptionalInputFacesArray:inOptions:error:
currentRevision
setValue:forPrivateOption:
valueForPrivateOption:
performInContext:error:
validateConfigurationAndReturnError:
cancel
dumpIntermediateImages
setDumpIntermediateImages:
setPreferredMetalContext:
disallowsGPUUse
setDisallowsGPUUse:
setMetalContextPriority:
setProcessingDevice:
cancellationSemaphore
setCancellationSemaphore:
modelFileBackingStore
setModelFileBackingStore:
requestName
_requestName
_completionHandler
_warningRecorder
_cancellationSemaphore
_cancellationQueue
_revision
_cachedResolvedRevision
_detectionLevel
_processingDevice
_metalContextPriority
_preferBackgroundProcessing
_dumpIntermediateImages
_cancellationTriggered
_results
_modelFileBackingStore
minimumDimension
maximumDimension
isAllowedDimension:
_minimumDimension
_maximumDimension
_idealDimension
isAllowedPixelsWide:pixelsHigh:
aspectRatioHandling
idealOrientation
isOrientationAgnostic
_orientationAgnostic
_idealImageFormat
_idealOrientation
_pixelsWideRange
_pixelsHighRange
_aspectRatioHandling
emptyVNTorsoprintForRevision:
elementCount
initWithData:elementCount:lengthInBytes:labelsAndConfidence:requestRevision:
initWithFaceprint:torsoPrint:requestRevision:
torsoprint
computeDistance:withDistanceFunction:error:
currentVersion
currentCodingVersion
codingTypesToCodingKeys
serializationMagicNumber
currentSerializationVersion
_faceprint
_torsoprint
_determineFacesToProcessFrom:outputFacesThatNeedNoProcessing:outputFacesThatNeedAlignment:outputfacesThatNeedAttributes:
landmarkPoints
_createTrackerWithLevel:options:error:
setUUID:
_convertDetectedObjectObservationsToFaceObservations:error:
_runBBoxAlignmentOnTrackingResults:trackerResults:error:
_runFaceClassifierOnBBoxAlignmentResults:trackingResults:bBoxAlignmentResults:error:
_postProcessTrackingResults:trackerResults:error:
faceClassifierBoostedPixelDifference
setConfidence:
trackerObservationClass
_parseInputObservations:imageBuffer:error:
setContext:
getKey:fromDictionary:withDefault:
context
setDebugMode:
setTimerMode:
setQualityCriteriaList:
setClusterSplitDistanceType:
setUseTimestampAdjustedDistances:
setPerformClustersPostprocessing:
setPerformSceneClassification:
setRoiAreaThreshold:
setInliersRatioThreshold:
setNumberOfKeypointsToConsider:
setNaturalClusteringDistanceThreshold:
getNextImage:
qualityCriteriaList
initWithImageData:andQualityCriteria:context:error:
computeClusteringTreeForImageDescriptors:error:
computeClusteringForClusteringTree:intoKGroups:error:
computeNaturalClusteringForClusteringTree:error:
computeClusteringTreeForImageDescriptors:assumeDescriptorsAreSorted:error:
getHostTime
sortImageDescriptorsChronologically:
computeHierarchicalClusteringOfImageDescriptors:results:context:
initWithNode:freeNodeOnDealloc:
node
computeClusteringIntoKGroups:forHierarchicalTree:context:
convertClusterNodesListToDescriptorsList:
performClustersPostprocessing:error:
computeClusteringUsingDistanceThreshold:forHierarchicalTree:context:
computeNaturalClusteringForHierarchicalTree:context:
exifTimestamp
descriptorId
processImagesFromDataProvider:error:
computeClusteringOfImageDescriptors:intoKGroups:error:
computeNaturalClusteringOfImageDescriptors:error:
computeClusteringForClusteringTree:usingThreshold:error:
_context
setNode:
setFreeNodeOnDealloc:
nodeId
descriptor
left
right
distance
avgDistance
leafsCount
getLeafNodes
freeNodeOnDealloc
_freeNodeOnDealloc
_node
imageSpecifierWithCVPixelBuffer:orientation:error:
initWithCVPixelBuffer:
imageSpecifierWithCGImage:orientation:error:
initWithCGImage:
imageSpecifierWithCIImage:orientation:error:
initWithCIImage:
imageSpecifierWithURL:orientation:error:
initWithURL:
imageSpecifierWithData:orientation:error:
initWithData:
imageSpecifierWithCVPixelBuffer:error:
imageSpecifierWithCGImage:error:
imageSpecifierWithCIImage:error:
imageSpecifierWithURL:error:
imageSpecifierWithData:error:
initInternal
newImageBufferWithOptions:error:
_imageBuffer
isSubclassOfClass:
imageSpecifierWithObject:error:
pixelBuffer
_pixelBuffer
cgImage
_cgImage
ciImage
_ciImage
absoluteString
_url
data
_data
updateExtrema:x:y:
computeRectFromExtremaUsingThreshold:vImage:
_extrema
_extremeValues
initWithRequestRevision:CVPixelBuffer:
decodeDoubleForKey:
setWithObjects:
decodeObjectOfClasses:forKey:
encodeDouble:forKey:
initWithRequestRevision:boundingBox:
_computeSalientObjects
_computeBoundingBoxes
createNSDataDumpFrom64x64FloatPixelBuffer:
create64x64FloatPixelBufferFromNSData:
initWithRequestRevision:rawSaliencyImage:originalImageSize:
createSaliencyImageAndReturnError:
salientObjects
narrowedBoundingBox
_mOriginalImageSize
_mSalientObjects
_mSalientRegion
_mHighlySalientRegion
_smartCamCombinedModelImageAestheticsClassificationsForRevision:performedInContext:error:
trackedCorners
_convertCornerObservationsToRectangleObservation:error:
setLastTrackedBBox:
setTrackedFrameNumber:
trackedFrameNumber
reset:
bottomLeft
_trackingRectAroundPoint:trackingRectSize:
bottomRight
topLeft
topRight
initWithRequestRevision:topLeft:bottomLeft:bottomRight:topRight:
_cornerTrackersImpl
_rectangleTrackingProcessingQueue
includeClusters
includeAllImageIdentifiers
includeAllImageStats
bestImageIdentifiers
setBestImageIdentifiers:
coverImageIdentifier
setCoverImageIdentifier:
isAction
setIsAction:
isPortrait
setIsPortrait:
allClusters
setClusters:
allImageIdentifiers
setAllImageIdentifiers:
allImageStats
setAllImageStats:
setIncludeClusters:
setIncludeAllImageIdentifiers:
setIncludeAllImageStats:
_includeClusters
_includeAllImageIdentifiers
_includeAllImageStats
initWithRequestRevision:smartCamprints:
requiredFaceObservationInOptions:withOptionName:error:
setExpressionsAndScores:
expressionTypeFromString:
createExpressionAndConfidencesDictionaryFromScores:
createExpressionDetectionDictionaryFromScores:
m_FaceAttributesImpl
m_LandmarkRefinerModelFileHandle
confidenceTypeForRevision:
dividerScore
leftImage
actionAmount
compareDividers:
compareIndices:
compareActionAmounts:
setDividerScore:
setLeftImage:
trueLocalMaximum
setTrueLocalMaximum:
setActionAmount:
noiseThreshold
setNoiseThreshold:
highNoiseThreshold
setHighNoiseThreshold:
setAlgorithm:
valueForKey:
textBoxesForImage:error:
reportCharacterBoxes
initWithDimensions:
minimumCharacterPixelHeight
setMinimumCharacterHeight:
detectDiacritics
setDetectDiacritics:
minimizeFalseDetections
setMinimizeFalseDetections:
setRecognitionLanguage:
setReturnSubFeatures:
detectFeaturesInBuffer:withRegionOfInterest:error:
type
corners
text
setText:
subFeatures
setCharacterBoxes:
algorithm
textRecognition
_detectTextWithRequestPerformingContext:requestRevision:error:
_detectCreditCardTextWithRequestPerformingContext:requestRevision:error:
setReportCharacterBoxes:
setTextRecognition:
setMinimumCharacterPixelHeight:
_algorithm
_textRecognition
_minimumCharacterPixelHeight
_reportCharacterBoxes
_detectDiacritics
_minimizeFalseDetections
_rotationAngle
_yawAngle
initWithDomain:code:userInfo:
numberOfPersonsInPersonsModel:
personsModel:uniqueIdentifierOfPersonAtIndex:
personsModel:indexOfPersonWithUniqueIdentifier:
personsModel:numberOfFaceObservationsForPersonAtIndex:
personsModel:faceObservationAtIndex:forPersonAtIndex:
faceModelIndexOfPersonWithUniqueIdentifier:
initWithPersonsModel:dataSource:
_personsModel
_dataSource
hasSuffix:
substringWithRange:
_modelClassForKind:error:
acceptableVersions
_readModelObjectsFromStream:options:actionBlock:progressBlock:modelClass:version:error:
_modelFromStream:options:error:
_modelFromUnopenedStream:options:error:
distantPast
initWithVersion:lastModificationDate:
_modelInformationFromUnopenedStream:error:
personCount
lastDataChangeSequenceNumberForPersonsModel:
validateFaceprintedFaceObservation:error:
personPredictionsForFace:withDescriptor:limit:canceller:error:
configurationFromLoadedObjects:error:
supportedReadVersions
modelFromStream:options:error:
modelFromData:options:error:
modelFromURL:options:error:
informationForModelWithData:error:
informationForModelWithURL:error:
readObjectForVersion1Tag:fromInputStream:intoObjectDictionary:md5Context:error:
predictPersonFromFaceObservation:limit:canceller:error:
personUniqueIdentifiers
faceCountForPersonWithUniqueIdentifier:
faceCountsForPersonsWithUniqueIdentifiers:
faceCountsForAllPersons
_configuration
_lastDataChangeSequenceNumber
isEqualToDate:
maximumAllowableIdentities
maximumAllowableFaceprintsPerIdentity
maximumFaceprintsPerIdentity
_maximumIdentities
_maximumFaceprintsPerIdentity
decodeFloatForKey:
initWithFaceObservation:predictedPersonUniqueIdentifier:confidence:
encodeFloat:forKey:
predictedPersonUniqueIdentifier
faceObservation
_faceObservation
_predictedPersonUniqueIdentifier
_confidence
setAcceptableVersions:
_acceptableVersions
setReadOnly:
encodeBool:forKey:
decodeBoolForKey:
subdataWithRange:
cascadeStepCountLoaded
cascadeStepCountInOriginalModel
_specialCaseLookUpOfExistingDetectorType:configuredWithOptions:
_synchronizationQueueForDetectorType:
_detectorClassForDetectorType:
detectorOfType:backingStore:options:error:
_detectorClassForDetectorType:options:detectorCreationOptions:
_detectorOfClass:type:configuredWithOptions:error:
keysOfEntriesPassingTest:
removeObjectsForKeys:
_removeCachedDetectorClasses:
_removeCachedDetectorTypes:
_forcedCleanupFacePipelineWithLevel:
_forcedCleanupScenePipelineWithLevel:
_forcedCleanupSmartCamPipelineWithLevel:
_forcedCleanupJunkPipelineWithLevel:
detectorClassForDetectorType:options:
_activeDetectorsCacheLock
_activeDetectorsCache
_detectorTypeToSynchronizationQueueLookupLock
_detectorTypeToSynchronizationQueueLookup
initWithImageBuffer:andOptions:error:
previousSequencedObservationsForRequest:
validateArray:named:hasElementsOfClass:requiredMinimumCount:allowedMaximumCount:error:
floatingImageSignature
optionNameForTargetedImageSpecifyingObject
_cachedFloatingImageBuffer
_cachedFloatingImageSignature
setLabel:
setAllLabelsWithConfidences:
label
allLabelsWithConfidences
computeLabel
_mostLikelyLabel
_allLabelsWithConfidences
setAgeCategory:
setGenderCategory:
setEyesCategory:
setSmilingCategory:
setFaceHairCategory:
setHairColorCategory:
setBaldCategory:
setGlassesCategory:
ageCategory
genderCategory
eyesCategory
smilingCategory
faceHairCategory
hairColorCategory
baldCategory
glassesCategory
_ageCategory
_genderCategory
_eyesCategory
_smilingCategory
_faceHairCategory
_hairColorCategory
_baldCategory
_glassesCategory
mHumanDetectorAlgorithmImpl_
objectType
setObjectType:
setBounds:
center
setCenter:
_center
_bounds
removeAllObjects
hasWarnings
recordWarnings:
_warnings
indexOfObject:
dateFromString:
timeIntervalSince1970
getHostTimeInNanos
createErrorWithCode:andMessage:
freeVImageBuffer:
parseExifTimestamp:
createSingleNetworkPlanFromResourceName:usingProcessingDevice:espressoContext:espressoPlan:espressoNetwork:error:
faceAttributeInputImageSize
setFaceAttributes:
tearDownEspressoContext:andPlan:
_mEspressoContext
_mEspressoPlan
_mEspressoNetwork
computeSharpnessQualityForImage:result:
computeImageQuality:forCriteria:error:
enableMontrealAndReturnError:
feedForwardEspressoBufferForNetwork:fromBufferWithName:toBufferWithName:firstFrame:error:
defaultCStringEncoding
setWithCapacity:
initWithUTF8String:
_calculateImageDescriptors:regionOfInterest:warningRecorder:canceller:descriptorBuffer:debugIntermediatesDumpPath:outputDebugDictionary:error:
computeLabelsAndConfidence:usingDescriptorBuffer:populateLabelsAndConfidence:options:metalContext:error:
computeImageDescriptorsWithImage:regionOfInterest:usingDescriptorProcessor:tileCount:augmentationMode:scalingImage:resultantDescriptorBuffer:debugIntermediatesDumpPath:outputDebugDictionary:options:metalContext:canceller:error:
convertRelationships:toStdRelationships:
mDescriptorProcessor
mClassifier
_blacklistedIdentifiers
resourcePath
setACBSBarcodeInfo:
symbology
barcodeDescriptor
acbsBarcodeInfo
payloadStringValue
initWithRequestRevision:symbology:descriptor:topLeft:bottomLeft:bottomRight:topRight:
initWithRequestRevision:symbology:descriptor:boundingBox:
_cachedPayloadStringValue
_symbology
_barcodeDescriptor
_acbsBarcodeInfo
sortedArrayUsingSelector:
_allBarcodeSymbologies
componentsJoinedByString:
unsignedCharValue
initWithPayload:symbolVersion:maskPattern:errorCorrectionLevel:
initWithPayload:isCompact:layerCount:dataCodewordCount:
initWithPayload:isCompact:rowCount:columnCount:
locateMode
initWithKeyOptions:valueOptions:capacity:
_ACBarcodeRecognizerLocateMode
symbologies
barcodeSymbologyForACBSBarcodeType:
_getCornerPointsFromCodeLocationPoints:bottomLeft:topLeft:topRight:bottomRight:
newBarcodeObservationForACBSBarcodeInfo:imageWidth:imageHeight:roiCroppingPixelRect:scanConfidence:requestRevision:error:
_createACBSConfigAndReturnError:
_barcodesDetectedInImageBuffer:usingACBSConfig:requestRevision:error:
ACBSBarcodeTypeForBarcodeSymbology:
supportedSymbologies
availableLocateModes
_newVNBarcodeSymbologyQRDescriptorForACBSBarcodeInfo:
_newVNBarcodeSymbologyAztecDescriptorForACBSBarcodeInfo:
_newVNBarcodeSymbologyPDF417DescriptorForACBSBarcodeInfo:
setSymbologies:
setLocateMode:
_symbologies
_locateMode
releaseAllTrackers
numberWithInteger:
_getTracker:
_createTracker:type:options:error:
_maximumTrackersOfType:
releaseManager
_trackerTypeToClassDictionary
_trackerClassToNameMapTable
_liveTrackerCounter
_trackingProcessingQueue
_trackersCollectionManagementQueue
_liveTrackerCounterLimit
_trackers
initWithVImage:externalImageId:andExifTimestampValue:error:
initWithCVPixelBufferImage:externalImageId:andExifTimestampValue:error:
initWithVImage:externalImageId:andExifTimestampString:error:
initWithCVPixelBufferImage:externalImageId:andExifTimestampString:error:
image
imageCVPixelBuffer
imageFilePath
setImageFilePath:
freeImageInDealloc
setFreeImageInDealloc:
externalImageId
_freeImageInDealloc
_image
_imageCVPixelBuffer
_imageFilePath
_externalImageId
_exifTimestamp
detectorPreferredImageSize
_actualSizeForDesiredSize:ofSourceImageWidth:height:
detectorWantsAnisotropicScaling
_createScaledImagePixelBufferFromCropRect:ofImageBuffer:inPixelFormat:forDetectorInputImageSize:usingAnisotropicScaling:options:error:
_createScaledImagePixelBufferFromImageBuffer:inPixelFormat:forDetectorInputImageSize:usingAnisotropicScaling:options:error:
detectorExecutionTimeInterval
initWithObjects:
setDetectorPreferredImageSize:
setDetectorWantsAnisotropicScaling:
setDetectorExecutionTimeInterval:
_detectorPreferredImageSize
_detectorWantsAnisotropicScaling
_detectorExecutionTimeInterval
cascadeStepCount
refineMouthRegion
refineLeftEyeRegion
refineRightEyeRegion
performBlinkDetection
_determineFacesToProcessFrom:outputFacesThatNeedNoProcessing:outputFacesThatNeedAlignment:outputFacesThatNeedLandmarks:
setCascadeStepCount:
setRefineMouthRegion:
setRefineLeftEyeRegion:
setRefineRightEyeRegion:
setPerformBlinkDetection:
_cascadeStepCount
_refineMouthRegion
_refineLeftEyeRegion
_refineRightEyeRegion
_performBlinkDetection
initWithImageDescriptor:type:requestRevision:
initWithRawColorGaborDescriptor:
serializeAsVNImageprintStateAndReturnError:
initWithState:startingAtByteOffset:error:
serializedLength
distanceFromDescriptor:
serializeStateIntoData:startingAtByteOffset:error:
dataWithLength:
isSerializedImageprintCompatibleWithCurrentVersion:
serializeStateAndReturnError:
initWithState:error:
distanceToImageprint:error:
setDescriptor:
setType:
_descriptor
initWithBytes:length:
setLandmarkPoints3d:
setPoseData:
mCameraCalibrationMatrix
registryID
allDevices
labelsAndConfidence
setDescriptorData:
setElementCount:
setLengthInBytes:
setLabelsAndConfidence:
confidenceScoreType
setDistanceMode:
_labelsAndConfidence
_descriptorData
_elementCount
_lengthInBytes
_confidenceScoreType
_distanceMode
sortUsingSelector:
allScorePropertyNames
_scoresDictionary
initWithRequestRevision:overallAestheticScore:wellFramedSubjectScore:wellChosenBackgroundScore:tastefullyBlurredScore:sharplyFocusedSubjectScore:wellTimedShotScore:pleasantLightingScore:pleasantReflectionsScore:harmoniousColorScore:livelyColorScore:pleasantSymmetryScore:pleasantPatternScore:immersivenessScore:pleasantPerspectiveScore:pleasantPostProcessingScore:noiseScore:failureScore:pleasantCompositionScore:interestingSubjectScore:intrusiveObjectPresenceScore:pleasantCameraTiltScore:lowKeyLightingScore:
wellChosenSubjectScore
aestheticScore
wellFramedSubjectScore
wellChosenBackgroundScore
tastefullyBlurredScore
sharplyFocusedSubjectScore
wellTimedShotScore
pleasantLightingScore
pleasantReflectionsScore
harmoniousColorScore
livelyColorScore
pleasantSymmetryScore
pleasantPatternScore
immersivenessScore
pleasantPerspectiveScore
pleasantPostProcessingScore
noiseScore
failureScore
pleasantCompositionScore
interestingSubjectScore
intrusiveObjectPresenceScore
pleasantCameraTiltScore
lowKeyLightingScore
_aestheticScore
_wellFramedSubjectScore
_wellChosenBackgroundScore
_tastefullyBlurredScore
_sharplyFocusedSubjectScore
_wellTimedShotScore
_pleasantLightingScore
_pleasantReflectionsScore
_harmoniousColorScore
_livelyColorScore
_pleasantSymmetryScore
_pleasantPatternScore
_immersivenessScore
_pleasantPerspectiveScore
_pleasantPostProcessingScore
_noiseScore
_failureScore
_pleasantCompositionScore
_interestingSubjectScore
_intrusiveObjectPresenceScore
_pleasantCameraTiltScore
_lowKeyLightingScore
setFaceRegionMap:
mFaceRegionMapAlgorithmImpl
encodeInt32:forKey:
decodeInt32ForKey:
scanString:intoString:
scanDouble:
vn_encodeCGAffineTransform:forKey:
vn_decodeCGAffineTransformForKey:
vn_encode3x3Matrix:forKey:
vn_decode3x3MatrixForKey:
vn_encode4x4Matrix:forKey:
vn_decode4x4MatrixForKey:
containsValueForKey:
_uuid
excludesBoundingBoxFromCoding
observationWithBoundingBox:
setBoundingBoxFromQuadrilateralPointsAtTopLeft:topRight:bottomRight:bottomLeft:
setIdentifier:
_boundingBox
_identifier
initForReadingFromData:error:
setClass:forClassName:
error
finishDecoding
_serializeVersion1StateAndReturnError:
_serializeVersion2StateAndReturnError:
platform
profile
setFaceprint:
setKey:
setPlatform:
setProfile:
faceprintInputPath
setFaceprintInputPath:
_platform
_profile
_key
_faceprintInputPath
landmarkPoints3d
poseData
faceIdConfidence
faceRegionMap
faceAttributes
faceLandmarkSegments
alignedMeanShape
landmarkScore
faceJunkinessIndex
faceOrientationIndex
setAlignedMeanShape:
setLandmarkPoints:
setTorsoprint:
setFaceTorsoprint:
setFaceLandmarkSegments:
setHasBBoxBeenAligned:
setAlignedBoundingBox:
setAlignedRotationAngle:
setLandmarkScore:
setFaceIdConfidence:
setFaceJunkinessIndex:
setFaceOrientationIndex:
faceObservationWithRequestRevision:boundingBox:andAlignedBoundingBox:
faceObservationWithRequestRevision:boundingBox:faceprint:
initWithPointsData:pointCount:userFacingBBox:alignedBBox:landmarkScore:
faceObservationWithBoundingBox:andAlignedBoundingBox:
faceObservationWithBoundingBox:faceprint:
faceObservationWithRequestRevision:boundingBox:roll:yaw:
computeYawPitchRollFromPoseMatrix:outputYaw:outputPitch:outputRoll:
expressionsAndConfidence
nameConfidence
landmarks3d
pose
getComputedRectifyingTransform:
blinkScore
alignedBoundingBoxAsCGRect
expressionsAndDetections
roll
setLandmarks:
_cachedLandmarks
_cachedLandmarks3d
_faceRegionMap
_faceAttributes
_faceTorsoprint
_faceLandmarkSegments
_landmarkScore
_isBlinking
_blinkScore
_expressionsAndScores
_faceJunkinessIndex
_faceOrientationIndex
_hasBBoxBeenAligned
_alignedRotationAngle
_faceIdConfidence
_roll
_yaw
_landmarks
_landmarkPoints
_landmarkPoints3d
_poseData
_alignedMeanShape
_faceId
_alignedBoundingBox
referenceImageSignature
alignmentTransform
setAlignmentTransform:
_referenceImageSignature
_floatingImageSignature
_alignmentTransform
warpTransform
_warpTransform
setBlurScore:
setExposureScore:
blurScore
exposureScore
_blurScore
_exposureScore
setImageprint:
initWithUUIDString:
imageprint
observationWithImageprint:error:
calculateDistanceFromImageprintObservation:
isImageprintValid
initWithRawImageprintDescriptor:
rawImageprintDescriptor
imageprintValid
imageprintVersion
_imageprintValid
_imageprint
_imageprintVersion
blurMeasure
brightness
initWithRequestRevision:boundingBox:confidence:labels:
labels
_labels
initWithRequestRevision:featureValue:
featureValue
_featureValue
createCVPixelBufferRefFromDictionaryRepresentation:
createDictionaryRepresentationOfCVPixelBuffer:
computeHashForCVPixelBuffer:
isCVPixelBuffer:equalToCVPixelBuffer:
initWithTopLeft:bottomLeft:bottomRight:topRight:
initWithBoundingBox:
_topLeft
_bottomLeft
_bottomRight
_topRight
setTransform:
setAngle:
transform
angle
_transform
_angle
suggestedIdsForRepresentative
representativenessById
setRepresentativenessById:
_shouldUpdateRepresentative
_objects
_clusterId
_totalObjectCount
_suggestedIdsForRepresentative
_representativenessById
clusters
suggestionsForCluster
clusterState
clusteredFaceIds
groupedClusteredFaceIdsForCluster
distancesById
setSuggestionsForCluster:
setClusterState:
setClusteredFaceIds:
setGroupedClusteredFaceIdsForCluster:
setDistance:
setDistancesById:
_clusters
_suggestionsForCluster
_clusterState
_clusteredFaceIds
_groupedClusteredFaceIdsForCluster
_distance
_distancesById
sceneprintCurrentVersion
isMajorVersion:equalToMajorVersion:
isMinorVersion:equalToMinorVersion:
sceneprintVersion
_sceneprints
_sceneprintVersion
smartCamprintCurrentVersion
setSmartCamprints:
smartCamprintVersion
smartCamprints
observationWithSmartCamprints:
_smartCamprints
_smartCamprintVersion
_allImageIdentifiers
_bestImageIdentifiers
_allImageStats
_coverImageIdentifier
_isAction
_isPortrait
characterBoxes
_characterBoxes
_text
scanFloat:
_determineFacesToProcessFrom:outputFacesThatNeedNoProcessing:outputFacesThatNeedProcessing:outputFacesThatNeed2DLandmarks:
isFullCoverageRegionOfInterest
indexesOfObjectsPassingTest:
objectsAtIndexes:
validateOptionalFaceObservations:forRequest:error:
_faceObservationsForRegionOfInterestContainingFaceObservations:
validateRequiredFaceObservations:error:
validatedInputFaceObservationsClippedToRegionOfInterest:error:
regionOfInterestNonIntegralPixelRectForWidth:height:
_regionOfInterest
_inputFaceObservations
componentsSeparatedByString:
_isSeparatedString:equalToString:atIndex:usingSeparator:
_smartCamCombinedModelImageSaliencyObservationsForRevision:performedInContext:error:
groupImageprints:withOptions:error:
_concatenateFaceprintImageDescriptorBuffer:withFaceprints:forIdentityWithSerialNumber:faceprintLabels:
initWithFaceIDModel:maximumElementsPerID:personUniqueIdentifierToSerialNumberMapping:
sortUsingComparator:
removeObjectsInRange:
_personPredictionsForFace:withDescriptor:limit:faceIDCanceller:error:
decodeIntForKey:
encodeInt:forKey:
dictionaryRepresentation
_serialNumberToPersonUniqueIdentifierMapTable
_faceIDModel
_maximumElementsPerID
initWithRequestRevision:espressoOutputBufferWidth:espressoOutputBufferHeight:espressoOutputBufferChannels:espressoOutputBufferData:
setEspressoOutputBufferWidth:
setEspressoOutputBufferHeight:
setEspressoOutputBufferChannels:
setEspressoOutputBufferData:
espressoOutputBufferWidth
espressoOutputBufferHeight
espressoOutputBufferChannels
espressoOutputBufferData
getFaceLanmarksSegmenterInputImageSize:forRequestRevision:error:
getNumberOfSupportedFaceLandmarkSegments:forRequestRevision:error:
_faceLandmarkIndexToFlagMap
_argmax3:index:segmentedImageSize:numberOfSegments:
createImageOfFaceLandmarkSegments:error:
_espressoOutputBufferWidth
_espressoOutputBufferHeight
_espressoOutputBufferChannels
_espressoOutputBufferData
persistentDomainForName:
burstId
createDirectoryAtPath:withIntermediateDirectories:attributes:error:
defaultVersionString
setVersionString:
versionString
setActionClassifier:
setStatsByImageIdentifier:
setClusterByImageIdentifier:
setBestImageIdentifiersArray:
setBurstCoverSelection:
stringWithCString:encoding:
setOverrideImage:
setOverrideProps:
burstDocumentDirectory
setBurstLogFileName:
burstLogFileName
fileExistsAtPath:
imageProps
imagePixelBuffer
_reorientFaceRects:imageSize:orientation:
_reorientROIRect:imageSize:orientation:
setImagePixelBuffer:
completionBlock
initWithImageData:dict:identifier:imageProps:completionBlock:
processClusters:
overrideImage
overrideProps
statsByImageIdentifier
_addImageInternal:properties:identifier:imageProps:completionBlock:
computeEmotion:
actionClassifier
countForObject:
computeCameraTravelDistance
computeBeginningVsEndAEMatrixDiffVsAverageAdjacent
computeActionSelectionThreshold
lastObject
computeAllImageScores
performEmotionalRejectionOnCluster:
findBestImage:useActionScores:
selectCoverPhotoFromMultiple:burstSize:
addItemsFromCluster:
secondsSinceStart
maxNumPendingFrames
enableAnalysis
enableFaceCore
dummyAnalysisCount
enableDumpYUV
burstCoverSelection
clusterByImageIdentifier
bestImageIdentifiersArray
addImage:properties:identifier:completionBlock:
imageClusterForIdentifier:
allImageClusters
isFaceDetectionForced
faceIDCounts
setFaceIDCounts:
faceAnalysisContext
setFaceAnalysisContext:
clusterArray
setClusterArray:
setMaxNumPendingFrames:
setEnableAnalysis:
setDummyAnalysisCount:
setEnableFaceCore:
setEnableDumpYUV:
setBurstId:
loggingCallback
_burstAnalyzerDispatchQueue
_pendingFramesSemaphore
_yuvdumpDispatchQueue
_temporalOrder
_maxNumPendingFrames
_enableAnalysis
_dummyAnalysisCount
_enableFaceCore
_enableDumpYUV
_burstLogFileHandle
_currentClusterIndexToProcess
_clusterArray
_faceAnalysisContext
_faceIDCounts
_burstId
_loggingCallback
_overrideImage
_overrideProps
_statsByImageIdentifier
_clusterByImageIdentifier
_burstLogFileName
_actionClassifier
_burstCoverSelection
_bestImageIdentifiersArray
_versionString
initWithSceneClassificationRequestRevision:detectionLevel:
newHierarchicalModelAndReturnError:
initWithArray:copyItems:
requestDetectionLevel
_addRelationships:error:
relationships
isEqualToDictionary:
customHierarchyWithAdditionalParent:children:error:
_hierarchicalModel_DO_NOT_ACCESS_DIRECTLY
_additionalRelationships
_sceneClassificationRequestRevision
_sceneClassificationRequestDetectionLevel
createHierarchicalModelForMultiDetectorModel:requestRevision:error:
useTimestampAdjustedDistances
computeTotalDistanceForDescriptorDistance:timestampDiff:useTimestampAdjustment:
setPreviousLeafId:
setNextLeafId:
setPreviousLeafDescriptorDistance:
setNextLeafDescriptorDistance:
setPreviousLeafTimestampDistance:
setNextLeafTimestampDistance:
setPreviousLeafTotalDistance:
setNextLeafTotalDistance:
clusterSplitDistanceType
getDistanceForClusterNode:splitDistanceType:
computeClusteringIntoKGroups:orUsingDistanceThreshold:forHierarchicalTree:context:
naturalClusteringDistanceThreshold
computeTimestampAdjustedDistanceForBaseDistance:andTimestampDiff:
quality
pathForEspressoResource:ofType:error:
createSingleNetworkPlanFromResourceName:usingProcessingDevice:explicitNetworkLayersStorageType:espressoContext:espressoPlan:espressoNetwork:error:
pathForEspressoNetworkModelFileWithName:error:
renderEspressoBufferImage:intoCVPixelBuffer:error:
pixelValueSizeInBytesForBuffer:error:
pathForEspressoResourceWithFilename:error:
createCVPixelBufferWithPixelFormat:fromImageInEspressoBuffer:error:
createCGImage:fromRect:
newCGImageFromCIImage:error:
newCIImageFromVImage:withType:error:
newVImageBufferFromCIImage:error:
setWithArray:
contentsOfDirectoryAtPath:error:
removeItemAtPath:error:
dumpDebugIntermediatesWithImageBuffer:lumaIntermediate:alignedBBoxInLumaIntermediateCoordinates:meanShapeInLumaIntermediate:landmarkPointsInLumaIntermediate:
_loadEspressoModelWithConfigurationOptions:error:
mFaceLandmarkAlgorithmImpl
mFaceLandmarkMouthRefinerImpl
mFaceLandmarkRightEyeRefinerImpl
mFaceLandmarkLeftEyeRefinerImpl
mCoreLandmarkModelFileHandle
_setDefaultParameters
_initMemory:height:nscales:error:
_setupPipelines
_setupBufferAndReturnError:
commandBuffer
commit
waitUntilCompleted
_zeroFlowWithCommandBuffer:uv_tex:
_createImagePyramidWithCommandBuffer:in_pixelbuf:I_idx:error:
_computeOpticalFlow
newFunctionWithName:
newComputePipelineStateWithFunction:error:
threadExecutionWidth
newBufferWithLength:options:
newTextureViewWithPixelFormat:
texture2DDescriptorWithPixelFormat:width:height:mipmapped:
setUsage:
newTextureWithDescriptor:
_computeFeaturesWithCommandBuffer:in_tex:out_tex:
_computeFeaturesDerivativesWithCommandBuffer:in_tex:out_tex:
_doSolverWithCommandBuffer:scale:scale_factor:in_uv_tex:out_uv_tex:out_w_tex:
_doNLRegularizationWithCommandBuffer:in_uv_tex:join_tex:w_tex:out_uv_tex:
computeCommandEncoder
setComputePipelineState:
setTexture:atIndex:
maxTotalThreadsPerThreadgroup
dispatchThreadgroups:threadsPerThreadgroup:
endEncoding
replaceRegion:mipmapLevel:withBytes:bytesPerRow:
_downscale2XWithCommandBuffer:in_u32_alias_tex:out_u32_alias_tex:
setBuffer:offset:atIndex:
setBytes:length:atIndex:
initWithMetalContext:width:height:nscales:error:
reset
setPreset:
setOutputUV:error:
estimateFlowFromReference:target:
estimateFlowStream:
isValid
needConversionBGRA2YUVA
setNeedConversionBGRA2YUVA:
nscales
streamFrameCount
nwarpings
setNwarpings:
useNonLocalRegularization
setUseNonLocalRegularization:
nlreg_radius
setNlreg_radius:
nlreg_padding
setNlreg_padding:
nlreg_sigma_l
setNlreg_sigma_l:
nlreg_sigma_c
setNlreg_sigma_c:
nlreg_sigma_w
setNlreg_sigma_w:
_mtlContext
_computePipelines
_maxThreadExecutionWidth
_pyramid_size
_I_tex
_I_u32_alias_tex
_G0_pxbuf
_G1_pxbuf
_G0_tex
_G1_tex
_C0_pxbuf
_C1_pxbuf
_C0_tex
_C1_tex
_Adiagb_buf
_Ixy_buf
_w_pxbuf
_w_tex
_uv_pxbuf
_uv_tex
_uv_u32_alias_tex
_current_frame_index
_streamFrameCount
_uv_tex_user_ref
_isValid
_needConversionBGRA2YUVA
_useNonLocalRegularization
_width
_height
_nscales
_nwarpings
_nlreg_radius
_nlreg_padding
_nlreg_sigma_l
_nlreg_sigma_c
_nlreg_sigma_w
fileURLWithPath:isDirectory:
classifyImageWithDescriptors:usingImageClassifier:andMinConfidenceForClassification:outputDebugDictionary:options:metalContext:error:
mJunkDescriptorImpl
mJunkClassifierImpl
_perMeshPtr
_calculateTorsoBBoxFromFaceBBox:insideImageWithSize:torsoBBox:error:
_torsoprintInputImageSize
_torsoprintDescriptorSize
_minimumTorsoInsideInputImageThreshold
setBlurDeterminationMethod:
maximumIntermediateSideLength
blurDeterminationMethod
setMaximumIntermediateSideLength:
_maximumIntermediateSideLength
_blurDeterminationMethod
faceBoundingBox
initWithFaceBoundingBox:
setPointCount:
_faceBoundingBox
_pointCount
pointAtIndex:
normalizedPoints
pointsInImageOfSize:
initWithFaceBoundingBox:points:pointCount:
setPoints:
_sizedPointsCache
_points
userFacingBBox
alignedBBox
pointsData
createPointArray:count:
isUserFacingBBoxEquivalentToAlignedBBox
setPointsData:
setAlignedBBox:
setUserFacingBBox:
_pointsData
_alignedBBox
_userFacingBBox
allPoints
faceContour
leftEyebrow
rightEyebrow
nose
noseCrest
medianLine
outerLips
innerLips
leftPupil
rightPupil
_allPoints
_faceContour
_leftEye
_rightEye
_leftEyebrow
_rightEyebrow
_nose
_noseCrest
_medianLine
_outerLips
_innerLips
_leftPupil
_rightPupil
_validateFaceObservations:withMinimumCount:forOptionalRequest:error:
getNSUIntegerValue:forKey:inOptions:error:
validateClassArray:named:hasElementsAncestoredFromClass:requiredMinimumCount:allowedMaximumCount:error:
validateRequiredFaceObservations:forRequest:error:
validateOptionalFaceObservations:error:
validateRequiredClusterIDs:error:
requiredArrayForKey:inOptions:withElementsOfClass:error:
requiredInputFacesArrayInOptions:error:
initWithData:forKey:
_imageprintDescriptor
_imageprintType
longValue
shortValue
colorGaborDescriptor
sceneClassifierDescriptor
imageRegistrationDescriptor
previousLeafId
nextLeafId
nextLeafDescriptorDistance
previousLeafDescriptorDistance
nextLeafTimestampDistance
previousLeafTimestampDistance
nextLeafTotalDistance
previousLeafTotalDistance
rawColorGaborDescriptor
_quality
_nextLeafDescriptorDistance
_previousLeafDescriptorDistance
_nextLeafTotalDistance
_previousLeafTotalDistance
_descriptorId
_colorGaborDescriptor
_sceneClassifierDescriptor
_imageRegistrationDescriptor
_previousLeafId
_nextLeafId
_nextLeafTimestampDistance
_previousLeafTimestampDistance
_rawColorGaborDescriptor
dataUsingEncoding:allowLossyConversion:
initWithData:encoding:
initWithMLModel:error:
modelDescription
setupInputImageFromModelDescription:
predictedFeatureName
objectBoundingBoxOutputDescription
outputDescriptionsByName
predictedProbabilitiesName
inputDescriptionsByName
imageConstraint
pixelsWide
pixelsHigh
pixelFormatType
inputImageKey
featureValueWithPixelBuffer:
initWithDictionary:error:
model
predictionFromFeatures:options:error:
modelForMLModel:error:
predictWithCVPixelBuffer:options:error:
setModel:
modelType
setModelType:
setInputImageKey:
predictedFeatureKey
setPredictedFeatureKey:
predictedProbabilitiesKey
boundingBoxOutputDescription
inputImageWidth
inputImageHeight
inputImageFormat
_uuidStringForCacheIdentifier
_modelType
_inputImageFormat
_model
_inputImageKey
_predictedFeatureKey
_predictedProbabilitiesKey
_boundingBoxOutputDescription
_inputImageWidth
_inputImageHeight
featureValueForName:
dictionaryValue
coordinatesFeatureName
confidenceFeatureName
multiArrayValue
shape
strides
labelNames
imageBufferValue
initWithOptions:model:error:
timeStamp
setTimeStamp:
_timeStamp
setCIContext:
resizeImage:byX:andY:
resizeImage:to:
getCGImageAsVImageBuffer:
getCGImageFromCIImage:
resizeImage:toWidth:andHeight:
setBurstImages:
setImageProps:
computeMergeCost:::
setCompletionBlock:
_imagePixelBuffer
_completionBlock
_burstImages
_imageProps
advise:
setResourcePath:
initWithMappedModel:
m_impl
computeDescriptorForImageData:context:error:
computeConvnetDescriptorForImageData:context:error:
computeRegistrationFeaturesForImageData:context:error:
initWithImageData:context:error:
computeQualityForImageData:andQualityCriteria:context:error:
numberWithLong:
numberWithShort:
timerMode
computeFinalDescriptorBasedDistanceForColorDistance:andSceneClassifierDistance:
initWithImageData:andCustomQualityScore:context:error:
_internalNonSerializedDescriptorId
initWithSequenceSummarizationObservation:frameTimeStamp:completionHandler:
setUseCenterTileOnly:
frameTimeStamp
initWithSequenceSummarizationObservation:frameTimeStamp:
setFrameTimeStamp:
_frameTimeStamp
_burstSet
computeTransform:forRegisteringImageSignature:withSignature:andOptions:minimumOverlap:error:
copyItemAtPath:toPath:error:
moveItemAtPath:toPath:error:
printDebugInfoFor:imageBuffer:originalImageLumaCrop:faceBBoxInLumaCropCoordinates:magnifiedBBoxInLumaCropCoordinates:
useLowPriorityMode
isFaceprinterCompatibleWithFaceprinterCreatedWithOptions:
m_FaceDescriptorImpl
m_FaceFrontalizerImpl
m_DescriptorAugmenter
m_FaceFrontalizerWorkingBuffer
m_FaceFrontalizerImageBuffer
m_RequiredImageSize
_useLowPriorityMode
_length
setReturnAllResults:
_returnAllResults
_addClassificationObservationsForSceneRequestRevisionNumber:toArray:fromSceneLabelsAndConfidences:
_createScaledImagePixelBufferFromCropRect:ofImageBuffer:forNetworkInputImageSize:usingAnisotropicScaling:options:error:
_createScaledImagePixelBufferFromImageBuffer:forNetworkInputImageSize:usingAnisotropicScaling:options:error:
_observationsForScene:sceneprint:aesthetics:saliencyHeatMap:of32BGRAImageInPixelBuffer:withOptions:originalImageSize:regionOfInterest:warningRecorder:error:
_defaultSceneClassificationHierarchicalModel
_imageAnalyzer
setFlag:atIndex:
clearFlag:atIndex:
releaseAllocations
checkFlag:atIndex:
copyFlagValue:toTarget:atIndex:
resetBoxBounds
makeAllocationsForWidth:
floatVectorSumProd
setFloatVectorSumProd:
pulseVectorHeightCharBox
setPulseVectorHeightCharBox:
pulseVectorHeightCharBoxAdaptive
setPulseVectorHeightCharBoxAdaptive:
charBoxFlags
setCharBoxFlags:
charboxROIFullVectorRowStart
setCharboxROIFullVectorRowStart:
charboxROIFullVectorHeight2
setCharboxROIFullVectorHeight2:
allocationSize
setAllocationSize:
mTop
setMTop:
mBottom
setMBottom:
bTop
setBTop:
bBottom
setBBottom:
posUL
setPosUL:
posLL
setPosLL:
posUR
setPosUR:
posLR
setPosLR:
medianHeightTop
setMedianHeightTop:
medianHeightBottom
setMedianHeightBottom:
loopBigBox
setLoopBigBox:
loopBigBoxPrev
setLoopBigBoxPrev:
filterWalkUpDownCount
setFilterWalkUpDownCount:
setDebugOut:
setDebugFilename:
setComputeZCVectorHighProbability:
setMidRow:
setMinHeight:
setMaxHeight:
setStartMaxFind:
setStopMaxFind:
setMmHeightCard:
setMmWidthCard:
setPixelHeightCard:
setPixelWidthCard:
setMinBoxWidth:
setMaxBoxWidth:
setStartNormal:
setStopNormal:
setStartSensitized:
setStopSensitized:
midRow
debugOut
debugFilename
examinePulseWindow:prodBoostNormalized:pwContext:minHeight:maxHeight:thresholdSet:
_allocateSumDerivVectors:size:
_computeColumnSumsOverRange:sampleImageAddress:rowSumOut:rowDerivOut:
_computeProdBoostNormalizedResult:size:binOverride:
generatePulses:minHeight:maxHeight:thresholdSet:prodBoostNormalized:pulseVectorFlag:
_freeSumDerivVectors:
minHeight
maxHeight
profileNormal
computeNumCropCols:width:start:
setIi:
setProfileNormal:
_allocateVImageWithWidth:height:rowBytes:imageOut:
_freeVImage:
createLumaImage:lumaImage:numCropRows:rowStartLocation:
createLumaImageAlternative:lumaImageAlternative:numCropRows:rowStartLocation:
computeMainStub:numCropRows:numCropColsOut:maxY:start:
allocateColorProfileContext:width:height:rowBytes:
getVotingHistogram:colorProfileContext:startCC:rowStartLocation:
getLumaHistogram:startCC:colorProfileContext:
determineColorProfileType:
generateHistogramBounds:rgbVector2Ref:numPixels1:numPixels2:minMaxRGB:lowHighRGB:
_generateBinarizationForImage:textOut:firstOrSecondPassIndicator:minMaxRGB:
freeColorProfileContext:
pixelHeightCard
pixelWidthCard
mmHeightCard
mmWidthCard
maxBoxWidth
groupEndpoints:finalCharBoxCoordCount:
computeDependentTopAndBottomComponents:finalCharBoxCoordCount:
computeIndependentTopAndBottomComponents:finalCharBoxCoordCount:
_getFilterResultOut:defaultRows:bufferHeight:minHeight:maxHeight:startRange:stopRange:startMaxFind:stopMaxFind:bytesPerRow:thresholdSet:sampleImageAddressRef:sampleImageFloatAddressRef:pulseVectorFlag:
minBoxWidth
computePulseVectorSum:start:stop:imageHeight:imageWidth:bottomHeight:topHeight:
tightenBox:startTop:startBottom:startPosition:stopPosition:imageHeight:halfWalk:
startSensitized
stopSensitized
_extractCharBoxCuts:heightConstraint:medianHeightTopVector:medianHeightBottomVector:isAdaptive:
computeZCVectorHighProbability
charBoxesFromTextBoxes:bigBoxes:ccYTopLocationsForSort:ccYBottomLocationsForSort:
extractCharBoxHeightInfo:ccCharBoxesFiltered:ccYTopLocationsForSort:ccYBottomLocationsForSort:aggregateGreenBoxesForStubCount:imageWidth:
calculateSumProd:prodROIRef:prodROIRotateRef:rowStartLocation2:height:
calculateSumProdAlternative:prodROIRef:prodROIRotateRef:rowStartLocation2:height:
startMaxFind
stopMaxFind
_generateZeroCrossingVector:zcVectorFlag:width:
extractBoxesForStub:bigBoxesAdapt:countBigBox:rowStartLocation2:rowStopLocation2:heightConstraint:imageWidth:height:ccCharBoxesAggr:ccCharBoxesFiltered:useLowLightEnhancement:
fillInOneVector:checkFlag:checkHeight:
_generateCRCharBoxInformation_zcFinalVectorOriginate:textOut:adaptOut:bigBoxes:bigBoxesAdapt:ccCharBoxesAggr:ccCharBoxesFiltered:height:rowStartLocation2:rowStopLocation2:singleVotingImageAddressRef:countBigBox:filterWalkUpDownCount:useLowLightEnhancement:
_generateCRCharBoxInformation_zcFinalVectorHighProbability:zcFinalRange:
_generateCRCharBoxInformation_zcFinalVectorFillIn:
_generateCRCharBoxInformation_spaceBoxRemovalMagicThresh:magicMinHeight:magicMaxHeight:rowStartLocation2:magicThresh:magicThreshPrev:useLowLightEnhancement:
_generateCRCharBoxInformation_spaceBoxRemovalHistogram:zcStartLeft:zcStopRight:rowStartLocation2:lowHighRGB:histCompliancePercent:varSpaceBox:
_generateCRCharBoxInformation_spaceBoxRemovalTruthFilter:magicThresh:zcStartLeft:zcStopRight:isSpaceBoxAccepted:histCompliancePercent:useLowLightEnhancement:
_generateCRCharBoxInformation_spaceBoxRemovalTightenBox:singleVotingImageAddressRef:adaptOut:textOut:zcStartLeft:zcStopRight:finalCoordinatesForStub:finalCoordinatesForStubRevised:finalCharBoxCoordCount:useLowLightEnhancement:
_generateCRCharBoxInformation_TrackBox:finalCharBoxCoordCount:
_generateCRCharBoxInformation_extendTextBoxes:countBigBox:rowStartLocation2:finalCharBoxCoordCount:finalCoordinatesForStubRevised:width:height:bigBoxIndicator:
startNormal
stopNormal
_getFilterResultOutBothSumDeriv:floatVectorResult:bufferHeight:minHeight:maxHeight:config:bytesPerRow:thresholdSet:sampleImageAddressRef:
_generatePulseAggregate:pulseVectorMain:pulseVectorResult:metricSelection:bufferHeight:bufferWidth:
_generateFilteredPulseVector:target:height:
generateDominantPulse:rowLocationsRef:debugOut:bufferHeight:bufferWidth:
_generatePulseAggregateSmallStubs:pulseVectorResult:bufferHeight:bufferWidth:
_allocateCRCharBoxContext:
initializeForImage:
_generateVotingImage:votingImage:useLowLightEnhancement:
_generatePulseVectorOutputs:votingImage:rowLocationsRef:
_generateAndApplyColorProfileForImage:votingImage:textOut:minMaxRGB:lowHighRGB:numCropRows:rowStartLocation:rowStopLocation:sumTextOutFirstPass:useLowLightEnhancement:
_generateSmoothedImage:uImage:
_generateBoxes:pulseVector:uImage:countBigBoxOut:bigBoxes:bigBoxesA:useLowLightEnhancement:
_generateAdaptiveBinarization:adaptImage:useLowLightEnhancement:
_generateCC:ccBigBoxes:textOut:countBigBox:bufferHeight:
_generateCRCharBoxInformation:inputImage:singleVotingImageAddressRef:bigBoxes:bigBoxesAdapt:textOut:adaptOut:lowHighRGB:countBigBox:useLowLightEnhancement:
_freeCRCharBoxContext
textBoxesForBuffer:error:
removeObjectsInArray:
charBoxContext
setCharBoxContext:
debugMatlab
setDebugMatlab:
_getFilter_callCount
_computeZCVectorHighProbability
_profileNormal
_debugMatlab
_debugOut
_midRow
_minHeight
_maxHeight
_startMaxFind
_stopMaxFind
_mmHeightCard
_mmWidthCard
_pixelHeightCard
_pixelWidthCard
_minBoxWidth
_maxBoxWidth
_startNormal
_stopNormal
_startSensitized
_stopSensitized
_charBoxContext
_debugFilename
setName:
setCountLimit:
setTotalCostLimit:
setObject:forKey:cost:
observationsForKey:
setObservations:forKey:
espressoModelFileNameForConfigurationOptions:
espressoModelInputImageDimensionsBlobNameForConfigurationOptions:
getWidth:height:ofEspressoModelNetworkBlobNamed:error:
espressoModelNetworkLayersStorageTypeForConfigurationOptions:
espressoContext
espressoPlan
espressoNetwork
networkRequiredInputImageWidth
networkRequiredInputImageHeight
_espressoContext
_espressoPlan
_espressoNetwork
_networkRequiredInputImageWidth
_networkRequiredInputImageHeight
doesAreaOverlapBetweenRect:andOtherRect:withOverlapRatioGreaterThan:
computeNormalizedCosineDistanceOfFaceprint:toFaceprint:
doesAreaOverlapSignificantlyBetweenRect:andOtherRect:
requiredVersion
minimumAspectRatio
maximumAspectRatio
quadratureTolerance
minimumSize
minimumConfidence
_detectorOptions
maximumObservations
setMinimumAspectRatio:
setMaximumAspectRatio:
setQuadratureTolerance:
setMinimumSize:
setMinimumConfidence:
setRequiredVersion:
setMaximumObservations:
_minimumAspectRatio
_maximumAspectRatio
_quadratureTolerance
_minimumSize
_minimumConfidence
_maximumObservations
_requiredVersion
initWithConfiguration:faceModel:
useCenterTileOnly
_canceller
_useCenterTileOnly
arrayByAddingObjectsFromArray:
removeObjectIdenticalTo:
_validateAndPrepareRequests:error:
_dependancyAnalyzedRequestsForRequests:
_orderedRequestsForRequests:
_performOrderedRequests:inContext:error:
performDependentRequests:inContext:error:
_requestLock
_requestsInFlight
_requestsPending
_sequencedRequestObservations
_trackerKeys
initWithImageprintObservations:clusteringDistanceThreshold:completionHandler:
inputImageprints
_inputImageprintsCacheKey
clusteringDistanceThreshold
initWithImageprintObservations:clusteringDistanceThreshold:
setInputImageprints:
setClusteringDistanceThreshold:
_inputImageprints
_clusteringDistanceThreshold
defaultLabradorDevice
defaultMetalDevice
directLabradorDevice
initWithFaceObservations:completionHandler:
_determineFacesToProcessFrom:outputFacesThatNeedNoProcessing:outputFacesThatNeedProcessing:
initWithType:cachePath:state:threshold:
initWithType:cachePath:state:threshold:torsoThreshold:
cachePath
setCachePath:
state
setState:
torsoThreshold
setTorsoThreshold:
_cachePath
clustererQueryWithOptions:error:
clustererBuilderWithOptions:error:
initWithModel:completionHandler:
imageCropAndScaleOption
initWithModel:
setImageCropAndScaleOption:
_imageCropAndScaleOption
debugMode
performClustersPostprocessing
performSceneClassification
roiAreaThreshold
inliersRatioThreshold
numberOfKeypointsToConsider
_useTimestampAdjustedDistances
_performClustersPostprocessing
_performSceneClassification
_debugMode
_timerMode
_clusterSplitDistanceType
_roiAreaThreshold
_inliersRatioThreshold
_numberOfKeypointsToConsider
_naturalClusteringDistanceThreshold
_qualityCriteriaList
distanceToDefaultBox
initWithBox:defaultBox:confidence:scale:rotationAngle:yawAngle:
smartDistance
setBox:
setDefaultBox:
setMergesCount:
setScale:
_area
_mergesCount
_scale
_box
_defaultBox
computeBrightnessScore:onImage:error:
initWithRectangleObservation:completionHandler:
initWithRectangleObservation:
_determineFacesToProcessFrom:outputFacesThatNeedNoProcessing:outputFacesThatNeedAlignment:outputFacesThatNeedFaceprints:
dumpDebugIntermediatesWithImageBuffer:lumaIntermediate:rawBBoxInLumaIntermediateCoordinates:alignedBBoxInLumaIntermediateCoordinates:meanShapeInLumaIntermediateCoordinates:rotationAngle:
mFaceBoxPoseAlignerImpl
mFaceBoxAlignerModelFileHandle
VNTrackerOptionToTrackerType:
_visionBBoxToTrackerBBox:trackedObjects:imageSize:results:error:
_updateTrackerWithModifiedBBoxForImageBuffer:error:
mTrackerImpl
_trackedFrameNumber
_level
_lastTrackedBBox
initWithModelPath:espressoEngineID:espressoDeviceID:espressoStorageType:threshold:
setInputShape:height:
runNetwork:inputIsBGR:
processVImage:inputIsBGR:
preferredSmallSide
processCIImage:
_logitsOutputs
_offsetsOutputs
_rollOutputs
_yawOutputs
_currentNetworkWidth
_currentNetworkHeight
_defaultBoxSizes
_classifyAesthetics:generateSaliencyHeatMap:for32BGRAImageInPixelBuffer:withOptions:modelInputImageSize:originalImageSize:regionOfInterest:warningRecorder:error:
initWithOriginalRequests:
compoundResults
_cachedDependencyProcessingOrdinality
initWithOriginalRequests:requestToObservationClassMap:
initWithSubrequestsAndUniqueObservationClasses:
_requestToObservationClassMap
_initializedLKTMetalContextAndReturnError:
targetedImageSpecifier
_createLKTPixelBufferFromPixelRegionOfInterest:inImageBuffer:error:
_createLKTVectorResultPixelBufferForImageWidth:height:error:
_calculateLKTVectorResult:fromPixelBuffer:toPixelBuffer:ofWidth:height:error:
initWithRequestPerformer:imageBuffer:observationsCache:qosClass:
indexLessThanIndex:
_qosClass
_requestPerformer_DO_NOT_DIRECTLY_ACCESS
_imageBuffer_DO_NOT_DIRECTLY_ACCESS
signature
mSignature_
delegate
firstIndex
removeIndex:
intersectSet:
_requestNewIdentitySerialNumberAndReturnError:
_removeAllFaceObservationsFromIdentityWithSerialNumber:
_removeExistingFaceObservations:fromIdentityWithSerialNumber:
_uniqueFaceObservationsWithRegistrationState:forFaceObservations:ofPersonWithUniqueIdentifier:error:
_addUniqueFaceObservations:toPersonWithUniqueIdentifier:error:
_removeExistingFaceObservations:fromPersonWithUniqueIdentifier:
_removePersonWithUniqueIdentifier:
_accessToMutableFaceObservationsForPersonAtIndex:
_personUniqueIdentifiers
_personUniqueIdentifierToSerialNumberMapping
_serialNumberToFaceObservationsMapping
_availablePersonSerialNumbers
_delegate
_faceLanmarksSegmenterMaximumInputImageAspectRatio
prepareNetworkInput:fromScaledFacePixelBuffer:
initWithTargetedCVPixelBuffer:options:completionHandler:
initWithTargetedCVPixelBuffer:orientation:options:completionHandler:
initWithTargetedImageSpecifier:completionHandler:
initWithTargetedCGImage:options:completionHandler:
initWithTargetedCGImage:orientation:options:completionHandler:
initWithTargetedCIImage:options:completionHandler:
initWithTargetedCIImage:orientation:options:completionHandler:
initWithTargetedImageURL:options:completionHandler:
initWithTargetedImageURL:orientation:options:completionHandler:
initWithTargetedImageData:options:completionHandler:
initWithTargetedImageData:orientation:options:completionHandler:
initWithTargetedCVPixelBuffer:
initWithTargetedCVPixelBuffer:options:
initWithTargetedCVPixelBuffer:orientation:options:
initWithTargetedCGImage:
initWithTargetedCGImage:options:
initWithTargetedCGImage:orientation:options:
initWithTargetedCIImage:
initWithTargetedCIImage:options:
initWithTargetedCIImage:orientation:options:
initWithTargetedImageURL:
initWithTargetedImageURL:options:
initWithTargetedImageURL:orientation:options:
initWithTargetedImageData:
initWithTargetedImageData:options:
initWithTargetedImageData:orientation:options:
_targetedImageSpecifier
initWithFaceObservation:completionHandler:
initWithFaceObservation:
initWithAPI:properties:
setParameter:to:
setCurrentContext:
CVML_Error
VNClipGenerator
VNSequenceSummarizationObservation
VNClusteringLogger
VNSuggestionsLogger
VNGreedyClusteringReadOnly
VNClusteringReadOnly
VNClusteringCancelling
VNGreedyClusteringReadWrite
VNClusteringWritable
BurstFaceConfigEntry
BurstFaceScoreEntry
BurstFaceInfo
BurstImageFaceAnalysisContext
BurstActionClassifier
VNClustererContextBase
VNClustererReadOnlyContext
VNClustererModelQuerying
VNClustererReadWriteContext
VNClustererModelBuilding
VNSceneClassifier
VNDetectorIdealImageSizeProviding
NSObject
VNFaceClassifierBoostedPixelDifference
LKTMetalContext
VNError
VNAppendBurstSequenceFrameRequest
VNFaceDetectorRevision2
BurstFaceStat
NSCopying
BurstImageStat
VNSequenceRequestHandler
VNRequestWarming
VNRequestCancelling
VNImageRequestHandler
ClusteringAdditions
VNPhotosRequestHandler
VNPhotosRequestHandlerSupport
VNHomographicImageRegistrationRequest
VNSceneClassificationRequest
VNImageIdealImageSizeProviding
Revisioning
ShotflowDetector
VNBlurSignature
VNBlurMeasure
VNTrackingRequest
VNMetalContext
VNDetector
VNDetectorKeyProviding
VNObservationsCacheKeyProviding
VNRequestRevisionProviding
VNImageAnalyzerCompoundRequestConfiguration
VNImageAnalyzerCompoundRequestConfigurations
VNImageAnalyzerCompoundRequest
VNFaceRegionMap
NSSecureCoding
NSCoding
VNLabradorProcessingDevice
VNRuntimeUtilities
VNCanceller
VNFaceLandmarkDetector
VNImageBufferManager
VNImageSourceManager
VNImageBuffer
VNCreateTorsoprintRequest
VNFaceObservationAccepting
VNMutablePersonsModel
VNPersonsModelDataDelegate
VNRequest
VNWarningRecorder
VNSequencedRequestSupporting
VNSizeRange
VNSupportedImageSize
VNFaceTorsoprint
VNSegmentFaceLandmarksRequest
VNFaceTracker
VNDetectHumanRectanglesRequest
VNOpticalFlowObservation
VNFaceDetectorRevision1
VNMomentProcessor
VNMPClusteringTreeNodeWrapper
VNImageSpecifier
OptionsDictionaryCompatability
_VNPixelBufferSpecifier
_VNCGImageSpecifier
_VNCIImageSpecifier
_VNURLImageSpecifier
_VNDataImageSpecifier
SaliencyExtrema
VNImageSaliencyObservation
VNClassifyImageAestheticsRequest
VNRectangleTracker
VNBurstAnalysisResultsRequest
VNSmartCamClassifier
VNFaceExpressionDetector
VNSceneprint
BurstClusterDivider
VNDetectTextRectanglesRequest
ANFDDetectedObject
_VNPersonsModelDataSourceBasedDataProvider
VNPersonsModelFaceModelDataProvider
VNPersonsModel
VNPersonsModelInformation
VNPersonsModelConfiguration
VNPersonsModelPrediction
VNPersonsModelReadOptions
VNPersonsModelWriteOptions
VNDetectorManager
VNImageRegistrationRequest
VNFaceAttributeCategory
VNFaceAttributes
VNHumanDetector
VNEspressoFaceDetectedObject
ExternalDetectedObject
VNSmartCamprint
VNMPUtils
VNFaceAttributesClassifier
VNMPImageQuality
VNSequenceSummarizationAnalyzer
VNEspressoModelClassifier
VNBarcodeObservation
VNDetectBarcodesRequest
VNTrackerManager
VNMPImageData
VNNOPRequest
VNDetectFaceLandmarksRequest
VNImageprint
VNSerializingInternal
VNSerializing
VNFaceGeometryEstimator
VNMetalProcessingDevice
VNEspressoModelImageprint
VNImageAestheticsObservation
VNFaceRegionMapGenerator
VisionAdditions
VNObservation
VNDetectedObjectObservation
VNFaceprint
VNFaceObservation
VNImageAlignmentObservation
VNImageTranslationAlignmentObservation
VNImageHomographicAlignmentObservation
VNImageScoreObservation
VNImageprintObservation
VNImageBlurObservation
VNImageBrightnessObservation
VNClassificationObservation
VNRecognizedObjectObservation
VNCoreMLFeatureValueObservation
VNPixelBufferObservation
VNRectangleObservation
VNHumanObservation
VNHorizonObservation
VNCluster
VNClusterObservation
VNSceneObservation
VNSmartCamObservation
VNBurstObservation
_VNTextObservationCharacterBox
VNTextObservation
VNHorizonDetector
VNDetectFacePoseRequest
VNImageBasedRequest
VNFaceObservationAcceptingInternal
VNVersionParser
VNGenerateImageSaliencyRequest
VNLabradorRuntimeDirectProcessingDevice
VNImageGrouper
VNCVPixelBufferConversionHelpers
VNClassifyFaceAttributesRequest
VNPersonsModelFaceModelAdditions
VNPersonsModelFaceModel
VNFaceLandmarkSegments
BurstImageSetInternal
VNSceneClassificationCustomHierarchy
_VNSceneClassifierSceneClassificationCustomHierarchy
_VNImageAnalyzerMultiDetectorSceneClassificationCustomHierarchy
VNMPImageGrouping
VNEspressoHelpers
ImageProcessing_CoreImageUtils
VNDetectFace3DLandmarksRequest
VNFaceLandmarkDetectorRevision1
LKTGPU
VNJunkIdentifier
VNRectangleDetector
VNCPUProcessingDevice
VNTorsoprintGenerator
VNImageBlurScoreRequest
VNFaceDetector
VNFaceLandmarkRegion
VNFaceLandmarkRegion2D
VNFaceLandmarkRegion3D
VNFaceLandmarks
VNFaceLandmarks2D
VNFaceLandmarks3D
VNValidationUtilities
CVMLFaceprint_LegacySupportDoNotChange
CVMLObservation_LegacySupportDoNotChange
CVMLImageprintObservation_LegacySupportDoNotChange
MPImageDescriptor_LegacySupportDoNotChange
VNTorsoprint
VNCoreMLModel
VNCoreMLTransformer
VNImageClassifier
VNCreateImageprintRequest
ShotflowHelpers
VNObjectTracker
BurstThumbnailCluster
VNModelFileImpl
VNModelFile
VNModelFilesCache
VNMPImageDescriptor
VNSequenceSummarizationRequest
VNDetectFaceRectanglesRequest
VNBurstContext
VNTranslationalImageRegistrationRequest
VNFaceprintGenerator
VNCreateSmartCamprintRequest
VNImageAnalyzerMultiDetector
VNCreateFaceRegionMapRequest
CCCharBoxContext
CCTextDetector
VNObservationsCache
VNEspressoModelFileBasedDetector
VNDebugHelpers
VNDetectRectanglesRequest
VNReadOnlyPersonsModel
VNPersonsModelDataSource
VNCreateSceneprintRequest
VNRequestPerformer
VNTrackerProviding
VNIdentifyJunkRequest
VNGroupImagesByTimeAndContentRequest
VNProcessingDevice
VNAlignFaceRectangleRequest
VNClustererOptions
VNClustererQueryOptions
VNClustererBuilderOptions
VNClustererQuery
VNClustererBuilder
VNCoreMLRequest
VNMPContext
ShotflowFaceDetection
VNDetectFaceExpressionsRequest
VNBrightnessMeasure
VNTrackRectangleRequest
VNCreateFaceTorsoprintRequest
VNFaceLandmarkDetectorRevision2
VNCreateFaceprintRequest
VNTrackObjectRequest
VNFaceBBoxAligner
VNTracker
ShotflowNetwork
VNImageExposureScoreRequest
VNSmartCamCombinedAestheticsAndSaliencyDetector
VNCompoundRequest
VNUniqueObservationClassCompoundRequest
VNGenerateOpticalFlowRequest
VNLabradorRuntimeProcessingDevice
VNRequestPerformingContext
VNImageBufferProviding
VNImageRegistrationSignature
VNImageRegistration
VNPersonsModelData
VNMPImageSharpness
VNFaceLandmarksSegmenter
VNImageprintGenerator
VNTargetedImageRequest
VNTrackFaceRequest
VNDetectHorizonRequest
@32@0:8q16@24
@32@0:8@16@24
@48@0:8@16@24@32^@40
@24@0:8Q16
v16@0:8
@"NSArray"
@"NSNumber"
B16@0:8
@40@0:8@16@24Q32
@24@0:8@16
v24@0:8@16
@24@0:8^{_NSZone=}16
@16@0:8
v32@0:8@16@24
@32@0:8@16q24
@36@0:8@16B24@28
@28@0:8@16B24
v32@0:8r^{map<long long, std::__1::vector<long long, std::__1::allocator<long long> >, std::__1::less<long long>, std::__1::allocator<std::__1::pair<const long long, std::__1::vector<long long, std::__1::allocator<long long> > > > >={__tree<std::__1::__value_type<long long, std::__1::vector<long long, std::__1::allocator<long long> > >, std::__1::__map_value_compare<long long, std::__1::__value_type<long long, std::__1::vector<long long, std::__1::allocator<long long> > >, std::__1::less<long long>, true>, std::__1::allocator<std::__1::__value_type<long long, std::__1::vector<long long, std::__1::allocator<long long> > > > >=^{__tree_end_node<std::__1::__tree_node_base<void *> *>}{__compressed_pair<std::__1::__tree_end_node<std::__1::__tree_node_base<void *> *>, std::__1::allocator<std::__1::__tree_node<std::__1::__value_type<long long, std::__1::vector<long long, std::__1::allocator<long long> > >, void *> > >={__tree_end_node<std::__1::__tree_node_base<void *> *>=^{__tree_node_base<void *>}}}{__compressed_pair<unsigned long, std::__1::__map_value_compare<long long, std::__1::__value_type<long long, std::__1::vector<long long, std::__1::allocator<long long> > >, std::__1::less<long long>, true> >=Q}}}16@24
v24@0:8r^{map<long long, std::__1::vector<long long, std::__1::allocator<long long> >, std::__1::less<long long>, std::__1::allocator<std::__1::pair<const long long, std::__1::vector<long long, std::__1::allocator<long long> > > > >={__tree<std::__1::__value_type<long long, std::__1::vector<long long, std::__1::allocator<long long> > >, std::__1::__map_value_compare<long long, std::__1::__value_type<long long, std::__1::vector<long long, std::__1::allocator<long long> > >, std::__1::less<long long>, true>, std::__1::allocator<std::__1::__value_type<long long, std::__1::vector<long long, std::__1::allocator<long long> > > > >=^{__tree_end_node<std::__1::__tree_node_base<void *> *>}{__compressed_pair<std::__1::__tree_end_node<std::__1::__tree_node_base<void *> *>, std::__1::allocator<std::__1::__tree_node<std::__1::__value_type<long long, std::__1::vector<long long, std::__1::allocator<long long> > >, void *> > >={__tree_end_node<std::__1::__tree_node_base<void *> *>=^{__tree_node_base<void *>}}}{__compressed_pair<unsigned long, std::__1::__map_value_compare<long long, std::__1::__value_type<long long, std::__1::vector<long long, std::__1::allocator<long long> > >, std::__1::less<long long>, true> >=Q}}}16
@"NSURL"
@"NSString"
v32@0:8@16^{ImageDescriptorBufferFloat32=^^?{vector<long long, std::__1::allocator<long long> >=^q^q{__compressed_pair<long long *, std::__1::allocator<long long> >=^q}}{map<long long, int, std::__1::less<long long>, std::__1::allocator<std::__1::pair<const long long, int> > >={__tree<std::__1::__value_type<long long, int>, std::__1::__map_value_compare<long long, std::__1::__value_type<long long, int>, std::__1::less<long long>, true>, std::__1::allocator<std::__1::__value_type<long long, int> > >=^{__tree_end_node<std::__1::__tree_node_base<void *> *>}{__compressed_pair<std::__1::__tree_end_node<std::__1::__tree_node_base<void *> *>, std::__1::allocator<std::__1::__tree_node<std::__1::__value_type<long long, int>, void *> > >={__tree_end_node<std::__1::__tree_node_base<void *> *>=^{__tree_node_base<void *>}}}{__compressed_pair<unsigned long, std::__1::__map_value_compare<long long, std::__1::__value_type<long long, int>, std::__1::less<long long>, true> >=Q}}}^vQQQBQi^f}24
v40@0:8@16@24^{ImageDescriptorBufferJoint=^^?{vector<long long, std::__1::allocator<long long> >=^q^q{__compressed_pair<long long *, std::__1::allocator<long long> >=^q}}{map<long long, int, std::__1::less<long long>, std::__1::allocator<std::__1::pair<const long long, int> > >={__tree<std::__1::__value_type<long long, int>, std::__1::__map_value_compare<long long, std::__1::__value_type<long long, int>, std::__1::less<long long>, true>, std::__1::allocator<std::__1::__value_type<long long, int> > >=^{__tree_end_node<std::__1::__tree_node_base<void *> *>}{__compressed_pair<std::__1::__tree_end_node<std::__1::__tree_node_base<void *> *>, std::__1::allocator<std::__1::__tree_node<std::__1::__value_type<long long, int>, void *> > >={__tree_end_node<std::__1::__tree_node_base<void *> *>=^{__tree_node_base<void *>}}}{__compressed_pair<unsigned long, std::__1::__map_value_compare<long long, std::__1::__value_type<long long, int>, std::__1::less<long long>, true> >=Q}}}^vQQQBQi^f{vector<bool, std::__1::allocator<bool> >=^QQ{__compressed_pair<unsigned long, std::__1::allocator<unsigned long> >=Q}}{vector<vision::mod::DescriptorItemSideInfo, std::__1::allocator<vision::mod::DescriptorItemSideInfo> >=^{DescriptorItemSideInfo}^{DescriptorItemSideInfo}{__compressed_pair<vision::mod::DescriptorItemSideInfo *, std::__1::allocator<vision::mod::DescriptorItemSideInfo> >=^{DescriptorItemSideInfo}}}}32
@32@0:8@16^@24
@40@0:8@16@24^@32
@36@0:8@16f24^@28
@24@0:8^@16
@"NSArray"36@0:8@"NSDictionary"16f24^@28
@"NSData"24@0:8^@16
@"NSSet"24@0:8^@16
@"NSArray"32@0:8@"NSNumber"16^@24
@"NSNumber"40@0:8@"NSNumber"16@"NSNumber"24^@32
@"NSDictionary"32@0:8@"NSArray"16^@24
@"NSArray"40@0:8@"NSArray"16@"NSDictionary"24^@32
@"NSDictionary"40@0:8@"NSArray"16@"NSArray"24^@32
@"NSNumber"24@0:8^@16
@"NSArray"40@0:8@"NSData"16@"NSString"24^@32
B24@0:8^@16
B32@0:8@16^@24
v32@0:8{shared_ptr<const vision::mod::FaceClustering>=^{FaceClustering}^{__shared_weak_count}}16
@24@0:8^{vector<std::__1::pair<long long, long long>, std::__1::allocator<std::__1::pair<long long, long long> > >=^{pair<long long, long long>}^{pair<long long, long long>}{__compressed_pair<std::__1::pair<long long, long long> *, std::__1::allocator<std::__1::pair<long long, long long> > >=^{pair<long long, long long>}}}16
@"VNClusteringLogger"
@"VNSuggestionsLogger"
@"NSData"
{shared_ptr<const vision::mod::FaceClustering>="__ptr_"^{FaceClustering}"__cntrl_"^{__shared_weak_count}}
@"NSArray"32@0:8@"NSDictionary"16^@24
@"NSUUID"16@0:8
q32@0:8^{ImageDescriptorBufferAbstract=^^?{vector<long long, std::__1::allocator<long long> >=^q^q{__compressed_pair<long long *, std::__1::allocator<long long> >=^q}}{map<long long, int, std::__1::less<long long>, std::__1::allocator<std::__1::pair<const long long, int> > >={__tree<std::__1::__value_type<long long, int>, std::__1::__map_value_compare<long long, std::__1::__value_type<long long, int>, std::__1::less<long long>, true>, std::__1::allocator<std::__1::__value_type<long long, int> > >=^{__tree_end_node<std::__1::__tree_node_base<void *> *>}{__compressed_pair<std::__1::__tree_end_node<std::__1::__tree_node_base<void *> *>, std::__1::allocator<std::__1::__tree_node<std::__1::__value_type<long long, int>, void *> > >={__tree_end_node<std::__1::__tree_node_base<void *> *>=^{__tree_node_base<void *>}}}{__compressed_pair<unsigned long, std::__1::__map_value_compare<long long, std::__1::__value_type<long long, int>, std::__1::less<long long>, true> >=Q}}}^vQQQB}16^{vector<std::__1::pair<long long, long long>, std::__1::allocator<std::__1::pair<long long, long long> > >=^{pair<long long, long long>}^{pair<long long, long long>}{__compressed_pair<std::__1::pair<long long, long long> *, std::__1::allocator<std::__1::pair<long long, long long> > >=^{pair<long long, long long>}}}24
{shared_ptr<vision::mod::FaceClustering>="__ptr_"^{FaceClustering}"__cntrl_"^{__shared_weak_count}}
@52@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16i48
i16@0:8
v20@0:8i16
{CGRect={CGPoint=dd}{CGSize=dd}}16@0:8
v48@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16
{CGRect="origin"{CGPoint="x"d"y"d}"size"{CGSize="width"d"height"d}}
@20@0:8f16
v20@0:8f16
f16@0:8
f48@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16
{CGPoint=dd}16@0:8
v32@0:8{CGPoint=dd}16
{CGSize=dd}16@0:8
v32@0:8{CGSize=dd}16
{CGPoint="x"d"y"d}
{CGSize="width"d"height"d}
{CGRect={CGPoint=dd}{CGSize=dd}}56@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16f48f52
{CGRect={CGPoint=dd}{CGSize=dd}}64@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16@48^B56
@56@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16@48
@48@0:8@16{CGSize=dd}24@40
i32@0:8^{__CVBuffer=}16@24
v32@0:8^{__CVBuffer=}16@24
v40@0:8@16{CGSize=dd}24
d16@0:8
v24@0:8d16
v20@0:8B16
@"NSMutableDictionary"
@"NSMutableArray"
@20@0:8i16
d24@0:8r^{BurstSupportVector=d[7d]}16
^{__SVMParameters=[7{__SVMScaleOffset=ff}]ddii^{BurstSupportVector}^{BurstSupportVector}}16@0:8
v24@0:8^{__SVMParameters=[7{__SVMScaleOffset=ff}]ddii^{BurstSupportVector}^{BurstSupportVector}}16
[7d]
^{__SVMParameters=[7{__SVMScaleOffset=ff}]ddii^{BurstSupportVector}^{BurstSupportVector}}
B48@0:8@16@24@32^@40
@56@0:8@16@24@32B40f44^@48
@60@0:8@16@24@32B40f44f48^@52
@40@0:8#16@24^@32
@44@0:8@16f24@28^@36
@"NSArray"32@0:8@"NSArray"16^@24
@"NSArray"44@0:8@"NSDictionary"16f24@"VNCanceller"28^@36
@"NSNumber"40@0:8@"VNFaceprint"16@"VNFaceprint"24^@32
@"NSNumber"40@0:8@"VNFaceObservation"16@"VNFaceObservation"24^@32
@52@0:8@16@24@32f40^@44
@56@0:8@16@24@32f40f44^@48
@"<VNClusteringReadOnly><VNClusteringCancelling>"
@56@0:8@16@24@32@40^@48
B32@0:8@"NSData"16^@24
@"NSArray"48@0:8@"NSArray"16@"NSArray"24@"VNCanceller"32^@40
@"NSArray"56@0:8@"NSArray"16@"NSArray"24@"NSArray"32@"VNCanceller"40^@48
@"NSArray"40@0:8@"NSArray"16@"VNCanceller"24^@32
@"<VNClusteringReadOnly><VNClusteringWritable><VNClusteringCancelling>"
{shared_ptr<vision::mod::ImageClassifierAbstract>=^{ImageClassifierAbstract}^{__shared_weak_count}}96@0:8{shared_ptr<vision::mod::ImageDescriptorProcessorAbstract>=^{ImageDescriptorProcessorAbstract}^{__shared_weak_count}}16r*32i40i44r*48{Options=BQ{shared_ptr<Espresso::abstract_context>=^{abstract_context}^{__shared_weak_count}}@}56
{shared_ptr<vision::mod::ImageDescriptorProcessorAbstract>=^{ImageDescriptorProcessorAbstract}^{__shared_weak_count}}76@0:8r*16i24i28i32{Options=BQ{shared_ptr<Espresso::abstract_context>=^{abstract_context}^{__shared_weak_count}}@}36
#16@0:8
@32@0:8@16Q24
v32@0:8^@16^@24
B24@0:8@16
@24@0:8:16
@32@0:8:16@24
@40@0:8:16@24@32
B24@0:8#16
B24@0:8:16
Vv16@0:8
Q16@0:8
^{_NSZone=}16@0:8
B24@0:8@"Protocol"16
@"NSString"16@0:8
@"NSArray"24@0:8@"NSDictionary"16
@72@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24@56^@64
B40@0:8@16^{shared_ptr<vision::mod::ImageDescriptorBufferAbstract>=^{ImageDescriptorBufferAbstract}^{__shared_weak_count}}24^@32
{shared_ptr<vision::mod::ImageClassifier_HierarchicalModel>="__ptr_"^{ImageClassifier_HierarchicalModel}"__cntrl_"^{__shared_weak_count}}
{shared_ptr<vision::mod::FaceClassifier_BoostedPixelDifference>="__ptr_"^{FaceClassifier_BoostedPixelDifference}"__cntrl_"^{__shared_weak_count}}
Q24@0:8Q16
@48@0:8^{__CVBuffer=}16Q24Q32^@40
@64@0:8^{__CVBuffer=}16Q24{CGSize=dd}32Q48^@56
Q32@0:8Q16Q24
@"<MTLDevice>"
@"<MTLCommandQueue>"
@"<MTLLibrary>"
@40@0:8q16@24@32
@40@0:8@16@24@32
@32@0:8Q16@24
@32@0:8Q16#24
@28@0:8i16@20
@32@0:8^{__CVBuffer=}16@?24
@32@0:8^{CGImage=}16@?24
@32@0:8@16@?24
@"NSDictionary"
{shared_ptr<vision::mod::ObjectDetector_DCNFaceDetector_v2>="__ptr_"^{ObjectDetector_DCNFaceDetector_v2}"__cntrl_"^{__shared_weak_count}}
@"VNFaceBBoxAligner"
v48@0:8^^f16^^f24^^f32^^f40
v24@0:8^f16
v48@0:8{vImage_Buffer=^vQQQ}16
{GridROI_t=iiii}16@0:8
f32@0:8@16@24
v32@0:8{GridROI_t=iiii}16
v40@0:8@16^f24^f32
f24@0:8@16
i24@0:8@16
v24@0:8^{__CVBuffer=}16
^S16@0:8
f20@0:8f16
q24@0:8@16
^f16@0:8
[1024f]
[256S]
{FastRegistration_Signatures="piRow"^f"nPiRow"Q"piRowTable"{Projections_meanStdTable="sumTable"^f"sumSqTable"^f}"piCol"^f"nPiCol"Q"piColTable"{Projections_meanStdTable="sumTable"^f"sumSqTable"^f}"_memoryContainer"*}
^{SharpnessGridElement_t=CCf}
{GridROI_t="startX"i"startY"i"endX"i"endY"i}
v32@0:8@16@?24
B32@0:8@"NSArray"16^@24
B40@0:8@16@24^@32
B40@0:8@16^{__CVBuffer=}24^@32
B44@0:8@16^{__CVBuffer=}24I32^@36
B40@0:8@16^{CGImage=}24^@32
B44@0:8@16^{CGImage=}24I32^@36
B44@0:8@16@24I32^@36
@"VNRequestPerformer"
@32@0:8^{__CVBuffer=}16@24
@36@0:8^{__CVBuffer=}16I24@28
@32@0:8^{CGImage=}16@24
@36@0:8^{CGImage=}16I24@28
@36@0:8@16I24@28
@"VNImageSpecifier"
@"VNObservationsCache"
@?16@0:8
v24@0:8@?16
@"NSObject"
^{__CVBuffer=}72@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24@56^@64
B60@0:8Q16^^{__CVBuffer}24I32Q36Q44^@52
B72@0:8^{?=[3]}16^{__CVBuffer=}24^{__CVBuffer=}32^{ImageRegistrationCtx_s=}40^v48r^{?=[3]}56^@64
r^{?=Q#Q}16@0:8
@32@0:8Q16^@24
@"NSArray"16@0:8
@40@0:8@16@24@?32
@24@0:8@?16
v24@0:8Q16
B40@0:8Q16@24^@32
@"VNSceneObservation"
@"VNSceneClassificationCustomHierarchy"
r^{?=Q{?=ii}{?=ii}{?=ii}}16@0:8
@44@0:8@16f24f28i32i36i40
@28@0:8@16f24
@28@0:8r^{vImage_Buffer=^vQQQ}16B24
@32@0:8@16f24f28
@"ShotflowNetwork"
@32@0:8^{__CVBuffer=}16^@24
@24@0:8^v16
v24@0:8^v16
^v16@0:8
B76@0:8^f16^{__CVBuffer=}24{CGRect={CGPoint=dd}{CGSize=dd}}32f64^@68
B44@0:8^f16^{__CVBuffer=}24f32^@36
B76@0:8^f16@24f32{CGRect={CGPoint=dd}{CGSize=dd}}36^@68
B40@0:8^f16^{__CVBuffer=}24^@32
B48@0:8^f16^{__CVBuffer=}24i32f36^@40
Q24@0:8@16
@"VNDetectedObjectObservation"
#24@0:8@16
@"<NSObject><NSCopying>"24@0:8@"NSDictionary"16
@"<NSObject><NSCopying>"16@0:8
@76@0:8I16@20{CGRect={CGPoint=dd}{CGSize=dd}}28@60^@68
B40@0:8^@16@24^@32
@"NSObject<OS_dispatch_queue>"
@"VNMetalContext"
v32@0:8@16#24
@32@0:8@16#24
v40@0:8@16Q24@32
v24@0:8@"NSCoder"16
@24@0:8@"NSCoder"16
@92@0:8Q16^{vImage_Buffer=^vQQQ}24B32{CGRect={CGPoint=dd}{CGSize=dd}}36{_Geometry2D_rect2D_={_Geometry2D_point2D_=ff}{_Geometry2D_size2D_=ff}}68@84
@32@0:8{CGPoint=dd}16
@48@0:8{CGPoint=dd}16{CGSize=dd}32
{vImage_Buffer="data"^v"height"Q"width"Q"rowBytes"Q}
{_Geometry2D_rect2D_="origin"{_Geometry2D_point2D_="x"f"y"f}"size"{_Geometry2D_size2D_="height"f"width"f}}
B32@0:8@16:24
v32@0:8@?16@?24
B32@0:8@?16@?24
{os_unfair_lock_s="_os_unfair_lock_opaque"I}
{_Geometry2D_point2D_=ff}36@0:8r^{_Geometry2D_point2D_=ff}16r^i24i32
^{__CVBuffer=}96@0:8@16{CGSize=dd}24{_Geometry2D_rect2D_={_Geometry2D_point2D_=ff}{_Geometry2D_size2D_=ff}}40^{vImage_Buffer=^vQQQ}56^{_Geometry2D_rect2D_={_Geometry2D_point2D_=ff}{_Geometry2D_size2D_=ff}}64^B72^f80^@88
@48@0:8r^{vImage_Buffer=^vQQQ}16r^{_Geometry2D_rect2D_={_Geometry2D_point2D_=ff}{_Geometry2D_size2D_=ff}}24r^v32^@40
B72@0:8r^{vImage_Buffer=^vQQQ}16@24^{_Geometry2D_rect2D_={_Geometry2D_point2D_=ff}{_Geometry2D_size2D_=ff}}32r^v40@48@56^@64
{shared_ptr<vision::mod::LandmarkAttributes>="__ptr_"^{LandmarkAttributes}"__cntrl_"^{__shared_weak_count}}
@"<VNModelFile>"
@"CIContext"
@"NSHashTable"
@"NSLock"
^{CGImageSource=}40@0:8^^{CGImageSource}16I24^{os_unfair_lock_s=I}28B36
^{CGImageSource=}16@0:8
^{CGImageSource=}24@0:8I16B20
^{CGImageSource=}
^{__CFDictionary=}16@0:8
^{CGColorSpace=}28@0:8I16^I20
^{__CVBuffer=}16@0:8
^{__CVBuffer=}52@0:8Q16Q24I32@36^@44
B104@0:8^{__CVBuffer=}16^^{__CVBuffer}24Q32Q40I48{CGRect={CGPoint=dd}{CGSize=dd}}52B84@88^@96
B104@0:8@16^^{__CVBuffer}24Q32Q40I48{CGRect={CGPoint=dd}{CGSize=dd}}52B84@88^@96
^{__CVBuffer=}84@0:8Q16Q24I32{CGRect={CGPoint=dd}{CGSize=dd}}36@68^@76
B24@0:8^f16
B24@0:8^{CGPoint=dd}16
B24@0:8^{?=[3]}16
B84@0:8Q16f24@28{CGRect={CGPoint=dd}{CGSize=dd}}36@?68^@76
{CGRect={CGPoint=dd}{CGSize=dd}}48@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16
^{__CVBuffer=}44@0:8Q16I24@28^@36
^{__CVBuffer=}76@0:8Q16{CGRect={CGPoint=dd}{CGSize=dd}}24I56@60^@68
@60@0:8Q16Q24I32@36@44^@52
@92@0:8Q16Q24I32{CGRect={CGPoint=dd}{CGSize=dd}}36@68@76^@84
B20@0:8I16
^{__CVBuffer=}
^{__CFArray=}
@"CIImage"
@"VNImageSourceManager"
v24@0:8@"NSArray"16
q16@0:8
v48@0:8@16@24@32@40
@40@0:8Q16@24^@32
v24@0:8@"VNPersonsModelData"16
B40@0:8@16^{CC_MD5state_st=IIIIII[16I]i}24^@32
B48@0:8@16@24^{CC_MD5state_st=IIIIII[16I]i}32^@40
B56@0:8Q16@24@32^{CC_MD5state_st=IIIIII[16I]i}40^@48
B44@0:8^^?16^:24Q32B40
B48@0:8^^?16^:24^Q32@40
@"VNPersonsModelData"
@"VNPersonsModelFaceModel"
B56@0:8^@16#24@32@40^@48
B48@0:8^d16@24@32^@40
B56@0:8^d16@24@32d40^@48
B48@0:8^f16@24@32^@40
B52@0:8^f16@24@32f40^@44
B56@0:8^@16@24@32#40^@48
Q20@0:8i16
Q32@0:8#16Q24
v32@0:8@"NSString"16@24
@24@0:8@"NSString"16
B48@0:8@16^Q24^Q32^@40
@"VNWarningRecorder"
@"NSObject<OS_dispatch_semaphore>"
@"VNProcessingDevice"
@40@0:8Q16Q24Q32
B24@0:8Q16
@52@0:8I16@20@28Q36I44B48
B32@0:8Q16Q24
I16@0:8
@"VNSizeRange"
@40@0:8@16Q24^@32
@"VNFaceprint"
@"VNTorsoprint"
{shared_ptr<vision::mod::ObjectDetector_DCNFaceDetector>="__ptr_"^{ObjectDetector_DCNFaceDetector}"__cntrl_"^{__shared_weak_count}}
@36@0:8@16i24^@28
@36@0:8@16B24^@28
@40@0:8{vector<MPClusteringTreeNode *, std::__1::allocator<MPClusteringTreeNode *> >=^^{MPClusteringTreeNode}^^{MPClusteringTreeNode}{__compressed_pair<MPClusteringTreeNode **, std::__1::allocator<MPClusteringTreeNode *> >=^^{MPClusteringTreeNode}}}16
@"VNMPContext"
@28@0:8^v16B24
@36@0:8^{__CVBuffer=}16I24^@28
@44@0:8^{__CVBuffer=}16I24@28^@36
@32@0:8^{CGImage=}16^@24
@36@0:8^{CGImage=}16I24^@28
@44@0:8^{CGImage=}16I24@28^@36
@36@0:8@16I24^@28
@44@0:8@16I24@28^@36
@"VNImageBuffer"
@24@0:8^{__CVBuffer=}16
@24@0:8^{CGImage=}16
^{CGImage=}16@0:8
^{CGImage=}
v28@0:8f16i20i24
{CGRect={CGPoint=dd}{CGSize=dd}}52@0:8f16{vImage_Buffer=^vQQQ}20
[4{CGPoint="x"d"y"d}]
[4f]
^{__CVBuffer=}24@0:8@16
@48@0:8Q16^{__CVBuffer=}24{CGSize=dd}32
^{__CVBuffer=}24@0:8^@16
B40@0:8@16Q24^@32
@32@0:8Q16Q24
@"<NSObject><NSCopying><NSSecureCoding>"24@0:8Q16
Q24@0:8@"<NSObject><NSCopying><NSSecureCoding>"16
@"VNFaceObservation"32@0:8Q16Q24
@"VNPersonsModel"
@"<VNPersonsModelDataSource>"
#28@0:8I16^@20
@40@0:8#16@24@32
@72@0:8@16@24@?32@?40^#48^Q56^@64
B52@0:8I16@20@28^{CC_MD5state_st=IIIIII[16I]i}36^@44
@48@0:8@16Q24@32^@40
@"VNPersonsModelConfiguration"
@"NSDate"
@36@0:8@16@24f32
@"VNFaceObservation"
@"<NSObject><NSCopying><NSSecureCoding>"
@"NSIndexSet"
@48@0:8#16@24@32^@40
#40@0:8@16@24^@32
#32@0:8@16@24
B56@0:8^@16^@24@32@40^@48
@"VNImageRegistrationSignature"
@"VNClassificationObservation"
@"VNFaceAttributeCategory"
^{TemplateObjectDetectorApply=iiiiB[2f][2f]fBBffif{hog={gradient=}}{ChnsFeat=ii{hog={gradient=}}{gradient=}}i{vector<vision::hum::DTreeApply, std::__1::allocator<vision::hum::DTreeApply> >=^{DTreeApply}^{DTreeApply}{__compressed_pair<vision::hum::DTreeApply *, std::__1::allocator<vision::hum::DTreeApply> >=^{DTreeApply}}}{vector<std::__1::map<int, vision::hum::DTreeNode, std::__1::less<int>, std::__1::allocator<std::__1::pair<const int, vision::hum::DTreeNode> > >, std::__1::allocator<std::__1::map<int, vision::hum::DTreeNode, std::__1::less<int>, std::__1::allocator<std::__1::pair<const int, vision::hum::DTreeNode> > > > >=^{map<int, vision::hum::DTreeNode, std::__1::less<int>, std::__1::allocator<std::__1::pair<const int, vision::hum::DTreeNode> > >}^{map<int, vision::hum::DTreeNode, std::__1::less<int>, std::__1::allocator<std::__1::pair<const int, vision::hum::DTreeNode> > >}{__compressed_pair<std::__1::map<int, vision::hum::DTreeNode, std::__1::less<int>, std::__1::allocator<std::__1::pair<const int, vision::hum::DTreeNode> > > *, std::__1::allocator<std::__1::map<int, vision::hum::DTreeNode, std::__1::less<int>, std::__1::allocator<std::__1::pair<const int, vision::hum::DTreeNode> > > > >=^{map<int, vision::hum::DTreeNode, std::__1::less<int>, std::__1::allocator<std::__1::pair<const int, vision::hum::DTreeNode> > >}}}{vector<float, std::__1::allocator<float> >=^f^f{__compressed_pair<float *, std::__1::allocator<float> >=^f}}{vector<vision::hum::DTreeNode *, std::__1::allocator<vision::hum::DTreeNode *> >=^^{DTreeNode}^^{DTreeNode}{__compressed_pair<vision::hum::DTreeNode **, std::__1::allocator<vision::hum::DTreeNode *> >=^^{DTreeNode}}}f{adaBoostApply={vector<vision::hum::DTreeApply, std::__1::allocator<vision::hum::DTreeApply> >=^{DTreeApply}^{DTreeApply}{__compressed_pair<vision::hum::DTreeApply *, std::__1::allocator<vision::hum::DTreeApply> >=^{DTreeApply}}}}{linearSVMApply={vector<float, std::__1::allocator<float> >=^f^f{__compressed_pair<float *, std::__1::allocator<float> >=^f}}f{vector<float, std::__1::allocator<float> >=^f^f{__compressed_pair<float *, std::__1::allocator<float> >=^f}}}@}
v24@0:8q16
@32@0:8f16f20f24f28
v24@0:8^{vImage_Buffer=^vQQQ}16
{?="plan"^v"network_index"i}
f40@0:8^{vImage_Buffer=^vQQQ}16@24^@32
{shared_ptr<vision::mod::ImageClassifier_HierarchicalModel>=^{ImageClassifier_HierarchicalModel}^{__shared_weak_count}}32@0:8Q16^@24
v32@0:8@16^{vector<std::__1::pair<std::__1::basic_string<char>, std::__1::basic_string<char> >, std::__1::allocator<std::__1::pair<std::__1::basic_string<char>, std::__1::basic_string<char> > > >=^{pair<std::__1::basic_string<char>, std::__1::basic_string<char> >}^{pair<std::__1::basic_string<char>, std::__1::basic_string<char> >}{__compressed_pair<std::__1::pair<std::__1::basic_string<char>, std::__1::basic_string<char> > *, std::__1::allocator<std::__1::pair<std::__1::basic_string<char>, std::__1::basic_string<char> > > >=^{pair<std::__1::basic_string<char>, std::__1::basic_string<char> >}}}24
B104@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24@56@64^{shared_ptr<vision::mod::ImageDescriptorBufferAbstract>=^{ImageDescriptorBufferAbstract}^{__shared_weak_count}}72@80@88^@96
{shared_ptr<vision::mod::ImageDescriptorProcessorAbstract>="__ptr_"^{ImageDescriptorProcessorAbstract}"__cntrl_"^{__shared_weak_count}}
{shared_ptr<vision::mod::ImageClassifierAbstract>="__ptr_"^{ImageClassifierAbstract}"__cntrl_"^{__shared_weak_count}}
@"NSSet"
@104@0:8Q16@24@32{CGPoint=dd}40{CGPoint=dd}56{CGPoint=dd}72{CGPoint=dd}88
@72@0:8Q16@24@32{CGRect={CGPoint=dd}{CGSize=dd}}40
@"CIBarcodeDescriptor"
^{ACBSConfig=}24@0:8^@16
B56@0:8@16^{CGPoint=dd}24^{CGPoint=dd}32^{CGPoint=dd}40^{CGPoint=dd}48
@92@0:8@16Q24Q32{CGRect={CGPoint=dd}{CGSize=dd}}40f72Q76^@84
@48@0:8@16^{ACBSConfig=}24Q32^@40
@"NSMapTable"
@48@0:8^{vImage_Buffer=^vQQQ}16@24@32^@40
@48@0:8^{vImage_Buffer=^vQQQ}16@24q32^@40
@48@0:8^{__CVBuffer=}16@24@32^@40
@48@0:8^{__CVBuffer=}16@24q32^@40
^{vImage_Buffer=^vQQQ}16@0:8
^{vImage_Buffer=^vQQQ}
{CGSize=dd}40@0:8@16Q24Q32
^{__CVBuffer=}96@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16@48I56{CGSize=dd}60B76@80^@88
^{__CVBuffer=}64@0:8@16I24{CGSize=dd}28B44@48^@56
@"VNSupportedImageSize"
Q40@0:8@16Q24^@32
Q40@0:8@"NSMutableData"16Q24^@32
@40@0:8@"NSData"16Q24^@32
@32@0:8@"NSData"16^@24
@40@0:8@16Q24Q32
@"VNMPImageDescriptor"
[9f]
@56@0:8r^v16Q24Q32@40Q48
@112@0:8Q16f24f28f32f36f40f44f48f52f56f60f64f68f72f76f80f84f88f92f96f100f104f108
{shared_ptr<vision::mod::FaceRegionMap>="__ptr_"^{FaceRegionMap}"__cntrl_"^{__shared_weak_count}}
v28@0:8I16@20
I24@0:8@16
v72@0:8{CGAffineTransform=dddddd}16@64
{CGAffineTransform=dddddd}24@0:8@16
v72@0:8{?=[3]}16@64
{?=[3]}24@0:8@16
v88@0:8{?=[4]}16@80
{?=[4]}24@0:8@16
@"NSUUID"
@48@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16
@56@0:8Q16{CGRect={CGPoint=dd}{CGSize=dd}}24
v80@0:8{CGPoint=dd}16{CGPoint=dd}32{CGPoint=dd}48{CGPoint=dd}64
@40@0:8@16q24^@32
v20@0:8I16
@80@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{CGRect={CGPoint=dd}{CGSize=dd}}48
@88@0:8Q16{CGRect={CGPoint=dd}{CGSize=dd}}24{CGRect={CGPoint=dd}{CGSize=dd}}56
@64@0:8Q16{CGRect={CGPoint=dd}{CGSize=dd}}24@56
@72@0:8Q16{CGRect={CGPoint=dd}{CGSize=dd}}24@56@64
B104@0:8{?=[4]}16^f80^f88^f96
{?=[4]}16@0:8
B24@0:8^{CGAffineTransform=dddddd}16
{_Geometry2D_rect2D_={_Geometry2D_point2D_=ff}{_Geometry2D_size2D_=ff}}16@0:8
v32@0:8{_Geometry2D_rect2D_={_Geometry2D_point2D_=ff}{_Geometry2D_size2D_=ff}}16
@"VNFaceLandmarks2D"
@"VNFaceLandmarks3D"
@"VNFaceRegionMap"
@"VNFaceAttributes"
@"VNFaceTorsoprint"
@"VNFaceLandmarkSegments"
{CGAffineTransform=dddddd}16@0:8
v64@0:8{CGAffineTransform=dddddd}16
{CGAffineTransform="a"d"b"d"c"d"d"d"tx"d"ty"d}
{?=[3]}16@0:8
v64@0:8{?=[3]}16
{?="columns"[3]}
@"VNImageprint"
@36@0:8Q16@24f32
@68@0:8Q16{CGRect={CGPoint=dd}{CGSize=dd}}24f56@60
@"MLFeatureValue"
@32@0:8Q16^{__CVBuffer=}24
@80@0:8{CGPoint=dd}16{CGPoint=dd}32{CGPoint=dd}48{CGPoint=dd}64
@88@0:8Q16{CGPoint=dd}24{CGPoint=dd}40{CGPoint=dd}56{CGPoint=dd}72
B36@0:8^@16B24^@28
@28@0:8B16^@20
@"NSArray"28@0:8B16^@20
{CGRect={CGPoint=dd}{CGSize=dd}}32@0:8Q16Q24
B48@0:8@16@24Q32@40
B32@0:8@16@24
B32@0:8^{__CVBuffer=}16^{__CVBuffer=}24
Q24@0:8^{__CVBuffer=}16
{shared_ptr<vision::mod::ImageDescriptorBufferFloat32>=^{ImageDescriptorBufferFloat32}^{__shared_weak_count}}52@0:8{shared_ptr<vision::mod::ImageDescriptorBufferFloat32>=^{ImageDescriptorBufferFloat32}^{__shared_weak_count}}16@32i40^{vector<int, std::__1::allocator<int> >=^i^i{__compressed_pair<int *, std::__1::allocator<int> >=^i}}44
@48@0:8{shared_ptr<vision::mod::FaceIDModel>=^{FaceIDModel}^{__shared_weak_count}}16Q32@40
@56@0:8@16r^{ImageDescriptorBufferFloat32=^^?{vector<long long, std::__1::allocator<long long> >=^q^q{__compressed_pair<long long *, std::__1::allocator<long long> >=^q}}{map<long long, int, std::__1::less<long long>, std::__1::allocator<std::__1::pair<const long long, int> > >={__tree<std::__1::__value_type<long long, int>, std::__1::__map_value_compare<long long, std::__1::__value_type<long long, int>, std::__1::less<long long>, true>, std::__1::allocator<std::__1::__value_type<long long, int> > >=^{__tree_end_node<std::__1::__tree_node_base<void *> *>}{__compressed_pair<std::__1::__tree_end_node<std::__1::__tree_node_base<void *> *>, std::__1::allocator<std::__1::__tree_node<std::__1::__value_type<long long, int>, void *> > >={__tree_end_node<std::__1::__tree_node_base<void *> *>=^{__tree_node_base<void *>}}}{__compressed_pair<unsigned long, std::__1::__map_value_compare<long long, std::__1::__value_type<long long, int>, std::__1::less<long long>, true> >=Q}}}^vQQQBQi^f}24Q32^{CVMLCanceller=^^?Bi}40^@48
@56@0:8@16r^{ImageDescriptorBufferFloat32=^^?{vector<long long, std::__1::allocator<long long> >=^q^q{__compressed_pair<long long *, std::__1::allocator<long long> >=^q}}{map<long long, int, std::__1::less<long long>, std::__1::allocator<std::__1::pair<const long long, int> > >={__tree<std::__1::__value_type<long long, int>, std::__1::__map_value_compare<long long, std::__1::__value_type<long long, int>, std::__1::less<long long>, true>, std::__1::allocator<std::__1::__value_type<long long, int> > >=^{__tree_end_node<std::__1::__tree_node_base<void *> *>}{__compressed_pair<std::__1::__tree_end_node<std::__1::__tree_node_base<void *> *>, std::__1::allocator<std::__1::__tree_node<std::__1::__value_type<long long, int>, void *> > >={__tree_end_node<std::__1::__tree_node_base<void *> *>=^{__tree_node_base<void *>}}}{__compressed_pair<unsigned long, std::__1::__map_value_compare<long long, std::__1::__value_type<long long, int>, std::__1::less<long long>, true> >=Q}}}^vQQQBQi^f}24Q32@40^@48
{shared_ptr<vision::mod::FaceIDModel>="__ptr_"^{FaceIDModel}"__cntrl_"^{__shared_weak_count}}
{map<int, int, std::__1::less<int>, std::__1::allocator<std::__1::pair<const int, int> > >={__tree<std::__1::__value_type<int, int>, std::__1::__map_value_compare<int, std::__1::__value_type<int, int>, std::__1::less<int>, true>, std::__1::allocator<std::__1::__value_type<int, int> > >=^{__tree_end_node<std::__1::__tree_node_base<void *> *>}{__compressed_pair<std::__1::__tree_end_node<std::__1::__tree_node_base<void *> *>, std::__1::allocator<std::__1::__tree_node<std::__1::__value_type<int, int>, void *> > >={__tree_end_node<std::__1::__tree_node_base<void *> *>=^{__tree_node_base<void *>}}}{__compressed_pair<unsigned long, std::__1::__map_value_compare<int, std::__1::__value_type<int, int>, std::__1::less<int>, true> >=Q}}}16@0:8
i52@0:8r^f16i24{CGSize=dd}28Q44
@56@0:8Q16Q24Q32Q40@48
^{__CVBuffer=}32@0:8Q16^@24
{CGRect={CGPoint=dd}{CGSize=dd}}68@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{CGSize=dd}48i64
v44@0:8@16{CGSize=dd}24i40
v56@0:8@16@24@32@40@?48
v48@0:8@16@24@32@?40
v28@0:8@16i24
^{__sFILE=*iiss{__sbuf=*i}i^v^?^?^?^?{__sbuf=*i}^{__sFILEX}i[3C][1C]{__sbuf=*i}iq}
@"BurstImageFaceAnalysisContext"
@"NSCountedSet"
@"BurstActionClassifier"
{shared_ptr<vision::mod::ImageClassifier_HierarchicalModel>=^{ImageClassifier_HierarchicalModel}^{__shared_weak_count}}24@0:8^@16
q40@0:8@16^^{MPClusteringTreeNode}24@32
f28@0:8^{MPClusteringTreeNode=@iffi^{MPClusteringTreeNode}^{MPClusteringTreeNode}^{MPClusteringTreeNode}^{MPClusteringTreeNode}}16i24
{vector<MPClusteringTreeNode *, std::__1::allocator<MPClusteringTreeNode *> >=^^{MPClusteringTreeNode}^^{MPClusteringTreeNode}{__compressed_pair<MPClusteringTreeNode **, std::__1::allocator<MPClusteringTreeNode *> >=^^{MPClusteringTreeNode}}}40@0:8i16f20^{MPClusteringTreeNode=@iffi^{MPClusteringTreeNode}^{MPClusteringTreeNode}^{MPClusteringTreeNode}^{MPClusteringTreeNode}}24@32
{vector<MPClusteringTreeNode *, std::__1::allocator<MPClusteringTreeNode *> >=^^{MPClusteringTreeNode}^^{MPClusteringTreeNode}{__compressed_pair<MPClusteringTreeNode **, std::__1::allocator<MPClusteringTreeNode *> >=^^{MPClusteringTreeNode}}}36@0:8i16^{MPClusteringTreeNode=@iffi^{MPClusteringTreeNode}^{MPClusteringTreeNode}^{MPClusteringTreeNode}^{MPClusteringTreeNode}}20@28
{vector<MPClusteringTreeNode *, std::__1::allocator<MPClusteringTreeNode *> >=^^{MPClusteringTreeNode}^^{MPClusteringTreeNode}{__compressed_pair<MPClusteringTreeNode **, std::__1::allocator<MPClusteringTreeNode *> >=^^{MPClusteringTreeNode}}}36@0:8f16^{MPClusteringTreeNode=@iffi^{MPClusteringTreeNode}^{MPClusteringTreeNode}^{MPClusteringTreeNode}^{MPClusteringTreeNode}}20@28
{vector<MPClusteringTreeNode *, std::__1::allocator<MPClusteringTreeNode *> >=^^{MPClusteringTreeNode}^^{MPClusteringTreeNode}{__compressed_pair<MPClusteringTreeNode **, std::__1::allocator<MPClusteringTreeNode *> >=^^{MPClusteringTreeNode}}}32@0:8^{MPClusteringTreeNode=@iffi^{MPClusteringTreeNode}^{MPClusteringTreeNode}^{MPClusteringTreeNode}^{MPClusteringTreeNode}}16@24
f28@0:8f16q20
f32@0:8f16q20B28
B64@0:8@16@24^^v32^^v40^{?=^vi}48^@56
B68@0:8@16@24i32^^v36^^v44^{?=^vi}52^@60
v32@0:8^v16^v24
^{__CVBuffer=}36@0:8I16r^{?=^v^v[4Q][4Q]QQQQQQQQQQi}20^@28
B40@0:8r^{?=^v^v[4Q][4Q]QQQQQQQQQQi}16^{__CVBuffer=}24^@32
Q32@0:8r^{?=^v^v[4Q][4Q]QQQQQQQQQQi}16^@24
B52@0:8^{?=^vi}16@24@32B40^@44
@36@0:8^{vImage_Buffer=^vQQQ}16i24^@28
^{CGImage=}32@0:8@16^@24
{vImage_Buffer=^vQQQ}32@0:8@16^@24
v64@0:8@16^{__CVBuffer=}24{_Geometry2D_rect2D_={_Geometry2D_point2D_=ff}{_Geometry2D_size2D_=ff}}32r^{vector<_Geometry2D_point2D_, std::__1::allocator<_Geometry2D_point2D_> >=^{_Geometry2D_point2D_}^{_Geometry2D_point2D_}{__compressed_pair<_Geometry2D_point2D_ *, std::__1::allocator<_Geometry2D_point2D_> >=^{_Geometry2D_point2D_}}}48r^{vector<_Geometry2D_point2D_, std::__1::allocator<_Geometry2D_point2D_> >=^{_Geometry2D_point2D_}^{_Geometry2D_point2D_}{__compressed_pair<_Geometry2D_point2D_ *, std::__1::allocator<_Geometry2D_point2D_> >=^{_Geometry2D_point2D_}}}56
{shared_ptr<vision::mod::LandmarkDetector>="__ptr_"^{LandmarkDetector}"__cntrl_"^{__shared_weak_count}}
@44@0:8@16i24i28i32^@36
B32@0:8^{__CVBuffer=}16^@24
i32@0:8^{__CVBuffer=}16^{__CVBuffer=}24
i24@0:8^{__CVBuffer=}16
B36@0:8i16i20i24^@28
B44@0:8@16^{__CVBuffer=}24i32^@36
i32@0:8@16@24
i40@0:8@16@24@32
i60@0:8@16i2428@36@44@52
i56@0:8@16@24@32@40@48
@"LKTMetalContext"
[9@"<MTLComputePipelineState>"]
[10{CGSize="width"d"height"d}]
[2[10@"<MTLTexture>"]]
[10@"<MTLTexture>"]
[2@"<MTLBuffer>"]
[2^{__CVBuffer}]
@"<MTLTexture>"
B80@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{CGSize=dd}48^{CGRect={CGPoint=dd}{CGSize=dd}}64^@72
24@0:8Q16
r^{CGPoint=dd}16@0:8
r^{CGPoint=dd}32@0:8{CGSize=dd}16
@64@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16^48Q56
r^16@0:8
v24@0:8r^16
^v32@0:8r^i16Q24
@84@0:8@16Q24{CGRect={CGPoint=dd}{CGSize=dd}}32{_Geometry2D_rect2D_={_Geometry2D_point2D_=ff}{_Geometry2D_size2D_=ff}}64f80
@"VNFaceLandmarkRegion2D"
@"VNFaceLandmarkRegion3D"
B48@0:8Q16Q24@?32^@40
B64@0:8@16@24#32Q40Q48^@56
B48@0:8@16Q24@32^@40
B48@0:8^Q16@24@32^@40
B72@0:8^@16@24@32#40Q48Q56^@64
@48@0:8@16@24#32^@40
B40@0:8^Q16@24^@32
B48@0:8^Q16#24@32^@40
@"MPImageDescriptor_LegacySupportDoNotChange"
@40@0:8^{__CVBuffer=}16@24^@32
@"MLModel"
@"MLObjectBoundingBoxOutputDescription"
@"VNCoreMLModel"
B132@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24^{ImageDescriptorProcessorAbstract=^^?}56i64I68B72^{ImageDescriptorBufferAbstract=^^?{vector<long long, std::__1::allocator<long long> >=^q^q{__compressed_pair<long long *, std::__1::allocator<long long> >=^q}}{map<long long, int, std::__1::less<long long>, std::__1::allocator<std::__1::pair<const long long, int> > >={__tree<std::__1::__value_type<long long, int>, std::__1::__map_value_compare<long long, std::__1::__value_type<long long, int>, std::__1::less<long long>, true>, std::__1::allocator<std::__1::__value_type<long long, int> > >=^{__tree_end_node<std::__1::__tree_node_base<void *> *>}{__compressed_pair<std::__1::__tree_end_node<std::__1::__tree_node_base<void *> *>, std::__1::allocator<std::__1::__tree_node<std::__1::__value_type<long long, int>, void *> > >={__tree_end_node<std::__1::__tree_node_base<void *> *>=^{__tree_node_base<void *>}}}{__compressed_pair<unsigned long, std::__1::__map_value_compare<long long, std::__1::__value_type<long long, int>, std::__1::less<long long>, true> >=Q}}}^vQQQB}76@84@92@100@108@116^@124
B64@0:8^{ImageClassifierAbstract=^^?{vector<std::__1::basic_string<char>, std::__1::allocator<std::__1::basic_string<char> > >=^{basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >}^{basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >}{__compressed_pair<std::__1::basic_string<char> *, std::__1::allocator<std::__1::basic_string<char> > >=^{basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >}}}{map<std::__1::basic_string<char>, float, std::__1::less<std::__1::basic_string<char> >, std::__1::allocator<std::__1::pair<const std::__1::basic_string<char>, float> > >={__tree<std::__1::__value_type<std::__1::basic_string<char>, float>, std::__1::__map_value_compare<std::__1::basic_string<char>, std::__1::__value_type<std::__1::basic_string<char>, float>, std::__1::less<std::__1::basic_string<char> >, true>, std::__1::allocator<std::__1::__value_type<std::__1::basic_string<char>, float> > >=^{__tree_end_node<std::__1::__tree_node_base<void *> *>}{__compressed_pair<std::__1::__tree_end_node<std::__1::__tree_node_base<void *> *>, std::__1::allocator<std::__1::__tree_node<std::__1::__value_type<std::__1::basic_string<char>, float>, void *> > >={__tree_end_node<std::__1::__tree_node_base<void *> *>=^{__tree_node_base<void *>}}}{__compressed_pair<unsigned long, std::__1::__map_value_compare<std::__1::basic_string<char>, std::__1::__value_type<std::__1::basic_string<char>, float>, std::__1::less<std::__1::basic_string<char> >, true> >=Q}}}{vector<std::__1::basic_string<char>, std::__1::allocator<std::__1::basic_string<char> > >=^{basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >}^{basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >}{__compressed_pair<std::__1::basic_string<char> *, std::__1::allocator<std::__1::basic_string<char> > >=^{basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >}}}iffii}16^{ImageDescriptorBufferAbstract=^^?{vector<long long, std::__1::allocator<long long> >=^q^q{__compressed_pair<long long *, std::__1::allocator<long long> >=^q}}{map<long long, int, std::__1::less<long long>, std::__1::allocator<std::__1::pair<const long long, int> > >={__tree<std::__1::__value_type<long long, int>, std::__1::__map_value_compare<long long, std::__1::__value_type<long long, int>, std::__1::less<long long>, true>, std::__1::allocator<std::__1::__value_type<long long, int> > >=^{__tree_end_node<std::__1::__tree_node_base<void *> *>}{__compressed_pair<std::__1::__tree_end_node<std::__1::__tree_node_base<void *> *>, std::__1::allocator<std::__1::__tree_node<std::__1::__value_type<long long, int>, void *> > >={__tree_end_node<std::__1::__tree_node_base<void *> *>=^{__tree_node_base<void *>}}}{__compressed_pair<unsigned long, std::__1::__map_value_compare<long long, std::__1::__value_type<long long, int>, std::__1::less<long long>, true> >=Q}}}^vQQQB}24^{vector<std::__1::map<std::__1::basic_string<char>, float, std::__1::less<std::__1::basic_string<char> >, std::__1::allocator<std::__1::pair<const std::__1::basic_string<char>, float> > >, std::__1::allocator<std::__1::map<std::__1::basic_string<char>, float, std::__1::less<std::__1::basic_string<char> >, std::__1::allocator<std::__1::pair<const std::__1::basic_string<char>, float> > > > >=^{map<std::__1::basic_string<char>, float, std::__1::less<std::__1::basic_string<char> >, std::__1::allocator<std::__1::pair<const std::__1::basic_string<char>, float> > >}^{map<std::__1::basic_string<char>, float, std::__1::less<std::__1::basic_string<char> >, std::__1::allocator<std::__1::pair<const std::__1::basic_string<char>, float> > >}{__compressed_pair<std::__1::map<std::__1::basic_string<char>, float, std::__1::less<std::__1::basic_string<char> >, std::__1::allocator<std::__1::pair<const std::__1::basic_string<char>, float> > > *, std::__1::allocator<std::__1::map<std::__1::basic_string<char>, float, std::__1::less<std::__1::basic_string<char> >, std::__1::allocator<std::__1::pair<const std::__1::basic_string<char>, float> > > > >=^{map<std::__1::basic_string<char>, float, std::__1::less<std::__1::basic_string<char> >, std::__1::allocator<std::__1::pair<const std::__1::basic_string<char>, float> > >}}}32@40@48^@56
@68@0:8r^{ImageDescriptorBufferAbstract=^^?{vector<long long, std::__1::allocator<long long> >=^q^q{__compressed_pair<long long *, std::__1::allocator<long long> >=^q}}{map<long long, int, std::__1::less<long long>, std::__1::allocator<std::__1::pair<const long long, int> > >={__tree<std::__1::__value_type<long long, int>, std::__1::__map_value_compare<long long, std::__1::__value_type<long long, int>, std::__1::less<long long>, true>, std::__1::allocator<std::__1::__value_type<long long, int> > >=^{__tree_end_node<std::__1::__tree_node_base<void *> *>}{__compressed_pair<std::__1::__tree_end_node<std::__1::__tree_node_base<void *> *>, std::__1::allocator<std::__1::__tree_node<std::__1::__value_type<long long, int>, void *> > >={__tree_end_node<std::__1::__tree_node_base<void *> *>=^{__tree_node_base<void *>}}}{__compressed_pair<unsigned long, std::__1::__map_value_compare<long long, std::__1::__value_type<long long, int>, std::__1::less<long long>, true> >=Q}}}^vQQQB}16^{ImageClassifierAbstract=^^?{vector<std::__1::basic_string<char>, std::__1::allocator<std::__1::basic_string<char> > >=^{basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >}^{basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >}{__compressed_pair<std::__1::basic_string<char> *, std::__1::allocator<std::__1::basic_string<char> > >=^{basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >}}}{map<std::__1::basic_string<char>, float, std::__1::less<std::__1::basic_string<char> >, std::__1::allocator<std::__1::pair<const std::__1::basic_string<char>, float> > >={__tree<std::__1::__value_type<std::__1::basic_string<char>, float>, std::__1::__map_value_compare<std::__1::basic_string<char>, std::__1::__value_type<std::__1::basic_string<char>, float>, std::__1::less<std::__1::basic_string<char> >, true>, std::__1::allocator<std::__1::__value_type<std::__1::basic_string<char>, float> > >=^{__tree_end_node<std::__1::__tree_node_base<void *> *>}{__compressed_pair<std::__1::__tree_end_node<std::__1::__tree_node_base<void *> *>, std::__1::allocator<std::__1::__tree_node<std::__1::__value_type<std::__1::basic_string<char>, float>, void *> > >={__tree_end_node<std::__1::__tree_node_base<void *> *>=^{__tree_node_base<void *>}}}{__compressed_pair<unsigned long, std::__1::__map_value_compare<std::__1::basic_string<char>, std::__1::__value_type<std::__1::basic_string<char>, float>, std::__1::less<std::__1::basic_string<char> >, true> >=Q}}}{vector<std::__1::basic_string<char>, std::__1::allocator<std::__1::basic_string<char> > >=^{basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >}^{basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >}{__compressed_pair<std::__1::basic_string<char> *, std::__1::allocator<std::__1::basic_string<char> > >=^{basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >}}}iffii}24f32@36@44@52^@60
@96@0:8r^{ImageDescriptorBufferAbstract=^^?{vector<long long, std::__1::allocator<long long> >=^q^q{__compressed_pair<long long *, std::__1::allocator<long long> >=^q}}{map<long long, int, std::__1::less<long long>, std::__1::allocator<std::__1::pair<const long long, int> > >={__tree<std::__1::__value_type<long long, int>, std::__1::__map_value_compare<long long, std::__1::__value_type<long long, int>, std::__1::less<long long>, true>, std::__1::allocator<std::__1::__value_type<long long, int> > >=^{__tree_end_node<std::__1::__tree_node_base<void *> *>}{__compressed_pair<std::__1::__tree_end_node<std::__1::__tree_node_base<void *> *>, std::__1::allocator<std::__1::__tree_node<std::__1::__value_type<long long, int>, void *> > >={__tree_end_node<std::__1::__tree_node_base<void *> *>=^{__tree_node_base<void *>}}}{__compressed_pair<unsigned long, std::__1::__map_value_compare<long long, std::__1::__value_type<long long, int>, std::__1::less<long long>, true> >=Q}}}^vQQQB}16^{ImageClassifierAbstract=^^?{vector<std::__1::basic_string<char>, std::__1::allocator<std::__1::basic_string<char> > >=^{basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >}^{basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >}{__compressed_pair<std::__1::basic_string<char> *, std::__1::allocator<std::__1::basic_string<char> > >=^{basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >}}}{map<std::__1::basic_string<char>, float, std::__1::less<std::__1::basic_string<char> >, std::__1::allocator<std::__1::pair<const std::__1::basic_string<char>, float> > >={__tree<std::__1::__value_type<std::__1::basic_string<char>, float>, std::__1::__map_value_compare<std::__1::basic_string<char>, std::__1::__value_type<std::__1::basic_string<char>, float>, std::__1::less<std::__1::basic_string<char> >, true>, std::__1::allocator<std::__1::__value_type<std::__1::basic_string<char>, float> > >=^{__tree_end_node<std::__1::__tree_node_base<void *> *>}{__compressed_pair<std::__1::__tree_end_node<std::__1::__tree_node_base<void *> *>, std::__1::allocator<std::__1::__tree_node<std::__1::__value_type<std::__1::basic_string<char>, float>, void *> > >={__tree_end_node<std::__1::__tree_node_base<void *> *>=^{__tree_node_base<void *>}}}{__compressed_pair<unsigned long, std::__1::__map_value_compare<std::__1::basic_string<char>, std::__1::__value_type<std::__1::basic_string<char>, float>, std::__1::less<std::__1::basic_string<char> >, true> >=Q}}}{vector<std::__1::basic_string<char>, std::__1::allocator<std::__1::basic_string<char> > >=^{basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >}^{basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >}{__compressed_pair<std::__1::basic_string<char> *, std::__1::allocator<std::__1::basic_string<char> > >=^{basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >}}}iffii}24^{ImageClassifier_HierarchicalModel=^{ImageClassfier_Graph}}32f40f44Q48Q56@64@72@80^@88
{shared_ptr<vImage_Buffer>=^{vImage_Buffer}^{__shared_weak_count}}24@0:8^{CGImage=}16
{shared_ptr<CGImage>=^{CGImage}^{__shared_weak_count}}24@0:8@16
@40@0:8@16{CGSize=dd}24
@56@0:8^{__CVBuffer=}16@24@32@40@?48
f36@0:8@16^i24i32
r^v16@0:8
v24@0:8@"NSString"16
@24@0:8r^{mapped_model_file=i^vQ}16
r^{mapped_model_file=i^vQ}
{unique_ptr<cvml::util::model_file_cache, std::__1::default_delete<cvml::util::model_file_cache> >="__ptr_"{__compressed_pair<cvml::util::model_file_cache *, std::__1::default_delete<cvml::util::model_file_cache> >="__value_"^{model_file_cache}}}
f24@0:8f16f20
@"VNSequenceSummarizationObservation"
@"BurstImageSetInternal"
v96@0:8^{__CVBuffer=}16@24{vImage_Buffer=^vQQQ}32{_Geometry2D_rect2D_={_Geometry2D_point2D_=ff}{_Geometry2D_size2D_=ff}}64{_Geometry2D_rect2D_={_Geometry2D_point2D_=ff}{_Geometry2D_size2D_=ff}}80
{shared_ptr<vision::mod::FaceFrontalizer>="__ptr_"^{FaceFrontalizer}"__cntrl_"^{__shared_weak_count}}
{shared_ptr<vision::mod::ImageDescriptorAugmenterFlip>="__ptr_"^{ImageDescriptorAugmenterFlip}"__cntrl_"^{__shared_weak_count}}
{shared_ptr<unsigned char>="__ptr_"*"__cntrl_"^{__shared_weak_count}}
{_Geometry2D_size2D_="height"f"width"f}
{shared_ptr<vision::mod::ImageClassifier_HierarchicalModel>=^{ImageClassifier_HierarchicalModel}^{__shared_weak_count}}40@0:8Q16Q24^@32
v40@0:8Q16@24r^{vector<std::__1::pair<std::__1::basic_string<char>, float>, std::__1::allocator<std::__1::pair<std::__1::basic_string<char>, float> > >=^{pair<std::__1::basic_string<char>, float>}^{pair<std::__1::basic_string<char>, float>}{__compressed_pair<std::__1::pair<std::__1::basic_string<char>, float> *, std::__1::allocator<std::__1::pair<std::__1::basic_string<char>, float> > >=^{pair<std::__1::basic_string<char>, float>}}}32
@112@0:8B16B20B24B28^{__CVBuffer=}32@40{CGSize=dd}48{CGRect={CGPoint=dd}{CGSize=dd}}64@96^@104
^{__CVBuffer=}52@0:8@16{_Geometry2D_size2D_=ff}24B32@36^@44
^{__CVBuffer=}84@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16@48{_Geometry2D_size2D_=ff}56B64@68^@76
{shared_ptr<vision::mod::ImageAnalyzer>="__ptr_"^{ImageAnalyzer}"__cntrl_"^{__shared_weak_count}}
v28@0:8Q16I24
I28@0:8Q16I24
v36@0:8Q16Q24I32
*16@0:8
v24@0:8*16
^Q16@0:8
v24@0:8^Q16
v24@0:8^S16
S16@0:8
v20@0:8S16
s16@0:8
v20@0:8s16
i48@0:8Q16Q24Q32^{vImage_Buffer=^vQQQ}40
i88@0:8{vImage_Buffer=^vQQQ}16{vImage_Buffer=^vQQQ}48*80
v56@0:8S16^f20^{__CCPulseWindowContext=^{__CCRange}SSsB}28C36C40{ThresholdSet_t=fff}44
i56@0:8S16C20C24{ThresholdSet_t=fff}28^f40Q48
v48@0:8^{__CCRange=SS}16*24^i32^i40
i28@0:8^{__CCSumDerivVectors=^f^f^f^f^fffii}16i24
v24@0:8^{__CCSumDerivVectors=^f^f^f^f^fffii}16
v32@0:8^{__CCSumDerivVectors=^f^f^f^f^fffii}16i24C28
i72@0:8C16^f20S28C32C36^{__CCFilterSumDerivConfig={__CCRange=SS}{__CCRange=SS}BBQQ}40S48{ThresholdSet_t=fff}52*64
i96@0:8^f16S24S28C32C36S40S44I48I52S56{ThresholdSet_t=fff}60*72^f80Q88
v52@0:8Q16Q24Q32C40S44S48
v40@0:8Q16Q24S32S36
v44@0:8Q16^S24C32S36S40
i56@0:8^{__rgbaColor=CCCC}16^{__rgbaColor=CCCC}24I32I36^{__rgbMinMaxU8=CCCCCC}40^{__rgbMinMaxFloat=ffffff}48
I92@0:8{vImage_Buffer=^vQQQ}16{vImage_Buffer=^vQQQ}48C80^{__rgbMinMaxFloat=ffffff}84
f88@0:8{vImage_Buffer=^vQQQ}16{vImage_Buffer=^vQQQ}48S80S84
v64@0:8{vImage_Buffer=^vQQQ}16^{__CCColorProfileContext={vImage_Buffer=^vQQQ}{vImage_Buffer=^vQQQ}{vImage_Buffer=^vQQQ}{vImage_Buffer=^vQQQ}iiSS}48S56S60
v60@0:8{vImage_Buffer=^vQQQ}16S48^{__CCColorProfileContext={vImage_Buffer=^vQQQ}{vImage_Buffer=^vQQQ}{vImage_Buffer=^vQQQ}{vImage_Buffer=^vQQQ}iiSS}52
S40@0:8^f16Q24^S32
i72@0:8{vImage_Buffer=^vQQQ}16S48^S52f60^S64
v24@0:8^{__CCColorProfileContext={vImage_Buffer=^vQQQ}{vImage_Buffer=^vQQQ}{vImage_Buffer=^vQQQ}{vImage_Buffer=^vQQQ}iiSS}16
i40@0:8^{__CCColorProfileContext={vImage_Buffer=^vQQQ}{vImage_Buffer=^vQQQ}{vImage_Buffer=^vQQQ}{vImage_Buffer=^vQQQ}iiSS}16S24S28Q32
i152@0:8{vImage_Buffer=^vQQQ}16{vImage_Buffer=^vQQQ}48{vImage_Buffer=^vQQQ}80^{__rgbMinMaxU8=CCCCCC}112^{__rgbMinMaxFloat=ffffff}120S128S132S136^I140C148
i84@0:8{vImage_Buffer=^vQQQ}16{vImage_Buffer=^vQQQ}48C80
i80@0:8{vImage_Buffer=^vQQQ}16{vImage_Buffer=^vQQQ}48
i92@0:8@16Q24{vImage_Buffer=^vQQQ}32^Q64^{__CCBigBox=SSSS[50{__CCCharBox=SSSSS}]}72^{__CCBigBox=SSSS[50{__CCCharBox=SSSSS}]}80C88
v72@0:8@16^{__CCBigBox=SSSS[50{__CCCharBox=SSSSS}]}24{vImage_Buffer=^vQQQ}32C64S68
v32@0:8^{__CCBox=SSSS}16^Q24
i32@0:8^{__CCBox=SSSS}16^Q24
i56@0:8*16S24S28Q32Q40S48S52
{__CCRange=SS}76@0:8{vImage_Buffer=^vQQQ}16S48S52S56S60Q64S72
v156@0:8{vImage_Buffer=^vQQQ}16*48{vImage_Buffer=^vQQQ}56{vImage_Buffer=^vQQQ}88S120S124^{__CCBox=SSSS}128^{__CCBox=SSSS}136^Q144C152
v60@0:8^f16^f24S32S36^B40^f48C56
v84@0:8{vImage_Buffer=^vQQQ}16S48S52S56^{__rgbMinMaxFloat=ffffff}60^f68^f76
i80@0:8{vImage_Buffer=^vQQQ}16f48f52S56^f60^f68C76
I48@0:8^{__CCBigBox=SSSS[50{__CCCharBox=SSSSS}]}16C24^S28^S36B44
v40@0:8Q16Q24Q32
v72@0:8{vImage_Buffer=^vQQQ}16^f48^f56S64S68
v56@0:8^{__CCCharBox=SSSSS}16^{__CCCharBox=SSSSS}24^S32^S40S48S52
S48@0:8^{__CCCharBox=SSSSS}16^{__CCBigBox=SSSS[50{__CCCharBox=SSSSS}]}24^S32^S40
i76@0:8^{__CCBigBox=SSSS[50{__CCCharBox=SSSSS}]}16^{__CCBigBox=SSSS[50{__CCCharBox=SSSSS}]}24C32S36S40C44S48S52^{__CCCharBox=SSSSS}56^{__CCCharBox=SSSSS}64C72
i180@0:8{vImage_Buffer=^vQQQ}16{vImage_Buffer=^vQQQ}48{vImage_Buffer=^vQQQ}80^{__CCBigBox=SSSS[50{__CCCharBox=SSSSS}]}112^{__CCBigBox=SSSS[50{__CCCharBox=SSSSS}]}120^{__CCCharBox=SSSSS}128^{__CCCharBox=SSSSS}136S144S148S152*156C164^S168C176
i52@0:8{vImage_Buffer=^vQQQ}16S48
v60@0:8{vImage_Buffer=^vQQQ}16Q48B56
i24@0:8Q16
i68@0:8^{__CCBigBox=SSSS[50{__CCCharBox=SSSSS}]}16C24S28Q32^{__CCBox=SSSS}40Q48S56*60
i160@0:8@16{vImage_Buffer=^vQQQ}24*56^{__CCBigBox=SSSS[50{__CCCharBox=SSSSS}]}64^{__CCBigBox=SSSS[50{__CCCharBox=SSSSS}]}72{vImage_Buffer=^vQQQ}80{vImage_Buffer=^vQQQ}112^{__rgbMinMaxFloat=ffffff}144C152C156
i88@0:8{vImage_Buffer=^vQQQ}16{vImage_Buffer=^vQQQ}48^S80
@56@0:8{vImage_Buffer=^vQQQ}16^@48
C16@0:8
v20@0:8C16
@"CCCharBoxContext"
@"NSCache"
B48@0:8^Q16^Q24@32^@40
{?=^vi}16@0:8
B80@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{CGRect={CGPoint=dd}{CGSize=dd}}48
B88@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{CGRect={CGPoint=dd}{CGSize=dd}}48d80
Q32@0:8@16@24
Q32@0:8@16Q24
Q24@0:8@"VNPersonsModel"16
@"<NSObject><NSCopying><NSSecureCoding>"32@0:8@"VNPersonsModel"16Q24
Q32@0:8@"VNPersonsModel"16@"<NSObject><NSCopying><NSSecureCoding>"24
Q32@0:8@"VNPersonsModel"16Q24
@"VNFaceObservation"40@0:8@"VNPersonsModel"16Q24Q32
@"VNCanceller"
@"VNTracker"32@0:8@"NSDictionary"16^@24
v24@0:8@"VNTracker"16
@"NSMutableSet"
@36@0:8@16f24@?28
v40@0:8@16@24@32
@44@0:8@16@24@32f40
@48@0:8@16@24@32f40f44
@"<VNClustererModelQuerying>"
@"<VNClustererModelQuerying><VNClustererModelBuilding>"
@96@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{CGRect={CGPoint=dd}{CGSize=dd}}48f80i84f88f92
@100@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{CGRect={CGPoint=dd}{CGSize=dd}}48f80i84f88f92i96
B32@0:8@16f24f28
B32@0:8@16f24i28
v76@0:8@16^{__CVBuffer=}24{_Geometry2D_rect2D_={_Geometry2D_point2D_=ff}{_Geometry2D_size2D_=ff}}32{_Geometry2D_rect2D_={_Geometry2D_point2D_=ff}{_Geometry2D_size2D_=ff}}48r^{vector<_Geometry2D_point2D_, std::__1::allocator<_Geometry2D_point2D_> >=^{_Geometry2D_point2D_}^{_Geometry2D_point2D_}{__compressed_pair<_Geometry2D_point2D_ *, std::__1::allocator<_Geometry2D_point2D_> >=^{_Geometry2D_point2D_}}}64f72
{shared_ptr<vision::mod::FaceBoxPoseAligner<signed char> >="__ptr_"^{FaceBoxPoseAligner<signed char>}"__cntrl_"^{__shared_weak_count}}
^{ObjectTrackerAbstract=^^?^{ObjectDetectorAbstract}{shared_ptr<vision::mod::ObjectTrackerOptions>=^{ObjectTrackerOptions}^{__shared_weak_count}}}40@0:8@16^{ObjectTrackerOptions=^^?@i}24^@32
B64@0:8@16^{vector<vision::mod::DetectedObject, std::__1::allocator<vision::mod::DetectedObject> >=^{DetectedObject}^{DetectedObject}{__compressed_pair<vision::mod::DetectedObject *, std::__1::allocator<vision::mod::DetectedObject> >=^{DetectedObject}}}24{CGSize=dd}32@48^@56
{shared_ptr<vision::mod::ObjectTrackerAbstract>="__ptr_"^{ObjectTrackerAbstract}"__cntrl_"^{__shared_weak_count}}
@40@0:8@16f24i28i32i36
@40@0:8@16i24i28i32f36
i32@0:8Q16Q24
v52@0:8{vImage_Buffer=^vQQQ}16B48
@52@0:8{vImage_Buffer=^vQQQ}16B48
{vector<std::__1::shared_ptr<espresso_buffer_t>, std::__1::allocator<std::__1::shared_ptr<espresso_buffer_t> > >="__begin_"^{shared_ptr<espresso_buffer_t>}"__end_"^{shared_ptr<espresso_buffer_t>}"__end_cap_"{__compressed_pair<std::__1::shared_ptr<espresso_buffer_t> *, std::__1::allocator<std::__1::shared_ptr<espresso_buffer_t> > >="__value_"^{shared_ptr<espresso_buffer_t>}}}
[6[2[2f]]]
@120@0:8B16B20^{__CVBuffer=}24@32{CGSize=dd}40{CGSize=dd}56{CGRect={CGPoint=dd}{CGSize=dd}}72@104^@112
^{__CVBuffer=}64@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16@48^@56
^{__CVBuffer=}40@0:8Q16Q24^@32
B64@0:8^{__CVBuffer=}16^{__CVBuffer=}24^{__CVBuffer=}32Q40Q48^@56
@"VNImageBuffer"24@0:8^@16
@44@0:8@16@24@32I40
r^{FastRegistration_Signatures=^fQ{Projections_meanStdTable=^f^f}^fQ{Projections_meanStdTable=^f^f}*}16@0:8
B60@0:8^{CGAffineTransform=dddddd}16@24@32@40f48^@52
@44@0:8B16@20@28^@36
@"NSMutableIndexSet"
@"<VNPersonsModelDataDelegate>"
q32@0:8^{vImage_Buffer=^vQQQ}16^f24
B40@0:8^{CGSize=dd}16Q24^@32
B40@0:8^Q16Q24^@32
v32@0:8^{?=^v^v[4Q][4Q]QQQQQQQQQQi}16^{__CVBuffer=}24
@40@0:8^{__CVBuffer=}16@24@?32
@44@0:8^{__CVBuffer=}16I24@28@?36
@40@0:8^{CGImage=}16@24@?32
@44@0:8^{CGImage=}16I24@28@?36
@44@0:8@16I24@28@?36
?P$
333?
Me!>
`Y?*
K/>n
4?Rc
As?@
[?>Y
5?+3A?&S
>;9W?@1
^y>?
>3PI>
j?}Y
?}wG?X
&>CVK?
q'?ep0?
xj>t
>: i?
6:>0
>=`Z?
=aR<>=
aK=e
>A*q?C
t=+jp>B
->g,&?"R;?
>?k}A>}
G?c%
O?Ig
>obp?
$X?7pG>
>h@=>H
\'?`
7?BZg?
{>?q
g?> 
=6rQ?
8?6v
y?F$&?~
L?6X
GS?R
>FA?6Wq?
{?OX
>9{3?1
*?<h6>
?X:/>
XQ?$~
>S\%>o
v#?Y
@#?o+u?
{v>U
*X?S]
>c~j?'
>5)E?
>:u)?w1E?
>}y)?
W?:U?<L
B%?-|
?sc~?
>}>2?
=?!"
6=JCA?
.?m<4?
I>O\R?
Dc?#I
D?R(
LA?:
>R`U?
\?@h}?:
?=`b?
L>h\
??yYS?
>WCB>mq
>k,a=T
a?V)}?
?]m}?
>$%M?
Oi>-
d?6x{?
>|HX?DnV>
>W!E>w
>~t*>+
OU=0cn?
E\?+5o?
)?kb
}?C<
uQ?1A
=Nd6>
Ch=R
Nx>(-\=c)
`I>K9
"~=p
=8hS?.
\)<]pF=
$?OZ
>9`'?
r=?}vp>5&<?-yx? 
R>x~
rj>?V$?
>L?;
E?oI2?
>b1*>
>?pB
>:X'?
C?AcB?
=?Vb>?
>BAY>T
~'> ~*?
d?%=
t?`w
@?5(j>yt
^$?v
-Z?mq
ms?@O
