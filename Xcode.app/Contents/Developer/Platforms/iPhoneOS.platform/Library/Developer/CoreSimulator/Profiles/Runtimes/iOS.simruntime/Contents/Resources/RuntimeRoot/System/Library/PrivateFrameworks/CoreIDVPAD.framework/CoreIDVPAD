@(#)PROGRAM:CoreIDVPAD  PROJECT:CoreIDV-5.421
4@NSt3__120__shared_ptr_emplaceIN6vision3mod21LivenessCheck_OptionsENS_9allocatorIS3_EEEE
NSt3__120__shared_ptr_emplaceIN6vision3mod22LivenessCheckPredictorENS_9allocatorIS3_EEEE
.@333333
com.apple.CoreIDV
IDVPAD
v16@?0@"NSError"8
v8@?0
@"PADSelfieAnalyzerRequest"8@?0
PADAntiSpoofing/%.0f
liveness.mp4
selfie.jpeg
flags
signature
livenessLabel
gestureSequence
assessmentsFAC
timestampsFAC
timestampsID
timestampsButtonPressed
assessmentFAC
assessmentTA
assessmentsPRD
assessmentFakePRD
assessmentLivePRD
assessmentID
ageLabel
sexLabel
skintoneLabel
ethnicityLabel
faceHairLabel
headgearLabel
glassesLabel
pose
assessmentsTA
deviceInfo
metadata.plist
LivenessCheck.plist
params.plist
%0.f.jpeg
FAC/%d/
PRD/
%@.png
FAC/FacePose
ProductType
<%@>
B8@?0
CIDVPAD.persist-capture-data
{ x:%.3f, y:%.3f, w:%.3f, h:%.3f }
-[PADVNSerialRequestsScheduler _dispatchVisionRequestForFrame:]
PADVNSerialRequestsScheduler.m
[_remainingRequests getValue] == 0
v24@?0@"NSObject<PADVNRequestProtocol>"8@"NSError"16
timestamps
This method should not be called on an VNRequest.
com.apple.coreidv.CoreIDVPAD.PADErrorDomain
CIDVPAD.process-cpu-only
CIDVPAD.use-verbose-logging
Recieved more faceprints than FAC timestamps. Each frame should contain no more than one faceprint.
@"PADAlgorithmFACFaceprintResult"8@?0
Could not complete faceprint detection on frames because another request is being processed.
An error occurred when detecting faceprints in the frame.
timestamp
PITCH
ROLL
PADFACFrameProcessingCompleteNotification
PADTAFrameProcessingCompleteNotification
PADPRDFrameProcessingCompleteNotification
PADFACFrameProcessingErrorOccurredNotification
PADFrameProcessingCompleteNotification
PADFrameUnretainedNotification
PADTAAdversarialThresholdMetNotification
PADFACGestureStarted
turnRight
turnLeft
lookUp
lookDown
blink
smile
openMouth
raiseEyebrows
idle
Unknown
shouldIgnore
assessment
nccSignal
-[_PADFaceActionSequenceClassifier assessment]
PADFaceActionSequenceClassifier.m
result >= 0 && result <= 1.0
@"NSData"8@?0
Unable to create asset track from asset
Unable to create AVAssetReader with file at URL %@
Unable to add output to AVAssetReader
com.apple.CoreIDVPAD.gestureComplete
gesture
result
bufferCount
gestureSkippedWithAccessibility
@"NSDictionary"8@?0
com.apple.CoreIDVPAD.gestureSequenceComplete
detectedGestureCount
gestureSkippedWithAcccessibility
gesture%d
PADModelLoader deallocated before the liveness models were loaded.
Could not complete identity verification because another request is being processed.
Could not obtain faceprint from selfie
Could not obtain attributes from selfie.
@"PADSelfieAnalyzerResult"8@?0
An error occurred when running PAD models on the selfie
@"PADPose"8@?0
-[PADVNFacePoseRequest _detectFaceBounds:error:]
PADVNFacePoseRequest.m
frame.faces.count == 0
Found unexpected number of faces %d
@"PADFace"8@?0
%0.f
roll
pitch
smilingConfidence
eyesClosedConfidence
Could not create predictor object.
FAC classifier failed with status: %d
Insufficient observations: %d. Expected at least: %d
TA could not convert image to required type.
TA model failed with status: %d
SC model failed with status: %d
Faceprints must contain at least 1 value.
Baseline faceprint must contain at least 1 dimension.
Dimension mismatch on faceprint %d.
ID model failed with status: %d
Invalid gesture %i.
iPhone10
vector
-[PADPrintReplayRequest _detectFaceBounds:error:]
PADPrintReplayRequest.m
-[PADPrintReplayRequest _doesNotMeetPoseRequirement:]
frame.faces.count == 1
%0.f-PRDs2
image
score0.1
embedding0.1
PADVNPrintReplayS2Model
mlmodelc
v24@?0@"MLModel"8@"NSError"16
image/Placeholder
softmax_expression
softmax_smile
softmax_pose
softmax_eye
pred_facepose
PADVNFacePoseModel
Successfully loaded FAC v%@
Starting to load ML models
Error loading ML models: %@
Successfully finished loading ML models
FAC Processing turned off
FAC Processing turned on
Refreshing FAC queue
SpoofClassification model failed (%{public}@).
Deallocating %@
Could not derive baseURL for persistence directory.
Could not create persistence directory (%{public}@).
Will persist capture data under: %{public}@.
Could not persist liveness video (%{public}@).
Could not persist selfie image (%{public}@).
Could not persist metadata file (%{public}@).
Could not persist params (%{public}@).
Could not find original params.
Could not retrieve ID Matching frames (%{public}@).
Could not create directory for ID Matching frames (%{public}@).
Could not persist ID matching frame %d (%{public}@).
Could not create directory for FAC matching frames (%{public}@).
Could not retrieve FAC frames (%{public}@).
Could not persist FAC frame (%d,%d) (%{public}@).
Did persist capture data.
Could not stat results dir at: %{public}@ (%{public}@).
Could not remove results dir at: %{public}@ (%{public}@).
Removed results dir because it was empty.
Could not create directory at %{public}@ (%{public}@).
Vision request dispatch failed (%{public}@).
Request %{public}@ failed (%{public}@).
Request %{public}@ did not obtain any observations.
Found %d faces in the frame. Invalidating current FAC buffer.
Pose: roll=%0.4f, pitch=%0.4f, yaw=%0.4f
Vision.FAC
Ignoring FAC composite: classifier is paused
Invalidating current FAC buffer.
SIML.FAC
FAC model failed (%{public}@).
FacePose composite set processing failed (%{public}@).
ID matching frames not selected for gesture. Selecting 2 random frames to use as default.
Invalid index %lu selected for ID matching frame in buffer.
Invalid index %lu selected for FAC reference frame in buffer.
Could not obtain faceprint from frame.
Vision.ID
Invalidating faceprints.
Vision.PRD
Found %d faces in the frame. Skipping PRD for this frame.
PRD unexpected number of results (%d).
SIML.TA
TA model failed (%{public}@).
TA: assessment=%d nccSignal=%0.4f
Stitch detected, nccSignal: %f, shouldIgnore: %d
Returning an invalid assessment because no frames have been analyzed by the TA module.
Returning an invalid high sensitivity max ncc signal because no frames have been analyzed by the TA module.
Returning an invalid low sensitivity max ncc signal because no frames have been analyzed by the TA module.
Returning an invalid max signal because no frames have been analyzed by the TA module.
Another liveness check is currently in progress. Ignoring request.
Requested gesture skipping but no gestures are being monitored.
Requested recorded gesture skipping but no gestures are being monitored.
Request to set gesture monitoring option to %ld failed because there aren't any gestures under monitoring
Request to restart gesture monitoring failed because there aren't any gestures under monitoring.
FAC Module: AX has been enabled.
Did finish monitoring gesture with assessment %d.
Obtained less faceprints than expected (%d).
Skipping gesture %d because attention awareness is disabled.
Skipping gesture %d because accessibility option is enabled.
Will start monitoring: %d.
The reported faceprint has invalid dimensions %d.
FaceprintRequest update failed (%{public}@).
AV.FrameImageData
AV.ReadFrame
Unable to retrieve next frame; AVAssetReaderTrackOutput not set
PADAnalyticsSendGestureCompleteEvent gesture = %@, result = %@, bufferCount = %lu, wasSkippedWithAccessibility = %d
PADAnalyticsSendGestureSequenceCompleteEvent detectedGestureCount = %lu, wasSkippedWithAccessibility = %d, scoresByGestures = %@
FaceLandmarksRequest update failed (%{public}@).
Processing ML Models with CPU Only
Loaded FacePose v%@, PRD S2 v%@
Deallocating: %@
Too many frames are currently retained (count = %d).
PAD.PixelBufferFromData
SelfieAnalyzer failed to set private faceAttributes revision: (%@), using default revision.
Could not obtain faceprint from selfie.
Could not obtain attributes from selfie.
SIML.ID
ID model failed (%{public}@).
Could not detect face bounds (%{public}@).
Could not obtain the face crop (%{public}@).
CoreML.FacePose
Could not complete FacePoseRequest (%{public}@).
FAC chose %lu frames for ID matching.
Invalid frame index %d.
The faceprint corresponding to observation %d was not found. Skipping frame selection for ID matching.
FAC used %lu frames as reference frames for the current gesture.
Faceprints must contain at least 1 value.
Baseline faceprint must contain at least 1 dimension.
Dimension mismatch on faceprint %d.
Could not complete detect face bounds (%{public}@).
More than one face detected in frame, skipping PRD assessment.
Does not meet pose requirement, skipping PRD assessment.
CoreML.PRDs2
PRD request stage 2 failed (%{public}@).
PRDs2: %.4f
Could not load PADVNPrintReplayS2Model.mlmodelc in the bundle resource
Could not load PADVNFacePoseModel.mlmodelc in the bundle resource
PADClassifierRequest
PADClassifierResult
_PADClassifier
PADClassifier
NSObject
_PADAuditDataRepository
PADAuditDataRepository
PADCounter
PADVNSerialRequestsScheduler
PADFrameMonitor
PADVNAdapter
PADVNRequestProtocol
VNFaceObservationAccepting
_PADFaceActionClassifier
PADFaceActionClassifier
_PADVNFaceprintDetector
PADVNFaceprintDetector
_PADPrintReplayDetector
PADPrintReplayDetector
_PADTrajectoryAnalyzer
PADTrajectoryAnalyzer
_PADFaceActionSequenceClassifier
PADFaceActionSequenceClassifier
PADFaceActionModuleDelegate
PADAVAssetReader
PADAVFrame
PADAVSerialAssetReader
PADModelLoader
PADPose
PADFace
PADFrame
PADSelfieAnalyzerRequest
PADSelfieAnalyzerResult
_PADSelfieAnalyzer
PADSelfieAnalyzer
PADVNFacePoseObservation
PADVNFacePoseRequest
PADAlgorithmFACFaceprintResult
PADAlgorithmFACResult
_PADAlgorithms
PADAlgorithms
PADPrintReplayObservation
PADPrintReplayRequest
PADVNPrintReplayS2ModelInput
MLFeatureProvider
PADVNPrintReplayS2ModelOutput
PADVNPrintReplayS2Model
PADVNFacePoseModelInput
PADVNFacePoseModelOutput
PADVNFacePoseModel
T@"MLMultiArray",&,N,V_score0_1
.cxx_destruct
T@"NSArray",&,N,V_timestampsFAC
T#,R
T@"NSArray",R,N
T@"<PADFaceActionModuleDelegate>",W,N,V_delegate
T@"NSNumber",&,N,V_assessmentTA
T@"MLMultiArray",&,N,V_embedding0_1
T@"PADSelfieAnalyzerResult",R,N
T@"MLMultiArray",&,N,V_softmax_expression
_assessmentsFAC
T@"MLMultiArray",&,N,V_softmax_pose
_bounds
T@"NSArray",&,N,V_assessmentsFAC
_config
T@"NSArray",&,N,V_faceprint
_dispatchVisionRequestForFrame:
T@"NSArray",&,N,V_faceprintsID
_framesAnalyzed
T@"NSArray",&,N,V_gestureSequence
_models
T@"NSArray",&,N,V_gestures
_output
T@"NSArray",&,N,V_timestampsButtonPressed
_selfie
T@"NSArray",R,N,V_results
analyzeObservationCompositeSet:
T@"NSMutableArray",&,N,V_assessmentsPRD
containsObject:
T@"NSMutableArray",&,N,V_faceprints
context
T@"NSMutableArray",&,N,V_gestureSequence
gesture
T@"NSMutableArray",&,N,V_timestampsID
glassesCategory
T@"NSNumber",&,N,V_assessment
initWithFormat:
T@"NSNumber",&,N,V_assessmentFakePRD
initWithModels:
T@"NSNumber",&,N,V_ignoredStitches
multiArrayValue
T@"NSNumber",&,N,V_maxNccHigh
numberWithBool:
T@"NSNumber",R,N
restart
T@"NSString",&,N,V_ageLabel
setOrientation:
T@"NSString",&,N,V_faceHairLabel
setSoftmax_eye:
T@"NSString",&,N,V_sexLabel
setYaw:
T@"NSString",R,C
.cxx_construct
T@"NSArray",&,N,V_assessmentsTA
JPEGRepresentationOfImage:colorSpace:options:
T@"NSArray",C,N
T@"<PADFaceActionModuleDelegate>",W,N
T@"NSNumber",&,N,V_assessmentID
T@"MLModel",R,N,V_model
T@"NSString",&,N,V_glassesLabel
T@"MLMultiArray",&,N,V_pred_facepose
_addNotificationCenterObservers
T@"MLMultiArray",&,N,V_softmax_eye
_assessmentsPRD
T@"MLMultiArray",&,N,V_softmax_smile
_buffer
T@"NSArray",&,N,V_assessmentsPRD
_currentObservationCompositeSet
T@"NSArray",&,N,V_faceprints
_ethnicityLabel
T@"NSArray",&,N,V_faces
_isDetectingFaceMovementGesture
T@"NSArray",&,N,V_gestureTypes
_onGestureStart
T@"NSArray",&,N,V_referenceFrameIndices
_result
T@"NSArray",&,N,V_timestampsID
_storeTAValues:
T@"NSMutableArray",&,N,V_assessmentsFAC
bundleForClass:
T@"NSMutableArray",&,N,V_assessmentsTA
containsString:
T@"NSMutableArray",&,N,V_faceprintsID
dealloc
T@"NSMutableArray",&,N,V_timestampsFAC
gestureSequence
T@"NSMutableArray",R,N,V_timestampsReferenceFAC
ignoredStitches
T@"NSNumber",&,N,V_assessmentFAC
initWithImageFromCGImage:error:
T@"NSNumber",&,N,V_assessmentLivePRD
isProxy
T@"NSNumber",&,N,V_livenessLabel
T@"NSNumber",&,N,V_maxNccLow
release
T@"NSSet",R,N
results
T@"NSString",&,N,V_ethnicityLabel
setPixelBuffer:
T@"NSString",&,N,V_headgearLabel
setUsesCPUOnly:
T@"NSString",&,N,V_skintoneLabel
startLivenessVideoProcessing:onGestureStart:onGesturesFinished:
T@"PADFrame",&,N,V_selfie
T@"PADPose",&,N,V_facePose
T@"PADPose",&,N,V_pose
T@"PADSelfieAnalyzerResult",R,N,V_result
T@"PADVNFacePoseModel",R,N,V_facePoseModel
T@"PADVNPrintReplayS2Model",R,N,V_printReplayS2Model
T@,W,N,V_pixelBuffer
TB,N,V_isHeadMovementDetected
TB,R,N
TB,R,N,V_idMatchingFramesSelected
TI,N,V_orientation
TQ,N,V_index
TQ,N,V_minNumberOfGestures
TQ,R
T^{__CVBuffer=},D,N
T^{__CVBuffer=},N,V_image
T^{__CVBuffer=},N,V_image_Placeholder
Td,N,V_assessment
Td,N,V_assessmentStage2
Td,N,V_eyesClosedConfidence
Td,N,V_pitch
Td,N,V_roll
Td,N,V_smilingConfidence
Td,N,V_timestamp
Td,N,V_yaw
Td,R,N
Tq,N
Tq,N,V_gesture
Tq,N,V_taOptions
Tq,R,N
T{?=qiIq},N,V_time
T{?=qiIq},N,V_timestamp
T{CGRect={CGPoint=dd}{CGSize=dd}},N,V_bounds
T{CGSize=dd},R,N
URLByAppendingPathComponent:
URLForResource:withExtension:
URLOfModelInThisBundle
URLsForDirectory:inDomains:
UTF8String
VN1uMyFtnYEWjbrdx3yAuDndKkPeyzNJhB
VN3iT1YRjjnIuELobV1olJiO1vvItN6Kdq
VN7exwFFmQF0AI9P7FjBljwEFu7QYUGCYE
VN7fiLHgGnvqPqG63cfDUCK4Xm8obUuWoP
_accessibilityOptionsEnabled
_ageLabel
_algorithms
_appendIDFaceprintResult:faceprint:
_areModelsLoading
_assessment
_assessmentFAC
_assessmentFakePRD
_assessmentID
_assessmentLivePRD
_assessmentStage2
_assessmentTA
_assessmentsTA
_assetReader
_audit
_auditStorePRDBuffer:name:
_completion
_consecutiveBuffersNotDetected
_counter
_createDirIfNotExists:
_createNewFACClassiferForGesture:
_currentFrameProcessingCompletion
_currentFrameTimestamp
_currentGestureBufferCount
_currentGestureIdx
_currentObservationComposite
_delegate
_detectFaceBounds:error:
_detectedGestures
_dir
_dispatchPRDRequestStage2ForFrame:error:
_doesNotMeetPoseRequirement:
_embedding0_1
_extractNestedTimestamps:
_extractTimestamps:
_eyesClosedConfidence
_facDidFinishProcessing
_facFramesToProcess
_facModule
_facOption
_facPoseValues
_faceHairLabel
_facePose
_facePoseModel
_faceprint
_faceprints
_faceprintsID
_faces
_finalize
_frameFromVideo:timestamp:size:error:
_generateDeviceInfoString
_gesture
_gestureSequence
_gestureTypes
_gestureTypesToSkip
_gestures
_glassesLabel
_handleGestureMonitorResult:
_handleIDMatchingAndReferenceFramesForFACResult:withObservationCompositeSet:
_hasSetRandomIDFrames
_headgearLabel
_idMatchingFramesSelected
_ignoredStitches
_image
_imageDataFromVideo:timestamp:size:error:
_imageSize
_image_Placeholder
_index
_isFACProcessing
_isHeadMovementDetected
_isProcessingFrame
_isReading
_livenessLabel
_maxNccHigh
_maxNccLow
_minNumberOfGestures
_minRequiredGestures
_model
_modelLoaderCompletion
_monitor
_newSampleBufferFromVideo:timestamp:size:error:
_obtainFaceCrop:error:
_onGesturesFinished
_onIncorrectGestureDetected
_orientation
_performSpoofClassificationWithCompletion:
_pitch
_pixelBuffer
_pose
_prdModule
_pred_facepose
_predictor
_prepareResultWithLabel:
_printReplayS2Model
_processObservationCompositeSetForIDMatchingFrames:
_processObservationCompositeSetWithFAC:
_processSingleBuffer
_queue
_referenceFrameIndices
_refreshQueuedFACFrames
_remainingRequests
_request
_requestError
_requests
_reset
_results
_retrieveFaceprintsFromPredictorUsingObservationCompositeSet:
_retrieveReferenceFramesFromPredictor
_roll
_score0_1
_scoresByGesture
_selfieModule
_sendFrameProcessingCompleteNotification
_setSize
_setup
_setupAssetReaderWithVideoURL:error:
_sexLabel
_shouldSkipAttentionAwareGestureAtIndex:
_shouldSkipGestureAtIndex:
_skintoneLabel
_smilingConfidence
_softmax_expression
_softmax_eye
_softmax_pose
_softmax_smile
_startNextGesture
_startProcessingNextFACFrameIfAvailable
_stitchCount
_storeBuffer:atURL:
_storeClassifierResult:imageData:signature:flags:
_storeCrop:forFrame:observation:
_storeFACPoseBuffer:identifier:values:
_storePRDBuffer:name:
_storeTAAssessment:nccSignal:frame:
_storeUnencryptedVideoFrom:
_taModule
_taOptions
_taValues
_time
_timestamp
_timestampsButtonPressed
_timestampsFAC
_timestampsID
_timestampsReferenceFAC
_wasGestureSkippedWithAccessibility
_wasSequenceSkippedWithAccessibility
_workQueue
_yaw
addObject:
addObjectsFromArray:
addObserver:selector:name:object:
addOutput:
ageCategory
ageLabel
allPoints
analyzeObservationComposite:
arrayWithCapacity:
arrayWithObjects:count:
assessment
assessmentFAC
assessmentFakePRD
assessmentID
assessmentLivePRD
assessmentStage2
assessmentTA
assessmentsFAC
assessmentsPRD
assessmentsTA
assetWithURL:
autorelease
boolValue
boundingBox
bounds
buffer
bytes
canAddOutput:
cancelReading
cancelWithCompletion:
class
close
conformsToProtocol:
contentsOfDirectoryAtPath:error:
contextWithOptions:
copy
copyItemAtPath:toPath:error:
copyItemAtURL:toURL:error:
copyNextSampleBuffer
count
countByEnumeratingWithState:objects:count:
createCGImage:fromRect:
createDirectoryAtPath:withIntermediateDirectories:attributes:error:
cropAndScaleBufferWithWidth:height:cropRect:format:imageCropAndScaleOption:options:error:calculatedNormalizedOriginOffset:calculatedScaleX:calculatedScaleY:
cvPixelBufferFromData:
debugDescription
decrement
decrementByValue:
defaultCenter
defaultManager
delegate
description
descriptorData
dictionaryWithCapacity:
dictionaryWithObjects:forKeys:count:
duration
elementCount
embedding0_1
enableAccessibilityOptions
errorWithDomain:code:userInfo:
ethnicityLabel
extent
eyesClosedConfidence
faceActionModuleDidFinishGestureDetectionWithAssessment:
faceAttributes
faceHairCategory
faceHairLabel
faceObservationWithRequestRevision:boundingBox:roll:yaw:
faceObservationWithRequestRevision:boundingBox:roll:yaw:pitch:
facePose
facePoseModel
faceprint
faceprints
faceprintsID
faces
fakeAssessment
featureNames
featureValueForName:
featureValueWithCGImage:pixelsWide:pixelsHigh:pixelFormatType:options:error:
featureValueWithImageAtURL:pixelsWide:pixelsHigh:pixelFormatType:options:error:
featureValueWithMultiArray:
featureValueWithPixelBuffer:
featuresAtIndex:
fileExistsAtPath:
fileExistsAtPath:isDirectory:
fileURLWithPath:
finishLivenessWithSelfie:idMatchingFaceprints:completion:
finishLivenessWithSelfie:performIDMatching:completion:
firstObject
floatValue
framesFromVideoURL:timestamps:error:
framesFromVideoURL:timestamps:size:error:
gestureTypes
gestures
getFACVersion
getPRDFakeFrameThreshold
getPRDLiveFrameThreshold
getValue
glassesLabel
handleObservationCompositeError:
handleResultForRequest:error:
hash
headgearLabel
idMatchingFramesSelected
identifier
image
imageBufferValue
imageWithCGImage:
imageWithCVPixelBuffer:
imageWithData:
image_Placeholder
imagesFromVideoURL:timestamps:error:
imagesFromVideoURL:timestamps:size:error:
increment
incrementByValue:
index
init
init:
init:models:
initWithAsset:error:
initWithCVPixelBuffer:orientation:options:
initWithCompletion:
initWithCompletionHandler:
initWithConfiguration:error:
initWithContentsOfURL:configuration:error:
initWithContentsOfURL:error:
initWithFeatureProviderArray:
initWithGesture:algorithms:models:
initWithImage:
initWithImageAtURL:error:
initWithImage_Placeholder:
initWithImage_PlaceholderAtURL:error:
initWithImage_PlaceholderFromCGImage:error:
initWithMLModel:
initWithQueue:
initWithScore0_1:embedding0_1:
initWithSoftmax_expression:softmax_smile:softmax_pose:softmax_eye:pred_facepose:
initWithTrack:outputSettings:
initWithVideoURL:error:
inputFaceObservations
intValue
integerValue
invalidate
isEqual:
isEqualToString:
isHeadMovementDetected
isKindOfClass:
isMemberOfClass:
label
landmarks
liveAssessment
livenessLabel
loadContentsOfURL:configuration:completionHandler:
loadWithConfiguration:completionHandler:
maxNccHigh
maxNccLow
metadata
minNumberOfGestures
model
modelDescription
modelWithContentsOfURL:configuration:error:
modelWithContentsOfURL:error:
mutableCopy
normalizedPoints
numberArray
numberWithDouble:
numberWithFloat:
numberWithInt:
numberWithInteger:
numberWithLongLong:
numberWithUnsignedInteger:
objectAtIndexedSubscript:
objectForKeyedSubscript:
observationCompositeSetSize
observationsFromRequest:
orientation
path
pathForResource:ofType:
pauseLiveness
performFAC:gesture:error:
performIDMatching:toFaceprints:error:
performOn:error:
performRequests:error:
performRequests:onCVPixelBuffer:orientation:error:
performSC:assessmentTA:assessmentFakePRD:assessmentLivePRD:assessmentID:error:
performSelector:
performSelector:withObject:
performSelector:withObject:withObject:
performTA:nccSignal:isSensitive:error:
persistentDomainForName:
pitch
pixelBuffer
pointCount
pose
postNotificationName:object:
postNotificationName:object:userInfo:
pred_facepose
predictionFromFeatures:error:
predictionFromFeatures:options:error:
predictionFromImage:error:
predictionFromImage_Placeholder:error:
predictionsFromBatch:options:error:
predictionsFromInputs:options:error:
prepareToResumeLiveness
printReplayS2Model
processFacePoseCompositeSet:gesture:error:
processFrame:
processFrames:completion:
processLivenessFrame:withOptions:taOptions:
processRecordedLivenessFrame:withPRD:FAC:
processRequest:completion:
raise:format:
referenceFrameIndices
removeAllObjects
removeItemAtPath:error:
removeObjectAtIndex:
render:toCVPixelBuffer:
requestsForFrame:handler:
requiredObservationSetSizeFAC:
respondsToSelector:
restartGesture
restartLivenessGesture
result
resumeLiveness
retain
retainCount
retrieveNextFrame
roll
score0_1
self
selfie
setAgeLabel:
setAlwaysCopiesSampleData:
setAssessment:
setAssessmentFAC:
setAssessmentFakePRD:
setAssessmentID:
setAssessmentLivePRD:
setAssessmentStage2:
setAssessmentTA:
setAssessmentsFAC:
setAssessmentsPRD:
setAssessmentsTA:
setBounds:
setBuffer:
setComputeUnits:
setDelegate:
setEmbedding0_1:
setEthnicityLabel:
setEyesClosedConfidence:
setFACOption:
setFaceHairLabel:
setFacePose:
setFaceprint:
setFaceprints:
setFaceprintsID:
setFaces:
setGesture:
setGestureSequence:
setGestureTypes:
setGestures:
setGlassesLabel:
setHeadgearLabel:
setIgnoredStitches:
setImage:
setImageWithCGImage:error:
setImageWithURL:error:
setImage_Placeholder:
setImage_PlaceholderWithCGImage:error:
setImage_PlaceholderWithURL:error:
setIndex:
setInputFaceObservations:
setIsHeadMovementDetected:
setLivenessLabel:
setMaxNccHigh:
setMaxNccLow:
setMinNumberOfGestures:
setObject:forKey:
setObject:forKeyedSubscript:
setPitch:
setPose:
setPred_facepose:
setReferenceFrameIndices:
setRevision:error:
setRoll:
setScore0_1:
setSelfie:
setSexLabel:
setSkintoneLabel:
setSmilingConfidence:
setSoftmax_expression:
setSoftmax_pose:
setSoftmax_smile:
setTaOptions:
setTime:
setTimeRange:
setTimestamp:
setTimestampsButtonPressed:
setTimestampsFAC:
setTimestampsID:
setToValue:
setWithArray:
sexLabel
shouldProcessFrame:
size
skintoneLabel
skipGesture
skipLivenessGesture
skipRecordedGesture
skipRecordedLivenessGesture
smilingConfidence
softmax_expression
softmax_eye
softmax_pose
softmax_smile
standardUserDefaults
startLiveness:onGestureStart:onIncorrectGestureDetected:onGesturesFinished:
startLivenessCheck:gestureTypes:minNumberOfGestures:processSingleBuffer:onGestureStart:onIncorrectGestureDetected:onGesturesFinished:
startReading
storeClassifierResult:imageData:signature:flags:
storeFACPoseBuffer:identifier:values:
storePRDBuffer:name:
storeTAValues:
storeUnencryptedVideoFrom:
stringWithFormat:
superclass
taOptions
time
timeIntervalSince1970
timestamp
timestampsButtonPressed
timestampsFAC
timestampsID
timestampsReferenceFAC
tracksWithMediaType:
unsignedIntegerValue
waitForCurrentFrameProcessingWithCompletion:
waitForModelLoaderIfNeededWithCompletion:
writeToFile:options:error:
writeToURL:atomically:
writeToURL:error:
writeToURL:options:error:
zone
@16@0:8
v24@0:8@16
Q16@0:8
v24@0:8Q16
v16@0:8
@"NSArray"
@"NSNumber"
@"NSString"
B24@0:8@16
#16@0:8
@24@0:8:16
@32@0:8:16@24
@40@0:8:16@24@32
B16@0:8
B24@0:8#16
B24@0:8:16
Vv16@0:8
^{_NSZone=}16@0:8
B24@0:8@"Protocol"16
@"NSString"16@0:8
v48@0:8@16@?24@?32@?40
v40@0:8@16@?24@?32
v35@0:8@16{PADClassifierFrameOptions=BBB}24q27
v32@0:8@16B24B28
v36@0:8@16B24@?28
v40@0:8@16@24@?32
v24@0:8@?16
v48@0:8@"PADClassifierRequest"16@?<v@?@"NSNumber">24@?<v@?>32@?<v@?@"NSError">40
v40@0:8@"PADClassifierRequest"16@?<v@?@"NSNumber">24@?<v@?@"NSError">32
v35@0:8@"PADFrame"16{PADClassifierFrameOptions=BBB}24q27
v32@0:8@"PADFrame"16B24B28
v36@0:8@"PADFrame"16B24@?<v@?@"PADClassifierResult"@"NSError">28
v40@0:8@"PADFrame"16@"NSArray"24@?<v@?@"PADClassifierResult"@"NSError">32
v24@0:8@?<v@?>16
@24@0:8q16
@"PADModelLoader"
@"<PADAlgorithms>"
@"<PADFaceActionSequenceClassifier>"
@"<PADTrajectoryAnalyzer>"
@"<PADPrintReplayDetector>"
@"<PADSelfieAnalyzer>"
@"NSMutableArray"
v48@0:8@16@24@32@40
v32@0:8^{__CVBuffer=}16@24
v40@0:8^{__CVBuffer=}16@24@32
v48@0:8@"PADClassifierResult"16@"NSData"24@"NSData"32@"NSData"40
v32@0:8^{__CVBuffer=}16@"NSString"24
v40@0:8^{__CVBuffer=}16@"NSString"24@"NSDictionary"32
v24@0:8@"NSDictionary"16
v24@0:8@"NSURL"16
@24@0:8@16
@"NSMutableDictionary"
@"NSObject<OS_dispatch_queue>"
@"NSURL"
v24@0:8@"PADFrame"16
@32@0:8@16@?24
v32@0:8@16@24
@"PADCounter"
@"NSError"
B32@0:8@16^@24
B32@0:8@"PADFrame"16^@24
@"NSArray"16@0:8
v24@0:8@"NSArray"16
v24@0:8q16
q16@0:8
@"<PADFaceActionModuleDelegate>"16@0:8
v24@0:8@"<PADFaceActionModuleDelegate>"16
@"NSNumber"16@0:8
@40@0:8q16@24@32
@"<PADFaceActionModuleDelegate>"
{CGSize="width"d"height"d}
v32@0:8@16@?24
v32@0:8@"NSArray"16@?<v@?@"NSError">24
@32@0:8@16@24
v36@0:8B16d20@28
@"<PADAuditDataRepository>"
v68@0:8@16@24Q32B40@?44@?52@?60
v68@0:8@"NSArray"16@"NSArray"24Q32B40@?<v@?@"NSNumber">44@?<v@?>52@?<v@?@"NSError">60
v24@0:8@"NSNumber"16
B24@0:8q16
@"<PADFaceActionClassifier>"
@40@0:8@16@24^@32
@56@0:8@16@24{CGSize=dd}32^@48
@72@0:8@16{?=qiIq}24{CGSize=dd}48^@64
^{opaqueCMSampleBuffer=}72@0:8@16{?=qiIq}24{CGSize=dd}48^@64
{?=qiIq}16@0:8
v40@0:8{?=qiIq}16
{?="value"q"timescale"i"flags"I"epoch"q}
@32@0:8@16^@24
@"AVAssetReader"
@"AVAssetReaderTrackOutput"
@24@0:8@?16
@"PADVNFacePoseModel"
@"PADVNPrintReplayS2Model"
d16@0:8
v24@0:8d16
{CGRect={CGPoint=dd}{CGSize=dd}}16@0:8
v48@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16
@"PADPose"
{CGRect="origin"{CGPoint="x"d"y"d}"size"{CGSize="width"d"height"d}}
^{__CVBuffer=}16@0:8
v24@0:8^{__CVBuffer=}16
{CGSize=dd}16@0:8
I16@0:8
v20@0:8I16
@"PADFrame"
v32@0:8@"PADSelfieAnalyzerRequest"16@?<v@?@"NSError">24
@"PADSelfieAnalyzerResult"16@0:8
@"PADSelfieAnalyzerRequest"
@"PADSelfieAnalyzerResult"
^{__CVBuffer=}32@0:8@16^@24
v20@0:8B16
Q24@0:8q16
@40@0:8@16q24^@32
B44@0:8^{__CVBuffer=}16^d24B32^@36
q64@0:8@16@24@32@40@48^@56
d40@0:8@16@24^@32
@"PADAlgorithmFACResult"40@0:8@"NSMutableArray"16q24^@32
q64@0:8@"NSNumber"16@"NSNumber"24@"NSNumber"32@"NSNumber"40@"NSNumber"48^@56
d40@0:8@"NSArray"16@"NSArray"24^@32
{shared_ptr<vision::mod::LivenessCheck_Options>="__ptr_"^{LivenessCheck_Options}"__cntrl_"^{__shared_weak_count}}
{shared_ptr<vision::mod::LivenessCheckPredictor>="__ptr_"^{LivenessCheckPredictor}"__cntrl_"^{__shared_weak_count}}
d32@0:8@16^@24
@"MLFeatureValue"24@0:8@"NSString"16
@"NSSet"16@0:8
@24@0:8^{__CVBuffer=}16
@32@0:8^{CGImage=}16^@24
B32@0:8^{CGImage=}16^@24
^{__CVBuffer=}
@"MLMultiArray"
@32@0:8^{__CVBuffer=}16^@24
@"MLModel"
@56@0:8@16@24@32@40@48
ARGB
@(#)PROGRAM:CoreIDVPAD  PROJECT:CoreIDV-5.421
?NSt3__120__shared_ptr_emplaceIN6vision3mod21LivenessCheck_OptionsENS_9allocatorIS3_EEEE
NSt3__120__shared_ptr_emplaceIN6vision3mod22LivenessCheckPredictorENS_9allocatorIS3_EEEE
333333
Pbt
com.apple.CoreIDV
IDVPAD
v16@?0@"NSError"8
v8@?0
@"PADSelfieAnalyzerRequest"8@?0
PADAntiSpoofing/%.0f
liveness.mp4
selfie.jpeg
flags
signature
livenessLabel
gestureSequence
assessmentsFAC
timestampsFAC
timestampsID
timestampsButtonPressed
assessmentFAC
assessmentTA
assessmentsPRD
assessmentFakePRD
assessmentLivePRD
assessmentID
ageLabel
sexLabel
skintoneLabel
ethnicityLabel
faceHairLabel
headgearLabel
glassesLabel
pose
assessmentsTA
deviceInfo
metadata.plist
LivenessCheck.plist
params.plist
%0.f.jpeg
FAC/%d/
PRD/
%@.png
FAC/FacePose
ProductType
<%@>
B8@?0
CIDVPAD.persist-capture-data
{ x:%.3f, y:%.3f, w:%.3f, h:%.3f }
-[PADVNSerialRequestsScheduler _dispatchVisionRequestForFrame:]
PADVNSerialRequestsScheduler.m
[_remainingRequests getValue] == 0
v24@?0@"NSObject<PADVNRequestProtocol>"8@"NSError"16
timestamps
This method should not be called on an VNRequest.
com.apple.coreidv.CoreIDVPAD.PADErrorDomain
CIDVPAD.process-cpu-only
CIDVPAD.use-verbose-logging
Recieved more faceprints than FAC timestamps. Each frame should contain no more than one faceprint.
@"PADAlgorithmFACFaceprintResult"8@?0
Could not complete faceprint detection on frames because another request is being processed.
An error occurred when detecting faceprints in the frame.
timestamp
PITCH
ROLL
PADFACFrameProcessingCompleteNotification
PADTAFrameProcessingCompleteNotification
PADPRDFrameProcessingCompleteNotification
PADFACFrameProcessingErrorOccurredNotification
PADFrameProcessingCompleteNotification
PADFrameUnretainedNotification
PADTAAdversarialThresholdMetNotification
PADFACGestureStarted
turnRight
turnLeft
lookUp
lookDown
blink
smile
openMouth
raiseEyebrows
idle
Unknown
shouldIgnore
assessment
nccSignal
-[_PADFaceActionSequenceClassifier assessment]
PADFaceActionSequenceClassifier.m
result >= 0 && result <= 1.0
@"NSData"8@?0
Unable to create asset track from asset
Unable to create AVAssetReader with file at URL %@
Unable to add output to AVAssetReader
com.apple.CoreIDVPAD.gestureComplete
gesture
result
bufferCount
gestureSkippedWithAccessibility
@"NSDictionary"8@?0
com.apple.CoreIDVPAD.gestureSequenceComplete
detectedGestureCount
gestureSkippedWithAcccessibility
gesture%d
PADModelLoader deallocated before the liveness models were loaded.
Could not complete identity verification because another request is being processed.
Could not obtain faceprint from selfie
Could not obtain attributes from selfie.
@"PADSelfieAnalyzerResult"8@?0
An error occurred when running PAD models on the selfie
@"PADPose"8@?0
-[PADVNFacePoseRequest _detectFaceBounds:error:]
PADVNFacePoseRequest.m
frame.faces.count == 0
Found unexpected number of faces %d
@"PADFace"8@?0
%0.f
roll
pitch
smilingConfidence
eyesClosedConfidence
Could not create predictor object.
FAC classifier failed with status: %d
Insufficient observations: %d. Expected at least: %d
TA could not convert image to required type.
TA model failed with status: %d
SC model failed with status: %d
Faceprints must contain at least 1 value.
Baseline faceprint must contain at least 1 dimension.
Dimension mismatch on faceprint %d.
ID model failed with status: %d
Invalid gesture %i.
iPhone10
vector
-[PADPrintReplayRequest _detectFaceBounds:error:]
PADPrintReplayRequest.m
-[PADPrintReplayRequest _doesNotMeetPoseRequirement:]
frame.faces.count == 1
%0.f-PRDs2
image
score0.1
embedding0.1
PADVNPrintReplayS2Model
mlmodelc
v24@?0@"MLModel"8@"NSError"16
image/Placeholder
softmax_expression
softmax_smile
softmax_pose
softmax_eye
pred_facepose
PADVNFacePoseModel
Successfully loaded FAC v%@
Starting to load ML models
Error loading ML models: %@
Successfully finished loading ML models
FAC Processing turned off
FAC Processing turned on
Refreshing FAC queue
SpoofClassification model failed (%{public}@).
Deallocating %@
Could not derive baseURL for persistence directory.
Could not create persistence directory (%{public}@).
Will persist capture data under: %{public}@.
Could not persist liveness video (%{public}@).
Could not persist selfie image (%{public}@).
Could not persist metadata file (%{public}@).
Could not persist params (%{public}@).
Could not find original params.
Could not retrieve ID Matching frames (%{public}@).
Could not create directory for ID Matching frames (%{public}@).
Could not persist ID matching frame %d (%{public}@).
Could not create directory for FAC matching frames (%{public}@).
Could not retrieve FAC frames (%{public}@).
Could not persist FAC frame (%d,%d) (%{public}@).
Did persist capture data.
Could not stat results dir at: %{public}@ (%{public}@).
Could not remove results dir at: %{public}@ (%{public}@).
Removed results dir because it was empty.
Could not create directory at %{public}@ (%{public}@).
Vision request dispatch failed (%{public}@).
Request %{public}@ failed (%{public}@).
Request %{public}@ did not obtain any observations.
Vision.FAC
Found %d faces in the frame. Invalidating current FAC buffer.
Pose: roll=%0.4f, pitch=%0.4f, yaw=%0.4f
Ignoring FAC composite: classifier is paused
Invalidating current FAC buffer.
SIML.FAC
FAC model failed (%{public}@).
FacePose composite set processing failed (%{public}@).
ID matching frames not selected for gesture. Selecting 2 random frames to use as default.
Invalid index %lu selected for ID matching frame in buffer.
Invalid index %lu selected for FAC reference frame in buffer.
Vision.ID
Could not obtain faceprint from frame.
Invalidating faceprints.
Vision.PRD
Found %d faces in the frame. Skipping PRD for this frame.
PRD unexpected number of results (%d).
SIML.TA
TA model failed (%{public}@).
TA: assessment=%d nccSignal=%0.4f
Stitch detected, nccSignal: %f, shouldIgnore: %d
Returning an invalid assessment because no frames have been analyzed by the TA module.
Returning an invalid high sensitivity max ncc signal because no frames have been analyzed by the TA module.
Returning an invalid low sensitivity max ncc signal because no frames have been analyzed by the TA module.
Returning an invalid max signal because no frames have been analyzed by the TA module.
Another liveness check is currently in progress. Ignoring request.
Requested gesture skipping but no gestures are being monitored.
Requested recorded gesture skipping but no gestures are being monitored.
Request to set gesture monitoring option to %ld failed because there aren't any gestures under monitoring
Request to restart gesture monitoring failed because there aren't any gestures under monitoring.
FAC Module: AX has been enabled.
Did finish monitoring gesture with assessment %d.
Obtained less faceprints than expected (%d).
Skipping gesture %d because attention awareness is disabled.
Skipping gesture %d because accessibility option is enabled.
Will start monitoring: %d.
The reported faceprint has invalid dimensions %d.
FaceprintRequest update failed (%{public}@).
AV.FrameImageData
AV.ReadFrame
Unable to retrieve next frame; AVAssetReaderTrackOutput not set
PADAnalyticsSendGestureCompleteEvent gesture = %@, result = %@, bufferCount = %lu, wasSkippedWithAccessibility = %d
PADAnalyticsSendGestureSequenceCompleteEvent detectedGestureCount = %lu, wasSkippedWithAccessibility = %d, scoresByGestures = %@
FaceLandmarksRequest update failed (%{public}@).
Processing ML Models with CPU Only
Loaded FacePose v%@, PRD S2 v%@
Deallocating: %@
Too many frames are currently retained (count = %d).
PAD.PixelBufferFromData
SelfieAnalyzer failed to set private faceAttributes revision: (%@), using default revision.
Could not obtain faceprint from selfie.
Could not obtain attributes from selfie.
SIML.ID
ID model failed (%{public}@).
Could not detect face bounds (%{public}@).
Could not obtain the face crop (%{public}@).
CoreML.FacePose
Could not complete FacePoseRequest (%{public}@).
FAC chose %lu frames for ID matching.
Invalid frame index %d.
The faceprint corresponding to observation %d was not found. Skipping frame selection for ID matching.
FAC used %lu frames as reference frames for the current gesture.
Faceprints must contain at least 1 value.
Baseline faceprint must contain at least 1 dimension.
Dimension mismatch on faceprint %d.
Could not complete detect face bounds (%{public}@).
More than one face detected in frame, skipping PRD assessment.
Does not meet pose requirement, skipping PRD assessment.
CoreML.PRDs2
PRD request stage 2 failed (%{public}@).
PRDs2: %.4f
Could not load PADVNPrintReplayS2Model.mlmodelc in the bundle resource
Could not load PADVNFacePoseModel.mlmodelc in the bundle resource
PADClassifierRequest
PADClassifierResult
_PADClassifier
PADClassifier
NSObject
_PADAuditDataRepository
PADAuditDataRepository
PADCounter
PADVNSerialRequestsScheduler
PADFrameMonitor
PADVNAdapter
PADVNRequestProtocol
VNFaceObservationAccepting
_PADFaceActionClassifier
PADFaceActionClassifier
_PADVNFaceprintDetector
PADVNFaceprintDetector
_PADPrintReplayDetector
PADPrintReplayDetector
_PADTrajectoryAnalyzer
PADTrajectoryAnalyzer
_PADFaceActionSequenceClassifier
PADFaceActionSequenceClassifier
PADFaceActionModuleDelegate
PADAVAssetReader
PADAVFrame
PADAVSerialAssetReader
PADModelLoader
PADPose
PADFace
PADFrame
PADSelfieAnalyzerRequest
PADSelfieAnalyzerResult
_PADSelfieAnalyzer
PADSelfieAnalyzer
PADVNFacePoseObservation
PADVNFacePoseRequest
PADAlgorithmFACFaceprintResult
PADAlgorithmFACResult
_PADAlgorithms
PADAlgorithms
PADPrintReplayObservation
PADPrintReplayRequest
PADVNPrintReplayS2ModelInput
MLFeatureProvider
PADVNPrintReplayS2ModelOutput
PADVNPrintReplayS2Model
PADVNFacePoseModelInput
PADVNFacePoseModelOutput
PADVNFacePoseModel
T@"MLMultiArray",&,N,V_score0_1
.cxx_destruct
T@"NSArray",&,N,V_timestampsFAC
T#,R
T@"NSArray",R,N
T@"<PADFaceActionModuleDelegate>",W,N,V_delegate
T@"NSNumber",&,N,V_assessmentTA
T@"MLMultiArray",&,N,V_embedding0_1
T@"PADSelfieAnalyzerResult",R,N
T@"MLMultiArray",&,N,V_softmax_expression
_assessmentsFAC
T@"MLMultiArray",&,N,V_softmax_pose
_bounds
T@"NSArray",&,N,V_assessmentsFAC
_config
T@"NSArray",&,N,V_faceprint
_dispatchVisionRequestForFrame:
T@"NSArray",&,N,V_faceprintsID
_framesAnalyzed
T@"NSArray",&,N,V_gestureSequence
_models
T@"NSArray",&,N,V_gestures
_output
T@"NSArray",&,N,V_timestampsButtonPressed
_selfie
T@"NSArray",R,N,V_results
analyzeObservationCompositeSet:
T@"NSMutableArray",&,N,V_assessmentsPRD
containsObject:
T@"NSMutableArray",&,N,V_faceprints
context
T@"NSMutableArray",&,N,V_gestureSequence
gesture
T@"NSMutableArray",&,N,V_timestampsID
glassesCategory
T@"NSNumber",&,N,V_assessment
initWithFormat:
T@"NSNumber",&,N,V_assessmentFakePRD
initWithModels:
T@"NSNumber",&,N,V_ignoredStitches
multiArrayValue
T@"NSNumber",&,N,V_maxNccHigh
numberWithBool:
T@"NSNumber",R,N
restart
T@"NSString",&,N,V_ageLabel
setOrientation:
T@"NSString",&,N,V_faceHairLabel
setSoftmax_eye:
T@"NSString",&,N,V_sexLabel
setYaw:
T@"NSString",R,C
.cxx_construct
T@"NSArray",&,N,V_assessmentsTA
JPEGRepresentationOfImage:colorSpace:options:
T@"NSArray",C,N
T@"<PADFaceActionModuleDelegate>",W,N
T@"NSNumber",&,N,V_assessmentID
T@"MLModel",R,N,V_model
T@"NSString",&,N,V_glassesLabel
T@"MLMultiArray",&,N,V_pred_facepose
_addNotificationCenterObservers
T@"MLMultiArray",&,N,V_softmax_eye
_assessmentsPRD
T@"MLMultiArray",&,N,V_softmax_smile
_buffer
T@"NSArray",&,N,V_assessmentsPRD
_currentObservationCompositeSet
T@"NSArray",&,N,V_faceprints
_ethnicityLabel
T@"NSArray",&,N,V_faces
_isDetectingFaceMovementGesture
T@"NSArray",&,N,V_gestureTypes
_onGestureStart
T@"NSArray",&,N,V_referenceFrameIndices
_result
T@"NSArray",&,N,V_timestampsID
_storeTAValues:
T@"NSMutableArray",&,N,V_assessmentsFAC
bundleForClass:
T@"NSMutableArray",&,N,V_assessmentsTA
containsString:
T@"NSMutableArray",&,N,V_faceprintsID
dealloc
T@"NSMutableArray",&,N,V_timestampsFAC
gestureSequence
T@"NSMutableArray",R,N,V_timestampsReferenceFAC
ignoredStitches
T@"NSNumber",&,N,V_assessmentFAC
initWithImageFromCGImage:error:
T@"NSNumber",&,N,V_assessmentLivePRD
isProxy
T@"NSNumber",&,N,V_livenessLabel
T@"NSNumber",&,N,V_maxNccLow
release
T@"NSSet",R,N
results
T@"NSString",&,N,V_ethnicityLabel
setPixelBuffer:
T@"NSString",&,N,V_headgearLabel
setUsesCPUOnly:
T@"NSString",&,N,V_skintoneLabel
startLivenessVideoProcessing:onGestureStart:onGesturesFinished:
T@"PADFrame",&,N,V_selfie
T@"PADPose",&,N,V_facePose
T@"PADPose",&,N,V_pose
T@"PADSelfieAnalyzerResult",R,N,V_result
T@"PADVNFacePoseModel",R,N,V_facePoseModel
T@"PADVNPrintReplayS2Model",R,N,V_printReplayS2Model
T@,W,N,V_pixelBuffer
TB,N,V_isHeadMovementDetected
TB,R,N
TB,R,N,V_idMatchingFramesSelected
TI,N,V_orientation
TQ,N,V_index
TQ,N,V_minNumberOfGestures
TQ,R
T^{__CVBuffer=},D,N
T^{__CVBuffer=},N,V_image
T^{__CVBuffer=},N,V_image_Placeholder
Td,N,V_assessment
Td,N,V_assessmentStage2
Td,N,V_eyesClosedConfidence
Td,N,V_pitch
Td,N,V_roll
Td,N,V_smilingConfidence
Td,N,V_timestamp
Td,N,V_yaw
Td,R,N
Tq,N
Tq,N,V_gesture
Tq,N,V_taOptions
Tq,R,N
T{?=qiIq},N,V_time
T{?=qiIq},N,V_timestamp
T{CGRect={CGPoint=dd}{CGSize=dd}},N,V_bounds
T{CGSize=dd},R,N
URLByAppendingPathComponent:
URLForResource:withExtension:
URLOfModelInThisBundle
URLsForDirectory:inDomains:
UTF8String
VN1uMyFtnYEWjbrdx3yAuDndKkPeyzNJhB
VN3iT1YRjjnIuELobV1olJiO1vvItN6Kdq
VN7exwFFmQF0AI9P7FjBljwEFu7QYUGCYE
VN7fiLHgGnvqPqG63cfDUCK4Xm8obUuWoP
_accessibilityOptionsEnabled
_ageLabel
_algorithms
_appendIDFaceprintResult:faceprint:
_areModelsLoading
_assessment
_assessmentFAC
_assessmentFakePRD
_assessmentID
_assessmentLivePRD
_assessmentStage2
_assessmentTA
_assessmentsTA
_assetReader
_audit
_auditStorePRDBuffer:name:
_completion
_consecutiveBuffersNotDetected
_counter
_createDirIfNotExists:
_createNewFACClassiferForGesture:
_currentFrameProcessingCompletion
_currentFrameTimestamp
_currentGestureBufferCount
_currentGestureIdx
_currentObservationComposite
_delegate
_detectFaceBounds:error:
_detectedGestures
_dir
_dispatchPRDRequestStage2ForFrame:error:
_doesNotMeetPoseRequirement:
_embedding0_1
_extractNestedTimestamps:
_extractTimestamps:
_eyesClosedConfidence
_facDidFinishProcessing
_facFramesToProcess
_facModule
_facOption
_facPoseValues
_faceHairLabel
_facePose
_facePoseModel
_faceprint
_faceprints
_faceprintsID
_faces
_finalize
_frameFromVideo:timestamp:size:error:
_generateDeviceInfoString
_gesture
_gestureSequence
_gestureTypes
_gestureTypesToSkip
_gestures
_glassesLabel
_handleGestureMonitorResult:
_handleIDMatchingAndReferenceFramesForFACResult:withObservationCompositeSet:
_hasSetRandomIDFrames
_headgearLabel
_idMatchingFramesSelected
_ignoredStitches
_image
_imageDataFromVideo:timestamp:size:error:
_imageSize
_image_Placeholder
_index
_isFACProcessing
_isHeadMovementDetected
_isProcessingFrame
_isReading
_livenessLabel
_maxNccHigh
_maxNccLow
_minNumberOfGestures
_minRequiredGestures
_model
_modelLoaderCompletion
_monitor
_newSampleBufferFromVideo:timestamp:size:error:
_obtainFaceCrop:error:
_onGesturesFinished
_onIncorrectGestureDetected
_orientation
_performSpoofClassificationWithCompletion:
_pitch
_pixelBuffer
_pose
_prdModule
_pred_facepose
_predictor
_prepareResultWithLabel:
_printReplayS2Model
_processObservationCompositeSetForIDMatchingFrames:
_processObservationCompositeSetWithFAC:
_processSingleBuffer
_queue
_referenceFrameIndices
_refreshQueuedFACFrames
_remainingRequests
_request
_requestError
_requests
_reset
_results
_retrieveFaceprintsFromPredictorUsingObservationCompositeSet:
_retrieveReferenceFramesFromPredictor
_roll
_score0_1
_scoresByGesture
_selfieModule
_sendFrameProcessingCompleteNotification
_setSize
_setup
_setupAssetReaderWithVideoURL:error:
_sexLabel
_shouldSkipAttentionAwareGestureAtIndex:
_shouldSkipGestureAtIndex:
_skintoneLabel
_smilingConfidence
_softmax_expression
_softmax_eye
_softmax_pose
_softmax_smile
_startNextGesture
_startProcessingNextFACFrameIfAvailable
_stitchCount
_storeBuffer:atURL:
_storeClassifierResult:imageData:signature:flags:
_storeCrop:forFrame:observation:
_storeFACPoseBuffer:identifier:values:
_storePRDBuffer:name:
_storeTAAssessment:nccSignal:frame:
_storeUnencryptedVideoFrom:
_taModule
_taOptions
_taValues
_time
_timestamp
_timestampsButtonPressed
_timestampsFAC
_timestampsID
_timestampsReferenceFAC
_wasGestureSkippedWithAccessibility
_wasSequenceSkippedWithAccessibility
_workQueue
_yaw
addObject:
addObjectsFromArray:
addObserver:selector:name:object:
addOutput:
ageCategory
ageLabel
allPoints
analyzeObservationComposite:
arrayWithCapacity:
arrayWithObjects:count:
assessment
assessmentFAC
assessmentFakePRD
assessmentID
assessmentLivePRD
assessmentStage2
assessmentTA
assessmentsFAC
assessmentsPRD
assessmentsTA
assetWithURL:
autorelease
boolValue
boundingBox
bounds
buffer
bytes
canAddOutput:
cancelReading
cancelWithCompletion:
class
close
conformsToProtocol:
contentsOfDirectoryAtPath:error:
contextWithOptions:
copy
copyItemAtPath:toPath:error:
copyItemAtURL:toURL:error:
copyNextSampleBuffer
count
countByEnumeratingWithState:objects:count:
createCGImage:fromRect:
createDirectoryAtPath:withIntermediateDirectories:attributes:error:
cropAndScaleBufferWithWidth:height:cropRect:format:imageCropAndScaleOption:options:error:calculatedNormalizedOriginOffset:calculatedScaleX:calculatedScaleY:
cvPixelBufferFromData:
debugDescription
decrement
decrementByValue:
defaultCenter
defaultManager
delegate
description
descriptorData
dictionaryWithCapacity:
dictionaryWithObjects:forKeys:count:
duration
elementCount
embedding0_1
enableAccessibilityOptions
errorWithDomain:code:userInfo:
ethnicityLabel
extent
eyesClosedConfidence
faceActionModuleDidFinishGestureDetectionWithAssessment:
faceAttributes
faceHairCategory
faceHairLabel
faceObservationWithRequestRevision:boundingBox:roll:yaw:pitch:
facePose
facePoseModel
faceprint
faceprints
faceprintsID
faces
fakeAssessment
featureNames
featureValueForName:
featureValueWithCGImage:pixelsWide:pixelsHigh:pixelFormatType:options:error:
featureValueWithImageAtURL:pixelsWide:pixelsHigh:pixelFormatType:options:error:
featureValueWithMultiArray:
featureValueWithPixelBuffer:
featuresAtIndex:
fileExistsAtPath:
fileExistsAtPath:isDirectory:
fileURLWithPath:
finishLivenessWithSelfie:idMatchingFaceprints:completion:
finishLivenessWithSelfie:performIDMatching:completion:
firstObject
floatValue
framesFromVideoURL:timestamps:error:
framesFromVideoURL:timestamps:size:error:
gestureTypes
gestures
getFACVersion
getPRDFakeFrameThreshold
getPRDLiveFrameThreshold
getValue
glassesLabel
handleObservationCompositeError:
handleResultForRequest:error:
hash
headgearLabel
idMatchingFramesSelected
identifier
image
imageBufferValue
imageWithCGImage:
imageWithCVPixelBuffer:
imageWithData:
image_Placeholder
imagesFromVideoURL:timestamps:error:
imagesFromVideoURL:timestamps:size:error:
increment
incrementByValue:
index
init
init:
init:models:
initWithAsset:error:
initWithCVPixelBuffer:orientation:options:
initWithCompletion:
initWithCompletionHandler:
initWithConfiguration:error:
initWithContentsOfURL:configuration:error:
initWithContentsOfURL:error:
initWithFeatureProviderArray:
initWithGesture:algorithms:models:
initWithImage:
initWithImageAtURL:error:
initWithImage_Placeholder:
initWithImage_PlaceholderAtURL:error:
initWithImage_PlaceholderFromCGImage:error:
initWithMLModel:
initWithQueue:
initWithScore0_1:embedding0_1:
initWithSoftmax_expression:softmax_smile:softmax_pose:softmax_eye:pred_facepose:
initWithTrack:outputSettings:
initWithVideoURL:error:
inputFaceObservations
intValue
integerValue
invalidate
isEqual:
isEqualToString:
isHeadMovementDetected
isKindOfClass:
isMemberOfClass:
label
landmarks
liveAssessment
livenessLabel
loadContentsOfURL:configuration:completionHandler:
loadWithConfiguration:completionHandler:
maxNccHigh
maxNccLow
metadata
minNumberOfGestures
model
modelDescription
modelWithContentsOfURL:configuration:error:
modelWithContentsOfURL:error:
mutableCopy
normalizedPoints
numberArray
numberWithDouble:
numberWithFloat:
numberWithInt:
numberWithInteger:
numberWithLongLong:
numberWithUnsignedInteger:
objectAtIndexedSubscript:
objectForKeyedSubscript:
observationCompositeSetSize
observationsFromRequest:
orientation
path
pathForResource:ofType:
pauseLiveness
performFAC:gesture:error:
performIDMatching:toFaceprints:error:
performOn:error:
performRequests:error:
performRequests:onCVPixelBuffer:orientation:error:
performSC:assessmentTA:assessmentFakePRD:assessmentLivePRD:assessmentID:error:
performSelector:
performSelector:withObject:
performSelector:withObject:withObject:
performTA:nccSignal:isSensitive:error:
persistentDomainForName:
pitch
pixelBuffer
pointCount
pose
postNotificationName:object:
postNotificationName:object:userInfo:
pred_facepose
predictionFromFeatures:error:
predictionFromFeatures:options:error:
predictionFromImage:error:
predictionFromImage_Placeholder:error:
predictionsFromBatch:options:error:
predictionsFromInputs:options:error:
prepareToResumeLiveness
printReplayS2Model
processFacePoseCompositeSet:gesture:error:
processFrame:
processFrames:completion:
processLivenessFrame:withOptions:taOptions:
processRecordedLivenessFrame:withPRD:FAC:
processRequest:completion:
raise:format:
referenceFrameIndices
removeAllObjects
removeItemAtPath:error:
removeObjectAtIndex:
render:toCVPixelBuffer:
requestsForFrame:handler:
requiredObservationSetSizeFAC:
respondsToSelector:
restartGesture
restartLivenessGesture
result
resumeLiveness
retain
retainCount
retrieveNextFrame
roll
score0_1
self
selfie
setAgeLabel:
setAlwaysCopiesSampleData:
setAssessment:
setAssessmentFAC:
setAssessmentFakePRD:
setAssessmentID:
setAssessmentLivePRD:
setAssessmentStage2:
setAssessmentTA:
setAssessmentsFAC:
setAssessmentsPRD:
setAssessmentsTA:
setBounds:
setBuffer:
setComputeUnits:
setDelegate:
setEmbedding0_1:
setEthnicityLabel:
setEyesClosedConfidence:
setFACOption:
setFaceHairLabel:
setFacePose:
setFaceprint:
setFaceprints:
setFaceprintsID:
setFaces:
setGesture:
setGestureSequence:
setGestureTypes:
setGestures:
setGlassesLabel:
setHeadgearLabel:
setIgnoredStitches:
setImage:
setImageWithCGImage:error:
setImageWithURL:error:
setImage_Placeholder:
setImage_PlaceholderWithCGImage:error:
setImage_PlaceholderWithURL:error:
setIndex:
setInputFaceObservations:
setIsHeadMovementDetected:
setLivenessLabel:
setMaxNccHigh:
setMaxNccLow:
setMinNumberOfGestures:
setObject:forKey:
setObject:forKeyedSubscript:
setPitch:
setPose:
setPred_facepose:
setReferenceFrameIndices:
setRevision:error:
setRoll:
setScore0_1:
setSelfie:
setSexLabel:
setSkintoneLabel:
setSmilingConfidence:
setSoftmax_expression:
setSoftmax_pose:
setSoftmax_smile:
setTaOptions:
setTime:
setTimeRange:
setTimestamp:
setTimestampsButtonPressed:
setTimestampsFAC:
setTimestampsID:
setToValue:
setWithArray:
sexLabel
shouldProcessFrame:
size
skintoneLabel
skipGesture
skipLivenessGesture
skipRecordedGesture
skipRecordedLivenessGesture
smilingConfidence
softmax_expression
softmax_eye
softmax_pose
softmax_smile
standardUserDefaults
startLiveness:onGestureStart:onIncorrectGestureDetected:onGesturesFinished:
startLivenessCheck:gestureTypes:minNumberOfGestures:processSingleBuffer:onGestureStart:onIncorrectGestureDetected:onGesturesFinished:
startReading
storeClassifierResult:imageData:signature:flags:
storeFACPoseBuffer:identifier:values:
storePRDBuffer:name:
storeTAValues:
storeUnencryptedVideoFrom:
stringWithFormat:
superclass
taOptions
time
timeIntervalSince1970
timestamp
timestampsButtonPressed
timestampsFAC
timestampsID
timestampsReferenceFAC
tracksWithMediaType:
unsignedIntegerValue
waitForCurrentFrameProcessingWithCompletion:
waitForModelLoaderIfNeededWithCompletion:
writeToFile:options:error:
writeToURL:atomically:
writeToURL:error:
writeToURL:options:error:
zone
@16@0:8
v24@0:8@16
Q16@0:8
v24@0:8Q16
v16@0:8
@"NSArray"
@"NSNumber"
@"NSString"
B24@0:8@16
#16@0:8
@24@0:8:16
@32@0:8:16@24
@40@0:8:16@24@32
B16@0:8
B24@0:8#16
B24@0:8:16
Vv16@0:8
^{_NSZone=}16@0:8
B24@0:8@"Protocol"16
@"NSString"16@0:8
v48@0:8@16@?24@?32@?40
v40@0:8@16@?24@?32
v35@0:8@16{PADClassifierFrameOptions=BBB}24q27
v32@0:8@16B24B28
v36@0:8@16B24@?28
v40@0:8@16@24@?32
v24@0:8@?16
v48@0:8@"PADClassifierRequest"16@?<v@?@"NSNumber">24@?<v@?>32@?<v@?@"NSError">40
v40@0:8@"PADClassifierRequest"16@?<v@?@"NSNumber">24@?<v@?@"NSError">32
v35@0:8@"PADFrame"16{PADClassifierFrameOptions=BBB}24q27
v32@0:8@"PADFrame"16B24B28
v36@0:8@"PADFrame"16B24@?<v@?@"PADClassifierResult"@"NSError">28
v40@0:8@"PADFrame"16@"NSArray"24@?<v@?@"PADClassifierResult"@"NSError">32
v24@0:8@?<v@?>16
@24@0:8q16
@"PADModelLoader"
@"<PADAlgorithms>"
@"<PADFaceActionSequenceClassifier>"
@"<PADTrajectoryAnalyzer>"
@"<PADPrintReplayDetector>"
@"<PADSelfieAnalyzer>"
@"NSMutableArray"
v48@0:8@16@24@32@40
v32@0:8^{__CVBuffer=}16@24
v40@0:8^{__CVBuffer=}16@24@32
v48@0:8@"PADClassifierResult"16@"NSData"24@"NSData"32@"NSData"40
v32@0:8^{__CVBuffer=}16@"NSString"24
v40@0:8^{__CVBuffer=}16@"NSString"24@"NSDictionary"32
v24@0:8@"NSDictionary"16
v24@0:8@"NSURL"16
@24@0:8@16
@"NSMutableDictionary"
@"NSObject<OS_dispatch_queue>"
@"NSURL"
v24@0:8@"PADFrame"16
@32@0:8@16@?24
v32@0:8@16@24
@"PADCounter"
@"NSError"
B32@0:8@16^@24
B32@0:8@"PADFrame"16^@24
@"NSArray"16@0:8
v24@0:8@"NSArray"16
v24@0:8q16
q16@0:8
@"<PADFaceActionModuleDelegate>"16@0:8
v24@0:8@"<PADFaceActionModuleDelegate>"16
@"NSNumber"16@0:8
@40@0:8q16@24@32
@"<PADFaceActionModuleDelegate>"
{CGSize="width"d"height"d}
v32@0:8@16@?24
v32@0:8@"NSArray"16@?<v@?@"NSError">24
@32@0:8@16@24
v36@0:8B16d20@28
@"<PADAuditDataRepository>"
v68@0:8@16@24Q32B40@?44@?52@?60
v68@0:8@"NSArray"16@"NSArray"24Q32B40@?<v@?@"NSNumber">44@?<v@?>52@?<v@?@"NSError">60
v24@0:8@"NSNumber"16
B24@0:8q16
@"<PADFaceActionClassifier>"
@40@0:8@16@24^@32
@56@0:8@16@24{CGSize=dd}32^@48
@72@0:8@16{?=qiIq}24{CGSize=dd}48^@64
^{opaqueCMSampleBuffer=}72@0:8@16{?=qiIq}24{CGSize=dd}48^@64
{?=qiIq}16@0:8
v40@0:8{?=qiIq}16
{?="value"q"timescale"i"flags"I"epoch"q}
@32@0:8@16^@24
@"AVAssetReader"
@"AVAssetReaderTrackOutput"
@24@0:8@?16
@"PADVNFacePoseModel"
@"PADVNPrintReplayS2Model"
d16@0:8
v24@0:8d16
{CGRect={CGPoint=dd}{CGSize=dd}}16@0:8
v48@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16
@"PADPose"
{CGRect="origin"{CGPoint="x"d"y"d}"size"{CGSize="width"d"height"d}}
^{__CVBuffer=}16@0:8
v24@0:8^{__CVBuffer=}16
{CGSize=dd}16@0:8
I16@0:8
v20@0:8I16
@"PADFrame"
v32@0:8@"PADSelfieAnalyzerRequest"16@?<v@?@"NSError">24
@"PADSelfieAnalyzerResult"16@0:8
@"PADSelfieAnalyzerRequest"
@"PADSelfieAnalyzerResult"
^{__CVBuffer=}32@0:8@16^@24
v20@0:8B16
Q24@0:8q16
@40@0:8@16q24^@32
B44@0:8^{__CVBuffer=}16^d24B32^@36
q64@0:8@16@24@32@40@48^@56
d40@0:8@16@24^@32
@"PADAlgorithmFACResult"40@0:8@"NSMutableArray"16q24^@32
q64@0:8@"NSNumber"16@"NSNumber"24@"NSNumber"32@"NSNumber"40@"NSNumber"48^@56
d40@0:8@"NSArray"16@"NSArray"24^@32
{shared_ptr<vision::mod::LivenessCheck_Options>="__ptr_"^{LivenessCheck_Options}"__cntrl_"^{__shared_weak_count}}
{shared_ptr<vision::mod::LivenessCheckPredictor>="__ptr_"^{LivenessCheckPredictor}"__cntrl_"^{__shared_weak_count}}
d32@0:8@16^@24
@"MLFeatureValue"24@0:8@"NSString"16
@"NSSet"16@0:8
@24@0:8^{__CVBuffer=}16
@32@0:8^{CGImage=}16^@24
B32@0:8^{CGImage=}16^@24
^{__CVBuffer=}
@"MLMultiArray"
@32@0:8^{__CVBuffer=}16^@24
@"MLModel"
@56@0:8@16@24@32@40@48
ARGB
