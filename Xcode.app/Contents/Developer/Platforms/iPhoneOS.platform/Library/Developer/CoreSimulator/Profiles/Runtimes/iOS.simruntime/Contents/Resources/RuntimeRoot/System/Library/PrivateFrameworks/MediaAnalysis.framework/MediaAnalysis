@(#)PROGRAM:MediaAnalysis  PROJECT:MediaAnalysis-1
&2>JVbn
N2ma19CameraMotionSegmentE
>N2ma19SubtleMotionSegmentE
B`e>;
fff>
?16MAComputeRequest
ffffff
$CV&
C2wACA
?22MAImageAnalysisRequest
NSt3__120__shared_ptr_emplaceI25VCPImageHumanPoseAnalyzerNS_9allocatorIS1_EEEE
N2ma17DescriptorSegmentE
16VCPProtoKeypoint
B>fff?
G!?=
Ga>R
=q=J
ff&?R
Q8?H
?N4dlib7array2dIhNS_33memory_manager_stateless_kernel_1IcEEEE
N4dlib10enumerableIhEE
N4dlib33memory_manager_stateless_kernel_1IhEE
14VCPProtoBounds
N2ma11EncodeStatsE
N2ma15EncodeStatsAVE1E
N2ma15EncodeStatsAVE2E
N2ma13EncodeStatsHWE
N2ma13EncodeStatsSWE
<0L&=!
<yX(=4
<0L&=!
<yX(=
b=;p
Sc=;p
<5^:
e=X94
Y=X94</n#
=B`e<M
u`=e
w=B`e
=333?fff?ff
?N2ma24FineSubjectMotionSegmentE
NSt3__120__shared_ptr_emplaceI21VCPCNNEspressoContextNS_9allocatorIS1_EEEE
@lwh
333?
G20MAImageComputeResult
zt?333333
?333?+
>N2ma22InterestingnessSegmentE
?ffffff
fff?
!$'*-0369<?BEHKNQ
TWZ]`cfilorux{~
@oDA
>43s?433?
@333?
N2ma19MovingObjectSegmentE
333333
N2ma18ObstructionSegmentE
N2ma14QualitySegmentE
N2ma15RotationSegmentE
@N2ma12SceneSegmentE
N2ma7SegmentE
MbP?
@ffffff
N4dlib17sequence_kernel_2INS_21lbfgs_search_strategy11data_helperENS_33memory_manager_stateless_kernel_1IcEEEE
N4dlib10enumerableINS_21lbfgs_search_strategy11data_helperEEE
N4dlib7removerINS_21lbfgs_search_strategy11data_helperEEE
N4dlib33memory_manager_stateless_kernel_1IdEE
NSt3__110__function6__funcINS_6__bindIPFdRKN4dlib6matrixIdLl0ELl0ENS3_33memory_manager_stateless_kernel_1IcEENS3_16row_major_layoutEEEPfSB_SB_SB_bEJRKNS_12placeholders4__phILi1EEERSB_SJ_RA12_fRA126_fbEEENS_9allocatorISO_EEFdS8_EEE
NSt3__110__function6__baseIFdN4dlib6matrixIdLl0ELl0ENS2_33memory_manager_stateless_kernel_1IcEENS2_16row_major_layoutEEEEEE
NSt3__16__bindIPFdRKN4dlib6matrixIdLl0ELl0ENS1_33memory_manager_stateless_kernel_1IcEENS1_16row_major_layoutEEEPfS9_S9_S9_bEJRKNS_12placeholders4__phILi1EEERS9_SH_RA12_fRA126_fbEEE
NSt3__118__weak_result_typeIPFdRKN4dlib6matrixIdLl0ELl0ENS1_33memory_manager_stateless_kernel_1IcEENS1_16row_major_layoutEEEPfS9_S9_S9_bEEE
NSt3__110__function6__funcINS_6__bindIPFN4dlib6matrixIdLl0ELl0ENS3_33memory_manager_stateless_kernel_1IcEENS3_16row_major_layoutEEERKS8_PfSB_SB_SB_bEJRKNS_12placeholders4__phILi1EEERSB_SJ_RA12_fRA126_fbEEENS_9allocatorISO_EEFS8_S8_EEE
NSt3__110__function6__baseIFN4dlib6matrixIdLl0ELl0ENS2_33memory_manager_stateless_kernel_1IcEENS2_16row_major_layoutEEES7_EEE
NSt3__16__bindIPFN4dlib6matrixIdLl0ELl0ENS1_33memory_manager_stateless_kernel_1IcEENS1_16row_major_layoutEEERKS6_PfS9_S9_S9_bEJRKNS_12placeholders4__phILi1EEERS9_SH_RA12_fRA126_fbEEE
NSt3__118__weak_result_typeIPFN4dlib6matrixIdLl0ELl0ENS1_33memory_manager_stateless_kernel_1IcEENS1_16row_major_layoutEEERKS6_PfS9_S9_S9_bEEE
NSt3__117bad_function_callE
N4dlib11fatal_errorE
N4dlib5errorE
NSt3__119basic_ostringstreamIcNS_11char_traitsIcEENS_9allocatorIcEEEE
NSt3__115basic_stringbufIcNS_11char_traitsIcEENS_9allocatorIcEEEE
NSt3__110__function6__funcINS_6__bindIPFdRKN4dlib6matrixIdLl0ELl0ENS3_33memory_manager_stateless_kernel_1IcEENS3_16row_major_layoutEEEPfSB_SB_SB_SB_SB_SB_PiSB_SB_EJRKNS_12placeholders4__phILi1EEERSB_RA12_fSK_SK_SK_SK_SB_SC_SK_SK_EEENS_9allocatorISN_EEFdS8_EEE
NSt3__16__bindIPFdRKN4dlib6matrixIdLl0ELl0ENS1_33memory_manager_stateless_kernel_1IcEENS1_16row_major_layoutEEEPfS9_S9_S9_S9_S9_S9_PiS9_S9_EJRKNS_12placeholders4__phILi1EEERS9_RA12_fSI_SI_SI_SI_S9_SA_SI_SI_EEE
NSt3__118__weak_result_typeIPFdRKN4dlib6matrixIdLl0ELl0ENS1_33memory_manager_stateless_kernel_1IcEENS1_16row_major_layoutEEEPfS9_S9_S9_S9_S9_S9_PiS9_S9_EEE
NSt3__110__function6__funcINS_6__bindIPFN4dlib6matrixIdLl0ELl0ENS3_33memory_manager_stateless_kernel_1IcEENS3_16row_major_layoutEEERKS8_PfSB_SB_SB_SB_SB_SB_PiSB_SB_SB_EJRKNS_12placeholders4__phILi1EEERSB_RA12_fSK_SK_SK_SK_SB_SC_SK_SK_SK_EEENS_9allocatorISN_EEFS8_S8_EEE
NSt3__16__bindIPFN4dlib6matrixIdLl0ELl0ENS1_33memory_manager_stateless_kernel_1IcEENS1_16row_major_layoutEEERKS6_PfS9_S9_S9_S9_S9_S9_PiS9_S9_S9_EJRKNS_12placeholders4__phILi1EEERS9_RA12_fSI_SI_SI_SI_S9_SA_SI_SI_SI_EEE
NSt3__118__weak_result_typeIPFN4dlib6matrixIdLl0ELl0ENS1_33memory_manager_stateless_kernel_1IcEENS1_16row_major_layoutEEERKS6_PfS9_S9_S9_S9_S9_S9_PiS9_S9_S9_EEE
NSt3__110__function6__funcINS_6__bindIPFdRKN4dlib6matrixIdLl0ELl0ENS3_33memory_manager_stateless_kernel_1IcEENS3_16row_major_layoutEEEPfSB_EJRKNS_12placeholders4__phILi1EEERA126_fRA189_fEEENS_9allocatorISN_EEFdS8_EEE
NSt3__16__bindIPFdRKN4dlib6matrixIdLl0ELl0ENS1_33memory_manager_stateless_kernel_1IcEENS1_16row_major_layoutEEEPfS9_EJRKNS_12placeholders4__phILi1EEERA126_fRA189_fEEE
NSt3__118__weak_result_typeIPFdRKN4dlib6matrixIdLl0ELl0ENS1_33memory_manager_stateless_kernel_1IcEENS1_16row_major_layoutEEEPfS9_EEE
N2ma17SlowMotionSegmentE
?N2ma20SubjectMotionSegmentE
>N2ma12TrackSegmentE
?33s?
ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+%
22MAMovieAnalysisRequest
228VCPProtoImageHumanPoseResult
(4@LXdp|
=AB/'
R[DmPJ>
@Z_g@
#=H!
@333?
?ff&?
>fff
?333?
mcpl)
xfua2vpe
Q9RI
Q9RI
7:RI
Q9RI
d*;4
>;_)K;
DX;B`e;
{r;l
k;B`e;
Q;_)K;
#;RI
7:RI
Q9RI
d*;4
7;_)K;
DX;B`e;
{r;o
{r;B`e;
DX;_)K;4
d*;RI
7:RI
Q9RI
7;_)K;
y;KY
3"<U
-<X94<
b<B`e<
4o<E
{r<!
ew<l
{r<E
h<B`e<e
#9<X94<
3"<RI
^;_)K;4
7:RI
Q9RI
k;KY
Q:RI
Q9RI
k:RI
DX;$
d*<4
?F<4
DX;4
Q:RI
>;B`e;'
#<X94<
z<KY
$=yX(=V
7=5^:=[
2D=]mE=
?F=]mE=
2D=\
<=5^:=
-2=2
+=yX(=
D<X94<
;B`e;
>;RI
{r<&
=R' =T
b=B`e=B>h=h
4o= Aq=
{r=F
{r= Aq=
(m=h
j=B>h=B`e=e
WJ=9
E6=|
%=R' =Qk
k:RI
7;B`e;
?$=V
b=B>h=D
s=Gry=
\~=n
\~=Gry=F
m=B>h=e
?$=v
;B`e;4
DX;'
3"<4
=R' =U
(=W[1=Z
B=_)K=a
S=d]\=
d=d]\=a
S=_)K=\
9=W[1=U
(=R' =P
j<<b
ew=I
{r<b
j<<>
ew<M
ew<>
=yX(=
A=;pN=
h=k+v=n
=k+v=
[=;pN=
5=yX(=
ew<b
7:RI
{r<M
7x=J{
/>:#
O/<N
=yX(=Y
U=B`e=
9#>/
$>0L&>
d*>1
'>0L&>/
Yu=B`e=
?F=Y
7=yX(=u
7;KY
=z6+=
/;=:
K=d]\=D
m=I.
V,>i
9>lx:>
<>6<=>
=>6<=>
;>lx:>l
u1>i
V,>
m=d]\=:
/;=z6+=
;_)K;
{r<)\
q,=6<==
Ga=F
s=J{
u >/
!N>`vO>
|P>`vO>
2D>7
~;>~
N=6<==
;_)K;o
<X94<
=z6+=6<==
ew=o
G!>ff&>
+>io0>F%5>
9>[B>>%uB>
qJ>_
QZ>?
+e>x
F>%uB>[B>>
9>F%5>io0>
+>ff&>
P=6<==z6+=u
/]<X94<r
Q9RI
{r<N
=yX(=
x=p_
/>aT
X>RI]>
Ga>f
n>W[q>}
=y>lxz>Zd{>
|>HP|>HP|>
|>Zd{>lxz>
s>W[q>
rh>f
Ga>RI]>P
/>o
/;=yX(=
y;RI
d*;KY
d*<b
ew=p_
2>Gr9>[
sW>v
8g>1
|>n4
3b>v
?>Gr9>
%>d;
Ga=:
d*<o
?F=d]\=F
8V>?
c>gDi>
t>5^z>$
>5^z>
4o>gDi>
s=d]\=
<;p
:_)K;RI
=yX(=
m=J{
P>>yX>
_>>yX>
kI>n
v>=yX(=
;_)K;
T<KY
L=B`e=I.
..>}
6>I.?>
h>iop>~
x>iop>
DX>r
G>I.?>}
c>]
=B`e=
;B`e;X9
c<)\
>B>(>W[1>lx:>
C>lx:>W[1>B>(>-!
;B`e;o
5=;pN=
>R' >z
!=>]
|P>]
g3>z
)>R' >b
xi=;pN=
{r;'
>R' >
m4>m
ZS>-
-r>HP|>
>HP|>
J*>R' >
?F<N
h=J{
H?>:#J>+
U>:#J>
m4>z
)>-!
=R' =
sW=k+v=(
>B>(>
g3>m
>>:#J>=
U>w-a>
l>w-a>=
U>:#J>m
g3>B>(>
=k+v=
:=R' =
#<B`
d*;u
Sc=n
%>W[1>
U>w-a>
\m>~
\m>w-a>+
!=>W[1>
Jj<1
d*;RI
=W[1=
..>lx:>]
ZS>]
F>lx:>
AO=W[1=
5<HP
7;RI
|P>-
:_)K;|
?$=\
Sc=o
/>>y
2>I.?>_
6Z>_
L>I.?>
%>>y
/>n4
Sc=\
;_)K;4
+=_)K=
(m=p
q,>#
q?_
q?h
q,>d;
(m=_)K=V
ew=r
\m>HP|>
c?r
>HP|>
%>>y
ew=a
2=tF
:B`e;
9=d]\=I
=d]\=Z
9=Qk
u`<RI
;B`e;
=R' =
d=&S
:p>7
R&?t
@=R' =
l=q=
~*>Gr9>p
H>>yX>
f%?=
+?-C,?
,?-C,?
h>>yX>p
H>Gr9>
{r<U
:0>[
_>iop>I
;.?{
;.?$
@"?$
>iop>
G!><
;X94<
fU=$
>ff&>
y'?u
c,?r
0?EG2?}
7?^K8?Y
8?^K8?
+5?}
3?EG2?
c,?u
y'?j
8V>9
5>ff&>
fU=|
<X94<B`
?n4 ?
,?I./?
1?I./?D
#?n4 ?
E6=tF
u >io0>
t>J{
$?B>(?
1?X94?
6?X94?
+?B>(?
@>io0>
@=B>h=
$>F%5>
sW>gDi>
V?E
%?gD)?
,?R'0?
S3?0L6?
>?R'@?
8E?a
B?R'@?_
9?0L6?
S3?R'0?
,?gD)?"
V?0
~{>gDi>
F>F%5>/
=B>h=
B<HP
41?4
=?io@?
H?q=J?
~K?(~L?6<M?H
M?6<M?(~L?
~K?q=J?U
B?io@?d
41?h
4o>v
WJ=F
s=Nb
->[B>>r
4!?"
1?]m5?
I<?Di??aTB?
O?2UP?
aQ?2UP?
E?aTB?Di??
8?]m5?
3b>r
O>[B>>V
=yX(=
N=Gry=*
0>%uB>
8g>5^z>
?n4 ?
$?gD)?h
F?GrI?
HN?2UP?
&R?F
V?p_W?
W?p_W?
&R?2UP?
K?GrI?}
-?gD)?
$?n4 ?
>5^z>
T>%uB>|
=Gry=
N=yX(=
Q<KY
S#>4
#?B>(?
41?]m5?c
A?o
X?GrY?
Z??W[?
[?h"\?h"\?
[??W[?
Z?GrY?~
\=?c
9?]m5?
,?B>(?
S#><
ZS=V
Y<RI
sW=n
qJ>RI]>
+?R'0?4
D?yXH?q
Q?tFT?}
Z?HP\?
_?R'`?
h`?R'`?$
]?HP\?h
V?tFT?n
K?yXH?
4?R'0?
p>RI]>
qJ>~
sW=2
<RI
~;>_
|@?o
D?yXH?
UO?A
R?]mU?~
c?=,d?Ttd?Ttd?=,d?
X?]mU?A
K?yXH?o
Ga>_
~;>
y'?D
1?0L6?
:?Di??
UO?X
.^?Nb`?aTb?
d?]me?
eg?+
Dh?+
f?]me?
d?aTb?Nb`?
UO?q
C?Di??
:?0L6?
V,>,
<B`e<
b='1
rh>[
*?I./?X94?
=?aTB?}
U?gDY?
I\?gDY?
F?aTB?d
9?X94?I./?u
rh>O
/>G
<B`e<
=5^:=B`e=q=
k>n4
;?io@?
E?GrI?
Q?]mU?
&b?O
(l?mVm??5n?
n??5n?mVm?
X?]mU?n
M?GrI?
E?io@?
=B`e=5^:=
<=B>h=
q?aT
:P?tFT?~
m?-!o?
Np?-!o?
X?tFT?
a!>r
=B>h=[
4o<z6
9#>X
\>W[q>
f%?
;?R'@?
g?U0j?
n?2Up?N
q?2Up?
l?U0j?
E?R'@?C
>W[q>?
(m=M
G!?=
,?EG2?
K?2UP?O
\?Nb`?}
?t?]mu?KYv?
v?KYv?]mu?
?t?<
c?Nb`?$
T?2UP?
7?EG2?$
G!?Zd
{r<V}
>0L&>l
;.?}
&R?+
^?aTb?
t?KYv?
w?>yx?
y?,ey?,ey?
y?>yx?
w?KYv?
e?aTb?
J9?}
9>0L&>tF
4o=\
B=Qk
2D= Aq=
'>lx:>
!N>x
8E?q=J?
X?HP\?
m?2Up?<
v?'1x?
ky?'1x?
r?2Up?
H`?HP\?~
O?q=J?
+5?{
!N>lx:>
= Aq=
t<RI
=]mE=
;>`vO>x
~K?2UP?
U?GrY?
a?]me?5
(l?-!o?N
?t?KYv?'1x?^
{?(~|?
|?(~|?
y?'1x?KYv?
?t?N
q?-!o?
(l?5
h?]me?
]?GrY?
U?2UP?
=y>x
d>`vO>
{r=]mE=
u<RI
ew<3
?F=F
+e>lxz>B>
eG?(~L?
aQ?4
j?mVm?
Np?S
s?]mu?
/|?q
w?]mu?S
Np?mVm?
aQ?(~L?
>lxz>
^)>=
?F=RI
)>6<=>
e>Zd{>9
R&?-C,?
$H?6<M?
V??W[?$
eg?c
j??5n?
s?KYv?>yx?
Wz?>yx?KYv?
4q??5n?c
_??W[?
&R?6<M?
2?-C,?
A ?:#
>Zd{>
4Q>6<=>
Yu=*
2?^K8?
R?p_W?
[?R'`?=,d?+
z?(~|?H
}?(~|?
g?=,d?R'`?
[?p_W?
=?^K8?\
d*>+
f>HP|>L7
W?h"\?
h`?Ttd?
Dh?6
>w?,ey?
C{?,ey?
Dh?Ttd?
h`?h"\?
!>?Y
>HP|>B
 <X9
f>HP|>L7
W?h"\?
h`?Ttd?
Dh?6
>w?,ey?
C{?,ey?
Dh?Ttd?
h`?h"\?
!>?Y
>HP|>B
 <X9
Yu=*
2?^K8?
R?p_W?
[?R'`?=,d?+
z?(~|?H
}?(~|?
g?=,d?R'`?
[?p_W?
=?^K8?\
d*>+
)>6<=>
e>Zd{>9
R&?-C,?
$H?6<M?
V??W[?$
eg?c
j??5n?
s?KYv?>yx?
Wz?>yx?KYv?
4q??5n?c
_??W[?
&R?6<M?
2?-C,?
A ?:#
>Zd{>
4Q>6<=>
ew<3
?F=F
+e>lxz>B>
eG?(~L?
aQ?4
j?mVm?
Np?S
s?]mu?
/|?q
w?]mu?S
Np?mVm?
aQ?(~L?
>lxz>
^)>=
?F=RI
=]mE=
;>`vO>x
~K?2UP?
U?GrY?
a?]me?5
(l?-!o?N
?t?KYv?'1x?^
{?(~|?
|?(~|?
y?'1x?KYv?
?t?N
q?-!o?
(l?5
h?]me?
]?GrY?
U?2UP?
=y>x
d>`vO>
{r=]mE=
u<RI
2D= Aq=
'>lx:>
!N>x
8E?q=J?
X?HP\?
m?2Up?<
v?'1x?
ky?'1x?
r?2Up?
H`?HP\?~
O?q=J?
+5?{
!N>lx:>
= Aq=
t<RI
{r<V}
>0L&>l
;.?}
&R?+
^?aTb?
t?KYv?
w?>yx?
y?,ey?,ey?
y?>yx?
w?KYv?
e?aTb?
J9?}
9>0L&>tF
4o=\
B=Qk
(m=M
G!?=
,?EG2?
K?2UP?O
\?Nb`?}
?t?]mu?KYv?
v?KYv?]mu?
?t?<
c?Nb`?$
T?2UP?
7?EG2?$
G!?Zd
4o<z6
9#>X
\>W[q>
f%?
;?R'@?
g?U0j?
n?2Up?N
q?2Up?
l?U0j?
E?R'@?C
>W[q>?
<=B>h=
q?aT
:P?tFT?~
m?-!o?
Np?-!o?
X?tFT?
a!>r
=B>h=[
=5^:=B`e=q=
k>n4
;?io@?
E?GrI?
Q?]mU?
&b?O
(l?mVm??5n?
n??5n?mVm?
X?]mU?n
M?GrI?
E?io@?
=B`e=5^:=
<B`e<
b='1
rh>[
*?I./?X94?
=?aTB?}
U?gDY?
I\?gDY?
F?aTB?d
9?X94?I./?u
rh>O
/>G
<B`e<
y'?D
1?0L6?
:?Di??
UO?X
.^?Nb`?aTb?
d?]me?
eg?+
Dh?+
f?]me?
d?aTb?Nb`?
UO?q
C?Di??
:?0L6?
V,>,
~;>_
|@?o
D?yXH?
UO?A
R?]mU?~
c?=,d?Ttd?Ttd?=,d?
X?]mU?A
K?yXH?o
Ga>_
~;>
Y<RI
sW=n
qJ>RI]>
+?R'0?4
D?yXH?q
Q?tFT?}
Z?HP\?
_?R'`?
h`?R'`?$
]?HP\?h
V?tFT?n
K?yXH?
4?R'0?
p>RI]>
qJ>~
sW=2
<RI
S#>4
#?B>(?
41?]m5?c
A?o
X?GrY?
Z??W[?
[?h"\?h"\?
[??W[?
Z?GrY?~
\=?c
9?]m5?
,?B>(?
S#><
ZS=V
=yX(=
N=Gry=*
0>%uB>
8g>5^z>
?n4 ?
$?gD)?h
F?GrI?
HN?2UP?
&R?F
V?p_W?
W?p_W?
&R?2UP?
K?GrI?}
-?gD)?
$?n4 ?
>5^z>
T>%uB>|
=Gry=
N=yX(=
Q<KY
WJ=F
s=Nb
->[B>>r
4!?"
1?]m5?
I<?Di??aTB?
O?2UP?
aQ?2UP?
E?aTB?Di??
8?]m5?
3b>r
O>[B>>V
41?4
=?io@?
H?q=J?
~K?(~L?6<M?H
M?6<M?(~L?
~K?q=J?U
B?io@?d
41?h
4o>v
@=B>h=
$>F%5>
sW>gDi>
V?E
%?gD)?
,?R'0?
S3?0L6?
>?R'@?
8E?a
B?R'@?_
9?0L6?
S3?R'0?
,?gD)?"
V?0
~{>gDi>
F>F%5>/
=B>h=
B<HP
u >io0>
t>J{
$?B>(?
1?X94?
6?X94?
+?B>(?
@>io0>
?n4 ?
,?I./?
1?I./?D
#?n4 ?
E6=tF
;X94<
fU=$
>ff&>
y'?u
c,?r
0?EG2?}
7?^K8?Y
8?^K8?
+5?}
3?EG2?
c,?u
y'?j
8V>9
5>ff&>
fU=|
<X94<B`
:0>[
_>iop>I
;.?{
;.?$
@"?$
>iop>
G!><
l=q=
~*>Gr9>p
H>>yX>
f%?=
+?-C,?
,?-C,?
h>>yX>p
H>Gr9>
{r<U
=R' =
d=&S
:p>7
R&?t
@=R' =
:B`e;
9=d]\=I
=d]\=Z
9=Qk
u`<RI
;B`e;
ew=r
\m>HP|>
c?r
>HP|>
%>>y
ew=a
2=tF
+=_)K=
(m=p
q,>#
q?_
q?h
q,>d;
(m=_)K=V
:_)K;|
?$=\
Sc=o
/>>y
2>I.?>_
6Z>_
L>I.?>
%>>y
/>n4
Sc=\
;_)K;4
|P>-
=W[1=
..>lx:>]
ZS>]
F>lx:>
AO=W[1=
5<HP
7;RI
d*;u
Sc=n
%>W[1>
U>w-a>
\m>~
\m>w-a>+
!=>W[1>
Jj<1
d*;RI
=R' =
sW=k+v=(
>B>(>
g3>m
>>:#J>=
U>w-a>
l>w-a>=
U>:#J>m
g3>B>(>
=k+v=
:=R' =
#<B`
h=J{
H?>:#J>+
U>:#J>
m4>z
)>-!
>R' >
m4>m
ZS>-
-r>HP|>
>HP|>
J*>R' >
?F<N
5=;pN=
>R' >z
!=>]
|P>]
g3>z
)>R' >b
xi=;pN=
{r;'
;B`e;X9
c<)\
>B>(>W[1>lx:>
C>lx:>W[1>B>(>-!
;B`e;o
T<KY
L=B`e=I.
..>}
6>I.?>
h>iop>~
x>iop>
DX>r
G>I.?>}
c>]
=B`e=
:_)K;RI
=yX(=
m=J{
P>>yX>
_>>yX>
kI>n
v>=yX(=
;_)K;
?F=d]\=F
8V>?
c>gDi>
t>5^z>$
>5^z>
4o>gDi>
s=d]\=
<;p
d*;KY
d*<b
ew=p_
2>Gr9>[
sW>v
8g>1
|>n4
3b>v
?>Gr9>
%>d;
Ga=:
d*<o
Q9RI
{r<N
=yX(=
x=p_
/>aT
X>RI]>
Ga>f
n>W[q>}
=y>lxz>Zd{>
|>HP|>HP|>
|>Zd{>lxz>
s>W[q>
rh>f
Ga>RI]>P
/>o
/;=yX(=
y;RI
<X94<
=z6+=6<==
ew=o
G!>ff&>
+>io0>F%5>
9>[B>>%uB>
qJ>_
QZ>?
+e>x
F>%uB>[B>>
9>F%5>io0>
+>ff&>
P=6<==z6+=u
/]<X94<r
;_)K;
{r<)\
q,=6<==
Ga=F
s=J{
u >/
!N>`vO>
|P>`vO>
2D>7
~;>~
N=6<==
;_)K;o
7;KY
=z6+=
/;=:
K=d]\=D
m=I.
V,>i
9>lx:>
<>6<=>
=>6<=>
;>lx:>l
u1>i
V,>
m=d]\=:
/;=z6+=
=yX(=Y
U=B`e=
9#>/
$>0L&>
d*>1
'>0L&>/
Yu=B`e=
?F=Y
7=yX(=u
{r<M
7x=J{
/>:#
O/<N
=yX(=
A=;pN=
h=k+v=n
=k+v=
[=;pN=
5=yX(=
ew<b
7:RI
j<<>
ew<M
ew<>
j<<b
ew=I
{r<b
DX;'
3"<4
=R' =U
(=W[1=Z
B=_)K=a
S=d]\=
d=d]\=a
S=_)K=\
9=W[1=U
(=R' =P
7;B`e;
?$=V
b=B>h=D
s=Gry=
\~=n
\~=Gry=F
m=B>h=e
?$=v
;B`e;4
{r<&
=R' =T
b=B`e=B>h=h
4o= Aq=
{r=F
{r= Aq=
(m=h
j=B>h=B`e=e
WJ=9
E6=|
%=R' =Qk
k:RI
>;B`e;'
#<X94<
z<KY
$=yX(=V
7=5^:=[
2D=]mE=
?F=]mE=
2D=\
<=5^:=
-2=2
+=yX(=
D<X94<
;B`e;
>;RI
DX;$
d*<4
?F<4
DX;4
Q:RI
Q9RI
k:RI
Q9RI
k;KY
Q:RI
Q9RI
7;_)K;
y;KY
3"<U
-<X94<
b<B`e<
4o<E
{r<!
ew<l
{r<E
h<B`e<e
#9<X94<
3"<RI
^;_)K;4
7:RI
Q9RI
d*;4
7;_)K;
DX;B`e;
{r;o
{r;B`e;
DX;_)K;4
d*;RI
7:RI
Q9RI
d*;4
>;_)K;
DX;B`e;
{r;l
k;B`e;
Q;_)K;
#;RI
7:RI
Q9RI
7:RI
Q9RI
_@q=J?\
~|zxvuusqpnmkjiggfecba`_^]]\[ZYXWVUTTSRQQPONNMMLKKJIIHGGGFFEDDCCBBBAA@@??>>===<<;;:::999888776666555444333322211110000////.....----,,,,+++++*****)))))((((('''''''&&&&&&%%%%%%$$$$$$$########""""""!!!!!!!!!        X|E
+>>%I
inputImage
angle
MonzaV4_1
mlmodelc
v24@?0@"MLModel"8@"NSError"16
SaveStabilizationRecipe
v8@?0
HumanKeypoints
cnn_human_pose.espresso.net
res_256x256
res_192x192
VCPHumanPoseEspresso
@"VCPCNNModelEspresso"8@?0
res_320x192
res_192x320
%@ %@
timestamp
qualityScoreForLivePhoto
visualPleasingScore
overallFaceQualityScore
exposureScore
penaltyScore
textureScore
sharpness
faceResults
globalQualityScore
contentScore
expressionChangeScore
HumanActionForcePersonDetection
GlobalXSum
GlobalYSum
Type
VCPCNNBlurAnalyzerEspresso.sharedModelPool-%lu
cnn_blurV2.espresso.net
cnn_blur.espresso.net
@"VCPObjectPool"8@?0
VCPBlurEspresso
res_299x299
res_400x400
res_400x300
res_300x400
cnn_blur.dat
  state                  : %ld
  originating face       : %@
cnn_landmark.espresso.net
VCPFaceLandmarkEspresso
Action
ActionScore
cnn_lm.dat
cnn_blink.espresso.net
VCPGazeEspresso
ImageAnalysis
MovieAnalysis
MAComputeRequestClass
VCPMADVIVisualSearchTask
v24@?0@"VISearchResult"8@"NSError"16
VIService_ParsedVisualSearch
VIService_VisualSearch
B8@?0
Error: failed to processSampleBuffer
float32StorageType
forceCPU
forceNNGraph
sharedContext
allocator<T>::allocate(size_t n) 'n' exceeds maximum supported size
rawTime
stabCropRect
homography
Width
Height
sourceSize
inputBounds
backwarpNonInterleaved
correlationNonInterleaved
.espresso.net
callback queue
Create Context Error
Create Plan Error
%@ Load Error
Build Model Error
Select Configuration Error
Build Plan Error
Clean Plan Error
flow_estimation_%d
t_38
v16@?0^{?=ii*}8
feature_extraction
t_19
t_57
t_76
t_95
t_114
landscape_1024x432
square_320x320
landscape_448x320
portrait_320x448
timeRange
confidence
cnn_pets.espresso.net
VCPPetsEspresso
@"VCPMADVIDocumentRecognitionResource"8@?0
VCPMADVIDocumentRecognitionTask
Image loading failed
Failed to load asset
Asset contains no video tracks
Failed to create video track output
Failed to start decoding video track
Video processor cancelled
Failed to complete video decoding
com.apple.mediaanalysis.VCPVideoProcessorSession
Video processing requests must have completion handler
Specified request already active; cannot add
Failed to create request with specified configuration
Specified request not found; cannot remove
Sample buffer does not contain video frame
Sample buffer must contain uncompressed video
cnn_facepose.espresso.net
VCPPoseEspresso
cnn_pose.dat
cnn_smile.espresso.net
VCPSmileEspresso
cnn_smile.dat
QueryInternalFields
com.apple.mediaanalysis.sql
SELECT id, version, dateModified, dateAnalyzed, analysisTypes, flags, quality, masterFingerprint, adjustedFingerprint
, statsFlags
 FROM Assets WHERE localIdentifier=(?);
SELECT resultsType, results FROM Results WHERE assetId=(?);
SELECT resultsType, results FROM Results WHERE assetId=(?
) AND resultsType IN (?
SELECT id, localIdentifier, version, dateModified, dateAnalyzed, analysisTypes, flags, quality, masterFingerprint, adjustedFingerprint
 FROM Assets WHERE localIdentifier IN (?
SELECT assetId, resultsType, results FROM Results WHERE assetId IN (?
SELECT date FROM Blacklist WHERE localIdentifier=(?) AND count>=(?);
i8@?0
SELECT localIdentifier FROM Blacklist WHERE count>=(?) AND localIdentifier IN (?
SELECT localIdentifier FROM Blacklist WHERE count>=(?);
SELECT localIdentifier FROM Assets WHERE dateAnalyzed>=(?) UNION SELECT localIdentifier FROM Blacklist WHERE count>=(?) AND date>=(?);
SELECT localIdentifier, status, attempts, (date + (%lu << (3*min(attempts - 1, 5)))) FROM ProcessingStatus WHERE taskID=(?) AND status!=(?) AND localIdentifier IN (?
SELECT localIdentifier FROM ProcessingStatus WHERE taskID=(?) AND status=(?);
SELECT COUNT(*) FROM ProcessingStatus WHERE taskID=(?) AND status=(?);
SELECT value FROM KeyValueStore WHERE key = (?);
VCPMADResourceManager
@"VCPMADResourceManager"8@?0
q24@?0@"VCPMADResourceEntry"8@"VCPMADResourceEntry"16
com.apple.mediaanalysisd.VCPMADResourceManager
VCPVideoCNNBackboneEspresso
cnn_human_action_rgb.espresso.net
t_421_out
t_422_out
Gemm_263
DeviceClass
iPad
pLzf7OiX5nWAPUMj7BfI4Q
marketing-name
mediaanalysis://asset.mov
analysisConfidence
gyroStabilization
@"VCPMADVIVisualSearchResource"8@?0
absoluteScore
relativeScore
humanScore
faceId
bounds
PHAssetScene
AveStats
Failed to parse AVE statistics frame attachment; re-generating statistics
iChatUsageString
EnableStatsCollect
EnableUserQPForFacetime
EnableUserRefForFacetime
EnableWeightedPrediction
UserFrameType
ReferenceFrameNumDriver
ReferenceL0
UserQpMap
MBStatistics
NotSync
com.apple.mediaanalysisd.timer
Orientation
Regions
Home face identification task cancelled
No face present in face crop
Photos identity model not present
mediaanalysisd
private/com.apple.mediaanalysisd/caches/vision
asset in (%@)
any person.personUUID in %@
total-allowed
face
ANY detectedFaces.uuid IN %@
PVPersonClusterManager
%@ | %@
Requested unavailable frame %d
DisableANEForFaceAnalysis
com.apple.mediaanalysis.FaceProcessingGroup
@"VNSession"8@?0
[FaceAnalyzer] No faceObservation and humanObservation
@"VNRequest"16@?0#8
@"VNObservation"24@?0@"NSUUID"8@"VNRequest"16
[FaceAnalyzer] No valid faceprint and torsoprint
[FaceAnalyzer] Unable to serialize faceTorsoprint - %@
[FaceAnalyzer] Unable to serialize faceprint
[FaceAnalyzer] Unable to determine normalized face bounding { { %f, %f } { %f, %f } }
AnimalHumanColocationThreshold
Photos
PetRecognition
personLocalIdentifier is empty
(verifiedType = %d) OR (verifiedType = %d)
personLocalIdentifier
B24@?0@"NSString"8@"NSDictionary"16
PVImage
Unable to find class %s
/System/Library/PrivateFrameworks/PhotoVision.framework/PhotoVision
/System/Library/PrivateFrameworks/PhotoVision.framework/Contents/MacOS/PhotoVision
textureness
hasFlash
stillTime
sceneprintBlob
useSceneprintInSceneAnalysis
EnableMLMotionField
mammal
bird
people
adult
animal
stuffed_animals
fire
fireplace
embers
flame
beach
liquid
ocean
lake
creek
river
snow
jacuzzi
pool
grass
plant
coral_reef
foliage
tree
grill
waterways
shore
waterfall
thunderstorm
manhole
aurora
light
spotlight
smoking_item
flag
flagpole
underwater
candle
kettle
teapot
storm
tornado
lightning
blossom
surfing
pyrotechnics
blizzard
fountain
billboards
curtain
lamp
drinking_glass
fondue
blender
storefront
garden
shrub
firecracker
bubble_soap
watersport
haze
volcano
aquarium
fishtank
flower
seaweed
jellyfish
fish
flashlight
bonfire
smoking
lakeshore
sparkler
sparkling_wine
shower
geyser
Point0
Point1
Radius
Theta
Length
q24@?0@"NSDictionary"8@"NSDictionary"16
cnn_human_pose_single.espresso.net
[VCPVNImageprintWrapper] Invalid imageprint type %lu
Cannot calculate distance - missing the other imageprint
Cannot calculate distance - mismatched imageprint type (%lu vs %lu)
Cannot calculate distance - mismatched versions (%d vs %d)
Cannot calculate distance - unarchive self.data - %@
Cannot calculate distance - unarchive theOtherImageprint.data - %@
torso-only
face-only
Cannot get distance between faceprints. Distance function returns nil
type: %lu, version: %d, and data[length:%lu]: <%p>
humanPoseResults
VCPVideoCNNActionClassifierEspresso
action_recognition_head.espresso.net
action_taxonomy.plist
input
boxes
q24@?0@"PHFace"8@"PHFace"16
cnn_faceblur.dat
%@ canceled
%@ is not yet implemented
faceAdjustmentVersion != nil
mediaAnalysisAttributes.characterRecognitionAttributes.algorithmVersion >= %d
additionalAttributes.sceneAnalysisVersion >= %d &&  additionalAttributes.sceneAnalysisVersion != %d
ScreenProgress
Library (%@) %@ processing progress %.2f%% %@
v24@?0@"NSDictionary"8@"NSError"16
Video stabilization task cancelled
Video stabilization processing failed
com.apple.mediaanalysisd.livephotoeffectanalysisresults
com.apple.mediaanalysisd.moviecurationresults
com.apple.mediaanalysisd.livephotokeyframeresults
com.apple.mediaanalysisd.das.dutycycle
com.apple.mediaanalysisd.das.dutycycle.task
com.apple.mediaanalysisd.analysis.pets
com.apple.mediaanalysisd.livePhotoFillingGaps
LivePhotoEffectsShortInputDecision
LivePhotoEffectsPreGateStillMetadataDecision
LivePhotoEffectsPreGateVideoTrimDecision
LivePhotoEffectsPreGateVideoMLDecision
LivePhotoEffectsPreGateFacesDecision
LivePhotoEffectsStabilizeGateDecision
LivePhotoEffectsPostGateDecision
LivePhotoEffectsFinalGateDecision
LivePhotoEffectsLoopActivityDecision
LivePhotoEffectsBounceActivityDecision
LivePhotoEffectsLongexpActivityDecision
LivePhotoEffectsStabilizeResult
MediaType
AutoPlayableScore
SummaryDuration
IsTrimmed
KeyFrameIsSuggested
KeyFrameScoreDifference
KeyFrameTimestampOffset
KeyFrameIsFaceQualityDominant
KeyFrameIsSharpnessDominant
KeyFrameIsSemanticDominant
KeyFrameIsSuggestedEdit
KeyFrameScoreDifferenceEdit
KeyFrameTimestampOffsetEdit
KeyFrameIsFaceQualityDominantEdit
KeyFrameIsSharpnessDominantEdit
KeyFrameIsSemanticDominantEdit
previousQoS
previousQoSDuration
requestedQoS
taskName
taskStatus
DownloadAssetCount
DownloadBytes
Duration
Delay
AvgSpeed
AssetType
NumberOfPetFacesDetected
NumberOfPetsDetected
ResourceType
SceneType
AggregatedBoundingBoxSizeRatio
LargestBoundingBoxSizeRatio
com.apple.mediaanalysis.coreanalytics
VCPMADCoreAnalyticsManager
@"VCPMADCoreAnalyticsManager"8@?0
v32@?0@"NSString"8@"NSDictionary"16^B24
vnpersonsmodel.bin
vnpetsmodel.bin
VIPPetClassificationThreshold
mediaAnalysisVersionState.plist
VCPFaceProcessingVersionManager-%@
@"VCPFaceProcessingVersionManager"8@?0
FaceProcessingInternalVersion
v20@?0B8@"NSError"12
Reset none
Reset AnalysisStates
Reset Clustering
com.apple.mediaanalysis.VCPImageManager.decodequeue
VCPImageManager
@"VCPImageManager"8@?0
LogImageManager
v16@?0@"NSData"8
v16@?0@"NSError"8
v32@?0@8Q16^B24
%@ <%p>:
  person1LocalIdentifier  : %@
  person2LocalIdentifier  : %@
  reason                  : %@
ClusterToIncludeTorsoOnlyFaces
asset.dateCreated
asset.addedDate
asset.filename
(faceAlgorithmVersion = %d) AND (((hidden = 0) AND (manual = 0) AND ((trainingType = %d) OR (trainingType = nil))) OR ((trainingType = %d) OR (trainingType = %d) OR (trainingType = %d)))
(clusterSequenceNumber > 0)
(manual == 0) AND (faceAlgorithmVersion = %d)
localIdentifier
Could not access the library
Canceled operation to get CSNs of faces missing from the library
v40@?0@"NSArray"8{_NSRange=QQ}16^B32
v32@?0@"NSString"8@"PHFetchResult"16^B24
(clusterSequenceNumber in %@)
Canceled operation to ungroup faces
v16@?0^B8
Canceled operation to uncluster faces
(clusterSequenceNumber = 0)
((clusterSequenceNumber > 0) AND (faceGroup = nil))
could not access the library
Canceled operation to cleanup grouped faces with CSN=0
No faceGroups found for person with localIdentifier '%@'
Failed to fetch faces from the faceGroup that contributed the most number of face to person with localIdentifier '%@'
photoLibrary is nil
clusterSequenceNumber IN %@
@"PHFace"16@?0@"NSNumber"8
v24@?0@"NSNumber"8^B16
Saving clustering results cancelled
Canceled operation to reset library clusters
-[VCPPhotosPersistenceDelegate updateKeyFacesOfPersonsWithLocalIdentifiers:forceUpdate:cancelOrExtendTimeoutBlock:error:]
person localIdentifiers
Failed to find persons with local identifiers: '%@'
B24@?0@"PHPerson"8@"NSDictionary"16
UpdateKeyFaces: Operation canceled
Unimplemented %s in VCPPhotosPersistenceDelecate
-[VCPPhotosPersistenceDelegate invalidFaceClusterSequenceNumbersInClusterSequenceNumbers:canceler:error:]
-[VCPPhotosPersistenceDelegate ungroupFaceClusterSequenceNumbers:batchSizeForUnclusteringFaces:canceler:error:]
-[VCPPhotosPersistenceDelegate cleanupUngroupedFacesWithNonZeroClusterSequenceNumbersWithCanceler:error:]
-[VCPPhotosPersistenceDelegate cleanupGroupedFacesWithClusterSequenceNumberSetToZeroWithCanceler:error:]
-[VCPPhotosPersistenceDelegate persistChangesToAlgorithmicFaceGroups:faceCSNByLocalIdentifierForNewlyClusteredFaces:faceCSNsOfUnclusteredFaces:localIdentifiersOfUnclusteredFaces:persistenceCompletionBlock:canceler:error:]
-[VCPPhotosPersistenceDelegate resetLibraryClustersWithCanceler:error:]
-[VCPPhotosPersistenceDelegate updateKeyFacesOfPersonsWithLocalIdentifiers:forceUpdate:canceler:error:]
-[VCPPhotosPersistenceDelegate bestRepresentativeFaceForPerson:qualityMeasureByFace:canceler:]
-[VCPPhotosPersistenceDelegate bestRepresentativeFaceForPerson:qualityMeasureByFace:candidateFaces:canceler:]
-[VCPPhotosPersistenceDelegate associateFace:withFaceCrop:error:]
-[VCPPhotosPersistenceDelegate clearDirtyStateOnFaceCrops:error:]
-[VCPPhotosPersistenceDelegate dirtyFaceCropsWithLimit:]
-[VCPPhotosPersistenceDelegate faceAssociatedWithFaceCrop:]
-[VCPPhotosPersistenceDelegate facesFromAsset:]
-[VCPPhotosPersistenceDelegate persistFaces:deleteFaces:forAsset:persistedFaces:error:]
-[VCPPhotosPersistenceDelegate persistGeneratedFaceCrops:error:]
-[VCPPhotosPersistenceDelegate recordNeedToPersonBuildOnFaceGroupContainingFace:error:]
-[VCPPhotosPersistenceDelegate suggestedPersonLocalIdentifierForPersonLocalIdentifier:faceClusterer:canceler:context:error:]
-[VCPPhotosPersistenceDelegate updateFaceprint:ofPersistedFace:error:]
-[VCPPhotosPersistenceDelegate buildPersonsWithFaceClusterer:keyFaceUpdateBlock:canceler:context:extendTimeoutBlock:]
(personBuilderState = %ld)
Canceled cleaning up merge candidates of verified persons
v24@?0@"PHFetchResult"8@"NSMutableSet"16
v24@?0@"VCPMergeCandidatePair"8^B16
Canceled cleaning up merge candidates
(trainingType = %d) || (trainingType = %d)
v32@?0@"PHPerson"8@"NSString"16^B24
B24@?0@"VCPMergeCandidatePair"8@"NSDictionary"16
(clusterSequenceNumber IN %@)
Person building cancelled
v32@?0@"NSNumber"8@"NSOrderedSet"16^B24
clusterSequenceNumber = %ld
clusterSequenceNumber != %ld
v32@?0@"NSString"8@"PHFaceCrop"16^B24
v32@?0@"PHPerson"8Q16^B24
v32@?0@"NSString"8@"PHPerson"16^B24
invalid merge candidate pair created from cluster rejections
potential invalid merge candidate pair created from cluster rejections
invalid merge candidate pair from cluster rejection for verified person
potential invalid merge candidate pair from cluster rejection for verified person
B16@?0^@8
no training faces in level1 cluster - create 'unverified person : verified/migrated person' candidate pair
all training faces on single verified person in level1 cluster - create 'unverified person : training person' candidate pair
all training faces on single verified person in level1 cluster - create 'unverified person : verified person' candidate pair
all training faces on single verified person in level1 cluster - create 'training person : verified person' candidate pair
invalid merge candidate pair because we may have a dirty level0 cluster
multiple training persons in level0 cluster - create 'training person : training person' pair
clusterSequenceNumber
single training person in level0 cluster - create 'training person : verified person with confirmed face' pair
single training person in level0 cluster - create 'verified persons with confirmed face : verified persons with confirmed face' pair
invalid merge candidate pair because one person has face rejected for the other
invalid merge candidate pair because we have > 3 verified persons in the face group
single training person in level1 cluster - create 'training person : verified person with confirmed face' pair
single training person in level1 cluster - create 'verified persons with confirmed face : verified persons with confirmed face' pair
level1 cluster - create 'training person : training person' pair
level1 cluster - create 'unverifed person : training person' pair
invalid merge candidate pair because we have a cluster rejection
v32@?0@"NSMutableSet"8@"NSMapTable"16@"NSSet"24
invalid merge candidate pair because we have a face on verified person but cluster-rejected on another verified person
-[VCPPhotosPersistenceDelegate buildPersonWithFaceClusterer:keyFaceUpdateBlock:context:cancelOrExtendTimeoutBlock:]
faceLocalIdentifier is nil
fetched %lu faces for %@
clusterSequenceNumber is nil
personLocalIdentifier is nil
fetched %lu persons for %@
Got a 'nil' photoLibrary. Cannot remove auto assigned faces
(manual = 0) AND ((nameSource = %d) OR (nameSource = %d) OR (nameSource = %d)) AND ((trainingType = %d) OR (trainingType = nil))
Operation to remove faces from verified persons has been canceled
Failed to removed faces from person with localIdentifiers '%@'
not known
PGGraphHelper
/System/Library/PrivateFrameworks/PhotosGraph.framework/PhotosGraph
/System/Library/PrivateFrameworks/PhotosGraph.framework/Contents/MacOS/PhotosGraph
cnn_person_detector.espresso.net
salientRegion
salientScore
cnn_saliency.dat
cnn_saliency.espresso.net
res_0
res_1
res_2
VCPSaliencyFullEspresso
DisableANEForSceneAnalysis
IncludeNSFW
IncludeLandmark
IncludeTaboo
IncludeSDG
DominantObjectDetection
SaliencyObjectnessDetection
IncludeMeme
ProbableRotation
enabled
disabled
IncludeDocument
MediaAnalysis
JunkR14J9
SharpnessModel
EnableSceneAssetConcurrency
com.apple.mediaanalysis.SceneProcessingGroup
@"CVNLPCommSafetyHandler"8@?0
%@%@
classID
size
score
v32@?0@"VNRecognizedObjectObservation"8Q16^B24
v32@?0@"NSString"8@"NSMutableArray"16^B24
v24@?0@"PVSceneTaxonomyNode"8^B16
v32@?0@"NSDictionary"8Q16^B24
meme_
v32@?0@"NSString"8@"NSString"16^B24
v16@?0Q8
PVSceneTaxonomy
{{%.*g, %.*g}, {%.*g, %.*g}}
VCPMADVIResource
v16@?0^{opaqueCMSampleBuffer=}8
VCPMAMLModel-%@
@"VCPMAMLModel"8@?0
precision
personID
personFaceCount
validFaceCount
identitySize
recall
AutoCounterGroundTruth.plist
[AutoCounter] Cannot load ground truth file URL
faceCount
uuid
no_name
AutoCounterDumpFaceprint
not to
AddedDate
unknown
user
graph
unverified
verifiedType
personName
faceRect
faceGroupID
faceprint
momentIdentifier
faceID
[AutoCounter] Failed to fetch person (%@)
FacesPerAsset
OptInDate
OptInDateSinceReferenceDate
OptInMADFaceVersion
OptInDetectionModelVersion
OptInRecognitionModelVersion
FaceCount
AssetCount
AdditionalInformation
AutoCounterClusters_Ver%d_DetectionVer%lu_RecognitionVer%lu.plist
[AutoCounter] Failed to retrive export URL
mergecandidates
faces
assetInformation
[AutoCounter] Failed to process FaceGroups
v32@?0@"NSString"8@"NSArray"16^B24
AutoCounterClusterAssetsToFaces_Ver%d_DetectionVer%lu_RecognitionVer%lu.plist
AutoCounterDumpAssetsToFaces
phFaceID
gtFaceID
gtPersonID
centerX
centerY
Unknown
v32@?0@"NSString"8@"NSNumber"16^B24
v24@?0@"NSString"8^B16
v32@?0@"NSString"8@"NSSet"16^B24
com.apple.photos.autocounter
date_optin
detection_version_current
detection_version_optin
mad_version_current
mad_version_optin
person_id
promoter_clusters
promoter_clusters_duplicates
promoter_precision
promoter_recall
promoter_version_current
promoter_version_optin
recognition_version_current
recognition_version_optin
total_assets
total_assets_optin
total_faces
total_faces_optin
type
userLabeledAge
userLabeledEthnicity
userLabeledGender
vision_clusters
vision_clusters_duplicates
vision_precision
vision_recall
nightly
nightly-Ver%d_DetectionVer%lu_RecognitionVer%lu_PersonVer%lu
AutoCounterCoreAnalytics
%@_Ver%d_DetectionVer%lu_RecognitionVer%lu.plist
self.lastPathComponent BEGINSWITH %@
v32@?0@"NSURL"8Q16^B24
visionCluster
weightedAveragePrecision
weightedAverageRecall
numSingletons
numValidSingletons
precisionPerCluster
recallPerPersonToGroundTruth
recallPerPersonExcludeMissDetection
personCluster
identity
PVPersonPromoterVersion
LogLevel
yyyy-MM-dd HH:mm:ss
featureVector
com.apple.mediaanalysis
com.apple.mediaanalysisd.analysis
com.apple.mediaanalysisd.photos
com.apple.mediaanalysisd.homekit
com.apple.mediaanalysisd.homekitsession
dateModified
dateAnalyzed
masterFingerprint
adjustedFingerprint
performedAnalysisTypes
metadataRanges
SyncPoint
FaceResults
ShotTypeResults
SceneResults
QualityResults
JunkResults
BlurResults
ExposureResults
FeatureVectorResults
CameraMotionResults
SubjectMotionResults
FineSubjectMotionResults
SubtleMotionResults
OrientationResults
DistanceResults
IrisRecommendResults
IrisSharpnessResults
PreEncodeResults
MovingObjectsResults
ObstructionResults
SaliencyResults
CompositionResults
ClassificationResults
InterestingnessResults
MusicResults
UtteranceResults
ActivityLevelResults
FacePrintResults
PetsResults
PetsFaceResults
MovieSummaryResults
MovieHighlightResults
MovieHighlightScoreResults
KeyFrameResults
KeyFrameBlurResults
KeyFrameStillResults
TrackingResults
LivePhotoEffectsResults
SceneChangeResults
ApplauseResults
BabbleResults
CheeringResults
LaughterResults
HumanPoseResults
HumanActionResults
HumanPoseInternalResults
HandsResults
LoudnessResults
KeyFrameResourceResults
SceneprintResults
VideoStabilizationResults
SongResults
HumanActionClassificationResults
InterpolationResults
WPResults
RotationAnalysisResults
ColorNormalizationResults
FaceQualityFlag
attributes
energyValues
peakValues
facePosition
facePoseYaw
facePrint
sharpnessFaces
saliencyBounds
saliencyConfidence
songSignature
sceneprint
WPValue
probableRotation
probableRotationConfidence
colorNormalizationData
vanishingPointConfidence
distance
sceneprintDistance
neighbor
neighborDateModified
slowMoFlicker
stabilizationRecipe
interpolationURL
Data
objectBounds
junk
petsBounds
petsConfidence
keyFrameTime
keyFrameScore
bestPlaybackCrop
maxHighlightStart
maxHighlightDuration
livePhotoEffectsRecipe
livePhotoEffectsGatingDescriptions
livePhotoEffectsMatchingScenes
aesthetic
sceneClassification
saliency
saliencyObjectness
overallScore
allScores
wellFramedSubjectScore
wellChosenBackgroundScore
tastefullyBlurredScore
sharplyFocusedSubjectScore
wellTimedShotScore
pleasantLightingScore
pleasantReflectionsScore
harmoniousColorScore
livelyColorScore
pleasantSymmetryScore
pleasantPatternScore
immersivenessScore
pleasantPerspectiveScore
pleasantPostProcessingScore
noiseScore
failureScore
pleasantCompositionScore
interestingSubjectScore
intrusiveObjectPresenceScore
pleasantCameraTiltScore
lowKeyLightingScore
acceptableCrop
preferredCrop
humanBounds
humanKeypoints
humanConfidence
humanID
humanActions
handsBounds
handsKeypoints
handsKeypointsConfidents
handsID
frameQualityScore
faceQualityScore
texture
flashFired
QualityOfService
DutyCycling
VCPTaskIDs
GyroStabilization
PixelStabilization
MaxNumberOfAssetToProcess
ForceFullScan
Full Face, 
Face, 
Voice, 
Full Scene, 
Scene, 
Junk, 
Blur, 
Exposure, 
Distance, 
Feature, 
Saliency, 
Composition, 
Classification, 
ActivityLevel, 
CurationScore, 
Pets, 
MovieCuration, 
Effects, 
Audio Classification, 
Human pose, 
Loudness Measure, 
Hands, 
Video Stabilization Pixel, 
Video Stabilization Gyro, 
Gyro Analytics, 
Song detection, 
Human action, 
Iris Recommendation, 
mShortInputDecision
mPreGateStillMetadataDecision
mPreGateVideoTrimDecision
mPreGateVideoMLDecision
mPreGateFacesDecision
stabilizeGateDecision
postGateDecision
finalDecision
loopActivityDecision
bounceActivityDecision
longexpActivityDecision
ALGatingResultError
ALGatingResultUnset
ALGatingResultFail
ALGatingResultPass
index
summaryTimerange
duplicate
SceneAnalysis
FaceAnalysis
EmbeddingAnalysis
OCRAnalysis
otgx_boz
otgx_fyqmjdju
otgx_hfojubmt
meme_document_check_or_checkbook
meme_curation_meme
meme_curation_screenshot
meme_document_boarding_pass
meme_document_currency_or_bill
meme_document_driving_license
meme_document_office_badge
meme_document_passport
meme_document_receipt
meme_document_social_security_number
meme_hier_negative
meme_hier_document
meme_hier_curation
meme_negative
meme_document
meme_screenshot_etc
hier_text_document
hier_tragic_failure
tragic_failure
screenshot
bad_framing
bad_lighting
blurry
food_or_drink
junk_other
medical_reference
negative
receipt_or_document
repair_reference
shopping_reference
utility_reference
junk_negative
hier_negative
junk_non_memorable
hier_non_memorable
junk_poor_quality
hier_poor_quality
No Resource
Soft Failure
Hard Failure
Duplicate Failure
Upload Failure
PhotoLibraries
ImageTooSmall
UsingBestResource
FacesToDelete
FacesToPersist
QuickFaceIdentification
VisionClustersMinusLibraryClusters
LibraryClustersMinusVisionClusters
processed
pet-vip-status
person-vip-status
Confidence
BoundingBox
BaseRetryInterval
UserInteractive
UserInitiated
Default
Utility
Background
Unspecified
OptInStatus
FileURL
GroundTruthURL
PersonIdentifier
PersonInformation
UserLabeledGender
UserLabeledAge
UserLabeledEthnicity
ModifyPersonRequest
SubTasks
NumberOfAssetsAllowedForPhotosFaceProcessing
NumberOfAssetsAnalyzedForPhotosFaceProcessing
IrisObjectsResults
MetaFocusResults
MetaMotionResults
MetaMotionProcessedResults
MetaStabilizationResults
MetaStabilizationFrameResults
MetaHomographyDimensionResults
MetaHomographyResults
MetaPresentationTimeResults
MetaMotionBlurResults
MetaPTSInNanosResults
MetaOriginalPTSInNanosResults
MetaItemPTSResultsKey
MetaAdjusterResults
MetaAdjusterRecipeResults
MetaAdjusterDisplacementKey
MetaInterpolatedFrameKey
MetaLensSwitchResults
summaryIsTrimmed
livePhoto
movie
HighlightMaxDuration
HighlightTargetDuration
HighlightStartRange
HighlightTolerance
HighlightIndex
HighlightBestTrim
HighlightFullResult
MinimumHighlightInSec
v32@?0@"VCPMovieHighlight"8Q16^B24
com.apple.mediaanalysisd.realtime
ContentType
faceMetadataArray
realtimeFaceRect
realtimeFaceRoll
realtimeFaceYaw
PriorityScore
InProcess
com.apple.mediaanalysis.service.management
com.apple.mediaanalysis.service.handler
MediaAnalysisService
Error issuing sandbox extension
v16@?0d8
[MediaAnalysis] Error connecting to background analysis service
Assets from multiple libraries not supported
v24@?0@"NSString"8@"NSError"16
PersonProcessingDeletePersons
PersonProcessingClusterFaces
PersonProcessingIncrementalFaceClustering
PersonProcessingRunBuildPersons
PersonProcessingIncrementalPersonBuilding
PersonProcessingRunPromotePersons
PersonProcessingRebuildFaceIDModel
PersonProcessingClassifyContactPictures
faceCSN
faceIdentifier
personIdentifier
confirmed
status
requestAdvancedStatus
advancedStatus
PLPhotoAnalysisVisionServiceFaceReclusteringThreshold
PLPhotoAnalysisVisionServiceFaceReclusteringDeletePersons
PLPhotoAnalysisVisionServiceFaceReclusteringShouldRecluster
v24@?0@"NSArray"8@"NSError"16
creationDate
AllowOnDemand
AllowOnDemandPixel
AllowOnDemandGyro
AllowStreaming
KeepPrivateResults
MaxHighlightDuration
Standalone
StoreAnalysis
ScaledSlomoTime
com.apple.mediaanalysis.ondemand
com.apple.mediaanalysis.storage
com.apple.mediaanalysis.VCPMediaAnalyzer.sandboxQueue
NoResultStrip
v16@?0@"NSString"8
VCPMediaAnalyzer
UseSceneprintDistance
VCPMADVIVisualSearchGatingTask
Failed to create visual search query context
VIService_VisualSearchGating
v24@?0@"VIParseResult"8@"NSError"16
frame idx = %d
size = %d, track_target_exist = %d, target_lost = %d, tracking_score = %6.2f
before filter: frame(%d): time_stamp=%f, ave_motion=(%f,%f)
frame(%d): time_stamp=%f, ave_motion=(%f,%f), acc_var=(%f, %f), motion_chg=(%f, %f)
VideoCNN
Skeleton
MaximumHighlightInSec
HumanAction
formatDescriptions
naturalSize
nominalFrameRate
preferredTransform
tracks
flags
keypoints
q24@?0@"PHAssetResource"8@"PHAssetResource"16
B16@?0@"PHAssetResource"8
com.apple.MediaAnalysis
com.apple.mediaanalysisd
Apple
kind == %d
mediaType == %d
kind == %d && kindsubtype != %d
mediaType == %d && !((mediaSubtype & %d) == %d)
kindsubtype == %d
(mediaSubtype & %d) == %d
mediaAnalysisAttributes.mediaAnalysisVersion < %d
UserOrig
UserAlgo
NoUserAlgo
NoAlgo
variation = %6.2f
VCPMADVisionResource
sum = %6.2f, tracking_score = %6.2f
Target Captured @ [%5.0f, %5.0f, %5.0f, %5.0f]
initial @ [%d %d] s = %6.5f
stop    @ [%d %d] s = %6.5f
lost = %d
[%6.2f, %6.2f, %6.2f, %6.2f]
box0: (x = %2.1f, y = %2.1f, w = %2.1f, h = %2.1f)
box1: (x = %2.1f, y = %2.1f, w = %2.1f, h = %2.1f)
box : (x = %2.1f, y = %2.1f, w = %2.1f, h = %2.1f)
overlap_area = %6.2f, max_area = %6.2f, weight = %6.2f
derr = %6.2f, terr = %6.2f
add new expert with weight %6.2f
expert %d was replaced: voting weight(%6.2f --> %6.2f)!
after voting --> update target
detector and tracker did not match well --> experts vote
detector and tracker matched well --> update experts
PHAssetFace
PHAssetFaceExpression
v32@?0@"NSString"8@"NSString"16@"NSError"24
FaceIDModelLastGenerationKey
PetIDModelLastGenerationKey
Person
Unknown(%lu)
com.apple.mediaanalysis.quickfaceid.management
VCPPersonVIPLoadModel
VCPPetVIPLoadModel
verifiedType = %@ OR verifiedType = %@
nameSource != %ld
isInVIPModel == YES
roll == 0.0
FastFaceMigration
OCRGatingThreshold
/var/mobile/Media/MediaAnalysis
private/com.apple.mediaanalysisd/MediaAnalysis
mediaanalysis.db
kind == %d && kindSubtype != %d
PhotoAnalysisServicePreferences.plist
faceWorkerState.plist
(faceAlgorithmVersion = %d) AND (clusterSequenceNumber = 0) AND (((hidden = 0) AND (manual = 0) AND ((trainingType = %d) OR (trainingType = nil))) OR ((trainingType = %d) OR (trainingType = %d) OR (trainingType = %d)))
Angle
PHAssetSceneprint
seg %d: [%d, %d], sceneCut=%d
prev(%d) [%d, %d][%6.1f, %6.1f] qs = %6.2f, curr(%d) [%d, %d] [%6.1f, %6.1f]qs = %6.2f:
dist({%d %d}, {%d %d}) = %6.2f, th = %6.2f
prev: type = %d, fast = %d, gmv_sum {%6.1f, %6.1f}, merge = %d, split = 0
curr: type = %d, fast = %d, gmv_sum {%6.1f, %6.1f}, merge = %d, split = 0
segments
===========SceneChangeSegments=============
[%f, %f]
Start
%@(v%d) (face: %.3f, %.3f, %.3f) (body: %.3f, %.3f, %.3f, %.3f) quality: %.3f
Human
face_model_tensor.dat
face_model_landmark_coordinates.dat
face_model_boundary.dat
com.apple.mediaanalysisd.VCPFaceShapeUpdate
Error detected at line 
Error detected in file 
Submodules/dlib/dlib/optimization/optimization.h
Error detected in function 
double dlib::find_min_box_constrained(search_strategy_type, stop_strategy_type, const funct &, const funct_der &, T &, const matrix_exp<EXP1> &, const matrix_exp<EXP2> &) [search_strategy_type = dlib::lbfgs_search_strategy, stop_strategy_type = dlib::objective_delta_stop_strategy, funct = std::function<double (dlib::matrix<double, 0, 0>)>, funct_der = std::function<dlib::matrix<double, 0, 0> (dlib::matrix<double, 0, 0>)>, T = dlib::matrix<double, 51, 1>, EXP1 = dlib::matrix_op<dlib::op_uniform_matrix_3<double>>, EXP2 = dlib::matrix_op<dlib::op_uniform_matrix_3<double>>]
Failing expression was 
is_col_vector(x) && is_col_vector(x_lower) && is_col_vector(x_upper) && x.size() == x_lower.size() && x.size() == x_upper.size()
double find_min_box_constrained()
 The inputs to this function must be equal length column vectors.
 is_col_vector(x):       
 is_col_vector(x_upper): 
 x.size():               
 x_lower.size():         
 x_upper.size():         
The objective function generated non-finite outputs
 ************************** FATAL ERROR DETECTED ************************** 
 ************************** FATAL ERROR DETECTED ************************** 
 ************************** FATAL ERROR DETECTED ************************** 
Two fatal errors have been detected, the first was inappropriately ignored. 
To prevent further fatal errors from being ignored this application will be 
terminated immediately and you should go fix this buggy program.
The error message from this fatal error was:
**************************** FATAL ERROR DETECTED ****************************
******************************************************************************
EPORT_IN_USE
ETIMEOUT
ECONNECTION
ELISTENER
ERESOLVE
EMONITOR
ECREATE_THREAD
ECREATE_MUTEX
ECREATE_SIGNALER
EUNSPECIFIED
EGENERAL_TYPE1
EGENERAL_TYPE2
EGENERAL_TYPE3
EINVALID_OPTION
ETOO_FEW_ARGS
ETOO_MANY_ARGS
ESOCKET
ETHREAD
EGUI
EFATAL
EBROKEN_ASSERT
EIMAGE_LOAD
EDIR_CREATE
EINCOMPATIBLE_OPTIONS
EMISSING_REQUIRED_OPTION
EINVALID_OPTION_ARG
EMULTIPLE_OCCURANCES
ECONFIG_READER
EIMAGE_SAVE
ECAST_TO_STRING
ESTRING_CAST
EUTF8_TO_UTF32
EOPTION_PARSE
undefined error type
iteration: 
   objective: 
Submodules/dlib/dlib/matrix/matrix.h
const dlib::matrix::literal_assign_helper &dlib::matrix<double, 2, 2>::literal_assign_helper::operator,(const T &) const [T = double, num_rows = 2, num_cols = 2, mem_manager = dlib::memory_manager_stateless_kernel_1<char>, layout = dlib::row_major_layout]
r < m->nr() && c < m->nc()
You have used the matrix comma based assignment incorrectly by attempting to
supply more values than there are elements in the matrix object being assigned to.
Did you forget to call set_size()?
 r: 
 c: 
 m->nr(): 
 m->nc(): 
dlib::matrix<double, 2, 2>::literal_assign_helper::~literal_assign_helper() [T = double, num_rows = 2, num_cols = 2, mem_manager = dlib::memory_manager_stateless_kernel_1<char>, layout = dlib::row_major_layout]
!has_been_used || r == m->nr()
You have used the matrix comma based assignment incorrectly by failing to
supply a full set of values for every element of a matrix object.
const dlib::matrix::literal_assign_helper &dlib::matrix<double, 2, 1>::literal_assign_helper::operator,(const T &) const [T = double, num_rows = 2, num_cols = 1, mem_manager = dlib::memory_manager_stateless_kernel_1<char>, layout = dlib::row_major_layout]
dlib::matrix<double, 2, 1>::literal_assign_helper::~literal_assign_helper() [T = double, num_rows = 2, num_cols = 1, mem_manager = dlib::memory_manager_stateless_kernel_1<char>, layout = dlib::row_major_layout]
You have to supply column vectors to this function
double dlib::find_min_using_approximate_derivatives(search_strategy_type, stop_strategy_type, const funct &, T &, double, double) [search_strategy_type = dlib::lbfgs_search_strategy, stop_strategy_type = dlib::objective_delta_stop_strategy, funct = std::function<double (dlib::matrix<double, 0, 0>)>, T = dlib::matrix<double, 6, 1>]
is_col_vector(x) && derivative_eps > 0
double find_min_using_approximate_derivatives()
x.nc():         
derivative_eps: 
<%@ %p, 
active cost: %0.2f,
inactive cost: %0.2f>
compatible
IODeviceTree:/arm-io
EnableR2D2ForSubtleMotion
Multiple cadence options specified
%@ value must be NSNumber
%@ value must be poisitive
%@ is not supported
v32@?0@"NSString"8@16^B24
Tracking
NumOfValidFrames
TrackingScore
revision
config
loadModel
res_256x160
res_160x256
cnn_human_pose_lite_v2.espresso.net
Destructive Trim Range: [%.2f - %.2f]
after repare
after consecutive short merge
after sparse short merge
after post processing
=========Segment %s==========
v32@?0@"VCPSegment"8Q16^B24
 [%.2f - %.2f]: %.2f
--[%.2f - %.2f]
Embedding
NotImplementedException
[VCPAsset %@] should not be called
mediaType
mediaSubtypes
pixelWidth
pixelHeight
Live Photo
Pano Photo
Screenshot
HDR Photo
SDOF Photo
Photo
Slow-mo Movie
Timelapse Movie
Movie
exif
imageWithPreferredDimension:
movie:
originalMovie:
quality
subjectMotionScore
objectsMotion
globalMotion
interestingnessScore
obstructionScore
trackingScore
sharpnessScore
sceneChangeScore
start
duration
VoiceResults
browDown_L
browDown_R
browInnerUp
browOuterUp_L
browOuterUp_R
cheekPuff
cheekSquint_L
cheekSquint_R
eyeBlink_L
eyeBlink_R
eyeLookDown_L
eyeLookDown_R
eyeLookIn_L
eyeLookIn_R
eyeLookOut_L
eyeLookOut_R
eyeLookUp_L
eyeLookUp_R
eyeSquint_L
eyeSquint_R
eyeWide_L
eyeWide_R
jawForward
jawLeft
jawOpen
jawRight
mouthClose
mouthDimple_L
mouthDimple_R
mouthFrown_L
mouthFrown_R
mouthFunnel
mouthLeft
mouthLowerDown_L
mouthLowerDown_R
mouthPress_L
mouthPress_R
mouthPucker
mouthRight
mouthRollLower
mouthRollUpper
mouthShrugLower
mouthShrugUpper
mouthSmile_L
mouthSmile_R
mouthStretch_L
mouthStretch_R
mouthUpperUp_L
mouthUpperUp_R
noseSneer_L
noseSneer_R
tongueOut
focalLengthInPixels
faceBounds
objects
faceRollAngles
faceAnchor
vertices
transform
blendshapes
geometry
dispatchQueue
regionsOfInterest
aggSubjectMotionScore
turboMode
frameWidth
frameHeight
VCPCaptureAnalysis
v24@?0f8Q12i20
com.apple.mediaanalysis.VCPClientDatabaseManager
cnn_content.dat
com.apple.mediaanalysis.VCPDefaultPhotoLibraryManager
v24@?0^v8Q16
[MediaAnalysis][%@]Unable to open movie, skip
[MediaAnalysis][%@]Failed to create asset
[%@] Analysis cancelled
[%@] Analysis failed to complete
checksum
data
SceneprintHyperplaneLSH, 
NeuralHyperplaneLSH, 
gesture_recognition.espresso.net
autoplay_head.espresso.net
input1
input2
output
v20@?0f8^B12
com.apple.Photos
FaceV6Models
FaceModelOverwrite
Error: failed to analyze motion flow
classification
com.apple.homekitanalysis.service.management
com.apple.homekitanalysis.service.handler
Failed to fetch person by local identifier (%@)
HMIAnalysisService
HMITaskService
mediaanalysis://in-memory
com.apple.mediaanalysisd.VCPInMemoryAVAsset
com.apple.mediaanalysis.reachability
Not c
None
TransientConnection
Reachable
ConnectionRequired
ConnectionOnTraffic
InterventionRequired
ConnectionOnDemand
IsLocalAddress
IsDirect
IsWWAN
VCPMADImageSafetyClassificationResource
@"VCPMADImageSafetyClassificationResource"8@?0
Image pre-processing failed
Frame: %u
%@ (face=%@ image=%@)
[VCPFaceCropGenerator] User canceled
v32@?0@"VCPFaceCropSourceDescriptor"8Q16^B24
[VCPFaceCropGenerator] Invalid face rect { { %f, %f }, { %f, %f} }
[VCPFaceCropGenerator] Failed to create image to generate facecrop - %@
[VCPFaceCropGenerator] faceCropSourceDescriptor.face is nil
[VCPFaceCropGenerator] faceCropSourceDescriptor.image is nil
[VCPFaceCropGenerator][%@] Failed to create VCPFaceCrop
[FacecropManager] Cannot generate facecrop %@ without originating face
[FacecropManager] Failed to find originating PHFace %@
[FacecropManager] Failed to generate facecrop on manual originating face %@
[FacecropManager] Facecrop is nil
[FacecropManager] Missing image data from facecrop %@
[FacecropManager] Invalid facecrop image data %@
[FacecropManager] Invalid facecrop bounding box %@
[FacecropManager] Facecrop image size equals to 0
[FacecropManager] Failed to normalize bound %@ with image (%.0fx%.0f)
[FacecropManager] Failed to obtain the facecrop image dimensions
[FacecropManager] Failed to create VNImageRequestHandler for facecrop
[FacecropManager] Failed to initialize VNDetectFaceRectanglesRequest
[FacecropManager] Failed to set VNCreateFaceprintRequest
[FacecropManager] Failed to set VNCreateFaceTorsoprintRequest
[FacecropManager] Failed to analyze facecrop: %@
[FacecropManager] Failed to create faceprint for facecrop: %@
[FacecropManager] Failed to create faceTorsoprint for facecrop: %@
[FacecropManager] Failed to create VCPPhotosFace from face observation
[FacecropManager] Failed to generate faceprint/faceTorsoprint
[FacecropManager] Face %@ has already been persisted with a facecrop
[FacecropManager] Face %@ does not have a faceprint
[FacecropManager] Failed to fetch facecrop: %@
[FacecropManager] Failed to publish facecrop analysis %@
[FacecropManager] faceLocalIdentifier is nil
[FacecropManager] Fetched %lu faces for %@, should be 1
[FacecropManager] Failed to fetch face %@
v40@?0@"VCPFaceCropSourceDescriptor"8Q16Q24@"NSError"32
Measurement
Min (s)
Max (s)
Avg (s)
Total
Count
Minimum
Maximum
Average
signpost
q24@?0@"PHSceneClassification"8@"PHSceneClassification"16
hand_keypoint_detector.espresso.net
version
types
date
statsFlags
typesWide
assetIdentifier
assetModificationDate
assetMasterFingerprint
assetAdjustedFingerprint
imageBlurResults
imageCompositionResults
imageFaceResults
imageFeatureResults
imageJunkResults
imageSaliencyResults
imageShotTypeResults
imagePetsResults
imagePetsFaceResults
imageSceneprintResults
livePhotoEffectsResults
livePhotoRecommendationResults
livePhotoSharpnessResults
livePhotoKeyFrameResults
livePhotoKeyFrameStillResults
movieActivityLevelResults
movieCameraMotionResults
movieClassificationResults
movieFaceResults
movieFaceprintResults
movieFeatureResults
movieFineSubjectMotionResults
movieInterestingnessResults
movieMovingObjectResults
movieMusicResults
movieObstructionResults
movieOrientationResults
moviePreEncodeResults
movieQualityResults
movieSaliencyResults
movieSceneResults
movieSceneprintResults
movieSubjectMotionResults
movieSubtleMotionResults
movieUtteranceResults
movieVoiceResults
movieSummaryResults
movieHighlightResults
imageExposureResults
imageHumanPoseResults
movieHumanPoseResults
movieApplauseResults
movieBabbleResults
movieCheeringResults
movieLaughterResults
movieHumanActionResults
movieLoudnessResults
moviePetsResults
moviePetsFaceResults
movieStabilizationResults
movieHighlightScoreResults
livePhotoHumanActionClassificationResults
propertyKey %s 
result is nil %s
width
height
input_image_1
input_image_2
cnn_moflow.espresso.net
res_landscape
res_portrait
res_sqaure
VCPMoflowEspresso
SHMutableSignature
/System/Library/Frameworks/ShazamKit.framework/ShazamKit
/System/Library/Frameworks/ShazamKit.framework/Contents/MacOS/ShazamKit
identifier
faceSharpness
vanishingPoint
dominantLine
exposure
underExpose
eyeExpression
faceQuality
featureBlob
hand_keypoint_detector_acc.espresso.net
Failed to load imageURL: %@
NeuralHash+LSH invalid imageSignatureHash
Invalid NeuralHash+LSH (=)
Face
shotType
stabilizeResult
outputFrameDurValue
cropRectX
cropRectY
cropRectHeight
cropRectWidth
timeScale
epoch
frameInstructions
autoloop
bounce
longexposure
stabilize
minVersion
AutoLoop
Bounce
LongExposure
Stabilize
NormStabilizeInstructions
Version
MinVersion
Params
loopFlavor
loopEnergy
outputFrameDur
loopSuggestionState
longExposureSuggestionState
recipeBlob
Error: failed to processImage
idx (%tu) is out of range (%tu)
timeValue
homographyParam
res_384x384
q24@?0@"NSNumber"8@"NSNumber"16
res_%dx%d
Home resident maintenance task cancelled
errorCode
loopFadeLen
loopPeriod
loopStart
ErrorCode
activityScore
motionType
isFast
com.apple.homekitanalysis.session.management
com.apple.homekitanalysis.session.handler
[HomeKit] XPC connection invalidated. Please restart the session.
No result handler registered
No VCPHomeKitAnalysisSession; cannot process message
HMIVideoAnalyzer
/System/Library/PrivateFrameworks/HomeAI.framework/HomeAI
/System/Library/PrivateFrameworks/HomeAI.framework/Contents/MacOS/HomeAI
highlightScore
faceprintBlob
maxNumberHands
humanActionWindowSize
mouthExpression
position
isCloseup
actionScore
colorNormalizationBlob
FramesPerSecond
v24@?0Q8^B16
v32@?0@"NSNumber"8Q16^B24
VCPClusteringStatusIsClustering
VCPClusteringStatusClusterRebuildRequired
VCPClusteringStatusEligibleFacesCount
VCPClusteringStatusPendingFacesCount
VCPSuggestionUpdateStarted
VCPSuggestionUpdateFinished
VCPSuggestionUpdateCancelled
com.apple.mediaanalysisd.clusterer.processing
com.apple.mediaanalysis.scheduleclustering
com.apple.mediaanalysisd.optional_clustering
com.apple.mediaanalysisd.forced_clustering
com.apple.mediaanalysisd.requested_clustering
Operation cancelled
v32@?0@"NSCountedSet"8Q16^B24
q24@?0@"NSCountedSet"8@"NSCountedSet"16
VCPClusterCompareTimestamp
VCPClusterer: Failed to get face CSNs from cluster cache, which should not be used
PVErrorInvalidClusterCacheFile - %@
VCPClusterer: Failed to get Vision cluster state - %@
VisionClusterState
clusteringType
threshold
VCPClusterer: Failed to archive cluster snapshot
VCPClusterer: Failed to rename file from '%@' to '%@'. Error = %d
VCPClusterer: Failed to write cluster snapshot to file '%@'
missing parameter clusterState
VCPClusterer: Cluster snapshot file '%s' is too small
VCPClusterer: Invalid magic number found in '%s'
VCPClusterer: Invalid version in '%s', %d != %d
VCPClusterer: Failed to read MD5 from header of '%s'
VCPClusterer: Failed to compute MD5 of '%s'
VCPClusterer: Failed MD5 check for '%s'
VCPClusterer: Failed to read size of vision cluster state blob from '%s'
VCPClusterer: Failed to read vision cluster state blob from '%s'
VCPClusterer: Failed to open cluster cache file '%s'
cmap
CVMLClusterState
CVMLClusteringAlgorithm_Greedy
VCPClusterer: Failed to restore cluster cache due to device ran out of memory
VCPClusterer needs a full sync
missing updateHandler
VCPClusterer is not ready
VCPClusterer: Failed to get suggestions from Vision framework %@
q24@?0@"NSMutableSet"8@"NSMutableSet"16
VCPClustererBringUpState
clustererState.plist
VCPClustererBringUpStateNeedsFullSync
VCPClustererBringUpStateNeedToCompareClusters
VCPClustererBringUpStateNeedsToResetClusterCache
VCPClustererBringUpStateNeedsToResetLibraryClusters
VCPClustererBringUpStateNeedsUpdate
VCPClustererBringUpStateReady
VCPClustererBringUpStateClustering
VCPClustererBringUpStateHasUnsavedClusterCache
VCPClustererBringUpStateSavingClusterCache
VCPClustererBringUpStateHasNewClusterCache
AlgoFaceClusterCache.data
temp
interestScore
energy
peak
output1
output2
output3
cnn_hand_detector_v2.espresso.net
q24@?0@8@16
yyyy-MM-dd-HH-mm-ss
suggestionLog_
suggestions.html
function addPlaceHolders() {
addPlaceholdersForSet("visionInput", inputFaces);
addPlaceholdersForSet("visionOutput", outputFaces);
addPlaceholdersForSet("visionFiltered", filteredFaces);
function isElementHidden(element) {
var style = window.getComputedStyle(element);
return (style.display === 'none')
function updateVisibility() {
var allDivs = document.getElementsByTagName("div");
for (var i = 0; i < allDivs.length; i++) {
var d = allDivs[i];
if (!d.attributes["img"]) continue;
var rect = d.getBoundingClientRect();
if (
rect.top >= -100 &&
rect.left >= -100 &&
rect.bottom - 100 <= (window.innerHeight || document.documentElement.clientHeight) &&
rect.right - 100 <= (window.innerWidth || document.documentElement.clientWidth)
if (d.childNodes.length == 0) {
d.innerHTML = "<img src='" + d.attributes["img"].value + "' width='100' height='100'>";
else {
if (d.childNodes.length != 0) {
d.innerHTML = "";
function addPlaceholdersForSet(containerId, elements) {
var content = "";
for (var i = 0; i < elements.length; i++) {
content += "<div style='float: left; width: 100px; height: 100px; margin: 3px; background-color: darkgray' img='" + elements[i] + "'></div>"
document.getElementById(containerId).innerHTML = content;
document.onscroll = function (e) {
updateVisibility();
</script>
</head>
<body>
<p>Vision input:</p>
<div id="visionInput">
</div>
<p style="clear: both;">Vision output:</p>
<div id="visionOutput">
</div>
<p style="clear: both;">Vision filtered output:</p>
<div id="visionFiltered">
</div>
</div>
<script>
document.addEventListener("DOMContentLoaded", function (event) {
addPlaceHolders();
</script>
</body>
</html>
could not obtain access to the photo library
photo library could not provide suggestions
_suggestionsForPersonWithLocalIdentifier cancelled
<html>
<head>
<script>
 var inputFaces = [
v32@?0@"NSString"8@"NSArray"16@"NSError"24
var outputFaces = [
var filteredFaces = [
suggestPersonsForPersonWithLocalIdentifier cancelled
Input parameter is empty or nil: '%@'
VCPClusterer is nil
v32@?0@"NSSet"8Q16^B24
verifiedType != %d
VCPFaceProcessingDeleteAllVerifiedPersons
succeeded
failed
VCPFaceProcessingReclusterFacesWithThreshold
VCPFaceProcessingBuildPersons
VCPBuildPersons failed %d
VCPFaceProcessingPromotePersons
VCPPromotePersons failed %d
B32@?0@"NSDictionary"8Q16^B24
PVPersonPromoter
VCPMADServiceImageProcessingTask
Failed to decode asset
%@ not currently implemented
q24@?0@"NSObject<VCPMADServiceImageProcessingSubtaskProtocol>"8@"NSObject<VCPMADServiceImageProcessingSubtaskProtocol>"16
Request was canceled
v32@?0@"PHFace"8Q16^B24
VNFaceGazeDirectionUnknown
VNFaceGazeDirectionCamera
VNFaceGazeDirectionAnotherFace
VNFaceGazeDirectionCommonLocation
VNFaceGazeDirectionSomewhereElse
VNFaceGazeDirectionDifficultToSay
Error VNFaceGazeDirection: %lu
PHFaceGazeTypeCannotInferGaze
PHFaceGazeTypeLookingAtCamera
PHFaceGazeTypeLookingAtAnotherFace
PHFaceGazeTypeLookingAtCommonLocation
PHFaceGazeTypeOther
Error PHFaceGazeType: %d
PersonBuilderMergeCandidatesEnabled
PersonBuilderLastMinimumFaceGroupSizeForCreatingMergeCandidates
personBuilderState != %lu
VCPFaceProcessingCleanupMergeCandidates
Sceneprint task cancelled
PanoVNRequestMethod
[%@] Thumbnail is not locally available
[%@] Failed to load thumbnail image
[%@] Invalid sceneprint result
orientation
statisticsBlob
qualityScore
distanceToPreviousScene
flickerScore
sceneprintDistanceToPreviousScene
hasAction
v12@?0B8
Face clustering threshold should be in the range: [0.1, 1.0]
VCPFaceProcessingResetFaceClusteringState
VCPFaceProcessingPerformFaceClusteringAndWait
clusterer is not available
VCPFaceProcessingClusterFacesWithExtendTimeoutBlock
com.apple.mediaanalysis.clusteringQueue
curationScore
keyFrame
autoPlayable
playbackCrop
com.apple/PhotoVision/FaceCrop/
PVFC
PVFC:PVFC
PVFC_VER
PVFC_FB
PVFC_CB
PVFC_GID
tiff:Orientation
Could not set output orientation
Could not register face crop namespace
Could not generate serialized metadata representation
Could not convert metadata representation into serialized format
Could not set face crop metadata
Could not create image source
No meta data exists on image
unexpected nil image source
invalid image source
zero dimensioned face rect submitted
could not create cropped face crop image
could not create face crop metadata
public.jpeg
could not create face crop data
could not write face crop data
VCPFaceCropUtils : newFaceCropFromImageData - %@
image url is nil
Could not create image source from URL
VCPFaceCropUtils:newFaceCropFromImageURL - %@
image data is nil
Could not create image source from data
VCPFaceCropUtils:newFaceCropFromImageData - %@
invalid face crop supplied
VCPFaceCropUtils:faceBoundsFromFaceCrop -- %@
VCPFaceCropUtils:cropBoundsInOriginalImageFromFaceCrop -- %@
the supplied data is not a facecrop
could not create an image source
Could not retrieve image properties
VCPFaceCropUtils:faceCropDimensionsFromFaceCrop -- %@
could not create image ref
Could not create image for rendering
Could not create buffer for rendering
Could not create srgb colorspace
Could not create cropped and subsampled image
Could not create bitmap context
value
timescale
CVPixelbuffer not IOSurface backed
inputBoundsX
inputBoundsY
inputBoundsHeight
inputBoundsWidth
sourceSizeHeight
sourceSizeWidth
homographyParams
com.apple.mediaanalysis.VCPSharedInstanceManager
Received action score %f - %f
=========%s==========
[%.2f - %.2f]: %.2f
capturePointSegmentIdx: %d
----[%.2f - %.2f]
startIdx = %d, endIdx = %d, count = %d, [%f, %f] with score %f captureTime=%f
interesting trim: [%f, %f], score = %.2f
 --[%.2f - %.2f]
sport
cnn_activitylevel.dat
@"VCPMADMachineReadableCodeResource"8@?0
VCPMADVIMachineReadableCodeDetectionTask
UseSegmentationPregating
TrackSegments
Hand_waving
Hand_clapping
Dancing
Walking
Running
Jumping
cnn_human_action.espresso.net
regressiontree_landmark.dat
rtree_landmark_tracking.dat
com.apple.mediaanalysisd.VCPVideoFaceValidation
face_validation_warp_tri_list.dat
face_validation_warp
face_validation_warp_params.dat
%@_%d.dat
Cannot align faces: PVImage misses CGImage, URL, Data or pixelBuffer
Cannot align faces: failed to create VNImageRequestHandler.
Cannot align faces: failed to create VNAlignFaceRectangleRequest.
Cannot align faces: error: %@
v16@?0@"NSArray"8
v32@?0@"NSNumber"8@"VCPFace"16^B24
q24@?0@"VCPFace"8@"VCPFace"16
/tmp/
v32@?0@"NSNumber"8@"VNFaceprint"16^B24
v32@?0@"NSNumber"8@"NSNumber"16^B24
v32@?0@"NSNumber"8@"NSArray"16^B24
privECMVct
privEMBVct
privDFArray
privET
privImgG
privTZF
privAFS
privAFSt
privFM
relSampleTime
trajectoryHomography
presentingTimestampInNanos
originalPresentingTimestampInNanos
sequenceAdjusterRecipe
sequenceAdjusterDisplacement
interpolatedFrame
LivePhotoMetadataSetupDataVersion
FrameworkVersions
CMCaptureCore
mdta/com.apple.quicktime.live-photo-info
45.1
SalientRegions
bound
plistRepresentation
v32@?0@"NSNumber"8@"VCPVideoObjectTracker"16^B24
q24@?0@"VCPSaliencyRegion"8@"VCPSaliencyRegion"16
q24@?0@"VNClassificationObservation"8@"VNClassificationObservation"16
q24@?0@"VCPClassification"8@"VCPClassification"16
[VideoTrackDecoder status] should not be called
[VideoTrackDecoder copyNextSampleBuffer] should not be called
[VideoTrackDecoder getNextCaptureSampleBuffer] should not be called
com.apple.mediaanalysis.VCPVideoTrackSyncDecoder
/Library/Audio/Tunings/Generic/AU/aufx-epv2-mediaanalysis-appl.plist
Could not load MonzaV4_1.mlmodelc in the bundle resource
    Analyzing Audio Track - ID: %d Start: %lld/%d (%0.3fs) End: %lld/%d (%0.3fs)
[MediaAnalysis] save pixel-based recipe regardless of confidence
    Pixel Stabilization confidence doesn't pass the threshold
[MediaAnalysis] Human pose analysis - save keypoints
Failed to lock CVPixelBuffer (%p, %d)
Cannot lock NULL CVPixelBuffer
Lock attempt failed; cannot unlock buffer
Multiple unlock attempts; cannot unlock buffer
Failed to unlock CVPixelBuffer (%p, %d)
Video track rotation angle is not multiple of 90
Human action - force person detection
Human action - no PHFaces found
 VCPCNNEspressoContext - failed to use ESPRESSO_ENGINE_METAL_MPS_GRAPH fall back to ESPRESSO_ENGINE_CPU
Gyro analytics stored via dodML
copyImageToBGRHandKeypointCallFromSPI
preProcessingHandKeypointCallFromSPI
VCPMADVIVisualSearchTask running...
VCPMADVIVisualSearchTask image loading failed
VCPMADVIVisualSearchTask failed to create visual search query context (%@)
VIService_ParsedVisualSearch
VIService_VisualSearch
VCPMADVIVisualSearchTask complete (%d)
Not implemented, please use initWithOptions
Flow decoder: fail to bind inputFeature
Flow decoder: fail to bind correlation
Flow decoder: fail to bind upscaled flow
Flow decoder: fail to bind output flow
Flow decoder: fail to bind buffers
Flow decoder: executing callback
Flow decoder: fail to execute
Feature extractor: fail to bind input
Feature extractor: fail to bind output at level %d
Feature extractor: fail to bind buffers
Feature extractor: executing callback
Feature extractor: fail to execute
[MotionFlow] Failed to lock/unlock pixelbuffer (errcode: %d)
Incompatible request (%@) specified to %@
VCPMADVIDocumentRecognitionTask running...
[DocumentRecognition] Custom request configuration; overriding to use cached data
VCPMADVIDocumentRecognitionTask image loading failed
VNImageRequestHandler_init
VNImageRequestHandler_performRequests
[DocumentRecognition] Custom request configuration; not persisting result
VCPMADVIDocumentRecognitionTask complete
Multiple sampling times (%0.1fs) intersect frame at %lld/%d
%@ skipping sample %lld at %lld/%d
%@ failed for sample at %lld/%d (%@)
 [%@] QuickFaceDetect: failed to persist classification results: %@
 [%@] QuickFaceDetect: analyzing asset (deferType: %d)
 [%@] QuickFaceDetect: asset is not image
 [%@] QuickFaceDetect: processed %lu faces
[VCPDatabaseReader] Including internal fields in queries
[MediaAnalysis] Failed to open database
Closed analysis database
[MediaAnalysis] Unknown result key for result type %u
[MediaAnalysis] Error querying blacklist status for %@
[MediaAnalysis] Failed to query blacklisted assets
[MediaAnalysis] Failed to query asset %@
[MediaAnalysis] Failed to query analysis properties of asset %@
[MediaAnalysis] queryAnalysesForAssets Failed
[MediaAnalysis] Failed to query assets since %@
[MediaAnalysis] Failed to query failed assets for taskID: %lu
[MediaAnalysis] WARNING: ProcessingStatus entry with nil localIdentifier
Failed to query KeyValueStore (error code: %d)
Failed to extract NSArray from column %d (%@)
[ResourceManager] Updating budget (%0.2f --> %0.2f)
[ResourceManager] Hit usage timeout; purging resources
[ResourceManager] Request to reserve budget [Budget: %0.2f][Target: %0.2f]
[ResourceManager] Pruning inactive resources
[ResourceManager] Purging inactive resource (%@)
[ResourceManager] Failed to reserve budget [Budget: %0.2f][Target: %0.2f]
[ResourceManager] Request to activate %@
[ResourceManager] Resource not cached (%@)
[ResourceManager] Resource cached but not active (%@)
[ResourceManager] Activating resource (%@)
[ResourceManager] Resource cached and active (%@)
[ResourceManager] Active resources exceed budget
[ResourceManager] Active count %d
[ResourceManager] Request to deactivate %@
[ResourceManager] Resource transition active --> inactive (%@)
[ResourceManager] Received request to deactivate un-tracked resource (%@)
[ResourceManager] Purging active resource (%@)
[ResourceManager] Purging %@
[ResourceManager] Request to purge all resources
Live photo effects - skip using PHSceneClassification from PHAsset
Creating faceprint for face crop
Multiple faces present in face crop; using first
Loading quick identification model
Performing quick identification
Quick identification match found: %@
No quick identification match found
Home face identification task failed (%@)
Getting no object IDs when fetching assets on moment %@
VCPFaceAnalyzerReleaseCachedResources
[FaceAnalyzer] faceprint.confidence is too low (%.3f < 0.1) %@ - junkinessIndex: %.3f
[FaceAnalyzer] Accepting faceprint with confidence: %.3f %@ - junkinessIndex: %.3f
[FaceAnalyzer] Faceprint request failed to return a faceprint
[FaceAnalyzer] Missing results for roll information
[FaceAnalyzer] Missing results from VNDetectFaceCaptureQualityRequest
[FaceAnalyzer] Missing results for yaw information
[FaceAnalyzer] Missing results from VNDetectFacePoseRequest
[FaceAnalyzer] Missing results from VNDetectFaceExpressionsRequest
[FaceAnalyzer] Missing results from VNClassifyFaceAttributesRequest
[FaceAnalyzer] Gaze: mask: %s, VNFaceGazeDirection: %@, PHFaceGazeType: %@ at (%.3f, %.3f)
[FaceAnalyzer] Missing results from VNDetectFaceGazeRequest
[PhotosFace] Overwriting animal/human co-location threshold to %f
[PhotosFace] IoU %f %@ %@
[PhotosFace] IoF %f %@ %@ 
[PhotosFace] Generate VCPPhotosFace %@ from %@ and %@
[PhotosFace] Failed to serialize torsoprint; %@
[PhotosFace] torsoOnlyObservation failed to return a faceprint
[PhotosFace] Ignoring co-locating animalObservation %@
[PhotosFace] Unable to determine normalized bounding box { { %f, %f } { %f, %f } }
[PhotosFace] Failed to serialize animalprintData; %@
[PhotosFace] animalObservation failed to return a faceprint
[PhotosFace] Generate VCPPhotosFace %@ from %@
VCPFaceAnalyzerImageRequestHandlerPerformRequest
Error: Face VNImageRequestHandler::performRequests: %@
Error: failed to create blur/exposure request
Error: blurScore %f out of bound [%f, %f]
Error: VNImageRequestHandler failed to perform blurRequests: %@
Error: exposureScore %f out of bound [%f, %f]
Error: VNImageRequestHandler failed to perform exposureRequests: %@
VCPFaceAnalyzerBlurExposureAnalysis
VCPFaceAnalyzerVCPFaceCreation
VCPFaceAnalyzerVerifyAndMergeFaces
 [%@] Analysis completed; facesDetected %lu | facesToPersist: %lu | facesToDelete: %lu
 [%@] QuickFaceID: no local resource
 [%@] QuickFaceID: failed to load image
 [%@] QuickFaceID: failed to analyze asset (%d)
 [%@] QuickFaceID: image too small to classify
 [%@] QuickFaceID: %lu faces detected in %.5f seconds; %.5f second per face
VCPFaceAnalyzerLoadImageRequestHandler
Failed to create VNImageRequestHandler
Failed to analyze PVImage
Failed to refine analysis
VCPFaceAnalyzerPerformAnalysis
Unexpected media type (%lu)
[%@] Unexpected media type (%d)
Incomparable images: this - %@ vs that - %@
Action classifier - empty torso bound in PHFace %@
Action classifier - found torso bound in PHFace %@
%@ canceled (%@)
%@ failed (%@)
[EmbeddingOnDemand] Incompatible request (%@) specified to %@
[EmbeddingOnDemand] Incompatible imageAsset (%@) specified to %@
VCPMADEmbeddingGenerationTask not supported on this platform
[MediaAnalysis] Image descriptor - found more than 1 VNImageprintObservations
VNImageprint init error: %@
Query progress: unsupport taskID (%lu)
Query progress: output parameter statistics must be non-nil
Query progress: unsupported taskID (%lu)
Query progress: unsupported taskID (%@)
Query cached face progress: %lu out of %lu
Failed to query library (%@) %@ processing progress (error code: %d)
VCPVideoStabilizationAssetProcessingTask
Video Stabilization processing failed
Failed to analyzeDetectedFaces - %@
[CoreAnalyticManager] Sending single event %@ (%lu) - %@
[CoreAnalyticManager] Sent %@ (%lu)
[CoreAnalyticManager] Session event name is nil; skipping
[CoreAnalyticManager] Session fields name is nil for event %@; skipping
[CoreAnalyticManager] Start session event %@ (total session count %lu)
[CoreAnalyticManager] Setting field %@ for event %@
[CoreAnalyticManager] Ignore 0-accumulation for event %@ field %@
[CoreAnalyticManager] Accumulating field %@ for event %@
[CoreAnalyticManager] Session event %@ not available; skip sending
[CoreAnalyticManager] Sending session event %@ (%lu) - %@
[CoreAnalyticManager] flushing analytics ... 
[CoreAnalyticManager] flushSessionAnalytics (total count %lu)
QuickFaceID: Failed to create faceprint from data : %@
QuickFaceID: Failed to create animalprint from data : %@
QuickFaceID Pet: defaults writes VIPPetClassificationThreshold to %f
QuickFaceID: Passing classify face confidence: %f
QuickFaceID: Failed passing classify face confidence: %f
QuickFaceID: Failed to predict at all
QuickFaceID Pet: Passing classify pet confidence: %f
QuickFaceID Pet: Failed passing classify pet confidence: %f
QuickFaceID Pet: Failed to predict pet at all
QuickFaceID %@ Model path is nil; skip loading
Failed to load VIP %@ Model
[FaceModelBump] Failed to update version state - %@
[FaceModelBump] No persistentURL to update version state - %@
[FaceModelBump] Resetting face data ... (%@)
[FaceModelBump] Failed to reset Face Analysis data for PhotoLibrary %@
[ImageManager] kCVPixelFormatType_32BGRA with kCGColorSpaceModelMonochrome, replace with DeviceRGB
[Decode] Downscaling %zux%zu --> %zux%zu
[Decode] %.0fx%.0f --> %zu; subsampling %dx on decode
[Decode] Accelerated decode failed; falling back to CGImage
Failed to load url %@ (%@)
Persistence Delegate defaults write set %s cluster torso-only faces
Found %lu faces with CSN > 0 but not in any face groups
VCP: %@
PersistFaceGroups: Photo library is missing a face with CSN = %@
PersistFaceGroups: Faces with these CSNs will be removed from the cluster cache: %@
PersistFaceGroups: Faces with these localIdentifiers will be re-clustered: %@
PersistFaceGroups: We should not get here! If we did, then we have a previously clustered face without a face group!
PersistFaceGroups: Failed to create a face group change request to add faces!
PersistFaceGroups: Failed to find a faceGroup for face '%@' with CSN: %d
PersistFaceGroups: No faces added to face groups!
PersistFaceGroups: Failed to find face with localIdentier: %@. Could not set its CSN to %@
PersistFaceGroups: Set personBuilderState of faceGroups: %@
PersistFaceGroups: Failed to delete empty face groups with error: %@
PersistFaceGroups: Canceled updating key faces unverified persons after persisting face groups.
PersistFaceGroups: Failed to update key faces unverified persons after persisting face groups. Error: %@
%s: Input parameter is empty or nil: '%@'
%s: %@
UpdateKeyFaces: Key Face exists. Ignoring %@
Updating key face %@ on person %@
Error: did not find single face group for unverified person, unable to set key face on face group, (number of face groups: %lu)
Error: could not set key face for person %@
Warning: cannot handle representativeness with imageprint type %d; ignoring
Warning: Couldn't get faceprint data for face: %@; ignoring
representativeness selection receives a torso-only print; ignoring
Failed to get VNFaceTorsoprint from faceprint data - %@
Warning: Could not get representativeness for faces, error: %@
PersonBuilder: Deleted duplicate graph-verified persons '%@' from face group %@
PersonBuilder: Failed to delete duplicate graph-verified persons '%@' from face group %@
PersonBuilder: Deduped graph-verified persons '%@' from face group %@
PersonBuilder: Failed to dedupe graph-verified persons '%@' from face group %@
personLocalIdentifier for PHFace %@ is null; skip processing
Found no persons rejected for a rejection training face: %@
PersonBuilder: Did not find merge candidate persons with local identifiers: '%@'
PersonBuilder: Found invalid merge candidate pair ['%@' : '%@']
PersonBuilder: Already found merge candidate pair ['%@' : '%@']
PersonBuilder: Unexpected error - could not create merge candidate pair '%@' : '%@'
PersonBuilder: Unexpected error - could not create invalid merge candidate pair '%@' : '%@'
PersonBuilder: Cleared personBuilderState of faceGroup: '%@'
PersonBuilder: merge candidate pair '%@' : '%@' - reason: '%@'
Could not find a face with clusterSequenceNumber '%@' in the library
[RejectedFaceCrop] Checking FaceGroup %@ with %lu faces: %@
[RejectedFaceCrop] faceCrop %@ face %@
[RejectedFaceCrop] Checking FaceGroup %@ with rejected person %@
[RejectedFaceCrop] Remove face %@ for person %@
[RejectedFaceCrop] Failed to update person - %@
[RejectedFaceCrop] After rejection, person %@ has face %@ (%@)
PersonBuilder: Got a 'nil' photoLibrary. Cannot build persons
PersonBuilder: Failed to find unverified person for faceGroups '%@'; These will be fixed up and retried later
PersonBuilder: Failed to fix up face groups without unverified person. Error: '%@'
PersonBuilder: Person Building faceGroup '%@'
PersonBuilder: Failed to find unverified person [unverifiedPerson: %@, unverifiedPersonLocalIdentifier: %@] for faceGroup '%@', skipping this face group
Person Builder: Quick classification faces found. Number of faces retained: %@. Number of faces reassigned %@
PersonBuilder: We may have a dirty level0 cluster, persons with training faces: %@
PersonBuilder: We may have a dirty level0 cluster, verified persons with confirmed face: %@
PersonBuilder: Unnamed unconfirmed faces in face group, '%@', without a training face: %@
PersonBuilder: Found training rejection, unassigned faces on trainingPersonLocalIdentifier in level0 cluster: %@
PersonBuilder: Skip processing level0 cluster since we have rejected face for training person '%@' in level1 cluster
PersonBuilder: Failed to build persons [Error: '%@']
PersonBuilder: ---> buildPersonWithFaceClusterer, %s
PersonBuilder: Person Building is Disabled!
PersonBuilder: Cleared personBuilderState of faceGroups: %@
PersonBuilder: Failed to clear personBuilderState of faceGroups: %@, error: %@
PersonBuilder: <--- buildPersonWithFaceClusterer
Got a 'nil' photoLibrary. Cannot remove auto assigned faces
Failed to remove auto-assigned faces from person '%@', error: %@
[PreAnalysis] Pre-warmed image unused (%dx%d)
[PreAnalysis] Image not pre-warmed; creating on-demand (%dx%d)
Defaults write set ProbableRotation %s
Apply Sharpness Model V1
Apply Sharpness Model V2
Unknown revision; apply Sharpness Model V%lu
Failed to load PVSceneTaxonomySoft
 [ProbableRotation] Failed to load %@
Including Pre Analysis VNRequests (%lu): %@
VCPSceneAnalyzerReleaseCachedResources
Failed to create VNClassifyImageAestheticsRequest
Failed to create VNSceneClassificationRequest
Failed to create VNCreateSceneprintRequest
Failed to create VNClassifyJunkImageRequest
Failed to create VNGenerateAttentionBasedSaliencyImageRequest
Failed to create VNGeneratePhotosAdjustmentsRequest
Failed to set VNCreateSceneprintRequest::setPrivateRevision %lu: %@
Failed to set VNClassifyJunkImageRequest::setPrivateRevision %lu: %@
Failed to create %@
Unsupported observation label in VCPSpecialLabelToSceneClassificationID %@
Unsupported observation label %@
[DO] detectedObjects count is 0; skip detectedObjects
[DO] invalid confidenceMax: %f; skip detectedObjects
[DO] Failed to choose the best bounding box c_max: %f, c_threshold (0.8x): %f from %@
[DO] Unsupported observation label in PVSceneTaxonomyNode %@
Unsupported observation label in PVSceneTaxonomyNode: %@
Unsupported observation label in VCPSpecialLabelToSceneClassificationID %@ (%@)
Error creating VNRequest
VCPSceneAnalyzerImageRequestHandlerPerformRequest
VCPSceneAnalyzerImageRequestHandlerPerformRequest_299
CVNLPCommSafetyHandler_IVS
VCPSceneAnalyzerImageRequestHandlerPerformRequest_360
VCPSceneAnalyzerImageRequestHandlerPerformRequest_256
Failed to run VNImageRequestHandler::performRequests: %@
VCPSceneAnalyzerImageBlurAnalysis
VCPSceneAnalyzerExposureAnalysis
VCPSceneAnalyzerRotationAnalysisScaling
[ProbableRotation] invalid coreML results
VCPSceneAnalyzerRotationAnalysisInference
VCPSceneAnalyzerLoadImageRequestHandler
Failed to load imageURL: %@
VCPSceneAnalyzerPerformAnalysis
Analysis Cancelled
SceneProcessingLoadAsset
SceneProcessingAnalyzeAsset
  [%@] Failed to decode last frame of video, fall back to thumbnail 
[MediaAnalysis] Junk analayzer - unexpected %d VNObservations
VIService_init
[%@][MAMLModel] Failed to open model file at url %@
[%@][MAMLModel] Failed to load compiled model (%@): %@
[MAMLModel] Input feature %@ %ldx%ld %d
[MAMLModel] Missing inputImage feature description %@
[MAMLModel] Mismatched inputImage width (%ld) and height (%ld)
[MAMLModel] Output feature %@ %@
[MAMLModel] Missing output feature %@
[AutoCounter] feature not supported on this OS variant
Defaults write set AutoCounter %s dump Faceprint
[AutoCounter] Failed to find asset for face: %@; skip
[AutoCounter] Asset without cloudIdentifier, use localIdentifier: %@
[AutoCounter] Person without localIdentifier; use face.personLocalidentifier
[AutoCounter] Face without personLocalIdentifier; skip
[AutoCounter] Fetched face/person not matching required person; skip
[AutoCounter] Face in a facegroup without localIdentifier; skip
[AutoCounter] No valid faceprint data; leave as unknown
[AutoCounter] No valid momentLocalIdentifier; leave as 'unknown'
[AutoCounter] Face without localIdentifier; skip
[AutoCounter] Failed to fetch person %@
[AutoCounter] Fail to load groundtruth file
[AutoCounter] Person (%@) already opt-in; skip
[AutoCounter] Cannot write opt-in groundtruth to %@ : %@
[AutoCounter] Export URL: %@
[AutoCounter] Failed to find facegroup for mergeCandidate: %@
[AutoCounter] Reach kVCPMaximumNumberOfMergeCandidatesShown (%lu); skip the rest
[AutoCounter][ClusterDump] FaceGroupCount %lu
[AutoCounter][ClusterDump] FaceCount %lu
[AutoCounter] Saved cluster state to %@
[AutoCounter] Cannot write to %@ : %@
[AutoCounter][P/R][GT] Fail to load groundtruth file: %@
[AutoCounter][P/R][GT] Invalid faceID for face: %@; ignore
[AutoCounter][P/R][GT] Invalid PersonID for faceID: %@; ignore
[AutoCounter][P/R][GT] Load faceID: %@ for PersonID: %@
[AutoCounter] Saved assets-to-faces details to %@
[AutoCounter] Cannot write assets-to-faces to %@ : %@
Defaults write set AutoCounter %s dump assets-to-faces details
[AutoCounter][P/R] Fail to load cluster state file: %@
[AutoCounter][P/R] Cluster contains no asset information
[AutoCounter][P/R] Cluster contains no data
[AutoCounter][P/R] Invalid information for asset %@ in cluster; ignore
[AutoCounter][P/R] Invalid ID(s) in cluster: %@; ignore
[AutoCounter][P/R] Invalid face rectangle in cluster state for faceID:%@; ignore
[AutoCounter][P/R] processing cluster state faceID: %@ forPersonID: %@
[AutoCounter][P/R] Invalid ground truth rect for faceID:%@
[AutoCounter][P/R][%@] %.4f library: %@, gt: %@ (fid:%@, pid:%@)
[AutoCounter][P/R] Co-location mapping faceID:faceGroupID %@:%@ -> %@:%@
[AutoCounter][P/R] Cannot find asset for id %@
[AutoCounter][P/R] Precision for FaceGroup (of size %d) for personID %@ (of size %lu) is %f
[AutoCounter][P/R] Valid singleton count = %lu, invalid singleton count = %lu
[AutoCounter][P/R] Valid face count for person %@ is %d
[AutoCounter][P/R] personID %@ Recall (of size %lu) is %f
[AutoCounter][P/R] personID %@ Recall (exclude detection miss) (of size %lu) is %f
[AutoCounter][P/R] Weighted Precision: %f, Weighted Recall: %f (number of best face: %.0f)
[AutoCounter][P/R] Weighted Recall (exclude detection miss): %f (number of best face: %.0f)
[AutoCounter][P/R][PV] Processing person cluster %@ with %lu faces
[AutoCounter][P/R][PV] Invalid faceID in person cluster: %@; ignore
[AutoCounter][P/R][PV] Failed to fetch asset for face %@; ignore
[AutoCounter][P/R][PV] Asset without cloudIdentifier, use localIdentifier: %@
[AutoCounter][P/R][PV] Invalid face rectangle in person cluster state for face: %@; ignore
[AutoCounter][P/R][PV] processing person cluster faceID: %@ for PersonID: %@ and clusterID: %@
[AutoCounter][P/R][PV] Valid faceID mapping faceID:personID %@:%@ -> %@:%@
[AutoCounter][P/R][PV] Invalid faceID mapping faceID:faceGroupID %@:%@ -> %@:%@
[AutoCounter][P/R][PV] Invalid ground truth face rectangle for faceID:%@
[AutoCounter][P/R][PV] Valid co-locate mapping faceID:personID %@:%@ -> %@:%@
[AutoCounter][P/R][PV] Invalid co-location mapping faceID:faceGroupID %@:%@ -> %@:%@
[AutoCounter][P/R][PV] Precision for cluster (of size %d) for personID %@ (of size %lu) is %f
[AutoCounter][P/R][PV] Valid singleton count = %lu, invalid singleton count = %lu
[AutoCounter][P/R][PV] Recall for personID %@ (of size %lu) is %f
[AutoCounter][P/R][PV] Weighted Precision: %f, Weighted Recall: %f
[AutoCounter][CA] Report CoreAnalytics: %@
[AutoCounter][CA] Failed to retrive CoreAnalytics export URL
[AutoCounter][CA] Saved CoreAnalytics to %@
[AutoCounter][CA] Cannot write CoreAnalytics to %@ - %@
[AutoCounter][CA] Cannot retrieve CoreAnalytics files %@
[AutoCounter][CA] Files in folder %@
[AutoCounter][CA] Report CoreAnalytics files: %@
[AutoCounter][CA] Report CoreAnalytics file: %@
[AutoCounter][CA] Finished reporting CoreAnalytics %@
[AutoCounter][P/R] Failed to measure Vision cluster state against ground truth
[AutoCounter][P/R][PV] Failed to measure Person cluster state against ground truth
[AutoCounter][P/R][PV] Failed to report CoreAnalytics
[AutoCounter][P/R][SIMLGT] Failed to load SIML ground truth - %@
[AutoCounter][P/R][SIMLGT] Failed to serialize SIML ground truth - %@
[AutoCounter][P/R][SIMLGT] Load faceID: %@ for PersonID: %@
[AutoCounter][P/R][SIML] Failed to export current clusters states
[AutoCounter][P/R][SIML] Validate cluster state  %@ against ground truth %@
[AutoCounter][P/R][SIML] Failed to measure Vision cluster state against SIML ground truth
Unknown Media Analysis version specified (%d)
[MediaAnalysis] No slow-mo timerange mapper available, fall back to Scaled Time
[MediaAnalysis] No slow-mo timerange mapper available, fall back to Original Time
Invalid Live Photo Gating result type key [%@]
VCPVersionForTask not implemented for %@ (%d); using MediaAnalysisVersion (%d)
  [%@] Unknown analysis version %d; discarding
Base retry interval override (%lu seconds)
[MediaAnalysis] Requested post process highlight with NULL input analysis
[MediaAnalysis] Post-process highlights returned NULL
[MediaAnalysisResultsTypesForAnalysisTypes] Unknown result type
Not all needed analysis are available for video highlights.
[%.2f - %.2f] expressionScore=%.2f, actionScore=%.2f, voiceScore=%.2f, Score=%.2f
[%.2f - %.2f] keyFrameScore=%.2f, expressionScore=%.2f, actionScore=%.2f, voiceScore=%.2f, humanActionScore=%.2f, humanPoseScore=%0.2f, qualityJunkScore = %.2f, Score=%.2f
Media analysis client XPC connection interrupted
Media analysis client XPC connection invalidated
[MediaAnalysis] [MediaAnalyzer requestAnalysisTypes] call with invalid resourceURLs
Failed to issue sandbox extension on %@
[MediaAnalysis] Error connecting to background analysis service
[MediaAnalysis] Request %d has completed
[MediaAnalysis] Error connecting to Photos background analysis service
[MediaAnalysis] Unsupported task %lu
[MediaAnalysis] Asset processing request %d has completed
In-Process quick face identification not supported
[MediaAnalysis] Error connecting to Photos Quick Face Identification service
[MediaAnalysis] Request %d is %.2f%% complete
[MediaAnalysis] Unknown analysis request %d; dropping cancellation request
[MediaAnalysis] No active analysis requests; dropping cancellation request
[MediaAnalysis] Failed to cancel background analysis: %@
[MediaAnalysis] Background analysis canceled
[MediaAnalysis] Error connecting to background analysis service: %@
[MediaAnalysis] Error connecting to request PersonPromoterStatus service
[MediaAnalysis] Request Person Preference %d has completed
[MediaAnalysis] Request VIP model filepath Preference %d has completed
[MediaAnalysis] Error connecting to request SuggestedPersons service
[MediaAnalysis] Request SuggestedPersons %d has completed
[MediaAnalysis] Error connecting to request UpdateKeyFacesOfPersons service
[MediaAnalysis] Request UpdateKeyFacesOfPersons %d has completed
[MediaAnalysis] Error connecting to request FaceCandidatesforKeyFace service
[MediaAnalysis] Request FaceCandidatesforKeyFace %d has completed
[MediaAnalysis] Error connecting to request ResetFaceClassificationModel service
[MediaAnalysis] Request ResetFaceClassificationModel %d has completed
[MediaAnalysis] Error connecting to request ResetPetClassificationModel service
[MediaAnalysis] Request ResetPetClassificationModel %d has completed
[MediaAnalysis] Error connecting to request SuggestedMePersonIdentifier service
[MediaAnalysis] Request SuggestedMePersonIdentifier %d has completed
[MediaAnalysis] Request PersonPromoterStatus %d has completed
[MediaAnalysis] Error connecting to request Face and Person workflow
[MediaAnalysis] Request Face and Person workflow %d has completed
[MediaAnalysis] Error connecting to request ClusterCacheValidation service
[MediaAnalysis] Request ClusterCacheValidation %d has completed
[MediaAnalysis] Error connecting to request ResetFaceClusteringState service
[MediaAnalysis] Request ResetFaceClusteringState %d has completed
[MediaAnalysis] Error connecting to request ReclusterFaces service
[MediaAnalysis] Request ReclusterFaces %d has completed
[MediaAnalysis] Error connecting to request RebuildPersons service
[MediaAnalysis] Request RebuildPersons %d has completed
[MediaAnalysis] Error connecting to query AutoCounter Opt-In status service
[MediaAnalysis] Query AutoCounter Opt-In status %d has completed
[MediaAnalysis] Error connecting to request Opt-In AutoCounter
[MediaAnalysis] Request Opt-In AutoCounter %d has completed
[MediaAnalysis] Error connecting to request AutoCounter dump
[MediaAnalysis] Request AutoCounter dump %d has completed
[MediaAnalysis] Error connecting to request AutoCounter calculation
[MediaAnalysis] Request AutoCounter calculation %d has completed
[MediaAnalysis] Request AutoCounter SIML validation %d has completed
[MediaAnalysis] Faces must be non-empty and completion block must be non-nil
[MediaAnalysis] Faces must all be from the same Photo Library
[MediaAnalysis] Error connecting to Media Analysis
[MediaAnalysis] nil specified for non-nullable parameter
[VCPMediaAnalyzer] Client XPC connection interrupted
[VCPMediaAnalyzer] Client XPC connection invalidated
[VCPMediaAnalyzer] Acquiring media analysis directory sandbox extension...
[VCPMediaAnalyzer] Failed to establish connection or connection lost to service %@; %@
[VCPMediaAnalyzer] Failed to consume media analysis directory sandbox extension
[VCPMediaAnalyzer] Consumed media analysis directory sandbox extension
[MediaAnalysis] failed to get database sandbox extension
[MediaAnalysis] failed to consume sandbox extension
[MediaAnalysis] Consumed sandbox extension
[MediaAnalysis] Failed to obtain analysis sandbox extension for Photo Library (%@); client may not be able to open analysis database
[MediaAnalysis] Requested max highlight duration longer than %.2fs, fall back to %.2fs
[MediaAnalysis][%@] Storing on-demand analysis
[MediaAnalysis][%@] Failed to store on-demand analysis
[MediaAnalysis][%@]Unable to open movie
[MediaAnalysis][%@]Failed to create asset
[MediaAnalysis][%@] Received analysis request: %@
[MediaAnalysis][%@] Analysis requested for blacklisted asset
[MediaAnalysis][%@] Existing analysis based on old modification
[MediaAnalysis][%@] Existing analysis based on degraded asset
[MediaAnalysis][%@] Existing analysis satisfies request (%@)
[MediaAnalysis][%@] Existing analysis doesn't match asset state
[MediaAnalysis][%@] Existing analysis doesn't satisfy request (%@)
[MediaAnalysis][%@] Generating analysis on-demand: %@
[MediaAnalysis][%@] Analysis served: (%@)
[MediaAnalysis] [MediaAnalyzer requestAnalysisForAsset] call from invalid instance
[MediaAnalysis] Failed to obtain database for Photo Library %@
[MediaAnalysis] [MediaAnalyzer assetsAnalyzedSinceDate] call from invalid instance
[MediaAnalysis] Failed to obtain database for Photo Library (%@)
Cannot load %@ for %@, NSData length: %lu, content: %@
Cannot load %@ from PHAsset, NSData length: %lu, content: %@
[MediaAnalysis] [MediaAnalyzer distanceFromAsset] call from invalid instance
[MediaAnalysis] Failed to obtain database for assets
[MediaAnalysis] failed to request analyses
[MediaAnalysis] [requestAnalysesForAssets] call from invalid instance
[MediaAnalysis] [requestAnalysesForAssets] in standalone mode but on-demand not allowed
[MediaAnalysis] call from invalid instance
[MediaAnalysis] on-demand analysis requested in standalone mode
Warning: On demand analysis is not supported.
[MediaAnalysis] Failed to obtain database for collection %@
[MediaAnalysis] [requestLivePhotoEffectsForAssets] call from invalid instance
[MediaAnalysis] [requestLivePhotoEffectsForAssets] in standalone mode but on-demand not allowed
Query context: %@
VCPMADVIVisualSearchGatingTask running...
VCPMADVIVisualSearchGatingTask image loading failed
VCPMADVIVisualSearchGatingTask failed to create visual search query context (%@)
VIService_VisualSearchGating
VCPMADVIVisualSearchGatingTask complete (%d)
Human action - use %@
Human action - option %@ not supported
VideoCNN model version v4.1
  [%@] Existing analysis outdated; dropping
VCPLightVideoAnalyzer
VCPVideoStabilizerPixel
VCPVideoFaceDetector
VCPFullVideoAnalyzer
VCPVideoSceneClassifier
VCPVideoActivityAnalyzer
VCPVideoSaliencyAnalyzer
VCPVideoHumanActionAnalyzer
VCPVideoHumanActionClassifier
VCPVideoPetsAnalyzer
VCPMovieCurationAnalyzer
VCPVideoStabilizer
VCPVideoCNNAnalyzer
    Analyzing Video Segment - Track ID: %d Start: %lld/%d (%0.3fs) End: %lld/%d (%0.3fs)
VCPAudioAnalyzer
    Video track has invalid full frame dimensions (%.f,%.f)
    Video track has invalid clean aperture rect
VCPVideoStabilizerGyro
  [%@] Asset doesn't have gyro metadata
  [%@] Asset does not have valid video track; all %lu tracks: %@
    Video track has invalid dimensions (%.f,%.f)
VCPMovieAnalyzer
[MotionFlowAnalyzer] Failed to request flow from VCPMotionFlowRequest: %@
Motion flow is null
VNSession_init
Face analysis - skip using PHFace from PHAsset
Face analysis - use PHFace expression from PHAsset
Face analysis - PHFace expression can only be used with PHFace
  [%@] Need Face Processing: no faceAdjustmentVersion
  [%@] Need Face Processing: faceAdjustmentVersion %@ != adjustmentTimestamp %@
  [%@] Fingerprint requested for asset with no objectID
  [%@] Fingerprinting failed
QuickFaceID Model: persistent storageDirectoryURL is nil
QuickFaceID Model: cannot load Persons Model: %@
VCPPersonVIPLoadModel
QuickFaceID Model: model with VNCreateFaceprintRequest revision %lu (FaceProcessing Version%d)
QuickFaceID Model: system is using VNCreateFaceprintRequest revision %lu (FaceProcessing Version%d)
QuickFaceID: failed to initialize face analyzer
QuickFaceID Pet Model: persistent storageDirectoryURL is nil; skip loading Model
QuickFaceID Pet Model: cannot load Model: %@
VCPPetVIPLoadModel
 [%@] QuickFaceID: classifying face: %@; skip processing face
 [%@] QuickFaceID: did not find a matching person for face located at (%.3f, %.3f)
 [%@] QuickFaceID: found person: %@
 [%@] QuickFaceID: %lu faces classified in %.5f seconds; %.5f second per face
 [%@] QuickFaceID: expected to find a person for person uuid = %@; skipping
 [%@] QuickFaceID: failed to persist classification results: %@
 QuickFaceID Persons Model is not ready; skip processing
 [%@] QuickFaceID: analyzing asset (deferType: %d)
 [%@] QuickFaceID: asset is not image
 [%@] QuickFaceID: processed %lu faces
 QuickFaceID Pets Model is not ready; skip classifying
 QuickFaceID Pet: pet (PHFace) %@ already has a nameSource %ld for petPerson %@; skip
 QuickFaceID Pet: pet (PHFace) %@ is used to train this VIP model with petPerson %@; skip
QuickFaceID Pet: Could not create animalprint for pet (PHFace) %@ - %@
 QuickFaceID Pet: Failed to classify %@ - %@; skip
 QuickFaceID Pet: did not match %@ (at %.3f, %.3f)
 QuickFaceID Pet: classified %@ to petPerson %@
 QuickFaceID Pet: no petPerson %@; skipping
 QuickFaceID Pet: failed to persist pet classification results: %@
 QuickFaceID Pet: classified and persisted %lu Pet PHFace
QuickFaceID Model: unknown VIP type (%lu); no entity fetched
QuickFaceID Pets Model: Begin Pets model generation
QuickFaceID Pets Model: Failed to initialize VNAnimalObservation
QuickFaceID Pets Model: Failed to create VNEntityIdentificationModelConfiguration - %@
Failed to create VNMutableEntityIdentificationModel - %@
QuickFaceID Pets Model: Model generation cancelled; quitting
QuickFaceID Pets Model: petPerson: %@, petFaceFetchResult(%lu): %@
QuickFaceID Pets Model: Could not create animalprint for pet (PHFace): %@ - %@
QuickFaceID Pets Model: Could not add animalObservation to model for pet (PHFace): %@.
QuickFaceID Pets Model: animalObservations(%lu): %@
QuickFaceID Pets Model: Could not add animalprint to model - %@
QuickFaceID Pets Model: Finished model generation
QuickFaceID Pets Model: Failed to persist pet model %@
QuickFaceID Pets Model: Could not get animalObservations for pet %@ - %@
QuickFaceID Pets Model: Could not persist isInVIPModel on trained pets - %@
QuickFaceID Pets Model: Finished model generation and persistence
QuickFaceID Model: Begin model generation
QuickFaceID Model: Model generation cancelled. Quitting
FaceID Model: fetch count for person %@: %lu
FaceID Model: fetch count without roll predicate for person %@: %lu
QuickFaceID Model: Could not create faceprint for face: %@. Error: %@
QuickFaceID Model: Could not add faceprint to model for face: %@.
QuickFaceID Model: Could not add faceprints to model. Error: %@
QuickFaceID Model: Finished model generation
QuickFaceID Model: Failed to persist model %@
QuickFaceID Model: Could not get face observations for person %@ - %@
QuickFaceID Model: Could not persist isInVIPModel on trained faces - %@
QuickFaceID Model: Finished model generation and persistence
QuickFaceID %@ Model: Last job generation %.0fs ago, job is due = %@
QuickFaceID [FastMigration]: asset processing progress: total: %ld, processed: %ld, failed: %ld
QuickFaceID [FastMigration]: asset processing rate: processed>90%%: %s, failure>10%%: %s, pass: %s
QuickFaceID [FastMigration]: defaults write is set %s
QuickFaceID [FastMigration]: persistent storageDirectoryURL is nil
QuickFaceID [FastMigration]: cannot load Persons Model: %@
QuickFaceID %@ Model: ignoreLastGenerationTime: %s
QuickFaceID %@ Model: No need to generate model
QuickFaceID Model: unknown VIP type (%lu); no model generated
[%@] Asset has no small video derivative; skipping
[%@] File size exceeds streaming threshold; skipping
[%@] Duration exceeds streaming threshold; skipping
Unknown VCPTaskID (%lu); redirect to VCPTaskID_MediaAnalysis
OCR gating threshold set to %0.2f via default
[%@] Text Confidence: %0.2f Passed Gating: %d
[%@] Text Confidence: 0.00f Passed Gating: 0 [Absent]
[%@] Asset scene properties unavailable or out-of-date
  Analyzing degraded version of Movie
  [%@] missing Pre Analysis result
  Analyzing degraded version of Photo
VCPImageFaceDetector
VCPImageFaceExpressionAnalyzer
VCPImageJunkAnalyzer
VCPImageBlurAnalyzer
VCPLowResImageBlurAnalyzer
VCPImageExposureAnalyzer
VCPImageLivePhotoBlurAnalyzer
VCPImageCompositionAnalyzer
VCPImageDescriptor
VCPImageSaliencyAnalyzer
VCPImagePetsAnalyzer
VCPImageHumanPoseAnalyzer
VCPImageHandsAnalyzer
VCPPhotosQuickFaceDetection
VCPLivePhotoAnalysis
VCPEffectsAnalyzer
[MediaAnalysis] PhotoAnalyzer - Original movie is not available, skip effects analysis
VCPLivePhotoKeyFrameAnalyzer
VCPPhotoAnalyzer
No persistentStorageDirectoryURL for photoLibrary: %@
Unable to serialize library analysis preferences for %@: %@
Unable to write library analysis preferences for %@: %@
Key for setLibraryAnalysisPreferencesValue is nil
Failed to fetch VIP model file path with unknown VCPMAVIPType (%lu)
Failed to fetch VIP model last generation date with unknown VCPMAVIPType (%lu)
PHPhotoLibrary+MediaAnalysis defaults write set %s cluster torso-only faces
[MediaAnalysis] Sceneprint data - skip fetching from PHAsset
Error -[VNCreateSceneprintRequest setPrivateRevision:error:]
Error -[VNImageRequestHandler requestHandler:error:]
NSKeyedUnarchiver error: %@
 VCPFaceShapeModel - caught exception in find_min_box_constrained()
VCPFaceShapeModel - caught exception in find_min()
 VCPFaceShapeModel - caught exception in find_min_using_approximate_derivatives()
%@ does not implement purge
Unknown device type; this may adversely impact capabilities & performance
[MotionFlowSubtleMotionAnalyzer] Failed to request flow from VCPMotionFlowRequest: %@
Fail to initialize motionFlowAnalyzer
Fail in generating motion flow
Invalid VNRequest configuration (%@)
VNRequest must be non-nil
 keypointsToObservation - unexpected keypoints count
incompatible input buffer size/format, check requiredInputFormat
VCPFaceGeometry initWithCoder - vertices data missing
VCPFaceAnchor initWithCoder - unexpected size of transform data
VCPCaptureAnalysis - missing resolution properties for prewarming
Failed to open analysis database for Photo Library (%@)
Specified Photo Library has no URL (<%@>); cannot find analysis database
Connecting to system photo library...
Opening system photo library...
Opened system photo library
Failed to open system photo library (%@)
Failed to obtain system photo library URL
Closed Photo Library
Photo Library unavailable (%@); closing Photo Library...
[DAS QoS] %@: %@ (%@) download %lu bytes
Requested resource exceeds maximum supported size
Resource already in the buffer. Skip downloading.
requestDownloadOfResource: %@
Download progress: %.2f
    Received %llu bytes (Overall: %llu/%llu)
Data received exceeds maximum supported size
Failed to download asset resource (%@)
Successfully downloaded asset resource
Failed to issue resource request
Download resource timed-out
Cancelling download
  [%@] Processing
[MediaAnalysis][%@]Unable to open movie, skip
  [%@] Analysis cancelled
  [%@] Analysis failed to complete
CNNFastGestureRecognition: start loading model
CNNFastGestureRecognition: inputBlob.height = %d, inputBlob.width = %d, inputBlob.channels = %d
CNNFastGestureRecognition: successfully loaded model
[OCR][%@] Re-using cached results
[Faces][%@] Asset not processed or outdated
[Faces][%@] Loading existing results from Photos
[NSFW][%@] Asset not processed or outdated
[NSFW][%@] Loading existing results from Photos
[%@] Selecting resource for Asset Type: %@ [%d/%d] Resolution: %dx%d
[%@] Evaluating resource (Type: %d Resolution: %dx%d)
[%@] No high-res resource available; falling back to thumbnail
[%@] Unsupported type (%@); skipping resource
[%@] Purging resource cache to load large resource
[%@] Decoding resource to %dx%d (scaled)
[%@] Decoding resource to %dx%d (full)
[%@] Loaded resource (Type: %d Actual Resolution: %dx%d)
[OCR][%@] Checking for existing results from Photos
[OCR][%@] Loading existing results from Photos
[OCR][%@] Failed to unarchive existing Photos results
[OCR][%@] Photos results exist, but no text was recognized
[OCR][%@] Asset does not have existing results
[OCR][%@] Successfully reused existing results
[OCR][%@] Persisting results to Photos
VNDocumentObservation_archive
[OCR][%@] Failed to archive results
[OCR][%@] No text recognized; skipping archive/persistence
[OCR][%@] Successfully persisted results to Photos
[OCR][%@] Failed to persist results to Photos
Cannot create VCPPersonBuilder
---> Canceling VCPBuildPersons
VCPBuildPersons canceled
VCPBuildPersons failed: %@
Cannot create PVPersonPromoter
---> Canceling VCPPromotePersons
Person Processing: Starting Person Promoting
Person Processing: Person Promoting %@
VCPPromotePersons canceled
VCPPromotePersons failed
Cannot create PVPersonPromoter for evaluation
---> Canceling VCPFetchPersonPromoterClusterForEvaluation
Person Processing: Start evaluatePersonPromoterWithUpdateBlock
Person Processing: Retrieved %lu unverified person
Person Processing: evaluatePersonPromoterWithUpdateBlock canceled
Unknown Photos Face Processing umbrella version %d
FaceV6Models is no longer supported
FaceModelOverwrite is no longer supported
HomeKit analysis client XPC connection interrupted
HomeKit analysis client XPC connection invalidated
[HomeKitAnalysis] Error connecting to background analysis service
[HomeKitAnalysis] Request %d is %.2f%% complete
[HomeKitAnalysis] Unknown analysis request %d; dropping cancellation request
[HomeKitAnalysis] No active analysis requests; dropping cancellation request
VNGeneratePhotosAdjustmentsRequest failed
  Fullfilled content request: %@
  Fullfilled data request: %@
Reachability initialization failed; assuming no connection
Reachability flags invalid; assuming no connection
%sonnected to internet via WiFi/Ethernet
Network reachability flag changed to: %@
CVNLPCommSafetyHandler_init
VCPMADImageSafetyClassificationTask running...
VCPMADImageSafetyClassificationTask image loading failed
VCPMADImageSafetyClassificationTask image pre-processing failed
CVNLPCommSafetyHandler_classifyPixelBuffer
VCPMADImageSafetyClassificationTask failed (%@)
VCPMADImageSafetyClassificationTask complete
[VCPFaceCropGenerator][%@] Generated faceCropData is nil
[VCPFaceCropGenerator] faceCropSourceDescriptors count is 0
[FacecropManager][%@] facesToPersist: %lu | facesToDelete: %lu
[FacecropManager][%@] Publish facecrop %@ 
[FacecropManager] No best face from faceDetectionRequest %@; force to create one
[FacecropManager] Failed to generate faceprint from facecrop %@ - %@
[FacecropManager] Failed to persist association of face %@ with facecrop %@ - %@
[FacecropManager] Failed to fetch just-persisted face %@ - %@
[FacecropManager] Failed to update faceprint of face %@ associated with facecrop %@ - %@
[FacecropManager] Set personBuilderState of faceGroup %@ for face %@
[%@] Analyzing facecrop (%.0fx%.0f)
[FacecropManager] %@ is not in a dirty state
[FacecropManager] %@ does not have a payload (image data)
[FacecropManager] Failed to update processed facecrop %@ - %@
[FacecropManager] Failed to record needing to Person Building for facecrop %@ - %@
[FacecropManager][%@] No faces detected; skip facecrop generation
[FacecropManager][%@] Facecrop will not be generated for the manual face %@
[FacecropManager] Library has %lu dirty face crops to analyze
[FacecropManager] Failed to process dirty facecrop %@ - %@
VCPFaceProcessingDirtyFaceCrops
[Perf] %s: %0.6fs
%-40s  %10s  %10s  %10s  %10s  %10s
  %-38s  %10.6f  %10.6f  %10.6f  %10.6f  %10zu
  [%@] No scene classification result fetched from pre analysis
Scene identifier %u has no name; ignoring
[MediaAnalysis][%@] No slow-mo timestamp mapping file URL found
[MediaAnalysis][%@] No slow-mo timestamp mapping file found
[%@] Asset has no small video derivative; cannot download
VCPPriorityAnalysis - Start initializing
VCPPriorityAnalysis - Finished initializing hand detector
VCPPriorityAnalysis - Finished initializing hand keypoint detector
VCPPriorityAnalysis - Finished initializing gesture recognizer
VCPPriorityAnalysis - Number of hand detected %lu
VCPPriorityAnalysis - dominant hand: %d, hand chirality counter: left: %d, right: %d
VCPPriorityAnalysis - frame interval %f ms
VCPPriorityAnalysis - gestureScoreRightHand %f, gestureScoreLeftHand %f
VCPPriorityAnalysis - gesture score = %f, priority score after thresholding = %f
VCPPriorityAnalysis - Analysis subsampling ratio = %f
VCPPriorityAnalysis - Face yaw: %d
VCPPriorityAnalysis - output priority score = %f
song analysis failed %@
VCPEmbeddingAnalyzerLoadImageRequestHandler
VCPNeuralHashprintRequest
NeuralHashprint Vision request failed: %lu - %@
VCPImageHashSignatureRequest
NeuralHash+LSH Vision request failed: %lu - %@
NeuralHash+LSH invalid imageSignatureHash
NeuralHash+LSH failed to encode hash: %@
Invalid NeuralHash+LSH (=)
%@: %c%c%c%c-%c%c%c%c-%c%c%c%c-%c%c%c%c
VCPObjectPool failed to allocate object
[NSFW] Failed to find label for identifier %d
ImageHandAnalyzer: input image aspectRatio = %f
ImageHandAnalyzer: aspectRatio = %@, queryAspectRatioVal = %@
ImageHandAnalyzer: feasibleShapeIndex = %d
ImageHandAnalyzer: detectorHeight = %d, detectorWidth = %d
Running Home Resident Maintenance task
Canceling Home Resident Maintenance task (%d)
HomeAI request submitted (%d)
[HomeKit] Failed to connect to analysis service (%@)
[HomeKit] VCPHomeKitAnalysisSession initialization fails (%@)
[HomeKit] Client XPC connection interrupted
[HomeKit] Client XPC connection invalidated
[HomeKit] Error connecting to background analysis service
VCPHandPoseImageRequest options: _revision = %d
VCPClusterer: Terminating ...
VCPClusterer: Terminated
VCPClusterer: Cluster triggering set to %lu
VCPClusterer: Scheduling to remove %@ and add %@ faces
VCPClusterer: Removing %lu faces from cluster cache
VCPClusterer: Failed to cluster faces - %@
VCPClusterer: Removed %lu faces from cluster cache [time: %f secs]
VCPClusterer: Number of orderedFaceIdentifiers (%lu) != number of _faceIdStrsToAdd (%lu)
VCPClusterer: %lu Faces remaining, already took %f seconds
VCPClusterer: Adding %lu faces to cluster cache
VCPClusteringGetFaceObservations
VCPClusterer: Number of faceTorsoprintsToAdd (%lu) !=  number to cluster (%lu)
VCPClusteringOverall
VCPClusterer: Added %lu faces to cluster cache
VCPClusterer: Added faces to cluster cache [time: %f secs]
VCPClusteringSync
VCPClusteringCluster
VCPClusteringCompletion
VCPClusterer: Start clustering
VCPClusterer: Clustered %lu faces, with normalized %.2f millisecond per face
VCPClusterer: Finished clustering
VCPClusterer: Vision failed to cluster - %@
VCPClusterer: Failed to save cluster cache - %@
VCPClusterer: Start to update database models
VCPClusterer: Failed to persist FaceGroups; will try next time - %@
VCPClusterer: Updated database models
VCPClusterer: Cannot cluster image print type %lu
VCPClusterer: Failed to get VNFaceTorsoprint from face %@ - %@
VCPClusterer: Missing faceprint data for face %@
VCPClusterer: Failed to remove empty FaceGroup(s) - %@
VCPClusterer: Start quick-syncing cluster cache with library
VCPClusterer: Failed to clean faces with valid CSN but not in any FaceGroup - %@
VCPClusterer: Failed to clean faces with CSN = 0 but found in any FaceGroup - %@
VCPClusterer: Number of clustered faces in the cluster cache (%lu) < number of clustered faces in the library (%lu)
VCPClusterer: Quick-syncing cluster cache with library, found > 10%% (%5.2f) difference in the number of faces that are in the cluster cache versus library
VCPClusterer: Finished quick-syncing cluster cache with library. Elapsed time: %f
VCPClusterer: Start syncing cluster cache with library
VCPClusterer: Retrieve clusters from cluster cache ...
VCPClusterer: Retrieved clusters from cluster cache
VCPClusterer: Failed to retrieve clusters from cluster cache - %@
VCPClusterer: Retrieve clusters from library ...
VCPClusterer: Retrieved clusters from library
VCPClusterer: Failed to retrieve clusters from library - %@
VCPClusterer: Syncing cluster cache with library, found %lu non-singleton clusters in the cluster cache that do not match those in the library
VCPClusterer: Syncing cluster cache with library, found %lu clusters in the library cache that do not match those in the cluster cache
VCPClusterer: Syncing cluster cache with library, found > 20%% (%5.2f) difference in the number of faces are in the cluster cache versus library
VCPClusterer: Failed to ungroup faces - %@
VCPClusterer: Successfully reset cluster cache, clustererBringUpState: %@
VCPClusterer: Failed to reset cluster cache, clustererBringUpState: %@ - %@
VCPClusterer: Delete FaceGroups and reset CSN of all previously clustered faces
VCPClusterer: Canceled syncing cluster cache [cancelation point: %d do loop]
VCPClusterer: Retry deleting FaceGroups and reset CSN of all previously clustered faces. Attempt %d of %d.
VCPClusterer: Deleted FaceGroups and reset CSN of all previously clustered faces
VCPClusterer: Failed to delete face groups and reset CSN of all previously clustered faces - %@
VCPClusterer: Syncing cluster cache with library, clustererBringUpState: %@
VCPClusterer: Schedule adding %lu faces to the cluster state
VCPClusterer: Failed to get faces that are no longer present in the library
VCPClusterer: Canceled syncing cluster cache [cancelation point: %d]
VCPClusterer: Schedule removing %lu faces from the cluster state
VCPClusterer: Done syncing cluster cache with library
%@ - %@
Creating VNClustererBuilderOptions with requestRevision: %lu and torsoprintRequestRevision: %lu
VCPClusterer: Failed to instantiate VNClustererBuilder - %@
VCPClusterer: Started resetting cluster cache
VCPClusterer: Failed to remove all cluster cache files - %@
VCPClusterer: Created a new cluster cache
VCPClusterer: Failed to save a new cluster cache - %@
VCPClusterer: Failed to create a new cluster cache - %@
VCPClusterer: Done resetting cluster cache
VCPClusterer: Failed to get old vision cluster cache filenames from vision cluster state
VCPClusterer: Failed to remove cluster mmap file at '%@' - %@
VCPClusterer: Failed to restore Vision clustering state - %@
VCPClusterer: Failed to unarchive cluster cache data blob from '%@'
VCPClusterer: Resetting cluster cache files - %@
VCPClusterer: Started restoring cluster cache
VCPClusterer: Failed to restore cluster cache - %@
VCPClusterer: Restored cluster cache. Clusterer bring-up state: '%@', time to restore: %f secs
VCPClusterer: Failed to get Vision clusters - %@
VCPClusterer: Getting clusters in cluster cache ...
VCPClusterer: Failed to get clusters in cluster cache - %@
VCPClusterer: Got clusters in cluster cache
VCPClusterer: Getting clusters in library ...
VCPClusterer: Failed to get clusters in library - %@
VCPClusterer: Got clusters in library
VCPClusterer: Failed to remove cluster snapshot at '%@': %@
VCPClusterer: Failed to remove cluster mmap file at '%@': %@
VCPClusterer: Bring-up state transition: '%@' -> '%@'
VCPClusterer - _calculateChecksumMD5ForFile: error reading %zu bytes from file
CNNHandsDetector: Loading model %@
CNNHandsDetector: adopting model config: %@
CNNHandsDetectorEspresso: updating model config to %@
copyImageToBGRHandDetectorCallFromSPI
scalerHandDetectorCallFromSPI
inferenceHandDetectorCallFromSPI
CNNHandsDetector: hand class index: %d
Restore clusterer error (ClusterState = %ld): %@
Restored clusterer, ClusterState = %ld
UpdateKeyFaces for: '%@'
could not update key faces for suggestions: %@
Loaded clustererState: %ld
Returning no suggestions because the clusterer is working
suggestions first phase query start
suggestions first phase query end
suggestions middle phase query start (includes face groups for person query)
suggestions middle phase query end
suggestions last phase query start
suggestions last phase query end
Getting suggestions for person: '%@', numberOfToBeConfirmedPersonSuggestions: %lu, numberOfToBeRejectedPersonSuggestions: %lu
Got %lu suggestions for person: '%@', numberOfToBeConfirmedPersonSuggestions: %lu, numberOfToBeConfirmedPersonSuggestions: %lu
Input parameter is empty or nil: '%@'
Persons Model: Failed to remove model at %@ - %@
Pets Model: Failed to remove model at %@ - %@
Person Processing: Starting Deleting Persons
VCPFaceProcessingDeleteAllVerifiedPersons
Person Processing: Deleting Persons %@
Person Processing: Starting Face Reclustering
VCPFaceProcessingReclusterFacesWithThreshold
Person Processing: Face Clustering %@
Person Processing: Starting Person Building
VCPFaceProcessingBuildPersons
Person Processing: Person Building %@
Person Processing: Starting Person Promotion
VCPFaceProcessingPromotePersons
Person Processing: Person Promotion %@
Attempt to download resource: %@
VCPMADServiceImageProcessingTask_Decode
Request canceled
%@ returned unexpected status (%d)
VCPMADServiceImageProcessingTask_Run
Defaults write set %s cluster torso-only faces
Choosing asset resource from preferred list: %@
Network is available, filtering list to remove the CPL Thumb, new list is: %@
No resources locally available, returning a downloadable hi-res resource: %@
[Face] No proper model revision for %@ with umbrellaVersion: %d
[Face] Failed setting %@ private revision: %@, umbrellaVersion: %d
Error resetting all FaceGroups Person Builder state: %@
Failed to clean up merge candidates. Error: %@
VCPFaceProcessingCleanupMergeCandidates
->->-> Enabling personBuilderMergeCandidates
Warning: Could not update the key faces of some merge candidates %@
Unsupported PanoVNRequestMethod (%lu); using default (URL)
Supported options: 0 - URL, 1 - FullBuffer, 2 - ScaledBuffer
Sceneprint task failed (%@)
Did cluster: %s
Reset restore clusterer error (ClusterState = %ld): %@
Reset restored clusterer, ClusterState = %ld
Person Processing: Starting Reset Face Clustering
VCPFaceProcessingResetFaceClusteringState
Person Processing: Reset Face Clustering Done
Person Processing: Starting Face Clustering
VCPFaceProcessingPerformFaceClusteringAndWait
Person Processing: Face Clustering Done
VCPFaceProcessingClusterFacesWithExtendTimeoutBlock
---> Start face cluster (%ld) with clustering status: %@
---> Finished face cluster (%ld) with clustering status: %@
---> Canceling face cluster
VCPFaceProcessingClusterFaces
VCPFaceProcessingClusterFacesIfNecessary
Real-time analysis client XPC connection interrupted
Real-time analysis client XPC connection invalidated
Pixel buffer not IOSurface-backed; dropping analysis request
Real-time analysis client XPC connection error
VCPSceneTaxonomy - Failed to load PVSceneTaxonomy
VCPSceneTaxonomy - cannot find scene name for id %d
VCPSceneTaxonomy - cannot find scene id for scene name %@
[MRC] Gating %s via default
VCPMADVIMachineReadableCodeDetectionTask running...
VCPMADVIMachineReadableCodeDetectionTask image loading failed
Failed to configure VNDetectBarcodesRequest
VCPMADVIMachineReadableCodeDetectionTask complete
VCPLandmarkValidator failed to validate image (%d)
Failed to align face bbox: aligner returned an empty rectange
Failed to align face bbox for faces in image, error: %@
Cannot merge new face (type:%lu) with %@ (type:%lu): %s print, candidate v%lu, context v%u
Cannot merge with %@: faceprintDistance (%f) < faceprintThreshold (%f)
Cannot merge with %@: could not get distance between queryFace %@ and candidateFace %@ - %@
Existing Face: %@
Detected Face: %@
Merge pair score: %f
 -> Matched Detected Face: %@
 -> Matched Existing Face: %@
Cannot Merge in final stage: [mutableDetectedFaces containsObject:detectedFace] %@ [facesToDelete containsObject:matchedExistingFace] %@ 
 -> Negative Matched Detected Face: %@
 -> Negative Matched Existing Face: %@
%lu Face(s) merged based on imageprints: %@
%lu Face(s) merged based on geometries (before): %@
%lu Face(s) merged based on geometries (filtered): %@
inferenceHandKeypointCallFromSPI
time=%.2f sharpness=%.2f, faceSharpness=%.2f, cameraM=%.2f, subjectM=%.2f, junk=%.2f, obstr=%.2f, exposure=%.2f, score=%.2f
VCPVideoKeyFrameBlurAnalyzer
VCPVideoKeyFrameFaceQualityAnalyzer
[MediaAnalysis] [VCPVideoMetaAnalyzer] Unknown analysis type %@
  Extreme aspect ratio %f; initialization failed
[MediaAnalysis] Sample at %lld/%d is being extended %0.1fx
softlink:r:path:/System/Library/PrivateFrameworks/PhotoVision.framework/PhotoVision
softlink:r:path:/System/Library/PrivateFrameworks/PhotosGraph.framework/PhotosGraph
softlink:r:path:/System/Library/PrivateFrameworks/PhotoVision.framework/PhotoVision
softlink:r:path:/System/Library/PrivateFrameworks/PhotosGraph.framework/PhotosGraph
softlink:r:path:/System/Library/PrivateFrameworks/PhotoVision.framework/PhotoVision
softlink:r:path:/System/Library/PrivateFrameworks/PhotosGraph.framework/PhotosGraph
softlink:r:path:/System/Library/PrivateFrameworks/PhotoVision.framework/PhotoVision
softlink:r:path:/System/Library/PrivateFrameworks/PhotosGraph.framework/PhotosGraph
softlink:r:path:/System/Library/PrivateFrameworks/PhotoVision.framework/PhotoVision
softlink:r:path:/System/Library/PrivateFrameworks/PhotosGraph.framework/PhotosGraph
softlink:r:path:/System/Library/PrivateFrameworks/PhotoVision.framework/PhotoVision
softlink:r:path:/System/Library/PrivateFrameworks/PhotosGraph.framework/PhotosGraph
softlink:r:path:/System/Library/PrivateFrameworks/PhotoVision.framework/PhotoVision
softlink:r:path:/System/Library/PrivateFrameworks/PhotosGraph.framework/PhotosGraph
softlink:r:path:/System/Library/PrivateFrameworks/PhotoVision.framework/PhotoVision
softlink:r:path:/System/Library/PrivateFrameworks/PhotosGraph.framework/PhotosGraph
softlink:r:path:/System/Library/PrivateFrameworks/PhotoVision.framework/PhotoVision
softlink:r:path:/System/Library/PrivateFrameworks/PhotosGraph.framework/PhotosGraph
softlink:r:path:/System/Library/PrivateFrameworks/PhotoVision.framework/PhotoVision
softlink:r:path:/System/Library/PrivateFrameworks/PhotosGraph.framework/PhotosGraph
softlink:r:path:/System/Library/PrivateFrameworks/PhotoVision.framework/PhotoVision
softlink:r:path:/System/Library/PrivateFrameworks/PhotosGraph.framework/PhotosGraph
softlink:r:path:/System/Library/PrivateFrameworks/PhotoVision.framework/PhotoVision
softlink:r:path:/System/Library/PrivateFrameworks/PhotosGraph.framework/PhotosGraph
softlink:r:path:/System/Library/PrivateFrameworks/PhotoVision.framework/PhotoVision
softlink:r:path:/System/Library/PrivateFrameworks/PhotosGraph.framework/PhotosGraph
softlink:r:path:/System/Library/PrivateFrameworks/PhotoVision.framework/PhotoVision
softlink:r:path:/System/Library/PrivateFrameworks/PhotosGraph.framework/PhotosGraph
softlink:r:path:/System/Library/PrivateFrameworks/PhotoVision.framework/PhotoVision
softlink:r:path:/System/Library/PrivateFrameworks/PhotosGraph.framework/PhotosGraph
softlink:r:path:/System/Library/PrivateFrameworks/HomeAI.framework/HomeAI
softlink:r:path:/System/Library/PrivateFrameworks/PhotoVision.framework/PhotoVision
softlink:r:path:/System/Library/PrivateFrameworks/PhotosGraph.framework/PhotosGraph
softlink:r:path:/System/Library/Frameworks/ShazamKit.framework/ShazamKit
softlink:r:path:/System/Library/PrivateFrameworks/HomeAI.framework/HomeAI
softlink:r:path:/System/Library/PrivateFrameworks/HomeAI.framework/HomeAI
softlink:r:path:/System/Library/PrivateFrameworks/PhotoVision.framework/PhotoVision
softlink:r:path:/System/Library/PrivateFrameworks/PhotosGraph.framework/PhotosGraph
softlink:r:path:/System/Library/PrivateFrameworks/PhotoVision.framework/PhotoVision
softlink:r:path:/System/Library/PrivateFrameworks/PhotosGraph.framework/PhotosGraph
softlink:r:path:/System/Library/PrivateFrameworks/PhotoVision.framework/PhotoVision
softlink:r:path:/System/Library/PrivateFrameworks/PhotosGraph.framework/PhotosGraph
softlink:r:path:/System/Library/PrivateFrameworks/PhotoVision.framework/PhotoVision
softlink:r:path:/System/Library/PrivateFrameworks/PhotosGraph.framework/PhotosGraph
softlink:r:path:/System/Library/PrivateFrameworks/PhotoVision.framework/PhotoVision
softlink:r:path:/System/Library/PrivateFrameworks/PhotosGraph.framework/PhotosGraph
softlink:r:path:/System/Library/PrivateFrameworks/PhotoVision.framework/PhotoVision
softlink:r:path:/System/Library/PrivateFrameworks/PhotosGraph.framework/PhotosGraph
softlink:r:path:/System/Library/PrivateFrameworks/PhotoVision.framework/PhotoVision
softlink:r:path:/System/Library/PrivateFrameworks/PhotosGraph.framework/PhotosGraph
d8F-
MonzaV4_1Input
MLFeatureProvider
MonzaV4_1Output
MonzaV4_1
VCPAudioAnalyzer
VCPSoundDetector
SNResultsObserving
NSObject
VCPAudioClassifier
VCPVideoStabilizer
VCPImageHumanPoseAnalyzer
CMTimerange
VCPSlowmo
VCPProtoLivePhotoKeyFrameResult
NSCopying
VCPBlurAnalyzer
VCPVideoCNNAnalyzer
VCPBoundingBox
LegacyConversion
VCPProtoResultLegacyConversionProtocol
VCPProtoMovieBabbleResult
VCPCNNBlock
VCPCNNBlurAnalyzer
VCPCNNBlurAnalyzerEspresso
VCPCNNBlurAnalyzerMPS
VCPCNNConvBlock
VCPCNNConvBlockGPU
VCPCNNConvBlockScalar
VCPCNNConvBlockVector
VCPCNNData
VCPCNNDataGPU
VCPFaceCrop
VCPCNNEspressoContext
VCPCNNFaceLandmarkDetector
VCPCNNFaceLandmarkDetectorEspresso
VCPCNNFaceLandmarkDetectorMPS
VCPCNNFlattenBlock
VCPCNNFullConnectionBlock
VCPCNNFullConnectionBlockGPU
VCPCNNFullConnectionBlockScalar
VCPCNNGazeAnalysis
VCPVideoGyroStabilizer
VCPCNNHandKeypointsDetector
VCPCNNMetalContext
VCPMADVIVisualSearchTask
VCPHumanPoseVideoRequest
VCPCNNModel
VCPCNNModelEspresso
VCPBackwarp
VCPCorrelation
VCPEspressoModel
VCPFlowDecoder
VCPFlowFeatureExtractor
VCPModelR2D2
VCPProtoKeypoint
VCPCNNPetsDetector
VCPProtoMovieApplauseResult
VCPCNNPetsDetectorEspresso
VCPCNNPoolingBlock
VCPCNNPoolingBlockGPU
VCPCNNPoolingBlockScalar
VCPMADVIDocumentRecognitionResource
VCPMADVIDocumentRecognitionTask
VCPVideoProcessor
PVPersonProtocol
VCPCNNPoolingBlockVector
VCPCNNPoseEstimator
VCPVideoProcessorSession
VCPCNNPoseEstimatorEspresso
VCPProtoLivePhotoKeyFrameFaceResult
VCPCNNPoseEstimatorMPS
VCPPhotosQuickFaceDetectionManager
VCPCNNSmileDetector
VCPCNNSmileDetectorEspresso
VCPCNNSmileDetectorMPS
VCPDatabaseBatchIterator
VCPDatabaseReader
VCPMADResourceLock
VCPMADResourceEntry
VCPMADResourceManager
VCPVideoCNNBackbone
VCPHuman
VCPDeviceInformation
VCPProtoMovieStabilizationResult
VCPMADVIVisualSearchResource
VCPProtoMovieHumanActionResult
VCPEdgeDetector
VCPEffectsAnalyzer
VCPImageExposurePreAnalyzer
VCPGeometryUtils
VCPTimer
VCPExifAnalyzer
VCPFace
VCPFaceDetectionRange
VCPTimeMeasurement
VCPHomeFaceIdentificationTask
PVPhotoLibraryProtocol
VCPFingerprint
VCPFaceAnalyzer
VCPFrameScoreFilter
FullAnalysis
VCPProtoLivePhotoKeyFrameStillResult
VCPProtoImageSceneprintResult
VCPFullVideoAnalyzer
VCPGaborFilter
VCPHoughTransform
VCPCNNPersonKeypointsDetector
VCPImageAnalyzer
VCPVNImageprintWrapper
VCPVideoCNNActionClassifier
VCPImageBlurAnalyzer
VCPMABaseTask
VCPMADTaskProtocol
VCPImageCompositionAnalyzer
VCPImageConverter
VCPMADEmbeddingGenerationTask
VCPMADServiceImageProcessingSubtaskProtocol
VCPImageDescriptor
VCPDistanceDescriptorProtocol
VCPImageExposureAnalyzer
VCPVideoCNNTask
VCPAnalysisProgressQuery
VCPImageFaceDetector
VCPImageFaceExpressionAnalyzer
VCPVideoStabilizationAssetProcessingTask
VCPImageFaceQualityAnalyzer
VCPMADCoreAnalyticsManager
VCPImageLivePhotoBlurAnalyzer
VCPFaceIDModel
VCPFaceProcessingVersionManager
VCPKeypoint
VCPPersonObservation
VCPHandObservation
VCPMotionFlowObservation
VCPMAMLFeatureProvider
VCPImageManager
VCPProtoMovieHumanPoseResult
PFPhotosFaceRepresentation
VCPPhotosPersistenceDelegateAdditions
MediaAnalysis
VCPMergeCandidatePair
VCPPhotosPersistenceDelegate
PVPersonPromoterDelegate
VCPCNNPersonDetector
VCPImagePetsAnalyzer
VCPImageQualityAnalyzer
VCPImageSaliencyAnalyzer
VCPImageSaliencyAnalyzerFull
VCPImageSaliencyAnalyzerFullEspresso
VCPSceneProcessingImageManager
VCPPreAnalysisImageEntry
VCPPreAnalysisImage
VCPPreAnalysisRequests
VCPPreAnalyzer
VCPInterAssetAnalyzer
VCPJunkAnalyzer
VCPMADVIResource
VCPLandmarkValidator
VCPLightMotionAnalyzer
VCPLightVideoAnalyzer
VCPMAMLModel
VCPClusteringAccuracyMeasures
VCPPhotosAutoCounterWorker
VCPLogManager
VCPVideoPersonDetector
VCPMovieCurationAnalyzer
VCPVideoKeyFrameResult
VCPMovieHighlightResult
VCPMovieCurationResults
VCPMovieHighlight
VCPExpressionSegment
VCPMovieHighlightAnalyzer
VCPMediaAnalysisServerProtocol
VCPMediaAnalysisClientProtocol
VCPMediaAnalysisService
FaceSuggestions
PersonBuilderAndPromoter
InternalTools
Hubble
VCPStorageServiceProtocol
VCPMediaAnalyzer
VCPMADVIVisualSearchGatingTask
VCPMetaSegment
VCPMAEmbeddingEntry
VCPMetaTrackDecoder
PVFaceProtocol
VCPMovieAnalyzer
VCPProtoImageHumanPoseResult
PVFetchResultProtocol
NSFastEnumeration
VCPMotionFlowAnalyzer
PHAssetResource
VCPMediaAnalysis
Exif
MediaAnalysisResults
MediaAnalysisPauseResume
VCPLivePhotoKeyFrameAnalyzer
VCPMADVisionResource
VCPPHFaces
VCPPhotosQuickFaceIdentificationManager
MediaAnalysisPhoto
MediaAnalysisMovie
MediaAnalysisSceneProcessing
MediaAnalysisOCRProcessing
MovieResource
VCPPhotoAnalyzer
VCPPnPSolver
VCPSceneprintDescriptor
VCPSceneChangeAnalyzer
VCPSceneChangeSegment
VCPVideoPixelStabilizer
VCPPhotosFace
VCPFaceShapeModel
VCPMADResource
VCPMotionFlowSubtleMotionAnalyzer
VCPVideoProcessorNode
VCPFaceTensorModel
VCPHumanPoseEspressoSession
VCPVanishingPointDetector
VCPActionAnalyzer
CoreDataProperties
VCPAsset
Image
LivePhoto
Movie
VCPFaceGeometry
NSSecureCoding
NSCoding
VCPFaceAnchor
VCPCaptureAnalysisSession
VCPClientDatabaseManager
VCPContentAnalysis
VCPDefaultPhotoLibraryManager
PHPhotoLibraryAvailabilityObserver
VCPDownloadManager
VCPFullAnalysisURLProcessingTask
MercuryBase64
VCPProcessingStatusEntry
VCPCNNFastGestureRecognition
VCPProtoMovieSceneprintResult
VCPMADServiceImagePixelBufferAsset
VCPMADServiceImageURLAsset
VCPMADServiceImagePhotosAsset
VCPMADServiceImageAsset
VCPProtoMoviePetsResult
VCPVideoCNNAutoplay
VCPMotionFlowRequest
VCPProtoLivePhotoHumanActionClassificationResult
VCPFrameAnalysisStats
VCPHomeKitAnalysisServerProtocol
VCPHomeKitAnalysisClientProtocol
VCPHomeKitAnalysisService
Client
Resident
VCPColorNormalizationAnalyzer
VCPInMemoryAVAsset
AVAssetResourceLoaderDelegate
VCPInternetReachability
VCPMADImageSafetyClassificationResource
VCPMADImageSafetyClassificationTask
VCPGeneralCanceller
VCPFaceCropSourceDescriptor
VCPFaceCropGenerator
VCPFaceCropManager
VCPPhotosAsset
VCPPriorityAnalysis
VCPProtoAssetAnalysis
VCPProtoBounds
CGRect
VCPImageMotionFlowAnalyzer
VCPSongDetector
VCPProtoClassification
VCPProtoImageBlurResult
VCPProtoImageCompositionResult
VCPProtoImageExposureResult
VCPLoudnessAnalyzer
VCPProtoImageFaceResult
VCPProtoImageFeatureResult
VCPProtoImageJunkResult
VCPProtoImagePetsFaceResult
VCPProtoMovieLaughterResult
VCPProtoImagePetsResult
VCPHandPoseVideoRequest
VCPMAEmbeddingAnalyzer
VCPLoaned
VCPObjectPool
VCPProtoImageSaliencyResult
VisualSearch
VCPProtoImageShotTypeResult
VCPProtoLine
VCPProtoMovieCheeringResult
CGPoint
VCPProtoLivePhotoEffectsRecipe
VCPProtoLivePhotoEffectsResult
VCPHumanPoseImageRequest
VCPProtoLivePhotoFrameInstruction
VCPProtoLivePhotoRecommendationResult
VCPProtoLivePhotoSharpnessResult
VCPImageHandsAnalyzer
VCPHomeResidentMaintenanceTask
VCPProtoLivePhotoVariationParams
VCPProtoMovieActivityLevelResult
VCPProtoMovieCameraMotionResult
VCPHomeKitAnalysisSessionServerProtocol
VCPHomeKitAnalysisSessionClientProtocol
VCPHomeKitAnalysisSession
VCPHomeKitSessionExportedObject
VCPHomeKitMotionAnalyzer
VCPProtoMovieClassificationResult
VCPHandPoseImageRequest
VCPProtoMovieHighlightScoreResult
VCPProtoMovieFaceprintResult
VCPRequest
VCPProtoMovieFaceResult
VCPProtoMovieFeatureResult
PVAssetProtocol
VCPProtoMovieFineSubjectMotionResult
VCPProtoMovieHighlightResult
VCPSuggestionRequest
VCPClusterer
PVFaceClusteringProtocol
VCPProtoMovieInterestingnessResult
PVMomentProtocol
VCPImageHumanPoseAnalyzerTopDown
VCPProtoMovieLoudnessResult
VCPProtoMovieMovingObjectResult
VCPProtoMovieMusicResult
VCPProtoMovieObstructionResult
VCPCNNHandsDetector
VCPFaceProcessingServiceWorker
VCPMADServiceImageProcessingTask
VCPPhotosFaceProcessingContext
VCPFaceUtils
VCPPersonBuilder
VCPVideoHumanActionAnalyzer
VCPPhotosSceneprintAssetProcessingTask
VCPProtoMovieOrientationResult
VCPProtoMoviePreEncodeResult
VCPProtoMovieQualityResult
VCPProtoMovieSaliencyResult
VCPProtoMovieSceneResult
VCPProtoMovieSubjectMotionResult
VCPFaceClusterer
PVFaceGroupProtocol
VCPProtoMovieSummaryResult
VCPFaceCropUtils
VCPProtoMovieUtteranceResult
VCPProtoMovieVoiceResult
VCPProtoPoint
VCPProtoMoviePetsFaceResult
VCPProtoTime
CMTime
VCPProtoTimeRange
CMTimeRange
VCPProtoVideoKeyFrame
VCPRealTimeAnalysisServerProtocol
VCPRealTimeAnalysisClientProtocol
VCPRealTimeAnalysisService
VCPRTLandmarkDetector
VCPProtoMovieStabilizationRecipe
VCPSceneTaxonomy
VCPSegment
VCPSharedInstanceManager
VCPTrimAnalyzer
VCPURLAsset
VCPVideoActivityAnalyzer
VCPCompactResult
VCPMADMachineReadableCodeResource
VCPMADVIMachineReadableCodeDetectionTask
VCPVideoActivityDescriptor
VCPVideoHumanActionClassifier
VCPProtoMovieSubtleMotionResult
VCPVideoAnalyzer
VCPVideoFaceDetector
VCPVideoFaceMeshAnalyzer
bRVA
VCPPhotosFacePair
VCPFaceMerger
VCPPetsRegion
VCPVideoPetsAnalyzer
VCPVideoFacePoseAnalyzer
VCPVideoFacePoseFilter
BackwardCompatability
VCPVideoFullFaceDetector
VCPCNNHandKeypointsDetectorEspresso
VCPVideoGlobalAnalyzer
VCPVideoKeyFrame
VCPVideoKeyFrameAnalyzer
VCPVideoLightFaceDetector
VCPVideoMetaAnalyzer
VCPVideoMetaFaceAnalyzer
VCPVideoMetaFocusAnalyzer
VCPVideoMetaFocusSegment
VCPVideoMetaLensSwitchAnalyzer
VCPVideoMetaLivePhotoMetaAnalyzer
VCPVideoMetaMotionAnalyzer
VCPVideoMetaMotionSegment
VCPVideMetaOrientationAnalyzer
VCPVideoObjectTracker
VCPSaliencyRegion
VCPVideoSaliencyAnalyzer
VCPClassification
VCPVideoSceneClassifier
VCPVideoTrackDecoder
VCPVideoTrackStandardDecoder
VCPVideoTrackSubsamplingDecoder
VCPVideoTrackSyncDecoder
VCPVoiceDetector
VCPVoiceDetectorV2
VCPCtrTracker
VCPBaseTracker
init
dealloc
featureValueWithCGImage:pixelsWide:pixelsHigh:pixelFormatType:options:error:
imageBufferValue
initWithInputImage:
featureValueWithImageAtURL:pixelsWide:pixelsHigh:pixelFormatType:options:error:
setWithArray:
isEqualToString:
featureValueWithPixelBuffer:
featureValueForName:
featureNames
T@"NSSet",R,N
initWithInputImageFromCGImage:error:
initWithInputImageAtURL:error:
setInputImageWithCGImage:error:
setInputImageWithURL:error:
inputImage
setInputImage:
_inputImage
T^{__CVBuffer=},N,V_inputImage
featureValueWithMultiArray:
initWithAngle:
angle
setAngle:
.cxx_destruct
_angle
T@"MLMultiArray",&,N,V_angle
bundleForClass:
pathForResource:ofType:
fileURLWithPath:
URLOfModelInThisBundle
initWithContentsOfURL:error:
initWithContentsOfURL:configuration:error:
modelWithContentsOfURL:error:
initWithMLModel:
modelWithContentsOfURL:configuration:error:
loadContentsOfURL:configuration:completionHandler:
predictionFromFeatures:options:error:
multiArrayValue
predictionFromFeatures:error:
initWithFeatureProviderArray:
predictionsFromBatch:options:error:
count
arrayWithCapacity:
featuresAtIndex:
addObject:
loadWithConfiguration:completionHandler:
initWithConfiguration:error:
predictionFromInputImage:error:
predictionsFromInputs:options:error:
model
_model
T@"MLModel",R,N,V_model
detector
initWithTypes:
dictionaryWithObjects:forKeys:count:
setupWithSample:andSampleBatchSize:
processAudioSamples:timestamp:
finalizeAnalysisAtTime:
dictionary
vcp_enabledTracksWithMediaType:
countByEnumeratingWithState:objects:count:
timeRange
trackID
initWithAsset:error:
audioFormatRequirements
assetReaderTrackOutputWithTrack:outputSettings:
addOutput:
startReading
copyNextSampleBuffer
setupWithSample:
processSampleBuffer:
status
results
addEntriesFromDictionary:
voiceDetections
initWithAnalysisTypes:forStreaming:
analyzeAsset:cancel:results:
analyzeSampleBuffer:
_inputBuffer
_audioTimestamp
_audioBufferList
_sampleBatchSize
_voiceDetector
_audioClassifier
_loudnessAnalyzer
_songDetector
_bufferedSamples
_initialized
array
objectForKey:
numberWithFloat:
confidence
addDetectionFromTime:toTime:confidence:
isEqual:
class
self
performSelector:
performSelector:withObject:
performSelector:withObject:withObject:
isProxy
isKindOfClass:
isMemberOfClass:
conformsToProtocol:
respondsToSelector:
retain
release
autorelease
retainCount
zone
hash
superclass
description
debugDescription
TQ,R
T#,R
T@"NSString",R,C
request:didProduceResult:
request:didFailWithError:
requestDidComplete:
initWithTrackStart:threshold:resultsKey:
_results
_activeStart
_activeEnd
_length
_sampleRate
_trackStart
_activeConfidence
_threshold
_minDetections
_resultsKey
initStandardFormatWithSampleRate:channels:
initWithFormat:
initWithPCMFormat:frameCapacity:
setFrameLength:
arrayWithObjects:count:
initWithSoundIdentifier:
objectForKeyedSubscript:
floatValue
addRequest:withObserver:error:
frameLength
mutableAudioBufferList
analyzeAudioBuffer:atAudioFramePosition:
_analysisTypes
_SNAnalyzer
_pcmBuffer
_framePosition
_detectors
saveStabilizationRecipe
resultFromLegacyDictionary:
correctionResultRef
setCorrectionResultRef:
cropFraction
setCropFraction:
motionBlurVector
setMotionBlurVector:
gyroStabilization
validStabilization
_gyroStabilization
_validStabilization
_cropFraction
_analysisConfidence
_analysisResultRef
_correctionResultRef
_motionBlurVector
T^v,N,V_analysisResultRef
T^v,N,V_correctionResultRef
T@"NSDictionary",&,N,V_results
Tf,N,V_cropFraction
T@"NSMutableArray",&,N,V_motionBlurVector
TB,N,V_gyroStabilization
Tf,N,V_analysisConfidence
TB,N,V_validStabilization
isEnabled
initWithSampleBuffer:
metadataItemsFromArray:filteredByIdentifier:
standardUserDefaults
persistentDomainForName:
boolValue
vcp_mediaAnalysisBundle
resourceURL
URLWithString:relativeToURL:
configForAspectRatio:
sharedModel:
numberWithBool:
initWithParameters:inputNames:outputNames:properties:
createModelWithHeight:srcWidth:
sharedManager
sharedInstanceWithIdentifier:andCreationBlock:
outputBlob
objectAtIndexedSubscript:
setObject:atIndexedSubscript:
numberWithInt:
intValue
flagsFromKeypoints:withMinConfidence:
setObject:forKeyedSubscript:
numberWithUnsignedInteger:
espressoForward:
parsePersons:width:height:
processPersons:width:height:
prepareModelWithConfig:
inputBlob
copyImage:toData:withChannels:
removeAllObjects
reInitModel
createInput:withBuffer:modelInputHeight:modelInputWidth:
generateHumanPose:
saveKeypoints
initWithKeypointsOption:aspectRatio:lightweight:forceCPU:sharedModel:flushModel:
updateModelForAspectRatio:
preferredInputFormat:height:format:
analyzePixelBuffer:flags:results:cancel:
trackingMode
setTrackingMode:
_modelEspresso
_netFileUrl
_inputData
_resConfig
_persons
_saveKeypoints
_inputWidth
_inputHeight
_heatmapNms
_forceCPU
_sharedModel
_flushModel
_trackingMode
TB,V_trackingMode
convertToOriginalTimeFromScaledTime:forExport:
vcp_convertToOriginalTimerangeFromScaledTimerange:
scaleTimeRange:toDuration:
vcp_scaleSlowmoTimeRange:withTimeMapping:inComposition:
initWithVideoAsset:videoAdjustments:
composition
insertTimeRange:ofAsset:atTime:error:
rampDown
rampUp
slowMotionRate
computeRampToTargetRate:forExport:outTimeSteps:outIntermediateRates:
slowMotionRampInRangeForExport:
slowMotionRampOutRangeForExport:
vcp_scaleRampWithIntervals:andRates:inSlowmoTimerange:withTimeMapping:inComposition:
removeTrack:
objectAtIndex:
dictionaryRepresentation
stringWithFormat:
numberWithDouble:
setObject:forKey:
initWithCapacity:
addFaceResults:
faceResultsCount
clearFaceResults
faceResultsAtIndex:
allocWithZone:
copyWithZone:
faceResultsType
setGlobalQualityScore:
setHasGlobalQualityScore:
hasGlobalQualityScore
setContentScore:
setHasContentScore:
hasContentScore
readFrom:
writeTo:
copyTo:
mergeFrom:
timestamp
setTimestamp:
qualityScoreForLivePhoto
setQualityScoreForLivePhoto:
visualPleasingScore
setVisualPleasingScore:
overallFaceQualityScore
setOverallFaceQualityScore:
exposureScore
setExposureScore:
penaltyScore
setPenaltyScore:
textureScore
setTextureScore:
sharpness
setSharpness:
faceResults
setFaceResults:
globalQualityScore
contentScore
expressionChangeScore
setExpressionChangeScore:
_timestamp
_contentScore
_exposureScore
_expressionChangeScore
_faceResults
_globalQualityScore
_overallFaceQualityScore
_penaltyScore
_qualityScoreForLivePhoto
_sharpness
_textureScore
_visualPleasingScore
_has
Td,N,V_timestamp
Tf,N,V_qualityScoreForLivePhoto
Tf,N,V_visualPleasingScore
Tf,N,V_overallFaceQualityScore
Tf,N,V_exposureScore
Tf,N,V_penaltyScore
Tf,N,V_textureScore
Tf,N,V_sharpness
T@"NSMutableArray",&,N,V_faceResults
TB,N
Tf,N,V_globalQualityScore
Tf,N,V_contentScore
Tf,N,V_expressionChangeScore
forcePersonDetection
cropAndScale:regionCrop:
copyImage:withChannels:
persons
runTasks:duration:persons:regionCrop:
initWithTimeOfInteret:frameRate:phFaces:timeRange:
_backbone
_tasks
_postTasks
_timeLastProcess
_timeLastDetection
_timeStart
_validFrames
_enoughFrames
_personDetector
_autoplay
_regionCrop
_timeEnd
_postInference
setMinX:
setMinY:
setMaxX:
setMaxY:
initWithXYAndSize:y:width:height:confidence:
intersect:
union:
area
initWithCenterAndSize:y:width:height:confidence:
computeIntersectionOverUnion:
getCGRectWithClipWidth:height:
flag
setFlag:
setClassIndex:
_minX
_maxX
_minY
_maxY
_flag
_classIndex
Tf,V_minX
Tf,V_maxX
Tf,V_minY
Tf,V_maxY
Tf,V_flag
Ti,V_classIndex
timeRangeWithCMTimeRange:
timeRangeValue
setInputSize:
outputSize
setOutputSize:
input
setInput:
setOutput:
generateOutput
setGenerateOutput:
_inputSize
_outputSize
_output
_generateOutput
_executedOnGPU
T@"NSMutableArray",W,V_inputSize
T@"NSMutableArray",&,V_outputSize
T@"VCPCNNData",W,V_input
T@"VCPCNNData",&,V_output
T@"VCPCNNMetalContext",R,V_context
TB,V_generateOutput
calculateTextureness:height:width:sdof:result:
analyzer
sdof
TB,V_sdof
objectPoolWithAllocator:
initWithRevision:
sharedModelPoolWithRevision:
getObject
object
getRevision
calculateScoreFromNetworkOutputV2:
calculateScoreFromNetworkOutput:outChannel:outHeight:outWidth:textureness:contrast:imgWidth:
copyBufferFrom:fromStride:toPtr:toStride:toWidth:toHeight:
prepareModelForSourceWidth:andSourceHeight:
getInputBuffer:srcWidth:cnnInputHeight:cnnInputWidth:
computeSharpnessScore:textureness:contrast:imgWidth:cancel:
_srcWidth
_srcHeight
dynamicForward:paramFileUrl:cancel:
_modelURL
supportGPU
supportVectorForward
convBlockClass:
initWithParameters:filterNum:chunk:reLU:padding:groups:stride:batchNorm:
cnnDataWithGPUContext:
cnnData
setSize:
allocBuffers:
size
convBlockWithFilterSize:filterNum:chunk:reLU:padding:
convBlockWithFilterSize:filterNum:chunk:reLU:padding:groups:stride:batchNorm:
constructBlock:context:
useGPU
_filterSize
_filterNum
_filter
_bias
_chunk
_reLU
_padding
_padSize
_stride
_groups
_batchNorm
isFilterSizeSupported:
readFromDisk:quantFactor:
data
straightForwardForChunkFour
chunkFourForward
forward
CalculateDotProductOfChunk
cnnDataClass
initWithGPUContext:
initWithParameters:height:width:context:
bufferAllocCPU
cnnDataWithPlane:height:width:context:
randInit
convertCPUData2GPU
convertGPUData2CPU
reallocGPUTemporalBuffers
copyImage:withChunk:
normalization
softmax
setData:
isInputOutput
setIsInputOutput:
context
setContext:
_isInputOutput
_size
_data
_context
T@"NSMutableArray",&,V_size
T^f,V_data
TB,V_isInputOutput
T@"VCPCNNMetalContext",W,V_context
copy
faceCropDimensionsFromFaceCrop:error:
stringWithString:
state
appendFormat:
localIdentifier
initWithLocalIdentifier:faceCropData:
initWithFaceCropData:originatingFace:
faceCropData
originatingFace
imageDimensions
setState:
_faceCropData
_originatingFace
_cachedImageDimensions
_state
_localIdentifier
T@"NSString",R,C,N,V_localIdentifier
Ts,N,V_state
createContextWithForceCPU:forceNNGraph:
sharedEspressoContext:forceNNGraph:
_espressoContext
T^v,R,N,V_espressoContext
_landmarks
normalization:
getInputBuffer
computeLandmarks:
_modelLandmarks
initWithParameters:
initWithParameters:NeuronType:
_weight
_numNeurons
_neuronType
readWeightsBias:weights:bias:inputDim:outputDim:quantFactor:
loadWeights:inputDim:outputDim:quantFactor:
copyImage:toData:
createInput:withBuffer:cnnInputHeight:cnnInputWidth:faceBounds:
detectEyeOpennessForFace:inBuffer:eyeOpenness:
_analysisDict
_metadata
_cropSize
init:sharedModel:modelName:
getInputBuffer:srcWidth:cnnInputHeight:cnnInputWidth:offset:
generateHandKeypoints:keypointConfidence:offset:
cvtHeatmaps2Keypoints:outHeight:outWidth:inHeight:inWidth:outChannel:keypoints:keypointConfidence:offset:
_std
_mean
initNewContext:
execute
device
setDevice:
commandQueue
setCommandQueue:
commandBuffer
setCommandBuffer:
_device
_commandQueue
_commandBuffer
T@"<MTLDevice>",&,V_device
T@"<MTLCommandQueue>",&,V_commandQueue
T@"<MTLCommandBuffer>",&,V_commandBuffer
uiScale
regionOfInterestResults
resultItems
objectKnowledge
knowledgeProperties
knowledgeGraphID
objectCategory
title
thumbnailURL
thumbnailAspectRatio
shortDescription
detailedDescription
webURL
initWithDomain:knowledgeGraphID:title:thumbnailURL:thumbnailAspectRatio:shortDescription:detailedDescription:webURL:knowledgeProperties:
normalizedBoundingBox
searchSections
initWithNormalizedBoundingBox:regionAttributes:andSearchSections:
initWithResultItems:
gatingResultItems
domains
initWithRegionOfInterest:domains:
vcp_textAnnotation
gatingPayload
queryWithPixelBuffer:orientation:imageRegions:textBlockAnnotation:queryContext:payload:
searchWithParsedVisualQuery:completion:
searchWithVisualQuery:completion:
clearCacheWithOption:
initWithOptions:
location
parseResults:observations:
associatePersons:withExisingPersons:
arrayWithArray:
personID
removeLastObject
insertObject:atIndex:
computeActionScoreForPerson:
errorWithDomain:code:userInfo:
keypoints
normDistance:point2:
computeVarWithID:index1:index2:interVar:intraVar:
setRelativeActionScore:
setAbsoluteActionScore:
setPersonID:
bodyDistance:withBodyB:
removeObject:
processSampleBuffer:withOptions:error:
preferredInputSizeWithOptions:error:
preferredPixelFormat
cleanupWithOptions:error:
_personID
_preferredWidth
_preferredHeight
_preferredFormat
_analyzer
_existingPersons
_existingPersonsArray
_blocks
_quantFactor
T@"VCPCNNData",R,V_output
initWithForceCPU:forceNNGraph:shared:
espressoContext
path
UTF8String
getPlanPhase
prepareModelInput:
prepareModelInputs:
numberWithUnsignedLong:
espressoForwardInputs:
getEspressoContext
inputBlobs
setInputBlobs:
outputBlobs
setOutputBlobs:
setInputBlob:
setOutputBlob:
resConfig
.cxx_construct
_net
_plan
_inputNames
_outputNames
_inputBlobs
_outputBlobs
_inputBlob
_outputBlob
T{vector<espresso_buffer_t, std::allocator<espresso_buffer_t>>=^{?}^{?}{__compressed_pair<espresso_buffer_t *, std::allocator<espresso_buffer_t>>=^{?}}},N,V_inputBlobs
T{vector<espresso_buffer_t, std::allocator<espresso_buffer_t>>=^{?}^{?}{__compressed_pair<espresso_buffer_t *, std::allocator<espresso_buffer_t>>=^{?}}},N,V_outputBlobs
T{?=^v^v[4Q][4Q]QQQQQQQQQQi},N,V_inputBlob
T{?=^v^v[4Q][4Q]QQQQQQQQQQi},N,V_outputBlob
T@"NSString",R,N,V_resConfig
boundsWithCGRect:
rectValue
newBufferWithLength:options:
contents
setBuffer:offset:atIndex:
_backwarpKernel
configureGPU
newDefaultLibraryWithBundle:error:
newFunctionWithName:
newComputePipelineStateWithFunction:error:
computeCommandEncoder
width
height
setComputePipelineState:
setTexture:atIndex:
dispatchThreadgroups:threadsPerThreadgroup:
endEncoding
initWithDevice:
encodeToCommandBuffer:firstInput:secondInput:correlation:
_mtlLibrary
_correlationKernel
stringByAppendingFormat:
prepareModelWithFile:engine:config:error:
freeModel
loadModel:
buildModelWithConfig:error:
initModelWithName:andConfig:
updateModelWithConfig:error:
_callbackQueue
bindWithBuffers:correlation:flow:outputFlow:
initModule:config:
estimateFlow:correlation:flow:outputFlow:callback:
_inputBlobNames
_outpuBlobName
bindWithBuffers:imgFeature:
extractFeatureFromImage:toFeature:callback:
setFeatureShape:height:width:level:
_numLevels
_inputBlobName
_featureBlobNames
_featureChannels
newTextureWithDescriptor:
newBufferWithIOSurface:
newLinearTextureWithDescriptor:offset:bytesPerRow:bytesPerImage:
getBytes:bytesPerRow:bytesPerImage:fromRegion:mipmapLevel:slice:
texture2DDescriptorWithPixelFormat:width:height:mipmapped:
setUsage:
setTextureType:
setArrayLength:
replaceRegion:mipmapLevel:slice:withBytes:bytesPerRow:bytesPerImage:
createModules
newCommandQueue
allocateFeatures
allocateStorages
extractFeatureFromImage:toFeature:
allocateCorreleationBuffer:forLevel:
estimateFlowForLevel:upperFlow:outputFlow:
encodeToCommandBuffer:input:output:flow:upscaledFlow:
commit
waitUntilCompleted
arrayLength
encodeToCommandBuffer:sourceTexture:destinationTexture:
updateModulesWithConfig:
releaseFeatureBuffers
releaseStorages
_featureExtractor
_flowDecoder
_correlation
_backwarp
_imageFeature
_storage
_flowDecoderSemaphore
_bilinearScale
Ti,R,N,V_inputHeight
Ti,R,N,V_inputWidth
Tf,N,V_x
Tf,N,V_y
postProcBoxes:maxNumRegions:
detector:
petsDetection:petsRegions:petsFaceRegions:cancel:
setTimeRange:
setConfidence:
_confidence
_timeRange
T@"VCPProtoTimeRange",&,N,V_timeRange
Tf,N,V_confidence
generatePetsRegions:outHeight:outWidth:boxes:faceBoxes:maxNumRegions:
initWithMaxNumRegions:
createModel:srcWidth:
generatePetsBoxes:faceBoxes:cancel:
initWithParameters:poolY:chunk:
poolingBlockWithPoolX:poolY:chunk:
sharedResource
maximumCandidateCount
usesLanguageDetection
documentObservations
canReuseResultsForRequest
initWithObservations:
loadPixelBuffer:orientation:
setError:
activateResource:
languages
setRecognitionLanguages:
setMaximumCandidateCount:
setUsesLanguageDetection:
session
initWithCVPixelBuffer:orientation:options:session:
executionNanoseconds
setExecutionNanoseconds:
setDocumentObservations:
reset
_cancelQueue
_weakRequest
vcp_imageOrientation
progressHandler
_analyzeWithStart:andDuration:error:
analyzeWithStart:andDuration:error:
_session
T@?,C,V_progressHandler
setVerifiedType:
setManualOrder:
keyFace
anonymizedName
favorite
setIsVerified:
pv_addMergeCandidatePersons:
personLocalIdentifiers
isVerified
manualOrder
Tq,N
T@"<PVFaceProtocol>",&,N
TB,D,N
completionHandler
validateConfiguration:withError:
request
nodeWithRequest:andConfiguration:
orientation
initWithCMSampleBuffer:orientation:options:
containsObject:
frameInterval
timeInterval
shouldProcessSampleWithTimeRange:atSamplingInterval:
performRequests:error:
processSampleBuffer:withEndTime:error:
addRequest:withConfiguration:error:
removeRequest:error:
processSampleBuffer:error:
flushWithEndTime:error:
setOrientation:
_queue
_nodes
_modified
_startTime
_nextSampleBuffer
_frameCount
_orientation
TI,N,V_orientation
computePoseScore:
setFaceBounds:
faceBounds
_faceBounds
T@"VCPProtoBounds",&,N,V_faceBounds
initWithParameters:useGPU:
getGPUContext
add:
fcBlockWithNumNeurons:NeuronType:
prepareNetworkFromURL:withInputSize:
forward:
output
_input
_persistFaces:forAsset:
computeSmileScore:
initWithDatabaseReader:forAssets:resultsTypes:batchSize:
nextBatch
subarrayWithRange:
queryAnalysesForAssets:withTypes:
iteratorForAssets:withDatabaseReader:resultTypes:batchSize:
next
asset
analysis
_reader
_assets
_resultsTypes
_batchSize
_idxLast
_idxCurrent
_batchAnalyses
_asset
_analysis
T@"PHAsset",R,N,V_asset
T@"NSDictionary",R,N,V_analysis
vcp_mediaAnalysisDatabaseFilepath
vcp_defaultMediaAnalysisDatabaseFilepath
shouldQueryInternalFields
parseHeader:startColumn:analysis:
parseResults:typeColumn:dataColumn:results:
closeDatabase
openDatabase
executeDatabaseBlock:
queryHeaderForAsset:analysis:assetId:
queryResultsForAssetId:analysis:
queryResultsForAssetId:withTypes:analysis:
queryHeadersForAssets:analyses:idMap:
queryResultsForAssets:withTypes:batchResults:
entryWithLocalIdentifier:andTaskID:andStatus:andAttempts:andNextRetryDate:
_queryValue:forKey:
queryBlacklistedLocalIdentifiers
queryAnalysisPropertiesForAsset:
queryLocalIdentifiersForTaskID:withStatus:
_sqlSerialQueue
_filepath
propertyListWithData:options:format:error:
vcp_hasFace
vcp_normalizedFaceBounds
vcp_hasBody
vcp_normalizedBodyBounds
deactivateResource:
initWithResourceManager:andResource:
_resourceManager
_resource
initWithResource:
resource
setResource:
activeCount
setActiveCount:
_activeCount
T@"VCPMADResource",&,N,V_resource
Tq,N,V_activeCount
_purgeAllResources
_setBudget:
checkTimeout
entryForResource:
_reserveBudget:
currentBudget
reserveBudget:
_budget
_timer
_lastActivation
_transaction
sharedModel:outputNames:properties:
initWithConfig:
inference:
outputLastLayer400
_outputLastLayer400
_outputBeforeFc
_outputBeforePooling
T^f,R,V_outputLastLayer400
T^f,R,V_outputBeforeFc
T^f,R,V_outputBeforePooling
flags
setFlags:
bounds
setBounds:
_flags
_bounds
TQ,V_flags
T{CGRect={CGPoint=dd}{CGSize=dd}},V_bounds
Tf,V_confidence
supportsFeatureSet:
canRenderVariation
isHomePod
marketingName
setFaceId:
hasFaceId
hasBounds
absoluteScore
setAbsoluteScore:
relativeScore
setRelativeScore:
humanScore
setHumanScore:
faceId
_absoluteScore
_faceId
_humanScore
_relativeScore
Tf,N,V_absoluteScore
Tf,N,V_relativeScore
Tf,N,V_humanScore
TB,R,N
T@"NSString",&,N,V_faceId
T@"VCPProtoBounds",&,N,V_bounds
noiseReduction:sigma:imageFiltered:
gradientEstimation:width:height:gradient:gradientMag:
isInImage:width:height:
initWithImage:edgeMap:width:height:widthExtension:heightExtension:
detectWithSigma:lowThreshold:highThreshold:
_widthPadded
_heightPadded
_width
_height
_widthExt
_heightExt
_gradient
_image
_imageFiltered
_nonMaxSuppressed
_gradientX
_gradientY
_gradientMag
_edgeMap
usePHAssetScene
initWithAnalysisResults:
_hasFaceOrPet
normalizedRectForRect:inBounds:
rectFromMappingNormalizedRect:toBounds:
normalizedRectForRect:inBoundsOfSize:
rectFromMappingNormalizedRect:toBoundsOfSize:
pointFromNormalizedPoint:inBounds:
initWithIntervalNanoseconds:isOneShot:andBlock:
timerWithInterval:unit:oneShot:andBlock:
destroy
timerWithIntervalSeconds:isOneShot:andBlock:
_source
_active
_isOneShot
transformUprightAboutTopLeft:
numberWithInteger:
addFaceResults:flags:
initWithProperties:forAnalysisTypes:
analyzeAsset:results:
_properties
_requestedAnalyses
faceBounds:height:
flagsForOrientation:width:height:
faceBoundsWithTransform:height:transform:
leftEyeClosed
setLeftEyeClosed:
rightEyeClosed
setRightEyeClosed:
smile
setSmile:
setYaw:
setTrackID:
faceQuality
setFaceQuality:
observation
setObservation:
_leftEyeClosed
_rightEyeClosed
_smile
_trackID
_faceQuality
_yaw
_observation
TB,V_leftEyeClosed
TB,V_rightEyeClosed
TB,V_smile
Tq,V_yaw
Ti,V_trackID
Tf,V_faceQuality
T@"VNFaceObservation",&,V_observation
start
setStart:
last
setLast:
position
setPosition:
faceID
setFaceID:
_position
_faceID
_start
_last
T{?=qiIq},V_start
T{?=qiIq},V_last
TQ,V_position
TQ,V_faceID
started
_elapsedTimeSeconds
Td,R,V_elapsedTimeSeconds
TB,R,V_started
initWithFaceCrop:andCompletionHandler:
configureRequest:withRevision:
initWithData:options:
setWantsIncrementalChangeDetails:
_defaultFetchOptions
_defaultAssetPropertySets
processInfo
processName
urlForApplicationDataFolderIdentifier:
URLForDirectory:inDomain:appropriateForURL:create:error:
fileExistsAtPath:isDirectory:
_phPeopleSortDescriptors
fetchAssetsForPersons:options:
fetchPersonsForAssetCollection:options:
fetchPersonsGroupedByAssetLocalIdentifierForAssets:options:
_defaultFacePropertySets
_phFaceSortDescriptors
pv_fetchFacesForPersonLocalIdentifiers:inMoment:
fetchedObjectIDs
andPredicateWithSubpredicates:
fetchFacesGroupedByAssetLocalIdentifierForAssets:options:
momentSortDescriptors
fetchMomentsWithOptions:
fetchAssetCollectionsWithLocalIdentifiers:options:
_defaultAssetFetchOptions
fetchAssetsForFaceGroups:options:
_progressFromWorkerStatesDictionary:
requestTotalProgressCountsForWorkerType:states:completion:
fetchAssetCollectionsWithType:subtype:options:
setIncludeAssetSourceTypes:
pv_performChangesAndWait:error:
pv_persistentStorageDirectoryURL
pv_fetchPersonsWithLocalIdentifiers:
pv_fetchPersonsWithType:
pv_fetchPersonsInMoment:
pv_fetchCandidatePersonsForPerson:
pv_fetchInvalidCandidatePersonsForPerson:
pv_fetchPersonsGroupedByAssetLocalIdentifierForAssets:
pv_numberOfFacesWithFaceprints
pv_fetchFacesWithLocalIdentifiers:
pv_fetchFacesForPerson:
pv_fetchFacesForPerson:inMoment:
pv_fetchFacesForFaceGroup:
pv_fetchFacesGroupedByAssetLocalIdentifierForAssets:
pv_fetchMoments
pv_fetchMomentsWithLocalIdentifiers:
pv_fetchMomentsForPerson:
pv_fetchMomentsForAssetsWithLocalIdentifiers:
pv_fetchAssetsWithLocalIdentifiers:
pv_fetchAssetsInMoment:
pv_fetchAssetsForPerson:
pv_fetchAssetsForFaceGroup:
pv_fetchFaceGroups
pv_fetchFaceGroupsForPerson:
pv_fetchInvalidAssetIdentifiersForCommonComparison
pv_lastAssetDate
pv_fetchAssetsForFaceLocalIdentifiers:
initWithMaster:adjusted:
master
adjusted
fingerprintWithMaster:adjusted:
isEqualToFingerprint:
_master
_adjusted
T@"NSString",R,V_master
T@"NSString",R,V_adjusted
initWithContext:
globalSession
releaseCachedResources
boundingBox
processingVersion
configureVNRequest:withClass:andProcessingVersion:
setBlurDeterminationMethod:
setMaximumIntermediateSideLength:
setRegionOfInterest:
warnings
addObjectsFromArray:
sourceWidth
sourceHeight
isLeftEyeClosed
isRightEyeClosed
hasSmile
blurScore
faceWithLocalIdentifier:
setSourceWidth:
setSourceHeight:
setManual:
setAlgorithmVersion:
setDetectionType:
uuid
faceprint
faceJunkinessIndex
torsoprint
initWithFaceprint:torsoprint:
serializeStateAndReturnError:
wrapperWithImageprintType:version:andData:
setImageprintWrapper:
setBodyWidth:
setBodyHeight:
setBodyCenterX:
setBodyCenterY:
faceTorsoprint
setCenterAndSizeFromNormalizedFaceRect:
roll
doubleValue
setRoll:
faceCaptureQuality
setQuality:
setPoseYaw:
pose
computeYawPitchRollFromPoseMatrix:outputYaw:outputPitch:outputRoll:
expressionsAndConfidence
setHasSmile:
faceAttributes
ageCategory
label
identifier
mad_PHValueFromVNAgeCategoryLabel:
setAgeType:
VN7exwFFmQF0AI9P7FjBljwEFu7QYUGCYE
mad_PHValueFromVNSexCategoryLabel:
setSexType:
eyesCategory
mad_PHValueFromVNEyesCategoryLabel:
setEyesState:
smilingCategory
mad_PHValueFromVNSmilingCategoryLabel:
setSmileType:
faceHairCategory
mad_PHValueFromVNFaceHairCategoryLabel:
setFacialHairType:
hairColorCategory
mad_PHValueFromVNHairColorCategoryLabel:
setHairColorType:
glassesCategory
mad_PHValueFromVNGlassesCategoryLabel:
setGlassesType:
VN4UfLbvVUqMvYV8bbGFQcxg5yRLm8ekI1
mad_PHValueFromVNExpressionCategoryLabel:
setExpressionType:
VN7fiLHgGnvqPqG63cfDUCK4Xm8obUuWoP
mad_PHValueFromVNHeadgearCategoryLabel:
setHeadgearType:
VN2riiZbQrloRhCzYW56f0rk4N3ROe151S
mad_PHValueFromVNFaceHairCategoryV2Label:
setHairType:
VNpLorzxnyAlLcPFNcKhgoNCmy9b5BRWyk
mad_PHValueFromVNPoseCategoryLabel:
setPoseType:
VN3iT1YRjjnIuELobV1olJiO1vvItN6Kdq
mad_PHValueFromVNSkintoneCategoryLabel:
setSkintoneType:
VN1uMyFtnYEWjbrdx3yAuDndKkPeyzNJhB
mad_PHValueFromVNEthnicityCategoryLabel:
setEthnicityType:
eyesState
setIsLeftEyeClosed:
setIsRightEyeClosed:
gaze
gazeMask
setHasFaceMask:
direction
mad_PHValueFromVNFaceGazeDirection:
setGazeType:
setGazeCenterX:
setGazeCenterY:
hasFaceMask
mad_VNFaceGazeDirectionDescription:
gazeType
mad_PHFaceGazeTypeDescription:
gazeCenterX
gazeCenterY
_calculateIoUBetweenObservation:andObservation:
mutableCopy
_calculateOverlappingBetweenFaceObservation:andHumanObservation:
getVCPPhotosFaceFromFaceObservation:humanObservation:sourceWidth:sourceHeight:visionRequests:algorithmVersion:force:andError:
setIsTooSmall:
setBlurScore:
_qualityMeasureForFace:countOfFacesOnAsset:
setQualityMeasure:
imageprintWrapper
_isColocatingAnimalObservation:withFaceObservations:orTorsoObservations:
labels
firstObject
animalprint
setPrecisionRecallThreshold:
_checkAnalysisRequests:forTooSmallFaceObservations:withAnalysisResults:
_createBlurRequests:andExposureRequests:forFaceObservations:
getVCPPhotosFacesFromFaceObservations:humanObservations:animalObservations:sourceWidth:sourceHeight:visionRequests:blurScorePerFace:exposureScorePerFace:tooSmallFaceObservations:algorithmVersion:
photoLibrary
librarySpecificFetchOptions
setIncludeNonvisibleFaces:
setIncludeTorsoOnlyDetectionData:
setIncludedDetectionTypes:
fetchFacesInAsset:options:
vcpPhotosFaceFromPHFace:copyPropertiesOption:
setPhotoLibrary:
predicateWithFormat:
setPredicate:
setMinimumVerifiedFaceCount:
fetchPersonsWithLocalIdentifiers:options:
removeObjectForKey:
_vcpPhotosFaceArrayFromAsset:
valueForKey:
null
predicateWithBlock:
filteredArrayUsingPredicate:
_verifiedPersonsFetchResultWithLocalIdentifiers:andPhotoLibrary:andError:
personLocalIdentifier
setPersonLocalIdentifier:
mergeExistingFaces:withDetectedFaces:forImage:
resourceForFaceProcessingWithAsset:allowStreaming:
vcp_isLocallyAvailable
privateFileURL
pixelWidth
pixelHeight
imageCreationOptions
adjustmentVersion
creationDate
imageWithURL:assetWidth:assetHeight:imageCreationOptions:adjustmentVersion:creationDate:
timeIntervalSinceReferenceDate
analyzeWithImage:andAsset:andOptions:andResults:
imageURL
initWithURL:orientation:options:session:
imageData
initWithData:orientation:options:session:
assetWidth
assetHeight
_performAnalysis:withRequestHandler:options:sourceWidth:sourceHeight:
_refineAnalysis:forAsset:andImage:
_loadPVImage:forAsset:
_detectFacesWithPVImage:forAsset:withAnalysis:
_allowANE
processAsset:withAnalysis:
_faceMerger
_processingGroup
_processingQueue
_sessionPool
_numFilterTabs
_scoreArray
_distanceVariance
_diffVariance
_numOfScores
vcp_isLivePhoto
vcp_fullAnalysisTypes
vcp_fullAnalysisTypesForAssetType:
textureness
setTextureness:
hasFlash
setHasFlash:
stillTime
setStillTime:
_stillTime
_textureness
_hasFlash
Tf,N,V_textureness
TB,N,V_hasFlash
Tf,N,V_stillTime
setSceneprintBlob:
sceneprintBlob
_sceneprintBlob
T@"NSData",&,N,V_sceneprintBlob
enableMoflow
useSceneprintInSceneAnalysis
initWithFilterTabs:distanceVariance:diffVariance:
vcp_orientation
preferredTransform
initWithFrameWidthInMb:heightInMb:
setVideoActivityDescriptor:
videoActivityDescriptor
analyzeFrame:withTimestamp:andDuration:properties:flags:
seedAnalyzersWithPixelBuffer:startTime:
analyzePixelBuffer:withFrame:withTimestamp:andDuration:
analyzePixelBuffer:withFrame:withTimestamp:andDuration:hasSubtleScene:
detectedFaces
estimateExpressionScore:encodeStats:frameWidth:frameHeight:
isStableMetaMotion:
frameExpressionScore
setFrameExpressionScore:
salientRegionsFromPixelBuffer:
reviseFrameTrackScore:saliencyRegions:
processAndEstimateQualityScore:
process:
ExtractActivityDescriptorFromStats:
setCameraMotionScore:
setSubjectActionScore:
setInterestingnessScore:
setColorfulnessScore:
setFrameProcessedByVideoAnalyzer:
setSubMbMotionAvailable:
computeExposureScoreOfFrame:
processFrameScore:validScore:
interestingnessScore
addSceneAnalysisResult:to:optional:
estimateQualityScore:
addResult:to:forKey:optional:
bound
initWithTransform:
initWithVideoTrack:withMetaOrientation:withPrivateResults:withFrameStats:isTimelapse:isIris:irisPhotoOffsetSec:irisPhotoExposureSec:slowMoRate:faceDominated:
prepareVideoAnalysisByScenes:
prepareLivePhotoAnalysisByScenes:
analyzeFrame:withTimestamp:andDuration:flags:
finishAnalysisPass:
privateResults
getSceneSwichFrequency
setNextCaptureFrame:
qualityScore
setQualityScore:
actionScore
setActionScore:
obstructionScore
setObstructionScore:
trackingScore
setTrackingScore:
objectsMotion
globalMotion
_encodeAnalysis
_preencodeAnalysis
_obstructionAnalysis
_sceneAnalysis
_motionFilter
_metadataAnalysis
_irisAnalysis
_frameBuffer
_idealHistogram
_isTimelapse
_isIris
_isSlowMo
_finalized
_hasInterestingScene
_isCaptureAnalysis
_privateResults
_videoFrameAnalysis
_trackScoreFilter
_metaMotionResults
_faceDominated
_useMoflow
_subtleMotionAnalyzer
_motionFlowAnalyzer
_sceneType
_qualityScore
_actionScore
_interestingnessScore
_obstructionScore
_trackingScore
_objectsMotion
_globalMotion
Tf,V_qualityScore
Tf,V_actionScore
Tf,V_interestingnessScore
Tf,V_obstructionScore
Tf,V_trackingScore
T@"NSDictionary",R,N,V_objectsMotion
T@"NSArray",R,N,V_globalMotion
createGaborFilterKernel:sigmaX:sigmaY:lambda:thetaInDegree:phaseInDegree:
initWithNumberOfScales:numOfOrientations:width:height:
processWithFilterScaleIdx:orientIdx:srcImage:outImage:width:height:
_filterBanks
_numScales
_numOrientations
_num
Transform
integerValue
sortUsingComparator:
initWithEdgeMap:mapWidth:mapHeight:angleStep:
DetectLinesWithThreshold:output:
_mapWidth
_mapHeight
_accumulator
_accWidth
_accHeight
_accHalfHeight
_angleStep
_verbose
minX
minY
maxX
maxY
createInput:withBuffer:cnnInputHeight:cnnInputWidth:box:
parseKeypoints:
initWithForceCPU:sharedModel:
analyzeFrame:withBox:keypoints:
_maxNumRegions
initWithImageprintType:version:andData:
isValidTorsoprint
isValidFaceprint
TQ,R,N,V_type
Ti,R,N,V_version
T@"NSData",R,N,V_data
initWithContentsOfURL:
sharedModel:inputNames:properties:
bodyWidth
bodyHeight
bodyCenterX
bodyCenterY
outputBeforePooling
stringValue
allKeys
dictionaryWithDictionary:
initWithPHFaces:
run:withPersons:andRegionCrop:atTime:andDuration:
_actions
_taxonomy
_inputsData
_phFaces
analyzerWithRevision:
setSdof:
initWithFaceResults:sdof:revision:
prepareFaceBlurModel:
scaleRegion:ofImage:toData:withWidth:andHeight:
getFaceScoreFromOutput:ratio:
computeSharpnessScore:forObjects:inImage:
computeRegionSharpness:width:height:stride:
estimateDistance:prevHomography:
analyzePixelBuffer:flags:withPreAnalysisScore:results:cancel:
computeLocalSharpness:
spatialPooling
computeCNNBasedSharpness:sharpnessScore:textureScore:contrast:cancel:
computeCNNFaceSharpness:result:cancel:
computeSharpnessScore:forFacesInImage:
computeGyroSharpness:
initWithFaceResults:sdof:
setGyroSharpnessParam:homographyResults:livePhotoStillDisplayTime:imageExposureTime:
_sharpnessBlocks
_faces
_framePTSResults
_homographyResults
_faceModel
_faceInput
_livePhotoStillDisplayTime
_imageExposureTime
_useGPU
_sdof
_contrast
_blurAnalyzer
Tf,R,V_sharpness
Tf,R,V_textureScore
run:
code
cachesResources
initWithCompletionHandler:
isCanceled
cancelBlock
setCancelBlock:
_started
_cancelBlock
_completionHandler
T@?,R,N,V_completionHandler
T@?,C,N,V_cancelBlock
initWithImage:
detect:withConfidence:dominantLine:
arrayWithObjects:
resize:height:
_pixelFormat
_rgbColorSpace
_cgContext
_rgbFrame
_yuvFrames
_rgbToYuv
initWithRequest:imageAsset:andSignpostPayload:
taskWithRequest:imageAsset:andSignpostPayload:
dependencies
resourceRequirement
autoCancellable
cancel
_request
_imageAsset
_signpostPayload
_canceled
initWithData:
initWithCVPixelBuffer:options:
setPreferBackgroundProcessing:
imageprint
initWithState:error:
distanceToImageprint:error:
usePHAssetData
descriptorWithImage:
descriptorWithData:
serialize
computeDistance:toDescriptor:
_imagePrint
computeRegionNoise:blockTextureness:average:width:height:stride:
computeNoiseLevel:width:height:stride:textureness:
Tf,R,N,V_exposureScore
queryAnalysisPropertiesForAssets:
vcp_dateModified
vcp_modificationDate
isEqualToDate:
vcp_version
faceAdjustmentVersion
vcp_needSceneProcessing
vcp_needsOCRProcessing
_countMediaAnalysisWithAssetBatch:andDatabase:
_countFaceAnalysisWithAssetBatch:
_countSceneAnalysisWithAssetBatch:
_countOCRAnalysisWithAssetBatch:
blacklistedLocalIdentifiersFromAssets:
queryFailedProcessingStatusFromAssets:forTaskID:
vcp_vipModelLastGenerationDateForVIPType:
timeIntervalSinceDate:
vcp_vipModelFilepathForVIPType:
loadVIPModelAtPath:withVIPType:error:
databaseForPhotoLibrary:
vcp_fetchOptionsForLibrary:forTaskID:
addFetchPropertySets:
fetchAssetsWithOptions:
_countAnalysisWithAssetBatch:andDatabase:andTaskID:
_countFailuresWithAssetBatch:andDatabase:andTaskID:
_vipStatusForPhotoLibrary:andType:
vcp_assetCountForTaskID:
_processedPredicateForTaskID:
vcp_assetCountWithInternalPredicate:forTaskID:
countForTaskID:withProcessingStatus:
_screenProgress
_queryProgressDetailExpress:forPhotoLibrary:andTaskID:
_scanPhotoLibrary:withTaskID:andStatistics:
queryProgressDetail:forPhotoLibrary:andTaskID:
unsignedIntegerValue
photoLibraryURL
queryProgress:forPhotoLibrary:andTaskID:
queryCachedFaceAnalysisProgress:forPhotoLibrary:
reportProgressForPhotoLibrary:andTaskID:
setRevision:
setInputFaceObservations:
estimator
detectSmileForFace:inBuffer:smile:
detectPoseForFace:inBuffer:yaw:
faceDetection:faces:cancel:
isDuplicate:withRect:
removeObjectsInArray:
analyzePixelBufferInTiles:results:cancel:
faceDetector
processTile:results:cancel:
aggregateTileResults:tileRect:imageSize:landscape:results:
initWithFaceResults:
initWithAssets:andOptions:andCompletionHandler:
exportToLegacyDictionary
queryAnalysisForAsset:
vcp_results
initWithPHAsset:withExistingAnalysis:forAnalysisTypes:
analyzeAsset:streamed:
deserializeStabilizationRecipeInAttributes:
main
taskWithAssets:andOptions:andCompletionHandler:
_photoLibrary
_database
_cancel
_stabilizationType
_onDemandPixel
_onDemandGyro
analyzeDetectedFaces:faceResults:cancel:
faceQualityScores
setFaceQualityScores:
_faceQualityScores
T@"NSMutableArray",&,V_faceQualityScores
flush
setValue:forKey:
longLongValue
numberWithLongLong:
enumerateKeysAndObjectsUsingBlock:
sendEvent:withAnalytics:
setValue:forField:andEvent:
accumulateInt64Value:forField:andEvent:
accumulateDoubleValue:forField:andEvent:
sendSessionEvent:
_managementQueue
_singleAnalyticsSentCount
_sessionAnalyticsSentCount
_sessionAnalytics
_movingObjects
setMaximumIdentities:
initWithConfiguration:
predictPersonFromFaceObservation:limit:canceller:error:
predictedPersonUniqueIdentifier
entityPredictionsForObservation:limit:canceller:error:
petClassificationThreashold
entityUniqueIdentifier
modelFromURL:options:error:
_loadModelAtPath:error:
_loadPetsModelAtPath:error:
setReadOnly:
writeToURL:options:error:
addFaceObservations:toPersonWithUniqueIdentifier:error:
personVIPModelFileName
petVIPModelFileName
animalObservationFromAnimalprintData:
_versionStateURL
absoluteString
initWithPhotoLibrary:
vcp_visionCacheStorageDirectoryURL
absoluteURL
URLByAppendingPathComponent:
writeToURL:error:
resetFaceAnalysisWithResetLevel:completionHandler:
currentProcessingVersion
_updateCurrentProcessingVersion:
resetAnalysisDataWithResetLevel:error:
sharedManagerForPhotoLibrary:
resetLevelDescription:
_updateVersionStateFileWithError:
defaultProcessingVersion
migrateFaceProcessingToVersion:
_versionState
setLocation:
_location
T{CGPoint=dd},N,V_location
setKeypoints:
relativeActionScore
absoluteActionScore
revision
_relativeActionScore
_absoluteActionScore
_revision
_keypoints
T@"NSArray",&,N,V_keypoints
Tf,N,V_relativeActionScore
Tf,N,V_absoluteActionScore
Ti,N,V_personID
Ti,N,V_revision
chirality
setChirality:
handID
setHandID:
_chirality
_handID
Ti,N,V_chirality
Ti,N,V_handID
pixelBuffer
setPixelBuffer:
_pixelBuffer
T^{__CVBuffer=},N,V_pixelBuffer
initWithCVPixelBuffer:andFeatureName:
setWithObject:
featureProviderWithCVPixelBuffer:andFeatureName:
_featureName
_buffer
conformsToType:
setNetworkAccessAllowed:
appendData:
defaultManager
requestDataForAssetResource:options:dataReceivedHandler:completionHandler:
dataWithContentsOfURL:
createPixelBufferWithWidth:height:pixelFormat:pixelBuffer:
longValue
convertPixelBuffer:toPixelFormat:
loggingEnabled
drawImage:pixelFormat:withOrientation:maxDimension:pixelBuffer:
canDecodeAcceleratedUniformTypeIdentifier:
acceleratedDecodeImageData:pixelFormat:maxDimension:pixelBuffer:orientation:flushCache:
decodeImageSource:withUniformTypeIdentifier:pixelFormat:maxDimension:orientation:pixelBuffer:
dataForResource:
vcp_uniformTypeIdentifier
pixelBufferWithFormat:andMaxDimension:fromData:withUniformTypeIdentifier:flushCache:orientation:
pixelBufferWithFormat:fromImageURL:flushCache:orientation:
getResourceValue:forKey:error:
dataWithContentsOfURL:options:error:
pixelBufferWithFormat:andMaxDimension:fromImageURL:orientation:
sharedImageManager
imageForResource:pixelFormat:
imageForResource:pixelFormat:maxDimension:
pixelBufferWithFormat:fromImageURL:flushCache:
pixelBufferWithFormat:andMaxDimension:fromImageURL:
flushCache
_decodeSession
_transferSession
_decodeQueue
unsignedIntValue
qualityMeasure
clusterSequenceNumber
quality
photosFaceRepresentationSourceWidth
photosFaceRepresentationSourceHeight
photosFaceRepresentationCenterX
photosFaceRepresentationCenterY
photosFaceRepresentationSize
photosFaceRepresentationBlurScore
photosFaceRepresentationHasSmile
photosFaceRepresentationIsLeftEyeClosed
photosFaceRepresentationIsRightEyeClosed
photosFaceRepresentationQualityMeasure
photosFaceRepresentationClusterSequenceNumber
photosFaceRepresentationLocalIdentifier
photosFaceRepresentationRoll
photosFaceRepresentationQuality
indexSetWithIndexesInRange:
objectsAtIndexes:
persistenceDelegate_enumerateInChunksOfSize:withOverageAllowance:usingBlock:
setWithCapacity:
resultsAsArray
resultsAsSet
initWithPerson:andPerson:reason:
person1LocalIdentifier
person2LocalIdentifier
mergeCandidatePairWithPerson:andPerson:reason:
reason
_hash
_person1LocalIdentifier
_person2LocalIdentifier
_reason
T@"NSString",R,V_person1LocalIdentifier
T@"NSString",R,V_person2LocalIdentifier
T@"NSString",R,V_reason
_includeTorsoOnlyFaces
newAllFacesFetchOptionsWithPhotoLibrary:
setShouldPrefetchCount:
setIncludeOnlyFacesWithFaceprints:
setInternalPredicate:
mad_countOfUnclusteredFaces
mad_unclusteredFacesFetchOptions
newFacesDeterministicSortDescriptors
setInternalSortDescriptors:
fetchFacesWithLocalIdentifiers:options:
fetchedObjects
fetchAssetsGroupedByFaceUUIDForFaces:
fetchMomentUUIDByAssetUUIDForAssets:options:
uuidFromLocalIdentifier:
nonGroupedGroupID
initWithUUIDString:
faceClusterSequenceNumbersOfFacesWithClusterSequenceNumbers:error:
minusSet:
fetchFaceGroupsGroupedByFaceLocalIdentifierForFaces:options:
enumerateFetchResult:withBatchSize:handler:
unionSet:
_ungroupFaceClusterSequenceNumbers:cancelOrExtendTimeoutBlock:error:
strongToStrongObjectsMapTable
_categorizeGroupedFacesInFetchResult:intoFaceLocalIdentifiersByFaceGroup:ungroupedFaceLocalIdentifiers:cancelOrExtendTimeoutBlock:photoLibrary:
_resetFaceClusterSequenceNumberOfFacesInFetchResult:inPhotoLibrary:cancelOrExtendTimeoutBlock:error:
domain
keyEnumerator
changeRequestForFace:
setClusterSequenceNumber:
changeRequestForFaceGroup:
removeFaces:
performCancellableChangesAndWait:error:
setIncludeOnlyFacesInFaceGroups:
_fetchResultForUngroupedFacesWithNonZeroClusterSequenceNumberInPhotoLibrary:
_fetchResultForGroupedFacesWithClusterSequenceNumberSetToZeroInPhotoLibrary:
fetchFacesForPerson:options:
newAllPersonsWithAtLeastOneFaceFetchOptionsWithPhotoLibrary:
fetchAssociatedPersonsGroupedByFaceGroupLocalIdentifierForFaceGroups:options:
objectEnumerator
dictionaryWithCapacity:
creationRequestForFaceGroup
placeholderForCreatedFaceGroup
fetchKeyFaceForFaceGroup:options:
setPersonBuilderState:
addFaces:
setKeyFace:
removeObjectsForKeys:
deleteEmptyGroupsAndReturnError:
_localIdentifiersOfUnverifiedPersonsAssociatedWithFaceGroups:cancelOrExtendTimeoutBlock:
updateKeyFacesOfPersonsWithLocalIdentifiers:forceUpdate:cancelOrExtendTimeoutBlock:error:
deleteFaceGroups:
cleanupUngroupedFacesWithNonZeroClusterSequenceNumbers:error:
fetchEmptyFaceGroupsWithOptions:
performChangesAndWait:error:
localizedDescription
newVisibleFacesFetchOptionsWithPhotoLibrary:
fetchKeyFaceForPerson:options:
bestRepresentativeFaceForPerson:qualityMeasureByFace:cancelOrExtendTimeoutBlock:
changeRequestForPerson:
setKeyFace:forCluster:
bestRepresentativeFaceForPerson:qualityMeasureByFace:candidateFaces:cancelOrExtendTimeoutBlock:
_facesFromFaceGroupWithMostNumberOfFacesOnPerson:options:error:
_faceToFaceCountMapForFaces:
qualityMeasureForFace:countOfFacesOnAsset:
_representativenessByFaceCSNFromFaces:cancelOrExtendTimeoutBlock:
selectRepresentativeFromFaces:qualityMeasureByLocalIdentifier:representativenessByCSN:candidateFaces:
detectionType
generateVNImageprintWithType:archiveData:andError:
setFaceTorsoprint:
setFaceprint:
representativenessForFaces:error:
newAssetFetchOptionsWithPhotoLibrary:
setFetchLimit:
intersectSet:
removeMergeCandidatePersons:
_cleanupMergeCandidatesForVerifiedPersons:minimumFaceGroupSize:cancelOrExtendTimeoutBlock:error:
minimumVerifiedFaceCount
minimumUnverifiedFaceCount
faceCount
predicate
evaluateWithObject:
newAllPersonsFetchOptionsWithPhotoLibrary:
_enumeratePersonsWithLocalIdentifiers:fetchOptions:personCache:usingBlock:
deletePersons:
changeRequestForDedupingGraphPersons:
fetchInvalidMergeCandidatePersonsForPerson:options:
nameSource
trainingType
isConfirmedFaceCropGenerationPending
newVerifiedPersonsFetchOptionsWithPhotoLibrary:
fetchRejectedPersonsForFace:options:
_getMergeCandidates:invalidMergeCandidates:forPersonsWithLocalIdentifiers:
fetchRejectedFacesForPerson:options:
progressWithTotalUnitCount:
filterUsingPredicate:
becomeCurrentWithPendingUnitCount:
setNameSource:
resignCurrent
personBuilderMergeCandidatesDisabled
addMergeCandidatePersons:
addInvalidMergeCandidatePersons:
fetchFacesOnAssetWithFace:options:
removeObjectAtIndex:
otherFacesOnAssetWithFace:options:
_duplicateFaceCSNsOnAssetForPerson:faceCSNsOnPerson:faceByCSNCache:
fetchFaceCropByFaceLocalIdentifierForFaces:fetchOptions:
_checkRejectedFaceCropsForFaceGroups:withCancelOrExtendTimeoutBlock:
dedupeGraphVerifiedPersonsInFaceGroup:personCache:
minimumFaceGroupSizeForCreatingMergeCandidates
_getTrainingFacesByPerson:confirmedFaceCSNs:faceCSNsByPerson:faceCSNsByMigratedPerson:faceCSNsByQuickClassificationPerson:mergeCandidates:invalidMergeCandidates:rejectedPersonsByPerson:faceInFaceGroupByCSN:inFaces:personCache:cancelOrExtendTimeoutBlock:
_getRejectedTrainingFaceCSNs:rejectedFaceCSNs:rejectedPersonLocalIdentifiers:forPerson:faceInFaceGroupByCSN:
_completePersonBuildingWithPersonsToUpdate:facesToRemoveByPerson:facesToAddByPerson:updateFaceGroup:newMergeCandidatePairs:newInvalidMergeCandidatePairs:faceInFaceGroupByCSN:personCache:keyFaceUpdateBlock:cancelOrExtendTimeoutBlock:context:error:
nextObject
anyObject
level0ClusterAsFaceCSNsByLevel0KeyFaceCSNForClusterIdentifiedByFaceCSN:error:
_level0ClusterIdForFaceCSN:level0Clusters:
setWithSet:
intersectsSet:
quarantineTwinsOnAssetEnabled
_updateFaceCSNsToAddByPerson:faceCSNsToRemoveByPerson:faceInFaceGroupByCSN:faceCSNsByPersonLocalIdentifier:faceCSNsByMigratedPersonLocalIdentifier:personsToUpdate:
personBuildingDisabled
_updatedFaceGroupByFGLocalIdentifierFromClusterCSNsWithCancelOrExtendTimeoutBlock:fetchLimit:
_buildPersonsFromUpdatedFaceGroups:faceClusterer:keyFaceUpdateBlock:cancelOrExtendTimeoutBlock:context:
suggestedMeIdentifierWithPersonClusterManager:forPersons:updateBlock:
socialGroupsOverTheYearsWithPersonClusterManager:forPersons:updateBlock:
multiLevelSocialGroupsWithPersonClusterManager:forPersons:updateBlock:
densityClusteringForObjects:maximumDistance:minimumNumberOfObjects:withDistanceBlock:
newVerifiedPersonsWithAtLeastOneFaceFetchOptionsWithPhotoLibrary:
keyFaceForPerson:qualityMeasureByFace:updateBlock:
performSocialGroupsIdentifiersWithPersonClusterManager:forPersons:overTheYearsComputation:updateBlock:
countOfFaces
countOfClusteringEligibleFaces
countOfUnclusteredClusteringEligibleFaces
countOfClusteredFaces
unclusteredClusteringEligibleFaceLocalIdentifiers:
deterministicallyOrderedFaceIdentifiersWithLocalIdentifiers:faceprintVersion:
facesForClusteringWithLocalIdentifiers:faceprintVersion:groupingIdentifiers:
invalidFaceClusterSequenceNumbersInClusterSequenceNumbers:cancelOrExtendTimeoutBlock:error:
ungroupFaceClusterSequenceNumbers:batchSizeForUnclusteringFaces:cancelOrExtendTimeoutBlock:error:
cleanupGroupedFacesWithClusterSequenceNumberSetToZero:error:
logPVDebugMessage:
logPVInfoMessage:
logPVWarningMessage:
logPVErrorMessage:
persistChangesToAlgorithmicFaceGroups:faceCSNByLocalIdentifierForNewlyClusteredFaces:faceCSNsOfUnclusteredFaces:localIdentifiersOfUnclusteredFaces:persistenceCompletionBlock:cancelOrExtendTimeoutBlock:error:
resetLibraryClustersWithCancelOrExtendTimeoutBlock:error:
groupedClusterSequenceNumbersOfFacesInFaceGroupsOfMinimumSize:error:
invalidFaceClusterSequenceNumbersInClusterSequenceNumbers:canceler:error:
ungroupFaceClusterSequenceNumbers:batchSizeForUnclusteringFaces:canceler:error:
cleanupUngroupedFacesWithNonZeroClusterSequenceNumbersWithCanceler:error:
cleanupGroupedFacesWithClusterSequenceNumberSetToZeroWithCanceler:error:
persistChangesToAlgorithmicFaceGroups:faceCSNByLocalIdentifierForNewlyClusteredFaces:faceCSNsOfUnclusteredFaces:localIdentifiersOfUnclusteredFaces:persistenceCompletionBlock:canceler:error:
resetLibraryClustersWithCanceler:error:
updateKeyFacesOfPersonsWithLocalIdentifiers:forceUpdate:canceler:error:
bestRepresentativeFaceForPerson:qualityMeasureByFace:canceler:
bestRepresentativeFaceForPerson:qualityMeasureByFace:candidateFaces:canceler:
associateFace:withFaceCrop:error:
clearDirtyStateOnFaceCrops:error:
dirtyFaceCropsWithLimit:
faceAssociatedWithFaceCrop:
facesFromAsset:
persistFaces:deleteFaces:forAsset:persistedFaces:error:
persistGeneratedFaceCrops:error:
recordNeedToPersonBuildOnFaceGroupContainingFace:error:
suggestedPersonLocalIdentifierForPersonLocalIdentifier:faceClusterer:canceler:context:error:
updateFaceprint:ofPersistedFace:error:
buildPersonsWithFaceClusterer:keyFaceUpdateBlock:canceler:context:extendTimeoutBlock:
needsPersonBuilding
cleanupMergeCandidatesWithMinimumFaceGroupSize:cancelOrExtendTimeoutBlock:error:
buildPersonWithFaceClusterer:keyFaceUpdateBlock:context:cancelOrExtendTimeoutBlock:
fetchFaceWithLocalIdentifier:error:
fetchFaceWithClusterSequenceNumber:error:
fetchPersonWithLocalIdentifier:options:error:
removeAutoAssignedFacesFromVerifiedPersonsAndPrepareForPersonBuilding:cancelOrExtendTimeoutBlock:error:
setPersonBuilderMergeCandidatesDisabled:
_personBuilderMergeCandidatesDisabled
TB,N,V_personBuilderMergeCandidatesDisabled
generatePersonRegions:boxes:maxNumRegions:
createInput:withBuffer:inputHeight:inputWidth:
generatePersonBoxes:
initWithMaxNumRegions:forceCPU:sharedModel:inputConfig:
personDetection:personRegions:cancel:
_outputsData
convertResultsToDict:results:
_petsDetector
vcp_quality
analyzeImageQuality:irisPhotoOffsetSec:cancel:
Tf,R,V_qualityScore
initWithMaxNumRegions:prune:
copyImage:toData:withChunk:
outputScaling
computeScore:width:height:posX:posY:
scaleImage:toData:withWidth:andHeight:
saliencyDetection:salientRegions:cancel:
pruneRegions:
analyzerWith:prune:
_region
_prune
generateSalientRegion:outHeight:outWidth:
getSalientRegions:
_createPixelBuffer:withWidth:andHeight:
_createPixelBuffer:withColorSpace:fromPixelBuffer:
_createPixelBuffer:withMinorDimension:fromFullPixelBuffer:
_pooledPixelBuffer:withDimension:
fullPixelBuffer:toScaledBuffer:
imageManager
loadFullPixelBuffer:scaledPixelBuffer299:scaledPixelBuffer360:fromImageURL:isPano:
scalePixelBuffer:toPixelBuffer:width:height:
_pixelBufferPools
initWithPixelBuffer:
setCount:
_count
T^{__CVBuffer=},R,N
TQ,N,V_count
valueWithSize:
sourcePixelBuffer
preWarmWidth:andHeight:
pixelBuffer:width:height:
_scaledPixelBuffers
_sourcePixelBuffer
aestheticsRequest
setAestheticsRequest:
classificationRequest
setClassificationRequest:
sceneprintRequest
setSceneprintRequest:
saliencyRequest
setSaliencyRequest:
junkImageRequest
setJunkImageRequest:
objectRequest
setObjectRequest:
saliencyObjectnessRequest
setSaliencyObjectnessRequest:
landmarkRequest
setLandmarkRequest:
nsfwRequest
setNsfwRequest:
tabooRequest
setTabooRequest:
semanticRequest
setSemanticRequest:
sceneprintRawRequest
setSceneprintRawRequest:
memeRequest
setMemeRequest:
adjustmentsRequest
setAdjustmentsRequest:
documentRequest
setDocumentRequest:
_aestheticsRequest
_classificationRequest
_sceneprintRequest
_saliencyRequest
_junkImageRequest
_objectRequest
_saliencyObjectnessRequest
_landmarkRequest
_nsfwRequest
_tabooRequest
_semanticRequest
_sceneprintRawRequest
_memeRequest
_adjustmentsRequest
_documentRequest
T@"VNClassifyImageAestheticsRequest",&,N,V_aestheticsRequest
T@"VNSceneClassificationRequest",&,N,V_classificationRequest
T@"VNCreateSceneprintRequest",&,N,V_sceneprintRequest
T@"VNGenerateAttentionBasedSaliencyImageRequest",&,N,V_saliencyRequest
T@"VNClassifyJunkImageRequest",&,N,V_junkImageRequest
T@"VNRecognizeObjectsRequest",&,N,V_objectRequest
T@"VNGenerateObjectnessBasedSaliencyImageRequest",&,N,V_saliencyObjectnessRequest
T@"VNClassifyPotentialLandmarkRequest",&,N,V_landmarkRequest
T@"VNVYvzEtX1JlUdu8xx5qhDI",&,N,V_nsfwRequest
T@"VN6Mb1ME89lyW3HpahkEygIG",&,N,V_tabooRequest
T@"VN5kJNH3eYuyaLxNpZr5Z7zi",&,N,V_semanticRequest
T@"VNCreateSceneprintRequest",&,N,V_sceneprintRawRequest
T@"VNClassifyMemeImageRequest",&,N,V_memeRequest
T@"VN1JC7R3k4455fKQz0dY1VhQ",&,N,V_adjustmentsRequest
T@"VNRecognizeDocumentElementsRequest",&,N,V_documentRequest
sharedTaxonomy
_includeRotation
vcp_sharedModelWithModelName:
_includeDO
_includeSO
_includeLM
_includeNSFW
_includeSE
_includeSDG
_includeWP
_includeMeme
_includeDocument
_includeIVS
numberWithUnsignedInt:
_createPixelBufferWithWidth:height:pixelFormat:pixelBuffer:
_createPixelBufferPool:withBufferSize:andPixelFormat:
_convertFromBuffer:toLumaPixelBuffer:isPano:
setMetalContextPriority:
defaultANEDevice
setProcessingDevice:
_configureRequest:withRevision:
maximumLeafObservations
setMaximumLeafObservations:
maximumHierarchicalObservations
setMaximumHierarchicalObservations:
setPrivateRevision:error:
_useR14J9
setReturnAllResults:
defaultMetalDevice
textElements
setRecognize:
machineReadableCodeElements
documentElements
hasPrefix:
nodeForName:
sceneClassId
enumerateObjectsUsingBlock:
_processBoundingBoxFromDetectedObjects:forSceneClassID:
_insertBoundingBox:toSortedBoundingBoxes:
highPrecisionThreshold
highRecallThreshold
threshold
detectors
_extractAndSortBoundingBoxFromDetectedObjects:
_parseClassificationObservations:toClassificationResults:
_parseClassificationObservations:withPrefix:toClassificationResults:
string
lowercaseString
length
characterAtIndex:
_generateSceneClassifications:fromRequests:
_obfuscateLabelName:
aestheticScore
wellFramedSubjectScore
wellChosenBackgroundScore
tastefullyBlurredScore
sharplyFocusedSubjectScore
wellTimedShotScore
pleasantLightingScore
pleasantReflectionsScore
harmoniousColorScore
livelyColorScore
pleasantSymmetryScore
pleasantPatternScore
immersivenessScore
pleasantPerspectiveScore
pleasantPostProcessingScore
noiseScore
failureScore
pleasantCompositionScore
interestingSubjectScore
intrusiveObjectPresenceScore
pleasantCameraTiltScore
lowKeyLightingScore
narrowedBoundingBox
salientObjects
sceneprints
archivedDataWithRootObject:requiringSecureCoding:error:
adjustmentKeys
adjustmentValuesForKey:
_createRequests:
initWithCVPixelBuffer:options:session:
generateClassificationScoresForPixelBuffer:
_collectSceneAnalysisResults:fromRequests:wpResults:ivsResults:
_getSHRevision
_performBlurAnalysis:withPixelBuffer:usingAnalyzer:
inputSize
inputFeatureName
outputFeatureName
numberWithShort:
_performSceneAnalysis:image:isPano:
_performBlurAnalysis:withLumaPixelBuffer:isPano:isSDOF:
_performExposureAnalysis:withLumaPixelBuffer:
_performRotationAnalysis:withColorPixelBuffer:
_loadImageURL:isPano:colorPixelBuffer:andLumaPixelBuffer:image:
_enableSceneAssetConcurrency
_performAnalysis:isPano:isSDOF:colorPixelBuffer:andLumaPixelBuffer:image:
analyzeWithImageURL:isPano:isSDOF:completionHandler:
analyzeWithPixelBuffer:isPano:isSDOF:results:cancel:
_imageManager
_sceneTaxonomy
_pool8Y
_rotationModel
_rotationPool
_ivsPool
vcp_hasLocalMovie:
vcp_thumbnailResource
vcp_size
vcp_avAsset:
thumbnailSizeForAsset:withResources:
initWithPixelFormat:
convertImage:yuvFrame:
vcp_localPhotoResourcesSorted:
_generateLastFrameDistanceDescriptor:withDescriptorClass:forAsset:
_getThumbnailForAsset:withResouces:andPixelFormat:
activeCost
inactiveCost
service
purge
_service
T@"VIService",R,N
initWithModelFile:paramFile:numTri:triList:angle:
validateOneImage:landmarks:numofLandmarks:score:
_transArray
_meanLandmarkLoc
_triIndexMap
_numTri
_triList
T^f,V_orientation
cameraMotionDetection:
generateThresholds:withConfidences:
autoLiveMotionScore:
initWithQueue:turbo:
prewarmWithWidth:height:
analyzeFrame:withTimestamp:andDuration:completion:
_frame
_stats
_cameraMotionParams
_cameraMotionConfidences
_turbo
Tf,R,V_actionScore
vcp_firstEnabledTrackWithMediaType:
formatDescriptions
findMetaTrackforType:
initWithTrack:
analyzerForTrackType:withTransform:requestAnalyses:formatDescription:
copyNextMetadataGroup
processMetadataGroup:flags:
finalizeAnalysis
publicResults
lastObject
processMetaTrackForType:cancel:flags:
checkTimeRangeConsistency
postProcessOrientationResults
initWithAVAsset:forAnalysisTypes:
analyzeAsset:flags:
_avAsset
_transform
_metaTracks
_publicMutableResults
_privateMutableResults
T@"NSDictionary",R,N
URLForResource:withExtension:
modelDescription
inputDescriptionsByName
imageConstraint
pixelsWide
pixelsHigh
pixelFormatType
outputDescriptionsByName
initWithModelName:
inputPixelFormat
_inputPixelFormat
_inputFeatureName
_outputFeatureName
Tq,R,N,V_inputSize
TI,R,N,V_inputPixelFormat
T@"NSString",R,N,V_inputFeatureName
T@"NSString",R,N,V_outputFeatureName
timeWithCMTime:
addClusterPrecision:forPersonID:personFaceCount:validFaceCount:identitySize:
addIdentityRecallToGroundTruth:forPersonID:personFaceCount:identitySize:
addIdentityRecallExcludeMissDetection:forPersonID:personFaceCount:identitySize:
weightedAveragePrecision
setWeightedAveragePrecision:
weightedAverageRecall
setWeightedAverageRecall:
numSingletons
setNumSingletons:
numValidSingletons
setNumValidSingletons:
precisionPerCluster
setPrecisionPerCluster:
recallPerPersonToGroundTruth
setRecallPerPersonToGroundTruth:
recallPerPersonExcludeMissDetection
setRecallPerPersonExcludeMissDetection:
_weightedAveragePrecision
_weightedAverageRecall
_numSingletons
_numValidSingletons
_precisionPerCluster
_recallPerPersonToGroundTruth
_recallPerPersonExcludeMissDetection
Tf,V_weightedAveragePrecision
Tf,V_weightedAverageRecall
Tf,V_numSingletons
Tf,V_numValidSingletons
T@"NSMutableArray",&,V_precisionPerCluster
T@"NSMutableArray",&,V_recallPerPersonToGroundTruth
T@"NSMutableArray",&,V_recallPerPersonExcludeMissDetection
fileExistsAtPath:
_groundTruthURL
_loadGroundTruthURL:toGroundTruth:error:
_loadGroundTruth:error:
setPersonContext:
sortDescriptorWithKey:ascending:
setSortDescriptors:
fetchPersonsWithOptions:
dataUsingEncoding:allowLossyConversion:
bytes
dataWithBytes:length:
base64EncodedStringWithOptions:
setFetchPropertySets:
fetchFacesInFaceGroup:options:
fetchAssetsForFaces:options:
cloudIdentifier
curationProperties
addedDate
fetchPersonWithFace:options:
verifiedType
name
centerX
centerY
_dumpFaceprint
faceClusteringProperties
faceprintData
fetchMomentsForAssetsWithLocalIdentifiers:options:
setMinimumUnverifiedFaceCount:
fetchFaceGroupsForPerson:options:
_processFetchedFaceGroup:forPersonID:facesPerAsset:assetInformation:extendTimeoutBlock:cancelBlock:
_fetchPersonWithIdentifier:facesPerAsset:assetInformation:extendTimeoutBlock:cancelBlock:
localizedStringFromDate:dateStyle:timeStyle:
fetchFacesWithOptions:
_fetchPeopleHomePersons
fetchMergeCandidatePersonsForPerson:options:
fetchFaceGroupsWithOptions:
_overlapRatioOf:with:
_dumpAssetsToFaces
fetchAssetsWithLocalIdentifiers:options:
originalFilename
_exportAssetsToFacesDetails:
allValues
allObjects
countForObject:
fetchFaces
contentsOfDirectoryAtURL:includingPropertiesForKeys:options:error:
arrayWithContentsOfURL:
_parseGroundTruthWithURL:faceCountPerPerson:personInformation:faceToPerson:assetToFaces:extendTimeoutBlock:cancelBlock:
_measureClusterWithClusterStateURL:groundTruthFaceCountPerPerson:groundTruthPersonInformation:groundTruthFaceToPerson:groundTruthAssetToFaces:measures:extendTimeoutBlock:cancelBlock:
_measurePVPersonClusters:groundTruthFaceCountPerPerson:groundTruthPersonInformation:groundTruthFaceToPerson:groundTruthAssetToFaces:measures:extendTimeoutBlock:cancelBlock:
_reportCoreAnalyticsWithVisionClusterMeasure:personClusterMeasure:personClusters:andGroundTruthInformation:
calculateAndReportClusterAccuracyWithVisionClusterURL:andPersonClusters:withGroundtruth:results:extendTimeoutBlock:cancelBlock:
JSONObjectWithData:options:error:
stringByDeletingPathExtension
_parseSIMLGroundTruthWithURL:faceCountPerPerson:personInformation:faceToPerson:assetToFaces:extendTimeoutBlock:cancelBlock:
exportClustersStates:error:extendTimeoutBlock:cancelBlock:
compare:options:
compare:
sortedArrayUsingComparator:
workerWithPhotoLibrary:
optInPersonCount
_anonymizedName:
optInStatus:error:
optInPerson:error:extendTimeoutBlock:cancelBlock:
calculateAndReportClusterAccuracyWithVisionClusterURL:andPersonClusters:results:extendTimeoutBlock:cancelBlock:
validateClusterAccuracyWithSIMLGroundtruth:results:extendTimeoutBlock:cancelBlock:
_detectionVersion
_recognitionVersion
_personClusterVersion
_processingVersion
_clusterDumpFaceFetched
currentLocale
setLocale:
sharedLogManager
dateFormatter
logLevel
_logLevel
Ti,R,V_logLevel
detectPersons:persons:
substringToIndex:
assetResourcesForAsset:
timeRangeMapperForSourceDuration:slowMotionRate:slowMotionTimeRange:forExport:
originalTimeForScaledTime:
scaledTimeForOriginalTime:
initWithTransform:timeRange:isLivePhoto:frameStats:keyFrameResults:
initWithAnalysisType:isLivePhoto:hadFlash:hadZoom:isTimelapse:preferredTimeRange:asset:
setMaxHighlightDuration:
analyzeFrame:withTimestamp:
preparePostProcessingStatsFromFaceRange:junkResults:
postProcess
postProcessKeyFrames
keyFrames
prepareRequiredQualityResult:junkDetectionResult:descriptorResult:faceResult:saliencyResult:actionResult:subtleMotionResult:voiceResult:keyFrameResult:sceneResults:humanActionResults:humanPoseResults:cameraMotionResults:orientationResults:frameSize:
generateHighlights
timerange
score
bestPlaybackCrop
keyFrame
colorNormalization
isTrimmed
isAutoPlayable
reportMovieCurationAnalysisResults:withSummaryAnalytics:
addHighlight:to:
highlightScoreResults
movieSummary
addSummary:to:
keyFrameScores
initWithAnalysisTypes:transform:timeRange:isLivePhoto:frameStats:hadFlash:hadZoom:keyFrameResults:isTimelapse:preferredTimeRange:asset:
analyzeKeyFrame:withTimestamp:andDuration:flags:
loadVideoAnalysisResults:audioAnalysisResults:andFaceRanges:frameSize:
generateMovieCurations
_keyFrameAnalyzer
_highlightAnalyzer
_descriptorResults
_qualityResuls
_junkResults
_actionResults
_subtleMotionResults
_voiceResults
_sceneResults
_humanActionResults
_humanPoseResults
_cameraMotionResults
_saliencyResults
_orientationResults
_faceRanges
_frameSize
_frameStats
_isLivePhoto
_hadFlash
_hadZoom
initWithTime:andScore:
timeStamp
_score
_timeStamp
T{?=qiIq},R,N,V_timeStamp
Tf,R,N,V_score
initWithTimeRange:score:andKeyFrame:
_keyFrame
_timerange
T{?={?=qiIq}{?=qiIq}},R,N,V_timerange
T@"VCPVideoKeyFrameResult",R,N,V_keyFrame
initWithPHAsset:
phAsset
highlights
setResults:
_phAsset
_highlights
T@"PHAsset",R,N,V_phAsset
T@"NSMutableArray",R,&,N,V_highlights
T@"NSMutableDictionary",&,N,V_results
averageScore
descriptor
junkScore
expressionScore
voiceScore
humanActionScore
humanPoseScore
initWithTimeRange:
mergeSegment:
isShort
copyScoresFrom:
checkAutoPlayable
setTimerange:
setScore:
setAverageScore:
setJunkScore:
setExpressionScore:
setVoiceScore:
setHumanActionScore:
setHumanPoseScore:
setBestPlaybackCrop:
setIsAutoPlayable:
setIsTrimmed:
setDescriptor:
setKeyFrame:
setColorNormalization:
_isAutoPlayable
_isTrimmed
_averageScore
_junkScore
_expressionScore
_voiceScore
_humanActionScore
_humanPoseScore
_descriptor
_colorNormalization
_bestPlaybackCrop
T{?={?=qiIq}{?=qiIq}},N,V_timerange
Tf,N,V_score
Tf,N,V_averageScore
Tf,N,V_junkScore
Tf,N,V_qualityScore
Tf,N,V_expressionScore
Tf,N,V_actionScore
Tf,N,V_voiceScore
Tf,N,V_humanActionScore
Tf,N,V_humanPoseScore
T{CGRect={CGPoint=dd}{CGSize=dd}},N,V_bestPlaybackCrop
TB,N,V_isAutoPlayable
TB,N,V_isTrimmed
T@"VCPImageDescriptor",&,N,V_descriptor
T@"VCPVideoKeyFrame",&,N,V_keyFrame
T@"NSData",&,N,V_colorNormalization
T{?={?=qiIq}{?=qiIq}},N,V_timeRange
getMinimumHighlightInSec
assetImageGeneratorWithAsset:
setAppliesPreferredTrackTransform:
setMaximumSize:
setRequestedTimeToleranceAfter:
setRequestedTimeToleranceBefore:
computeHighlightScoreResults
selectHighlightsForTimelapse
selectHighlights
evaluateSegment:
computeColorNormalization
loadHighlightScoreResults:
maxTrimMovieHighlight:
targetProcessRange:maxRange:
targetMovieHighlight:mergedRange:maxRange:
isGoodQuality:
targetTrimRange:searchRange:
targetExtendRange:maxRange:
findBestHighlightSegment:targetTrim:
findBestTrim:
highlightScoreForTimeRange:average:
computeQualityTrimFor:withKeyFrame:
computeTrimWithHighlightScoreFor:
pickKeyFramesInRange:
searchFeatureVectorOfSegment:
computeBestPlaybackCrop:
copyCGImageAtTime:actualTime:error:
analyzeCGImage:results:
computeActionFaceTrimFor:
computeSteadyTranslationTrimFor:
checkCameraZoom:
generateExpressionSegments:
analyzeOverallQuality:
computeExpressionScoreInTimerange:
computeActionScoreInTimerange:
computeVoiceScoreInTimeRange:
computeHighlightScoreOfSegment:
addSegment:
qualityScoreForTimerange:
junkScoreForTimerange:lengthScale:
computeSubtleMotionScoreInTimerange:
cameraMotionScoreForTimerange:
computeHumanActionScoreInTimerange:
computeHumanPoseScoreInTimerange:
actionScoreForTimerange:
subtleMotionScoreForTimerange:
expressionScoreForTimerange:
voiceScoreForTimerange:
visualPleasingScoreForTimerange:
initWithPostProcessOptions:
postProcessMovieHighlight:
computeHighlightScoreOfRange:
SetKeyFramesForSegments:
pickHighlightsFrom:
_qualityResults
_featureResults
_keyFrameResults
_expressionSegments
_internalResults
_highlightResults
_internalConstraintResults
_maxDurationInSeconds
_minDurationInSeconds
_targetDurationInSeconds
_toleranceInSeconds
_targetHighlightIndex
_startRange
_isMaxTrim
_requestBestTrim
_requestFullResult
_maxHighlightScore
_minHighlightScore
_preferredTimeRange
_imageGenerator
_colorNormalizationAnalyzer
requestMediaAnalysisDatabaseAccessSandboxExtensionWithPhotoLibraryURL:andReply:
requestImageProcessing:forIOSurface:withOrientation:identifier:requestID:andReply:
requestImageProcessing:forAssetURL:withSandboxToken:identifier:requestID:andReply:
requestImageProcessing:forAssetWithLocalIdentifier:fromPhotoLibraryWithURL:requestID:andReply:
requestImageProcessing:forIOSurface:withOrientation:assetLocalIdentifier:photoLibraryURL:requestID:andReply:
requestImageProcessing:forAssetWithCloudIdentifier:requestID:andReply:
queryPerformanceMeasurementsWithReply:
resetPerformanceMeasurements
startEntryPointWithQueryID:
endEntryPoint
requestURLAssetAnalysis:forAssetWithResourcePaths:withOptions:analysisTypes:sandboxTokens:withReply:
requestAssetAnalysis:forPhotoLibraryURL:withLocalIdentifiers:realTime:withReply:
requestLibraryProcessing:withTaskID:forPhotoLibraryURL:withOptions:andReply:
requestAssetProcessing:withTaskID:forLocalIdentifiers:fromPhotoLibraryWithURL:withOptions:andReply:
cancelRequest:
cancelAllRequests
cancelBackgroundActivityWithReply:
currentOutstandingTasksWithReply:
notifyLibraryAvailableAtURL:
requestSuggestedPersons:withPersonWithLocalIdentifier:toBeConfirmedPersonSuggestions:toBeRejectedPersonSuggestions:andPhotoLibraryURL:andReply:
requestUpdateKeyFacesOfPersons:withLocalIdentifiers:andForceUpdate:andPhotoLibraryURL:andReply:
requestFaceCandidatesforKeyFace:withPersonsWithLocalIdentifiers:andPhotoLibraryURL:andReply:
requestResetFaceClassificationModel:withPhotoLibraryURL:andReply:
requestResetPetClassificationModel:withPhotoLibraryURL:andReply:
requestSuggestedMePersonIdentifier:withContext:andPhotoLibraryURL:andReply:
requestPersonPromoterStatus:withAdvancedFlag:andPhotoLibraryURL:andReply:
requestClusterCacheValidation:withPhotoLibraryURL:andReply:
requestResetFaceClusteringState:withPhotoLibraryURL:andReply:
requestReclusterFaces:withPhotoLibraryURL:andReply:
requestRebuildPersons:withLocalIdentifiers:andPhotoLibraryURL:andReply:
requestPersonPreferenceForPhotoLibraryURL:andReply:
requestVIPModelStorageFilepathForPhotoLibraryURL:forModelType:andReply:
queryAutoCounterOptInStatus:withPhotoLibraryURL:personLocalIdentifiers:andReply:
requestOptInAutoCounter:withPhotoLibraryURL:persons:andReply:
requestDumpAutoCounter:withPhotoLibraryURL:andReply:
requestAutoCounterAccuracyCalculation:withPhotoLibraryURL:andReply:
requestAutoCounterAccuracyCalculation:withPhotoLibraryURL:clusterStateURL:groundTruthURL:andReply:
requestAutoCounterSIMLValidation:withPhotoLibraryURL:simlGroundTruthURL:andReply:
requestIdentificationOfFacesWithLocalIdentifiers:fromPhotoLibraryWithURL:withRequestID:andReply:
requestProcessingTypes:forAssetsWithLocalIdentifiers:fromPhotoLibraryWithURL:withRequestID:andReply:
interfaceWithProtocol:
setClasses:forSelector:argumentIndex:ofReply:
initWithMachServiceName:options:
setExportedObject:
setRemoteObjectInterface:
reportProgress:forRequest:
setExportedInterface:
setInterruptionHandler:
setInvalidationHandler:
resume
taskForURLAsset:withOptions:analysisTypes:progressHandler:completionHandler:
stringWithUTF8String:
connection
remoteObjectProxyWithErrorHandler:
vcp_defaultURL
requestBackgroundAnalysisForAssets:fromPhotoLibraryWithURL:realTime:progessHandler:completionHandler:
requestProcessingWithTaskID:forPhotoLibrary:withOptions:progessHandler:andCompletionHandler:
errorWithDescription:
requestProcessingWithTaskID:forAssets:withOptions:progressHandler:andCompletionHandler:
taskWithAssets:andCompletionHandler:
synchronousRemoteObjectProxyWithErrorHandler:
invalidate
isMultiLibraryModeEnabled
initWithPhotoLibraryURL:
vcp_defaultPhotoLibrary
sharedAnalysisService
analysisService
queryProgressDetail:forPhotoLibraryURL:andTaskID:
requestAnalysisTypes:forAssetWithResourceURLs:withOptions:progressHandler:andCompletionHandler:
requestBackgroundAnalysisForAssets:realTime:progessHandler:completionHandler:
requestBackgroundProcessingWithTaskID:forPhotoLibrary:progessHandler:completionHandler:
requestSceneProcessingForPhotoLibrary:withOptions:progressHandler:andCompletionHandler:
requestFaceProcessingForPhotoLibrary:withOptions:progressHandler:andCompletionHandler:
requestMultiWorkerProcessingForPhotoLibrary:withOptions:progressHandler:andCompletionHandler:
requestFullProcessingForPhotoLibrary:withOptions:progressHandler:andCompletionHandler:
requestLivePhotoEffectsForAssets:withOptions:progressHandler:andCompletionHandler:
requestSceneProcessingForAssets:withOptions:progressHandler:andCompletionHandler:
requestFaceProcessingForAssets:withOptions:progressHandler:andCompletionHandler:
requestQuickFaceIdentificationForPhotoLibraryURL:withOptions:andCompletionHandler:
requestSceneprintProcessingForAssets:withOptions:progressHandler:andCompletionHandler:
requestVideoStabilizationForAssets:withOptions:progressHandler:andCompletionHandler:
cancelBackgroundActivity
requestPersonPreferenceForPhotoLibraryURL:completionHandler:
requestVIPModelFilepathForPhotoLibraryURL:forModelType:completionHandler:
_connection
_handlerQueue
_progressBlocks
_nextRequestID
requestSuggestedPersonsForPersonWithLocalIdentifier:toBeConfirmedPersonSuggestions:toBeRejectedPersonSuggestions:photoLibraryURL:progessHandler:completionHandler:
requestUpdateKeyFacesOfPersonsWithLocalIdentifiers:forceUpdate:photoLibraryURL:progessHandler:completionHandler:
requestFaceCandidatesforKeyFaceForPersonsWithLocalIdentifiers:photoLibraryURL:progessHandler:completionHandler:
requestResetFaceClassificationModelForPhotoLibraryURL:progressHandler:completionHandler:
requestResetPetClassificationModelForPhotoLibraryURL:progressHandler:completionHandler:
requestSuggestedMePersonIdentifierWithContext:photoLibraryURL:progressHandler:completionHandler:
requestPersonPromoterStatusWithAdvancedFlag:photoLibraryURL:progressHandler:completionHandler:
requestPersonProcessingForPhotoLibraryURL:options:progressHandler:completionHandler:
requestClusterCacheValidationWithPhotoLibraryURL:progressHandler:completionHandler:
requestResetFaceClusteringStateWithPhotoLibraryURL:progressHandler:completionHandler:
requestReclusterFacesWithPhotoLibraryURL:progressHandler:completionHandler:
requestRebuildPersonsWithLocalIdentifiers:photoLibraryURL:progressHandler:completionHandler:
queryAutoCounterOptInStatusForPhotoLibraryURL:withPersonLocalIdentifiers:completionHandler:
requestOptInAutoCounterForPhotoLibraryURL:withPersons:completionHandler:
requestDumpAutoCounterForPhotoLibraryURL:completionHandler:
requestAutoCounterAccuracyCalculationForPhotoLibraryURL:completionHandler:
requestAutoCounterAccuracyCalculationForPhotoLibraryURL:clusterStateURL:groundTruthURL:completionHandler:
requestAutoCounterSIMLValidationForPhotoLibraryURL:simlGroundTruthURL:completionHandler:
requestIdentificationOfFaces:withCompletionHandler:
requestProcessingTypes:forAssetsWithLocalIdentifiers:fromPhotoLibraryWithURL:progressHandler:completionHandler:
_setupMediaAnalysisServiceConnection
storeAnalysis:forAsset:fromPhotoLibraryURL:withReply:
registerClient:forPhotoLibraryURL:withReply:
_getSandboxExtensionForMediaAnalysisDatabaseWithPhotoLibraryURL:
sharedDatabaseForPhotoLibrary:
sceneClassifications
sceneIdentifier
vcp_setVersion:
vcp_setDateModified:
date
vcp_setDateAnalyzed:
vcp_setFlags:
vcp_fingerprint:
vcp_setFingerprint:
vcp_setResult:forKey:
vcp_addTypes:
mediaType
vcp_isVideoSlowmo
vcp_allResourcesForAsset:
vcp_fullAnalysisTypesForResources:
sharedInstance
hasWifiOrEthernetConnection
vcp_eligibleForStreaming:
vcp_eligibleForVideoDownload:
isVideo
canAnalyzeUndegraded:withResources:
setAllowStreaming:
hasAdjustments
vcp_hasLocalPhoto:
analyzeAsset:
isPhoto
vcp_types
movieAssetWithURL:
analyzerWithVCPAsset:withExistingAnalysis:forAnalysisTypes:
livePhotoAssetWithImageURL:andMovieURL:
imageAssetWithURL:
analyzerWithVCPAsset:forAnalysisTypes:
isAssetBlacklisted:blacklistDate:
_postProcessMovieHighlights:analysis:withOptions:
_addClassificationResults:analysis:
vcp_fingerprint
vcp_degraded
_metaAnalysisTypesForAsset:
_analyzeOndemand:forAnalysisTypes:withExistingAnalysis:andOptions:storeAnalysis:
sharedMediaAnalyzer
_databaseForPhotoLibrary:
requestAnalysis:forAsset:withExistingAnalysis:andDatabase:andOptions:
queryAssetsAnalyzedSince:
assetsFromPhotoLibrary:analyzedSinceDate:completionHandler:
fetchPropertySetsIfNeeded
sceneprintProperties
sceneprint
distanceIdentity
queryAnalysisForAsset:withTypes:
_getSceneDescriptors:asDescriptorClass:withSceneRange:andAnalysisResults:
_getDistanceDescriptorClass
_checkDuplicate:withAsset:duplicate:
_queryDistanceDescriptor:ofAsset:withExistingAnalysis:andDatabase:timeRange:lastFeature:isDegraded:
computeDistance:fromArray:toArray:
computeDistance:withDescriptorClass:fromAsset:toAsset:
vcp_flags
canUseLastFrameOfAsset:withResources:
generateDistanceDescriptor:withDescriptorClass:forAsset:withResources:lastFrame:
arrayWithObject:
_typesToRemove:requested:
requestAnalysis:forAssets:withOptions:andProgressHandler:andCompletionHandler:
requestAnalysisTypes:forAssets:withOptions:andProgressHandler:analyses:
requestAnalysis:forAssets:withOptions:andProgressHandler:andError:
fetchAssetsInAssetCollection:options:
sortedArrayUsingSelector:
reverseObjectEnumerator
vcp_isShortMovie
vcp_queryPHFaces:results:
initWithFlagHasFaceOrPet:
assetWithPHAsset:
analyzeAsset:onDemand:cancel:statsFlags:results:
numberWithUnsignedLongLong:
_getDatabaseSandboxExtensionForPhotoLibraryURL:
requestAnalysisTypes:forAssetWithResourceURLs:withOptions:error:
analyzeOndemand:pairedURL:forAnalysisTypes:error:
requestAnalysisForAsset:analysisTypes:progressHandler:completionHandler:
cancelAnalysisWithRequestID:
assetsAnalyzedSinceDate:completionHandler:
distanceFromAsset:toAsset:duplicate:distance:
distanceFromAsset:timeRange:toAsset:timeRange:duplicate:distance:
requestAnalysesForAssets:analysisTypes:allowOndemand:progressHandler:completionHandler:
requestAnalysisTypes:forAssets:allowOndemand:progressHandler:error:
curateMovieAssetsForCollection:withAlreadyCuratedAssets:andDesiredCount:allowOnDemand:
requestMovieHighlightsForAssets:withOptions:
postProcessMovieHighlightDuration:withOptions:
requestLivePhotoEffectsForAssets:allowOnDemand:flags:
completeStorage
_analysisQueue
_storageQueue
_storageGroup
_standalone
_minHighlightDuration
_noResultStrip
_mediaAnalysisServiceConnection
_sandboxQueue
_sandboxHandles
queryID
referralURL
imageType
featureIdentifier
contextWithDictionary:error:
visualUnderstanding
imageRegions
domainInfo
domainKey
labelName
glyphName
hasFocalPoint
focalPoint
initWithDomain:label:glyphName:hasFocalPoint:andFocalPoint:
regionOfInterest
initWithNormalizedBoundingBox:andDomains:
payload
initWithResultItems:andPayload:
createQueryContextWithError:
vcp_annotation:
queryWithPixelBuffer:orientation:normalizedRegionOfInterest:annotation:queryContext:
stop
storeResults:
elapsedTimeSeconds
parseWithVisualQuery:completion:
_cancellable
numOfFrames
updateSegment:
resetSegment:
finalizeAtTime:
_numOfFrames
T{?={?=qiIq}{?=qiIq}},R,N,V_timeRange
TQ,R,N,V_numOfFrames
assetReaderOutputMetadataAdaptorWithAssetReaderTrackOutput:
nextTimedMetadataGroup
_readerOutput
_readerOutputAdaptor
T@"NSData",R,N
TS,R,N
Tq,D,N
getMaximumHighlightInSec
vcp_setStatsFlags:
initWithVCPAsset:withExistingAnalysis:forAnalysisTypes:
vcp_setTypes:
vcp_statsFlags
vcp_addEntriesFromResults:
vcp_syncPoint
vcp_setSyncPoint:
tracksWithMediaType:
loadValuesAsynchronouslyForKeys:completionHandler:
vcp_addFlags:
processExistingAnalysisForTimeRange:analysisTypes:
createDecoderForTrack:timerange:forAnalysisTypes:
createVideoAnalyzer:withFrameStats:
videoStabilizerforAnalysisType:withMetadata:sourceSize:cropRect:
initWithFrameStats:timeOfInterest:phFaces:
getHumanActionClassiferType
postProcessAutoPlayable:
vcp_appendResults:
vcp_endTime
analyzeVideoSegment:timerange:forAnalysisTypes:cancel:
allowStreaming
loadPropertiesForAsset:
vcp_setQuality:
performMetadataAnalysisOnAsset:withCancelBlock:
vcp_fullFrameSize
vcp_cleanApertureRect
initWithMetadata:sourceSize:cropRect:
storeAnalytics:isLivePhoto:
tracks
vcp_startTime
analyzeVideoTrack:start:forAnalysisTypes:cancel:
generateKeyFrameResource:
vcp_removeSyncPoint
analyzeOverallQuality:withFpsRate:
generateLivePhotoRecommendationForResults:andPrivateResults:usingFaceAction:
setActivityLevel:
vcp_addStatsFlags:
vcp_appendResult:forKey:
initWithPHAsset:withPausedAnalysis:forAnalysisTypes:
maxHighlightDuration
faceDominated
setFaceDominated:
_supportConditionalAnalysis
_existingAnalysis
_prepareLivePhotoScenes
_allowStreaming
_maxHighlightDuration
TB,N,V_allowStreaming
Tf,N,V_maxHighlightDuration
TB,N,V_faceDominated
Tq,R,V_status
addKeypoints:
keypointsCount
clearKeypoints
keypointsAtIndex:
keypointsType
Ti,N,V_flags
T@"NSMutableArray",&,N,V_keypoints
convertPixelBuffer:toPixelBuffer:withPixelFormat:
estimateMotionBetweenFirstImage:andSecondImage:error:
convertFlow:
prepareAnalyzerWithCVPixelBuffer:
preProcessing:
generateMotionFlow
_flow
_scale
_scaler
_moflowRequest
_frameArray
_frameWidth
_frameHeight
_downScaleWidth
_downScaleHeight
_flowWidth
_flowHeight
_frameNum
vcp_sortBySize
vcp_isPhotoResourceUsable:
vcp_isMovie
vcp_isVideoResourceUsable:
vcp_isOriginalLocal
vcp_hasLocalAdjustments
vcp_resourceWithType:
vcp_smallResourceMeetingCriteria:
vcp_isPhoto
vcp_localMovieResourcesSorted:
vcp_photoResourcesSorted:
bundleWithIdentifier:
vcp_captureDeviceMake
vcp_captureDeviceModel
vcp_isAppleCapture
unsignedLongValue
vcp_dateAnalyzed
vcp_streamedVideo
vcp_mutableResults
vcp_setResults:
vcp_time
vcp_timerange
vcp_setTimerange:
vcp_imagesPredicate:
vcp_stillImagesPredicate:
vcp_livePhotosPredicate:
vcp_nonPanoImagesPredicate:
vcp_moviesPredicate:
vcp_analysisVersionPredicate:
createFaceHeatMap:imageFaces:
computeOverallFaceQualityScore:
selectKeyFrameRangeWithMotion:stillTimestamp:isMetaMotion:
fetchAndComputeScoreForKeyFrame:withResult:
semanticScore
setSemanticScore:
copyFrom:
computeScoreForPhoto:withRefKeyFrame:
reportLivePhotoKeyFrameAnalysisResults:selectedKeyFrame:originalStillKeyFrame:stillScore:stillFQScore:stillTimestamp:useSemanticOnly:isKeyFrameSuggested:
getFaceHeat:
updateFaceHeatMap:
initWithWidth:height:
analyzeLivePhotoKeyFrame:irisPhotoOffsetSec:originalIrisPhotoOffsetSec:photoTextureScore:hadFlash:cancel:
_photoSharpnessReliable
_photoSharpness
_petsDominant
_ignoreFace
_faceHeatMap
T@"VNSession",R,N
vcp_faceRectFrom:
vcp_flagsForPHFace:withFaceRect:
vcp_PHFaces
objectID
_computeFingerPrintsOfAsset:completionHandler:
fetchAssetsMatchingAdjustedFingerPrint:photoLibrary:
fetchAssetsMatchingMasterFingerPrint:photoLibrary:
vcp_fetchAssetsMatchingFingerprint:forPhotoLibrary:
configuration
faceprintRequestRevision
faceObservationFromFaceprintData:
classifyFaceObservation:withModel:error:
quickClassificationFaceAdjustmentVersion
_loadPersonsModelAndInitializeFaceAnalyzer
deferredProcessingNeeded
_classifyFaces:forAsset:withResults:
_persistResults:withFaces:forAsset:
_loadPetsModel
isInVIPModel
initWithAnimalprint:confidence:
classifyAnimalObservation:withModel:error:
persistModel:toPath:error:
persistPetsModel:toPath:error:
_fetchPersonsToFeedVIPModel
_fetchPetsToFeedVIPModel
fetchEntityForModelType:
newConfigurationForEntityPrintsGeneratedByRequest:error:
modelWithConfiguration:error:
addObservations:toEntityWithUniqueIdentifier:error:
_persistPetsModel:error:
trainingObservationsForEntityWithUniqueIdentifier:canceller:error:
arrayByAddingObjectsFromArray:
setIsInVIPModel:
newMutablePersonsModel
addFaceObservations:forPersonIdentifier:toModel:error:
_persistPersonsModel:error:
trainingFaceObservationsForPersonWithUniqueIdentifier:canceller:error:
vcp_analysisPreferences
_fastFaceMigrationEnabled
_faceProcessingPassGoal
isSystemPhotoLibrary
_modelLastGenerationDidExceedTimeIntervalForType:
_keepCurrentPersonsModel
_needToGenerateModelWithType:ignoreLastGenerationTime:
_generatePersonsModelWithExtendTimeoutBlock:cancel:
_generatePetsModelWithExtendTimeoutBlock:cancel:
processAsset:
classifyVIPPets
generateVIPModelWithType:ignoreLastGenerationTime:extendTimeout:andCancel:
_personsModel
_petsModel
_management
adjustmentTimestamp
setExcludeMontageAssets:
setIncludeTrashedAssets:
orPredicateWithSubpredicates:
setIncludeGuestAssets:
vcp_isSdofPhoto
vcp_isVideoTimelapse
sceneAnalysisProperties
sceneAnalysisVersion
sceneAnalysisTimestamp
vcp_ocrGatingThreshold
vcp_passedOCRGating
uniformTypeIdentifier
typeWithIdentifier:
instancesRespondToSelector:
attributesOfItemAtPath:error:
resourceForAsset:withResources:
vcp_originalSize
processExistingAnalyses:
mediaAnalysisProperties
blurrinessScore
vcp_usePHFace
dictionaryWithObjectsAndKeys:
updateDegradedFlagForMajorDimension:
downscaleImage:scaledImage:majorDimension:
vcp_usePHFaceExpression
initWithMovingObjectsResults:
_reportPetsAnalysisWithResults:
vcp_quickFaceClassificationDone
existingAnalysisForMovieAnalyzer
checkFaceDominant
initWithDictionary:
analyzeImage:performedAnalyses:cancel:
vcp_removeResultForKey:
_irisAnalyses
_phFaceResults
_phFaceFlags
_imageBlurTextureScore
_preAnalysisSharpnessScore
_requirePHFaceAnalysis
stringByAppendingPathComponent:
vcp_mediaAnalysisDirectory
fetchAssetsWithMediaType:options:
internalPredicate
isCloudPhotoLibraryEnabled
cplStatus
lastSuccessfulSyncDate
isExceedingQuota
lastCompletePrefetchDate
vcp_isCPLEnabled
vcp_isCPLDownloadComplete
vcp_supportsInMemoryDownload
vcp_assetCountWithMediaType:forTaskID:
_analysisPreferencesURL
dataWithPropertyList:format:options:error:
_updateAnalysisPreferencesWithEntries:keysToRemove:
wellKnownPhotoLibraryIdentifier
vcp_isCPLSyncComplete
vcp_allowInMemoryDownload
vcp_libraryScaleShortDescription
computeControlPointsCamera:Vt:
computePoints3DCamera
correctSigns
computeRT:T:
computeProjectionError:T:
configureGaussNewton:R6x1:betas:jacobian:residual:
getControlPoints
computeBarycentricCoordinates
computeSVDVt:Vt:
computeL6x10:L6x10:
computeR6x1:
estimateBetasN1:R6x1:betas:
estimateBetasN2:R6x1:betas:
estimateBetasN3:R6x1:betas:
optimizeBetas:R6x1:betas:
estimateRT:betas:R:T:projectionError:
estimatePose:
_numPoints
_controlPointsWorld
_controlPointsCamera
_pointsWorld
_pointsImage
_alphas
_points3DCamera
_cameraOrientation
unarchivedObjectOfClass:fromData:error:
computeDistance:withDistanceFunction:error:
_sceneprint
ComputeSceneDelta:
decideLensSwitchPoint:
PrintSegments
finalizeAnalysisPass:
isSegmentPoint
_sceneDeltaBuffer
_activeSegment
_sceneSegments
_firstFrame
_frameTimeRange
_currentStatus
_isSegmentPoint
setGyroStabilization:
valueWithCMTime:
setAnalysisResultRef:
analysisResultRef
setAnalysisConfidence:
analysisConfidence
setValidStabilization:
convertAnalysisResult
_analysisSessionRef
initWithLocalIdentifier:
isTooSmall
_hidden
_isInTrash
_manual
_isTooSmall
_hasSmile
_isLeftEyeClosed
_isRightEyeClosed
_hasFaceMask
_detectionType
_ageType
_sexType
_eyesState
_smileType
_facialHairType
_hairColorType
_glassesType
_expressionType
_headgearType
_hairType
_poseType
_skintoneType
_ethnicityType
_gazeType
_trainingType
_personLocalIdentifier
_sourceWidth
_sourceHeight
_centerX
_centerY
_bodyCenterX
_bodyCenterY
_bodyWidth
_bodyHeight
_blurScore
_adjustmentVersion
_nameSource
_poseYaw
_algorithmVersion
_clusterSequenceNumber
_qualityMeasure
_gazeCenterX
_gazeCenterY
_groupingIdentifier
_imageprintWrapper
_roll
T@"NSString",C,N,V_personLocalIdentifier
Tq,N,V_sourceWidth
Tq,N,V_sourceHeight
Ts,N,V_detectionType
Td,N,V_centerX
Td,N,V_centerY
Td,N,V_size
Td,N,V_bodyCenterX
Td,N,V_bodyCenterY
Td,N,V_bodyWidth
Td,N,V_bodyHeight
TB,N,V_hidden
TB,N,V_isInTrash
TB,N,V_manual
TB,N,V_isTooSmall
TB,N,V_hasSmile
Td,N,V_blurScore
Td,N,V_exposureScore
TB,N,V_isLeftEyeClosed
TB,N,V_isRightEyeClosed
T@"NSString",C,N,V_adjustmentVersion
Tq,N,V_nameSource
Ti,N,V_trainingType
Td,N,V_poseYaw
TQ,N,V_algorithmVersion
Tq,N,V_clusterSequenceNumber
Tq,N,V_qualityMeasure
TS,N,V_ageType
TS,N,V_sexType
TS,N,V_eyesState
TS,N,V_smileType
TS,N,V_facialHairType
TS,N,V_hairColorType
TS,N,V_glassesType
TS,N,V_expressionType
TS,N,V_headgearType
TS,N,V_hairType
TS,N,V_poseType
TS,N,V_skintoneType
TS,N,V_ethnicityType
TB,N,V_hasFaceMask
TS,N,V_gazeType
Td,N,V_gazeCenterX
Td,N,V_gazeCenterY
T@"NSString",C,N,V_groupingIdentifier
T@"VCPVNImageprintWrapper",&,N,V_imageprintWrapper
Td,N,V_roll
setupModel:
initWithFocalLengthInPixels:principalPoint:cameraTowardsPositiveZ:
updateIntrinsic:vc:
updateFocalLengthInPixels:
initWithModelFile:
numVertices
meanBlendshape
getInternal3dLandmarksCoordinates:lm3dPos:
componentsBlendshape
getOneInternalLandmarkCoordinates:lmCoord:lmWeight:lm3dPos:
updateBoundaryLandmarkCoordinates:pts2D:lm2D:lm3dPos:
project3Dto2D:intrinsinc:extrinsic:numVert:out2dpts:
calculateMesh:numVertices:blendshapes:outputMesh:
updateBoundaryLmForShapeOptimization
updateShapeCoeff:extrinsicMatrix:pts2D:exprWeights:outputblendshapes:
moveBoundaryLandmarks:output:isInput:
projectAndUpdateBoundary
optimizeProjectionMatrix:tracking:firstPass:
updateBoundary3dLandmarkBlendshapes:numBlendshapes:pts2D:lm2D:lmBlendshapes:
calculateBlendshapeWeights:prevWeights:lmBlendshapes:maxIter:
updateMeshAndLm3dAfterExpressionChange
calculateIdentityCoefficients:extrinsicMatrix:pts2D:exprWeights:lm3DMeanBlendshapes:lm3DComponents:maxIter:
calculateModelBlendshapes:outputBlendshapes:
tensorCoeff
blendshapeComponentIndex
calculatePosePnpSolver:
reestimateProjectionMatrixPnP
updateIdentityShape:
getPoseParam
estimateExtrinsicsWith:andPoints3D:andNumPoints:
initWithMode:
isIdentityInit
setCameraIntrinsics:uc:vc:
getEulerAngle:
resetIdentityAndExpressions
trackFaceMesh:
fitOneImage:
getPose
blendShapes
updateMeshVertices
processingMode
setProcessingMode:
meshVertices
vertexCount
detectionModeCounterShapeModel
setDetectionModeCounterShapeModel:
_tensorModel
_numVertices
_curMesh
_cur2D
_numInternalLms
_lmCoord
_lmWeight
_numBoundaryLms
_boundaryLmIndices
_numBoundaryVertices
_boundaryVertices
_boundaryLandmarkValidity
_chPts
_chPtSelected
_boundaryLmUpdated
_chCount
_curBlendshapes
_curCoeff
_curExprWeights
_prevExprWeights
_exprWeightDiagMatrix
_transformedCoeff
_blendShapeDelta
_trans
_intrinsicMatrix
_extrinsicMatrix
_eulerAngle
_rotMatrix
_LM2D
_LM3D
_lm3dBlendshapes
_lm3dMeanBlendshapes
_lm3dBlendshapeComponents
_numFrmsSinceLastShapeUpdate
_shapeUpdateInProgress
_poseSolver
_updateShapeGroup
_updateShapeQueue
_asyncBlendshapes
_asyncLmBlendshapes
_asyncExtMat
_asyncLm2d
_asyncWeights
_identityInit
_processingMode
_detectionModeCounterShapeModel
_meshVertices
_vertexCount
Ti,V_processingMode
T^,R,V_meshVertices
TQ,R,V_vertexCount
Ti,V_detectionModeCounterShapeModel
Td,R,N
enableR2D2
generateSubleMotionScore:
subtleMotionScore
_block
_useR2D2
_subtleMotionScore
Tf,R,V_subtleMotionScore
initWithRequest:andConfiguration:
_frameInterval
_timeInterval
T@"VNRequest",R,N,V_request
T{?=qiIq},R,N,V_timeInterval
TQ,R,N,V_frameInterval
_numBlendshapePlusOne
_numComponents
_numIdentities
_meanBlendshape
_tensorCoeff
_componentsBlendshape
_blendshapeComponentIndex
Ti,R,V_numVertices
T^f,R,V_meanBlendshape
T^f,R,V_tensorCoeff
T^f,R,V_componentsBlendshape
T^i,R,V_blendshapeComponentIndex
keypointsFromTensor:width:height:channels:withOptions:results:
keypointsToObservation:
keypointsFromTensor:withOptions:results:
requiredInputFormat:height:format:
processFrame:withOptions:results:
_modelOutput16bit
_modelOutputSize
_loadModel
_ctx
prepareImage:
calculateOrientationResponses
generateOrientationMap
generateLineWeightMap:weightMap:
voteVanishingPoint:
searchVanishingPointandDominantLine:lineGroup:vanishingPoint:vanishingPointConfidence:dominantLine:
extractUsefulAreaFrom:to:withOffset:stridePadded:width:height:
averageOrientationResponses:withCurrentMap:
smoothFiltering:width:height:
calculateConfidence:lineDistance:vaninshingPoint:vanishingPointConfidence:
isVerticalOrHorizontal:
_orientationResponses
_orientionMap
_confidenceMap
_edgeWeightMap
_stridePadded
_offset
_validDimension
_pixelMean
_pixelVar
_gaborFilter
isScoreValid:
decideSegmentPointUsingHinkleyDetector:
isActive:
updateActiveThreshold
mergeSameTypeSegments
printSegments:
prepareTrimmingWithTrimStart:andTrimEnd:
mergeConsecutiveShortSegments
mergeSparseShortSegments
analyzeFrameWithTimeRange:andActionScore:
decideSegmentPointBasedOnActionScore:
finalizeWithDestructiveTrimStart:trimEnd:
postProcessSegmentsWithCaptureTime:trimStart:
segments
activeSegment
_activeHinkleyDetector
_activeThreshold
_postProcessStart
fetchRequestWithEntityName:
fetchRequest
assetCreationDate
T@"NSDate",&,D,N
checksum
T@"NSData",&,D,N
T@"NSString",C,D,N
embeddingType
embeddingVersion
processed
random
exceptionWithName:reason:userInfo:
unimplementedExceptionForMethodName:
mediaSubtypes
modificationDate
fingerprint
isImage
isMovie
mainFileURL
allScenes
scenes
faces
typeDescription
Tq,R,N
TQ,R,N
T@"NSDate",R,N
T@"VCPFingerprint",R,N
T@"NSString",R,N
T@"NSURL",R,N
T@"NSArray",R,N
T@"PHFetchResult",R,N
isPano
isLivePhoto
isScreenshot
isHDR
isSDOF
exif
imageWithPreferredDimension:
vcp_flashFired
vcp_scaledExposureTime
hadFlash
exposureTimeSeconds
photoOffsetSeconds
originalPhotoOffsetSeconds
Tf,R,N
isTimelapse
isSlowmo
duration
slowmoRate
slomoRange
timelapseRate
movie
streamedMovie
originalMovie
originalMovieSize
T{?={?=qiIq}{?=qiIq}},R,N
encodeObject:forKey:
decodeObjectOfClass:forKey:
initWithVertices:vertexCount:
supportsSecureCoding
encodeWithCoder:
initWithCoder:
TB,R
vertices
_vertices
TQ,R,N,V_vertexCount
Tr^,R,N
getBytes:length:
initWithTransform:blendShapes:geometry:
transform
geometry
_blendShapes
_geometry
T{?=[4]},R,N,V_transform
T@"NSDictionary",R,N,V_blendShapes
T@"VCPFaceGeometry",R,N,V_geometry
initWithFocalLengthInPixels:offline:isFastMode:
initWithFocalLengthInPixels:
initWithAnalysisTypes:withPreferredTransform:withFocalLengthInPixels:withAnalysisQueue:withTurbo:
frameFaceResults
transformForAngle:pixelBuffer:
flipTransform:
analyzeFrameForPose:withFaceRect:withTimestamp:
rotateTransform:byAngle:
analyzeFrame:withFaceRect:withRotation:withTimestamp:
isTracked
regionsOfInterest
analyzeFrameWithTimeRange:analysisData:
isReady
shouldCutAt:stillPTS:withCut:
analyzerForAnalysisTypes:withPreferredTransform:properties:
aggregateAnalysisForTypes:withFramesMeta:properties:
prewarmWithProperties:
updatePreferredTransform:properties:
analyzePixelBuffer:withTimestamp:andDuration:properties:error:
analyzePixelBuffer:withTimestamp:andDuration:properties:completion:
analyzeAudioBuffer:
aggregatedResults
_poseAnalyzer
_meshAnalyzer
_videoAnalysis
_audioAnalyzer
_faceDetector
_sceneChangeAnalyzer
_lightMotionAnalyzer
_trimAnalyzer
_homeKitMotionAnalyzer
_rotator
_rotatorForFacePose
_preferredTransform
_focalLengthInPixels
_aggregatedResults
_rotationAngleForFacePose
_preferredAngle
_preWarmed
T@"NSDictionary",R
sharedDatabaseManager
_databases
copyBlock:withStride:toBlock:
blockContentDetection:
contentAnalysis
detectPixelBuffer:contentType:
_previousContentType
_argbPixelBuffer
_argbTransferSession
sharedPhotoLibrary
systemPhotoLibraryURL
openAndWaitWithUpgrade:error:
registerAvailabilityObserver:
unregisterAvailabilityObserver:
close
closedefaultPhotoLibrary
photoLibraryDidBecomeUnavailable:
defaultPhotoLibrary
_defaultPhotoLibraryURL
_defaultPhotoLibrary
fileSize
assetLocalIdentifier
mutableBytes
initWithBytesNoCopy:length:deallocator:
dataWithLength:
setDownloadIsTransient:
setProgressHandler:
_reportDownload:
cancelDataRequest:
maxSizeBytes
requestDownloadOfResource:
setCancel:
_mutex
_semaphore
_dataTask
T@?,C,N,V_cancel
initWithURLAsset:withOptions:analysisTypes:progressHandler:completionHandler:
_assetURL
_pairedAssetURL
_progressHandler
vcp_isMercuryBase64
characterSetWithCharactersInString:
rangeOfCharacterFromSet:options:
stringByReplacingOccurrencesOfString:withString:options:range:
initWithBase64EncodedString:options:
initWithUUIDBytes:
localIdentifierWithUUID:
vcp_mercuryBase64ToLocalIdentifier
initWithLocalIdentifier:andTaskID:andStatus:andAttempts:andNextRetryDate:
taskID
attempts
nextRetryDate
_taskID
_attempts
_nextRetryDate
TQ,R,N,V_taskID
T@"NSString",R,N,V_localIdentifier
TQ,R,N,V_status
TQ,R,N,V_attempts
T@"NSDate",R,N,V_nextRetryDate
createInput:keypoints:
getDetectionScore:
planDestroy
gestureDetection:score:
T@"VCPProtoTime",&,N,V_timestamp
initWithClientBundleID:andClientTeamID:
initWithPixelBuffer:orientation:andIdentifier:clientBundleID:clientTeamID:
_identifier
_documentObservations
initWithURL:identifier:clientBundleID:clientTeamID:
_url
vcp_needFaceProcessing
vcp_typeDescription
type
purgeAllResources
characterRecognitionProperties
algorithmVersion
getTranscript
changeRequestForAsset:
setCharacterRecognitionData:algorithmVersion:adjustmentVersion:
performChanges:completionHandler:
initWithPhotosAsset:clientBundleID:clientTeamID:
initWithPhotosAsset:pixelBuffer:orientation:clientBundleID:clientTeamID:
nsfwClassifications
thumbnailResource
_resources
assetWithPixelBuffer:orientation:identifier:clientBundleID:clientTeamID:
assetWithURL:identifier:clientBundleID:clientTeamID:
assetWithPhotosAsset:clientBundleID:clientTeamID:
assetWithPhotosAsset:pixelBuffer:orientation:clientBundleID:clientTeamID:
clientBundleID
clientTeamID
_clientBundleID
_clientTeamID
T@"NSString",R,N,V_clientBundleID
T@"NSString",R,N,V_clientTeamID
T@"CLLocation",R,N
T@"NSArray",C,N,V_documentObservations
outputBeforeFc
loadAnalysisResults:audioResults:
_summaryResults
_analysisInput
_skip
promoteUnverifiedPersonsWithUpdateBlock:
evaluatePersonPromoterWithUpdateBlock:
initUsingLightweight:aspectRatio:numLevels:startLevel:
allocateInputAndOutputBuffers
inputWidth
inputHeight
copyImage:toBuffer:withChannels:
createInput:withImage:modelInputHeight:modelInputWidth:
extractFeaturesFromFirst:andSecond:
estimateMotionFlow:
scaleFlowTo:inFlow:
releaseInputAndOutputBuffers
releaseMemory
_startLevel
_firstBuffer
_secondBuffer
_outputFlow
addClassification:
classificationsCount
clearClassifications
classificationAtIndex:
classificationType
classifications
setClassifications:
_classifications
T@"NSMutableArray",&,N,V_classifications
frameProcessedByVideoAnalyzer
cameraMotionScore
subjectActionScore
colorfulnessScore
subMbMotionAvailable
faceArea
setFaceArea:
frameProcessedByHumanAnalyzer
setFrameProcessedByHumanAnalyzer:
frameProcessedByFaceDetector
setFrameProcessedByFaceDetector:
setDetectedFaces:
_frameProcessedByVideoAnalyzer
_subMbMotionAvailable
_frameProcessedByHumanAnalyzer
_frameProcessedByFaceDetector
_cameraMotionScore
_subjectActionScore
_colorfulnessScore
_frameExpressionScore
_faceArea
_detectedFaces
_videoActivityDescriptor
TB,N,V_frameProcessedByVideoAnalyzer
Tf,N,V_cameraMotionScore
Tf,N,V_subjectActionScore
Tf,N,V_interestingnessScore
Tf,N,V_obstructionScore
Tf,N,V_colorfulnessScore
TB,N,V_subMbMotionAvailable
Tf,N,V_frameExpressionScore
Tf,N,V_faceArea
TB,N,V_frameProcessedByHumanAnalyzer
TB,N,V_frameProcessedByFaceDetector
T@"NSMutableArray",&,N,V_detectedFaces
T@"VCPVideoActivityDescriptor",&,N,V_videoActivityDescriptor
requestAnalysis:ofFragmentData:withRequestID:properties:andReply:
requestAnalysis:ofFragmentSurface:withRequestID:properties:andReply:
requestIdentification:forFaceCrop:withOptions:andReply:
requestResidentMaintenance:withOptions:andReply:
expectedClasses
requestAnalysis:ofAssetData:withProperties:progressHandler:andCompletionHandler:
requestAnalysis:ofAssetSurface:withProperties:progressHandler:andCompletionHandler:
taskWithFaceCrop:andCompletionHandler:
requestIdentificationForFaceCrop:withOptions:andCompletionHandler:
requestResidentMaintenanceWithOptions:andCompletionHandler:
initWithCGImage:options:session:
URLWithString:
resourceLoader
setDelegate:queue:
contentInformationRequest
setContentType:
setContentLength:
setByteRangeAccessSupported:
dataRequest
requestedOffset
requestedLength
respondWithData:
finishLoading
resourceLoader:shouldWaitForLoadingOfRequestedResource:
resourceLoader:shouldWaitForRenewalOfRequestedResource:
resourceLoader:didCancelLoadingRequest:
resourceLoader:shouldWaitForResponseToAuthenticationChallenge:
resourceLoader:didCancelAuthenticationChallenge:
setReachabilityForFlags:update:
_reachability
_hasWifiOrEthernetConnection
TB,R,N,V_hasWifiOrEthernetConnection
appendString:
handler
_handler
stagedText
conversationIdentifier
classifyPixelBuffer:stagedText:inConversationWithIdentifier:error:
initWithIsSensitive:andAttributes:
setUpdateBlock:
updateBlock
cancelerWithUpdateBlock:
canceled
setCanceled:
_updateBlock
TB,N,V_canceled
T@?,C,N,V_updateBlock
initWithFace:image:
descriptorForFace:image:
face
image
_face
enumerateObjectsAtIndexes:options:usingBlock:
imageRectForNormalizedRect:
groupingIdentifier
newFaceCropFromImageURL:withFaceRect:groupingIdentifier:error:
newFaceCropFromImageData:withFaceRect:groupingIdentifier:error:
normalizedFaceRect
_faceCropDataForImage:andNormalizedFaceRect:error:
_generateFaceCropWithDescriptor:andCancelBlock:error:
_reportCancellationOfRemainingFaceCropSourceDescriptors:withStartingIndex:andFailureBlock:
generateFaceCropsFromSourceDescriptors:withProgressBlock:andFailureBlock:andCancelBlock:
phFacesFromVCPPhotosFaces:withFetchOptions:
deleteFaces:
phFaceFromVCPPhotosFace:withFetchOptions:
creationRequestForFace
assignPropertiesOfVCPPhotosFace:toPHFaceChangeRequest:
placeholderForCreatedFace
setFaceAdjustmentVersion:
manual
creationRequestsForFaceCropsWithOriginatingFace:resourceData:
fetchFaceCropsWithLocalIdentifiers:options:
fetchFacesForFaceCrop:options:
isValidFaceCrop:
faceBoundsFromFaceCrop:error:
initWithData:orientation:options:
_bestFaceForFaceDetectionRequest:withRect:
faceObservationWithRequestRevision:boundingBox:andAlignedBoundingBox:
setForceFaceprintCreation:
changeRequestForFaceCrop:
setFace:
_faceFromFaceCrop:error:
_clearDirtyStateOnFaceCrops:error:
_associateFace:withFaceCrop:error:
version
initWithFaceprintData:faceprintVersion:
faceprintVersion
setFaceAlgorithmVersion:
_updateFaceprint:withFace:error:
fetchFaceGroupsWithFace:options:
_faceAssociatedWithFaceCrop:
_generateAndAssociateFaceprintedFaceForFaceCrop:error:
_updateFace:withFaceCrop:error:
_recordNeedToPersonBuildOnFaceGroupContainingFace:error:
resourceData
_persistGeneratedFaceCrops:forAsset:error:
fetchFaceCropsNeedingFaceDetectionWithOptions:
_vcpFaceCropFromPHFaceCrop:
_processDirtyFaceCrop:error:
initWithPhotoLibrary:andContext:
_persistFaceAnalysis:forPHAsset:
generateAndPersistFaceCropsForFaces:withAsset:andImage:error:
processDirtyFaceCropsWithCancelBlock:andExtendTimeoutBlock:
_faceAnalyzer
assetResourcesForAsset:includeDerivatives:
resources
vcp_originalResource
fetchSceneClassificationsGroupedByAssetLocalIdentifierForAssets:
sceneNameFromSceneId:
_cachedResources
_onceExif
_cachedExif
_onceScenes
_cachedScenes
vcp_isDecodable
vcp_exifFromImageURL:
vcp_fileSize
photoIrisProperties
photoIrisStillDisplayTime
vcp_originalVideoResource
isLocallyAvailable
assetWithURL:
vcp_livePhotoStillDisplayTime
vcp_getFpsRate
vcp_adjustmentsResource
initWithURL:
hasSlowMotionAdjustments
slowMotionTimeRange
vcp_hasAdjustments:
vcp_avAsset
vcp_hasLocalSlowmo:
vcp_assetWithoutAdjustments:duration:
vcp_smallMovieDerivativeResource
assetWithData:
detector:forceCPU:sharedModel:inputConfig:revision:
detector:sharedModel:modelName:
replaceObjectAtIndex:withObject:
handsDetection:handsRegions:cancel:
classIndex
handKeypointsDetection:box:keypoints:keypointConfidence:forGFT:
addKeypointsToNSArray:keypointConfidence:handBox:keypointsArray:
fastSignLanguageDetection:ofPixelBuffer:withMetadata:
priorityAnalysis
calculatePriorityScore:ofPixelBuffer:withMetadata:
_prevComputedScore
_rotationAngle
_frameCounter
_dominantHand
_handChiralityCounter
_handDetectedInPreviousFrame
_handsDetector
_handsKeypointsDetector
_fastGestureDetector
_prevFrameHandKeypoint
_prevTimeStampHandDetected
_prevTimeSignLanguageDetected
_frameEndTimeStamp
_frameStartTimeStamp
_classIndexTracker
_handKeypointTracker
_leftHandKeypointTracker
_rightHandKeypointTracker
_singleFrameExecutionTime
_prevHandCenter
addImageBlurResults:
addImageCompositionResults:
addImageFaceResults:
addImageFeatureResults:
addImageJunkResults:
addImageSaliencyResults:
addImageShotTypeResults:
addLivePhotoRecommendationResults:
addLivePhotoSharpnessResults:
addMovieActivityLevelResults:
addMovieCameraMotionResults:
addMovieClassificationResults:
addMovieFaceResults:
addMovieFaceprintResults:
addMovieFeatureResults:
addMovieFineSubjectMotionResults:
addMovieInterestingnessResults:
addMovieMovingObjectResults:
addMovieMusicResults:
addMovieObstructionResults:
addMovieOrientationResults:
addMoviePreEncodeResults:
addMovieQualityResults:
addMovieSaliencyResults:
addMovieSceneResults:
addMovieSubjectMotionResults:
addMovieUtteranceResults:
addMovieVoiceResults:
addImagePetsResults:
addMovieSummaryResults:
addMovieHighlightResults:
addImageExposureResults:
addLivePhotoEffectsResults:
addImagePetsFaceResults:
addImageSceneprintResults:
addMovieSceneprintResults:
addImageHumanPoseResults:
addMovieHumanPoseResults:
addMovieApplauseResults:
addMovieBabbleResults:
addMovieCheeringResults:
addMovieLaughterResults:
addLivePhotoKeyFrameResults:
addLivePhotoKeyFrameStillResults:
addMovieHumanActionResults:
addMovieSubtleMotionResults:
addMovieLoudnessResults:
addMoviePetsResults:
addMoviePetsFaceResults:
addMovieStabilizationResults:
addMovieHighlightScoreResults:
addLivePhotoHumanActionClassificationResults:
setAssetIdentifier:
setAssetMasterFingerprint:
setAssetAdjustedFingerprint:
imageBlurResultsCount
clearImageBlurResults
imageBlurResultsAtIndex:
imageCompositionResultsCount
clearImageCompositionResults
imageCompositionResultsAtIndex:
imageFaceResultsCount
clearImageFaceResults
imageFaceResultsAtIndex:
imageFeatureResultsCount
clearImageFeatureResults
imageFeatureResultsAtIndex:
imageJunkResultsCount
clearImageJunkResults
imageJunkResultsAtIndex:
imageSaliencyResultsCount
clearImageSaliencyResults
imageSaliencyResultsAtIndex:
imageShotTypeResultsCount
clearImageShotTypeResults
imageShotTypeResultsAtIndex:
livePhotoRecommendationResultsCount
clearLivePhotoRecommendationResults
livePhotoRecommendationResultsAtIndex:
livePhotoSharpnessResultsCount
clearLivePhotoSharpnessResults
livePhotoSharpnessResultsAtIndex:
movieActivityLevelResultsCount
clearMovieActivityLevelResults
movieActivityLevelResultsAtIndex:
movieCameraMotionResultsCount
clearMovieCameraMotionResults
movieCameraMotionResultsAtIndex:
movieClassificationResultsCount
clearMovieClassificationResults
movieClassificationResultsAtIndex:
movieFaceResultsCount
clearMovieFaceResults
movieFaceResultsAtIndex:
movieFaceprintResultsCount
clearMovieFaceprintResults
movieFaceprintResultsAtIndex:
movieFeatureResultsCount
clearMovieFeatureResults
movieFeatureResultsAtIndex:
movieFineSubjectMotionResultsCount
clearMovieFineSubjectMotionResults
movieFineSubjectMotionResultsAtIndex:
movieInterestingnessResultsCount
clearMovieInterestingnessResults
movieInterestingnessResultsAtIndex:
movieMovingObjectResultsCount
clearMovieMovingObjectResults
movieMovingObjectResultsAtIndex:
movieMusicResultsCount
clearMovieMusicResults
movieMusicResultsAtIndex:
movieObstructionResultsCount
clearMovieObstructionResults
movieObstructionResultsAtIndex:
movieOrientationResultsCount
clearMovieOrientationResults
movieOrientationResultsAtIndex:
moviePreEncodeResultsCount
clearMoviePreEncodeResults
moviePreEncodeResultsAtIndex:
movieQualityResultsCount
clearMovieQualityResults
movieQualityResultsAtIndex:
movieSaliencyResultsCount
clearMovieSaliencyResults
movieSaliencyResultsAtIndex:
movieSceneResultsCount
clearMovieSceneResults
movieSceneResultsAtIndex:
movieSubjectMotionResultsCount
clearMovieSubjectMotionResults
movieSubjectMotionResultsAtIndex:
movieUtteranceResultsCount
clearMovieUtteranceResults
movieUtteranceResultsAtIndex:
movieVoiceResultsCount
clearMovieVoiceResults
movieVoiceResultsAtIndex:
imagePetsResultsCount
clearImagePetsResults
imagePetsResultsAtIndex:
movieSummaryResultsCount
clearMovieSummaryResults
movieSummaryResultsAtIndex:
movieHighlightResultsCount
clearMovieHighlightResults
movieHighlightResultsAtIndex:
imageExposureResultsCount
clearImageExposureResults
imageExposureResultsAtIndex:
livePhotoEffectsResultsCount
clearLivePhotoEffectsResults
livePhotoEffectsResultsAtIndex:
imagePetsFaceResultsCount
clearImagePetsFaceResults
imagePetsFaceResultsAtIndex:
imageSceneprintResultsCount
clearImageSceneprintResults
imageSceneprintResultsAtIndex:
movieSceneprintResultsCount
clearMovieSceneprintResults
movieSceneprintResultsAtIndex:
imageHumanPoseResultsCount
clearImageHumanPoseResults
imageHumanPoseResultsAtIndex:
movieHumanPoseResultsCount
clearMovieHumanPoseResults
movieHumanPoseResultsAtIndex:
movieApplauseResultsCount
clearMovieApplauseResults
movieApplauseResultsAtIndex:
movieBabbleResultsCount
clearMovieBabbleResults
movieBabbleResultsAtIndex:
movieCheeringResultsCount
clearMovieCheeringResults
movieCheeringResultsAtIndex:
movieLaughterResultsCount
clearMovieLaughterResults
movieLaughterResultsAtIndex:
livePhotoKeyFrameResultsCount
clearLivePhotoKeyFrameResults
livePhotoKeyFrameResultsAtIndex:
livePhotoKeyFrameStillResultsCount
clearLivePhotoKeyFrameStillResults
livePhotoKeyFrameStillResultsAtIndex:
movieHumanActionResultsCount
clearMovieHumanActionResults
movieHumanActionResultsAtIndex:
movieSubtleMotionResultsCount
clearMovieSubtleMotionResults
movieSubtleMotionResultsAtIndex:
movieLoudnessResultsCount
clearMovieLoudnessResults
movieLoudnessResultsAtIndex:
moviePetsResultsCount
clearMoviePetsResults
moviePetsResultsAtIndex:
moviePetsFaceResultsCount
clearMoviePetsFaceResults
moviePetsFaceResultsAtIndex:
movieStabilizationResultsCount
clearMovieStabilizationResults
movieStabilizationResultsAtIndex:
movieHighlightScoreResultsCount
clearMovieHighlightScoreResults
movieHighlightScoreResultsAtIndex:
livePhotoHumanActionClassificationResultsCount
clearLivePhotoHumanActionClassificationResults
livePhotoHumanActionClassificationResultsAtIndex:
imageBlurResultsType
imageCompositionResultsType
imageFaceResultsType
imageFeatureResultsType
imageJunkResultsType
imageSaliencyResultsType
imageShotTypeResultsType
imagePetsResultsType
imagePetsFaceResultsType
imageSceneprintResultsType
livePhotoEffectsResultsType
livePhotoRecommendationResultsType
livePhotoSharpnessResultsType
livePhotoKeyFrameResultsType
livePhotoKeyFrameStillResultsType
movieActivityLevelResultsType
movieCameraMotionResultsType
movieClassificationResultsType
movieFaceResultsType
movieFaceprintResultsType
movieFeatureResultsType
movieFineSubjectMotionResultsType
movieInterestingnessResultsType
movieMovingObjectResultsType
movieMusicResultsType
movieObstructionResultsType
movieOrientationResultsType
moviePreEncodeResultsType
movieQualityResultsType
movieSaliencyResultsType
movieSceneResultsType
movieSceneprintResultsType
movieSubjectMotionResultsType
movieSubtleMotionResultsType
movieUtteranceResultsType
movieVoiceResultsType
movieSummaryResultsType
movieHighlightResultsType
imageExposureResultsType
imageHumanPoseResultsType
movieHumanPoseResultsType
movieApplauseResultsType
movieBabbleResultsType
movieCheeringResultsType
movieLaughterResultsType
movieHumanActionResultsType
movieLoudnessResultsType
moviePetsResultsType
moviePetsFaceResultsType
movieStabilizationResultsType
movieHighlightScoreResultsType
livePhotoHumanActionClassificationResultsType
setHasQuality:
hasQuality
setStatsFlags:
setHasStatsFlags:
hasStatsFlags
setTypesWide:
setHasTypesWide:
hasTypesWide
hasAssetAdjustedFingerprint
setVersion:
types
setTypes:
setDate:
statsFlags
typesWide
assetIdentifier
assetModificationDate
setAssetModificationDate:
assetMasterFingerprint
assetAdjustedFingerprint
imageBlurResults
setImageBlurResults:
imageCompositionResults
setImageCompositionResults:
imageFaceResults
setImageFaceResults:
imageFeatureResults
setImageFeatureResults:
imageJunkResults
setImageJunkResults:
imageSaliencyResults
setImageSaliencyResults:
imageShotTypeResults
setImageShotTypeResults:
imagePetsResults
setImagePetsResults:
imagePetsFaceResults
setImagePetsFaceResults:
imageSceneprintResults
setImageSceneprintResults:
livePhotoEffectsResults
setLivePhotoEffectsResults:
livePhotoRecommendationResults
setLivePhotoRecommendationResults:
livePhotoSharpnessResults
setLivePhotoSharpnessResults:
livePhotoKeyFrameResults
setLivePhotoKeyFrameResults:
livePhotoKeyFrameStillResults
setLivePhotoKeyFrameStillResults:
movieActivityLevelResults
setMovieActivityLevelResults:
movieCameraMotionResults
setMovieCameraMotionResults:
movieClassificationResults
setMovieClassificationResults:
movieFaceResults
setMovieFaceResults:
movieFaceprintResults
setMovieFaceprintResults:
movieFeatureResults
setMovieFeatureResults:
movieFineSubjectMotionResults
setMovieFineSubjectMotionResults:
movieInterestingnessResults
setMovieInterestingnessResults:
movieMovingObjectResults
setMovieMovingObjectResults:
movieMusicResults
setMovieMusicResults:
movieObstructionResults
setMovieObstructionResults:
movieOrientationResults
setMovieOrientationResults:
moviePreEncodeResults
setMoviePreEncodeResults:
movieQualityResults
setMovieQualityResults:
movieSaliencyResults
setMovieSaliencyResults:
movieSceneResults
setMovieSceneResults:
movieSceneprintResults
setMovieSceneprintResults:
movieSubjectMotionResults
setMovieSubjectMotionResults:
movieSubtleMotionResults
setMovieSubtleMotionResults:
movieUtteranceResults
setMovieUtteranceResults:
movieVoiceResults
setMovieVoiceResults:
movieSummaryResults
setMovieSummaryResults:
movieHighlightResults
setMovieHighlightResults:
imageExposureResults
setImageExposureResults:
imageHumanPoseResults
setImageHumanPoseResults:
movieHumanPoseResults
setMovieHumanPoseResults:
movieApplauseResults
setMovieApplauseResults:
movieBabbleResults
setMovieBabbleResults:
movieCheeringResults
setMovieCheeringResults:
movieLaughterResults
setMovieLaughterResults:
movieHumanActionResults
setMovieHumanActionResults:
movieLoudnessResults
setMovieLoudnessResults:
moviePetsResults
setMoviePetsResults:
moviePetsFaceResults
setMoviePetsFaceResults:
movieStabilizationResults
setMovieStabilizationResults:
movieHighlightScoreResults
setMovieHighlightScoreResults:
livePhotoHumanActionClassificationResults
setLivePhotoHumanActionClassificationResults:
_assetModificationDate
_date
_quality
_statsFlags
_typesWide
_assetAdjustedFingerprint
_assetIdentifier
_assetMasterFingerprint
_imageBlurResults
_imageCompositionResults
_imageExposureResults
_imageFaceResults
_imageFeatureResults
_imageHumanPoseResults
_imageJunkResults
_imagePetsFaceResults
_imagePetsResults
_imageSaliencyResults
_imageSceneprintResults
_imageShotTypeResults
_livePhotoEffectsResults
_livePhotoHumanActionClassificationResults
_livePhotoKeyFrameResults
_livePhotoKeyFrameStillResults
_livePhotoRecommendationResults
_livePhotoSharpnessResults
_movieActivityLevelResults
_movieApplauseResults
_movieBabbleResults
_movieCameraMotionResults
_movieCheeringResults
_movieClassificationResults
_movieFaceResults
_movieFaceprintResults
_movieFeatureResults
_movieFineSubjectMotionResults
_movieHighlightResults
_movieHighlightScoreResults
_movieHumanActionResults
_movieHumanPoseResults
_movieInterestingnessResults
_movieLaughterResults
_movieLoudnessResults
_movieMovingObjectResults
_movieMusicResults
_movieObstructionResults
_movieOrientationResults
_moviePetsFaceResults
_moviePetsResults
_moviePreEncodeResults
_movieQualityResults
_movieSaliencyResults
_movieSceneResults
_movieSceneprintResults
_movieStabilizationResults
_movieSubjectMotionResults
_movieSubtleMotionResults
_movieSummaryResults
_movieUtteranceResults
_movieVoiceResults
_types
_version
TI,N,V_version
TI,N,V_types
TI,N,V_flags
Td,N,V_date
Td,N,V_quality
TQ,N,V_statsFlags
TQ,N,V_typesWide
T@"NSString",&,N,V_assetIdentifier
Td,N,V_assetModificationDate
T@"NSString",&,N,V_assetMasterFingerprint
T@"NSString",&,N,V_assetAdjustedFingerprint
T@"NSMutableArray",&,N,V_imageBlurResults
T@"NSMutableArray",&,N,V_imageCompositionResults
T@"NSMutableArray",&,N,V_imageFaceResults
T@"NSMutableArray",&,N,V_imageFeatureResults
T@"NSMutableArray",&,N,V_imageJunkResults
T@"NSMutableArray",&,N,V_imageSaliencyResults
T@"NSMutableArray",&,N,V_imageShotTypeResults
T@"NSMutableArray",&,N,V_imagePetsResults
T@"NSMutableArray",&,N,V_imagePetsFaceResults
T@"NSMutableArray",&,N,V_imageSceneprintResults
T@"NSMutableArray",&,N,V_livePhotoEffectsResults
T@"NSMutableArray",&,N,V_livePhotoRecommendationResults
T@"NSMutableArray",&,N,V_livePhotoSharpnessResults
T@"NSMutableArray",&,N,V_livePhotoKeyFrameResults
T@"NSMutableArray",&,N,V_livePhotoKeyFrameStillResults
T@"NSMutableArray",&,N,V_movieActivityLevelResults
T@"NSMutableArray",&,N,V_movieCameraMotionResults
T@"NSMutableArray",&,N,V_movieClassificationResults
T@"NSMutableArray",&,N,V_movieFaceResults
T@"NSMutableArray",&,N,V_movieFaceprintResults
T@"NSMutableArray",&,N,V_movieFeatureResults
T@"NSMutableArray",&,N,V_movieFineSubjectMotionResults
T@"NSMutableArray",&,N,V_movieInterestingnessResults
T@"NSMutableArray",&,N,V_movieMovingObjectResults
T@"NSMutableArray",&,N,V_movieMusicResults
T@"NSMutableArray",&,N,V_movieObstructionResults
T@"NSMutableArray",&,N,V_movieOrientationResults
T@"NSMutableArray",&,N,V_moviePreEncodeResults
T@"NSMutableArray",&,N,V_movieQualityResults
T@"NSMutableArray",&,N,V_movieSaliencyResults
T@"NSMutableArray",&,N,V_movieSceneResults
T@"NSMutableArray",&,N,V_movieSceneprintResults
T@"NSMutableArray",&,N,V_movieSubjectMotionResults
T@"NSMutableArray",&,N,V_movieSubtleMotionResults
T@"NSMutableArray",&,N,V_movieUtteranceResults
T@"NSMutableArray",&,N,V_movieVoiceResults
T@"NSMutableArray",&,N,V_movieSummaryResults
T@"NSMutableArray",&,N,V_movieHighlightResults
T@"NSMutableArray",&,N,V_imageExposureResults
T@"NSMutableArray",&,N,V_imageHumanPoseResults
T@"NSMutableArray",&,N,V_movieHumanPoseResults
T@"NSMutableArray",&,N,V_movieApplauseResults
T@"NSMutableArray",&,N,V_movieBabbleResults
T@"NSMutableArray",&,N,V_movieCheeringResults
T@"NSMutableArray",&,N,V_movieLaughterResults
T@"NSMutableArray",&,N,V_movieHumanActionResults
T@"NSMutableArray",&,N,V_movieLoudnessResults
T@"NSMutableArray",&,N,V_moviePetsResults
T@"NSMutableArray",&,N,V_moviePetsFaceResults
T@"NSMutableArray",&,N,V_movieStabilizationResults
T@"NSMutableArray",&,N,V_movieHighlightScoreResults
T@"NSMutableArray",&,N,V_livePhotoHumanActionClassificationResults
setAttributesFromLegacyDictionary:
setResults:withClass:forPropertyKey:
exportResultsWithPropertyKey:toLegacyDictionary:withKey:
imageAnalysisFromLegacyDictionary:
movieAnalysisFromLegacyDictionary:
setX0:
setY0:
setWidth:
setHeight:
Td,N,V_x0
Td,N,V_y0
Td,N,V_width
Td,N,V_height
sharedModel:inputNames:
prepareModelWithAspectRatio:
creatModel
createInput:withBuffer:cnnInputHeight:cnnInputWidth:
analyzeImages:secondImage:cancel:
getFlowWithHeight:andWidth:
_cnnOutputWidth
_cnnOutputHeight
initWithCommonFormat:sampleRate:channels:interleaved:
appendBuffer:atTime:error:
initRequiringSecureCoding:
encodedData
_signature
_endTime
setIdentifier:
TI,N,V_identifier
setHasFaceSharpness:
hasFaceSharpness
setVanishingPoint:
setDominantLine:
vanishingPoint
dominantLine
_dominantLine
_vanishingPoint
T@"VCPProtoPoint",&,N,V_vanishingPoint
T@"VCPProtoLine",&,N,V_dominantLine
pointWithPoint:
lineFromPoint:toPoint:
pointValue
startPointValue
endPointValue
setUnderExpose:
setHasUnderExpose:
hasUnderExpose
exposure
setExposure:
underExpose
_exposure
_underExpose
Tf,N,V_exposure
Tf,N,V_underExpose
_processFormat
_peakValues
_momentaryEnergyValues
_loudnessSampleBuffer
_loudnessResults
_samplesFor100ms
_samplesForProcessingBufferList
setHasFaceQuality:
hasFaceQuality
eyeExpression
setEyeExpression:
_eyeExpression
Ti,N,V_eyeExpression
Ti,N,V_yaw
Tf,N,V_faceQuality
setFeatureBlob:
featureBlob
_featureBlob
T@"NSData",&,N,V_featureBlob
initWithKeypointsOption:forceCPU:sharedModel:aspectRatio:modelName:revision:
associateHands:withExisingHands:
handDistance:withhandB:
_existingHands
_loadImageURL:withSession:andRequestHandler:
_configureRequest:withRevision:preferANE:
initWithImageSignatureprintType:imageSignatureHashType:
imageNeuralHashprint
setInputSignatureprint:
imageSignatureHash
descriptorData
elementCount
elementType
encodeHashDescriptorWithBase64EncodingAndReturnError:
analyzeWithImageURL:requestTypes:completionHandler:
returnObject:
initWithObject:fromPool:
_object
_pool
T@,R,N,V_object
initWithAllocator:
_allocator
_objects
textBlockWithDocumentObservations:
initWithLabel:normalizedBoundingBox:confidence:
initWithAnnotations:humanAnnotations:nsfwAnnotations:textBlockAnnotation:
shotType
setShotType:
_shotType
Ti,N,V_shotType
setEnd:
_end
T@"VCPProtoPoint",&,N,V_start
T@"VCPProtoPoint",&,N,V_end
addFrameInstructions:
frameInstructionsCount
clearFrameInstructions
frameInstructionsAtIndex:
setAutoloop:
setBounce:
setLongexposure:
setStabilize:
frameInstructionsType
setEpoch:
setHasEpoch:
hasEpoch
setHasFlags:
hasFlags
stabilizeResult
setStabilizeResult:
outputFrameDurValue
setOutputFrameDurValue:
cropRectX
setCropRectX:
cropRectY
setCropRectY:
cropRectHeight
setCropRectHeight:
cropRectWidth
setCropRectWidth:
timeScale
setTimeScale:
epoch
frameInstructions
setFrameInstructions:
autoloop
bounce
longexposure
stabilize
minVersion
setMinVersion:
_epoch
_outputFrameDurValue
_autoloop
_bounce
_cropRectHeight
_cropRectWidth
_cropRectX
_cropRectY
_frameInstructions
_longexposure
_minVersion
_stabilize
_stabilizeResult
_timeScale
Ti,N,V_stabilizeResult
Tq,N,V_outputFrameDurValue
Ti,N,V_cropRectX
Ti,N,V_cropRectY
Ti,N,V_cropRectHeight
Ti,N,V_cropRectWidth
Ti,N,V_timeScale
Tq,N,V_epoch
T@"NSMutableArray",&,N,V_frameInstructions
T@"VCPProtoLivePhotoVariationParams",&,N,V_autoloop
T@"VCPProtoLivePhotoVariationParams",&,N,V_bounce
T@"VCPProtoLivePhotoVariationParams",&,N,V_longexposure
T@"VCPProtoLivePhotoVariationParams",&,N,V_stabilize
Ti,N,V_minVersion
Ti,N,V_version
exportToLegacyDictionaryFromFrameInstruction:
exportToLegacyDictionaryFromParam:withLoopFlavor:
setRecipeBlob:
hasRecipeBlob
loopSuggestionState
setLoopSuggestionState:
longExposureSuggestionState
setLongExposureSuggestionState:
recipeBlob
_longExposureSuggestionState
_loopSuggestionState
_recipeBlob
TQ,N,V_loopSuggestionState
TQ,N,V_longExposureSuggestionState
T@"NSData",&,N,V_recipeBlob
updateWithOptions:error:
processImage:withOptions:error:
raise
homographyParamsCount
clearHomographyParams
homographyParamAtIndex:
addHomographyParam:
homographyParams
setHomographyParams:count:
timeValue
setTimeValue:
_homographyParams
_timeValue
Tq,N,V_timeValue
T^f,R,N
indexOfObject:inSortedRange:options:usingComparator:
getClosestAspectRatio:
updateModelWithResConfig:
convertSingleResultToDict:keypointConfidence:box:results:
initWithOptions:andCompletionHandler:
taskService
cancelTask:
submitTaskWithOptions:completionHandler:
taskWithOptions:andCompletionHandler:
_options
setLoopFadeLen:
setHasLoopFadeLen:
hasLoopFadeLen
setLoopPeriod:
setHasLoopPeriod:
hasLoopPeriod
setLoopStart:
setHasLoopStart:
hasLoopStart
errorCode
setErrorCode:
loopFadeLen
loopPeriod
loopStart
_errorCode
_loopFadeLen
_loopPeriod
_loopStart
Ti,N,V_errorCode
Ti,N,V_loopFadeLen
Ti,N,V_loopPeriod
Ti,N,V_loopStart
activityScore
setActivityScore:
_activityScore
Tf,N,V_activityScore
motionType
setMotionType:
isFast
setIsFast:
_motionType
_isFast
Ti,N,V_motionType
TB,N,V_isFast
startSessionWithProperties:andReply:
initWithProperties:withResultsHandler:andInterruptionHandler:
setWeakSession:
processMessageWithOptions:andReply:
processVideoFragmentAssetData:withOptions:andReply:
processResults:withReply:
allowedClasses
sessionWithProperties:andResultsHandler:
sessionWithProperties:withResultsHandler:andInterruptionHandler:
processVideoFragmentAssetData:withOptions:andErrorHandler:
processVideoFragmentAssetData:withOptions:andCompletionHandler:
processMessageWithOptions:andCompletionHandler:
_formatDescription
_resultsHandler
_interruptionHander
weakSession
_weakSession
T@"VCPHomeKitAnalysisSession",W,N,V_weakSession
calculateFrameDifference:
computeRegionsofInterest
_regions
_diff
_ptrFirst
_ptrLast
_blockSize
_widthBlockNum
_heightBlockNum
highlightScore
setHighlightScore:
_highlightScore
Tf,N,V_highlightScore
setFaceprintBlob:
faceprintBlob
_faceprintBlob
TI,N,V_faceID
T@"NSData",&,N,V_faceprintBlob
useCPUOnly
_useCPUOnly
_maxNumHands
_humanActionWindowSize
TB,R,N,V_useCPUOnly
TI,R,N,V_revision
mouthExpression
setMouthExpression:
isCloseup
setIsCloseup:
_mouthExpression
_isCloseup
Ti,N,V_mouthExpression
Ti,N,V_position
TB,N,V_isCloseup
Ti,N,V_faceID
clsDistanceIdentity
filename
locationCoordinate
T{CLLocationCoordinate2D=dd},R,N
setColorNormalizationBlob:
hasColorNormalizationBlob
colorNormalizationBlob
_colorNormalizationBlob
T@"NSData",&,N,V_colorNormalizationBlob
initWithIndexesInRange:
removeIndex:
enumerateIndexesUsingBlock:
UUID
UUIDString
initWithFaceClusterIds:clusterFlags:updateHandler:
requestWithFaceClusterIds:clusterFlags:updateHandler:
requestId
clusterFlagByClusterId
csns
cflags
updateHandler
setUpdateHandler:
canceller
_type
_requestId
_clusterFlagByClusterId
_csns
_cflags
_updateHandler
_canceller
TQ,R,V_type
T@"NSString",R,V_requestId
T@"NSMutableDictionary",R,V_clusterFlagByClusterId
T@"NSArray",R,V_csns
T@"NSArray",R,V_cflags
T@?,C,V_updateHandler
T@"VNCanceller",R,V_canceller
faceClusteringDisabled
faceClusteringThreshold
_readPropertyDictionary
_cancelClusteringAndRestoreClusterCache:
cancelAllSuggestionRequests
minFaceCountToTriggerClustering
maxFaceCountForClustering
_recordIncrementCountOfPendingFacesToAdd:
_processingQueueDetermineNextClusterTriggeringAccumulatedChangesCountIfNecessary
_recordClusteringState:
_faceTorsoprintsFromFaceCSNs:
setClustererBringUpState:
_performAndPersistClustersWithFaceTorsoprintsToAdd:groupingIdentifiersToAdd:faceTorsoprintsToRemove:updatedFaces:andError:
_faceTorsoprintsFromFaceIdentifiers:assignClusterSeqNumberIfNeeded:updatedFaces:groupingIdentifiers:
_recordCountOfPendingFacesToAdd:
needsFullSync
needsUpdate
_processingQueueSyncClustererWithPhotoLibraryUsingFacesInClusterCache:
_processingQueuePerformForcedFaceClustering:
signalCancellation
restoreClusterCacheAndSyncWithLibrary:error:
lock
unlock
updateModelByAddingPersons:withGroupingIdentifiers:andRemovingPersons:canceller:error:
clusterId
objects
_processingQueueSaveClusterCache:
scheduleClusteringAfterRemovingFaceCSNs:addingFaceIdStrs:
setPersonId:
_faceTorsoprintsFromFaces:assignClusterSeqNumberIfNeeded:updatedFaces:
_stringForVCPClustererBringUpState:
_processingQueueQuickSyncClustererWithPhotoLibraryUsingFacesInClusterCache:visionClusters:
_processingQueueGetVisionClusters:minimumClusterSize:returnClusterAsCountedSet:error:
_removeEmptyGroups
_setPropertyDictionaryValue:forKey:
_processingQueueResetClusterCache:
_recordCurrentStatus:
allClusteredFaceIdsAndReturnError:
clustererBringUpState
saveAndReturnCurrentModelState:
finishEncoding
fileSystemRepresentation
_removeVisionClusterCacheFilesNotReferencedByVisionClusterState:
_processingQueueGetFaceClusterSequenceNumbersInClusterCache:lastClusterSequenceNumber:error:
clustererModelFileNamesFromState:storedInPath:error:
initWithType:cachePath:state:threshold:torsoThreshold:requestRevision:torsoprintRequestRevision:
initWithType:cachePath:state:threshold:torsoThreshold:requestRevision:
clustererBuilderWithOptions:error:
removeClusteringStateCacheWithURL:error:
_processingQueueRestoreClusteringCacheWithCacheDirectoryURL:clusterState:threshold:error:
dataWithBytesNoCopy:length:freeWhenDone:
_visionClusterMemmapFileInCacheDirectoryURL:clusterState:error:
pathExtension
lastPathComponent
removeItemAtURL:error:
_visionClusterStateDataBlobFromClusterSnapshotFileAtURL:error:
initForReadingFromData:error:
finishDecoding
URLByDeletingLastPathComponent
_processingQueueRestoreFromClusterSnapshotFileAtURL:error:
_processingQueueRestoreClusterCacheAndSyncWithLibrary:error:
_recordClusterRebuildRequired:
clustererState
facePrimarySuggestionsThreshold
suggestionsForClustersWithFaceIds:affinityThreshold:canceller:error:
wasSignalled
l1ClusteredFaceIdsGroupedByL0ClustersForClustersContainingFaceIds:error:
usesCPUOnly
_propertyDictionaryFileURL
dateWithTimeIntervalSinceReferenceDate:
writeToURL:atomically:
clusteredFaceIdsForClusterContainingFaceId:error:
distanceBetweenClustersWithFaceId:andFaceId:error:
distanceBetweenLevel1Clusters:error:
distanceBetweenLevel0ClusterIdentifiedByFaceCSN:andLevel0ClusterIdentifiedByFaceCSN:error:
distancesFromClustersIdentifiedByFaceCSNs:toClustersIdentifiedByFaceCSNs:error:
initWithPhotoLibrary:context:extendTimeoutBlock:andCancelBlock:
terminate
numberOfAccumulatedClusterChanges
clusterIfNecessaryAndWait
clusterAndWait
performClusteringWithCompletion:
cancelClustering
suggestedFaceClusterSequenceNumbersForFaceClusterSequenceNumbersRepresentingClusters:error:
requestSuggestionsForFaceClusterSequenceNumbers:withClusteringFlags:updateHandler:error:
cancelSuggestionRequest:
isReadyToReturnSuggestions
differencesBetweenClustersInClusterCacheAndLibrary:
getClusters:threshold:utilizingGPU:error:
_persistenceDelegate
_cacheDirUrl
_cacheFileUrl
_clusteringType
_faceCSNsInClusterCache
_nextSeqNum
_faceIdStrsToAdd
_faceCSNsToRemove
_accumulatedChangesCount
_nextClusterTriggeringAccumulatedChangesCount
_visionCanceler
_clusterBuilder
_rebuildClusterer
_outstandingSuggestionRequests
_currentSuggestionRequest
_suggestionLock
_currentStatusSnapshotLock
_currentStatusSnapshot
_currentStatusSnapshotIsValid
_propertyDictionaryLock
_propertyDictionary
_clustererBringUpState
_timestampOfLastClusterComparison
_timebase
_extendTimeoutBlock
ready
TB,R,N,GisReady
TQ,N,V_clustererBringUpState
URLByAppendingPathExtension:
interestScore
setInterestScore:
_interestScore
Tf,N,V_interestScore
approximateLocation
coordinate
gpsHorizontalAccuracy
approximateCoordinate
isCoarse
startDate
endDate
estimatedAssetCount
_personKeypointsDetector
energy
setEnergy:
peak
setPeak:
_energy
_peak
Td,N,V_energy
Td,N,V_peak
addBounds:
boundsCount
clearBounds
boundsAtIndex:
boundsType
T@"NSMutableArray",&,N,V_bounds
initWithMaxNumRegions:forceCPU:sharedModel:inputConfig:revision:
createModelWithResConfig:
generateHandsRegions:boxes:maxNumRegions:
retrieveBoxes:outHeight:outWidth:boxes:anchorBox:
nonMaxSuppression:
drawLine:width:height:stride:point0:point1:drawPoint:
createInput:withBuffer:
generateHandsBoxes:
drawRectangle:width:height:stride:keypoints:
_cnnInputWidth
_cnnInputHeight
_numClass
setDateFormat:
stringForObjectValue:
stringByAppendingString:
createDirectoryAtURL:withIntermediateDirectories:attributes:error:
copyItemAtURL:toURL:error:
setIncludeHiddenAssets:
setIncludeAllBurstAssets:
_appendToSuggestionsLog:
minimumSuggestionSize
suggestionsForPersonLocalIdentifier:clusterSequenceNumbers:excludePersonLocalIdentifiers:minimumSuggestionFaceCount:
updateKeyFacesOfPersonsWithLocalIdentifiers:forceUpdate:context:reply:
_closeSuggestionsLoggingSession
_startAndSyncClusterCacheWithLibrary:reply:
_openSuggestionsLoggingSession
faceClusterSequenceNumbersOfKeyFacesInAlgorithmicFaceGroupsForPerson:verifiedClusterSequenceNumbers:
_logFaceToSuggestionsLog:
_suggestionsForPersonLocalIdentifier:clusterSequenceNumbers:excludePersonLocalIdentifiers:cancel:context:error:
_finalizeSuggestionsLog
_suggestionsForPersonWithLocalIdentifier:toBeConfirmedPersonSuggestions:toBeRejectedPersonSuggestions:cancel:error:
distantPast
vcp_setAnalysisPreferencesValue:forKey:
removeItemAtPath:error:
requestSuggestedMePersonIdentifierAtURL:withError:
hasProcessedForLibrary:
initWithPhotoLibrary:andDelegate:
advancedStatus
setProcessed:forLibrary:
_deleteAllVerifiedPersonsWithError:
reclusterFacesWithThreshold:shouldRecluster:withContext:extendTimeout:cancel:error:
workerWithPhotoLibrary:andContext:
_copyImageAtURLToSuggestionsLoggingSession:
suggestPersonsForPersonWithLocalIdentifier:toBeConfirmedPersonSuggestions:toBeRejectedPersonSuggestions:context:reply:cancel:
faceCandidatesforKeyFaceForPersonsWithLocalIdentifiers:context:reply:
resetPersonsModelWithReply:
resetPetsModelWithReply:
requestSuggestedMePersonIdentifierWithContext:reply:
personPromoterStatusWithContext:reply:
validateClusterCacheWithContext:reply:
resetFaceClusteringStateWithContext:reply:
reclusterFacesWithContext:reply:extendTimeout:cancel:
rebuildPersonsWithContext:reply:extendTimeout:cancel:
_clusterer
_suggestionLoggingDirectory
_suggestionLoggingSessionOpen
_suggestionsLoggingEnabled
indexesOfObjectsPassingTest:
removeObjectsAtIndexes:
vcp_reportDownload:withTaskID:
vcp_inMemoryDownload:withTaskID:toData:cancel:
initWithRequests:forAsset:cancelBlock:andCompletionHandler:
vcp_taskWithImageAsset:andSignpostPayload:
taskWithRequests:forAsset:cancelBlock:andCompletionHandler:
signpostPayload
setSignpostPayload:
_requests
_subtasks
T@"NSString",&,N,V_signpostPayload
contextWithPhotoLibrary:
setFaceClusteringThreshold:
faceClusteringJunkThreshold
setFaceClusteringJunkThreshold:
faceClusteringAgeThreshold
setFaceClusteringAgeThreshold:
faceMergeFaceprintDistanceThreshold
setFaceMergeFaceprintDistanceThreshold:
setFacePrimarySuggestionsThreshold:
setMinimumSuggestionSize:
setQuarantineTwinsOnAssetEnabled:
setMinFaceCountToTriggerClustering:
setMaxFaceCountForClustering:
suggestionsLogEnabled
setSuggestionsLogEnabled:
setFaceClusteringDisabled:
setMinimumFaceGroupSizeForCreatingMergeCandidates:
setPersonBuildingDisabled:
advancedStatusMergeCandidateLimit
setAdvancedStatusMergeCandidateLimit:
advancedStatusVerifiedPersonLimit
setAdvancedStatusVerifiedPersonLimit:
clusterIncludeTorsoOnlyFaces
setClusterIncludeTorsoOnlyFaces:
setProcessingVersion:
_quarantineTwinsOnAssetEnabled
_suggestionsLogEnabled
_faceClusteringDisabled
_personBuildingDisabled
_clusterIncludeTorsoOnlyFaces
_faceClusteringThreshold
_faceClusteringJunkThreshold
_faceClusteringAgeThreshold
_faceMergeFaceprintDistanceThreshold
_facePrimarySuggestionsThreshold
_minimumSuggestionSize
_minFaceCountToTriggerClustering
_maxFaceCountForClustering
_minimumFaceGroupSizeForCreatingMergeCandidates
_advancedStatusMergeCandidateLimit
_advancedStatusVerifiedPersonLimit
Tf,V_faceClusteringThreshold
Tf,V_faceClusteringJunkThreshold
Tf,V_faceClusteringAgeThreshold
Tf,V_faceMergeFaceprintDistanceThreshold
Tf,V_facePrimarySuggestionsThreshold
TQ,V_minimumSuggestionSize
TB,V_quarantineTwinsOnAssetEnabled
TQ,V_minFaceCountToTriggerClustering
TQ,V_maxFaceCountForClustering
TB,V_suggestionsLogEnabled
TB,V_faceClusteringDisabled
TQ,V_minimumFaceGroupSizeForCreatingMergeCandidates
TB,V_personBuildingDisabled
TB,V_personBuilderMergeCandidatesDisabled
TQ,V_advancedStatusMergeCandidateLimit
TQ,V_advancedStatusVerifiedPersonLimit
TB,V_clusterIncludeTorsoOnlyFaces
Ti,V_processingVersion
setCenterX:
setCenterY:
poseYaw
faceAlgorithmVersion
isHidden
setHidden:
isInTrash
setIsInTrash:
setAdjustmentVersion:
setTrainingType:
setGroupingIdentifier:
_vnFaceAttributeAgeToPHFaceAgeTypeMap
_vnFaceAttributeSexToPHFaceSexTypeMap
_vnFaceAttributeEyesToPHEyesStateMap
_vnFaceAttributeSmileToPHFaceSmileTypeMap
_vnFaceAttributeFacialHairToPHFacialHairTypeMap
_vnFaceAttributeHairColorToPHFaceHairColorTypeMap
_vnFaceAttributeGlassesToPHFaceGlassesTypeMap
_vnFaceAttributeFacialHairToPHFaceExpressionType
_vnFaceAttributeHeadGearToPHFaceHeadGearType
_vnFaceAttributeHairTypeToPHFaceHairType
_vnFaceAttributePoseToPHFacePoseType
_vnFaceAttributeSkintoneToPHFaceSkintoneType
_vnFaceAttributeEthnicityToPHFaceEthnicityType
_vnFaceGazeDirectionToPHFaceGazeType
ageType
sexType
smileType
facialHairType
hairColorType
glassesType
expressionType
setFaceExpressionType:
headgearType
hairType
poseType
skintoneType
ethnicityType
hidden
setInTrash:
analysisType
_firstLocallyAvailableResourceFromResources:
vcp_isSyndicationLibrary
vcp_descendingSizeComparator
preferredResourcesForFaceProcessingWithAsset:
resourceForFaceProcessing:allowStreaming:
_vcpFacesArrayFromPHFetchResult:copyPropertiesOption:
_readFaceAnalysisState
vcp_faceAnalysisStateFilepath
initWithContentsOfFile:
writeToFile:atomically:
_setFaceAnalysisStateValue:forKey:
setChunkSizeForFetch:
setLastMinimumFaceGroupSizeForCreatingMergeCandidate:
pv_faceProcessingProgress
_setAllFaceGroupsNeedPersonBuilding
setPersonBuilderMergeCandidatesEnabled:
clusterer
initWithPhotoLibrary:andFaceClusterer:andContext:
performPersonBuildingWithCancelOrExtendTimeoutBlock:error:
_faceClusterer
_lastMinimumFaceGroupSizeForCreatingMergeCandidates
_personBuilderMergeCandidatesEnabled
computeVar:index2:interVar:intraVar:
scaleRect:scaleX:scaleY:
computeActionScore
associatePerson:withPHFaces:
intersectionOverUnion:rect:
addActiveResults:
processPersons:humanBounds:dominantPersonIdx:frame:timestamp:duration:
_timeLastProcessFullFrame
_bodyArray
_maxScore
_keyPersonResults
_poseResults
_activePoseResults
_crop
_humanRect
_actionScoreAbsolute
_actionScoreRelative
_scoreAbsoluteMax
_scoreRelativeMax
_lastHumanTimestamp
_tracker
_tracking
_timeOfInterest
initWithAssets:andCompletionHandler:
vcp_isPano
_panoVNRequestMethod
initWithURL:options:
Ti,N,V_orientation
setStatisticsBlob:
statisticsBlob
_statisticsBlob
T@"NSData",&,N,V_statisticsBlob
setDistanceToPreviousScene:
setHasDistanceToPreviousScene:
hasDistanceToPreviousScene
setFlickerScore:
setHasFlickerScore:
hasFlickerScore
setSceneprintDistanceToPreviousScene:
setHasSceneprintDistanceToPreviousScene:
hasSceneprintDistanceToPreviousScene
distanceToPreviousScene
flickerScore
sceneprintDistanceToPreviousScene
_distanceToPreviousScene
_flickerScore
_sceneprintDistanceToPreviousScene
Tf,N,V_distanceToPreviousScene
Tf,N,V_flickerScore
Tf,N,V_sceneprintDistanceToPreviousScene
hasAction
setHasAction:
_hasAction
TB,N,V_hasAction
startAndSyncClusterCacheWithLibrary:reply:
resetFaceClusteringState:
performFaceClusteringAndWait
_resetFaceClusteringStateWithContext:error:
clusterFacesWithExtendTimeoutBlock:andCancelBlock:
clusteringStatus
performFaceClusteringWithCompletion:
cancelFaceClustering
performFaceClusteringIfNecessaryAndWait
scheduleClusteringOfFacesWithLocalIdentifiers:
scheduleUnclusteringOfFacesWithClusterSequenceNumbers:
numberOfFacesPendingClustering
reclusterFacesWithThreshold:shouldRecluster:error:
getFaceClusters:clusteringThreshold:utilizingGPU:error:
clustererIsReadyToReturnSuggestions
resetClusterer
clusterFacesIfNecessaryWithExtendTimeoutBlock:andCancelBlock:
fetchPersonAssociatedWithFaceGroup:options:
faceCountInFaceGroup
isDirty
setPlaybackCrop:
hasKeyFrame
hasPlaybackCrop
curationScore
setCurationScore:
autoPlayable
setAutoPlayable:
playbackCrop
_curationScore
_playbackCrop
_autoPlayable
Tf,N,V_curationScore
T@"VCPProtoVideoKeyFrame",&,N,V_keyFrame
TB,N,V_autoPlayable
T@"VCPProtoBounds",&,N,V_playbackCrop
dataWithJSONObject:options:error:
initWithData:encoding:
dataUsingEncoding:
newDictionaryRepresentationOfFaceCropDataFromFaceBox:andCropRegion:andGroupingIdentifier:
createOutputMetadataFromDictionary:
newDictionaryWithCGImageSourceOptions
newFaceCropFromCGImageSource:withFaceRect:groupingIdentifier:error:
newDictionaryPopulatedWithFaceCropDataFromImageData:
isEqualToNumber:
cropBoundsInOriginalImageFromFaceCrop:error:
groupingIdentifierFromFaceCrop:error:
setX:
setY:
Td,N,V_x
Td,N,V_y
value
setValue:
timescale
setTimescale:
_value
_timescale
Tq,N,V_value
Ti,N,V_timescale
T@"VCPProtoTime",&,N,V_start
T@"VCPProtoTime",&,N,V_duration
requestAnalysis:ofIOSurface:withProperties:withReply:
errorWithStatus:andDescription:
requestAnalysis:ofPixelBuffer:withProperties:withCompletionHandler:
_connectionLock
initFromConfigFile:numStage:numLandmarks:numTreePerStage:depthOfTree:numFeatures:
detectLandmark:width:height:stride:facerect:prevResult:result:
calculateFaceRectFromPrevLM:result:numOfLandmarks:
_internalLandmarkDetector
_numOfLandmarks
timeValuesCount
clearTimeValues
timeValueAtIndex:
addTimeValue:
homographyParamsAtIndex:
addHomographyParams:
timeValues
setTimeValues:count:
inputBoundsX
setInputBoundsX:
inputBoundsY
setInputBoundsY:
inputBoundsHeight
setInputBoundsHeight:
inputBoundsWidth
setInputBoundsWidth:
sourceSizeHeight
setSourceSizeHeight:
sourceSizeWidth
setSourceSizeWidth:
_timeValues
_inputBoundsHeight
_inputBoundsWidth
_inputBoundsX
_inputBoundsY
_sourceSizeHeight
_sourceSizeWidth
Tf,N,V_cropRectX
Tf,N,V_cropRectY
Tf,N,V_cropRectHeight
Tf,N,V_cropRectWidth
Tf,N,V_inputBoundsX
Tf,N,V_inputBoundsY
Tf,N,V_inputBoundsHeight
Tf,N,V_inputBoundsWidth
Tf,N,V_sourceSizeHeight
Tf,N,V_sourceSizeWidth
T^q,R,N
nodeForSceneClassId:
sceneIdFromSceneName:
numOfValidFrames
sumOfScore
initWithTimestamp:score:valid:
updateWithFirstFrame:score:valid:
updateSegment:score:valid:
updateDuration:
trimSegment:fromStart:
isContentTooShort
_sumOfScore
_numOfValidFrames
TQ,R,N,V_numOfValidFrames
resetSharedInstanceWithIdentifier:
serialQueue_
sharedInstances_
generateCurationSegment
generateInterestingTrimBasedOnCaptureTime:
updateCurationThreshold
calculateCandidateScoreWithRangeAdjust:endIdx:candidateTimeRange:captureTime:
isCurated:
isTimestampSkipable:
checkTrimAt:captureTime:
finalizeWithDestructiveTrimStart:trimEnd:andCaptureTime:
bestTrimTimeRange
_actionAnalyzer
_bestTrimTimeRange
_curationThreshold
_inTrimStart
_inTrimEnd
_captureTime
_ready
progressWithTotalUnitCount:parent:pendingUnitCount:
vcp_childWithPendingUnitCount:
_imageURL
_movie
_mediaType
_mediaSubtypes
_pixelWidth
_pixelHeight
initWithImageURL:isSDOF:
sdofImageAssetWithURL:
initWithImageURL:andMovieURL:
nominalFrameRate
initWithMovieURL:
descriptors
normalizeActivityDescriptor
initWithTimerange:andScore:
prepareActivityStats
generateActivityDescriptor
computeActivityScoreAtTime:
resetActivityStatsAtTime:
extractRequiredInfoFrom:toArray:
extractRequiredClassificationInfoFrom:toArray:
extractRequiredFaceInfoFrom:toArray:
validationScoreOfTimeRange:fromResult:startIdx:
actionScoreInTimeRange:
validateActivityScores
scaleBasedOnFaceForTimeRange:
addSceneSwitchFrequencyConstributionToActivityLevel:
addSceneClassificationContributionToActivityLevel:
initWithFrameStats:
preProcessQualityResults:interestingnessResults:obstructionResults:classificationResults:fineActionResults:faceResults:sceneSwitchFrequency:
finishAnalysisPass:fpsRate:
_activityDescriptor
_activityScores
_validActivityScores
_interestingnessResults
_obstructionResults
_classificationResults
_fineActionResults
_sceneSwitchFrequency
_lastProcessTime
_overallActivityLevel
_sportsSceneId
T{?={?=qiIq}{?=qiIq}},V_timerange
Tf,V_score
symbologies
setSymbologies:
enableGating
setUseSegmentationPregating:
spatialDescriptorWithMvMagnitudeMean:
_widthInMb
_heightInMb
_motionMagnitudeHistogram
_motionMagnitude
T^f,R
createModel
prepareData:
detect
keypointsFromObservations:
analyzeBodyArray:
_inputChannels
_action
_poseRequest
_valid
initWithTransform:withExistingFaceprints:frameStats:
initWithTransform:frameStats:faceDominated:
faceDetectorWithTransform:withExistingFaceprints:frameStats:tracking:faceDominated:cancel:
faceRanges
_activeFaces
makeValidationDecision
updateIntrinsicWhenRotated
setFrame:
checkResolutionChange:withRotation:
validateFace:eulerAngles:
rotateLandmarks:width:height:landmarks:numLandmarks:
mapToCameraNegativeZ
bufferRotated
_faceCount
_inDetectionMode
_lmDetector
_lmTracker
_prevLM
_curLM
_detectionModeCounter
_trackingModeCounter
_lostTrackCounter
_angleStable
_validationScore
_validateFailedOnce
_validationQueue
_validationGroup
_valBuffer
_valBufferRotated
_valAngle
_valLM
_shapeModel
_faceValidator
_offline
_bufferRotated
T{?=[4]},R,N,V_pose
Tr^f,R,N
TB,R,N,V_bufferRotated
initWithFace:andFace:andScore:
pairWithFace:andFace:andScore:
face1
face2
_face1
_face2
T@"VCPPhotosFace",R,N,V_face1
T@"VCPPhotosFace",R,N,V_face2
Td,R,N,V_score
observationWithBoundingBox:
CGImage
initWithURL:orientation:options:
initWithCGImage:orientation:options:
initWithFaceObservations:
_faceObservationsWithBBoxFromVCPPhotosFaces:mapping:
_bboxAlignedFaceObservationsFromFaceObservations:inImage:withError:
alignedBoundingBoxAsCGRect
calculateDistance:toWrapper:andError:
gistDescription
_alignBBoxForVCPPhotosFaces:forImage:
replaceCoordinatesAndFeaturesFromDetectedFace:
_sortedViableFaceMergePairsFromQueryFaces:andCandidateFaces:
sortedViableMergeCandidateFacesFor:from:ignoreSourceAssetDimensions:matchScores:
initWith:confidence:
setBound:
_bound
T{CGRect={CGPoint=dd}{CGSize=dd}},V_bound
parseResults:toDetections:atTime:fromTime:addActiveRegions:
addDetectionToDict:withActiveRegions:forPetsDetections:fromTime:
_petsDetections
_petsFaceDetections
_petsStart
_petsFaceStart
_petsAnalyer
_petsActiveRegions
_petsFaceActiveRegions
analyzeFrame:withFaceBounds:
landmarks
setPose:
_landmarkDetector
_poseEstimator
_lastTimestamp
_points2D
_points3D
_pose
T{?=[4]},V_pose
rotationToEulerAngles:angles:
kalmanFiltering:T:
eulerAnglesToRotation:R:
filteringPose:
_previousState
_previousCovar
_previousStateIsValid
updateModelByAddingFaces:andRemovingFaces:canceller:error:
updateModelByAddingFaces:error:
vcp_updateModelByAddingFaces:
minProcessTimeIntervalInSecs
detectFaces:faces:
compareFace:withFace:
removeSmallestKeyFace
detectTrackFacesInFrame:withTimestamp:faces:
requestRevision
initWithType:cachePath:state:threshold:requestRevision:
clusterFaces
updateWithExistingFaces
locationChange:relativeTo:landscape:
_latestTrackID
_smileDetector
_existingFaceprints
_latestFrameArea
_faceTrackers
_keyFaces
_reservedIDs
_facePrints
_allFaces
_frameFaceResults
hasMeaningfulSceneSegment:withFpsRate:
assetQualityScoreFromAnalysis:withFpsRate:
assetActionScoreFromAnalysis:
assetExpressionScoreFromAnalysis:
assetVoiceScoreFromAnalysis:
assetJunkScoreFromAnalysis:
assetCameraMotionScoreFromAnalysis:
checkCameraZoom:cameraMotionResults:
scaleForTimeRange:basedOnFace:
isJunkTimeRange:basedOnResults:
subjectActivityInTimeRange:fromResults:
cameraActivityfromQuality:
assetActivityLevelFromAnalysisResults:
faceSharpness
isHeadingFrame
computeGlobalQuality
computeScoreFromColorfulness
computeScoreFromExposure
computeExpressionScore
computeScoreFromAction
computeGlobalQualityForLivePhoto
computeVisualPleasingScore
computePenaltyScore
computeContentScore
computeCurationScoreComponents
storeFrameResults
printStats
setFrameResults:
_subjectAction
_cameraMotion
_interestingness
_obstruction
_colorfulness
_subMb
_isHeadingFrame
_semanticScore
_faceSharpness
_frameResults
T{?=qiIq},N,V_timestamp
Tf,N,V_semanticScore
Tf,N,V_faceSharpness
TB,N,V_isHeadingFrame
T@"NSMutableArray",&,N,V_faceQualityScores
T@"NSMutableDictionary",&,N,V_frameResults
initWithLivePhoto:
setKeyFrameTime:isHeadingFrame:
prepareFrameStats:
computeSharpnessOfFrame:
computeFaceQualityOfFrame:
finalizeKeyFrame
loadKeyFrameResult:timestamp:
adjustScoreByFace
modulateByJunk
modulateByTimeRange
setBlurAnalyzerFaceResults:
setFaceSharpness:
computeCurationScore
frameResults
hasGoodSubjectAction
setIsHeadingFrame:
resetStatsFlag
loadKeyFrameResults:
setFaceStatsFlag:detectedFaces:
setMotionStatsFlag:cameraMotion:subjectAction:interestingness:obstruction:colorfulness:exposureScore:humanActionStatsFlag:humanPoseScore:humanActionScore:subMb:
modulateByExposure
computeMinDistanceBetween:withSet:
_faceQualityAnalyzer
_keyFrames
_activeKeyFrame
_keyFrameScores
_inputKeyFrameResults
_lastestFaceID
_numFacesLastFrame
_lastVertices
_lastJawOpenness
initWithRequestAnalyses:formatDescription:
T@"NSDictionary",R,&,N
items
time
resetSegment:atTime:
focusStatus
addSegmentToResults
initWithFocusStatus:atTime:
updateSegment:atTime:
processFrameMetadata:
_mutableResults
T@"NSArray",R,&,N
setFocusStatus:
_focusStatus
Tq,V_focusStatus
hadZoom
setHadZoom:
minZoom
setMinZoom:
maxZoom
setMaxZoom:
_minZoom
_maxZoom
TB,N,V_hadZoom
Tf,N,V_minZoom
Tf,N,V_maxZoom
readGyroHomographyDimension:
gyroHomographyVersionIsValid:
readSoftwareStackVersion:
referenceSoftwareStackVersion
compareSoftwareStackVersion:withReferenceVersion:
getSetupDataFrom:
getFirstAtomWithFourCharCode:fromSetupData:
compareNumericVersion:withReferenceVersion:
componentsSeparatedByString:
numberFromString:
numberWithChar:
numberWithUnsignedChar:
increaseLengthBy:
resetBytesInRange:
convertLivePhotoStruct:toDictionary:
dataType
dataValue
convertLivePhotoBinary:toDictionary:
defaultDesiredKeys
_prevEstimatedCenterMv
_deSerializedMetaBuffer
_metaFocusAnalyzer
_metaMotionAnalyzer
_requestAnalyses
_metadataStabilizationArray
_frameTimestampArray
_originalFrameTimestampArray
_metadataItemTimestampArray
_adjusterArray
_interpolatedFrameArray
_metaLensSwitchAnalzer
_gyroHomographyIsValid
_gyroHomographyDimension
stabilityScore
decideSegmentPointBasedOn:
initWithAbsMotion:atTime:
mergeSimilarSegments
absMotion
_hinkleyDetector
setAbsMotion:
setStabilityScore:
_absMotion
_stabilityScore
Tf,V_absMotion
Tf,V_stabilityScore
numberValue
setupTrackerWithReferenceFrame:withROI:
trackInFrame:
lostTrackInd
initWithObjectBounds:inFrame:timestamp:
trackObjectInFrame:
objectBoundsInitial
objectBounds
lostCount
_correlationTracker
_lostCount
_objectBoundsInitial
_objectBounds
T{CGRect={CGPoint=dd}{CGSize=dd}},R,N,V_objectBoundsInitial
T{CGRect={CGPoint=dd}{CGSize=dd}},R,N,V_objectBounds
Tf,R,N,V_confidence
T{?=qiIq},R,N,V_start
Ti,R,N,V_lostCount
initWithPlistRepresentation:
attachSalientRegions:toPixelBuffer:
plistRepresentation
isOutOfBoundary:
updateConfidence:prevBound:newBound:width:height:
pruneRegions:withOverlapRatio:
boundDistance:relativeTo:landscape:
_detections
_latestRegions
_timeLastTracking
_saliencyAnalyer
_trackers
_confidences
_activeRegions
initWithSceneId:withDuration:withConfidence:
sceneId
setSceneId:
setDuration:
sumConfidence
setSumConfidence:
_duration
_sumConfidence
_sceneId
T@"NSString",&,V_sceneId
Tf,V_duration
Tf,V_sumConfidence
addResult:start:duration:keyIsName:
compareObjectsOfInterest:withScenes:
addAggregatedScenes:timerange:
frameScenes
sceneResults
setSceneResults:
_existingScenes
_sceneTaxomy
_internalFrameScenes
T@"NSArray",&,V_sceneResults
naturalSize
decodeDimensionsForTrack:
settings
getNextCaptureSampleBuffer
_track
initWithTrack:timerange:withSettings:applyTransform:
track
_status
assetReaderWithAsset:error:
cancelReading
initWithTrack:timerange:atInterval:
_assetReader
_trackOutput
_decodeEnd
_sampleDuration
_nextSampleTime
_currentSample
_nextSample
assetReaderSampleReferenceOutputWithTrack:
canAddOutput:
findNextSample:timerange:
decodeSample:sample:
decodeTask
initWithTrack:timerange:
_trackReader
_launchOnce
_group
_inputSemaphore
_outputSemaphore
_cancelDecode
_decodeError
_decodeFinished
_decodedFrames
_outputFrameIdx
_sampleBuffer
setVoiceDetections:
_audioStream
_voiceActivity
_voiceStart
_voiceDetections
_utteranceDetections
_musicDetections
T@"NSMutableArray",&,V_voiceDetections
dictionaryWithContentsOfURL:
addDetectionFromTime:toTime:result:
setupWithAudioStream:
loadModel
_voiceActivityNew
_audioUnit
setBox:
stableInd
setStableInd:
setLostTrackInd:
T^{CGPoint=dd}
stable
lostTrack
T^{CGPoint=dd},VP
TB,Vstable
TB,VlostTrack
@24@0:8@16
@16@0:8
@"MLFeatureValue"24@0:8@"NSString"16
@"NSSet"16@0:8
@24@0:8^{__CVBuffer=}16
v16@0:8
@32@0:8^{CGImage=}16^@24
@32@0:8@16^@24
B32@0:8^{CGImage=}16^@24
B32@0:8@16^@24
^{__CVBuffer=}16@0:8
v24@0:8^{__CVBuffer=}16
^{__CVBuffer=}
v24@0:8@16
@"MLMultiArray"
v32@0:8@16@?24
v40@0:8@16@24@?32
@40@0:8@16@24^@32
@32@0:8^{__CVBuffer=}16^@24
@"MLModel"
@28@0:8Q16B24
i24@0:8^{opaqueCMSampleBuffer=}16
i88@0:8^{AudioBufferList=I[1{AudioBuffer=II^v}]}16{AudioTimeStamp=dQdQ{SMPTETime=ssIIIssss}II}24
i24@0:8r^{?=qiIq}16
i40@0:8@16@?24^@32
{AudioTimeStamp="mSampleTime"d"mHostTime"Q"mRateScalar"d"mWordClockTime"Q"mSMPTETime"{SMPTETime="mSubframes"s"mSubframeDivisor"s"mCounter"I"mType"I"mFlags"I"mHours"s"mMinutes"s"mSeconds"s"mFrames"s}"mFlags"I"mReserved"I}
{AudioBufferList="mNumberBuffers"I"mBuffers"[1{AudioBuffer="mNumberChannels"I"mDataByteSize"I"mData"^v}]}
@"VCPVoiceDetector"
@"VCPAudioClassifier"
@"VCPLoudnessAnalyzer"
@"VCPSongDetector"
B24@0:8@16
#16@0:8
@24@0:8:16
@32@0:8:16@24
@40@0:8:16@24@32
B16@0:8
B24@0:8#16
B24@0:8:16
Vv16@0:8
Q16@0:8
^{_NSZone=}16@0:8
B24@0:8@"Protocol"16
@"NSString"16@0:8
v32@0:8@16@24
v32@0:8@"<SNRequest>"16@"<SNResult>"24
v32@0:8@"<SNRequest>"16@"NSError"24
v24@0:8@"<SNRequest>"16
@52@0:8{?=qiIq}16f40@44
v36@0:8r^{?=qiIq}16r^{?=qiIq}24f32
@"NSMutableArray"
{?="value"q"timescale"i"flags"I"epoch"q}
@"NSString"
@24@0:8Q16
i28@0:8^{opaqueCMSampleBuffer=}16i24
@"SNAudioStreamAnalyzer"
@"AVAudioPCMBuffer"
@80@0:8Q16@24{CGSize=dd}32{CGRect={CGPoint=dd}{CGSize=dd}}48
@44@0:8B16@20B28B32B36B40
i16@0:8
i24@0:8@16
i40@0:8^i16^i24^I32
i28@0:8f16i20i24
i24@0:8^{__CVBuffer=}16
i36@0:8^{__CVBuffer=}16^f24i32
i24@0:8i16i20
i40@0:8^f16^{__CVBuffer=}24i32i36
i48@0:8^{__CVBuffer=}16^Q24^@32@?40
v20@0:8B16
@"VCPCNNModelEspresso"
@"NSURL"
v80@0:8{?={?=qiIq}{?=qiIq}}16@64@72
v96@0:8@16@24{?={?=qiIq}{?=qiIq}}32@80@88
@32@0:8@16d24
@24@0:8^{_NSZone=}16
v20@0:8f16
d16@0:8
v24@0:8d16
f16@0:8
{?="contentScore"b1"globalQualityScore"b1}
f40@0:8*16i24i28q32
i40@0:8^f16@24^{__CVBuffer=}32
@84@0:8@16f24@28{?={?=qiIq}{?=qiIq}}36
^{__CVBuffer=}56@0:8^{__CVBuffer=}16{CGRect={CGPoint=dd}{CGSize=dd}}24
i104@0:8{?=qiIq}16{?=qiIq}40@64{CGRect={CGPoint=dd}{CGSize=dd}}72
@"VCPVideoCNNBackbone"
@"VCPVideoPersonDetector"
@"VCPVideoCNNAutoplay"
@36@0:8f16f20f24f28f32
{CGRect={CGPoint=dd}{CGSize=dd}}24@0:8f16f20
@"NSDictionary"16@0:8
@24@0:8@"NSDictionary"16
i48@0:8^{CGRect={CGPoint=dd}{CGSize=dd}}16^{__CVBuffer=}24^f32i40i44
f52@0:8^f16i24i28i32*36f44i48
i52@0:8^{__CVBuffer=}16^f24^f32f40@?44
f24@0:8^f16
^f40@0:8i16i20^i24^i32
i48@0:8^f16*24f32i36@?40
v56@0:8*16q24^f32q40i48i52
@"VCPLoaned"
#20@0:8i16
@36@0:8i16i20i24B28B32
@48@0:8i16i20i24B28B32i36i40B44
i32@0:8@16@24
@"VCPCNNData"
B20@0:8i16
i28@0:8^{__sFILE=*iiss{__sbuf=*i}i^v^?^?^?^?{__sbuf=*i}^{__sFILEX}i[3C][1C]{__sbuf=*i}iq}16s24
@36@0:8i16i20i24@28
i20@0:8B16
i28@0:8^{__CVBuffer=}16i24
^f16@0:8
v24@0:8^f16
@"VCPCNNMetalContext"
@32@0:8@16@24
{CGSize=dd}16@0:8
s16@0:8
v20@0:8s16
@"NSData"
@"VCPPhotosFace"
{CGSize="width"d"height"d}
^v24@0:8B16B20
@28@0:8B16B20B24
i56@0:8^{__CVBuffer=}16{CGRect={CGPoint=dd}{CGSize=dd}}24
i24@0:8^f16
@20@0:8i16
i52@0:8^{__sFILE=*iiss{__sbuf=*i}i^v^?^?^?^?{__sbuf=*i}^{__sFILEX}i[3C][1C]{__sbuf=*i}iq}16^f24^f32i40i44i48
i36@0:8^{__sFILE=*iiss{__sbuf=*i}i^v^?^?^?^?{__sbuf=*i}^{__sFILEX}i[3C][1C]{__sbuf=*i}iq}16i24i28i32
i32@0:8^{__CVBuffer=}16^f24
i72@0:8^f16^{__CVBuffer=}24i32i36{CGRect={CGPoint=dd}{CGSize=dd}}40
i64@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16^{__CVBuffer=}48^B56
@72@0:8@16{CGSize=dd}24{CGRect={CGPoint=dd}{CGSize=dd}}40
v68@0:8{CGAffineTransform=dddddd}16B64
@32@0:8B16B20@24
i36@0:8^{CGPoint=dd}16^f24f32
^f48@0:8i16i20^i24^i32^f40
i64@0:8^f16i24i28i32i36i40^{CGPoint=dd}44^f52f60
i52@0:8^{__CVBuffer=}16@24[21{CGPoint=dd}]32[21f]40B48
@20@0:8B16
@"<MTLDevice>"
@"<MTLCommandQueue>"
@"<MTLCommandBuffer>"
@"MADVIVisualSearchRequest"
f32@0:8@16@24
@40@0:8^{opaqueCMSampleBuffer=}16@24^@32
v48@0:8@16i24i28^f32^f40
f48@0:8{CGPoint=dd}16{CGPoint=dd}32
{CGSize=dd}32@0:8@16^@24
I16@0:8
@"VCPImageHumanPoseAnalyzer"
@"NSArray"
@24@0:8s16B20
[200@"VCPCNNBlock"]
@48@0:8@16@24@32@40
i40@0:8{vector<float *, std::allocator<float *>>=^^f^^f{__compressed_pair<float **, std::allocator<float *>>=^^f}}16
^v16@0:8
{vector<espresso_buffer_t, std::allocator<espresso_buffer_t>>=^{?}^{?}{__compressed_pair<espresso_buffer_t *, std::allocator<espresso_buffer_t>>=^{?}}}16@0:8
v40@0:8{vector<espresso_buffer_t, std::allocator<espresso_buffer_t>>=^{?}^{?}{__compressed_pair<espresso_buffer_t *, std::allocator<espresso_buffer_t>>=^{?}}}16
{?=^v^v[4Q][4Q]QQQQQQQQQQi}16@0:8
v184@0:8{?=^v^v[4Q][4Q]QQQQQQQQQQi}16
{?="plan"^v"network_index"i}
@"VCPCNNEspressoContext"
{vector<espresso_buffer_t, std::allocator<espresso_buffer_t>>="__begin_"^{?}"__end_"^{?}"__end_cap_"{__compressed_pair<espresso_buffer_t *, std::allocator<espresso_buffer_t>>="__value_"^{?}}}
{?="data"^v"reserved"^v"dim"[4Q]"stride"[4Q]"width"Q"height"Q"channels"Q"batch_number"Q"sequence_length"Q"stride_width"Q"stride_height"Q"stride_channels"Q"stride_batch_number"Q"stride_sequence_length"Q"storage_type"i}
i56@0:8@16@24@32@40@48
i48@0:8@16@24@32@40
@"<MTLLibrary>"
@"<MTLComputePipelineState>"
B44@0:8@16i24@28^@36
@"NSObject<OS_dispatch_queue>"
@28@0:8i16@20
i48@0:8^{__CVBuffer=}16^{__CVBuffer=}24^{__CVBuffer=}32^{__CVBuffer=}40
i56@0:8^{__CVBuffer=}16^{__CVBuffer=}24^{__CVBuffer=}32^{__CVBuffer=}40@?48
i32@0:8^{__CVBuffer=}16^{?=[7{?=iii}][7^{__CVBuffer}]}24
i40@0:8^{__CVBuffer=}16^{?=[7{?=iii}][7^{__CVBuffer}]}24@?32
i36@0:8^{?=iii}16i24i28i32
@36@0:8B16@20i28i32
i28@0:8^^{__CVBuffer}16I24
i36@0:8i16^{__CVBuffer=}20^{__CVBuffer=}28
@"VCPFlowFeatureExtractor"
[7@"VCPFlowDecoder"]
@"VCPCorrelation"
@"VCPBackwarp"
[2{?="featureShape"[7{?="channels"i"height"i"width"i}]"feature"[7^{__CVBuffer}]}]
{?="correlations"[7^{__CVBuffer}]"flows"[7^{__CVBuffer}]"upscaledFlows"[7^{__CVBuffer}]"warpedBuffers"[7^{__CVBuffer}]}
@"<MTLDeviceSPI>"
@"MPSImageBilinearScale"
i52@0:8^f16i24i28@32@40i48
i28@0:8@16i24
i48@0:8^{__CVBuffer=}16@24@32@?40
@"VCPProtoTimeRange"
i40@0:8@16@24@?32
@28@0:8i16i20i24
@"MADVIDocumentRecognitionRequest"
@"VNImageBasedRequest"
B72@0:8{?=qiIq}16{?=qiIq}40^@64
@"VCPVideoProcessorSession"
v24@0:8@"<PVFetchResultProtocol>"16
@"<PVFaceProtocol>"16@0:8
v24@0:8@"<PVFaceProtocol>"16
i64@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16^{__CVBuffer=}48^q56
B40@0:8@16@24^@32
B88@0:8{?={?=qiIq}{?=qiIq}}16{?=qiIq}64
B56@0:8^{opaqueCMSampleBuffer=}16{?=qiIq}24^@48
B32@0:8^{opaqueCMSampleBuffer=}16^@24
B48@0:8{?=qiIq}16^@40
v20@0:8I16
{CF<opaqueCMSampleBuffer *>="value_"^{opaqueCMSampleBuffer}}
@"VCPCNNModel"
@44@0:8@16@24@32i40
@"VCPDatabaseReader"
@"NSSet"
@"NSDictionary"
@"PHAsset"
i36@0:8^{sqlite3_stmt=}16i24@28
i40@0:8^{sqlite3_stmt=}16i24i28@32
i40@0:8@16^@24^q32
i32@0:8q16@24
i40@0:8q16@24@32
@32@0:8Q16Q24
i32@0:8^q16@24
^{sqlite3=}
@"VCPMADResourceManager"
@"VCPMADResource"
@"VCPTimer"
@"NSObject<OS_os_transaction>"
Q28@0:8@16f24
v24@0:8Q16
{CGRect={CGPoint=dd}{CGSize=dd}}16@0:8
v48@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16
{CGRect="origin"{CGPoint="x"d"y"d}"size"{CGSize="width"d"height"d}}
@"VCPProtoBounds"
@56@0:8^f16^f24Q32Q40i48i52
i28@0:8f16f20f24
i36@0:8^f16f24^f28
i64@0:8^f16Q24Q32{DSPSplitComplex=^f^f}40^f56
B28@0:8i16i20i24
{DSPSplitComplex="realp"^f"imagp"^f}
i52@0:8@16B24@?28^Q36^@44
{CGRect={CGPoint=dd}{CGSize=dd}}64@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{CGSize=dd}48
{CGRect={CGPoint=dd}{CGSize=dd}}80@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{CGRect={CGPoint=dd}{CGSize=dd}}48
{CGPoint=dd}64@0:8{CGPoint=dd}16{CGRect={CGPoint=dd}{CGSize=dd}}32
@44@0:8Q16Q24B32@?36
@36@0:8Q16B24@?28
@"NSObject<OS_dispatch_source>"
@32@0:8@16Q24
{CGAffineTransform=dddddd}20@0:8I16
i32@0:8@16^Q24
i32@0:8^Q16^@24
@"NSMutableDictionary"
Q36@0:8B16Q20Q28
{CGRect={CGPoint=dd}{CGSize=dd}}32@0:8Q16Q24
{CGRect={CGPoint=dd}{CGSize=dd}}80@0:8Q16Q24{CGAffineTransform=dddddd}32
q16@0:8
v24@0:8q16
v20@0:8i16
@"VNFaceObservation"
{?=qiIq}16@0:8
v40@0:8{?=qiIq}16
d24@0:8@16
@"NSURL"16@0:8
B32@0:8@?<v@?>16^@24
@"<PVFetchResultProtocol>"24@0:8@"NSArray"16
@"<PVFetchResultProtocol>"24@0:8Q16
@"<PVFetchResultProtocol>"24@0:8@"<PVMomentProtocol>"16
@"<PVFetchResultProtocol>"24@0:8@"<PVPersonProtocol>"16
@"NSDictionary"24@0:8@"<PVFetchResultProtocol>"16
@"<PVFetchResultProtocol>"32@0:8@"<PVPersonProtocol>"16@"<PVMomentProtocol>"24
@"<PVFetchResultProtocol>"32@0:8@"NSArray"16@"<PVMomentProtocol>"24
@"<PVFetchResultProtocol>"24@0:8@"<PVFaceGroupProtocol>"16
@"<PVFetchResultProtocol>"16@0:8
@"<PVFetchResultProtocol>"24@0:8@"<NSFastEnumeration>"16
@"NSDate"16@0:8
i40@0:8^@16^@24@32
v40@0:8@16@24@32
q32@0:8@16Q24
@72@0:8@16@24Q32Q40@48i56B60^@64
d32@0:8@16@24
B40@0:8@16@24@32
@92@0:8@16@24@32Q40Q48@56@64@72@80i88
i56@0:8^@16@24@32Q40Q48
i40@0:8^@16@24@32
i32@0:8^@16@24
i40@0:8@16@24^@32
i48@0:8@16@24@32^@40
i32@0:8@16^@24
@"VCPPhotosFaceProcessingContext"
@"VCPFaceMerger"
@"NSObject<OS_dispatch_group>"
@"VCPObjectPool"
@28@0:8i16f20f24
f24@0:8f16B20
Q24@0:8Q16
@64@0:8{CGAffineTransform=dddddd}16
@72@0:8@16@24@32@40B48B52f56f60f64B68
i48@0:8^{__CVBuffer=}16{?=qiIq}24
i80@0:8^{__CVBuffer=}16{?=qiIq}24{?=qiIq}48^Q72
i88@0:8^{__CVBuffer=}16{?=qiIq}24{?=qiIq}48@72^Q80
i40@0:8{?=qiIq}16
f40@0:8@16^{EncodeStats=^^?^B^B^{MotionVector}^{MotionVector}^{MotionVector}^{MotionVector}^S^S^I^S^S^S^S^S^S^S^S^S^S^SIBBBii}24i32i36
i64@0:8{?={?=qiIq}{?=qiIq}}16
i20@0:8i16
v24@0:8^v16
f24@0:8^v16
i36@0:8@16@24B32
i44@0:8^{__CFArray=}16@24@32B40
v32@0:8^v16@24
^{MotionFilter=^{FrameBuffer}BB}
^{MetaDataAnalysis=B^{FrameBuffer}{Translation=fff}{Translation=fff}}
^{IrisAnalysis=ffiiB^{__CFArray}}
{FrameBuffer="frame_count_"i"buffer_"[35{Frame="frame_idx_"i"timestamp_"{?="value"q"timescale"i"flags"I"epoch"q}"duration_"{?="value"q"timescale"i"flags"I"epoch"q}"ave_motion_"{Translation="x_"f"y_"f"z_"f}"org_motion_"{Translation="x_"f"y_"f"z_"f}"quality_score_"f"distortion_"Q"distortion_norm_"f"motion_change_"{Translation="x_"f"y_"f"z_"f}"compressed_bytes_"I"blur_"B"acc_var_"{Translation="x_"f"y_"f"z_"f}"texture_"f"motion_result_"{MotionResult="significant_values_"[6f]"confidence_"[6f]"fine_action_score_"f"action_score_"f"track_score_"f"rotation_angle_"f"subtle_motion_score_"f"is_stable_"B"action_blocks_"i"action_motion_"f"valid_mb_"B"valid_submb_"B"support_size_"i"residual_var_"f"gmv_"[2f]"duration_"{?="value"q"timescale"i"flags"I"epoch"q}"scene_delta_"f"scene_delta_ratio_"f"objects_"{Vector<ma::Object *>="vector_"^{__CFArray}}"detect_objects_"{Vector<ma::Object *>="vector_"^{__CFArray}}"imported_objects_"{Vector<ma::Object *>="vector_"^{__CFArray}}}"flow_"^f"interestingness_"f"obstruction_"f"colorfulness_score_"f"histogram_"{Histogram="extremities_"f"overexposes_"f"histogram_"[3^i]"moments_hist_"[2^i]}}]}
{Histogram="extremities_"f"overexposes_"f"histogram_"[3^i]"moments_hist_"[2^i]}
@"VCPFrameAnalysisStats"
@"VCPFrameScoreFilter"
@"VCPMotionFlowSubtleMotionAnalyzer"
@"VCPMotionFlowAnalyzer"
@40@0:8i16i20Q24Q32
i56@0:8i16i20r^f24^f32Q40Q48
i60@0:8{Kernel=^fQQ}16f40f44f48f52f56
^^{Kernel}
@36@0:8^f16i24i28f32
i28@0:8i16@20
@24@0:8B16B20
i48@0:8^f16^{__CVBuffer=}24i32i36@40
i40@0:8^{__CVBuffer=}16@24@32
i44@0:8^f16i24i28B32*36
@36@0:8Q16i24@28
@40@0:8Q16@24^@32
B40@0:8^f16@24^@32
@40@0:8@16@24@32
i112@0:8@16@24{CGRect={CGPoint=dd}{CGSize=dd}}32{?=qiIq}64{?=qiIq}88
{vector<float *, std::allocator<float *>>="__begin_"^^f"__end_"^^f"__end_cap_"{__compressed_pair<float **, std::allocator<float *>>="__value_"^^f}}
@36@0:8@16B24Q28
@28@0:8@16B24
v40@0:8@16@24f32f36
f28@0:8@16f24
i40@0:8^{__CVBuffer=}16^f24@?32
i32@0:8^f16^{__CVBuffer=}24
i48@0:8^{CGRect={CGPoint=dd}{CGSize=dd}}16^{__CVBuffer=}24@32i40i44
i52@0:8^{__CVBuffer=}16^Q24f32^@36@?44
[16f]
@"VCPCNNBlurAnalyzer"
@24@0:8@?16
B24@0:8^@16
@?16@0:8
v24@0:8@?16
i32@0:8^{CGImage=}16^^{__CVBuffer}24
^{CGColorSpace=}
^{CGContext=}
^{__CVPixelBufferPool=}
@40@0:8@"MADRequest"16@"VCPMADServiceImageAsset"24@"NSString"32
@"NSArray"16@0:8
@"MADEmbeddingGenerationRequest"
@"VCPMADServiceImageAsset"
{atomic<bool>="__a_"{__cxx_atomic_impl<bool, std::__cxx_atomic_base_impl<bool>>="__a_value"AB}}
i32@0:8^f16@24
@"NSData"16@0:8
i32@0:8^f16@"<VCPDistanceDescriptorProtocol>"24
@24@0:8@"NSData"16
@"VNImageprint"
f56@0:8*16*24*32i40i44q48
f48@0:8*16i24i28q32*40
Q32@0:8@16@24
Q24@0:8@16
Q40@0:8@16@24Q32
Q32@0:8@16Q24
i40@0:8@16Q24^@32
i40@0:8^@16@24Q32
i40@0:8^f16@24Q32
i40@0:8^{__CVBuffer=}16@24@?32
B80@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{CGRect={CGPoint=dd}{CGSize=dd}}48
i84@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24{CGSize=dd}56B72@76
@40@0:8@16@24@?32
@"PHPhotoLibrary"
v40@0:8q16@24@32
v40@0:8d16@24@32
@40@0:8@16Q24^@32
@24@0:8q16
i32@0:8q16^@24
{CGPoint=dd}16@0:8
v32@0:8{CGPoint=dd}16
{CGPoint="x"d"y"d}
@32@0:8^{__CVBuffer=}16@24
i44@0:8Q16Q24i32^^{__CVBuffer}36
i28@0:8^^{__CVBuffer}16i24
i56@0:8@16i24Q28^^{__CVBuffer}36^I44B52
i48@0:8^{CGImage=}16i24I28Q32^^{__CVBuffer}40
i60@0:8^{CGImageSource=}16@24i32Q36^I44^^{__CVBuffer}52
^{__CVBuffer=}56@0:8i16Q20@28@36B44^I48
^{__CVBuffer=}28@0:8@16i24
^{__CVBuffer=}36@0:8@16i24Q28
^{__CVBuffer=}32@0:8i16@20B28
^{__CVBuffer=}40@0:8i16@20B28^I32
^{__CVBuffer=}36@0:8i16Q20@28
^{__CVBuffer=}44@0:8i16Q20@28^I36
^{CMPhotoDecompressionSession=}
^{OpaqueVTPixelTransferSession=}
v40@0:8Q16Q24@?32
v40@0:8@16Q24@?32
@44@0:8@16@24B32@?36
@48@0:8@16d24Q32@?40
@"<PVFaceProtocol>"40@0:8@"<PVPersonProtocol>"16@"NSMapTable"24@?<v@?f^B>32
@"NSString"40@0:8@"PVPersonClusterManager"16@"NSSet"24@?<v@?f^B>32
@"NSArray"44@0:8@"PVPersonClusterManager"16@"NSSet"24B32@?<v@?f^B>36
@"NSArray"48@0:8@"NSArray"16d24Q32@?<d@?@@>40
@24@0:8^@16
@28@0:8@16i24
@36@0:8@16i24@28
@40@0:8@16@?24^@32
v56@0:8@16@24^@32@?40@48
B48@0:8@16Q24@?32^@40
B40@0:8@16@?24^@32
B48@0:8@16@24@?32^@40
B32@0:8@?16^@24
@32@0:8@16@?24
B72@0:8@16@24@32@40@?48@?56^@64
B44@0:8@16B24@?28^@36
@48@0:8@16@24@32@?40
@32@0:8Q16^@24
B48@0:8@16Q24@32^@40
B72@0:8@16@24@32@40@?48@56^@64
B44@0:8@16B24@28^@36
B56@0:8@16@24@32^@40^@48
@56@0:8@16@24@32@40^@48
v56@0:8@16@?24@32@40@?48
B40@0:8Q16@?24^@32
@32@0:8@?16Q24
v48@0:8@16@24@32@?40
v112@0:8^@16^@24^@32^@40^@48^@56^@64^@72^@80@88@96@?104
v56@0:8^@16^@24^@32@40@48
B112@0:8@16@24@32@40@48@56@64@72@?80@?88@96^@104
Q32@0:8Q16@24
v64@0:8@16@24@32@40@48@56
v56@0:8@16@24@?32@?40@48
v48@0:8@16@?24@32@?40
@36@0:8i16B20B24@28
@"VCPCNNPetsDetector"
i36@0:8@16f24@?28
@24@0:8i16B20
i40@0:8^{__CVBuffer=}16^f24i32i36
f40@0:8^f16i24i28i32i36
i32@0:8^f16i24i28
[5{CGRect="origin"{CGPoint="x"d"y"d}"size"{CGSize="width"d"height"d}}]
[5f]
i24@0:8@?16
i40@0:8^^{__CVBuffer}16Q24Q32
i40@0:8^^{__CVBuffer}16Q24^{__CVBuffer=}32
i32@0:8^^{__CVBuffer}16Q24
i32@0:8^{__CVBuffer=}16^{__CVBuffer=}24
i40@0:8^^{__CVBuffer}16^{CGColorSpace=}24^{__CVBuffer=}32
i52@0:8^^{__CVBuffer}16^^{__CVBuffer}24^^{__CVBuffer}32@40B48
i48@0:8^{__CVBuffer=}16^^{__CVBuffer}24Q32Q40
{CF<__CVBuffer *>="value_"^{__CVBuffer}}
i32@0:8Q16Q24
@"VNClassifyImageAestheticsRequest"
@"VNSceneClassificationRequest"
@"VNCreateSceneprintRequest"
@"VNGenerateAttentionBasedSaliencyImageRequest"
@"VNClassifyJunkImageRequest"
@"VNRecognizeObjectsRequest"
@"VNGenerateObjectnessBasedSaliencyImageRequest"
@"VNClassifyPotentialLandmarkRequest"
@"VNVYvzEtX1JlUdu8xx5qhDI"
@"VN6Mb1ME89lyW3HpahkEygIG"
@"VN5kJNH3eYuyaLxNpZr5Z7zi"
@"VNClassifyMemeImageRequest"
@"VN1JC7R3k4455fKQz0dY1VhQ"
@"VNRecognizeDocumentElementsRequest"
i36@0:8^^{__CVPixelBufferPool}16q24I32
i36@0:8^{__CVBuffer=}16^^{__CVBuffer}24B32
i52@0:8@16B24^^{__CVBuffer}28^^{__CVBuffer}36^@44
v32@0:8@16Q24
i48@0:8^@16@24@32@40
i36@0:8^@16@24B32
i40@0:8^@16^{__CVBuffer=}24@32
i40@0:8^@16^{__CVBuffer=}24B32B36
i32@0:8^@16^{__CVBuffer=}24
i56@0:8^@16B24B28^{__CVBuffer=}32^{__CVBuffer=}40@48
v40@0:8@16B24B28@?32
i48@0:8^{__CVBuffer=}16B24B28^@32@?40
@"VCPSceneProcessingImageManager"
@"PVSceneTaxonomy"
{CF<__CVPixelBufferPool *>="value_"^{__CVPixelBufferPool}}
{CF<OpaqueVTPixelTransferSession *>="value_"^{OpaqueVTPixelTransferSession}}
@"VCPMAMLModel"
B32@0:8@16@24
{CGSize=dd}32@0:8@16@24
i40@0:8^@16#24@32
^{__CVBuffer=}36@0:8@16@24i32
i52@0:8^@16#24@32@40B48
i48@0:8^f16@24@32@40
i40@0:8^f16@24@32
@"VIService"
@52@0:8@16^{__sFILE=*iiss{__sbuf=*i}i^v^?^?^?^?{__sbuf=*i}^{__sFILEX}i[3C][1C]{__sbuf=*i}iq}24C32*36^f44
i44@0:8^{__CVBuffer=}16^f24i32^f36
f24@0:8@16
i32@0:8[6f]16[6f]24
i24@0:8^v16
v80@0:8^{__CVBuffer=}16{?=qiIq}24{?=qiIq}48@?72
{Frame="frame_idx_"i"timestamp_"{?="value"q"timescale"i"flags"I"epoch"q}"duration_"{?="value"q"timescale"i"flags"I"epoch"q}"ave_motion_"{Translation="x_"f"y_"f"z_"f}"org_motion_"{Translation="x_"f"y_"f"z_"f}"quality_score_"f"distortion_"Q"distortion_norm_"f"motion_change_"{Translation="x_"f"y_"f"z_"f}"compressed_bytes_"I"blur_"B"acc_var_"{Translation="x_"f"y_"f"z_"f}"texture_"f"motion_result_"{MotionResult="significant_values_"[6f]"confidence_"[6f]"fine_action_score_"f"action_score_"f"track_score_"f"rotation_angle_"f"subtle_motion_score_"f"is_stable_"B"action_blocks_"i"action_motion_"f"valid_mb_"B"valid_submb_"B"support_size_"i"residual_var_"f"gmv_"[2f]"duration_"{?="value"q"timescale"i"flags"I"epoch"q}"scene_delta_"f"scene_delta_ratio_"f"objects_"{Vector<ma::Object *>="vector_"^{__CFArray}}"detect_objects_"{Vector<ma::Object *>="vector_"^{__CFArray}}"imported_objects_"{Vector<ma::Object *>="vector_"^{__CFArray}}}"flow_"^f"interestingness_"f"obstruction_"f"colorfulness_score_"f"histogram_"{Histogram="extremities_"f"overexposes_"f"histogram_"[3^i]"moments_hist_"[2^i]}}
^{EncodeStatsHW=^^?^B^B^{MotionVector}^{MotionVector}^{MotionVector}^{MotionVector}^S^S^I^S^S^S^S^S^S^S^S^S^S^SIBBBiii^{OpaqueVTCompressionSession}^{__CFData}{?=qiIq}iiB}
[6[5f]]
i40@0:8@16@?24^Q32
i32@0:8@?16^Q24
@"AVAsset"
{CGAffineTransform="a"d"b"d"c"d"d"d"tx"d"ty"d}
v52@0:8f16@20Q28Q36Q44
v44@0:8f16@20Q28Q36
i40@0:8@16^@24^@32
i32@0:8^@16^@24
i64@0:8@16@24@32@40@?48@?56
v56@0:8@16@24@32@?40@?48
B48@0:8@16^@24@?32@?40
i48@0:8^@16^@24@?32@?40
d80@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{CGRect={CGPoint=dd}{CGSize=dd}}48
i72@0:8@16^@24^@32^@40^@48@?56@?64
i80@0:8@16@24@32@40@48^@56@?64@?72
i64@0:8@16@24@32^@40@?48@?56
i56@0:8@16@24^@32@?40@?48
i48@0:8@16^@24@?32@?40
@208@0:8Q16{CGAffineTransform=dddddd}24{?={?=qiIq}{?=qiIq}}72B120@124B132B136@140B148{?={?=qiIq}{?=qiIq}}152@200
v56@0:8@16@24@32{CGSize=dd}40
@"VCPVideoKeyFrameAnalyzer"
@"VCPMovieHighlightAnalyzer"
{?="start"{?="value"q"timescale"i"flags"I"epoch"q}"duration"{?="value"q"timescale"i"flags"I"epoch"q}}
@44@0:8{?=qiIq}16f40
@76@0:8{?={?=qiIq}{?=qiIq}}16f64@68
{?={?=qiIq}{?=qiIq}}16@0:8
@"VCPVideoKeyFrameResult"
@64@0:8{?={?=qiIq}{?=qiIq}}16
v64@0:8{?={?=qiIq}{?=qiIq}}16
@"VCPImageDescriptor"
@"VCPVideoKeyFrame"
@96@0:8Q16B24B28B32B36{?={?=qiIq}{?=qiIq}}40@88
i144@0:8@16@24@32@40@48@56@64@72@80@88@96@104@112@120{CGSize=dd}128
B64@0:8{?={?=qiIq}{?=qiIq}}16
@120@0:8@16{?={?=qiIq}{?=qiIq}}24{?={?=qiIq}{?=qiIq}}72
@112@0:8{?={?=qiIq}{?=qiIq}}16{?={?=qiIq}{?=qiIq}}64
{?={?=qiIq}{?=qiIq}}64@0:8{?={?=qiIq}{?=qiIq}}16
f68@0:8{?={?=qiIq}{?=qiIq}}16B64
@68@0:8{?={?=qiIq}{?=qiIq}}16B64
{CGRect={CGPoint=dd}{CGSize=dd}}64@0:8{?={?=qiIq}{?=qiIq}}16
{?={?=qiIq}{?=qiIq}}68@0:8{?={?=qiIq}{?=qiIq}}16B64
f64@0:8{?={?=qiIq}{?=qiIq}}16
@"AVAssetImageGenerator"
@"VCPColorNormalizationAnalyzer"
v56@0:8@16@24I32@36i44@?48
v60@0:8@16@24@32@40i48@?52
v52@0:8@16@24@32i40@?44
v64@0:8@16@24I32@36@44i52@?56
v44@0:8@16@24i32@?36
v60@0:8i16@20@28Q36@44@?52
v48@0:8i16@20@28B36@?40
v52@0:8i16Q20@28@36@?44
v60@0:8i16Q20@28@36@44@?52
v60@0:8i16@20@28@36@44@?52
v48@0:8i16@20B28@32@?40
v44@0:8i16@20@28@?36
v36@0:8i16@20@?28
v40@0:8i16B20@24@?32
v52@0:8i16@20@28@36@?44
v52@0:8Q16@24@32i40@?44
v32@0:8@"NSURL"16@?<v@?@"NSString">24
v56@0:8@"NSArray"16@"IOSurface"24I32@"NSString"36i44@?<v@?@"NSArray"@"NSError">48
v60@0:8@"NSArray"16@"NSURL"24@"NSString"32@"NSString"40i48@?<v@?@"NSArray"@"NSError">52
v52@0:8@"NSArray"16@"NSString"24@"NSURL"32i40@?<v@?@"NSArray"@"NSError">44
v64@0:8@"NSArray"16@"IOSurface"24I32@"NSString"36@"NSURL"44i52@?<v@?@"NSArray"@"NSError">56
v44@0:8@"NSArray"16@"NSString"24i32@?<v@?@"NSArray"@"NSError">36
v24@0:8@?<v@?@"NSDictionary">16
v60@0:8i16@"NSArray"20@"NSDictionary"28Q36@"NSArray"44@?<v@?@"NSDictionary"@"NSError">52
v48@0:8i16@"NSURL"20@"NSArray"28B36@?<v@?@"NSDictionary"@"NSError">40
v52@0:8i16Q20@"NSURL"28@"NSDictionary"36@?<v@?@"NSError">44
v60@0:8i16Q20@"NSArray"28@"NSURL"36@"NSDictionary"44@?<v@?@"NSDictionary"@"NSError">52
v24@0:8@?<v@?@"NSError">16
v24@0:8@?<v@?Q>16
v24@0:8@"NSURL"16
v60@0:8i16@"NSString"20@"NSArray"28@"NSArray"36@"NSURL"44@?<v@?@"NSArray"@"NSError">52
v48@0:8i16@"NSArray"20B28@"NSURL"32@?<v@?B@"NSError">40
v44@0:8i16@"NSArray"20@"NSURL"28@?<v@?@"NSArray"@"NSError">36
v36@0:8i16@"NSURL"20@?<v@?B@"NSError">28
v44@0:8i16@"NSDictionary"20@"NSURL"28@?<v@?@"NSString"@"NSError">36
v40@0:8i16B20@"NSURL"24@?<v@?@"NSDictionary"@"NSError">32
v36@0:8i16@"NSURL"20@?<v@?@"NSDictionary"@"NSError">28
v44@0:8i16@"NSArray"20@"NSURL"28@?<v@?B@"NSError">36
v32@0:8@"NSURL"16@?<v@?@"NSDictionary"@"NSError">24
v40@0:8@"NSURL"16Q24@?<v@?@"NSString"@"NSError">32
v44@0:8i16@"NSURL"20@"NSArray"28@?<v@?@"NSDictionary"@"NSError">36
v52@0:8i16@"NSURL"20@"NSURL"28@"NSURL"36@?<v@?@"NSDictionary"@"NSError">44
v44@0:8i16@"NSURL"20@"NSURL"28@?<v@?@"NSDictionary"@"NSError">36
v44@0:8@"NSArray"16@"NSURL"24i32@?<v@?@"NSDictionary"@"NSError">36
v52@0:8Q16@"NSArray"24@"NSURL"32i40@?<v@?@"NSError">44
v28@0:8d16i24
i56@0:8Q16@24@32@?40@?48
i52@0:8@16@24B32@?36@?44
i44@0:8@16B24@?28@?36
i48@0:8Q16@24@?32@?40
i48@0:8@16@24@?32@?40
i32@0:8@16@?24
i40@0:8@16Q24@?32
@"NSXPCConnection"
i52@0:8@16B24@28@?36@?44
i40@0:8@16@?24@?32
i44@0:8B16@20@?28@?36
i48@0:8@16@24@32@?40
v48@0:8@"NSDictionary"16@"NSString"24@"NSURL"32@?<v@?>40
v40@0:8@"NSString"16@"NSURL"24@?<v@?@"NSString">32
q24@0:8@16
Q32@0:8Q16Q24
@52@0:8@16Q24@32@40B48
@48@0:8Q16@24@32^@40
@48@0:8@16@24Q32^@40
@56@0:8Q16@24@32@40@48
i48@0:8@16Q24@?32@?40
v40@0:8@16@24^q32
v88@0:8@16#24{?={?=qiIq}{?=qiIq}}32@80
@108@0:8#16@24@32@40{?={?=qiIq}{?=qiIq}}48B96^B100
v48@0:8@16@24^q32^f40
v144@0:8@16{?={?=qiIq}{?=qiIq}}24@72{?={?=qiIq}{?=qiIq}}80^q128^f136
i56@0:8Q16@24@32@?40@48
i52@0:8@16Q24B32@?36@?44
@52@0:8Q16@24B32@?36^@44
@56@0:8Q16@24@32@?40^@48
@44@0:8@16@24Q32B40
{?={?=qiIq}{?=qiIq}}32@0:8@16@24
@"NSNumber"
@"MADVIVisualSearchGatingRequest"
@"<VICancellable>"
@"AVAssetReaderOutputMetadataAdaptor"
@40@0:8@16@24Q32
@72@0:8{?={?=qiIq}{?=qiIq}}16^Q64
@40@0:8@16r^{?={?=qiIq}{?=qiIq}}24Q32
i48@0:8@16r^{?={?=qiIq}{?=qiIq}}24Q32@?40
i64@0:8@16{?=qiIq}24Q48@?56
@32@0:8@?16^B24
@"VCPAsset"
Q40@0:8^{?=Q^@^Q[5Q]}16^@24Q32
i36@0:8^{__CVBuffer=}16^^{__CVBuffer}24i32
i80@0:8^{__CVBuffer=}16^v24{?=qiIq}32{?=qiIq}56
{Scaler="pool_"^{__CVPixelBufferPool}"width_"i"height_"i"crop_rect_"{CGRect="origin"{CGPoint="x"d"y"d}"size"{CGSize="width"d"height"d}}"sw_scaler_"^{OpaqueVTPixelTransferSession}}
@"VCPMotionFlowRequest"
{vector<__CVBuffer *, std::allocator<__CVBuffer *>>="__begin_"^^{__CVBuffer}"__end_"^^{__CVBuffer}"__end_cap_"{__compressed_pair<__CVBuffer **, std::allocator<__CVBuffer *>>="__value_"^^{__CVBuffer}}}
@24@0:8i16i20
{?={?=qiIq}{?=qiIq}}32@0:8@16f24B28
i48@0:8@16f24f28f32B36@?40
i56@0:8B16@20@28f36f40f44B48B52
f28@0:8f16@20
f48@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16
@"VNSession"
Q56@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24
{CGRect={CGPoint=dd}{CGSize=dd}}24@0:8@16
i40@0:8@16@24@32
B24@0:8Q16
B28@0:8Q16B24
i44@0:8Q16B24@?28@?36
@"VNPersonsModel"
@"VNEntityIdentificationModel"
i40@0:8^Q16^Q24@?32
Q32@0:8q16Q24
@40@0:8f16{CGPoint=dd}20B36
v32@0:8r^f16^v24
i32@0:8^v16^v24
i40@0:8^v16^v24^f32
f32@0:8[3[3f]]16[3f]24
i56@0:8^v16r^f24[3[3f]]32[3f]40^f48
i56@0:8^v16^v24[4f]32^v40^v48
i40@0:8^v16^v24[4f]32
i36@0:8r^f16r^f24i32
[4[3f]]
@"VNSceneprint"
B24@0:8^v16
[10f]
@"VCPSceneChangeSegment"
S16@0:8
v20@0:8S16
@"VCPVNImageprintWrapper"
v24@0:8f16f20
v52@0:8^f16^f24^f32i40^f44
v48@0:8r^f16r^i24r^f32^f40
v48@0:8r^f16r^f24r^f32^f40
v32@0:8r^f16^f24
v52@0:8r^f16i24r^f28r^f36^f44
v36@0:8r^f16^f24B32
B24@0:8^f16
v56@0:8^f16^f24^f32^f40^f48
v44@0:8^f16^f24^f32i40
v68@0:8^f16^f24^f32^f40^f48^f56i64
{matrix<double, 6, 1, dlib::memory_manager_stateless_kernel_1<char>, dlib::row_major_layout>={layout<double, 6, 1, dlib::memory_manager_stateless_kernel_1<char>, 1>=[6d]}}16@0:8
B28@0:8i16B20B24
{?=[4]}16@0:8
^16@0:8
@"VCPFaceTensorModel"
[200{?="x"f"y"f"index"i}]
[200B]
[8f]
[9f]
[12f]
[3f]
[126f]
[189f]
@"VCPPnPSolver"
[51f]
i84@0:8^{__CVBuffer=}16^v24{?=qiIq}32{?=qiIq}56i80
@"VCPImageMotionFlowAnalyzer"
@"VNRequest"
@24@0:8^{__sFILE=*iiss{__sbuf=*i}i^v^?^?^?^?{__sbuf=*i}^{__sFILEX}i[3C][1C]{__sbuf=*i}iq}16
v32@0:8^f16^f24
v44@0:8^f16i24^f28^f36
^i16@0:8
i40@0:8^{?=^v^v[4Q][4Q]QQQQQQQQQQi}16@24@32
i64@0:8^ 16Q24Q32Q40@48@56
i40@0:8^{CGPoint=dd}16^f24^@32
v64@0:8^f16^f24Q32Q40Q48Q56
v28@0:8i16^f20
v40@0:8^f16Q24Q32
i56@0:8^f16@24^{CGPoint=dd}32^f40^@48
i52@0:8@16f24{CGPoint=dd}28^f44
i32@0:8@16^f24
[8^f]
@"VCPGaborFilter"
i68@0:8{?={?=qiIq}{?=qiIq}}16f64
i64@0:8{?=qiIq}16{?=qiIq}40
@"VCPSegment"
^{HinkleyDetector=ffi{HinkleyStats=ffff}}
^{__CVBuffer=}24@0:8Q16
v24@0:8@"NSCoder"16
@24@0:8@"NSCoder"16
@32@0:8r^16Q24
r^16@0:8
@96@0:8{?=[4]}16@80@88
@"VCPFaceGeometry"
{?="columns"[4]}
@80@0:8Q16{CGAffineTransform=dddddd}24@72
@40@0:8Q16@24@32
@88@0:8Q16{CGAffineTransform=dddddd}24f72@76B84
B32@0:8r^{CGAffineTransform=dddddd}16@24
{CGAffineTransform=dddddd}64@0:8{CGAffineTransform=dddddd}16
{CGAffineTransform=dddddd}28@0:8i16^{__CVBuffer=}20
{?=[4]}84@0:8{?=[4]}16i80
@88@0:8^{__CVBuffer=}16{?=qiIq}24{?=qiIq}48@72^@80
i88@0:8^{__CVBuffer=}16{?=qiIq}24{?=qiIq}48@72@?80
i72@0:8{?={?=qiIq}{?=qiIq}}16@64
B68@0:8{?=qiIq}16{?=qiIq}40B64
@"VCPVideoFacePoseAnalyzer"
@"VCPVideoFaceMeshAnalyzer"
@"VCPFullVideoAnalyzer"
@"VCPImageBlurAnalyzer"
@"VCPAudioAnalyzer"
@"VCPVideoFullFaceDetector"
@"VCPSceneChangeAnalyzer"
@"VCPLightMotionAnalyzer"
@"VCPTrimAnalyzer"
@"VCPHomeKitMotionAnalyzer"
^{Rotator=^{__CVPixelBufferPool}iii^{OpaqueVTImageRotationSession}}
v40@0:8*16Q24^f32
i24@0:8^Q16
i32@0:8^{__CVBuffer=}16^Q24
v24@0:8@"PHPhotoLibrary"16
@"NSObject<OS_dispatch_semaphore>"
@"NSMutableData"
@"NSURLSessionDataTask"
@56@0:8@16@24Q32@?40@?48
@56@0:8@16Q24Q32Q40@48
@"VCPProtoTime"
@52@0:8^{__CVBuffer=}16I24@28@36@44
i32@0:8^^{__CVBuffer}16^I24
@52@0:8@16^{__CVBuffer=}24I32@36@44
i36@0:8^{__CVBuffer=}16^{__CVBuffer=}24i32
i40@0:8^{__CVBuffer=}16^{__CVBuffer=}24i32i36
@40@0:8^{__CVBuffer=}16^{__CVBuffer=}24^@32
@"VCPModelR2D2"
@"VCPVideoActivityDescriptor"
v52@0:8Q16@24i32@36@?44
v52@0:8Q16@"NSData"24i32@"NSDictionary"36@?<v@?@"NSDictionary"@"NSError">44
v52@0:8Q16@"IOSurface"24i32@"NSDictionary"36@?<v@?@"NSDictionary"@"NSError">44
v44@0:8i16@"NSData"20@"NSDictionary"28@?<v@?@"NSString"@"NSError">36
v36@0:8i16@"NSDictionary"20@?<v@?@"NSDictionary"@"NSError">28
i32@0:8^{CGImage=}16^@24
B32@0:8@"AVAssetResourceLoader"16@"AVAssetResourceLoadingRequest"24
B32@0:8@"AVAssetResourceLoader"16@"AVAssetResourceRenewalRequest"24
v32@0:8@"AVAssetResourceLoader"16@"AVAssetResourceLoadingRequest"24
B32@0:8@"AVAssetResourceLoader"16@"NSURLAuthenticationChallenge"24
v32@0:8@"AVAssetResourceLoader"16@"NSURLAuthenticationChallenge"24
v24@0:8I16B20
^{__SCNetworkReachability=}
@"CVNLPCommSafetyHandler"
@"MADImageSafetyClassificationRequest"
@"PVImage"
@64@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24^@56
@48@0:8@16@?24@?32@40
@56@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24
B48@0:8@16@24@32^@40
i32@0:8@?16@?24
@"VCPFaceAnalyzer"
i48@0:8^{CGPoint=dd}16[21f]24@32@40
i40@0:8^f16^{__CVBuffer=}24@32
{vector<int, std::allocator<int>>="__begin_"^i"__end_"^i"__end_cap_"{__compressed_pair<int *, std::allocator<int>>="__value_"^i}}
@"VCPCNNHandsDetector"
@"VCPCNNHandKeypointsDetector"
@"VCPCNNFastGestureRecognition"
@"NSDate"
{?="quality"b1"statsFlags"b1"typesWide"b1}
B40@0:8@16#24@32
@48@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16
i20@0:8f16
i40@0:8^{__CVBuffer=}16^{__CVBuffer=}24@?32
^f32@0:8^i16^i24
@"SHMutableSignature"
{?="faceSharpness"b1}
@"VCPProtoLine"
@"VCPProtoPoint"
{?="underExpose"b1}
^{LkFsMeasure=IIqBIIddddqqIII[30[6f]]^f^f^f^{DspLibBiquad}^{DspLibBiquad}}
^{CAStreamBasicDescription=dIIIIIIII}
{vector<double, std::allocator<double>>="__begin_"^d"__end_"^d"__end_cap_"{__compressed_pair<double *, std::allocator<double>>="__value_"^d}}
{vector<float, std::allocator<float>>="__begin_"^f"__end_"^f"__end_cap_"{__compressed_pair<float *, std::allocator<float>>="__value_"^f}}
^{AUOutputBL={CAStreamBasicDescription=dIIIIIIII}*^{AudioBufferList}III}
{?="faceQuality"b1}
@"VCPImageHandsAnalyzer"
v36@0:8@16Q24B32
@48@0:8{CGPoint=dd}16{CGPoint=dd}32
@"VCPProtoLivePhotoVariationParams"
{?="epoch"b1"flags"b1}
@40@0:8^{__CVBuffer=}16@24^@32
f24@0:8Q16
v32@0:8^f16Q24
{?="list"^f"count"Q"size"Q}
@48@0:8i16B20B24@28@36i44
i48@0:8[21{CGPoint=dd}]16^f24@32@40
{?="loopFadeLen"b1"loopPeriod"b1"loopStart"b1}
v32@0:8@"NSDictionary"16@?<v@?@"NSError">24
v32@0:8@"NSDictionary"16@?<v@?@"NSDictionary"@"NSError">24
v40@0:8@"NSData"16@"NSDictionary"24@?<v@?@"NSDictionary"@"NSError">32
@40@0:8@16@?24@?32
{CF<const opaqueCMFormatDescription *>="value_"^{opaqueCMFormatDescription}}
@"VCPHomeKitAnalysisSession"
{CLLocationCoordinate2D=dd}16@0:8
@"VNCanceller"
@40@0:8Q16Q24^@32
@"NSDictionary"32@0:8Q16^@24
@"NSNumber"40@0:8Q16Q24^@32
@"NSDictionary"40@0:8@"NSArray"16@"NSArray"24^@32
@48@0:8@16@24@?32@?40
B20@0:8B16
v40@0:8{?=QQBB}16
B56@0:8@16@24@32@40^@48
@44@0:8@16B24@28@36
@36@0:8@16B24@28
v32@0:8@16^@24
B40@0:8^@16^Q24^@32
B28@0:8B16^@20
Q28@0:8B16^@20
@48@0:8@16@24@?32^@40
B44@0:8@16Q24B32^@36
B48@0:8^@16^d24^B32^@40
@"VCPPhotosPersistenceDelegate"
@"NSMutableSet"
@"VNClustererBuilder"
@"VCPSuggestionRequest"
@"NSLock"
{?="countOfEligibleFaces"Q"countOfFacesPendingToAdd"Q"isClustering"B"rebuildRequired"B}
{mach_timebase_info="numer"I"denom"I}
@"VCPCNNPersonDetector"
@"VCPCNNPersonKeypointsDetector"
@40@0:8i16B20B24@28i36
i48@0:8^f16i24i28@32[3[2f]]40
i36@0:8r^v16@24i32
i72@0:8*16i24i28i32{CGPoint=dd}36{CGPoint=dd}52i68
i44@0:8*16i24i28i32^{CGPoint=dd}36
v28@0:8B16@?20
v44@0:8@16B24@28@?36
@64@0:8@16@24@32@?40@48^@56
@56@0:8@16@24@32@?40^@48
v64@0:8@16@24@32@40@?48@?56
v48@0:8@16@?24@?32@?40
@"VCPClusterer"
v32@0:8Q16Q24
i48@0:8@16Q24^@32@?40
@32@0:8@16q24
S24@0:8@16
@20@0:8S16
S24@0:8q16
i36@0:8^@16#24i32
@"VCPFaceClusterer"
v40@0:8i16i20^f24^f32
{CGRect={CGPoint=dd}{CGSize=dd}}56@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16f48f52
f80@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{CGRect={CGPoint=dd}{CGSize=dd}}48
@56@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16@48
i92@0:8@16@24i32^{__CVBuffer=}36{?=qiIq}44{?=qiIq}68
@"VCPVideoObjectTracker"
{?="distanceToPreviousScene"b1"flickerScore"b1"sceneprintDistanceToPreviousScene"b1}
B36@0:8@16B24^@28
B60@0:8@16B24@28@?36@?44^@52
@"VCPProtoVideoKeyFrame"
^{CGImageMetadata=}24@0:8@16
@88@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{CGRect={CGPoint=dd}{CGSize=dd}}48@80
@72@0:8r^{CGImageSource=}16{CGRect={CGPoint=dd}{CGSize=dd}}24@56^@64
@72@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24@56^@64
{CGRect={CGPoint=dd}{CGSize=dd}}32@0:8@16^@24
@32@0:8{CGPoint=dd}16
@40@0:8{?=qiIq}16
v48@0:8Q16@24@32@?40
v48@0:8Q16@"IOSurface"24@"NSDictionary"32@?<v@?@"NSDictionary"@"NSError">40
v48@0:8Q16^{__CVBuffer=}24@32@?40
@44@0:8@16i24i28i32i36i40
v60@0:8*16i24i28i32^f36^f44^f52
v36@0:8^f16^f24i32
^{LandmarkDetector=iiiiiiiB^f^f^f^i^{ZPoint}^{RegressionTree}^?}
^q16@0:8
q24@0:8Q16
v32@0:8^q16Q24
{?="list"^q"count"Q"size"Q}
@20@0:8I16
@48@0:8{?=qiIq}16f40B44
v72@0:8{?={?=qiIq}{?=qiIq}}16f64B68
v44@0:8{?=qiIq}16B40
i88@0:8{?=qiIq}16{?=qiIq}40{?=qiIq}64
f56@0:8i16i20^{?={?=qiIq}{?=qiIq}}24{?=qiIq}32
B40@0:8{?=qiIq}16
B64@0:8{?=qiIq}16{?=qiIq}40
@"VCPActionAnalyzer"
@"AVURLAsset"
i68@0:8@16@24@32@40@48@56f64
f80@0:8{?={?=qiIq}{?=qiIq}}16@64^i72
@68@0:8{?={?=qiIq}{?=qiIq}}16f64
@"MADVIMachineReadableCodeDetectionRequest"
v24@0:8^{EncodeStats=^^?^B^B^{MotionVector}^{MotionVector}^{MotionVector}^{MotionVector}^S^S^I^S^S^S^S^S^S^S^S^S^S^SIBBBii}16
@"VCPHumanPoseImageRequest"
@96@0:8{CGAffineTransform=dddddd}16@64@72B80B84@?88
@28@0:8f16B20B24
v40@0:8i16i20i24^f28i36
i84@0:8^{__CVBuffer=}16{CGRect={CGPoint=dd}{CGSize=dd}}24i56{?=qiIq}60
r^f16@0:8
@"VCPRTLandmarkDetector"
@"VCPFaceShapeModel"
[5@"VCPLandmarkValidator"]
@40@0:8@16@24d32
@52@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16f48
@72@0:8@16@24{?=qiIq}32^{?=qiIq}56@64
v104@0:8{?={?=qiIq}{?=qiIq}}16@64@72{?=qiIq}80
@"VCPImagePetsAnalyzer"
@20@0:8f16
B20@0:8f16
i80@0:8^{__CVBuffer=}16{CGRect={CGPoint=dd}{CGSize=dd}}24{?=qiIq}56
v80@0:8{?=[4]}16
@"VCPCNNFaceLandmarkDetector"
@"VCPVideoFacePoseFilter"
[14f]
[21f]
i32@0:8[3[3f]]16[3f]24
i32@0:8[3f]16[3[3f]]24
i32@0:8[3f]16[3f]24
i24@0:8^{?=[4]}16
{Matrix<float, 12, 1, false>="m_data"[12f]}
{Matrix<float, 12, 12, false>="m_data"[144f]}
@80@0:8{CGAffineTransform=dddddd}16@64@72
i32@0:8^{__CVBuffer=}16@24
B84@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{CGRect={CGPoint=dd}{CGSize=dd}}48B80
i56@0:8^{__CVBuffer=}16{?=qiIq}24@48
@"VCPCNNSmileDetector"
@"VCPCNNPoseEstimator"
B72@0:8{?={?=qiIq}{?=qiIq}}16@64
B28@0:8@16f24
f72@0:8{?={?=qiIq}{?=qiIq}}16@64
f20@0:8f16
i48@0:8@16{?=qiIq}24
v28@0:8B16@20
v60@0:8B16f20f24f28f32f36f40B44f48f52B56
@132@0:8{CGAffineTransform=dddddd}16{?={?=qiIq}{?=qiIq}}64B112@116@124
@"VCPImageFaceQualityAnalyzer"
@76@0:8{CGAffineTransform=dddddd}16@64B72
@88@0:8@16{CGAffineTransform=dddddd}24Q72^{opaqueCMFormatDescription=}80
@"VCPVideoMetaFocusSegment"
@48@0:8q16{?=qiIq}24
v48@0:8q16{?=qiIq}24
@32@0:8Q16^{opaqueCMFormatDescription=}24
B24@0:8^{opaqueCMFormatDescription=}16
{CGSize=dd}24@0:8^{opaqueCMFormatDescription=}16
@24@0:8^{opaqueCMFormatDescription=}16
^{__CFData=}24@0:8^{opaqueCMFormatDescription=}16
^{__CFData=}28@0:8I16^{__CFData=}20
i32@0:8^{FigLivePhotoMetadata=I{FigLivePhotoMetadataV1Struct=fqffffffccSI[0{FigLivePhotoDetectedFaceV1Struct=qffffisS}]}}16@24
{CGVector="dx"d"dy"d}
@"VCPVideoMetaFocusAnalyzer"
@"VCPVideoMetaMotionAnalyzer"
@"VCPVideoMetaLensSwitchAnalyzer"
{HinkleyDetector="sensitivity_"f"threshold_"f"min_length_"i"stats_"{HinkleyStats="upper_"f"lower_"f"max_"f"min_"f}}
@"VCPVideoMetaMotionSegment"
@44@0:8f16{?=qiIq}20
v44@0:8f16{?=qiIq}20
@80@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16^{__CVBuffer=}48{?=qiIq}56
@"VCPCtrTracker"
v32@0:8@16^{__CVBuffer=}24
f92@0:8f16{CGRect={CGPoint=dd}{CGSize=dd}}20{CGRect={CGPoint=dd}{CGSize=dd}}52i84i88
B48@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16
@28@0:8@16f24
f84@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{CGRect={CGPoint=dd}{CGSize=dd}}48B80
@"VCPImageSaliencyAnalyzer"
@32@0:8@16f24f28
v76@0:8@16{?=qiIq}24{?=qiIq}48B72
v72@0:8@16{?={?=qiIq}{?=qiIq}}24
@"VCPSceneTaxonomy"
{?=ii}24@0:8@16
^{opaqueCMSampleBuffer=}16@0:8
@"AVAssetTrack"
@44@0:8@16r^{?={?=qiIq}{?=qiIq}}24@32B40
@40@0:8@16r^{?={?=qiIq}{?=qiIq}}24r^{?=qiIq}32
@"AVAssetReader"
@"AVAssetReaderTrackOutput"
^{opaqueCMSampleBuffer=}
@32@0:8@16r^{?={?=qiIq}{?=qiIq}}24
i28@0:8B16^{?={?=qiIq}{?=qiIq}}20
i72@0:8{?={?=qiIq}{?=qiIq}}16^^{opaqueCMSampleBuffer}64
@"AVAssetReaderSampleReferenceOutput"
[2^{opaqueCMSampleBuffer}]
v40@0:8r^{?=qiIq}16r^{?=qiIq}24@32
{AudioStreamBasicDescription="mSampleRate"d"mFormatID"I"mFormatFlags"I"mBytesPerPacket"I"mFramesPerPacket"I"mBytesPerFrame"I"mChannelsPerFrame"I"mBitsPerChannel"I"mReserved"I}
i24@0:8r^{AudioStreamBasicDescription=dIIIIIIII}16
^{OpaqueAudioComponentInstance=}
v32@0:8^{__CVBuffer=}16^{CGPoint=dd}24
^{CGPoint=dd}16@0:8
v24@0:8^{CGPoint=dd}16
^{CGPoint=dd}
^{?=^{?}^{?}^{?}^{tplTracker_resampler_context}^{?}}
mcpl
v024
ARGB
v024
mcpl
