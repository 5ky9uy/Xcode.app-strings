ASRPreheatContextTimestamp
ASRAppleNeuralEngineModelInitializationContextTimestamp
ASRAppleNeuralEngineModelInitializationContextFileName
-[ESSelfHelper initWithTask:isSpeechAPIRequest:]
-[ESSelfHelper logFinalResultGeneratedWithEARPackage:]
-[ESSelfHelper logRequestEndedOrFailedOrCancelledWithError:recognizerComponents:averageActiveTokensPerFrame:languageModelInterpolationWeights:signalToNoiseRatioInDecibels:recognitionDurationInSec:audioDurationMs:eagerUsed:utteranceDetectionEnabled:utteranceConcatenationEnabled:cpuRealTimeFactor:numLmeDataStreams:phoneticMatchDecoderName:pauseDurations:itnDurationInNs:isEmojiPersonalizationUsed:isEmojiDisambiguationUsed:isEmojiExpectedButNotRecognized:recognizedEmojis:totalITNDurationInNs:totalITNRuns:totalSecondaryPassDurationInNs:totalSecondaryPasses:cpuInstructionsInMillionsPerSecond:]
floatValue
-[ESSelfHelper wrapAndEmitTopLevelEvent:timestampInTicks:]
v32@?0@"NSArray"8Q16^B24
CESRProfileErrorDomain
\NT-contact
\NT-appname
\NT-correction
overrides
keyboardLM
locationOfInterest
spatialLocationOfInterest
interaction
search
calendarEvent
pexNamedEntity
frequency
charsToTrim
charsToSplit
tagName
templateName
tagNameList
minimumWordLength
com.apple.siri.embeddedspeech.profilegeneration-keepalive
-[ESSpeechProfileBuilderConnection initWithXPCConnection:]_block_invoke
v8@?0
Could not create EAR profile builder
-[ESSpeechProfileBuilderConnection getVersionForCategory:completion:]
-[ESSpeechProfileBuilderConnection beginWithCategoriesAndVersions:bundleId:completion:]
Begin called while there are already active categories.
Speech category %@ has already been committed with a call to Begin followed by Finish
Speech category %@ is unsupported
Add called before categories have been set with Begin
-[ESSpeechProfileBuilderConnection addVocabularyItems:isBoosted:completion:]
-[ESSpeechProfileBuilderConnection finishAndSaveProfile:completion:]
v32@?0@"NSString"8@"NSMutableArray"16^B24
Failed to write profile
Failed to load speech assets
-[ESSpeechProfileBuilderConnection _profileWithError:]
Failed to read the existing speech profile
+[ESSpeechProfileBuilderConnection personalizationRecipeForAssetConfig:modelOverridePath:]
personalization.json
+[ESSpeechProfileBuilderConnection userProfileWithAssetConfig:modelOverridePath:overrides:isJit:returningFoundPath:error:]
mini.json
en_US_napg.json
vocdelta.voc
pg.voc
mrec.psh
Error during _EARUserProfile initialization
v32@?0@"CESRVocabularyCategory"8@"NSSet"16^B24
\correction-first
+[ESSpeechProfileBuilderConnection _adaptRecipe:userData:profile:]_block_invoke
v32@?0@"NSString"8@"NSDictionary"16^B24
+[ESSpeechProfileBuilderConnection _parseRequiredParameter:expectedClass:domain:recipe:error:]
Missing %@ for %@
+[ESSpeechProfileBuilderConnection addWordsToUserProfile:templateName:wordArrays:]
v32@?0@"NSString"8@"NSString"16^B24
+[ESSpeechProfileBuilderConnection _runAdaptationRecipeForNamesWithFrequency:tagNames:templateName:charSetToTrim:charSetToSplit:minimumWordLength:nameFrequencies:profile:]
+[ESSpeechProfileBuilderConnection _runAdaptationRecipeForNamesWithFrequency:tagNames:templateName:charSetToTrim:charSetToSplit:minimumWordLength:nameFrequencies:profile:]_block_invoke_2
v32@?0@"NSString"8Q16^B24
v32@?0@"NSString"8@"NSNumber"16^B24
+[ESSpeechProfileBuilderConnection _runAdaptationRecipeForKeyboardLMWithFrequency:tagName:templateName:charSetToTrim:charSetToSplit:minimumWordLength:userData:profile:]
+[ESSpeechProfileBuilderConnection _runAdaptationRecipeForLocationOfInterestWithFrequency:tagName:templateName:charSetToTrim:charSetToSplit:minimumWordLength:locationOfInterestNames:profile:]
+[ESSpeechProfileBuilderConnection _runAdaptionRecipeForCalendarEventsWithFrequency:tagName:templateName:charSetToTrim:charSetToSplit:minimumWordLength:userData:profile:]
com.apple.MobileSMS
Dictation
ResultCandidateId
SpeechProfile
EARUserProfileContainerLoadDate
+[CESRUtilities speechProfileRootDirectories]
+[CESRUtilities speechProfilePathsWithLanguage:]
+[CESRUtilities loadSpeechProfiles:language:]
v32@?0@"_EARSpeechRecognitionToken"8Q16^B24
Param
v32@?0{_NSRange=QQ}8^B24
v32@?0@"EARVoiceCommandArgument"8Q16^B24
@"NSArray"24@?0@"NSArray"8@"NSArray"16
+[CESRUtilities AFSpeechInfoPackageForEARSpeechRecognitionResultPackage:]_block_invoke
unconstrained
reduced
avoid
com.apple.assistant
AFSpeechLatticeMitigatorResultForEar
quasarModelPath
type
com.apple.trial.NamespaceUpdate.SIRI_UNDERSTANDING_ASR_ASSISTANT
trial
GeoLMAssetsInfo
com.apple.siri.embeddedspeech.ESAssetManager
v12@?0i8
-[ESAssetManager registerNotifications]
-[ESAssetManager installationStatusForLanguagesIgnoringCache:assetType:withDetailedStatus:withError:]
-[ESAssetManager _invalidateInstallationStatusCacheForAssetType:]
-[ESAssetManager installedModelInfoForAssetConfig:error:triggerDownload:ignoreSpellingModel:]
-[ESAssetManager purgeInstalledAssetsExceptLanguages:assetType:error:]_block_invoke
-[ESAssetManager startMissingAssetDownload]
com.apple.internal.ck
disableTrialAssetDelivery
-[ESAssetManager isTrialAssetDeliveryEnabled]
enableTrialAssetDelivery
-[ESAssetManager prepareHammerConfigFile:]
-[ESAssetManager _installedGeoLMRegionMappingForLanguage:]
-[ESAssetManager geoLMRegionIdForLanguage:location:]
-[ESAssetManager installedGeoLMRegionSpecificAssetForLanguage:regionId:mainAssetConfig:]
-[ESAssetManager _geoLMCompatibleWithMainAsset:geoAssetConfig:]
-[ESAssetManager purgeUnusedGeoLMAssetsForLangauge:]_block_invoke
q24@?0@8@16
-[ESAssetManager purgeUnusedGeoLMAssetsForLangauge:]_block_invoke_2
-[ESAssetManager lastUsedGeoLMRegionIdForLanguage:]
-[ESAssetManager _purgeUserDefaultsGeoLMAssetsInfoDictExceptLanguages:cesrTrialAssetManager:]
ondevice_preheat_time
Failed
Success
AlreadyLoaded
-[ESProfiler logPendingPreheatContextEvents:]
ES: ANE Model Init
-[ESProfiler logPendingANEModelInitializationContextEvents:]
ES: Engine Init
ES: Update Active Config
ES: JIT Profile Build Load
ES: AOT Profile Load
confidenceThresholds
confidenceModels
wordConfidenceThreshold
utteranceConfidenceThreshold
continuousListening
shouldHandleCapitalization
QuasarModel
replicateDataPacketPersonalization
distributedEvaluation
adaptation
WallRTF
DecodeDuration
AverageActiveTokensPerFrame
lm_interp_weights
jitQueryDurationInMs
jitLmeDurationInMs
jitDataStats
version
data
language
assetPath
PERSONALINFO
com.apple.fides.asr
com.apple.siri.speech-dictation-personalization
dictation
ModelOverrideURL
TP/tantor/voice_commands
textfield-editing-suite.plist
com.apple.siri.ESConnection
-[ESConnection initWithXPCConnection:]
-[ESConnection initWithXPCConnection:]_block_invoke
-[ESConnection dealloc]
com.apple.siri.ESConnection.fidesEval
com.apple.siri.ESConnection.redecode
v24@?0@"NSDictionary"8@"NSError"16
-[ESConnection fetchModelInfoForAssetConfig:triggerDownload:completion:]
Corrupt assets found at: %@
-[ESConnection fetchModelPropertiesForAssetConfig:completion:]_block_invoke
-[ESConnection getOfflineAssetStatusIgnoringCache:assetType:withDetailedStatus:withCompletion:]
SiriX
enableTelemetry=YES
+[ESConnection _speechRecognizerWithAssetConfig:geoLMRegionId:enableITN:overrides:modelOverrideURL:preloadModels:requestSource:enableParallelLoading:isHighPriority:geoLMLoadedOut:error:]
enableParallelLoading
preheatSource
ASR task for memory lock
taskForMemoryLock
Siri
dictation_emoji_recognition
dictation_voice_commands
Failed to create recognizer from %@
keepANEModelLoaded
-[ESConnection preheatSpeechRecognitionWithAssetConfig:preheatSource:modelOverrideURL:]
Preheating
 with CustomModelURL %@
(none)
Failure
Preheat time interval in seconds: %lf
-[ESConnection shouldWriteDictationRecord:]
v24@?0@"CESRModelProperties"8@"NSError"16
-[ESConnection startSpeechRecognitionWithParameters:didStartHandlerWithInfo:]
Recognizer is busy
@"NSDictionary"8@?0
jitDataProcessing
-[ESConnection startSpeechRecognitionWithParameters:didStartHandlerWithInfo:]_block_invoke
-[ESConnection sendSpeechCorrectionInfo:interactionIdentifier:]_block_invoke
\jit
v32@?0@"NSData"8Q16^B24
-[ESConnection readProfileAndUserDataWithLanguage:allowOverride:completion:]
Not a dictionary: %@
Not an array: %@
orth
prons
freq
v32@?0@"NSString"8@"NSArray"16^B24
updateSpeechProfileWithLanguage is currently unsupported.
-[ESConnection _scheduleCompilationTimer:]_block_invoke
-[ESConnection _scheduleCompilationTimer:]
Cooldown timeout for EAR
+[ESConnection _scheduleCooldownTimer]
com.apple.siri.embeddedspeech.keepalive
+[ESConnection _cancelCooldownTimer]
+[ESConnection _cooldownTimerFired]
+[ESConnection _cachedRecognizerCleanUp]
+[ESConnection prepareToExit]
+[ESConnection _sendPendingAnalyticsEvents]
recipeType
-[ESConnection readTableFromURL:]
-[ESConnection runRecipeEvaluationDistributedEvaluation:recordData:attachments:completion:]
returnHypothesis
returnOverallWER
returnOverallRTF
returnPerUtteranceWER
returnPerUtteranceRTF
locale
task
sampleRate
wordSenseWhitelist
wav.scp
raw.ref
-[ESConnection runRecipeEvaluationDistributedEvaluation:recordData:attachments:completion:]_block_invoke
token
confidence
transcription
\\\S+$
EditDistance
details
modelVersion
overall-rtf
overall-wer
v24@?0@"NSData"8@"NSString"16
SiriCoreLocalSpeechUserData
-[ESConnection runDefaultAdaptationEvaluation:recordData:attachments:completion:]
Recipe has no profile
Voicemail audio file deleted by user
%@ %@
.model
Malformed overrides
ConfidenceMean
ConfidenceMin
ConfidenceMax
AlternateConfidenceMean
AlternateConfidenceMin
AlternateConfidenceMax
Baseline
CustomModel
ModelVersion
Adapted
Alignment
name
callStackReturnAddresses
callStackSymbols
reason
userInfo
(unknown C++ exception)
-[ESConnection redecodeWithAudioDatas:language:task:samplingRate:completion:]_block_invoke
Interrupted corrected text redecoding during speech recognition
-[ESConnection runCorrectedTextEvaluationWithAudioDatas:recordDatas:language:samplingRate:caseSensitive:skipLME:wordSenseAccessListSet:completion:]_block_invoke
No tokenizer for %@
Interrupted corrected text evaluation during speech recognition
-[ESConnection runCorrectedTextEvaluationWithAudioDatas:recordDatas:language:samplingRate:caseSensitive:skipLME:wordSenseAccessListSet:completion:]_block_invoke_2
correctedOutput
recognizedOutput
editDistanceRecognizedCorrected
editDistanceRecognizedTTSASR
timestamp
interactionId
asrSelfComponentIdentifier
results
-[ESConnection _modelRootWithAssetConfig:modelOverridePath:overrides:error:]
asrSelfComponentId
asset
applicationName
metrics
tokens
alignments
uttInfos
uttInfosCompressed
alignmentReferences
usePersonalizedLM
corrected
recognized
useJIT
disableAOT
contextualData
overrideFiles
restoreAOT
evaluations
-[ESConnection runEvaluationWithDESRecordDatas:language:recipe:attachments:fidesPersonalizedLMPath:fidesPersonalizedLMTrainingAsset:scrubResult:completion:]_block_invoke
Unknown evaluation name found in alignmentReferences: %@
scoreNbest
compress
-[ESConnection runEvaluationWithDESRecordDatas:language:recipe:attachments:fidesPersonalizedLMPath:fidesPersonalizedLMTrainingAsset:scrubResult:completion:]_block_invoke_2
Interrupted evaluation redecoding
Interrupted evaluation tokenization
-[ESConnection _deleteTemporaryDirectoryIfExists:]
-[ESConnection resetCacheAndCompileAllAssetsWithCompletion:]
-[ESConnection pauseRecognition]
-[ESConnection resumeRecognitionWithPrefixText:postfixText:selectedText:]
-[ESConnection _writeDESRecord:oneRecordPerDay:]
-[ESConnection speechRecognizer:didProduceEndpointFeaturesWithWordCount:trailingSilenceDuration:eosLikelihood:pauseCounts:silencePosterior:processedAudioDurationInMilliseconds:]_block_invoke
\entity-first
-[ESConnection speechRecognizer:didProduceLoggablePackage:]_block_invoke
-[ESConnection speechRecognizer:didRecognizePartialResult:]_block_invoke
Unsupported EAR build?
Unsupported EAR package build?
DUMMYTOKEN
-[ESConnection speechRecognizer:didRecognizeFinalResultCandidatePackage:]_block_invoke
-[ESConnection speechRecognizer:didRecognizeFinalResultPackage:]_block_invoke
%@ %@:%f,
%@ TTAW:%f MEAN:%f, MAX:%f, MIN:%f 
RECOGNITION_SUCCESS
RECOGNITION_FAILED
RECOGNITION_CANCELLED
RECOGNITION_REJECTED
-[ESConnection speechRecognizer:didFinishRecognitionWithError:]_block_invoke
recognizer-components
audioDurationMs
recognitionDurationMs
v32@?0@"NSNumber"8Q16^B24
@sum.self
EagerUsed
numLmeDataStreams
PM-decoder
PM-input
PM-output
-[ESConnection speechRecognizer:didProcessAudioDuration:]_block_invoke
tokenName
-[ESConnection speechRecognizer:didRecognizeRawEagerRecognitionCandidate:]_block_invoke_2
v16@?0@"_EARFormatter"8
-[ESConnection speechRecognizer:didRecognizeRawEagerRecognitionCandidate:]_block_invoke
lastPathComponent == %@
_ESDecompressArchiveWithURL
VoiceTriggerFidesArchive
Failed to specify compression algorithm
Failed to specify format
Failed to open file for reading
Unable to extract file to: %@
Task %@ not available for %@, supported tasks are %@
etiquette.json
ReplacementDictionaryForAssetConfig
ReplacementDictionaryForAssetConfig_block_invoke
v32@?0@"NSString"8@16^B24
voicemail_confidence_subtraction.json
sampling rate = %@
task type = %@
far field = %@
device ID = %@
Bluetooth device ID = %@
ANE context = %@
CPU context = %@
GPU context = %@
ReadAudioDataFromFileURL
\\\S*$
Insertions
Deletions
Substitutions
ReferenceSize
\contact-first
CONTACTFIRSTNAME
\contact-middle
CONTACTMIDDLENAME
\contact-last
CONTACTLASTNAME
\contact-nickname
\app-first
APPNAMEFIRSTNAME
\company-first
COMPANYFIRSTNAME
\interaction-first
\interaction-middle
\interaction-last
INLINEFIRSTNAME
\contact-first-derived
\contact-middle-derived
\contact-last-derived
\contact-nickname-derived
Recipe evaluation failed
Empty recognition Result
Error
UtteranceLength
NumberOfNonTerminals
WordsAboveThreshold
UtteranceAboveThreshold
Override file is not found in attachments or device
restoreJIT
configurationFile
JIT LME: JIT profile builder is not initialized
JIT LME: required configuration/file is missing
JIT LME: configuration file is not found in attachments or device
zlibCompressedJson
dictationUIInteractionIdentifier
interactionIdentifier
samplingTimestamp
codec
samplingRate
inferenceSpeakerCode
numTrainedFrames
trainingNnetVersion
isSpeakerCodeUsed
-[ESStoreAudioData initWithUUIDString:language:task:codec:samplingRate:inferenceSpeakerCode:numTrainedFrames:trainingNnetVersion:isSpeakerCodeUsed:isSamplingForDictation:selfLogger:]
-[ESStoreAudioData saveAudioToDisk]
-[ESStoreAudioData _deleteItemAtPath:]
-[ESStoreAudioData _cleanupCacheAndReset:]
-[ESStoreAudioData _saveAudioToCache:]
-[ESStoreAudioData _moveAudioToVarMobile:]
%@.plist
-[ESStoreAudioData _saveAudioMetadataToFilePath:]
-[ESStoreAudioData _createAudioFilePath]
%.0f
%@_%@_%@.pcm
unixTime
samplingDate
success
failed
errorCode
errorDomain
UNDEFINED
description
underlyingErrorCode
underlyingErrorDomain
Audio file to be moved nil
Sampling Date is nil
Unable to create sampling directory
Unable to create dated directory
status
com.apple.siri.embeddedspeech
com.apple.private.des-service
Rejecting %@, no %@ entitlement
Rejecting %@, no %@ or %@ entitlement
main_block_invoke
TB,R,N,V_isSamplingForDictation
AFSpeechInfoPackageForEARSpeechRecognitionResult:
_audioDataPostRecognitionStart:
JSONObjectWithData:options:error:
_fidesEvalQueue
T@"ASRSchemaASRPreheatContext",R,N,V_preheatContext
_isProfileValidFromVersionsMap:
T@"ESSelfHelper",&,N,V_selfLogger
_recognizedText
T@"NSArray",C,N,V_recognizedTokens
_signpostHelper
T@"NSDate",C,N
_userId
T@"NSMutableDictionary",&,N,V_audioMetadata
allKeys
T@"NSNumber",&,N,V_personalizedLmAgeInSec
audioDurationInMs:samplingRate:
T@"NSNumber",R,N,V_numTrainedFrames
captureSnapshot
T@"NSString",&,N
containsObject:
T@"NSString",&,N,V_unrepairedPostItn
countForObject:
T@"NSString",C,N,V_UUIDString
currentCalendar
T@"NSString",C,N,V_correctedText
dealloc
T@"NSString",C,N,V_interactionId
endTime
T@"NSString",C,N,V_logPrefix
indexes
T@"NSString",C,N,V_task
initWithConfig:
T@"NSString",R,C
initWithSuites:resourceBaseURL:
T@"NSString",R,C,N,V_interactionId
isEmojiExpectedButNotRecognized
T@"NSString",R,C,N,V_taskName
isProxy
T@"NSString",R,N,V_recognitionTask
itnDurationInNs
T@"SPIEventContext",R,N,V_powerEventContext
logPowerSnapshotForProcessEnded
TB,N,V_continuousListeningEnabled
markRecognition
TB,R,N,V_hasRecognizedAnything
numberWithBool:
TQ,N,V_samplingRate
phrases
TQ,R,N,V_samplingRate
profile
Tq,N,V_numberOfDeletions
qualify
Tq,N,V_numberOfSubstitutions
recordFromData:
URLByAppendingPathComponent:
saveAudioToDisk
UUID
_UUID
setCorrectedText:interactionId:
_adaptRecipe:userData:profile:
setErrorDomain:
_addPendingProfilerANEModelInitializationEvent:
setInputOrigin:
_applicationName
setModelLocale:
_assetConfig
setWithObjects:
_audioBuffer
_audioDurationProcessingTimestampInTicks
speechProfilePathsWithLanguage:
_audioPackets
stringByAppendingPathComponent:
_bestGuessTaskStringsFromPreheatSource:
version
_bufferedAudioEnded
.cxx_destruct
Td,N,V_collectedAudioDurationMS
AFSpeechInfoPackageForEARSpeechRecognitionResultPackage:
_committedItems
T#,R
_isHighPriority
T@"CESRSignpostHelper",R,N,V_signpostHelper
_preheatContext
T@"ESSelfHelper",R,N,V_selfHelper
_seenCategories
T@"NSArray",C,N,V_selectedAlternatives
_storeAudioData
T@"NSMutableData",&,N,V_audioPackets
addAudioPacket:
T@"NSNumber",&,N
applicationName
T@"NSNumber",&,N,V_personalizedLmWeight
base64EncodedStringWithOptions:
T@"NSNumber",R,N,V_trainingNnetVersion
confidenceScore
T@"NSString",&,N,V_codec
context
T@"NSString",C,N
createANEModelInitializationStartedEventWithTimeStamp:fileName:
T@"NSString",C,N,V_applicationName
T@"NSString",C,N,V_currentAudioFilePath
T@"NSString",C,N,V_language
hasData
T@"NSString",C,N,V_recognizedText
infersQoSFromInstanceUUIDForEAR
T@"NSString",C,N,V_taskName
initWithNSUUID:
T@"NSString",R,C,N,V_asrId
interpretations
T@"NSString",R,C,N,V_language
isFinal
T@"NSString",R,N,V_inferenceSpeakerCode
isSpeakerCodeTrainingSupported:
T@"NSUUID",R,N,V_asrId
logPendingPreheatContextEvents:
T@"SPIPowerLoggerSnapshot",R,N,V_powerSnapshot
lowercaseString
TB,N,V_hasRecognizedAnything
metrics
TB,R,N,V_isSpeakerCodeUsed
oneBest
TQ,R
prepareModelInfo:withAssetType:
Td,N,V_recognitionEndTime
promoteModelInfo:withAssetType:
Tq,N,V_numberOfInsertions
recognitionTask
Tq,N,V_totalCost
release
UTF8String
serviceListener
UUIDString
setAverageActiveTokensPerFrame:
_UUIDString
setEndTimeInNs:
_addPendingAnalyticsEvent:
setExtraLmList:
_addPendingProfilerPreheatEvent:
setIsModifiedByAutoPunctuation:
_asrId
setUserProfile:
_assistantAssetUpdatedNotificationToken
sharedAnalytics
_audioDurationMs
speakerCodeInfo
_audioMetadata
statusForError:
_baseDirectory
tagName
_biomeRecord
vocabularyWords
_bufferedAudioPacketTimestamps
_bufferedAudioPacketTimestampsInTicks
_bufferedAudioPackets
_cachedRecognizerCleanUp
_cancelCooldownTimer
_cleanupCacheAndReset:
_clearPendingAnalyticsEvents
_clearPendingProfilerEvents
_codec
_collectedAudioDurationMS
_committedCategoryToVersion
_compilationTimer
_connection
_continuousListeningEnabled
_cooldownTimerFired
_cooldownTimerTimeoutSeconds
_correctedText
_createAudioFilePath
_createCachesDirectoryIfItDoesNotExist
_currentAudioFilePath
_dataProtectionClass
_delegate
_deleteItemAtPath:
_deleteTemporaryDirectoryIfExists:
_desRecord
_desRecordDictation
_disableDeliveringAsrFeatures
_firstAudioPacketProcessed
_firstAudioPacketReceivedTime
_firstAudioPacketReceivedTimeInTicks
_firstAudioPacketTimeUntilFirstPartial
_geoLMAssetsInfoDict
_geoLMCompatibleWithMainAsset:geoAssetConfig:
_hasRecognizedAnything
_inferenceSpeakerCode
_installedGeoLMRegionMappingForLanguage:
_interactionId
_invalidateInstallationStatusCacheForAssetType:
_invalidated
_isLoggingAllowedForCurrentRequestWithTask:isSpeechAPIRequest:
_isNonTier1Message:
_isSamplingForDictation
_isSpeakerCodeUsed
_isSpeechAPIRequest
_isTier1LoggingAllowedForCurrentRequest
_isTier1LoggingAllowedForCurrentRequestWithTask:
_language
_languageInstallationCache
_lastAudioPacketReceivedTime
_lastAudioPacketReceivedTimeInTicks
_lastRecognizedMetadataPackage
_lastRecognizedPackage
_lastWordCount
_loadGeoLMAssetsInfoDictForLanguage:
_localMetrics
_logAudioSampledEventsWithStatus:error:customReasonForFailure:
_logPrefix
_modelRootWithAssetConfig:modelOverridePath:overrides:error:
_moveAudioToVarMobile:
_numTrainedFrames
_numberOfDeletions
_numberOfInsertions
_numberOfSubstitutions
_packageLogged
_parseRequiredParameter:expectedClass:domain:recipe:error:
_partialResultsTimeList
_personalizedLmAgeInSec
_personalizedLmWeight
_powerEventContext
_powerSnapshot
_processAudioPacket:packetReceivedTime:packetReceivedTimeInTicks:
_processBufferedAudioPackets
_processedAudioDuration
_profile
_profileAssetPathsInUse
_profileWithError:
_purgeUserDefaultsGeoLMAssetsInfoDictExceptLanguages:cesrTrialAssetManager:
_queue
_recognitionAbsoluteEndTime
_recognitionBeginTime
_recognitionEndTime
_recognitionTask
_recognizedTokens
_recognizer
_recognizerAssetPathsInUse
_redecodeQueue
_requestCompletion
_resultCandidateId
_runAdaptationRecipeForDomain:frequency:tagNames:templateName:charSetToTrim:charSetToSplit:minimumWordLength:userData:profile:
_runAdaptationRecipeForKeyboardLMWithFrequency:tagName:templateName:charSetToTrim:charSetToSplit:minimumWordLength:userData:profile:
_runAdaptationRecipeForLocationOfInterestWithFrequency:tagName:templateName:charSetToTrim:charSetToSplit:minimumWordLength:locationOfInterestNames:profile:
_runAdaptationRecipeForNamesWithFrequency:tagNames:templateName:charSetToTrim:charSetToSplit:minimumWordLength:nameFrequencies:profile:
_runAdaptionRecipeForCalendarEventsWithFrequency:tagName:templateName:charSetToTrim:charSetToSplit:minimumWordLength:userData:profile:
_samplingRate
_saveAudioMetadataToFilePath:
_saveAudioToCache:
_scheduleCompilationTimer:
_scheduleCooldownTimer
_selectedAlternatives
_selfHelper
_selfLogger
_sendPendingAnalyticsEvents
_setQueue:
_shouldStoreDictationAudioOnDevice
_speakerCodeWriter
_speechRecognizerWithAssetConfig:enableITN:isHighPriority:error:
_speechRecognizerWithAssetConfig:geoLMRegionId:enableITN:overrides:modelOverrideURL:preloadModels:requestSource:enableParallelLoading:isHighPriority:geoLMLoadedOut:error:
_stagedCategoryToVersion
_stagedItems
_task
_taskName
_taskToUse
_timeUntilRecognitionStartInMs
_totalCost
_trainingNnetVersion
_transaction
_trimAudioIfNeeded:
_unrepairedPostItn
_updateAudioDuration:
_updateGeoLMAssetsInfoDictWithRegionId:language:
_updateUserDefaultsWithGeoLMAssetsInfoDict:language:
_userDefaultsGeoLMAssetsInfoDictKeyForLangauge:
_userProfileWithAssetConfig:modelOverridePath:overrides:isJit:returningFoundPath:error:
_validDomains
_weakFidesRecognizer
_writeDESRecord:
_writeDESRecord:oneRecordPerDay:
aFEnableFeatureAndCheckPreferenceValueWithKey:
acousticFeatureValuePerFrame
acousticFeatures
activeConfiguration
activeConfigurationForEverything
activeConfigurationForNothing
activeDictationLanguages
adaptUserProfileWithUserData:personalizationRecipe:userData:endOfUserData:
addAudioSamples:count:
addEntriesFromDictionary:
addObject:
addObjectsFromArray:
addOutput:
addVocabularyItems:isBoosted:completion:
addWordWithParts:templateName:
addWordsToUserProfile:templateName:wordArrays:
afRecognitionForEARSausage:processedAudioDuration:
afSpeechPackageForEARPackage:processedAudioDuration:speechProfileUsed:
afTokensForEARTokens:removeSpaceBefore:
afVoiceCommandGrammarParseResultForEARTokenString:withEARVoiceCommandInterpretations:
allItnRunIntervals
allObjects
allValues
allocWithZone:
alternativesSelectedInfo
aneContextFilter
appNames
appendData:
appendString:
appendedAutoPunctuation
arguments
array
arrayByAddingObject:
arrayByAddingObjectsFromArray:
arrayWithObjects:count:
asrId
asrSelfComponentIdentifier
assetReaderAudioMixOutputWithAudioTracks:audioSettings:
assetReaderWithAsset:error:
assetType
assetWithURL:
assistantIsEnabled
attributesOfItemAtPath:error:
audioAnalytics
audioData:withBytesFromEnd:
audioLengthInBytes:samplingRate:
audioMetadata
audioPackets
autorelease
beginWithCategoriesAndVersions:bundleId:completion:
bluetoothDeviceIdFilter
boolValue
bytes
calculateDiffInDaysFromTimestamp:
callStackReturnAddresses
callStackSymbols
canAddOutput:
cancelRecognition
cancelWithCompletion:
captureESStartTimeInTicks
censorSpeech
characterSetWithCharactersInString:
class
cleanupCacheAndReset
code
codec
collectedAudioDurationMS
commandIdentifier
compare:
compileRecognizerModelsWithConfiguration:
component
components
components:fromDate:toDate:options:
componentsJoinedByString:
componentsSeparatedByCharactersInSet:
componentsSeparatedByString:
compressedDataUsingAlgorithm:error:
concatenateUtterances
concatenatedAudioPackets
confidence
conformsToProtocol:
contactWordsWithFrequency
containsEntity
contextualData
continuousListening
continuousListeningEnabled
convertLanguageCodeToSchemaLocale:
copy
copyNextSampleBuffer
copyWithZone:
correctPartialResultIndexList
correctedText
corrections
count
countByEnumeratingWithState:objects:count:
cpuContextFilter
createANEModelInitializationEndedEventWithTimeStamp:fileName:
createDirectoryAtPath:withIntermediateDirectories:attributes:error:
createDirectoryAtURL:withIntermediateDirectories:attributes:error:
createInlineLmeUserDataForContextData:speechProfile:
createInlineLmeUserDataForContextStrings:
createPreheatEndedEventWithPreheatAlreadyDone:
createPreheatFailedEvent
createPreheatStartedOrChangedEvent
createSamplingDirectory
createSpeechProfileWithLanguage:modelOverridePath:JSONData:completion:
currentAudioFilePath
data
dataProfile
dataWithBytes:length:
dataWithContentsOfFile:options:error:
dataWithJSONObject:options:error:
date
dateWithTimeIntervalSince1970:
debugDescription
decodeObjectOfClass:forKey:
defaultManager
defaultStore
deleteAllDESRecordsForDictationPersonalizationWithCompletion:
deleteAllRecordsForPlugin:completion:
deleteItemAtFilePath:
deleteLinkId
deliverEagerPackage
description
detectUtterances
deviceIdFilter
dictationIsEnabled
dictationUIInteractionIdentifier
dictionary
dictionaryForKey:
dictionaryRepresentation
dictionaryWithContentsOfFile:
dictionaryWithContentsProfilePathForLanguage:errorOut:
dictionaryWithDictionary:
dictionaryWithObjects:forKeys:count:
didFinishModelInitializing:
didStartModelInitializing:
disableDeliveringAsrFeatures
domain
doubleValue
dummyResultPackage:
earTokensForAFTokens:appendedAutoPunctuation:
earTokensToString:
emitMessage:
emitMessage:timestamp:
enableAutoPunctuation
enableEmojiRecognition
enableTransactions
enableVoiceCommands
endAudio
ended
endpointStart
enumerateKeysAndObjectsUsingBlock:
enumerateObjectsUsingBlock:
enumerateRangesUsingBlock:
enumeratorAtPath:
errorWithDomain:code:userInfo:
eventLocationNames
eventTitles
exists
failed
farField
farFieldFilter
fetchAndDestroySignpostIdForEventName:
fetchAssetsForAssetConfig:completion:
fetchAssetsForLanguage:completion:
fetchModelInfoForAssetConfig:completion:
fetchModelInfoForAssetConfig:triggerDownload:completion:
fetchModelPropertiesForAssetConfig:completion:
fetchNamedEntitiesWithTimeInterval:
fileExistsAtPath:
fileExistsAtPath:isDirectory:
fileName
fileSystemRepresentation
fileURLWithPath:
fileURLWithPath:isDirectory:
filteredArrayUsingPredicate:
finishAndSaveProfile:completion:
finishAudio
finishDecoding
firstMatchInString:options:range:
firstObject
firstResultAfterResume
floatValue
formattedRecognitionWithNBestList:
frameDuration
frequency
geoLMRegionIdForLanguage:location:
getFormatterWithBlock:
getHostClockFrequency
getOfflineAssetStatusIgnoringCache:assetType:withCompletion:
getOfflineAssetStatusIgnoringCache:assetType:withDetailedStatus:withCompletion:
getOfflineDictationStatusIgnoringCache:withCompletion:
getRecognizerQueue
getVersionForCategory:completion:
gpuContextFilter
hasRecognizedAnything
hasRecognizedAnythingInAFSpeechPackage:
hasSpaceAfter
hasSpaceBefore
hasSuffix:
hash
hostTimeToSeconds:
incrementCost
incrementDeletions
incrementInsertions
incrementSampledRequestCount
incrementSubstitutions
indexOfObject:
inferenceSpeakerCode
init
initForReadingFromData:error:
initWithAcousticFeatureValue:frameDuration:
initWithAsrId:interactionId:language:taskName:samplingRate:
initWithCapacity:
initWithCommandGrammarParsePackage:
initWithCommandId:isComplete:paramMatches:
initWithConfiguration:language:overrides:sdapiOverrides:emptyVoc:pgVoc:paramsetHolder:isJit:
initWithConfiguration:overrides:overrideConfigFiles:language:activeConfiguration:modelLoadingOptions:enableSpeakerCodeTraining:supportEmojiRecognition:voiceCommandActiveSet:modelContextDelegate:enableItn:
initWithConfiguration:taskName:applicationName:
initWithConfiguration:useQuasarFormatter:activeConfiguration:
initWithContentsOfFile:options:error:
initWithCurrentProcess
initWithDictionary:
initWithDomain:code:userInfo:
initWithESSelfHelper:signpostHelper:
initWithExplicitlyRejectedString:
initWithImplicitlyEngagedString:
initWithInterpretationIndices:confidenceScore:
initWithInterpretations:isLowConfidence:
initWithItems:language:
initWithLanguage:assetType:
initWithLanguage:task:
initWithModelRoot:
initWithModelVersion:modelType:modelRoot:
initWithNBestParses:preITNNBestParses:
initWithNcsRoot:
initWithOrthography:pronunciations:tag:
initWithOrthography:pronunciations:tagName:frequency:
initWithPath:error:
initWithPhrases:utterances:processedAudioDuration:
initWithPlistJSONDictionary:
initWithPreheatContext:powerSnapshot:powerEventContext:
initWithRecognition:unfilteredRecognition:rawRecognition:audioAnalytics:isFinal:utteranceStart:latticeMitigatorResult:recognitionPaused:
initWithRecognition:unfilteredRecognition:rawRecognition:audioAnalytics:isFinal:utteranceStart:latticeMitigatorResult:recognitionPaused:speechProfileUsed:resultCandidateId:
initWithResults:score:threshold:
initWithSpeechRecognitionFeatures:acousticFeatures:snr:
initWithSuiteName:
initWithTask:isSpeechAPIRequest:
initWithText:
initWithTokenName:start:end:silenceStart:confidence:hasSpaceAfter:hasSpaceBefore:phoneSequence:ipaPhoneSequence:appendedAutoPunctuation:
initWithUUIDBytes:
initWithUUIDString:
initWithUUIDString:language:task:codec:samplingRate:inferenceSpeakerCode:numTrainedFrames:trainingNnetVersion:isSpeakerCodeUsed:isSamplingForDictation:selfLogger:
initWithUtterance:parseCandidates:
initWithXPCConnection:
initialize
initializeSharedPowerLoggerIfNeeded
inputOrigin
installationStatusForLanguagesForAssetType:includeDetailedStatus:error:
installationStatusForLanguagesIgnoringCache:assetType:withDetailedStatus:withError:
installedAssetWithConfig:
installedAssetWithConfig:regionId:
installedAssetWithConfig:regionId:triggerDownload:
installedGeoLMRegionSpecificAssetForLanguage:regionId:mainAssetConfig:
installedHammerConfigFileForLanguage:
installedModelInfoForAssetConfig:error:
installedModelInfoForAssetConfig:error:triggerDownload:
installedModelInfoForAssetConfig:error:triggerDownload:ignoreSpellingModel:
installedQuasarModelPathForAssetConfig:error:
installedQuasarModelPathForAssetConfig:error:triggerDownload:
installedQuasarModelPathForAssetConfig:error:triggerDownload:ignoreSpellingModel:
intValue
integerValue
interactionId
interactionIdentifier
interactionSenderDisplayNames
interpretationIndices
interruptTraining
invalidate
invalidatePersonalizedLM
invalidateUaapLm
ipaPhoneSequence
isDictationHIPAACompliant
isEmojiDisambiguationUsed
isEmojiPersonalizationUsed
isEqual:
isEqualToDictionary:
isEqualToString:
isFilePathValid:
isKindOfClass:
isMemberOfClass:
isModifiedByAutoPunctuation
isRequestSelectedForSamplingForTask:
isRequestSelectedForSamplingFromConfigForLanguage:
isSamplingForDictation
isSiriXEnabled
isSpeakerCodeUsed
isSpeechAPIRequest
isTrialAssetDeliveryEnabled
itemFromBuffer:error:
jitGrammar
jsonFilenameForAssetType:
keyboardLMDynamicVocabularyItems
keysSortedByValueUsingComparator:
language
languageCode
languageStringForLocaleString:
lastObject
lastPathComponent
lastUsedGeoLMRegionIdForLanguage:
latticeMitigatorResult
length
linkId
listener:shouldAcceptNewConnection:
loadConfigs
loadDate
loadSpeechProfiles:language:
localeStringForLanguageString:
localizedDescription
location
locationOfInterestNames
logActiveConfigUpdateEnded
logActiveConfigUpdateEndedWithTimeInTicks:
logActiveConfigUpdateStartedOrChangedWithTimeInTicks:
logAotLmeEndedWithTimeInTicks:
logAotLmeStartedOrChangedWithTimeInTicks:
logAppleNeuralEngineModelInitializationEnded
logAppleNeuralEngineModelInitializationEndedWithTimeInTicks:fileName:
logAppleNeuralEngineModelInitializationStartedOrChangedWithTimeInTicks:
logAppleNeuralEngineModelInitializationStartedOrChangedWithTimeInTicks:fileName:
logAudioPacketArrivalEndedWithTimeInTicks:
logAudioPacketArrivalStartedOrChangedWithTimeInTicks:
logESStartWithTimeInTicks:
logEventWithType:context:
logEvents:
logFinalAudioPacketContainingSpeechReceivedWithTimeInTicks:loggableSharedUserId:
logFinalResultGeneratedWithEARPackage:
logFinalResultGeneratedWithEARPackage:timeInTicks:
logFirstAudioPacketContainingSpeechReceivedWithTimeInTicks:loggableSharedUserId:
logFirstAudioPacketProcessed
logFirstAudioPacketProcessedWithTimeInTicks:
logFrameProcessingReady
logFrameProcessingReadyWithTimeInTicks:
logInitializationEnded
logInitializationEndedWithTimeInTicks:
logInitializationStartedOrChangedWithTimeInTicks:
logInitializationStartedOrChangedWithTimeInTicks:cachedRecognizerExisted:newRecognizerCreated:
logIntermediateUtteranceInfoTier1WithPmInput:pmOutput:
logIntermediateUtteranceInfoTier1WithPmInput:pmOutput:unrepairedPostItn:loggableSharedUserId:
logIntermediateUtteranceInfoTier1WithUnrepairedPostItn:loggableSharedUserId:
logJitLmeEndedAndEndedTier1WithDialogContext:
logJitLmeEndedAndEndedTier1WithDialogContext:timeInTicks:
logJitLmeStartedOrChangedWithTimeInTicks:
logLeadingSilenceProcessedWithTimestampInTicks:
logLocalRecognitionLoadedForLanguage:duration:
logPackageGeneratedAndRecognitionResultTier1WithEARPackage:loggableSharedUserId:
logPackageGeneratedAndRecognitionResultTier1WithEARPackage:loggableSharedUserId:timeInTicks:
logPartialResultGenerated
logPartialResultGenerated:ofSize:timeInTicks:
logPauseRecognitionWithTimeInTicks:
logPendingANEModelInitializationContextEvents:
logPostSpeechStartOneSecondAudioProcessedWithTimestampInTicks:
logPostSpeechStartOneSecondLaterAudioPacketReceivedWithTimeInTicks:loggableSharedUserId:
logPowerSnapshotForProcessStarted
logPrefix
logRequestEndedOrFailedOrCancelledWithError:recognizerComponents:averageActiveTokensPerFrame:languageModelInterpolationWeights:signalToNoiseRatioInDecibels:recognitionDurationInSec:audioDurationMs:eagerUsed:utteranceDetectionEnabled:utteranceConcatenationEnabled:cpuRealTimeFactor:numLmeDataStreams:phoneticMatchDecoderName:pauseDurations:itnDurationInNs:isEmojiPersonalizationUsed:isEmojiDisambiguationUsed:isEmojiExpectedButNotRecognized:recognizedEmojis:allItnRunIntervals:allSecondaryPassIntervals:
logRequestEndedOrFailedOrCancelledWithError:recognizerComponents:averageActiveTokensPerFrame:languageModelInterpolationWeights:signalToNoiseRatioInDecibels:recognitionDurationInSec:audioDurationMs:eagerUsed:utteranceDetectionEnabled:utteranceConcatenationEnabled:cpuRealTimeFactor:numLmeDataStreams:phoneticMatchDecoderName:pauseDurations:itnDurationInNs:isEmojiPersonalizationUsed:isEmojiDisambiguationUsed:isEmojiExpectedButNotRecognized:recognizedEmojis:totalITNDurationInNs:totalITNRuns:totalSecondaryPassDurationInNs:totalSecondaryPasses:cpuInstructionsInMillionsPerSecond:
logRequestLinkWithRequestId:
logRequestStartedOrChangedWithTask:modelLocale:modelVersion:isHighQualityAsset:hammerVersion:geoLanguageModelRegion:geoLanguageModelLoaded:speechProfileAgeInSec:dictationUIInteractionId:portraitExperimentVariantName:
logResumeRecognitionWithTimeInTicks:
logSampledAudioFileStoredSuccessfully
logSampledAudioFileStoredWithError:customFailureReason:
logTotalAudioProcessedOfDuration:timeInTicks:
logWithEventContext:
logWithEventContext:asrIdentifier:
loggingContext
longLongValue
machAbsoluteTimeToMachContinuousTime:
mapContextOptionToString:
maximumRecognitionDuration
methodForSelector:
modelInfo
modelOverrideURL
modelQualityTypeStatusStringWithConfig:
modelRoot
modelType
modelVersion
moveItemAtPath:toPath:error:
mutableCopy
nBest
nBestVoiceCommandInterpretations
name
narrowband
nextObject
nnetVersion
numFrames
numTrainedFrames
numberOfDeletions
numberOfInsertions
numberOfSubstitutions
numberWithDouble:
numberWithFloat:
numberWithInteger:
numberWithUnsignedInt:
numberWithUnsignedInteger:
numberWithUnsignedLongLong:
objectAtIndex:
objectAtIndexedSubscript:
objectEnumerator
objectForKey:
objectForKeyedSubscript:
offlineDictationProfileOverridePath
orderedSetWithArray:
originalAudioFileURL
overrides
packetArrivalTimestampFromAudioTime:
path
pathComponents
pathWithComponents:
pauseDurations
pauseRecognition
performSelector:
performSelector:withObject:
performSelector:withObject:withObject:
personalizationRecipeForAssetConfig:modelOverridePath:
personalizedLmAgeInSec
personalizedLmWeight
pexNamedEntityNames
phoneSequence
postfixText
powerEventContext
powerSnapshot
preITNNBestVoiceCommandInterpretations
preITNRecognition
preITNTokens
preITNVoiceCommandInterpretations
predicateWithFormat:
prefixText
preheatContext
preheatSpeechRecognitionWithAssetConfig:preheatSource:modelOverrideURL:
preheatSpeechRecognitionWithLanguage:modelOverrideURL:
prepareHammerConfigFile:
prepareToExit
prependedAutoPunctuation
presence
processInfo
profileFilePathFromBasePath:language:userId:
promoteAssetsForAssetType:
promoteHammerConfigFile
pronunciationsForOrthography:
punctuationCharacterSet
purgeInstalledAssetForAssetType:language:regionId:error:
purgeInstalledAssetsExceptLanguages:assetType:completion:
purgeInstalledAssetsExceptLanguages:assetType:error:
purgeInstalledAssetsExceptLanguages:completion:
purgeUnusedGeoLMAssetsForLangauge:
raise:format:
range
rangeOfCharacterFromSet:
rangeOfString:
rawRecognition
readProfileAndUserDataWithLanguage:allowOverride:completion:
readTableFromURL:
readUserProfile:
readUserProfileWithPath:reuseProfile:
reason
recognition
recognitionEndTime
recognitionMetrics
recognitionPaused
recognitionResultsWithAudioData:userProfileData:language:task:samplingRate:extraLanguageModel:
recognitionStart
recognitionStatistics
recognitionUtteranceInfos
recognitionUtterenceStatistics
recognizedEmojis
recognizedText
recognizedTokens
recognizerSourceForTask:
recordWithLanguage:task:context:narrowband:farField:interactionIdentifier:asrSelfComponentIdentifier:pluginId:
recordWithLanguage:task:context:narrowband:farField:interactionIdentifier:asrSelfComponentIdentifier:pluginId:frequency:
redecodeWithAudioDatas:language:task:samplingRate:completion:
registerFeedback:completion:
registerNotifications
regularExpressionWithPattern:options:error:
releaseClients
remoteObjectProxy
removeAllObjects
removeAllWords
removeItemAtURL:error:
removeLmeDataForTemplateName:
removeObjectAtIndex:
removeObjectForKey:
removePersonalizedLMForFidesOnly:completion:
removeSpaceAfter
removeSpaceBefore
replaceCorruptAssetWithConfig:
requestIdentifier
resetCacheAndCompileAllAssetsWithCompletion:
respondsToSelector:
resultCandidateId
resume
resumeRecognitionWithLeftContext:rightContext:selectedText:
resumeRecognitionWithPrefixText:postfixText:selectedText:
retain
retainCount
reverseObjectEnumerator
runAdaptationRecipeEvaluation:recordData:attachments:completion:
runCorrectedTextEvaluationWithAudioDatas:recordDatas:language:samplingRate:caseSensitive:skipLME:wordSenseAccessListSet:completion:
runDefaultAdaptationEvaluation:recordData:attachments:completion:
runEvaluationWithDESRecordDatas:language:recipe:attachments:fidesPersonalizedLMPath:fidesPersonalizedLMTrainingAsset:scrubResult:completion:
runRecipeEvaluationDistributedEvaluation:recordData:attachments:completion:
runRecognitionWithResultStream:speakerCodeWriter:language:task:samplingRate:
sampledCachesSubDirectoryPath
samplingDateAsString
samplingRate
samplingRateFilter
save
saveOneRecordPerDay
score
searchEventValues
secondsToHostTime:
secureOfflineOnly
selectedAlternatives
selectedText
self
selfHelper
selfLogger
sendEvent
sendSpeechCorrectionInfo:interactionIdentifier:
setActiveConfigUpdateContext:
setActiveConfiguration:
setAllowUtteranceDelay:
setAlternatives:
setAneContextFilter:
setAppendSpaceAfter:
setAppleNeuralEngineCompilationContext:
setAppleNeuralEngineModelInitializationContext:
setApplicationName:
setAsrId:
setAssetLoadContext:
setAssetsProvisionalForAssetType:
setAssetsPurgeabilityExceptLanguages:assetType:
setAudioDurationInNs:
setAudioMetadata:
setAudioPacketArrivalContext:
setAudioPackets:
setAudioSpeechPacketArrivalContext:
setAudioSpeechPacketFirstSecondAfterFirstSpeechPacketArrived:
setBluetoothDeviceIdFilter:
setByAddingObject:
setByAddingObjectsFromSet:
setCancelled:
setClass:forClassName:
setClientIdentifier:
setCodec:
setCollectedAudioDurationMS:
setComponent:
setConcatenateUtterances:
setConfidence:
setConfidenceScore:
setContinuousListeningEnabled:
setCorrectPartialResultIndexLists:
setCorrectedText:
setCpuContextFilter:
setCpuInstructionsInMillionsPerSecond:
setCpuRealTimeFactor:
setCurrentAudioFilePath:
setDatapackVersion:
setDecodable:
setDecoder:
setDelegate:
setDetectUtterances:
setDeviceIdFilter:
setDialogContexts:
setDictationUiInteractionId:
setDisableAutoPunctuation:
setESSelfHelper:
setEagerEnabled:
setEmbeddedSpeechProcessContext:
setEmojiMetrics:
setEnableSpeakerCodeTraining:
setEnableVoiceCommands:
setEndTime:
setEnded:
setEndpointStart:
setErrorCode:
setEventMetadata:
setExists:
setExportedInterface:
setExportedObject:
setFailed:
setFarField:
setFarFieldFilter:
setFileName:
setFinalAudioPacketContainingSpeechReceived:
setFinalResultGenerated:
setFirstAudioPacketProcessed:
setFirstSecondAfterLeadingSilenceProcessed:
setFormatAcrossUtterances:
setFrameProcessingReady:
setFrontend:
setGeoLanguageModelLoaded:
setGeoLanguageModelRegion:
setGpuContextFilter:
setHammerVersion:
setHasRecognizedAnything:
setHighPriority:
setInitializationContext:
setInteractionId:
setIntermediateUtteranceInfoTier1:
setInterpretationIndices:
setInterpretations:
setInterruptionHandler:
setInvalidationHandler:
setInverseTextNormalizationDurationForFinalResultInNs:
setInverseTextNormalizationDurationInNs:
setIpaPhoneSequence:
setIsAfterResume:
setIsAutoPunctuation:
setIsBoosted:
setIsEmojiDisambiguationUsed:
setIsEmojiExpectedButNotRecognized:
setIsEmojiPersonalizationUsed:
setIsFinal:
setIsHighQualityAsset:
setIsLowConfidence:
setJitLanguageModelEnrollmentEndedTier1:
setJitProfileData:
setLanguage:
setLanguageModelEnrollmentContext:
setLanguageModelInterpolationWeights:
setLeadingSilenceProcessed:
setLeftContextText:
setLinkId:
setLinkIndex:
setLoadDate:
setLogPrefix:
setMaximumRecognitionDuration:
setMetrics:
setModelRoot:
setModelType:
setNumLanguageModelEnrollmentDataStreams:
setNumberOfDeletions:
setNumberOfInsertions:
setNumberOfInverseTextNormalizationRuns:
setNumberOfSubstitutions:
setObject:atIndexedSubscript:
setObject:forKey:
setObject:forKeyedSubscript:
setOriginalAudioFileURL:
setPackage:
setPackageGenerated:
setPartialResultGenerated:
setPausedAudioDurationsInNs:
setPersonalizedLanguageModelAgeInNs:
setPersonalizedLanguageModelWeight:
setPersonalizedLmAgeInSec:
setPersonalizedLmWeight:
setPhoneSequence:
setPhoneticMatchDecoderName:
setPhoneticMatchInput:
setPhoneticMatchOutput:
setPhrases:
setPortraitExperimentVariantName:
setPostITNRecognizedText:
setPostItn:
setPreheatContext:
setProfile:
setProfileConfigWithLanguage:profileDir:userId:dataProtectionClass:completion:
setPunctuationText:
setRawRecognition:
setReason:
setRecognitionConfidenceSubtraction:
setRecognitionDurationInNs:
setRecognitionEndTime:
setRecognitionReplacements:
setRecognitionResultTier1:
setRecognizeEagerCandidates:
setRecognizeEmoji:
setRecognizedEmojis:
setRecognizedText:
setRecognizedTokens:
setRecognizerComponents:
setRemoteObjectInterface:
setRemoveSpaceAfter:
setRemoveSpaceBefore:
setRequestContext:
setResultCandidateId:
setRightContext:
setSampledAudioFileStorageFailed:
setSampledAudioFileStored:
setSampledAudioStorageFailureReason:
setSamplingRate:
setSamplingRateFilter:
setScoreNbest:
setScoreNbestExtraLmList:
setSelectedAlternatives:
setSelectedText:
setSelfLogger:
setSignalToNoiseRatioInDecibels:
setSilenceStartTime:
setSilenceStartTimeInNs:
setSource:
setSpeechProfileAgeInNs:
setStartTime:
setStartTimeInNs:
setStartedOrChanged:
setStatus:
setTarget:
setTask:
setTaskName:
setTaskTypeFilter:
setTemplateToVersion:
setText:
setTimeZone:
setTimestampInTicks:
setTokenSilenceStartTimeInNsLists:
setTokens:
setTotalCost:
setUUIDString:
setUnderlyingErrorCode:
setUnderlyingErrorDomain:
setUnrepairedPostItn:
setUserId:
setUserProfileData:
setUtteranceConcatenationEnabled:
setUtteranceDetectionEnabled:
setUtterances:
setUuid:
setValue:forKey:
setWeights:
setWithArray:
setWithObject:
sharedInstance
sharedManager
sharedPreferences
sharedProfiler
sharedStream
shouldHandleCapitalization
shouldStoreAudioOnDevice
shouldWriteDictationRecord:
signalEndOfUserData
signpostHelper
silenceStart
silenceStartTime
siriDataSharingOptInStatus
siriDataSharingOptedIn
spatialLocationOfInterestNames
speechProfileDateLastModifiedForLanguage:
speechProfileRootDirectories
speechRecognitionFeatures
speechRecognizer:didFinishRecognitionWithError:
speechRecognizer:didProcessAudioDuration:
speechRecognizer:didProduceEndpointFeaturesWithWordCount:trailingSilenceDuration:eosLikelihood:pauseCounts:silencePosterior:processedAudioDurationInMilliseconds:
speechRecognizer:didProduceLoggablePackage:
speechRecognizer:didRecognizeFinalResultCandidatePackage:
speechRecognizer:didRecognizeFinalResultPackage:
speechRecognizer:didRecognizeFinalResults:
speechRecognizer:didRecognizeFinalResults:tokenSausage:nBestChoices:
speechRecognizer:didRecognizePartialResult:
speechRecognizer:didRecognizePartialResultNbest:
speechRecognizer:didRecognizeRawEagerRecognitionCandidate:
speechRecognizer:didReportStatus:statusContext:
speechServiceDidFinishRecognitionWithStatistics:error:
speechServiceDidProcessAudioDuration:
speechServiceDidProduceEndpointFeaturesWithWordCount:trailingSilenceDuration:eosLikelihood:pauseCounts:silencePosterior:processedAudioDurationInMilliseconds:
speechServiceDidProduceLoggablePackage:
speechServiceDidRecognizeFinalResultCandidatePackage:
speechServiceDidRecognizePackage:
speechServiceDidRecognizePackage:withMetadata:
speechServiceDidRecognizeRawEagerRecognitionCandidate:
speechServiceDidRecognizeTokens:
speechServiceDidRecognizeTokens:withMetadata:
speechServiceDidSelectRecognitionModelWithModelProperties:
standardUserDefaults
start
startMissingAssetDownload
startReading
startRequestActivityWithCompletion:
startSpeechRecognitionWithParameters:didStartHandler:
startSpeechRecognitionWithParameters:didStartHandlerWithInfo:
startTime
startedOrChanged
status
storeSignpostId:forEventName:
string
stringByAppendingFormat:
stringByAppendingString:
stringByDeletingLastPathComponent
stringByDeletingPathExtension
stringByReplacingMatchesInString:options:range:withTemplate:
stringByReplacingOccurrencesOfString:withString:
stringByTrimmingCharactersInSet:
stringValue
stringWithContentsOfURL:encoding:error:
stringWithFormat:
stringWithUTF8String:
subdataWithRange:
substringFromIndex:
substringToIndex:
substringWithRange:
superclass
supportedCategories
supportedLanguagesWithAssetType:
switchToNewAssetsForAssetType:
systemUptime
task
taskName
taskTypeFilter
tasks
templateName
templateToVersion
text
threshold
timeIntervalSince1970
timeIntervalSinceNow
timeIntervalSinceReferenceDate
timeZoneWithAbbreviation:
timestamp
timestampInTicks
timestampInTicksForAudioProcessed:
tokenName
tokenSausage
tokenize:
tokens
totalCost
tracksWithMediaType:
trainingNnetVersion
unrepairedPostItn
unrepairedRecognition
unsignedIntValue
unsignedIntegerValue
unsignedLongLongValue
unsignedLongValue
updateSpeechProfileWithLanguage:modelOverridePath:existingProfile:existingAssetPath:completion:
usage_start
userData
userInfo
userProfileWithAssetConfig:modelOverridePath:overrides:isJit:returningFoundPath:error:
utteranceStart
utterances
valueForEntitlement:
valueForKey:
valueForKeyPath:
variantNameWithError:
voiceCommandInterpretations
voiceCommandsParamKeyBuilder:
weakObjectsHashTable
whitespaceAndNewlineCharacterSet
whitespaceCharacterSet
wrapAndEmitTopLevelEvent:timestampInTicks:
writeDESRecord
writeProfileToFile:protectionClass:length:error:
writeToFile:atomically:
writeToFile:options:error:
writeToURL:atomically:encoding:error:
zone
%s SELF: Logging object created successfully (logging allowed for current request). asrId=%@, recognitionTask=%@, isSpeechAPIRequest=%@, isHipaaCompliant=%@, siriOptInStatus=%@, isTier1LoggingAllowed=%@
%s SELF: Logging disabled because it is not allowed for the current request. recognitionTask=%@, isSpeechAPIRequest=%@
%s SELF: Correct Partial Result Index List is %s, Silence Start Time List is %s
%s SELF: Encountered malformed string during SELF logging for recognizer components in speech results from recognizer. String: (%@)
%s SELF: Expected three recognizer components separated by delimiter '::'. Ex: 'dnn-rfdnn-aa-cache::dnn-lazy-16k-rfdnn-dictation::msg'
%s SELF: Encountered malformed string during SELF logging for interpolation weights in speech results from recognizer. String: %@
%s SELF: Expected interpolation weight sets separated by delimiter ';' - starting with a set of weights delimited by ',' and ending the with start/end times delimited by ':'. Ex: '0.999646,0.000354:0:4280;0.947514,0.000158:0:3859'
%s SELF: Logging ASRRequestContext->failed in SELF based on error result from recognizer.
%s SELF: Logging ASRRequestContext->cancelled with reason RECOGNITION_CANCELLED in SELF based on error result from recognizer.
%s SELF: Logging ASRRequestContext->cancelled with reason RECOGNITION_REJECTED in SELF based on error result from recognizer.
%s SELF: Logging ASRRequestContext->cancelled with reason RECOGNITION_REJECTED in SELF because nothing was recognized (SpeechNoMatch).
%s SELF: Logging ASRRequestContext->ended in SELF based on success status from recognizer.
%s SELF: Failed trying to wrap and emit top-level ASR event because event type was not mapped to loggable message type in the ASR SELF schema.
%s SELF: Wrapping and logging an event of type %@
%s %@ cancelling instance %@
%s %@.
%s Internal inconsistency error: KVItems list and corresponding isBoosted booleans list are out of sync. This batch of items will default to not being boosted.
%s Failed to deserialize a KVItem. It will be ignored.
%s Starting to process KVItems into CESRUserData.
%s Created CESRUserData from KVItems.
%s %@: %@
%s Adapted profile using CESRUserData according to personalization recipe.
%s Speech profile updated successfully. Wrote %lu bytes to %@
%s Created _EARProfileBuilder from asset config.
%s Existing profile found at %@. It will be reused when updating the profile.
%s Could not locate asset: %@
%s Could not read personalization.json: %@
%s Could not parse personalization.json: %@
%s Set model root to %@
%s Use currently installed asset.
%s Recipe has invalid json for "%{public}@"
%s Recipe has invalid tagName for "%{public}@": "%{public}@"
%s Error executing recipe for domain %{public}@
%s Recipe for %{public}@ is missing "%{public}@"
%s Recipe for %{public}@ contains parameter %{public}@, expected type %{public}@ but got %{public}@
%s User Profile: Starting AddWordsToUserProfile
%s Using name frequencies adaption for names: %@ into slot %@ for template %@
%s Ignoring name part "%@" because it is too short (minimum length is %lu)
%s Using keyboardLMAdaptation adaption for names %@ into slot %@ for template %@
%s Ignoring keyboardDynamicVocabularyItem "%@" because it is too short (minimum length is %lu)
%s Using locationOfInterestNameAdaptationFrequency adaption for names %@ into slot %@ for template %@
%s Ignoring locationOfInterestName "%@" because it is too short (minimum length is %lu)
%s Using eventLocationNameAdaptationFrequency adaption for names %@ into slot %@ for template %@
%s Ignoring calendar event word "%@" because it is too short (minimum length is %lu)
%s Root directories for new type of speech profile: %{private}@
%s speechProfilePathsWithLanguage was incorrectly called with language=nil
%s Mapped language=nil
%s loadSpeechProfiles was incorrectly called with profiles=nil
%s Reused new type of speech profile: path=%{private}@
%s Loaded new type of speech profile: path=%{private}@ profile=%d
%s Count of command interpretation sets does not match count of speech recognition results
%s AFSpeechLatticeMitigatorResult Score = %f, Threshold = %f
%s AFSpeechLatticeMitigatorResult nil
%s Failed to register for assistant asset update notifications.
%s Installation status for languages (ignoring cache: %@)
%s Invalidating installation status cache for %lu
%s Language installation status query failed: %@
%s Siri and Dictation are both disabled, no need to access asset
%s Using ASR Trial assets at %@
%s No assets available for language: %@ asset type: %{public}@
%s Purging Trial assets failed: %@
%s Checking for missing assets.
%s Purging all assistant ASR assets except for %@
%s Purging all assistant ASR assets
%s Encountered error trying to purge unused assistant ASR assets: %@
%s Trial asset delivery disabled for assistant ASR assets. Bailing out of missing asset check.
%s Trial asset delivery is explicitly disabled!
%s Hammer model info=%@
%s Exception thrown while reading hammer config
%s GeoLM: region mapping json file=%@
%s GeoLM: region mapping json file is nil Or there is no regionMapping for given language=%@
%s GeoLM: Not Supported
%s GeoLM: region specific [%@] geo-config json file=%@
%s GeoLM: geoLM region specific [%@] asset exists on device, but not compatible.
%s GeoLM: Exception thrown while reading geo-config json
%s GeoLM: region specific asset is not found for given language=%@ regionId=%@
%s GeoLM: model-info.version doesn't match. mainASRModelInfo.version=%@ geoLMModelInfo.version=%@ mainAssetConfig=%@ geoAssetConfig=%@
%s GeoLM: Exception thrown while reading json configs
%s GeoLM: language is nil. Skipping.
%s GeoLM: regionIdToBePurged: %@, lastWhenUsed: %ld days ago
%s GeoLM: regionIdToBePurged: %@, _geoLMAssetsInfoDict count: %ld
%s GeoLM: supportedLanguages count:%ld
%s GeoLM: Going to delete: %@
%{public, signpost.description:begin_time}llu
%{public, signpost.description:end_time}llu Status=%s
ondevice_preheat_time
%s Unexpected pending preheat event count %lu
%{public, signpost.description:begin_time}llu FileName=%s
%{public, signpost.description:end_time}llu FileName=%s
ES: ANE Model Init
%s Unexpected pending ane model initialization event count %lu
%{public, signpost.description:begin_time}llu cachedRecognizerExisted = %u, newRecognizerCreated = %u
ES: Engine Init
%{public, signpost.description:end_time}llu
ES: Update Active Config
ES: JIT Profile Build Load
ES: AOT Profile Load
%{public, signpost.description:event_time}llu
ES: First Audio Packet
ES: Last Audio Packet
ES: First Audio Packet Process
ES: Last Speech Audio Packet
ES: Frame Processing Ready
%{public, signpost.description:event_time}lluWords=%s, WordCount=%u
ES: Partial Recognition
%{public, signpost.description:event_time}lluOneBest=%s, isFinal=%u, isAfterResume=%u, rcId=%ld
ES: Package Recognition
%{public, signpost.description:event_time}llu OneBest=%s, isFinal=%u, isAfterResume=%u, rcId=%ld
ES: Final Recognition
ES: Leading Silence Process
ES: First Speech Second Process
ES: ITN Process
ES: Secondary Pass Process
%{public, signpost.description:event_time}llu SNR = %f, AudioDurationMs = %llu, RecognitionDurationMs = %llu, CPU RTF = %f
ES: Engine Complete
ES: Pause Recognition
ES: Resume Recognition
%s ASR: Using high priority configuration.
%s %@ deallocating
%s Failing to fetch assets for nil language
%s Could not get offline language for fetch fallback: %{public}@
%s Fell back asset fetch from %{public}@ to %{public}@
%s Failed to fall back asset fetch from %{public}@ to %{public}@, got %{public}@
%s Could not get installed offline language statuses: %{public}@
%s %@
%s ASR: enable parallel loading
%s ASR: filtering for tasks: %@
%s ASR: taskForMemoryLock: %@
%s Override json files=%@
%s Unable to locate or read dictation voice commands assets
%s Failed to create recognizer from %{public}@
%s EmbeddedSpeechMetric: Created recognizer in %lf sec from %@
%s %s
ondevice_CreateRecognizer
%{public, signpost.description:end_time}llu keepANEModelLoaded = %s, enableParallelLoading = %s, preloadModels = %u, preheatSource = %s, taskForMemoryLock = %s
ES: Recognizer Create
%s Preheat for %@
%s Preheat for %@ dismissed because recognition in progress
%s Skipping preheat for %@; recognizer already loaded
%s sRecognizerTracker: Too many recognizers active during preheat: %lu
%s Preheated for language %{public}@, source %{public}@, regionId %@%{public}@
%s Could not preheat for language %{public}@, source %{public}@, regionId %@%{public}@: %@
%s dictationCapable=%d task=%@ aneCapable=%d
%s Starting
%s Recognizer is busy
%s Previous recognizer on other XPC connection is busy. Send sync cancel
ES: Previous Request Cancellation
%s Cached recognizer for language %{public}@, regionId %@ already loaded
%s Cached recognizer is for language %{public}@, regionId %@, requesting recognizer for language %{public}@, asset type %{public}@, regionId %@
ES: Previous Recognizer Unload
%s sRecognizerTracker: Too many recognizers active: %lu
%s No cached recognizer.
%s EndpointStart > 0 but asr features delivery is disabled!
%s EndpointStart < 0
%s Setting new profile: %d, old profile: %d
%s Failed to get model root, error: %@
%s Fetching contextual data for task: %@
%s Injecting contextual data to recognizer
%s Built inline LME from contextual data, size: %zu
%s Failed to initialize jit profile builder due to error : %@
%s Set inputOrigin to: %@
%s Switching off UC/UD for this request
%s Changing active configuration from 
%@ to 
%s Injected %lu jit strings or contextual data to recognizer, length: %lu
%s Duration spent in jit processing on critical path = %{public}lfs
%s Create DES record
%s Cancelling delayedBlock
%s Create DES record for Dictation with interactionId=%@
%s Saving profile snapshot: %lu bytes
%s _storeAudioData should be nil. Critical Error. Please check.
%s Received correctedText, interactionId: %@, correctedText: %@
%s Sending dictation feedback to Portrait based on correction
%s Set correctedText for CESRFidesASRRecord, interactionId: %@, correctedText: %@
%s Set alternativeSelection for CESRFidesASRRecord, interactionId: %@, alternatives: %@
%s Interaction identifier did not match the DES record in memory
%s Using override profile at %@
%s Could not use override profile at %@: %@
%s Mismatch in speech profile language in content (%{public}@) and filename (%{public}@)
%s Profile version on disk (%{public}@) does not match the expected version (%{public}@)
%s Successfully deserialized existing speech profile for %@
%s Compilation timer fired.
%s Compilation scheduled for %zus.
%s Acquired os_transaction during cooldown start
%s On-Device ASR: Cooldown is disabled.
%s On-Device ASR: Cooldown scheduled for %zus.
%s Cooldown timer triggered TRIClient release
%s Received termination signal. Cleaning up immediately
%s Sending %lu events
%s Unable to load the contents of file %@: %@
%s Invalid file format
%s Running distributed evaluation for ASR
%s No attachments given, cannot run distributed evaluation
%s Failed to extract test set: %@
%s Cannot initialize recognizer for locale: %@ task: %@
%s Loaded speech profile
%s Unable to load speech profile
%s Test set contains more utterances than allowed, only running %d utterances
%s Unable to load audio file %@
%s Unable to find reference transcriptions for %@
%s Recipe has no profile
%s Stop adaption recipe. Audio file not readable. Voicemail has been deleted by user
%s Read %lu bytes from audio file
%s Using on device personalization recipe for baseline
%s Recognizer doesn't support the task %{public}@
%s Couldn't create create path for temporary confidence model overrides at %@
%s Couldn't write data to temporary confidence model file at path %@
%s Could not make baseline results
%s Failed to extract quasar model: %@
%s No recognizer created for custom model: %@
%s Could not make results with custom model
%s Profile overrides failed
%s Ignoring malformed overrides: %{public}@
%s Recipe has no recognizer
%s Could not make adapted results
%s Exception evaluating recipe: %@
%s Unknown exception evaluating recipe
%s Loaded speech profile: %lu bytes
%s Interrupted corrected text evaluation redecoding
%s No tokenizer for %@
%s Failed to load old type of speech profile. Trying new type.
%s Examining localSpeechDESRecord: %@
%s Unable to load localSpeechDESRecord
%s No audio data provided for UUID %@
%s Recognition result %@, %lu
%s Edit distance between tts ASR and original ASR %@
%s correctedOutput: %@, recognizedOutput %@
%s Unknown evaluation name found in alignmentReferences: %@
%s modelRoot: %@
%s No modelRoot for %@: %@
%s Unable to load audio
%s Recognizer doesn't support the task %{public}@: %@
%s Interrupted evaluation redecoding
%s Running recognition for evalName: %@
%s Failed to get override files, error: %@
%s Creating recognizer with overrides: %@
%s Using Personalized LM
%s Not using Personalized LM
%s Unable to restore speech profile
%s Using JIT LME
%s JIT LME: Injecting JIT data, jitStats: %@
%s JIT LME: Error fetching JIT data, error: %@
%s No results for evalName %@: %@
%s Tokenizing correctedText
%s Interrupted evaluation tokenization
%s Computing alignments
%s Failed to delete temporary directory: %@
%s Starting to compile assistant language: %@
%s Error when compiling assistant language model: %@
%s Finished compiling all assets.
%s Error when getting assistant language status: %@
%s No DES record, nothing to write
%s Not saving DES Record with no data or recognition
%s wordCount = %ld, trailingSilenceDuration = %ld, eosLikelihood = %f, pauseCounts = %@, silencePosterior = %f
%s Setting recognized text
%s %lu results
%s EmbeddedSpeechMetric: first audio packet to first partial result = %lf secs
%s AFSpeechInfo Package %@
%{public, signpost.description:begin_time}llu %s
ES: Time to Word
Partially Recognized Results: %@
ES: TTAW
%s Recognition finished with status %@
%s Audio finish to recognizer finish = %lf sec, connection is %@, error %@
%s _recognitionBeginTime (%@) is greater than _recognitionEndTime (%@)
%s PauseDuration = %@
%s Local speech recognition completed without error, write DES record when needed
%s Writing DES record after 30 seconds delay: interactionId=%@
%s Submitted delayedBlock to dispatch_after
%s didProcessAudioDuration %f
%s #ASR on device eager formatted recognition candidate: %@
%s raw eager recognition candidate: %@
%s Could not make temporary attachment directory at %@: %@
%s Failed to specify compression algorithm: %s
%s Failed to specify format: %s
%s Start extracting archive at path: %s
%s Failed to open archive for reading: %s
%s Entry extraction path: %@
%s Unable to extract file to: %@
%s Finished extracting archive to: %@
%s Could not read %@: %@
%s Could not parse %@: %@
%s %@ is wrong type: %@
%s %@ contains bogus key/value pair: %@ => %@
%s VoiceMail asset could not be read: %@
%s Could not create asset reader output
%s Cannot add output
%s Could not get data pointer: %d
%s JSON serialization failed: %@
%s Compression failed: %@
%s Sampling: Error while initializing ESStoreAudioData because uuid is invalid.
%s %@ Sampling: Won't save audio because - has not recognized anything or has no data.
%s %@ Sampling: Won't save audio because - _currentAudioFilePath is null
%s %@ Sampling: invalid filePath or it is null.
%s %@ Sampling: Done with cleanup of audioFile=%@ and reset of variables.
%s %@ Sampling: Failed to save audio to cache dir. Error: %@
%s %@ Sampling: Successfully saved audio file to cache dir, path=%@
%s %@ Sampling: audioFileToBeMoved is nil
%s %@ Sampling: currentSamplingDate is nil
%s %@ Sampling: Error while creating Sampled directory in /var/mobile
%s %@ Sampling: Error while creating dated Sampled directory in %@ with date - %@
%s %@ Sampling: Error while moving file from cache directory to var/mobile/Library - %@
%s %@ Sampling: Successfully moved audio file to var/mobile/Library dir, path=%@
%s %@ Sampling: Error while writing audio metadata dict to plist - %@
%s %@ Sampling: currentSamplingDateString is null
%s Received SIGTERM. Cleaning and Exiting
ESSelfHelper
Timestamp
ESSelfPreheatWithPowerContainer
ESSpeechProfileBuilderConnection
CESRSpeechProfileBuilderService
NSObject
ESBiomeRecord
ESBiomeAsrRecord
CESRUtilitiesAdditions
CESRUtilities
ResultCandidateId
ESAssetManager
ESProfiler
ESAlignmentState
NSCopying
ESConnectionModelInfo
ESConnection
_EARSpeechRecognitionResultStream
CESRSpeechService
ESStoreAudioData
ESListenerDelegate
NSXPCListenerDelegate
@16@0:8
@20@0:8B16
@32@0:8@16@24
v16@0:8
@28@0:8@16B24
B28@0:8@16B24
B24@0:8@16
v24@0:8@16
v32@0:8@16@24
v92@0:8@16@24@32@40@48@56B64@68@76@84
v48@0:8@16@24@32@40
v188@0:8@16@24@32@40@48@56@64@72B80B84@88@96@104@112@120B128B132B136@140@148@156@164@172@180
v32@0:8@16q24
B16@0:8
v20@0:8B16
@"NSUUID"
@"NSString"
@"NSNumber"
@40@0:8@16@24@32
@"ASRSchemaASRPreheatContext"
@"SPIPowerLoggerSnapshot"
@"SPIEventContext"
@60@0:8@16@24@32B40^@44^@52
v44@0:8@16@24@32B40
v40@0:8@16@24@32
@56@0:8@16#24@32@40^@48
B88@0:8@16d24@32@40@48@56Q64@72@80
B80@0:8d16@24@32@40@48Q56@64@72
#16@0:8
@24@0:8:16
@32@0:8:16@24
@40@0:8:16@24@32
B24@0:8#16
B24@0:8:16
Vv16@0:8
Q16@0:8
^{_NSZone=}16@0:8
B24@0:8@"Protocol"16
@"NSString"16@0:8
Vv56@0:8@16@24@32q40@?48
Vv32@0:8@16@?24
Vv40@0:8@16@24@?32
Vv24@0:8@?16
Vv28@0:8B16@?20
Vv56@0:8@"NSString"16@"NSString"24@"NSString"32q40@?<v@?B@"NSError">48
Vv32@0:8@"NSString"16@?<v@?q@"NSError">24
Vv40@0:8@"NSDictionary"16@"NSString"24@?<v@?B@"NSError">32
Vv40@0:8@"NSArray"16@"NSArray"24@?<v@?B@"NSError">32
Vv24@0:8@?<v@?B@"NSError">16
Vv28@0:8B16@?<v@?B@"NSError">20
@24@0:8@16
@24@0:8^@16
@"NSMutableArray"
@"NSMutableDictionary"
@"NSMutableSet"
@"CESRAssetConfig"
@"NSXPCConnection"
@"NSObject<OS_os_transaction>"
@"_EARUserProfile"
d16@0:8
v24@0:8d16
@56@0:8@16@24@32@40Q48
@"NSArray"
Q20@0:8f16
f24@0:8Q16
Q24@0:8Q16
q24@0:8@16
@32@0:8@16d24
@36@0:8@16d24B32
@24@0:8Q16
@32@0:8@16Q24
Q32@0:8d16Q24
d32@0:8@16Q24
B24@0:8^{__CFString=}16
@40@0:8B16Q20B28^@32
v24@0:8Q16
@32@0:8@16^@24
@36@0:8@16^@24B32
@40@0:8@16^@24B32B36
v32@0:8@16Q24
B40@0:8@16Q24^@32
B32@0:8@16@24
@"NSObject<OS_dispatch_queue>"
@24@0:8d16
v32@0:8@16B24B28
v164@0:8@16@24@32@40@48@56@64@72B80B84@88@96@104@112@120B128B132B136@140@148@156
v32@0:8d16@24
{rusage_info_v6="ri_uuid"[16C]"ri_user_time"Q"ri_system_time"Q"ri_pkg_idle_wkups"Q"ri_interrupt_wkups"Q"ri_pageins"Q"ri_wired_size"Q"ri_resident_size"Q"ri_phys_footprint"Q"ri_proc_start_abstime"Q"ri_proc_exit_abstime"Q"ri_child_user_time"Q"ri_child_system_time"Q"ri_child_pkg_idle_wkups"Q"ri_child_interrupt_wkups"Q"ri_child_pageins"Q"ri_child_elapsed_abstime"Q"ri_diskio_bytesread"Q"ri_diskio_byteswritten"Q"ri_cpu_time_qos_default"Q"ri_cpu_time_qos_maintenance"Q"ri_cpu_time_qos_background"Q"ri_cpu_time_qos_utility"Q"ri_cpu_time_qos_legacy"Q"ri_cpu_time_qos_user_initiated"Q"ri_cpu_time_qos_user_interactive"Q"ri_billed_system_time"Q"ri_serviced_system_time"Q"ri_logical_writes"Q"ri_lifetime_max_phys_footprint"Q"ri_instructions"Q"ri_cycles"Q"ri_billed_energy"Q"ri_serviced_energy"Q"ri_interval_max_phys_footprint"Q"ri_runnable_time"Q"ri_flags"Q"ri_user_ptime"Q"ri_system_ptime"Q"ri_pinstructions"Q"ri_pcycles"Q"ri_energy_nj"Q"ri_penergy_nj"Q"ri_reserved"[14Q]}
@"ESSelfHelper"
@"CESRSignpostHelper"
@24@0:8^{_NSZone=}16
q16@0:8
v24@0:8q16
@40@0:8@16B24B28^@32
@88@0:8@16@24B32@36@44B52@56B64B68^@72^@80
v32@0:8@16d24
v72@0:8@16q24q32d40@48d56q64
v40@0:8@16Q24@32
v32@0:8@"_EARSpeechRecognizer"16@"_EARSpeechRecognitionResult"24
v32@0:8@"_EARSpeechRecognizer"16@"NSError"24
v32@0:8@"_EARSpeechRecognizer"16@"NSArray"24
v48@0:8@"_EARSpeechRecognizer"16@"NSArray"24@"NSArray"32@"NSArray"40
v32@0:8@"_EARSpeechRecognizer"16@"_EARSpeechRecognitionResultPackage"24
v32@0:8@"_EARSpeechRecognizer"16d24
v32@0:8@"_EARSpeechRecognizer"16@"_EARSpeechRecognition"24
v72@0:8@"_EARSpeechRecognizer"16q24q32d40@"NSArray"48d56q64
v40@0:8@"_EARSpeechRecognizer"16Q24@"NSDictionary"32
Vv32@0:8@16@24
Vv40@0:8@16@24@32
Vv24@0:8@16
Vv48@0:8@16@24@32@?40
Vv56@0:8@16@24@32@40@?48
Vv36@0:8B16Q20@?28
Vv40@0:8B16Q20B28@?32
Vv72@0:8@16@24@32Q40B48B52@56@?64
v56@0:8@16@24@32Q40@?48
Vv36@0:8@16B24@?28
Vv40@0:8@16Q24@?32
Vv32@0:8@16Q24
Vv76@0:8@16@24@32@40@48@56B64@?68
Vv24@0:8@?<v@?@"NSError">16
Vv32@0:8@"NSString"16@"NSURL"24
Vv40@0:8@"CESRAssetConfig"16@"NSString"24@"NSURL"32
Vv24@0:8@?<v@?>16
Vv32@0:8@"CESRSpeechParameters"16@?<v@?@"CESRModelProperties"@"NSError">24
Vv32@0:8@"CESRSpeechParameters"16@?<v@?@"NSString"@"NSString"@"NSError">24
Vv24@0:8@"NSData"16
Vv48@0:8@"NSString"16@"NSString"24@"NSData"32@?<v@?@"NSData"@"NSError">40
Vv56@0:8@"NSString"16@"NSString"24@"NSData"32@"NSString"40@?<v@?@"NSData"@"NSString"@"NSError">48
Vv36@0:8B16Q20@?<v@?@"NSDictionary"@"NSError">28
Vv40@0:8B16Q20B28@?<v@?@"NSDictionary"@"NSError">32
Vv28@0:8B16@?<v@?@"NSDictionary"@"NSError">20
Vv32@0:8@"NSString"16@?<v@?@"NSString"@"NSError">24
Vv32@0:8@"CESRAssetConfig"16@?<v@?@"NSString"@"NSError">24
Vv32@0:8@"CESRAssetConfig"16@?<v@?@"CESRModelProperties"@"NSError">24
Vv48@0:8@"NSDictionary"16@"NSData"24@"NSArray"32@?<v@?@"NSDictionary"@"NSData"@"NSError">40
Vv72@0:8@"NSDictionary"16@"NSDictionary"24@"NSString"32Q40B48B52@"NSSet"56@?<v@?@"NSDictionary"@"NSError">64
v56@0:8@"NSDictionary"16@"NSString"24@"NSString"32Q40@?<v@?@"NSDictionary"@"NSError">48
Vv36@0:8@"NSString"16B24@?<v@?@"NSData"@"NSString">28
Vv32@0:8@"NSSet"16@?<v@?@"NSNumber"@"NSError">24
Vv40@0:8@"NSSet"16Q24@?<v@?B@"NSError">32
Vv32@0:8@"NSSet"16Q24
Vv32@0:8@"AFSpeechCorrectionInfo"16@"NSString"24
Vv28@0:8B16@?<v@?>20
Vv76@0:8@"NSDictionary"16@"NSString"24@"NSDictionary"32@"NSArray"40@"NSString"48@"NSString"56B64@?<v@?@"NSDictionary"@"NSError">68
Vv40@0:8@"NSString"16@"NSString"24@"NSString"32
v36@0:8@16B24@?28
v40@0:8@16d24@32
v24@0:8@?16
v48@0:8@16@24@32@?40
Vv56@0:8@16@24@32Q40@?48
@48@0:8@16@24@32^@40
v24@0:8^@16
v28@0:8^@16B24
@"_EARSpeechRecognizer"
@"_EARSpeechRecognitionAudioBuffer"
@"AFSpeechPackage"
@"AFSpeechInfoPackage"
@"NSSet"
@"ESStoreAudioData"
@"ESBiomeRecord"
@"_EARSpeakerCodeWriter"
@"CESRFidesASRRecord"
@"NSObject<OS_dispatch_source>"
@96@0:8@16@24@32@40Q48@56@64@72B80B84@88
v40@0:8q16@24q32
@"NSMutableData"
B32@0:8@"NSXPCListener"16@"NSXPCConnection"24
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
<key>application-identifier</key>
<string>com.apple.siri.embeddedspeech</string>
<key>com.apple.CoreRoutine.LocationOfInterest</key>
<true/>
<key>com.apple.accounts.appleaccount.fullaccess</key>
<true/>
<key>com.apple.application-identifier</key>
<string>com.apple.siri.embeddedspeech</string>
<key>com.apple.coreaudio.allow-amr-decode</key>
<true/>
<key>com.apple.coreduetd.allow</key>
<true/>
<key>com.apple.coreduetd.people</key>
<true/>
<key>com.apple.developer.homekit</key>
<true/>
<key>com.apple.frontboardservices.display-layout-monitor</key>
<true/>
<key>com.apple.locationd.effective_bundle</key>
<true/>
<key>com.apple.private.DistributedEvaluation.RecordAccess-com.apple.fides.asr</key>
<true/>
<key>com.apple.private.DistributedEvaluation.RecordAccess-com.apple.siri.speech-dictation-personalization</key>
<true/>
<key>com.apple.private.assets.accessible-asset-types</key>
<array>
<string>com.apple.MobileAsset.Trial.Siri.SiriUnderstandingAsrAssistant</string>
<string>com.apple.MobileAsset.Trial.Siri.SiriUnderstandingAsrHammer</string>
<string>com.apple.MobileAsset.Trial.Siri.SiriDictationAssets</string>
<string>com.apple.MobileAsset.EmbeddedSpeech</string>
<string>com.apple.MobileAsset.EmbeddedSpeechWatch</string>
<string>com.apple.MobileAsset.EmbeddedSpeechMac</string>
</array>
<key>com.apple.private.assets.bypass-asset-types-check</key>
<true/>
<key>com.apple.private.attribution.implicitly-assumed-identity</key>
<dict>
<key>type</key>
<string>path</string>
<key>value</key>
<string>/System/Library/PrivateFrameworks/CoreEmbeddedSpeechRecognition.framework/XPCServices/com.apple.siri.embeddedspeech.xpc/com.apple.siri.embeddedspeech</string>
</dict>
<key>com.apple.private.biome.read-write</key>
<array>
<string>SiriDictation</string>
<string>Dictation.UserEdit</string>
</array>
<key>com.apple.private.calendar.allow-suggestions</key>
<true/>
<key>com.apple.private.contacts</key>
<true/>
<key>com.apple.private.corerecents</key>
<true/>
<key>com.apple.private.corespotlight.internal</key>
<true/>
<key>com.apple.private.homekit</key>
<true/>
<key>com.apple.private.security.storage.SiriVocabulary</key>
<true/>
<key>com.apple.private.security.storage.SpeechPersonalizedLM</key>
<true/>
<key>com.apple.private.tcc.allow</key>
<array>
<string>kTCCServiceAddressBook</string>
<string>kTCCServiceCalendar</string>
<string>kTCCServiceWillow</string>
<string>kTCCServiceMediaLibrary</string>
</array>
<key>com.apple.proactive.PersonalizationPortrait.Config</key>
<true/>
<key>com.apple.proactive.PersonalizationPortrait.NamedEntity.readOnly</key>
<true/>
<key>com.apple.proactive.eventtracker</key>
<true/>
<key>com.apple.security.exception.files.absolute-path.read-only</key>
<array>
<string>/private/var/MobileAsset/</string>
<string>/private/var/tmp/com.apple.siri-distributed-evaluation/</string>
</array>
<key>com.apple.security.exception.mach-lookup.global-name</key>
<array>
<string>com.apple.suggestd.contacts</string>
<string>com.apple.mobileasset.autoasset</string>
</array>
<key>com.apple.security.exception.shared-preference.read-only</key>
<array>
<string>com.apple.assistant</string>
<string>com.apple.assistant.backedup</string>
<string>com.apple.assistant.support</string>
</array>
<key>com.apple.security.iokit-user-client-class</key>
<array>
<string>AGXCommandQueue</string>
<string>AGXDevice</string>
<string>AGXDeviceUserClient</string>
<string>AGXSharedUserClient</string>
<string>H11ANEInDirectPathClient</string>
<string>IOAccelContext</string>
<string>IOAccelContext2</string>
<string>IOAccelDevice</string>
<string>IOAccelDevice2</string>
<string>IOAccelSharedUserClient</string>
<string>IOAccelSharedUserClient2</string>
<string>IOAccelSubmitter2</string>
<string>IOSurfaceRootUserClient</string>
</array>
<key>com.apple.security.personal-information.addressbook</key>
<true/>
<key>com.apple.security.temporary-exception.mach-lookup.global-name</key>
<array>
<string>com.apple.triald.namespace-management</string>
</array>
<key>com.apple.siriknowledged</key>
<true/>
<key>com.apple.spotlight.search</key>
<true/>
<key>com.apple.trial.client</key>
<array>
<string>372</string>
<string>401</string>
<string>751</string>
</array>
</dict>
</plist>
application-identifier
com.apple.siri.embeddedspeech0-(com.apple.CoreRoutine.LocationOfInterest
0/*com.apple.accounts.appleaccount.fullaccess
0A com.apple.application-identifier
com.apple.siri.embeddedspeech0)$com.apple.coreaudio.allow-amr-decode
com.apple.coreduetd.allow
com.apple.coreduetd.people
com.apple.developer.homekit
083com.apple.frontboardservices.display-layout-monitor
0)$com.apple.locationd.effective_bundle
0MHcom.apple.private.DistributedEvaluation.RecordAccess-com.apple.fides.asr
0idcom.apple.private.DistributedEvaluation.RecordAccess-com.apple.siri.speech-dictation-personalization
b/com.apple.private.assets.accessible-asset-types0
->com.apple.MobileAsset.Trial.Siri.SiriUnderstandingAsrAssistant;com.apple.MobileAsset.Trial.Siri.SiriUnderstandingAsrHammer4com.apple.MobileAsset.Trial.Siri.SiriDictationAssets$com.apple.MobileAsset.EmbeddedSpeech)com.apple.MobileAsset.EmbeddedSpeechWatch'com.apple.MobileAsset.EmbeddedSpeechMac061com.apple.private.assets.bypass-asset-types-check
9com.apple.private.attribution.implicitly-assumed-identity
type
path0
value
/System/Library/PrivateFrameworks/CoreEmbeddedSpeechRecognition.framework/XPCServices/com.apple.siri.embeddedspeech.xpc/com.apple.siri.embeddedspeech0I"com.apple.private.biome.read-write0#
SiriDictation
Dictation.UserEdit01,com.apple.private.calendar.allow-suggestions
com.apple.private.contacts
com.apple.private.corerecents
0-(com.apple.private.corespotlight.internal
com.apple.private.homekit
061com.apple.private.security.storage.SiriVocabulary
0<7com.apple.private.security.storage.SpeechPersonalizedLM
com.apple.private.tcc.allow0Y
kTCCServiceAddressBook
kTCCServiceCalendar
kTCCServiceWillow
kTCCServiceMediaLibrary072com.apple.proactive.PersonalizationPortrait.Config
0E@com.apple.proactive.PersonalizationPortrait.NamedEntity.readOnly
0% com.apple.proactive.eventtracker
:com.apple.security.exception.files.absolute-path.read-only0T
/private/var/MobileAsset/7/private/var/tmp/com.apple.siri-distributed-evaluation/0v4com.apple.security.exception.mach-lookup.global-name0>
com.apple.suggestd.contacts
com.apple.mobileasset.autoasset0
8com.apple.security.exception.shared-preference.read-only0P
com.apple.assistant
com.apple.assistant.backedup
com.apple.assistant.support0
.*com.apple.security.iokit-user-client-class0
AGXCommandQueue
AGXDevice
AGXDeviceUserClient
AGXSharedUserClient
H11ANEInDirectPathClient
IOAccelContext
IOAccelContext2
IOAccelDevice
IOAccelDevice2
IOAccelSharedUserClient
IOAccelSharedUserClient2
IOAccelSubmitter2
IOSurfaceRootUserClient083com.apple.security.personal-information.addressbook
0i>com.apple.security.temporary-exception.mach-lookup.global-name0'%com.apple.triald.namespace-management0
com.apple.siriknowledged
com.apple.spotlight.search
com.apple.trial.client0
372
401
mcpl
ASRPreheatContextTimestamp
ASRAppleNeuralEngineModelInitializationContextTimestamp
ASRAppleNeuralEngineModelInitializationContextFileName
-[ESSelfHelper initWithTask:isSpeechAPIRequest:]
-[ESSelfHelper logFinalResultGeneratedWithEARPackage:]
-[ESSelfHelper logRequestEndedOrFailedOrCancelledWithError:recognizerComponents:averageActiveTokensPerFrame:languageModelInterpolationWeights:signalToNoiseRatioInDecibels:recognitionDurationInSec:audioDurationMs:eagerUsed:utteranceDetectionEnabled:utteranceConcatenationEnabled:cpuRealTimeFactor:numLmeDataStreams:phoneticMatchDecoderName:pauseDurations:itnDurationInNs:isEmojiPersonalizationUsed:isEmojiDisambiguationUsed:isEmojiExpectedButNotRecognized:recognizedEmojis:totalITNDurationInNs:totalITNRuns:totalSecondaryPassDurationInNs:totalSecondaryPasses:cpuInstructionsInMillionsPerSecond:]
floatValue
-[ESSelfHelper wrapAndEmitTopLevelEvent:timestampInTicks:]
v32@?0@"NSArray"8Q16^B24
CESRProfileErrorDomain
\NT-contact
\NT-appname
\NT-correction
overrides
keyboardLM
locationOfInterest
spatialLocationOfInterest
interaction
search
calendarEvent
pexNamedEntity
frequency
charsToTrim
charsToSplit
tagName
templateName
tagNameList
minimumWordLength
com.apple.siri.embeddedspeech.profilegeneration-keepalive
-[ESSpeechProfileBuilderConnection initWithXPCConnection:]_block_invoke
v8@?0
Could not create EAR profile builder
-[ESSpeechProfileBuilderConnection getVersionForCategory:completion:]
-[ESSpeechProfileBuilderConnection beginWithCategoriesAndVersions:bundleId:completion:]
Begin called while there are already active categories.
Speech category %@ has already been committed with a call to Begin followed by Finish
Speech category %@ is unsupported
Add called before categories have been set with Begin
-[ESSpeechProfileBuilderConnection addVocabularyItems:isBoosted:completion:]
-[ESSpeechProfileBuilderConnection finishAndSaveProfile:completion:]
v32@?0@"NSString"8@"NSMutableArray"16^B24
Failed to write profile
Failed to load speech assets
-[ESSpeechProfileBuilderConnection _profileWithError:]
Failed to read the existing speech profile
+[ESSpeechProfileBuilderConnection personalizationRecipeForAssetConfig:modelOverridePath:]
personalization.json
+[ESSpeechProfileBuilderConnection userProfileWithAssetConfig:modelOverridePath:overrides:isJit:returningFoundPath:error:]
mini.json
en_US_napg.json
vocdelta.voc
pg.voc
mrec.psh
Error during _EARUserProfile initialization
v32@?0@"CESRVocabularyCategory"8@"NSSet"16^B24
\correction-first
+[ESSpeechProfileBuilderConnection _adaptRecipe:userData:profile:]_block_invoke
v32@?0@"NSString"8@"NSDictionary"16^B24
+[ESSpeechProfileBuilderConnection _parseRequiredParameter:expectedClass:domain:recipe:error:]
Missing %@ for %@
+[ESSpeechProfileBuilderConnection addWordsToUserProfile:templateName:wordArrays:]
v32@?0@"NSString"8@"NSString"16^B24
+[ESSpeechProfileBuilderConnection _runAdaptationRecipeForNamesWithFrequency:tagNames:templateName:charSetToTrim:charSetToSplit:minimumWordLength:nameFrequencies:profile:]
+[ESSpeechProfileBuilderConnection _runAdaptationRecipeForNamesWithFrequency:tagNames:templateName:charSetToTrim:charSetToSplit:minimumWordLength:nameFrequencies:profile:]_block_invoke_2
v32@?0@"NSString"8Q16^B24
v32@?0@"NSString"8@"NSNumber"16^B24
+[ESSpeechProfileBuilderConnection _runAdaptationRecipeForKeyboardLMWithFrequency:tagName:templateName:charSetToTrim:charSetToSplit:minimumWordLength:userData:profile:]
+[ESSpeechProfileBuilderConnection _runAdaptationRecipeForLocationOfInterestWithFrequency:tagName:templateName:charSetToTrim:charSetToSplit:minimumWordLength:locationOfInterestNames:profile:]
+[ESSpeechProfileBuilderConnection _runAdaptionRecipeForCalendarEventsWithFrequency:tagName:templateName:charSetToTrim:charSetToSplit:minimumWordLength:userData:profile:]
com.apple.MobileSMS
Dictation
ResultCandidateId
SpeechProfile
EARUserProfileContainerLoadDate
+[CESRUtilities speechProfileRootDirectories]
+[CESRUtilities speechProfilePathsWithLanguage:]
+[CESRUtilities loadSpeechProfiles:language:]
v32@?0@"_EARSpeechRecognitionToken"8Q16^B24
Param
v32@?0{_NSRange=QQ}8^B24
v32@?0@"EARVoiceCommandArgument"8Q16^B24
@"NSArray"24@?0@"NSArray"8@"NSArray"16
+[CESRUtilities AFSpeechInfoPackageForEARSpeechRecognitionResultPackage:]_block_invoke
unconstrained
reduced
avoid
com.apple.assistant
AFSpeechLatticeMitigatorResultForEar
quasarModelPath
type
com.apple.trial.NamespaceUpdate.SIRI_UNDERSTANDING_ASR_ASSISTANT
trial
GeoLMAssetsInfo
com.apple.siri.embeddedspeech.ESAssetManager
v12@?0i8
-[ESAssetManager registerNotifications]
-[ESAssetManager installationStatusForLanguagesIgnoringCache:assetType:withDetailedStatus:withError:]
-[ESAssetManager _invalidateInstallationStatusCacheForAssetType:]
-[ESAssetManager installedModelInfoForAssetConfig:error:triggerDownload:ignoreSpellingModel:]
-[ESAssetManager purgeInstalledAssetsExceptLanguages:assetType:error:]_block_invoke
-[ESAssetManager startMissingAssetDownload]
com.apple.internal.ck
disableTrialAssetDelivery
-[ESAssetManager isTrialAssetDeliveryEnabled]
enableTrialAssetDelivery
-[ESAssetManager prepareHammerConfigFile:]
-[ESAssetManager _installedGeoLMRegionMappingForLanguage:]
-[ESAssetManager geoLMRegionIdForLanguage:location:]
-[ESAssetManager installedGeoLMRegionSpecificAssetForLanguage:regionId:mainAssetConfig:]
-[ESAssetManager _geoLMCompatibleWithMainAsset:geoAssetConfig:]
-[ESAssetManager purgeUnusedGeoLMAssetsForLangauge:]_block_invoke
q24@?0@8@16
-[ESAssetManager purgeUnusedGeoLMAssetsForLangauge:]_block_invoke_2
-[ESAssetManager lastUsedGeoLMRegionIdForLanguage:]
-[ESAssetManager _purgeUserDefaultsGeoLMAssetsInfoDictExceptLanguages:cesrTrialAssetManager:]
ondevice_preheat_time
Failed
Success
AlreadyLoaded
-[ESProfiler logPendingPreheatContextEvents:]
ES: ANE Model Init
-[ESProfiler logPendingANEModelInitializationContextEvents:]
ES: Engine Init
ES: Update Active Config
ES: JIT Profile Build Load
ES: AOT Profile Load
confidenceThresholds
confidenceModels
wordConfidenceThreshold
utteranceConfidenceThreshold
continuousListening
shouldHandleCapitalization
QuasarModel
replicateDataPacketPersonalization
distributedEvaluation
adaptation
WallRTF
DecodeDuration
AverageActiveTokensPerFrame
lm_interp_weights
jitQueryDurationInMs
jitLmeDurationInMs
jitDataStats
version
data
language
assetPath
PERSONALINFO
com.apple.fides.asr
com.apple.siri.speech-dictation-personalization
dictation
ModelOverrideURL
TP/tantor/voice_commands
textfield-editing-suite.plist
com.apple.siri.ESConnection
-[ESConnection initWithXPCConnection:]
-[ESConnection initWithXPCConnection:]_block_invoke
-[ESConnection dealloc]
com.apple.siri.ESConnection.fidesEval
com.apple.siri.ESConnection.redecode
v24@?0@"NSDictionary"8@"NSError"16
-[ESConnection fetchModelInfoForAssetConfig:triggerDownload:completion:]
Corrupt assets found at: %@
-[ESConnection fetchModelPropertiesForAssetConfig:completion:]_block_invoke
-[ESConnection getOfflineAssetStatusIgnoringCache:assetType:withDetailedStatus:withCompletion:]
SiriX
enableTelemetry=YES
+[ESConnection _speechRecognizerWithAssetConfig:geoLMRegionId:enableITN:overrides:modelOverrideURL:preloadModels:requestSource:enableParallelLoading:isHighPriority:geoLMLoadedOut:error:]
enableParallelLoading
preheatSource
ASR task for memory lock
taskForMemoryLock
Siri
dictation_emoji_recognition
dictation_voice_commands
Failed to create recognizer from %@
keepANEModelLoaded
-[ESConnection preheatSpeechRecognitionWithAssetConfig:preheatSource:modelOverrideURL:]
Preheating
 with CustomModelURL %@
(none)
Failure
Preheat time interval in seconds: %lf
-[ESConnection shouldWriteDictationRecord:]
v24@?0@"CESRModelProperties"8@"NSError"16
-[ESConnection startSpeechRecognitionWithParameters:didStartHandlerWithInfo:]
Recognizer is busy
@"NSDictionary"8@?0
jitDataProcessing
-[ESConnection startSpeechRecognitionWithParameters:didStartHandlerWithInfo:]_block_invoke
-[ESConnection sendSpeechCorrectionInfo:interactionIdentifier:]_block_invoke
\jit
v32@?0@"NSData"8Q16^B24
-[ESConnection readProfileAndUserDataWithLanguage:allowOverride:completion:]
Not a dictionary: %@
Not an array: %@
orth
prons
freq
v32@?0@"NSString"8@"NSArray"16^B24
updateSpeechProfileWithLanguage is currently unsupported.
-[ESConnection _scheduleCompilationTimer:]_block_invoke
-[ESConnection _scheduleCompilationTimer:]
Cooldown timeout for EAR
+[ESConnection _scheduleCooldownTimer]
com.apple.siri.embeddedspeech.keepalive
+[ESConnection _cancelCooldownTimer]
+[ESConnection _cooldownTimerFired]
+[ESConnection _cachedRecognizerCleanUp]
+[ESConnection prepareToExit]
+[ESConnection _sendPendingAnalyticsEvents]
recipeType
-[ESConnection readTableFromURL:]
-[ESConnection runRecipeEvaluationDistributedEvaluation:recordData:attachments:completion:]
returnHypothesis
returnOverallWER
returnOverallRTF
returnPerUtteranceWER
returnPerUtteranceRTF
locale
task
sampleRate
wordSenseWhitelist
wav.scp
raw.ref
-[ESConnection runRecipeEvaluationDistributedEvaluation:recordData:attachments:completion:]_block_invoke
token
confidence
transcription
\\\S+$
EditDistance
details
modelVersion
overall-rtf
overall-wer
v24@?0@"NSData"8@"NSString"16
SiriCoreLocalSpeechUserData
-[ESConnection runDefaultAdaptationEvaluation:recordData:attachments:completion:]
Recipe has no profile
Voicemail audio file deleted by user
%@ %@
.model
Malformed overrides
ConfidenceMean
ConfidenceMin
ConfidenceMax
AlternateConfidenceMean
AlternateConfidenceMin
AlternateConfidenceMax
Baseline
CustomModel
ModelVersion
Adapted
Alignment
name
callStackReturnAddresses
callStackSymbols
reason
userInfo
(unknown C++ exception)
-[ESConnection redecodeWithAudioDatas:language:task:samplingRate:completion:]_block_invoke
Interrupted corrected text redecoding during speech recognition
-[ESConnection runCorrectedTextEvaluationWithAudioDatas:recordDatas:language:samplingRate:caseSensitive:skipLME:wordSenseAccessListSet:completion:]_block_invoke
No tokenizer for %@
-[ESConnection runCorrectedTextEvaluationWithAudioDatas:recordDatas:language:samplingRate:caseSensitive:skipLME:wordSenseAccessListSet:completion:]_block_invoke_2
Interrupted corrected text evaluation during speech recognition
correctedOutput
recognizedOutput
editDistanceRecognizedCorrected
editDistanceRecognizedTTSASR
timestamp
interactionId
asrSelfComponentIdentifier
results
-[ESConnection _modelRootWithAssetConfig:modelOverridePath:overrides:error:]
asrSelfComponentId
asset
applicationName
metrics
tokens
alignments
uttInfos
uttInfosCompressed
alignmentReferences
usePersonalizedLM
corrected
recognized
useJIT
disableAOT
contextualData
overrideFiles
restoreAOT
evaluations
-[ESConnection runEvaluationWithDESRecordDatas:language:recipe:attachments:fidesPersonalizedLMPath:fidesPersonalizedLMTrainingAsset:scrubResult:completion:]_block_invoke
Unknown evaluation name found in alignmentReferences: %@
scoreNbest
compress
-[ESConnection runEvaluationWithDESRecordDatas:language:recipe:attachments:fidesPersonalizedLMPath:fidesPersonalizedLMTrainingAsset:scrubResult:completion:]_block_invoke_3
Interrupted evaluation redecoding
-[ESConnection runEvaluationWithDESRecordDatas:language:recipe:attachments:fidesPersonalizedLMPath:fidesPersonalizedLMTrainingAsset:scrubResult:completion:]_block_invoke_2
Interrupted evaluation tokenization
-[ESConnection _deleteTemporaryDirectoryIfExists:]
-[ESConnection resetCacheAndCompileAllAssetsWithCompletion:]
-[ESConnection pauseRecognition]
-[ESConnection resumeRecognitionWithPrefixText:postfixText:selectedText:]
-[ESConnection _writeDESRecord:oneRecordPerDay:]
-[ESConnection speechRecognizer:didProduceEndpointFeaturesWithWordCount:trailingSilenceDuration:eosLikelihood:pauseCounts:silencePosterior:processedAudioDurationInMilliseconds:]_block_invoke
\entity-first
-[ESConnection speechRecognizer:didProduceLoggablePackage:]_block_invoke
-[ESConnection speechRecognizer:didRecognizePartialResult:]_block_invoke
Unsupported EAR build?
Unsupported EAR package build?
DUMMYTOKEN
-[ESConnection speechRecognizer:didRecognizeFinalResultCandidatePackage:]_block_invoke
-[ESConnection speechRecognizer:didRecognizeFinalResultPackage:]_block_invoke
%@ %@:%f,
%@ TTAW:%f MEAN:%f, MAX:%f, MIN:%f 
RECOGNITION_SUCCESS
RECOGNITION_FAILED
RECOGNITION_CANCELLED
RECOGNITION_REJECTED
-[ESConnection speechRecognizer:didFinishRecognitionWithError:]_block_invoke
recognizer-components
audioDurationMs
recognitionDurationMs
v32@?0@"NSNumber"8Q16^B24
@sum.self
EagerUsed
numLmeDataStreams
PM-decoder
PM-input
PM-output
-[ESConnection speechRecognizer:didProcessAudioDuration:]_block_invoke
tokenName
-[ESConnection speechRecognizer:didRecognizeRawEagerRecognitionCandidate:]_block_invoke_2
v16@?0@"_EARFormatter"8
-[ESConnection speechRecognizer:didRecognizeRawEagerRecognitionCandidate:]_block_invoke
lastPathComponent == %@
_ESDecompressArchiveWithURL
VoiceTriggerFidesArchive
Failed to specify compression algorithm
Failed to specify format
Failed to open file for reading
Unable to extract file to: %@
Task %@ not available for %@, supported tasks are %@
etiquette.json
ReplacementDictionaryForAssetConfig
ReplacementDictionaryForAssetConfig_block_invoke
v32@?0@"NSString"8@16^B24
voicemail_confidence_subtraction.json
sampling rate = %@
task type = %@
far field = %@
device ID = %@
Bluetooth device ID = %@
ANE context = %@
CPU context = %@
GPU context = %@
ReadAudioDataFromFileURL
\\\S*$
Insertions
Deletions
Substitutions
ReferenceSize
\contact-first
CONTACTFIRSTNAME
\contact-middle
CONTACTMIDDLENAME
\contact-last
CONTACTLASTNAME
\contact-nickname
\app-first
APPNAMEFIRSTNAME
\company-first
COMPANYFIRSTNAME
\interaction-first
\interaction-middle
\interaction-last
INLINEFIRSTNAME
\contact-first-derived
\contact-middle-derived
\contact-last-derived
\contact-nickname-derived
Recipe evaluation failed
Empty recognition Result
Error
UtteranceLength
NumberOfNonTerminals
WordsAboveThreshold
UtteranceAboveThreshold
Override file is not found in attachments or device
restoreJIT
configurationFile
JIT LME: JIT profile builder is not initialized
JIT LME: required configuration/file is missing
JIT LME: configuration file is not found in attachments or device
zlibCompressedJson
dictationUIInteractionIdentifier
interactionIdentifier
samplingTimestamp
codec
samplingRate
inferenceSpeakerCode
numTrainedFrames
trainingNnetVersion
isSpeakerCodeUsed
-[ESStoreAudioData initWithUUIDString:language:task:codec:samplingRate:inferenceSpeakerCode:numTrainedFrames:trainingNnetVersion:isSpeakerCodeUsed:isSamplingForDictation:selfLogger:]
-[ESStoreAudioData saveAudioToDisk]
-[ESStoreAudioData _deleteItemAtPath:]
-[ESStoreAudioData _cleanupCacheAndReset:]
-[ESStoreAudioData _saveAudioToCache:]
-[ESStoreAudioData _moveAudioToVarMobile:]
%@.plist
-[ESStoreAudioData _saveAudioMetadataToFilePath:]
-[ESStoreAudioData _createAudioFilePath]
%.0f
%@_%@_%@.pcm
unixTime
samplingDate
success
failed
errorCode
errorDomain
UNDEFINED
description
underlyingErrorCode
underlyingErrorDomain
Audio file to be moved nil
Sampling Date is nil
Unable to create sampling directory
Unable to create dated directory
status
com.apple.siri.embeddedspeech
com.apple.private.des-service
Rejecting %@, no %@ entitlement
Rejecting %@, no %@ or %@ entitlement
main_block_invoke
TB,R,N,V_isSamplingForDictation
AFSpeechInfoPackageForEARSpeechRecognitionResult:
_audioDataPostRecognitionStart:
JSONObjectWithData:options:error:
_fidesEvalQueue
T@"ASRSchemaASRPreheatContext",R,N,V_preheatContext
_isProfileValidFromVersionsMap:
T@"ESSelfHelper",&,N,V_selfLogger
_recognizedText
T@"NSArray",C,N,V_recognizedTokens
_signpostHelper
T@"NSDate",C,N
_userId
T@"NSMutableDictionary",&,N,V_audioMetadata
allKeys
T@"NSNumber",&,N,V_personalizedLmAgeInSec
audioDurationInMs:samplingRate:
T@"NSNumber",R,N,V_numTrainedFrames
captureSnapshot
T@"NSString",&,N
containsObject:
T@"NSString",&,N,V_unrepairedPostItn
countForObject:
T@"NSString",C,N,V_UUIDString
currentCalendar
T@"NSString",C,N,V_correctedText
dealloc
T@"NSString",C,N,V_interactionId
endTime
T@"NSString",C,N,V_logPrefix
indexes
T@"NSString",C,N,V_task
initWithConfig:
T@"NSString",R,C
initWithSuites:resourceBaseURL:
T@"NSString",R,C,N,V_interactionId
isEmojiExpectedButNotRecognized
T@"NSString",R,C,N,V_taskName
isProxy
T@"NSString",R,N,V_recognitionTask
itnDurationInNs
T@"SPIEventContext",R,N,V_powerEventContext
logPowerSnapshotForProcessEnded
TB,N,V_continuousListeningEnabled
markRecognition
TB,R,N,V_hasRecognizedAnything
numberWithBool:
TQ,N,V_samplingRate
phrases
TQ,R,N,V_samplingRate
profile
Tq,N,V_numberOfDeletions
qualify
Tq,N,V_numberOfSubstitutions
recordFromData:
URLByAppendingPathComponent:
saveAudioToDisk
UUID
_UUID
setCorrectedText:interactionId:
_adaptRecipe:userData:profile:
setErrorDomain:
_addPendingProfilerANEModelInitializationEvent:
setInputOrigin:
_applicationName
setModelLocale:
_assetConfig
setWithObjects:
_audioBuffer
_audioDurationProcessingTimestampInTicks
speechProfilePathsWithLanguage:
_audioPackets
stringByAppendingPathComponent:
_bestGuessTaskStringsFromPreheatSource:
version
_bufferedAudioEnded
.cxx_destruct
Td,N,V_collectedAudioDurationMS
AFSpeechInfoPackageForEARSpeechRecognitionResultPackage:
_committedItems
T#,R
_isHighPriority
T@"CESRSignpostHelper",R,N,V_signpostHelper
_preheatContext
T@"ESSelfHelper",R,N,V_selfHelper
_seenCategories
T@"NSArray",C,N,V_selectedAlternatives
_storeAudioData
T@"NSMutableData",&,N,V_audioPackets
addAudioPacket:
T@"NSNumber",&,N
applicationName
T@"NSNumber",&,N,V_personalizedLmWeight
base64EncodedStringWithOptions:
T@"NSNumber",R,N,V_trainingNnetVersion
confidenceScore
T@"NSString",&,N,V_codec
context
T@"NSString",C,N
createANEModelInitializationStartedEventWithTimeStamp:fileName:
T@"NSString",C,N,V_applicationName
T@"NSString",C,N,V_currentAudioFilePath
T@"NSString",C,N,V_language
hasData
T@"NSString",C,N,V_recognizedText
infersQoSFromInstanceUUIDForEAR
T@"NSString",C,N,V_taskName
initWithNSUUID:
T@"NSString",R,C,N,V_asrId
interpretations
T@"NSString",R,C,N,V_language
isFinal
T@"NSString",R,N,V_inferenceSpeakerCode
isSpeakerCodeTrainingSupported:
T@"NSUUID",R,N,V_asrId
logPendingPreheatContextEvents:
T@"SPIPowerLoggerSnapshot",R,N,V_powerSnapshot
lowercaseString
TB,N,V_hasRecognizedAnything
metrics
TB,R,N,V_isSpeakerCodeUsed
oneBest
TQ,R
prepareModelInfo:withAssetType:
Td,N,V_recognitionEndTime
promoteModelInfo:withAssetType:
Tq,N,V_numberOfInsertions
recognitionTask
Tq,N,V_totalCost
release
UTF8String
serviceListener
UUIDString
setAverageActiveTokensPerFrame:
_UUIDString
setEndTimeInNs:
_addPendingAnalyticsEvent:
setExtraLmList:
_addPendingProfilerPreheatEvent:
setIsModifiedByAutoPunctuation:
_asrId
setUserProfile:
_assistantAssetUpdatedNotificationToken
sharedAnalytics
_audioDurationMs
speakerCodeInfo
_audioMetadata
statusForError:
_baseDirectory
tagName
_biomeRecord
vocabularyWords
_bufferedAudioPacketTimestamps
_bufferedAudioPacketTimestampsInTicks
_bufferedAudioPackets
_cachedRecognizerCleanUp
_cancelCooldownTimer
_cleanupCacheAndReset:
_clearPendingAnalyticsEvents
_clearPendingProfilerEvents
_codec
_collectedAudioDurationMS
_committedCategoryToVersion
_compilationTimer
_connection
_continuousListeningEnabled
_cooldownTimerFired
_cooldownTimerTimeoutSeconds
_correctedText
_createAudioFilePath
_createCachesDirectoryIfItDoesNotExist
_currentAudioFilePath
_dataProtectionClass
_delegate
_deleteItemAtPath:
_deleteTemporaryDirectoryIfExists:
_desRecord
_desRecordDictation
_disableDeliveringAsrFeatures
_firstAudioPacketProcessed
_firstAudioPacketReceivedTime
_firstAudioPacketReceivedTimeInTicks
_firstAudioPacketTimeUntilFirstPartial
_geoLMAssetsInfoDict
_geoLMCompatibleWithMainAsset:geoAssetConfig:
_hasRecognizedAnything
_inferenceSpeakerCode
_installedGeoLMRegionMappingForLanguage:
_interactionId
_invalidateInstallationStatusCacheForAssetType:
_invalidated
_isLoggingAllowedForCurrentRequestWithTask:isSpeechAPIRequest:
_isNonTier1Message:
_isSamplingForDictation
_isSpeakerCodeUsed
_isSpeechAPIRequest
_isTier1LoggingAllowedForCurrentRequest
_isTier1LoggingAllowedForCurrentRequestWithTask:
_language
_languageInstallationCache
_lastAudioPacketReceivedTime
_lastAudioPacketReceivedTimeInTicks
_lastRecognizedMetadataPackage
_lastRecognizedPackage
_lastWordCount
_loadGeoLMAssetsInfoDictForLanguage:
_localMetrics
_logAudioSampledEventsWithStatus:error:customReasonForFailure:
_logPrefix
_modelRootWithAssetConfig:modelOverridePath:overrides:error:
_moveAudioToVarMobile:
_numTrainedFrames
_numberOfDeletions
_numberOfInsertions
_numberOfSubstitutions
_packageLogged
_parseRequiredParameter:expectedClass:domain:recipe:error:
_partialResultsTimeList
_personalizedLmAgeInSec
_personalizedLmWeight
_powerEventContext
_powerSnapshot
_processAudioPacket:packetReceivedTime:packetReceivedTimeInTicks:
_processBufferedAudioPackets
_processedAudioDuration
_profile
_profileAssetPathsInUse
_profileWithError:
_purgeUserDefaultsGeoLMAssetsInfoDictExceptLanguages:cesrTrialAssetManager:
_queue
_recognitionAbsoluteEndTime
_recognitionBeginTime
_recognitionEndTime
_recognitionTask
_recognizedTokens
_recognizer
_recognizerAssetPathsInUse
_redecodeQueue
_requestCompletion
_resultCandidateId
_runAdaptationRecipeForDomain:frequency:tagNames:templateName:charSetToTrim:charSetToSplit:minimumWordLength:userData:profile:
_runAdaptationRecipeForKeyboardLMWithFrequency:tagName:templateName:charSetToTrim:charSetToSplit:minimumWordLength:userData:profile:
_runAdaptationRecipeForLocationOfInterestWithFrequency:tagName:templateName:charSetToTrim:charSetToSplit:minimumWordLength:locationOfInterestNames:profile:
_runAdaptationRecipeForNamesWithFrequency:tagNames:templateName:charSetToTrim:charSetToSplit:minimumWordLength:nameFrequencies:profile:
_runAdaptionRecipeForCalendarEventsWithFrequency:tagName:templateName:charSetToTrim:charSetToSplit:minimumWordLength:userData:profile:
_samplingRate
_saveAudioMetadataToFilePath:
_saveAudioToCache:
_scheduleCompilationTimer:
_scheduleCooldownTimer
_selectedAlternatives
_selfHelper
_selfLogger
_sendPendingAnalyticsEvents
_setQueue:
_shouldStoreDictationAudioOnDevice
_speakerCodeWriter
_speechRecognizerWithAssetConfig:enableITN:isHighPriority:error:
_speechRecognizerWithAssetConfig:geoLMRegionId:enableITN:overrides:modelOverrideURL:preloadModels:requestSource:enableParallelLoading:isHighPriority:geoLMLoadedOut:error:
_stagedCategoryToVersion
_stagedItems
_task
_taskName
_taskToUse
_timeUntilRecognitionStartInMs
_totalCost
_trainingNnetVersion
_transaction
_trimAudioIfNeeded:
_unrepairedPostItn
_updateAudioDuration:
_updateGeoLMAssetsInfoDictWithRegionId:language:
_updateUserDefaultsWithGeoLMAssetsInfoDict:language:
_userDefaultsGeoLMAssetsInfoDictKeyForLangauge:
_userProfileWithAssetConfig:modelOverridePath:overrides:isJit:returningFoundPath:error:
_validDomains
_weakFidesRecognizer
_writeDESRecord:
_writeDESRecord:oneRecordPerDay:
aFEnableFeatureAndCheckPreferenceValueWithKey:
acousticFeatureValuePerFrame
acousticFeatures
activeConfiguration
activeConfigurationForEverything
activeConfigurationForNothing
activeDictationLanguages
adaptUserProfileWithUserData:personalizationRecipe:userData:endOfUserData:
addAudioSamples:count:
addEntriesFromDictionary:
addObject:
addObjectsFromArray:
addOutput:
addVocabularyItems:isBoosted:completion:
addWordWithParts:templateName:
addWordsToUserProfile:templateName:wordArrays:
afRecognitionForEARSausage:processedAudioDuration:
afSpeechPackageForEARPackage:processedAudioDuration:speechProfileUsed:
afTokensForEARTokens:removeSpaceBefore:
afVoiceCommandGrammarParseResultForEARTokenString:withEARVoiceCommandInterpretations:
allItnRunIntervals
allObjects
allValues
allocWithZone:
alternativesSelectedInfo
aneContextFilter
appNames
appendData:
appendString:
appendedAutoPunctuation
arguments
array
arrayByAddingObject:
arrayByAddingObjectsFromArray:
arrayWithObjects:count:
asrId
asrSelfComponentIdentifier
assetReaderAudioMixOutputWithAudioTracks:audioSettings:
assetReaderWithAsset:error:
assetType
assetWithURL:
assistantIsEnabled
attributesOfItemAtPath:error:
audioAnalytics
audioData:withBytesFromEnd:
audioLengthInBytes:samplingRate:
audioMetadata
audioPackets
autorelease
beginWithCategoriesAndVersions:bundleId:completion:
bluetoothDeviceIdFilter
boolValue
bytes
calculateDiffInDaysFromTimestamp:
callStackReturnAddresses
callStackSymbols
canAddOutput:
cancelRecognition
cancelWithCompletion:
captureESStartTimeInTicks
censorSpeech
characterSetWithCharactersInString:
class
cleanupCacheAndReset
code
codec
collectedAudioDurationMS
commandIdentifier
compare:
compileRecognizerModelsWithConfiguration:
component
components
components:fromDate:toDate:options:
componentsJoinedByString:
componentsSeparatedByCharactersInSet:
componentsSeparatedByString:
compressedDataUsingAlgorithm:error:
concatenateUtterances
concatenatedAudioPackets
confidence
conformsToProtocol:
contactWordsWithFrequency
containsEntity
contextualData
continuousListening
continuousListeningEnabled
convertLanguageCodeToSchemaLocale:
copy
copyNextSampleBuffer
copyWithZone:
correctPartialResultIndexList
correctedText
corrections
count
countByEnumeratingWithState:objects:count:
cpuContextFilter
createANEModelInitializationEndedEventWithTimeStamp:fileName:
createDirectoryAtPath:withIntermediateDirectories:attributes:error:
createDirectoryAtURL:withIntermediateDirectories:attributes:error:
createInlineLmeUserDataForContextData:speechProfile:
createInlineLmeUserDataForContextStrings:
createPreheatEndedEventWithPreheatAlreadyDone:
createPreheatFailedEvent
createPreheatStartedOrChangedEvent
createSamplingDirectory
createSpeechProfileWithLanguage:modelOverridePath:JSONData:completion:
currentAudioFilePath
data
dataProfile
dataWithBytes:length:
dataWithContentsOfFile:options:error:
dataWithJSONObject:options:error:
date
dateWithTimeIntervalSince1970:
debugDescription
decodeObjectOfClass:forKey:
defaultManager
defaultStore
deleteAllDESRecordsForDictationPersonalizationWithCompletion:
deleteAllRecordsForPlugin:completion:
deleteItemAtFilePath:
deleteLinkId
deliverEagerPackage
description
detectUtterances
deviceIdFilter
dictationIsEnabled
dictationUIInteractionIdentifier
dictionary
dictionaryForKey:
dictionaryRepresentation
dictionaryWithContentsOfFile:
dictionaryWithContentsProfilePathForLanguage:errorOut:
dictionaryWithDictionary:
dictionaryWithObjects:forKeys:count:
didFinishModelInitializing:
didStartModelInitializing:
disableDeliveringAsrFeatures
domain
doubleValue
dummyResultPackage:
earTokensForAFTokens:appendedAutoPunctuation:
earTokensToString:
emitMessage:
emitMessage:timestamp:
enableAutoPunctuation
enableEmojiRecognition
enableTransactions
enableVoiceCommands
endAudio
ended
endpointStart
enumerateKeysAndObjectsUsingBlock:
enumerateObjectsUsingBlock:
enumerateRangesUsingBlock:
enumeratorAtPath:
errorWithDomain:code:userInfo:
eventLocationNames
eventTitles
exists
failed
farField
farFieldFilter
fetchAndDestroySignpostIdForEventName:
fetchAssetsForAssetConfig:completion:
fetchAssetsForLanguage:completion:
fetchModelInfoForAssetConfig:completion:
fetchModelInfoForAssetConfig:triggerDownload:completion:
fetchModelPropertiesForAssetConfig:completion:
fetchNamedEntitiesWithTimeInterval:
fileExistsAtPath:
fileExistsAtPath:isDirectory:
fileName
fileSystemRepresentation
fileURLWithPath:
fileURLWithPath:isDirectory:
filteredArrayUsingPredicate:
finishAndSaveProfile:completion:
finishAudio
finishDecoding
firstMatchInString:options:range:
firstObject
firstResultAfterResume
floatValue
formattedRecognitionWithNBestList:
frameDuration
frequency
geoLMRegionIdForLanguage:location:
getFormatterWithBlock:
getHostClockFrequency
getOfflineAssetStatusIgnoringCache:assetType:withCompletion:
getOfflineAssetStatusIgnoringCache:assetType:withDetailedStatus:withCompletion:
getOfflineDictationStatusIgnoringCache:withCompletion:
getRecognizerQueue
getVersionForCategory:completion:
gpuContextFilter
hasRecognizedAnything
hasRecognizedAnythingInAFSpeechPackage:
hasSpaceAfter
hasSpaceBefore
hasSuffix:
hash
hostTimeToSeconds:
incrementCost
incrementDeletions
incrementInsertions
incrementSampledRequestCount
incrementSubstitutions
indexOfObject:
inferenceSpeakerCode
init
initForReadingFromData:error:
initWithAcousticFeatureValue:frameDuration:
initWithAsrId:interactionId:language:taskName:samplingRate:
initWithCapacity:
initWithCommandGrammarParsePackage:
initWithCommandId:isComplete:paramMatches:
initWithConfiguration:language:overrides:sdapiOverrides:emptyVoc:pgVoc:paramsetHolder:isJit:
initWithConfiguration:overrides:overrideConfigFiles:language:activeConfiguration:modelLoadingOptions:enableSpeakerCodeTraining:supportEmojiRecognition:voiceCommandActiveSet:modelContextDelegate:enableItn:
initWithConfiguration:taskName:applicationName:
initWithConfiguration:useQuasarFormatter:activeConfiguration:
initWithContentsOfFile:options:error:
initWithCurrentProcess
initWithDictionary:
initWithDomain:code:userInfo:
initWithESSelfHelper:signpostHelper:
initWithExplicitlyRejectedString:
initWithImplicitlyEngagedString:
initWithInterpretationIndices:confidenceScore:
initWithInterpretations:isLowConfidence:
initWithItems:language:
initWithLanguage:assetType:
initWithLanguage:task:
initWithModelRoot:
initWithModelVersion:modelType:modelRoot:
initWithNBestParses:preITNNBestParses:
initWithNcsRoot:
initWithOrthography:pronunciations:tag:
initWithOrthography:pronunciations:tagName:frequency:
initWithPath:error:
initWithPhrases:utterances:processedAudioDuration:
initWithPlistJSONDictionary:
initWithPreheatContext:powerSnapshot:powerEventContext:
initWithRecognition:unfilteredRecognition:rawRecognition:audioAnalytics:isFinal:utteranceStart:latticeMitigatorResult:recognitionPaused:
initWithRecognition:unfilteredRecognition:rawRecognition:audioAnalytics:isFinal:utteranceStart:latticeMitigatorResult:recognitionPaused:speechProfileUsed:resultCandidateId:
initWithResults:score:threshold:
initWithSpeechRecognitionFeatures:acousticFeatures:snr:
initWithSuiteName:
initWithTask:isSpeechAPIRequest:
initWithText:
initWithTokenName:start:end:silenceStart:confidence:hasSpaceAfter:hasSpaceBefore:phoneSequence:ipaPhoneSequence:appendedAutoPunctuation:
initWithUUIDBytes:
initWithUUIDString:
initWithUUIDString:language:task:codec:samplingRate:inferenceSpeakerCode:numTrainedFrames:trainingNnetVersion:isSpeakerCodeUsed:isSamplingForDictation:selfLogger:
initWithUtterance:parseCandidates:
initWithXPCConnection:
initialize
initializeSharedPowerLoggerIfNeeded
inputOrigin
installationStatusForLanguagesForAssetType:includeDetailedStatus:error:
installationStatusForLanguagesIgnoringCache:assetType:withDetailedStatus:withError:
installedAssetWithConfig:
installedAssetWithConfig:regionId:
installedAssetWithConfig:regionId:triggerDownload:
installedGeoLMRegionSpecificAssetForLanguage:regionId:mainAssetConfig:
installedHammerConfigFileForLanguage:
installedModelInfoForAssetConfig:error:
installedModelInfoForAssetConfig:error:triggerDownload:
installedModelInfoForAssetConfig:error:triggerDownload:ignoreSpellingModel:
installedQuasarModelPathForAssetConfig:error:
installedQuasarModelPathForAssetConfig:error:triggerDownload:
installedQuasarModelPathForAssetConfig:error:triggerDownload:ignoreSpellingModel:
intValue
integerValue
interactionId
interactionIdentifier
interactionSenderDisplayNames
interpretationIndices
interruptTraining
invalidate
invalidatePersonalizedLM
invalidateUaapLm
ipaPhoneSequence
isDictationHIPAACompliant
isEmojiDisambiguationUsed
isEmojiPersonalizationUsed
isEqual:
isEqualToDictionary:
isEqualToString:
isFilePathValid:
isKindOfClass:
isMemberOfClass:
isModifiedByAutoPunctuation
isRequestSelectedForSamplingForTask:
isRequestSelectedForSamplingFromConfigForLanguage:
isSamplingForDictation
isSiriXEnabled
isSpeakerCodeUsed
isSpeechAPIRequest
isTrialAssetDeliveryEnabled
itemFromBuffer:error:
jitGrammar
jsonFilenameForAssetType:
keyboardLMDynamicVocabularyItems
keysSortedByValueUsingComparator:
language
languageCode
languageStringForLocaleString:
lastObject
lastPathComponent
lastUsedGeoLMRegionIdForLanguage:
latticeMitigatorResult
length
linkId
listener:shouldAcceptNewConnection:
loadConfigs
loadDate
loadSpeechProfiles:language:
localeStringForLanguageString:
localizedDescription
location
locationOfInterestNames
logActiveConfigUpdateEnded
logActiveConfigUpdateEndedWithTimeInTicks:
logActiveConfigUpdateStartedOrChangedWithTimeInTicks:
logAotLmeEndedWithTimeInTicks:
logAotLmeStartedOrChangedWithTimeInTicks:
logAppleNeuralEngineModelInitializationEnded
logAppleNeuralEngineModelInitializationEndedWithTimeInTicks:fileName:
logAppleNeuralEngineModelInitializationStartedOrChangedWithTimeInTicks:
logAppleNeuralEngineModelInitializationStartedOrChangedWithTimeInTicks:fileName:
logAudioPacketArrivalEndedWithTimeInTicks:
logAudioPacketArrivalStartedOrChangedWithTimeInTicks:
logESStartWithTimeInTicks:
logEventWithType:context:
logEvents:
logFinalAudioPacketContainingSpeechReceivedWithTimeInTicks:loggableSharedUserId:
logFinalResultGeneratedWithEARPackage:
logFinalResultGeneratedWithEARPackage:timeInTicks:
logFirstAudioPacketContainingSpeechReceivedWithTimeInTicks:loggableSharedUserId:
logFirstAudioPacketProcessed
logFirstAudioPacketProcessedWithTimeInTicks:
logFrameProcessingReady
logFrameProcessingReadyWithTimeInTicks:
logInitializationEnded
logInitializationEndedWithTimeInTicks:
logInitializationStartedOrChangedWithTimeInTicks:
logInitializationStartedOrChangedWithTimeInTicks:cachedRecognizerExisted:newRecognizerCreated:
logIntermediateUtteranceInfoTier1WithPmInput:pmOutput:
logIntermediateUtteranceInfoTier1WithPmInput:pmOutput:unrepairedPostItn:loggableSharedUserId:
logIntermediateUtteranceInfoTier1WithUnrepairedPostItn:loggableSharedUserId:
logJitLmeEndedAndEndedTier1WithDialogContext:
logJitLmeEndedAndEndedTier1WithDialogContext:timeInTicks:
logJitLmeStartedOrChangedWithTimeInTicks:
logLeadingSilenceProcessedWithTimestampInTicks:
logLocalRecognitionLoadedForLanguage:duration:
logPackageGeneratedAndRecognitionResultTier1WithEARPackage:loggableSharedUserId:
logPackageGeneratedAndRecognitionResultTier1WithEARPackage:loggableSharedUserId:timeInTicks:
logPartialResultGenerated
logPartialResultGenerated:ofSize:timeInTicks:
logPauseRecognitionWithTimeInTicks:
logPendingANEModelInitializationContextEvents:
logPostSpeechStartOneSecondAudioProcessedWithTimestampInTicks:
logPostSpeechStartOneSecondLaterAudioPacketReceivedWithTimeInTicks:loggableSharedUserId:
logPowerSnapshotForProcessStarted
logPrefix
logRequestEndedOrFailedOrCancelledWithError:recognizerComponents:averageActiveTokensPerFrame:languageModelInterpolationWeights:signalToNoiseRatioInDecibels:recognitionDurationInSec:audioDurationMs:eagerUsed:utteranceDetectionEnabled:utteranceConcatenationEnabled:cpuRealTimeFactor:numLmeDataStreams:phoneticMatchDecoderName:pauseDurations:itnDurationInNs:isEmojiPersonalizationUsed:isEmojiDisambiguationUsed:isEmojiExpectedButNotRecognized:recognizedEmojis:allItnRunIntervals:allSecondaryPassIntervals:
logRequestEndedOrFailedOrCancelledWithError:recognizerComponents:averageActiveTokensPerFrame:languageModelInterpolationWeights:signalToNoiseRatioInDecibels:recognitionDurationInSec:audioDurationMs:eagerUsed:utteranceDetectionEnabled:utteranceConcatenationEnabled:cpuRealTimeFactor:numLmeDataStreams:phoneticMatchDecoderName:pauseDurations:itnDurationInNs:isEmojiPersonalizationUsed:isEmojiDisambiguationUsed:isEmojiExpectedButNotRecognized:recognizedEmojis:totalITNDurationInNs:totalITNRuns:totalSecondaryPassDurationInNs:totalSecondaryPasses:cpuInstructionsInMillionsPerSecond:
logRequestLinkWithRequestId:
logRequestStartedOrChangedWithTask:modelLocale:modelVersion:isHighQualityAsset:hammerVersion:geoLanguageModelRegion:geoLanguageModelLoaded:speechProfileAgeInSec:dictationUIInteractionId:portraitExperimentVariantName:
logResumeRecognitionWithTimeInTicks:
logSampledAudioFileStoredSuccessfully
logSampledAudioFileStoredWithError:customFailureReason:
logTotalAudioProcessedOfDuration:timeInTicks:
logWithEventContext:
logWithEventContext:asrIdentifier:
loggingContext
longLongValue
machAbsoluteTimeToMachContinuousTime:
mapContextOptionToString:
maximumRecognitionDuration
methodForSelector:
modelInfo
modelOverrideURL
modelQualityTypeStatusStringWithConfig:
modelRoot
modelType
modelVersion
moveItemAtPath:toPath:error:
mutableCopy
nBest
nBestVoiceCommandInterpretations
name
narrowband
nextObject
nnetVersion
numFrames
numTrainedFrames
numberOfDeletions
numberOfInsertions
numberOfSubstitutions
numberWithDouble:
numberWithFloat:
numberWithInteger:
numberWithUnsignedInt:
numberWithUnsignedInteger:
numberWithUnsignedLongLong:
objectAtIndex:
objectAtIndexedSubscript:
objectEnumerator
objectForKey:
objectForKeyedSubscript:
offlineDictationProfileOverridePath
orderedSetWithArray:
originalAudioFileURL
overrides
packetArrivalTimestampFromAudioTime:
path
pathComponents
pathWithComponents:
pauseDurations
pauseRecognition
performSelector:
performSelector:withObject:
performSelector:withObject:withObject:
personalizationRecipeForAssetConfig:modelOverridePath:
personalizedLmAgeInSec
personalizedLmWeight
pexNamedEntityNames
phoneSequence
postfixText
powerEventContext
powerSnapshot
preITNNBestVoiceCommandInterpretations
preITNRecognition
preITNTokens
preITNVoiceCommandInterpretations
predicateWithFormat:
prefixText
preheatContext
preheatSpeechRecognitionWithAssetConfig:preheatSource:modelOverrideURL:
preheatSpeechRecognitionWithLanguage:modelOverrideURL:
prepareHammerConfigFile:
prepareToExit
prependedAutoPunctuation
presence
processInfo
profileFilePathFromBasePath:language:userId:
promoteAssetsForAssetType:
promoteHammerConfigFile
pronunciationsForOrthography:
punctuationCharacterSet
purgeInstalledAssetForAssetType:language:regionId:error:
purgeInstalledAssetsExceptLanguages:assetType:completion:
purgeInstalledAssetsExceptLanguages:assetType:error:
purgeInstalledAssetsExceptLanguages:completion:
purgeUnusedGeoLMAssetsForLangauge:
raise:format:
range
rangeOfCharacterFromSet:
rangeOfString:
rawRecognition
readProfileAndUserDataWithLanguage:allowOverride:completion:
readTableFromURL:
readUserProfile:
readUserProfileWithPath:reuseProfile:
reason
recognition
recognitionEndTime
recognitionMetrics
recognitionPaused
recognitionResultsWithAudioData:userProfileData:language:task:samplingRate:extraLanguageModel:
recognitionStart
recognitionStatistics
recognitionUtteranceInfos
recognitionUtterenceStatistics
recognizedEmojis
recognizedText
recognizedTokens
recognizerSourceForTask:
recordWithLanguage:task:context:narrowband:farField:interactionIdentifier:asrSelfComponentIdentifier:pluginId:
recordWithLanguage:task:context:narrowband:farField:interactionIdentifier:asrSelfComponentIdentifier:pluginId:frequency:
redecodeWithAudioDatas:language:task:samplingRate:completion:
registerFeedback:completion:
registerNotifications
regularExpressionWithPattern:options:error:
releaseClients
remoteObjectProxy
removeAllObjects
removeAllWords
removeItemAtURL:error:
removeLmeDataForTemplateName:
removeObjectAtIndex:
removeObjectForKey:
removePersonalizedLMForFidesOnly:completion:
removeSpaceAfter
removeSpaceBefore
replaceCorruptAssetWithConfig:
requestIdentifier
resetCacheAndCompileAllAssetsWithCompletion:
respondsToSelector:
resultCandidateId
resume
resumeRecognitionWithLeftContext:rightContext:selectedText:
resumeRecognitionWithPrefixText:postfixText:selectedText:
retain
retainCount
reverseObjectEnumerator
runAdaptationRecipeEvaluation:recordData:attachments:completion:
runCorrectedTextEvaluationWithAudioDatas:recordDatas:language:samplingRate:caseSensitive:skipLME:wordSenseAccessListSet:completion:
runDefaultAdaptationEvaluation:recordData:attachments:completion:
runEvaluationWithDESRecordDatas:language:recipe:attachments:fidesPersonalizedLMPath:fidesPersonalizedLMTrainingAsset:scrubResult:completion:
runRecipeEvaluationDistributedEvaluation:recordData:attachments:completion:
runRecognitionWithResultStream:speakerCodeWriter:language:task:samplingRate:
sampledCachesSubDirectoryPath
samplingDateAsString
samplingRate
samplingRateFilter
save
saveOneRecordPerDay
score
searchEventValues
secondsToHostTime:
secureOfflineOnly
selectedAlternatives
selectedText
self
selfHelper
selfLogger
sendEvent
sendSpeechCorrectionInfo:interactionIdentifier:
setActiveConfigUpdateContext:
setActiveConfiguration:
setAllowUtteranceDelay:
setAlternatives:
setAneContextFilter:
setAppendSpaceAfter:
setAppleNeuralEngineCompilationContext:
setAppleNeuralEngineModelInitializationContext:
setApplicationName:
setAsrId:
setAssetLoadContext:
setAssetsProvisionalForAssetType:
setAssetsPurgeabilityExceptLanguages:assetType:
setAudioDurationInNs:
setAudioMetadata:
setAudioPacketArrivalContext:
setAudioPackets:
setAudioSpeechPacketArrivalContext:
setAudioSpeechPacketFirstSecondAfterFirstSpeechPacketArrived:
setBluetoothDeviceIdFilter:
setByAddingObject:
setByAddingObjectsFromSet:
setCancelled:
setClass:forClassName:
setClientIdentifier:
setCodec:
setCollectedAudioDurationMS:
setComponent:
setConcatenateUtterances:
setConfidence:
setConfidenceScore:
setContinuousListeningEnabled:
setCorrectPartialResultIndexLists:
setCorrectedText:
setCpuContextFilter:
setCpuInstructionsInMillionsPerSecond:
setCpuRealTimeFactor:
setCurrentAudioFilePath:
setDatapackVersion:
setDecodable:
setDecoder:
setDelegate:
setDetectUtterances:
setDeviceIdFilter:
setDialogContexts:
setDictationUiInteractionId:
setDisableAutoPunctuation:
setESSelfHelper:
setEagerEnabled:
setEmbeddedSpeechProcessContext:
setEmojiMetrics:
setEnableSpeakerCodeTraining:
setEnableVoiceCommands:
setEndTime:
setEnded:
setEndpointStart:
setErrorCode:
setEventMetadata:
setExists:
setExportedInterface:
setExportedObject:
setFailed:
setFarField:
setFarFieldFilter:
setFileName:
setFinalAudioPacketContainingSpeechReceived:
setFinalResultGenerated:
setFirstAudioPacketProcessed:
setFirstSecondAfterLeadingSilenceProcessed:
setFormatAcrossUtterances:
setFrameProcessingReady:
setFrontend:
setGeoLanguageModelLoaded:
setGeoLanguageModelRegion:
setGpuContextFilter:
setHammerVersion:
setHasRecognizedAnything:
setHighPriority:
setInitializationContext:
setInteractionId:
setIntermediateUtteranceInfoTier1:
setInterpretationIndices:
setInterpretations:
setInterruptionHandler:
setInvalidationHandler:
setInverseTextNormalizationDurationForFinalResultInNs:
setInverseTextNormalizationDurationInNs:
setIpaPhoneSequence:
setIsAfterResume:
setIsAutoPunctuation:
setIsBoosted:
setIsEmojiDisambiguationUsed:
setIsEmojiExpectedButNotRecognized:
setIsEmojiPersonalizationUsed:
setIsFinal:
setIsHighQualityAsset:
setIsLowConfidence:
setJitLanguageModelEnrollmentEndedTier1:
setJitProfileData:
setLanguage:
setLanguageModelEnrollmentContext:
setLanguageModelInterpolationWeights:
setLeadingSilenceProcessed:
setLeftContextText:
setLinkId:
setLinkIndex:
setLoadDate:
setLogPrefix:
setMaximumRecognitionDuration:
setMetrics:
setModelRoot:
setModelType:
setNumLanguageModelEnrollmentDataStreams:
setNumberOfDeletions:
setNumberOfInsertions:
setNumberOfInverseTextNormalizationRuns:
setNumberOfSubstitutions:
setObject:atIndexedSubscript:
setObject:forKey:
setObject:forKeyedSubscript:
setOriginalAudioFileURL:
setPackage:
setPackageGenerated:
setPartialResultGenerated:
setPausedAudioDurationsInNs:
setPersonalizedLanguageModelAgeInNs:
setPersonalizedLanguageModelWeight:
setPersonalizedLmAgeInSec:
setPersonalizedLmWeight:
setPhoneSequence:
setPhoneticMatchDecoderName:
setPhoneticMatchInput:
setPhoneticMatchOutput:
setPhrases:
setPortraitExperimentVariantName:
setPostITNRecognizedText:
setPostItn:
setPreheatContext:
setProfile:
setProfileConfigWithLanguage:profileDir:userId:dataProtectionClass:completion:
setPunctuationText:
setRawRecognition:
setReason:
setRecognitionConfidenceSubtraction:
setRecognitionDurationInNs:
setRecognitionEndTime:
setRecognitionReplacements:
setRecognitionResultTier1:
setRecognizeEagerCandidates:
setRecognizeEmoji:
setRecognizedEmojis:
setRecognizedText:
setRecognizedTokens:
setRecognizerComponents:
setRemoteObjectInterface:
setRemoveSpaceAfter:
setRemoveSpaceBefore:
setRequestContext:
setResultCandidateId:
setRightContext:
setSampledAudioFileStorageFailed:
setSampledAudioFileStored:
setSampledAudioStorageFailureReason:
setSamplingRate:
setSamplingRateFilter:
setScoreNbest:
setScoreNbestExtraLmList:
setSelectedAlternatives:
setSelectedText:
setSelfLogger:
setSignalToNoiseRatioInDecibels:
setSilenceStartTime:
setSilenceStartTimeInNs:
setSource:
setSpeechProfileAgeInNs:
setStartTime:
setStartTimeInNs:
setStartedOrChanged:
setStatus:
setTarget:
setTask:
setTaskName:
setTaskTypeFilter:
setTemplateToVersion:
setText:
setTimeZone:
setTimestampInTicks:
setTokenSilenceStartTimeInNsLists:
setTokens:
setTotalCost:
setUUIDString:
setUnderlyingErrorCode:
setUnderlyingErrorDomain:
setUnrepairedPostItn:
setUserId:
setUserProfileData:
setUtteranceConcatenationEnabled:
setUtteranceDetectionEnabled:
setUtterances:
setUuid:
setValue:forKey:
setWeights:
setWithArray:
setWithObject:
sharedInstance
sharedManager
sharedPreferences
sharedProfiler
sharedStream
shouldHandleCapitalization
shouldStoreAudioOnDevice
shouldWriteDictationRecord:
signalEndOfUserData
signpostHelper
silenceStart
silenceStartTime
siriDataSharingOptInStatus
siriDataSharingOptedIn
spatialLocationOfInterestNames
speechProfileDateLastModifiedForLanguage:
speechProfileRootDirectories
speechRecognitionFeatures
speechRecognizer:didFinishRecognitionWithError:
speechRecognizer:didProcessAudioDuration:
speechRecognizer:didProduceEndpointFeaturesWithWordCount:trailingSilenceDuration:eosLikelihood:pauseCounts:silencePosterior:processedAudioDurationInMilliseconds:
speechRecognizer:didProduceLoggablePackage:
speechRecognizer:didRecognizeFinalResultCandidatePackage:
speechRecognizer:didRecognizeFinalResultPackage:
speechRecognizer:didRecognizeFinalResults:
speechRecognizer:didRecognizeFinalResults:tokenSausage:nBestChoices:
speechRecognizer:didRecognizePartialResult:
speechRecognizer:didRecognizePartialResultNbest:
speechRecognizer:didRecognizeRawEagerRecognitionCandidate:
speechRecognizer:didReportStatus:statusContext:
speechServiceDidFinishRecognitionWithStatistics:error:
speechServiceDidProcessAudioDuration:
speechServiceDidProduceEndpointFeaturesWithWordCount:trailingSilenceDuration:eosLikelihood:pauseCounts:silencePosterior:processedAudioDurationInMilliseconds:
speechServiceDidProduceLoggablePackage:
speechServiceDidRecognizeFinalResultCandidatePackage:
speechServiceDidRecognizePackage:
speechServiceDidRecognizePackage:withMetadata:
speechServiceDidRecognizeRawEagerRecognitionCandidate:
speechServiceDidRecognizeTokens:
speechServiceDidRecognizeTokens:withMetadata:
speechServiceDidSelectRecognitionModelWithModelProperties:
standardUserDefaults
start
startMissingAssetDownload
startReading
startRequestActivityWithCompletion:
startSpeechRecognitionWithParameters:didStartHandler:
startSpeechRecognitionWithParameters:didStartHandlerWithInfo:
startTime
startedOrChanged
status
storeSignpostId:forEventName:
string
stringByAppendingFormat:
stringByAppendingString:
stringByDeletingLastPathComponent
stringByDeletingPathExtension
stringByReplacingMatchesInString:options:range:withTemplate:
stringByReplacingOccurrencesOfString:withString:
stringByTrimmingCharactersInSet:
stringValue
stringWithContentsOfURL:encoding:error:
stringWithFormat:
stringWithUTF8String:
subdataWithRange:
substringFromIndex:
substringToIndex:
substringWithRange:
superclass
supportedCategories
supportedLanguagesWithAssetType:
switchToNewAssetsForAssetType:
systemUptime
task
taskName
taskTypeFilter
tasks
templateName
templateToVersion
text
threshold
timeIntervalSince1970
timeIntervalSinceNow
timeIntervalSinceReferenceDate
timeZoneWithAbbreviation:
timestamp
timestampInTicks
timestampInTicksForAudioProcessed:
tokenName
tokenSausage
tokenize:
tokens
totalCost
tracksWithMediaType:
trainingNnetVersion
unrepairedPostItn
unrepairedRecognition
unsignedIntValue
unsignedIntegerValue
unsignedLongLongValue
unsignedLongValue
updateSpeechProfileWithLanguage:modelOverridePath:existingProfile:existingAssetPath:completion:
usage_start
userData
userInfo
userProfileWithAssetConfig:modelOverridePath:overrides:isJit:returningFoundPath:error:
utteranceStart
utterances
valueForEntitlement:
valueForKey:
valueForKeyPath:
variantNameWithError:
voiceCommandInterpretations
voiceCommandsParamKeyBuilder:
weakObjectsHashTable
whitespaceAndNewlineCharacterSet
whitespaceCharacterSet
wrapAndEmitTopLevelEvent:timestampInTicks:
writeDESRecord
writeProfileToFile:protectionClass:length:error:
writeToFile:atomically:
writeToFile:options:error:
writeToURL:atomically:encoding:error:
zone
%s SELF: Logging object created successfully (logging allowed for current request). asrId=%@, recognitionTask=%@, isSpeechAPIRequest=%@, isHipaaCompliant=%@, siriOptInStatus=%@, isTier1LoggingAllowed=%@
%s SELF: Logging disabled because it is not allowed for the current request. recognitionTask=%@, isSpeechAPIRequest=%@
%s SELF: Correct Partial Result Index List is %s, Silence Start Time List is %s
%s SELF: Encountered malformed string during SELF logging for recognizer components in speech results from recognizer. String: (%@)
%s SELF: Expected three recognizer components separated by delimiter '::'. Ex: 'dnn-rfdnn-aa-cache::dnn-lazy-16k-rfdnn-dictation::msg'
%s SELF: Encountered malformed string during SELF logging for interpolation weights in speech results from recognizer. String: %@
%s SELF: Expected interpolation weight sets separated by delimiter ';' - starting with a set of weights delimited by ',' and ending the with start/end times delimited by ':'. Ex: '0.999646,0.000354:0:4280;0.947514,0.000158:0:3859'
%s SELF: Logging ASRRequestContext->failed in SELF based on error result from recognizer.
%s SELF: Logging ASRRequestContext->cancelled with reason RECOGNITION_CANCELLED in SELF based on error result from recognizer.
%s SELF: Logging ASRRequestContext->cancelled with reason RECOGNITION_REJECTED in SELF based on error result from recognizer.
%s SELF: Logging ASRRequestContext->cancelled with reason RECOGNITION_REJECTED in SELF because nothing was recognized (SpeechNoMatch).
%s SELF: Logging ASRRequestContext->ended in SELF based on success status from recognizer.
%s SELF: Failed trying to wrap and emit top-level ASR event because event type was not mapped to loggable message type in the ASR SELF schema.
%s SELF: Wrapping and logging an event of type %@
%s %@ cancelling instance %@
%s %@: %@
%s %@
%s %@.
%s Internal inconsistency error: KVItems list and corresponding isBoosted booleans list are out of sync. This batch of items will default to not being boosted.
%s Failed to deserialize a KVItem. It will be ignored.
%s Starting to process KVItems into CESRUserData.
%s Created CESRUserData from KVItems.
%s Adapted profile using CESRUserData according to personalization recipe.
%s Speech profile updated successfully. Wrote %lu bytes to %@
%s Created _EARProfileBuilder from asset config.
%s Existing profile found at %@. It will be reused when updating the profile.
%s Could not locate asset: %@
%s Could not read personalization.json: %@
%s Could not parse personalization.json: %@
%s Set model root to %@
%s Use currently installed asset.
%s Recipe has invalid json for "%{public}@"
%s Recipe has invalid tagName for "%{public}@": "%{public}@"
%s Error executing recipe for domain %{public}@
%s Recipe for %{public}@ is missing "%{public}@"
%s Recipe for %{public}@ contains parameter %{public}@, expected type %{public}@ but got %{public}@
%s User Profile: Starting AddWordsToUserProfile
%s Using name frequencies adaption for names: %@ into slot %@ for template %@
%s Ignoring name part "%@" because it is too short (minimum length is %lu)
%s Using keyboardLMAdaptation adaption for names %@ into slot %@ for template %@
%s Ignoring keyboardDynamicVocabularyItem "%@" because it is too short (minimum length is %lu)
%s Using locationOfInterestNameAdaptationFrequency adaption for names %@ into slot %@ for template %@
%s Ignoring locationOfInterestName "%@" because it is too short (minimum length is %lu)
%s Using eventLocationNameAdaptationFrequency adaption for names %@ into slot %@ for template %@
%s Ignoring calendar event word "%@" because it is too short (minimum length is %lu)
%s Root directories for new type of speech profile: %{private}@
%s speechProfilePathsWithLanguage was incorrectly called with language=nil
%s Mapped language=nil
%s loadSpeechProfiles was incorrectly called with profiles=nil
%s Reused new type of speech profile: path=%{private}@
%s Loaded new type of speech profile: path=%{private}@ profile=%d
%s Count of command interpretation sets does not match count of speech recognition results
%s AFSpeechLatticeMitigatorResult Score = %f, Threshold = %f
%s AFSpeechLatticeMitigatorResult nil
%s Failed to register for assistant asset update notifications.
%s Installation status for languages (ignoring cache: %@)
%s Invalidating installation status cache for %lu
%s Language installation status query failed: %@
%s Siri and Dictation are both disabled, no need to access asset
%s Using ASR Trial assets at %@
%s No assets available for language: %@ asset type: %{public}@
%s Purging Trial assets failed: %@
%s Checking for missing assets.
%s Purging all assistant ASR assets except for %@
%s Purging all assistant ASR assets
%s Encountered error trying to purge unused assistant ASR assets: %@
%s Trial asset delivery disabled for assistant ASR assets. Bailing out of missing asset check.
%s Trial asset delivery is explicitly disabled!
%s Hammer model info=%@
%s Exception thrown while reading hammer config
%s GeoLM: region mapping json file=%@
%s GeoLM: region mapping json file is nil Or there is no regionMapping for given language=%@
%s GeoLM: Not Supported
%s GeoLM: region specific [%@] geo-config json file=%@
%s GeoLM: geoLM region specific [%@] asset exists on device, but not compatible.
%s GeoLM: Exception thrown while reading geo-config json
%s GeoLM: region specific asset is not found for given language=%@ regionId=%@
%s GeoLM: model-info.version doesn't match. mainASRModelInfo.version=%@ geoLMModelInfo.version=%@ mainAssetConfig=%@ geoAssetConfig=%@
%s GeoLM: Exception thrown while reading json configs
%s GeoLM: language is nil. Skipping.
%s GeoLM: regionIdToBePurged: %@, lastWhenUsed: %ld days ago
%s GeoLM: regionIdToBePurged: %@, _geoLMAssetsInfoDict count: %ld
%s GeoLM: supportedLanguages count:%ld
%s GeoLM: Going to delete: %@
%{public, signpost.description:begin_time}llu
%{public, signpost.description:end_time}llu Status=%s
ondevice_preheat_time
%s Unexpected pending preheat event count %lu
%{public, signpost.description:begin_time}llu FileName=%s
%{public, signpost.description:end_time}llu FileName=%s
ES: ANE Model Init
%s Unexpected pending ane model initialization event count %lu
%{public, signpost.description:begin_time}llu cachedRecognizerExisted = %u, newRecognizerCreated = %u
ES: Engine Init
%{public, signpost.description:end_time}llu
ES: Update Active Config
ES: JIT Profile Build Load
ES: AOT Profile Load
%{public, signpost.description:event_time}llu
ES: First Audio Packet
ES: Last Audio Packet
ES: First Audio Packet Process
ES: Last Speech Audio Packet
ES: Frame Processing Ready
%{public, signpost.description:event_time}lluWords=%s, WordCount=%u
ES: Partial Recognition
%{public, signpost.description:event_time}lluOneBest=%s, isFinal=%u, isAfterResume=%u, rcId=%ld
ES: Package Recognition
%{public, signpost.description:event_time}llu OneBest=%s, isFinal=%u, isAfterResume=%u, rcId=%ld
ES: Final Recognition
ES: Leading Silence Process
ES: First Speech Second Process
ES: ITN Process
ES: Secondary Pass Process
%{public, signpost.description:event_time}llu SNR = %f, AudioDurationMs = %llu, RecognitionDurationMs = %llu, CPU RTF = %f
ES: Engine Complete
ES: Pause Recognition
ES: Resume Recognition
%s ASR: Using high priority configuration.
%s %@ deallocating
%s Failing to fetch assets for nil language
%s Could not get offline language for fetch fallback: %{public}@
%s Fell back asset fetch from %{public}@ to %{public}@
%s Failed to fall back asset fetch from %{public}@ to %{public}@, got %{public}@
%s Could not get installed offline language statuses: %{public}@
%s ASR: enable parallel loading
%s ASR: filtering for tasks: %@
%s ASR: taskForMemoryLock: %@
%s Override json files=%@
%s Unable to locate or read dictation voice commands assets
%s Failed to create recognizer from %{public}@
%s EmbeddedSpeechMetric: Created recognizer in %lf sec from %@
%s %s
ondevice_CreateRecognizer
%{public, signpost.description:end_time}llu keepANEModelLoaded = %s, enableParallelLoading = %s, preloadModels = %u, preheatSource = %s, taskForMemoryLock = %s
ES: Recognizer Create
%s Preheat for %@
%s Preheat for %@ dismissed because recognition in progress
%s Skipping preheat for %@; recognizer already loaded
%s sRecognizerTracker: Too many recognizers active during preheat: %lu
%s Preheated for language %{public}@, source %{public}@, regionId %@%{public}@
%s Could not preheat for language %{public}@, source %{public}@, regionId %@%{public}@: %@
%s dictationCapable=%d task=%@ aneCapable=%d
%s Starting
%s Recognizer is busy
%s Previous recognizer on other XPC connection is busy. Send sync cancel
ES: Previous Request Cancellation
%s Cached recognizer for language %{public}@, regionId %@ already loaded
%s Cached recognizer is for language %{public}@, regionId %@, requesting recognizer for language %{public}@, asset type %{public}@, regionId %@
ES: Previous Recognizer Unload
%s sRecognizerTracker: Too many recognizers active: %lu
%s No cached recognizer.
%s EndpointStart > 0 but asr features delivery is disabled!
%s EndpointStart < 0
%s Setting new profile: %d, old profile: %d
%s Failed to get model root, error: %@
%s Fetching contextual data for task: %@
%s Injecting contextual data to recognizer
%s Built inline LME from contextual data, size: %zu
%s Failed to initialize jit profile builder due to error : %@
%s Set inputOrigin to: %@
%s Switching off UC/UD for this request
%s Changing active configuration from 
%@ to 
%s Injected %lu jit strings or contextual data to recognizer, length: %lu
%s Duration spent in jit processing on critical path = %{public}lfs
%s Create DES record
%s Cancelling delayedBlock
%s Create DES record for Dictation with interactionId=%@
%s Saving profile snapshot: %lu bytes
%s _storeAudioData should be nil. Critical Error. Please check.
%s Received correctedText, interactionId: %@, correctedText: %@
%s Sending dictation feedback to Portrait based on correction
%s Set correctedText for CESRFidesASRRecord, interactionId: %@, correctedText: %@
%s Set alternativeSelection for CESRFidesASRRecord, interactionId: %@, alternatives: %@
%s Interaction identifier did not match the DES record in memory
%s Using override profile at %@
%s Could not use override profile at %@: %@
%s Mismatch in speech profile language in content (%{public}@) and filename (%{public}@)
%s Profile version on disk (%{public}@) does not match the expected version (%{public}@)
%s Successfully deserialized existing speech profile for %@
%s Compilation timer fired.
%s Compilation scheduled for %zus.
%s Acquired os_transaction during cooldown start
%s On-Device ASR: Cooldown is disabled.
%s On-Device ASR: Cooldown scheduled for %zus.
%s Cooldown timer triggered TRIClient release
%s Received termination signal. Cleaning up immediately
%s Sending %lu events
%s Unable to load the contents of file %@: %@
%s Invalid file format
%s Running distributed evaluation for ASR
%s No attachments given, cannot run distributed evaluation
%s Failed to extract test set: %@
%s Cannot initialize recognizer for locale: %@ task: %@
%s Loaded speech profile
%s Unable to load speech profile
%s Test set contains more utterances than allowed, only running %d utterances
%s Unable to load audio file %@
%s Unable to find reference transcriptions for %@
%s Recipe has no profile
%s Stop adaption recipe. Audio file not readable. Voicemail has been deleted by user
%s Read %lu bytes from audio file
%s Using on device personalization recipe for baseline
%s Recognizer doesn't support the task %{public}@
%s Couldn't create create path for temporary confidence model overrides at %@
%s Couldn't write data to temporary confidence model file at path %@
%s Could not make baseline results
%s Failed to extract quasar model: %@
%s No recognizer created for custom model: %@
%s Could not make results with custom model
%s Profile overrides failed
%s Ignoring malformed overrides: %{public}@
%s Recipe has no recognizer
%s Could not make adapted results
%s Exception evaluating recipe: %@
%s Unknown exception evaluating recipe
%s Loaded speech profile: %lu bytes
%s Interrupted corrected text evaluation redecoding
%s No tokenizer for %@
%s Failed to load old type of speech profile. Trying new type.
%s Examining localSpeechDESRecord: %@
%s Unable to load localSpeechDESRecord
%s No audio data provided for UUID %@
%s Recognition result %@, %lu
%s Edit distance between tts ASR and original ASR %@
%s correctedOutput: %@, recognizedOutput %@
%s Unknown evaluation name found in alignmentReferences: %@
%s modelRoot: %@
%s No modelRoot for %@: %@
%s Unable to load audio
%s Recognizer doesn't support the task %{public}@: %@
%s Interrupted evaluation redecoding
%s Running recognition for evalName: %@
%s Failed to get override files, error: %@
%s Creating recognizer with overrides: %@
%s Using Personalized LM
%s Not using Personalized LM
%s Unable to restore speech profile
%s Using JIT LME
%s JIT LME: Injecting JIT data, jitStats: %@
%s JIT LME: Error fetching JIT data, error: %@
%s No results for evalName %@: %@
%s Tokenizing correctedText
%s Interrupted evaluation tokenization
%s Computing alignments
%s Failed to delete temporary directory: %@
%s Starting to compile assistant language: %@
%s Error when compiling assistant language model: %@
%s Finished compiling all assets.
%s Error when getting assistant language status: %@
%s No DES record, nothing to write
%s Not saving DES Record with no data or recognition
%s wordCount = %ld, trailingSilenceDuration = %ld, eosLikelihood = %f, pauseCounts = %@, silencePosterior = %f
%s Setting recognized text
%s %lu results
%s EmbeddedSpeechMetric: first audio packet to first partial result = %lf secs
%s AFSpeechInfo Package %@
%{public, signpost.description:begin_time}llu %s
ES: Time to Word
Partially Recognized Results: %@
ES: TTAW
%s Recognition finished with status %@
%s Audio finish to recognizer finish = %lf sec, connection is %@, error %@
%s _recognitionBeginTime (%@) is greater than _recognitionEndTime (%@)
%s PauseDuration = %@
%s Local speech recognition completed without error, write DES record when needed
%s Writing DES record after 30 seconds delay: interactionId=%@
%s Submitted delayedBlock to dispatch_after
%s didProcessAudioDuration %f
%s #ASR on device eager formatted recognition candidate: %@
%s raw eager recognition candidate: %@
%s Could not make temporary attachment directory at %@: %@
%s Failed to specify compression algorithm: %s
%s Failed to specify format: %s
%s Start extracting archive at path: %s
%s Failed to open archive for reading: %s
%s Entry extraction path: %@
%s Unable to extract file to: %@
%s Finished extracting archive to: %@
%s Could not read %@: %@
%s Could not parse %@: %@
%s %@ is wrong type: %@
%s %@ contains bogus key/value pair: %@ => %@
%s VoiceMail asset could not be read: %@
%s Could not create asset reader output
%s Cannot add output
%s Could not get data pointer: %d
%s JSON serialization failed: %@
%s Compression failed: %@
%s Sampling: Error while initializing ESStoreAudioData because uuid is invalid.
%s %@ Sampling: Won't save audio because - has not recognized anything or has no data.
%s %@ Sampling: Won't save audio because - _currentAudioFilePath is null
%s %@ Sampling: invalid filePath or it is null.
%s %@ Sampling: Done with cleanup of audioFile=%@ and reset of variables.
%s %@ Sampling: Failed to save audio to cache dir. Error: %@
%s %@ Sampling: Successfully saved audio file to cache dir, path=%@
%s %@ Sampling: audioFileToBeMoved is nil
%s %@ Sampling: currentSamplingDate is nil
%s %@ Sampling: Error while creating Sampled directory in /var/mobile
%s %@ Sampling: Error while creating dated Sampled directory in %@ with date - %@
%s %@ Sampling: Error while moving file from cache directory to var/mobile/Library - %@
%s %@ Sampling: Successfully moved audio file to var/mobile/Library dir, path=%@
%s %@ Sampling: Error while writing audio metadata dict to plist - %@
%s %@ Sampling: currentSamplingDateString is null
%s Received SIGTERM. Cleaning and Exiting
ESSelfHelper
Timestamp
ESSelfPreheatWithPowerContainer
ESSpeechProfileBuilderConnection
CESRSpeechProfileBuilderService
NSObject
ESBiomeRecord
ESBiomeAsrRecord
CESRUtilitiesAdditions
CESRUtilities
ResultCandidateId
ESAssetManager
ESProfiler
ESAlignmentState
NSCopying
ESConnectionModelInfo
ESConnection
_EARSpeechRecognitionResultStream
CESRSpeechService
ESStoreAudioData
ESListenerDelegate
NSXPCListenerDelegate
@16@0:8
@20@0:8B16
@32@0:8@16@24
v16@0:8
@28@0:8@16B24
B28@0:8@16B24
B24@0:8@16
v24@0:8@16
v32@0:8@16@24
v92@0:8@16@24@32@40@48@56B64@68@76@84
v48@0:8@16@24@32@40
v188@0:8@16@24@32@40@48@56@64@72B80B84@88@96@104@112@120B128B132B136@140@148@156@164@172@180
v32@0:8@16q24
B16@0:8
v20@0:8B16
@"NSUUID"
@"NSString"
@"NSNumber"
@40@0:8@16@24@32
@"ASRSchemaASRPreheatContext"
@"SPIPowerLoggerSnapshot"
@"SPIEventContext"
@60@0:8@16@24@32B40^@44^@52
v44@0:8@16@24@32B40
v40@0:8@16@24@32
@56@0:8@16#24@32@40^@48
B88@0:8@16d24@32@40@48@56Q64@72@80
B80@0:8d16@24@32@40@48Q56@64@72
#16@0:8
@24@0:8:16
@32@0:8:16@24
@40@0:8:16@24@32
B24@0:8#16
B24@0:8:16
Vv16@0:8
Q16@0:8
^{_NSZone=}16@0:8
B24@0:8@"Protocol"16
@"NSString"16@0:8
Vv56@0:8@16@24@32q40@?48
Vv32@0:8@16@?24
Vv40@0:8@16@24@?32
Vv24@0:8@?16
Vv28@0:8B16@?20
Vv56@0:8@"NSString"16@"NSString"24@"NSString"32q40@?<v@?B@"NSError">48
Vv32@0:8@"NSString"16@?<v@?q@"NSError">24
Vv40@0:8@"NSDictionary"16@"NSString"24@?<v@?B@"NSError">32
Vv40@0:8@"NSArray"16@"NSArray"24@?<v@?B@"NSError">32
Vv24@0:8@?<v@?B@"NSError">16
Vv28@0:8B16@?<v@?B@"NSError">20
@24@0:8@16
@24@0:8^@16
@"NSMutableArray"
@"NSMutableDictionary"
@"NSMutableSet"
@"CESRAssetConfig"
@"NSXPCConnection"
@"NSObject<OS_os_transaction>"
@"_EARUserProfile"
d16@0:8
v24@0:8d16
@56@0:8@16@24@32@40Q48
@"NSArray"
Q20@0:8f16
f24@0:8Q16
Q24@0:8Q16
q24@0:8@16
@32@0:8@16d24
@36@0:8@16d24B32
@24@0:8Q16
@32@0:8@16Q24
Q32@0:8d16Q24
d32@0:8@16Q24
B24@0:8^{__CFString=}16
@40@0:8B16Q20B28^@32
v24@0:8Q16
@32@0:8@16^@24
@36@0:8@16^@24B32
@40@0:8@16^@24B32B36
v32@0:8@16Q24
B40@0:8@16Q24^@32
B32@0:8@16@24
@"NSObject<OS_dispatch_queue>"
@24@0:8d16
v32@0:8@16B24B28
v164@0:8@16@24@32@40@48@56@64@72B80B84@88@96@104@112@120B128B132B136@140@148@156
v32@0:8d16@24
{rusage_info_v6="ri_uuid"[16C]"ri_user_time"Q"ri_system_time"Q"ri_pkg_idle_wkups"Q"ri_interrupt_wkups"Q"ri_pageins"Q"ri_wired_size"Q"ri_resident_size"Q"ri_phys_footprint"Q"ri_proc_start_abstime"Q"ri_proc_exit_abstime"Q"ri_child_user_time"Q"ri_child_system_time"Q"ri_child_pkg_idle_wkups"Q"ri_child_interrupt_wkups"Q"ri_child_pageins"Q"ri_child_elapsed_abstime"Q"ri_diskio_bytesread"Q"ri_diskio_byteswritten"Q"ri_cpu_time_qos_default"Q"ri_cpu_time_qos_maintenance"Q"ri_cpu_time_qos_background"Q"ri_cpu_time_qos_utility"Q"ri_cpu_time_qos_legacy"Q"ri_cpu_time_qos_user_initiated"Q"ri_cpu_time_qos_user_interactive"Q"ri_billed_system_time"Q"ri_serviced_system_time"Q"ri_logical_writes"Q"ri_lifetime_max_phys_footprint"Q"ri_instructions"Q"ri_cycles"Q"ri_billed_energy"Q"ri_serviced_energy"Q"ri_interval_max_phys_footprint"Q"ri_runnable_time"Q"ri_flags"Q"ri_user_ptime"Q"ri_system_ptime"Q"ri_pinstructions"Q"ri_pcycles"Q"ri_energy_nj"Q"ri_penergy_nj"Q"ri_reserved"[14Q]}
@"ESSelfHelper"
@"CESRSignpostHelper"
@24@0:8^{_NSZone=}16
q16@0:8
v24@0:8q16
@40@0:8@16B24B28^@32
@88@0:8@16@24B32@36@44B52@56B64B68^@72^@80
v32@0:8@16d24
v72@0:8@16q24q32d40@48d56q64
v40@0:8@16Q24@32
v32@0:8@"_EARSpeechRecognizer"16@"_EARSpeechRecognitionResult"24
v32@0:8@"_EARSpeechRecognizer"16@"NSError"24
v32@0:8@"_EARSpeechRecognizer"16@"NSArray"24
v48@0:8@"_EARSpeechRecognizer"16@"NSArray"24@"NSArray"32@"NSArray"40
v32@0:8@"_EARSpeechRecognizer"16@"_EARSpeechRecognitionResultPackage"24
v32@0:8@"_EARSpeechRecognizer"16d24
v32@0:8@"_EARSpeechRecognizer"16@"_EARSpeechRecognition"24
v72@0:8@"_EARSpeechRecognizer"16q24q32d40@"NSArray"48d56q64
v40@0:8@"_EARSpeechRecognizer"16Q24@"NSDictionary"32
Vv32@0:8@16@24
Vv40@0:8@16@24@32
Vv24@0:8@16
Vv48@0:8@16@24@32@?40
Vv56@0:8@16@24@32@40@?48
Vv36@0:8B16Q20@?28
Vv40@0:8B16Q20B28@?32
Vv72@0:8@16@24@32Q40B48B52@56@?64
v56@0:8@16@24@32Q40@?48
Vv36@0:8@16B24@?28
Vv40@0:8@16Q24@?32
Vv32@0:8@16Q24
Vv76@0:8@16@24@32@40@48@56B64@?68
Vv24@0:8@?<v@?@"NSError">16
Vv32@0:8@"NSString"16@"NSURL"24
Vv40@0:8@"CESRAssetConfig"16@"NSString"24@"NSURL"32
Vv24@0:8@?<v@?>16
Vv32@0:8@"CESRSpeechParameters"16@?<v@?@"CESRModelProperties"@"NSError">24
Vv32@0:8@"CESRSpeechParameters"16@?<v@?@"NSString"@"NSString"@"NSError">24
Vv24@0:8@"NSData"16
Vv48@0:8@"NSString"16@"NSString"24@"NSData"32@?<v@?@"NSData"@"NSError">40
Vv56@0:8@"NSString"16@"NSString"24@"NSData"32@"NSString"40@?<v@?@"NSData"@"NSString"@"NSError">48
Vv36@0:8B16Q20@?<v@?@"NSDictionary"@"NSError">28
Vv40@0:8B16Q20B28@?<v@?@"NSDictionary"@"NSError">32
Vv28@0:8B16@?<v@?@"NSDictionary"@"NSError">20
Vv32@0:8@"NSString"16@?<v@?@"NSString"@"NSError">24
Vv32@0:8@"CESRAssetConfig"16@?<v@?@"NSString"@"NSError">24
Vv32@0:8@"CESRAssetConfig"16@?<v@?@"CESRModelProperties"@"NSError">24
Vv48@0:8@"NSDictionary"16@"NSData"24@"NSArray"32@?<v@?@"NSDictionary"@"NSData"@"NSError">40
Vv72@0:8@"NSDictionary"16@"NSDictionary"24@"NSString"32Q40B48B52@"NSSet"56@?<v@?@"NSDictionary"@"NSError">64
v56@0:8@"NSDictionary"16@"NSString"24@"NSString"32Q40@?<v@?@"NSDictionary"@"NSError">48
Vv36@0:8@"NSString"16B24@?<v@?@"NSData"@"NSString">28
Vv32@0:8@"NSSet"16@?<v@?@"NSNumber"@"NSError">24
Vv40@0:8@"NSSet"16Q24@?<v@?B@"NSError">32
Vv32@0:8@"NSSet"16Q24
Vv32@0:8@"AFSpeechCorrectionInfo"16@"NSString"24
Vv28@0:8B16@?<v@?>20
Vv76@0:8@"NSDictionary"16@"NSString"24@"NSDictionary"32@"NSArray"40@"NSString"48@"NSString"56B64@?<v@?@"NSDictionary"@"NSError">68
Vv40@0:8@"NSString"16@"NSString"24@"NSString"32
v36@0:8@16B24@?28
v40@0:8@16d24@32
v24@0:8@?16
v48@0:8@16@24@32@?40
Vv56@0:8@16@24@32Q40@?48
@48@0:8@16@24@32^@40
v24@0:8^@16
v28@0:8^@16B24
@"_EARSpeechRecognizer"
@"_EARSpeechRecognitionAudioBuffer"
@"AFSpeechPackage"
@"AFSpeechInfoPackage"
@"NSSet"
@"ESStoreAudioData"
@"ESBiomeRecord"
@"_EARSpeakerCodeWriter"
@"CESRFidesASRRecord"
@"NSObject<OS_dispatch_source>"
@96@0:8@16@24@32@40Q48@56@64@72B80B84@88
v40@0:8q16@24q32
@"NSMutableData"
B32@0:8@"NSXPCListener"16@"NSXPCConnection"24
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
<key>application-identifier</key>
<string>com.apple.siri.embeddedspeech</string>
<key>com.apple.CoreRoutine.LocationOfInterest</key>
<true/>
<key>com.apple.accounts.appleaccount.fullaccess</key>
<true/>
<key>com.apple.application-identifier</key>
<string>com.apple.siri.embeddedspeech</string>
<key>com.apple.coreaudio.allow-amr-decode</key>
<true/>
<key>com.apple.coreduetd.allow</key>
<true/>
<key>com.apple.coreduetd.people</key>
<true/>
<key>com.apple.developer.homekit</key>
<true/>
<key>com.apple.frontboardservices.display-layout-monitor</key>
<true/>
<key>com.apple.locationd.effective_bundle</key>
<true/>
<key>com.apple.private.DistributedEvaluation.RecordAccess-com.apple.fides.asr</key>
<true/>
<key>com.apple.private.DistributedEvaluation.RecordAccess-com.apple.siri.speech-dictation-personalization</key>
<true/>
<key>com.apple.private.assets.accessible-asset-types</key>
<array>
<string>com.apple.MobileAsset.Trial.Siri.SiriUnderstandingAsrAssistant</string>
<string>com.apple.MobileAsset.Trial.Siri.SiriUnderstandingAsrHammer</string>
<string>com.apple.MobileAsset.Trial.Siri.SiriDictationAssets</string>
<string>com.apple.MobileAsset.EmbeddedSpeech</string>
<string>com.apple.MobileAsset.EmbeddedSpeechWatch</string>
<string>com.apple.MobileAsset.EmbeddedSpeechMac</string>
</array>
<key>com.apple.private.assets.bypass-asset-types-check</key>
<true/>
<key>com.apple.private.attribution.implicitly-assumed-identity</key>
<dict>
<key>type</key>
<string>path</string>
<key>value</key>
<string>/System/Library/PrivateFrameworks/CoreEmbeddedSpeechRecognition.framework/XPCServices/com.apple.siri.embeddedspeech.xpc/com.apple.siri.embeddedspeech</string>
</dict>
<key>com.apple.private.biome.read-write</key>
<array>
<string>SiriDictation</string>
<string>Dictation.UserEdit</string>
</array>
<key>com.apple.private.calendar.allow-suggestions</key>
<true/>
<key>com.apple.private.contacts</key>
<true/>
<key>com.apple.private.corerecents</key>
<true/>
<key>com.apple.private.corespotlight.internal</key>
<true/>
<key>com.apple.private.homekit</key>
<true/>
<key>com.apple.private.security.storage.SiriVocabulary</key>
<true/>
<key>com.apple.private.security.storage.SpeechPersonalizedLM</key>
<true/>
<key>com.apple.private.tcc.allow</key>
<array>
<string>kTCCServiceAddressBook</string>
<string>kTCCServiceCalendar</string>
<string>kTCCServiceWillow</string>
<string>kTCCServiceMediaLibrary</string>
</array>
<key>com.apple.proactive.PersonalizationPortrait.Config</key>
<true/>
<key>com.apple.proactive.PersonalizationPortrait.NamedEntity.readOnly</key>
<true/>
<key>com.apple.proactive.eventtracker</key>
<true/>
<key>com.apple.security.exception.files.absolute-path.read-only</key>
<array>
<string>/private/var/MobileAsset/</string>
<string>/private/var/tmp/com.apple.siri-distributed-evaluation/</string>
</array>
<key>com.apple.security.exception.mach-lookup.global-name</key>
<array>
<string>com.apple.suggestd.contacts</string>
<string>com.apple.mobileasset.autoasset</string>
</array>
<key>com.apple.security.exception.shared-preference.read-only</key>
<array>
<string>com.apple.assistant</string>
<string>com.apple.assistant.backedup</string>
<string>com.apple.assistant.support</string>
</array>
<key>com.apple.security.iokit-user-client-class</key>
<array>
<string>AGXCommandQueue</string>
<string>AGXDevice</string>
<string>AGXDeviceUserClient</string>
<string>AGXSharedUserClient</string>
<string>H11ANEInDirectPathClient</string>
<string>IOAccelContext</string>
<string>IOAccelContext2</string>
<string>IOAccelDevice</string>
<string>IOAccelDevice2</string>
<string>IOAccelSharedUserClient</string>
<string>IOAccelSharedUserClient2</string>
<string>IOAccelSubmitter2</string>
<string>IOSurfaceRootUserClient</string>
</array>
<key>com.apple.security.personal-information.addressbook</key>
<true/>
<key>com.apple.security.temporary-exception.mach-lookup.global-name</key>
<array>
<string>com.apple.triald.namespace-management</string>
</array>
<key>com.apple.siriknowledged</key>
<true/>
<key>com.apple.spotlight.search</key>
<true/>
<key>com.apple.trial.client</key>
<array>
<string>372</string>
<string>401</string>
<string>751</string>
</array>
</dict>
</plist>
application-identifier
com.apple.siri.embeddedspeech0-(com.apple.CoreRoutine.LocationOfInterest
0/*com.apple.accounts.appleaccount.fullaccess
0A com.apple.application-identifier
com.apple.siri.embeddedspeech0)$com.apple.coreaudio.allow-amr-decode
com.apple.coreduetd.allow
com.apple.coreduetd.people
com.apple.developer.homekit
083com.apple.frontboardservices.display-layout-monitor
0)$com.apple.locationd.effective_bundle
0MHcom.apple.private.DistributedEvaluation.RecordAccess-com.apple.fides.asr
0idcom.apple.private.DistributedEvaluation.RecordAccess-com.apple.siri.speech-dictation-personalization
b/com.apple.private.assets.accessible-asset-types0
->com.apple.MobileAsset.Trial.Siri.SiriUnderstandingAsrAssistant;com.apple.MobileAsset.Trial.Siri.SiriUnderstandingAsrHammer4com.apple.MobileAsset.Trial.Siri.SiriDictationAssets$com.apple.MobileAsset.EmbeddedSpeech)com.apple.MobileAsset.EmbeddedSpeechWatch'com.apple.MobileAsset.EmbeddedSpeechMac061com.apple.private.assets.bypass-asset-types-check
9com.apple.private.attribution.implicitly-assumed-identity
type
path0
value
/System/Library/PrivateFrameworks/CoreEmbeddedSpeechRecognition.framework/XPCServices/com.apple.siri.embeddedspeech.xpc/com.apple.siri.embeddedspeech0I"com.apple.private.biome.read-write0#
SiriDictation
Dictation.UserEdit01,com.apple.private.calendar.allow-suggestions
com.apple.private.contacts
com.apple.private.corerecents
0-(com.apple.private.corespotlight.internal
com.apple.private.homekit
061com.apple.private.security.storage.SiriVocabulary
0<7com.apple.private.security.storage.SpeechPersonalizedLM
com.apple.private.tcc.allow0Y
kTCCServiceAddressBook
kTCCServiceCalendar
kTCCServiceWillow
kTCCServiceMediaLibrary072com.apple.proactive.PersonalizationPortrait.Config
0E@com.apple.proactive.PersonalizationPortrait.NamedEntity.readOnly
0% com.apple.proactive.eventtracker
:com.apple.security.exception.files.absolute-path.read-only0T
/private/var/MobileAsset/7/private/var/tmp/com.apple.siri-distributed-evaluation/0v4com.apple.security.exception.mach-lookup.global-name0>
com.apple.suggestd.contacts
com.apple.mobileasset.autoasset0
8com.apple.security.exception.shared-preference.read-only0P
com.apple.assistant
com.apple.assistant.backedup
com.apple.assistant.support0
.*com.apple.security.iokit-user-client-class0
AGXCommandQueue
AGXDevice
AGXDeviceUserClient
AGXSharedUserClient
H11ANEInDirectPathClient
IOAccelContext
IOAccelContext2
IOAccelDevice
IOAccelDevice2
IOAccelSharedUserClient
IOAccelSharedUserClient2
IOAccelSubmitter2
IOSurfaceRootUserClient083com.apple.security.personal-information.addressbook
0i>com.apple.security.temporary-exception.mach-lookup.global-name0'%com.apple.triald.namespace-management0
com.apple.siriknowledged
com.apple.spotlight.search
com.apple.trial.client0
372
401
mcpl
