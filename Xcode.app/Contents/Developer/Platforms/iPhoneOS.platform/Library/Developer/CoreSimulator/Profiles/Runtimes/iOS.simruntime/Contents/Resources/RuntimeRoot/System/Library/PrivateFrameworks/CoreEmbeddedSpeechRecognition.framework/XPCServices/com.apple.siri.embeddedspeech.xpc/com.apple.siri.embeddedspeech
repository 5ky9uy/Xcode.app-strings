ASRPreheatContextTimestamp
ASRAppleNeuralEngineCompilationContextTimestamp
-[ESSelfHelper initWithTask:isSpeechAPIRequest:]
-[ESSelfHelper logFinalResultGeneratedWithEARPackage:]
-[ESSelfHelper logRequestEndedOrFailedOrCancelledWithError:recognizerComponents:averageActiveTokensPerFrame:languageModelInterpolationWeights:signalToNoiseRatioInDecibels:recognitionDurationInSec:audioDurationMs:eagerUsed:utteranceDetectionEnabled:utteranceConcatenationEnabled:cpuRealTimeFactor:numLmeDataStreams:phoneticMatchDecoderName:pauseDurations:itnDurationInNs:isEmojiPersonalizationUsed:isEmojiDisambiguationUsed:isEmojiExpectedButNotRecognized:recognizedEmojis:]
floatValue
-[ESSelfHelper wrapAndEmitTopLevelEvent:timestampInTicks:]
v32@?0@"NSArray"8Q16^B24
CESRProfileErrorDomain
\NT-contact
\NT-appname
\NT-correction
overrides
keyboardLM
locationOfInterest
spatialLocationOfInterest
interaction
search
calendarEvent
pexNamedEntity
frequency
charsToTrim
charsToSplit
tagName
templateName
tagNameList
minimumWordLength
com.apple.siri.embeddedspeech.profilegeneration-keepalive
-[ESSpeechProfileBuilderConnection initWithXPCConnection:]_block_invoke
v8@?0
Siri
unified_asset_namespace
Could not create profile from asset and profile configurations
-[ESSpeechProfileBuilderConnection getVersionForCategory:completion:]
Begin called while there are already active categories.
-[ESSpeechProfileBuilderConnection beginWithCategoriesAndVersions:bundleId:completion:]
Speech category %@ has already been saved
Speech category %@ is unsupported
Add called before categories have been set with Begin
-[ESSpeechProfileBuilderConnection addVocabularyItems:isBoosted:completion:]
Deserialization of KVItem failed
-[ESSpeechProfileBuilderConnection finishAndSaveProfile:completion:]
v32@?0@"NSString"8@"NSMutableArray"16^B24
Failed to write profile
-[ESSpeechProfileBuilderConnection _profileWithError:]
+[ESSpeechProfileBuilderConnection personalizationRecipeForAssetConfig:modelOverridePath:]
personalization.json
+[ESSpeechProfileBuilderConnection _userProfileWithAssetConfig:modelOverridePath:overrides:foundPath:error:]
mini.json
en_US_napg.json
dispatch.voc
vocdelta.voc
pg.voc
lexicon.enh
token_s.enh
mrec.psh
Error during _EARUserProfile initialization
v32@?0@"CESRVocabularyCategory"8@"NSSet"16^B24
\correction-first
+[ESSpeechProfileBuilderConnection _adaptRecipe:userData:profile:]_block_invoke
v32@?0@"NSString"8@"NSDictionary"16^B24
+[ESSpeechProfileBuilderConnection _parseRequiredParameter:expectedClass:domain:recipe:error:]
Missing %@ for %@
+[ESSpeechProfileBuilderConnection addWordsToUserProfile:templateName:wordArrays:]
v32@?0@"NSString"8@"NSString"16^B24
+[ESSpeechProfileBuilderConnection _runAdaptationRecipeForNamesWithFrequency:tagNames:templateName:charSetToTrim:charSetToSplit:minimumWordLength:nameFrequencies:profile:]
+[ESSpeechProfileBuilderConnection _runAdaptationRecipeForNamesWithFrequency:tagNames:templateName:charSetToTrim:charSetToSplit:minimumWordLength:nameFrequencies:profile:]_block_invoke_2
v32@?0@"NSString"8Q16^B24
v32@?0@"NSString"8@"NSNumber"16^B24
+[ESSpeechProfileBuilderConnection _runAdaptationRecipeForKeyboardLMWithFrequency:tagName:templateName:charSetToTrim:charSetToSplit:minimumWordLength:userData:profile:]
+[ESSpeechProfileBuilderConnection _runAdaptationRecipeForLocationOfInterestWithFrequency:tagName:templateName:charSetToTrim:charSetToSplit:minimumWordLength:locationOfInterestNames:profile:]
+[ESSpeechProfileBuilderConnection _runAdaptionRecipeForCalendarEventsWithFrequency:tagName:templateName:charSetToTrim:charSetToSplit:minimumWordLength:userData:profile:]
com.apple.MobileSMS
SpeechProfile
EARUserProfileContainerLoadDate
+[CESRUtilities speechProfileRootDirectories]
+[CESRUtilities speechProfilePathsWithLanguage:]
+[CESRUtilities loadSpeechProfiles:language:]
v32@?0@"_EARSpeechRecognitionToken"8Q16^B24
Param
v32@?0{_NSRange=QQ}8^B24
v32@?0@"EARVoiceCommandArgument"8Q16^B24
@"NSArray"24@?0@"NSArray"8@"NSArray"16
+[CESRUtilities AFSpeechInfoPackageForEARSpeechRecognitionResultPackage:]_block_invoke
unconstrained
reduced
avoid
AFSpeechLatticeMitigatorResultForEar
quasarModelPath
type
com.apple.trial.NamespaceUpdate.SIRI_DICTATION_ASSETS
com.apple.trial.NamespaceUpdate.SIRI_UNDERSTANDING_ASR_ASSISTANT
trial
QuasarDir
PreferOverServer
SupportsContinuousListening
SupportsOnDeviceSearch
GeoLMAssetsInfo
-[MAAsset(ESAdditions) _es_purgeSync]
Asset: content version: %@, mastered version %@, installed %@, language: %@, path: %@
Language
-[MAAsset(ESAdditions) _es_isInstalled]
com.apple.siri.embeddedspeech.ESAssetManager
v12@?0i8
-[ESAssetManager registerNotifications]
-[ESAssetManager installationStatusForLanguagesIgnoringCache:assetType:withDetailedStatus:withError:]
-[ESAssetManager installationStatusForLanguagesIgnoringCache:assetType:withDetailedStatus:withError:]_block_invoke
-[ESAssetManager _invalidateInstallationStatusCacheForAssetType:]
-[ESAssetManager _queryInstallationStatusForLanguagesWithError:]
-[ESAssetManager installedModelInfoForAssetConfig:error:triggerDownload:ignoreSpellingModel:]
+[ESAssetManager _assetQueryForLanguage:]
-[ESAssetManager purgeInstalledAssetsExceptLanguages:assetType:error:]_block_invoke
-[ESAssetManager purgeOutdatedAssets]_block_invoke
-[ESAssetManager _purgeMobileAssetsForLanguage:error:]_block_invoke
v24@?0@"MAAsset"8^B16
-[ESAssetManager startMissingAssetDownload]
-[ESAssetManager startMissingAssetDownload]_block_invoke
: %@
%@: ModelInfo=%@: AssetId=%@:
com.apple.internal.ck
disableTrialAssetDelivery
-[ESAssetManager trialAssetDeliveryEnabled:]
enableTrialAssetDelivery
trial_dictation_asset_delivery
-[ESAssetManager prepareHammerConfigFile:]
-[ESAssetManager _installedGeoLMRegionMappingForLanguage:]
-[ESAssetManager geoLMRegionIdForLanguage:location:]
-[ESAssetManager installedGeoLMRegionSpecificAssetForLanguage:regionId:mainAssetConfig:]
-[ESAssetManager _geoLMCompatibleWithMainAsset:geoAssetConfig:]
-[ESAssetManager purgeUnusedGeoLMAssetsForLangauge:]_block_invoke
q24@?0@8@16
-[ESAssetManager purgeUnusedGeoLMAssetsForLangauge:]_block_invoke_2
-[ESAssetManager _purgeUserDefaultsGeoLMAssetsInfoDictExceptLanguages:]
EnumerateInstalledAssets
confidenceThresholds
confidenceModels
wordConfidenceThreshold
utteranceConfidenceThreshold
continuousListening
shouldHandleCapitalization
QuasarModel
replicateDataPacketPersonalization
distributedEvaluation
adaptation
WallRTF
DecodeDuration
AverageActiveTokensPerFrame
lm_interp_weights
jitQueryDurationInMs
jitLmeDurationInMs
jitDataStats
version
data
language
assetPath
PERSONALINFO
com.apple.fides.asr
com.apple.siri.speech-dictation-personalization
dictation
ModelOverrideURL
TP/tantor/voice_commands
textfield-editing-suite.plist
com.apple.siri.ESConnection
-[ESConnection initWithXPCConnection:]
-[ESConnection initWithXPCConnection:]_block_invoke
-[ESConnection dealloc]
com.apple.siri.ESConnection.fidesEval
v24@?0@"NSDictionary"8@"NSError"16
-[ESConnection fetchModelInfoForAssetConfig:triggerDownload:completion:]
-[ESConnection fetchUserDataForLanguage:completion:]_block_invoke
B8@?0
v16@?0@"CESRUserData"8
v24@?0@"CESRUserDataOptions"8@"NSError"16
-[ESConnection getOfflineAssetStatusIgnoringCache:assetType:withDetailedStatus:withCompletion:]
SiriX
enableTelemetry=YES
+[ESConnection _speechRecognizerWithAssetConfig:geoLMRegionId:enableITN:overrides:modelOverrideURL:preloadModels:enableParallelLoading:isHighPriority:geoLMLoadedOut:error:]
enableParallelLoading
ASR task for memory lock
com.apple.assistant
taskForMemoryLock
dictation_emoji_recognition
dictation_voice_commands
itn_s.enh
Failed to create recognizer from %@
AlreadyLoaded
Preheating
-[ESConnection preheatSpeechRecognitionWithAssetConfig:modelOverrideURL:]
 with CustomModelURL %@
Success
(none)
Failure
-[ESConnection shouldWriteDictationRecord:]
v24@?0@"CESRModelProperties"8@"NSError"16
-[ESConnection startSpeechRecognitionWithParameters:didStartHandlerWithInfo:]
Recognizer is busy
@"NSDictionary"8@?0
-[ESConnection sendSpeechCorrectionInfo:interactionIdentifier:]_block_invoke
\jit
-[ESConnection readProfileAndUserDataWithLanguage:allowOverride:completion:]
Not a dictionary: %@
Not an array: %@
orth
prons
freq
v32@?0@"NSString"8@"NSArray"16^B24
-[ESConnection updateSpeechProfileWithLanguage:modelOverridePath:existingProfile:existingAssetPath:completion:]
Could not build empty user profile
-[ESConnection updateSpeechProfileWithLanguage:modelOverridePath:existingProfile:existingAssetPath:completion:]_block_invoke_3
-[ESConnection updateSpeechProfileWithLanguage:modelOverridePath:existingProfile:existingAssetPath:completion:]_block_invoke
Empty user data
User data unchanged
Could not create user profile
UserData
+[ESConnection _parseRequiredParameter:expectedClass:domain:recipe:error:]
+[ESConnection _runAdaptationRecipeForNamesWithFrequency:tagNames:templateName:charSetToTrim:charSetToSplit:minimumWordLength:nameFrequencies:profile:]
+[ESConnection _runAdaptationRecipeForNamesWithFrequency:tagNames:templateName:charSetToTrim:charSetToSplit:minimumWordLength:nameFrequencies:profile:]_block_invoke_2
+[ESConnection _runAdaptationRecipeForKeyboardLMWithFrequency:tagName:templateName:charSetToTrim:charSetToSplit:minimumWordLength:userData:profile:]
+[ESConnection _runAdaptationRecipeForLocationOfInterestWithFrequency:tagName:templateName:charSetToTrim:charSetToSplit:minimumWordLength:locationOfInterestNames:profile:]
+[ESConnection _runAdaptionRecipeForCalendarEventsWithFrequency:tagName:templateName:charSetToTrim:charSetToSplit:minimumWordLength:userData:profile:]
Cooldown timeout for EAR
+[ESConnection _scheduleCooldownTimer]
com.apple.siri.embeddedspeech.cooldown-keepalive
+[ESConnection _cancelCooldownTimer]
+[ESConnection _cooldownTimerFired]
+[ESConnection _cachedRecognizerCleanUp]
+[ESConnection purgeOutdatedAssets]_block_invoke
+[ESConnection prepareToExit]
+[ESConnection _sendPendingAnalyticsEvents]
+[ESConnection _adaptRecipe:userData:profile:]_block_invoke
recipeType
-[ESConnection readTableFromURL:]
-[ESConnection runRecipeEvaluationDistributedEvaluation:recordData:attachments:completion:]
returnHypothesis
returnOverallWER
returnOverallRTF
returnPerUtteranceWER
returnPerUtteranceRTF
locale
task
sampleRate
wordSenseWhitelist
wav.scp
raw.ref
-[ESConnection runRecipeEvaluationDistributedEvaluation:recordData:attachments:completion:]_block_invoke
token
confidence
transcription
\\\S+$
EditDistance
details
modelVersion
overall-rtf
overall-wer
v24@?0@"NSData"8@"NSString"16
SiriCoreLocalSpeechUserData
-[ESConnection runDefaultAdaptationEvaluation:recordData:attachments:completion:]
Recipe has no profile
Voicemail audio file deleted by user
%@ %@
.model
Malformed overrides
ConfidenceMean
ConfidenceMin
ConfidenceMax
AlternateConfidenceMean
AlternateConfidenceMin
AlternateConfidenceMax
Baseline
CustomModel
ModelVersion
Adapted
Alignment
name
callStackReturnAddresses
callStackSymbols
reason
userInfo
(unknown C++ exception)
-[ESConnection runCorrectedTextEvaluationWithAudioDatas:recordDatas:language:samplingRate:caseSensitive:skipLME:wordSenseAccessListSet:completion:]_block_invoke
No tokenizer for %@
Interrupted corrected text evaluation during speech recognition
correctedOutput
recognizedOutput
editDistanceRecognizedCorrected
editDistanceRecognizedTTSASR
timestamp
interactionId
asrSelfComponentIdentifier
results
-[ESConnection _userProfileWithAssetConfig:modelOverridePath:overrides:foundPath:isJit:error:]
-[ESConnection _userProfileConfigWithAssetConfig:modelOverridePath:overrides:error:]
_EARUserProfileConfig initialization failed: %@ with reason: %@
-[ESConnection _modelRootWithAssetConfig:modelOverridePath:overrides:error:]
asrSelfComponentId
asset
applicationName
metrics
tokens
alignments
uttInfos
uttInfosCompressed
alignmentReferences
usePersonalizedLM
corrected
recognized
useJIT
disableAOT
contextualData
overrideFiles
restoreAOT
evaluations
-[ESConnection runEvaluationWithDESRecordDatas:language:recipe:attachments:fidesPersonalizedLMPath:fidesPersonalizedLMTrainingAsset:scrubResult:completion:]_block_invoke
Unknown evaluation name found in alignmentReferences: %@
scoreNbest
compress
Interrupted evaluation redecoding
-[ESConnection runEvaluationWithDESRecordDatas:language:recipe:attachments:fidesPersonalizedLMPath:fidesPersonalizedLMTrainingAsset:scrubResult:completion:]_block_invoke_2
Interrupted evaluation tokenization
-[ESConnection _deleteTemporaryDirectoryIfExists:]
-[ESConnection resetCacheAndCompileAllAssetsWithCompletion:]
-[ESConnection _fetchUserDataOptionsWithAssetConfig:modelOverridePath:overrides:completion:]_block_invoke
Failed to fetch user data options, for unknown reason
-[ESConnection pauseRecognition]
-[ESConnection resumeRecognitionWithPrefixText:postfixText:selectedText:]
-[ESConnection _writeDESRecord:oneRecordPerDay:]
-[ESConnection _writeDESRecord:oneRecordPerDay:]_block_invoke
-[ESConnection _writeDESRecord:oneRecordPerDay:]_block_invoke_2
-[ESConnection speechRecognizer:didProduceEndpointFeaturesWithWordCount:trailingSilenceDuration:eosLikelihood:pauseCounts:silencePosterior:processedAudioDurationInMilliseconds:]_block_invoke
Dictation
\entity-first
-[ESConnection speechRecognizer:didProduceLoggablePackage:]_block_invoke
-[ESConnection speechRecognizer:didRecognizePartialResult:]_block_invoke
Unsupported EAR build?
Unsupported EAR package build?
DUMMYTOKEN
-[ESConnection speechRecognizer:didRecognizeFinalResultPackage:]_block_invoke
RECOGNITION_SUCCESS
RECOGNITION_FAILED
RECOGNITION_CANCELLED
RECOGNITION_REJECTED
-[ESConnection speechRecognizer:didFinishRecognitionWithError:]_block_invoke
recognizer-components
audioDurationMs
recognitionDurationMs
@sum.self
EagerUsed
numLmeDataStreams
PM-decoder
PM-input
PM-output
tokenName
-[ESConnection speechRecognizer:didRecognizeRawEagerRecognitionCandidate:]_block_invoke_2
v16@?0@"_EARFormatter"8
-[ESConnection speechRecognizer:didRecognizeRawEagerRecognitionCandidate:]_block_invoke
lastPathComponent == %@
_ESDecompressArchiveWithURL
VoiceTriggerFidesArchive
Failed to specify compression algorithm
Failed to specify format
Failed to open file for reading
Unable to extract file to: %@
Task %@ not available for %@, supported tasks are %@
etiquette.json
ReplacementDictionaryForAssetConfig
ReplacementDictionaryForAssetConfig_block_invoke
v32@?0@"NSString"8@16^B24
voicemail_confidence_subtraction.json
sampling rate = %@
task type = %@
far field = %@
device ID = %@
Bluetooth device ID = %@
ANE context = %@
CPU context = %@
GPU context = %@
PersonalizationRecipeForAssetConfig
AddWordsToUserProfile
ReadAudioDataFromFileURL
\\\S*$
Insertions
Deletions
Substitutions
ReferenceSize
\contact-first
CONTACTFIRSTNAME
\contact-middle
CONTACTMIDDLENAME
\contact-last
CONTACTLASTNAME
\contact-nickname
\app-first
APPNAMEFIRSTNAME
\company-first
COMPANYFIRSTNAME
\interaction-first
\interaction-middle
\interaction-last
INLINEFIRSTNAME
\contact-first-derived
\contact-middle-derived
\contact-last-derived
\contact-nickname-derived
Recipe evaluation failed
Empty recognition Result
Error
UtteranceLength
NumberOfNonTerminals
WordsAboveThreshold
UtteranceAboveThreshold
Override file is not found in attachments or device
restoreJIT
configurationFile
JIT LME: JIT profile builder is not initialized
JIT LME: required configuration/file is missing
JIT LME: configuration file is not found in attachments or device
zlibCompressedJson
dictationUIInteractionIdentifier
interactionIdentifier
samplingTimestamp
codec
samplingRate
inferenceSpeakerCode
numTrainedFrames
trainingNnetVersion
isSpeakerCodeUsed
-[ESStoreAudioData initWithUUIDString:language:task:codec:samplingRate:inferenceSpeakerCode:numTrainedFrames:trainingNnetVersion:isSpeakerCodeUsed:isSamplingForDictation:selfLogger:]
-[ESStoreAudioData saveAudioToDisk]
-[ESStoreAudioData _deleteItemAtPath:]
-[ESStoreAudioData _cleanupCacheAndReset:]
-[ESStoreAudioData _saveAudioToCache:]
-[ESStoreAudioData _moveAudioToVarMobile:]
%@.plist
-[ESStoreAudioData _saveAudioMetadataToFilePath:]
-[ESStoreAudioData _createAudioFilePath]
%.0f
%@_%@_%@.pcm
unixTime
samplingDate
success
failed
errorCode
errorDomain
UNDEFINED
description
underlyingErrorCode
underlyingErrorDomain
Audio file to be moved nil
Sampling Date is nil
Unable to create sampling directory
Unable to create dated directory
status
com.apple.siri.embeddedspeech
com.apple.private.des-service
Rejecting %@, no %@ entitlement
Rejecting %@, no %@ or %@ entitlement
main_block_invoke
init
_isLoggingAllowedForCurrentRequestWithTask:isSpeechAPIRequest:
UUID
_isTier1LoggingAllowedForCurrentRequestWithTask:
sharedPreferences
isDictationHIPAACompliant
siriDataSharingOptInStatus
isEqualToString:
setWithObjects:
containsObject:
initWithNSUUID:
setUuid:
setComponent:
setSource:
setTarget:
sharedStream
emitMessage:
setExists:
numberWithUnsignedLongLong:
setTimestampInTicks:
setStartedOrChanged:
captureSnapshot
context
initWithPreheatContext:powerSnapshot:powerEventContext:
setFailed:
setStatus:
setEnded:
countByEnumeratingWithState:objects:count:
preheatContext
timestampInTicks
wrapAndEmitTopLevelEvent:timestampInTicks:
powerSnapshot
powerEventContext
logWithEventContext:asrIdentifier:
setStarted:
setLinkId:
setDialogContexts:
setTask:
stringByReplacingOccurrencesOfString:withString:
convertLanguageCodeToSchemaLocale:
setModelLocale:
boolValue
setIsHighQualityAsset:
unsignedLongLongValue
setSpeechProfileAgeInNs:
initWithUUIDString:
setDictationUiInteractionId:
setDatapackVersion:
setHammerVersion:
setGeoLanguageModelRegion:
setGeoLanguageModelLoaded:
setPortraitExperimentVariantName:
logIntermediateUtteranceInfoTier1WithPmInput:pmOutput:unrepairedPostItn:loggableSharedUserId:
setPhoneticMatchInput:
setPhoneticMatchOutput:
setUnrepairedPostItn:
recognition
interpretationIndices
count
isFinal
setIsFinal:
setPackage:
correctPartialResultIndexList
setCorrectPartialResultIndexLists:
preITNRecognition
oneBest
silenceStart
utteranceStart
numberWithDouble:
addObject:
setTokenSilenceStartTimeInNsLists:
firstResultAfterResume
setIsAfterResume:
componentsJoinedByString:
UTF8String
setPersonalizedLanguageModelAgeInNs:
floatValue
setPersonalizedLanguageModelWeight:
setAverageActiveTokensPerFrame:
setSignalToNoiseRatioInDecibels:
setRecognitionDurationInNs:
setAudioDurationInNs:
setEagerEnabled:
setCpuRealTimeFactor:
setNumLanguageModelEnrollmentDataStreams:
setUtteranceDetectionEnabled:
setUtteranceConcatenationEnabled:
setContinuousListeningEnabled:
setPhoneticMatchDecoderName:
setInverseTextNormalizationDurationInNs:
setIsEmojiPersonalizationUsed:
setIsEmojiDisambiguationUsed:
setIsEmojiExpectedButNotRecognized:
setRecognizedEmojis:
setEmojiMetrics:
componentsSeparatedByString:
objectAtIndexedSubscript:
setFrontend:
setDecoder:
setDecodable:
setRecognizerComponents:
doubleValue
setStartTimeInNs:
setEndTimeInNs:
valueForKey:
setWeights:
setLanguageModelInterpolationWeights:
array
setPausedAudioDurationsInNs:
code
setMetrics:
setReason:
setCancelled:
setErrorCode:
domain
setErrorDomain:
userInfo
objectForKey:
setUnderlyingErrorCode:
setUnderlyingErrorDomain:
setSampledAudioStorageFailureReason:
_isNonTier1Message:
deleteLinkId
methodForSelector:
setAsrId:
setEventMetadata:
setPreheatContext:
setRequestContext:
setPartialResultGenerated:
setPackageGenerated:
setRecognitionResultTier1:
setFinalResultGenerated:
setIntermediateUtteranceInfoTier1:
setInitializationContext:
setAssetLoadContext:
setLanguageModelEnrollmentContext:
setJitLanguageModelEnrollmentEndedTier1:
setAudioPacketArrivalContext:
setFirstAudioPacketProcessed:
setFinalAudioPacketContainingSpeechReceived:
setSampledAudioFileStored:
setSampledAudioFileStorageFailed:
setAppleNeuralEngineCompilationContext:
emitMessage:timestamp:
initWithCurrentProcess
logWithEventContext:
createPreheatStartedOrChangedEvent
createPreheatFailedEvent
createPreheatEndedEventWithPreheatAlreadyDone:
createANECompilationStartedEventWithTimeStamp:
createANECompilationEndedEventWithTimeStamp:
initializeSharedPowerLoggerIfNeeded
logPowerSnapshotForProcessStarted
logPowerSnapshotForProcessEnded
initWithTask:isSpeechAPIRequest:
logRequestLinkWithRequestId:
logPendingPreheatContextEvents:
logPendingANECompilationContextEvents:
logInitializationStartedOrChangedWithTimeInTicks:
logInitializationEnded
logAssetLoadStartedOrChanged
logAssetLoadEnded
logJitLmeStartedOrChangedWithTimeInTicks:
logJitLmeEndedAndEndedTier1WithDialogContext:
logAudioPacketArrivalStartedOrChangedWithTimeInTicks:
logAudioPacketArrivalEndedWithTimeInTicks:
logFirstAudioPacketProcessed
logFinalAudioPacketContainingSpeechReceivedWithTimeInTicks:loggableSharedUserId:
logRequestStartedOrChangedWithTask:modelLocale:modelVersion:isHighQualityAsset:hammerVersion:geoLanguageModelRegion:geoLanguageModelLoaded:speechProfileAgeInSec:dictationUIInteractionId:portraitExperimentVariantName:
logPartialResultGenerated
logIntermediateUtteranceInfoTier1WithPmInput:pmOutput:
logIntermediateUtteranceInfoTier1WithUnrepairedPostItn:loggableSharedUserId:
logPackageGeneratedAndRecognitionResultTier1WithEARPackage:loggableSharedUserId:
logFinalResultGeneratedWithEARPackage:
logRequestEndedOrFailedOrCancelledWithError:recognizerComponents:averageActiveTokensPerFrame:languageModelInterpolationWeights:signalToNoiseRatioInDecibels:recognitionDurationInSec:audioDurationMs:eagerUsed:utteranceDetectionEnabled:utteranceConcatenationEnabled:cpuRealTimeFactor:numLmeDataStreams:phoneticMatchDecoderName:pauseDurations:itnDurationInNs:isEmojiPersonalizationUsed:isEmojiDisambiguationUsed:isEmojiExpectedButNotRecognized:recognizedEmojis:
logSampledAudioFileStoredSuccessfully
logSampledAudioFileStoredWithError:customFailureReason:
asrId
recognitionTask
unrepairedPostItn
personalizedLmWeight
setPersonalizedLmWeight:
personalizedLmAgeInSec
setPersonalizedLmAgeInSec:
continuousListeningEnabled
.cxx_destruct
_asrId
_recognitionTask
_packageLogged
_isTier1LoggingAllowedForCurrentRequest
_continuousListeningEnabled
_unrepairedPostItn
_personalizedLmWeight
_personalizedLmAgeInSec
T@"NSUUID",R,N,V_asrId
T@"NSString",R,N,V_recognitionTask
T@"NSString",&,N,V_unrepairedPostItn
T@"NSNumber",&,N,V_personalizedLmWeight
T@"NSNumber",&,N,V_personalizedLmAgeInSec
TB,N,V_continuousListeningEnabled
T@"NSNumber",&,N
_preheatContext
_powerSnapshot
_powerEventContext
T@"ASRSchemaASRPreheatContext",R,N,V_preheatContext
T@"SPIPowerLoggerSnapshot",R,N,V_powerSnapshot
T@"SPIEventContext",R,N,V_powerEventContext
setRawRecognition:
setPostItn:
tokenSausage
setPhrases:
setUtterances:
linkId
orderedSetWithArray:
setInterpretationIndices:
objectAtIndex:
unsignedIntegerValue
setTokens:
indexOfObject:
numberWithUnsignedInteger:
arrayByAddingObject:
enumerateObjectsUsingBlock:
copy
setInterpretations:
initWithCapacity:
start
hasSpaceAfter
setAppendSpaceAfter:
setSilenceStartTimeInNs:
confidence
setConfidence:
appendedAutoPunctuation
prependedAutoPunctuation
setIsAutoPunctuation:
isModifiedByAutoPunctuation
setIsModifiedByAutoPunctuation:
tokenName
length
punctuationCharacterSet
rangeOfCharacterFromSet:
setPunctuationText:
setText:
phoneSequence
setPhoneSequence:
ipaPhoneSequence
setIpaPhoneSequence:
setLinkIndex:
_UUID
invalidate
setInterruptionHandler:
setInvalidationHandler:
dealloc
initWithLanguage:assetType:
_profileWithError:
dictionaryWithObjects:forKeys:count:
errorWithDomain:code:userInfo:
longLongValue
supportedCategories
allKeys
stringWithFormat:
addEntriesFromDictionary:
itemFromBuffer:error:
setIsBoosted:
addObjectsFromArray:
setObject:forKeyedSubscript:
initWithItems:language:
enumerateKeysAndObjectsUsingBlock:
removeAllObjects
setTemplateToVersion:
setUserId:
personalizationRecipeForAssetConfig:modelOverridePath:
adaptUserProfileWithUserData:personalizationRecipe:userData:endOfUserData:
removeLmeDataForTemplateName:
profileFilePathFromBasePath:language:userId:
stringByDeletingLastPathComponent
defaultManager
createDirectoryAtPath:withIntermediateDirectories:attributes:error:
writeProfileToFile:protectionClass:length:error:
_userProfileWithAssetConfig:modelOverridePath:overrides:foundPath:error:
fileExistsAtPath:
readUserProfileWithPath:reuseProfile:
templateToVersion
mutableCopy
language
sharedInstance
installedQuasarModelPathForAssetConfig:error:triggerDownload:
stringByAppendingPathComponent:
dataWithContentsOfFile:options:error:
JSONObjectWithData:options:error:
assetType
switchToNewAssetsForAssetType:
installedQuasarModelPathForAssetConfig:error:triggerDownload:ignoreSpellingModel:
initWithConfiguration:language:overrides:sdapiOverrides:generalVoc:emptyVoc:pgVoc:lexiconEnh:tokenEnh:paramsetHolder:
removeAllWords
contactWordsWithFrequency
addWordsToUserProfile:templateName:wordArrays:
vocabularyWords
tagName
initWithOrthography:pronunciations:tagName:frequency:
arrayWithObjects:count:
templateName
addWordWithParts:templateName:
corrections
pronunciationsForOrthography:
appNames
initWithOrthography:pronunciations:tag:
_adaptRecipe:userData:profile:
signalEndOfUserData
_parseRequiredParameter:expectedClass:domain:recipe:error:
characterSetWithCharactersInString:
_runAdaptationRecipeForDomain:frequency:tagNames:templateName:charSetToTrim:charSetToSplit:minimumWordLength:userData:profile:
components
objectForKeyedSubscript:
removeObjectForKey:
frequency
firstObject
_runAdaptationRecipeForKeyboardLMWithFrequency:tagName:templateName:charSetToTrim:charSetToSplit:minimumWordLength:userData:profile:
locationOfInterestNames
_runAdaptationRecipeForLocationOfInterestWithFrequency:tagName:templateName:charSetToTrim:charSetToSplit:minimumWordLength:locationOfInterestNames:profile:
spatialLocationOfInterestNames
interactionSenderDisplayNames
_runAdaptationRecipeForNamesWithFrequency:tagNames:templateName:charSetToTrim:charSetToSplit:minimumWordLength:nameFrequencies:profile:
searchEventValues
_runAdaptionRecipeForCalendarEventsWithFrequency:tagName:templateName:charSetToTrim:charSetToSplit:minimumWordLength:userData:profile:
pexNamedEntityNames
setObject:forKey:
componentsSeparatedByCharactersInSet:
lastObject
keyboardLMDynamicVocabularyItems
dictionaryWithDictionary:
whitespaceCharacterSet
countForObject:
eventTitles
eventLocationNames
arrayByAddingObjectsFromArray:
stringByTrimmingCharactersInSet:
isEqual:
class
self
performSelector:
performSelector:withObject:
performSelector:withObject:withObject:
isProxy
isKindOfClass:
isMemberOfClass:
conformsToProtocol:
respondsToSelector:
retain
release
autorelease
retainCount
zone
hash
superclass
description
debugDescription
TQ,R
T#,R
T@"NSString",R,C
setProfileConfigWithLanguage:profileDir:userId:dataProtectionClass:
getVersionForCategory:completion:
beginWithCategoriesAndVersions:bundleId:completion:
addVocabularyItems:isBoosted:completion:
cancelWithCompletion:
finishAndSaveProfile:completion:
initWithXPCConnection:
_stagedItems
_committedItems
_stagedCategoryToVersion
_committedCategoryToVersion
_seenCategories
_baseDirectory
_language
_userId
_dataProtectionClass
_assetConfig
_connection
_transaction
_profile
markRecognition
sendEvent
recognitionEndTime
setRecognitionEndTime:
applicationName
setApplicationName:
interactionId
setInteractionId:
taskName
setTaskName:
hasRecognizedAnything
_hasRecognizedAnything
_recognitionEndTime
_applicationName
_interactionId
_taskName
Td,N,V_recognitionEndTime
T@"NSString",C,N,V_applicationName
T@"NSString",C,N,V_interactionId
T@"NSString",C,N,V_taskName
TB,R,N,V_hasRecognizedAnything
loadDate
setLoadDate:
T@"NSDate",C,N
timeZoneWithAbbreviation:
setTimeZone:
dateWithTimeIntervalSince1970:
date
currentCalendar
components:fromDate:toDate:options:
utterances
setStartTime:
setSilenceStartTime:
setEndTime:
setRemoveSpaceBefore:
setConfidenceScore:
initWithPhrases:utterances:processedAudioDuration:
afRecognitionForEARSausage:processedAudioDuration:
audioAnalytics
latticeMitigatorResult
initWithRecognition:unfilteredRecognition:rawRecognition:audioAnalytics:isFinal:utteranceStart:latticeMitigatorResult:recognitionPaused:speechProfileUsed:
instancesRespondToSelector:
recognitionPaused
initWithRecognition:unfilteredRecognition:rawRecognition:audioAnalytics:isFinal:utteranceStart:latticeMitigatorResult:recognitionPaused:
path
speechProfileRootDirectories
enumeratorAtPath:
pathWithComponents:
speechProfilePathsWithLanguage:
attributesOfItemAtPath:error:
timeIntervalSinceReferenceDate
initWithPath:error:
hasSpaceBefore
appendString:
stringByAppendingFormat:
arguments
presence
indexes
substringWithRange:
enumerateRangesUsingBlock:
initWithText:
voiceCommandsParamKeyBuilder:
commandIdentifier
initWithCommandId:isComplete:paramMatches:
initWithUtterance:parseCandidates:
tokens
earTokensToString:
voiceCommandInterpretations
afVoiceCommandGrammarParseResultForEARTokenString:withEARVoiceCommandInterpretations:
preITNTokens
preITNVoiceCommandInterpretations
initWithNBestParses:preITNNBestParses:
initWithCommandGrammarParsePackage:
nBest
nBestVoiceCommandInterpretations
preITNNBestVoiceCommandInterpretations
text
startTime
endTime
silenceStartTime
confidenceScore
removeSpaceAfter
removeSpaceBefore
initWithTokenName:start:end:silenceStart:confidence:hasSpaceAfter:hasSpaceBefore:phoneSequence:ipaPhoneSequence:appendedAutoPunctuation:
calculateDiffInDaysFromTimestamp:
hasRecognizedAnythingInAFSpeechPackage:
afTokensForEARTokens:removeSpaceBefore:
afSpeechPackageForEARPackage:processedAudioDuration:speechProfileUsed:
loadSpeechProfiles:language:
AFSpeechInfoPackageForEARSpeechRecognitionResult:
AFSpeechInfoPackageForEARSpeechRecognitionResultPackage:
earTokensForAFTokens:appendedAutoPunctuation:
mapContextOptionToString:
initWithInterpretationIndices:confidenceScore:
setIsLowConfidence:
interpretations
acousticFeatures
setValue:forKey:
speechRecognitionFeatures
initWithSpeechRecognitionFeatures:acousticFeatures:snr:
acousticFeatureValuePerFrame
frameDuration
initWithAcousticFeatureValue:frameDuration:
score
threshold
version
initWithResults:score:threshold:
_es_quasarModelPath
purgeCompiledRecognizerModelsWithConfiguration:
purgeSync
_es_isInstalled
_es_contentVersion
_es_masteredVersion
_es_language
_es_path
attributes
stringValue
getLocalFileUrl
state
fileExistsAtPath:isDirectory:
refreshState
_es_purgeSync
_es_description
_es_status
_es_quasarDir
_es_preferOverServer
_es_supportsContinuousListening
_es_supportsOnDeviceSearch
registerNotifications
dictionary
_invalidateInstallationStatusCacheForAssetType:
trialAssetDeliveryEnabled:
installationStatusForLanguagesForAssetType:includeDetailedStatus:error:
installationStatusForLanguagesForAssetType:error:
_assetQueryForLanguage:
queryMetaDataSync
numberWithInteger:
results
modelTypeStatusStringAndVersionWithAsset:
installedModelInfoForAssetConfig:error:
installedModelInfoForAssetConfig:error:triggerDownload:
installedModelInfoForAssetConfig:error:triggerDownload:ignoreSpellingModel:
installedAssetWithConfig:regionId:triggerDownload:
modelQualityTypeStatusStringWithConfig:
numberWithBool:
setAssetsProvisionalForAssetType:
promoteAssetsForAssetType:
initWithType:
returnTypes:
addKeyValuePair:with:
_purgeUserDefaultsGeoLMAssetsInfoDictExceptLanguages:
purgeInstalledAssetsExceptLanguages:assetType:error:
languageCode
assistantIsEnabled
dictationIsEnabled
activeDictationLanguages
_queryInstallationStatusForLanguagesWithError:
_purgeMobileAssetsForLanguage:error:
enabledDictationLocales
modelAttributesStatusStringWithAsset:
assetId
initWithSuiteName:
isSiriXEnabled
installedAssetWithConfig:
jsonFilenameForAssetType:
initWithConfig:
_installedGeoLMRegionMappingForLanguage:
coordinate
initWithConfiguration:
regionIdForLatitude:longitude:
installedAssetWithConfig:regionId:
_geoLMCompatibleWithMainAsset:geoAssetConfig:
_updateGeoLMAssetsInfoDictWithRegionId:language:
purgeInstalledAssetForAssetType:language:regionId:error:
_loadGeoLMAssetsInfoDictForLanguage:
timeIntervalSince1970
_updateUserDefaultsWithGeoLMAssetsInfoDict:language:
compare:
keysSortedByValueUsingComparator:
supportedLanguagesWithAssetType:
_userDefaultsGeoLMAssetsInfoDictKeyForLangauge:
standardUserDefaults
dictionaryForKey:
stringByAppendingString:
installationStatusForLanguagesIgnoringCache:assetType:withDetailedStatus:withError:
installedQuasarModelPathForAssetConfig:error:
prepareModelInfo:withAssetType:
promoteModelInfo:withAssetType:
isBelowLimitForLocale:
purgeOutdatedAssets
startMissingAssetDownload
installedHammerConfigFileForLanguage:
prepareHammerConfigFile:
promoteHammerConfigFile
geoLMRegionIdForLanguage:location:
installedGeoLMRegionSpecificAssetForLanguage:regionId:mainAssetConfig:
purgeUnusedGeoLMAssetsForLangauge:
_queue
_languageInstallationCache
_dictationAssetUpdatedNotificationToken
_assistantAssetUpdatedNotificationToken
_recognizerAssetPathsInUse
_profileAssetPathsInUse
_geoLMAssetsInfoDict
numberWithLongLong:
allocWithZone:
numberOfInsertions
setNumberOfInsertions:
numberOfSubstitutions
setNumberOfSubstitutions:
numberOfDeletions
setNumberOfDeletions:
totalCost
setTotalCost:
copyWithZone:
incrementInsertions
incrementDeletions
incrementSubstitutions
incrementCost
_numberOfInsertions
_numberOfDeletions
_numberOfSubstitutions
_totalCost
Tq,N,V_numberOfInsertions
Tq,N,V_numberOfDeletions
Tq,N,V_numberOfSubstitutions
Tq,N,V_totalCost
setModelType:
modelType
setModelRoot:
modelRoot
T@"NSString",C,N
infersQoSFromInstanceUUIDForEAR
_setQueue:
cancelRecognition
remoteObjectProxy
fetchAssetsForAssetConfig:completion:
fetchModelInfoForAssetConfig:completion:
fetchModelInfoForAssetConfig:triggerDownload:completion:
setWithArray:
initWithModelVersion:modelType:modelRoot:
data
dictionaryRepresentation
dataWithJSONObject:options:error:
fetchUserDataWithLanguage:options:keepGoing:completion:
_fetchUserDataOptionsWithAssetConfig:modelOverridePath:overrides:completion:
getOfflineAssetStatusIgnoringCache:assetType:withDetailedStatus:withCompletion:
getOfflineAssetStatusIgnoringCache:assetType:withCompletion:
_speechRecognizerWithAssetConfig:geoLMRegionId:enableITN:overrides:modelOverrideURL:preloadModels:enableParallelLoading:isHighPriority:geoLMLoadedOut:error:
processInfo
systemUptime
activeConfigurationForEverything
setSamplingRateFilter:
setTaskTypeFilter:
setFarFieldFilter:
setDeviceIdFilter:
setBluetoothDeviceIdFilter:
setAneContextFilter:
setCpuContextFilter:
setGpuContextFilter:
initWithConfiguration:useQuasarFormatter:activeConfiguration:
dictionaryWithContentsOfFile:
initWithPlistJSONDictionary:
setWithObject:
fileURLWithPath:isDirectory:
initWithSuites:resourceBaseURL:
initWithConfiguration:overrides:overrideConfigFiles:generalVoc:lexiconEnh:itnEnh:language:activeConfiguration:modelLoadingOptions:enableSpeakerCodeTraining:supportEmojiRecognition:voiceCommandActiveSet:modelContextDelegate:
initWithConfiguration:overrides:overrideConfigFiles:generalVoc:lexiconEnh:itnEnh:language:activeConfiguration:modelLoadingOptions:enableSpeakerCodeTraining:supportEmojiRecognition:voiceCommandActiveSet:
initWithConfiguration:overrides:overrideConfigFiles:language:activeConfiguration:modelLoadingOptions:enableSpeakerCodeTraining:supportEmojiRecognition:voiceCommandActiveSet:modelContextDelegate:
initWithConfiguration:overrides:overrideConfigFiles:language:activeConfiguration:modelLoadingOptions:enableSpeakerCodeTraining:supportEmojiRecognition:voiceCommandActiveSet:
setHighPriority:
setDetectUtterances:
setRecognizeEagerCandidates:
setConcatenateUtterances:
logLocalRecognitionLoadedForLanguage:duration:
preheatSpeechRecognitionWithAssetConfig:modelOverrideURL:
_clearPendingAnalyticsEvents
_addPendingAnalyticsEvent:
_clearPendingSelfEvents
_addPendingSelfPreheatEvent:
_scheduleCooldownTimer
modelVersion
startSpeechRecognitionWithParameters:didStartHandlerWithInfo:
_sendPendingAnalyticsEvents
startRequestActivityWithCompletion:
overrides
continuousListening
shouldHandleCapitalization
modelOverrideURL
task
initWithLanguage:task:
location
isSpeakerCodeTrainingSupported:
interruptTraining
isEqualToDictionary:
modelInfo
_delegate
speechServiceDidSelectRecognitionModelWithModelProperties:
isSpeechAPIRequest
speechProfileDataLastModifiedDataForLanguage:
timeIntervalSinceNow
defaultStore
variantNameWithError:
dictationUIInteractionIdentifier
requestIdentifier
censorSpeech
setRecognitionReplacements:
setRecognitionConfidenceSubtraction:
disableDeliveringAsrFeatures
endpointStart
setEndpointStart:
profile
setUserProfileData:
setUserProfile:
_modelRootWithAssetConfig:modelOverridePath:overrides:error:
jitGrammar
initWithConfiguration:taskName:applicationName:
_userProfileWithAssetConfig:modelOverridePath:overrides:foundPath:isJit:error:
createInlineLmeUserDataForContextStrings:
dataProfile
contextualData
fetchNamedEntitiesWithTimeInterval:
createInlineLmeUserDataForContextData:speechProfile:
setJitProfileData:
inputOrigin
setInputOrigin:
setExtraLmList:
detectUtterances
deliverEagerPackage
maximumRecognitionDuration
setMaximumRecognitionDuration:
farField
setFarField:
setAllowUtteranceDelay:
setFormatAcrossUtterances:
enableAutoPunctuation
setDisableAutoPunctuation:
enableEmojiRecognition
setRecognizeEmoji:
prefixText
setLeftContextText:
setRightContext:
postfixText
setSelectedText:
selectedText
enableVoiceCommands
setEnableVoiceCommands:
narrowband
codec
activeConfiguration
farFieldFilter
setByAddingObject:
samplingRateFilter
taskTypeFilter
setActiveConfiguration:
runRecognitionWithResultStream:speakerCodeWriter:language:task:samplingRate:
sharedManager
isRequestSelectedForSamplingFromConfigForLanguage:
updateRequestCountWithFlag:
shouldStoreAudioOnDevice
isRequestSelectedForSamplingForTask:
secureOfflineOnly
siriDataSharingOptedIn
loggingContext
UUIDString
recordWithLanguage:task:context:narrowband:farField:interactionIdentifier:asrSelfComponentIdentifier:pluginId:
setProfile:
originalAudioFileURL
setOriginalAudioFileURL:
shouldWriteDictationRecord:
recordWithLanguage:task:context:narrowband:farField:interactionIdentifier:asrSelfComponentIdentifier:pluginId:frequency:
speakerCodeInfo
inferenceSpeakerCode
numFrames
nnetVersion
isSpeakerCodeUsed
initWithUUIDString:language:task:codec:samplingRate:inferenceSpeakerCode:numTrainedFrames:trainingNnetVersion:isSpeakerCodeUsed:isSamplingForDictation:selfLogger:
bytes
addAudioSamples:count:
addAudioPacket:
updateAudioDuration:
endAudio
_cancelCooldownTimer
correctedText
rangeOfString:
initWithExplicitlyRejectedString:
initWithImplicitlyEngagedString:
setClientIdentifier:
registerFeedback:completion:
interactionIdentifier
setCorrectedText:
_writeDESRecord:oneRecordPerDay:
_writeDESRecord:
offlineDictationProfileOverridePath
dictionaryWithContentsProfilePathForLanguage:errorOut:
objectEnumerator
nextObject
integerValue
readUserProfile:
peopleSuggesterConfig
contactsCount
setPeopleSuggesterContactsCount:
bestContactsCount
setBestPeopleSuggesterContactsCount:
bestContactsBonus
setBestPeopleSuggesterContactsBonus:
_cooldownTimerFired
_cooldownTimerTimeoutSeconds
_cachedRecognizerCleanUp
releaseClients
sharedAnalytics
logEvents:
runDefaultAdaptationEvaluation:recordData:attachments:completion:
runRecipeEvaluationDistributedEvaluation:recordData:attachments:completion:
stringWithContentsOfURL:encoding:error:
whitespaceAndNewlineCharacterSet
substringToIndex:
substringFromIndex:
URLByAppendingPathComponent:
readTableFromURL:
_speechRecognizerWithAssetConfig:enableITN:isHighPriority:error:
_deleteTemporaryDirectoryIfExists:
recognitionResultsWithAudioData:userProfileData:language:task:samplingRate:extraLanguageModel:
regularExpressionWithPattern:options:error:
firstMatchInString:options:range:
range
numberWithFloat:
readProfileAndUserDataWithLanguage:allowOverride:completion:
initForReadingFromData:error:
setClass:forClassName:
decodeObjectOfClass:forKey:
finishDecoding
samplingRate
audioPackets
appendData:
userData
fileURLWithPath:
createDirectoryAtURL:withIntermediateDirectories:attributes:error:
writeToURL:atomically:encoding:error:
name
callStackReturnAddresses
callStackSymbols
reason
initWithDomain:code:userInfo:
_fidesEvalQueue
initWithNcsRoot:
_invalidated
recordFromData:
recognizedText
tokenize:
removeObjectAtIndex:
timestamp
asrSelfComponentIdentifier
initWithConfiguration:language:overrides:sdapiOverrides:generalVoc:emptyVoc:pgVoc:lexiconEnh:tokenEnh:paramsetHolder:isJit:
initWithConfiguration:overrides:
initWithContentsOfFile:options:error:
concatenatedAudioPackets
setScoreNbest:
setScoreNbestExtraLmList:
setEnableSpeakerCodeTraining:
recognitionUtterenceStatistics
recognitionUtteranceInfos
removeItemAtURL:error:
compileRecognizerModelsWithConfiguration:
loadConfigs
deleteAllRecordsForPlugin:completion:
_userProfileConfigWithAssetConfig:modelOverridePath:overrides:error:
pauseRecognition
resumeRecognitionWithLeftContext:rightContext:selectedText:
hasData
saveOneRecordPerDay
save
pluginId
setUserData:
_addPendingSelfANECompilationEvent:
speechServiceDidProduceEndpointFeaturesWithWordCount:trailingSilenceDuration:eosLikelihood:pauseCounts:silencePosterior:processedAudioDurationInMilliseconds:
speechServiceDidProduceLoggablePackage:
rawRecognition
phrases
hasSuffix:
setRecognizedText:
unrepairedRecognition
packetArrivalTimestampFromAudioTime:
speechServiceDidRecognizeTokens:withMetadata:
speechServiceDidRecognizeTokens:
raise:format:
setRemoveSpaceAfter:
initWithInterpretations:isLowConfidence:
concatenateUtterances
dummyResultPackage:
speechServiceDidRecognizePackage:withMetadata:
speechServiceDidRecognizePackage:
numberWithUnsignedInt:
statusForError:
recognitionStatistics
speechServiceDidFinishRecognitionWithStatistics:error:
recognitionMetrics
pauseDurations
valueForKeyPath:
itnDurationInNs
isEmojiPersonalizationUsed
isEmojiDisambiguationUsed
isEmojiExpectedButNotRecognized
recognizedEmojis
decrementRequestCount
saveAudioToDisk
speechServiceDidProcessAudioDuration:
formattedRecognitionWithNBestList:
speechServiceDidRecognizeRawEagerRecognitionCandidate:
getFormatterWithBlock:
initialize
prepareToExit
didStartModelInitializing:
didFinishModelInitializing:
getRecognizerQueue
speechRecognizer:didRecognizePartialResult:
speechRecognizer:didFinishRecognitionWithError:
speechRecognizer:didRecognizeFinalResults:
speechRecognizer:didRecognizePartialResultPackage:
speechRecognizer:didRecognizeFinalResults:tokenSausage:nBestChoices:
speechRecognizer:didRecognizeFinalResultPackage:
speechRecognizer:didProcessAudioDuration:
speechRecognizer:didRecognizeRawEagerRecognitionCandidate:
speechRecognizer:didProduceEndpointFeaturesWithWordCount:trailingSilenceDuration:eosLikelihood:pauseCounts:silencePosterior:processedAudioDurationInMilliseconds:
speechRecognizer:didRecognizePartialResultNbest:
speechRecognizer:didProduceLoggablePackage:
resetCacheAndCompileAllAssetsWithCompletion:
preheatSpeechRecognitionWithLanguage:modelOverrideURL:
startSpeechRecognitionWithParameters:didStartHandler:
finishAudio
createSpeechProfileWithLanguage:modelOverridePath:JSONData:completion:
updateSpeechProfileWithLanguage:modelOverridePath:existingProfile:existingAssetPath:completion:
getOfflineDictationStatusIgnoringCache:withCompletion:
fetchAssetsForLanguage:completion:
fetchModelPropertiesForAssetConfig:completion:
fetchUserDataForLanguage:completion:
runAdaptationRecipeEvaluation:recordData:attachments:completion:
runCorrectedTextEvaluationWithAudioDatas:recordDatas:language:samplingRate:caseSensitive:skipLME:wordSenseAccessListSet:completion:
purgeInstalledAssetsExceptLanguages:completion:
purgeInstalledAssetsExceptLanguages:assetType:completion:
setAssetsPurgeabilityExceptLanguages:assetType:
writeDESRecord
sendSpeechCorrectionInfo:interactionIdentifier:
invalidatePersonalizedLM
removePersonalizedLMForFidesOnly:completion:
runEvaluationWithDESRecordDatas:language:recipe:attachments:fidesPersonalizedLMPath:fidesPersonalizedLMTrainingAsset:scrubResult:completion:
deleteAllDESRecordsForDictationPersonalizationWithCompletion:
invalidateUaapLm
resumeRecognitionWithPrefixText:postfixText:selectedText:
_recognizer
_audioBuffer
_shouldStoreDictationAudioOnDevice
_disableDeliveringAsrFeatures
_isHighPriority
_lastRecognizedPackage
_lastRecognizedMetadataPackage
_bufferedAudioPackets
_bufferedAudioEnded
_validDomains
_requestCompletion
_storeAudioData
_biomeRecord
_selfHelper
_samplingRate
_audioDurationMs
_processedAudioDuration
_firstAudioPacketReceivedTime
_firstAudioPacketTimeUntilFirstPartial
_lastAudioPacketReceivedTime
_firstAudioPacketReceivedTimeInTicks
_lastAudioPacketReceivedTimeInTicks
_firstAudioPacketProcessedTime
_localMetrics
_recognitionBeginTime
_recognitionAbsoluteEndTime
_speakerCodeWriter
_weakFidesRecognizer
_lastWordCount
_taskToUse
_desRecord
_desRecordDictation
predicateWithFormat:
filteredArrayUsingPredicate:
fileSystemRepresentation
stringWithUTF8String:
tasks
deviceIdFilter
bluetoothDeviceIdFilter
aneContextFilter
cpuContextFilter
gpuContextFilter
assetWithURL:
tracksWithMediaType:
assetReaderWithAsset:error:
assetReaderAudioMixOutputWithAudioTracks:audioSettings:
canAddOutput:
addOutput:
startReading
copyNextSampleBuffer
dataWithBytes:length:
setObject:atIndexedSubscript:
lowercaseString
stringByReplacingMatchesInString:options:range:withTemplate:
reverseObjectEnumerator
allObjects
lastPathComponent
metrics
compressedDataUsingAlgorithm:error:
base64EncodedStringWithOptions:
setUUIDString:
setLanguage:
component
_createAudioFilePath
_saveAudioToCache:
_moveAudioToVarMobile:
_logAudioSampledEventsWithStatus:error:customReasonForFailure:
_cleanupCacheAndReset:
deleteItemAtFilePath:
_deleteItemAtPath:
cleanupCacheAndReset
writeToFile:options:error:
pathComponents
samplingDateAsString
createSamplingDirectory
moveItemAtPath:toPath:error:
stringByDeletingPathExtension
_saveAudioMetadataToFilePath:
writeToFile:atomically:
_createCachesDirectoryIfItDoesNotExist
sampledCachesSubDirectoryPath
initWithDictionary:
localizedDescription
logEventWithType:context:
_trimAudioIfNeeded:
setCodec:
setSamplingRate:
setAudioPackets:
setHasRecognizedAnything:
numTrainedFrames
trainingNnetVersion
isSamplingForDictation
selfLogger
setSelfLogger:
audioMetadata
setAudioMetadata:
collectedAudioDurationMS
setCollectedAudioDurationMS:
currentAudioFilePath
setCurrentAudioFilePath:
logPrefix
setLogPrefix:
_isSpeakerCodeUsed
_isSamplingForDictation
_UUIDString
_task
_codec
_audioPackets
_inferenceSpeakerCode
_numTrainedFrames
_trainingNnetVersion
_selfLogger
_audioMetadata
_collectedAudioDurationMS
_currentAudioFilePath
_logPrefix
T@"NSString",C,N,V_UUIDString
T@"NSString",C,N,V_language
T@"NSString",C,N,V_task
T@"NSString",&,N,V_codec
TQ,N,V_samplingRate
T@"NSMutableData",&,N,V_audioPackets
TB,N,V_hasRecognizedAnything
T@"NSString",R,N,V_inferenceSpeakerCode
T@"NSNumber",R,N,V_numTrainedFrames
T@"NSNumber",R,N,V_trainingNnetVersion
TB,R,N,V_isSpeakerCodeUsed
TB,R,N,V_isSamplingForDictation
T@"ESSelfHelper",&,N,V_selfLogger
T@"NSMutableDictionary",&,N,V_audioMetadata
Td,N,V_collectedAudioDurationMS
T@"NSString",C,N,V_currentAudioFilePath
T@"NSString",C,N,V_logPrefix
initWithUUIDBytes:
valueForEntitlement:
setExportedInterface:
setExportedObject:
setRemoteObjectInterface:
resume
listener:shouldAcceptNewConnection:
enableTransactions
serviceListener
setDelegate:
%s SELF: Logging object created successfully (logging allowed for current request). asrId=%@, recognitionTask=%@, isSpeechAPIRequest=%@, isHipaaCompliant=%@, siriOptInStatus=%@, isTier1LoggingAllowed=%@
%s SELF: Logging disabled because it is not allowed for the current request. recognitionTask=%@, isSpeechAPIRequest=%@
%s SELF: Correct Partial Result Index List is %s, Silence Start Time List is %s
%s SELF: Encountered malformed string during SELF logging for recognizer components in speech results from recognizer. String: (%@)
%s SELF: Expected three recognizer components separated by delimiter '::'. Ex: 'dnn-rfdnn-aa-cache::dnn-lazy-16k-rfdnn-dictation::msg'
%s SELF: Encountered malformed string during SELF logging for interpolation weights in speech results from recognizer. String: %@
%s SELF: Expected interpolation weight sets separated by delimiter ';' - starting with a set of weights delimited by ',' and ending the with start/end times delimited by ':'. Ex: '0.999646,0.000354:0:4280;0.947514,0.000158:0:3859'
%s SELF: Logging ASRRequestContext->failed in SELF based on error result from recognizer.
%s SELF: Logging ASRRequestContext->cancelled with reason RECOGNITION_CANCELLED in SELF based on error result from recognizer.
%s SELF: Logging ASRRequestContext->cancelled with reason RECOGNITION_REJECTED in SELF based on error result from recognizer.
%s SELF: Logging ASRRequestContext->cancelled with reason RECOGNITION_REJECTED in SELF because nothing was recognized (SpeechNoMatch).
%s SELF: Logging ASRRequestContext->ended in SELF based on success status from recognizer.
%s SELF: Failed trying to wrap and emit top-level ASR event because event type was not mapped to loggable message type in the ASR SELF schema.
%s SELF: Wrapping and logging an event of type %@
%s %@ cancelling instance %@
%s Failed to delete category %@: %@. %@
%s %@
%s Internal inconsistency error: KVItems list and corresponding isBoosted booleans list are out of sync. This batch of items will default to not being boosted.
%s %@.
%s Starting to process KVItems into CESRUserData.
%s Created CESRUserData from KVItems.
%s %@: %@
%s Adapted profile using CESRUserData according to personalization recipe.
%s Speech profile updated successfully. Wrote %lu bytes to %@
%s Created _EARProfileBuilder from asset config.
%s Existing profile found at %@. It will be reused when updating the profile.
%s Could not locate asset: %@
%s Could not read personalization.json: %@
%s Could not parse personalization.json: %@
%s Set model root to %@
%s Use currently installed asset.
%s Recipe has invalid json for "%{public}@"
%s Recipe has invalid tagName for "%{public}@": "%{public}@"
%s Error executing recipe for domain %{public}@
%s Recipe for %{public}@ is missing "%{public}@"
%s Recipe for %{public}@ contains parameter %{public}@, expected type %{public}@ but got %{public}@
%s User Profile: Starting AddWordsToUserProfile
%s Using name frequencies adaption for names: %@ into slot %@ for template %@
%s Ignoring name part "%@" because it is too short (minimum length is %lu)
%s Using keyboardLMAdaptation adaption for names %@ into slot %@ for template %@
%s Ignoring keyboardDynamicVocabularyItem "%@" because it is too short (minimum length is %lu)
%s Using locationOfInterestNameAdaptationFrequency adaption for names %@ into slot %@ for template %@
%s Ignoring locationOfInterestName "%@" because it is too short (minimum length is %lu)
%s Using eventLocationNameAdaptationFrequency adaption for names %@ into slot %@ for template %@
%s Ignoring calendar event word "%@" because it is too short (minimum length is %lu)
%s Root directories for new type of speech profile: %{private}@
%s speechProfilePathsWithLanguage was incorrectly called with language=nil
%s Mapped language=nil
%s loadSpeechProfiles was incorrectly called with profiles=nil
%s Reused new type of speech profile: path=%{private}@
%s Loaded new type of speech profile: path=%{private}@ profile=%d
%s Count of command interpretation sets does not match count of speech recognition results
%s AFSpeechLatticeMitigatorResult Score = %f, Threshold = %f
%s AFSpeechLatticeMitigatorResult nil
%s Purging compiled assets if there are any.
%s Previously installed asset has been removed: %{public}@
%s Installed asset is corrupt! Triggering emergency purge %{public}@
%s Failed to register for assistant asset update notifications.
%s Failed to register for dictation asset update notifications.
%s Installation status for languages (ignoring cache: %@)
%s Trial asset delivery is enabled!
%s Invalidating installation status cache for %lu
%s Language installation status query failed: %@
%s MobileAsset query failed: %ld
%s Found assets: %@
%s Using ASR Trial assets at %@
%s No assets available for language: %@ asset type: %{public}@
%s Purging Trial assets failed: %@
%s Purging outdated assets.
%s Error purging (%@): %ld
%s Purged old asset %@
%s Just ignoring %@
%s Purging asset: %@, language %@
%s Purging failed: %lu
%s Checking for missing assets.
%s Purging all assistant ASR assets except for %@
%s Purging all assistant ASR assets
%s Encountered error trying to purge unused assistant ASR assets: %@
%s Trial asset delivery disabled for assistant assets. Bailing out of missing asset check.
%s Hit error querying installation status of MobileAssets: %@
%s Purging assets from MobileAssets for locale: %@
%s Hit error purging assets from MobileAssets: %d %@
%s Trial asset delivery disabled for dictation assets. Bailing out of missing asset check.
%s Purging all legacy dictation ASR assets
%s Encountered error trying to purge legacy dictation ASR assets: %@
%s Trial asset delivery explicitly disabled!
%s Trial asset delivery explicitly enabled!
%s Hammer model info=%@
%s Exception thrown while reading hammer config
%s GeoLM: region mapping json file=%@
%s GeoLM: region mapping json file is nil Or there is no regionMapping for given language=%@
%s GeoLM: For the given location, selected regionId=%@
%s GeoLM: location is nil.
%s GeoLM: region specific [%@] geo-config json file=%@
%s GeoLM: geoLM asset exisits on device, but not compatible. Deleting...
%s GeoLM: Exception thrown while reading geo-config json
%s GeoLM: region specific asset is not found for given language=%@ regionId=%@
%s GeoLM: model-info.version doesn't match. mainASRModelInfo.version=%@ geoLMModelInfo.version=%@ mainAssetConfig=%@ geoAssetConfig=%@
%s GeoLM: Exception thrown while reading json configs
%s GeoLM: language is nil. Skipping.
%s GeoLM: regionIdToBePurged: %@, lastWhenUsed: %ld days ago
%s GeoLM: regionIdToBePurged: %@, _geoLMAssetsInfoDict count: %ld
%s GeoLM: supportedLanguages count:%ld
%s GeoLM: Going to delete: %@
%s ASR: Using high priority configuration.
%s %@ deallocating
%s Failing to fetch assets for nil language
%s Could not get offline language for fetch fallback: %{public}@
%s Fell back asset fetch from %{public}@ to %{public}@
%s Failed to fall back asset fetch from %{public}@ to %{public}@, got %{public}@
%s Can't get user data options: %{public}@
%s Unable to serialize user profile to JSON data: %{public}@
%s Could not get installed offline language statuses: %{public}@
%s ASR: enable parallel loading
%s ASR: taskForMemoryLock: %@
%s Override json files=%@
%s Unable to locate or read dictation voice commands assets
%s modelRoot: %@
%s Failed to create recognizer from %{public}@
%s EmbeddedSpeechMetric: Created recognizer in %lf sec from %@
%s %s
ondevice_CreateRecognizer
%s Skipping preheat for %@; recognizer already loaded
%s Preheated for language %{public}@, asset type %{public}@, regionId %@%{public}@
%s Could not preheat for language %{public}@, asset type %{public}@, regionId %@%{public}@: %@
%s dictationCapable=%d task=%@ aneCapable=%d
%s Starting
%s Recognizer is busy
%s Cached recognizer for language %{public}@, asset type %{public}@, regionId %@ already  loaded
%s Cached recognizer is for language %{public}@, asset type %{public}@, regionId %@,  requesting recognizer for language %{public}@, asset type %{public}@, regionId %@
%s No cached recognizer.
%s EndpointStart > 0 but asr features delivery is disabled!
%s EndpointStart < 0
%s Setting new profile: %d, old profile: %d
%s Injecting contextual data to recognizer
%s Built inline LME from contextual data, size: %zu
%s Injected %lu jit strings or contextual data to recognizer
%s Failed to build jitData for jitGrammar or contextual data
%s Failed to initialize jit profile builder due to error : %@
%s Failed to get model root, error: %@
%s Duration spent in adding jit strings = %{public}lfs
%s Set inputOrigin to: %@
%s Switching off UC/UD for this request
%s Changing active configuration from 
%@ to 
%s Create DES record
%s Cancelling delayedBlock
%s Create DES record for Dictation with interactionId=%@
%s Saving profile snapshot: %lu bytes
%s _storeAudioData should be nil. Critical Error. Please check.
%s Received correctedText, interactionId: %@, correctedText: %@
%s Sending dictation feedback to Portrait based on correction
%s Interaction identifier did not match the DES record in memory
First Audio Packet
ES: First Audio Packet
%s Using override profile at %@
%s Could not use override profile at %@: %@
%s Deserialization of existing speech profile failed: %{public}@
%s Mismatch in speech profile language in content (%{public}@) and filename (%{public}@)
%s Profile version on disk (%{public}@) does not match the expected version (%{public}@)
%s Successfully deserialized existing speech profile for %@
%s Creating profile for %@
%s Re-using existing profile data because the asset (%@) is unchanged
%s Ignoring existing profile data because the asset changed (%@ to %@)
%s Cancelling profile update for %@ due to invalidation
%s Got no data from CESRUserData
%s Skipping profile update for %@ because user data has not actually changed
%s Created new profile with %ld bytes (was %ld bytes)
%s On-Device ASR: Cooldown is disabled.
%s Acquired os_transaction for cooldown start
%s On-Device ASR: Cooldown scheduled for %zus.
%s Cooldown timer triggered TRIClient release
%s Cooldown timer triggered asset purge
%s embeddedspeech process launch triggered asset purge
%s Received termination signal. Cleaning up immediately
%s Sending %lu events
%s Unable to load the contents of file %@: %@
%s Invalid file format
%s Running distributed evaluation for ASR
%s No attachments given, cannot run distributed evaluation
%s Failed to extract test set: %@
%s Cannot initialize recognizer for locale: %@ task: %@
%s Loaded speech profile
%s Unable to load speech profile
%s Test set contains more utterances than allowed, only running %d utterances
%s Unable to load audio file %@
%s Unable to find reference transcriptions for %@
%s Recipe has no profile
%s Stop adaption recipe. Audio file not readable. Voicemail has been deleted by user
%s Read %lu bytes from audio file
%s Using on device personalization recipe for baseline
%s Recognizer doesn't support the task %{public}@
%s Couldn't create create path for temporary confidence model overrides at %@
%s Couldn't write data to temporary confidence model file at path %@
%s Could not make baseline results
%s Failed to extract quasar model: %@
%s No recognizer created for custom model: %@
%s Could not make results with custom model
%s Profile overrides failed
%s Ignoring malformed overrides: %{public}@
%s Recipe has no recognizer
%s Could not make adapted results
%s Exception evaluating recipe: %@
%s Unknown exception evaluating recipe
%s No tokenizer for %@
%s Interrupted corrected text evaluation redecoding
%s Examining localSpeechDESRecord: %@
%s Unable to load localSpeechDESRecord
%s No audio data provided for UUID %@
%s Recognition result %@, %lu
%s Edit distance between tts ASR and original ASR %@
%s correctedOutput: %@, recognizedOutput %@
%s %{public}@
%s Unknown evaluation name found in alignmentReferences: %@
%s No modelRoot for %@: %@
%s Failed to load old type of speech profile. Trying new type.
%s Loaded speech profile: %lu bytes
%s Unable to load audio
%s Recognizer doesn't support the task %{public}@: %@
%s Interrupted evaluation redecoding
%s Running recognition for evalName: %@
%s Failed to get override files, error: %@
%s Creating recognizer with overrides: %@
%s Using Personalized LM
%s Not using Personalized LM
%s Unable to restore speech profile
%s Using JIT LME
%s JIT LME: Injecting JIT data, jitStats: %@
%s JIT LME: Error fetching JIT data, error: %@
%s No results for evalName %@: %@
%s Tokenizing correctedText
%s Interrupted evaluation tokenization
%s Computing alignments
%s Failed to delete temporary directory: %@
%s Error when getting dictation language status: %@
%s Starting to compile dictation language: %@
%s Error when compiling dictation language model: %@
%s Starting to compile assistant language: %@
%s Error when compiling assistant language model: %@
%s Error when getting assistant language status: %@
%s Failed to fetch user data options
%s No DES record, nothing to write
%s Not saving DES Record with no data or recognition
%s Fetch user data for language: %@
%s Can't get user data options for DES record language: %@, not writing record
%s Got nil user data for DES record language: %@, not writing record
%s wordCount = %ld, trailingSilenceDuration = %ld, eosLikelihood = %f, pauseCounts = %@, silencePosterior = %f
%s Setting recognized text
%s %lu results
%s EmbeddedSpeechMetric: first audio packet to first partial result = %lf secs
Words recognized: %ld
ES: Partial Recognition
%s AFSpeechInfo Package %@
%s Recognition finished with status %@
%s Audio finish to recognizer finish = %lf sec, connection is %@, error %@
%s _recognitionBeginTime (%@) is greater than _recognitionEndTime (%@)
%s Local speech recognition completed without error, write DES record when needed
%s Writing DES record after 30 seconds delay: interactionId=%@
%s Submitted delayedBlock to dispatch_after
%s #ASR on device eager formatted recognition candidate: %@
%s raw eager recognition candidate: %@
%s Could not make temporary attachment directory at %@: %@
%s Failed to specify compression algorithm: %s
%s Failed to specify format: %s
%s Start extracting archive at path: %s
%s Failed to open archive for reading: %s
%s Entry extraction path: %@
%s Unable to extract file to: %@
%s Finished extracting archive to: %@
%s Could not read %@: %@
%s Could not parse %@: %@
%s %@ is wrong type: %@
%s %@ contains bogus key/value pair: %@ => %@
%s VoiceMail asset could not be read: %@
%s Could not create asset reader output
%s Cannot add output
%s Could not get data pointer: %d
%s JSON serialization failed: %@
%s Compression failed: %@
%s Sampling: Error while initializing ESStoreAudioData because uuid is invalid.
%s %@ Sampling: Won't save audio because - has not recognized anything or has no data.
%s %@ Sampling: Won't save audio because - _currentAudioFilePath is null
%s %@ Sampling: invalid filePath or it is null.
%s %@ Sampling: Done with cleanup of audioFile=%@ and reset of variables.
%s %@ Sampling: Failed to save audio to cache dir. Error: %@
%s %@ Sampling: Successfully saved audio file to cache dir, path=%@
%s %@ Sampling: audioFileToBeMoved is nil
%s %@ Sampling: currentSamplingDate is nil
%s %@ Sampling: Error while creating Sampled directory in /var/mobile
%s %@ Sampling: Error while creating dated Sampled directory in %@ with date - %@
%s %@ Sampling: Error while moving file from cache directory to var/mobile/Library - %@
%s %@ Sampling: Successfully moved audio file to var/mobile/Library dir, path=%@
%s %@ Sampling: Error while writing audio metadata dict to plist - %@
%s %@ Sampling: currentSamplingDateString is null
%s Received SIGTERM. Cleaning and Exiting
ESSelfHelper
Timestamp
ESSelfPreheatWithPowerContainer
ESSpeechProfileBuilderConnection
CESRSpeechProfileBuilderService
NSObject
ESBiomeRecord
CESRUtilitiesAdditions
CESRUtilities
ESAdditions
ESAssetManager
ESAlignmentState
NSCopying
ESConnectionModelInfo
ESConnection
_EARSpeechRecognitionResultStream
CESRSpeechService
ESStoreAudioData
ESListenerDelegate
NSXPCListenerDelegate
@16@0:8
@20@0:8B16
@24@0:8@16
v16@0:8
@28@0:8@16B24
B28@0:8@16B24
B24@0:8@16
v24@0:8@16
v32@0:8@16@24
v92@0:8@16@24@32@40@48@56B64@68@76@84
v48@0:8@16@24@32@40
v148@0:8@16@24@32@40@48@56@64@72B80B84@88@96@104@112@120B128B132B136@140
v32@0:8@16q24
B16@0:8
v20@0:8B16
@"NSUUID"
@"NSString"
@"NSNumber"
@40@0:8@16@24@32
@"ASRSchemaASRPreheatContext"
@"SPIPowerLoggerSnapshot"
@"SPIEventContext"
@32@0:8@16@24
@56@0:8@16@24@32^@40^@48
v44@0:8@16@24@32B40
v40@0:8@16@24@32
@56@0:8@16#24@32@40^@48
B88@0:8@16d24@32@40@48@56Q64@72@80
B80@0:8d16@24@32@40@48Q56@64@72
#16@0:8
@24@0:8:16
@32@0:8:16@24
@40@0:8:16@24@32
B24@0:8#16
B24@0:8:16
Vv16@0:8
Q16@0:8
^{_NSZone=}16@0:8
B24@0:8@"Protocol"16
@"NSString"16@0:8
Vv48@0:8@16@24@32q40
Vv32@0:8@16@?24
Vv40@0:8@16@24@?32
Vv24@0:8@?16
Vv28@0:8B16@?20
Vv48@0:8@"NSString"16@"NSString"24@"NSString"32q40
Vv32@0:8@"NSString"16@?<v@?q@"NSError">24
Vv40@0:8@"NSDictionary"16@"NSString"24@?<v@?B@"NSError">32
Vv40@0:8@"NSArray"16@"NSArray"24@?<v@?B@"NSError">32
Vv24@0:8@?<v@?B@"NSError">16
Vv28@0:8B16@?<v@?B@"NSError">20
@24@0:8^@16
@"NSMutableArray"
@"NSMutableDictionary"
@"NSMutableSet"
@"CESRAssetConfig"
@"NSXPCConnection"
@"NSObject<OS_os_transaction>"
@"_EARUserProfile"
d16@0:8
v24@0:8d16
q24@0:8@16
@32@0:8@16d24
@36@0:8@16d24B32
@24@0:8Q16
q16@0:8
@40@0:8B16Q20B28^@32
v24@0:8Q16
@32@0:8@16^@24
@36@0:8@16^@24B32
@40@0:8@16^@24B32B36
v32@0:8@16Q24
B40@0:8@16Q24^@32
B32@0:8@16^@24
B24@0:8Q16
B32@0:8@16@24
@"NSObject<OS_dispatch_queue>"
@24@0:8^{_NSZone=}16
v24@0:8q16
@40@0:8@16B24B28^@32
@80@0:8@16@24B32@36@44B52B56B60^@64^@72
v32@0:8@16d24
v72@0:8@16q24q32d40@48d56q64
v32@0:8@"_EARSpeechRecognizer"16@"_EARSpeechRecognitionResult"24
v32@0:8@"_EARSpeechRecognizer"16@"NSError"24
v32@0:8@"_EARSpeechRecognizer"16@"NSArray"24
v32@0:8@"_EARSpeechRecognizer"16@"_EARSpeechRecognitionResultPackage"24
v48@0:8@"_EARSpeechRecognizer"16@"NSArray"24@"NSArray"32@"NSArray"40
v32@0:8@"_EARSpeechRecognizer"16d24
v32@0:8@"_EARSpeechRecognizer"16@"_EARSpeechRecognition"24
v72@0:8@"_EARSpeechRecognizer"16q24q32d40@"NSArray"48d56q64
Vv32@0:8@16@24
Vv24@0:8@16
Vv48@0:8@16@24@32@?40
Vv56@0:8@16@24@32@40@?48
Vv36@0:8B16Q20@?28
Vv40@0:8B16Q20B28@?32
Vv72@0:8@16@24@32Q40B48B52@56@?64
Vv36@0:8@16B24@?28
Vv40@0:8@16Q24@?32
Vv32@0:8@16Q24
Vv76@0:8@16@24@32@40@48@56B64@?68
Vv40@0:8@16@24@32
Vv24@0:8@?<v@?@"NSError">16
Vv32@0:8@"NSString"16@"NSURL"24
Vv32@0:8@"CESRAssetConfig"16@"NSURL"24
Vv24@0:8@?<v@?>16
Vv32@0:8@"CESRSpeechParameters"16@?<v@?@"CESRModelProperties"@"NSError">24
Vv32@0:8@"CESRSpeechParameters"16@?<v@?@"NSString"@"NSString"@"NSError">24
Vv24@0:8@"NSData"16
Vv48@0:8@"NSString"16@"NSString"24@"NSData"32@?<v@?@"NSData"@"NSError">40
Vv56@0:8@"NSString"16@"NSString"24@"NSData"32@"NSString"40@?<v@?@"NSData"@"NSString"@"NSError">48
Vv36@0:8B16Q20@?<v@?@"NSDictionary"@"NSError">28
Vv40@0:8B16Q20B28@?<v@?@"NSDictionary"@"NSError">32
Vv28@0:8B16@?<v@?@"NSDictionary"@"NSError">20
Vv32@0:8@"NSString"16@?<v@?@"NSString"@"NSError">24
Vv32@0:8@"CESRAssetConfig"16@?<v@?@"NSString"@"NSError">24
Vv32@0:8@"CESRAssetConfig"16@?<v@?@"CESRModelProperties"@"NSError">24
Vv32@0:8@"NSString"16@?<v@?@"NSData">24
Vv48@0:8@"NSDictionary"16@"NSData"24@"NSArray"32@?<v@?@"NSDictionary"@"NSData"@"NSError">40
Vv72@0:8@"NSDictionary"16@"NSDictionary"24@"NSString"32Q40B48B52@"NSSet"56@?<v@?@"NSDictionary"@"NSError">64
Vv36@0:8@"NSString"16B24@?<v@?@"NSData"@"NSString">28
Vv32@0:8@"NSSet"16@?<v@?@"NSNumber"@"NSError">24
Vv40@0:8@"NSSet"16Q24@?<v@?B@"NSError">32
Vv32@0:8@"NSSet"16Q24
Vv32@0:8@"AFSpeechCorrectionInfo"16@"NSString"24
Vv28@0:8B16@?<v@?>20
Vv76@0:8@"NSDictionary"16@"NSString"24@"NSDictionary"32@"NSArray"40@"NSString"48@"NSString"56B64@?<v@?@"NSDictionary"@"NSError">68
Vv40@0:8@"NSString"16@"NSString"24@"NSString"32
v36@0:8@16B24@?28
v48@0:8@16@24@32@?40
@60@0:8@16@24@32^@40B48^@52
@48@0:8@16@24@32^@40
v24@0:8^@16
v28@0:8^@16B24
@"_EARSpeechRecognizer"
@"_EARSpeechRecognitionAudioBuffer"
@"AFSpeechPackage"
@"AFSpeechInfoPackage"
@"NSSet"
@"ESStoreAudioData"
@"ESBiomeRecord"
@"ESSelfHelper"
@"_EARSpeakerCodeWriter"
@"CESRFidesASRRecord"
@96@0:8@16@24@32@40Q48@56@64@72B80B84@88
v40@0:8q16@24q32
@"NSMutableData"
B32@0:8@"NSXPCListener"16@"NSXPCConnection"24
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
<key>application-identifier</key>
<string>com.apple.siri.embeddedspeech</string>
<key>com.apple.CoreRoutine.LocationOfInterest</key>
<true/>
<key>com.apple.accounts.appleaccount.fullaccess</key>
<true/>
<key>com.apple.application-identifier</key>
<string>com.apple.siri.embeddedspeech</string>
<key>com.apple.coreaudio.allow-amr-decode</key>
<true/>
<key>com.apple.coreduetd.allow</key>
<true/>
<key>com.apple.coreduetd.people</key>
<true/>
<key>com.apple.developer.homekit</key>
<true/>
<key>com.apple.frontboardservices.display-layout-monitor</key>
<true/>
<key>com.apple.locationd.effective_bundle</key>
<true/>
<key>com.apple.private.DistributedEvaluation.RecordAccess-com.apple.fides.asr</key>
<true/>
<key>com.apple.private.DistributedEvaluation.RecordAccess-com.apple.siri.speech-dictation-personalization</key>
<true/>
<key>com.apple.private.assets.accessible-asset-types</key>
<array>
<string>com.apple.MobileAsset.Trial.Siri.SiriUnderstandingAsrAssistant</string>
<string>com.apple.MobileAsset.Trial.Siri.SiriUnderstandingAsrHammer</string>
<string>com.apple.MobileAsset.Trial.Siri.SiriDictationAssets</string>
<string>com.apple.MobileAsset.EmbeddedSpeech</string>
<string>com.apple.MobileAsset.EmbeddedSpeechWatch</string>
<string>com.apple.MobileAsset.EmbeddedSpeechMac</string>
</array>
<key>com.apple.private.assets.bypass-asset-types-check</key>
<true/>
<key>com.apple.private.attribution.implicitly-assumed-identity</key>
<dict>
<key>type</key>
<string>path</string>
<key>value</key>
<string>/System/Library/PrivateFrameworks/CoreEmbeddedSpeechRecognition.framework/XPCServices/com.apple.siri.embeddedspeech.xpc/com.apple.siri.embeddedspeech</string>
</dict>
<key>com.apple.private.biome.read-write</key>
<array>
<string>SiriDictation</string>
</array>
<key>com.apple.private.calendar.allow-suggestions</key>
<true/>
<key>com.apple.private.contacts</key>
<true/>
<key>com.apple.private.corerecents</key>
<true/>
<key>com.apple.private.corespotlight.internal</key>
<true/>
<key>com.apple.private.homekit</key>
<true/>
<key>com.apple.private.security.storage.SiriVocabulary</key>
<true/>
<key>com.apple.private.security.storage.SpeechPersonalizedLM</key>
<true/>
<key>com.apple.private.tcc.allow</key>
<array>
<string>kTCCServiceAddressBook</string>
<string>kTCCServiceCalendar</string>
<string>kTCCServiceWillow</string>
<string>kTCCServiceMediaLibrary</string>
</array>
<key>com.apple.proactive.PersonalizationPortrait.Config</key>
<true/>
<key>com.apple.proactive.PersonalizationPortrait.NamedEntity.readOnly</key>
<true/>
<key>com.apple.proactive.eventtracker</key>
<true/>
<key>com.apple.security.exception.files.absolute-path.read-only</key>
<array>
<string>/private/var/MobileAsset/</string>
<string>/private/var/tmp/com.apple.siri-distributed-evaluation/</string>
</array>
<key>com.apple.security.exception.mach-lookup.global-name</key>
<array>
<string>com.apple.suggestd.contacts</string>
<string>com.apple.mobileasset.autoasset</string>
</array>
<key>com.apple.security.exception.shared-preference.read-only</key>
<array>
<string>com.apple.assistant</string>
<string>com.apple.assistant.backedup</string>
<string>com.apple.assistant.support</string>
</array>
<key>com.apple.security.iokit-user-client-class</key>
<array>
<string>AGXCommandQueue</string>
<string>AGXDevice</string>
<string>AGXDeviceUserClient</string>
<string>AGXSharedUserClient</string>
<string>H11ANEInDirectPathClient</string>
<string>IOAccelContext</string>
<string>IOAccelContext2</string>
<string>IOAccelDevice</string>
<string>IOAccelDevice2</string>
<string>IOAccelSharedUserClient</string>
<string>IOAccelSharedUserClient2</string>
<string>IOAccelSubmitter2</string>
<string>IOSurfaceRootUserClient</string>
</array>
<key>com.apple.security.personal-information.addressbook</key>
<true/>
<key>com.apple.security.temporary-exception.mach-lookup.global-name</key>
<array>
<string>com.apple.triald.namespace-management</string>
</array>
<key>com.apple.siriknowledged</key>
<true/>
<key>com.apple.spotlight.search</key>
<true/>
<key>com.apple.trial.client</key>
<array>
<string>372</string>
<string>401</string>
<string>751</string>
</array>
</dict>
</plist>
mcpl
ASRPreheatContextTimestamp
ASRAppleNeuralEngineCompilationContextTimestamp
-[ESSelfHelper initWithTask:isSpeechAPIRequest:]
-[ESSelfHelper logFinalResultGeneratedWithEARPackage:]
-[ESSelfHelper logRequestEndedOrFailedOrCancelledWithError:recognizerComponents:averageActiveTokensPerFrame:languageModelInterpolationWeights:signalToNoiseRatioInDecibels:recognitionDurationInSec:audioDurationMs:eagerUsed:utteranceDetectionEnabled:utteranceConcatenationEnabled:cpuRealTimeFactor:numLmeDataStreams:phoneticMatchDecoderName:pauseDurations:itnDurationInNs:isEmojiPersonalizationUsed:isEmojiDisambiguationUsed:isEmojiExpectedButNotRecognized:recognizedEmojis:]
floatValue
-[ESSelfHelper wrapAndEmitTopLevelEvent:timestampInTicks:]
v32@?0@"NSArray"8Q16^B24
CESRProfileErrorDomain
\NT-contact
\NT-appname
\NT-correction
overrides
keyboardLM
locationOfInterest
spatialLocationOfInterest
interaction
search
calendarEvent
pexNamedEntity
frequency
charsToTrim
charsToSplit
tagName
templateName
tagNameList
minimumWordLength
com.apple.siri.embeddedspeech.profilegeneration-keepalive
-[ESSpeechProfileBuilderConnection initWithXPCConnection:]_block_invoke
v8@?0
Siri
unified_asset_namespace
Could not create profile from asset and profile configurations
-[ESSpeechProfileBuilderConnection getVersionForCategory:completion:]
Begin called while there are already active categories.
-[ESSpeechProfileBuilderConnection beginWithCategoriesAndVersions:bundleId:completion:]
Speech category %@ has already been saved
Speech category %@ is unsupported
Add called before categories have been set with Begin
-[ESSpeechProfileBuilderConnection addVocabularyItems:isBoosted:completion:]
Deserialization of KVItem failed
-[ESSpeechProfileBuilderConnection finishAndSaveProfile:completion:]
v32@?0@"NSString"8@"NSMutableArray"16^B24
Failed to write profile
-[ESSpeechProfileBuilderConnection _profileWithError:]
+[ESSpeechProfileBuilderConnection personalizationRecipeForAssetConfig:modelOverridePath:]
personalization.json
+[ESSpeechProfileBuilderConnection _userProfileWithAssetConfig:modelOverridePath:overrides:foundPath:error:]
mini.json
en_US_napg.json
dispatch.voc
vocdelta.voc
pg.voc
lexicon.enh
token_s.enh
mrec.psh
Error during _EARUserProfile initialization
v32@?0@"CESRVocabularyCategory"8@"NSSet"16^B24
\correction-first
+[ESSpeechProfileBuilderConnection _adaptRecipe:userData:profile:]_block_invoke
v32@?0@"NSString"8@"NSDictionary"16^B24
+[ESSpeechProfileBuilderConnection _parseRequiredParameter:expectedClass:domain:recipe:error:]
Missing %@ for %@
+[ESSpeechProfileBuilderConnection addWordsToUserProfile:templateName:wordArrays:]
v32@?0@"NSString"8@"NSString"16^B24
+[ESSpeechProfileBuilderConnection _runAdaptationRecipeForNamesWithFrequency:tagNames:templateName:charSetToTrim:charSetToSplit:minimumWordLength:nameFrequencies:profile:]
+[ESSpeechProfileBuilderConnection _runAdaptationRecipeForNamesWithFrequency:tagNames:templateName:charSetToTrim:charSetToSplit:minimumWordLength:nameFrequencies:profile:]_block_invoke_2
v32@?0@"NSString"8Q16^B24
v32@?0@"NSString"8@"NSNumber"16^B24
+[ESSpeechProfileBuilderConnection _runAdaptationRecipeForKeyboardLMWithFrequency:tagName:templateName:charSetToTrim:charSetToSplit:minimumWordLength:userData:profile:]
+[ESSpeechProfileBuilderConnection _runAdaptationRecipeForLocationOfInterestWithFrequency:tagName:templateName:charSetToTrim:charSetToSplit:minimumWordLength:locationOfInterestNames:profile:]
+[ESSpeechProfileBuilderConnection _runAdaptionRecipeForCalendarEventsWithFrequency:tagName:templateName:charSetToTrim:charSetToSplit:minimumWordLength:userData:profile:]
com.apple.MobileSMS
SpeechProfile
EARUserProfileContainerLoadDate
+[CESRUtilities speechProfileRootDirectories]
+[CESRUtilities speechProfilePathsWithLanguage:]
+[CESRUtilities loadSpeechProfiles:language:]
v32@?0@"_EARSpeechRecognitionToken"8Q16^B24
Param
v32@?0{_NSRange=QQ}8^B24
v32@?0@"EARVoiceCommandArgument"8Q16^B24
@"NSArray"24@?0@"NSArray"8@"NSArray"16
+[CESRUtilities AFSpeechInfoPackageForEARSpeechRecognitionResultPackage:]_block_invoke
unconstrained
reduced
avoid
AFSpeechLatticeMitigatorResultForEar
quasarModelPath
type
com.apple.trial.NamespaceUpdate.SIRI_DICTATION_ASSETS
com.apple.trial.NamespaceUpdate.SIRI_UNDERSTANDING_ASR_ASSISTANT
trial
QuasarDir
PreferOverServer
SupportsContinuousListening
SupportsOnDeviceSearch
GeoLMAssetsInfo
-[MAAsset(ESAdditions) _es_purgeSync]
Asset: content version: %@, mastered version %@, installed %@, language: %@, path: %@
Language
-[MAAsset(ESAdditions) _es_isInstalled]
com.apple.siri.embeddedspeech.ESAssetManager
v12@?0i8
-[ESAssetManager registerNotifications]
-[ESAssetManager installationStatusForLanguagesIgnoringCache:assetType:withDetailedStatus:withError:]
-[ESAssetManager installationStatusForLanguagesIgnoringCache:assetType:withDetailedStatus:withError:]_block_invoke
-[ESAssetManager _invalidateInstallationStatusCacheForAssetType:]
-[ESAssetManager _queryInstallationStatusForLanguagesWithError:]
-[ESAssetManager installedModelInfoForAssetConfig:error:triggerDownload:ignoreSpellingModel:]
+[ESAssetManager _assetQueryForLanguage:]
-[ESAssetManager purgeInstalledAssetsExceptLanguages:assetType:error:]_block_invoke
-[ESAssetManager purgeOutdatedAssets]_block_invoke
-[ESAssetManager _purgeMobileAssetsForLanguage:error:]_block_invoke
v24@?0@"MAAsset"8^B16
-[ESAssetManager startMissingAssetDownload]
-[ESAssetManager startMissingAssetDownload]_block_invoke
: %@
%@: ModelInfo=%@: AssetId=%@:
com.apple.internal.ck
disableTrialAssetDelivery
-[ESAssetManager trialAssetDeliveryEnabled:]
enableTrialAssetDelivery
trial_dictation_asset_delivery
-[ESAssetManager prepareHammerConfigFile:]
-[ESAssetManager _installedGeoLMRegionMappingForLanguage:]
-[ESAssetManager geoLMRegionIdForLanguage:location:]
-[ESAssetManager installedGeoLMRegionSpecificAssetForLanguage:regionId:mainAssetConfig:]
-[ESAssetManager _geoLMCompatibleWithMainAsset:geoAssetConfig:]
-[ESAssetManager purgeUnusedGeoLMAssetsForLangauge:]_block_invoke
q24@?0@8@16
-[ESAssetManager purgeUnusedGeoLMAssetsForLangauge:]_block_invoke_2
-[ESAssetManager _purgeUserDefaultsGeoLMAssetsInfoDictExceptLanguages:]
EnumerateInstalledAssets
confidenceThresholds
confidenceModels
wordConfidenceThreshold
utteranceConfidenceThreshold
continuousListening
shouldHandleCapitalization
QuasarModel
replicateDataPacketPersonalization
distributedEvaluation
adaptation
WallRTF
DecodeDuration
AverageActiveTokensPerFrame
lm_interp_weights
jitQueryDurationInMs
jitLmeDurationInMs
jitDataStats
version
data
language
assetPath
PERSONALINFO
com.apple.fides.asr
com.apple.siri.speech-dictation-personalization
dictation
ModelOverrideURL
TP/tantor/voice_commands
textfield-editing-suite.plist
com.apple.siri.ESConnection
-[ESConnection initWithXPCConnection:]
-[ESConnection initWithXPCConnection:]_block_invoke
-[ESConnection dealloc]
com.apple.siri.ESConnection.fidesEval
v24@?0@"NSDictionary"8@"NSError"16
-[ESConnection fetchModelInfoForAssetConfig:triggerDownload:completion:]
-[ESConnection fetchUserDataForLanguage:completion:]_block_invoke
B8@?0
-[ESConnection fetchUserDataForLanguage:completion:]_block_invoke_2
v16@?0@"CESRUserData"8
v24@?0@"CESRUserDataOptions"8@"NSError"16
-[ESConnection getOfflineAssetStatusIgnoringCache:assetType:withDetailedStatus:withCompletion:]
SiriX
enableTelemetry=YES
+[ESConnection _speechRecognizerWithAssetConfig:geoLMRegionId:enableITN:overrides:modelOverrideURL:preloadModels:enableParallelLoading:isHighPriority:geoLMLoadedOut:error:]
enableParallelLoading
ASR task for memory lock
com.apple.assistant
taskForMemoryLock
dictation_emoji_recognition
dictation_voice_commands
itn_s.enh
Failed to create recognizer from %@
AlreadyLoaded
Preheating
-[ESConnection preheatSpeechRecognitionWithAssetConfig:modelOverrideURL:]
 with CustomModelURL %@
Success
(none)
Failure
-[ESConnection shouldWriteDictationRecord:]
v24@?0@"CESRModelProperties"8@"NSError"16
-[ESConnection startSpeechRecognitionWithParameters:didStartHandlerWithInfo:]
Recognizer is busy
@"NSDictionary"8@?0
-[ESConnection sendSpeechCorrectionInfo:interactionIdentifier:]_block_invoke
\jit
-[ESConnection readProfileAndUserDataWithLanguage:allowOverride:completion:]
Not a dictionary: %@
Not an array: %@
orth
prons
freq
v32@?0@"NSString"8@"NSArray"16^B24
-[ESConnection updateSpeechProfileWithLanguage:modelOverridePath:existingProfile:existingAssetPath:completion:]
Could not build empty user profile
-[ESConnection updateSpeechProfileWithLanguage:modelOverridePath:existingProfile:existingAssetPath:completion:]_block_invoke_3
-[ESConnection updateSpeechProfileWithLanguage:modelOverridePath:existingProfile:existingAssetPath:completion:]_block_invoke
Empty user data
User data unchanged
Could not create user profile
UserData
+[ESConnection _parseRequiredParameter:expectedClass:domain:recipe:error:]
+[ESConnection _runAdaptationRecipeForNamesWithFrequency:tagNames:templateName:charSetToTrim:charSetToSplit:minimumWordLength:nameFrequencies:profile:]
+[ESConnection _runAdaptationRecipeForNamesWithFrequency:tagNames:templateName:charSetToTrim:charSetToSplit:minimumWordLength:nameFrequencies:profile:]_block_invoke_2
+[ESConnection _runAdaptationRecipeForKeyboardLMWithFrequency:tagName:templateName:charSetToTrim:charSetToSplit:minimumWordLength:userData:profile:]
+[ESConnection _runAdaptationRecipeForLocationOfInterestWithFrequency:tagName:templateName:charSetToTrim:charSetToSplit:minimumWordLength:locationOfInterestNames:profile:]
+[ESConnection _runAdaptionRecipeForCalendarEventsWithFrequency:tagName:templateName:charSetToTrim:charSetToSplit:minimumWordLength:userData:profile:]
Cooldown timeout for EAR
+[ESConnection _scheduleCooldownTimer]
com.apple.siri.embeddedspeech.cooldown-keepalive
+[ESConnection _cancelCooldownTimer]
+[ESConnection _cooldownTimerFired]
+[ESConnection _cachedRecognizerCleanUp]
+[ESConnection purgeOutdatedAssets]_block_invoke
+[ESConnection prepareToExit]
+[ESConnection _sendPendingAnalyticsEvents]
+[ESConnection _adaptRecipe:userData:profile:]_block_invoke
recipeType
-[ESConnection readTableFromURL:]
-[ESConnection runRecipeEvaluationDistributedEvaluation:recordData:attachments:completion:]
returnHypothesis
returnOverallWER
returnOverallRTF
returnPerUtteranceWER
returnPerUtteranceRTF
locale
task
sampleRate
wordSenseWhitelist
wav.scp
raw.ref
-[ESConnection runRecipeEvaluationDistributedEvaluation:recordData:attachments:completion:]_block_invoke
token
confidence
transcription
\\\S+$
EditDistance
details
modelVersion
overall-rtf
overall-wer
v24@?0@"NSData"8@"NSString"16
SiriCoreLocalSpeechUserData
-[ESConnection runDefaultAdaptationEvaluation:recordData:attachments:completion:]
Recipe has no profile
Voicemail audio file deleted by user
%@ %@
.model
Malformed overrides
ConfidenceMean
ConfidenceMin
ConfidenceMax
AlternateConfidenceMean
AlternateConfidenceMin
AlternateConfidenceMax
Baseline
CustomModel
ModelVersion
Adapted
Alignment
name
callStackReturnAddresses
callStackSymbols
reason
userInfo
(unknown C++ exception)
-[ESConnection runCorrectedTextEvaluationWithAudioDatas:recordDatas:language:samplingRate:caseSensitive:skipLME:wordSenseAccessListSet:completion:]_block_invoke
No tokenizer for %@
Interrupted corrected text evaluation during speech recognition
-[ESConnection runCorrectedTextEvaluationWithAudioDatas:recordDatas:language:samplingRate:caseSensitive:skipLME:wordSenseAccessListSet:completion:]_block_invoke_2
correctedOutput
recognizedOutput
editDistanceRecognizedCorrected
editDistanceRecognizedTTSASR
timestamp
interactionId
asrSelfComponentIdentifier
results
-[ESConnection _userProfileWithAssetConfig:modelOverridePath:overrides:foundPath:isJit:error:]
-[ESConnection _userProfileConfigWithAssetConfig:modelOverridePath:overrides:error:]
_EARUserProfileConfig initialization failed: %@ with reason: %@
-[ESConnection _modelRootWithAssetConfig:modelOverridePath:overrides:error:]
asrSelfComponentId
asset
applicationName
metrics
tokens
alignments
uttInfos
uttInfosCompressed
alignmentReferences
usePersonalizedLM
corrected
recognized
useJIT
disableAOT
contextualData
overrideFiles
restoreAOT
evaluations
-[ESConnection runEvaluationWithDESRecordDatas:language:recipe:attachments:fidesPersonalizedLMPath:fidesPersonalizedLMTrainingAsset:scrubResult:completion:]_block_invoke
Unknown evaluation name found in alignmentReferences: %@
scoreNbest
compress
-[ESConnection runEvaluationWithDESRecordDatas:language:recipe:attachments:fidesPersonalizedLMPath:fidesPersonalizedLMTrainingAsset:scrubResult:completion:]_block_invoke_3
Interrupted evaluation redecoding
-[ESConnection runEvaluationWithDESRecordDatas:language:recipe:attachments:fidesPersonalizedLMPath:fidesPersonalizedLMTrainingAsset:scrubResult:completion:]_block_invoke_2
Interrupted evaluation tokenization
-[ESConnection _deleteTemporaryDirectoryIfExists:]
-[ESConnection resetCacheAndCompileAllAssetsWithCompletion:]
-[ESConnection _fetchUserDataOptionsWithAssetConfig:modelOverridePath:overrides:completion:]_block_invoke
Failed to fetch user data options, for unknown reason
-[ESConnection pauseRecognition]
-[ESConnection resumeRecognitionWithPrefixText:postfixText:selectedText:]
-[ESConnection _writeDESRecord:oneRecordPerDay:]
-[ESConnection _writeDESRecord:oneRecordPerDay:]_block_invoke
-[ESConnection _writeDESRecord:oneRecordPerDay:]_block_invoke_2
-[ESConnection speechRecognizer:didProduceEndpointFeaturesWithWordCount:trailingSilenceDuration:eosLikelihood:pauseCounts:silencePosterior:processedAudioDurationInMilliseconds:]_block_invoke
Dictation
\entity-first
-[ESConnection speechRecognizer:didProduceLoggablePackage:]_block_invoke
-[ESConnection speechRecognizer:didRecognizePartialResult:]_block_invoke
Unsupported EAR build?
Unsupported EAR package build?
DUMMYTOKEN
-[ESConnection speechRecognizer:didRecognizeFinalResultPackage:]_block_invoke
RECOGNITION_SUCCESS
RECOGNITION_FAILED
RECOGNITION_CANCELLED
RECOGNITION_REJECTED
-[ESConnection speechRecognizer:didFinishRecognitionWithError:]_block_invoke
recognizer-components
audioDurationMs
recognitionDurationMs
@sum.self
EagerUsed
numLmeDataStreams
PM-decoder
PM-input
PM-output
tokenName
-[ESConnection speechRecognizer:didRecognizeRawEagerRecognitionCandidate:]_block_invoke_2
v16@?0@"_EARFormatter"8
-[ESConnection speechRecognizer:didRecognizeRawEagerRecognitionCandidate:]_block_invoke
lastPathComponent == %@
_ESDecompressArchiveWithURL
VoiceTriggerFidesArchive
Failed to specify compression algorithm
Failed to specify format
Failed to open file for reading
Unable to extract file to: %@
Task %@ not available for %@, supported tasks are %@
etiquette.json
ReplacementDictionaryForAssetConfig
ReplacementDictionaryForAssetConfig_block_invoke
v32@?0@"NSString"8@16^B24
voicemail_confidence_subtraction.json
sampling rate = %@
task type = %@
far field = %@
device ID = %@
Bluetooth device ID = %@
ANE context = %@
CPU context = %@
GPU context = %@
PersonalizationRecipeForAssetConfig
AddWordsToUserProfile
ReadAudioDataFromFileURL
\\\S*$
Insertions
Deletions
Substitutions
ReferenceSize
\contact-first
CONTACTFIRSTNAME
\contact-middle
CONTACTMIDDLENAME
\contact-last
CONTACTLASTNAME
\contact-nickname
\app-first
APPNAMEFIRSTNAME
\company-first
COMPANYFIRSTNAME
\interaction-first
\interaction-middle
\interaction-last
INLINEFIRSTNAME
\contact-first-derived
\contact-middle-derived
\contact-last-derived
\contact-nickname-derived
Recipe evaluation failed
Empty recognition Result
Error
UtteranceLength
NumberOfNonTerminals
WordsAboveThreshold
UtteranceAboveThreshold
Override file is not found in attachments or device
restoreJIT
configurationFile
JIT LME: JIT profile builder is not initialized
JIT LME: required configuration/file is missing
JIT LME: configuration file is not found in attachments or device
zlibCompressedJson
dictationUIInteractionIdentifier
interactionIdentifier
samplingTimestamp
codec
samplingRate
inferenceSpeakerCode
numTrainedFrames
trainingNnetVersion
isSpeakerCodeUsed
-[ESStoreAudioData initWithUUIDString:language:task:codec:samplingRate:inferenceSpeakerCode:numTrainedFrames:trainingNnetVersion:isSpeakerCodeUsed:isSamplingForDictation:selfLogger:]
-[ESStoreAudioData saveAudioToDisk]
-[ESStoreAudioData _deleteItemAtPath:]
-[ESStoreAudioData _cleanupCacheAndReset:]
-[ESStoreAudioData _saveAudioToCache:]
-[ESStoreAudioData _moveAudioToVarMobile:]
%@.plist
-[ESStoreAudioData _saveAudioMetadataToFilePath:]
-[ESStoreAudioData _createAudioFilePath]
%.0f
%@_%@_%@.pcm
unixTime
samplingDate
success
failed
errorCode
errorDomain
UNDEFINED
description
underlyingErrorCode
underlyingErrorDomain
Audio file to be moved nil
Sampling Date is nil
Unable to create sampling directory
Unable to create dated directory
status
com.apple.siri.embeddedspeech
com.apple.private.des-service
Rejecting %@, no %@ entitlement
Rejecting %@, no %@ or %@ entitlement
main_block_invoke
createInlineLmeUserDataForContextData:speechProfile:
writeToFile:options:error:
writeToFile:atomically:
writeProfileToFile:protectionClass:length:error:
whitespaceCharacterSet
whitespaceAndNewlineCharacterSet
voiceCommandInterpretations
vocabularyWords
version
variantNameWithError:
valueForKeyPath:
valueForKey:
valueForEntitlement:
utterances
utteranceStart
userInfo
userData
updateRequestCountWithFlag:
unsignedLongLongValue
unsignedIntegerValue
unrepairedRecognition
tracksWithMediaType:
tokens
tokenize:
tokenSausage
tokenName
timestamp
timeZoneWithAbbreviation:
timeIntervalSinceReferenceDate
timeIntervalSinceNow
timeIntervalSince1970
threshold
text
templateToVersion
templateName
tasks
taskTypeFilter
tagName
systemUptime
switchToNewAssetsForAssetType:
supportedLanguagesWithAssetType:
supportedCategories
substringWithRange:
substringToIndex:
substringFromIndex:
stringWithUTF8String:
stringWithFormat:
stringWithContentsOfURL:encoding:error:
stringValue
stringByTrimmingCharactersInSet:
stringByReplacingOccurrencesOfString:withString:
stringByReplacingMatchesInString:options:range:withTemplate:
stringByDeletingPathExtension
stringByDeletingLastPathComponent
stringByAppendingString:
stringByAppendingPathComponent:
stringByAppendingFormat:
state
startTime
startReading
start
standardUserDefaults
speechServiceDidSelectRecognitionModelWithModelProperties:
speechServiceDidRecognizeTokens:
speechServiceDidRecognizeRawEagerRecognitionCandidate:
speechServiceDidRecognizePackage:
speechServiceDidProduceEndpointFeaturesWithWordCount:trailingSilenceDuration:eosLikelihood:pauseCounts:silencePosterior:processedAudioDurationInMilliseconds:
speechServiceDidProcessAudioDuration:
speechServiceDidFinishRecognitionWithStatistics:error:
speechRecognitionFeatures
speechProfileDataLastModifiedDataForLanguage:
speakerCodeInfo
spatialLocationOfInterestNames
siriDataSharingOptInStatus
silenceStartTime
silenceStart
shouldStoreAudioOnDevice
shouldHandleCapitalization
sharedStream
sharedPreferences
sharedManager
sharedAnalytics
setWithObjects:
setWithObject:
setWithArray:
setWeights:
setValue:forKey:
setUuid:
setUtterances:
setUtteranceDetectionEnabled:
setUtteranceConcatenationEnabled:
setUserProfileData:
setUserProfile:
setUserId:
setUserData:
setUnderlyingErrorDomain:
setUnderlyingErrorCode:
setTokens:
setTokenSilenceStartTimeInNsLists:
setTimeZone:
setText:
setTemplateToVersion:
setTaskTypeFilter:
setTarget:
setStatus:
setStartedOrChanged:
setStarted:
setStartTimeInNs:
setStartTime:
setSpeechProfileAgeInNs:
setSource:
setSilenceStartTimeInNs:
setSilenceStartTime:
setSignalToNoiseRatioInDecibels:
setScoreNbestExtraLmList:
setScoreNbest:
setSamplingRateFilter:
setSampledAudioStorageFailureReason:
setSampledAudioFileStored:
setSampledAudioFileStorageFailed:
setRequestContext:
setRemoveSpaceBefore:
setRemoveSpaceAfter:
setRemoteObjectInterface:
setRecognizerComponents:
setRecognizedText:
setRecognizedEmojis:
setRecognizeEmoji:
setRecognizeEagerCandidates:
setRecognitionResultTier1:
setRecognitionReplacements:
setRecognitionDurationInNs:
setRecognitionConfidenceSubtraction:
setReason:
setRawRecognition:
setPunctuationText:
setProfile:
setPreheatContext:
setPostItn:
setPortraitExperimentVariantName:
setPhrases:
setPhoneticMatchOutput:
setPhoneticMatchInput:
setPhoneticMatchDecoderName:
setPhoneSequence:
setPersonalizedLanguageModelWeight:
setPersonalizedLanguageModelAgeInNs:
setPeopleSuggesterContactsCount:
setPausedAudioDurationsInNs:
setPartialResultGenerated:
setPackageGenerated:
setPackage:
setOriginalAudioFileURL:
setObject:forKeyedSubscript:
setObject:forKey:
setObject:atIndexedSubscript:
setNumLanguageModelEnrollmentDataStreams:
setModelLocale:
setMetrics:
setMaximumRecognitionDuration:
setLinkIndex:
setLinkId:
setLeftContextText:
setLanguageModelInterpolationWeights:
setLanguageModelEnrollmentContext:
setJitProfileData:
setJitLanguageModelEnrollmentEndedTier1:
setIsModifiedByAutoPunctuation:
setIsLowConfidence:
setIsHighQualityAsset:
setIsFinal:
setIsEmojiPersonalizationUsed:
setIsEmojiExpectedButNotRecognized:
setIsEmojiDisambiguationUsed:
setIsBoosted:
setIsAutoPunctuation:
setIsAfterResume:
setIpaPhoneSequence:
setInverseTextNormalizationDurationInNs:
setInvalidationHandler:
setInterruptionHandler:
setInterpretations:
setInterpretationIndices:
setIntermediateUtteranceInfoTier1:
setInputOrigin:
setInitializationContext:
setHighPriority:
setHammerVersion:
setGpuContextFilter:
setGeoLanguageModelRegion:
setGeoLanguageModelLoaded:
setFrontend:
setFormatAcrossUtterances:
setFirstAudioPacketProcessed:
setFinalResultGenerated:
setFinalAudioPacketContainingSpeechReceived:
setFarFieldFilter:
setFarField:
setFailed:
setExtraLmList:
setExportedObject:
setExportedInterface:
setExists:
setEventMetadata:
setErrorDomain:
setErrorCode:
setEndpointStart:
setEnded:
setEndTimeInNs:
setEndTime:
setEnableVoiceCommands:
setEnableSpeakerCodeTraining:
setEmojiMetrics:
setEagerEnabled:
setDisableAutoPunctuation:
setDictationUiInteractionId:
setDialogContexts:
setDeviceIdFilter:
setDetectUtterances:
setDelegate:
setDecoder:
setDecodable:
setDatapackVersion:
setCpuRealTimeFactor:
setCpuContextFilter:
setCorrectedText:
setCorrectPartialResultIndexLists:
setConfidenceScore:
setConfidence:
setConcatenateUtterances:
setComponent:
setClientIdentifier:
setClass:forClassName:
setCancelled:
setByAddingObject:
setBluetoothDeviceIdFilter:
setBestPeopleSuggesterContactsCount:
setBestPeopleSuggesterContactsBonus:
setAverageActiveTokensPerFrame:
setAudioPacketArrivalContext:
setAudioDurationInNs:
setAssetsProvisionalForAssetType:
setAssetLoadContext:
setAsrId:
setAppleNeuralEngineCompilationContext:
setAppendSpaceAfter:
setAneContextFilter:
setAllowUtteranceDelay:
setActiveConfiguration:
serviceListener
selectedText
secureOfflineOnly
searchEventValues
score
saveOneRecordPerDay
save
samplingRateFilter
samplingDateAsString
sampledCachesSubDirectoryPath
runRecognitionWithResultStream:speakerCodeWriter:language:task:samplingRate:
reverseObjectEnumerator
returnTypes:
resume
results
requestIdentifier
removeSpaceBefore
removeObjectForKey:
removeSpaceAfter
removeObjectAtIndex:
removeLmeDataForTemplateName:
removeItemAtURL:error:
removeAllWords
removeAllObjects
remoteObjectProxy
releaseClients
regularExpressionWithPattern:options:error:
registerFeedback:completion:
regionIdForLatitude:longitude:
refreshState
recordWithLanguage:task:context:narrowband:farField:interactionIdentifier:asrSelfComponentIdentifier:pluginId:frequency:
recordWithLanguage:task:context:narrowband:farField:interactionIdentifier:asrSelfComponentIdentifier:pluginId:
recordFromData:
recognizedText
recognizedEmojis
recognitionUtterenceStatistics
recognitionUtteranceInfos
recognitionStatistics
recognitionResultsWithAudioData:userProfileData:language:task:samplingRate:extraLanguageModel:
recognitionPaused
recognitionMetrics
recognition
reason
readUserProfileWithPath:reuseProfile:
readUserProfile:
rawRecognition
rangeOfString:
rangeOfCharacterFromSet:
range
raise:format:
queryMetaDataSync
purgeSync
purgeInstalledAssetForAssetType:language:regionId:error:
purgeCompiledRecognizerModelsWithConfiguration:
punctuationCharacterSet
pronunciationsForOrthography:
promoteAssetsForAssetType:
profileFilePathFromBasePath:language:userId:
profile
processInfo
presence
prependedAutoPunctuation
prefixText
predicateWithFormat:
preITNVoiceCommandInterpretations
preITNTokens
preITNRecognition
preITNNBestVoiceCommandInterpretations
postfixText
pluginId
phrases
phoneSequence
pexNamedEntityNames
peopleSuggesterConfig
pauseDurations
pathWithComponents:
pathComponents
path
packetArrivalTimestampFromAudioTime:
overrides
originalAudioFileURL
orderedSetWithArray:
oneBest
offlineDictationProfileOverridePath
objectForKeyedSubscript:
objectForKey:
objectEnumerator
objectAtIndexedSubscript:
objectAtIndex:
numberWithUnsignedLongLong:
numberWithUnsignedInteger:
numberWithUnsignedInt:
numberWithLongLong:
numberWithInteger:
numberWithFloat:
numberWithDouble:
numberWithBool:
numFrames
nnetVersion
nextObject
narrowband
name
nBestVoiceCommandInterpretations
nBest
mutableCopy
moveItemAtPath:toPath:error:
modelVersion
modelQualityTypeStatusStringWithConfig:
modelOverrideURL
modelInfo
metrics
methodForSelector:
maximumRecognitionDuration
lowercaseString
longLongValue
loggingContext
logWithEventContext:asrIdentifier:
logWithEventContext:
logLocalRecognitionLoadedForLanguage:duration:
logEvents:
logEventWithType:context:
locationOfInterestNames
location
localizedDescription
loadConfigs
linkId
length
latticeMitigatorResult
lastPathComponent
lastObject
languageCode
keysSortedByValueUsingComparator:
keyboardLMDynamicVocabularyItems
jsonFilenameForAssetType:
jitGrammar
itnDurationInNs
itemFromBuffer:error:
isSpeechAPIRequest
isSpeakerCodeTrainingSupported:
isSiriXEnabled
isRequestSelectedForSamplingFromConfigForLanguage:
isRequestSelectedForSamplingForTask:
isModifiedByAutoPunctuation
isFinal
isEqualToString:
isEqualToDictionary:
isEmojiPersonalizationUsed
isEmojiExpectedButNotRecognized
isEmojiDisambiguationUsed
isDictationHIPAACompliant
ipaPhoneSequence
invalidate
interruptTraining
interpretations
interpretationIndices
interactionSenderDisplayNames
interactionIdentifier
integerValue
instancesRespondToSelector:
installedAssetWithConfig:regionId:triggerDownload:
installedAssetWithConfig:regionId:
installedAssetWithConfig:
installationStatusForLanguagesForAssetType:includeDetailedStatus:error:
installationStatusForLanguagesForAssetType:error:
inputOrigin
initWithUtterance:parseCandidates:
initWithUUIDString:
initWithUUIDBytes:
initWithType:
initWithTokenName:start:end:silenceStart:confidence:hasSpaceAfter:hasSpaceBefore:phoneSequence:ipaPhoneSequence:appendedAutoPunctuation:
initWithText:
initWithSuites:resourceBaseURL:
initWithSuiteName:
initWithSpeechRecognitionFeatures:acousticFeatures:snr:
initWithResults:score:threshold:
initWithRecognition:unfilteredRecognition:rawRecognition:audioAnalytics:isFinal:utteranceStart:latticeMitigatorResult:recognitionPaused:
initWithPlistJSONDictionary:
initWithPhrases:utterances:processedAudioDuration:
initWithPath:error:
initWithOrthography:pronunciations:tagName:frequency:
initWithOrthography:pronunciations:tag:
initWithNcsRoot:
initWithNSUUID:
initWithNBestParses:preITNNBestParses:
initWithModelVersion:modelType:modelRoot:
initWithLanguage:task:
initWithLanguage:assetType:
initWithItems:language:
initWithInterpretations:isLowConfidence:
initWithInterpretationIndices:confidenceScore:
initWithImplicitlyEngagedString:
initWithExplicitlyRejectedString:
initWithDomain:code:userInfo:
initWithDictionary:
initWithCurrentProcess
initWithContentsOfFile:options:error:
initWithConfiguration:useQuasarFormatter:activeConfiguration:
initWithConfiguration:taskName:applicationName:
initWithConfiguration:overrides:overrideConfigFiles:language:activeConfiguration:modelLoadingOptions:enableSpeakerCodeTraining:supportEmojiRecognition:voiceCommandActiveSet:
JSONObjectWithData:options:error:
URLByAppendingPathComponent:
UTF8String
UUID
_UUID
_setQueue:
acousticFeatureValuePerFrame
acousticFeatures
activeConfiguration
activeConfigurationForEverything
addAudioSamples:count:
addEntriesFromDictionary:
addKeyValuePair:with:
addObject:
addObjectsFromArray:
addOutput:
addWordWithParts:templateName:
allKeys
allObjects
allocWithZone:
aneContextFilter
appNames
appendData:
appendString:
appendedAutoPunctuation
arguments
array
arrayByAddingObject:
arrayByAddingObjectsFromArray:
arrayWithObjects:count:
asrSelfComponentIdentifier
assetId
assetReaderAudioMixOutputWithAudioTracks:audioSettings:
assetReaderWithAsset:error:
assetType
assetWithURL:
assistantIsEnabled
attributes
attributesOfItemAtPath:error:
audioAnalytics
base64EncodedStringWithOptions:
bestContactsBonus
bestContactsCount
bluetoothDeviceIdFilter
boolValue
bytes
callStackReturnAddresses
callStackSymbols
canAddOutput:
cancelRecognition
captureSnapshot
censorSpeech
characterSetWithCharactersInString:
code
commandIdentifier
compare:
compileRecognizerModelsWithConfiguration:
component
components
components:fromDate:toDate:options:
componentsJoinedByString:
componentsSeparatedByCharactersInSet:
componentsSeparatedByString:
compressedDataUsingAlgorithm:error:
concatenateUtterances
concatenatedAudioPackets
confidence
confidenceScore
contactWordsWithFrequency
contactsCount
containsObject:
context
contextualData
continuousListening
convertLanguageCodeToSchemaLocale:
coordinate
copy
copyNextSampleBuffer
correctPartialResultIndexList
correctedText
corrections
count
countByEnumeratingWithState:objects:count:
countForObject:
cpuContextFilter
createDirectoryAtPath:withIntermediateDirectories:attributes:error:
createDirectoryAtURL:withIntermediateDirectories:attributes:error:
writeToURL:atomically:encoding:error:
createInlineLmeUserDataForContextStrings:
createSamplingDirectory
currentCalendar
data
dataProfile
dataWithBytes:length:
dataWithContentsOfFile:options:error:
dataWithJSONObject:options:error:
date
dateWithTimeIntervalSince1970:
decodeObjectOfClass:forKey:
decrementRequestCount
defaultManager
defaultStore
deleteAllRecordsForPlugin:completion:
deleteItemAtFilePath:
deliverEagerPackage
detectUtterances
deviceIdFilter
dictationIsEnabled
dictationUIInteractionIdentifier
dictionary
dictionaryForKey:
dictionaryRepresentation
dictionaryWithContentsOfFile:
dictionaryWithContentsProfilePathForLanguage:errorOut:
dictionaryWithDictionary:
dictionaryWithObjects:forKeys:count:
disableDeliveringAsrFeatures
domain
doubleValue
emitMessage:
emitMessage:timestamp:
enableAutoPunctuation
enableEmojiRecognition
enableTransactions
enableVoiceCommands
enabledDictationLocales
endAudio
endTime
endpointStart
enumerateKeysAndObjectsUsingBlock:
enumerateObjectsUsingBlock:
enumerateRangesUsingBlock:
enumeratorAtPath:
errorWithDomain:code:userInfo:
eventLocationNames
eventTitles
farField
farFieldFilter
fetchNamedEntitiesWithTimeInterval:
fetchUserDataWithLanguage:options:keepGoing:completion:
fileExistsAtPath:
fileExistsAtPath:isDirectory:
fileSystemRepresentation
fileURLWithPath:
fileURLWithPath:isDirectory:
filteredArrayUsingPredicate:
finishDecoding
firstMatchInString:options:range:
firstObject
firstResultAfterResume
floatValue
formattedRecognitionWithNBestList:
frameDuration
frequency
getFormatterWithBlock:
getLocalFileUrl
gpuContextFilter
hasSpaceAfter
hasSpaceBefore
hasSuffix:
indexOfObject:
indexes
infersQoSFromInstanceUUIDForEAR
initForReadingFromData:error:
initWithAcousticFeatureValue:frameDuration:
initWithCapacity:
initWithCommandGrammarParsePackage:
initWithCommandId:isComplete:paramMatches:
initWithConfig:
initWithConfiguration:
initWithConfiguration:language:overrides:sdapiOverrides:generalVoc:emptyVoc:pgVoc:lexiconEnh:tokenEnh:paramsetHolder:
initWithConfiguration:overrides:
initWithConfiguration:overrides:overrideConfigFiles:generalVoc:lexiconEnh:itnEnh:language:activeConfiguration:modelLoadingOptions:enableSpeakerCodeTraining:supportEmojiRecognition:voiceCommandActiveSet:
init
deleteLinkId
createPreheatStartedOrChangedEvent
createPreheatFailedEvent
createPreheatEndedEventWithPreheatAlreadyDone:
createANECompilationStartedEventWithTimeStamp:
createANECompilationEndedEventWithTimeStamp:
initializeSharedPowerLoggerIfNeeded
logPowerSnapshotForProcessStarted
logPowerSnapshotForProcessEnded
initWithTask:isSpeechAPIRequest:
_isLoggingAllowedForCurrentRequestWithTask:isSpeechAPIRequest:
_isTier1LoggingAllowedForCurrentRequestWithTask:
_isNonTier1Message:
logRequestLinkWithRequestId:
logPendingPreheatContextEvents:
logPendingANECompilationContextEvents:
logInitializationStartedOrChangedWithTimeInTicks:
logInitializationEnded
logAssetLoadStartedOrChanged
logAssetLoadEnded
logJitLmeStartedOrChangedWithTimeInTicks:
logJitLmeEndedAndEndedTier1WithDialogContext:
logAudioPacketArrivalStartedOrChangedWithTimeInTicks:
logAudioPacketArrivalEndedWithTimeInTicks:
logFirstAudioPacketProcessed
logFinalAudioPacketContainingSpeechReceivedWithTimeInTicks:loggableSharedUserId:
logRequestStartedOrChangedWithTask:modelLocale:modelVersion:isHighQualityAsset:hammerVersion:geoLanguageModelRegion:geoLanguageModelLoaded:speechProfileAgeInSec:dictationUIInteractionId:portraitExperimentVariantName:
logPartialResultGenerated
logIntermediateUtteranceInfoTier1WithPmInput:pmOutput:
logIntermediateUtteranceInfoTier1WithUnrepairedPostItn:loggableSharedUserId:
logIntermediateUtteranceInfoTier1WithPmInput:pmOutput:unrepairedPostItn:loggableSharedUserId:
logPackageGeneratedAndRecognitionResultTier1WithEARPackage:loggableSharedUserId:
logFinalResultGeneratedWithEARPackage:
logRequestEndedOrFailedOrCancelledWithError:recognizerComponents:averageActiveTokensPerFrame:languageModelInterpolationWeights:signalToNoiseRatioInDecibels:recognitionDurationInSec:audioDurationMs:eagerUsed:utteranceDetectionEnabled:utteranceConcatenationEnabled:cpuRealTimeFactor:numLmeDataStreams:phoneticMatchDecoderName:pauseDurations:itnDurationInNs:isEmojiPersonalizationUsed:isEmojiDisambiguationUsed:isEmojiExpectedButNotRecognized:recognizedEmojis:
logSampledAudioFileStoredSuccessfully
logSampledAudioFileStoredWithError:customFailureReason:
wrapAndEmitTopLevelEvent:timestampInTicks:
asrId
recognitionTask
unrepairedPostItn
setUnrepairedPostItn:
personalizedLmWeight
setPersonalizedLmWeight:
personalizedLmAgeInSec
setPersonalizedLmAgeInSec:
continuousListeningEnabled
setContinuousListeningEnabled:
.cxx_destruct
_asrId
_recognitionTask
_packageLogged
_isTier1LoggingAllowedForCurrentRequest
_continuousListeningEnabled
_unrepairedPostItn
_personalizedLmWeight
_personalizedLmAgeInSec
T@"NSUUID",R,N,V_asrId
T@"NSString",R,N,V_recognitionTask
T@"NSString",&,N,V_unrepairedPostItn
T@"NSNumber",&,N,V_personalizedLmWeight
T@"NSNumber",&,N,V_personalizedLmAgeInSec
TB,N,V_continuousListeningEnabled
setTimestampInTicks:
timestampInTicks
T@"NSNumber",&,N
initWithPreheatContext:powerSnapshot:powerEventContext:
preheatContext
powerSnapshot
powerEventContext
_preheatContext
_powerSnapshot
_powerEventContext
T@"ASRSchemaASRPreheatContext",R,N,V_preheatContext
T@"SPIPowerLoggerSnapshot",R,N,V_powerSnapshot
T@"SPIEventContext",R,N,V_powerEventContext
dealloc
signalEndOfUserData
personalizationRecipeForAssetConfig:modelOverridePath:
_userProfileWithAssetConfig:modelOverridePath:overrides:foundPath:error:
adaptUserProfileWithUserData:personalizationRecipe:userData:endOfUserData:
_adaptRecipe:userData:profile:
_parseRequiredParameter:expectedClass:domain:recipe:error:
addWordsToUserProfile:templateName:wordArrays:
_runAdaptationRecipeForDomain:frequency:tagNames:templateName:charSetToTrim:charSetToSplit:minimumWordLength:userData:profile:
_runAdaptationRecipeForNamesWithFrequency:tagNames:templateName:charSetToTrim:charSetToSplit:minimumWordLength:nameFrequencies:profile:
_runAdaptationRecipeForKeyboardLMWithFrequency:tagName:templateName:charSetToTrim:charSetToSplit:minimumWordLength:userData:profile:
_runAdaptationRecipeForLocationOfInterestWithFrequency:tagName:templateName:charSetToTrim:charSetToSplit:minimumWordLength:locationOfInterestNames:profile:
_runAdaptionRecipeForCalendarEventsWithFrequency:tagName:templateName:charSetToTrim:charSetToSplit:minimumWordLength:userData:profile:
isEqual:
class
self
performSelector:
performSelector:withObject:
performSelector:withObject:withObject:
isProxy
isKindOfClass:
isMemberOfClass:
conformsToProtocol:
respondsToSelector:
retain
release
autorelease
retainCount
zone
hash
superclass
description
debugDescription
TQ,R
T#,R
T@"NSString",R,C
setProfileConfigWithLanguage:profileDir:userId:dataProtectionClass:
getVersionForCategory:completion:
beginWithCategoriesAndVersions:bundleId:completion:
addVocabularyItems:isBoosted:completion:
cancelWithCompletion:
finishAndSaveProfile:completion:
initWithXPCConnection:
_profileWithError:
_stagedItems
_committedItems
_stagedCategoryToVersion
_committedCategoryToVersion
_seenCategories
_baseDirectory
_language
_userId
_dataProtectionClass
_assetConfig
_connection
_transaction
_profile
markRecognition
sendEvent
recognitionEndTime
setRecognitionEndTime:
applicationName
setApplicationName:
interactionId
setInteractionId:
taskName
setTaskName:
hasRecognizedAnything
_hasRecognizedAnything
_recognitionEndTime
_applicationName
_interactionId
_taskName
Td,N,V_recognitionEndTime
T@"NSString",C,N,V_applicationName
T@"NSString",C,N,V_interactionId
T@"NSString",C,N,V_taskName
TB,R,N,V_hasRecognizedAnything
loadDate
setLoadDate:
T@"NSDate",C,N
initWithRecognition:unfilteredRecognition:rawRecognition:audioAnalytics:isFinal:utteranceStart:latticeMitigatorResult:recognitionPaused:speechProfileUsed:
calculateDiffInDaysFromTimestamp:
hasRecognizedAnythingInAFSpeechPackage:
afTokensForEARTokens:removeSpaceBefore:
afRecognitionForEARSausage:processedAudioDuration:
afSpeechPackageForEARPackage:processedAudioDuration:speechProfileUsed:
speechProfileRootDirectories
speechProfilePathsWithLanguage:
loadSpeechProfiles:language:
earTokensToString:
voiceCommandsParamKeyBuilder:
afVoiceCommandGrammarParseResultForEARTokenString:withEARVoiceCommandInterpretations:
AFSpeechInfoPackageForEARSpeechRecognitionResult:
AFSpeechInfoPackageForEARSpeechRecognitionResultPackage:
earTokensForAFTokens:appendedAutoPunctuation:
mapContextOptionToString:
_es_purgeSync
_es_description
_es_language
_es_masteredVersion
_es_contentVersion
_es_path
_es_isInstalled
_es_status
_es_quasarModelPath
_es_quasarDir
_es_preferOverServer
_es_supportsContinuousListening
_es_supportsOnDeviceSearch
sharedInstance
_assetQueryForLanguage:
registerNotifications
installationStatusForLanguagesIgnoringCache:assetType:withDetailedStatus:withError:
_invalidateInstallationStatusCacheForAssetType:
_queryInstallationStatusForLanguagesWithError:
installedQuasarModelPathForAssetConfig:error:
installedQuasarModelPathForAssetConfig:error:triggerDownload:
installedQuasarModelPathForAssetConfig:error:triggerDownload:ignoreSpellingModel:
installedModelInfoForAssetConfig:error:
installedModelInfoForAssetConfig:error:triggerDownload:
installedModelInfoForAssetConfig:error:triggerDownload:ignoreSpellingModel:
prepareModelInfo:withAssetType:
promoteModelInfo:withAssetType:
isBelowLimitForLocale:
purgeInstalledAssetsExceptLanguages:assetType:error:
purgeOutdatedAssets
_purgeMobileAssetsForLanguage:error:
startMissingAssetDownload
activeDictationLanguages
modelAttributesStatusStringWithAsset:
modelTypeStatusStringAndVersionWithAsset:
trialAssetDeliveryEnabled:
installedHammerConfigFileForLanguage:
prepareHammerConfigFile:
promoteHammerConfigFile
_installedGeoLMRegionMappingForLanguage:
geoLMRegionIdForLanguage:location:
installedGeoLMRegionSpecificAssetForLanguage:regionId:mainAssetConfig:
_geoLMCompatibleWithMainAsset:geoAssetConfig:
_updateGeoLMAssetsInfoDictWithRegionId:language:
purgeUnusedGeoLMAssetsForLangauge:
_purgeUserDefaultsGeoLMAssetsInfoDictExceptLanguages:
_loadGeoLMAssetsInfoDictForLanguage:
_updateUserDefaultsWithGeoLMAssetsInfoDict:language:
_userDefaultsGeoLMAssetsInfoDictKeyForLangauge:
_queue
_languageInstallationCache
_dictationAssetUpdatedNotificationToken
_assistantAssetUpdatedNotificationToken
_recognizerAssetPathsInUse
_profileAssetPathsInUse
_geoLMAssetsInfoDict
copyWithZone:
incrementInsertions
incrementDeletions
incrementSubstitutions
incrementCost
numberOfInsertions
setNumberOfInsertions:
numberOfDeletions
setNumberOfDeletions:
numberOfSubstitutions
setNumberOfSubstitutions:
totalCost
setTotalCost:
_numberOfInsertions
_numberOfDeletions
_numberOfSubstitutions
_totalCost
Tq,N,V_numberOfInsertions
Tq,N,V_numberOfDeletions
Tq,N,V_numberOfSubstitutions
Tq,N,V_totalCost
setModelType:
modelType
setModelRoot:
modelRoot
T@"NSString",C,N
initWithConfiguration:overrides:overrideConfigFiles:generalVoc:lexiconEnh:itnEnh:language:activeConfiguration:modelLoadingOptions:enableSpeakerCodeTraining:supportEmojiRecognition:voiceCommandActiveSet:modelContextDelegate:
initWithConfiguration:overrides:overrideConfigFiles:language:activeConfiguration:modelLoadingOptions:enableSpeakerCodeTraining:supportEmojiRecognition:voiceCommandActiveSet:modelContextDelegate:
setRightContext:
setSelectedText:
initWithConfiguration:language:overrides:sdapiOverrides:generalVoc:emptyVoc:pgVoc:lexiconEnh:tokenEnh:paramsetHolder:isJit:
pauseRecognition
resumeRecognitionWithLeftContext:rightContext:selectedText:
speechServiceDidProduceLoggablePackage:
speechServiceDidRecognizeTokens:withMetadata:
speechServiceDidRecognizePackage:withMetadata:
initialize
_speechRecognizerWithAssetConfig:enableITN:isHighPriority:error:
_speechRecognizerWithAssetConfig:geoLMRegionId:enableITN:overrides:modelOverrideURL:preloadModels:enableParallelLoading:isHighPriority:geoLMLoadedOut:error:
_cooldownTimerTimeoutSeconds
_scheduleCooldownTimer
_cancelCooldownTimer
_cooldownTimerFired
_cachedRecognizerCleanUp
prepareToExit
_addPendingAnalyticsEvent:
_sendPendingAnalyticsEvents
_clearPendingAnalyticsEvents
_addPendingSelfPreheatEvent:
_addPendingSelfANECompilationEvent:
_clearPendingSelfEvents
didStartModelInitializing:
didFinishModelInitializing:
getRecognizerQueue
speechRecognizer:didRecognizePartialResult:
speechRecognizer:didFinishRecognitionWithError:
speechRecognizer:didRecognizeFinalResults:
speechRecognizer:didRecognizePartialResultPackage:
speechRecognizer:didRecognizeFinalResults:tokenSausage:nBestChoices:
speechRecognizer:didRecognizeFinalResultPackage:
speechRecognizer:didProcessAudioDuration:
speechRecognizer:didRecognizeRawEagerRecognitionCandidate:
speechRecognizer:didProduceEndpointFeaturesWithWordCount:trailingSilenceDuration:eosLikelihood:pauseCounts:silencePosterior:processedAudioDurationInMilliseconds:
speechRecognizer:didRecognizePartialResultNbest:
speechRecognizer:didProduceLoggablePackage:
resetCacheAndCompileAllAssetsWithCompletion:
preheatSpeechRecognitionWithLanguage:modelOverrideURL:
preheatSpeechRecognitionWithAssetConfig:modelOverrideURL:
startRequestActivityWithCompletion:
startSpeechRecognitionWithParameters:didStartHandlerWithInfo:
startSpeechRecognitionWithParameters:didStartHandler:
addAudioPacket:
finishAudio
createSpeechProfileWithLanguage:modelOverridePath:JSONData:completion:
updateSpeechProfileWithLanguage:modelOverridePath:existingProfile:existingAssetPath:completion:
getOfflineAssetStatusIgnoringCache:assetType:withCompletion:
getOfflineAssetStatusIgnoringCache:assetType:withDetailedStatus:withCompletion:
getOfflineDictationStatusIgnoringCache:withCompletion:
fetchAssetsForLanguage:completion:
fetchAssetsForAssetConfig:completion:
fetchModelPropertiesForAssetConfig:completion:
fetchUserDataForLanguage:completion:
runAdaptationRecipeEvaluation:recordData:attachments:completion:
runCorrectedTextEvaluationWithAudioDatas:recordDatas:language:samplingRate:caseSensitive:skipLME:wordSenseAccessListSet:completion:
readProfileAndUserDataWithLanguage:allowOverride:completion:
purgeInstalledAssetsExceptLanguages:completion:
purgeInstalledAssetsExceptLanguages:assetType:completion:
setAssetsPurgeabilityExceptLanguages:assetType:
writeDESRecord
sendSpeechCorrectionInfo:interactionIdentifier:
invalidatePersonalizedLM
removePersonalizedLMForFidesOnly:completion:
runEvaluationWithDESRecordDatas:language:recipe:attachments:fidesPersonalizedLMPath:fidesPersonalizedLMTrainingAsset:scrubResult:completion:
deleteAllDESRecordsForDictationPersonalizationWithCompletion:
invalidateUaapLm
resumeRecognitionWithPrefixText:postfixText:selectedText:
_delegate
_fidesEvalQueue
_invalidated
fetchModelInfoForAssetConfig:completion:
fetchModelInfoForAssetConfig:triggerDownload:completion:
siriDataSharingOptedIn
shouldWriteDictationRecord:
updateAudioDuration:
readTableFromURL:
runRecipeEvaluationDistributedEvaluation:recordData:attachments:completion:
runDefaultAdaptationEvaluation:recordData:attachments:completion:
_userProfileWithAssetConfig:modelOverridePath:overrides:foundPath:isJit:error:
_userProfileConfigWithAssetConfig:modelOverridePath:overrides:error:
_modelRootWithAssetConfig:modelOverridePath:overrides:error:
_deleteTemporaryDirectoryIfExists:
_fetchUserDataOptionsWithAssetConfig:modelOverridePath:overrides:completion:
_writeDESRecord:
_writeDESRecord:oneRecordPerDay:
dummyResultPackage:
statusForError:
_recognizer
_audioBuffer
_shouldStoreDictationAudioOnDevice
_disableDeliveringAsrFeatures
_isHighPriority
_lastRecognizedPackage
_lastRecognizedMetadataPackage
_bufferedAudioPackets
_bufferedAudioEnded
_validDomains
_requestCompletion
_storeAudioData
_biomeRecord
_selfHelper
_samplingRate
_audioDurationMs
_processedAudioDuration
_firstAudioPacketReceivedTime
_firstAudioPacketTimeUntilFirstPartial
_lastAudioPacketReceivedTime
_firstAudioPacketReceivedTimeInTicks
_lastAudioPacketReceivedTimeInTicks
_firstAudioPacketProcessedTime
_localMetrics
_recognitionBeginTime
_recognitionAbsoluteEndTime
_speakerCodeWriter
_weakFidesRecognizer
_lastWordCount
_taskToUse
_desRecord
_desRecordDictation
initWithUUIDString:language:task:codec:samplingRate:inferenceSpeakerCode:numTrainedFrames:trainingNnetVersion:isSpeakerCodeUsed:isSamplingForDictation:selfLogger:
hasData
saveAudioToDisk
cleanupCacheAndReset
_deleteItemAtPath:
_cleanupCacheAndReset:
_saveAudioToCache:
_trimAudioIfNeeded:
_moveAudioToVarMobile:
_saveAudioMetadataToFilePath:
_createAudioFilePath
_createCachesDirectoryIfItDoesNotExist
_logAudioSampledEventsWithStatus:error:customReasonForFailure:
UUIDString
setUUIDString:
language
setLanguage:
task
setTask:
codec
setCodec:
samplingRate
setSamplingRate:
audioPackets
setAudioPackets:
setHasRecognizedAnything:
inferenceSpeakerCode
numTrainedFrames
trainingNnetVersion
isSpeakerCodeUsed
isSamplingForDictation
selfLogger
setSelfLogger:
audioMetadata
setAudioMetadata:
collectedAudioDurationMS
setCollectedAudioDurationMS:
currentAudioFilePath
setCurrentAudioFilePath:
logPrefix
setLogPrefix:
_isSpeakerCodeUsed
_isSamplingForDictation
_UUIDString
_task
_codec
_audioPackets
_inferenceSpeakerCode
_numTrainedFrames
_trainingNnetVersion
_selfLogger
_audioMetadata
_collectedAudioDurationMS
_currentAudioFilePath
_logPrefix
T@"NSString",C,N,V_UUIDString
T@"NSString",C,N,V_language
T@"NSString",C,N,V_task
T@"NSString",&,N,V_codec
TQ,N,V_samplingRate
T@"NSMutableData",&,N,V_audioPackets
TB,N,V_hasRecognizedAnything
T@"NSString",R,N,V_inferenceSpeakerCode
T@"NSNumber",R,N,V_numTrainedFrames
T@"NSNumber",R,N,V_trainingNnetVersion
TB,R,N,V_isSpeakerCodeUsed
TB,R,N,V_isSamplingForDictation
T@"ESSelfHelper",&,N,V_selfLogger
T@"NSMutableDictionary",&,N,V_audioMetadata
Td,N,V_collectedAudioDurationMS
T@"NSString",C,N,V_currentAudioFilePath
T@"NSString",C,N,V_logPrefix
listener:shouldAcceptNewConnection:
%s SELF: Logging object created successfully (logging allowed for current request). asrId=%@, recognitionTask=%@, isSpeechAPIRequest=%@, isHipaaCompliant=%@, siriOptInStatus=%@, isTier1LoggingAllowed=%@
%s SELF: Logging disabled because it is not allowed for the current request. recognitionTask=%@, isSpeechAPIRequest=%@
%s SELF: Correct Partial Result Index List is %s, Silence Start Time List is %s
%s SELF: Encountered malformed string during SELF logging for recognizer components in speech results from recognizer. String: (%@)
%s SELF: Expected three recognizer components separated by delimiter '::'. Ex: 'dnn-rfdnn-aa-cache::dnn-lazy-16k-rfdnn-dictation::msg'
%s SELF: Encountered malformed string during SELF logging for interpolation weights in speech results from recognizer. String: %@
%s SELF: Expected interpolation weight sets separated by delimiter ';' - starting with a set of weights delimited by ',' and ending the with start/end times delimited by ':'. Ex: '0.999646,0.000354:0:4280;0.947514,0.000158:0:3859'
%s SELF: Logging ASRRequestContext->failed in SELF based on error result from recognizer.
%s SELF: Logging ASRRequestContext->cancelled with reason RECOGNITION_CANCELLED in SELF based on error result from recognizer.
%s SELF: Logging ASRRequestContext->cancelled with reason RECOGNITION_REJECTED in SELF based on error result from recognizer.
%s SELF: Logging ASRRequestContext->cancelled with reason RECOGNITION_REJECTED in SELF because nothing was recognized (SpeechNoMatch).
%s SELF: Logging ASRRequestContext->ended in SELF based on success status from recognizer.
%s SELF: Failed trying to wrap and emit top-level ASR event because event type was not mapped to loggable message type in the ASR SELF schema.
%s SELF: Wrapping and logging an event of type %@
%s %@ cancelling instance %@
%s Failed to delete category %@: %@. %@
%s %@
%s %@.
%s Internal inconsistency error: KVItems list and corresponding isBoosted booleans list are out of sync. This batch of items will default to not being boosted.
%s Starting to process KVItems into CESRUserData.
%s Created CESRUserData from KVItems.
%s Adapted profile using CESRUserData according to personalization recipe.
%s %@: %@
%s Speech profile updated successfully. Wrote %lu bytes to %@
%s Created _EARProfileBuilder from asset config.
%s Existing profile found at %@. It will be reused when updating the profile.
%s Could not locate asset: %@
%s Could not read personalization.json: %@
%s Could not parse personalization.json: %@
%s Set model root to %@
%s Use currently installed asset.
%s Recipe has invalid json for "%{public}@"
%s Recipe has invalid tagName for "%{public}@": "%{public}@"
%s Error executing recipe for domain %{public}@
%s Recipe for %{public}@ is missing "%{public}@"
%s Recipe for %{public}@ contains parameter %{public}@, expected type %{public}@ but got %{public}@
%s User Profile: Starting AddWordsToUserProfile
%s Using name frequencies adaption for names: %@ into slot %@ for template %@
%s Ignoring name part "%@" because it is too short (minimum length is %lu)
%s Using keyboardLMAdaptation adaption for names %@ into slot %@ for template %@
%s Ignoring keyboardDynamicVocabularyItem "%@" because it is too short (minimum length is %lu)
%s Using locationOfInterestNameAdaptationFrequency adaption for names %@ into slot %@ for template %@
%s Ignoring locationOfInterestName "%@" because it is too short (minimum length is %lu)
%s Using eventLocationNameAdaptationFrequency adaption for names %@ into slot %@ for template %@
%s Ignoring calendar event word "%@" because it is too short (minimum length is %lu)
%s Root directories for new type of speech profile: %{private}@
%s speechProfilePathsWithLanguage was incorrectly called with language=nil
%s Mapped language=nil
%s loadSpeechProfiles was incorrectly called with profiles=nil
%s Reused new type of speech profile: path=%{private}@
%s Loaded new type of speech profile: path=%{private}@ profile=%d
%s Count of command interpretation sets does not match count of speech recognition results
%s AFSpeechLatticeMitigatorResult Score = %f, Threshold = %f
%s AFSpeechLatticeMitigatorResult nil
%s Purging compiled assets if there are any.
%s Previously installed asset has been removed: %{public}@
%s Installed asset is corrupt! Triggering emergency purge %{public}@
%s Failed to register for assistant asset update notifications.
%s Failed to register for dictation asset update notifications.
%s Installation status for languages (ignoring cache: %@)
%s Trial asset delivery is enabled!
%s Invalidating installation status cache for %lu
%s Language installation status query failed: %@
%s MobileAsset query failed: %ld
%s Found assets: %@
%s Using ASR Trial assets at %@
%s No assets available for language: %@ asset type: %{public}@
%s Purging Trial assets failed: %@
%s Purging outdated assets.
%s Error purging (%@): %ld
%s Purged old asset %@
%s Just ignoring %@
%s Purging asset: %@, language %@
%s Purging failed: %lu
%s Checking for missing assets.
%s Purging all assistant ASR assets except for %@
%s Purging all assistant ASR assets
%s Encountered error trying to purge unused assistant ASR assets: %@
%s Trial asset delivery disabled for assistant assets. Bailing out of missing asset check.
%s Hit error querying installation status of MobileAssets: %@
%s Purging assets from MobileAssets for locale: %@
%s Hit error purging assets from MobileAssets: %d %@
%s Trial asset delivery disabled for dictation assets. Bailing out of missing asset check.
%s Purging all legacy dictation ASR assets
%s Encountered error trying to purge legacy dictation ASR assets: %@
%s Trial asset delivery explicitly disabled!
%s Trial asset delivery explicitly enabled!
%s Hammer model info=%@
%s Exception thrown while reading hammer config
%s GeoLM: region mapping json file=%@
%s GeoLM: region mapping json file is nil Or there is no regionMapping for given language=%@
%s GeoLM: For the given location, selected regionId=%@
%s GeoLM: location is nil.
%s GeoLM: region specific [%@] geo-config json file=%@
%s GeoLM: geoLM asset exisits on device, but not compatible. Deleting...
%s GeoLM: Exception thrown while reading geo-config json
%s GeoLM: region specific asset is not found for given language=%@ regionId=%@
%s GeoLM: model-info.version doesn't match. mainASRModelInfo.version=%@ geoLMModelInfo.version=%@ mainAssetConfig=%@ geoAssetConfig=%@
%s GeoLM: Exception thrown while reading json configs
%s GeoLM: language is nil. Skipping.
%s GeoLM: regionIdToBePurged: %@, lastWhenUsed: %ld days ago
%s GeoLM: regionIdToBePurged: %@, _geoLMAssetsInfoDict count: %ld
%s GeoLM: supportedLanguages count:%ld
%s GeoLM: Going to delete: %@
%s ASR: Using high priority configuration.
%s %@ deallocating
%s Failing to fetch assets for nil language
%s Could not get offline language for fetch fallback: %{public}@
%s Fell back asset fetch from %{public}@ to %{public}@
%s Failed to fall back asset fetch from %{public}@ to %{public}@, got %{public}@
%s Can't get user data options: %{public}@
%s Unable to serialize user profile to JSON data: %{public}@
%s Could not get installed offline language statuses: %{public}@
%s ASR: enable parallel loading
%s ASR: taskForMemoryLock: %@
%s Override json files=%@
%s Unable to locate or read dictation voice commands assets
%s modelRoot: %@
%s Failed to create recognizer from %{public}@
%s EmbeddedSpeechMetric: Created recognizer in %lf sec from %@
%s %s
ondevice_CreateRecognizer
%s Skipping preheat for %@; recognizer already loaded
%s Preheated for language %{public}@, asset type %{public}@, regionId %@%{public}@
%s Could not preheat for language %{public}@, asset type %{public}@, regionId %@%{public}@: %@
%s dictationCapable=%d task=%@ aneCapable=%d
%s Starting
%s Recognizer is busy
%s Cached recognizer for language %{public}@, asset type %{public}@, regionId %@ already  loaded
%s Cached recognizer is for language %{public}@, asset type %{public}@, regionId %@,  requesting recognizer for language %{public}@, asset type %{public}@, regionId %@
%s No cached recognizer.
%s EndpointStart > 0 but asr features delivery is disabled!
%s EndpointStart < 0
%s Setting new profile: %d, old profile: %d
%s Injecting contextual data to recognizer
%s Built inline LME from contextual data, size: %zu
%s Injected %lu jit strings or contextual data to recognizer
%s Failed to build jitData for jitGrammar or contextual data
%s Failed to initialize jit profile builder due to error : %@
%s Failed to get model root, error: %@
%s Duration spent in adding jit strings = %{public}lfs
%s Set inputOrigin to: %@
%s Switching off UC/UD for this request
%s Changing active configuration from 
%@ to 
%s Create DES record
%s Cancelling delayedBlock
%s Create DES record for Dictation with interactionId=%@
%s Saving profile snapshot: %lu bytes
%s _storeAudioData should be nil. Critical Error. Please check.
%s Received correctedText, interactionId: %@, correctedText: %@
%s Sending dictation feedback to Portrait based on correction
%s Interaction identifier did not match the DES record in memory
First Audio Packet
ES: First Audio Packet
%s Using override profile at %@
%s Could not use override profile at %@: %@
%s Deserialization of existing speech profile failed: %{public}@
%s Mismatch in speech profile language in content (%{public}@) and filename (%{public}@)
%s Profile version on disk (%{public}@) does not match the expected version (%{public}@)
%s Successfully deserialized existing speech profile for %@
%s Creating profile for %@
%s Re-using existing profile data because the asset (%@) is unchanged
%s Ignoring existing profile data because the asset changed (%@ to %@)
%s Cancelling profile update for %@ due to invalidation
%s Got no data from CESRUserData
%s Skipping profile update for %@ because user data has not actually changed
%s Created new profile with %ld bytes (was %ld bytes)
%s On-Device ASR: Cooldown is disabled.
%s Acquired os_transaction for cooldown start
%s On-Device ASR: Cooldown scheduled for %zus.
%s Cooldown timer triggered TRIClient release
%s Cooldown timer triggered asset purge
%s embeddedspeech process launch triggered asset purge
%s Received termination signal. Cleaning up immediately
%s Sending %lu events
%s Unable to load the contents of file %@: %@
%s Invalid file format
%s Running distributed evaluation for ASR
%s No attachments given, cannot run distributed evaluation
%s Failed to extract test set: %@
%s Cannot initialize recognizer for locale: %@ task: %@
%s Loaded speech profile
%s Unable to load speech profile
%s Test set contains more utterances than allowed, only running %d utterances
%s Unable to load audio file %@
%s Unable to find reference transcriptions for %@
%s Recipe has no profile
%s Stop adaption recipe. Audio file not readable. Voicemail has been deleted by user
%s Read %lu bytes from audio file
%s Using on device personalization recipe for baseline
%s Recognizer doesn't support the task %{public}@
%s Couldn't create create path for temporary confidence model overrides at %@
%s Couldn't write data to temporary confidence model file at path %@
%s Could not make baseline results
%s Failed to extract quasar model: %@
%s No recognizer created for custom model: %@
%s Could not make results with custom model
%s Profile overrides failed
%s Ignoring malformed overrides: %{public}@
%s Recipe has no recognizer
%s Could not make adapted results
%s Exception evaluating recipe: %@
%s Unknown exception evaluating recipe
%s No tokenizer for %@
%s Interrupted corrected text evaluation redecoding
%s Examining localSpeechDESRecord: %@
%s Unable to load localSpeechDESRecord
%s No audio data provided for UUID %@
%s Recognition result %@, %lu
%s Edit distance between tts ASR and original ASR %@
%s correctedOutput: %@, recognizedOutput %@
%s %{public}@
%s Unknown evaluation name found in alignmentReferences: %@
%s No modelRoot for %@: %@
%s Failed to load old type of speech profile. Trying new type.
%s Loaded speech profile: %lu bytes
%s Unable to load audio
%s Recognizer doesn't support the task %{public}@: %@
%s Interrupted evaluation redecoding
%s Running recognition for evalName: %@
%s Failed to get override files, error: %@
%s Creating recognizer with overrides: %@
%s Using Personalized LM
%s Not using Personalized LM
%s Unable to restore speech profile
%s Using JIT LME
%s JIT LME: Injecting JIT data, jitStats: %@
%s JIT LME: Error fetching JIT data, error: %@
%s No results for evalName %@: %@
%s Tokenizing correctedText
%s Interrupted evaluation tokenization
%s Computing alignments
%s Failed to delete temporary directory: %@
%s Error when getting dictation language status: %@
%s Starting to compile dictation language: %@
%s Error when compiling dictation language model: %@
%s Starting to compile assistant language: %@
%s Error when compiling assistant language model: %@
%s Error when getting assistant language status: %@
%s Failed to fetch user data options
%s No DES record, nothing to write
%s Not saving DES Record with no data or recognition
%s Fetch user data for language: %@
%s Can't get user data options for DES record language: %@, not writing record
%s Got nil user data for DES record language: %@, not writing record
%s wordCount = %ld, trailingSilenceDuration = %ld, eosLikelihood = %f, pauseCounts = %@, silencePosterior = %f
%s Setting recognized text
%s %lu results
%s EmbeddedSpeechMetric: first audio packet to first partial result = %lf secs
Words recognized: %ld
ES: Partial Recognition
%s AFSpeechInfo Package %@
%s Recognition finished with status %@
%s Audio finish to recognizer finish = %lf sec, connection is %@, error %@
%s _recognitionBeginTime (%@) is greater than _recognitionEndTime (%@)
%s Local speech recognition completed without error, write DES record when needed
%s Writing DES record after 30 seconds delay: interactionId=%@
%s Submitted delayedBlock to dispatch_after
%s #ASR on device eager formatted recognition candidate: %@
%s raw eager recognition candidate: %@
%s Could not make temporary attachment directory at %@: %@
%s Failed to specify compression algorithm: %s
%s Failed to specify format: %s
%s Start extracting archive at path: %s
%s Failed to open archive for reading: %s
%s Entry extraction path: %@
%s Unable to extract file to: %@
%s Finished extracting archive to: %@
%s Could not read %@: %@
%s Could not parse %@: %@
%s %@ is wrong type: %@
%s %@ contains bogus key/value pair: %@ => %@
%s VoiceMail asset could not be read: %@
%s Could not create asset reader output
%s Cannot add output
%s Could not get data pointer: %d
%s JSON serialization failed: %@
%s Compression failed: %@
%s Sampling: Error while initializing ESStoreAudioData because uuid is invalid.
%s %@ Sampling: Won't save audio because - has not recognized anything or has no data.
%s %@ Sampling: Won't save audio because - _currentAudioFilePath is null
%s %@ Sampling: invalid filePath or it is null.
%s %@ Sampling: Done with cleanup of audioFile=%@ and reset of variables.
%s %@ Sampling: Failed to save audio to cache dir. Error: %@
%s %@ Sampling: Successfully saved audio file to cache dir, path=%@
%s %@ Sampling: audioFileToBeMoved is nil
%s %@ Sampling: currentSamplingDate is nil
%s %@ Sampling: Error while creating Sampled directory in /var/mobile
%s %@ Sampling: Error while creating dated Sampled directory in %@ with date - %@
%s %@ Sampling: Error while moving file from cache directory to var/mobile/Library - %@
%s %@ Sampling: Successfully moved audio file to var/mobile/Library dir, path=%@
%s %@ Sampling: Error while writing audio metadata dict to plist - %@
%s %@ Sampling: currentSamplingDateString is null
%s Received SIGTERM. Cleaning and Exiting
ESSelfHelper
Timestamp
ESSelfPreheatWithPowerContainer
ESSpeechProfileBuilderConnection
CESRSpeechProfileBuilderService
NSObject
ESBiomeRecord
CESRUtilitiesAdditions
CESRUtilities
ESAdditions
ESAssetManager
ESAlignmentState
NSCopying
ESConnectionModelInfo
ESConnection
_EARSpeechRecognitionResultStream
CESRSpeechService
ESStoreAudioData
ESListenerDelegate
NSXPCListenerDelegate
@16@0:8
@20@0:8B16
@24@0:8@16
v16@0:8
@28@0:8@16B24
B28@0:8@16B24
B24@0:8@16
v24@0:8@16
v32@0:8@16@24
v92@0:8@16@24@32@40@48@56B64@68@76@84
v48@0:8@16@24@32@40
v148@0:8@16@24@32@40@48@56@64@72B80B84@88@96@104@112@120B128B132B136@140
v32@0:8@16q24
B16@0:8
v20@0:8B16
@"NSUUID"
@"NSString"
@"NSNumber"
@40@0:8@16@24@32
@"ASRSchemaASRPreheatContext"
@"SPIPowerLoggerSnapshot"
@"SPIEventContext"
@32@0:8@16@24
@56@0:8@16@24@32^@40^@48
v44@0:8@16@24@32B40
v40@0:8@16@24@32
@56@0:8@16#24@32@40^@48
B88@0:8@16d24@32@40@48@56Q64@72@80
B80@0:8d16@24@32@40@48Q56@64@72
#16@0:8
@24@0:8:16
@32@0:8:16@24
@40@0:8:16@24@32
B24@0:8#16
B24@0:8:16
Vv16@0:8
Q16@0:8
^{_NSZone=}16@0:8
B24@0:8@"Protocol"16
@"NSString"16@0:8
Vv48@0:8@16@24@32q40
Vv32@0:8@16@?24
Vv40@0:8@16@24@?32
Vv24@0:8@?16
Vv28@0:8B16@?20
Vv48@0:8@"NSString"16@"NSString"24@"NSString"32q40
Vv32@0:8@"NSString"16@?<v@?q@"NSError">24
Vv40@0:8@"NSDictionary"16@"NSString"24@?<v@?B@"NSError">32
Vv40@0:8@"NSArray"16@"NSArray"24@?<v@?B@"NSError">32
Vv24@0:8@?<v@?B@"NSError">16
Vv28@0:8B16@?<v@?B@"NSError">20
@24@0:8^@16
@"NSMutableArray"
@"NSMutableDictionary"
@"NSMutableSet"
@"CESRAssetConfig"
@"NSXPCConnection"
@"NSObject<OS_os_transaction>"
@"_EARUserProfile"
d16@0:8
v24@0:8d16
q24@0:8@16
@32@0:8@16d24
@36@0:8@16d24B32
@24@0:8Q16
q16@0:8
@40@0:8B16Q20B28^@32
v24@0:8Q16
@32@0:8@16^@24
@36@0:8@16^@24B32
@40@0:8@16^@24B32B36
v32@0:8@16Q24
B40@0:8@16Q24^@32
B32@0:8@16^@24
B24@0:8Q16
B32@0:8@16@24
@"NSObject<OS_dispatch_queue>"
@24@0:8^{_NSZone=}16
v24@0:8q16
@40@0:8@16B24B28^@32
@80@0:8@16@24B32@36@44B52B56B60^@64^@72
v32@0:8@16d24
v72@0:8@16q24q32d40@48d56q64
v32@0:8@"_EARSpeechRecognizer"16@"_EARSpeechRecognitionResult"24
v32@0:8@"_EARSpeechRecognizer"16@"NSError"24
v32@0:8@"_EARSpeechRecognizer"16@"NSArray"24
v32@0:8@"_EARSpeechRecognizer"16@"_EARSpeechRecognitionResultPackage"24
v48@0:8@"_EARSpeechRecognizer"16@"NSArray"24@"NSArray"32@"NSArray"40
v32@0:8@"_EARSpeechRecognizer"16d24
v32@0:8@"_EARSpeechRecognizer"16@"_EARSpeechRecognition"24
v72@0:8@"_EARSpeechRecognizer"16q24q32d40@"NSArray"48d56q64
Vv32@0:8@16@24
Vv24@0:8@16
Vv48@0:8@16@24@32@?40
Vv56@0:8@16@24@32@40@?48
Vv36@0:8B16Q20@?28
Vv40@0:8B16Q20B28@?32
Vv72@0:8@16@24@32Q40B48B52@56@?64
Vv36@0:8@16B24@?28
Vv40@0:8@16Q24@?32
Vv32@0:8@16Q24
Vv76@0:8@16@24@32@40@48@56B64@?68
Vv40@0:8@16@24@32
Vv24@0:8@?<v@?@"NSError">16
Vv32@0:8@"NSString"16@"NSURL"24
Vv32@0:8@"CESRAssetConfig"16@"NSURL"24
Vv24@0:8@?<v@?>16
Vv32@0:8@"CESRSpeechParameters"16@?<v@?@"CESRModelProperties"@"NSError">24
Vv32@0:8@"CESRSpeechParameters"16@?<v@?@"NSString"@"NSString"@"NSError">24
Vv24@0:8@"NSData"16
Vv48@0:8@"NSString"16@"NSString"24@"NSData"32@?<v@?@"NSData"@"NSError">40
Vv56@0:8@"NSString"16@"NSString"24@"NSData"32@"NSString"40@?<v@?@"NSData"@"NSString"@"NSError">48
Vv36@0:8B16Q20@?<v@?@"NSDictionary"@"NSError">28
Vv40@0:8B16Q20B28@?<v@?@"NSDictionary"@"NSError">32
Vv28@0:8B16@?<v@?@"NSDictionary"@"NSError">20
Vv32@0:8@"NSString"16@?<v@?@"NSString"@"NSError">24
Vv32@0:8@"CESRAssetConfig"16@?<v@?@"NSString"@"NSError">24
Vv32@0:8@"CESRAssetConfig"16@?<v@?@"CESRModelProperties"@"NSError">24
Vv32@0:8@"NSString"16@?<v@?@"NSData">24
Vv48@0:8@"NSDictionary"16@"NSData"24@"NSArray"32@?<v@?@"NSDictionary"@"NSData"@"NSError">40
Vv72@0:8@"NSDictionary"16@"NSDictionary"24@"NSString"32Q40B48B52@"NSSet"56@?<v@?@"NSDictionary"@"NSError">64
Vv36@0:8@"NSString"16B24@?<v@?@"NSData"@"NSString">28
Vv32@0:8@"NSSet"16@?<v@?@"NSNumber"@"NSError">24
Vv40@0:8@"NSSet"16Q24@?<v@?B@"NSError">32
Vv32@0:8@"NSSet"16Q24
Vv32@0:8@"AFSpeechCorrectionInfo"16@"NSString"24
Vv28@0:8B16@?<v@?>20
Vv76@0:8@"NSDictionary"16@"NSString"24@"NSDictionary"32@"NSArray"40@"NSString"48@"NSString"56B64@?<v@?@"NSDictionary"@"NSError">68
Vv40@0:8@"NSString"16@"NSString"24@"NSString"32
v36@0:8@16B24@?28
v48@0:8@16@24@32@?40
@60@0:8@16@24@32^@40B48^@52
@48@0:8@16@24@32^@40
v24@0:8^@16
v28@0:8^@16B24
@"_EARSpeechRecognizer"
@"_EARSpeechRecognitionAudioBuffer"
@"AFSpeechPackage"
@"AFSpeechInfoPackage"
@"NSSet"
@"ESStoreAudioData"
@"ESBiomeRecord"
@"ESSelfHelper"
@"_EARSpeakerCodeWriter"
@"CESRFidesASRRecord"
@96@0:8@16@24@32@40Q48@56@64@72B80B84@88
v40@0:8q16@24q32
@"NSMutableData"
B32@0:8@"NSXPCListener"16@"NSXPCConnection"24
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
<key>application-identifier</key>
<string>com.apple.siri.embeddedspeech</string>
<key>com.apple.CoreRoutine.LocationOfInterest</key>
<true/>
<key>com.apple.accounts.appleaccount.fullaccess</key>
<true/>
<key>com.apple.application-identifier</key>
<string>com.apple.siri.embeddedspeech</string>
<key>com.apple.coreaudio.allow-amr-decode</key>
<true/>
<key>com.apple.coreduetd.allow</key>
<true/>
<key>com.apple.coreduetd.people</key>
<true/>
<key>com.apple.developer.homekit</key>
<true/>
<key>com.apple.frontboardservices.display-layout-monitor</key>
<true/>
<key>com.apple.locationd.effective_bundle</key>
<true/>
<key>com.apple.private.DistributedEvaluation.RecordAccess-com.apple.fides.asr</key>
<true/>
<key>com.apple.private.DistributedEvaluation.RecordAccess-com.apple.siri.speech-dictation-personalization</key>
<true/>
<key>com.apple.private.assets.accessible-asset-types</key>
<array>
<string>com.apple.MobileAsset.Trial.Siri.SiriUnderstandingAsrAssistant</string>
<string>com.apple.MobileAsset.Trial.Siri.SiriUnderstandingAsrHammer</string>
<string>com.apple.MobileAsset.Trial.Siri.SiriDictationAssets</string>
<string>com.apple.MobileAsset.EmbeddedSpeech</string>
<string>com.apple.MobileAsset.EmbeddedSpeechWatch</string>
<string>com.apple.MobileAsset.EmbeddedSpeechMac</string>
</array>
<key>com.apple.private.assets.bypass-asset-types-check</key>
<true/>
<key>com.apple.private.attribution.implicitly-assumed-identity</key>
<dict>
<key>type</key>
<string>path</string>
<key>value</key>
<string>/System/Library/PrivateFrameworks/CoreEmbeddedSpeechRecognition.framework/XPCServices/com.apple.siri.embeddedspeech.xpc/com.apple.siri.embeddedspeech</string>
</dict>
<key>com.apple.private.biome.read-write</key>
<array>
<string>SiriDictation</string>
</array>
<key>com.apple.private.calendar.allow-suggestions</key>
<true/>
<key>com.apple.private.contacts</key>
<true/>
<key>com.apple.private.corerecents</key>
<true/>
<key>com.apple.private.corespotlight.internal</key>
<true/>
<key>com.apple.private.homekit</key>
<true/>
<key>com.apple.private.security.storage.SiriVocabulary</key>
<true/>
<key>com.apple.private.security.storage.SpeechPersonalizedLM</key>
<true/>
<key>com.apple.private.tcc.allow</key>
<array>
<string>kTCCServiceAddressBook</string>
<string>kTCCServiceCalendar</string>
<string>kTCCServiceWillow</string>
<string>kTCCServiceMediaLibrary</string>
</array>
<key>com.apple.proactive.PersonalizationPortrait.Config</key>
<true/>
<key>com.apple.proactive.PersonalizationPortrait.NamedEntity.readOnly</key>
<true/>
<key>com.apple.proactive.eventtracker</key>
<true/>
<key>com.apple.security.exception.files.absolute-path.read-only</key>
<array>
<string>/private/var/MobileAsset/</string>
<string>/private/var/tmp/com.apple.siri-distributed-evaluation/</string>
</array>
<key>com.apple.security.exception.mach-lookup.global-name</key>
<array>
<string>com.apple.suggestd.contacts</string>
<string>com.apple.mobileasset.autoasset</string>
</array>
<key>com.apple.security.exception.shared-preference.read-only</key>
<array>
<string>com.apple.assistant</string>
<string>com.apple.assistant.backedup</string>
<string>com.apple.assistant.support</string>
</array>
<key>com.apple.security.iokit-user-client-class</key>
<array>
<string>AGXCommandQueue</string>
<string>AGXDevice</string>
<string>AGXDeviceUserClient</string>
<string>AGXSharedUserClient</string>
<string>H11ANEInDirectPathClient</string>
<string>IOAccelContext</string>
<string>IOAccelContext2</string>
<string>IOAccelDevice</string>
<string>IOAccelDevice2</string>
<string>IOAccelSharedUserClient</string>
<string>IOAccelSharedUserClient2</string>
<string>IOAccelSubmitter2</string>
<string>IOSurfaceRootUserClient</string>
</array>
<key>com.apple.security.personal-information.addressbook</key>
<true/>
<key>com.apple.security.temporary-exception.mach-lookup.global-name</key>
<array>
<string>com.apple.triald.namespace-management</string>
</array>
<key>com.apple.siriknowledged</key>
<true/>
<key>com.apple.spotlight.search</key>
<true/>
<key>com.apple.trial.client</key>
<array>
<string>372</string>
<string>401</string>
<string>751</string>
</array>
</dict>
</plist>
mcpl
