ASRPreheatContextTimestamp
ASRAppleNeuralEngineCompilationContextTimestamp
-[ESSelfHelper initWithTask:isSpeechAPIRequest:]
-[ESSelfHelper logFinalResultGeneratedWithEARPackage:]
-[ESSelfHelper logRequestEndedOrFailedOrCancelledWithError:recognizerComponents:averageActiveTokensPerFrame:languageModelInterpolationWeights:signalToNoiseRatioInDecibels:recognitionDurationInSec:audioDurationMs:eagerUsed:utteranceDetectionEnabled:utteranceConcatenationEnabled:cpuRealTimeFactor:numLmeDataStreams:phoneticMatchDecoderName:pauseDurations:itnDurationInNs:isEmojiPersonalizationUsed:isEmojiDisambiguationUsed:isEmojiExpectedButNotRecognized:recognizedEmojis:]
floatValue
-[ESSelfHelper wrapAndEmitTopLevelEvent:timestampInTicks:]
v32@?0@"NSArray"8Q16^B24
CESRProfileErrorDomain
\NT-contact
\NT-appname
\NT-correction
overrides
keyboardLM
locationOfInterest
spatialLocationOfInterest
interaction
search
calendarEvent
pexNamedEntity
frequency
charsToTrim
charsToSplit
tagName
templateName
tagNameList
minimumWordLength
com.apple.siri.embeddedspeech.profilegeneration-keepalive
-[ESSpeechProfileBuilderConnection initWithXPCConnection:]_block_invoke
v8@?0
Siri
unified_asset_namespace
Could not create EAR profile builder
-[ESSpeechProfileBuilderConnection getVersionForCategory:completion:]
-[ESSpeechProfileBuilderConnection beginWithCategoriesAndVersions:bundleId:completion:]
Begin called while there are already active categories.
Speech category %@ has already been committed with a call to Begin followed by Finish
Speech category %@ is unsupported
Add called before categories have been set with Begin
-[ESSpeechProfileBuilderConnection addVocabularyItems:isBoosted:completion:]
-[ESSpeechProfileBuilderConnection finishAndSaveProfile:completion:]
v32@?0@"NSString"8@"NSMutableArray"16^B24
Failed to write profile
Failed to load speech assets
-[ESSpeechProfileBuilderConnection _profileWithError:]
Failed to read the existing speech profile
+[ESSpeechProfileBuilderConnection personalizationRecipeForAssetConfig:modelOverridePath:]
personalization.json
+[ESSpeechProfileBuilderConnection userProfileWithAssetConfig:modelOverridePath:overrides:isJit:returningFoundPath:error:]
mini.json
en_US_napg.json
dispatch.voc
vocdelta.voc
pg.voc
lexicon.enh
token_s.enh
mrec.psh
Error during _EARUserProfile initialization
v32@?0@"CESRVocabularyCategory"8@"NSSet"16^B24
\correction-first
+[ESSpeechProfileBuilderConnection _adaptRecipe:userData:profile:]_block_invoke
v32@?0@"NSString"8@"NSDictionary"16^B24
+[ESSpeechProfileBuilderConnection _parseRequiredParameter:expectedClass:domain:recipe:error:]
Missing %@ for %@
+[ESSpeechProfileBuilderConnection addWordsToUserProfile:templateName:wordArrays:]
v32@?0@"NSString"8@"NSString"16^B24
+[ESSpeechProfileBuilderConnection _runAdaptationRecipeForNamesWithFrequency:tagNames:templateName:charSetToTrim:charSetToSplit:minimumWordLength:nameFrequencies:profile:]
+[ESSpeechProfileBuilderConnection _runAdaptationRecipeForNamesWithFrequency:tagNames:templateName:charSetToTrim:charSetToSplit:minimumWordLength:nameFrequencies:profile:]_block_invoke_2
v32@?0@"NSString"8Q16^B24
v32@?0@"NSString"8@"NSNumber"16^B24
+[ESSpeechProfileBuilderConnection _runAdaptationRecipeForKeyboardLMWithFrequency:tagName:templateName:charSetToTrim:charSetToSplit:minimumWordLength:userData:profile:]
+[ESSpeechProfileBuilderConnection _runAdaptationRecipeForLocationOfInterestWithFrequency:tagName:templateName:charSetToTrim:charSetToSplit:minimumWordLength:locationOfInterestNames:profile:]
+[ESSpeechProfileBuilderConnection _runAdaptionRecipeForCalendarEventsWithFrequency:tagName:templateName:charSetToTrim:charSetToSplit:minimumWordLength:userData:profile:]
com.apple.MobileSMS
ResultCandidateId
SpeechProfile
EARUserProfileContainerLoadDate
+[CESRUtilities speechProfileRootDirectories]
+[CESRUtilities speechProfilePathsWithLanguage:]
+[CESRUtilities loadSpeechProfiles:language:]
v32@?0@"_EARSpeechRecognitionToken"8Q16^B24
Param
v32@?0{_NSRange=QQ}8^B24
v32@?0@"EARVoiceCommandArgument"8Q16^B24
@"NSArray"24@?0@"NSArray"8@"NSArray"16
+[CESRUtilities AFSpeechInfoPackageForEARSpeechRecognitionResultPackage:]_block_invoke
unconstrained
reduced
avoid
AFSpeechLatticeMitigatorResultForEar
quasarModelPath
type
com.apple.trial.NamespaceUpdate.SIRI_DICTATION_ASSETS
com.apple.trial.NamespaceUpdate.SIRI_UNDERSTANDING_ASR_ASSISTANT
trial
QuasarDir
PreferOverServer
SupportsContinuousListening
SupportsOnDeviceSearch
GeoLMAssetsInfo
-[MAAsset(ESAdditions) _es_purgeSync]
Asset: content version: %@, mastered version %@, installed %@, language: %@, path: %@
Language
-[MAAsset(ESAdditions) _es_isInstalled]
com.apple.siri.embeddedspeech.ESAssetManager
v12@?0i8
-[ESAssetManager registerNotifications]
-[ESAssetManager installationStatusForLanguagesIgnoringCache:assetType:withDetailedStatus:withError:]
-[ESAssetManager installationStatusForLanguagesIgnoringCache:assetType:withDetailedStatus:withError:]_block_invoke
-[ESAssetManager _invalidateInstallationStatusCacheForAssetType:]
-[ESAssetManager _queryInstallationStatusForLanguagesWithError:]
-[ESAssetManager installedModelInfoForAssetConfig:error:triggerDownload:ignoreSpellingModel:]
+[ESAssetManager _assetQueryForLanguage:]
-[ESAssetManager purgeInstalledAssetsExceptLanguages:assetType:error:]_block_invoke
-[ESAssetManager purgeOutdatedAssets]_block_invoke
-[ESAssetManager _purgeMobileAssetsForLanguage:error:]_block_invoke
v24@?0@"MAAsset"8^B16
-[ESAssetManager startMissingAssetDownload]
-[ESAssetManager startMissingAssetDownload]_block_invoke
: %@
%@: ModelInfo=%@: AssetId=%@:
com.apple.internal.ck
disableTrialAssetDelivery
-[ESAssetManager trialAssetDeliveryEnabled:]
enableTrialAssetDelivery
trial_dictation_asset_delivery
-[ESAssetManager prepareHammerConfigFile:]
-[ESAssetManager _installedGeoLMRegionMappingForLanguage:]
-[ESAssetManager geoLMRegionIdForLanguage:location:]
-[ESAssetManager installedGeoLMRegionSpecificAssetForLanguage:regionId:mainAssetConfig:]
-[ESAssetManager _geoLMCompatibleWithMainAsset:geoAssetConfig:]
-[ESAssetManager purgeUnusedGeoLMAssetsForLangauge:]_block_invoke
q24@?0@8@16
-[ESAssetManager purgeUnusedGeoLMAssetsForLangauge:]_block_invoke_2
-[ESAssetManager lastUsedGeoLMRegionIdForLanguage:]
-[ESAssetManager _purgeUserDefaultsGeoLMAssetsInfoDictExceptLanguages:]
EnumerateInstalledAssets
confidenceThresholds
confidenceModels
wordConfidenceThreshold
utteranceConfidenceThreshold
continuousListening
shouldHandleCapitalization
QuasarModel
replicateDataPacketPersonalization
distributedEvaluation
adaptation
WallRTF
DecodeDuration
AverageActiveTokensPerFrame
lm_interp_weights
jitQueryDurationInMs
jitLmeDurationInMs
jitDataStats
version
data
language
assetPath
PERSONALINFO
com.apple.fides.asr
com.apple.siri.speech-dictation-personalization
dictation
ModelOverrideURL
TP/tantor/voice_commands
textfield-editing-suite.plist
com.apple.siri.ESConnection
-[ESConnection initWithXPCConnection:]
-[ESConnection initWithXPCConnection:]_block_invoke
-[ESConnection dealloc]
com.apple.siri.ESConnection.fidesEval
v24@?0@"NSDictionary"8@"NSError"16
-[ESConnection fetchModelInfoForAssetConfig:triggerDownload:completion:]
Corrupt assets found at: %@
-[ESConnection fetchModelPropertiesForAssetConfig:completion:]_block_invoke
-[ESConnection getOfflineAssetStatusIgnoringCache:assetType:withDetailedStatus:withCompletion:]
SiriX
enableTelemetry=YES
+[ESConnection _speechRecognizerWithAssetConfig:geoLMRegionId:enableITN:overrides:modelOverrideURL:preloadModels:requestSource:enableParallelLoading:isHighPriority:geoLMLoadedOut:error:]
enableParallelLoading
preheatSource
ASR task for memory lock
com.apple.assistant
taskForMemoryLock
dictation_emoji_recognition
dictation_voice_commands
itn_s.enh
Failed to create recognizer from %@
-[ESConnection preheatSpeechRecognitionWithAssetConfig:preheatSource:modelOverrideURL:]
AlreadyLoaded
Preheating
 with CustomModelURL %@
Success
(none)
Failure
Preheat time interval in seconds: %lf
-[ESConnection shouldWriteDictationRecord:]
v24@?0@"CESRModelProperties"8@"NSError"16
-[ESConnection startSpeechRecognitionWithParameters:didStartHandlerWithInfo:]
Recognizer is busy
@"NSDictionary"8@?0
jitDataProcessing
-[ESConnection startSpeechRecognitionWithParameters:didStartHandlerWithInfo:]_block_invoke
-[ESConnection sendSpeechCorrectionInfo:interactionIdentifier:]_block_invoke
\jit
-[ESConnection readProfileAndUserDataWithLanguage:allowOverride:completion:]
Not a dictionary: %@
Not an array: %@
orth
prons
freq
v32@?0@"NSString"8@"NSArray"16^B24
updateSpeechProfileWithLanguage is currently unsupported.
Cooldown timeout for EAR
+[ESConnection _scheduleCooldownTimer]
com.apple.siri.embeddedspeech.keepalive
+[ESConnection _cancelCooldownTimer]
+[ESConnection _cooldownTimerFired]
+[ESConnection _cachedRecognizerCleanUp]
+[ESConnection purgeOutdatedAssets]_block_invoke
+[ESConnection prepareToExit]
+[ESConnection _sendPendingAnalyticsEvents]
recipeType
-[ESConnection readTableFromURL:]
-[ESConnection runRecipeEvaluationDistributedEvaluation:recordData:attachments:completion:]
returnHypothesis
returnOverallWER
returnOverallRTF
returnPerUtteranceWER
returnPerUtteranceRTF
locale
task
sampleRate
wordSenseWhitelist
wav.scp
raw.ref
-[ESConnection runRecipeEvaluationDistributedEvaluation:recordData:attachments:completion:]_block_invoke
token
confidence
transcription
\\\S+$
EditDistance
details
modelVersion
overall-rtf
overall-wer
v24@?0@"NSData"8@"NSString"16
SiriCoreLocalSpeechUserData
-[ESConnection runDefaultAdaptationEvaluation:recordData:attachments:completion:]
Recipe has no profile
Voicemail audio file deleted by user
%@ %@
.model
Malformed overrides
ConfidenceMean
ConfidenceMin
ConfidenceMax
AlternateConfidenceMean
AlternateConfidenceMin
AlternateConfidenceMax
Baseline
CustomModel
ModelVersion
Adapted
Alignment
name
callStackReturnAddresses
callStackSymbols
reason
userInfo
(unknown C++ exception)
-[ESConnection runCorrectedTextEvaluationWithAudioDatas:recordDatas:language:samplingRate:caseSensitive:skipLME:wordSenseAccessListSet:completion:]_block_invoke
No tokenizer for %@
Interrupted corrected text evaluation during speech recognition
correctedOutput
recognizedOutput
editDistanceRecognizedCorrected
editDistanceRecognizedTTSASR
timestamp
interactionId
asrSelfComponentIdentifier
results
-[ESConnection _modelRootWithAssetConfig:modelOverridePath:overrides:error:]
asrSelfComponentId
asset
applicationName
metrics
tokens
alignments
uttInfos
uttInfosCompressed
alignmentReferences
usePersonalizedLM
corrected
recognized
useJIT
disableAOT
contextualData
overrideFiles
restoreAOT
evaluations
-[ESConnection runEvaluationWithDESRecordDatas:language:recipe:attachments:fidesPersonalizedLMPath:fidesPersonalizedLMTrainingAsset:scrubResult:completion:]_block_invoke
Unknown evaluation name found in alignmentReferences: %@
scoreNbest
compress
Interrupted evaluation redecoding
-[ESConnection runEvaluationWithDESRecordDatas:language:recipe:attachments:fidesPersonalizedLMPath:fidesPersonalizedLMTrainingAsset:scrubResult:completion:]_block_invoke_2
Interrupted evaluation tokenization
-[ESConnection _deleteTemporaryDirectoryIfExists:]
-[ESConnection resetCacheAndCompileAllAssetsWithCompletion:]
-[ESConnection pauseRecognition]
-[ESConnection resumeRecognitionWithPrefixText:postfixText:selectedText:]
-[ESConnection _writeDESRecord:oneRecordPerDay:]
-[ESConnection speechRecognizer:didProduceEndpointFeaturesWithWordCount:trailingSilenceDuration:eosLikelihood:pauseCounts:silencePosterior:processedAudioDurationInMilliseconds:]_block_invoke
Dictation
\entity-first
-[ESConnection speechRecognizer:didProduceLoggablePackage:]_block_invoke
-[ESConnection speechRecognizer:didRecognizePartialResult:]_block_invoke
Unsupported EAR build?
Unsupported EAR package build?
DUMMYTOKEN
-[ESConnection speechRecognizer:didRecognizeFinalResultCandidatePackage:]_block_invoke
-[ESConnection speechRecognizer:didRecognizeFinalResultPackage:]_block_invoke
RECOGNITION_SUCCESS
RECOGNITION_FAILED
RECOGNITION_CANCELLED
RECOGNITION_REJECTED
-[ESConnection speechRecognizer:didFinishRecognitionWithError:]_block_invoke
recognizer-components
audioDurationMs
recognitionDurationMs
@sum.self
EagerUsed
numLmeDataStreams
PM-decoder
PM-input
PM-output
tokenName
-[ESConnection speechRecognizer:didRecognizeRawEagerRecognitionCandidate:]_block_invoke_2
v16@?0@"_EARFormatter"8
-[ESConnection speechRecognizer:didRecognizeRawEagerRecognitionCandidate:]_block_invoke
lastPathComponent == %@
_ESDecompressArchiveWithURL
VoiceTriggerFidesArchive
Failed to specify compression algorithm
Failed to specify format
Failed to open file for reading
Unable to extract file to: %@
Task %@ not available for %@, supported tasks are %@
etiquette.json
ReplacementDictionaryForAssetConfig
ReplacementDictionaryForAssetConfig_block_invoke
v32@?0@"NSString"8@16^B24
voicemail_confidence_subtraction.json
sampling rate = %@
task type = %@
far field = %@
device ID = %@
Bluetooth device ID = %@
ANE context = %@
CPU context = %@
GPU context = %@
ReadAudioDataFromFileURL
\\\S*$
Insertions
Deletions
Substitutions
ReferenceSize
\contact-first
CONTACTFIRSTNAME
\contact-middle
CONTACTMIDDLENAME
\contact-last
CONTACTLASTNAME
\contact-nickname
\app-first
APPNAMEFIRSTNAME
\company-first
COMPANYFIRSTNAME
\interaction-first
\interaction-middle
\interaction-last
INLINEFIRSTNAME
\contact-first-derived
\contact-middle-derived
\contact-last-derived
\contact-nickname-derived
Recipe evaluation failed
Empty recognition Result
Error
UtteranceLength
NumberOfNonTerminals
WordsAboveThreshold
UtteranceAboveThreshold
Override file is not found in attachments or device
restoreJIT
configurationFile
JIT LME: JIT profile builder is not initialized
JIT LME: required configuration/file is missing
JIT LME: configuration file is not found in attachments or device
zlibCompressedJson
dictationUIInteractionIdentifier
interactionIdentifier
samplingTimestamp
codec
samplingRate
inferenceSpeakerCode
numTrainedFrames
trainingNnetVersion
isSpeakerCodeUsed
-[ESStoreAudioData initWithUUIDString:language:task:codec:samplingRate:inferenceSpeakerCode:numTrainedFrames:trainingNnetVersion:isSpeakerCodeUsed:isSamplingForDictation:selfLogger:]
-[ESStoreAudioData saveAudioToDisk]
-[ESStoreAudioData _deleteItemAtPath:]
-[ESStoreAudioData _cleanupCacheAndReset:]
-[ESStoreAudioData _saveAudioToCache:]
-[ESStoreAudioData _moveAudioToVarMobile:]
%@.plist
-[ESStoreAudioData _saveAudioMetadataToFilePath:]
-[ESStoreAudioData _createAudioFilePath]
%.0f
%@_%@_%@.pcm
unixTime
samplingDate
success
failed
errorCode
errorDomain
UNDEFINED
description
underlyingErrorCode
underlyingErrorDomain
Audio file to be moved nil
Sampling Date is nil
Unable to create sampling directory
Unable to create dated directory
status
com.apple.siri.embeddedspeech
com.apple.private.des-service
Rejecting %@, no %@ entitlement
Rejecting %@, no %@ or %@ entitlement
main_block_invoke
init
_isLoggingAllowedForCurrentRequestWithTask:isSpeechAPIRequest:
UUID
_isTier1LoggingAllowedForCurrentRequestWithTask:
sharedPreferences
isDictationHIPAACompliant
siriDataSharingOptInStatus
isEqualToString:
setWithObjects:
containsObject:
initWithNSUUID:
setUuid:
setComponent:
setSource:
setTarget:
sharedStream
emitMessage:
setExists:
numberWithUnsignedLongLong:
setTimestampInTicks:
setStartedOrChanged:
captureSnapshot
context
initWithPreheatContext:powerSnapshot:powerEventContext:
setFailed:
setStatus:
setEnded:
countByEnumeratingWithState:objects:count:
preheatContext
timestampInTicks
wrapAndEmitTopLevelEvent:timestampInTicks:
powerSnapshot
powerEventContext
logWithEventContext:asrIdentifier:
setStarted:
setLinkId:
setDialogContexts:
setTask:
stringByReplacingOccurrencesOfString:withString:
convertLanguageCodeToSchemaLocale:
setModelLocale:
boolValue
setIsHighQualityAsset:
unsignedLongLongValue
setSpeechProfileAgeInNs:
initWithUUIDString:
setDictationUiInteractionId:
setDatapackVersion:
setHammerVersion:
setGeoLanguageModelRegion:
setGeoLanguageModelLoaded:
setPortraitExperimentVariantName:
logIntermediateUtteranceInfoTier1WithPmInput:pmOutput:unrepairedPostItn:loggableSharedUserId:
setPhoneticMatchInput:
setPhoneticMatchOutput:
setUnrepairedPostItn:
recognition
interpretationIndices
count
isFinal
setIsFinal:
setPackage:
resultCandidateId
stringValue
setResultCandidateId:
correctPartialResultIndexList
setCorrectPartialResultIndexLists:
preITNRecognition
oneBest
silenceStart
utteranceStart
numberWithDouble:
addObject:
setTokenSilenceStartTimeInNsLists:
firstResultAfterResume
setIsAfterResume:
componentsJoinedByString:
UTF8String
setPersonalizedLanguageModelAgeInNs:
floatValue
setPersonalizedLanguageModelWeight:
setAverageActiveTokensPerFrame:
setSignalToNoiseRatioInDecibels:
setRecognitionDurationInNs:
setAudioDurationInNs:
setEagerEnabled:
setCpuRealTimeFactor:
setNumLanguageModelEnrollmentDataStreams:
setUtteranceDetectionEnabled:
setUtteranceConcatenationEnabled:
setContinuousListeningEnabled:
setPhoneticMatchDecoderName:
setInverseTextNormalizationDurationInNs:
setIsEmojiPersonalizationUsed:
setIsEmojiDisambiguationUsed:
setIsEmojiExpectedButNotRecognized:
setRecognizedEmojis:
setEmojiMetrics:
componentsSeparatedByString:
objectAtIndexedSubscript:
setFrontend:
setDecoder:
setDecodable:
setRecognizerComponents:
doubleValue
setStartTimeInNs:
setEndTimeInNs:
valueForKey:
setWeights:
setLanguageModelInterpolationWeights:
array
setPausedAudioDurationsInNs:
code
setMetrics:
setReason:
setCancelled:
setErrorCode:
domain
setErrorDomain:
userInfo
objectForKey:
setUnderlyingErrorCode:
setUnderlyingErrorDomain:
setSampledAudioStorageFailureReason:
_isNonTier1Message:
deleteLinkId
methodForSelector:
setAsrId:
setEventMetadata:
setPreheatContext:
setRequestContext:
setPartialResultGenerated:
setPackageGenerated:
setRecognitionResultTier1:
setFinalResultGenerated:
setIntermediateUtteranceInfoTier1:
setInitializationContext:
setAssetLoadContext:
setLanguageModelEnrollmentContext:
setJitLanguageModelEnrollmentEndedTier1:
setAudioPacketArrivalContext:
setFirstAudioPacketProcessed:
setFinalAudioPacketContainingSpeechReceived:
setSampledAudioFileStored:
setSampledAudioFileStorageFailed:
setAppleNeuralEngineCompilationContext:
setEmbeddedSpeechProcessContext:
emitMessage:timestamp:
initWithCurrentProcess
logWithEventContext:
createPreheatStartedOrChangedEvent
createPreheatFailedEvent
createPreheatEndedEventWithPreheatAlreadyDone:
createANECompilationStartedEventWithTimeStamp:
createANECompilationEndedEventWithTimeStamp:
initializeSharedPowerLoggerIfNeeded
logPowerSnapshotForProcessStarted
logPowerSnapshotForProcessEnded
initWithTask:isSpeechAPIRequest:
logRequestLinkWithRequestId:
logPendingPreheatContextEvents:
logESStartWithTimeInTicks:
logPendingANECompilationContextEvents:
logInitializationStartedOrChangedWithTimeInTicks:
logInitializationEnded
logAssetLoadStartedOrChanged
logAssetLoadEnded
logJitLmeStartedOrChangedWithTimeInTicks:
logJitLmeEndedAndEndedTier1WithDialogContext:
logAudioPacketArrivalStartedOrChangedWithTimeInTicks:
logAudioPacketArrivalEndedWithTimeInTicks:
logFirstAudioPacketProcessed
logFinalAudioPacketContainingSpeechReceivedWithTimeInTicks:loggableSharedUserId:
logRequestStartedOrChangedWithTask:modelLocale:modelVersion:isHighQualityAsset:hammerVersion:geoLanguageModelRegion:geoLanguageModelLoaded:speechProfileAgeInSec:dictationUIInteractionId:portraitExperimentVariantName:
logPartialResultGenerated
logIntermediateUtteranceInfoTier1WithPmInput:pmOutput:
logIntermediateUtteranceInfoTier1WithUnrepairedPostItn:loggableSharedUserId:
logPackageGeneratedAndRecognitionResultTier1WithEARPackage:loggableSharedUserId:
logFinalResultGeneratedWithEARPackage:
logRequestEndedOrFailedOrCancelledWithError:recognizerComponents:averageActiveTokensPerFrame:languageModelInterpolationWeights:signalToNoiseRatioInDecibels:recognitionDurationInSec:audioDurationMs:eagerUsed:utteranceDetectionEnabled:utteranceConcatenationEnabled:cpuRealTimeFactor:numLmeDataStreams:phoneticMatchDecoderName:pauseDurations:itnDurationInNs:isEmojiPersonalizationUsed:isEmojiDisambiguationUsed:isEmojiExpectedButNotRecognized:recognizedEmojis:
logSampledAudioFileStoredSuccessfully
logSampledAudioFileStoredWithError:customFailureReason:
asrId
recognitionTask
unrepairedPostItn
personalizedLmWeight
setPersonalizedLmWeight:
personalizedLmAgeInSec
setPersonalizedLmAgeInSec:
continuousListeningEnabled
.cxx_destruct
_asrId
_recognitionTask
_packageLogged
_isTier1LoggingAllowedForCurrentRequest
_continuousListeningEnabled
_unrepairedPostItn
_personalizedLmWeight
_personalizedLmAgeInSec
T@"NSUUID",R,N,V_asrId
T@"NSString",R,N,V_recognitionTask
T@"NSString",&,N,V_unrepairedPostItn
T@"NSNumber",&,N,V_personalizedLmWeight
T@"NSNumber",&,N,V_personalizedLmAgeInSec
TB,N,V_continuousListeningEnabled
T@"NSNumber",&,N
_preheatContext
_powerSnapshot
_powerEventContext
T@"ASRSchemaASRPreheatContext",R,N,V_preheatContext
T@"SPIPowerLoggerSnapshot",R,N,V_powerSnapshot
T@"SPIEventContext",R,N,V_powerEventContext
setRawRecognition:
setPostItn:
tokenSausage
setPhrases:
setUtterances:
linkId
orderedSetWithArray:
setInterpretationIndices:
objectAtIndex:
unsignedIntegerValue
setTokens:
indexOfObject:
numberWithUnsignedInteger:
arrayByAddingObject:
enumerateObjectsUsingBlock:
copy
setInterpretations:
initWithCapacity:
start
hasSpaceAfter
setAppendSpaceAfter:
setSilenceStartTimeInNs:
confidence
setConfidence:
appendedAutoPunctuation
prependedAutoPunctuation
setIsAutoPunctuation:
isModifiedByAutoPunctuation
setIsModifiedByAutoPunctuation:
tokenName
length
punctuationCharacterSet
rangeOfCharacterFromSet:
setPunctuationText:
setText:
phoneSequence
setPhoneSequence:
ipaPhoneSequence
setIpaPhoneSequence:
setLinkIndex:
_UUID
invalidate
setInterruptionHandler:
setInvalidationHandler:
dealloc
initWithLanguage:assetType:
_profileWithError:
longLongValue
dictionaryWithObjects:forKeys:count:
errorWithDomain:code:userInfo:
supportedCategories
allKeys
stringWithFormat:
addEntriesFromDictionary:
itemFromBuffer:error:
setIsBoosted:
addObjectsFromArray:
setObject:forKeyedSubscript:
initWithItems:language:
enumerateKeysAndObjectsUsingBlock:
removeAllObjects
setTemplateToVersion:
setUserId:
personalizationRecipeForAssetConfig:modelOverridePath:
adaptUserProfileWithUserData:personalizationRecipe:userData:endOfUserData:
removeLmeDataForTemplateName:
profileFilePathFromBasePath:language:userId:
stringByDeletingLastPathComponent
defaultManager
createDirectoryAtPath:withIntermediateDirectories:attributes:error:
writeProfileToFile:protectionClass:length:error:
userProfileWithAssetConfig:modelOverridePath:overrides:isJit:returningFoundPath:error:
fileExistsAtPath:
readUserProfileWithPath:reuseProfile:
templateToVersion
mutableCopy
_isProfileValidFromVersionsMap:
allValues
language
sharedInstance
installedQuasarModelPathForAssetConfig:error:triggerDownload:
stringByAppendingPathComponent:
dataWithContentsOfFile:options:error:
JSONObjectWithData:options:error:
assetType
switchToNewAssetsForAssetType:
installedQuasarModelPathForAssetConfig:error:triggerDownload:ignoreSpellingModel:
initWithConfiguration:language:overrides:sdapiOverrides:generalVoc:emptyVoc:pgVoc:lexiconEnh:tokenEnh:paramsetHolder:isJit:
removeAllWords
contactWordsWithFrequency
addWordsToUserProfile:templateName:wordArrays:
vocabularyWords
tagName
initWithOrthography:pronunciations:tagName:frequency:
arrayWithObjects:count:
templateName
addWordWithParts:templateName:
corrections
pronunciationsForOrthography:
appNames
initWithOrthography:pronunciations:tag:
_adaptRecipe:userData:profile:
signalEndOfUserData
_parseRequiredParameter:expectedClass:domain:recipe:error:
characterSetWithCharactersInString:
_runAdaptationRecipeForDomain:frequency:tagNames:templateName:charSetToTrim:charSetToSplit:minimumWordLength:userData:profile:
components
objectForKeyedSubscript:
removeObjectForKey:
frequency
firstObject
_runAdaptationRecipeForKeyboardLMWithFrequency:tagName:templateName:charSetToTrim:charSetToSplit:minimumWordLength:userData:profile:
locationOfInterestNames
_runAdaptationRecipeForLocationOfInterestWithFrequency:tagName:templateName:charSetToTrim:charSetToSplit:minimumWordLength:locationOfInterestNames:profile:
spatialLocationOfInterestNames
interactionSenderDisplayNames
_runAdaptationRecipeForNamesWithFrequency:tagNames:templateName:charSetToTrim:charSetToSplit:minimumWordLength:nameFrequencies:profile:
searchEventValues
_runAdaptionRecipeForCalendarEventsWithFrequency:tagName:templateName:charSetToTrim:charSetToSplit:minimumWordLength:userData:profile:
pexNamedEntityNames
setObject:forKey:
componentsSeparatedByCharactersInSet:
lastObject
keyboardLMDynamicVocabularyItems
dictionaryWithDictionary:
whitespaceCharacterSet
countForObject:
eventTitles
eventLocationNames
arrayByAddingObjectsFromArray:
stringByTrimmingCharactersInSet:
isEqual:
class
self
performSelector:
performSelector:withObject:
performSelector:withObject:withObject:
isProxy
isKindOfClass:
isMemberOfClass:
conformsToProtocol:
respondsToSelector:
retain
release
autorelease
retainCount
zone
hash
superclass
description
debugDescription
TQ,R
T#,R
T@"NSString",R,C
setProfileConfigWithLanguage:profileDir:userId:dataProtectionClass:completion:
getVersionForCategory:completion:
beginWithCategoriesAndVersions:bundleId:completion:
addVocabularyItems:isBoosted:completion:
cancelWithCompletion:
finishAndSaveProfile:completion:
initWithXPCConnection:
_stagedItems
_committedItems
_stagedCategoryToVersion
_committedCategoryToVersion
_seenCategories
_baseDirectory
_language
_userId
_dataProtectionClass
_assetConfig
_connection
_transaction
_profile
markRecognition
sendEvent
recognitionEndTime
setRecognitionEndTime:
applicationName
setApplicationName:
interactionId
setInteractionId:
taskName
setTaskName:
hasRecognizedAnything
_hasRecognizedAnything
_recognitionEndTime
_applicationName
_interactionId
_taskName
Td,N,V_recognitionEndTime
T@"NSString",C,N,V_applicationName
T@"NSString",C,N,V_interactionId
T@"NSString",C,N,V_taskName
TB,R,N,V_hasRecognizedAnything
loadDate
setLoadDate:
T@"NSDate",C,N
timeZoneWithAbbreviation:
setTimeZone:
dateWithTimeIntervalSince1970:
date
currentCalendar
components:fromDate:toDate:options:
utterances
setStartTime:
setSilenceStartTime:
setEndTime:
setRemoveSpaceBefore:
setConfidenceScore:
initWithPhrases:utterances:processedAudioDuration:
afRecognitionForEARSausage:processedAudioDuration:
audioAnalytics
latticeMitigatorResult
intValue
recognitionPaused
initWithRecognition:unfilteredRecognition:rawRecognition:audioAnalytics:isFinal:utteranceStart:latticeMitigatorResult:recognitionPaused:speechProfileUsed:resultCandidateId:
path
speechProfileRootDirectories
enumeratorAtPath:
pathWithComponents:
speechProfilePathsWithLanguage:
attributesOfItemAtPath:error:
timeIntervalSinceReferenceDate
initWithPath:error:
hasSpaceBefore
appendString:
stringByAppendingFormat:
arguments
presence
indexes
substringWithRange:
enumerateRangesUsingBlock:
initWithText:
voiceCommandsParamKeyBuilder:
commandIdentifier
initWithCommandId:isComplete:paramMatches:
initWithUtterance:parseCandidates:
tokens
earTokensToString:
voiceCommandInterpretations
afVoiceCommandGrammarParseResultForEARTokenString:withEARVoiceCommandInterpretations:
preITNTokens
preITNVoiceCommandInterpretations
initWithNBestParses:preITNNBestParses:
initWithCommandGrammarParsePackage:
nBest
nBestVoiceCommandInterpretations
preITNNBestVoiceCommandInterpretations
text
startTime
endTime
silenceStartTime
confidenceScore
removeSpaceAfter
removeSpaceBefore
initWithTokenName:start:end:silenceStart:confidence:hasSpaceAfter:hasSpaceBefore:phoneSequence:ipaPhoneSequence:appendedAutoPunctuation:
fileExistsAtPath:isDirectory:
calculateDiffInDaysFromTimestamp:
hasRecognizedAnythingInAFSpeechPackage:
afTokensForEARTokens:removeSpaceBefore:
afSpeechPackageForEARPackage:processedAudioDuration:speechProfileUsed:
loadSpeechProfiles:language:
AFSpeechInfoPackageForEARSpeechRecognitionResult:
AFSpeechInfoPackageForEARSpeechRecognitionResultPackage:
earTokensForAFTokens:appendedAutoPunctuation:
mapContextOptionToString:
languageStringForLocaleString:
localeStringForLanguageString:
isFilePathValid:
initWithInterpretationIndices:confidenceScore:
setIsLowConfidence:
interpretations
acousticFeatures
setValue:forKey:
speechRecognitionFeatures
initWithSpeechRecognitionFeatures:acousticFeatures:snr:
acousticFeatureValuePerFrame
frameDuration
initWithAcousticFeatureValue:frameDuration:
score
threshold
version
initWithResults:score:threshold:
_es_quasarModelPath
purgeCompiledRecognizerModelsWithConfiguration:
purgeSync
_es_isInstalled
_es_contentVersion
_es_masteredVersion
_es_language
_es_path
attributes
getLocalFileUrl
state
refreshState
_es_purgeSync
_es_description
_es_status
_es_quasarDir
_es_preferOverServer
_es_supportsContinuousListening
_es_supportsOnDeviceSearch
registerNotifications
dictionary
_invalidateInstallationStatusCacheForAssetType:
trialAssetDeliveryEnabled:
installationStatusForLanguagesForAssetType:includeDetailedStatus:error:
_assetQueryForLanguage:
queryMetaDataSync
numberWithInteger:
results
modelTypeStatusStringAndVersionWithAsset:
installedModelInfoForAssetConfig:error:
installedModelInfoForAssetConfig:error:triggerDownload:
installedModelInfoForAssetConfig:error:triggerDownload:ignoreSpellingModel:
dictationIsEnabled
assistantIsEnabled
installedAssetWithConfig:regionId:triggerDownload:
modelQualityTypeStatusStringWithConfig:
numberWithBool:
setAssetsProvisionalForAssetType:
promoteAssetsForAssetType:
initWithType:
returnTypes:
addKeyValuePair:with:
_purgeUserDefaultsGeoLMAssetsInfoDictExceptLanguages:
purgeInstalledAssetsExceptLanguages:assetType:error:
languageCode
activeDictationLanguages
_queryInstallationStatusForLanguagesWithError:
_purgeMobileAssetsForLanguage:error:
installationStatusForLanguagesIgnoringCache:assetType:withDetailedStatus:withError:
installedAssetWithConfig:
modelAttributesStatusStringWithAsset:
assetId
initWithSuiteName:
isSiriXEnabled
jsonFilenameForAssetType:
initWithConfig:
installedAssetWithConfig:regionId:
_updateGeoLMAssetsInfoDictWithRegionId:language:
_geoLMCompatibleWithMainAsset:geoAssetConfig:
_loadGeoLMAssetsInfoDictForLanguage:
timeIntervalSince1970
_updateUserDefaultsWithGeoLMAssetsInfoDict:language:
compare:
keysSortedByValueUsingComparator:
purgeInstalledAssetForAssetType:language:regionId:error:
supportedLanguagesWithAssetType:
_userDefaultsGeoLMAssetsInfoDictKeyForLangauge:
standardUserDefaults
dictionaryForKey:
stringByAppendingString:
installedQuasarModelPathForAssetConfig:error:
prepareModelInfo:withAssetType:
promoteModelInfo:withAssetType:
isBelowLimitForLocale:
purgeOutdatedAssets
startMissingAssetDownload
installedHammerConfigFileForLanguage:
prepareHammerConfigFile:
promoteHammerConfigFile
_installedGeoLMRegionMappingForLanguage:
geoLMRegionIdForLanguage:location:
installedGeoLMRegionSpecificAssetForLanguage:regionId:mainAssetConfig:
purgeUnusedGeoLMAssetsForLangauge:
lastUsedGeoLMRegionIdForLanguage:
_queue
_languageInstallationCache
_dictationAssetUpdatedNotificationToken
_assistantAssetUpdatedNotificationToken
_recognizerAssetPathsInUse
_profileAssetPathsInUse
_geoLMAssetsInfoDict
numberWithLongLong:
allocWithZone:
numberOfInsertions
setNumberOfInsertions:
numberOfSubstitutions
setNumberOfSubstitutions:
numberOfDeletions
setNumberOfDeletions:
totalCost
setTotalCost:
copyWithZone:
incrementInsertions
incrementDeletions
incrementSubstitutions
incrementCost
_numberOfInsertions
_numberOfDeletions
_numberOfSubstitutions
_totalCost
Tq,N,V_numberOfInsertions
Tq,N,V_numberOfDeletions
Tq,N,V_numberOfSubstitutions
Tq,N,V_totalCost
setModelType:
modelType
setModelRoot:
modelRoot
T@"NSString",C,N
infersQoSFromInstanceUUIDForEAR
_setQueue:
cancelRecognition
remoteObjectProxy
fetchAssetsForAssetConfig:completion:
fetchModelInfoForAssetConfig:completion:
fetchModelInfoForAssetConfig:triggerDownload:completion:
setWithArray:
replaceCorruptAssetWithConfig:
initWithModelVersion:modelType:modelRoot:
getOfflineAssetStatusIgnoringCache:assetType:withDetailedStatus:withCompletion:
getOfflineAssetStatusIgnoringCache:assetType:withCompletion:
_speechRecognizerWithAssetConfig:geoLMRegionId:enableITN:overrides:modelOverrideURL:preloadModels:requestSource:enableParallelLoading:isHighPriority:geoLMLoadedOut:error:
processInfo
systemUptime
activeConfigurationForEverything
_bestGuessTaskStringsFromPreheatSource:
taskTypeFilter
setByAddingObjectsFromSet:
setTaskTypeFilter:
setSamplingRateFilter:
setFarFieldFilter:
setDeviceIdFilter:
setBluetoothDeviceIdFilter:
setAneContextFilter:
setCpuContextFilter:
setGpuContextFilter:
initWithConfiguration:useQuasarFormatter:activeConfiguration:
dictionaryWithContentsOfFile:
initWithPlistJSONDictionary:
setWithObject:
fileURLWithPath:isDirectory:
initWithSuites:resourceBaseURL:
initWithConfiguration:overrides:overrideConfigFiles:generalVoc:lexiconEnh:itnEnh:language:activeConfiguration:modelLoadingOptions:enableSpeakerCodeTraining:supportEmojiRecognition:voiceCommandActiveSet:modelContextDelegate:
instancesRespondToSelector:
initWithConfiguration:overrides:overrideConfigFiles:generalVoc:lexiconEnh:itnEnh:language:activeConfiguration:modelLoadingOptions:enableSpeakerCodeTraining:supportEmojiRecognition:voiceCommandActiveSet:
initWithConfiguration:overrides:overrideConfigFiles:language:activeConfiguration:modelLoadingOptions:enableSpeakerCodeTraining:supportEmojiRecognition:voiceCommandActiveSet:modelContextDelegate:
initWithConfiguration:overrides:overrideConfigFiles:language:activeConfiguration:modelLoadingOptions:enableSpeakerCodeTraining:supportEmojiRecognition:voiceCommandActiveSet:
setHighPriority:
setDetectUtterances:
setRecognizeEagerCandidates:
setConcatenateUtterances:
logLocalRecognitionLoadedForLanguage:duration:
preheatSpeechRecognitionWithAssetConfig:preheatSource:modelOverrideURL:
_clearPendingAnalyticsEvents
_addPendingAnalyticsEvent:
_clearPendingSelfEvents
_addPendingSelfPreheatEvent:
_scheduleCooldownTimer
activeConfigurationForNothing
setActiveConfiguration:
allObjects
weakObjectsHashTable
modelVersion
startSpeechRecognitionWithParameters:didStartHandlerWithInfo:
_sendPendingAnalyticsEvents
startRequestActivityWithCompletion:
overrides
continuousListening
shouldHandleCapitalization
modelOverrideURL
task
initWithLanguage:task:
location
isSpeakerCodeTrainingSupported:
interruptTraining
isEqualToDictionary:
modelInfo
_delegate
speechServiceDidSelectRecognitionModelWithModelProperties:
isSpeechAPIRequest
speechProfileDataLastModifiedDataForLanguage:
timeIntervalSinceNow
defaultStore
variantNameWithError:
dictationUIInteractionIdentifier
requestIdentifier
censorSpeech
setRecognitionReplacements:
setRecognitionConfidenceSubtraction:
disableDeliveringAsrFeatures
endpointStart
setEndpointStart:
profile
setUserProfileData:
setUserProfile:
_modelRootWithAssetConfig:modelOverridePath:overrides:error:
initWithConfiguration:taskName:applicationName:
jitGrammar
fetchNamedEntitiesWithTimeInterval:
containsEntity
_userProfileWithAssetConfig:modelOverridePath:overrides:isJit:returningFoundPath:error:
createInlineLmeUserDataForContextStrings:
dataProfile
contextualData
createInlineLmeUserDataForContextData:speechProfile:
inputOrigin
setInputOrigin:
setExtraLmList:
detectUtterances
deliverEagerPackage
maximumRecognitionDuration
setMaximumRecognitionDuration:
farField
setFarField:
setAllowUtteranceDelay:
setFormatAcrossUtterances:
enableAutoPunctuation
setDisableAutoPunctuation:
enableEmojiRecognition
setRecognizeEmoji:
prefixText
setLeftContextText:
setRightContext:
postfixText
setSelectedText:
selectedText
enableVoiceCommands
setEnableVoiceCommands:
narrowband
codec
activeConfiguration
farFieldFilter
setByAddingObject:
samplingRateFilter
setJitProfileData:
runRecognitionWithResultStream:speakerCodeWriter:language:task:samplingRate:
sharedManager
isRequestSelectedForSamplingFromConfigForLanguage:
updateRequestCountWithFlag:
shouldStoreAudioOnDevice
isRequestSelectedForSamplingForTask:
secureOfflineOnly
siriDataSharingOptedIn
loggingContext
UUIDString
recordWithLanguage:task:context:narrowband:farField:interactionIdentifier:asrSelfComponentIdentifier:pluginId:
setProfile:
originalAudioFileURL
setOriginalAudioFileURL:
shouldWriteDictationRecord:
recordWithLanguage:task:context:narrowband:farField:interactionIdentifier:asrSelfComponentIdentifier:pluginId:frequency:
data
speakerCodeInfo
inferenceSpeakerCode
numFrames
nnetVersion
isSpeakerCodeUsed
initWithUUIDString:language:task:codec:samplingRate:inferenceSpeakerCode:numTrainedFrames:trainingNnetVersion:isSpeakerCodeUsed:isSamplingForDictation:selfLogger:
bytes
addAudioSamples:count:
addAudioPacket:
updateAudioDuration:
endAudio
_cancelCooldownTimer
correctedText
rangeOfString:
initWithExplicitlyRejectedString:
initWithImplicitlyEngagedString:
setClientIdentifier:
registerFeedback:completion:
interactionIdentifier
setCorrectedText:
_writeDESRecord:
offlineDictationProfileOverridePath
dictionaryWithContentsProfilePathForLanguage:errorOut:
objectEnumerator
nextObject
integerValue
initWithDomain:code:userInfo:
_cooldownTimerFired
_cooldownTimerTimeoutSeconds
_cachedRecognizerCleanUp
releaseClients
sharedAnalytics
logEvents:
runDefaultAdaptationEvaluation:recordData:attachments:completion:
runRecipeEvaluationDistributedEvaluation:recordData:attachments:completion:
stringWithContentsOfURL:encoding:error:
whitespaceAndNewlineCharacterSet
substringToIndex:
substringFromIndex:
URLByAppendingPathComponent:
readTableFromURL:
_speechRecognizerWithAssetConfig:enableITN:isHighPriority:error:
_deleteTemporaryDirectoryIfExists:
recognitionResultsWithAudioData:userProfileData:language:task:samplingRate:extraLanguageModel:
regularExpressionWithPattern:options:error:
firstMatchInString:options:range:
range
numberWithFloat:
readProfileAndUserDataWithLanguage:allowOverride:completion:
initForReadingFromData:error:
setClass:forClassName:
decodeObjectOfClass:forKey:
finishDecoding
samplingRate
audioPackets
appendData:
readUserProfile:
userData
fileURLWithPath:
createDirectoryAtURL:withIntermediateDirectories:attributes:error:
writeToURL:atomically:encoding:error:
name
callStackReturnAddresses
callStackSymbols
reason
_fidesEvalQueue
initWithNcsRoot:
_invalidated
recordFromData:
recognizedText
tokenize:
removeObjectAtIndex:
timestamp
asrSelfComponentIdentifier
initWithConfiguration:language:overrides:sdapiOverrides:generalVoc:emptyVoc:pgVoc:lexiconEnh:tokenEnh:paramsetHolder:
initWithContentsOfFile:options:error:
concatenatedAudioPackets
setScoreNbest:
setScoreNbestExtraLmList:
setEnableSpeakerCodeTraining:
recognitionUtterenceStatistics
recognitionUtteranceInfos
removeItemAtURL:error:
compileRecognizerModelsWithConfiguration:
loadConfigs
deleteAllRecordsForPlugin:completion:
pauseRecognition
resumeRecognitionWithLeftContext:rightContext:selectedText:
_writeDESRecord:oneRecordPerDay:
hasData
saveOneRecordPerDay
save
_addPendingSelfANECompilationEvent:
speechServiceDidProduceEndpointFeaturesWithWordCount:trailingSilenceDuration:eosLikelihood:pauseCounts:silencePosterior:processedAudioDurationInMilliseconds:
speechServiceDidProduceLoggablePackage:
rawRecognition
phrases
hasSuffix:
setRecognizedText:
unrepairedRecognition
packetArrivalTimestampFromAudioTime:
speechServiceDidRecognizeTokens:withMetadata:
speechServiceDidRecognizeTokens:
raise:format:
setRemoveSpaceAfter:
initWithInterpretations:isLowConfidence:
initWithRecognition:unfilteredRecognition:rawRecognition:audioAnalytics:isFinal:utteranceStart:latticeMitigatorResult:recognitionPaused:
speechServiceDidRecognizeFinalResultCandidatePackage:
concatenateUtterances
dummyResultPackage:
speechServiceDidRecognizePackage:withMetadata:
speechServiceDidRecognizePackage:
numberWithUnsignedInt:
statusForError:
recognitionStatistics
speechServiceDidFinishRecognitionWithStatistics:error:
recognitionMetrics
pauseDurations
valueForKeyPath:
itnDurationInNs
isEmojiPersonalizationUsed
isEmojiDisambiguationUsed
isEmojiExpectedButNotRecognized
recognizedEmojis
decrementRequestCount
saveAudioToDisk
speechServiceDidProcessAudioDuration:
formattedRecognitionWithNBestList:
speechServiceDidRecognizeRawEagerRecognitionCandidate:
getFormatterWithBlock:
initialize
captureESStartTimeInTicks
prepareToExit
didStartModelInitializing:
didFinishModelInitializing:
getRecognizerQueue
speechRecognizer:didRecognizePartialResult:
speechRecognizer:didFinishRecognitionWithError:
speechRecognizer:didRecognizeFinalResults:
speechRecognizer:didRecognizeFinalResults:tokenSausage:nBestChoices:
speechRecognizer:didRecognizeFinalResultPackage:
speechRecognizer:didProcessAudioDuration:
speechRecognizer:didRecognizeRawEagerRecognitionCandidate:
speechRecognizer:didProduceEndpointFeaturesWithWordCount:trailingSilenceDuration:eosLikelihood:pauseCounts:silencePosterior:processedAudioDurationInMilliseconds:
speechRecognizer:didRecognizePartialResultNbest:
speechRecognizer:didProduceLoggablePackage:
speechRecognizer:didRecognizeFinalResultCandidatePackage:
resetCacheAndCompileAllAssetsWithCompletion:
preheatSpeechRecognitionWithLanguage:modelOverrideURL:
startSpeechRecognitionWithParameters:didStartHandler:
finishAudio
createSpeechProfileWithLanguage:modelOverridePath:JSONData:completion:
updateSpeechProfileWithLanguage:modelOverridePath:existingProfile:existingAssetPath:completion:
getOfflineDictationStatusIgnoringCache:withCompletion:
fetchAssetsForLanguage:completion:
fetchModelPropertiesForAssetConfig:completion:
runAdaptationRecipeEvaluation:recordData:attachments:completion:
runCorrectedTextEvaluationWithAudioDatas:recordDatas:language:samplingRate:caseSensitive:skipLME:wordSenseAccessListSet:completion:
purgeInstalledAssetsExceptLanguages:completion:
purgeInstalledAssetsExceptLanguages:assetType:completion:
setAssetsPurgeabilityExceptLanguages:assetType:
writeDESRecord
sendSpeechCorrectionInfo:interactionIdentifier:
invalidatePersonalizedLM
removePersonalizedLMForFidesOnly:completion:
runEvaluationWithDESRecordDatas:language:recipe:attachments:fidesPersonalizedLMPath:fidesPersonalizedLMTrainingAsset:scrubResult:completion:
deleteAllDESRecordsForDictationPersonalizationWithCompletion:
invalidateUaapLm
resumeRecognitionWithPrefixText:postfixText:selectedText:
_recognizer
_audioBuffer
_shouldStoreDictationAudioOnDevice
_disableDeliveringAsrFeatures
_isHighPriority
_lastRecognizedPackage
_lastRecognizedMetadataPackage
_bufferedAudioPackets
_bufferedAudioEnded
_validDomains
_requestCompletion
_storeAudioData
_biomeRecord
_selfHelper
_samplingRate
_audioDurationMs
_processedAudioDuration
_firstAudioPacketReceivedTime
_firstAudioPacketTimeUntilFirstPartial
_lastAudioPacketReceivedTime
_firstAudioPacketReceivedTimeInTicks
_lastAudioPacketReceivedTimeInTicks
_firstAudioPacketProcessedTime
_localMetrics
_recognitionBeginTime
_recognitionAbsoluteEndTime
_speakerCodeWriter
_weakFidesRecognizer
_lastWordCount
_taskToUse
_resultCandidateId
_desRecord
_desRecordDictation
predicateWithFormat:
filteredArrayUsingPredicate:
fileSystemRepresentation
stringWithUTF8String:
tasks
deviceIdFilter
bluetoothDeviceIdFilter
aneContextFilter
cpuContextFilter
gpuContextFilter
assetWithURL:
tracksWithMediaType:
assetReaderWithAsset:error:
assetReaderAudioMixOutputWithAudioTracks:audioSettings:
canAddOutput:
addOutput:
startReading
copyNextSampleBuffer
dataWithBytes:length:
setObject:atIndexedSubscript:
lowercaseString
stringByReplacingMatchesInString:options:range:withTemplate:
reverseObjectEnumerator
lastPathComponent
metrics
dataWithJSONObject:options:error:
compressedDataUsingAlgorithm:error:
base64EncodedStringWithOptions:
dictionaryRepresentation
setUUIDString:
setLanguage:
component
_createAudioFilePath
_saveAudioToCache:
_moveAudioToVarMobile:
_logAudioSampledEventsWithStatus:error:customReasonForFailure:
_cleanupCacheAndReset:
deleteItemAtFilePath:
_deleteItemAtPath:
cleanupCacheAndReset
writeToFile:options:error:
pathComponents
samplingDateAsString
createSamplingDirectory
moveItemAtPath:toPath:error:
stringByDeletingPathExtension
_saveAudioMetadataToFilePath:
writeToFile:atomically:
_createCachesDirectoryIfItDoesNotExist
sampledCachesSubDirectoryPath
initWithDictionary:
localizedDescription
logEventWithType:context:
_trimAudioIfNeeded:
setCodec:
setSamplingRate:
setAudioPackets:
setHasRecognizedAnything:
numTrainedFrames
trainingNnetVersion
isSamplingForDictation
selfLogger
setSelfLogger:
audioMetadata
setAudioMetadata:
collectedAudioDurationMS
setCollectedAudioDurationMS:
currentAudioFilePath
setCurrentAudioFilePath:
logPrefix
setLogPrefix:
_isSpeakerCodeUsed
_isSamplingForDictation
_UUIDString
_task
_codec
_audioPackets
_inferenceSpeakerCode
_numTrainedFrames
_trainingNnetVersion
_selfLogger
_audioMetadata
_collectedAudioDurationMS
_currentAudioFilePath
_logPrefix
T@"NSString",C,N,V_UUIDString
T@"NSString",C,N,V_language
T@"NSString",C,N,V_task
T@"NSString",&,N,V_codec
TQ,N,V_samplingRate
T@"NSMutableData",&,N,V_audioPackets
TB,N,V_hasRecognizedAnything
T@"NSString",R,N,V_inferenceSpeakerCode
T@"NSNumber",R,N,V_numTrainedFrames
T@"NSNumber",R,N,V_trainingNnetVersion
TB,R,N,V_isSpeakerCodeUsed
TB,R,N,V_isSamplingForDictation
T@"ESSelfHelper",&,N,V_selfLogger
T@"NSMutableDictionary",&,N,V_audioMetadata
Td,N,V_collectedAudioDurationMS
T@"NSString",C,N,V_currentAudioFilePath
T@"NSString",C,N,V_logPrefix
initWithUUIDBytes:
valueForEntitlement:
setExportedInterface:
setExportedObject:
setRemoteObjectInterface:
resume
listener:shouldAcceptNewConnection:
enableTransactions
serviceListener
setDelegate:
%s SELF: Logging object created successfully (logging allowed for current request). asrId=%@, recognitionTask=%@, isSpeechAPIRequest=%@, isHipaaCompliant=%@, siriOptInStatus=%@, isTier1LoggingAllowed=%@
%s SELF: Logging disabled because it is not allowed for the current request. recognitionTask=%@, isSpeechAPIRequest=%@
%s SELF: Correct Partial Result Index List is %s, Silence Start Time List is %s
%s SELF: Encountered malformed string during SELF logging for recognizer components in speech results from recognizer. String: (%@)
%s SELF: Expected three recognizer components separated by delimiter '::'. Ex: 'dnn-rfdnn-aa-cache::dnn-lazy-16k-rfdnn-dictation::msg'
%s SELF: Encountered malformed string during SELF logging for interpolation weights in speech results from recognizer. String: %@
%s SELF: Expected interpolation weight sets separated by delimiter ';' - starting with a set of weights delimited by ',' and ending the with start/end times delimited by ':'. Ex: '0.999646,0.000354:0:4280;0.947514,0.000158:0:3859'
%s SELF: Logging ASRRequestContext->failed in SELF based on error result from recognizer.
%s SELF: Logging ASRRequestContext->cancelled with reason RECOGNITION_CANCELLED in SELF based on error result from recognizer.
%s SELF: Logging ASRRequestContext->cancelled with reason RECOGNITION_REJECTED in SELF based on error result from recognizer.
%s SELF: Logging ASRRequestContext->cancelled with reason RECOGNITION_REJECTED in SELF because nothing was recognized (SpeechNoMatch).
%s SELF: Logging ASRRequestContext->ended in SELF based on success status from recognizer.
%s SELF: Failed trying to wrap and emit top-level ASR event because event type was not mapped to loggable message type in the ASR SELF schema.
%s SELF: Wrapping and logging an event of type %@
%s %@ cancelling instance %@
%s %@.
%s Internal inconsistency error: KVItems list and corresponding isBoosted booleans list are out of sync. This batch of items will default to not being boosted.
%s Failed to deserialize a KVItem. It will be ignored.
%s Starting to process KVItems into CESRUserData.
%s Created CESRUserData from KVItems.
%s %@: %@
%s Adapted profile using CESRUserData according to personalization recipe.
%s Speech profile updated successfully. Wrote %lu bytes to %@
%s Created _EARProfileBuilder from asset config.
%s Existing profile found at %@. It will be reused when updating the profile.
%s Could not locate asset: %@
%s Could not read personalization.json: %@
%s Could not parse personalization.json: %@
%s Set model root to %@
%s Use currently installed asset.
%s Recipe has invalid json for "%{public}@"
%s Recipe has invalid tagName for "%{public}@": "%{public}@"
%s Error executing recipe for domain %{public}@
%s Recipe for %{public}@ is missing "%{public}@"
%s Recipe for %{public}@ contains parameter %{public}@, expected type %{public}@ but got %{public}@
%s User Profile: Starting AddWordsToUserProfile
%s Using name frequencies adaption for names: %@ into slot %@ for template %@
%s Ignoring name part "%@" because it is too short (minimum length is %lu)
%s Using keyboardLMAdaptation adaption for names %@ into slot %@ for template %@
%s Ignoring keyboardDynamicVocabularyItem "%@" because it is too short (minimum length is %lu)
%s Using locationOfInterestNameAdaptationFrequency adaption for names %@ into slot %@ for template %@
%s Ignoring locationOfInterestName "%@" because it is too short (minimum length is %lu)
%s Using eventLocationNameAdaptationFrequency adaption for names %@ into slot %@ for template %@
%s Ignoring calendar event word "%@" because it is too short (minimum length is %lu)
%s Root directories for new type of speech profile: %{private}@
%s speechProfilePathsWithLanguage was incorrectly called with language=nil
%s Mapped language=nil
%s loadSpeechProfiles was incorrectly called with profiles=nil
%s Reused new type of speech profile: path=%{private}@
%s Loaded new type of speech profile: path=%{private}@ profile=%d
%s Count of command interpretation sets does not match count of speech recognition results
%s AFSpeechLatticeMitigatorResult Score = %f, Threshold = %f
%s AFSpeechLatticeMitigatorResult nil
%s Purging compiled assets if there are any.
%s Previously installed asset has been removed: %{public}@
%s Installed asset is corrupt! Triggering emergency purge %{public}@
%s Failed to register for assistant asset update notifications.
%s Failed to register for dictation asset update notifications.
%s Installation status for languages (ignoring cache: %@)
%s Trial asset delivery is enabled!
%s Invalidating installation status cache for %lu
%s Language installation status query failed: %@
%s MobileAsset query failed: %ld
%s Found assets: %@
%s Siri and Dictation are both disabled, no need to access asset
%s Using ASR Trial assets at %@
%s No assets available for language: %@ asset type: %{public}@
%s Purging Trial assets failed: %@
%s Purging outdated assets.
%s Error purging (%@): %ld
%s Purged old asset %@
%s Just ignoring %@
%s Purging asset: %@, language %@
%s Purging failed: %lu
%s Checking for missing assets.
%s Purging all assistant ASR assets except for %@
%s Purging all assistant ASR assets
%s Encountered error trying to purge unused assistant ASR assets: %@
%s Trial asset delivery disabled for assistant assets. Bailing out of missing asset check.
%s Hit error querying installation status of MobileAssets: %@
%s Purging assets from MobileAssets for locale: %@
%s Hit error purging assets from MobileAssets: %d %@
%s Trial asset delivery disabled for dictation assets. Bailing out of missing asset check.
%s Trial asset delivery explicitly disabled!
%s Trial asset delivery explicitly enabled!
%s Hammer model info=%@
%s Exception thrown while reading hammer config
%s GeoLM: region mapping json file=%@
%s GeoLM: region mapping json file is nil Or there is no regionMapping for given language=%@
%s GeoLM: Not Supported
%s GeoLM: region specific [%@] geo-config json file=%@
%s GeoLM: geoLM region specific [%@] asset exists on device, but not compatible.
%s GeoLM: Exception thrown while reading geo-config json
%s GeoLM: region specific asset is not found for given language=%@ regionId=%@
%s GeoLM: model-info.version doesn't match. mainASRModelInfo.version=%@ geoLMModelInfo.version=%@ mainAssetConfig=%@ geoAssetConfig=%@
%s GeoLM: Exception thrown while reading json configs
%s GeoLM: language is nil. Skipping.
%s GeoLM: regionIdToBePurged: %@, lastWhenUsed: %ld days ago
%s GeoLM: regionIdToBePurged: %@, _geoLMAssetsInfoDict count: %ld
%s GeoLM: supportedLanguages count:%ld
%s GeoLM: Going to delete: %@
%s ASR: Using high priority configuration.
%s %@ deallocating
%s Failing to fetch assets for nil language
%s Could not get offline language for fetch fallback: %{public}@
%s Fell back asset fetch from %{public}@ to %{public}@
%s Failed to fall back asset fetch from %{public}@ to %{public}@, got %{public}@
%s Could not get installed offline language statuses: %{public}@
%s %@
%s ASR: enable parallel loading
%s ASR: filtering for tasks: %@
%s ASR: taskForMemoryLock: %@
%s Override json files=%@
%s Unable to locate or read dictation voice commands assets
%s modelRoot: %@
%s Failed to create recognizer from %{public}@
%s EmbeddedSpeechMetric: Created recognizer in %lf sec from %@
%s %s
ondevice_CreateRecognizer
%s Preheat for %@
%s Preheat for %@ dismissed because recognition in progress
%s Skipping preheat for %@; recognizer already loaded
%s sRecognizerTracker: Too many recognizers active during preheat: %lu
%s Preheated for language %{public}@, source %{public}@, regionId %@%{public}@
%s Could not preheat for language %{public}@, source %{public}@, regionId %@%{public}@: %@
%{public, signpost.description:begin_time}llu
%{public, signpost.description:end_time}llu
ondevice_preheat_time
%s dictationCapable=%d task=%@ aneCapable=%d
%s Starting
%s Recognizer is busy
%s Previous recognizer on other XPC connection is busy. Send sync cancel
%s Cached recognizer for language %{public}@, regionId %@ already loaded
%s Cached recognizer is for language %{public}@, regionId %@, requesting recognizer for language %{public}@, asset type %{public}@, regionId %@
%s sRecognizerTracker: Too many recognizers active: %lu
%s No cached recognizer.
%s EndpointStart > 0 but asr features delivery is disabled!
%s EndpointStart < 0
%s Setting new profile: %d, old profile: %d
%s Failed to get model root, error: %@
%s Fetching contextual data for task: %@
%s Injecting contextual data to recognizer
%s Built inline LME from contextual data, size: %zu
%s Failed to initialize jit profile builder due to error : %@
%s Set inputOrigin to: %@
%s Switching off UC/UD for this request
%s Changing active configuration from 
%@ to 
%s Injected %lu jit strings or contextual data to recognizer, length: %lu
%s Duration spent in adding jit strings = %{public}lfs
%s Create DES record
%s Cancelling delayedBlock
%s Create DES record for Dictation with interactionId=%@
%s Saving profile snapshot: %lu bytes
%s _storeAudioData should be nil. Critical Error. Please check.
%s Received correctedText, interactionId: %@, correctedText: %@
%s Sending dictation feedback to Portrait based on correction
%s Interaction identifier did not match the DES record in memory
First Audio Packet
ES: First Audio Packet
%s Using override profile at %@
%s Could not use override profile at %@: %@
%s Deserialization of existing speech profile failed: %{public}@
%s Mismatch in speech profile language in content (%{public}@) and filename (%{public}@)
%s Profile version on disk (%{public}@) does not match the expected version (%{public}@)
%s Successfully deserialized existing speech profile for %@
%s Acquired os_transaction during cooldown start
%s On-Device ASR: Cooldown is disabled.
%s On-Device ASR: Cooldown scheduled for %zus.
%s Cooldown timer triggered TRIClient release
%s Cooldown timer triggered asset purge
%s embeddedspeech process launch triggered asset purge
%s Received termination signal. Cleaning up immediately
%s Sending %lu events
%s Unable to load the contents of file %@: %@
%s Invalid file format
%s Running distributed evaluation for ASR
%s No attachments given, cannot run distributed evaluation
%s Failed to extract test set: %@
%s Cannot initialize recognizer for locale: %@ task: %@
%s Loaded speech profile
%s Unable to load speech profile
%s Test set contains more utterances than allowed, only running %d utterances
%s Unable to load audio file %@
%s Unable to find reference transcriptions for %@
%s Recipe has no profile
%s Stop adaption recipe. Audio file not readable. Voicemail has been deleted by user
%s Read %lu bytes from audio file
%s Using on device personalization recipe for baseline
%s Recognizer doesn't support the task %{public}@
%s Couldn't create create path for temporary confidence model overrides at %@
%s Couldn't write data to temporary confidence model file at path %@
%s Could not make baseline results
%s Failed to extract quasar model: %@
%s No recognizer created for custom model: %@
%s Could not make results with custom model
%s Profile overrides failed
%s Ignoring malformed overrides: %{public}@
%s Recipe has no recognizer
%s Could not make adapted results
%s Exception evaluating recipe: %@
%s Unknown exception evaluating recipe
%s No tokenizer for %@
%s Interrupted corrected text evaluation redecoding
%s Examining localSpeechDESRecord: %@
%s Unable to load localSpeechDESRecord
%s No audio data provided for UUID %@
%s Recognition result %@, %lu
%s Edit distance between tts ASR and original ASR %@
%s correctedOutput: %@, recognizedOutput %@
%s Unknown evaluation name found in alignmentReferences: %@
%s No modelRoot for %@: %@
%s Failed to load old type of speech profile. Trying new type.
%s Loaded speech profile: %lu bytes
%s Unable to load audio
%s Recognizer doesn't support the task %{public}@: %@
%s Interrupted evaluation redecoding
%s Running recognition for evalName: %@
%s Failed to get override files, error: %@
%s Creating recognizer with overrides: %@
%s Using Personalized LM
%s Not using Personalized LM
%s Unable to restore speech profile
%s Using JIT LME
%s JIT LME: Injecting JIT data, jitStats: %@
%s JIT LME: Error fetching JIT data, error: %@
%s No results for evalName %@: %@
%s Tokenizing correctedText
%s Interrupted evaluation tokenization
%s Computing alignments
%s Failed to delete temporary directory: %@
%s Error when getting dictation language status: %@
%s Starting to compile dictation language: %@
%s Error when compiling dictation language model: %@
%s Starting to compile assistant language: %@
%s Error when compiling assistant language model: %@
%s Error when getting assistant language status: %@
%s No DES record, nothing to write
%s Not saving DES Record with no data or recognition
%s wordCount = %ld, trailingSilenceDuration = %ld, eosLikelihood = %f, pauseCounts = %@, silencePosterior = %f
%s Setting recognized text
%s %lu results
%s EmbeddedSpeechMetric: first audio packet to first partial result = %lf secs
Words recognized: %ld
ES: Partial Recognition
%s AFSpeechInfo Package %@
%s Recognition finished with status %@
%s Audio finish to recognizer finish = %lf sec, connection is %@, error %@
%s _recognitionBeginTime (%@) is greater than _recognitionEndTime (%@)
%s Local speech recognition completed without error, write DES record when needed
%s Writing DES record after 30 seconds delay: interactionId=%@
%s Submitted delayedBlock to dispatch_after
%s #ASR on device eager formatted recognition candidate: %@
%s raw eager recognition candidate: %@
%s Could not make temporary attachment directory at %@: %@
%s Failed to specify compression algorithm: %s
%s Failed to specify format: %s
%s Start extracting archive at path: %s
%s Failed to open archive for reading: %s
%s Entry extraction path: %@
%s Unable to extract file to: %@
%s Finished extracting archive to: %@
%s Could not read %@: %@
%s Could not parse %@: %@
%s %@ is wrong type: %@
%s %@ contains bogus key/value pair: %@ => %@
%s VoiceMail asset could not be read: %@
%s Could not create asset reader output
%s Cannot add output
%s Could not get data pointer: %d
%s JSON serialization failed: %@
%s Compression failed: %@
%s Sampling: Error while initializing ESStoreAudioData because uuid is invalid.
%s %@ Sampling: Won't save audio because - has not recognized anything or has no data.
%s %@ Sampling: Won't save audio because - _currentAudioFilePath is null
%s %@ Sampling: invalid filePath or it is null.
%s %@ Sampling: Done with cleanup of audioFile=%@ and reset of variables.
%s %@ Sampling: Failed to save audio to cache dir. Error: %@
%s %@ Sampling: Successfully saved audio file to cache dir, path=%@
%s %@ Sampling: audioFileToBeMoved is nil
%s %@ Sampling: currentSamplingDate is nil
%s %@ Sampling: Error while creating Sampled directory in /var/mobile
%s %@ Sampling: Error while creating dated Sampled directory in %@ with date - %@
%s %@ Sampling: Error while moving file from cache directory to var/mobile/Library - %@
%s %@ Sampling: Successfully moved audio file to var/mobile/Library dir, path=%@
%s %@ Sampling: Error while writing audio metadata dict to plist - %@
%s %@ Sampling: currentSamplingDateString is null
%s Received SIGTERM. Cleaning and Exiting
ESSelfHelper
Timestamp
ESSelfPreheatWithPowerContainer
ESSpeechProfileBuilderConnection
CESRSpeechProfileBuilderService
NSObject
ESBiomeRecord
CESRUtilitiesAdditions
CESRUtilities
ResultCandidateId
ESAdditions
ESAssetManager
ESAlignmentState
NSCopying
ESConnectionModelInfo
ESConnection
_EARSpeechRecognitionResultStream
CESRSpeechService
ESStoreAudioData
ESListenerDelegate
NSXPCListenerDelegate
@16@0:8
@20@0:8B16
@24@0:8@16
v16@0:8
@28@0:8@16B24
B28@0:8@16B24
B24@0:8@16
v24@0:8@16
v32@0:8@16@24
v92@0:8@16@24@32@40@48@56B64@68@76@84
v48@0:8@16@24@32@40
v148@0:8@16@24@32@40@48@56@64@72B80B84@88@96@104@112@120B128B132B136@140
v32@0:8@16q24
B16@0:8
v20@0:8B16
@"NSUUID"
@"NSString"
@"NSNumber"
@40@0:8@16@24@32
@"ASRSchemaASRPreheatContext"
@"SPIPowerLoggerSnapshot"
@"SPIEventContext"
@32@0:8@16@24
@60@0:8@16@24@32B40^@44^@52
v44@0:8@16@24@32B40
v40@0:8@16@24@32
@56@0:8@16#24@32@40^@48
B88@0:8@16d24@32@40@48@56Q64@72@80
B80@0:8d16@24@32@40@48Q56@64@72
#16@0:8
@24@0:8:16
@32@0:8:16@24
@40@0:8:16@24@32
B24@0:8#16
B24@0:8:16
Vv16@0:8
Q16@0:8
^{_NSZone=}16@0:8
B24@0:8@"Protocol"16
@"NSString"16@0:8
Vv56@0:8@16@24@32q40@?48
Vv32@0:8@16@?24
Vv40@0:8@16@24@?32
Vv24@0:8@?16
Vv28@0:8B16@?20
Vv56@0:8@"NSString"16@"NSString"24@"NSString"32q40@?<v@?B@"NSError">48
Vv32@0:8@"NSString"16@?<v@?q@"NSError">24
Vv40@0:8@"NSDictionary"16@"NSString"24@?<v@?B@"NSError">32
Vv40@0:8@"NSArray"16@"NSArray"24@?<v@?B@"NSError">32
Vv24@0:8@?<v@?B@"NSError">16
Vv28@0:8B16@?<v@?B@"NSError">20
@24@0:8^@16
@"NSMutableArray"
@"NSMutableDictionary"
@"NSMutableSet"
@"CESRAssetConfig"
@"NSXPCConnection"
@"NSObject<OS_os_transaction>"
@"_EARUserProfile"
d16@0:8
v24@0:8d16
q24@0:8@16
@32@0:8@16d24
@36@0:8@16d24B32
@24@0:8Q16
q16@0:8
@40@0:8B16Q20B28^@32
v24@0:8Q16
@32@0:8@16^@24
@36@0:8@16^@24B32
@40@0:8@16^@24B32B36
v32@0:8@16Q24
B40@0:8@16Q24^@32
B32@0:8@16^@24
B24@0:8Q16
B32@0:8@16@24
@"NSObject<OS_dispatch_queue>"
@24@0:8^{_NSZone=}16
v24@0:8q16
@40@0:8@16B24B28^@32
@88@0:8@16@24B32@36@44B52@56B64B68^@72^@80
v32@0:8@16d24
v72@0:8@16q24q32d40@48d56q64
v32@0:8@"_EARSpeechRecognizer"16@"_EARSpeechRecognitionResult"24
v32@0:8@"_EARSpeechRecognizer"16@"NSError"24
v32@0:8@"_EARSpeechRecognizer"16@"NSArray"24
v48@0:8@"_EARSpeechRecognizer"16@"NSArray"24@"NSArray"32@"NSArray"40
v32@0:8@"_EARSpeechRecognizer"16@"_EARSpeechRecognitionResultPackage"24
v32@0:8@"_EARSpeechRecognizer"16d24
v32@0:8@"_EARSpeechRecognizer"16@"_EARSpeechRecognition"24
v72@0:8@"_EARSpeechRecognizer"16q24q32d40@"NSArray"48d56q64
Vv32@0:8@16@24
Vv40@0:8@16@24@32
Vv24@0:8@16
Vv48@0:8@16@24@32@?40
Vv56@0:8@16@24@32@40@?48
Vv36@0:8B16Q20@?28
Vv40@0:8B16Q20B28@?32
Vv72@0:8@16@24@32Q40B48B52@56@?64
Vv36@0:8@16B24@?28
Vv40@0:8@16Q24@?32
Vv32@0:8@16Q24
Vv76@0:8@16@24@32@40@48@56B64@?68
Vv24@0:8@?<v@?@"NSError">16
Vv32@0:8@"NSString"16@"NSURL"24
Vv40@0:8@"CESRAssetConfig"16@"NSString"24@"NSURL"32
Vv24@0:8@?<v@?>16
Vv32@0:8@"CESRSpeechParameters"16@?<v@?@"CESRModelProperties"@"NSError">24
Vv32@0:8@"CESRSpeechParameters"16@?<v@?@"NSString"@"NSString"@"NSError">24
Vv24@0:8@"NSData"16
Vv48@0:8@"NSString"16@"NSString"24@"NSData"32@?<v@?@"NSData"@"NSError">40
Vv56@0:8@"NSString"16@"NSString"24@"NSData"32@"NSString"40@?<v@?@"NSData"@"NSString"@"NSError">48
Vv36@0:8B16Q20@?<v@?@"NSDictionary"@"NSError">28
Vv40@0:8B16Q20B28@?<v@?@"NSDictionary"@"NSError">32
Vv28@0:8B16@?<v@?@"NSDictionary"@"NSError">20
Vv32@0:8@"NSString"16@?<v@?@"NSString"@"NSError">24
Vv32@0:8@"CESRAssetConfig"16@?<v@?@"NSString"@"NSError">24
Vv32@0:8@"CESRAssetConfig"16@?<v@?@"CESRModelProperties"@"NSError">24
Vv48@0:8@"NSDictionary"16@"NSData"24@"NSArray"32@?<v@?@"NSDictionary"@"NSData"@"NSError">40
Vv72@0:8@"NSDictionary"16@"NSDictionary"24@"NSString"32Q40B48B52@"NSSet"56@?<v@?@"NSDictionary"@"NSError">64
Vv36@0:8@"NSString"16B24@?<v@?@"NSData"@"NSString">28
Vv32@0:8@"NSSet"16@?<v@?@"NSNumber"@"NSError">24
Vv40@0:8@"NSSet"16Q24@?<v@?B@"NSError">32
Vv32@0:8@"NSSet"16Q24
Vv32@0:8@"AFSpeechCorrectionInfo"16@"NSString"24
Vv28@0:8B16@?<v@?>20
Vv76@0:8@"NSDictionary"16@"NSString"24@"NSDictionary"32@"NSArray"40@"NSString"48@"NSString"56B64@?<v@?@"NSDictionary"@"NSError">68
Vv40@0:8@"NSString"16@"NSString"24@"NSString"32
v36@0:8@16B24@?28
v48@0:8@16@24@32@?40
@48@0:8@16@24@32^@40
v24@0:8^@16
v28@0:8^@16B24
@"_EARSpeechRecognizer"
@"_EARSpeechRecognitionAudioBuffer"
@"AFSpeechPackage"
@"AFSpeechInfoPackage"
@"NSSet"
@"ESStoreAudioData"
@"ESBiomeRecord"
@"ESSelfHelper"
@"_EARSpeakerCodeWriter"
@"CESRFidesASRRecord"
@96@0:8@16@24@32@40Q48@56@64@72B80B84@88
v40@0:8q16@24q32
@"NSMutableData"
B32@0:8@"NSXPCListener"16@"NSXPCConnection"24
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
<key>application-identifier</key>
<string>com.apple.siri.embeddedspeech</string>
<key>com.apple.CoreRoutine.LocationOfInterest</key>
<true/>
<key>com.apple.accounts.appleaccount.fullaccess</key>
<true/>
<key>com.apple.application-identifier</key>
<string>com.apple.siri.embeddedspeech</string>
<key>com.apple.coreaudio.allow-amr-decode</key>
<true/>
<key>com.apple.coreduetd.allow</key>
<true/>
<key>com.apple.coreduetd.people</key>
<true/>
<key>com.apple.developer.homekit</key>
<true/>
<key>com.apple.frontboardservices.display-layout-monitor</key>
<true/>
<key>com.apple.locationd.effective_bundle</key>
<true/>
<key>com.apple.private.DistributedEvaluation.RecordAccess-com.apple.fides.asr</key>
<true/>
<key>com.apple.private.DistributedEvaluation.RecordAccess-com.apple.siri.speech-dictation-personalization</key>
<true/>
<key>com.apple.private.assets.accessible-asset-types</key>
<array>
<string>com.apple.MobileAsset.Trial.Siri.SiriUnderstandingAsrAssistant</string>
<string>com.apple.MobileAsset.Trial.Siri.SiriUnderstandingAsrHammer</string>
<string>com.apple.MobileAsset.Trial.Siri.SiriDictationAssets</string>
<string>com.apple.MobileAsset.EmbeddedSpeech</string>
<string>com.apple.MobileAsset.EmbeddedSpeechWatch</string>
<string>com.apple.MobileAsset.EmbeddedSpeechMac</string>
</array>
<key>com.apple.private.assets.bypass-asset-types-check</key>
<true/>
<key>com.apple.private.attribution.implicitly-assumed-identity</key>
<dict>
<key>type</key>
<string>path</string>
<key>value</key>
<string>/System/Library/PrivateFrameworks/CoreEmbeddedSpeechRecognition.framework/XPCServices/com.apple.siri.embeddedspeech.xpc/com.apple.siri.embeddedspeech</string>
</dict>
<key>com.apple.private.biome.read-write</key>
<array>
<string>SiriDictation</string>
</array>
<key>com.apple.private.calendar.allow-suggestions</key>
<true/>
<key>com.apple.private.contacts</key>
<true/>
<key>com.apple.private.corerecents</key>
<true/>
<key>com.apple.private.corespotlight.internal</key>
<true/>
<key>com.apple.private.homekit</key>
<true/>
<key>com.apple.private.security.storage.SiriVocabulary</key>
<true/>
<key>com.apple.private.security.storage.SpeechPersonalizedLM</key>
<true/>
<key>com.apple.private.tcc.allow</key>
<array>
<string>kTCCServiceAddressBook</string>
<string>kTCCServiceCalendar</string>
<string>kTCCServiceWillow</string>
<string>kTCCServiceMediaLibrary</string>
</array>
<key>com.apple.proactive.PersonalizationPortrait.Config</key>
<true/>
<key>com.apple.proactive.PersonalizationPortrait.NamedEntity.readOnly</key>
<true/>
<key>com.apple.proactive.eventtracker</key>
<true/>
<key>com.apple.security.exception.files.absolute-path.read-only</key>
<array>
<string>/private/var/MobileAsset/</string>
<string>/private/var/tmp/com.apple.siri-distributed-evaluation/</string>
</array>
<key>com.apple.security.exception.mach-lookup.global-name</key>
<array>
<string>com.apple.suggestd.contacts</string>
<string>com.apple.mobileasset.autoasset</string>
</array>
<key>com.apple.security.exception.shared-preference.read-only</key>
<array>
<string>com.apple.assistant</string>
<string>com.apple.assistant.backedup</string>
<string>com.apple.assistant.support</string>
</array>
<key>com.apple.security.iokit-user-client-class</key>
<array>
<string>AGXCommandQueue</string>
<string>AGXDevice</string>
<string>AGXDeviceUserClient</string>
<string>AGXSharedUserClient</string>
<string>H11ANEInDirectPathClient</string>
<string>IOAccelContext</string>
<string>IOAccelContext2</string>
<string>IOAccelDevice</string>
<string>IOAccelDevice2</string>
<string>IOAccelSharedUserClient</string>
<string>IOAccelSharedUserClient2</string>
<string>IOAccelSubmitter2</string>
<string>IOSurfaceRootUserClient</string>
</array>
<key>com.apple.security.personal-information.addressbook</key>
<true/>
<key>com.apple.security.temporary-exception.mach-lookup.global-name</key>
<array>
<string>com.apple.triald.namespace-management</string>
</array>
<key>com.apple.siriknowledged</key>
<true/>
<key>com.apple.spotlight.search</key>
<true/>
<key>com.apple.trial.client</key>
<array>
<string>372</string>
<string>401</string>
<string>751</string>
</array>
</dict>
</plist>
mcpl
ASRPreheatContextTimestamp
ASRAppleNeuralEngineCompilationContextTimestamp
-[ESSelfHelper initWithTask:isSpeechAPIRequest:]
-[ESSelfHelper logFinalResultGeneratedWithEARPackage:]
-[ESSelfHelper logRequestEndedOrFailedOrCancelledWithError:recognizerComponents:averageActiveTokensPerFrame:languageModelInterpolationWeights:signalToNoiseRatioInDecibels:recognitionDurationInSec:audioDurationMs:eagerUsed:utteranceDetectionEnabled:utteranceConcatenationEnabled:cpuRealTimeFactor:numLmeDataStreams:phoneticMatchDecoderName:pauseDurations:itnDurationInNs:isEmojiPersonalizationUsed:isEmojiDisambiguationUsed:isEmojiExpectedButNotRecognized:recognizedEmojis:]
floatValue
-[ESSelfHelper wrapAndEmitTopLevelEvent:timestampInTicks:]
v32@?0@"NSArray"8Q16^B24
CESRProfileErrorDomain
\NT-contact
\NT-appname
\NT-correction
overrides
keyboardLM
locationOfInterest
spatialLocationOfInterest
interaction
search
calendarEvent
pexNamedEntity
frequency
charsToTrim
charsToSplit
tagName
templateName
tagNameList
minimumWordLength
com.apple.siri.embeddedspeech.profilegeneration-keepalive
-[ESSpeechProfileBuilderConnection initWithXPCConnection:]_block_invoke
v8@?0
Siri
unified_asset_namespace
Could not create EAR profile builder
-[ESSpeechProfileBuilderConnection getVersionForCategory:completion:]
-[ESSpeechProfileBuilderConnection beginWithCategoriesAndVersions:bundleId:completion:]
Begin called while there are already active categories.
Speech category %@ has already been committed with a call to Begin followed by Finish
Speech category %@ is unsupported
Add called before categories have been set with Begin
-[ESSpeechProfileBuilderConnection addVocabularyItems:isBoosted:completion:]
-[ESSpeechProfileBuilderConnection finishAndSaveProfile:completion:]
v32@?0@"NSString"8@"NSMutableArray"16^B24
Failed to write profile
Failed to load speech assets
-[ESSpeechProfileBuilderConnection _profileWithError:]
Failed to read the existing speech profile
+[ESSpeechProfileBuilderConnection personalizationRecipeForAssetConfig:modelOverridePath:]
personalization.json
+[ESSpeechProfileBuilderConnection userProfileWithAssetConfig:modelOverridePath:overrides:isJit:returningFoundPath:error:]
mini.json
en_US_napg.json
dispatch.voc
vocdelta.voc
pg.voc
lexicon.enh
token_s.enh
mrec.psh
Error during _EARUserProfile initialization
v32@?0@"CESRVocabularyCategory"8@"NSSet"16^B24
\correction-first
+[ESSpeechProfileBuilderConnection _adaptRecipe:userData:profile:]_block_invoke
v32@?0@"NSString"8@"NSDictionary"16^B24
+[ESSpeechProfileBuilderConnection _parseRequiredParameter:expectedClass:domain:recipe:error:]
Missing %@ for %@
+[ESSpeechProfileBuilderConnection addWordsToUserProfile:templateName:wordArrays:]
v32@?0@"NSString"8@"NSString"16^B24
+[ESSpeechProfileBuilderConnection _runAdaptationRecipeForNamesWithFrequency:tagNames:templateName:charSetToTrim:charSetToSplit:minimumWordLength:nameFrequencies:profile:]
+[ESSpeechProfileBuilderConnection _runAdaptationRecipeForNamesWithFrequency:tagNames:templateName:charSetToTrim:charSetToSplit:minimumWordLength:nameFrequencies:profile:]_block_invoke_2
v32@?0@"NSString"8Q16^B24
v32@?0@"NSString"8@"NSNumber"16^B24
+[ESSpeechProfileBuilderConnection _runAdaptationRecipeForKeyboardLMWithFrequency:tagName:templateName:charSetToTrim:charSetToSplit:minimumWordLength:userData:profile:]
+[ESSpeechProfileBuilderConnection _runAdaptationRecipeForLocationOfInterestWithFrequency:tagName:templateName:charSetToTrim:charSetToSplit:minimumWordLength:locationOfInterestNames:profile:]
+[ESSpeechProfileBuilderConnection _runAdaptionRecipeForCalendarEventsWithFrequency:tagName:templateName:charSetToTrim:charSetToSplit:minimumWordLength:userData:profile:]
com.apple.MobileSMS
ResultCandidateId
SpeechProfile
EARUserProfileContainerLoadDate
+[CESRUtilities speechProfileRootDirectories]
+[CESRUtilities speechProfilePathsWithLanguage:]
+[CESRUtilities loadSpeechProfiles:language:]
v32@?0@"_EARSpeechRecognitionToken"8Q16^B24
Param
v32@?0{_NSRange=QQ}8^B24
v32@?0@"EARVoiceCommandArgument"8Q16^B24
@"NSArray"24@?0@"NSArray"8@"NSArray"16
+[CESRUtilities AFSpeechInfoPackageForEARSpeechRecognitionResultPackage:]_block_invoke
unconstrained
reduced
avoid
AFSpeechLatticeMitigatorResultForEar
quasarModelPath
type
com.apple.trial.NamespaceUpdate.SIRI_DICTATION_ASSETS
com.apple.trial.NamespaceUpdate.SIRI_UNDERSTANDING_ASR_ASSISTANT
trial
QuasarDir
PreferOverServer
SupportsContinuousListening
SupportsOnDeviceSearch
GeoLMAssetsInfo
-[MAAsset(ESAdditions) _es_purgeSync]
Asset: content version: %@, mastered version %@, installed %@, language: %@, path: %@
Language
-[MAAsset(ESAdditions) _es_isInstalled]
com.apple.siri.embeddedspeech.ESAssetManager
v12@?0i8
-[ESAssetManager registerNotifications]
-[ESAssetManager installationStatusForLanguagesIgnoringCache:assetType:withDetailedStatus:withError:]
-[ESAssetManager installationStatusForLanguagesIgnoringCache:assetType:withDetailedStatus:withError:]_block_invoke
-[ESAssetManager _invalidateInstallationStatusCacheForAssetType:]
-[ESAssetManager _queryInstallationStatusForLanguagesWithError:]
-[ESAssetManager installedModelInfoForAssetConfig:error:triggerDownload:ignoreSpellingModel:]
+[ESAssetManager _assetQueryForLanguage:]
-[ESAssetManager purgeInstalledAssetsExceptLanguages:assetType:error:]_block_invoke
-[ESAssetManager purgeOutdatedAssets]_block_invoke
-[ESAssetManager _purgeMobileAssetsForLanguage:error:]_block_invoke
v24@?0@"MAAsset"8^B16
-[ESAssetManager startMissingAssetDownload]
-[ESAssetManager startMissingAssetDownload]_block_invoke
: %@
%@: ModelInfo=%@: AssetId=%@:
com.apple.internal.ck
disableTrialAssetDelivery
-[ESAssetManager trialAssetDeliveryEnabled:]
enableTrialAssetDelivery
trial_dictation_asset_delivery
-[ESAssetManager prepareHammerConfigFile:]
-[ESAssetManager _installedGeoLMRegionMappingForLanguage:]
-[ESAssetManager geoLMRegionIdForLanguage:location:]
-[ESAssetManager installedGeoLMRegionSpecificAssetForLanguage:regionId:mainAssetConfig:]
-[ESAssetManager _geoLMCompatibleWithMainAsset:geoAssetConfig:]
-[ESAssetManager purgeUnusedGeoLMAssetsForLangauge:]_block_invoke
q24@?0@8@16
-[ESAssetManager purgeUnusedGeoLMAssetsForLangauge:]_block_invoke_2
-[ESAssetManager lastUsedGeoLMRegionIdForLanguage:]
-[ESAssetManager _purgeUserDefaultsGeoLMAssetsInfoDictExceptLanguages:]
EnumerateInstalledAssets
confidenceThresholds
confidenceModels
wordConfidenceThreshold
utteranceConfidenceThreshold
continuousListening
shouldHandleCapitalization
QuasarModel
replicateDataPacketPersonalization
distributedEvaluation
adaptation
WallRTF
DecodeDuration
AverageActiveTokensPerFrame
lm_interp_weights
jitQueryDurationInMs
jitLmeDurationInMs
jitDataStats
version
data
language
assetPath
PERSONALINFO
com.apple.fides.asr
com.apple.siri.speech-dictation-personalization
dictation
ModelOverrideURL
TP/tantor/voice_commands
textfield-editing-suite.plist
com.apple.siri.ESConnection
-[ESConnection initWithXPCConnection:]
-[ESConnection initWithXPCConnection:]_block_invoke
-[ESConnection dealloc]
com.apple.siri.ESConnection.fidesEval
v24@?0@"NSDictionary"8@"NSError"16
-[ESConnection fetchModelInfoForAssetConfig:triggerDownload:completion:]
Corrupt assets found at: %@
-[ESConnection fetchModelPropertiesForAssetConfig:completion:]_block_invoke
-[ESConnection getOfflineAssetStatusIgnoringCache:assetType:withDetailedStatus:withCompletion:]
SiriX
enableTelemetry=YES
+[ESConnection _speechRecognizerWithAssetConfig:geoLMRegionId:enableITN:overrides:modelOverrideURL:preloadModels:requestSource:enableParallelLoading:isHighPriority:geoLMLoadedOut:error:]
enableParallelLoading
preheatSource
ASR task for memory lock
com.apple.assistant
taskForMemoryLock
dictation_emoji_recognition
dictation_voice_commands
itn_s.enh
Failed to create recognizer from %@
-[ESConnection preheatSpeechRecognitionWithAssetConfig:preheatSource:modelOverrideURL:]
AlreadyLoaded
Preheating
 with CustomModelURL %@
Success
(none)
Failure
Preheat time interval in seconds: %lf
-[ESConnection shouldWriteDictationRecord:]
v24@?0@"CESRModelProperties"8@"NSError"16
-[ESConnection startSpeechRecognitionWithParameters:didStartHandlerWithInfo:]
Recognizer is busy
@"NSDictionary"8@?0
jitDataProcessing
-[ESConnection startSpeechRecognitionWithParameters:didStartHandlerWithInfo:]_block_invoke
-[ESConnection sendSpeechCorrectionInfo:interactionIdentifier:]_block_invoke
\jit
-[ESConnection readProfileAndUserDataWithLanguage:allowOverride:completion:]
Not a dictionary: %@
Not an array: %@
orth
prons
freq
v32@?0@"NSString"8@"NSArray"16^B24
updateSpeechProfileWithLanguage is currently unsupported.
Cooldown timeout for EAR
+[ESConnection _scheduleCooldownTimer]
com.apple.siri.embeddedspeech.keepalive
+[ESConnection _cancelCooldownTimer]
+[ESConnection _cooldownTimerFired]
+[ESConnection _cachedRecognizerCleanUp]
+[ESConnection purgeOutdatedAssets]_block_invoke
+[ESConnection prepareToExit]
+[ESConnection _sendPendingAnalyticsEvents]
recipeType
-[ESConnection readTableFromURL:]
-[ESConnection runRecipeEvaluationDistributedEvaluation:recordData:attachments:completion:]
returnHypothesis
returnOverallWER
returnOverallRTF
returnPerUtteranceWER
returnPerUtteranceRTF
locale
task
sampleRate
wordSenseWhitelist
wav.scp
raw.ref
-[ESConnection runRecipeEvaluationDistributedEvaluation:recordData:attachments:completion:]_block_invoke
token
confidence
transcription
\\\S+$
EditDistance
details
modelVersion
overall-rtf
overall-wer
v24@?0@"NSData"8@"NSString"16
SiriCoreLocalSpeechUserData
-[ESConnection runDefaultAdaptationEvaluation:recordData:attachments:completion:]
Recipe has no profile
Voicemail audio file deleted by user
%@ %@
.model
Malformed overrides
ConfidenceMean
ConfidenceMin
ConfidenceMax
AlternateConfidenceMean
AlternateConfidenceMin
AlternateConfidenceMax
Baseline
CustomModel
ModelVersion
Adapted
Alignment
name
callStackReturnAddresses
callStackSymbols
reason
userInfo
(unknown C++ exception)
-[ESConnection runCorrectedTextEvaluationWithAudioDatas:recordDatas:language:samplingRate:caseSensitive:skipLME:wordSenseAccessListSet:completion:]_block_invoke
No tokenizer for %@
Interrupted corrected text evaluation during speech recognition
-[ESConnection runCorrectedTextEvaluationWithAudioDatas:recordDatas:language:samplingRate:caseSensitive:skipLME:wordSenseAccessListSet:completion:]_block_invoke_2
correctedOutput
recognizedOutput
editDistanceRecognizedCorrected
editDistanceRecognizedTTSASR
timestamp
interactionId
asrSelfComponentIdentifier
results
-[ESConnection _modelRootWithAssetConfig:modelOverridePath:overrides:error:]
asrSelfComponentId
asset
applicationName
metrics
tokens
alignments
uttInfos
uttInfosCompressed
alignmentReferences
usePersonalizedLM
corrected
recognized
useJIT
disableAOT
contextualData
overrideFiles
restoreAOT
evaluations
-[ESConnection runEvaluationWithDESRecordDatas:language:recipe:attachments:fidesPersonalizedLMPath:fidesPersonalizedLMTrainingAsset:scrubResult:completion:]_block_invoke
Unknown evaluation name found in alignmentReferences: %@
scoreNbest
compress
-[ESConnection runEvaluationWithDESRecordDatas:language:recipe:attachments:fidesPersonalizedLMPath:fidesPersonalizedLMTrainingAsset:scrubResult:completion:]_block_invoke_3
Interrupted evaluation redecoding
-[ESConnection runEvaluationWithDESRecordDatas:language:recipe:attachments:fidesPersonalizedLMPath:fidesPersonalizedLMTrainingAsset:scrubResult:completion:]_block_invoke_2
Interrupted evaluation tokenization
-[ESConnection _deleteTemporaryDirectoryIfExists:]
-[ESConnection resetCacheAndCompileAllAssetsWithCompletion:]
-[ESConnection pauseRecognition]
-[ESConnection resumeRecognitionWithPrefixText:postfixText:selectedText:]
-[ESConnection _writeDESRecord:oneRecordPerDay:]
-[ESConnection speechRecognizer:didProduceEndpointFeaturesWithWordCount:trailingSilenceDuration:eosLikelihood:pauseCounts:silencePosterior:processedAudioDurationInMilliseconds:]_block_invoke
Dictation
\entity-first
-[ESConnection speechRecognizer:didProduceLoggablePackage:]_block_invoke
-[ESConnection speechRecognizer:didRecognizePartialResult:]_block_invoke
Unsupported EAR build?
Unsupported EAR package build?
DUMMYTOKEN
-[ESConnection speechRecognizer:didRecognizeFinalResultCandidatePackage:]_block_invoke
-[ESConnection speechRecognizer:didRecognizeFinalResultPackage:]_block_invoke
RECOGNITION_SUCCESS
RECOGNITION_FAILED
RECOGNITION_CANCELLED
RECOGNITION_REJECTED
-[ESConnection speechRecognizer:didFinishRecognitionWithError:]_block_invoke
recognizer-components
audioDurationMs
recognitionDurationMs
@sum.self
EagerUsed
numLmeDataStreams
PM-decoder
PM-input
PM-output
tokenName
-[ESConnection speechRecognizer:didRecognizeRawEagerRecognitionCandidate:]_block_invoke_2
v16@?0@"_EARFormatter"8
-[ESConnection speechRecognizer:didRecognizeRawEagerRecognitionCandidate:]_block_invoke
lastPathComponent == %@
_ESDecompressArchiveWithURL
VoiceTriggerFidesArchive
Failed to specify compression algorithm
Failed to specify format
Failed to open file for reading
Unable to extract file to: %@
Task %@ not available for %@, supported tasks are %@
etiquette.json
ReplacementDictionaryForAssetConfig
ReplacementDictionaryForAssetConfig_block_invoke
v32@?0@"NSString"8@16^B24
voicemail_confidence_subtraction.json
sampling rate = %@
task type = %@
far field = %@
device ID = %@
Bluetooth device ID = %@
ANE context = %@
CPU context = %@
GPU context = %@
ReadAudioDataFromFileURL
\\\S*$
Insertions
Deletions
Substitutions
ReferenceSize
\contact-first
CONTACTFIRSTNAME
\contact-middle
CONTACTMIDDLENAME
\contact-last
CONTACTLASTNAME
\contact-nickname
\app-first
APPNAMEFIRSTNAME
\company-first
COMPANYFIRSTNAME
\interaction-first
\interaction-middle
\interaction-last
INLINEFIRSTNAME
\contact-first-derived
\contact-middle-derived
\contact-last-derived
\contact-nickname-derived
Recipe evaluation failed
Empty recognition Result
Error
UtteranceLength
NumberOfNonTerminals
WordsAboveThreshold
UtteranceAboveThreshold
Override file is not found in attachments or device
restoreJIT
configurationFile
JIT LME: JIT profile builder is not initialized
JIT LME: required configuration/file is missing
JIT LME: configuration file is not found in attachments or device
zlibCompressedJson
dictationUIInteractionIdentifier
interactionIdentifier
samplingTimestamp
codec
samplingRate
inferenceSpeakerCode
numTrainedFrames
trainingNnetVersion
isSpeakerCodeUsed
-[ESStoreAudioData initWithUUIDString:language:task:codec:samplingRate:inferenceSpeakerCode:numTrainedFrames:trainingNnetVersion:isSpeakerCodeUsed:isSamplingForDictation:selfLogger:]
-[ESStoreAudioData saveAudioToDisk]
-[ESStoreAudioData _deleteItemAtPath:]
-[ESStoreAudioData _cleanupCacheAndReset:]
-[ESStoreAudioData _saveAudioToCache:]
-[ESStoreAudioData _moveAudioToVarMobile:]
%@.plist
-[ESStoreAudioData _saveAudioMetadataToFilePath:]
-[ESStoreAudioData _createAudioFilePath]
%.0f
%@_%@_%@.pcm
unixTime
samplingDate
success
failed
errorCode
errorDomain
UNDEFINED
description
underlyingErrorCode
underlyingErrorDomain
Audio file to be moved nil
Sampling Date is nil
Unable to create sampling directory
Unable to create dated directory
status
com.apple.siri.embeddedspeech
com.apple.private.des-service
Rejecting %@, no %@ entitlement
Rejecting %@, no %@ or %@ entitlement
main_block_invoke
raise:format:
objectForKeyedSubscript:
deliverEagerPackage
initWithConfiguration:overrides:overrideConfigFiles:language:activeConfiguration:modelLoadingOptions:enableSpeakerCodeTraining:supportEmojiRecognition:voiceCommandActiveSet:
stringValue
confidence
setObject:forKey:
requestIdentifier
fileSystemRepresentation
purgeInstalledAssetForAssetType:language:regionId:error:
jsonFilenameForAssetType:
deleteItemAtFilePath:
sharedStream
setGeoLanguageModelRegion:
arrayByAddingObject:
setScoreNbestExtraLmList:
utterances
concatenatedAudioPackets
objectForKey:
setComponent:
installationStatusForLanguagesForAssetType:includeDetailedStatus:error:
setObject:atIndexedSubscript:
replaceCorruptAssetWithConfig:
stringByTrimmingCharactersInSet:
jitGrammar
fileExistsAtPath:isDirectory:
array
sharedPreferences
setGeoLanguageModelLoaded:
purgeCompiledRecognizerModelsWithConfiguration:
deleteAllRecordsForPlugin:completion:
setScoreNbest:
objectEnumerator
concatenateUtterances
setClientIdentifier:
inputOrigin
utteranceStart
initWithConfiguration:overrides:overrideConfigFiles:generalVoc:lexiconEnh:itnEnh:language:activeConfiguration:modelLoadingOptions:enableSpeakerCodeTraining:supportEmojiRecognition:voiceCommandActiveSet:
removeSpaceBefore
stringByReplacingOccurrencesOfString:withString:
itnDurationInNs
sharedManager
arguments
fileExistsAtPath:
punctuationCharacterSet
setFrontend:
setClass:forClassName:
defaultStore
setSamplingRateFilter:
objectAtIndexedSubscript:
compressedDataUsingAlgorithm:error:
stringByReplacingMatchesInString:options:range:withTemplate:
removeSpaceAfter
fetchNamedEntitiesWithTimeInterval:
setFormatAcrossUtterances:
pronunciationsForOrthography:
itemFromBuffer:error:
defaultManager
objectAtIndex:
userInfo
setSampledAudioStorageFailureReason:
setCancelled:
stringByDeletingPathExtension
initWithConfiguration:language:overrides:sdapiOverrides:generalVoc:emptyVoc:pgVoc:lexiconEnh:tokenEnh:paramsetHolder:
removeObjectForKey:
componentsSeparatedByString:
sharedAnalytics
isSpeechAPIRequest
decrementRequestCount
appendedAutoPunctuation
setFirstAudioPacketProcessed:
userData
setByAddingObjectsFromSet:
initWithUtterance:parseCandidates:
setSampledAudioFileStored:
numberWithUnsignedLongLong:
removeObjectAtIndex:
initWithConfig:
componentsSeparatedByCharactersInSet:
stringByDeletingLastPathComponent
setNumLanguageModelEnrollmentDataStreams:
decodeObjectOfClass:forKey:
appendString:
setWithObjects:
setFinalResultGenerated:
setByAddingObject:
updateRequestCountWithFlag:
setSampledAudioFileStorageFailed:
numberWithUnsignedInteger:
stringByAppendingString:
initWithCommandId:isComplete:paramMatches:
removeLmeDataForTemplateName:
componentsJoinedByString:
appendData:
isSpeakerCodeTrainingSupported:
setWithObject:
numberWithUnsignedInt:
promoteAssetsForAssetType:
setFinalAudioPacketContainingSpeechReceived:
setBluetoothDeviceIdFilter:
initWithUUIDString:
removeItemAtURL:error:
initWithCommandGrammarParsePackage:
components:fromDate:toDate:options:
stringByAppendingPathComponent:
farFieldFilter
setFarFieldFilter:
setWithArray:
initWithUUIDBytes:
isSiriXEnabled
appNames
profileFilePathFromBasePath:language:userId:
numberWithLongLong:
dateWithTimeIntervalSince1970:
setAverageActiveTokensPerFrame:
unsignedLongLongValue
stringByAppendingFormat:
components
removeAllWords
initWithCapacity:
setModelLocale:
aneContextFilter
setFarField:
isRequestSelectedForSamplingFromConfigForLanguage:
farField
profile
setWeights:
initWithType:
numberWithInteger:
date
component
unsignedIntegerValue
setRequestContext:
setAudioPacketArrivalContext:
initWithAcousticFeatureValue:frameDuration:
removeAllObjects
setMetrics:
isRequestSelectedForSamplingForTask:
unrepairedRecognition
processInfo
allocWithZone:
dataWithJSONObject:options:error:
setValue:forKey:
eventTitles
setFailed:
numberWithFloat:
initWithTokenName:start:end:silenceStart:confidence:hasSpaceAfter:hasSpaceBefore:phoneSequence:ipaPhoneSequence:appendedAutoPunctuation:
setRemoveSpaceBefore:
JSONObjectWithData:options:error:
setAudioDurationInNs:
compileRecognizerModelsWithConfiguration:
initForReadingFromData:error:
setMaximumRecognitionDuration:
logLocalRecognitionLoadedForLanguage:duration:
remoteObjectProxy
state
setExtraLmList:
allValues
setUuid:
setRemoveSpaceAfter:
isModifiedByAutoPunctuation
presence
eventLocationNames
setAssetsProvisionalForAssetType:
dataWithContentsOfFile:options:error:
releaseClients
numberWithDouble:
initWithText:
compare:
startTime
setExportedObject:
setRemoteObjectInterface:
allObjects
errorWithDomain:code:userInfo:
setUtterances:
isFinal
prependedAutoPunctuation
tracksWithMediaType:
dataWithBytes:length:
regularExpressionWithPattern:options:error:
numberWithBool:
setAssetLoadContext:
infersQoSFromInstanceUUIDForEAR
commandIdentifier
setLinkIndex:
setExportedInterface:
dataProfile
enumeratorAtPath:
allKeys
setUtteranceDetectionEnabled:
initWithSuites:resourceBaseURL:
setAsrId:
setRecognizerComponents:
_UUID
setLinkId:
enumerateRangesUsingBlock:
setUtteranceConcatenationEnabled:
setExists:
isEqualToString:
setRecognizedText:
data
initWithSuiteName:
tokens
startReading
registerFeedback:completion:
setLeftContextText:
indexes
code
isEqualToDictionary:
setUserProfileData:
tokenize:
enumerateObjectsUsingBlock:
setEventMetadata:
initWithSpeechRecognitionFeatures:acousticFeatures:snr:
currentCalendar
setAppleNeuralEngineCompilationContext:
setRecognizedEmojis:
refreshState
indexOfObject:
setLanguageModelInterpolationWeights:
UUID
setUserProfile:
createSamplingDirectory
enumerateKeysAndObjectsUsingBlock:
setErrorDomain:
setRecognizeEmoji:
numFrames
initWithResults:score:threshold:
tokenSausage
setAppendSpaceAfter:
recordWithLanguage:task:context:narrowband:farField:interactionIdentifier:asrSelfComponentIdentifier:pluginId:frequency:
characterSetWithCharactersInString:
setLanguageModelEnrollmentContext:
UTF8String
start
isEmojiPersonalizationUsed
endpointStart
nnetVersion
setUserId:
setErrorCode:
setAneContextFilter:
setRecognizeEagerCandidates:
initWithRecognition:unfilteredRecognition:rawRecognition:audioAnalytics:isFinal:utteranceStart:latticeMitigatorResult:recognitionPaused:speechProfileUsed:resultCandidateId:
tokenName
censorSpeech
recordWithLanguage:task:context:narrowband:farField:interactionIdentifier:asrSelfComponentIdentifier:pluginId:
standardUserDefaults
isEmojiExpectedButNotRecognized
setEndpointStart:
prefixText
initWithRecognition:unfilteredRecognition:rawRecognition:audioAnalytics:isFinal:utteranceStart:latticeMitigatorResult:recognitionPaused:
endTime
nextObject
setAllowUtteranceDelay:
setRecognitionResultTier1:
recordFromData:
speechServiceDidSelectRecognitionModelWithModelProperties:
setJitProfileData:
captureSnapshot
isEmojiDisambiguationUsed
predicateWithFormat:
endAudio
timestamp
addWordWithParts:templateName:
setUnderlyingErrorDomain:
setEnded:
setRecognitionReplacements:
setActiveConfiguration:
narrowband
recognizedText
setJitLanguageModelEnrollmentEndedTier1:
addOutput:
preITNVoiceCommandInterpretations
name
setUnderlyingErrorCode:
setEndTimeInNs:
isDictationHIPAACompliant
URLByAppendingPathComponent:
createInlineLmeUserDataForContextStrings:
initWithPlistJSONDictionary:
timeZoneWithAbbreviation:
recognizedEmojis
speechServiceDidRecognizeTokens:
hasSuffix:
logEvents:
cancelRecognition
setIsModifiedByAutoPunctuation:
enableVoiceCommands
preITNTokens
serviceListener
setEndTime:
addObjectsFromArray:
ipaPhoneSequence
createInlineLmeUserDataForContextData:speechProfile:
nBestVoiceCommandInterpretations
timeIntervalSinceReferenceDate
initWithPhrases:utterances:processedAudioDuration:
setRecognitionDurationInNs:
logEventWithType:context:
speechServiceDidRecognizeRawEagerRecognitionCandidate:
hasSpaceBefore
setIsLowConfidence:
recognitionUtterenceStatistics
canAddOutput:
enableTransactions
invalidate
addObject:
preITNRecognition
setEnableVoiceCommands:
timeIntervalSinceNow
setRecognitionConfidenceSubtraction:
initWithPath:error:
nBest
createDirectoryAtURL:withIntermediateDirectories:attributes:error:
recognitionUtteranceInfos
callStackSymbols
setIsHighQualityAsset:
hasSpaceAfter
setEnableSpeakerCodeTraining:
interruptTraining
enableEmojiRecognition
selectedText
addKeyValuePair:with:
preITNNBestVoiceCommandInterpretations
setTokens:
timeIntervalSince1970
initWithOrthography:pronunciations:tagName:frequency:
mutableCopy
createDirectoryAtPath:withIntermediateDirectories:attributes:error:
setReason:
speechServiceDidRecognizePackage:
recognitionStatistics
callStackReturnAddresses
setIsFinal:
enableAutoPunctuation
interpretations
addEntriesFromDictionary:
moveItemAtPath:toPath:error:
setEmojiMetrics:
setTokenSilenceStartTimeInNsLists:
secureOfflineOnly
threshold
initWithOrthography:pronunciations:tag:
setRawRecognition:
recognitionResultsWithAudioData:userProfileData:language:task:samplingRate:extraLanguageModel:
setIsEmojiPersonalizationUsed:
addAudioSamples:count:
setEmbeddedSpeechProcessContext:
emitMessage:timestamp:
interpretationIndices
initWithNcsRoot:
modelVersion
text
searchEventValues
setPunctuationText:
speechServiceDidProduceEndpointFeaturesWithWordCount:trailingSilenceDuration:eosLikelihood:pauseCounts:silencePosterior:processedAudioDurationInMilliseconds:
bytes
recognitionPaused
setIsEmojiExpectedButNotRecognized:
postfixText
setTimeZone:
cpuContextFilter
interactionSenderDisplayNames
setEagerEnabled:
emitMessage:
setProfile:
score
templateToVersion
initWithNSUUID:
speechServiceDidProcessAudioDuration:
recognitionMetrics
gpuContextFilter
setIsEmojiDisambiguationUsed:
boolValue
phrases
interactionIdentifier
setDisableAutoPunctuation:
saveOneRecordPerDay
setText:
initWithNBestParses:preITNNBestParses:
countForObject:
templateName
setPreheatContext:
speechServiceDidFinishRecognitionWithStatistics:error:
bluetoothDeviceIdFilter
locationOfInterestNames
setIsBoosted:
recognition
writeToURL:atomically:encoding:error:
activeDictationLanguages
setTemplateToVersion:
phoneSequence
setDictationUiInteractionId:
tasks
initWithModelVersion:modelType:modelRoot:
setPostItn:
countByEnumeratingWithState:objects:count:
base64EncodedStringWithOptions:
reason
location
writeToFile:options:error:
speechRecognitionFeatures
setIsAutoPunctuation:
integerValue
activeConfigurationForNothing
setTaskTypeFilter:
save
pexNamedEntityNames
doubleValue
setDialogContexts:
initWithLanguage:task:
taskTypeFilter
modelQualityTypeStatusStringWithConfig:
count
setPortraitExperimentVariantName:
localizedDescription
getLocalFileUrl
writeToFile:atomically:
readUserProfileWithPath:reuseProfile:
setIsAfterResume:
activeConfigurationForEverything
setDeviceIdFilter:
intValue
domain
modelOverrideURL
samplingRateFilter
corrections
initWithLanguage:assetType:
setPhrases:
getFormatterWithBlock:
readUserProfile:
audioAnalytics
setIpaPhoneSequence:
disableDeliveringAsrFeatures
activeConfiguration
instancesRespondToSelector:
setDetectUtterances:
writeProfileToFile:protectionClass:length:error:
correctedText
modelInfo
initWithItems:language:
setPhoneticMatchOutput:
speechProfileDataLastModifiedDataForLanguage:
setDelegate:
attributesOfItemAtPath:error:
setInverseTextNormalizationDurationInNs:
acousticFeatures
samplingDateAsString
dictionaryWithObjects:forKeys:count:
setTarget:
initWithInterpretations:isLowConfidence:
tagName
correctPartialResultIndexList
setPhoneticMatchInput:
keyboardLMDynamicVocabularyItems
arrayByAddingObjectsFromArray:
valueForEntitlement:
setGpuContextFilter:
shouldHandleCapitalization
fileURLWithPath:
setObject:forKeyedSubscript:
confidenceScore
stringWithContentsOfURL:encoding:error:
valueForKey:
setSignalToNoiseRatioInDecibels:
setConfidence:
installedAssetWithConfig:
_setQueue:
detectUtterances
setHammerVersion:
arrayWithObjects:count:
keysSortedByValueUsingComparator:
offlineDictationProfileOverridePath
fileURLWithPath:isDirectory:
shouldStoreAudioOnDevice
logWithEventContext:
stringWithFormat:
contactWordsWithFrequency
results
setOriginalAudioFileURL:
initWithConfiguration:taskName:applicationName:
oneBest
setConfidenceScore:
setSilenceStartTime:
installedAssetWithConfig:regionId:
deviceIdFilter
purgeSync
setHighPriority:
valueForKeyPath:
filteredArrayUsingPredicate:
setPackage:
logWithEventContext:asrIdentifier:
initWithConfiguration:useQuasarFormatter:activeConfiguration:
containsEntity
stringWithUTF8String:
orderedSetWithArray:
setSilenceStartTimeInNs:
resume
installedAssetWithConfig:regionId:triggerDownload:
dictationIsEnabled
setInitializationContext:
languageCode
variantNameWithError:
finishDecoding
asrSelfComponentIdentifier
containsObject:
setPackageGenerated:
loggingContext
substringFromIndex:
initWithContentsOfFile:options:error:
setSource:
setCorrectPartialResultIndexLists:
version
dictationUIInteractionIdentifier
silenceStart
lastObject
queryMetaDataSync
originalAudioFileURL
assetId
setInputOrigin:
firstMatchInString:options:range:
initWithCurrentProcess
context
setPartialResultGenerated:
longLongValue
returnTypes:
substringToIndex:
setSpeechProfileAgeInNs:
vocabularyWords
dictionary
overrides
firstObject
silenceStartTime
setCorrectedText:
assetReaderAudioMixOutputWithAudioTracks:audioSettings:
setConcatenateUtterances:
lastPathComponent
setPausedAudioDurationsInNs:
substringWithRange:
contextualData
lowercaseString
reverseObjectEnumerator
setStartTime:
voiceCommandInterpretations
initWithDictionary:
dictionaryForKey:
setCpuContextFilter:
siriDataSharingOptInStatus
firstResultAfterResume
setIntermediateUtteranceInfoTier1:
packetArrivalTimestampFromAudioTime:
range
assetReaderWithAsset:error:
setPersonalizedLanguageModelAgeInNs:
supportedCategories
continuousListening
initWithDomain:code:userInfo:
setStartTimeInNs:
setCpuRealTimeFactor:
path
floatValue
setInterpretationIndices:
rangeOfCharacterFromSet:
dictionaryRepresentation
assetType
latticeMitigatorResult
convertLanguageCodeToSchemaLocale:
setPersonalizedLanguageModelWeight:
supportedLanguagesWithAssetType:
maximumRecognitionDuration
initWithExplicitlyRejectedString:
setStarted:
setDatapackVersion:
dictionaryWithContentsOfFile:
weakObjectsHashTable
setInterpretations:
length
pathComponents
rangeOfString:
assetWithURL:
formattedRecognitionWithNBestList:
methodForSelector:
copy
setPhoneSequence:
switchToNewAssetsForAssetType:
runRecognitionWithResultStream:speakerCodeWriter:language:task:samplingRate:
setDecodable:
setStartedOrChanged:
pathWithComponents:
initWithImplicitlyEngagedString:
dictionaryWithContentsProfilePathForLanguage:errorOut:
whitespaceAndNewlineCharacterSet
linkId
setInterruptionHandler:
spatialLocationOfInterestNames
frameDuration
assistantIsEnabled
rawRecognition
setPhoneticMatchDecoderName:
initWithInterpretationIndices:confidenceScore:
systemUptime
copyNextSampleBuffer
metrics
acousticFeatureValuePerFrame
setStatus:
setDecoder:
sampledCachesSubDirectoryPath
dictionaryWithDictionary:
whitespaceCharacterSet
frequency
attributes
setInvalidationHandler:
pauseDurations
speakerCodeInfo
loadConfigs
init
deleteLinkId
createPreheatStartedOrChangedEvent
createPreheatFailedEvent
createPreheatEndedEventWithPreheatAlreadyDone:
createANECompilationStartedEventWithTimeStamp:
createANECompilationEndedEventWithTimeStamp:
initializeSharedPowerLoggerIfNeeded
logPowerSnapshotForProcessStarted
logPowerSnapshotForProcessEnded
initWithTask:isSpeechAPIRequest:
_isLoggingAllowedForCurrentRequestWithTask:isSpeechAPIRequest:
_isTier1LoggingAllowedForCurrentRequestWithTask:
_isNonTier1Message:
logRequestLinkWithRequestId:
logPendingPreheatContextEvents:
logESStartWithTimeInTicks:
logPendingANECompilationContextEvents:
logInitializationStartedOrChangedWithTimeInTicks:
logInitializationEnded
logAssetLoadStartedOrChanged
logAssetLoadEnded
logJitLmeStartedOrChangedWithTimeInTicks:
logJitLmeEndedAndEndedTier1WithDialogContext:
logAudioPacketArrivalStartedOrChangedWithTimeInTicks:
logAudioPacketArrivalEndedWithTimeInTicks:
logFirstAudioPacketProcessed
logFinalAudioPacketContainingSpeechReceivedWithTimeInTicks:loggableSharedUserId:
logRequestStartedOrChangedWithTask:modelLocale:modelVersion:isHighQualityAsset:hammerVersion:geoLanguageModelRegion:geoLanguageModelLoaded:speechProfileAgeInSec:dictationUIInteractionId:portraitExperimentVariantName:
logPartialResultGenerated
logIntermediateUtteranceInfoTier1WithPmInput:pmOutput:
logIntermediateUtteranceInfoTier1WithUnrepairedPostItn:loggableSharedUserId:
logIntermediateUtteranceInfoTier1WithPmInput:pmOutput:unrepairedPostItn:loggableSharedUserId:
logPackageGeneratedAndRecognitionResultTier1WithEARPackage:loggableSharedUserId:
logFinalResultGeneratedWithEARPackage:
logRequestEndedOrFailedOrCancelledWithError:recognizerComponents:averageActiveTokensPerFrame:languageModelInterpolationWeights:signalToNoiseRatioInDecibels:recognitionDurationInSec:audioDurationMs:eagerUsed:utteranceDetectionEnabled:utteranceConcatenationEnabled:cpuRealTimeFactor:numLmeDataStreams:phoneticMatchDecoderName:pauseDurations:itnDurationInNs:isEmojiPersonalizationUsed:isEmojiDisambiguationUsed:isEmojiExpectedButNotRecognized:recognizedEmojis:
logSampledAudioFileStoredSuccessfully
logSampledAudioFileStoredWithError:customFailureReason:
wrapAndEmitTopLevelEvent:timestampInTicks:
asrId
recognitionTask
unrepairedPostItn
setUnrepairedPostItn:
personalizedLmWeight
setPersonalizedLmWeight:
personalizedLmAgeInSec
setPersonalizedLmAgeInSec:
continuousListeningEnabled
setContinuousListeningEnabled:
.cxx_destruct
_asrId
_recognitionTask
_packageLogged
_isTier1LoggingAllowedForCurrentRequest
_continuousListeningEnabled
_unrepairedPostItn
_personalizedLmWeight
_personalizedLmAgeInSec
T@"NSUUID",R,N,V_asrId
T@"NSString",R,N,V_recognitionTask
T@"NSString",&,N,V_unrepairedPostItn
T@"NSNumber",&,N,V_personalizedLmWeight
T@"NSNumber",&,N,V_personalizedLmAgeInSec
TB,N,V_continuousListeningEnabled
setTimestampInTicks:
timestampInTicks
T@"NSNumber",&,N
initWithPreheatContext:powerSnapshot:powerEventContext:
preheatContext
powerSnapshot
powerEventContext
_preheatContext
_powerSnapshot
_powerEventContext
T@"ASRSchemaASRPreheatContext",R,N,V_preheatContext
T@"SPIPowerLoggerSnapshot",R,N,V_powerSnapshot
T@"SPIEventContext",R,N,V_powerEventContext
dealloc
signalEndOfUserData
_isProfileValidFromVersionsMap:
personalizationRecipeForAssetConfig:modelOverridePath:
userProfileWithAssetConfig:modelOverridePath:overrides:isJit:returningFoundPath:error:
adaptUserProfileWithUserData:personalizationRecipe:userData:endOfUserData:
_adaptRecipe:userData:profile:
_parseRequiredParameter:expectedClass:domain:recipe:error:
addWordsToUserProfile:templateName:wordArrays:
_runAdaptationRecipeForDomain:frequency:tagNames:templateName:charSetToTrim:charSetToSplit:minimumWordLength:userData:profile:
_runAdaptationRecipeForNamesWithFrequency:tagNames:templateName:charSetToTrim:charSetToSplit:minimumWordLength:nameFrequencies:profile:
_runAdaptationRecipeForKeyboardLMWithFrequency:tagName:templateName:charSetToTrim:charSetToSplit:minimumWordLength:userData:profile:
_runAdaptationRecipeForLocationOfInterestWithFrequency:tagName:templateName:charSetToTrim:charSetToSplit:minimumWordLength:locationOfInterestNames:profile:
_runAdaptionRecipeForCalendarEventsWithFrequency:tagName:templateName:charSetToTrim:charSetToSplit:minimumWordLength:userData:profile:
isEqual:
class
self
performSelector:
performSelector:withObject:
performSelector:withObject:withObject:
isProxy
isKindOfClass:
isMemberOfClass:
conformsToProtocol:
respondsToSelector:
retain
release
autorelease
retainCount
zone
hash
superclass
description
debugDescription
TQ,R
T#,R
T@"NSString",R,C
setProfileConfigWithLanguage:profileDir:userId:dataProtectionClass:completion:
getVersionForCategory:completion:
beginWithCategoriesAndVersions:bundleId:completion:
addVocabularyItems:isBoosted:completion:
cancelWithCompletion:
finishAndSaveProfile:completion:
initWithXPCConnection:
_profileWithError:
_stagedItems
_committedItems
_stagedCategoryToVersion
_committedCategoryToVersion
_seenCategories
_baseDirectory
_language
_userId
_dataProtectionClass
_assetConfig
_connection
_transaction
_profile
markRecognition
sendEvent
recognitionEndTime
setRecognitionEndTime:
applicationName
setApplicationName:
interactionId
setInteractionId:
taskName
setTaskName:
hasRecognizedAnything
_hasRecognizedAnything
_recognitionEndTime
_applicationName
_interactionId
_taskName
Td,N,V_recognitionEndTime
T@"NSString",C,N,V_applicationName
T@"NSString",C,N,V_interactionId
T@"NSString",C,N,V_taskName
TB,R,N,V_hasRecognizedAnything
loadDate
setLoadDate:
T@"NSDate",C,N
calculateDiffInDaysFromTimestamp:
hasRecognizedAnythingInAFSpeechPackage:
afTokensForEARTokens:removeSpaceBefore:
afRecognitionForEARSausage:processedAudioDuration:
afSpeechPackageForEARPackage:processedAudioDuration:speechProfileUsed:
speechProfileRootDirectories
speechProfilePathsWithLanguage:
loadSpeechProfiles:language:
earTokensToString:
voiceCommandsParamKeyBuilder:
afVoiceCommandGrammarParseResultForEARTokenString:withEARVoiceCommandInterpretations:
AFSpeechInfoPackageForEARSpeechRecognitionResult:
AFSpeechInfoPackageForEARSpeechRecognitionResultPackage:
earTokensForAFTokens:appendedAutoPunctuation:
mapContextOptionToString:
languageStringForLocaleString:
localeStringForLanguageString:
isFilePathValid:
setResultCandidateId:
resultCandidateId
_es_purgeSync
_es_description
_es_language
_es_masteredVersion
_es_contentVersion
_es_path
_es_isInstalled
_es_status
_es_quasarModelPath
_es_quasarDir
_es_preferOverServer
_es_supportsContinuousListening
_es_supportsOnDeviceSearch
sharedInstance
_assetQueryForLanguage:
registerNotifications
installationStatusForLanguagesIgnoringCache:assetType:withDetailedStatus:withError:
_invalidateInstallationStatusCacheForAssetType:
_queryInstallationStatusForLanguagesWithError:
installedQuasarModelPathForAssetConfig:error:
installedQuasarModelPathForAssetConfig:error:triggerDownload:
installedQuasarModelPathForAssetConfig:error:triggerDownload:ignoreSpellingModel:
installedModelInfoForAssetConfig:error:
installedModelInfoForAssetConfig:error:triggerDownload:
installedModelInfoForAssetConfig:error:triggerDownload:ignoreSpellingModel:
prepareModelInfo:withAssetType:
promoteModelInfo:withAssetType:
isBelowLimitForLocale:
purgeInstalledAssetsExceptLanguages:assetType:error:
purgeOutdatedAssets
_purgeMobileAssetsForLanguage:error:
startMissingAssetDownload
modelAttributesStatusStringWithAsset:
modelTypeStatusStringAndVersionWithAsset:
trialAssetDeliveryEnabled:
installedHammerConfigFileForLanguage:
prepareHammerConfigFile:
promoteHammerConfigFile
_installedGeoLMRegionMappingForLanguage:
geoLMRegionIdForLanguage:location:
installedGeoLMRegionSpecificAssetForLanguage:regionId:mainAssetConfig:
_geoLMCompatibleWithMainAsset:geoAssetConfig:
_updateGeoLMAssetsInfoDictWithRegionId:language:
purgeUnusedGeoLMAssetsForLangauge:
lastUsedGeoLMRegionIdForLanguage:
_purgeUserDefaultsGeoLMAssetsInfoDictExceptLanguages:
_loadGeoLMAssetsInfoDictForLanguage:
_updateUserDefaultsWithGeoLMAssetsInfoDict:language:
_userDefaultsGeoLMAssetsInfoDictKeyForLangauge:
_queue
_languageInstallationCache
_dictationAssetUpdatedNotificationToken
_assistantAssetUpdatedNotificationToken
_recognizerAssetPathsInUse
_profileAssetPathsInUse
_geoLMAssetsInfoDict
copyWithZone:
incrementInsertions
incrementDeletions
incrementSubstitutions
incrementCost
numberOfInsertions
setNumberOfInsertions:
numberOfDeletions
setNumberOfDeletions:
numberOfSubstitutions
setNumberOfSubstitutions:
totalCost
setTotalCost:
_numberOfInsertions
_numberOfDeletions
_numberOfSubstitutions
_totalCost
Tq,N,V_numberOfInsertions
Tq,N,V_numberOfDeletions
Tq,N,V_numberOfSubstitutions
Tq,N,V_totalCost
setModelType:
modelType
setModelRoot:
modelRoot
T@"NSString",C,N
initWithConfiguration:overrides:overrideConfigFiles:generalVoc:lexiconEnh:itnEnh:language:activeConfiguration:modelLoadingOptions:enableSpeakerCodeTraining:supportEmojiRecognition:voiceCommandActiveSet:modelContextDelegate:
initWithConfiguration:overrides:overrideConfigFiles:language:activeConfiguration:modelLoadingOptions:enableSpeakerCodeTraining:supportEmojiRecognition:voiceCommandActiveSet:modelContextDelegate:
setRightContext:
setSelectedText:
initWithConfiguration:language:overrides:sdapiOverrides:generalVoc:emptyVoc:pgVoc:lexiconEnh:tokenEnh:paramsetHolder:isJit:
pauseRecognition
resumeRecognitionWithLeftContext:rightContext:selectedText:
speechServiceDidProduceLoggablePackage:
speechServiceDidRecognizeTokens:withMetadata:
speechServiceDidRecognizeFinalResultCandidatePackage:
speechServiceDidRecognizePackage:withMetadata:
initialize
captureESStartTimeInTicks
_speechRecognizerWithAssetConfig:enableITN:isHighPriority:error:
_speechRecognizerWithAssetConfig:geoLMRegionId:enableITN:overrides:modelOverrideURL:preloadModels:requestSource:enableParallelLoading:isHighPriority:geoLMLoadedOut:error:
_bestGuessTaskStringsFromPreheatSource:
_cooldownTimerTimeoutSeconds
_scheduleCooldownTimer
_cancelCooldownTimer
_cooldownTimerFired
_cachedRecognizerCleanUp
prepareToExit
_addPendingAnalyticsEvent:
_sendPendingAnalyticsEvents
_clearPendingAnalyticsEvents
_addPendingSelfPreheatEvent:
_addPendingSelfANECompilationEvent:
_clearPendingSelfEvents
didStartModelInitializing:
didFinishModelInitializing:
getRecognizerQueue
speechRecognizer:didRecognizePartialResult:
speechRecognizer:didFinishRecognitionWithError:
speechRecognizer:didRecognizeFinalResults:
speechRecognizer:didRecognizeFinalResults:tokenSausage:nBestChoices:
speechRecognizer:didRecognizeFinalResultPackage:
speechRecognizer:didProcessAudioDuration:
speechRecognizer:didRecognizeRawEagerRecognitionCandidate:
speechRecognizer:didProduceEndpointFeaturesWithWordCount:trailingSilenceDuration:eosLikelihood:pauseCounts:silencePosterior:processedAudioDurationInMilliseconds:
speechRecognizer:didRecognizePartialResultNbest:
speechRecognizer:didProduceLoggablePackage:
speechRecognizer:didRecognizeFinalResultCandidatePackage:
resetCacheAndCompileAllAssetsWithCompletion:
preheatSpeechRecognitionWithLanguage:modelOverrideURL:
preheatSpeechRecognitionWithAssetConfig:preheatSource:modelOverrideURL:
startRequestActivityWithCompletion:
startSpeechRecognitionWithParameters:didStartHandlerWithInfo:
startSpeechRecognitionWithParameters:didStartHandler:
addAudioPacket:
finishAudio
createSpeechProfileWithLanguage:modelOverridePath:JSONData:completion:
updateSpeechProfileWithLanguage:modelOverridePath:existingProfile:existingAssetPath:completion:
getOfflineAssetStatusIgnoringCache:assetType:withCompletion:
getOfflineAssetStatusIgnoringCache:assetType:withDetailedStatus:withCompletion:
getOfflineDictationStatusIgnoringCache:withCompletion:
fetchAssetsForLanguage:completion:
fetchAssetsForAssetConfig:completion:
fetchModelPropertiesForAssetConfig:completion:
runAdaptationRecipeEvaluation:recordData:attachments:completion:
runCorrectedTextEvaluationWithAudioDatas:recordDatas:language:samplingRate:caseSensitive:skipLME:wordSenseAccessListSet:completion:
readProfileAndUserDataWithLanguage:allowOverride:completion:
purgeInstalledAssetsExceptLanguages:completion:
purgeInstalledAssetsExceptLanguages:assetType:completion:
setAssetsPurgeabilityExceptLanguages:assetType:
writeDESRecord
sendSpeechCorrectionInfo:interactionIdentifier:
invalidatePersonalizedLM
removePersonalizedLMForFidesOnly:completion:
runEvaluationWithDESRecordDatas:language:recipe:attachments:fidesPersonalizedLMPath:fidesPersonalizedLMTrainingAsset:scrubResult:completion:
deleteAllDESRecordsForDictationPersonalizationWithCompletion:
invalidateUaapLm
resumeRecognitionWithPrefixText:postfixText:selectedText:
_delegate
_fidesEvalQueue
_invalidated
fetchModelInfoForAssetConfig:completion:
fetchModelInfoForAssetConfig:triggerDownload:completion:
siriDataSharingOptedIn
shouldWriteDictationRecord:
updateAudioDuration:
readTableFromURL:
runRecipeEvaluationDistributedEvaluation:recordData:attachments:completion:
runDefaultAdaptationEvaluation:recordData:attachments:completion:
_userProfileWithAssetConfig:modelOverridePath:overrides:isJit:returningFoundPath:error:
_modelRootWithAssetConfig:modelOverridePath:overrides:error:
_deleteTemporaryDirectoryIfExists:
_writeDESRecord:
_writeDESRecord:oneRecordPerDay:
dummyResultPackage:
statusForError:
_recognizer
_audioBuffer
_shouldStoreDictationAudioOnDevice
_disableDeliveringAsrFeatures
_isHighPriority
_lastRecognizedPackage
_lastRecognizedMetadataPackage
_bufferedAudioPackets
_bufferedAudioEnded
_validDomains
_requestCompletion
_storeAudioData
_biomeRecord
_selfHelper
_samplingRate
_audioDurationMs
_processedAudioDuration
_firstAudioPacketReceivedTime
_firstAudioPacketTimeUntilFirstPartial
_lastAudioPacketReceivedTime
_firstAudioPacketReceivedTimeInTicks
_lastAudioPacketReceivedTimeInTicks
_firstAudioPacketProcessedTime
_localMetrics
_recognitionBeginTime
_recognitionAbsoluteEndTime
_speakerCodeWriter
_weakFidesRecognizer
_lastWordCount
_taskToUse
_resultCandidateId
_desRecord
_desRecordDictation
initWithUUIDString:language:task:codec:samplingRate:inferenceSpeakerCode:numTrainedFrames:trainingNnetVersion:isSpeakerCodeUsed:isSamplingForDictation:selfLogger:
hasData
saveAudioToDisk
cleanupCacheAndReset
_deleteItemAtPath:
_cleanupCacheAndReset:
_saveAudioToCache:
_trimAudioIfNeeded:
_moveAudioToVarMobile:
_saveAudioMetadataToFilePath:
_createAudioFilePath
_createCachesDirectoryIfItDoesNotExist
_logAudioSampledEventsWithStatus:error:customReasonForFailure:
UUIDString
setUUIDString:
language
setLanguage:
task
setTask:
codec
setCodec:
samplingRate
setSamplingRate:
audioPackets
setAudioPackets:
setHasRecognizedAnything:
inferenceSpeakerCode
numTrainedFrames
trainingNnetVersion
isSpeakerCodeUsed
isSamplingForDictation
selfLogger
setSelfLogger:
audioMetadata
setAudioMetadata:
collectedAudioDurationMS
setCollectedAudioDurationMS:
currentAudioFilePath
setCurrentAudioFilePath:
logPrefix
setLogPrefix:
_isSpeakerCodeUsed
_isSamplingForDictation
_UUIDString
_task
_codec
_audioPackets
_inferenceSpeakerCode
_numTrainedFrames
_trainingNnetVersion
_selfLogger
_audioMetadata
_collectedAudioDurationMS
_currentAudioFilePath
_logPrefix
T@"NSString",C,N,V_UUIDString
T@"NSString",C,N,V_language
T@"NSString",C,N,V_task
T@"NSString",&,N,V_codec
TQ,N,V_samplingRate
T@"NSMutableData",&,N,V_audioPackets
TB,N,V_hasRecognizedAnything
T@"NSString",R,N,V_inferenceSpeakerCode
T@"NSNumber",R,N,V_numTrainedFrames
T@"NSNumber",R,N,V_trainingNnetVersion
TB,R,N,V_isSpeakerCodeUsed
TB,R,N,V_isSamplingForDictation
T@"ESSelfHelper",&,N,V_selfLogger
T@"NSMutableDictionary",&,N,V_audioMetadata
Td,N,V_collectedAudioDurationMS
T@"NSString",C,N,V_currentAudioFilePath
T@"NSString",C,N,V_logPrefix
listener:shouldAcceptNewConnection:
%s SELF: Logging object created successfully (logging allowed for current request). asrId=%@, recognitionTask=%@, isSpeechAPIRequest=%@, isHipaaCompliant=%@, siriOptInStatus=%@, isTier1LoggingAllowed=%@
%s SELF: Logging disabled because it is not allowed for the current request. recognitionTask=%@, isSpeechAPIRequest=%@
%s SELF: Correct Partial Result Index List is %s, Silence Start Time List is %s
%s SELF: Encountered malformed string during SELF logging for recognizer components in speech results from recognizer. String: (%@)
%s SELF: Expected three recognizer components separated by delimiter '::'. Ex: 'dnn-rfdnn-aa-cache::dnn-lazy-16k-rfdnn-dictation::msg'
%s SELF: Encountered malformed string during SELF logging for interpolation weights in speech results from recognizer. String: %@
%s SELF: Expected interpolation weight sets separated by delimiter ';' - starting with a set of weights delimited by ',' and ending the with start/end times delimited by ':'. Ex: '0.999646,0.000354:0:4280;0.947514,0.000158:0:3859'
%s SELF: Logging ASRRequestContext->failed in SELF based on error result from recognizer.
%s SELF: Logging ASRRequestContext->cancelled with reason RECOGNITION_CANCELLED in SELF based on error result from recognizer.
%s SELF: Logging ASRRequestContext->cancelled with reason RECOGNITION_REJECTED in SELF based on error result from recognizer.
%s SELF: Logging ASRRequestContext->cancelled with reason RECOGNITION_REJECTED in SELF because nothing was recognized (SpeechNoMatch).
%s SELF: Logging ASRRequestContext->ended in SELF based on success status from recognizer.
%s SELF: Failed trying to wrap and emit top-level ASR event because event type was not mapped to loggable message type in the ASR SELF schema.
%s SELF: Wrapping and logging an event of type %@
%s %@ cancelling instance %@
%s %@: %@
%s %@
%s %@.
%s Internal inconsistency error: KVItems list and corresponding isBoosted booleans list are out of sync. This batch of items will default to not being boosted.
%s Failed to deserialize a KVItem. It will be ignored.
%s Starting to process KVItems into CESRUserData.
%s Created CESRUserData from KVItems.
%s Adapted profile using CESRUserData according to personalization recipe.
%s Speech profile updated successfully. Wrote %lu bytes to %@
%s Created _EARProfileBuilder from asset config.
%s Existing profile found at %@. It will be reused when updating the profile.
%s Could not locate asset: %@
%s Could not read personalization.json: %@
%s Could not parse personalization.json: %@
%s Set model root to %@
%s Use currently installed asset.
%s Recipe has invalid json for "%{public}@"
%s Recipe has invalid tagName for "%{public}@": "%{public}@"
%s Error executing recipe for domain %{public}@
%s Recipe for %{public}@ is missing "%{public}@"
%s Recipe for %{public}@ contains parameter %{public}@, expected type %{public}@ but got %{public}@
%s User Profile: Starting AddWordsToUserProfile
%s Using name frequencies adaption for names: %@ into slot %@ for template %@
%s Ignoring name part "%@" because it is too short (minimum length is %lu)
%s Using keyboardLMAdaptation adaption for names %@ into slot %@ for template %@
%s Ignoring keyboardDynamicVocabularyItem "%@" because it is too short (minimum length is %lu)
%s Using locationOfInterestNameAdaptationFrequency adaption for names %@ into slot %@ for template %@
%s Ignoring locationOfInterestName "%@" because it is too short (minimum length is %lu)
%s Using eventLocationNameAdaptationFrequency adaption for names %@ into slot %@ for template %@
%s Ignoring calendar event word "%@" because it is too short (minimum length is %lu)
%s Root directories for new type of speech profile: %{private}@
%s speechProfilePathsWithLanguage was incorrectly called with language=nil
%s Mapped language=nil
%s loadSpeechProfiles was incorrectly called with profiles=nil
%s Reused new type of speech profile: path=%{private}@
%s Loaded new type of speech profile: path=%{private}@ profile=%d
%s Count of command interpretation sets does not match count of speech recognition results
%s AFSpeechLatticeMitigatorResult Score = %f, Threshold = %f
%s AFSpeechLatticeMitigatorResult nil
%s Purging compiled assets if there are any.
%s Previously installed asset has been removed: %{public}@
%s Installed asset is corrupt! Triggering emergency purge %{public}@
%s Failed to register for assistant asset update notifications.
%s Failed to register for dictation asset update notifications.
%s Installation status for languages (ignoring cache: %@)
%s Trial asset delivery is enabled!
%s Invalidating installation status cache for %lu
%s Language installation status query failed: %@
%s MobileAsset query failed: %ld
%s Found assets: %@
%s Siri and Dictation are both disabled, no need to access asset
%s Using ASR Trial assets at %@
%s No assets available for language: %@ asset type: %{public}@
%s Purging Trial assets failed: %@
%s Purging outdated assets.
%s Error purging (%@): %ld
%s Purged old asset %@
%s Just ignoring %@
%s Purging asset: %@, language %@
%s Purging failed: %lu
%s Checking for missing assets.
%s Purging all assistant ASR assets except for %@
%s Purging all assistant ASR assets
%s Encountered error trying to purge unused assistant ASR assets: %@
%s Trial asset delivery disabled for assistant assets. Bailing out of missing asset check.
%s Hit error querying installation status of MobileAssets: %@
%s Purging assets from MobileAssets for locale: %@
%s Hit error purging assets from MobileAssets: %d %@
%s Trial asset delivery disabled for dictation assets. Bailing out of missing asset check.
%s Trial asset delivery explicitly disabled!
%s Trial asset delivery explicitly enabled!
%s Hammer model info=%@
%s Exception thrown while reading hammer config
%s GeoLM: region mapping json file=%@
%s GeoLM: region mapping json file is nil Or there is no regionMapping for given language=%@
%s GeoLM: Not Supported
%s GeoLM: region specific [%@] geo-config json file=%@
%s GeoLM: geoLM region specific [%@] asset exists on device, but not compatible.
%s GeoLM: Exception thrown while reading geo-config json
%s GeoLM: region specific asset is not found for given language=%@ regionId=%@
%s GeoLM: model-info.version doesn't match. mainASRModelInfo.version=%@ geoLMModelInfo.version=%@ mainAssetConfig=%@ geoAssetConfig=%@
%s GeoLM: Exception thrown while reading json configs
%s GeoLM: language is nil. Skipping.
%s GeoLM: regionIdToBePurged: %@, lastWhenUsed: %ld days ago
%s GeoLM: regionIdToBePurged: %@, _geoLMAssetsInfoDict count: %ld
%s GeoLM: supportedLanguages count:%ld
%s GeoLM: Going to delete: %@
%s ASR: Using high priority configuration.
%s %@ deallocating
%s Failing to fetch assets for nil language
%s Could not get offline language for fetch fallback: %{public}@
%s Fell back asset fetch from %{public}@ to %{public}@
%s Failed to fall back asset fetch from %{public}@ to %{public}@, got %{public}@
%s Could not get installed offline language statuses: %{public}@
%s ASR: enable parallel loading
%s ASR: filtering for tasks: %@
%s ASR: taskForMemoryLock: %@
%s Override json files=%@
%s Unable to locate or read dictation voice commands assets
%s modelRoot: %@
%s Failed to create recognizer from %{public}@
%s EmbeddedSpeechMetric: Created recognizer in %lf sec from %@
%s %s
ondevice_CreateRecognizer
%s Preheat for %@
%s Preheat for %@ dismissed because recognition in progress
%s Skipping preheat for %@; recognizer already loaded
%s sRecognizerTracker: Too many recognizers active during preheat: %lu
%s Preheated for language %{public}@, source %{public}@, regionId %@%{public}@
%s Could not preheat for language %{public}@, source %{public}@, regionId %@%{public}@: %@
%{public, signpost.description:begin_time}llu
%{public, signpost.description:end_time}llu
ondevice_preheat_time
%s dictationCapable=%d task=%@ aneCapable=%d
%s Starting
%s Recognizer is busy
%s Previous recognizer on other XPC connection is busy. Send sync cancel
%s Cached recognizer for language %{public}@, regionId %@ already loaded
%s Cached recognizer is for language %{public}@, regionId %@, requesting recognizer for language %{public}@, asset type %{public}@, regionId %@
%s sRecognizerTracker: Too many recognizers active: %lu
%s No cached recognizer.
%s EndpointStart > 0 but asr features delivery is disabled!
%s EndpointStart < 0
%s Setting new profile: %d, old profile: %d
%s Failed to get model root, error: %@
%s Fetching contextual data for task: %@
%s Injecting contextual data to recognizer
%s Built inline LME from contextual data, size: %zu
%s Failed to initialize jit profile builder due to error : %@
%s Set inputOrigin to: %@
%s Switching off UC/UD for this request
%s Changing active configuration from 
%@ to 
%s Injected %lu jit strings or contextual data to recognizer, length: %lu
%s Duration spent in adding jit strings = %{public}lfs
%s Create DES record
%s Cancelling delayedBlock
%s Create DES record for Dictation with interactionId=%@
%s Saving profile snapshot: %lu bytes
%s _storeAudioData should be nil. Critical Error. Please check.
%s Received correctedText, interactionId: %@, correctedText: %@
%s Sending dictation feedback to Portrait based on correction
%s Interaction identifier did not match the DES record in memory
First Audio Packet
ES: First Audio Packet
%s Using override profile at %@
%s Could not use override profile at %@: %@
%s Deserialization of existing speech profile failed: %{public}@
%s Mismatch in speech profile language in content (%{public}@) and filename (%{public}@)
%s Profile version on disk (%{public}@) does not match the expected version (%{public}@)
%s Successfully deserialized existing speech profile for %@
%s Acquired os_transaction during cooldown start
%s On-Device ASR: Cooldown is disabled.
%s On-Device ASR: Cooldown scheduled for %zus.
%s Cooldown timer triggered TRIClient release
%s Cooldown timer triggered asset purge
%s embeddedspeech process launch triggered asset purge
%s Received termination signal. Cleaning up immediately
%s Sending %lu events
%s Unable to load the contents of file %@: %@
%s Invalid file format
%s Running distributed evaluation for ASR
%s No attachments given, cannot run distributed evaluation
%s Failed to extract test set: %@
%s Cannot initialize recognizer for locale: %@ task: %@
%s Loaded speech profile
%s Unable to load speech profile
%s Test set contains more utterances than allowed, only running %d utterances
%s Unable to load audio file %@
%s Unable to find reference transcriptions for %@
%s Recipe has no profile
%s Stop adaption recipe. Audio file not readable. Voicemail has been deleted by user
%s Read %lu bytes from audio file
%s Using on device personalization recipe for baseline
%s Recognizer doesn't support the task %{public}@
%s Couldn't create create path for temporary confidence model overrides at %@
%s Couldn't write data to temporary confidence model file at path %@
%s Could not make baseline results
%s Failed to extract quasar model: %@
%s No recognizer created for custom model: %@
%s Could not make results with custom model
%s Profile overrides failed
%s Ignoring malformed overrides: %{public}@
%s Recipe has no recognizer
%s Could not make adapted results
%s Exception evaluating recipe: %@
%s Unknown exception evaluating recipe
%s No tokenizer for %@
%s Interrupted corrected text evaluation redecoding
%s Examining localSpeechDESRecord: %@
%s Unable to load localSpeechDESRecord
%s No audio data provided for UUID %@
%s Recognition result %@, %lu
%s Edit distance between tts ASR and original ASR %@
%s correctedOutput: %@, recognizedOutput %@
%s Unknown evaluation name found in alignmentReferences: %@
%s No modelRoot for %@: %@
%s Failed to load old type of speech profile. Trying new type.
%s Loaded speech profile: %lu bytes
%s Unable to load audio
%s Recognizer doesn't support the task %{public}@: %@
%s Interrupted evaluation redecoding
%s Running recognition for evalName: %@
%s Failed to get override files, error: %@
%s Creating recognizer with overrides: %@
%s Using Personalized LM
%s Not using Personalized LM
%s Unable to restore speech profile
%s Using JIT LME
%s JIT LME: Injecting JIT data, jitStats: %@
%s JIT LME: Error fetching JIT data, error: %@
%s No results for evalName %@: %@
%s Tokenizing correctedText
%s Interrupted evaluation tokenization
%s Computing alignments
%s Failed to delete temporary directory: %@
%s Error when getting dictation language status: %@
%s Starting to compile dictation language: %@
%s Error when compiling dictation language model: %@
%s Starting to compile assistant language: %@
%s Error when compiling assistant language model: %@
%s Error when getting assistant language status: %@
%s No DES record, nothing to write
%s Not saving DES Record with no data or recognition
%s wordCount = %ld, trailingSilenceDuration = %ld, eosLikelihood = %f, pauseCounts = %@, silencePosterior = %f
%s Setting recognized text
%s %lu results
%s EmbeddedSpeechMetric: first audio packet to first partial result = %lf secs
Words recognized: %ld
ES: Partial Recognition
%s AFSpeechInfo Package %@
%s Recognition finished with status %@
%s Audio finish to recognizer finish = %lf sec, connection is %@, error %@
%s _recognitionBeginTime (%@) is greater than _recognitionEndTime (%@)
%s Local speech recognition completed without error, write DES record when needed
%s Writing DES record after 30 seconds delay: interactionId=%@
%s Submitted delayedBlock to dispatch_after
%s #ASR on device eager formatted recognition candidate: %@
%s raw eager recognition candidate: %@
%s Could not make temporary attachment directory at %@: %@
%s Failed to specify compression algorithm: %s
%s Failed to specify format: %s
%s Start extracting archive at path: %s
%s Failed to open archive for reading: %s
%s Entry extraction path: %@
%s Unable to extract file to: %@
%s Finished extracting archive to: %@
%s Could not read %@: %@
%s Could not parse %@: %@
%s %@ is wrong type: %@
%s %@ contains bogus key/value pair: %@ => %@
%s VoiceMail asset could not be read: %@
%s Could not create asset reader output
%s Cannot add output
%s Could not get data pointer: %d
%s JSON serialization failed: %@
%s Compression failed: %@
%s Sampling: Error while initializing ESStoreAudioData because uuid is invalid.
%s %@ Sampling: Won't save audio because - has not recognized anything or has no data.
%s %@ Sampling: Won't save audio because - _currentAudioFilePath is null
%s %@ Sampling: invalid filePath or it is null.
%s %@ Sampling: Done with cleanup of audioFile=%@ and reset of variables.
%s %@ Sampling: Failed to save audio to cache dir. Error: %@
%s %@ Sampling: Successfully saved audio file to cache dir, path=%@
%s %@ Sampling: audioFileToBeMoved is nil
%s %@ Sampling: currentSamplingDate is nil
%s %@ Sampling: Error while creating Sampled directory in /var/mobile
%s %@ Sampling: Error while creating dated Sampled directory in %@ with date - %@
%s %@ Sampling: Error while moving file from cache directory to var/mobile/Library - %@
%s %@ Sampling: Successfully moved audio file to var/mobile/Library dir, path=%@
%s %@ Sampling: Error while writing audio metadata dict to plist - %@
%s %@ Sampling: currentSamplingDateString is null
%s Received SIGTERM. Cleaning and Exiting
ESSelfHelper
Timestamp
ESSelfPreheatWithPowerContainer
ESSpeechProfileBuilderConnection
CESRSpeechProfileBuilderService
NSObject
ESBiomeRecord
CESRUtilitiesAdditions
CESRUtilities
ResultCandidateId
ESAdditions
ESAssetManager
ESAlignmentState
NSCopying
ESConnectionModelInfo
ESConnection
_EARSpeechRecognitionResultStream
CESRSpeechService
ESStoreAudioData
ESListenerDelegate
NSXPCListenerDelegate
@16@0:8
@20@0:8B16
@24@0:8@16
v16@0:8
@28@0:8@16B24
B28@0:8@16B24
B24@0:8@16
v24@0:8@16
v32@0:8@16@24
v92@0:8@16@24@32@40@48@56B64@68@76@84
v48@0:8@16@24@32@40
v148@0:8@16@24@32@40@48@56@64@72B80B84@88@96@104@112@120B128B132B136@140
v32@0:8@16q24
B16@0:8
v20@0:8B16
@"NSUUID"
@"NSString"
@"NSNumber"
@40@0:8@16@24@32
@"ASRSchemaASRPreheatContext"
@"SPIPowerLoggerSnapshot"
@"SPIEventContext"
@32@0:8@16@24
@60@0:8@16@24@32B40^@44^@52
v44@0:8@16@24@32B40
v40@0:8@16@24@32
@56@0:8@16#24@32@40^@48
B88@0:8@16d24@32@40@48@56Q64@72@80
B80@0:8d16@24@32@40@48Q56@64@72
#16@0:8
@24@0:8:16
@32@0:8:16@24
@40@0:8:16@24@32
B24@0:8#16
B24@0:8:16
Vv16@0:8
Q16@0:8
^{_NSZone=}16@0:8
B24@0:8@"Protocol"16
@"NSString"16@0:8
Vv56@0:8@16@24@32q40@?48
Vv32@0:8@16@?24
Vv40@0:8@16@24@?32
Vv24@0:8@?16
Vv28@0:8B16@?20
Vv56@0:8@"NSString"16@"NSString"24@"NSString"32q40@?<v@?B@"NSError">48
Vv32@0:8@"NSString"16@?<v@?q@"NSError">24
Vv40@0:8@"NSDictionary"16@"NSString"24@?<v@?B@"NSError">32
Vv40@0:8@"NSArray"16@"NSArray"24@?<v@?B@"NSError">32
Vv24@0:8@?<v@?B@"NSError">16
Vv28@0:8B16@?<v@?B@"NSError">20
@24@0:8^@16
@"NSMutableArray"
@"NSMutableDictionary"
@"NSMutableSet"
@"CESRAssetConfig"
@"NSXPCConnection"
@"NSObject<OS_os_transaction>"
@"_EARUserProfile"
d16@0:8
v24@0:8d16
q24@0:8@16
@32@0:8@16d24
@36@0:8@16d24B32
@24@0:8Q16
q16@0:8
@40@0:8B16Q20B28^@32
v24@0:8Q16
@32@0:8@16^@24
@36@0:8@16^@24B32
@40@0:8@16^@24B32B36
v32@0:8@16Q24
B40@0:8@16Q24^@32
B32@0:8@16^@24
B24@0:8Q16
B32@0:8@16@24
@"NSObject<OS_dispatch_queue>"
@24@0:8^{_NSZone=}16
v24@0:8q16
@40@0:8@16B24B28^@32
@88@0:8@16@24B32@36@44B52@56B64B68^@72^@80
v32@0:8@16d24
v72@0:8@16q24q32d40@48d56q64
v32@0:8@"_EARSpeechRecognizer"16@"_EARSpeechRecognitionResult"24
v32@0:8@"_EARSpeechRecognizer"16@"NSError"24
v32@0:8@"_EARSpeechRecognizer"16@"NSArray"24
v48@0:8@"_EARSpeechRecognizer"16@"NSArray"24@"NSArray"32@"NSArray"40
v32@0:8@"_EARSpeechRecognizer"16@"_EARSpeechRecognitionResultPackage"24
v32@0:8@"_EARSpeechRecognizer"16d24
v32@0:8@"_EARSpeechRecognizer"16@"_EARSpeechRecognition"24
v72@0:8@"_EARSpeechRecognizer"16q24q32d40@"NSArray"48d56q64
Vv32@0:8@16@24
Vv40@0:8@16@24@32
Vv24@0:8@16
Vv48@0:8@16@24@32@?40
Vv56@0:8@16@24@32@40@?48
Vv36@0:8B16Q20@?28
Vv40@0:8B16Q20B28@?32
Vv72@0:8@16@24@32Q40B48B52@56@?64
Vv36@0:8@16B24@?28
Vv40@0:8@16Q24@?32
Vv32@0:8@16Q24
Vv76@0:8@16@24@32@40@48@56B64@?68
Vv24@0:8@?<v@?@"NSError">16
Vv32@0:8@"NSString"16@"NSURL"24
Vv40@0:8@"CESRAssetConfig"16@"NSString"24@"NSURL"32
Vv24@0:8@?<v@?>16
Vv32@0:8@"CESRSpeechParameters"16@?<v@?@"CESRModelProperties"@"NSError">24
Vv32@0:8@"CESRSpeechParameters"16@?<v@?@"NSString"@"NSString"@"NSError">24
Vv24@0:8@"NSData"16
Vv48@0:8@"NSString"16@"NSString"24@"NSData"32@?<v@?@"NSData"@"NSError">40
Vv56@0:8@"NSString"16@"NSString"24@"NSData"32@"NSString"40@?<v@?@"NSData"@"NSString"@"NSError">48
Vv36@0:8B16Q20@?<v@?@"NSDictionary"@"NSError">28
Vv40@0:8B16Q20B28@?<v@?@"NSDictionary"@"NSError">32
Vv28@0:8B16@?<v@?@"NSDictionary"@"NSError">20
Vv32@0:8@"NSString"16@?<v@?@"NSString"@"NSError">24
Vv32@0:8@"CESRAssetConfig"16@?<v@?@"NSString"@"NSError">24
Vv32@0:8@"CESRAssetConfig"16@?<v@?@"CESRModelProperties"@"NSError">24
Vv48@0:8@"NSDictionary"16@"NSData"24@"NSArray"32@?<v@?@"NSDictionary"@"NSData"@"NSError">40
Vv72@0:8@"NSDictionary"16@"NSDictionary"24@"NSString"32Q40B48B52@"NSSet"56@?<v@?@"NSDictionary"@"NSError">64
Vv36@0:8@"NSString"16B24@?<v@?@"NSData"@"NSString">28
Vv32@0:8@"NSSet"16@?<v@?@"NSNumber"@"NSError">24
Vv40@0:8@"NSSet"16Q24@?<v@?B@"NSError">32
Vv32@0:8@"NSSet"16Q24
Vv32@0:8@"AFSpeechCorrectionInfo"16@"NSString"24
Vv28@0:8B16@?<v@?>20
Vv76@0:8@"NSDictionary"16@"NSString"24@"NSDictionary"32@"NSArray"40@"NSString"48@"NSString"56B64@?<v@?@"NSDictionary"@"NSError">68
Vv40@0:8@"NSString"16@"NSString"24@"NSString"32
v36@0:8@16B24@?28
v48@0:8@16@24@32@?40
@48@0:8@16@24@32^@40
v24@0:8^@16
v28@0:8^@16B24
@"_EARSpeechRecognizer"
@"_EARSpeechRecognitionAudioBuffer"
@"AFSpeechPackage"
@"AFSpeechInfoPackage"
@"NSSet"
@"ESStoreAudioData"
@"ESBiomeRecord"
@"ESSelfHelper"
@"_EARSpeakerCodeWriter"
@"CESRFidesASRRecord"
@96@0:8@16@24@32@40Q48@56@64@72B80B84@88
v40@0:8q16@24q32
@"NSMutableData"
B32@0:8@"NSXPCListener"16@"NSXPCConnection"24
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
<key>application-identifier</key>
<string>com.apple.siri.embeddedspeech</string>
<key>com.apple.CoreRoutine.LocationOfInterest</key>
<true/>
<key>com.apple.accounts.appleaccount.fullaccess</key>
<true/>
<key>com.apple.application-identifier</key>
<string>com.apple.siri.embeddedspeech</string>
<key>com.apple.coreaudio.allow-amr-decode</key>
<true/>
<key>com.apple.coreduetd.allow</key>
<true/>
<key>com.apple.coreduetd.people</key>
<true/>
<key>com.apple.developer.homekit</key>
<true/>
<key>com.apple.frontboardservices.display-layout-monitor</key>
<true/>
<key>com.apple.locationd.effective_bundle</key>
<true/>
<key>com.apple.private.DistributedEvaluation.RecordAccess-com.apple.fides.asr</key>
<true/>
<key>com.apple.private.DistributedEvaluation.RecordAccess-com.apple.siri.speech-dictation-personalization</key>
<true/>
<key>com.apple.private.assets.accessible-asset-types</key>
<array>
<string>com.apple.MobileAsset.Trial.Siri.SiriUnderstandingAsrAssistant</string>
<string>com.apple.MobileAsset.Trial.Siri.SiriUnderstandingAsrHammer</string>
<string>com.apple.MobileAsset.Trial.Siri.SiriDictationAssets</string>
<string>com.apple.MobileAsset.EmbeddedSpeech</string>
<string>com.apple.MobileAsset.EmbeddedSpeechWatch</string>
<string>com.apple.MobileAsset.EmbeddedSpeechMac</string>
</array>
<key>com.apple.private.assets.bypass-asset-types-check</key>
<true/>
<key>com.apple.private.attribution.implicitly-assumed-identity</key>
<dict>
<key>type</key>
<string>path</string>
<key>value</key>
<string>/System/Library/PrivateFrameworks/CoreEmbeddedSpeechRecognition.framework/XPCServices/com.apple.siri.embeddedspeech.xpc/com.apple.siri.embeddedspeech</string>
</dict>
<key>com.apple.private.biome.read-write</key>
<array>
<string>SiriDictation</string>
</array>
<key>com.apple.private.calendar.allow-suggestions</key>
<true/>
<key>com.apple.private.contacts</key>
<true/>
<key>com.apple.private.corerecents</key>
<true/>
<key>com.apple.private.corespotlight.internal</key>
<true/>
<key>com.apple.private.homekit</key>
<true/>
<key>com.apple.private.security.storage.SiriVocabulary</key>
<true/>
<key>com.apple.private.security.storage.SpeechPersonalizedLM</key>
<true/>
<key>com.apple.private.tcc.allow</key>
<array>
<string>kTCCServiceAddressBook</string>
<string>kTCCServiceCalendar</string>
<string>kTCCServiceWillow</string>
<string>kTCCServiceMediaLibrary</string>
</array>
<key>com.apple.proactive.PersonalizationPortrait.Config</key>
<true/>
<key>com.apple.proactive.PersonalizationPortrait.NamedEntity.readOnly</key>
<true/>
<key>com.apple.proactive.eventtracker</key>
<true/>
<key>com.apple.security.exception.files.absolute-path.read-only</key>
<array>
<string>/private/var/MobileAsset/</string>
<string>/private/var/tmp/com.apple.siri-distributed-evaluation/</string>
</array>
<key>com.apple.security.exception.mach-lookup.global-name</key>
<array>
<string>com.apple.suggestd.contacts</string>
<string>com.apple.mobileasset.autoasset</string>
</array>
<key>com.apple.security.exception.shared-preference.read-only</key>
<array>
<string>com.apple.assistant</string>
<string>com.apple.assistant.backedup</string>
<string>com.apple.assistant.support</string>
</array>
<key>com.apple.security.iokit-user-client-class</key>
<array>
<string>AGXCommandQueue</string>
<string>AGXDevice</string>
<string>AGXDeviceUserClient</string>
<string>AGXSharedUserClient</string>
<string>H11ANEInDirectPathClient</string>
<string>IOAccelContext</string>
<string>IOAccelContext2</string>
<string>IOAccelDevice</string>
<string>IOAccelDevice2</string>
<string>IOAccelSharedUserClient</string>
<string>IOAccelSharedUserClient2</string>
<string>IOAccelSubmitter2</string>
<string>IOSurfaceRootUserClient</string>
</array>
<key>com.apple.security.personal-information.addressbook</key>
<true/>
<key>com.apple.security.temporary-exception.mach-lookup.global-name</key>
<array>
<string>com.apple.triald.namespace-management</string>
</array>
<key>com.apple.siriknowledged</key>
<true/>
<key>com.apple.spotlight.search</key>
<true/>
<key>com.apple.trial.client</key>
<array>
<string>372</string>
<string>401</string>
<string>751</string>
</array>
</dict>
</plist>
mcpl
