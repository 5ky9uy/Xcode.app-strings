_bundleIdentifier
_compatibilityVersion
_contentVersion
_masteredVersion
_downloadSize
_isPurgeable
v8@?0
audio_duration
audio_output_route
audio_queue_latency
can_use_server_tts
character_count
client_bundle_identifier
error_code
experiment_identifier
is_server_stream_tts
is_server_timeout
is_server_tts_racing
is_speech_request
is_synthesis_cached
is_warm_start
neural_alignment_stall
neural_audio_click
prompt_count
real_time_factor
server_first_packet_latency
server_last_packet_latency
server_streamed_audio_duration
source_of_tts
synthesis_to_speech_time_gap
tts_synthesis_latency
tts_total_latency
voice_asset_key
voice_resource_asset_key
  "%@": %@;
_utterance
_voiceAssetKey
_voiceResourceAssetKey
_audioOutputRoute
_clientBundleIdentifier
_experimentIdentifier
_promptCount
_sourceOfTTS
_errorCode
_requestCreatedTimestamp
_synthesisBeginTimestamp
_synthesisEndTimestamp
_speechBeginTimestamp
_speechEndTimestamp
_audioStartTimestampDiffs
_serverFirstPacketTimestamp
_audioDuration
_serverStreamedAudioDuration
_isWarmStart
_isCacheHitFromDisk
_isCacheHitFromMemory
_isSpeechRequest
_canUseServerTTS
_isServerTTS
_isServerStreamTTS
_isServerTimeout
_isServerTTSRacing
_neuralAlignmentStall
_neuralAudioClick
is_server_tts
osprey_round_trip
osprey_streaming
ace_round_trip
ace_inline_streaming
device
inline_tts
unknown
com.apple.voiceservices.notification.voice-purge
VSMobileAssetServiceErrorDomain
MobileAssetProperties
com.apple.MobileAsset.VoiceServicesVocalizerVoice
com.apple.MobileAsset.VoiceServices.CustomVoice
com.apple.MobileAsset.VoiceServices.GryphonVoice
com.apple.MobileAsset.VoiceServices.VoiceResources
Name
Languages
LanguagesCompatibility
Language
Gender
Footprint
Type
TTSResources/PreinstallAssets/
/private/var/mobile/Library/VoiceServices/voices/
%@.tmp
VSMobileAssetsManager.m
negative size
v16@?0q8
com.apple.voiced.assetQueryQueue
com.apple.voiceservices
preinstall_metadata.plist
_RelativePath
AssetData
Assets
q24@?0@"MAAsset"8@"MAAsset"16
VSMobileAssetManager
Cleaning voice assets is disabled in internal setting.
v24@?0@"NSMutableArray"8q16
%@_%@_%@_%@_%@
%@_%@
B24@?0@"MAAsset"8@"NSDictionary"16
v20@?0d8f16
v16@?0@"NSError"8
Unable to cancel an asset download: %@
Unable to download catalog
v16@?0@"MAProgressNotification"8
Unable to download asset
discretionary
immediate
Asset is in an unknown state
q24@?0@"VSVoiceAssetSelection"8@"VSVoiceAssetSelection"16
Info.plist
CFBundleIdentifier
@"NSString"8@?0
local_voice
_default
VSRecognition
<VSRecognition: %p model=%@>
recognition is already active
recognition reuse attempted
error converting disambiguation context to server
error converting audio input path to server
error converting model identifier
connection failure
recognition request denied
couldn't establish connection
recognition cancelled
connection lost
com.apple.voiceserviceslogging
default
event
enableAudioDump
logSensitiveText
DisableCaching
DisableAssetCleaning
AllowAnyAssetSubscription
EnableLocalVoices
whisper
ServerTTSTimeout
defaultVolume
forceServerTTS
disableServerTTS
disableInlineStreamTTS
disableDeviceRacing
disableOspreyStreaming
StreamBufferDuration
useBetaVoice
ospreyEndpointURL
simulateNetworkStall
disableDeviceNeuralTTS
useSSMLInput
disableMobileAssetURLReset
ignoreThermalState
tts_languages
plist
tts_language_fallbacks
en-US
_VSServerConnection
com.apple.voiced
%@:%@:%@
allowing_cellular
download_size
download_duration
download_progress
note.server.died
note.recog.prepare
note.recog.start
note.recog.results
note.recog.cancel
note.recog.error
key.recog.results
key.recog.errordesc
key.recog.errorcode
VSErrorDomain
vsplist
yyyy-MM-dd-HH-mm-ss'.vslog'
Library
Logs
CrashReporter
VoiceServicesRecognition
Latest.vslog
HH-mm-ss-
MaxLogCount
q24@?0@"NSURL"8@"NSURL"16
classes
phrases
modelid
handler
) <%@>
VSRecognitionResult
%@:%@
seqtag
known
knownp
ambig
ambigp
VSRecognitionDisambiguationContext
<VSRecognitionDisambiguationContext %p [%@]> 
 will disambiguate with sequence tag %@
 known class values:
constrained ambiguous classes:
  %@ = "%@" (%@)
  %@ = "%@"
  %@ =
     "%@" (%@)
     "%@"
com.apple.voiceservices.kwidxchanged
top-level
class-kws
__model-kws
DeviceClassNumber
+N9mZUAHooNvMiQnjeTJ8g
eJGhnVvylF3dMOHBKJzeiw
InternalBuild
com.apple.voiceservices.logging
VSLogLevel
VSOutputLevel
com.apple.voiceservices.language
lang-id
%@Library/Managed Preferences/mobile/%@.plist
mobile
com.apple.AppSupport.loggingDefaultsChanged
%@-%@
en-GB
zh-HK
zh-TW
RecognitionResources/Express
TTSResources
language_fallbacks.plist
zh-Hans
zh-CN
com.apple.language.changed
range
\audio=
\audio="%@"\
/System/Library/PrivateFrameworks/VoiceServices.framework/TTSResources/Tones/
\mrk=%@=%@\
\toi=lhp\
\toi=orth\
\tn=normal\
\tn=
.wav
\\pause=['"]?([0-9]+)['"]?\\
<break time=['"]([0-9]+)ms['"]\s*/>
\\toi=lhp\\([^
'(?:[^'\\]|\\.)*'
"(?:[^"\\]|\\.)*"
(?:%@|%@)
\w+=%@
<phoneme\s+(%@)\s+(%@)\s*/>
alphabet=
\\toi=\w+\\.*?
\\toi=orth\\
\\toi=\w+\\.*
<phoneme.*?/>
\\\w+=.+?\\
</?.*?>
<speak>
speak
<phoneme alphabet="lhp" ph="
phoneme
"></phoneme>
SSML error
unbalanced phoneme tag
say-as
</say-as>
unbalanced say-as tag
Error in tn override tag, ignore
<say-as interpret-as="%@">
</%@>
NSString *soft_AXSpeechTransformTextWithLanguage(NSString *__strong, AXSpeechTransformOptions, NSString * _Nullable __strong, NSMutableArray * _Nullable __strong)
NSString+VSSpeechService.m
AXSpeechTransformTextWithLanguage
void *AccessibilityUtilitiesLibrary(void)
/System/Library/PrivateFrameworks/AccessibilityUtilities.framework/AccessibilityUtilities
/System/Library/PrivateFrameworks/AccessibilityUtilities.framework/Contents/MacOS/AccessibilityUtilities
com.apple.voiceservices.assetInstalled
common
Library/VoiceServices/Assets
FormatVersion
/System/Library/PrivateFrameworks/VoiceServices.framework/
.migrated
NotForSiri
voice_format_version.plist
broker.hdr
broker.hdr.tmp
broker.hdr.asset
Tones
%@%@
.tmp
word_timings
TTSWordTimings
female
male
VSRecognitionSessionKeywordsDidChangeNotification
action already in progress
session is finished or invalid
session is invalid or not finished
session is already speaking
VSSpeechServiceDecoderErrorDomain
@"AVAudioBuffer"20@?0I8^q12
v32@?0@"NSData"8Q16^B24
Failed to create opus decoder
i32@?0^I8^{AudioBufferList=I[1{AudioBuffer=II^v}]}16^^{AudioStreamPacketDescription}24
decoder gave us %d bytes bytes but we really only expected %d
Could not finish decoding, res %@
<private>
recognition action not implemented
VOICE_SERVICES_OOB_SIRI_INTRO
VOICE_SERVICES_OOB_TRY_SAY_0
VOICE_SERVICES_OOB_SIRI_CAPABILITIES
VOICE_SERVICES_OOB_SIRI_CAPABILITY_HOME
VOICE_SERVICES_OOB_SIRI_CAPABILITY_NEWS
VOICE_SERVICES_OOB_SIRI_CAPABILITY_WEATHER
VOICE_SERVICES_OOB_SIRI_CAPABILITY_CLOCK
VOICE_SERVICES_OOB_SIRI_CAPABILITY_MUSIC
VOICE_SERVICES_OOB_TRY_SAY_1
VOICE_SERVICES_OOB_HEY_SIRI_CAPABILITIES
VOICE_SERVICES_OOB_HEY_SIRI_HOME
VOICE_SERVICES_OOB_HEY_SIRI_NEWS
VOICE_SERVICES_OOB_HEY_SIRI_WEATHER
VOICE_SERVICES_OOB_HEY_SIRI_WEATHER_2
VOICE_SERVICES_OOB_HEY_SIRI_MUSIC
VOICE_SERVICES_OOB_HEY_SIRI_CLOCK
VOICE_SERVICES_OOB_HEY_SIRI_CLOCK_PLANK_TIMER
VOICE_SERVICES_NETWORK_STALL
strings
%@_%d
Interstitials
DeviceSetup
VSSpeechLangCharset
CharacterBinaryMaps
could not create recognition instance
recognition already attempted or in progress
com.apple.voiceservices.metrics
com.apple.voiceservices.download
@"NSDictionary"8@?0
voice_configs.plist
vocalizer_resource_order
vocalizer_resources
_voices
VoiceServices-Config.plist
voice_speech_rate
voice_speech_pitch
voice_speech_volume
VoiceResource: %@_CV%@
_languages
_searchPathURL
_startTime
_textRange
model <%@> class <%@>
com.apple.yn
0x%x
no URL to launch
{AudioStreamBasicDescription=dIIIIIIII}
_audioData
_text
_identifier
_accessoryID
_audioSessionID
_enqueue
%@%@kHz
Opus
PCM%@KHz
sessionId %u, clientId %@, %@ bytes, input format %@, output format %@, requestCreatedTime %@, text '%@', identifier: %@, accessoryID:%@
match
replace
case-sensitive
eat-punct
com.apple.voiceservices.class
title
name
textForAttributes
attributes
text:'%@', language:%@, name:%@, gender:%@, type:%@, footprint:%@, rate:%.2f, pitch:%.2f, volume:%.2f, shouldWhisper:%d, canUseServerTTS:%d, shouldWaitCurrentSpeaking:%d, shouldCache:%d, contextInfo:%@, customResourceURLs:%@, startTime: %llu, accessoryID:%@
_languageCode
_voiceName
_footprint
_voiceType
_gender
_outputPath
_rate
_pitch
_volume
_shouldWhisper
_shouldCache
_disableCompactVoiceFallback
_forceServerTTS
_retryDeviceOnNetworkStall
_shouldStreamAudioData
_shouldWaitCurrentSpeaking
_resourceListURL
_resourceSearchPathURL
_customResourceURLs
_contextInfo
_pointer
%@%@: %@
v32@?0@8@16^B24
VSMappedData%p
%02x
 %@ %@
com.apple.voiceservices.keepalive
com.apple.voiceservices.tts
VoiceServicesErrorDomain
Request is nil.
language is not set.
Request has been used before. Please make a new copy of it.
text is not set.
Audio request has invalid audio data.
Missing text of inline streaming request.
Invalid audio request. Audio is invalid.
Audio caching request must be either inline streaming or audio request.
v16@?0@"VSVoiceResourceAsset"8
v12@?0B8
VSSpeechSynthesizer
VSSpeechSynthesizerCallbackThread
language_fallbacks
VSSpeechSynthesizer_%p@%@_%d
stop presynthesized request timeout
stop request timeout
pause request timeout
ar-SA
da-DK
it-IT
ja-JP
nb-NO
nl-NL
ru-RU
sv-SE
%@:%d
generic
-[VSSpeechSynthesizer estimateDurationOfRequest:completion:]_block_invoke
v24@?0d8@"NSError"16
-[VSSpeechSynthesizer connection:speechRequest:didStopAtEnd:phonemesSpoken:error:]_block_invoke
-[VSSpeechSynthesizer connection:synthesisRequest:didFinishWithInstrumentMetrics:error:]_block_invoke
-[VSSpeechSynthesizer setAutoDownloadedVoiceAssets:]
Auto Downloaded Assets
allowCellularVoiceUpdate
autoDownloadedAssets
subscribedAssets
lastTTSRequestDate
deviceID
com.apple.AssistantServices
com.apple.voiceservices.xpcconnection
-[VSSpeechConnection _remoteObjectSync]_block_invoke
v32@?0@"VSSpeechRequest"8Q16^B24
v32@?0@"VSPresynthesizedAudioRequest"8Q16^B24
v32@?0@"NSValue"8@"VSSpeechRequest"16^B24
-[VSSpeechConnection queryPhaticCapabilityWithRequest:]_block_invoke
v16@?0@"NSArray"8
-[VSSpeechConnection isSystemSpeaking]_block_invoke
-[VSSpeechConnection isSystemSpeakingOnBehalfOfCurrentConnection]_block_invoke
-[VSSpeechConnection forwardStreamObject:]_block_invoke
audioData
packetDescription
packetCount
asbd
undefined
compact
premium
premiumhigh
beta
vocalizer
custom
gryphon
neural
MasteredVersion
ContentVersion
builtin
%@:%@:%@:%@:%@:%@
%@:%@:%@:%@:%@:CV%@:MV%@:Compatibility%@
Type:%@, Name: %@, Languages: %@, Gender: %@, Footprint: %@, Installed: %@, ContentVersion: %@, MasteredVersion: %@, downloadSize: %@, isBuiltIn: %@
_name
_type
_isInstalled
_isBuiltInVoice
_isVoiceReadyToUse
B24@?0@8@"NSDictionary"16
v16@?0@8
_endpoint
MbP?
@mcpl
@mcpl
@supo
@supo
@mcpl
ASAttributeCompatibilityVersion should be NSNumber, got NSString for %@
CV: %@
Compatibility: %@
#MobileAsset migrate '%@', error: %@
#MobileAsset migrate '%@', success
Found cached voice resource %@ for %@
Cached voice resource is corrupted %@
Unable to find asset for VoiceResources %@
Unable to get content of VoiceResource '%{public}@', error: %{public}@
Empty asset folder '%{public}@'
Purge corrupted asset: %{public}@
Unable to find VoiceResource for %@ after the corrupted asset is purged
#MobileAsset Missing language in voice data: %@
#MobileAsset Can't find VoiceResources for %@
#MobileAsset current in-use asset, %@
Cleaning unused voice assets.
#MobileAsset Cleaning voice assets is disabled in internal setting. Skip cleaning...
#MobileAsset Purged asset: %@
#MobileAsset Unable to clean asset: %@, error: %@
#MobileAsset Ignore cached voice selection for voice query key %@ since it is not installed anymore.
Found cached voice selection %@ for voice query key %@
#MobileAsset Search local voices for lang: %{public}@, name: %{public}@
#MobileAsset Built-in voice is requested.
#MobileAsset Search beta voice asset for lang: %{public}@, name: %@, type: %@, gender: %@, footprint: %@
#MobileAsset Search voice asset for lang: %{public}@, name: %@, type: %@, gender: %@, footprint: %@
#MobileAsset Ignore neural voice since device neural TTS is disabled.
#MobileAsset Ignore neural voice since it is not ready for use.
#MobileAsset Neural voice not found, search gryphon voice asset
#MobileAsset Gryphon premiumHigh voice not found, search gryphon premium voice asset
#MobileAsset Gryphon voice not found, search premium custom voice asset with same gender
#MobileAsset Search voices in pre-installed location as fallback
#MobileAsset Search custom compact voice assets with same gender
#MobileAsset Search available vocalizer voice assets with same gender
#MobileAsset Fallback to built-in compact voice for lang: %{public}@
#MobileAsset Selected %{public}@
#MobileAsset Unable to amend voice that is not well defined: %@
#MobileAsset Start querying for amendment: %{public}@
No available neural voice found, fallback to amend with gryphon voice.
#MobileAsset Unable to download voice that is not well defined: %@
#MobileAsset Enqueued downloading: %{public}@
#MobileAsset Start querying for download: %{public}@
No available neural voice found, fallback to download gryphon voice.
No available premiumHigh voice found, fallback to download premium voice.
No available gryphon voice found, fallback to download vocalizer voice.
#MobileAsset Start querying for fallback: %{public}@
#MobileAsset Can't download due to unfound asset: %{public}@
#MobileAsset Asset is installed already: %{public}@
#MobileAsset Enqueued download cancellation for: %@
#MobileAsset No assets results: can't cancel anything
#MobileAsset Download cancellation success count: %lu. The rest was not downloading.
#MobileAsset Download cancellation failure: %@
#MobileAsset Removing voice: %@
#MobileAsset not removed because it is not present: %@
#MobileAsset Catalog Start downloading for: %@
#MobileAsset Start downloading: %@
#MobileAsset Asset not found for %@
#MobileAsset WARNING query '%@', error: %@
#MobileAsset Catalog '%@' download error: %@
#MobileAsset err %@, unable to download asset %@
#MobileAsset Finished downloading asset %@
#MobileAsset Begin %@ download, allowing cellular %{BOOL}d: %@
#MobileAsset Asset is already downloading: %@
#MobileAsset download skipped, asset is already installed: %@
#MobileAsset download skipped, asset is in an unknown state: %@
#MobileAsset purge asset: %@
#MobileAsset purge error: %@
#MobileAsset cancel downloading asset: %@
#MobileAsset Cancel download error: %@
#MobileAsset Purge cannot find asset: %@
#MobileAsset not removed because it is required by the OS: %@
#MobileAsset Couldn't find any built-in voice for lang: %@
Unable to query local voice directory, %@
Failed to convert %ld recognition results
process is not running as user Mobile: it won't share the same UserDefaults as voiced
No cache directory from which to get the size: %@
Error getting cache path: %@, error: %@
Unable to get URL: %@, URL resources: %@
Can't remove file '%@', error: %@
Cleaned directory: '%@', LRU limit: %d, latency: %f
compact explicitly specified, look at framework asset first
Now looking for the fallback language: %@
Looking for the resources in %@
Can't find the resources anywhere!
Found the resources here: %@
No TTS resources for asset. Trying fallback language for %@
Using asset fallback language %@
Couldn't get the engine format version for language %@
VoiceEngineFormatVersions hasn't been initialized, voices may not be compatible
Trying to migrate asset because it's not valid: %@
After migrating asset, it's still not valid -- sorry
---> asset's version is %@
---> engine's version is %@
Couldn't get asset's version and/or language
Deleting the asset located here: %@ because the format versions don't match
***ERROR*** There was an error (%d - %s) when trying to symlink %s to %s
Symlinked %s to %s
Successfully appended broker header files
***ERROR*** couldn't append broker header files (%d - %s)
***ERROR*** couldn't move broker header file to final location (%d - %s)
Couldn't initialize the engine voice format versions
Unable to lock temporary asset: %s; presumably peer was first - error: %d
Temporary asset: %s has moved; presumably peer was first - error: %d
Couldn't move the temporary asset: %s to the real asset: %s - error: %d
Moved the temporary asset: %s to the real asset: %s
Couldn't get the contents of the assets directory (error %ld)
Deleting asset at url: %@
Unable to locate word '%@' in '%@'
Found prepared word timing info for text: '%@'
converter.maximumOutputPacketSize is 0. Falling back to maximumPacketSize 1024. Converter is %@
Invalid target asbd: %@
Could not create Opus decoder: %{public}@
Only expecting to get 1 packet at a time, not %lu
Localize for '%@' in '%@', '%@'
Unable to find '%@' localized string for key '%@', return empty string
Searching predefined string for '%@' in '%@', '%@'
Unable to find '%@' predefined string for key '%@', return default en-US string
Unable to find '%@' predefined string for key '%@', return empty string
CoreAnalytics eventName:%@ not sent. Event name must not be in current config
Successfully reportEvent with domain '%@'
Unable to load plist at '%{public}@', error: %{public}@
Unable to read voice_configs.plist
voice_configs.plist does not have key '_voices'
You're using a deprecated method [VSVoiceResourceAsset defaultVoiceType]; use `VSSpeechVoiceDataTypeUndefined` or `[VSVoiceResourceAsset defaultVoiceNameForGender:]` instead
You're using a deprecated method [VSVoiceResourceAsset defaultVoice]; use `defaultVoiceGender` instead
Out of word boundary: %ld is greater than %ld
Enqueuing request: %@
Queue is now:
Dispatching open URL: %@
Open URL failed: %@
Unable to resize mapped file, errno: %d, error: %s
Unable to mmap file, errno: %d, error: %s
Invalid #PrewarmRequest: %@, error: %@
#PrewarmRequest %llu from client:%@, request: %@
Stop #SpeechPresynthesizedAudioRequest %llu from client:%@, synchronously: %{BOOL}d
Stop #SpeechPresynthesizedAudioRequest from client:%@ was ignored, no request to stop
Stop #SpeechRequest from client:%@ was ignored, no request to stop
Stop #SpeechRequest %llu from client:%@, boundary: %@, synchronously: %{BOOL}d
Pause #SpeechRequest %llu from client:%@, boundary: %@, synchronously: %{BOOL}d
Pause #SpeechRequest from client:%@ was ignored, no request to pause
Invalid #SynthesisRequest: %@, error: %@
Start #SynthesisRequest %llu from client:%@, %@
Invalid #SpeechRequest: %@, error: %@
Start #SpeechRequest %llu from client:%@, %{public}@
Invalid #PresynthesizedAudioRequest: %@, error: %@
Start #PresynthesizedAudioRequest %llu: %@
Invalid #AudioCachingRequest: %@, error: %@
Cache #PresynthesizedAudioRequest %llu: %@
Cancel #SpeechRequest from client:%@ was ignored, no request to stop
Cancel #SpeechRequest %llu from client:%@
Cancel #PresynthesizedAudioRequest from client:%@ was ignored, no request to stop
Cancel #PresynthesizedAudioRequest %llu from client:%@
Error Stop #SpeechRequest %@
Error Stop #PresynthesizedAudioRequest %@
Resume #SpeechRequest %llu from client:%@
Resume #SpeechRequest from client:%@ was ignored, no request to resume
#RoughEstimateDuration Request utterance: %@
#RoughEstimateDuration calculated duration: %.2f, using %@ locale, for text: %@
#EstimateDuration Request text: %@
Error %s, %@
#EstimateDuration Received duration: %.2f, for text: %@
%s, callback received in framework. %@
Current xpc connection %@ does not match %@
Delegate %@ uses deprecated callback %@
Notify daemon crash from: %@
#AudioPower Begin update
#AudioPower End update
%@ is forgetting asset.name for %s
#AutoDownloadRequest #MobileAsset, client: %@, accessory: %@, language: %@, gender: %ld, type: %ld, footprint: %ld, name: %{public}@
Request to download with cellular, client: %@, language: %@, gender: %ld, type: %ld, footprint: %ld, name: %{public}@
%{public}@ is not TTS language, fallback to %{public}@
%@ on accessory %@ has a subscribed voice: %{public}@
%@ has a subscribed voice: %{public}@
clearing auto-downloaded voice preferences for accessory %@
Closing xpc connection %p
%s, error: %@
Error updateWithConnectionIdentifier: %@
Can't prewarm %@
Error at %s , %@ 
Error estimateDurationWithRequest:reply: %@
Error %@ asking for voices
Can't start VoicePreview: %@
%s, Error: %@
Failed is_neural_voice_ready: %s
Failed has_ota_ane_model: %s
Failed is_ane_model_compiled: %s
Failed compile_ane_model: %s
softlink:r:path:/System/Library/PrivateFrameworks/AccessibilityUtilities.framework/AccessibilityUtilities
VSAssetBase
NSSecureCoding
NSCoding
VSInstrumentMetrics
MobileAsset
VSVoiceAssetSelection
VSMobileAssetsManager
VSSpeechInternalSettings
VSSpeechSynthesizerPreference
VSDownloadMetrics
VoiceServices
VSUtilities
VSSpeechService
VSWordTimingService
VSRecognitionResultHandler
NSObject
VSRecognitionResult
VSRecognitionSession
VSRecognitionSessionKeywords
VSOpusEncoder
VSOpusDecoder
VSRecognitionAction
VSLocalizedString
VSRecognitionDisambiguateAction
VSSpeechCharacterSet
VSRecognitionRecognizeAction
VSAnalytics
VSVoiceResourceAsset
VSSpeechWordTimingInfo
VSCacheUpdateListener
VSCacheUpdateRequest
VSRecognitionModelDataProvider
VSRecognitionConfirmAction
VS4CC
VSRecognitionURLAction
VSRecognitionSpeakAction
VSPresynthesizedAudioRequest
NSCopying
VSRecognitionResultHandlingThread
VSRecognitionResultHandlingRequest
VSTextPreProcessor
VSTextPreProcessorRule
VSSpeechAdditions
VSFormatArgument
VSSpeechRequest
VSMappedData
Hash
VSDurationRequest
VSSpeechSynthesizer
VSSpeechConnectionDelegate
VSVoiceSubscription
VSPreferencesInterface
VSRemoteKeepAlive
VSKeepAlive
VSSpeechXPCServiceProtocol
VSSpeechServiceDelegate
VSSpeechConnection
VSSpeechConnectionDelegateWrapper
VSAudioData
VSVoiceAsset
VSGenericUpdate
VSGenericUpdateEndpoint
NSXPCListenerDelegate
VSGenericBlockHolder
VSNeuralTTSUtils
encodeObject:forKey:
encodeBool:forKey:
init
decodeObjectOfClass:forKey:
decodeBoolForKey:
supportsSecureCoding
encodeWithCoder:
initWithCoder:
TB,R
bundleIdentifier
setBundleIdentifier:
compatibilityVersion
setCompatibilityVersion:
contentVersion
setContentVersion:
masteredVersion
setMasteredVersion:
downloadSize
setDownloadSize:
isPurgeable
setIsPurgeable:
.cxx_destruct
_isPurgeable
_bundleIdentifier
_compatibilityVersion
_contentVersion
_masteredVersion
_downloadSize
T@"NSString",C,N,V_bundleIdentifier
T@"NSNumber",C,N,V_compatibilityVersion
T@"NSNumber",C,N,V_contentVersion
T@"NSString",C,N,V_masteredVersion
T@"NSNumber",C,N,V_downloadSize
TB,N,V_isPurgeable
setMinimumFractionDigits:
setMaximumFractionDigits:
stringWithString:
dictionaryMetrics
countByEnumeratingWithState:objects:count:
objectForKeyedSubscript:
descriptionFormatter
stringFromNumber:
appendFormat:
appendString:
encodeInteger:forKey:
encodeInt64:forKey:
encodeDouble:forKey:
decodeIntegerForKey:
decodeInt64ForKey:
decodeDoubleForKey:
_clockFactor
ttsSynthesisLatency
realTimeFactor
numberWithDouble:
length
numberWithUnsignedInteger:
numberWithBool:
timeToSpeakLatency
audioQueueLatency
eagerRequestTimeGap
isSynthesisCached
numberWithInteger:
serverStreamFirstPacketLatency
serverStreamLastPacketLatency
cappedRealTimeFactor
dictionaryWithObjects:forKeys:count:
stringOfSourceOfTTS:
description
utterance
setUtterance:
voiceAssetKey
setVoiceAssetKey:
voiceResourceAssetKey
setVoiceResourceAssetKey:
audioOutputRoute
setAudioOutputRoute:
clientBundleIdentifier
setClientBundleIdentifier:
experimentIdentifier
setExperimentIdentifier:
requestCreatedTimestamp
setRequestCreatedTimestamp:
eagerRequestCreatedTimeStampDiffs
setEagerRequestCreatedTimeStampDiffs:
synthesisBeginTimestamp
setSynthesisBeginTimestamp:
synthesisEndTimestamp
setSynthesisEndTimestamp:
speechBeginTimestamp
setSpeechBeginTimestamp:
speechEndTimestamp
setSpeechEndTimestamp:
audioStartTimestampDiffs
setAudioStartTimestampDiffs:
serverFirstPacketTimestamp
setServerFirstPacketTimestamp:
serverLastPacketTimestamp
setServerLastPacketTimestamp:
serverStreamedAudioDuration
setServerStreamedAudioDuration:
audioDuration
setAudioDuration:
isWarmStart
setIsWarmStart:
isServerTTS
setIsServerTTS:
isServerStreamTTS
setIsServerStreamTTS:
isServerTimeout
setIsServerTimeout:
isServerTTSRacing
setIsServerTTSRacing:
canUseServerTTS
setCanUseServerTTS:
neuralAlignmentStall
setNeuralAlignmentStall:
neuralAudioClick
setNeuralAudioClick:
promptCount
setPromptCount:
errorCode
setErrorCode:
sourceOfTTS
setSourceOfTTS:
isSpeechRequest
setIsSpeechRequest:
isCacheHitFromDisk
setIsCacheHitFromDisk:
isCacheHitFromMemory
setIsCacheHitFromMemory:
_isWarmStart
_isServerTTS
_isServerStreamTTS
_isServerTimeout
_isServerTTSRacing
_canUseServerTTS
_neuralAlignmentStall
_neuralAudioClick
_isSpeechRequest
_isCacheHitFromDisk
_isCacheHitFromMemory
_utterance
_voiceAssetKey
_voiceResourceAssetKey
_audioOutputRoute
_clientBundleIdentifier
_experimentIdentifier
_requestCreatedTimestamp
_eagerRequestCreatedTimeStampDiffs
_synthesisBeginTimestamp
_synthesisEndTimestamp
_speechBeginTimestamp
_speechEndTimestamp
_audioStartTimestampDiffs
_serverFirstPacketTimestamp
_serverLastPacketTimestamp
_serverStreamedAudioDuration
_audioDuration
_promptCount
_errorCode
_sourceOfTTS
T@"NSString",C,V_utterance
T@"NSString",C,V_voiceAssetKey
T@"NSString",C,V_voiceResourceAssetKey
T@"NSString",C,V_audioOutputRoute
T@"NSString",C,V_clientBundleIdentifier
T@"NSString",C,V_experimentIdentifier
Tq,V_requestCreatedTimestamp
Tq,V_eagerRequestCreatedTimeStampDiffs
Tq,V_synthesisBeginTimestamp
Tq,V_synthesisEndTimestamp
Tq,V_speechBeginTimestamp
Tq,V_speechEndTimestamp
Tq,V_audioStartTimestampDiffs
Tq,V_serverFirstPacketTimestamp
Tq,V_serverLastPacketTimestamp
Td,V_serverStreamedAudioDuration
Td,V_audioDuration
TB,V_isWarmStart
TB,V_isServerTTS
TB,V_isServerStreamTTS
TB,V_isServerTimeout
TB,V_isServerTTSRacing
TB,V_canUseServerTTS
TB,V_neuralAlignmentStall
TB,V_neuralAudioClick
Tq,V_promptCount
Tq,V_errorCode
Tq,V_sourceOfTTS
TB,V_isSpeechRequest
TB,V_isCacheHitFromDisk
TB,V_isCacheHitFromMemory
languagesFromMobileAssetAttributes:
setLanguages:
genderFromString:
setGender:
typeFromString:
setType:
footprintFromString:
setFootprint:
amendNameVersionAndSizeWithMobileAssetAttributes:
name
setName:
compatibilityVersionFromMobileAssetAttributes:
count
arrayWithCapacity:
stringByReplacingOccurrencesOfString:withString:
addObject:
arrayWithObjects:count:
integerValue
initFromMobileAssetAttributes:
voiceData
voiceKey
descriptiveKey
asset
getLocalUrl
path
languages
firstObject
stringWithFormat:
stringByAppendingPathComponent:
defaultManager
fileExistsAtPath:
attributes
currentHandler
handleFailureInMethod:object:file:lineNumber:description:
wasLocal
state
type
isVoiceReadyToUse
footprint
floatValue
voicePath
size
isInstalled
isDownloading
preferenceScore
setVoiceData:
setAsset:
builtInVoicePath
setBuiltInVoicePath:
setVoicePath:
_voiceData
_asset
_builtInVoicePath
_voicePath
T@"VSVoiceAsset",&,V_voiceData
T@"MAAsset",&,V_asset
T@"NSString",&,V_builtInVoicePath
T@"NSString",&,N,V_voicePath
migrateAssetIfNeededWithAssetType:
initWithType:
returnTypes:
downloadCatalog:options:completion:
queryMetaData:
setCountLimit:
numberWithLong:
longValue
_getVoiceAssetsForType:voicename:language:gender:footprint:returnTypes:
voiceDataFromAsset:
gender
pickCorrectAssetFromLocalAssets:
legacyLocalVocalizerVoiceAssetForLanguage:
bundleWithIdentifier:
bundlePath
preinstallAssetsDirectory
dictionaryWithContentsOfFile:
setIsInstalled:
setIsBuiltInVoice:
setIsVoiceReadyToUse:
preinstallAssetsMetadata
array
isEqualToString:
voiceAssetFromPreinstallMetadata:
preinstalledVoicesForLanguage:gender:
allKeys
_builtInVoiceForLanguage:
queryForVoiceResourceAsset:returnTypes:
_getResults:
sortUsingComparator:
lastObject
cachedVoiceResources
objectForKey:
searchPathURL
contentsOfDirectoryAtPath:error:
removeObjectForKey:
_installedVoiceResourceAssetForLanguage:
getLocalFileUrl
_purgeAsset:
voiceResourceFromAsset:
setObject:forKey:
initWithCapacity:
bundleIdentifierForVoiceType:
arrayWithObjects:
addKeyValueArray:with:
arrayWithObject:
typeStringFromType:
addKeyValuePair:with:
footprintStringFromFootprint:
genderStringFromGender:
intValue
selectVoiceResourceAssetForLanguage:
amendVoice:withDefaultSettingsFrom:
defaultVoiceGender
defaultVoiceNameForGender:
isNeuralTTSPlatform
isHomePod
defaultInstance
subscribedVoicesForClientID:accessoryID:
voice
selectVoiceForLang:name:type:gender:footprint:
containsObject:
_nonCacheVoiceSelectionForLanguage:name:type:gender:footprint:
activeVoiceAssets
installedAssetsForType:voicename:language:gender:footprint:
assetType
standardInstance
disableAssetCleaning
errorWithDomain:code:userInfo:
inactiveVoiceAssets
purge:
resetCache
dictionary
setObject:forKeyedSubscript:
cachedVoiceSelections
removeAllObjects
amendVoiceWithDefaultSettings:
enableLocalVoices
_localVoiceForLanguageAndNamePath:
useBetaVoice
disableDeviceNeuralTTS
selectPreinstalledVoiceForLanguage:gender:
setDiscretionary:
setRequiresPowerPluggedIn:
isVoiceAssetWellDefined:
predicateWithBlock:
filteredArrayUsingPredicate:
getLatestAssetFromArray:
downloadOptionsWithBattery:
downloadVoiceAsset:options:progressUpdateHandler:
assetQueryQueue
downloadCatalog:options:
initWithVoiceName:languageCode:gender:
mainBundle
allowsCellularAccess
setIsCellularAllowed:
discretionary
setDownloadProgress:
code
reportDownloadMetrics:
_downloadAsset:options:progress:completion:
arrayByAddingObjectsFromArray:
cancelDownloads:error:
cancelDownload:
downloadVoiceResource:options:completion:
downloadVoiceResourceCatalogWithCompletion:
queryMetaDataSync
results
queryForLanguage:forType:voicename:gender:footprint:returnTypes:
isWatch
setAllowsCellularAccess:
lastFetchDate
date
timeIntervalSinceDate:
currentRunLoop
dateWithTimeIntervalSinceNow:
runUntilDate:
startCatalogDownload:options:then:
isStalled
expectedTimeRemaining
totalExpected
totalWritten
attachProgressCallBack:
startDownload:then:
defaultCenter
postNotificationName:object:
purgeSync
cancelDownloadSync
setSearchPathURL:
wasPurgeable
sortedArrayUsingComparator:
pathWithComponents:
voiceDataWithBundleIdentifier:attributes:voicePathCallback:
componentsSeparatedByString:
typeFromBundleIdentifier:
isNeuralVoiceReady:
sharedManager
migrateAssets
builtInVoices
cleanUnusedVoiceAssets
cleanOldVoiceResources
resetResourcesCache
installedVoiceResources
amendVoiceAssetWithLatestKnownData:
downloadVoiceAsset:useBattery:progressUpdateHandler:
cancelDownload:completion:
cancelResourceDownload:completion:
removeVoiceAsset:completion:
downloadVoiceResource:completion:
removeVoiceResource:completion:
voiceAssetWithName:localOnly:outError:
purgeAsset:
installedLocalVoices
setAssetQueryQueue:
setCachedVoiceSelections:
setCachedVoiceResources:
_assetQueryQueue
_cachedVoiceSelections
_cachedVoiceResources
T@"NSObject<OS_dispatch_queue>",&,N,V_assetQueryQueue
T@"NSCache",&,N,V_cachedVoiceSelections
T@"NSCache",&,N,V_cachedVoiceResources
isInternalBuild
initWithSuiteName:
boolForKey:
setBool:forKey:
floatForKey:
setFloat:forKey:
stringForKey:
enableAudioDump
setEnableAudioDump:
logSensitiveText
setLogSensitiveText:
disableCache
setDisableCache:
setDisableAssetCleaning:
allowAnyAssetSubscriber
setAllowAnyAssetSubscriber:
setEnableLocalVoices:
whisper
setWhisper:
serverTTSTimeout
setServerTTSTimeout:
defaultVolume
setDefaultVolume:
forceServerTTS
setForceServerTTS:
disableServerTTS
setDisableServerTTS:
disableInlineStreamTTS
setDisableInlineStreamTTS:
disableDeviceRacing
setDisableDeviceRacing:
disableOspreyStreaming
setDisableOspreyStreaming:
streamBufferDuration
setStreamBufferDuration:
setUseBetaVoice:
ospreyEndpointURL
setOspreyEndpointURL:
simulateNetworkStall
setSimulateNetworkStall:
setDisableDeviceNeuralTTS:
useSSMLInput
disableMobileAssetURLReset
ignorePowerAndThermalState
setIgnorePowerAndThermalState:
internalDefaults
setInternalDefaults:
_internalDefaults
T@"NSUserDefaults",&,N,V_internalDefaults
TB,N
Tf,N
T@"NSString",C,N
TB,R,N
bundleForClass:
pathForResource:ofType:
arrayWithContentsOfFile:
setWithArray:
availableLanguages
rangeOfString:
substringWithRange:
fallbackLanguageForLanguage:
downloadDuration
numberWithFloat:
endMetrics
isCellularAllowed
downloadProgress
voiceDownloadKey
downloadBeginTimestamp
downloadEndTimestamp
_isCellularAllowed
_discretionary
_downloadProgress
_voiceDownloadKey
_downloadBeginTimestamp
_downloadEndTimestamp
T@"NSString",R,V_voiceDownloadKey
Tq,R,V_downloadBeginTimestamp
Tq,R,V_downloadEndTimestamp
TB,V_isCellularAllowed
TB,V_discretionary
T@"NSNumber",C,V_downloadSize
Tf,V_downloadProgress
fileURLWithPath:isDirectory:
contentsOfDirectoryAtURL:includingPropertiesForKeys:options:error:
resourceValuesForKeys:error:
longLongValue
fileURLWithPath:
removeItemAtURL:error:
getResourceValue:forKey:error:
compare:
subarrayWithRange:
directorySize:
removeDirectory:
cleanDirectory:withLRULimit:
cleanDirectory:withDateOlderThan:
hasANE
hasAMX
isSeedBuild
initWithFormat:
initWithContentsOfFile:
rangeOfString:options:range:
hasPrefix:
hasSuffix:
insertString:atIndex:
replaceCharactersInRange:withString:
string
vs_markerStringForContext:
stringByAppendingString:
regularExpressionWithPattern:options:error:
matchesInString:options:range:
characterAtIndex:
whitespaceCharacterSet
characterIsMember:
punctuationCharacterSet
rangeAtIndex:
_vs_countPhoneticSyllables_lhp:
numberOfRanges
containsString:
_vs_countPhoneticSyllables_xsampa:
stringByReplacingMatchesInString:options:range:withTemplate:
addCharactersInRange:
vs_isCJKCharacter:
initWithString:
removeLastObject
raise:format:
vs_textifyEmojiWithLanguage:
vs_substituteAudioWithLocalPath
vs_insertContextInfo:
vs_measurePauses
vs_countPhoneticSyllables
vs_removePhonetics
vs_removeSpeechTags
vs_hasCJKCharacter
vs_convertToSSML
stringWithUTF8String:
handleFailureInFunction:file:lineNumber:description:
stringByStandardizingPath
initWithContentsOfURL:
filePathURL
attributesOfItemAtPath:error:
textRange
setTextRange:
allValues
doubleValue
setStartTime:
pathForResource:ofType:inDirectory:
timingPlistForLanguage:
timingInfosFrom:withText:
estimatedTTSWordTimingForText:withLanguage:withGender:
wordTimings
setWordTimings:
_wordTimings
T@"NSDictionary",&,N,V_wordTimings
objectAtIndex:
mutableCopy
removeObjectAtIndex:
replaceObjectAtIndex:withObject:
modelIdentifier
recognitionResultWithModelIdentifier:classIdentifiers:values:
elementCount
getElementClassIdentifier:value:atIndex:
bundleWithPath:
load
isEqual:
class
self
performSelector:
performSelector:withObject:
performSelector:withObject:withObject:
isProxy
isKindOfClass:
isMemberOfClass:
conformsToProtocol:
respondsToSelector:
retain
release
autorelease
retainCount
zone
hash
superclass
debugDescription
TQ,R
T#,R
T@"NSString",R,C
actionForRecognitionResult:
actionForRecognitionResults:
instancesRespondToSelector:
initialize
recognitionResultByReplacingValueForClassIdentifier:withValue:
valueOfFirstElementWithClassIdentifier:
createHandler
setRecognitionAction:
recognitionAction
reset
_init
cancel
setDelegate:
setActive:
dealloc
recognitionSessionDidBeginAction:
recognitionSessionWillBeginAction:
recognitionSession:openURL:
recognitionSession:openURL:completion:
recognitionSession:didCompleteActionWithError:
recognitionSession:didFinishSpeakingFeedbackStringWithError:
_notifyDelegateActionStarted
_continueAfterDeferredStart
perform
_hasDeferredStartCallback
_currentRecognizeAction
_debugDumpPath
setAudioType:
setKeepAudioSessionActive:
initWithModelIdentifier:
_setAction:
_isRecognizing
_isActivelyRecognizing
completionType
_setBluetoothInputAllowed:
invalidate
stopSpeakingAtNextBoundary:synchronously:error:
cancelMaintainingKeepAlive:
methodSignatureForSelector:
invocationWithMethodSignature:
setSelector:
setTarget:
setArgument:atIndex:
retainArguments
scheduledTimerWithTimeInterval:invocation:repeats:
_setSession:
_setDebugDumpPath:
_setDebugDumpEnabled:
_setPreferredEngine:
_setAudioInputPath:
_setInputLevelUpdateInterval:
_setEngineResetRequired:
_handledThreadedResults:nextAction:
spokenFeedbackString
spokenFeedbackAttributedString
resultDisplayString
statusDisplayString
_inputLevel
_inputLevelDB
_keywordAtIndex:
_keywordCount
_scrambledKeywordsAndAddToSet:
_nextKeywordUsingCursors:
removeObject:
allObjects
_createKeywordIndex
_createPhaseSortedKeywordsFromArray:
_topLevelKeywords
_keywordIndexChanged
postNotificationName:object:userInfo:
_beginSpeakingAttributedString:
beginSpeakingString:
_beginSpeakingString:attributedString:
_notifyDelegateFinishedSpeakingWithError:
initForInputFeedback
setAttributedText:
setText:
setLanguageCode:
setOutputPath:
startSpeakingRequest:
beginNextAction
isRecognizing
isActivelyRecognizing
isFinished
isValid
hasDeferredAction
isBusy
nextActionWillTerminateSession
nextActionWillRecognize
setSensitiveActionsEnabled:
sensitiveActionsEnabled
setBluetoothInputAllowed:
_actionCompleted:nextAction:error:
_actionStarted:
_notifyDelegateOpenURL:completion:
_recognitionResultHandlingThread
recognitionResultHandlingThread:didHandleResults:nextAction:
displayResultString
displayStatusString
setInputLevelUpdateInterval:
inputLevel
inputLevelDB
setKeywordPhase:
keywordAtIndex:
keywordCount
_keywordsForModelIdentifier:
beginSpeakingFeedbackString
speechSynthesizer:didFinishSpeaking:withError:
setDebugDumpEnabled:
debugDumpPath
setNextRecognitionAudioInputPath:
setNextRecognitionRequiresReset:
setPreferredEngine:
setPerformRecognitionHandlerActions:
_modelIdentifier
_keepAlive
_delegate
_currentAction
_handlingThread
_synthesizer
_languageID
_audioInputPath
_levelInterval
_keywordPhase
_sessionFlags
exchangeObjectAtIndex:withObjectAtIndex:
initWithStreamDescription:
initFromFormat:toFormat:
maximumOutputPacketSize
initWithFormat:packetCapacity:maximumPacketSize:
streamDescription
data
initWithPCMFormat:frameCapacity:
setFrameLength:
mutableAudioBufferList
bytes
convertToBuffer:error:withInputFromBlock:
audioBufferList
appendBytes:length:
packetCount
packetDescriptions
encodeChunk:
initWithSourceASBD:
setOpusDataHandler:
setErrorHandler:
beginEncoding
endEncoding
opusDataHandler
errorHandler
fromFormat
setFromFormat:
toFormat
setToFormat:
converter
setConverter:
outputBuffer
setOutputBuffer:
opusDataOffset
setOpusDataOffset:
_opusDataHandler
_errorHandler
_fromFormat
_toFormat
_converter
_outputBuffer
_opusDataOffset
T@?,C,N,V_opusDataHandler
T@?,C,N,V_errorHandler
T@"AVAudioFormat",&,N,V_fromFormat
T@"AVAudioFormat",&,N,V_toFormat
T@"AVAudioConverter",&,N,V_converter
T@"AVAudioCompressedBuffer",&,N,V_outputBuffer
Tq,N,V_opusDataOffset
vs_stringFrom4CC:
beginChunkDecoderForStreamDescription:
decodeChunk:outError:
appendData:
enumerateObjectsUsingBlock:
endChunkDecoding
_opusDecoder:
initWithLength:
mutableBytes
dataWithBytes:length:
sharedInstance
decodeChunks:streamDescription:outError:
_decoder
_asbd
_session
setResultDisplayString:
setStatusDisplayString:
setSpokenFeedbackString:
setSpokenFeedbackAttributedString:
completeWithNextAction:error:
_resultString
_statusString
_spokenString
_spokenStringIsAttributed
uppercaseString
localizations
preferredLocalizationsFromArray:forPreferences:
URLForResource:withExtension:subdirectory:localization:
predefinedStringForKey:language:gender:table:
appendRandomizationKey:withCount:
localizedStringForKey:language:gender:table:
localizedInterstitialStringForKey:language:gender:
localizedOOBStringForKey:language:gender:
_disambiguationContext
_reset
setRepeatedSpokenFeedbackString:
repeatedSpokenFeedbackString
sequenceTag
setSequenceTag:
knownValueForClassIdentifier:
setKnownValue:phoneticValue:forClassIdentifier:
knownValuesForClassIdentifier:
setKnownValues:phoneticValues:forClassIdentifier:
ambiguousValuesForClassIdentifier:
setAmbiguousValues:phoneticValues:forClassIdentifier:
_keywords
setKeywords:
_createRecognitionInstanceWithCallbacks:info:
_actionForEmptyResults
_repeatedSpokenFeedbackString
_sequenceTag
_knownValues
_knownPhoneticValues
_ambiguousValues
_ambiguousPhoneticValues
_context
addObjectsFromArray:
languageMapping
dataWithContentsOfFile:
characterSetWithBitmapRepresentation:
characterSet
valueWithRange:
initWithLanguage:
unspeakableRangeOfText:
setCharacterSet:
_characterSet
T@"NSCharacterSet",&,N,V_characterSet
_setDebugDumpEnabled:dumpPath:
_configureNewRecognitionInstance
_setResults:
_releaseFromPrepare
requiresThreadedProcessing
handleResults:withHandler:
makeObjectsPerformSelector:withObject:
_handleRecognitionPrepared:
_handleRecognitionStarted:
_handleRecognitionCompleted:withResults:error:
_recognition
_results
_recognizeFlags
reportEvent:payload:
reportInstrumentMetrics:
componentsJoinedByString:
decodeObjectOfClasses:forKey:
vocalizerConfig
URLByAppendingPathComponent:
dictionaryWithContentsOfURL:
dictionaryWithContentsOfURL:error:
voiceConfig
_defaultVoices
rate
pitch
volume
resourceMimeTypes
resourceList
defaultVoiceType
defaultVoice
setVoiceConfig:
setRate:
setPitch:
setVolume:
setVocalizerConfig:
setResourceList:
setResourceMimeTypes:
_rate
_pitch
_volume
_languages
_searchPathURL
_voiceConfig
_vocalizerConfig
_resourceList
_resourceMimeTypes
T@"NSDictionary",C,N,V_voiceConfig
Tf,N,V_rate
Tf,N,V_pitch
Tf,N,V_volume
T@"NSDictionary",&,N,V_vocalizerConfig
T@"NSArray",C,N,V_languages
T@"NSArray",C,N,V_resourceList
T@"NSDictionary",C,N,V_resourceMimeTypes
T@"NSURL",C,N,V_searchPathURL
rangeValue
whitespaceAndNewlineCharacterSet
UTF8String
lengthOfBytesUsingEncoding:
startTime
extraBytesFromUTF8ToUTF16With:totalLength:begin:end:
wordTimingInfoFrom:timestamps:
utf16TimingInfoWithUTF8Range:withText:
_startTime
_textRange
Td,N,V_startTime
T{_NSRange=QQ},N,V_textRange
stopListening
_initShared
lock
_spokenLanguageChanged:
addObserver:selector:name:object:
unlock
beginReportingChanges
makeObjectsPerformSelector:
stopReportingChanges
removeObserver:name:object:
_flush
initWithModelIdentifier:classIdentifier:
_enqueueRequest:
performUpdateForModelIdentifier:classIdentifier:
coalescedRequest:
timerWithTimeInterval:target:selector:userInfo:repeats:
mainRunLoop
addTimer:forMode:
setFireDate:
classIdentifier
sharedListener
sharedListenerIfExists
startListening
_lock
_updateRequestQueue
_dataProviders
_flushTimer
_isListening
_modelID
_classID
valueCountForClassWithIdentifier:inModelWithIdentifier:
valueAtIndex:forClassWithIdentifier:inModelWithIdentifier:
getValue:weight:atIndex:forClassWithIdentifier:inModelWithIdentifier:
phoneticValueAtIndex:forClassWithIdentifier:inModelWithIdentifier:
cacheValidityIdentifier
isCacheValidityIdentifierValid:
_setConfirmed:
setConfirmedAction:
confirmedAction
setDeniedAction:
deniedAction
_confirmedAction
_deniedAction
_confirmFlags
setURL:
_url
pathForResource:ofType:inDirectory:forLocalization:
initFileURLWithPath:
initWithSpokenFeedbackString:willTerminate:
_shouldTerminate
canLogRequestText
copy
encodeValueOfObjCType:at:
encodeInt32:forKey:
initWithAudioData:decoderStreamDescription:playerStreamDescription:
decodeValueOfObjCType:at:size:
decodeInt32ForKey:
numberWithUnsignedLongLong:
logText
copyWithZone:
initWithAudioData:playerStreamDescription:
initWithIdentifier:
hasValidAudio
setAccessoryID:
audioSessionID
setAudioSessionID:
audioData
decoderStreamDescription
playerStreamDescription
enqueue
setEnqueue:
text
identifier
setIdentifier:
accessoryID
pcmDataSize
setPcmDataSize:
stopHandler
setStopHandler:
_enqueue
_audioSessionID
_audioData
_text
_identifier
_accessoryID
_pcmDataSize
_stopHandler
_decoderStreamDescription
_playerStreamDescription
T@"NSString",C,N,V_clientBundleIdentifier
T@"NSUUID",C,N,V_accessoryID
TQ,N,V_pcmDataSize
T@?,C,N,V_stopHandler
TI,N,V_audioSessionID
T@"NSData",R,C,N,V_audioData
T{AudioStreamBasicDescription=dIIIIIIII},R,N,V_decoderStreamDescription
T{AudioStreamBasicDescription=dIIIIIIII},R,N,V_playerStreamDescription
TB,N,V_enqueue
T@"NSString",&,N,V_text
T@"NSString",&,N,V_identifier
TQ,N,V_requestCreatedTimestamp
initWithCondition:
initWithHandler:results:
_handleRequests
detachNewThreadSelector:toTarget:withObject:
unlockWithCondition:
nextAction
lockWhenCondition:
handler
setNextAction:
_notifyRequestHandled:
performSelectorOnMainThread:withObject:waitUntilDone:
_requests
_resultHandlingFlags
_handler
_action
initWithDictionaryRepresentation:
matchedString:forTokenInRange:
initWithContentsOfPath:languageIdentifier:
processedTextFromString:
_rules
_tokenizer
boolValue
_matchPattern
_replacement
_caseSensitive
_eatPunctuation
appendString:withAttributes:
attributedStringWithFormatAndAttributes:
formatSpecifier
formattedArg
setAttributes:range:
contextInfoString
languageCode
contextInfo
isEqualToDictionary:
voiceType
voiceName
shouldWhisper
customResourceURLs
isEqualToArray:
allocWithZone:
setVoiceType:
setVoiceName:
outputPath
shouldCache
setShouldCache:
setShouldWhisper:
setContextInfo:
disableCompactVoiceFallback
setDisableCompactVoiceFallback:
resourceListURL
setResourceListURL:
resourceSearchPathURL
setResourceSearchPathURL:
setCustomResourceURLs:
decodePropertyListForKey:
enumerateKeysAndObjectsUsingBlock:
logUtterance
isSimilarTo:
shouldWaitCurrentSpeaking
setShouldWaitCurrentSpeaking:
retryDeviceOnNetworkStall
setRetryDeviceOnNetworkStall:
attributedText
shouldStreamAudioData
setShouldStreamAudioData:
pauseHandler
setPauseHandler:
pointer
setPointer:
_shouldWaitCurrentSpeaking
_shouldCache
_shouldWhisper
_disableCompactVoiceFallback
_forceServerTTS
_retryDeviceOnNetworkStall
_shouldStreamAudioData
_languageCode
_voiceName
_footprint
_voiceType
_gender
_outputPath
_contextInfo
_resourceListURL
_resourceSearchPathURL
_customResourceURLs
_attributedText
_pauseHandler
_pointer
T@"NSAttributedString",C,N,V_attributedText
TB,N,V_shouldStreamAudioData
T@"NSString",C,N,V_utterance
T@?,C,N,V_pauseHandler
Tq,N,V_pointer
T@"NSString",C,N,V_text
T@"NSString",C,N,V_languageCode
T@"NSString",C,N,V_voiceName
Tq,N,V_footprint
Tq,N,V_voiceType
Tq,N,V_gender
T@"NSURL",C,N,V_outputPath
TB,N,V_shouldWaitCurrentSpeaking
TB,N,V_shouldCache
Td,N,V_rate
Td,N,V_pitch
Td,N,V_volume
TB,N,V_shouldWhisper
T@"NSDictionary",C,N,V_contextInfo
TB,N,V_disableCompactVoiceFallback
TB,N,V_forceServerTTS
TB,N,V_canUseServerTTS
TB,N,V_retryDeviceOnNetworkStall
T@"NSURL",C,N,V_resourceListURL
T@"NSURL",C,N,V_resourceSearchPathURL
T@"NSArray",&,N,V_customResourceURLs
createFileAtPath:contents:attributes:
fileHandleForUpdatingAtPath:
fileDescriptor
closeFile
initWithFilePath:initialSize:
removeItemAtPath:error:
_convertToFallbackMemory
_appendToFallbackMemory:
_appendToMappedMemory:
bytesAtOffset:
filePath
setFilePath:
totalLength
setTotalLength:
mmappedData
setMmappedData:
mappedLength
setMappedLength:
fallbackInMemoryData
setFallbackInMemoryData:
shouldCleanFile
setShouldCleanFile:
_shouldCleanFile
_filePath
_totalLength
_mmappedData
_mappedLength
_fallbackInMemoryData
T@"NSString",&,N,V_filePath
TQ,N,V_totalLength
T^v,N,V_mmappedData
TQ,N,V_mappedLength
T@"NSMutableData",&,N,V_fallbackInMemoryData
TB,N,V_shouldCleanFile
stringWithCapacity:
stringByAppendingFormat:
sha256hex
preinstalledAudioHashForLanguage:name:
completion
setCompletion:
_completion
T@?,C,N,V_completion
validatePrewarmRequest:
getVoiceResourceForLanguage:reply:
playVoicePreviewForLanguageCode:gender:completion:
playVoicePreviewForLanguageCode:voiceName:previewType:completion:
startVoicePreviewForLanguageCode:gender:reply:
startVoicePreviewForLanguageCode:voiceName:previewType:reply:
stopPlayingVoicePreview
stopVoicePreview
initWithAccessoryID:
preferredLocalizations
processInfo
processName
processIdentifier
opaqueSessionID
speechSynthesizer:didFinishPrewarmRequest:withError:
prewarmIfNeededWithRequest:reply:
queryPhaticCapabilityWithRequest:
speechSynthesizer:didStartSpeakingRequest:
speechSynthesizerDidStartSpeaking:
speechSynthesizer:didFinishSpeakingRequest:successfully:withError:
speechSynthesizer:didFinishSpeakingRequest:successfully:phonemesSpoken:withError:
speechSynthesizer:didFinishSpeaking:phonemesSpoken:withError:
speechSynthesizer:didPauseSpeakingRequest:
speechSynthesizerDidPauseSpeaking:
speechSynthesizer:didContinueSpeakingRequest:
speechSynthesizerDidContinueSpeaking:
speechSynthesizer:willSpeakRangeOfSpeechString:forRequest:
speechSynthesizer:willSpeakRangeOfSpeechString:
speechSynthesizer:didFinishSpeakingRequest:withInstrumentMetrics:
speechSynthesizer:withSynthesisRequest:didGenerateAudioChunk:
stopPresynthesizedAudioRequest:
stopSpeechRequest:atMark:
currentRequest
pauseSpeechRequest:atMark:
_setDelegate:
validateRequest:
startSynthesisRequest:
startSpeechRequest:
validateAudioRequest:
startPresynthesizedAudioRequest:
validateAudioCachingRequest:
cachePresynthesizedAudioRequest:
_stopSpeakingRequest:atNextBoundary:synchronously:
currentAudioRequest
_stopSpeakingPresynthesizedAudioRequest:synchronously:
_pauseSpeakingRequestAtNextBoundary:synchronously:
isSystemSpeakingOnBehalfOfCurrentConnection
isSystemSpeaking
continueSpeechRequest:
decimalDigitCharacterSet
numberWithInt:
characterClassCountForUtterance:language:
objectAtIndexedSubscript:
unsignedIntegerValue
estimateDurationWithRequest:reply:
speechSynthesizer:withRequest:didReceiveTimingInfo:
valueWithNonretainedObject:
durationRequests
speechSynthesizer:didFinishSynthesisRequest:withInstrumentMetrics:error:
speechSynthesizer:didFinishSynthesisRequest:withError:
speechSynthesizer:didStartPresynthesizedAudioRequest:
speechSynthesizer:didStopPresynthesizedAudioRequest:atEnd:error:
speechSynthesizer:didStopPresynthesizedAudioRequestAtEnd:error:
speechSynthesizer:didFinishPresynthesizedAudioRequest:withInstrumentMetrics:error:
speechSynthesizer:daemonDidCrashWithError:
setLogToFile:
getLogToFile:
_continueSpeakingRequest
forwardStreamObject:
cancelDownloads:
invokeDaemon:
killDaemon
beginAudioPowerUpdateWithReply:
endAudioPowerUpdate
cleanUnusedAssets:
getLocalVoiceAssetsForLanguage:reply:
getLocalVoiceResources:
setSubscribedVoiceAssets:withClientID:forAccessoryID:
triggerCellularDownloadedVoiceAssets:withClientID:
getSubscribedVoiceAssetsWithClientID:forAccessoryID:reply:
getVoiceInfoForLanguageCode:name:footprint:gender:type:reply:
availableVoicesForLanguageCode:
availableFootprintsForVoice:languageCode:
errorWithReason:
_getVoiceResourceForLanguage:
playVoicePreviewForLanguageCode:voiceName:completion:
connection:speechRequestDidStart:
connection:speechRequestDidPause:
connection:speechRequestDidContinue:
connection:speechRequest:didStopAtEnd:phonemesSpoken:error:
connection:speechRequest:willSpeakMark:inRange:
connection:speechRequest:successWithInstrumentMetrics:
connection:speechRequest:didReceiveTimingInfo:
connection:speechRequest:didGenerateAudioChunk:
connection:synthesisRequest:didFinishWithInstrumentMetrics:error:
connection:presynthesizedAudioRequestDidStart:
connection:presynthesizedAudioRequest:didStopAtEnd:error:
connection:presynthesizedAudioRequest:successWithInstrumentMetrics:error:
connection:invalidatedWithError:
prewarmIfNeededWithRequest:
queryPhaticCapability:
startSynthesizingRequest:
startSpeakingPresynthesizedAudioRequest:
cancelRequest:
cancelAudioRequest:
stopSpeakingPresynthesizedAudioSynchronously:error:
pauseSpeakingAtNextBoundary:synchronously:error:
isSpeaking
speechString
minimumRate
maximumRate
estimateDurationOfRequest:
estimateDurationOfRequest:completion:
setMaintainPersistentConnection:
setMaintainInactivePersistentConnection:
useSharedAudioSession:
useSpecificAudioSession:
continueSpeakingWithError:
getLocalVoiceAssets:
setAutoDownloadedVoiceAssets:
triggerCellularDownloadedVoiceAssets:
getAutoDownloadedVoiceAssets:
getVoiceInfoForLanguageCode:footprint:gender:type:reply:
availableLanguageCodes
delegate
setVoice:
language
setLanguage:
setDurationRequests:
_queue
_callbackQueue
_xpcConnection
_synthesizerFlags
_voice
_language
_durationRequests
T@"NSString",C,N,V_language
T@"NSMutableDictionary",&,N,V_durationRequests
T@"<VSSpeechSynthesizerDelegate>",W,N,V_delegate
T@"NSString",&,N,V_voice
initWithClient:accessory:voice:
clientID
setClientID:
_clientID
T@"NSString",&,N,V_clientID
T@"NSString",&,N,V_accessoryID
T@"VSVoiceAsset",&,N,V_voice
migrateDefaults
arrayForKey:
dictionaryForKey:
dictionaryRepresentation
dictionaryRepresentationOfVoices:
UUID
UUIDString
T@"VSPreferencesInterface",R
setSubscribedVoices:forClientID:accessoryID:
removeSubscriptionsForAccessory:
setLastTTSRequestDate:
lastTTSRequestDate
deviceUUID
defaults
setDefaults:
setLock:
_defaults
T@"NSUserDefaults",&,N,V_defaults
T{_opaque_pthread_mutex_t=q[56c]},N,V_lock
T@"NSDate",&,N
T@"NSString",R,N
initWithMachServiceName:options:
maintainWithAudioType:keepAudioSessionActive:
interfaceWithProtocol:
setRemoteObjectInterface:
_ensureKeepAliveMaintenance
setInterruptionHandler:
resume
_serverConnection
remoteObjectProxy
_remoteKeepAlive
audioType
active
keepAudioSessionActive
_audioType
_active
_keepAudioSessionActive
Tq,N,V_audioType
TB,N,V_active
TB,N,V_keepAudioSessionActive
setConnection:
updateWithConnectionIdentifier:
queryPhaticCapabilityWithRequest:reply:
startSpeechRequest:reply:
getVoiceNamesForLanguage:reply:
getFootprintsForVoiceName:languageCode:reply:
getSpeechIsActiveReply:
getSpeechIsActiveForConnectionReply:
getLocalVoicesForLanguage:reply:
getLocalVoiceResourcesReply:
setClasses:forSelector:argumentIndex:ofReply:
_connectionInvalidated
setInvalidationHandler:
xpcConnection
speechRequestDidStart:
speechRequestDidPause:
speechRequestDidContinue:
speechRequest:didStartWithMark:forRange:
speechRequest:didStopWithSuccess:phonemesSpoken:error:
speechRequest:didReportInstrumentMetrics:
speechRequest:didReceiveTimingInfo:
synthesisRequest:didReceiveTimingInfo:
synthesisRequest:didGenerateAudioChunk:
synthesisRequest:didFinishWithInstrumentMetrics:error:
audioRequestDidStart:
audioRequest:didStopAtEnd:error:
audioRequest:didReportInstrumentMetrics:error:
setExportedInterface:
delegateWrapper
setExportedObject:
synchronousRemoteObjectProxyWithErrorHandler:
remoteObjectProxyWithErrorHandler:
requests
audioRequests
concurrentSynthesisRequests
setXpcConnection:
_remoteObjectWithErrorHandler:
_remoteObject
insertObject:atIndex:
_remoteObjectSync
localizedDescription
setDelegateWrapper:
threadSafeQueue
setThreadSafeQueue:
_delegateWrapper
_threadSafeQueue
T@"NSXPCConnection",&,N,V_xpcConnection
T@"VSSpeechConnectionDelegateWrapper",&,N,V_delegateWrapper
T@"NSObject<OS_dispatch_queue>",&,N,V_threadSafeQueue
T@"<VSSpeechConnectionDelegate>",W,N,V_delegate
getLocalRequest:
getLocalAudioRequest:
setCurrentRequest:
setRequests:
setConcurrentSynthesisRequests:
setCurrentAudioRequest:
setAudioRequests:
connection
_currentRequest
_concurrentSynthesisRequests
_currentAudioRequest
_audioRequests
_connection
T@"VSSpeechRequest",&,N,V_currentRequest
T@"NSMutableArray",&,N,V_requests
T@"NSMutableDictionary",&,N,V_concurrentSynthesisRequests
T@"VSPresynthesizedAudioRequest",&,N,V_currentAudioRequest
T@"NSMutableArray",&,N,V_audioRequests
T@"VSSpeechConnection",W,N,V_connection
asbd
setAsbd:
setAudioData:
setPacketCount:
setPacketDescriptions:
mutableAudioData
mutableDescription
setData:
encodeBytes:length:forKey:
decodeBytesForKey:returnedLength:
duration
totalFrames
concatenateWithAudio:
setMutableAudioData:
setMutableDescription:
_packetCount
_mutableAudioData
_mutableDescription
T@"NSMutableData",&,N,V_mutableAudioData
T@"NSMutableData",&,N,V_mutableDescription
T@"NSData",&,N
T{AudioStreamBasicDescription=dIIIIIIII},N,V_asbd
Tq,N,V_packetCount
nameKey
lowercaseString
isBuiltInVoice
_isInstalled
_isBuiltInVoice
_isVoiceReadyToUse
_name
_type
T@"NSString",C,N,V_name
Tq,N,V_type
TB,N,V_isInstalled
TB,N,V_isBuiltInVoice
TB,N,V_isVoiceReadyToUse
anonymousListener
setQueue:
setHandler:
setListener:
endpoint
setEndpoint:
_setQueue:
invokeUpdateWithObject:
initWithBlock:
initWithListenerEndpoint:
configuredEndpointWithUpdateHandler:withConnection:
remoteUpdateHanderForEndpoint:
listener:shouldAcceptNewConnection:
queue
listener
_endpoint
_listener
T@"NSXPCListenerEndpoint",&,N,V_endpoint
T@"NSObject<OS_dispatch_queue>",&,N,V_queue
T@"NSXPCListener",&,N,V_listener
T@?,C,N,V_handler
_block
hasOTAANEModel:
isANEModelCompiled:
compileANEModel:
B16@0:8
v24@0:8@16
@24@0:8@16
v24@0:8@"NSCoder"16
@24@0:8@"NSCoder"16
@16@0:8
v20@0:8B16
v16@0:8
@"NSString"
@"NSNumber"
@24@0:8q16
d16@0:8
q16@0:8
v24@0:8q16
v24@0:8d16
Q16@0:8
@"VSVoiceAsset"
@"MAAsset"
@64@0:8@16q24@32q40q48q56
@32@0:8@16q24
v32@0:8@16@24
@20@0:8B16
q24@0:8@16
B24@0:8@16
@56@0:8@16@24q32q40q48
@56@0:8q16@24@32q40q48
v36@0:8@16B24@?28
v40@0:8@16@24@?32
v32@0:8@16@?24
B32@0:8@16^@24
v24@0:8@?16
@36@0:8@16B24^@28
@64@0:8q16@24@32q40q48q56
@32@0:8@16@24
v48@0:8@16@24@?32@?40
@40@0:8@16@24@?32
@"NSObject<OS_dispatch_queue>"
@"NSCache"
f16@0:8
v20@0:8f16
@"NSUserDefaults"
@40@0:8@16@24q32
Q24@0:8@16
v32@0:8@16Q24
B20@0:8S16
@"NSDictionary"
#16@0:8
@24@0:8:16
@32@0:8:16@24
@40@0:8:16@24@32
B24@0:8#16
B24@0:8:16
Vv16@0:8
^{_NSZone=}16@0:8
B24@0:8@"Protocol"16
@"NSString"16@0:8
@"VSRecognitionAction"24@0:8@"VSRecognitionResult"16
@"VSRecognitionAction"24@0:8@"NSArray"16
@40@0:8@16@24@32
B40@0:8^@16^@24q32
B20@0:8B16
v40@0:8@16@24@32
v24@0:8Q16
^{__CFDictionary=}16@0:8
v36@0:8@16B24@28
B20@0:8i16
@"VSKeepAlive"
@"<VSRecognitionSessionDelegate>"
@"VSRecognitionAction"
@"NSArray"
@"VSSpeechSynthesizer"
{?="delegateWillBegin"b1"delegateBegin"b1"delegateOpenURL"b1"delegateOpenURLAsync"b1"delegateFinishedSpeaking"b1"delegateComplete"b1"debugDumpEnabled"b1"preferredEngine"b2"performHandlerActions"b1"allowSensitiveActions"b1"bluetoothAllowed"b1"resetNextAction"b1"isSpeaking"b1"actionBegan"b1"actionBeginning"b1"actionBeginDeferred"b1"invalid"b1"observeKeywordChange"b1}
@24@0:8^{__CFDictionary=}16
@56@0:8{AudioStreamBasicDescription=dIIIIIIII}16
@?16@0:8
@"AVAudioFormat"
@"AVAudioConverter"
@"AVAudioCompressedBuffer"
^{OpaqueAudioConverter=}56@0:8{AudioStreamBasicDescription=dIIIIIIII}16
@72@0:8@16{AudioStreamBasicDescription=dIIIIIIII}24^@64
@32@0:8@16^@24
^{OpaqueAudioConverter=}
{AudioStreamBasicDescription="mSampleRate"d"mFormatID"I"mFormatFlags"I"mBytesPerPacket"I"mFramesPerPacket"I"mBytesPerFrame"I"mChannelsPerFrame"I"mBitsPerChannel"I"mReserved"I}
i16@0:8
(?="stringValue"@"NSString""attributedStringValue"@"NSAttributedString")
@"VSRecognitionSession"
@48@0:8@16@24@32@40
@28@0:8@16i24
^{__VSRecognitionDisambiguationContext=}16@0:8
^{__VSRecognition=}32@0:8^{?=^?^?^?}16^v24
@"NSMutableDictionary"
@"NSCharacterSet"
B24@0:8d16
B28@0:8B16@20
v24@0:8^{__VSRecognition=}16
v40@0:8^{__VSRecognition=}16^{__CFArray=}24^{__CFError=}32
{?="debugDumpEnabled"b1"preferredEngine"b2"resetEngine"b1"bluetoothAllowed"b1"hasStarted"b1}
@"NSURL"
Q48@0:8r*16Q24Q32Q40
{_NSRange=QQ}16@0:8
v32@0:8{_NSRange=QQ}16
{_NSRange="location"Q"length"Q}
@"NSLock"
@"NSMutableArray"
@"NSTimer"
q32@0:8@16@24
@40@0:8q16@24@32
B56@0:8^@16^q24q32@40@48
q32@0:8@"NSString"16@"NSString"24
@"NSString"40@0:8q16@"NSString"24@"NSString"32
B56@0:8^@16^q24q32@"NSString"40@"NSString"48
@"NSDictionary"16@0:8
B24@0:8@"NSDictionary"16
{?="initializing"b1"confirmed"b1}
@20@0:8i16
@28@0:8@16B24
@24@0:8^{_NSZone=}16
@64@0:8@16{AudioStreamBasicDescription=dIIIIIIII}24
@104@0:8@16{AudioStreamBasicDescription=dIIIIIIII}24{AudioStreamBasicDescription=dIIIIIIII}64
I16@0:8
v20@0:8I16
{AudioStreamBasicDescription=dIIIIIIII}16@0:8
@"NSData"
@"NSUUID"
@"<VSRecognitionResultHandlingThreadDelegate>"
@"NSConditionLock"
{?="running"b1"delegateDidHandleResults"b1"valid"b1}
@"<VSRecognitionResultHandler>"
^{__CFStringTokenizer=}
@32@0:8@16^{?=qq}24
@"NSAttributedString"
@32@0:8@16Q24
{_NSRange=QQ}24@0:8@16
^v24@0:8Q16
^v16@0:8
v24@0:8^v16
@"NSMutableData"
B40@0:8@16q24@?32
B40@0:8@16@24@?32
B48@0:8@16@24q32@?40
v52@0:8@16@24B32@36@44
v56@0:8@16@24q32{_NSRange=QQ}40
v48@0:8@16@24@32@40
v44@0:8@16@24B32@36
v32@0:8@"VSSpeechConnection"16@"VSSpeechRequest"24
v52@0:8@"VSSpeechConnection"16@"VSSpeechRequest"24B32@"NSString"36@"NSError"44
v56@0:8@"VSSpeechConnection"16@"VSSpeechRequest"24q32{_NSRange=QQ}40
v40@0:8@"VSSpeechConnection"16@"VSSpeechRequest"24@"VSInstrumentMetrics"32
v40@0:8@"VSSpeechConnection"16@"VSSpeechRequest"24@"NSArray"32
v40@0:8@"VSSpeechConnection"16@"VSSpeechRequest"24@"VSAudioData"32
v48@0:8@"VSSpeechConnection"16@"VSSpeechRequest"24@"VSInstrumentMetrics"32@"NSError"40
v32@0:8@"VSSpeechConnection"16@"VSPresynthesizedAudioRequest"24
v44@0:8@"VSSpeechConnection"16@"VSPresynthesizedAudioRequest"24B32@"NSError"36
v48@0:8@"VSSpeechConnection"16@"VSPresynthesizedAudioRequest"24@"VSInstrumentMetrics"32@"NSError"40
v32@0:8@"VSSpeechConnection"16@"NSError"24
v40@0:8@16q24@?32
v48@0:8@16@24q32@?40
@36@0:8@16q24B32
@28@0:8q16B24
B36@0:8q16B24^@28
B28@0:8B16^@20
d24@0:8@16
B24@0:8^@16
v56@0:8@16q24q32q40@?48
v64@0:8@16@24q32q40q48@?56
@"VSSpeechConnection"
{?="delegateStart"b1"delegateFinish"b1"delegateFinishWithPhonemesSpoken"b1"delegatePause"b1"delegateContinue"b1"delegateWillSpeak"b1"delegateStartWithRequest"b1"delegateFinishWithRequest"b1"delegateFinishWithPhonemesSpokenWithRequest"b1"delegateSuccessWithInstrumentMetrics"b1"delegatePauseWithRequest"b1"delegateContinueWithRequest"b1"delegateWillSpeakWithRequest"b1"delegateStreamSynthesisAudioData"b1"willUseInput"b1}
@"<VSSpeechSynthesizerDelegate>"
{_opaque_pthread_mutex_t=q[56c]}16@0:8
v80@0:8{_opaque_pthread_mutex_t=q[56c]}16
{_opaque_pthread_mutex_t="__sig"q"__opaque"[56c]}
Vv28@0:8q16B24
@"NSXPCConnection"
Vv24@0:8@16
Vv32@0:8@16@?24
Vv32@0:8@16q24
Vv40@0:8@16@24@?32
Vv24@0:8@?16
Vv40@0:8@16q24@?32
Vv48@0:8@16@24q32@?40
Vv40@0:8@16@24@32
Vv32@0:8@16@24
Vv64@0:8@16@24q32q40q48@?56
Vv20@0:8B16
Vv24@0:8@"NSString"16
Vv32@0:8@"VSSpeechRequest"16@?<v@?@"NSError">24
Vv32@0:8@"VSSpeechRequest"16@?<v@?B>24
Vv32@0:8@"VSSpeechRequest"16@?<v@?d@"NSError">24
Vv32@0:8@"VSSpeechRequest"16@?<v@?>24
Vv24@0:8@"VSSpeechRequest"16
Vv32@0:8@"VSSpeechRequest"16q24
Vv24@0:8@"VSPresynthesizedAudioRequest"16
Vv32@0:8@"NSString"16@?<v@?@"NSArray">24
Vv40@0:8@"NSString"16@"NSString"24@?<v@?@"NSArray">32
Vv24@0:8@?<v@?B>16
Vv40@0:8@"NSString"16q24@?<v@?B>32
Vv48@0:8@"NSString"16@"NSString"24q32@?<v@?B>40
Vv24@0:8@?<v@?@"AFXPCWrapper">16
Vv24@0:8@?<v@?@"NSError">16
Vv32@0:8@"NSString"16@?<v@?@"NSArray"@"NSError">24
Vv24@0:8@?<v@?@"NSArray"@"NSError">16
Vv40@0:8@"NSArray"16@"NSString"24@"NSUUID"32
Vv40@0:8@"NSString"16@"NSUUID"24@?<v@?@"NSArray">32
Vv32@0:8@"NSArray"16@"NSString"24
Vv32@0:8@"NSString"16@?<v@?@"VSVoiceResourceAsset">24
Vv64@0:8@"NSString"16@"NSString"24q32q40q48@?<v@?@"VSVoiceAsset">56
Vv24@0:8@?<v@?>16
Vv24@0:8@"SATTSSpeechSynthesisStreaming"16
Vv24@0:8@?<v@?i>16
Vv48@0:8@16q24{_NSRange=QQ}32
Vv44@0:8@16B24@28@36
Vv36@0:8@16B24@28
Vv48@0:8@"VSSpeechRequest"16q24{_NSRange=QQ}32
Vv44@0:8@"VSSpeechRequest"16B24@"NSString"28@"NSError"36
Vv32@0:8@"VSSpeechRequest"16@"VSInstrumentMetrics"24
Vv32@0:8@"VSSpeechRequest"16@"NSArray"24
Vv32@0:8@"VSSpeechRequest"16@"VSAudioData"24
Vv40@0:8@"VSSpeechRequest"16@"VSInstrumentMetrics"24@"NSError"32
Vv36@0:8@"VSPresynthesizedAudioRequest"16B24@"NSError"28
Vv40@0:8@"VSPresynthesizedAudioRequest"16@"VSInstrumentMetrics"24@"NSError"32
@24@0:8@?16
v32@0:8@16q24
@"<VSSpeechConnectionDelegate>"
@"VSSpeechConnectionDelegateWrapper"
@"VSSpeechRequest"
@"VSPresynthesizedAudioRequest"
v56@0:8{AudioStreamBasicDescription=dIIIIIIII}16
@32@0:8@?16@24
@?24@0:8@16
B32@0:8@16@24
B32@0:8@"NSXPCListener"16@"NSXPCConnection"24
@"NSXPCListenerEndpoint"
@"NSXPCListener"
|?5^
