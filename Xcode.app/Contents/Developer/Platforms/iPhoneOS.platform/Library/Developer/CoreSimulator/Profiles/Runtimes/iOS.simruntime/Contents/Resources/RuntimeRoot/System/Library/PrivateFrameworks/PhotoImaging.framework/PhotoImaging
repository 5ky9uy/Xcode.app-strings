?F]k
?YAH
?ffffff
@<,_
?ffffff
>333333
?UUUUUU
?______
?TTTTTT
?333333
yE>)\
Ww'&l
MbP?{
333333
?333333
?ffffff
?ffffff
?333333
333333
ffffff
333333
333333
@UUUUUU
@333333
?ffffff
z>UUUUUU
?YAH
?YAH
pCh?
zt?{
p?33
UUUUUU
?UUUUUU
4@333333
sU?gDi?
z?-C
Mb@?
UUU?
?~0d>
@(#)PROGRAM:PhotoImaging  PROJECT:Neutrino-402.5.140
0Xr
?g|_\
ADBE
mntrRGB XYZ 
;acspAPPL
none
-ADBE
cprt
2desc
kwtpt
bkpt
rTRC
gTRC
bTRC
rXYZ
gXYZ
bXYZ
text
Copyright 2000 Adobe Systems Incorporated
desc
Adobe WGG linear
XYZ 
XYZ 
curv
XYZ 
XYZ 
iXYZ 
#?6t>=
333333
?333333
?ffffff
Fail: %{public}@
job: %{public}@
Trace:
%{public}@
Trace:
%{public}@
Tap-to-track: client requested stop at %@
Tap-to-track: tracking lost at %@
PIColorNormalizationAnalysis
Setting face bounding boxes based on orientation (%@) and observations: %@
Setting bounding boxes based on orientation (%@) and observations: %@
Tried to set sceneLabel to unsupported value: %ld
class %@ is not the correct type, its superclass should be %@
Unable to serialize finalized composition: %{public}@
Continue: %{public}@
CPV rendering failure, returned status %d
Could not apply PIPortraitVideoProcessor: %@
CINE: allocating new PTRenderPipeline: quality: %d, device: %p, colorSize: %.f x %.f, disparitySize %.f x %.f, debug: %ld
facerect yiq = %.5f, %.5f, %.5f
Buffer sizes are not the same: %{public}@ vs %{public}@
Buffer size is %@, bytes per row is %ld, dark_thr=%f, sat_thr=%f, noise_thr=%f
all done summing, np_gw is %d, np_ge is %d
wp_ge {%f, %f, %f, %f} wp_gw {%f, %f, %f, %f}, Sushi? %@
aperture=%@, shutterSpeed=%@, iso=%@
Brightness is %@, greenChannelPercentage is %f, Sushi: %@
Choosing gray world instead of gray edge
RGB = %f, %f, %f
YIQ = %f, %f, %f from %f, %f, %f
Skipping applyVideoOrientationAsMetadata. Bounces and Autoloops are not supported. Falling back to burned-in orientation.
Skipping applyVideoOrientationAsMetadata. Flipped orientations are not supported. Falling back to burned-in orientation.
Failed to export video: %@
NUVideoRotationExportRequest failed. Error:%@
[PICompositionExporter shouldTryVideoRotationFastPath] failed. Error:%@
failed to render auxiliary image data: %@
Unexpected Live Photo export format pairing. Video codec (%{public}@) and image export format (%{public}@)
Export Composition: exportImage: %{public}@ / exportVideoComplement: %{public}@ / exportVideo: %{public}@ / exportVideoPosterFrame: %{public}@
Export Composition: exporting image
Export Composition: exporting video complement
Export Composition: exporting video
Export Composition: exporting video poster frame
Export Composition: exporting overcapture image
Export Composition: exporting overcapture video
Export Composition: waiting on notify
Export Composition: notify triggered
Export Composition: calling completion with error: %{public}@
Export Composition: callling completion with result: %{public}@
Export Composition: exporting overcapture video complete
Export Composition: exporting overcapture image complete
Export Composition: exporting video poster frame complete
Export Composition: exporting video complete
Export Composition: exporting video complement complete
Export Composition: exporting image complete
Failed to prepare video metadata: %@
Failed to export image to data: %@
Failed to export image to %@: %@
invalid format version: %@
Error unarchiving cameraInfo metadata: %@
PIPortraitVideoMetadataSample: unexpected number of items for identifier %@: %@
PIPortraitVideoMetadataSample: item %@ is not of expected class %@
Requesting forced cleanup of Vision caches
Can PLPhotoEditPFDataConverter interpret identifier %{public}@? %@. Version %{public}@? %@
validation issue: recipe is blank on the autoloop adjustment
validation issue: no flavor specified on the autoloop adjustment
validation issue: Missing inputCorrectionInfo on a redeye adjustment
validation issue: Missing depthInfo on a depth adjustment
validation issue: Missing portraitInfo on a portrait adjustment
validation issue: invalid trim startTime or endTime
HDR asset not editable because this device doesn't support 10-bit HEVC encoding
HDR asset not editable because this device doesn't support HDR
HDR asset not editable because editing not supported on DolbyVision 5
CPV asset checking for editability
CPV asset not editable because it's in a legacy, unsupported format
CPV asset editable due to override
CPV asset not editable because CPV assets are not editable on this device
CPV asset not editable because this asset is not playable on this device
CPV asset is editable
CPV assets aren't editable because there is no metal device with support for portrait rendering
CPV assets aren't editable because the device has no ANE
CPV assets are editable
CPV asset editability: onAppleSilicon: %s, hasMetalDeviceForPortrait: %s, oniOS: %s
Writing long-exposure motion mask to %{public}@
Writing long-exposure image to %{public}@
waitUntilReadyForMoreData: waited for %0.1fms
High-resolution image registration failure : %@
Still image node:
Still image geometry:
Still image:
clap=%@, crop=%@, fullSize=%@, videoScale=%@ => guide rect: %@
Missing inputImage
Missing inputMaskImage
Missing inputStillImage
Missing inputRenderScale
Missing inputVideoScale
Missing inputAlignmentExtent
Malformed inputAlignmentExtent vector
Missing inputAlignmentTransform
Malformed inputAlignmentTransform vector
Writing intermediate long exposure fusion images to : %@
Loading long-exposure fusion tuning parameters from file: %@
Failed to load fusion tuning parameters.
Using fusion tuning parameters: {kNCCBlurHalfSize=%d, kNCCEdge0=%f, kNCCEdge1=%f, kBlendMaskThreshold0=%f, kBlendMaskThreshold1=%f}
Evaluating pipeline as HDR input
Received error while finalizing: %@
Starting horizon auto calculator
Starting perspective auto calculator
Finished perspective auto calculator: %@
Finished horizon auto calculator: %@
Beginning finalization for candidates: %@
No overcapture source found in composition, removing any candidate bits for reframing
Finalizer result contains roll angle degrees: %f pitch angle degrees: %f yaw angle degrees: %f
Finalizer result did not contain a change
Could not write PICropAutoCalculator debug diagnostics. %@
Failed to compute overcapture rect %@
Error creating asset reader: %@
Can't add metadata output for asset
Failed to start reading asset
Starting metadata extraction
Finished metadata extraction
Can't find enabled video track in asset: %@
Asset does not contain Live Photo metadata track
Track does not contain V3 metadata
PISmartToneAutoCalculator submit isHDRComposition: %s
PISmartToneAutoCalculator going to commit and wait
PISmartToneAutoCalculator committed
PISmartToneAutoCalculator completed
PISmartToneAutoCalculator error: %{public}@
PISmartToneAutoCalculator coalesced
PISmartToneAutoCalculator global result: %{public}@
PISmartToneAutoCalculator submitSynchronous isHDRComposition: %s
PISmartToneAutoCalculator smartTone request submitting: %{public}@
PISmartToneAutoCalculator smartTone result: %{public}@
PISmartToneAutoCalculator localLight request submitting: %{public}@
PISmartToneAutoCalculator localLight result: %{public}@
Failed to apply red eye repair. error: %{public}@
PIPortraitAutoCalculator getMinMaxSimulatedAperture returned error:%i (minAperture:%f maxAperture:%f version:%d)
Depth effects not supported: %@
Couldn't deserialize face observations from metadata: %@, assuming empty, error: %@.
Invalid face indexes from metadata: %@, ignored. Error: %@
Couldn't deserialize face observations from metadata: %@
Invalid focus rect: {%g,%g,%g,%g}
Cant get focusRect - Metadata dictionary missing fullSizeWith or fullSizeHeight:
Cant get focusRect - Metadata dictionary missing exif aux dictionary:
Cant get focusRect - Exif aux dictionary missing MWG region dictionary:
Cant get focusRect - MWG region dictionary missing region list:
Cant get focusRect - Region list does not contain a focus rect:
Cant get focusRect - Malformed focus rect dictionary:
s-curve black: %f
s-curve point %d: (%f, %f)
s-curve white: %f
red curve: blackPoint = %f, whitePoint = %f
green curve: blackPoint = %f, whitePoint = %f
blue curve: blackPoint = %f, whitePoint = %f
-[PITapToTrackRenderJob prepare:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-402.5.140/workspaces/neutrino/PhotoImaging/Autocalculators/PITapToTrackRequest.m
Failed to get finalized track from tracking session
Failed to add detection and start tracking
Failed to find rect to track at initial point
Failed to read first frame for video
Failed to make asset reader for video
Failed to find output video asset
No metal device for request
Invalid parameter not satisfying: %s
+[PIPhotoEffectHDR kernel]_block_invoke
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-402.5.140/workspaces/neutrino/PhotoImaging/Adjustments/HDR/PIPhotoEffectHDR.m
s_hdrOffsetPos kernel is nil
kernel vec4 _hdrOffsetPosBW(sampler image) { vec4 im = sample(image, samplerCoord(image)); float offset = (im.r + im.g + im.b) / 3.0f; offset = max(0.0f, offset - 1.0f); return vec4(offset, offset, offset, 0.0f); }
-[PIPhotoEffectHDR outputImage]
inputBackgroundImage
CIAdditionCompositing
inputImage cannot be nil
+[PIPhotoEffectBlackAndWhiteHDR kernelBlackAndWhite]_block_invoke
s_hdrOffsetPosBlackAndWhite kernel is nil
-[PIPhotoEffectBlackAndWhiteHDR outputImage]
+[PIPhotoEffect3DHDR kernel]_block_invoke
kernel vec4 _hdrOffsetPos(sampler image) { vec4 im = sample(image, samplerCoord(image)); float r = max(im.r - 1.0f, 0.0f); float g = max(im.g - 1.0f, 0.0f); float b = max(im.b - 1.0f, 0.0f); return vec4(r, g, b, 0.0f); }
-[PIPhotoEffect3DHDR outputImage]
inputDepthMap
inputThreshold
+[PIPhotoEffect3DBlackAndWhiteHDR kernelBlackAndWhite]_block_invoke
-[PIPhotoEffect3DBlackAndWhiteHDR outputImage]
CISmartColorFilter
CISmartToneFilter
PIHighKey
PITempTintFilter
inputTint
tint
inputTemperature
temperature
normalization != nil
+[PIColorNormalizationFilter colorCubeForNormalization:dimension:targetColorSpace:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-402.5.140/workspaces/neutrino/PhotoImaging/Adjustments/PIColorNormalizationFilter.m
colorSpace != NULL
PIColorNormalization
PhotoImaging
PIColorNormalizationFilter
facts.candidateForVideo == true
facts.candidateForStill == true
state.hasAdjustments == true
state.reframingAllowed == false
state.pano == true
state.video == true AND state.livePhoto == true
state.deviceIsStationary == true
v16@?0@"NURuleSystem"8
state.backFacing == false
state.largeSubject == true
facts.isStitched == true AND facts.hasHorizonLine == true
facts.isStitched == true AND facts.hasBuilding == true
facts.isStitched == true AND facts.validSubjects == true
(state.videoDuration * $VideoDurationBelowMinZoomRatio) < state.videoDurationBelowMinZoom
state.videoQualityScore < $VideoQualityScoreThreshold
(state.videoDuration * $VideoDurationWithSubjectsRatio) > state.videoDurationWithSubjects
state.videoDuration > $VideoDurationUpperBound
state.videoDuration < $VideoDurationLowerBound
state.video == true
state.horizonLineAngleInDegrees > -$HorizonAngleInDegreesMinThreshold AND state.horizonLineAngleInDegrees < $HorizonAngleInDegreesMinThreshold
state.horizonLineAngleInDegrees < -$HorizonAngleInDegreesMaxThreshold OR state.horizonLineAngleInDegrees > $HorizonAngleInDegreesMaxThreshold
hasHorizonLine
state.horizonLinePresent == true
state.largestSubjectArea <= $MinimumSubjectSize
state.largeSubjectFace == true
(state.subjectCount - state.intersectingSubjectCount) > $MaxDisparateSubjects
state.allSubjectsInsidePrimaryBounds == true
validSubjects
state.subjectCount >= $SubjectCountMinThreshold
state.buildingConfidence > $BuildingConfidenceThreshold
hasBuilding
state.buildingCount > 0
state.stitchConfidence < $StitchConfidenceThreshold
isStitched
state.stitched == true OR state.perfectlyStitched == true
addRule exception : %@
v24@?0@"NSString"8@"NSString"16
v24@?0@"NSString"8@?<v@?@"NURuleSystem">16
candidateForHorizon
candidateForPerspective
candidateForReframe
candidateForVideo
candidateForStill
platedFood
genericLandscape
sunriseSunset
boundingBoxes
faceBoundingBoxes
sceneConfidence
sceneLabel
intensity
-[PIDisparitySampleJob render:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-402.5.140/workspaces/neutrino/PhotoImaging/Autocalculators/PIDisparitySampleRequest.m
Incompatible disparity buffer
Failed to read video frame
inputScale
inputStrength
CIEdgePreserveUpsampleFilter
inputLumaSigma
inputSpatialSigma
inputSmallImage
CIColorClamp
inputMaxComponents
inputMinComponents
CIColorMatrix
inputBiasVector
inputAVector
inputBVector
inputGVector
inputRVector
vec3 localContrast(vec3 im, vec3 shc, float amt) { float midAmt = amt; vec3 neg = min(im, 0.0); vec3 pos = max(im, 1.0)-1.0; im = clamp(im, 0.0, 1.0); float y = dot(im, vec3(0.3333)); y = sqrt(y); y = y*(1.0-y); im = sqrt(im); float pivot = sqrt(shc.g); float a = midAmt*y; float b = -pivot*a; vec3 pix = im.r * vec3(0.299*a) + im.g * vec3(0.587*a) + im.b * vec3(0.114*a) + im + vec3(b); im = mix(im, vec3(pivot), -y*midAmt); im = mix(im, pix, 0.8); im = max(im, 0.0); im *= im; im = im + neg + pos; return im; } vec3 localContrastHLG(vec3 im, vec3 shc, float hlg_scale, float amt) { return localContrast(im.rgb/hlg_scale, shc.rgb/hlg_scale, amt).rgb * hlg_scale; } kernel vec4 _localContrastHDR(__sample im, __sample shc, float hlg_scale, float amt) { float max_comp = max(im.r,max(im.g,im.b)); float threshold = 0.75 * hlg_scale; if (max_comp <= 1.0) { im.rgb = localContrast(im.rgb, shc.rgb, amt); } else if (max_comp < threshold) { vec3 retSDR = localContrast(im.rgb, shc.rgb, amt); vec3 retHDR = localContrastHLG(im.rgb, shc.rgb, hlg_scale, amt); float lerp_t = (max_comp - 1.0) / (threshold - 1.0); im.rgb = mix(retSDR, retHDR, lerp_t); } else { im.rgb = localContrastHLG(im.rgb, shc.rgb, hlg_scale, amt); } return im; }
+[PIHighKey kernel]_block_invoke
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-402.5.140/workspaces/neutrino/PhotoImaging/Adjustments/PIHighKey.m
high key kernel is nil
kernel vec4 highKey(__sample im, float str)
vec3 neg = min(im.rgb, 0.0);
vec3 pos = max(im.rgb, 1.0) - 1.0;
im = clamp(im, 0.0, 1.0);
vec4 im2 = 1.0-((im-1.0)*(im-1.0));
im2 = sqrt(im2);
float y = dot(im.rgb, vec3(.333333));
float y2 = sqrt(1.0-(y-1.0)*(y-1.0));
y2 = mix(y2, smoothstep(0.0, 1.0, y2), 0.5);
vec4 im3 = (y>0) ? im*y2/y : vec4(0.0, 0.0, 0.0, 1.0);
im3 = mix(im3, im2, .7*sqrt(y2));
im3 = mix(im, im3, sqrt(y));
im.rgb = mix(im.rgb, im3.rgb, str) + pos + neg;
return im;
-[PIHighKey outputImage]
inputStrength cannot be nil
smartTone
smartColor
smartBlackAndWhite
grain
autoEnhance
whiteBalance
cropStraighten
redEye
apertureRedEye
depthEffect
livePhotoKeyFrame
videoPosterFrame
trim
slomo
portraitEffect
orientation
autoLoop
highResFusion
retouch
vignette
sharpen
noiseReduction
definition
curves
levels
selectiveColor
effect
effect3D
mute
rawNoiseReduction
source
videoReframe
debug
sourceSelect
sourceSpatialOvercapture
portraitVideo
videoStabilize
videoCrossfadeLoop
semanticEnhance
v16@?0@"PISmartToneAdjustmentController"8
PICompositionController(0x%X): %@
enabled
primary
value
com.apple.photo
adjustmentFormatIdentifier
adjustmentFormatVersion
adjustmentData
schemaRevision
appVersion
buildNumber
platform
CFBundleVersion
composition
Invalid parameter not satisfying: %@
PICompositionSerializer.m
+[PICompositionSerializer _sanitizeComposition:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-402.5.140/workspaces/neutrino/PhotoImaging/Controllers/Conversion/PICompositionSerializer.m
Adjustment for %@ is identity
PICompositionSerializer
exception
PICompositionSerializerDomain
data is not an NSData: %@: %@
dictionary
v32@?0@"NSString"8#16^B24
Value for key %@ has type %@; expected type %@
Missing required key: %@
masterWidth
masterHeight
adjustments
formatVersion
metadata
+[PICompositionSerializer serializeComposition:versionInfo:serializerMetadata:error:]
versionInfo
Serialization map does not contain identifierName
formatIdentifier
identifier
auto
current
inputKeys
omitIfDisabled
settings
Serialization map has no entry for %@
Missing identifierName
PICompositionSerializer-geometry
Orientation
nil identifierName
Configuration does not support CTM adjustments.
Configuration does not support Aperature adjustments.
Unsupported adjustment: 
Missing definition in conversion map for the adjustment key: 
dummy
file://dummy.jpg
inputWhiteDstBlue
inputWhiteSrcBlue
inputHilightDstBlue
inputHilightSrcBlue
inputMidDstBlue
inputMidSrcBlue
inputShadowDstBlue
inputShadowSrcBlue
inputBlackDstBlue
inputBlackSrcBlue
inputWhiteDstGreen
inputWhiteSrcGreen
inputHilightDstGreen
inputHilightSrcGreen
inputMidDstGreen
inputMidSrcGreen
inputShadowDstGreen
inputShadowSrcGreen
inputBlackDstGreen
inputBlackSrcGreen
inputWhiteDstRed
inputWhiteSrcRed
inputHilightDstRed
inputHilightSrcRed
inputMidDstRed
inputMidSrcRed
inputShadowDstRed
inputShadowSrcRed
inputBlackDstRed
inputBlackSrcRed
inputWhiteDstRGB
inputWhiteSrcRGB
inputHilightDstRGB
inputHilightSrcRGB
inputMidDstRGB
inputMidSrcRGB
inputShadowDstRGB
inputShadowSrcRGB
inputBlackDstRGB
inputBlackSrcRGB
inputWhiteDst
inputWhiteSrc
inputHilightDst
inputHilightSrc
inputMidDst
inputMidSrc
inputShadowDst
inputShadowSrc
inputBlackDst
inputBlackSrc
-[PILevelsFilterHDR _LUTImage]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-402.5.140/workspaces/neutrino/PhotoImaging/Adjustments/HDR/PILevelsFilterHDR.m
Failed converting data to RGBAh: %ld
Blue
Green
kernel vec4 levelsHDR (sampler src, sampler LUT) { vec4 p,r; vec2 c1,c2; float f; p = sample(src, samplerCoord(src)); vec3 neg = min(p.rgb, 0.0); vec3 pos = max(p.rgb, 1.0)-1.0; p.rgb = clamp(p.rgb, 0.0, 1.0); f = p.r * 1024; c1 = vec2(floor(f)+0.5, 0.5); c2 = vec2(ceil(f)+0.5, 0.5); r = mix(sample(LUT,samplerTransform(LUT,c1)), sample(LUT,samplerTransform(LUT,c2)), fract(f)); p.r = r.r; f = p.g * 1024; c1 = vec2(floor(f)+0.5, 0.5); c2 = vec2(ceil(f)+0.5, 0.5); r = mix(sample(LUT,samplerTransform(LUT,c1)), sample(LUT,samplerTransform(LUT,c2)), fract(f)); p.g = r.g; f = p.b * 1024; c1 = vec2(floor(f)+0.5, 0.5); c2 = vec2(ceil(f)+0.5, 0.5); r = mix(sample(LUT,samplerTransform(LUT,c1)), sample(LUT,samplerTransform(LUT,c2)), fract(f)); p.b = r.b; p.rgb = max(p.rgb, 0.0); p.rgb = p.rgb + pos + neg; return p; }
version
score
-[PIColorNormalizationAutoCalculator submit:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-402.5.140/workspaces/neutrino/PhotoImaging/Autocalculators/PIColorNormalizationAutoCalculator.m
No color normalization available
Feature Unavailable
/Master/Source
data != nil
+[PIColorNormalizationAutoCalculator autoCalculatorWithImageData:orientation:]
+[PIFalseColorHDRDebug kernel]_block_invoke
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-402.5.140/workspaces/neutrino/PhotoImaging/Adjustments/HDR/PIFalseColorHDRDebug.m
s_falseColorHDRDebugKernelSource kernel is nil
kernel vec4 _falseColorHDRDebug(sampler image, float cutoff){   vec4 im = sample(image, samplerCoord(image));   if (im.x < 0.0 && im.y < 0.0 && im.z < 0.0) {        return vec4(0.0 , 0.6 , 0.2 , 1.0);   }   if (im.x > cutoff && im.y > cutoff && im.z > cutoff) {        return vec4(1.0 , 0.0 , 1.0 , 1.0);   }   if (im.x > cutoff || im.y > cutoff || im.z > cutoff) {        return vec4(0.6 , 0.0 , 0.6 , 1.0);   }   return im;}
-[PIFalseColorHDRDebug outputImage]
-[PIAutoLoopExportRequest initWithComposition:destinationURL:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-402.5.140/workspaces/neutrino/PhotoImaging/AutoLoop/PIAutoLoopRequests.m
Initializer not available: -[%@ %@], use designated initializer instead.
-[PIAutoLoopExportRequest initWithRequest:]
renderPipeline
+[PIPortraitVideoProcessor applyWithRenderPipeline:renderState:globalRenderingMetadata:timedRenderingMetadata:inputImage:disparityImage:aperture:focusedDisparity:error:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-402.5.140/workspaces/neutrino/PhotoImaging/Render/PIPortraitVideoFilter.m
renderState
inputImage
disparityImage
error
focusDistance
aperture
imageExtents
timedRenderingMetadataKey
globalRenderingMetadataKey
renderStateKey
pipelineKey
+[PIPortraitVideoProcessor formatForInputAtIndex:]
Invalid index
+[PIPortraitVideoProcessor processWithInputs:arguments:output:error:]
Pipeline objects were released
expected a texture
expected a device
unexpected inputs
Height
Width
stabCropRect
NormStabilizeInstructions
recipe
flavor
-[PIVideoCrossfadeLoopNode _evaluateAudioMix:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-402.5.140/workspaces/neutrino/PhotoImaging/Video Stabilization/PIVideoCrossfadeLoopNode.m
-[PIVideoCrossfadeLoopNode _evaluateVideoComposition:]
secondary
Unsupported video configuration
-[PIVideoCrossfadeLoopNode _evaluateVideo:]
Failed to update audio track when inserting the post-crossfade content with source range %@ from track %@ to time %@ in track %@.
Failed to update video track when inserting the post-crossfade content with source range %@ from track %@ to time %@ in track %@.
Failed to update audio track when inserting the crossfade content with source range %@ from track %@ to time %@ in track %@.
Failed to update video track when inserting the crossfade content with source range %@ from track %@ to time %@ in track %@.
Failed to update audio track when inserting the pre-crossfade content with source range %@ from track %@ to time %@ in track %@.
Failed to update video track when inserting the pre-crossfade content with source range %@ from track %@ to time %@ in track %@.
No input video track found
[[AVMutableComposition alloc] init] failed.
-[PIVideoCrossfadeLoopNode nodeByReplayingAgainstCache:pipelineState:error:]
CIDissolveTransition
Missing secondary video frame
Missing primary video frame
video
input != nil
-[PIVideoCrossfadeLoopNode initWithInput:timeRange:crossfadeDuration:startTime:]
CMTIME_IS_VALID(crossfadeDuration)
CMTIMERANGE_IS_VALID(loopTimeRange)
loopTimeRange
startTime
crossfadeDuration
-[PIVideoCrossfadeLoopNode initWithSettings:inputs:]
scale
time
falloff
radius
PIAdjustmentController(0x%X): %@
-[PIAdjustmentController setIsAuto:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-402.5.140/workspaces/neutrino/PhotoImaging/Controllers/PIAdjustmentController.m
Adjustment does not have an auto setting
-[PIAdjustmentController isAuto]
-[PIAdjustmentController setEnabled:]
Adjustment does not have an enabled setting
-[PIAdjustmentController enabled]
-[PIAdjustmentController inputKeys]
Adjustment empty
-[PIAdjustmentController displayName]
-[PIPortraitVideoRenderNode _evaluateImage:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-402.5.140/workspaces/neutrino/PhotoImaging/Render/PIPortraitVideoRenderNode.m
debugMode
No PTRenderPipeline or PTRenderState found
Could not get the render time
Could not get the cinematic metadata
Pipeline has the wrong disparitySize
Pipeline has the wrong colorSize
Could not get the disparityImage
Could not get the input image
-[PIPortraitVideoRenderNode _allocatePortraitRenderPipelineWithImageInput:disparityInput:pipelineState:error:]
Could not get geometry for inputs
Shouldn't reallocate portraitRenderPipeline
-[PIPortraitVideoRenderNode nodeByReplayingAgainstCache:pipelineState:error:]
PIPortraitVideoRenderNode: expected a portraitVideoMetadata sample with a non-nil metadataGroup
PIPortraitVideoRenderNode: expected a non-nil portraitVideoMetadata sample
mdta/com.apple.quicktime.disparity-float
portraitVideoMetadata
PIPortraitVideoRenderNode: cached RenderVideo node does not have a render pipeline
Expected a cached renderVideo node
PIPortraitVideoRenderNode: missing disparity image input
PIPortraitVideoRenderNode: missing image input
__dominantInputSettingsKey
PIFaceObservationCache
PIFaceObservationCache-newRequest
spatialOvercaptureFused
spatialOvercapture
timeScale
timeValue
expandedBounds
bounds
edgeBleed
confidence
type
(%.4f, %.4f, %.4f, %.4f)
  edgeBleed=%@
maxY 
maxX 
minY 
minX 
  expandedBounds=%@
  bounds=%@
<%@ %p %@ %@ id=%lu conf=%.2f>
Human Face
Human Body
Cat Body
Cat Head
Dog Body
Dog Head
Salient Object
Unknown
cinematographyState
apertureKeyframes
disparityKeyframes
homography
<%@:%p time:%@>
-[PILongExposureFusionAutoCalculator submit:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-402.5.140/workspaces/neutrino/PhotoImaging/Autocalculators/PILongExposureFusionAutoCalculator.m
AutoLoop not supported
pre-AutoLoop
PILongExposureFusionAutoCalculator-videoProperties
kind
-[PIAutoLoopAutoCalculator submit:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-402.5.140/workspaces/neutrino/PhotoImaging/Autocalculators/PIAutoLoopAutoCalculator.m
B32@?0@"AVMetadataItem"8Q16^B24
-[PITempTintFilter outputImage]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-402.5.140/workspaces/neutrino/PhotoImaging/Adjustments/PITempTintFilter.m
-[PIAutoLoopAnalysisJob render:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-402.5.140/workspaces/neutrino/PhotoImaging/AutoLoop/PIAutoLoopJob+Analysis.m
AutoLoop is not supported
-[PIAutoLoopAnalysisJob prepare:]
unable to find video source node
-[PIVideoStabilizeRenderJob prepare:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-402.5.140/workspaces/neutrino/PhotoImaging/Autocalculators/PIVideoStabilizeRequest.m
void CreateKeyframesFromHomographyDictionary(NSDictionary *__strong, NUPixelSize, NSMutableArray<PIReframeKeyframe *> *__strong, NUPixelRect *)
unexpected homography
PIVideoStabilizeRequest.m
rawTime
frameInstructions
Failure in ICCalcCinematicL1Corrections
Stabilize request was cancelled
No available analysis types were allowed
Failure in ICAnalyzeInputMotion
Failure in ICSynthesizeAnalysis
neutralGray
faceBalance
tempTint
warmTint
PIFaceBalanceAutoCalculator-imageProperties
+[PIFaceBalanceAutoCalculator faceBalanceFromFaceImage:forFaceRect:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-402.5.140/workspaces/neutrino/PhotoImaging/Autocalculators/PIWhiteBalanceAutoCalculators.m
unsupported format - incorrect white balance
+[PIFaceBalanceAutoCalculator faceBalanceResultFromFaceObservations:request:error:]
PIRAWFaceBalanceAutoCalculator.responseQueue
WarmFace
WarmTint
WarmTemp
Strength
Warmth
OrigQ
OrigI
warmTemp
Failure in rendering image
PIWhiteBalanceAutoCalculator-face-balance
+[PIFaceBalanceAutoCalculator calculateRAWWithRequest:completion:]
PIFaceBalanceAutoCalculator-RAW-face-request
+[PIFaceBalanceAutoCalculator calculateWithRequest:completion:]
PIFaceBalanceAutoCalculator-calculate
CIFaceBalance
return Filter('CIEdges', { 'inputImage' : input, 'inputIntensity' : 1.0 });
PIWhiteColorCalculator-grayEdges
PIWhiteColorCalculator-grayWorld
PIWhiteColorCalculator-computeGreenPercentage
CIAreaAverage
v16@?0@"<NUBuffer>"8
color
PIWhiteBalanceAutoCalculator
-[PIWhiteBalanceAutoCalculator submit:]
PIFaceBalanceAutoCalculator.responseQueue
colorType
grayColor
faceStrength
faceWarmth
faceQ
faceI
PIWhiteBalanceAutoCalculator-imageProperties
{?=[4d]}
{?={?=[4d]}{?=[4d]}d}
vec4 meaningBlur(vec4 im, vec4 b)
vec4 result = im;
float thresh = 0.1;
float g1 = max(max(im.r, im.g), im.b);
float g2 = dot(b.rgb, vec3(1.0/3.0));
float diff = max(g2-g1, -1.0);
diff = smoothstep(0.1-thresh, 0.1+thresh, diff);
result.rgb = mix(im.rgb, b.rgb, diff+0.5);
return result;
vec4 clarityNew(vec4 s, vec4 b, float intensity)
float sl = (s.r + s.g + s.b);
float bl = (b.r + b.g + b.b);
float dl = sl + (sl - bl) * intensity;
float mult = dl / max(sl, 0.0001);
mult = 1.571 * (mult - 1.0);
mult = mult / (1.0 + abs(mult));
mult += 1.0;
mult = clamp(mult, 1.0 - 0.5 * abs(intensity), 1.0 + 1.0 * abs(intensity));
s.rgb = s.rgb * mult;
return s;
kernel vec4 definition(sampler image, sampler blur, float intensity)
vec4 imgSample = sample(image, samplerCoord(image));
vec4 blurSample = sample(blur, samplerCoord(blur));
vec4 meaning = meaningBlur(imgSample, blurSample);
vec4 clarity = clarityNew(imgSample, meaning, intensity);
return clarity;
inputSeed
inputHue
inputGrain
inputTone
inputNeutralGamma
tone
neutral
strength
<%@:%p - priority: %@, color space: %@, scale policy: %@>
<%@:%p - priority: %@, color space: %@, scale policy: %@, video codec type: %@, bypass settings: %@, orientation as metadata: %@, hardware encoder: %@>
-[PICompositionExporterImageOptions imageExportFormatForURL:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-402.5.140/workspaces/neutrino/PhotoImaging/Util/PICompositionExporter.m
Export URL UTI (%@) does not match expected export format (%@)
PICompositionExporter-noOrientation-videoProperties
PICompositionExporter-video
height
width
yOrigin
xOrigin
roll
pitch
PICompositionExporter-shouldTryVideoRotationFastPath-geometry
reference
-[PICompositionExporter addVideoProperties:composition:options:error:]
Unknown autoloop flavor
PICompositionExporter.imageProperties.transaction
PICompositionExporter-%@
PICompositionExporter-auxPropertiesRequest
PICompositionExporter-image
unable to prepare image properties
_companion
Unexpected image export format when attempting to export Live Photo
Unexpected video codec when attempting to export Live Photo
Expecting HEIF/HEVC or JPEG/H264 when exporting Live Photo still and video complement
PICompositionExporter-videoProperties
-[PICompositionExporter init]
metadataConverter must be set
Regions
convertFromLabToRGB
convertFromRGBToLab
bilateralFinalize
bilateralAdd_9
bilateralAdd_8
bilateralAdd_7
bilateralAdd_6
bilateralAdd_5
bilateralAdd_4
bilateralAdd_3
bilateralAdd_2
bilateralAdd_1
kernel vec4 convertFromRGBToLab(sampler src)
vec3 f;
vec4 pix, color;
pix = unpremultiply(sample(src, samplerCoord(src)));
color.xyz = pix.r * vec3(0.449695,  0.316251, 0.18452  )
+ pix.g * vec3(0.244634,  0.672034, 0.0833318)
+ pix.b * vec3(0.0251829, 0.141184, 0.922602 );
color.xyz *= vec3(1.052111, 1.0, 0.918417);
f = compare(color.xyz - 0.00885645, 7.787037 * color.xyz + 0.137931, pow(color.xyz, vec3(0.333333)));
color.r = 116.0 * f.y - 16.0;
color.g = 500.0 * (f.x - f.y);
color.b = 200.0 * (f.y - f.z);
color.rgb *= 0.005;
color.a = 1.0;
return color;
kernel vec4 convertFromLabToRGB(sampler src, sampler original)
vec3 f, cie;
vec4 color, pix, opix;
pix = sample(src, samplerCoord(src));
opix = sample(original, samplerCoord(original));
pix.rgb *= 200.0;
f.y = (pix.r + 16.0) / 116.0;
f.x = f.y + pix.g * 0.002;
f.z = f.y - pix.b * 0.005;
color.xyz = f * f * f;
cie = compare(color.xyz - 0.00885645, (f.xyz - 0.137931) / 7.787037, color.xyz);
cie *= vec3(0.95047, 1.0, 1.08883);
color.rgb = cie.x * vec3(2.95176,   -1.28951, -0.47388  )
+ cie.y * vec3(-1.0851,    1.99084,  0.0372023)
+ cie.z * vec3(0.0854804, -0.269456, 1.09113  );
color.a = opix.a;
return premultiply(color);
float luminanceWeight(vec4 pix, vec4 center, float slope)
return max(1.0 + (slope * length(pix.rgb - center.rgb)), 0.0);
vec4 bilateralRow_1(sampler src, float slope, vec2 offset1, float weight1)
float diff1;
vec2 coord;
vec4 pix1, center;
coord = destCoord();
center = sample(src, samplerCoord(src));
pix1 = sample(src, samplerTransform(src, coord + offset1));
diff1 = luminanceWeight(pix1, center, slope) * weight1;
return vec4(diff1 * pix1.rgb, diff1);
kernel vec4 bilateralAdd_1(sampler src, sampler sums, float slope, vec2 offset1, float weight1, float sumWeight)
vec4 sum;
sum = sample(sums, samplerCoord(sums))*sumWeight;
return sum + bilateralRow_1(src, slope, offset1, weight1);
vec4 bilateralRow_2(sampler src, float slope, vec2 offset1, vec2 offset2, vec2 weight1)
float diff1, diff2;
vec2 coord;
vec4 pix1, pix2, center;
coord = destCoord();
center = sample(src, samplerCoord(src));
pix1 = sample(src, samplerTransform(src, coord + offset1));
pix2 = sample(src, samplerTransform(src, coord + offset2));
diff1 = luminanceWeight(pix1, center, slope) * weight1.x;
diff2 = luminanceWeight(pix2, center, slope) * weight1.y;
return vec4(diff1 * pix1.rgb + diff2 * pix2.rgb, diff1 + diff2);
kernel vec4 bilateralAdd_2(sampler src, sampler sums, float slope, vec2 offset1, vec2 offset2, vec2 weight1, float sumWeight)
vec4 sum;
sum = sample(sums, samplerCoord(sums))*sumWeight;
return sum + bilateralRow_2(src, slope, offset1, offset2, weight1);
vec4 bilateralRow_3(sampler src, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec3 weight1)
float diff1, diff2, diff3;
vec2 coord;
vec4 pix1, pix2, pix3, center;
coord = destCoord();
center = sample(src, samplerCoord(src));
pix1 = sample(src, samplerTransform(src, coord + offset1));
pix2 = sample(src, samplerTransform(src, coord + offset2));
pix3 = sample(src, samplerTransform(src, coord + offset3));
diff1 = luminanceWeight(pix1, center, slope) * weight1.x;
diff2 = luminanceWeight(pix2, center, slope) * weight1.y;
diff3 = luminanceWeight(pix3, center, slope) * weight1.z;
return vec4(diff1 * pix1.rgb + diff2 * pix2.rgb + diff3 * pix3.rgb, diff1 + diff2 + diff3);
kernel vec4 bilateralAdd_3(sampler src, sampler sums, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec3 weight1, float sumWeight)
vec4 sum;
sum = sample(sums, samplerCoord(sums))*sumWeight;
return sum + bilateralRow_3(src, slope, offset1, offset2, offset3, weight1);
vec4 bilateralRow_4(sampler src, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec2 offset4, vec4 weight1)
float diff1, diff2, diff3, diff4;
vec2 coord;
vec4 pix1, pix2, pix3, pix4, center;
coord = destCoord();
center = sample(src, samplerCoord(src));
pix1 = sample(src, samplerTransform(src, coord + offset1));
pix2 = sample(src, samplerTransform(src, coord + offset2));
pix3 = sample(src, samplerTransform(src, coord + offset3));
pix4 = sample(src, samplerTransform(src, coord + offset4));
diff1 = luminanceWeight(pix1, center, slope) * weight1.x;
diff2 = luminanceWeight(pix2, center, slope) * weight1.y;
diff3 = luminanceWeight(pix3, center, slope) * weight1.z;
diff4 = luminanceWeight(pix4, center, slope) * weight1.w;
return vec4(diff1 * pix1.rgb + diff2 * pix2.rgb + diff3 * pix3.rgb + diff4 * pix4.rgb,
diff1 + diff2 + diff3 + diff4);
kernel vec4 bilateralAdd_4(sampler src, sampler sums, float slope, vec2 offset1, vec2 offset2, vec2 offset3,
vec2 offset4, vec4 weight1, float sumWeight)
vec4 sum;
sum = sample(sums, samplerCoord(sums))*sumWeight;
return sum + bilateralRow_4(src, slope, offset1, offset2, offset3, offset4, weight1);
vec4 bilateralRow_5(sampler src, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec2 offset4, vec2 offset5,
vec4 weight1, float weight2)
float diff1, diff2, diff3, diff4, diff5;
vec2 coord;
vec4 pix1, pix2, pix3, pix4, pix5, center;
coord = destCoord();
center = sample(src, samplerCoord(src));
pix1 = sample(src, samplerTransform(src, coord + offset1));
pix2 = sample(src, samplerTransform(src, coord + offset2));
pix3 = sample(src, samplerTransform(src, coord + offset3));
pix4 = sample(src, samplerTransform(src, coord + offset4));
pix5 = sample(src, samplerTransform(src, coord + offset5));
diff1 = luminanceWeight(pix1, center, slope) * weight1.x;
diff2 = luminanceWeight(pix2, center, slope) * weight1.y;
diff3 = luminanceWeight(pix3, center, slope) * weight1.z;
diff4 = luminanceWeight(pix4, center, slope) * weight1.w;
diff5 = luminanceWeight(pix5, center, slope) * weight2;
return vec4(diff1 * pix1.rgb + diff2 * pix2.rgb + diff3 * pix3.rgb + diff4 * pix4.rgb + diff5 * pix5.rgb,
diff1 + diff2 + diff3 + diff4 + diff5);
kernel vec4 bilateralAdd_5(sampler src, sampler sums, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec2 offset4,
vec2 offset5, vec4 weight1, float weight2, float sumWeight)
vec4 sum;
sum = sample(sums, samplerCoord(sums))*sumWeight;
return sum + bilateralRow_5(src, slope, offset1, offset2, offset3, offset4, offset5, weight1, weight2);
vec4 bilateralRow_6(sampler src, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec2 offset4, vec2 offset5,
vec2 offset6, vec4 weight1, vec2 weight2)
float diff1, diff2, diff3, diff4, diff5, diff6;
vec2 coord;
vec4 pix1, pix2, pix3, pix4, pix5, pix6, center;
coord = destCoord();
center = sample(src, samplerCoord(src));
pix1 = sample(src, samplerTransform(src, coord + offset1));
pix2 = sample(src, samplerTransform(src, coord + offset2));
pix3 = sample(src, samplerTransform(src, coord + offset3));
pix4 = sample(src, samplerTransform(src, coord + offset4));
pix5 = sample(src, samplerTransform(src, coord + offset5));
pix6 = sample(src, samplerTransform(src, coord + offset6));
diff1 = luminanceWeight(pix1, center, slope) * weight1.x;
diff2 = luminanceWeight(pix2, center, slope) * weight1.y;
diff3 = luminanceWeight(pix3, center, slope) * weight1.z;
diff4 = luminanceWeight(pix4, center, slope) * weight1.w;
diff5 = luminanceWeight(pix5, center, slope) * weight2.x;
diff6 = luminanceWeight(pix6, center, slope) * weight2.y;
return vec4(diff1 * pix1.rgb + diff2 * pix2.rgb + diff3 * pix3.rgb + diff4 * pix4.rgb + diff5 * pix5.rgb + diff6 * pix6.rgb,
diff1 + diff2 + diff3 + diff4 + diff5 + diff6);
kernel vec4 bilateralAdd_6(sampler src, sampler sums, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec2 offset4,
vec2 offset5, vec2 offset6, vec4 weight1, vec2 weight2, float sumWeight)
vec4 sum;
sum = sample(sums, samplerCoord(sums))*sumWeight;
return sum + bilateralRow_6(src, slope, offset1, offset2, offset3, offset4, offset5, offset6, weight1, weight2);
vec4 bilateralRow_7(sampler src, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec2 offset4, vec2 offset5,
vec2 offset6, vec2 offset7, vec4 weight1, vec3 weight2)
float diff1, diff2, diff3, diff4, diff5, diff6, diff7;
vec2 coord;
vec4 pix1, pix2, pix3, pix4, pix5, pix6, pix7, center;
coord = destCoord();
center = sample(src, samplerCoord(src));
pix1 = sample(src, samplerTransform(src, coord + offset1));
pix2 = sample(src, samplerTransform(src, coord + offset2));
pix3 = sample(src, samplerTransform(src, coord + offset3));
pix4 = sample(src, samplerTransform(src, coord + offset4));
pix5 = sample(src, samplerTransform(src, coord + offset5));
pix6 = sample(src, samplerTransform(src, coord + offset6));
pix7 = sample(src, samplerTransform(src, coord + offset7));
diff1 = luminanceWeight(pix1, center, slope) * weight1.x;
diff2 = luminanceWeight(pix2, center, slope) * weight1.y;
diff3 = luminanceWeight(pix3, center, slope) * weight1.z;
diff4 = luminanceWeight(pix4, center, slope) * weight1.w;
diff5 = luminanceWeight(pix5, center, slope) * weight2.x;
diff6 = luminanceWeight(pix6, center, slope) * weight2.y;
diff7 = luminanceWeight(pix7, center, slope) * weight2.z;
return vec4(diff1 * pix1.rgb + diff2 * pix2.rgb + diff3 * pix3.rgb + diff4 * pix4.rgb + diff5 * pix5.rgb + diff6 * pix6.rgb + diff7 * pix7.rgb,
diff1 + diff2 + diff3 + diff4 + diff5 + diff6 + diff7);
kernel vec4 bilateralAdd_7(sampler src, sampler sums, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec2 offset4,
vec2 offset5, vec2 offset6, vec2 offset7, vec4 weight1, vec3 weight2, float sumWeight)
vec4 sum;
sum = sample(sums, samplerCoord(sums))*sumWeight;
return sum + bilateralRow_7(src, slope, offset1, offset2, offset3, offset4, offset5, offset6, offset7, weight1, weight2);
vec4 bilateralRow_8(sampler src, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec2 offset4, vec2 offset5, vec2 offset6,
vec2 offset7, vec2 offset8, vec4 weight1, vec4 weight2)
float diff1, diff2, diff3, diff4, diff5, diff6, diff7, diff8;
vec2 coord;
vec4 pix1, pix2, pix3, pix4, pix5, pix6, pix7, pix8, center;
coord = destCoord();
center = sample(src, samplerCoord(src));
pix1 = sample(src, samplerTransform(src, coord + offset1));
pix2 = sample(src, samplerTransform(src, coord + offset2));
pix3 = sample(src, samplerTransform(src, coord + offset3));
pix4 = sample(src, samplerTransform(src, coord + offset4));
pix5 = sample(src, samplerTransform(src, coord + offset5));
pix6 = sample(src, samplerTransform(src, coord + offset6));
pix7 = sample(src, samplerTransform(src, coord + offset7));
pix8 = sample(src, samplerTransform(src, coord + offset8));
diff1 = luminanceWeight(pix1, center, slope) * weight1.x;
diff2 = luminanceWeight(pix2, center, slope) * weight1.y;
diff3 = luminanceWeight(pix3, center, slope) * weight1.z;
diff4 = luminanceWeight(pix4, center, slope) * weight1.w;
diff5 = luminanceWeight(pix5, center, slope) * weight2.x;
diff6 = luminanceWeight(pix6, center, slope) * weight2.y;
diff7 = luminanceWeight(pix7, center, slope) * weight2.z;
diff8 = luminanceWeight(pix8, center, slope) * weight2.w;
return vec4(diff1 * pix1.rgb + diff2 * pix2.rgb + diff3 * pix3.rgb + diff4 * pix4.rgb + diff5 * pix5.rgb + diff6 * pix6.rgb + diff7 * pix7.rgb + diff8 * pix8.rgb,
diff1 + diff2 + diff3 + diff4 + diff5 + diff6 + diff7 + diff8);
kernel vec4 bilateralAdd_8(sampler src, sampler sums, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec2 offset4, vec2 offset5,
vec2 offset6, vec2 offset7, vec2 offset8, vec4 weight1, vec4 weight2, float sumWeight)
vec4 sum;
sum = sample(sums, samplerCoord(sums))*sumWeight;
return sum + bilateralRow_8(src, slope, offset1, offset2, offset3, offset4, offset5, offset6, offset7, offset8, weight1, weight2);
vec4 bilateralRow_9(sampler src, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec2 offset4, vec2 offset5, vec2 offset6, vec2 offset7,
vec2 offset8, vec2 offset9, vec4 weight1, vec4 weight2, float weight3)
float diff1, diff2, diff3, diff4, diff5, diff6, diff7, diff8, diff9;
vec2 coord;
vec4 pix1, pix2, pix3, pix4, pix5, pix6, pix7, pix8, pix9, center;
coord = destCoord();
center = sample(src, samplerCoord(src));
pix1 = sample(src, samplerTransform(src, coord + offset1));
pix2 = sample(src, samplerTransform(src, coord + offset2));
pix3 = sample(src, samplerTransform(src, coord + offset3));
pix4 = sample(src, samplerTransform(src, coord + offset4));
pix5 = sample(src, samplerTransform(src, coord + offset5));
pix6 = sample(src, samplerTransform(src, coord + offset6));
pix7 = sample(src, samplerTransform(src, coord + offset7));
pix8 = sample(src, samplerTransform(src, coord + offset8));
pix9 = sample(src, samplerTransform(src, coord + offset9));
diff1 = luminanceWeight(pix1, center, slope) * weight1.x;
diff2 = luminanceWeight(pix2, center, slope) * weight1.y;
diff3 = luminanceWeight(pix3, center, slope) * weight1.z;
diff4 = luminanceWeight(pix4, center, slope) * weight1.w;
diff5 = luminanceWeight(pix5, center, slope) * weight2.x;
diff6 = luminanceWeight(pix6, center, slope) * weight2.y;
diff7 = luminanceWeight(pix7, center, slope) * weight2.z;
diff8 = luminanceWeight(pix8, center, slope) * weight2.w;
diff9 = luminanceWeight(pix9, center, slope) * weight3;
return vec4(diff1 * pix1.rgb + diff2 * pix2.rgb + diff3 * pix3.rgb + diff4 * pix4.rgb + diff5 * pix5.rgb + diff6 * pix6.rgb + diff7 * pix7.rgb + diff8 * pix8.rgb + diff9 * pix9.rgb,
diff1 + diff2 + diff3 + diff4 + diff5 + diff6 + diff7 + diff8 + diff9);
kernel vec4 bilateralAdd_9(sampler src, sampler sums, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec2 offset4, vec2 offset5,
vec2 offset6, vec2 offset7, vec2 offset8, vec2 offset9, vec4 weight1, vec4 weight2, float weight3, float sumWeight)
vec4 sum;
sum = sample(sums, samplerCoord(sums))*sumWeight;
return sum + bilateralRow_9(src, slope, offset1, offset2, offset3, offset4, offset5, offset6, offset7, offset8, offset9,
weight1, weight2, weight3);
kernel vec4 bilateralFinalize(sampler sums)
vec4 sum;
sum = sample(sums, samplerCoord(sums));
return vec4(sum.rgb / max(sum.a, 0.001), 1.0);
points.count > 0
-[GUBilateralConvolution boundsForPointArray:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-402.5.140/workspaces/neutrino/PhotoImaging/Adjustments/PIBilateralFilter.m
bilateralLoop11
bilateralLoop5
bilateralLoop2
kernel vec4 bilateralLoop2(sampler src, float slope, vec2 weights12)
vec2 coord;
vec4 center = sample(src, samplerCoord(src));
vec4 sum;
vec4 pix0, pix1, pix2;
vec4 pix3,       pix5;
vec4 pix6, pix7, pix8;
vec2 dx = samplerTransform(src, vec2( 1.0, 0.0)) - samplerTransform(src, vec2(0.0, 0.0));
vec2 dy = samplerTransform(src, vec2(-2.0, 1.0)) - samplerTransform(src, vec2(0.0, 0.0));
coord = samplerTransform(src, destCoord() + vec2(-1.0, -1.0));
pix0 = sample(src, coord);
coord = coord + dx;
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dy;
pix3 = sample(src, coord);
coord = coord + dx;
coord = coord + dx;
pix5 = sample(src, coord);
coord = coord + dy;
pix6 = sample(src, coord);
coord = coord + dx;
pix7 = sample(src, coord);
coord = coord + dx;
pix8 = sample(src, coord);
sum = (1.0 - min(1.0, (slope * length(pix0 - center)))) * (pix0 * weights12.y);
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights12.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights12.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights12.x) + sum;
sum =                                        center                            + sum;
sum = (1.0 - min(1.0, (slope * length(pix5 - center)))) * (pix5 * weights12.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix6 - center)))) * (pix6 * weights12.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix7 - center)))) * (pix7 * weights12.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix8 - center)))) * (pix8 * weights12.y) + sum;
return vec4(sum.rgb / sum.a, 1.0);
kernel vec4 bilateralLoop5(sampler src, float slope, vec4 weights1245)
vec2  coord;
vec4  center = sample(src, samplerCoord(src));
vec4  sum;
vec4  pix0, pix1, pix2, pix3, pix4;
vec2 dx = samplerTransform(src, vec2( 1.0, 0.0)) - samplerTransform(src, vec2(0.0, 0.0));
vec2 dy = samplerTransform(src, vec2(-4.0, 1.0)) - samplerTransform(src, vec2(0.0, 0.0));
coord = samplerTransform(src, destCoord() + vec2(-1.0, -2.0));
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dx;
pix3 = sample(src, coord);
coord = coord + dx + dy;
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights1245.w);
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights1245.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights1245.w) + sum;
pix0 = sample(src, coord);
coord = coord + dx;
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dx;
pix3 = sample(src, coord);
coord = coord + dx;
pix4 = sample(src, coord);
coord = coord + dy;
sum = (1.0 - min(1.0, (slope * length(pix0 - center)))) * (pix0 * weights1245.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights1245.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights1245.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights1245.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix4 - center)))) * (pix4 * weights1245.w) + sum;
pix0 = sample(src, coord);
coord = coord + dx;
pix1 = sample(src, coord);
coord = coord + dx + dx;
pix3 = sample(src, coord);
coord = coord + dx;
pix4 = sample(src, coord);
coord = coord + dy;
sum = (1.0 - min(1.0, (slope * length(pix0 - center)))) * (pix0 * weights1245.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights1245.x) + sum;
sum =                                        center                              + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights1245.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix4 - center)))) * (pix4 * weights1245.z) + sum;
pix0 = sample(src, coord);
coord = coord + dx;
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dx;
pix3 = sample(src, coord);
coord = coord + dx;
pix4 = sample(src, coord);
coord = coord + dx + dy;
sum = (1.0 - min(1.0, (slope * length(pix0 - center)))) * (pix0 * weights1245.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights1245.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights1245.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights1245.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix4 - center)))) * (pix4 * weights1245.w) + sum;
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dx;
pix3 = sample(src, coord);
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights1245.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights1245.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights1245.w) + sum;
return vec4(sum.rgb / sum.a, 1.0);
kernel vec4 bilateralLoop11(sampler src, float slope, vec4 weights1245, vec4 weights89AD)
vec2  coord;
vec4  center = sample(src, samplerCoord(src));
vec4  sum;
vec4  pix0, pix1, pix2, pix3, pix4, pix5, pix6;
vec2 dx = samplerTransform(src, vec2( 1.0, 0.0)) - samplerTransform(src, vec2(0.0, 0.0));
vec2 dy = samplerTransform(src, vec2(-6.0, 1.0)) - samplerTransform(src, vec2(0.0, 0.0));
coord = samplerTransform(src, destCoord() + vec2(-2.0, -3.0));
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dx;
pix3 = sample(src, coord);
coord = coord + dx;
pix4 = sample(src, coord);
coord = coord + dx;
pix5 = sample(src, coord);
coord = coord + dx + dy;
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights89AD.w);
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights89AD.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights89AD.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix4 - center)))) * (pix4 * weights89AD.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix5 - center)))) * (pix5 * weights89AD.w) + sum;
pix0 = sample(src, coord);
coord = coord + dx;
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dx;
pix3 = sample(src, coord);
coord = coord + dx;
pix4 = sample(src, coord);
coord = coord + dx;
pix5 = sample(src, coord);
coord = coord + dx;
pix6 = sample(src, coord);
coord = coord + dy;
sum = (1.0 - min(1.0, (slope * length(pix0 - center)))) * (pix0 * weights89AD.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights89AD.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights1245.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights1245.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix4 - center)))) * (pix4 * weights1245.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix5 - center)))) * (pix5 * weights89AD.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix6 - center)))) * (pix6 * weights89AD.w) + sum;
pix0 = sample(src, coord);
coord = coord + dx;
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dx;
pix3 = sample(src, coord);
coord = coord + dx;
pix4 = sample(src, coord);
coord = coord + dx;
pix5 = sample(src, coord);
coord = coord + dx;
pix6 = sample(src, coord);
coord = coord + dy;
sum = (1.0 - min(1.0, (slope * length(pix0 - center)))) * (pix0 * weights89AD.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights1245.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights1245.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights1245.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix4 - center)))) * (pix4 * weights1245.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix5 - center)))) * (pix5 * weights1245.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix6 - center)))) * (pix6 * weights89AD.z) + sum;
pix0 = sample(src, coord);
coord = coord + dx;
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dx + dx;
pix4 = sample(src, coord);
coord = coord + dx;
pix5 = sample(src, coord);
coord = coord + dx;
pix6 = sample(src, coord);
coord = coord + dy;
sum = (1.0 - min(1.0, (slope * length(pix0 - center)))) * (pix0 * weights89AD.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights1245.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights1245.x) + sum;
sum =                                        center                              + sum;
sum = (1.0 - min(1.0, (slope * length(pix4 - center)))) * (pix4 * weights1245.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix5 - center)))) * (pix5 * weights1245.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix6 - center)))) * (pix6 * weights89AD.y) + sum;
pix0 = sample(src, coord);
coord = coord + dx;
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dx;
pix3 = sample(src, coord);
coord = coord + dx;
pix4 = sample(src, coord);
coord = coord + dx;
pix5 = sample(src, coord);
coord = coord + dx;
pix6 = sample(src, coord);
coord = coord + dy;
sum = (1.0 - min(1.0, (slope * length(pix0 - center)))) * (pix0 * weights89AD.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights1245.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights1245.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights1245.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix4 - center)))) * (pix4 * weights1245.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix5 - center)))) * (pix5 * weights1245.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix6 - center)))) * (pix6 * weights89AD.z) + sum;
pix0 = sample(src, coord);
coord = coord + dx;
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dx;
pix3 = sample(src, coord);
coord = coord + dx;
pix4 = sample(src, coord);
coord = coord + dx;
pix5 = sample(src, coord);
coord = coord + dx;
pix6 = sample(src, coord);
coord = coord + dx + dy;
sum = (1.0 - min(1.0, (slope * length(pix0 - center)))) * (pix0 * weights89AD.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights89AD.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights1245.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights1245.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix4 - center)))) * (pix4 * weights1245.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix5 - center)))) * (pix5 * weights89AD.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix6 - center)))) * (pix6 * weights89AD.w) + sum;
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dx;
pix3 = sample(src, coord);
coord = coord + dx;
pix4 = sample(src, coord);
coord = coord + dx;
pix5 = sample(src, coord);
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights89AD.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights89AD.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights89AD.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix4 - center)))) * (pix4 * weights89AD.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix5 - center)))) * (pix5 * weights89AD.w) + sum;
return vec4(sum.rgb / sum.a, 1.0);
inputEdgeDetail
-[PIBilateralFilter outputImage]
unable to allocate convolution table in bilateral filter
inputWeights
inputPoints
ridiculously large radius for bilateral filter
com.apple.livephoto
com.apple.video
com.apple.video.slomo
%lu.%lu%@%@
.%lu
inputCast
inputVibrancy
CIVibrance
inputAmount
kernel vec4 _smartcolor_cast_HDR (__sample im, float lum, float grayI, float grayQ, float strength) 
 vec4 pix = clamp(im, 0.0, 1.0); 
 pix.rgb = pow(pix.rgb, vec3(.25)); 
 pix.rgb = pix.r * vec3(0.299, 0.595716, 0.211456) + 
 pix.g * vec3(0.587, -0.274453, -0.522591) + 
 pix.b * vec3(0.114, -0.321263, 0.311135); 
 vec2 grayOffset = vec2(grayI, grayQ) ; 
 vec3 result = pix.rgb; 
 float newStrength = 1.0 + (strength-1.0)*(1.0-pix.r) ; 
 result.gb = pix.gb + newStrength*grayOffset ; 
 float damp = max(min(1.0, pix.r/(lum+0.00001)),0.0) ; 
 result.rgb = mix(pix.rgb, result.rgb, damp) ; 
 pix.rgb = result.r * vec3(1.0) + 
 result.g * vec3(0.956296, -0.272122, -1.10699) + 
 result.b * vec3(0.621024, -0.647381, 1.70461); 
 pix.rgb = clamp(pix.rgb, 0.0, 1.0); 
 pix.rgb *= pix.rgb*pix.rgb*pix.rgb; 
 pix.rgb += min(im.rgb, 0.0) + max(im.rgb,1.0) -1.0; 
 return pix; 
kernel vec4 _smartcolor_vibrancy_lt1_HDR (__sample im, float amt) 
 float gray = dot(im.rgb, vec3(0.333333)); 
 im.rgb = mix(vec3(gray), im.rgb, amt); 
 return im; 
kernel vec4 _smartcolor_vibrancy_gt1_HDR (__sample im, float amt) 
 float gray = dot(clamp(im.rgb, 0.0, 1.0), vec3(.3, .5, .2)); 
 float y = dot(clamp(im.rgb, 0.0, 1.0), vec3(.4, .2, .1)); 
 float damp = 1.0-4.0*y*(1.0-y); 
 float s = 1.0/(im.r+im.g+im.b); 
 float r = im.r*s; 
 float b = im.b*s; 
 float d = 1.0-.8*smoothstep(0.2, 0.4, r-b); 
 damp *= d; 
 damp = amt > 2.5 ? min(damp+(amt-2.5)/5.0, 1.0) : damp; 
 float sat = min(amt, 3.0); 
 vec4 result; 
 result.rgb = (im.rgb - gray)*sat + gray; 
 result.rgb = mix(im.rgb, result.rgb, damp); 
 result.a = im.a; 
 return result; 
kernel vec4 _smartcolor_contrast_darken_HDR (__sample im, float amt) 
 vec3 diff = im.rgb-dot(im.rgb, vec3(.0, .3, .5)); 
 float dist = distance(diff, vec3(0.0)); 
 dist = smoothstep(0.0, 1.0, dist); 
 float strength = 5.0*dist*amt; 
 vec3 pos = max(im.rgb, 1.0)-1.0 + min(im.rgb, 0.0); 
 im.rgb = clamp(im.rgb, 0.0, 1.0); 
 strength *= (im.b-im.g); 
 float gray = 1.0-min(dot(im.rgb, vec3(0.5, 0.7, -0.20)), 1.0); 
 vec4 result; 
 result.rgb = strength < 0.0 ? pow(im.rgb, vec3(1.0-strength*gray)) : im.rgb/(strength+1.0-(im.rgb*strength)); 
 result.rgb += pos; result.a = im.a; 
 return result; 
kernel vec4 _smartcolor_contrast_HDR (__sample im, float amt) 
 vec3 diff = im.rgb-dot(im.rgb, vec3(.0, .3, .5)); 
 float dist = distance(diff, vec3(0.0)); 
 dist = smoothstep(0.0, 1.0, dist); 
 float strength = 5.0*dist*amt; 
 vec3 pos = max(im.rgb, 1.0)-1.0 + min(im.rgb, 0.0); 
 im.rgb = clamp(im.rgb, 0.0, 1.0); 
 strength *= (im.b-im.g); 
 strength = max(strength, -0.35); 
 vec4 result; 
 result.rgb = im.rgb/(strength + 1.0 - (im.rgb*strength)) + pos; 
 result.a = im.a; 
 return result; 
satAutoValue
satPercentileG98
satPercentile98
satPercentile75
PISmartColorHDR-sat-histogram
timeRangeDurationTimescale
timeRangeDurationValue
timeRangeStartTimescale
timeRangeStartValue
startTimeTimescale
startTimeValue
crossfadeDurationTimescale
crossfadeDurationValue
spillMatteAllowed
portraitInfo
@"NUAdjustment"16@?0@"NUAdjustment"8
warmFace
grayY
grayWarmth
grayQ
grayI
-[PIOrientationAdjustmentController setOrientation:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-402.5.140/workspaces/neutrino/PhotoImaging/Controllers/PIOrientationAdjustmentController.m
mdta/com.apple.quicktime.camera-dictionary
Missing aperture rendering metadata
mdta/com.apple.quicktime.aperture-float
Missing disparity rendering metadata
Could not decode timed rendering data
mdta/com.apple.quicktime.cinematic-video.rendering
outError != nil
metadataGroup != nil
PIPortraitVideoMetadata.m
kernel vec4 _highKeyHDR(__sample im, float str) 
 vec3 neg = min(im.rgb, 0.0); 
 vec3 pos = max(im.rgb, 1.0) - 1.0; 
 im = clamp(im, 0.0, 1.0); 
 vec4 im2 = 1.0-((im-1.0)*(im-1.0)); 
 im2 = sqrt(im2); 
 float y = dot(im.rgb, vec3(.333333)); 
 float y2 = sqrt(1.0-(y-1.0)*(y-1.0)); 
 y2 = mix(y2, smoothstep(0.0, 1.0, y2), 0.5); 
 vec4 im3 = (y>0) ? im*y2/y : vec4(0.0, 0.0, 0.0, 1.0) ; 
 im3 = mix(im3, im2, .7*sqrt(y2)); 
 im3 = mix(im, im3, sqrt(y)) ; 
 im.rgb = mix(im.rgb, im3.rgb, str) + pos + neg; 
 return im; } 
None
UnexpectedRuntimeError
NoBuildingDetected
PanoAndFFCNotSupported
FacesTooLarge
ConfidenceTooLow
LimitExceeded
NotEnoughLines
CorrectedAreaIsSmall
LessThanMinimumCorrection
-[PIPerspectiveAutoCalculator submitVerified:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-402.5.140/workspaces/neutrino/PhotoImaging/Autocalculators/PIPerspectiveAutoCalculator.m
angle
PIPerspectiveAutoCalculator-getPrimaryOrientation
yawError
ciYawError
pitchError
ciPitchError
yawCorrectionAreaCoverage
pitchCorrectionAreaCoverage
debugImage
rollAngleInDegreesCW
yawExpandLeftDegrees
pitchExpandTopDegrees
passesMinimumCorrectionCheck
passesConfidenceCheck
submitVerified
CIAutoPerspective
minimumYawCorrectionArea
minimumPitchCorrectionArea
rollLimit
yawLimit
pitchLimit
focalLength
canGenerateNewCropRect
-[PIPerspectiveAutoCalculator passesMinimumCorrectionCheck:error:]
minimumAngleCorrection
minimumYawCorrection
minimumPitchCorrection
-[PIPerspectiveAutoCalculator passesConfidenceCheck:error:]
minConfidence
Not supported by Core Image. Default: YES
supported
-[PIPerspectiveAutoCalculator canGenerateNewCropRect:]
setToPrimary
preseedRoll
-[PIPerspectiveAutoCalculator primaryImageProperties:]
PIPerspectiveAutoCalculator-primaryImageProperties
-[PIPerspectiveAutoCalculator overcaptureImageProperties:]
PIPerspectiveAutoCalculator-overcaptureImageProperties
No overcapture
-[PIPerspectiveAutoCalculator submit:]
passesBuildingCheck
passesFaceCheck
passesImagePropertiesCheck
-[PIPerspectiveAutoCalculator passesBuildingCheck:]
PIPerspectiveAutoCalculator-classify
-[PIPerspectiveAutoCalculator passesImagePropertiesCheck:]
isSelfieCam
aspectRatio
PIPerspectiveAutoCalculator-selfieAndAspectRatioCheck
camera
front
Apple
-[PIPerspectiveAutoCalculator passesFaceCheck:]
maxFaceSize
faceSize
PIPerspectiveAutoCalculator-faceCheck
faces != nil
-[PIPerspectiveAutoCalculator getSizeOfAllFaces:]
PIPerspectiveAutoCalculator-debug
Edit
%@PerspectiveLineDetection-png.DBG
%@PerspectiveEvaluation-txt.DBG
yawError.underlyingError
pitchError.underlyingError
%@.underlyingError
%@.error
%@.%@
-[PIPerspectiveAutoCalculator initWithComposition:]
pitch != nil
+[PIPerspectiveAutoCalculator undoOrientation:forPitch:yaw:angle:]
yaw != nil
angle != nil
luminance
Flash
ApertureValue
FNumber
ExposureTime
ShutterSpeedValue
ISOSpeedRatings
-[PICropAdjustmentController setCropRect:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-402.5.140/workspaces/neutrino/PhotoImaging/Controllers/PICropAdjustmentController.m
cannot set empty crop rect
originalCrop
smart
constraintHeight
constraintWidth
inputDecoderVersion
localAutoValue
inputLight
overcaptureStatistics
statistics
inputRawHighlights
inputLocalLight
inputBlack
inputHighlights
inputShadows
inputBrightness
inputContrast
inputExposure
offsetShadows
offsetLocalLight
offsetHighlights
offsetExposure
offsetContrast
offsetBrightness
offsetBlack
inputLightMapHeight
inputLightMapWidth
inputLightMap
fuse_image_compute
blur_image_compute_3x3
blur_image_compute_5x5
blur_image_compute_7x7
ncc_coarse_compute
ncc_compute
warp_homography
rgba_to_luma
jointbilateralfilter
kernel vec4 jointbilateralfilter(sampler mask, sampler guide, vec2 rad, vec3 sig) __attribute__((outputFormat(kCIFormatRh)))
vec2 coord = destCoord();
float rh = rad.x;
float rv = rad.y;
vec4 p0 = sample(mask, samplerTransform(mask, coord));
vec4 g0 = sample(guide, samplerTransform(guide, coord));
vec4 result = vec4(0.0f);
float w_sum = 0.0f;
for(float yo=-rad.y;  yo<=rad.y;  yo++) {
for(float xo=-rad.x;  xo<=rad.x;  xo++) {
vec2 dc = vec2(xo, yo);
vec4 p = sample(mask, samplerTransform(mask, coord + dc));
vec4 g = sample(guide, samplerTransform(guide, coord + dc));
float d = dot(dc, dc);
float pdif = dot(p-p0,p-p0);
float gdif = dot(g-g0,g-g0);
float w = exp(dot(sig, vec3(d,pdif,gdif)));
result += w*p;
w_sum += w;
result /= w_sum;
return vec4(result.rgb, 1.0);
kernel vec4 rgba_to_luma(__sample rgb)
const vec4 rgb2y = vec4(0.2999,0.5870,0.1140,0.0);
float luma = dot(rgb, rgb2y);
return vec4(luma, luma, luma, 1.0);
kernel vec2 warp_homography(vec3 h012, vec3 h345, vec3 h678)
vec3 coord = vec3(destCoord(), 1.f);
float norm = 1.f / dot(h678, coord);
float x = norm * dot(h012, coord);
float y = norm * dot(h345, coord);
return vec2(x,y);
kernel vec4 ncc_compute(sampler img1, sampler img2) __attribute__((outputFormat(kCIFormatR8)))
vec2 coord = destCoord();
float sum1   = float(0.0);
float sum2   = float(0.0);
float sumSq1 = float(0.0);
float sumSq2 = float(0.0);
float sumCrs = float(0.0);
float ncc;
float p1, p2;
const int halfKernel = 3;
const vec4 meanOp = vec4(1.0/3.0, 1.0/3.0, 1.0/3.0, 0.0);
float count = 0.0;
for(int y=-halfKernel; y<=halfKernel; y++) {
for(int x=-halfKernel; x<=halfKernel; x++) {
p1 = dot(meanOp, sample(img1, samplerTransform(img1, coord + vec2(x, y))));
p2 = dot(meanOp, sample(img2, samplerTransform(img2, coord + vec2(x, y))));
sum1 += p1;
sum2 += p2;
sumSq1 += p1*p1;
sumSq2 += p2*p2;
sumCrs += p1*p2;
count += 1.0;
float denom = (sumSq1-sum1*sum1/count) * (sumSq2-sum2*sum2/count);
ncc = compare(denom, 0.0, ((sumCrs-(sum1*sum2/count)))/sqrt(denom));
ncc = max(ncc, 0.0);
return vec4(ncc, ncc, ncc, 1.0);
kernel vec4 ncc_coarse_compute(__sample ncc, vec2 edge) __attribute__((outputFormat(kCIFormatR8)))
float value = smoothstep(edge.x, edge.y, ncc.r);
return vec4(value, value, value, 1.0);
kernel vec4 blur_image_compute_3x3(sampler img)
const int halfKernel = 1;
vec2 coord = destCoord();
vec4 sum = vec4(0.0);
float count = 0.0;
for(int y=-halfKernel; y<=halfKernel; y++) {
for(int x=-halfKernel; x<=halfKernel; x++) {
sum += sample(img, samplerTransform(img, coord + vec2(x, y)));
count += 1.0;
sum = sum/(count);
return sum;
kernel vec4 blur_image_compute_5x5(sampler img)
const int halfKernel = 2;
vec2 coord = destCoord();
vec4 sum = vec4(0.0);
float count = 0.0;
for(int y=-halfKernel; y<=halfKernel; y++) {
for(int x=-halfKernel; x<=halfKernel; x++) {
sum += sample(img, samplerTransform(img, coord + vec2(x, y)));
count += 1.0;
sum = sum/(count);
return sum;
kernel vec4 blur_image_compute_7x7(sampler img)
const int halfKernel = 3;
vec2 coord = destCoord();
vec4 sum = vec4(0.0);
float count = 0.0;
for(int y=-halfKernel; y<=halfKernel; y++) {
for(int x=-halfKernel; x<=halfKernel; x++) {
sum += sample(img, samplerTransform(img, coord + vec2(x, y)));
count += 1.0;
sum = sum/(count);
return sum;
kernel vec4 fuse_image_compute(sampler refImg, sampler guideImg, sampler blurImg, sampler weightImg, sampler maskImg, vec2 threshold)
vec4 ref       = sample(refImg, samplerCoord(refImg));
vec4 refBlur   = ref;
vec4 guide     = sample(guideImg, samplerCoord(guideImg));
vec4 guideBlur = sample(blurImg, samplerCoord(blurImg));
float weight   = sample(weightImg, samplerCoord(weightImg)).r;
float mask     = sample(maskImg, samplerCoord(maskImg)).r;
vec3 CbCoef = vec3(-0.1726, -0.3391, 0.5117);
vec3 CrCoef = vec3(0.5115, -0.4282, -0.0830);
float CbRef   = dot(CbCoef, ref.rgb);
float CbGuide = dot(CbCoef, guideBlur.rgb);
float CrRef   = dot(CrCoef, ref.rgb);
float CrGuide = dot(CrCoef, guideBlur.rgb);
float crDiff = smoothstep(0.0, 0.2, abs(CbRef-CbGuide));
float cbDiff = smoothstep(0.0, 0.2, abs(CrRef-CrGuide));
float chDiff = smoothstep(0.0,0.3,crDiff+cbDiff);
float weight1 = (1.0-smoothstep(threshold.x,threshold.y,mask));
weight *= weight1 * (1.0-chDiff);
vec4 value = mix(ref, (0.9*(guide-guideBlur)+refBlur), weight);
return value;
kernel vec4 dynamismMap(sampler imageMax, sampler imageMin, float logisticGain, float logisticMid) __attribute__((outputFormat(kCIFormatRGBAh)))
const float MAX_VAL = 1.74;
const float THRESHOLD = 0.05;
vec2 dc = destCoord();
vec2 sc;
sc = dc + vec2(-1,-1);
vec4 pMax00 = sample(imageMax, samplerTransform(imageMax, sc));
vec4 pMin00 = sample(imageMin, samplerTransform(imageMin, sc));
sc = dc + vec2(-1,0);
vec4 pMax01 = sample(imageMax, samplerTransform(imageMax, sc));
vec4 pMin01 = sample(imageMin, samplerTransform(imageMin, sc));
sc = dc + vec2(-1,1);
vec4 pMax02 = sample(imageMax, samplerTransform(imageMax, sc));
vec4 pMin02 = sample(imageMin, samplerTransform(imageMin, sc));
sc = dc + vec2(0,-1);
vec4 pMax10 = sample(imageMax, samplerTransform(imageMax, sc));
vec4 pMin10 = sample(imageMin, samplerTransform(imageMin, sc));
vec4 pMax11 = sample(imageMax, samplerTransform(imageMax, dc));
vec4 pMin11 = sample(imageMin, samplerTransform(imageMin, dc));
sc = dc + vec2(0,1);
vec4 pMax12 = sample(imageMax, samplerTransform(imageMax, sc));
vec4 pMin12 = sample(imageMin, samplerTransform(imageMin, sc));
sc = dc + vec2(1,-1);
vec4 pMax20 = sample(imageMax, samplerTransform(imageMax, sc));
vec4 pMin20 = sample(imageMin, samplerTransform(imageMin, sc));
sc = dc + vec2(1,0);
vec4 pMax21 = sample(imageMax, samplerTransform(imageMax, sc));
vec4 pMin21 = sample(imageMin, samplerTransform(imageMin, sc));
sc = dc + vec2(1,1);
vec4 pMax22 = sample(imageMax, samplerTransform(imageMax, sc));
vec4 pMin22 = sample(imageMin, samplerTransform(imageMin, sc));
float minDevCMaxNMin = distance(pMax11, pMin00);
minDevCMaxNMin = min( minDevCMaxNMin , distance(pMax11, pMin01) );
minDevCMaxNMin = min( minDevCMaxNMin , distance(pMax11, pMin02) );
minDevCMaxNMin = min( minDevCMaxNMin , distance(pMax11, pMin10) );
minDevCMaxNMin = min( minDevCMaxNMin , distance(pMax11, pMin11) );
minDevCMaxNMin = min( minDevCMaxNMin , distance(pMax11, pMin12) );
minDevCMaxNMin = min( minDevCMaxNMin , distance(pMax11, pMin20) );
minDevCMaxNMin = min( minDevCMaxNMin , distance(pMax11, pMin21) );
minDevCMaxNMin = min( minDevCMaxNMin , distance(pMax11, pMin22) );
float minDevCMinNMax = distance(pMin11, pMax00);
minDevCMinNMax = min( minDevCMinNMax , distance(pMin11, pMax01) );
minDevCMinNMax = min( minDevCMinNMax , distance(pMin11, pMax02) );
minDevCMinNMax = min( minDevCMinNMax , distance(pMin11, pMax10) );
minDevCMinNMax = min( minDevCMinNMax , distance(pMin11, pMax11) );
minDevCMinNMax = min( minDevCMinNMax , distance(pMin11, pMax12) );
minDevCMinNMax = min( minDevCMinNMax , distance(pMin11, pMax20) );
minDevCMinNMax = min( minDevCMinNMax , distance(pMin11, pMax21) );
minDevCMinNMax = min( minDevCMinNMax , distance(pMin11, pMax22) );
float outVal = min(minDevCMaxNMin , minDevCMinNMax) / MAX_VAL;
outVal = 1.0f / (1.0f + exp(-logisticGain*(outVal - logisticMid)));
vec4 outPixel = vec4(outVal, outVal, outVal, 1.0);
return outPixel;
kernel vec4 alphaCompositing(__sample src, __sample dst, float a)
return mix(dst, src, a);
SkipShaderPrewarm
endScale
startScale
start
depthInfo
inputCorrectionInfo
PIPhotoEditHelper.m
/post-Geometry
/pre-Geometry
ResetTagInput('/pre-Geometry', input);
return GetTag('/post-Geometry');
/ShowOriginalSource
VideoReframe
/pre-VideoReframe
var image = input;
ResetTagInput('/pre-VideoReframe', image);
image = GetTag('VideoReframe');
ResetTagInput('/pre-Geometry', image);
return GetTag('/post-Geometry');
v32@?0@"NSString"8@"NSString"16^B24
overrideVideoEditability
+[PIPhotoEditHelper videoRenderRequestWithComposition:fitInSize:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-402.5.140/workspaces/neutrino/PhotoImaging/Util/PIPhotoEditHelper.m
+[PIPhotoEditHelper _imageRenderRequestWithComposition:wideGamut:]
PIPhotoEditHelper-basic-image
targetSize.width > 0 && targetSize.height > 0
+[PIPhotoEditHelper imageRenderRequestWithComposition:fillInSize:wideGamut:]
PIPhotoEditHelper-fillSquare-image
+[PIPhotoEditHelper imageRenderRequestWithComposition:fitInSize:wideGamut:]
PIPhotoEditHelper-targetSize-image
+[PIPhotoEditHelper videoPropertiesRequestWithComposition:]
+[PIPhotoEditHelper imagePropertiesRequestWithComposition:]
+[PIPhotoEditHelper geometryRequestWithComposition:]
name != nil
+[PIPhotoEditHelper newImageRenderClientWithName:]
identifier != nil
+[PIPhotoEditHelper newAdjustmentWithIdentifier:]
+[PIPhotoEditHelper newAdjustmentWithName:]
photoSource != nil
+[PIPhotoEditHelper livePhotoSourceWithPhotoSource:videoSource:]
videoSource != nil
%@+%@
+[PIPhotoEditHelper videoSourceWithURL:]
+[PIPhotoEditHelper imageSourceWithCIImage:orientation:]
+[PIPhotoEditHelper imageSourceWithURL:type:proxyImage:orientation:useEmbeddedPreview:]
+[PIPhotoEditHelper imageSourceWithURL:type:useEmbeddedPreview:]
%@ %a
%@ (preview) %a
StudioV2
Light
ContourV2
Contour
Commercial
CIPortraitEffectStudioV2
CIPortraitEffectLight
CIPortraitEffectContourV2
CIPortraitEffectContour
CIPortraitEffectCommercial
StageWhite
StageV2
StageMonoV2
BlackoutMono
Black
CIPortraitEffectStageWhite
CIPortraitEffectStageV2
CIPortraitEffectStageMonoV2
CIPortraitEffectBlackoutMono
CIPortraitEffectBlack
3DVividWarm
3DVividCool
3DVivid
3DSilverplate
3DNoir
3DDramaticWarm
3DDramaticCool
3DDramatic
CIPhotoEffect3DVividWarm
CIPhotoEffect3DVividCool
CIPhotoEffect3DVivid
CIPhotoEffect3DSilverplate
CIPhotoEffect3DNoir
CIPhotoEffect3DDramaticWarm
CIPhotoEffect3DDramaticCool
CIPhotoEffect3DDramatic
Transfer
Tonal
Process
Noir
Mono
Instant
Fade
Chrome
CIPhotoEffectTransfer
CIPhotoEffectTonal
CIPhotoEffectProcess
CIPhotoEffectNoir
CIPhotoEffectMono
CIPhotoEffectInstant
CIPhotoEffectFade
CIPhotoEffectChrome
+[PIForwardFakeBoost kernel]_block_invoke
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-402.5.140/workspaces/neutrino/PhotoImaging/Adjustments/PIFakeBoost.m
boost kernel is nil
kernel vec4 forwardBoostWithBoost(sampler image, float boost){
vec4 im = sample(image, samplerCoord(image));
vec4 orig = im;
float n = 1.8;
float k = 0.8;
vec3 pos = max(im.rgb, k) - k;
vec3 neg = min(im.rgb, 0.0);
neg *= 2.8;
im.rgb = clamp(im.rgb, 0.0, k);
im.rgb = ((1.0+n)*im.rgb)/(1.0+(n*im.rgb)) + neg;
im.r = pos.r > 0.0 ? (.91803 + .470304*pos.r) : im.r;
im.g = pos.g > 0.0 ? (.91803 + .470304*pos.g) : im.g;
im.b = pos.b > 0.0 ? (.91803 + .470304*pos.b) : im.b;
im.rgb = mix(orig.rgb, im.rgb, boost);
return im;
-[PIForwardFakeBoost outputImage]
+[PIInverseFakeBoost kernel]_block_invoke
inverse boost kernel is nil
kernel vec4 inverseBoostWithBoost(sampler image, float boost){
vec4 im = sample(image, samplerCoord(image));
vec4 orig = im;
float n = 1.8;
float kinv = 0.91803;
vec3 pos = max(im.rgb, kinv);
vec3 neg = min(im.rgb, 0.0);
im.rgb = clamp(im.rgb, 0.0, kinv);
neg *= .35714286;
im.rgb = im.rgb/(n+1.0-(im.rgb*n)) + neg;
im.r = pos.r > kinv ? 0.8 + 2.126286*(pos.r-kinv) : im.r;
im.g = pos.g > kinv ? 0.8 + 2.126286*(pos.g-kinv) : im.g;
im.b = pos.b > kinv ? 0.8 + 2.126286*(pos.b-kinv) : im.b;
im.rgb = mix(orig.rgb, im.rgb, boost);
return im;
-[PIInverseFakeBoost outputImage]
AutoEnhance
AutoLoop
Crop
DepthEffect
Effect
Effect3D
Grain
HighResolutionFusion
LivePhotoKeyFrame
Mute
PortraitEffect
PortraitVideo
RedEyeBB
SelectiveColor
SemanticEnhance
SlowMotion
SmartBlackAndWhite
SmartColor
SmartTone
SourceSelect
Trim
VideoCrossfadeLoop
VideoPosterFrame
VideoStabilize
WhiteBalance
compositionName
identifierName
custom
customSerialization
customCompositionName
customIdentifierName
autoValue
requiresEnabled
v24@?0@"NSMutableDictionary"8@"NUAdjustment"16
timeRange
analysisType
cropFraction
keyframes
portraitEffectFilterName
glassesMatteAllowed
alignment
timescale
epoch
flags
rate
regions
endTime
redEyeCorrections
sensitivity
center
pointY
pointX
inputSpots
RKRedEyeOperation
inputDetectedFaces
detectedFaces
data
pressure
yLocation
xLocation
points
hasSource
sourceOffset
repairEdges
opacity
softness
mode
brushStroke
inputStrokes
RKRetouchOperation
corrections
RKSelectiveColorOperation
redCurvePoints
RGBCurvePoints
greenCurvePoints
blueCurvePoints
pointsR
pointsL
pointsG
pointsB
RKCurvesOperation
whiteSrcRed
whiteSrcRGB
whiteSrcGreen
whiteSrcBlue
whiteDstRed
whiteDstRGB
whiteDstGreen
whiteDstBlue
shadowSrcRed
shadowSrcRGB
shadowSrcGreen
shadowSrcBlue
shadowDstRed
shadowDstRGB
shadowDstGreen
shadowDstBlue
midSrcRed
midSrcRGB
midSrcGreen
midSrcBlue
midDstRed
midDstRGB
midDstGreen
midDstBlue
hilightSrcRed
hilightSrcRGB
hilightSrcGreen
hilightSrcBlue
hilightDstRed
hilightDstRGB
hilightDstGreen
hilightDstBlue
blackSrcRed
blackSrcRGB
blackSrcGreen
blackSrcBlue
blackDstRed
blackDstRGB
blackDstGreen
blackDstBlue
inputColorSpace
colorSpace
RKLevelsOperation
warm
face
inputMethodVersion
RKRawDecodeOperation
inputLNRAmount
inputCNRAmount
inputDetailAmount
detail
DGRAWReduceNoiseOperation
straightenAngle
effectIntensity
effectVersion
effectName
inputSharpness
inputFalloff
inputEdgeScale
edges
RKProSharpenOperation
inputRadius
edgeDetail
RKNoiseReductionOperation
inputIntensity
DGDefinition2Operation
DGVignetteEffectOperation
seed
amount
offsetNeutralGamma
inputBlackAndWhite
offsetTone
offsetGrain
offsetStrength
offsetCast
offsetSaturation
inputColor
lightMapAvg
lightMap
offsetBlackPoint
v24@?0@"NSDictionary"8@"NUAdjustment"16
autoRedEyeCorrections
allCorrections
RedEye
ApertureRedEye
B16@?0@"NSDictionary"8
Retouch
Curves
Display P3
sRGB
Adobe RGB
Generic P3
Levels
RawNoiseReduction
CropStraighten
@"NSString"24@?0@"NSDictionary"8@"NUAdjustment"16
@"NSString"16@?0@"NSDictionary"8
Sharpen
NoiseReduction
Definition
Vignette
v32@?0@8@16^B24
vec4 _curve_sample_HDR(float x, sampler2D table, vec2 domain, vec2 normalizer) { x = (x - domain.x) / (domain.y - domain.x); x = clamp(x, 0.0001, 0.9999); x = normalizer.x * x + normalizer.y; return texture2D(table, vec2(x, 0.5)); } kernel vec4 _curves_rgb_lum_HDR(__sample color, sampler2D table, vec2 domain, vec2 normalizer) { vec4 pixel = color; pixel.r = _curve_sample_HDR(pixel.r, table, domain, normalizer).r; pixel.g = _curve_sample_HDR(pixel.g, table, domain, normalizer).g; pixel.b = _curve_sample_HDR(pixel.b, table, domain, normalizer).b; float lum0 = dot(pixel.rgb, vec3(0.3, 0.59, 0.11)); float lum1 = _curve_sample_HDR(lum0, table, domain, normalizer).a; float lum1c = clamp(lum1, -8.0 * abs(lum0), 8.0 * abs(lum0)); float lum_scale = (lum0 == 0.0 ? 0.0 : lum1c / lum0); float lum_offset = lum1 - lum1c; pixel.rgb = lum_scale * pixel.rgb + lum_offset; pixel.rgb = min(pixel.rgb, 1.0); return pixel; }
-[PILongExposureAccumulator _exportOutputImage:format:colorSpace:toURL:uti:error:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-402.5.140/workspaces/neutrino/PhotoImaging/AutoLoop/PIAutoLoopJob+LongExposure.m
destinationURL != nil
Failed to create CGImageRef
Failed to create CGImageDestinationRef
Failed to finalize image destination
-[PILongExposureAccumulator writeMaskImage:UTI:error:]
v16@?0@"CIImage"8
-[PILongExposureAccumulator writeLongExposureImage:UTI:colorSpace:error:]
-[PILongExposureAccumulator _accumulate:error:]
B16@?0@"CIRenderDestination"8
failed to render maximum image
PILongExposureAccumulator-maximum-j%lld
failed to render minimum image
PILongExposureAccumulator-minimum-j%lld
CIBoxBlur
failed to render average image
PILongExposureAccumulator-average-j%lld
frame != nil
-[PILongExposureAccumulator accumulate:error:]
failed to init accumulator
PILongExposureAccumulator-main-j%lld
Accumulation was cancelled
PILongExposureAccumulator.state
PILongExposureAccumulator.accum
-[PILongExposureRegistrationJob render:]
Image registration failure (expected 1 observation)
Failed to render luma
PILongExposureRegistrationJob-reference-j%lld
PILongExposureRegistrationJob-render-j%lld
Failed to allocate intermediate pixel buffer
-[PILongExposureRegistrationJob prepare:]
Malformed AutoLoop recipe : crop
return Source(composition.source, {'skipOrientation':true});
/AutoLoop/LongExposure
/RAW/SushiLevel1
/Master%@
CILanczosScaleTransform
CIRedEyeCorrection
inputCameraModel
dynamic
@"NSString"16@?0@"NUAdjustment"8
1.5.1
convexHull
SDOFRenderingVersion
overcaptureComputed
computed
/private/var/mobile/Media/PhotoData/CaptureDebug/
Repair
Clone
RepairML
-[PIPortraitVideoDebugDetectionsRenderNode _evaluateImage:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-402.5.140/workspaces/neutrino/PhotoImaging/Render/PIPortraitVideoDebugDetectionsRenderNode.m
monochrome
Helvetica
%@ G:%@
%@ %@
Face
Head
Torso
Sports Ball
AutoFocus
-[PIPortraitVideoDebugDetectionsRenderNode resolvedNodeWithCachedInputs:settings:pipelineState:error:]
%@-labelImageCache
v20@?0B8@"NSError"12
assetURL
inputRAWGamutMapMax
falseColorHDR
outputExposure
id<NUBuffer> computeRepairMask(__strong id<NUBuffer>, uint16_t *)
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-402.5.140/workspaces/neutrino/PhotoImaging/Adjustments/ApertureRedEye/PIApertureRedEye.mm
void seedFill(__strong id<NUMutableBuffer>, __strong id<NUBuffer>, const uint32_t, const NSInteger, const NSInteger)
repairBuffer != nil
void applyRepairMask(__strong id<NUMutableBuffer>, __strong id<NUBuffer>, double, const uint32_t)
reddesty out of bounds
reddestx out of bounds
Buffer must be RGBA16 type for red eye repairs
kernel vec4 facebalance(__sample src, vec2 gamma, vec3 matchMinusOrigStrength)
vec4 pix = src;
pix.rgb = pow(max(pix.rgb, 0.0), vec3(gamma.x));
pix.rgb = pix.r * vec3(0.299,  0.595716,  0.211456) +
pix.g * vec3(0.587, -0.274453, -0.522591) +
pix.b * vec3(0.114, -0.321263,  0.311135);
vec4 orig = pix ;
float chroma = min(1.0, 2.0*sqrt(pix.g*pix.g + pix.b*pix.b) );
pix.gb +=  matchMinusOrigStrength.rg;
float strength = matchMinusOrigStrength.z*pow(chroma, .4) ;
pix.gb = mix(orig.gb, pix.gb, strength) ;
pix.rgb = pix.rrr +
pix.g * vec3(0.956296, -0.272122, -1.10699) +
pix.b * vec3(0.621024, -0.647381, 1.70461);
pix.rgb = max(pix.rgb, vec3(0.0));
pix.rgb = pow(pix.rgb, vec3(gamma.y));
return pix;
facebalance
tableR != nil
+[PICurvesLUTFilter tableImageFromRed:green:blue:luminance:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-402.5.140/workspaces/neutrino/PhotoImaging/Adjustments/PICurvesFilter.mm
tableG != nil
tableB != nil
tableL != nil
vec4 curve_sample(float x, sampler2D table, vec2 domain, vec2 normalizer)
x = (x - domain.x) / (domain.y - domain.x);
x = clamp(x, 0.0001, 0.9999);
x = normalizer.x * x + normalizer.y;
return texture2D(table, vec2(x, 0.5));
kernel vec4 curves_rgb_lum(__sample color, sampler2D table, vec2 domain, vec2 normalizer)
vec4 pixel = color;
pixel.r = curve_sample(pixel.r, table, domain, normalizer).r;
pixel.g = curve_sample(pixel.g, table, domain, normalizer).g;
pixel.b = curve_sample(pixel.b, table, domain, normalizer).b;
float lum0 = dot(pixel.rgb, vec3(0.3, 0.59, 0.11));
float lum1 = curve_sample(lum0, table, domain, normalizer).a;
float lum1c = clamp(lum1, -8.0 * abs(lum0), 8.0 * abs(lum0));
float lum_scale = (lum0 == 0.0 ? 0.0 : lum1c / lum0);
float lum_offset = lum1 - lum1c;
pixel.rgb = lum_scale * pixel.rgb + lum_offset;
return pixel;
colorBalance
PPtogHDR
gHDRtoPP
kernel vec4 gHDRtoPP(sampler image)
vec4 pix ;
vec3 pix2;
pix = sample(image, samplerCoord(image));
pix2 = pix.r * vec3(0.615429622407401,   0.114831839141528,   0.011544126697221) +
pix.g * vec3(0.367479646665836,   0.797943554457996,   0.064077744191180) +
pix.b * vec3(  0.016956659608091,   0.087783443422360,   0.924405601458102);
return vec4(pix2, pix.a);
kernel vec4 PPtogHDR(sampler image)
vec4 pix ;
vec3 pix2;
pix = sample(image, samplerCoord(image));
pix2 = pix.r * vec3(1.777445503202045,  -0.255296595099306,  -0.004500433755654) +
pix.g * vec3( -0.822224875430495,   1.380948853784730,  -0.085456231694984) +
pix.b * vec3(0.045475917061484,  -0.126454737973025,   1.089973874037625);
return vec4(pix2, pix.a);
kernel vec4 colorBalance(__sample image, float colorI, float colorQ, float str, vec2 gamma)
vec4 pix = image;
vec3 neg = min(pix.rgb, 0.0);
vec3 pos = max(pix.rgb, 1.0) - 1.0;
pix.rgb = pow(clamp(pix.rgb, 0.0, 1.0), vec3(gamma.x));
pix.rgb = pix.r * vec3(0.299,  0.595716,  0.211456) +
pix.g * vec3(0.587, -0.274453, -0.522591) +
pix.b * vec3(0.114, -0.321263,  0.311135);
vec4 orig = pix ;
float chroma = min(1.0, 2.0*sqrt(pix.g*pix.g + pix.b*pix.b) );
pix.gb += vec2(colorI,colorQ);
float strength = str*pow(chroma, .4) ;
pix.gb = mix(orig.gb, pix.gb, strength) ;
pix.rgb = pix.rrr +
pix.g * vec3(0.956296, -0.272122, -1.10699) +
pix.b * vec3(0.621024, -0.647381, 1.70461);
pix.rgb = pow(max(pix.rgb, 0.0), vec3(gamma.y));
pix.rgb += pos + neg;
return pix;
-[PIColorBalanceFilter outputImage]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-402.5.140/workspaces/neutrino/PhotoImaging/Adjustments/PIColorBalanceFilter.m
PILevelsFilter
PISelectiveColorFilter
PICurvesFilter
PICurvesLUTFilter
PIBilateralFilter
PINeutralGrayWhiteBalanceFilter
PIRAWFaceBalance
PIColorBalanceFilter
PIFalseColorHDRDebug
CISmartTone
CISmartColor
CILocalLight
CIVignetteEffect
@"NSString"16@?0@"NSString"8
CISmartBlackAndWhite
CIPhotoGrain
CILocalLightMapPrepare
CILocalLightFilter
PILocalContrastHDR
CILocalContrast
CIHighKey
CIProSharpenEdges
CIExposureAdjust
CIGammaAdjust
CIMix
debugDecoratorFiltersEnabled
PIGlobalSettings
photoEditingSettings
PXSettingsArchiveKey
PURootSettings
editSettings
IPXRootSettings
-[PIAutoLoopExportJob initWithVideoExportRequest:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-402.5.140/workspaces/neutrino/PhotoImaging/AutoLoop/PIAutoLoopJob+Export.m
B8@?0
PI_AUTOLOOP_EXPORT_USE_METAL
OvercaptureRectForAutoCrop
{CGRect={CGPoint=dd}{CGSize=dd}}64@?0{CGSize=dd}8{CGSize=dd}24{CGSize=dd}40@"JSValue"56
@"NUJSRenderNode"24@?0@"NUJSRenderNode"8@"JSValue"16
-[PIJSRenderPipeline setUpContext:]_block_invoke
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-402.5.140/workspaces/neutrino/PhotoImaging/Pipeline/PIPhotosPipeline.m
Unable to unwrap the input node!
input to VideoCrossfadeLoop cannot be nil
Missing duration for crossfade
Invalid data type for adjustmentValue
/System/Library/Frameworks/JavaScriptCore.framework/JavaScriptCore
/System/Library/Frameworks/JavaScriptCore.framework/Contents/MacOS/JavaScriptCore
void *JavaScriptCoreLibrary(void)
JSContext
Class getJSContextClass(void)_block_invoke
Unable to find class %s
JavaScriptCoreSoftLinking.h
input to VideoReframe cannot be nil
Invalid data type for stabCropRect
Invalid data type for keyframes
+[NUJSRenderPipeline(PhotosPipeline) newPhotosPipeline:]
Couldn't find the specified Pipeline
PhotosPipeline
Couldn't find bundle for class %@
redEyeSpots
-[PIApertureRedEyeAutoCalculator submit:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-402.5.140/workspaces/neutrino/PhotoImaging/Autocalculators/PIApertureRedEyeAutoCalculator.m
pre-Adjustments
PIApertureRedEyeAutoCalculator-faceDetection
NSNumber
CIEdgePreserveUpsampleRGFilter
_lightMapImageFromData_block_invoke
PILocalLightHDR.m
x == 0
y == 0
width == lmWidth
height == lmHeight
PILocalLightHDR
inputSmartShadows
-[PILocalLightFilterHDR outputImage]
CGRectEqualToRect([guideImage extent], [inputImage extent])
inputImage != nil
guideImage != nil
inputLocalLight != nil
CGRectEqualToRect([lightMapImage extent], [inputImage extent])
kernel vec4 _polyKernelHDR(__sample im, __sample adj, float str) { adj.r = 3.4*adj.r-1.2; vec3 neg = min(im.rgb, 0.0); vec3 pos = max(im.rgb, 1.0)-1.0; im.rgb = clamp(im.rgb, 0.0, 1.0); vec4 orig = im; float y = sqrt(dot(im.rgb, vec3(.33333))); float s = mix(0.0, adj.r, str); vec3 gain = s > 0.0 ? vec3(1.5*s) : vec3(1.75*s, 1.75*s, 1.55*s); im.rgb = im.rgb*im.rgb*gain + im.rgb*(1.0-gain); im.rgb = (clamp(im.rgb, 0.0, 1.0)); float midAmt = min(str, .5); y = y*(1.0-y); im.rgb = sqrt(im.rgb); float pivot = max(adj.g, 0.5); float a = midAmt*y; float b = -pivot*a; vec3 pix = im.r * vec3(0.299*a) + 
 im.g * vec3(0.587*a) + 
 im.b * vec3(0.114*a) + 
 im.rgb + vec3(b); 
 im.rgb = mix(im.rgb, vec3(pivot), -y*midAmt); im.rgb = mix(im.rgb, pix, 0.8); im.rgb = max(im.rgb, 0.0); im.rgb *= im.rgb; im.rgb = clamp(im.rgb, 0.0,1.0)+pos+neg; return im; }
kernel vec4 _shadowKernelHDR(__sample im, __sample adj, float str) { adj.r = 3.4*adj.r-1.2; vec3 neg = min(im.rgb, 0.0); vec3 pos = max(im.rgb, 1.0)-1.0; im.rgb = clamp(im.rgb, 0.0, 1.0); vec4 orig = im; float y = sqrt(dot(im.rgb, vec3(.33333))); float s = mix(0.0, adj.r, str); vec3 gain = s > 0.0 ? vec3(0.0) : vec3(s*s) * vec3(-2.75, -2.75, -2.5); im.rgb = im.rgb*im.rgb*gain + im.rgb*(1.0-gain); float m = 1.0 + 1.85*s*(max(0.1-y, 0.0)) ; im.rgb = (clamp(m*im.rgb, 0.0, 1.0)); float midAmt = s < 0.0 ? min(s*s,1.0) : 0.0; y = y*(1.0-y); im.rgb = sqrt(im.rgb); float pivot = .4; float a = midAmt*y; float b = -pivot*a; vec3 pix = im.r * vec3(0.299*a) + 
 im.g * vec3(0.587*a) + 
 im.b * vec3(0.114*a) + 
 im.rgb + vec3(b); im.rgb = mix(im.rgb, vec3(pivot), -y*midAmt); im.rgb = mix(im.rgb, pix, 0.8); im.rgb = max(im.rgb, 0.0); im.rgb *= im.rgb; im.rgb = clamp(im.rgb, 0.0,1.0)+pos+neg; return im; }
lightMapHeight
lightMapWidth
PILocalLightHDR-stats
-[PISmartBlackAndWhiteAutoCalculator submit:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-402.5.140/workspaces/neutrino/PhotoImaging/Autocalculators/PISmartBlackAndWhiteAutoCalculator.mm
PISmartBlackAndWhiteAutoCalculator-renderInputImage
v24@?0@"<NUBufferTile>"8^B16
v16@?0@"<NUMutableBuffer>"8
void calculate_bw_auto_matrix_lum_contrast(__strong id<NUBuffer>, CGColorSpaceRef, MsgBlackAndWhiteSettings *)
Produced invalid BlackAndWhite settings, using defaults
xi < w && yi < h
MatrixFloatType &MsgMatrix<double>::operator()(size_t, size_t) [MatrixFloatType = double]
index < data.size()
allocator<T>::allocate(size_t n) 'n' exceeds maximum supported size
num == w
void MsgMatrix<double>::AppendRow(const NumberType *, uint) [MatrixFloatType = double, NumberType = double]
inputScaleFactor
PIPhotoGrainHDR
inputISO
kernel vec4 _smartBlackAndWhiteHDR(__sample imageHDR, sampler2D hueImage, vec4 rgbWeights, vec4 normalizer) { float hueTableScaleFactor = rgbWeights.w; float hueImageWidth = normalizer.x; float huePixelCenter = normalizer.y; float neutralGamma = normalizer.z; float phototone = normalizer.w; float bw = dot(imageHDR.rgb / 12.0, rgbWeights.rgb); bw = clamp(bw, 0.0, 1.0); vec3 lms; lms.x = dot(imageHDR.rgb, vec3(0.3139902162, 0.6395129383, 0.0464975462)); lms.y = dot(imageHDR.rgb, vec3(0.155372406, 0.7578944616, 0.0867014186)); lms.z = dot(imageHDR.rgb, vec3(0.017752387, 0.109442094, 0.8725692246)); lms = pow(lms, vec3(0.43)); float i = dot(lms, vec3(0.4,0.4,0.2)); float p = dot(lms, vec3(4.4550,-4.8510,0.3960)); float t = dot(lms, vec3(0.8056,0.3572,-1.1628)); float chroma = sqrt(p*p+t*t); float hue = 0.5 + (atan(t, p) / 6.28318530718); vec2 huePt = vec2(hue * hueImageWidth + huePixelCenter, 0.5); float hueGamma = hueTableScaleFactor * texture2D(hueImage, huePt).a; float cd = 0.06 + 0.53 * abs(i-0.5); float lowSaturationDamp = smoothstep(0.0, 1.0, (chroma)/cd); float intensityDamp = smoothstep(0.0, 1.0, 1.0 - i); float lowLuminosityDamp = smoothstep(0.0, 1.0, 25.0 * i); float hWeight = lowSaturationDamp * intensityDamp * lowLuminosityDamp; hueGamma -= 1; hueGamma *= hWeight; hueGamma += 1; bw = pow(bw, hueGamma); float bwSDR = clamp(bw * 12.0, 0.0, 1.0); float midLumWeight = bwSDR*(1.0 - bwSDR); float grayWeight = 1.0 - smoothstep(0.0, 1.0, chroma * 10.0); float nWeight = midLumWeight * grayWeight; neutralGamma -= 1; neutralGamma *= nWeight; neutralGamma *= -2; neutralGamma += 1; bw = pow(bw, neutralGamma); bw = bw * 12.0; bw = clamp(bw, 0.0, 12.0); float df0 = 0.812379; float result; if (bw < df0) { result = 1.8031*bw*bw*bw - 2.1972*bw*bw + 1.3823*bw; } else { float scale = 12.0 - df0; float x = (bw - df0) / scale; result = 1.8031*x*x*x - 2.1972*x*x + 1.3823*x; result = result * scale + df0; result -= 0.158305860; } bw = mix(bw, result,-phototone); return vec4(bw,bw,bw,imageHDR.a); }
Just use [CIFilter filterWithName:@"PISmartBlackAndWhiteHDR"] instead.
There is no need to call smartBlackAndWhiteAdjustmentsForValue:andStatistics:.
There is no need to call smartBlackAndWhiteStatistics.
whiteDst
whiteSrc
hilightDst
hilightSrc
midDst
midSrc
shadowDst
shadowSrc
blackDst
blackSrc
-[PILevelsAutoCalculator calculateSettingsForImageHistogram:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-402.5.140/workspaces/neutrino/PhotoImaging/Autocalculators/PILevelsAutoCalculator.m
This is an abstract method! Subclass '%@' should provide concrete implementation
-[PILevelsAutoCalculator submit:]
pre-Levels
PILevelsAutoCalculator-histogram
-[PICompositionController(AdjustmentExtensions) _adjustmentControllerForKey:creatingIfNecessary:expectedClass:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-402.5.140/workspaces/neutrino/PhotoImaging/Controllers/PICompositionController+AdjustmentExtensions.m
Adjustment controller for key %@ is of class: %@, but was expected to be %@
not editable
editable
<%@:%p> [(%.3f, %.3f), %s]
CIHighlightShadowAdjust
inputHighlightAmount
inputShadowAmount
kernel vec4 _rawHighlightsHDR(__sample pix, float gain) 
 vec3 high = gain*pix.rgb; 
 float lum = clamp(dot(pix.rgb, vec3(.3333)), 0.0, 1.0); 
 vec3 neg = min(high, 0.0); 
 high.rgb = mix(max(pix.rgb, 0.0), high.rgb, lum*lum) + neg; 
 return vec4(high, pix.a); 
kernel vec4 _smarttone_highlightcontrast_HDR (__sample pix, float highAmt, float sat) 
 float maxVal = 1.0; vec3 neg = min(pix.rgb, 0.0); 
 vec3 pos = max(pix.rgb, maxVal) - maxVal; 
 pix.rgb = clamp(pix.rgb, 0.0, maxVal); 
 float lum = clamp(dot(pix.rgb, vec3(.3333)),0.0,1.0); 
 vec3 high = pow(pix.rgb, vec3(3.0 - 2.0*highAmt)); 
 float pivot = 0.8; 
 vec3 pix1 = (high - pivot)*(4.0 - 3.0*highAmt) + pivot; 
 float h = highAmt*highAmt*highAmt*highAmt; 
 float a = (4.0 - 3.0*h); 
 vec3 pix2 = (lum-pivot)*a+pivot + high.rgb -lum; 
 high = mix(pix2, pix1, sat); 
 pix.rgb = mix(pix.rgb, high, lum*lum); 
 pix.rgb = pix.rgb + neg + pos; 
 return pix; 
kernel vec4 _smarttone_contrast_HDR (__sample im, float midAmt, float maxVal) 
 midAmt = midAmt / maxVal; 
 vec3 neg = min(im.rgb, 0.0); 
 vec3 pos = max(im.rgb, maxVal) - maxVal; 
 im.rgb = clamp(im.rgb, 0.0, maxVal); 
 float y = dot(im.rgb, vec3(0.3333)); 
 y = sqrt(y); 
 im.rgb = sqrt(im.rgb); 
 float sat = dot(im.rgb - vec3(y), im.rgb - vec3(y)) / sqrt(maxVal); 
 y = y*(sqrt(maxVal)-y); 
 float a = midAmt*y; 
 float b = -0.5*a; 
 vec3 pix = im.r * vec3(0.299*a) + 
 im.g * vec3(0.587*a) + 
 im.b * vec3(0.114*a) + 
 im.rgb + vec3(b); 
 im.rgb = mix(im.rgb, vec3(0.5), -a); 
 im.rgb = mix(im.rgb, pix, clamp(0.8+sat, 0.0, 1.0)); 
 im.rgb = max(im.rgb, 0.0); 
 im.rgb *= im.rgb; 
 im.rgb = im.rgb + neg + pos; 
 return im; 
kernel vec4 _smarttone_contrast_HDR (__sample im, float midAmt) 
 vec3 neg = min(im.rgb, 0.0); 
 vec3 pos = max(im.rgb, 1.0)-1.0; 
 im.rgb = clamp(im.rgb, 0.0, 1.0); 
 float y = dot(im.rgb, vec3(0.3333)); 
 y = sqrt(y); 
 float sat = (im.r-y)*(im.r-y)+(im.g-y)*(im.g-y)+(im.b-y)*(im.b-y); 
 y = y*(1.0-y); 
 im.rgb = sqrt(im.rgb); 
 float a = midAmt*y; 
 float b = -0.5*a; 
 vec3 pix = im.r * vec3(0.299*a) + 
 im.g * vec3(0.587*a) + 
 im.b * vec3(0.114*a) + 
 im.rgb + vec3(b); 
 im.rgb = mix(im.rgb, vec3(0.5), -y*midAmt); 
 im.rgb = mix(im.rgb, pix, 0.8+sat); 
 im.rgb = max(im.rgb, 0.0); 
 im.rgb *= im.rgb; 
 im.rgb = im.rgb + neg + pos; 
 return im; 
kernel vec4 _smarttone_brightness_pos_HDR (__sample c, float gamma) 
 vec3 neg = min(c.rgb, 0.0); 
 vec3 pos = max(c.rgb, 1.0)-1.0; 
 c.rgb = clamp(c.rgb, 0.0, 1.0); 
 vec3 m = 1.0-c.rgb; 
 float a = 0.6; 
 vec4 result = c; 
 result.rgb = 1.0 - (pow(m, vec3(gamma))+a*( ((gamma-1.0)*m*(1.0-m*m))/(gamma*gamma))); 
 c.rgb = pow(c.rgb, vec3(1.0-((min(gamma, 2.95)-1.0)/2.6))); 
 result.rgb = mix(c.rgb, result.rgb, .85); 
 result.rgb = result.rgb+neg+pos; 
 return result; 
kernel vec4 _smarttone_brightness_neg_HDR (__sample c, float gamma) 
 vec3 neg = min(c.rgb, 0.0); 
 c.rgb = max(c.rgb, 0.0); 
 vec3 pix = pow(c.rgb, vec3(gamma)); 
 float lum = dot(c.rgb, vec3(0.39, .5, .11)); 
 vec3 pix2 = lum>0.0 ? c.rgb*pow(lum, gamma)/lum : vec3(0.0); 
 pix = mix(pix, pix2, 0.8) + neg; 
 return vec4(pix, c.a); 
whitePoint
blackPoint
highKey
tonalRange
PISmartToneFilterHDR-histogram
Warning smartToneStatistics will soon need [receiver properties] be non-nil so flash-fired state can be determined.
long-exp-fusion-image.tiff
long-exp-refined-mask-image.tiff
long-exp-ncc-map-image.tiff
long-exp-guide-image.tiff
long-exp-still-image.tiff
long-exp-mask-image.tiff
long-exp-input-image.tiff
PI_LONG_EXPOSURE_DUMP_INTERMEDIATES
kBlendMaskThreshold1
kBlendMaskThreshold0
kNCCEdge1
kNCCEdge0
kNCCBlurHalfSize
@"NSString"8@?0
PI_LONG_EXPOSURE_FUSION_PARAMS
{?={?=qq}{?=qq}}
{?=qq}
inputCutoff
Master/RAW/Linear
PIInverseFakeBoost
post-Adjustments
inputCenter
post-Geometry
pre-Orientation
projectUsingEstimatedCleanAperture
com.apple.quicktime.live-photo.vitality-disabled
projectUsingOriginalSize
projectUsingCleanAperture
resetCleanAperture
perspectiveStraighten
pre-Crop
pre-Geometry
pre-VideoCrossfadeLoop
pre-VideoStabilize
pre-VideoReframe
filterVersion
CIPhotoEffect%@
saturation
hueShift
spread
blue
green
inputCorrections
inputTableImage
inputPointsL
inputPointsB
inputPointsG
inputPointsR
pre-Curves
inputSaturation
inputLightMapImage
inputGuideImage
inputTime
inputTargetImage
CIDynamicRender
CIDynamicFood
Unknown sceneLabel when rendering semantic enhance adjustment
sunsetOrSunrise
inputBoundingBoxArray
inputFaceBoxArray
inputConfidence
SloMo
pre-SloMo
pre-Trim
pre-LivePhotoKeyFrame
pre-Mute
warmth
inputWarmth
inputOrigQ
inputOrigI
inputIsRaw
inputHasFace
inputWarmTint
inputWarmTemp
pre-WB
masterSpace
inputGainMap
CIDepthEffectApplyBlurMap
inputShape
shape
inputLumaNoiseScale
lumaNoiseScale
nativeScale
auxiliaryImageType
PortraitV2
PortraitV2-zeroStrength
CIPortraitEffect%@
inputFullSizeImage
inputGenerateSpillMatte
CIPortraitEffectV2
inputTeethMask
inputHairMask
inputFaceMask
inputBlurMap
inputMatte
inputDisparity
inputFaceLandmarkArray
faceLandmarks
inputEV
inputPower
inputGlassesImage
inputHairImage
inputMatteImage
CIDepthEffectMakeBlurMap
inputOriginalSize
inputAuxDataMetadata
inputCalibrationData
inputShiftmapImage
Missing required depth settings
chinY
chinX
noseY
noseX
rightEyeY
rightEyeX
leftEyeY
leftEyeX
inputChinPosition
inputFaceMidPoint
inputRightEyePosition
inputLeftEyePosition
inputFocusRect
inputAperture
faces
focusRect
MeteorPlusGainMap
GlassesSegmentationMatte
TeethSegmentationMatte
SkinSegmentationMatte
HairSegmentationMatte
PortraitEffectsMatte
Source does not contain depth
Image
Disparity
Intermediate
keepCacheWhenAtOneToOne
PIForwardFakeBoost
inputBoost
ShowOriginalSource
Source
RAW/Linear
unsupported sourceSelect adjustment
Master
skipOrientation
LongExposure
defaultFrameTime
hardCropCleanAperture
mediaComponentType
Video
inputNeutralTint
inputNeutralTemperature
inputNoiseReductionContrastAmount
contrast
inputNoiseReductionSharpnessAmount
sharpness
inputUILuminanceNoiseReductionAmount
inputUIColorNoiseReductionAmount
inputNoiseReductionDetailAmount
inputGamutMapMax
kCGImageSourceShouldExtendRaw
inputSushiLevel
composition != nil
-[PIModernPhotosPipeline _processedRenderNodeForComposition:input:pipelineState:error:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-402.5.140/workspaces/neutrino/PhotoImaging/Pipeline/PIModernPhotosPipeline.m
pipelineState != nil
kernel vec4 levelsNewGammaForP3 (sampler src, sampler LUT)
vec4
p,r;
vec2
c1,c2;
float
p  = sample(src, samplerCoord(src));
vec3 neg = min(p.rgb, 0.0);
vec3 pos = max(p.rgb, 1.0)-1.0;
p.rgb = clamp(p.rgb, 0.0, 1.0);
f = p.r * 255.0 + 256.0;
c1 = vec2(floor(f)+0.5, 0.5);
c2 = vec2(ceil(f)+0.5, 0.5);
r = mix(sample(LUT,samplerTransform(LUT,c1)), sample(LUT,samplerTransform(LUT,c2)), fract(f));
p.r = r.r * 4.0 - 1.0;
f = p.g * 255.0 + 256.0;
c1 = vec2(floor(f)+0.5, 0.5);
c2 = vec2(ceil(f)+0.5, 0.5);
r = mix(sample(LUT,samplerTransform(LUT,c1)), sample(LUT,samplerTransform(LUT,c2)), fract(f));
p.g = r.g * 4.0 - 1.0;
f = p.b * 255.0 + 256.0;
c1 = vec2(floor(f)+0.5, 0.5);
c2 = vec2(ceil(f)+0.5, 0.5);
r = mix(sample(LUT,samplerTransform(LUT,c1)), sample(LUT,samplerTransform(LUT,c2)), fract(f));
p.b = r.b * 4.0 - 1.0;
p.rgb = max(p.rgb, 0.0);
p.rgb = p.rgb + pos + neg;
return p;
-[PILevelsFilter _LUTImage]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-402.5.140/workspaces/neutrino/PhotoImaging/Adjustments/PILevelsFilter.m
Mirror
PhotosComposition
+[PISchema registerPhotosSchema]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-402.5.140/workspaces/neutrino/PhotoImaging/Pipeline/PISchema.m
failed to register %@: %@
+[PISchema photosCompositionSchema]
Failed to register schema %@: %@
Composition
WhiteBalance~1.0
Vignette~1.0
VideoStabilize~1.0
VideoReframe~1.0
VideoPosterFrame~1.0
VideoCrossfadeLoop~1.0
Trim~1.0
SourceSelect~1.0
SmartTone~1.0
SmartColor~1.0
SmartBlackAndWhite~1.0
SlowMotion~1.0
Sharpen~1.0
SemanticEnhance~1.0
SelectiveColor~1.0
Retouch~1.0
com.apple.photo:Source~1.0
RedEye~1.0
RawNoiseReduction~1.0
RAW~1.0
PortraitVideo~1.0
PortraitEffect~1.0
Orientation~1.0
NoiseReduction~1.0
Mute~1.0
LivePhotoKeyFrame~1.0
Levels~1.0
HighResolutionFusion~1.0
Grain~1.0
Effect3D~1.0
Effect~1.0
DepthEffect~1.0
Definition~1.0
Debug~1.0
Curves~1.0
CropStraighten~1.0
AutoLoop~1.0
ApertureRedEye~1.0
required
contents
+[PISchema portraitVideoSchema]
bool
default
opaque
number
minimum
maximum
Adjustment
+[PISchema semanticEnhance]
properties
compound
content
array
enum
values
+[PISchema debugSchema]
ui_minimum
ui_maximum
Debug
+[PISchema videoCrossfadeLoopSchema]
+[PISchema videoStabilizeSchema]
gyro
pixel
+[PISchema videoReframeSchema]
+[PISchema selectiveColorSchema]
+[PISchema curvesSchema]
+[PISchema levelsSchema]
+[PISchema whiteBalanceSchema]
identity
grayStrength
+[PISchema noiseReductionSchema]
+[PISchema definitionSchema]
+[PISchema orientationSchema]
+[PISchema vignetteSchema]
+[PISchema retouchSchema]
+[PISchema apertureRedEyeSchema]
+[PISchema redEyeSchema]
iPhone
+[PISchema effectSchema]
+[PISchema portraitEffectSchema]
+[PISchema effect3DSchema]
+[PISchema depthEffectSchema]
+[PISchema highResFusionSchema]
+[PISchema autoLoopSchema]
+[PISchema videoPosterFrameSchema]
+[PISchema muteSchema]
+[PISchema livePhotoKeyFrameSchema]
+[PISchema slomoSchema]
+[PISchema trimSchema]
+[PISchema cropSchema]
+[PISchema sharpenSchema]
+[PISchema grainSchema]
+[PISchema smartBlackAndWhiteSchema]
+[PISchema smartColorSchema]
+[PISchema smartToneSchema]
+[PISchema rawNoiseReductionSchema]
+[PISchema rawSchema]
+[PISchema sourceSelectSchema]
enable_perspective_correction
Capture
minRotateCameraPerspectiveCorrection
minVerticalCameraPerspectiveCorrection
minHorizontalCameraPerspectiveCorrection
maxCameraRotatePerspectiveCorrection
maxCameraHorizontalPerspectiveCorrection
maxCameraVerticalPerspectiveCorrection
maxCameraAutoStraightenCorrection
com.apple.mobileslideshow.reframe.type.photo.perspective
com.apple.mobileslideshow.reframe.type.photo.horizon
com.apple.mobileslideshow.reframe.photo.eligible-pass
com.apple.mobileslideshow.reframe.photo.eligible-fail
com.apple.mobileslideshow.reframe.video.eligible-pass
com.apple.mobileslideshow.reframe.video.eligible-fail
v16@?0@"PIAdjustmentController"8
PICompositionFinalizer-overcaptureImageProperties
PICompositionFinalizer-primaryImageProperties
horizon
perpsective
reframe
-[PICropAutoCalculator submit:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-402.5.140/workspaces/neutrino/PhotoImaging/Autocalculators/PICropAutoCalculator.m
{CGRect={CGPoint=dd}{CGSize=dd}}
%@AutoCropEvaluation-txt.DBG
result
autoCrop
belowMinimum
limitExceeded
straightenAngleInDegreesCCW
CIAutoStraighten
filterOptions
maxAutoStraighten
PICropAutoCalculator-faceDetection
-[PICropAutoCalculator undoExifOrientation:error:]
Source geometry has 0 size
PICropAutoCalculator-imageProperties
+[PICropAutoCalculator(Utilities) updateCropAdjustment:after:error:]
v16@?0@"PICropAdjustmentController"8
PICropAutoCalculator-getAfterGeometry
PICropAutoCalculator-getBeforeGeometry
PICropAutoCalculator-stitchedOvercaptureRect
v16@?0@"PISourceSelectAdjustmentController"8
v16@?0@"PIAutoLoopAdjustmentController"8
iptLumHueSatTable
add_gaussian
CIConstantColorGenerator
iptToSRGB
srgbToIPT
kernel vec4 iptLumHueSatTable(sampler image, __table sampler hueSatLumTable)
vec4 im = sample(image, samplerCoord(image)) ;
vec4 result = im;
float hueIdx = 359.0 * 0.5 * (atan(im.b, im.g)/3.1416 + 1.0);
hueIdx = clamp(hueIdx, 0.0, 359.0) + 0.5;
float hueChange = (sample(hueSatLumTable, samplerTransform(hueSatLumTable, vec2(hueIdx,0.5))).r);
float satChange = (sample(hueSatLumTable, samplerTransform(hueSatLumTable, vec2(hueIdx,0.5))).g);
float lumChange = (sample(hueSatLumTable, samplerTransform(hueSatLumTable, vec2(hueIdx,0.5))).b);
float chroma = sqrt(im.g*im.g+im.b*im.b) ;
chroma *= satChange ;
float hue = atan(im.b, im.g) + hueChange ;
vec3 adjustIm = im.rgb;
float hueAngle = hue  ;
lumChange = mix(1.0, lumChange, clamp(chroma,-0.7,0.7));
adjustIm.r *= lumChange;
adjustIm.g = chroma * cos(hueAngle) ;
adjustIm.b = chroma * sin(hueAngle) ;
result.rgb = adjustIm.rgb;
result.a = im.a ;
return result ;
kernel vec4 srgbToIPT(sampler image){
vec4 im = sample(image, samplerCoord(image));
vec3 lms, ipt;
lms = im.r * vec3(0.3139902162, 0.155372406, 0.017752387) +
im.g * vec3(0.6395129383, 0.7578944616, 0.109442094) +
im.b * vec3(0.0464975462, 0.0867014186, 0.8725692246);
lms = sign(lms)*pow(abs(lms), vec3(0.43, 0.43, 0.43));
ipt = lms.r * vec3(0.4, 4.455, 0.8056) +
lms.g * vec3(0.4, -4.851, 0.3572) +
lms.b * vec3(0.2, 0.3960, -1.1628);
return vec4(ipt, im.a);
kernel vec4 iptToSRGB(sampler image){
vec4 im = sample(image, samplerCoord(image));
vec3 lms, rgb;
lms = im.rrr +
im.g * vec3(0.09756893,-0.11387649,0.03261511) +
im.b * vec3(0.20522644, 0.13321716,  -0.67688718);
lms = sign(lms)*pow(abs(lms), vec3(1.0/.43));
rgb = lms.r * vec3( 5.472212058380287,  -1.125241895533569,   0.029801651173470) +
lms.g * vec3(-4.641960098354470, 2.293170938060623, -0.193180728257140) +
lms.b * vec3(0.169637076827974,  -0.167895202223709, 1.163647892783812);
return vec4(rgb, im.a);
kernel vec4 add_gaussian(sampler srcTable, float tableSize, float hueAmplitude, float satAmplitude, float lumAmplitude, float gaussX, float gaussSigmaSquared) {
vec2 d = destCoord();
vec4 src = sample(srcTable, samplerCoord(srcTable));
float x = d.x / (tableSize - 1.0);
float dist = min(min(abs(x - gaussX), abs(x - 1.0 - gaussX)), abs(x + 1.0 - gaussX));
float p = -((dist * dist) / (2.0 * gaussSigmaSquared));
float ep = exp(p);
float hue = hueAmplitude * ep;
float sat = satAmplitude * ep;
float lum = lumAmplitude * ep;
float h = clamp(src.r + hue, -1.0, 1.0);
float s = clamp(src.g + sat, -1.0, 1.0);
float l = clamp(src.b + lum, -1.0, 1.0);
return vec4(h,s,l,1.0);
CIPerspectiveTransform
inputBottomRight
inputBottomLeft
inputTopRight
inputTopLeft
pipelineState
Could not get the input image properties
Could not get the input geometry
error != nil
-[PIVideoReframeNode _evaluateImageGeometry:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-402.5.140/workspaces/neutrino/PhotoImaging/Render/PIVideoReframeNode.m
Failed to get input geometry
-[PIVideoReframeNode _evaluateVideoProperties:]
-[PIVideoReframeNode initWithSettings:inputs:]
showStabilizationWatermark
invalid crop rect
PIVideoReframeNode.m
-[PIVideoReframeMetadataExtractor extractMetadata]
PIVideoReframeMetadataExtractor.mm
mdataGroup.items.count == 1
0 && "unexpected metadata identifier"
0 && "unexpected metadata data type"
err == noErr
-[PIVideoReframeMetadataExtractor motionBlurVectorFromMetadata:]
-[PIVideoReframeMetadataExtractor overwriteTrackingMetadataWithPlist:]
type >= 0
Confidence
DogBody
CatBody
HumanFace
HumanBody
Type
Subjects
Time
Invalid plist at path: %@
init is not a valid initializer
convertFromYIQToRGB
convertFromRGBToYIQ
kernel vec4 convertFromRGBToYIQ(sampler src, float gamma)
vec4 pix ;
vec3 pix2;
pix = sample(src, samplerCoord(src));
pix.rgb = sign(pix.rgb)*pow(abs(pix.rgb), vec3(1.0/gamma)) ;
pix2.rgb = pix.r * vec3(0.299,  0.595716,  0.211456) +
pix.g * vec3(0.587, -0.274453, -0.522591) +
pix.b * vec3(0.114, -0.321263,  0.311135);
return vec4(pix2, pix.a);
kernel vec4 convertFromYIQToRGB(sampler src, float gamma)
vec4 color, pix;
pix = sample(src, samplerCoord(src));
color.rgb = pix.rrr +
pix.g * vec3(0.956296, -0.272122, -1.10699) +
pix.b * vec3(0.621024, -0.647381, 1.70461);
color.rgb = sign(color.rgb)*pow(abs(color.rgb), vec3(gamma, gamma, gamma) );
color.a = pix.a;
return color;
kernel vec4 whiteBalance(sampler image, float grayY, float grayI, float grayQ, float strength)
vec4 im = sample(image, samplerCoord(image)) ;
vec2 grayOffset = vec2(grayI, grayQ) ;
vec4 result ;
float newStrength = 1.0 + (strength-1.0)*(1.0-im.r) ;
result.r = im.r ;
result.gb = im.gb + newStrength*grayOffset ;
float damp = max(min(1.0, im.r/(grayY+0.00001)),0.0) ;
result.rgb = mix(im.rgb, result.rgb, damp) ;
result.a = im.a ;
return result ;
kernel vec4 gHDRtoPP(sampler image)
vec4 pix ;
vec3 pix2;
pix = sample(image, samplerCoord(image));
pix2 = pix.r * vec3(0.615429622407401,   0.114831839141528,   0.011544126697221) +
pix.g * vec3(0.367479646665836,   0.797943554457996,   0.064077744191180) +
pix.b * vec3(  0.016956659608091,   0.087783443422360,   0.924405601458102);
return vec4(pix2, pix.a);
kernel vec4 PPtogHDR(sampler image)
vec4 pix ;
vec3 pix2;
pix = sample(image, samplerCoord(image));
pix2 = pix.r * vec3(1.777445503202045,  -0.255296595099306,  -0.004500433755654) +
pix.g * vec3( -0.822224875430495,   1.380948853784730,  -0.085456231694984) +
pix.b * vec3(0.045475917061484,  -0.126454737973025,   1.089973874037625);
return vec4(pix2, pix.a);
-[PINeutralGrayWhiteBalanceFilter outputImage]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-402.5.140/workspaces/neutrino/PhotoImaging/Adjustments/PINeutralGrayWhiteBalanceFilter.m
-[PISmartToneAutoCalculator submit:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-402.5.140/workspaces/neutrino/PhotoImaging/Autocalculators/PIAutoCalculators.mm
PISmartToneAutoCalculator
v8@?0
-[PISmartColorAutoCalculator submit:]
-[PIRedEyeAutoCalculator submit:]
PIRedEyeAutoCalculator-getLensInfo
touchDiameter
locationY
locationX
/masterSpace
{CGRect={CGPoint=dd}{CGSize=dd}}44@?0i8{CGRect={CGPoint=dd}{CGSize=dd}}12
1536 x 1536 noise grain image prov
kernel vec4 _grainGenCombineHDR (__sample r, __sample g, __sample b, __sample a)
{ return vec4(r.x, g.x, b.x, a.x); }
CIUnsharpMask
v56@?0^v8Q16Q24Q32Q40Q48
generateNoiseImage_block_invoke
PIPhotoGrainHDR.m
width==512*3
height==512*3
x==0
y==0
kernel vec2 _paddedTile2(vec4 k) { return fract(destCoord() * k.zw) * k.xy + vec2(1.0); }
kernel vec4 _grainBlendAndMixHDR(__sample img, __sample grainImage, float contrast, float mixAmount)
  vec3 rgb = img.rgb;
  float luminance = clamp(dot(rgb, vec3(.333333)), 0.0, 1.0); 
  float gamma = 4.01 - 2.0*luminance;
  rgb = sign(rgb) * pow(abs(rgb), vec3(1.0/gamma));
  float grain = grainImage.r - 0.5;
  float mult = contrast * grain;
  rgb += (max(luminance, 0.5) * mult * (1.0-luminance));
  rgb = sign(rgb) * pow(abs(rgb), vec3(gamma));
  rgb = min(rgb, 12.0);
  return mix(img, vec4(rgb,img.a), mixAmount);
kernel vec4 _blendGrainsHDR(__sample isoImages, float log10iso)
  vec4 c = isoImages; 
  float mix10_50    = mix(c.r, c.g, log10iso*1.43067655809 
                                           - 1.43067655809); 
  float mix50_400   = mix(c.g, c.b, log10iso*1.10730936496 
                                           - 1.88128539659); 
  float mix400_3200 = mix(c.b, c.a, log10iso*1.10730936496 
                                           - 2.88128539659); 
  float v = compare(log10iso - 1.69897000434,                     mix10_50,                     compare(log10iso - 2.60205999133,                             mix50_400,                             mix400_3200)); 
  return vec4(v,v,v,1.0);
%@-%d-%ld.tiff
cgImage != NULL
+[PIImageIO writeCGImage:fileURL:options:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-402.5.140/workspaces/neutrino/PhotoImaging/Util/PIImageIO.m
fileURL != nil
Failed to write image to file %@
Successfully wrote image to file %@
+[PIImageIO writeCGImage:fileURL:]
GU %@ %@
Unhandled bit depth: %ld
image != nil
+[PIImageIO writeImage:fileURL:]
+[PIApertureRedEyeProcessorKernel processWithInputs:arguments:output:error:]
PIApertureRedEyeFilter.mm
output.format == kCIFormatRGBAf
Sensitivity
+[PIApertureRedEyeProcessorKernel convertFixed16:toFloat:count:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-402.5.140/workspaces/neutrino/PhotoImaging/Adjustments/ApertureRedEye/PIApertureRedEyeFilter.mm
Bad fixed 16 to float conversion
+[PIApertureRedEyeProcessorKernel convertFloat:toFixed16:count:]
Bad float to fixed 16 conversion
inputSensitivity
portraitStrength
capturedPortraitMajorVersion
capturedPortraitMinorVersion
portraitDisableStage
<%@:%p aperture:%f minimumAperture:%@ maximumAperture:%@ portraitStrength:%f SDOFRenderingVersion:%lu portraitMajorVersion:%lu portraitMinorVersion:%lu>
+[PIValuesAtCapture valuesAtCaptureFromImageProperties:error:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-402.5.140/workspaces/neutrino/PhotoImaging/Autocalculators/PIPortraitAutoCalculator.m
We only know up to sDOF version %d. Found: %d
maxSDOFRenderingVersionSupported
Depth data version mismatch, asset has %@ but we can only handle %@
depthData:DepthDataVersion
Missing auxiliary metadata
Failed to load auxiliary data
Missing camera calibration data
Low quality depth data is not supported
Unfiltered depth data is not supported
Failed to load depth data
Portrait was previously applied.
v16@?0@"NUResponse"8
PIPortraitAutoCalculator-faceDetect
-[PIPortraitAutoCalculator submit:]
capturedPortraitStrength
capturedAperture
PIPortraitAutoCalculator-getValuesAtCapture-imageProperties
metadata != nil
+[PIPortraitAutoCalculator portraitInfoDictionaryFromCameraMetadata:]
faceObservations != nil
+[PIPortraitAutoCalculator portraitEffectInfoDictionaryFromFaceObservations:orientation:valuesAtCapture:]
NUOrientationIsValid(orientation)
rightPupil
leftPupil
innerLips
outerLips
medianLine
noseCrest
nose
rightEyebrow
leftEyebrow
rightEye
leftEye
faceContour
allPoints
faceOrientationIndex
faceJunkinessIndex
faceBoundingBox
+[PIPortraitAutoCalculator depthEffectInfoDictionaryFromFaceObservations:focus:valuesAtCapture:lumaNoiseScale:orientation:]
maximumAperture
minimumAperture
Insufficient number of landmark points
+[PIPortraitAutoCalculator portraitInfoDictionaryFromFaceObservations:metadata:orientation:valuesAtCapture:]
-[PIDepthEffectApertureAutoCalculator submit:]
PIDepthEffectApertureAutoCalculator-getValuesAtCapture-imageProperties
+[PIPipelineFilters oneShotPortraitV2ExportFilter]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-402.5.140/workspaces/neutrino/PhotoImaging/Pipeline/PIPipelineFilters.m
Could not construct oneShotPortraitV2ExportFilter filter from inline source
/PortraitV2
/PortraitV2-zeroStrength
return input;
+[PIPipelineFilters spatialOvercaptureVideoSourceFilter]
Could not construct overcaptureSourceFilter filter from inline source
return Source(composition.sourceSpatialOvercapture, {  })
+[PIPipelineFilters primarySourceFilter]
Could not construct primarySourceFilter filter from inline source
return Source(composition.source, { 'skipOrientation' : true })
+[PIPipelineFilters overcaptureSourceFilter]
return Source(composition.sourceSpatialOvercapture, { 'skipOrientation' : true })
+[PIPipelineFilters autoloopStabilizedVideoFilter]
Could not construct autoloopStabilizedVideoFilter filter from inline source
/AutoLoop/Output
/AutoLoop/StabilizedVideo
ResetTagInput('/AutoLoop/Output', GetTag('/AutoLoop/StabilizedVideo'));
return input;
+[PIPipelineFilters applyOrientationFilter]
Could not construct pipeline filter from source: %@
var output = input;
var orientation = composition.orientation;
if (orientation) {
    output = Orient(input, orientation.value);
return output;
+[PIPipelineFilters stopAtTagIncludeOrientationFilter:]
Could not construct stopAtTagIncludeOrientationFilter from inline source
@"NURenderNode"40@?0@"NURenderPipelineHelper"8@"NUComposition"16@"NURenderNode"24^@32
/Orientation
/pre-Orientation
var tagNode = GetTag('%@');
if (tagNode) {
    ResetTagInput('/pre-Orientation', tagNode);
return GetTag('/Orientation');
+[PIPipelineFilters stopAtTagIncludeGeometryFilter:]
Could not construct stopAtTagIncludeGeometryFilter from inline source
var tagNode = GetTag('%@');
if (tagNode) {
    ResetTagInput('/pre-Geometry', tagNode);
return GetTag('/post-Geometry');
/perspectiveStraighten
+[PIPipelineFilters perspectiveStraightenWithoutCropFilter]
Could not construct perspectiveStraightenWithoutCropFilter filter from inline source
/Crop
var preCrop = GetTag('/perspectiveStraighten');if (preCrop) {   ResetTagInput('/Crop', preCrop);
}return input;
+[PIPipelineFilters orientationAsMetaDataFilter]
return null;
+[PIPipelineFilters noOrientationFilter]
Could not construct noOrientationFilter filter from inline source
var preOrientation = GetTag('/pre-Orientation');ResetTagInput('/Orientation', preOrientation);
return input;
+[PIPipelineFilters noGeometryFilter]
Could not construct noGeometry filter from inline source
let preGeometry = GetTag('/pre-Geometry');ResetTagInput('/post-Geometry', preGeometry);
return input;
+[PIPipelineFilters stripAllTimeAdjustmentsFilter]
Could not construct stripAllTimeAdjustmentsFilter filter from inline source
/pre-Trim
var image = GetTag('pre-Trim');ResetTagInput('Trim', image);
image = GetTag('pre-SloMo');ResetTagInput('SloMo', image);
return input;
+[PIPipelineFilters iosCropToolFilter]_block_invoke
/post-Adjustments
let preGeometry = GetTag('/pre-Geometry');ResetTagInput('/post-Adjustments', preGeometry);
return input;
+[PIPipelineFilters noCropFilter]
Could not construct noCropFilter filter from inline source
/pre-Crop
var preCrop = GetTag('/pre-Crop');ResetTagInput('/pre-Orientation', preCrop);
return input;
+[PIPipelineFilters noMuteFilter]
Could not construct noMuteFilter filter from inline source
var preMute = GetTag('pre-Mute');ResetTagInput('Mute', preMute);
return GetTag('Image');
+[PIPipelineFilters noTrimFilter]
Could not construct noTrimFilter filter from inline source
/SloMo
var output = input
var preTrim = GetTag('/pre-Trim');ResetTagInput('/SloMo', preTrim);
let slomo = composition.slomo
if (slomo) {
    var startTime = TimeMake(slomo.start, slomo.startScale)
    var endTime = TimeMake(slomo.end, slomo.endScale)
    output = SloMo(input, startTime, endTime, slomo.rate)
return output;
+[PIPipelineFilters noRedEyeFilter]
Could not construct noRedEye filter from inline source
/post-RedEye
/pre-RedEye
var source = GetTag('/pre-RedEye');ResetTagInput('/post-RedEye', source);
return GetTag('/post-RedEye');
var sourceSettings = { 'skipOrientation' : true }; 
var raw = composition.raw; 
if (raw) { 
  sourceSettings['inputDecoderVersion'] = raw.inputDecoderVersion; 
  var sushiLevel = raw.inputSushiLevel;
  if (sushiLevel) {
    sourceSettings['kCGImageSourceShouldExtendRaw'] = sushiLevel;
  return Source(composition.source, sourceSettings); 
} else {
  return GetTag("/ShowOriginalSource");
+[PIPipelineFilters rawSourceFilterIncludingOrientation]_block_invoke
expected RAW in rawSourceFilterIncludingOrientation
var sourceSettings = { }; 
var raw = composition.raw; 
if (raw) { 
  sourceSettings['inputDecoderVersion'] = raw.inputDecoderVersion; 
  var sushiLevel = raw.inputSushiLevel;
  if (sushiLevel) {
    sourceSettings['kCGImageSourceShouldExtendRaw'] = sushiLevel;
} else { throw "expected RAW in rawSourceFilterIncludingOrientation"; }
return Source(composition.source, sourceSettings); 
Raw/Linear
var sourceSettings = { 'skipOrientation': true };
var rawAdjustment = composition.raw
if (rawAdjustment) {
    sourceSettings['inputDecoderVersion'] = rawAdjustment.inputDecoderVersion
var image = Source(composition.source, sourceSettings)
var rawImage = GetTagInput('RAW/Linear')
if (rawImage) {
    image = rawImage
    image = Filter('PIForwardFakeBoost', {'inputImage' : image, 'inputBoost' : 1.0,})
var orientation = composition.orientation
if (orientation) { image = Orient(image, orientation.value) }
return image
/pre-Adjustments
var source = GetTag('/pre-Adjustments');
ResetTagInput('/pre-Crop', source);
source = GetTag('/Crop');
orientation = source.geometry.orientation;
source = Orient(source, orientation);
return source;
var tagNode = GetTag('/pre-Adjustments');
orientation = tagNode.geometry.orientation;
var node = Orient(tagNode, orientation);
return node;
+[PIPipelineFilters(Debug) socPseudoColorFilter]_block_invoke
let output = input;
var image = GetTag('pre-VideoReframe');
if (image) {
    var videoReframe = composition.videoReframe;
    if (videoReframe && videoReframe.enabled) {
        image = VideoReframe(image, videoReframe);
        image = Filter(`CIColorMatrix`, {'inputImage' : image, 'inputBVector' : CIVectorMake(1,1,1,0)});
        ResetTagInput('VideoReframe', image);
    }
else { // image or live-photo { 
    var crop = composition.cropStraighten;
    var image = GetTag('pre-Crop');
    if (crop && crop.smart == 1 && image) {
        if (crop.pitch || crop.yaw) {
            var perspectiveTransform = PerspectiveTransformMake(crop.angle, crop.pitch, crop.yaw, image.geometry);
            image = Transform(image, perspectiveTransform);
        } else {
            var straightenTransform = StraightenTransformMake(crop.angle, image.geometry);
            image = Transform(image, straightenTransform);
        }
        // Apply the crop rect to the incoming image;
        image = Crop(image, crop.xOrigin, crop.yOrigin, crop.width, crop.height);
        image = Filter(`CIColorMatrix`, {'inputImage' : image, 'inputBVector' : CIVectorMake(1,1,1,0)});
        ResetTagInput('Crop', image);
    }
return output;
none
PIRedEye
inputDestinationImage
PIApertureRedEyeFilter
PILongExposureFusion
inputMaskImage
inputAlignmentTransform
inputAlignmentExtent
inputRenderScale
inputVideoScale
inputStillImage
AutoLoop/LongExposureMotion
transform
extent
error != NULL
-[NURenderPipelineHelper(PI) portraitVideoDebugDetectedObjects:source:cinematographyState:monochrome:error:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-402.5.140/workspaces/neutrino/PhotoImaging/Pipeline/PIPhotosPipelineHelper.m
Unexpected source type
-[NURenderPipelineHelper(PI) portraitVideo:disparityInput:disparityKeyframes:apertureKeyframes:debugMode:error:]
@"NSMutableArray"16@?0@"NSArray"8
-[NURenderPipelineHelper(PI) videoCrossfadeLoop:crossfadeAdjustment:error:]
-[NURenderPipelineHelper(PI) videoReframe:reframes:error:]
recipe != nil
NSDictionary * _Nonnull PIAutoLoopRecipeForFlavor(NSDictionary *__strong _Nonnull, PIAutoLoopFlavor)
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-402.5.140/workspaces/neutrino/PhotoImaging/AutoLoop/PIAutoLoop.m
CGRect PIAutoLoopRecipeGetCropRect(NSDictionary *__strong _Nonnull)
origin_y
origin_x
BOOL PIAutoLoopRecipeHasGoodStabilization(NSDictionary *__strong _Nonnull)
frameTransform != nil
CMTime PIAutoLoopRecipeFrameTransformGetTime(NSDictionary *__strong)
frameTransform_rawTime
matrix_float3x3 PIAutoLoopRecipeFrameTransformGetHomography(NSDictionary *__strong)
frameTransform_homography
NSDictionary * _Nonnull PIAutoLoopRecipeGetInstructionAtTime(NSDictionary *__strong _Nonnull, CMTime)
CMTIME_IS_VALID(time)
q24@?0@"NSDictionary"8@"NSDictionary"16
loopFrameData_presTime
loopRecipe_frameInstructions
CMTime PIAutoLoopRecipeGetFrameDuration(NSDictionary *__strong _Nonnull)
NSDictionary * _Nullable PIAutoLoopRecipeGetFlavorParameters(NSDictionary *__strong _Nonnull, PIAutoLoopFlavor)
CMTimeRange PIAutoLoopRecipeGetTimeRangeForFlavor(NSDictionary *__strong _Nonnull, PIAutoLoopFlavor)
-[PICurvesAutoCalculator computeCurvesForImageHistogram:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-402.5.140/workspaces/neutrino/PhotoImaging/Autocalculators/PICurvesAutoCalculator.m
completion != nil
-[PICurvesAutoCalculator submit:]
softlink:r:path:/System/Library/Frameworks/JavaScriptCore.framework/JavaScriptCore
_PITapToTrackRenderResult
PITapToTrackResult
NURenderResult
NSObject
PITapToTrackRenderJob
PITapToTrackRequest
PIPhotoEffectHDR
PIPhotoEffectBlackAndWhiteHDR
PIPhotoEffectNoirHDR
PIPhotoEffectChromeHDR
PIPhotoEffectFadeHDR
PIPhotoEffectInstantHDR
PIPhotoEffectMonoHDR
PIPhotoEffectProcessHDR
PIPhotoEffectTonalHDR
PIPhotoEffectTransferHDR
PIPhotoEffectStageMonoHDR
PIPhotoEffect3DHDR
PIPhotoEffect3DBlackAndWhiteHDR
PIPhotoEffect3DVividHDR
PIPhotoEffect3DVividWarmHDR
PIPhotoEffect3DVividCoolHDR
PIPhotoEffect3DDramaticHDR
PIPhotoEffect3DDramaticCoolHDR
PIPhotoEffect3DDramaticWarmHDR
PIPhotoEffect3DSilverplateHDR
PIPhotoEffect3DNoirHDR
PIColorNormalizationFilter
PIReframeRules
PISemanticEnhanceAdjustmentController
_PIDisparitySampleResult
PIDisparitySampleResult
PIDisparitySampleJob
PIDisparitySampleRequest
PILocalContrastHDR
PIHighKey
PICompositionController
NSCopying
PICompositionSerializer
PICompositionSerializationResult
PICompositionSerializerMetadata
PILevelsFilterHDR
PIColorNormalizationAutoCalculator
NUTimeBased
PIFalseColorHDRDebug
PIAutoCalculatorUtils
PIAutoLoopExportRequest
PIAutoLoopExportClient
PIAutoLoopAnalysisRequest
PIAutoLoopAnalysisClient
PILongExposureRegistrationRequest
PIPortraitVideoProcessor
PIPortraitVideoFilter
PIAutoLoopAdjustmentController
PIVideoCrossfadeLoopNode
PILivePhotoKeyFrameAdjustmentController
PIVignetteAdjustmentController
PIAdjustmentController
PIPortraitVideoRenderNode
PIFaceObservationCache
PISourceSelectAdjustmentController
PIScalarKeyframe
PIDictionaryRepresentable
PIReframeSubject
NSSecureCoding
NSCoding
PIPortraitVideoAdjustmentController
PIReframeKeyframe
PIReframeKeyframeSequence
PILongExposureFusionAutoCalculator
PIEffectAdjustmentController
PIAutoLoopAutoCalculator
PIMakerNoteUtilities
PITempTintFilter
PITimeVaryingPipelineStateSetting
PIAutoLoopAnalysisResult
_PIAutoLoopAnalysisResult
PIAutoLoopAnalysisJob
ICFlowControl
_PIVideoStabilizeFlowControl
_PIVideoStabilizeResult
PIVideoStabilizeResult
PIVideoStabilizeRenderJob
PIVideoStabilizeRequest
PIFaceBalanceAutoCalculator
_PIWhiteColorCalculator
PIWhiteBalanceAutoCalculator
GrayColorResult
RGBResult
PIDefinitionFilter
PISmartBlackAndWhiteAdjustmentController
PICompositionExporterMetadataConverter
PICompositionExporterDefaultMetadataConverter
PICompositionExportImagePrepareResult
PICompositionExportResult
PICompositionExportAuxiliaryResult
PICompositionExportDataResult
PICompositionExporterOptions
PICompositionExporterVideoOptions
PICompositionExporterImageOptions
PICompositionExporterAuxiliaryOptions
PICompositionExporter
GUBilateralConvolution
GUBWBilateralConvolution
PIBilateralFilter
PIPhotoEditAdjustmentsVersion
PINoiseReductionAdjustmentController
PISmartColorFilterHDR
PrivateSmartColorHDR
PIVideoCrossfadeLoopAdjustmentController
PIPortraitAdjustmentController
PICompositionNoOpRemoval
PIOrientationAdjustmentController
PIPortraitVideoMetadataSample
PIDefinitionAdjustmentController
PIHighKeyHDR
PIPerspectiveAutoCalculator
PIFaceObservingAutoCalculator
PIRawNoiseReductionAdjustmentController
PICropAdjustmentController
PIRawAdjustmentController
PISmartToneAdjustmentController
PIAutoLoopKernels
PIPhotoEditHelper
PIAdjustmentConstants
PIForwardFakeBoost
PIInverseFakeBoost
PICoreImageUtilities
PICompositionSerializerConstants
PICurvesFilterHDR
PILongExposureAccumulator
PILongExposureRegistrationResult
_PILongExposureRegistrationResult
PILongExposureRegistrationJob
PIRAWTempTintSampler
PITagColorSampler
PIRedEye
PIVideoPosterFrameAdjustmentController
PICompositionSerializerFormatVersion
PICaptureDebugUtilities
PISlomoAdjustmentController
PIPortraitVideoDebugDetectionsRenderNode
PIDebugAdjustmentController
PIRAWFaceBalance
PICurvesLUTFilter
PICurvesFilter
PIColorBalanceFilter
PIPhotosPipelineHDRFilters
PIHighResFusionAdjustmentController
PIGlobalSettings
PIAutoLoopExportJob
PIJSRenderPipeline
PhotosPipeline
PIApertureRedEyeAutoCalculator
PILocalLightMapPrepareHDR
PILocalLightFilterHDR
PrivateLocalLightHDR
PIMsgImageBuffer
PISmartBlackAndWhiteAutoCalculator
PISmartBlackAndWhiteHDR
PrivateSmartBlackAndWhite
PITrimAdjustmentController
PIRedEyeAdjustmentController
PILevelsAutoCalculator
PILevelsLuminanceAutoCalculator
PILevelsRGBAutoCalculator
AdjustmentExtensions
PICurveControlPoint
PISmartToneFilterHDR
PrivateSmartToneHDR
PILongExposureFusion
PIModernPhotosPipeline
PILevelsFilter
PISchema
PICompositionFinalizer
PICompositionFinalizerResult
PIVideoStabilizeAdjustmentController
PICropAutoCalculator
Utilities
PISmartColorAdjustmentController
PISelectiveColorFilter
PIVideoReframeNode
PIVideoReframe
PIVideoReframeTimedMetadata
PIVideoReframeMetadataExtractor
PINeutralGrayWhiteBalanceFilter
PISmartToneAutoCalculator
PISmartColorAutoCalculator
PIRedEyeAutoCalculator
PIManualRedEyeAutoCalculator
PISourceSampler
PIPhotoGrainHDR
PIImageIO
PISharpenAdjustmentController
PIApertureRedEyeProcessorKernel
PIApertureRedEyeFilter
PIValuesAtCapture
PIPortraitAutoCalculator
PIDepthEffectApertureAutoCalculator
PIPipelineFilters
Debug
PIWhiteBalanceAdjustmentController
PIDepthAdjustmentController
PIVideoReframeAdjustmentController
PICurvesAutoCalculator
PICurvesLuminanceAutoCalculator
PICurvesRGBAutoCalculator
T@"PTCinematographyTrack",R,N,V_completedTrack
statistics
T@"<NURenderStatistics>",R
hash
TQ,R
superclass
T#,R
description
T@"NSString",R,C
debugDescription
_completedTrack
T@"PTCinematographyTrack",R,N
completedTrack
isEqual:
class
self
performSelector:
performSelector:withObject:
performSelector:withObject:withObject:
isProxy
isKindOfClass:
isMemberOfClass:
conformsToProtocol:
respondsToSelector:
retain
release
autorelease
retainCount
zone
initWithCompletedTrack:
.cxx_destruct
T{?=qiIq},N,V_startTime
T{CGPoint=dd},N,V_normalizedImagePoint
T@?,C,N,V_progressHandler
T@"PTCinematographyTrack",&,N,V_completedTrack
TB,V_clientRequestedStop
_clientRequestedStop
_progressHandler
_normalizedImagePoint
_startTime
wantsRenderStage
wantsCompleteStage
wantsOutputVideo
scalePolicy
result
prepare:
_reportProgressAtTime:rect:confidence:
startTime
setStartTime:
normalizedImagePoint
setNormalizedImagePoint:
progressHandler
setProgressHandler:
setCompletedTrack:
clientRequestedStop
setClientRequestedStop:
responseQueue
finalizeTrack
stopReadingFrames
addDetectionForNextFrameAt:colorBuffer:disparityBuffer:
addDetectionAndStartTrackingRect:time:colorBuffer:disparityBuffer:
disparityBuffer
rect
getRectForPoint:colorBuffer:
colorBuffer
nextFrame
startReadingFrames:atTime:error:
initWithAsset:
outputVideo
initWithCommandQueue:
newCommandQueue
missingError:object:
metalDevice
mainDevice
currentPlatform
device
componentsJoinedByString:
callStackSymbols
stringWithFormat:
oneToOneScalePolicy
initWithComposition:startTime:pointToTrack:
copyWithZone:
newRenderJob
mediaComponentType
submit:
submitGeneric:
setPipelineFilters:
arrayWithObjects:count:
kernel
kernelWithString:
T@"CIImage",&,V_inputImage
_inputImage
photoEffectName
outputImage
inputImage
setInputImage:
setValue:forKey:
filterWithName:
applyWithExtent:arguments:
stringByReplacingOccurrencesOfString:withString:
kernelBlackAndWhite
T@"CIImage",&,V_inputDepthMap
Tf,V_inputThreshold
_inputThreshold
_inputDepthMap
inputDepthMap
setInputDepthMap:
inputThreshold
setInputThreshold:
numberWithFloat:
T@"CIImage",&,N,VinputImage
T@"PFStoryRecipeDisplayAssetNormalization",&,N,VinputNormalization
T@"PFStoryRecipeDisplayAssetNormalization",R,N
inputNormalization
outputNormalization
setInputNormalization:
initWithAnalysisData:
adjustmentValuesForKey:
countByEnumeratingWithState:objects:count:
firstObject
results
performRequests:error:
setRevision:
initWithCIImage:options:
logger
smartColorProperties
smartToneProperties
highKeyProperties
imageByApplyingFilter:withInputParameters:
dictionaryWithObjects:forKeys:count:
tempTintProperties
analysisAvailable
TB,R,N,GisAnalysisAvailable
isAnalysisAvailable
colorCubeForNormalization:dimension:targetColorSpace:
contextWithOptions:
TB,R
isCandidateForReframe
isCandidateForPerspective
isCandidateForHorizon
gradeForFact:
factCandidateForHorizon
factCandidateForPerspective
factCandidateForReframe
factCandidateForStill
factCandidateForVideo
sharedPregateRules
pregateRulesSystemWithConstants:
addRulesFromArray:
setConstants:
setEnableLogging:
initWithArray:
retractFact:
addObject:
ruleWithPredicate:retractingFact:grade:
predicateWithFormat:
ruleWithPredicate:assertingFact:grade:
ruleWithPredicate:action:
initWithCapacity:
Td,N
Tq,R,N
Td,R,N
T@"NSArray",R,C,N
isSettingEqual:forKey:
setIntensity:
intensity
scene
sceneConfidence
boundingBoxes
setScene:confidence:
setBoundingBoxesFromObservations:orientation:
setFaceBoundingBoxesFromObservations:orientation:
faceBoundingBoxesKey
copy
numberWithDouble:
boundingBox
arrayWithCapacity:
boundingBoxesKey
sceneConfidenceKey
sceneLabelKey
floatValue
genericLandscapeSceneLabel
sunriseSunsetSceneLabel
isEqualToString:
platedFoodSceneLabel
intensityKey
isEqualToArray:
doubleValue
currentHandler
stringWithUTF8String:
handleFailureInFunction:file:lineNumber:description:
Tf,R,N,V_sampledDisparityValue
_sampledDisparityValue
Tf,R,N
sampledDisparityValue
initWithDisparityValue:
T{?=qiIq},N,V_sampleTime
T{CGRect={CGPoint=dd}{CGSize=dd}},N,V_sampleRect
Tf,N,V_sampledDisparityValue
_sampleTime
_sampleRect
render:
sampleTime
setSampleTime:
sampleRect
setSampleRect:
setSampledDisparityValue:
invalidError:object:
failureError:object:
initWithComposition:time:sampleRect:
customAttributes
inputStrength
inputScale
_kernelLocalContrast
imageByCroppingToRect:
imageByApplyingGaussianBlurWithSigma:
imageByApplyingTransform:
imageByClampingToExtent
HLGOpticalScale
vectorWithX:Y:Z:W:
T@"NSNumber",&,N,VinputStrength
setInputStrength:
T@"NUComposition",R,C,N
T@"<PICompositionControllerDelegate>",W,N,V_changeDelegate
Tq,N,V_mediaType
Tq,N,V_imageOrientation
Tq,N
_composition
_delegateFlags
_identifierMap
_changeDelegate
_mediaType
_imageOrientation
initWithComposition:
composition
setChangeDelegate:
compositionKeys
adjustmentKeys
availableKeys
addAdjustmentWithKey:
replaceAdjustment:withKey:
removeAdjustmentWithKey:
adjustmentControllerForKey:
modifyAdjustmentWithKey:modificationBlock:
applyChangesFromCompositionController:
isEqual:visualChangesOnly:
isEqual:forKeys:visualChangesOnly:
isEqual:forKeys:comparisonBlock:
differingAdjustmentsWithComposition:
userOrientation
sourceSelection
setSourceSelection:
setOvercaptureSource:
overcaptureSource
_adjustmentControllerClassForKey:
setSource:mediaType:
source
setMediaType:
changeDelegate
mediaType
imageOrientation
setImageOrientation:
isSubclassOfClass:
compositionController:adjustmentControllerClassForKey:
adjustmentControllerClassForKey:
boolValue
_keyToIdentifierMap
integerValue
containsObject:
allKeys
schema
compositionController:didUpdateAdjustments:
compositionController:didUpdateAdjustment:
reset
initWithIdentifier:
compositionController:didRemoveAdjustment:
compositionController:didAddAdjustment:
addObjectsFromArray:
contents
schemaForKey:
settingForAdjustmentKey:settingKey:
photosSchema
schemaWithIdentifier:
sharedRegistry
initialize
disableApertureEffects:
canInterpretDataWithFormatIdentifier:formatVersion:
deserializeCompositionFromData:formatIdentifier:formatVersion:error:
validateCompositionWithMissingSource:error:
deserializeCompositionFromAdjustments:metadata:formatIdentifier:formatVersion:error:
serializeComposition:versionInfo:error:
serializeComposition:versionInfo:serializerMetadata:error:
validateAdjustmentsEnvelope:error:
_validateValueTypesForKeys:requiredKeys:inDictionary:error:
serializeDictionary:error:
deserializeDictionaryFromData:error:
_sanitizeComposition:
adjustmentInformationForComposition:error:
formatVersion
formatIdentifier
data
dictionary
objectForKey:
infoDictionary
mainBundle
handleFailureInMethod:object:file:lineNumber:description:
JSONObjectWithData:options:error:
decompressData:options:error:
errorWithDomain:code:userInfo:
compressData:options:error:
dataWithJSONObject:options:error:
enumerateKeysAndObjectsUsingBlock:
initWithDomain:code:userInfo:
setWithArray:
setData:
setFormatVersion:
setFormatIdentifier:
T@"NSData",&,N,V_data
T@"NSString",&,N,V_formatIdentifier
T@"NSString",&,N,V_formatVersion
_data
_formatIdentifier
_formatVersion
setWithObjects:
array
orientation
height
numberWithInteger:
width
setOrientation:
setHeight:
setWidth:
size
setName:
Tq,N,V_width
Tq,N,V_height
Tq,N,V_orientation
_width
_height
_orientation
initWithName:
stringByAppendingString:
validateComposition:error:
URLWithString:
defaultValueForKey:
_customAttributesForKey:
dictionaryWithObjectsAndKeys:
containsString:
T@"CIImage",&,N,V_inputImage
T@"NSNumber",&,N,V_inputBlackSrcRGB
T@"NSNumber",&,N,V_inputBlackDstRGB
T@"NSNumber",&,N,V_inputShadowSrcRGB
T@"NSNumber",&,N,V_inputShadowDstRGB
T@"NSNumber",&,N,V_inputMidSrcRGB
T@"NSNumber",&,N,V_inputMidDstRGB
T@"NSNumber",&,N,V_inputHilightSrcRGB
T@"NSNumber",&,N,V_inputHilightDstRGB
T@"NSNumber",&,N,V_inputWhiteSrcRGB
T@"NSNumber",&,N,V_inputWhiteDstRGB
T@"NSNumber",&,N,V_inputBlackSrcRed
T@"NSNumber",&,N,V_inputBlackDstRed
T@"NSNumber",&,N,V_inputShadowSrcRed
T@"NSNumber",&,N,V_inputShadowDstRed
T@"NSNumber",&,N,V_inputMidSrcRed
T@"NSNumber",&,N,V_inputMidDstRed
T@"NSNumber",&,N,V_inputHilightSrcRed
T@"NSNumber",&,N,V_inputHilightDstRed
T@"NSNumber",&,N,V_inputWhiteSrcRed
T@"NSNumber",&,N,V_inputWhiteDstRed
T@"NSNumber",&,N,V_inputBlackSrcGreen
T@"NSNumber",&,N,V_inputBlackDstGreen
T@"NSNumber",&,N,V_inputShadowSrcGreen
T@"NSNumber",&,N,V_inputShadowDstGreen
T@"NSNumber",&,N,V_inputMidSrcGreen
T@"NSNumber",&,N,V_inputMidDstGreen
T@"NSNumber",&,N,V_inputHilightSrcGreen
T@"NSNumber",&,N,V_inputHilightDstGreen
T@"NSNumber",&,N,V_inputWhiteSrcGreen
T@"NSNumber",&,N,V_inputWhiteDstGreen
T@"NSNumber",&,N,V_inputBlackSrcBlue
T@"NSNumber",&,N,V_inputBlackDstBlue
T@"NSNumber",&,N,V_inputShadowSrcBlue
T@"NSNumber",&,N,V_inputShadowDstBlue
T@"NSNumber",&,N,V_inputMidSrcBlue
T@"NSNumber",&,N,V_inputMidDstBlue
T@"NSNumber",&,N,V_inputHilightSrcBlue
T@"NSNumber",&,N,V_inputHilightDstBlue
T@"NSNumber",&,N,V_inputWhiteSrcBlue
T@"NSNumber",&,N,V_inputWhiteDstBlue
T@"NSString",&,N,V_inputColorSpace
_inputBlackSrcRGB
_inputBlackDstRGB
_inputShadowSrcRGB
_inputShadowDstRGB
_inputMidSrcRGB
_inputMidDstRGB
_inputHilightSrcRGB
_inputHilightDstRGB
_inputWhiteSrcRGB
_inputWhiteDstRGB
_inputBlackSrcRed
_inputBlackDstRed
_inputShadowSrcRed
_inputShadowDstRed
_inputMidSrcRed
_inputMidDstRed
_inputHilightSrcRed
_inputHilightDstRed
_inputWhiteSrcRed
_inputWhiteDstRed
_inputBlackSrcGreen
_inputBlackDstGreen
_inputShadowSrcGreen
_inputShadowDstGreen
_inputMidSrcGreen
_inputMidDstGreen
_inputHilightSrcGreen
_inputHilightDstGreen
_inputWhiteSrcGreen
_inputWhiteDstGreen
_inputBlackSrcBlue
_inputBlackDstBlue
_inputShadowSrcBlue
_inputShadowDstBlue
_inputMidSrcBlue
_inputMidDstBlue
_inputHilightSrcBlue
_inputHilightDstBlue
_inputWhiteSrcBlue
_inputWhiteDstBlue
_inputColorSpace
P3KernelHDR
setDefaults
floatValueForKey:defaultValue:clearIfNotDefault:
_LUTImage
inputBlackSrcRGB
setInputBlackSrcRGB:
inputBlackDstRGB
setInputBlackDstRGB:
inputShadowSrcRGB
setInputShadowSrcRGB:
inputShadowDstRGB
setInputShadowDstRGB:
inputMidSrcRGB
setInputMidSrcRGB:
inputMidDstRGB
setInputMidDstRGB:
inputHilightSrcRGB
setInputHilightSrcRGB:
inputHilightDstRGB
setInputHilightDstRGB:
inputWhiteSrcRGB
setInputWhiteSrcRGB:
inputWhiteDstRGB
setInputWhiteDstRGB:
inputBlackSrcRed
setInputBlackSrcRed:
inputBlackDstRed
setInputBlackDstRed:
inputShadowSrcRed
setInputShadowSrcRed:
inputShadowDstRed
setInputShadowDstRed:
inputMidSrcRed
setInputMidSrcRed:
inputMidDstRed
setInputMidDstRed:
inputHilightSrcRed
setInputHilightSrcRed:
inputHilightDstRed
setInputHilightDstRed:
inputWhiteSrcRed
setInputWhiteSrcRed:
inputWhiteDstRed
setInputWhiteDstRed:
inputBlackSrcGreen
setInputBlackSrcGreen:
inputBlackDstGreen
setInputBlackDstGreen:
inputShadowSrcGreen
setInputShadowSrcGreen:
inputShadowDstGreen
setInputShadowDstGreen:
inputMidSrcGreen
setInputMidSrcGreen:
inputMidDstGreen
setInputMidDstGreen:
inputHilightSrcGreen
setInputHilightSrcGreen:
inputHilightDstGreen
setInputHilightDstGreen:
inputWhiteSrcGreen
setInputWhiteSrcGreen:
inputWhiteDstGreen
setInputWhiteDstGreen:
inputBlackSrcBlue
setInputBlackSrcBlue:
inputBlackDstBlue
setInputBlackDstBlue:
inputShadowSrcBlue
setInputShadowSrcBlue:
inputShadowDstBlue
setInputShadowDstBlue:
inputMidSrcBlue
setInputMidSrcBlue:
inputMidDstBlue
setInputMidDstBlue:
inputHilightSrcBlue
setInputHilightSrcBlue:
inputHilightDstBlue
setInputHilightDstBlue:
inputWhiteSrcBlue
setInputWhiteSrcBlue:
inputWhiteDstBlue
setInputWhiteDstBlue:
inputColorSpace
setInputColorSpace:
imageByPremultiplyingAlpha
imageByColorMatchingColorSpaceToWorkingSpace:
applyWithExtent:roiCallback:arguments:options:
definition
samplerWithImage:
imageByUnpremultiplyingAlpha
imageByColorMatchingWorkingSpaceToColorSpace:
CGColorSpace
itur2100HLGColorSpace
samplerWithImage:keysAndValues:
imageWithBitmapData:bytesPerRow:size:format:colorSpace:
dataWithBytesNoCopy:length:freeWhenDone:
valueForKey:
T{?=qiIq},N,Vtime
time
T{?=qiIq},N
setTime:
initWithResult:
numberWithUnsignedInteger:
requestRevision
observations
result:
initWithTargetPixelCount:
setVisionRequests:
initWithError:
unsupportedError:object:
available
TB,R,N,GisAvailable
isAvailable
autoCalculatorWithImageData:orientation:
imageWithData:
Td,V_inputCutoff
_inputCutoff
inputCutoff
setInputCutoff:
convertFacePoint:toImagePointWithFaceRect:orientation:
averagePoints:pointCount:
averageCGPoints:pointCount:
T@"NSString",R,V_destinationUTI
T@"NSURL",R,V_destinationLongExposureURL
T@"NSURL",R,V_destinationMaskURL
T@"NUColorSpace",R
_destinationUTI
_destinationLongExposureURL
_destinationMaskURL
initWithRequest:
initWithComposition:destinationURL:
initWithComposition:stabilizedVideoURL:longExposureDestinationURL:maskDestinationURL:destinationUTI:
outputColorSpace
destinationUTI
destinationLongExposureURL
destinationMaskURL
sRGBColorSpace
colorSpaceFromVideoColorProperties:
outputSettings
setCompletionBlock:
submitRequest:
submitGenericRequest:
setGenericCompletionBlock:
Tq,N,V_flavor
_flavor
flavor
setFlavor:
T@"NSDictionary",C,N,V_recipe
T{?={?=qq}{?=qq}},N,V_cleanAperture
_recipe
_cleanAperture
recipe
setRecipe:
cleanAperture
setCleanAperture:
processWithInputs:arguments:output:error:
formatForInputAtIndex:
outputFormat
outputIsOpaque
synchronizeInputs
allowPartialOutputRegion
roiForInput:arguments:outputRect:
applyWithRenderPipeline:renderState:globalRenderingMetadata:timedRenderingMetadata:inputImage:disparityImage:aperture:focusedDisparity:error:
applyWithExtent:inputs:arguments:error:
mutableCopy
vectorWithCGRect:
objectAtIndexedSubscript:
setObject:forKey:
strongToWeakObjectsMapTable
CGRectValue
encodeRenderTo:withRenderRequest:
setFocusDistance:
setDestinationColor:
setSourceDisparity:
setSourceColor:
setRenderState:
setTransferFunction:
createRGBA:
metalTexture
metalCommandBuffer
T@"CIImage",&,N,V_inputDisparityImage
T@"PTRenderPipeline",&,N,V_inputRenderPipeline
T@"<PTRenderState>",&,N,V_inputRenderState
T@"PTGlobalRenderingMetadata",&,N,V_inputGlobalRenderingMetadata
T@"PIPortraitVideoMetadataSample",&,N,V_inputTimedRenderingMetadata
T@"NSNumber",&,N,V_inputAperture
T@"NSNumber",&,N,V_inputFocusedDisparity
_inputDisparityImage
_inputRenderPipeline
_inputRenderState
_inputGlobalRenderingMetadata
_inputTimedRenderingMetadata
_inputAperture
_inputFocusedDisparity
inputDisparityImage
setInputDisparityImage:
inputRenderPipeline
setInputRenderPipeline:
inputRenderState
setInputRenderState:
inputGlobalRenderingMetadata
setInputGlobalRenderingMetadata:
inputTimedRenderingMetadata
setInputTimedRenderingMetadata:
inputAperture
setInputAperture:
inputFocusedDisparity
setInputFocusedDisparity:
T@"NSDictionary",C,N
T@"NSString",C,N
T{CGRect={CGPoint=dd}{CGSize=dd}},R,N
stabilizedCropRect
flavorKey
recipeKey
T{?=qiIq},R,N,V_startTime
T{?={?=qiIq}{?=qiIq}},R,N,V_loopTimeRange
T{?=qiIq},R,N,V_crossfadeDuration
_crossfadeDuration
_loopTimeRange
initWithSettings:inputs:
initWithInput:timeRange:crossfadeDuration:startTime:
input
resolvedNodeWithCachedInputs:settings:pipelineState:error:
shouldCacheNodeForPipelineState:
nodeByReplayingAgainstCache:pipelineState:error:
_evaluateVideo:
requiresVideoComposition
_evaluateVideoComposition:
requiresAudioMix
_evaluateAudioMix:
loopTimeRange
crossfadeDuration
setInputParameters:
setVolumeRampFromStartVolume:toEndVolume:timeRange:
setVolume:atTime:
audioMixInputParametersWithTrack:
audioMix
tracksWithMediaType:
timeRange
firstEnabledVideoTrackInAsset:error:
outputVideo:
setSourceTrackIDForFrameTiming:
setInstructions:
setRenderSize:
renderSize
setSourceIdentifier:forTrackID:
setRequiredSourceTrackIDs:
numberWithInt:
trackID
setTimeRange:
instructions
outputVideoComposition:
inputs
conformRange:inRange:
debugDescriptionOfAssetTrack:
insertTimeRange:ofTrack:atTime:error:
addMutableTrackWithMediaType:preferredTrackID:
errorWithCode:reason:object:underlyingError:
errorWithCode:reason:object:
setEvaluatedForMode:
nodeFromCache:cache:
initWithFilterName:settings:inputs:
setVideoFrames:
videoFrames
evaluationMode
initWithAdjustment:
keyFrameTime
setKeyFrameTime:
scaleKey
timeKey
numberWithLongLong:
intValue
longLongValue
radius
setRadius:
falloff
setFalloff:
falloffKey
radiusKey
T@"NSString",R,N
T@"NSDictionary",R,N
TB,N
T@"NUIdentifier",&,N,V_identifier
T@"NUAdjustment",R,N,V_adjustment
T@"NSArray",R,N
TB,R,N
_changes
_identifier
_adjustment
displayName
displayInputKeys
inputKeys
settingForKey:
hasInputKey:
enabled
setEnabled:
canBeEnabled
canHaveAuto
hasAutoKeyInSchema
isAuto
setIsAuto:
objectForKeyedSubscript:
setObject:forKeyedSubscript:
setValue:forUndefinedKey:
valueForUndefinedKey:
valuesForArrayInputKey:
setFromAdjustment:
interpolateFromStart:toEnd:progress:
timeFromInputKey:timescaleKey:
visualInputKeys
isEqualToAdjustmentController:
isEqual:forKeys:
_setPrimitiveValue:forKey:
_primitiveValueForKey:
settings
_isDefault
identifier
setIdentifier:
adjustment
defaultValue
values
autoKey
numberWithBool:
enabledKey
name
T@"NSArray",C,N,V_disparityKeyframes
T@"NSArray",C,N,V_apertureKeyframes
T{?=qiIq},N,V_renderTime
T@"PTRenderPipeline",&,N,V_portraitRenderPipeline
T@"<PTRenderState>",&,N,V_portraitRenderState
T@"PTGlobalRenderingMetadata",&,N,V_globalRenderingMetadata
T@"PIPortraitVideoMetadataSample",&,N,V_timedRenderingMetadata
T{CGSize=dd},N,V_pipelineColorSize
T{CGSize=dd},N,V_pipelineDisparitySize
_disparityKeyframes
_apertureKeyframes
_portraitRenderPipeline
_portraitRenderState
_globalRenderingMetadata
_timedRenderingMetadata
_pipelineColorSize
_pipelineDisparitySize
_renderTime
initWithInput:disparityInput:disparityKeyframes:apertureKeyframes:debugMode:
uniqueInputNode
evaluateSettings:pipelineState:error:
_scaledSizeForInput:scale:error:
_allocatePortraitRenderPipelineWithImageInput:disparityInput:pipelineState:error:
_updateRenderStateWithCameraInfo:
_evaluateImage:
disparityKeyframes
setDisparityKeyframes:
apertureKeyframes
setApertureKeyframes:
renderTime
setRenderTime:
portraitRenderPipeline
setPortraitRenderPipeline:
portraitRenderState
setPortraitRenderState:
globalRenderingMetadata
setGlobalRenderingMetadata:
timedRenderingMetadata
setTimedRenderingMetadata:
pipelineColorSize
setPipelineColorSize:
pipelineDisparitySize
setPipelineDisparitySize:
imageByCompositingOverImage:
outputImage:
inputForKey:
setReadNoise_8x:
setReadNoise_1x:
setConversionGain:
setFocalLenIn35mmFilm:
setRawSensorHeight:
setRawSensorWidth:
setTotalSensorCrop:
applyToRenderState:
deserializeMetadataWithType:fromGlobalMetadata:error:
value
metadata
videoProperties:
prewarm
createRenderStateWithQuality:
initWithDescriptor:
setUseRGBA:
setDebugRendering:
initWithDevice:version:colorSize:disparitySize:
scale
outputImageGeometry:
metadataGroup
outputTimedMetadataSampleWithIdentifier:atTime:error:
videoMetadataSamples
setEvaluationMode:
_group
_queue
_result
init
submit:response:
submitSynchronous:error:
submitGenericSynchronous:
faceRequestWithRequest:
sourceSelectionKey
stringForSourceSelection:
sourceSelectionForString:
_value
_time
initWithDictionaryRepresentation:
dictionaryRepresentation
initWithTime:value:
keyframeInArray:closestToTime:
Tq,R,N,V_type
Tq,R,N,V_identifier
Td,R,N,V_confidence
Tq,R,N,V_source
T{CGRect={CGPoint=dd}{CGSize=dd}},N,V_bounds
T{CGRect={CGPoint=dd}{CGSize=dd}},N,V_expandedBounds
Tq,N,V_edgeBleed
_type
_confidence
_source
_edgeBleed
_bounds
_expandedBounds
supportsSecureCoding
encodeWithCoder:
initWithCoder:
initWithType:source:identifier:confidence:
setBounds:
isHuman
isAnimal
type
confidence
bounds
expandedBounds
setExpandedBounds:
edgeBleed
setEdgeBleed:
stringValue
componentsSeparatedByString:
decodeObjectForKey:
decodeDoubleForKey:
decodeIntegerForKey:
encodeObject:forKey:
encodeDouble:forKey:
encodeInteger:forKey:
allocWithZone:
appendFormat:
appendString:
labels
T@"NSArray",C,N
T@"NSNumber",&,N
T@"NSDictionary",&,N
_keyframesForKey:class:
_setKeyframes:forKey:
aperture
setAperture:
cinematographyState
setCinematographyState:
debugMode
setDebugMode:
removeObject:
unsignedIntValue
T{?=qiIq},R,N,V_time
T{?=[3]},R,N,V_homography
T@"NSDictionary",R,C,N
_homography
initWithTime:homography:
homography
count
keyframesFromDictionaryRepresentations:
TQ,R,N
_homographySequence
initWithKeyframeArray:
interpolation
homographyAtTime:
sparseSequence
sampleAtTime:
initWithCount:times:values:
_computeCleanAperture:
stopAtTagFilter:
setKind:
kind
setVersion:
version
versionKey
kindKey
addAssetIdentifier:toMetadataDictionary:
addAssetIdentifier:toMetadataArray:
removeAssetIdentifierFromMetadataArray:
removeObjectAtIndex:
indexOfObjectPassingTest:
setValue:
metadataItem
Td,N,V_temperature
Td,N,V_tint
_temperature
_tint
setInputVectorsForFilter:
temperature
setTemperature:
tint
setTint:
T{?=qiIq},N,V_time
T{?=qiIq},N,V_rawTime
Tq,N,V_sampleMode
_sampleMode
_rawTime
nu_evaluateWithPipelineState:error:
rawTime
setRawTime:
sampleMode
setSampleMode:
T@"NSDictionary",R
T@"NSDictionary",&,V_recipe
T@"AVAsset",&,N,V_videoSource
T@"NSDictionary",&,N,V_recipe
_videoSource
wantsOutputGeometry
cacheKey
analysisRequest
videoSource
setVideoSource:
asset:
renderNode
finalize
nu_updateDigest:
ICShouldBeCanceled
ICReportProgress:
Td,N,V_rangeMin
Td,N,V_rangeMax
T@?,C,N,V_shouldCancelHandler
_rangeMin
_rangeMax
_shouldCancelHandler
rangeMin
setRangeMin:
rangeMax
setRangeMax:
shouldCancelHandler
setShouldCancelHandler:
T@"NSArray",R,C,N,V_keyframes
T{?={?=qq}{?=qq}},R,N,V_stabCropRect
TQ,R,N,V_analysisType
T@"NSDictionary",R,N,V_rawHomographies
_keyframes
_analysisType
_rawHomographies
_stabCropRect
T{?={?=qq}{?=qq}},R,N
keyframes
stabCropRect
analysisType
rawHomographies
initWithKeyframes:stabCropRect:analysisType:rawHomographies:
TQ,N,V_allowedAnalysisTypes
Td,N,V_allowedCropFraction
_allowedAnalysisTypes
_allowedCropFraction
allowedAnalysisTypes
setAllowedAnalysisTypes:
allowedCropFraction
setAllowedCropFraction:
nominalFrameRate
canceledError:object:
isCanceled
cleanApertureOfTrack:oriented:
canPerformGyroBasedStabilizationForAsset:
Tq,R,V_rawState
_rawState
initWithRequest:isRAW:
rawState
rawProperties
calculateRAWWithRequest:completion:
calculateWithRequest:completion:
faceBalanceResultFromFaceObservations:request:error:
faceRectFromNormalizedFaceRet:forImageExtent:scaleX:scaleY:
faceBalanceFromFaceImage:forFaceRect:
readBufferRegion:withBlock:
bytesAtPoint:
frameRect
buffer
validRegion
BGRA8
RGBA8
isEqualToPixelFormat:
ARGB8
maximumValue
minimumValue
setResponseQueue:
setRegionPolicy:
initWithRect:
addRect:
imageSize
faces
initWithRequest:dataExtractor:options:
_bufferRenderClient
_imageDataClient
_useSushi
initWithComposition:useSushi:
readBufferFromImage:withRGBAfBufferBlock:
calculateColorWithProperties:completion:
_brightnessMultiplierFromImageProperties:
_computeWhitePointColorWithGrayEdgesBuffer:grayWorldBuffer:greenChannelPercentage:RAWCameraSpaceProperties:
_whitePointColorFromGrayEdgesImage:grayWorldImage:greenChannelPercentage:RAWCameraSpaceProperties:
_configureRequest:
_computeGreenPercentage:
_submitGWRenderRequest:
_submitGERenderRequest:
initWithScript:
initWithSource:
submitRequest:completion:
initWithComposition:dataExtractor:options:
setTileSize:
genericRGBLinearColorSpace
setPixelFormat:
RGBAh
whiteFactor
whiteValue
rowBytes
commitAndNotifyOnQueue:withBlock:
pi_valueWithGrayColorResult:
rawCameraSpaceProperties
begin
returnStorage:
writeBufferInRegion:block:
regionWithRect:
newStorageWithSize:format:
RGBAf
bufferFactory
sharedFactory
initWithName:responseQueue:
_useTempTint:
_correctedRGBResultFromResult:
_chooseNeutralGrayForNonSushi:
_chooseTempTintForSushi:RAWProperties:brightness:
inputNeutralXYFromRGB:
valueWithRGBResult:
pi_grayColorResultValue
valueWithBytes:objCType:
getValue:
RGBResultValue
T{?={?=[4d]}{?=[4d]}d},R
T{?=[4d]},R
definitionKernel
T@"CIImage",&,V_inputBlurImage
T@"NSNumber",&,V_inputIntensity
_inputBlurImage
_inputIntensity
inputBlurImage
setInputBlurImage:
inputIntensity
setInputIntensity:
strengthKey
neutralKey
toneKey
grainKey
hueKey
inputStrengthKey
inputNeutralGammaKey
inputToneKey
inputGrainKey
inputHueKey
inputSeedKey
attributeStrengthKey
attributeNeutralGammaKey
attributeToneKey
attributeHueKey
attributeGrainKey
setStrength:
strength
setNeutral:
neutral
setTone:
tone
setGrain:
grain
setHue:
videoMetadataForVariation:error:
setImageVariation:properties:error:
photoProcessingFlagsFromProperties:error:
setPhotoProcessingFlags:properties:error:
photoFeatureFlags:error:
setPhotoFeatureFlags:properties:error:
writeMetadataType:value:toCGImageProperties:error:
readMetadataType:fromCGImageProperties:value:error:
metadataItemsWithMetadataType:value:error:
T@"NUImageExportRequest",&,V_request
T{?=qq},V_inputSize
_request
_inputSize
request
setRequest:
inputSize
setInputSize:
T@"NUImageGeometry",&,V_geometry
_geometry
geometry
setGeometry:
T@"NSDictionary",&,V_auxiliaryImages
TB,V_canPropagateOriginalAuxiliaryData
T@"NSDictionary",C,V_properties
T{?=qq},D
T@"NSData",&,V_companionImageData
T@"NSURL",&,V_companionVideoURL
_canPropagateOriginalAuxiliaryData
_companionImageData
_companionVideoURL
_auxiliaryImages
_properties
companionImageData
setCompanionImageData:
companionVideoURL
setCompanionVideoURL:
auxiliaryImages
setAuxiliaryImages:
canPropagateOriginalAuxiliaryData
setCanPropagateOriginalAuxiliaryData:
properties
setProperties:
T@"NSData",&,V_data
T@"NUPriority",&,V_priority
T@"NUColorSpace",&,V_colorSpace
T@"NSString",C,V_pairingIdentifier
T@"<NUScalePolicy>",&,V_scalePolicy
_priority
_colorSpace
_pairingIdentifier
_scalePolicy
priority
setPriority:
colorSpace
setColorSpace:
pairingIdentifier
setPairingIdentifier:
setScalePolicy:
displayP3ColorSpace
initWithLevel:
T@?,C,V_metadataProcessor
TB,N,V_increaseBitRateIfNecessary
T@"NSString",C,N,V_videoCodecType
TB,N,V_preserveSourceColorSpace
TB,N,V_bypassOutputSettingsIfNoComposition
TB,N,V_applyVideoOrientationAsMetadata
TB,N,V_requireHardwareEncoder
_increaseBitRateIfNecessary
_preserveSourceColorSpace
_bypassOutputSettingsIfNoComposition
_applyVideoOrientationAsMetadata
_requireHardwareEncoder
_metadataProcessor
_videoCodecType
metadataProcessor
setMetadataProcessor:
increaseBitRateIfNecessary
setIncreaseBitRateIfNecessary:
videoCodecType
setVideoCodecType:
preserveSourceColorSpace
setPreserveSourceColorSpace:
bypassOutputSettingsIfNoComposition
setBypassOutputSettingsIfNoComposition:
applyVideoOrientationAsMetadata
setApplyVideoOrientationAsMetadata:
requireHardwareEncoder
setRequireHardwareEncoder:
T@"NUImageExportFormat",C,V_imageExportFormat
Td,V_JPEGCompressionQuality
TB,V_optimizeForSharing
TB,V_applyImageOrientationAsMetadata
_optimizeForSharing
_applyImageOrientationAsMetadata
_imageExportFormat
_JPEGCompressionQuality
imageExportFormatForURL:
imageExportFormat
setImageExportFormat:
JPEGCompressionQuality
setJPEGCompressionQuality:
optimizeForSharing
setOptimizeForSharing:
applyImageOrientationAsMetadata
setApplyImageOrientationAsMetadata:
setCompressionQuality:
defaultFormatForURL:
fileType
typeWithFilenameExtension:
pathExtension
T@"NSURL",&,V_primaryURL
T@"NSURL",&,V_videoComplementURL
T@"NSURL",&,V_videoPosterFrameURL
TB,V_renderCompanionResources
T@"NUAdjustment",&,V_reframeCropAdjustment
T@"NUAdjustment",&,V_reframeVideoAdjustment
TB,V_applyVideoOrientationAsMetadata
_renderCompanionResources
_primaryURL
_videoComplementURL
_videoPosterFrameURL
_reframeCropAdjustment
_reframeVideoAdjustment
primaryURL
setPrimaryURL:
videoComplementURL
setVideoComplementURL:
videoPosterFrameURL
setVideoPosterFrameURL:
renderCompanionResources
setRenderCompanionResources:
reframeCropAdjustment
setReframeCropAdjustment:
reframeVideoAdjustment
setReframeVideoAdjustment:
exportImageToURL:composition:options:completion:
exportImageToDataWithComposition:options:completion:
exportVideoToURL:composition:options:completion:
exportComposition:toPrimaryURL:videoComplementURL:videoPosterFrameURL:priority:completionQueue:completion:
exportComposition:options:completionQueue:completion:
variationForFlavor:
prepareImageExportRequest:options:completion:
prepareAuxiliaryImagesFetchProperties:options:completion:
addImageProperties:composition:options:error:
addVideoProperties:composition:options:error:
shouldTryVideoRotationFastPath:options:
_exportVideoToURL:composition:options:properties:progress:completion:
_exportVideoToURLFull:composition:options:properties:progress:completion:
submitWithProgress:completion:
setOutputSettings:
defaultExportCodecForComposition:
setBitRateMultiplicationFactor:
setPreferredTransform:
initWithCGAffineTransform:
preferredTransformFromOrientation:size:
originalSize
setMetadata:
metadataConverter
unsignedIntegerValue
auxiliaryImage
setAuxiliaryImageType:
auxiliaryImagesProperties
setRenderWithIOSurface:
setAuxImages:
setImageProperties:
disableIOSurfacePortaitExport
unknownError:object:
resetImageProperties:preserveRegions:
URLByAppendingPathComponent:isDirectory:
stringByAppendingPathExtension:
stringByDeletingPathExtension
lastPathComponent
URLForDirectory:inDomain:appropriateForURL:create:error:
defaultManager
conformsToType:
mismatchError:object:
UUIDString
UUID
initWithProperties:
discreteProgressWithTotalUnitCount:
destinationData
setFormat:
setRenderToData:
setDestinationURL:
T@"<PICompositionExporterMetadataConverter>",&
setMetadataConverter:
removeObjectForKey:
bilateralKernels
RGBToLabKernels
bilateralAdd1Kernel
bilateralAdd2Kernel
bilateralAdd3Kernel
bilateralAdd4Kernel
bilateralAdd5Kernel
bilateralAdd6Kernel
bilateralAdd7Kernel
bilateralAdd8Kernel
bilateralAdd9Kernel
bilateralFinalizeKernel
RGBToLabKernel
LabToRGBKernel
T@"NSArray",&,V_inputPoints
T@"NSArray",&,V_inputWeights
T@"NSNumber",&,V_inputEdgeDetail
T@"NSNumber",&,V_inputVersion
_inputPoints
_inputWeights
_inputEdgeDetail
_inputVersion
samplesPerPass
boundsForPointArray:
enlargedBounds:withPoints:
bilateralAddROI:destRect:userInfo:
doBilateralPass:points:weights:sums:slope:
inputPoints
setInputPoints:
inputWeights
setInputWeights:
inputEdgeDetail
setInputEdgeDetail:
inputVersion
setInputVersion:
subarrayWithRange:
samplerWithImage:options:
vectorWithX:Y:Z:
vectorWithX:Y:
applyWithExtent:roiCallback:arguments:
objectAtIndex:
unionWith:
shapeWithRect:
BWBilateralKernels
bilateralLoop2Kernel
bilateralLoop5Kernel
bilateralLoop11Kernel
T@"NSNumber",&,V_inputBorder
_inputBorder
bilateralROI:destRect:userInfo:
doBilateralLoop:points:weights:slope:
inputBorder
setInputBorder:
T@"NSNumber",&,V_inputRadius
_inputRadius
inputRadius
setInputRadius:
T@"NSString",R,W,N
TQ,R,N,V_majorVersion
TQ,R,N,V_minorVersion
TQ,R,N,V_subMinorVersion
T@"NSString",R,C,N,V_platform
_majorVersion
_minorVersion
_subMinorVersion
_platform
initWithMajor:minor:subMinor:platform:
initWithMajor:minor:subMinor:
string
asOrderedInteger
compare:
isEqualToAdjustmentVersion:
majorVersion
minorVersion
subMinorVersion
platform
stringByAppendingFormat:
caseInsensitiveCompare:
versionWithMajor:minor:subMinor:platform:
versionFromString:
rangeOfCharacterFromSet:
length
invertedSet
decimalDigitCharacterSet
amountKey
T@"NSNumber",&,N,VinputVibrancy
T@"NSNumber",&,N,VinputContrast
T@"NSNumber",&,N,VinputCast
inputVibrancy
inputContrast
inputCast
_isIdentity
_kernelCPos
_kernelCNeg
_kernelV_gt1
_kernelV_lt1
_kernelCast
setInputVibrancy:
setInputContrast:
setInputCast:
smartColorHDRStatistics
dataWithLength:
mutableBytes
render:toBitmap:rowBytes:bounds:format:colorSpace:
T{?={?=qiIq}{?=qiIq}},N
setLoopTimeRange:
setCrossfadeDuration:
startTimeTimescaleKey
startTimeValueKey
crossfadeDurationTimescaleKey
crossfadeDurationValueKey
loopTimeRangeDurationTimescaleKey
loopTimeRangeDurationValueKey
loopTimeRangeStartTimescaleKey
loopTimeRangeStartValueKey
Tq,N,V_version
T@"NSNumber",C,N
_version
setPortraitInfo:
portraitInfo
canRenderPortraitEffect
setSpillMatteAllowed:
spillMatteAllowed
spillMatteAllowedKey
portraitInfoKey
_noOpRemovalFunctions
noOpRemovalFunctions
copyOfAdjustmentRemovingNoOps:identifier:
copyOfCompositionRemovingNoOps:
valueKey
T@"PTTimedRenderingMetadata",&,N,V_timedMetadata
Td,R,N,V_focusedDisparity
Td,R,N,V_aperture
T@"NSDictionary",R,N,V_cameraInfo
_cameraInfo
_timedMetadata
_focusedDisparity
_aperture
initWithMetadataGroup:majorVersion:minorVersion:error:
applyToRenderRequest:
valueWithIdentifier:inGroup:ofClass:
_cameraInfoFromMetadataGroup:
cameraInfo
timedMetadata
setTimedMetadata:
focusedDisparity
unarchivedObjectOfClasses:fromData:error:
metadataItemsFromArray:filteredByIdentifier:
items
objectFromData:withMajorVersion:minorVersion:
_highKeyHDR
T@"CIImage",&,N,V_debugLineDetectionImage
T@"NSNumber",C,V_maxAutoYaw
T@"NSNumber",C,V_maxAutoPitch
T@"NSNumber",C,V_maxAutoAngle
Td,V_minimumPitchCorrection
Td,V_minimumYawCorrection
Td,V_minimumAngleCorrection
Td,V_minimumConfidence
Td,V_maxFaceSize
Td,V_minimumPitchCorrectionArea
Td,V_minimumYawCorrectionArea
TB,V_disableOnPanos
TB,V_disableOnFrontFacingCameraImages
TB,V_shouldRunBuildingCheck
Td,V_angleSeedDegreesCCW
TB,V_debugFilesEnabled
T@"NSString",C,V_debugFilesPrefix
T@"NSMutableDictionary",R,V_debugDiagnostics
T@"PIFaceObservationCache",&,N,V_faceObservationCache
_disableOnPanos
_disableOnFrontFacingCameraImages
_shouldRunBuildingCheck
_debugFilesEnabled
_faceObservationCache
_maxAutoYaw
_maxAutoPitch
_maxAutoAngle
_minimumPitchCorrection
_minimumYawCorrection
_minimumAngleCorrection
_minimumConfidence
_maxFaceSize
_minimumPitchCorrectionArea
_minimumYawCorrectionArea
_angleSeedDegreesCCW
_debugFilesPrefix
_debugDiagnostics
_debugLineDetectionImage
T@"PIFaceObservationCache",&,N
faceObservationCache
setFaceObservationCache:
perspectiveErrorFromCoreImage:
addMethodDiagnostics:details:
addMethodResultToDiagnostics:error:setYawPitchError:
wrapAsUnexpectedError:
writeDebugDiagnosticsToDisk
getSizeOfAllFaces:
passesFaceCheck:
hasFrontFacingCameraDimentions:
isFrontFacingCameraImage:pixelSize:
passesImagePropertiesCheck:
passesBuildingCheck:
overcaptureImageProperties:
primaryImageProperties:
canGenerateNewCropRect:
passesConfidenceCheck:error:
passesMinimumCorrectionCheck:error:
submitVerified:
maxAutoYaw
setMaxAutoYaw:
maxAutoPitch
setMaxAutoPitch:
maxAutoAngle
setMaxAutoAngle:
minimumPitchCorrection
setMinimumPitchCorrection:
minimumYawCorrection
setMinimumYawCorrection:
minimumAngleCorrection
setMinimumAngleCorrection:
minimumConfidence
setMinimumConfidence:
maxFaceSize
setMaxFaceSize:
minimumPitchCorrectionArea
setMinimumPitchCorrectionArea:
minimumYawCorrectionArea
setMinimumYawCorrectionArea:
disableOnPanos
setDisableOnPanos:
disableOnFrontFacingCameraImages
setDisableOnFrontFacingCameraImages:
shouldRunBuildingCheck
setShouldRunBuildingCheck:
angleSeedDegreesCCW
setAngleSeedDegreesCCW:
debugFilesEnabled
setDebugFilesEnabled:
debugFilesPrefix
setDebugFilesPrefix:
debugDiagnostics
debugLineDetectionImage
setDebugLineDetectionImage:
undoOrientation:forPitch:yaw:angle:
null
defaultFocalLength
setRollAngle:constrainCropRectWithTargetArea:
initWithMasterImageSize:
initWithMasterImageSize:stitchedImageSize:
isFusedOvercapture
setComposition:
nonLocalizedFailureReason
lowercaseString
PNGRepresentationOfImage:format:colorSpace:options:
writeToURL:atomically:
createDirectoryAtURL:withIntermediateDirectories:attributes:error:
URLByAppendingPathComponent:
userInfo
requestVisionCleanUp
releaseCachedResources
globalSession
luminanceKey
shortValue
T{CGRect={CGPoint=dd}{CGSize=dd}},N
smart
TB,N,GisSmart
originalCrop
TB,N,GisOriginalCrop
isGeometryIdentityForImageSize:
isCropConstrained
isCropIdentityForImageSize:
cropRect
constraintWidth
constraintHeight
angle
angleRadians
pitch
pitchRadians
yawRadians
autoCropped
isSmart
isOriginalCrop
setCropRect:
setConstraintWidth:
setConstraintHeight:
setAngle:
setAngleRadians:
setPitch:
setPitchRadians:
setYaw:
setYawRadians:
setAutoCropped:
setSmart:
setOriginalCrop:
originalCropKey
smartKey
heightKey
widthKey
yOriginKey
xOriginKey
yawKey
pitchKey
angleKey
constraintHeightKey
constraintWidthKey
setInputDecoderVersion:
inputDecoderVersion
_smartSettings
_updateSettingsWithInputLight:
computedSettings
setInputLight:
inputLight
setInputExposure:
inputExposure
setInputBrightness:
inputBrightness
setInputShadows:
inputShadows
setInputHighlights:
inputHighlights
setInputBlack:
inputBlack
setInputLocalLight:
inputLocalLight
setInputRawHighlights:
inputRawHighlights
setStatistics:
setOvercaptureStatistics:
overcaptureStatistics
offsetBlack
setOffsetBlack:
offsetBrightness
setOffsetBrightness:
offsetContrast
setOffsetContrast:
offsetExposure
setOffsetExposure:
offsetHighlights
setOffsetHighlights:
offsetLocalLight
setOffsetLocalLight:
offsetShadows
setOffsetShadows:
offsetShadowsKey
offsetLocalLightKey
offsetHighlightsKey
offsetExposureKey
offsetContrastKey
offsetBrightnessKey
offsetBlackKey
overcaptureStatisticsKey
statisticsKey
inputRawHighlightsKey
inputLocalLightKey
inputBlackKey
inputHighlightsKey
inputShadowsKey
inputContrastKey
inputBrightnessKey
inputExposureKey
inputLightKey
smartToneAdjustmentsForValue:localLightAutoValue:andStatistics:
attributeBrightnessKey
attributeContrastKey
attributeExposureKey
attributeHighlightsKey
attributeShadowsKey
attributeBlackPointKey
attributeLocalLightKey
attributeLightMapKey
attributeLightMapWidthKey
attributeLightMapHeightKey
alphaCompositingKernel
dynamismMapKernel
longExposureFusionKernels
dynamismMapRefineKernel
rgbToLumaKernel
homographyKernel
nccKernel
nccCoarseKernel
blur7x7Kernel
blur5x5Kernel
blur3x3Kernel
fusionKernel
assetIdentifierForURL:type:useEmbeddedPreview:
imageSourceWithURL:type:useEmbeddedPreview:
imageSourceWithURL:type:proxyImage:orientation:useEmbeddedPreview:
imageSourceWithCIImage:orientation:
videoSourceWithURL:
livePhotoSourceWithPhotoSource:videoSource:
newComposition
compositionByRemovingVideoAndLivePhotoAdjustments:
newAdjustmentWithName:
newAdjustmentWithIdentifier:
newImageRenderClientWithName:
geometryRequestWithComposition:
imagePropertiesRequestWithComposition:
videoPropertiesRequestWithComposition:
imageRenderRequestWithComposition:fitInSize:wideGamut:
imageRenderRequestWithComposition:fillInSize:wideGamut:
_imageRenderRequestWithComposition:wideGamut:
newCGImageFromBufferImage:
priorityWithLevel:
videoRenderRequestWithComposition:fitInSize:
is3DEffect:
isPortraitEffect:
isPortraitStageEffect:
areCPVAssetsEditable
isAVAssetEditable:
effectNameForFilterName:
filterNameForEffectName:
pipelineFiltersForCropping
pipelineFiltersForOriginalGeometry
pipelineFiltersForShowingOriginal
pipelineFiltersForShowingOriginalWithGeometry
pipelineFiltersForRAWShowingOriginalWithGeometry
newCompositionControllerWithComposition:
adjustmentConstants
handlePIGlobalSettings:
validatedCompositionCopyForComposition:mediaType:
knownFormatsVersionsMap
updateCropAdjustmentController:after:error:
preheatEditDependencies
prepareForPerformingRequests:error:
boolForKey:
standardUserDefaults
PIMuteAdjustmentKey
PITrimAdjustmentKey
PIPortraitAdjustmentKey
PIDepthAdjustmentKey
PIRedEyeAdjustmentKey
PIAutoLoopAdjustmentKey
T@"NSString",R,N,V_PISmartToneAdjustmentKey
T@"NSString",R,N,V_PISmartColorAdjustmentKey
T@"NSString",R,N,V_PISmartBWAdjustmentKey
T@"NSString",R,N,V_PIGrainAdjustmentKey
T@"NSString",R,N,V_PIAutoEnhanceAdjustmentKey
T@"NSString",R,N,V_PIWhiteBalanceAdjustmentKey
T@"NSString",R,N,V_PIRedEyeAdjustmentKey
T@"NSString",R,N,V_PIApertureRedEyeAdjustmentKey
T@"NSString",R,N,V_PIRetouchAdjustmentKey
T@"NSString",R,N,V_PIVignetteAdjustmentKey
T@"NSString",R,N,V_PISharpenAdjustmentKey
T@"NSString",R,N,V_PINoiseReductionAdjustmentKey
T@"NSString",R,N,V_PIDefinitionAdjustmentKey
T@"NSString",R,N,V_PICurvesAdjustmentKey
T@"NSString",R,N,V_PILevelsAdjustmentKey
T@"NSString",R,N,V_PISelectiveColorAdjustmentKey
T@"NSString",R,N,V_PIEffectAdjustmentKey
T@"NSString",R,N,V_PIEffect3DAdjustmentKey
T@"NSString",R,N,V_PICropAdjustmentKey
T@"NSString",R,N,V_PITrimAdjustmentKey
T@"NSString",R,N,V_PISlomoAdjustmentKey
T@"NSString",R,N,V_PILivePhotoKeyFrameAdjustmentKey
T@"NSString",R,N,V_PIVideoPosterFrameAdjustmentKey
T@"NSString",R,N,V_PIAutoLoopAdjustmentKey
T@"NSString",R,N,V_PIHighResFusionAdjustmentKey
T@"NSString",R,N,V_PIMuteAdjustmentKey
T@"NSString",R,N,V_PIDepthAdjustmentKey
T@"NSString",R,N,V_PIPortraitAdjustmentKey
T@"NSString",R,N,V_PIOrientationAdjustmentKey
T@"NSString",R,N,V_PIRawAdjustmentKey
T@"NSString",R,N,V_PIRawNoiseReductionAdjustmentKey
T@"NSString",R,N,V_PISemanticEnhanceAdjustmentKey
T@"NSString",R,N,V_PIVideoReframeAdjustmentKey
T@"NSString",R,N,V_PISourceSelectAdjustmentKey
T@"NSString",R,N,V_PIVideoStabilizeAdjustmentKey
T@"NSString",R,N,V_PIVideoCrossfadeLoopAdjustmentKey
T@"NSString",R,N,V_PIPortraitVideoAdjustmentKey
T@"NSString",R,N,V_PISourceAdjustmentKey
T@"NSString",R,N,V_PIOvercaptureSourceAdjustmentKey
_PISmartToneAdjustmentKey
_PISmartColorAdjustmentKey
_PISmartBWAdjustmentKey
_PIGrainAdjustmentKey
_PIAutoEnhanceAdjustmentKey
_PIWhiteBalanceAdjustmentKey
_PIRedEyeAdjustmentKey
_PIApertureRedEyeAdjustmentKey
_PIRetouchAdjustmentKey
_PIVignetteAdjustmentKey
_PISharpenAdjustmentKey
_PINoiseReductionAdjustmentKey
_PIDefinitionAdjustmentKey
_PICurvesAdjustmentKey
_PILevelsAdjustmentKey
_PISelectiveColorAdjustmentKey
_PIEffectAdjustmentKey
_PIEffect3DAdjustmentKey
_PICropAdjustmentKey
_PITrimAdjustmentKey
_PISlomoAdjustmentKey
_PILivePhotoKeyFrameAdjustmentKey
_PIVideoPosterFrameAdjustmentKey
_PIAutoLoopAdjustmentKey
_PIHighResFusionAdjustmentKey
_PIMuteAdjustmentKey
_PIDepthAdjustmentKey
_PIPortraitAdjustmentKey
_PIOrientationAdjustmentKey
_PIRawAdjustmentKey
_PIRawNoiseReductionAdjustmentKey
_PISemanticEnhanceAdjustmentKey
_PIVideoReframeAdjustmentKey
_PISourceSelectAdjustmentKey
_PIVideoStabilizeAdjustmentKey
_PIVideoCrossfadeLoopAdjustmentKey
_PIPortraitVideoAdjustmentKey
_PISourceAdjustmentKey
_PIOvercaptureSourceAdjustmentKey
allAdjustmentTypes
nonVisualAdjustmentTypes
PISmartToneAdjustmentKey
PISmartColorAdjustmentKey
PISmartBWAdjustmentKey
PIGrainAdjustmentKey
PIAutoEnhanceAdjustmentKey
PIWhiteBalanceAdjustmentKey
PIApertureRedEyeAdjustmentKey
PIRetouchAdjustmentKey
PIVignetteAdjustmentKey
PISharpenAdjustmentKey
PINoiseReductionAdjustmentKey
PIDefinitionAdjustmentKey
PICurvesAdjustmentKey
PILevelsAdjustmentKey
PISelectiveColorAdjustmentKey
PIEffectAdjustmentKey
PIEffect3DAdjustmentKey
PICropAdjustmentKey
PISlomoAdjustmentKey
PILivePhotoKeyFrameAdjustmentKey
PIVideoPosterFrameAdjustmentKey
PIHighResFusionAdjustmentKey
PIOrientationAdjustmentKey
PIRawAdjustmentKey
PIRawNoiseReductionAdjustmentKey
PISemanticEnhanceAdjustmentKey
PIVideoReframeAdjustmentKey
PISourceSelectAdjustmentKey
PIVideoStabilizeAdjustmentKey
PIVideoCrossfadeLoopAdjustmentKey
PIPortraitVideoAdjustmentKey
PISourceAdjustmentKey
PIOvercaptureSourceAdjustmentKey
getTagWithPath:error:
resetTag:input:
initWithScript:block:
allValues
decodingSupportForAVAsset:
capabilitiesForCurrentDevice
isAssetUnsupportedLegacyPortraitVideo:
metadataTrackWithPortraitVideoDataInAsset:
isAVAssetDolbyProfile5:error:
deviceSupportsHighDynamicRangeVideo
deviceSupportsHardware10BitHEVCEncoding
videoAssetIsHighDynamicRange:
isExportable
isPlayable
isReadable
supportsANE
isMetalDeviceSupported:
initWithTargetSize:
setExtentPolicy:
setResolvedSourceDefinition:
initWithImageSourceDefinition:videoSourceDefinition:
resolvedSourceDefinition
setAssetIdentifier:
assetIdentifier
initWithSourceDefinitions:
initWithURL:UTI:
initWithCIImage:orientation:
setUseEmbeddedPreview:
absoluteString
typeWithIdentifier:
timeIntervalSinceReferenceDate
getResourceValue:forKey:error:
dictionaryWithDictionary:
addEntriesFromDictionary:
Td,V_inputBoost
_inputBoost
inputBoost
setInputBoost:
kernelsDictionaryWithString:
pi_createColorCubeDataForFilters:dimension:colorSpace:
kernelsWithString:
conversionMap
mapForSerialization
arrayWithArray:
base64EncodedStringWithOptions:
initWithBase64EncodedString:options:
T@"CIImage",&,V_inputTableImage
_inputTableImage
curvesKernel
inputTableImage
setInputTableImage:
T@"NSError",&,V__accumError
_pixelSize
_renderer
_temporaryDestinationStorage
_averageAccumulationStorage
_minimumAccumulationStorage
_maximumAccumulationStorage
_frameCount
_jobNumber
_stateQueue
_accumQueue
_accumSemaphore
_readySemaphore
_doneGroup
_inputFrames
_finished
_imageOptions
__accumError
initWithSize:renderer:jobNumber:
dealloc
workingColorSpace
cancel
start:
_initializeStorage:image:error:
isReadyForMoreData
_isReadyForMoreData
markAsFinished
_markAsFinished
waitUntilDone
accumulate:error:
_appendInputFrame:
nextInputFrame
_nextInputFrame
_start
_initializeAccumulation
_initializeAccumulation:
_accumulate:
_accumulate:error:
writeLongExposureImage:UTI:colorSpace:error:
writeMaskImage:UTI:error:
_dynamismMapWithMinImage:maxImage:extent:
_exportOutputImage:format:colorSpace:toURL:uti:error:
_accumError
set_accumError:
createCGImage:fromRect:format:colorSpace:deferred:
extent
context
useAsCIImageWithOptions:renderer:block:
useAsCIRenderDestinationWithRenderer:block:
renderImage:rect:toDestination:atPoint:error:
setLabel:
componentMin
componentMax
imageWithCVPixelBuffer:options:
CVPixelBuffer
imageWithColor:
colorWithRed:green:blue:colorSpace:
sRGBLinearColorSpace
surfaceStoragePool
T@"VNImageHomographicAlignmentObservation",R,C
observation
T@"VNImageHomographicAlignmentObservation",C,N,V_observation
T{?={?=qq}{?=qq}},N,V_extent
_observation
_extent
setObservation:
setExtent:
T{?={?=qq}{?=qq}},N,V_guideExtent
T@"CIImage",&,N,V_stillImage
T@"VNImageHomographicAlignmentObservation",&,N,V_observation
_stillImage
_guideExtent
wantsOutputImage
wantsRenderScaleClampedToNativeScale
registrationRequest
newRenderPipelineStateForEvaluationMode:
guideExtent
setGuideExtent:
stillImage
setStillImage:
initWithCVPixelBuffer:options:
initWithTargetedCVPixelBuffer:options:
waitUntilCompletedAndReturnError:
jobNumber
initWithPixelBuffer:
newPixelBufferOfSize:format:
renderScale
outputGeometry
prepareNode
nodeByReplayingAgainstCache:error:
setScale:
prepareNodeWithPipelineState:error:
_shouldWaitForDependentJobs
initWithComposition:responseQueue:
_pipelineFilters
initWithComposition:tag:responseQueue:
T@"CIImage",&,N,V_inputDestinationImage
T@"NSArray",&,N,V_inputCorrectionInfo
T@"NSString",&,N,V_inputCameraModel
_inputDestinationImage
_inputCorrectionInfo
_inputCameraModel
inputDestinationImage
setInputDestinationImage:
inputCorrectionInfo
setInputCorrectionInfo:
inputCameraModel
setInputCameraModel:
posterFrameTime
setPosterFrameTime:
locallySupportedFormatVersions
currentFormatVersion
adjustmentHasPerspective:settings:
adjustmentHasCTM:settings:
_versionRules
versionRules
formatVersionForAdjustment:identifier:
adjustmentDataFormatVersionForComposition:
indexOfObject:
captureDebugDirectoryForComposition:
fileURLWithPath:
sourceDefinitions
endTime
setEndTime:
rate
setRate:
rateKey
endScaleKey
endKey
startScaleKey
startKey
T@"PTCinematographyScript",&,N,V_cinematographyScript
T@"NSCache",&,N,V_labelImageCache
_cinematographyScript
_labelImageCache
initWithInput:assetURL:cinematographyState:monochrome:
_imageByAddingDetection:toImage:isPrimary:canvasSize:inverseOrientation:
cinematographyScript
setCinematographyScript:
labelImageCache
setLabelImageCache:
trackIdentifier
detectionType
allDetections
focusDetection
frameNearestTime:
setFontSize:
setFontName:
setText:
textImageGeneratorFilter
setBackgroundImage:
sourceOutCompositingFilter
blackColor
setColor:
roundedRectangleGeneratorFilter
groupIdentifier
colorWithRed:green:blue:
loadWithAsset:changesDictionary:completion:
assetWithURL:
outputExposureKey
falseColorHDRKey
inputRAWGamutMapMaxKey
bytesPerPixel
initWithSize:format:
mutableBytesAtPoint:
RG16
faceBalanceKernels
newLinearWideGamutColorSpace
linearWideGamutColorSpace
dataWithBytes:length:
Td,N,V_inputOrigI
Td,N,V_inputOrigQ
Td,N,V_inputStrength
Td,N,V_inputWarmth
_inputOrigI
_inputOrigQ
_inputStrength
_inputWarmth
inputOrigI
setInputOrigI:
inputOrigQ
setInputOrigQ:
inputWarmth
setInputWarmth:
T@"NSArray",&,V_inputPointsR
T@"NSArray",&,V_inputPointsG
T@"NSArray",&,V_inputPointsB
T@"NSArray",&,V_inputPointsL
_inputPointsR
_inputPointsG
_inputPointsB
_inputPointsL
inputPointsR
setInputPointsR:
inputPointsG
setInputPointsG:
inputPointsB
setInputPointsB:
inputPointsL
setInputPointsL:
tableImageFromRed:green:blue:luminance:
calculateCurveTable:
curvePointsFromDictionaries:
initWithImageProvider:width:height:format:colorSpace:options:
initWithLength:
colorBalanceKernels
gHDRtoPPKernel
PPtogHDRKernel
colorBalanceKernel
T@"NSNumber",&,N,V_inputWarmTemp
T@"NSNumber",&,N,V_inputWarmTint
T@"NSNumber",&,N,V_inputStrength
T@"NSNumber",&,N,V_inputHasFace
T@"NSNumber",&,N,V_inputIsRaw
_inputWarmTemp
_inputWarmTint
_inputHasFace
_inputIsRaw
applyInputConversion:
applyOutputConversion:
inputWarmTemp
setInputWarmTemp:
inputWarmTint
setInputWarmTint:
inputHasFace
setInputHasFace:
inputIsRaw
setInputIsRaw:
_map
getMap
HDRFilterForSDRFilter:
substringFromIndex:
alignment
setAlignment:
alignmentKey
TB,N,V_forceGlassesMatteOff
TB,N,V_forceSpillMatteOff
TB,N,V_allowSpillMatteOnOlderPortraitV2Captures
_settings
_forceGlassesMatteOff
_forceSpillMatteOff
_allowSpillMatteOnOlderPortraitV2Captures
decoratorRenderFiltersForImages
decoratorRenderFiltersForVideos
forceGlassesMatteOff
setForceGlassesMatteOff:
forceSpillMatteOff
setForceSpillMatteOff:
allowSpillMatteOnOlderPortraitV2Captures
setAllowSpillMatteOnOlderPortraitV2Captures:
globalSettings
IPXEditSettings
PUEditSettings
editSettings
falseColorHDR
setFalseColorHDR:
initWithAutoLoopExportRequest:
initWithVideoExportRequest:
autoLoopExportRequest
renderer:
metalRenderer
shouldUseMetalRenderer
initWithMetalDevice:options:
boolSettingForKey:defaultValue:
setUpContext:
toRect
initWithNode:context:
node
toDictionary
setError:
isObject
contextForContext:
currentContext
jsContext
bundleForClass:
URLForResource:withExtension:
newPhotosPipelineAtSourceURL:error:
initWithURL:
newPhotosPipeline:
_faceRequest
apertureRedEyeResultFromFaceObservations:imageSize:
normalizedPoints
pointCount
rightEye
leftEye
landmarks
cancelAllRequests
renderContext
inputLightMap
inputLightMapWidth
inputLightMapHeight
inputGuideImage
inputLightMapImage
inputSmartShadows
_shadowKernelHDR
_polyKernelHDR
localLightHDRStatisticsNoProxy
T@"NSMutableData",&,Vdata
TQ,R,VelementByteSize
TQ,R,VrowElements
TQ,R,Vwidth
TQ,R,Vheight
Ti,R,Vformat
elementByteSize
rowElements
format
bufferColorspace
initWithBytes:width:height:bytesPerRow:bytesPerComponent:format:colorspace:
initWithData:width:height:bytesPerRow:bytesPerComponent:format:colorspace:
bytes
image
_calculateBlackAndWhiteSettingsFromBufferImage:
initWithTargetPixelSize:
CIFormat
T@"NSNumber",C,N,VinputStrength
T@"NSNumber",C,N,VinputNeutralGamma
T@"NSNumber",C,N,VinputTone
T@"NSNumber",C,N,VinputHue
T@"NSNumber",C,N,VinputGrain
T@"NSNumber",C,N,VinputSeed
T@"NSNumber",C,N,VinputScaleFactor
inputNeutralGamma
inputTone
inputHue
inputGrain
inputSeed
inputScaleFactor
getNonNormalizedSettings:
createHueArray
hueArrayImage:
smartBlackWhiteKernel
setInputNeutralGamma:
setInputTone:
setInputHue:
setInputGrain:
setInputSeed:
setInputScaleFactor:
imageWithBitmapData:bytesPerRow:size:format:options:
smartBlackAndWhiteStatistics
smartBlackAndWhiteAdjustmentsForValue:andStatistics:
hasCorrections
inputCorrectionInfoKey
calculateSettingsForImageHistogram:
calculateSettingsForSingleChannelHistogram:suffix:
percentile:
histogram
setHistogramCalculationColorSpace:
histogramCalculationColorSpace
luminance
blue
green
portraitVideoAdjustmentControllerCreatingIfNecessary:
smartToneAdjustmentController
smartColorAdjustmentController
smartBWAdjustmentController
cropAdjustmentController
redEyeAdjustmentController
livePhotoKeyFrameAdjustmentController
videoPosterFrameAdjustmentController
depthAdjustmentController
trimAdjustmentController
slomoAdjustmentController
effectAdjustmentController
effect3DAdjustmentController
portraitAdjustmentController
orientationAdjustmentController
autoLoopAdjustmentController
highResFusionAdjustmentController
rawAdjustmentController
rawNoiseReductionAdjustmentController
sharpenAdjustmentController
whiteBalanceAdjustmentController
noiseReductionAdjustmentController
definitionAdjustmentController
vignetteAdjustmentController
videoReframeAdjustmentController
sourceSelectAdjustmentController
videoStabilizeAdjustmentController
videoCrossfadeLoopAdjustmentController
semanticEnhanceAdjustmentController
portraitVideoAdjustmentController
smartToneAdjustmentControllerCreatingIfNecessary:
_adjustmentControllerForKey:creatingIfNecessary:expectedClass:
smartColorAdjustmentControllerCreatingIfNecessary:
smartBWAdjustmentControllerCreatingIfNecessary:
cropAdjustmentControllerCreatingIfNecessary:
redEyeAdjustmentControllerCreatingIfNecessary:
livePhotoKeyFrameAdjustmentControllerCreatingIfNecessary:
videoPosterFrameAdjustmentControllerCreatingIfNecessary:
depthAdjustmentControllerCreatingIfNecessary:
trimAdjustmentControllerCreatingIfNecessary:
slomoAdjustmentControllerCreatingIfNecessary:
effectAdjustmentControllerCreatingIfNecessary:
effect3DAdjustmentControllerCreatingIfNecessary:
portraitAdjustmentControllerCreatingIfNecessary:
orientationAdjustmentControllerCreatingIfNecessary:
autoLoopAdjustmentControllerCreatingIfNecessary:
highResFusionAdjustmentControllerCreatingIfNecessary:
rawAdjustmentControllerCreatingIfNecessary:
rawNoiseReductionAdjustmentControllerCreatingIfNecessary:
sharpenAdjustmentControllerCreatingIfNecessary:
whiteBalanceAdjustmentControllerCreatingIfNecessary:
noiseReductionAdjustmentControllerCreatingIfNecessary:
definitionAdjustmentControllerCreatingIfNecessary:
vignetteAdjustmentControllerCreatingIfNecessary:
videoReframeAdjustmentControllerCreatingIfNecessary:
sourceSelectAdjustmentControllerCreatingIfNecessary:
videoStabilizeAdjustmentControllerCreatingIfNecessary:
videoCrossfadeLoopAdjustmentControllerCreatingIfNecessary:
semanticEnhanceAdjustmentControllerCreatingIfNecessary:
T@"PIAdjustmentConstants",R,N
Td,R,N,V_x
Td,R,N,V_y
editable
TB,R,N,GisEditable,V_editable
_editable
initWithX:y:editable:
initWithDictionary:
isEditable
T@"NSNumber",&,N,VinputExposure
T@"NSNumber",&,N,VinputBrightness
T@"NSNumber",&,N,VinputShadows
T@"NSNumber",&,N,VinputHighlights
T@"NSNumber",&,N,VinputBlack
T@"NSNumber",&,N,VinputRawHighlights
_kernelBneg
_kernelBpos
_kernelC
_kernelC_hdr
_kernelH
_kernelRH
smartToneHDRStatistics
T@"CIImage",&,N,V_inputStillImage
T@"CIImage",&,N,V_inputMaskImage
T@"NSNumber",&,N,V_inputRenderScale
T@"NSNumber",&,N,V_inputVideoScale
T@"CIVector",&,N,V_inputAlignmentExtent
T@"CIVector",&,N,V_inputAlignmentTransform
_inputStillImage
_inputMaskImage
_inputRenderScale
_inputVideoScale
_inputAlignmentExtent
_inputAlignmentTransform
alignImage:transform:extent:
_computeNCCMapFromImage:toImage:scale:
_refineMaskImage:guideImage:scale:
_fuseImage:withGuideImage:weightImage:maskImage:
inputStillImage
setInputStillImage:
inputMaskImage
setInputMaskImage:
inputRenderScale
setInputRenderScale:
inputVideoScale
setInputVideoScale:
inputAlignmentExtent
setInputAlignmentExtent:
inputAlignmentTransform
setInputAlignmentTransform:
imageByApplyingTransform:highQualityDownsample:
applyWithExtent:roiCallback:inputImage:arguments:
writeToTIFF:
currentDirectoryPath
debugDumpIntermediateImages
valueAtIndex:
loadFusionTuningParameters
_debugDumpIntermediateImages
dictionaryWithContentsOfFile:
stringSettingForKey:defaultValue:
_processedRenderNodeForComposition:input:pipelineState:error:
evaluate:input:pipelineState:error:
scaledVector:
orientedNode:withOrientation:
originalCleanAperture
straightenTransformWithAngle:extent:
perspectiveTransformWithPitch:yaw:roll:imageRect:
sharpnessWithIntensity:
grainInputSeedFromFrameTime
createSloMoWithInput:startTime:endTime:rate:error:
trimInput:startTime:endTime:error:
livePhotoKeyFrameMetadataFromNode:time:error:
initWithInput:
isEqualToNumber:
cropNode:cropRect:cropSettings:
isSourceAvailable:sourceSettings:
initWithInput:scale:
isCIFilterAvailable:propertyName:
scaleMultiplyOfScalar:
depthCameraCalibrationData
auxiliaryDataInfoMetadata
vectorWithFloats:
auxiliaryImageFromComposition:type:mediaComponentType:error:
scaledSize
scaleNode:scale:error:
cacheNode:type:settings:error:
isHDR
initWithCGColorSpace:
allAssetsCanUseHDRPipeline
auxiliaryImageType
enableHDRSupport
endGroupWithName:error:
addTagWithName:inputNode:error:
inputForPath:error:
transformNodeWithInput:transform:error:
initWithAffineTransform:
renderNodeFromSource:settings:error:
beginGroupWithName:error:
valueWithCMTime:
hasStaticTime
mediaTypeForComposition:
isEnabled
initWithPipelineState:
P3Kernel
T@"NUIdentifier",R
sourceSelectSchema
rawSchema
rawNoiseReductionSchema
smartToneSchema
smartColorSchema
smartBlackAndWhiteSchema
grainSchema
sharpenSchema
cropSchema
trimSchema
slomoSchema
livePhotoKeyFrameSchema
muteSchema
videoPosterFrameSchema
autoLoopSchema
highResFusionSchema
depthEffectSchema
effect3DSchema
portraitEffectSchema
effectSchema
redEyeSchema
apertureRedEyeSchema
retouchSchema
vignetteSchema
orientationSchema
definitionSchema
noiseReductionSchema
whiteBalanceSchema
levelsSchema
curvesSchema
selectiveColorSchema
videoReframeSchema
videoStabilizeSchema
videoCrossfadeLoopSchema
debugSchema
semanticEnhance
portraitVideoSchema
photosCompositionSchema
registeredPhotosSchemaIdentifier
registerPhotosSchema
registerRenderPipeline:forIdentifier:
registerSchemas:error:
renderPipelineForIdentifier:
deserializeFromDictionary:error:
T@"NSError",&,N,V_finalizerError
TQ,N,V_performedActions
Td,N,V_rollAngleDegrees
Td,N,V_pitchAngleDegrees
Td,N,V_yawAngleDegrees
TQ,N,V_candidacy
_candidacy
_finalizerError
_performedActions
_rollAngleDegrees
_pitchAngleDegrees
_yawAngleDegrees
performNextActionWithCompletion:
shouldPerformAction:
hasPerformedAction:
markActionAsPerformed:
performHorizonCorrectionWithCompletion:
performPerspectiveCorrectionWithCompletion:
shouldAllowPerspectiveCorrection
processHorizonResult:
processPerspectiveResult:
candidacy
setCandidacy:
finalizerError
setFinalizerError:
performedActions
setPerformedActions:
rollAngleDegrees
setRollAngleDegrees:
pitchAngleDegrees
setPitchAngleDegrees:
yawAngleDegrees
setYawAngleDegrees:
floatForKey:
initWithDisposition:composition:
Tq,R,V_disposition
T@"NUComposition",R,C,V_composition
_disposition
disposition
getCropRectThatCompletelyContainsMasterImageForPitch:yaw:roll:
descriptionForCandidacy:
TQ,N
cropFraction
setCropFraction:
setAnalysisType:
TB,V_shouldPerformAutoCrop
TB,V_shouldPerformAutoStraighten
TB,V_shouldUseAutoStraightenVerticalDetector
T@"NSNumber",C,V_autoStraightenVerticalAngleThreshold
T@"NSNumber",C,V_autoStraightenDominantAngleDiffThreshold
Td,V_maxAutoStraighten
Td,V_minAutoStraighten
_shouldPerformAutoCrop
_shouldPerformAutoStraighten
_shouldUseAutoStraightenVerticalDetector
_autoStraightenVerticalAngleThreshold
_autoStraightenDominantAngleDiffThreshold
_maxAutoStraighten
_minAutoStraighten
imageProperties:
undoExifOrientation:error:
shouldPerformAutoCrop
setShouldPerformAutoCrop:
shouldPerformAutoStraighten
setShouldPerformAutoStraighten:
shouldUseAutoStraightenVerticalDetector
setShouldUseAutoStraightenVerticalDetector:
autoStraightenVerticalAngleThreshold
setAutoStraightenVerticalAngleThreshold:
autoStraightenDominantAngleDiffThreshold
setAutoStraightenDominantAngleDiffThreshold:
maxAutoStraighten
setMaxAutoStraighten:
minAutoStraighten
setMinAutoStraighten:
overcaptureRectForStitchedOvercaptureSize:overcaptureVideoComplementSize:primarySize:autoLoopStabilizedCropRect:
stitchedOvercaptureRect:primaryRect:forComposition:error:
initWithMasterImageRect:stitchedImageRect:
setRollRadians:
integralCropRect:
updateCropAdjustment:after:error:
_stats
_updateSettingsWithInputColor:
setInputColor:
inputColor
setInputSaturation:
inputSaturation
setOffsetCast:
offsetCast
setOffsetSaturation:
offsetSaturation
offsetSaturationKey
offsetCastKey
inputCastKey
inputSaturationKey
inputColorKey
attributeVibrancyKey
attributeCastKey
T@"NSArray",&,N,V_inputCorrections
_inputCorrections
hueSatLumTable
inputCorrections
setInputCorrections:
convertFromIPT:
selectiveColorKernels
convertToIPT:
iptHueAngleFromRed:green:blue:
colorWithRed:green:blue:alpha:colorSpace:
iptFromLinearInto:fromRed:green:blue:
hueAngleFrom:
T@"PIReframeKeyframeSequence",&,N,V_keyframeSequence
T{?={?=qq}{?=qq}},N,V_stabCropRect
T@"<NUVideoProperties>",&,N,V_inputVideoProperties
T{?=qiIq},N,V_frameDuration
TB,N,V_shouldApplyWatermark
_shouldApplyWatermark
_keyframeSequence
_inputVideoProperties
_frameDuration
initWithKeyframes:stabCropRect:input:
_evaluateVideoProperties:
_evaluateImageGeometry:
canPropagateOriginalLivePhotoMetadataTrack
_stabilizeImage:cleanRect:cropRect:transform:geometry:
keyframeSequence
setKeyframeSequence:
setStabCropRect:
inputVideoProperties
setInputVideoProperties:
frameDuration
setFrameDuration:
shouldApplyWatermark
setShouldApplyWatermark:
pi_imageByApplyingStabilizationWatermark
vectorWithCGPoint:
initWithExtent:renderScale:orientation:
setSize:
dictionaryForKey:
imageWithCGImage:
T{?=qiIq},R,V_time
T@"NSArray",R,V_subjects
T{CGVector=dd},R,V_estimatedCenterMotion
T{CGVector=dd},R,V_estimatedMotionBlur
T{?=[3]},R,V_trajectoryHomography
_subjects
_estimatedCenterMotion
_estimatedMotionBlur
_trajectoryHomography
setSubjects:
setEstimatedCenterMotion:
setEstimatedMotionBlur:
setTrajectoryHomography:
subjects
estimatedCenterMotion
estimatedMotionBlur
trajectoryHomography
T@"NSArray",R,N,VtimedMetadataArray
_asset
_videoTrack
_mdataTrack
ndcMetadataTransform
pxlMetadataTransform
timedMetadataArray
initWithAVAsset:
overwriteTrackingMetadataWithPlist:
subjectsFromMetadata:
centerMotionVectorFromMetadata:
motionBlurVectorFromMetadata:
trajectoryeHomographyFromMetadata:containsV3Metadata:
extractMetadata
dataValue
dataType
nextTimedMetadataGroup
startReading
initWithAssetReaderTrackOutput:
addOutput:
canAddOutput:
initWithTrack:outputSettings:
assetReaderWithAsset:error:
initWithContentsOfFile:
encodedPixelSizeOfTrack:oriented:
formatDescriptions
tracks
exceptionWithName:reason:userInfo:
canProvideMetadataForAVAsset:
RGBToYIQKernel
YIQToRGBKernel
whiteBalanceKernel
T@"NSNumber",&,N,V_strength
T@"NSNumber",&,N,V_warmth
T@"NSNumber",&,N,V_y
T@"NSNumber",&,N,V_i
T@"NSNumber",&,N,V_q
_strength
_warmth
isDefaultWarmth:
warmth
setWarmth:
setY:
setI:
setQ:
vectorWithX:
submitSynchronous:
setDataExtractor:
isHDRComposition:
TB,V_force
_force
_options
force
setForce:
setOptions:
options
internalComposition
initWithRequest:options:
_location
_touchDiameter
initWithComposition:location:touchDiameter:
T@"NSNumber",C,N,VinputISO
T@"NSNumber",C,N,VinputAmount
inputISO
inputAmount
_interpolateGrainKernel
_grainBlendAndMixKernel
_paddedTileKernel
setInputISO:
setInputAmount:
setPerservesAlpha:
writeImage:fileURL:
writeCGImage:fileURL:
writeCGImage:fileURL:options:
writeImage:toTemporaryDirectoryWithBasename:
writeImage:toDirectoryAtPath:withBasename:
stringByAppendingPathComponent:
path
buildNumber
currentSoftwareVersion
edgesKey
ROIForCenterPoint:radius:
convertFloat:toFixed16:count:
convertFixed16:toFloat:count:
copyPixelsFromImage:srcRect:destImage:destOrigin:
initWithMutableBuffer:colorSpace:validRegion:
initWithBuffer:colorSpace:validRegion:
initWithSize:format:rowBytes:bytes:
initWithSize:format:rowBytes:mutableBytes:
RGBA16
region
bytesPerRow
baseAddress
inputSpots
pointValue
Tf,N,V_aperture
Tf,N,V_portraitStrength
T@"NSNumber",&,N,V_minimumAperture
T@"NSNumber",&,N,V_maximumAperture
TQ,N,V_SDOFRenderingVersion
TQ,N,V_portraitMajorVersion
TQ,N,V_portraitMinorVersion
T{?=ii},N,V_depthVersionInfo
_portraitStrength
_minimumAperture
_maximumAperture
_SDOFRenderingVersion
_portraitMajorVersion
_portraitMinorVersion
_depthVersionInfo
initFromMinAperture:maxAperture:aperture:portraitStrength:SDOFRenderingVerion:depthVersionInfo:
portraitStrength
setPortraitStrength:
minimumAperture
setMinimumAperture:
maximumAperture
setMaximumAperture:
SDOFRenderingVersion
setSDOFRenderingVersion:
portraitMajorVersion
setPortraitMajorVersion:
portraitMinorVersion
setPortraitMinorVersion:
depthVersionInfo
setDepthVersionInfo:
valuesAtCaptureFromImageProperties:error:
portraitLightingEffectStrength
depthBlurEffectSimulatedAperture
methodForSelector:
getMinMaxSimulatedApertureFrom:minValue:maxValue:version:
depthBlurEffectRenderingParameters
auxiliaryCoreGraphicsInfoDictionary:
cameraCalibrationData
depthDataQuality
isDepthDataFiltered
underlyingAVDepthData
auxiliaryImage:
_calculateWithImageProperties:valuesAtCapture:completion:
portraitInfoDictionaryFromFaceObservations:metadata:orientation:valuesAtCapture:
isStillImageDisparity:
focusRectDictionaryFromMetadata:
focusRectDictionaryFromRect:
canApplyPortraitEffectsWithMetadata:
depthEffectInfoDictionaryFromFaceObservations:metadata:orientation:valuesAtCapture:
depthEffectInfoDictionaryFromFaceObservations:focus:valuesAtCapture:lumaNoiseScale:orientation:
portraitEffectInfoDictionaryFromFaceObservations:orientation:valuesAtCapture:
portraitInfoDictionaryFromCameraMetadata:
luminanceNoiseAmplitude
apertureFocalRatio
maximumApertureFocalRatio
minimumApertureFocalRatio
focusRectangle
faceOrientation
objectsAtIndexes:
indexesOfShallowDepthOfFieldObservations
faceObservationsData
roll
faceOrientationIndex
faceJunkinessIndex
allPoints
nose
autoCropFilter
exifOrientationAndCropStraightenOnly
rawFaceBalanceFilter
rawSourceFilterIncludingOrientation
sourceFilterNoOrientation
sushiLevel1Filter
noRedEyeFilter
noTrimFilter
noMuteFilter
noCropFilter
iosCropToolFilter
stripAllTimeAdjustmentsFilter
noGeometryFilter
noOrientationFilter
orientationAsMetaDataFilter
perspectiveStraightenWithoutCropFilter
preGeometryFilter
postGeometryFilter
inputToCropFilter
stopAtTagIncludeGeometryFilter:
stopAtTagIncludeOrientationFilter:
applyOrientationFilter
autoloopStabilizedVideoFilter
overcaptureSourceFilter
primarySourceFilter
spatialOvercaptureVideoSourceFilter
oneShotPortraitV2ExportFilter
resetTag:input:error:
socPseudoColorFilter
colorType
setColorType:
faceStrength
setFaceStrength:
faceWarmth
setFaceWarmth:
faceI
setFaceI:
faceQ
setFaceQ:
grayStrength
setGrayStrength:
grayWarmth
setGrayWarmth:
grayY
setGrayY:
grayI
setGrayI:
grayQ
setGrayQ:
warmTemp
setWarmTemp:
warmTint
setWarmTint:
warmFace
setWarmFace:
warmFaceKey
warmTintKey
warmTempKey
tintKey
temperatureKey
grayQKey
grayIKey
grayYKey
grayWarmthKey
grayStrengthKey
faceQKey
faceIKey
faceWarmthKey
faceStrengthKey
colorTypeKey
stringForColorType:
colorTypeForString:
sourceDefinition:
nodeByApplyingFilterWithName:useHDRFilter:settingsAndInputs:
redEyeSpotsWithCorrectionInfo:
overcaptureRectForAutoCrop:overcaptureVideoComplementSize:primarySize:CGRect:
remapPortraitV2Strength:portraitEffectKind:
videoReframe:reframes:error:
videoCrossfadeLoop:crossfadeAdjustment:error:
portraitVideo:disparityInput:disparityKeyframes:apertureKeyframes:debugMode:error:
portraitVideoDebugDetectedObjects:source:cinematographyState:monochrome:error:
versionForPortraitEffect:
performLongExposureFusionForComposition:longExposureImage:useHDRFilter:error:
performApertureRedeyeOnImage:useHDRFilter:redEyeAdjustment:
performRedeyeOnImage:useHDRFilter:redEyeAdjustment:
debugNodeByApplyingFilterWithName:useHDRFilter:settingsAndInputs:
canRenderDepth
setDepthInfo:
depthInfo
capturedAperture
setGlassesMatteAllowed:
glassesMatteAllowed
glassesMatteAllowedKey
depthInfoKey
apertureKey
T{?={?=qq}{?=qq}},N
setKeyframes:
copyKeyframesTrimmingToTimeRange:
stabCropRectKey
keyframesKey
indexOfObject:inSortedRange:options:usingComparator:
lastObject
dictionariesFromPoints:
_defaultCurveArray
autoValuesForBlackPoint:whitePoint:
replaceObjectAtIndex:withObject:
computeCurvesForImageHistogram:
setParameters:
setColorMatrix:
curvePointAtIndex:blackPoint:whitePoint:histogram:
insertObject:atIndex:
@"PTCinematographyTrack"
@"PTCinematographyTrack"16@0:8
@16@0:8
@"<NURenderStatistics>"16@0:8
B24@0:8@16
#16@0:8
@24@0:8:16
@32@0:8:16@24
@40@0:8:16@24@32
B16@0:8
B24@0:8#16
B24@0:8@"Protocol"16
B24@0:8:16
Vv16@0:8
Q16@0:8
^{_NSZone=}16@0:8
@"NSString"16@0:8
@24@0:8@16
v16@0:8
{CGPoint="x"d"y"d}
{?="value"q"timescale"i"flags"I"epoch"q}
B24@0:8o^@16
v76@0:8{?=qiIq}16{CGRect={CGPoint=dd}{CGSize=dd}}40f72
{?=qiIq}16@0:8
v40@0:8{?=qiIq}16
{CGPoint=dd}16@0:8
v32@0:8{CGPoint=dd}16
@?16@0:8
v24@0:8@?16
v24@0:8@16
v20@0:8B16
@64@0:8@16{?=qiIq}24{CGPoint=dd}48
@24@0:8^{_NSZone=}16
q16@0:8
@"CIImage"
f16@0:8
v20@0:8f16
@"PFStoryRecipeDisplayAssetNormalization"
@40@0:8@16q24^{CGColorSpace=}32
B32@0:8@16@24
v24@0:8d16
d16@0:8
v32@0:8q16d24
v32@0:8@16q24
@20@0:8f16
{CGRect="origin"{CGPoint="x"d"y"d}"size"{CGSize="width"d"height"d}}
{CGRect={CGPoint=dd}{CGSize=dd}}16@0:8
v48@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16
@80@0:8@16{?=qiIq}24{CGRect={CGPoint=dd}{CGSize=dd}}48
@"NSNumber"
@"NUComposition"
{?="hasDidAdd"B"hasDidRemove"B"hasDidUpdate"B"hasDidUpdateMultiple"B"hasClassForController"B}
@"NSDictionary"
@"<PICompositionControllerDelegate>"
v32@0:8@16@24
v32@0:8@16@?24
B28@0:8@16B24
B36@0:8@16@24B32
B40@0:8@16@24@?32
v24@0:8q16
#24@0:8@16
@32@0:8@16@24
@48@0:8@16@24@32o^@40
B32@0:8@16o^@24
@56@0:8@16@24@32@40o^@48
@40@0:8@16@24o^@32
B48@0:8@16@24@32o^@40
@32@0:8@16o^@24
@"NSData"
@"NSString"
d40@0:8@16d24^B32
@32@0:8@16q24
{CGPoint=dd}72@0:8{CGPoint=dd}16{CGRect={CGPoint=dd}{CGSize=dd}}32q64
{CGPoint=dd}32@0:8r^{CGPoint=dd}16Q24
@"NSURL"
@56@0:8@16@24@32@40@48
{?="origin"{?="x"q"y"q}"size"{?="width"q"height"q}}
{?={?=qq}{?=qq}}16@0:8
v48@0:8{?={?=qq}{?=qq}}16
B48@0:8@16@24@32^@40
i20@0:8i16
i16@0:8
{CGRect={CGPoint=dd}{CGSize=dd}}60@0:8i16@20{CGRect={CGPoint=dd}{CGSize=dd}}28
@88@0:8@16@24@32@40@48@56@64@72o^@80
@"PTRenderPipeline"
@"<PTRenderState>"
@"PTGlobalRenderingMetadata"
@"PIPortraitVideoMetadataSample"
{?="start"{?="value"q"timescale"i"flags"I"epoch"q}"duration"{?="value"q"timescale"i"flags"I"epoch"q}}
@120@0:8@16{?={?=qiIq}{?=qiIq}}24{?=qiIq}72{?=qiIq}96
@24@0:8o^@16
{?={?=qiIq}{?=qiIq}}16@0:8
@"NSMutableDictionary"
@"NUIdentifier"
@"NUAdjustment"
v40@0:8@16@24d32
{?=qiIq}32@0:8@16@24
@"NSArray"
{CGSize="width"d"height"d}
@56@0:8@16@24@32@40q48
{CGSize=dd}48@0:8@16{?=qq}24o^@40
{CGSize=dd}16@0:8
v32@0:8{CGSize=dd}16
@"NSObject<OS_dispatch_group>"
@"NSObject<OS_dispatch_queue>"
@"<NUFaceDetectionResult>"
q24@0:8@16
@24@0:8q16
@24@0:8@"NSDictionary"16
@"NSDictionary"16@0:8
@48@0:8{?=qiIq}16d40
@48@0:8@16{?=qiIq}24
v24@0:8@"NSCoder"16
@24@0:8@"NSCoder"16
@48@0:8q16q24q32d40
@32@0:8@16#24
{?="columns"[3]}
@88@0:8{?=qiIq}16{?=[3]}40
{?=[3]}16@0:8
@"NUKeyframeSequenceMatrixFloat33"
{?=[3]}40@0:8{?=qiIq}16
@"AVAsset"
@"NSArray"16@0:8
@72@0:8@16{?={?=qq}{?=qq}}24Q56@64
v24@0:8Q16
@28@0:8@16B24
{?={?=qq}{?=qq}}96@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{?={?=qq}{?=qq}}48d80d88
{?=ddd}56@0:8@16{?={?=qq}{?=qq}}24
@"NUBufferRenderClient"
@"NUImageDataClient"
{?={?=[4d]}{?=[4d]}d}48@0:8@16@24d32@40
B48@0:8{?=[4d]}16
{?=[4d]}48@0:8{?=[4d]}16
{?=[4d]}88@0:8{?={?=[4d]}{?=[4d]}d}16
{?=dd}104@0:8{?={?=[4d]}{?=[4d]}d}16@88d96
{?={?=[4d]}{?=[4d]}d}16@0:8
@88@0:8{?={?=[4d]}{?=[4d]}d}16
{?=[4d]}16@0:8
@48@0:8{?=[4d]}16
@"NSArray"32@0:8@16^@24
B40@0:8@16@"NSMutableDictionary"24^@32
@"NSNumber"32@0:8@"NSDictionary"16^@24
B40@0:8@"NSNumber"16@"NSMutableDictionary"24^@32
@32@0:8@16^@24
B40@0:8@16@24^@32
@"NUImageExportRequest"
{?="width"q"height"q}
{?=qq}16@0:8
v32@0:8{?=qq}16
@"NUImageGeometry"
@"NUPriority"
@"NUColorSpace"
@"<NUScalePolicy>"
@"NUImageExportFormat"
v48@0:8@16@24@32@?40
v40@0:8@16@24@?32
@48@0:8@16@24@32@?40
@72@0:8@16@24@32@40@48@56@?64
@48@0:8@16@24@32^@40
v64@0:8@16@24@32@40@48@?56
{CGRect={CGPoint=dd}{CGSize=dd}}24@0:8@16
{CGRect={CGPoint=dd}{CGSize=dd}}56@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16@48
{CGRect={CGPoint=dd}{CGSize=dd}}64@0:8q16{CGRect={CGPoint=dd}{CGSize=dd}}24@56
@48@0:8@16@24@32@40
@48@0:8Q16Q24Q32@40
@40@0:8Q16Q24Q32
v64@0:8{?={?=qiIq}{?=qiIq}}16
@"PTTimedRenderingMetadata"
@40@0:8@16I24I28o^@32
@40@0:8@16@24#32
@"PIFaceObservationCache"
@"PIFaceObservationCache"16@0:8
v24@0:8@"PIFaceObservationCache"16
v36@0:8@16@24B32
f24@0:8@16
B32@0:8{?=qq}16
B40@0:8@16{?=qq}24
v48@0:8q16^d24^d32^d40
B32@0:8{CGSize=dd}16
{?="exposure"d"contrast"d"brightness"d"shadows"d"highlights"d"black"d"rawHighlights"d"localLight"d}
@36@0:8@16@24B32
@52@0:8@16@24@32q40B48
@44@0:8@16{CGSize=dd}24B40
^{CGImage=}24@0:8@16
@40@0:8@16{CGSize=dd}24
@40@0:8@16Q24^{CGColorSpace=}32
@"<NURenderer>"
@"<NUSurfaceStorage>"
@"NSObject<OS_dispatch_semaphore>"
@"NSMutableArray"
@"NSError"
@48@0:8{?=qq}16@32Q40
B40@0:8@16@24o^@32
@64@0:8@16@24{?={?=qq}{?=qq}}32
B60@0:8@16i24^{CGColorSpace=}28@36@44o^@52
@"VNImageHomographicAlignmentObservation"16@0:8
@"VNImageHomographicAlignmentObservation"
@"PTCinematographyScript"
@"NSCache"
@44@0:8@16@24@32B40
@60@0:8@16@24B32{CGSize=dd}36q52
^{CGColorSpace=}16@0:8
{vector<float, std::allocator<float>>=^f^f{__compressed_pair<float *, std::allocator<float>>=^f}}24@0:8@16
@48@0:8r^f16r^f24r^f32r^f40
@"NUFaceDetectionRequest"
@40@0:8@16{?=qq}24
^{CGColorSpace=}
@"NSMutableData"
@68@0:8^v16Q24Q32q40Q48i56^{CGColorSpace=}60
@68@0:8@16Q24Q32q40Q48i56^{CGColorSpace=}60
^v16@0:8
v24@0:8^{?=Bffff[3f]}16
^f16@0:8
@24@0:8^f16
@32@0:8d16@24
@36@0:8@16B24#28
@20@0:8B16
@36@0:8d16d24B32
@"CIVector"
@104@0:8@16{?=[3]}24{CGRect={CGPoint=dd}{CGSize=dd}}72
@40@0:8@16@24d32
B24@0:8Q16
@32@0:8q16@24
@24@0:8Q16
B32@0:8^{?={?=qq}{?=qq}}16o^@24
B48@0:8^{CGRect={CGPoint=dd}{CGSize=dd}}16^{CGRect={CGPoint=dd}{CGSize=dd}}24@32o^@40
{CGRect={CGPoint=dd}{CGSize=dd}}96@0:8{?=qq}16{?=qq}32{?=qq}48{CGRect={CGPoint=dd}{CGSize=dd}}64
{?="p75"d"p98"d"autoValue"d"g98"d}
{?="sat"d"contrast"d"cast"d}
v36@0:8^f16f24f28f32
f24@0:8r^f16
d40@0:8d16d24d32
@"PIReframeKeyframeSequence"
@"<NUVideoProperties>"
@64@0:8@16{?={?=qq}{?=qq}}24@56
@144@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24{CGRect={CGPoint=dd}{CGSize=dd}}56{?=[3]}88@136
{CGVector="dx"d"dy"d}
v32@0:8{CGVector=dd}16
v64@0:8{?=[3]}16
{CGVector=dd}16@0:8
@"AVAssetTrack"
{CGAffineTransform="a"d"b"d"c"d"d"d"tx"d"ty"d}
@24@0:8r^{FigLivePhotoMetadata=I{FigLivePhotoMetadataV1Struct=fqffffffccSI[0{FigLivePhotoDetectedFaceV1Struct=qffffisS}]}}16
{CGVector=dd}24@0:8r^{FigLivePhotoMetadata=I{FigLivePhotoMetadataV1Struct=fqffffffccSI[0{FigLivePhotoDetectedFaceV1Struct=qffffisS}]}}16
{?=[3]}32@0:8r^{FigLivePhotoMetadata=I{FigLivePhotoMetadataV1Struct=fqffffffccSI[0{FigLivePhotoDetectedFaceV1Struct=qffffisS}]}}16^B24
B24@0:8d16
@48@0:8@16{CGPoint=dd}24d40
B32@0:8^{CGImage=}16@24
B40@0:8^{CGImage=}16@24@32
@40@0:8@16@24@32
{?={?=qq}{?=qq}}40@0:8{CGPoint=dd}16d32
v40@0:8r^f16^S24Q32
v40@0:8r^S16^f24Q32
{?="major"i"minor"i}
@48@0:8f16f20f24f28Q32{?=ii}40
{?=ii}16@0:8
v24@0:8{?=ii}16
@48@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16
@48@0:8@16@24q32@40
@52@0:8@16@24@32f40q44
@40@0:8@16q24@32
@64@0:8@16@24@32@40q48o^@56
@52@0:8@16@24@32B40o^@44
@44@0:8@16@24B32o^@36
@36@0:8@16B24@28
@64@0:8{?={?=qiIq}{?=qiIq}}16
@32@0:8d16d24
{CGPoint=dd}44@0:8i16d20d28@36
ffffff
