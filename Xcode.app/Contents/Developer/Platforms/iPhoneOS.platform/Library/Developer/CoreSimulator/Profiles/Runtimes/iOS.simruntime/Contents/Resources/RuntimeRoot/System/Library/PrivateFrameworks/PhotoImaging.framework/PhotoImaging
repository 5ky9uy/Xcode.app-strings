?F]k
433333
?ffffff
?YAH
@<,_
?ffffff
?333333
333333
333333
?UUUUUU
?______
?TTTTTT
yE>)\
Ww'&l
MbP?
333333
?ffffff
?ffffff
?333333
333333
ffffff
333333
333333
@UUUUUU
@333333
?ffffff
z>UUUUUU
?YAH
?YAH
pCh?
zt?{
p?33
UUUUUU
?UUUUUU
4@333333
sU?gDi?
z?-C
Mb@?
UUU?
]?~0d>
@(#)PROGRAM:PhotoImaging  PROJECT:Neutrino-512.0.160
0Xr
?g|_\
ADBE
mntrRGB XYZ 
;acspAPPL
none
-ADBE
cprt
2desc
kwtpt
bkpt
rTRC
gTRC
bTRC
rXYZ
gXYZ
bXYZ
text
Copyright 2000 Adobe Systems Incorporated
desc
Adobe WGG linear
XYZ 
XYZ 
curv
XYZ 
XYZ 
iXYZ 
#?6t>=
<Xa<
<R>)<
A><QMI<
N-=y#3=*
OE=&
N,>;
2>F(6>
<>&T@>u
dN>;
Gl>#.p>
?Jy!?
V*?9%,?w
1?l{3?!X5?
77?"
NF=O
 >Me!>
">LP#>
)>k'*>"
^+>w
,>o+->D
->O\.>
1>uv2>N
T5>y
5>pw6>
7>t'8>M
D9>F
`:> 
z;>%
=>34>>Y
>>=H?>
A> zB>
E>>"F>]
F>9*G>
0H>K
;K>o
<L>l
;M>Z
M>;:N>
O>U2P>
P>k,Q>
Q>1%R>0
gW>i
HY>#
Y>&7Z>)
$[>$
]>B[^>
Mc>r
0d>h
f>cCg>
ni>r
Jj> 
%k>G
l>oFm>
n>gao>
q>!wq>L
q>4Kr>
+u>`
u>Acv>
x>35y>
z>U3{>
{>(c|>
,}>h
?%"?
+?Y5?
>?|H?
n?Bx?
< ?(D ?8L ?GT ?W\ ?gd ?vl ?
!?F%!?E-!?D5!?C=!?AE!?@M!?PU!?O]!?Me!?Lm!?Ku!?9}!?8
,"?v4"?u<"?cD"?QL"??T"?-\"?
9#?lA#?ZI#?7Q#?
x#?g
<$?kD$?8L$?
c$?{k$?Gs$?
%?B&%?
=%?RE%?
\%?Rd%?
{%?A
&?c%&?
<&?1D&?
S&?B[&?
j&?Tr&?
"'?9*'?
1'?~9'?)A'?
H'?nP'?
_'?Mg'?
v'?,~'?
(?l%(?
4(?)<(?
C(?^K(?
i(?.q(?
x(?R
)?R&)?
-)?T5)?
<)?gD)?
K)?iS)?
Z)?kb)?
i)?~q)?
*?1%*?
,*?"4*?
C*?{J*?
Q*?mY*?
`*?^h*?
o*?>w*?
+?)"+?
0+?a8+?
?+?0G+?
V+?h]+?
d+?7l+?
{+?o
+,?a3,?
B,?fI,?
X,?l_,?
n,?qu,?
-?$%-?|,-?
;-?_B-?
P-?CX-?
n-?\u-?
+.?<2.?
G.?3O.?zV.?
d.?+l.?`s.?
6/?K=/?
Y/?5a/?jh/?
?0?AF0?fM0?
q0?!x0?E
?1?,F1?@M1?TT1?h[1?|b1?
=2?1D2?4K2?7R2?KY2?N`2?Qg2?Tn2?Xu2?[|2?^
3?t$3?w+3?j23?m93?_@3?cG3?UN3?XU3?K\3?=c3?@j3?2q3?6x3?(
V4?t]4?gd4?Hk4?;r4?
35?`:5?BA5?$H5?
c5?{j5?\q5?>x5? 
#6?o*6?@16?
L6?eS6?GZ6?
u6?[|6?,
7?|&7?<-7?
A7?oH7?/O7?
c7?Qj7?
~7?b
8?E
'8?E.8?
;8?tB8?4I8?
V8?d]8?$d8?
q8?Sx8?
&9?w-9?&49?
A9?4H9?
U9?B\9?
i9?Pp9?
}9?^
:?5$:?
1:?"8:?
>:?oE:?
R:?\Y:?
f:?Hm:?
z:?$
;?$;?
3;?X:;?
G;?#N;?
T;?O[;?
a;?{h;?
u;?5|;?
<?f
-<?+4<?
:<?FA<?
G<?QN<?
T<?l[<?
u<? |<?
%=?I,=?
2=?T9=?
?=?NF=?
L=?XS=?
Y=?R`=?
f=?Lm=?
s=?Wz=?
">?A)>?
/>?*6>?
<>?$C>?
P>?zV>?
\>?cc>?
i>?Lp>?
v>?5}>?
*??]1??
7??6>??
K??zQ??
W??B^??
w??N~??
*@?H1@?
=@?[D@?
J@?"Q@?~W@?
]@?4d@?
p@?Ww@?
)A?o/A?
<A?pBA?
OA?aUA?
bA?bhA?
uA?S{A?
B?kB?
+B?+2B?v8B?
EB?UKB?
WB?5^B?odB?
qB?OwB?
C?u C?
,C?33C?m9C?
EC?+LC?eRC?
^C?#eC?]kC?
~C?U
&D?M,D?v2D?
ED?MKD?
]D?$dD?MjD?
0E??6E?h<E?
UE?6[E?_aE?
zE?,
,F?+2F?T8F?l>F?
]F?0cF?IiF?aoF?yuF?
G?S G?l&G?
PG?WG?%]G?=cG?DiG?]oG?uuG?|{G?
H?)%H?A+H?H1H?P7H?W=H?pCH?wIH?
I?S"I?[(I?Q.I?Y4I?`:I?W@I?_FI?ULI?]RI?SXI?[^I?cdI?YjI?apI?WvI?N|I?V
SJ?}YJ?t_J?keJ?akJ?XqJ?OwJ?F}J?,
)K?w/K?n5K?T;K?JAK?0GK?'MK?
vK?x|K?o
3L?i9L?>?L?$EL?
bL?whL?]nL?CtL?
M?:$M? *M?
AM?ZGM?/MM?
dM?{jM?PpM?%vM?
0N?~6N?S<N?
SN?\YN?1_N?
pN?dvN?9|N?
O?1$O?
/O?~5O?T;O?
LO?eRO?*XO?
cO?wiO?;oO?
!P?t'P?8-P?
8P?u>P?(DP?
OP?eUP?)[P?
fP?UlP?
}P?E
Q?E
Q?p#Q?$)Q?
4Q??:Q?
EQ?ZKQ?
VQ?t\Q?(bQ?
mQ?CsQ?
~Q?^
R?s
R?}#R?0)R?
4R?*:R?
ER?4KR?
VR?-\R?
gR?&mR?
rR?}xR? ~R?
'S?<-S?
8S?%>S?
CS?kIS?
TS?SZS?
eS?+kS?
pS?qvS?
/T?H5T?
:T?}@T?
KT?DQT?
VT?y\T?
gT?@mT?
rT?uxT?
U?h%U?
;U?BAU?
FU?ULU?
QU?yWU?
bU?0hU?
mU?CsU?
xU?g~U?
$V?=*V?
/V?P5V?
:V?d@V?
EV?wKV?
VV?\V?
lV?"rV?
wV?5}V?
W?}"W?
SW?yYW?
^W?|dW?
iW?moW?
tW?pzW?
X?>$X?
)X?0/X?
4X?!:X?
OX?eUX?
ZX?W`X?
eX?8kX?
pX?)vX?
Y?l$Y?
)Y?</Y?
DY?oJY?
OY??UY?
ZY? `Y?
kY?apY?
uY?B{Y?
#Z?g(Z?
-Z?73Z?
>Z?gCZ?
HZ?8NZ?
XZ?h^Z?
cZ?'iZ?
sZ?WyZ?
[?! [?p%[?
*[?/0[?
:[?O@[?
K[?]P[?
[[?|`[?
e[?+k[?
u[?J{[?
!\?g&\?
1\?e6\?
A\?cF\?
Q\?`V\?
a\?^f\?
p\?\v\?
]?m ]?
+]?Y0]?
:]?F@]?
J]?"P]?qU]?
`]?^e]?
o]?:u]?
(^?,.^?j3^?
=^?FC^?
S^?OX^?
h^?Ym^?
w^?$}^?s
_?>%_?|*_?
4_?6:_?u?_?
I_?/O_?\T_?
d_?Ui_?
y_?=~_?{
 `?[%`?
4`?2:`?`?`?
O`?7T`?dY`?
i`?;n`?is`?
#a?$)a?Q.a?
Ca?4Ha?bMa?
ba?Ega?rla?
b?|b?
!b?>&b?Z+b?
Eb?IJb?fOb?
cb?'ib?Dnb?qsb?
,c?<1c?Y6c?u;c?
Uc?2Zc?N_c?kdc?
d?:!d?F&d?c+d?
Td?2Yd?>^d?Zcd?fhd?
$e?5)e?A.e?L3e?X8e?u=e?
ze? 
f?[f?f
g?xg?s
g?u g?
%g?|*g?w/g?
4g?~9g?y>g?
Hg?{Mg?
Wg?}\g?xag?
kg?zpg?uug?
zg?|
sh?uxh?p}h?k
Mi?~Ri?iWi?d\i?Nai?8fi?3ki?
&j?k+j?U0j??5j?;:j?%?j?
aj?ffj?Qkj?;pj?%uj?
%k?|*k?f/k?P4k?*9k?
Vk?`[k?J`k?4ek?
l?S#l?,(l?
;l?}@l?VEl?0Jl?
bl?Zgl?3ll?
m?=m?
m?l$m?E)m?
<m?tAm?NFm?
Ym?l^m?Ecm?
vm?t{m?=
n?%$n?
2n?X7n?2<n?
Jn?fOn?.Tn?
bn?sgn?<ln?
zn?o
o?l"o?5'o?
5o?X:o?
Ho?jMo?3Ro?
`o?Veo?
so?hxo?1}o?
p?Ap?
-p?U2p?
@p?VEp?
Sp?XXp? ]p?
fp?Ykp?"pp?
yp?Z~p?#
q?Z
q?J$q?
-q?r2q?*7q?
@q?cEq?
Sq?SXq?
aq?{fq?3kq?
tq?[yq?
,r?@1r?
:r?W?r?
Hr?nMr?&Rr?
[r?=`r?
ir?Tnr?sr?
wr?l|r?$
 s?>%s?
.s?D3s?
<s?JAs?
Js?QOs?
Xs?W]s?
fs?]ks?
ts?dys?
t?K!t?
*t?@/t?
8t?6=t?
Ft?+Kt?
Ot?zTt?!Yt?
]t?obt?
kt?ept?ut?
yt?J~t?
 u?F%u?
.u?+3u?
7u?h<u?
Eu?MJu?
Su?2Xu?
\u?pau?
ju?Tou?
xu?(}u?
v?9v?
v?:#v?
'v?w,v?
5v?K:v?
>v?xCv?
Lv?LQv?
Uv?yZv?
cv?Mhv?
lv?zqv?
zv?N
w?t$w?
-w?82w?
6w?T;w?
Mw?DRw?
Vw?`[w?
dw?#iw?
mw??rw?
vw?l{w?
x?O$x?
(x?k-x?
6x?;x?
?x?(Dx?
Hx?DMx?
Qx?`Vx?
Zx?|_x?
qx?:vx?
zx?V
"y?='y?
+y?Y0y?
4y?e9y?
=y?pBy?
Fy?{Ky?
]y?(by?
fy?3ky?
oy??ty?
xy?J}y?
z?W$z?
(z?b-z?
1z?m6z?
:z?h?z?
Cz?sHz?
Lz?nQz?
Uz?hZz?
^z?scz?
gz?nlz?
pz?yuz?
yz?t~z?
{?! {?
L{?zQ{?
U{?dZ{?
^{?^c{?
g{?Yl{?
p{?Cu{?
y{?=~{?
0|?i5|?
9|?S>|?
B|?=G|?
K|?'P|?
a|?of|?
j|?Xo|?
s|?Bx|?
||?,
}?%!}?
)}?s.}?
2}?]7}?
;}?5@}?
Q}?\V}?
Z}?5_}?
p}?\u}?
y}?F~}?
!~?>&~?
/~?{3~?
7~?S<~?
@~?,E~?
M~?YR~?
V~?1[~?
d~?nh~?
l~?7q~?
z~?t~~?
333333
?333333
?ffffff
Fail: %{public}@
job: %{public}@
Trace:
%{public}@
Trace:
%{public}@
Tap-to-track: client requested stop at %@
Tap-to-track: tracking lost at %@
PIColorNormalizationAnalysis
Tried to set sceneLabel to unsupported value: %ld
Setting bounding boxes based on orientation (%@) and observations: %@
Setting face bounding boxes based on orientation (%@) and observations: %@
Continue: %{public}@
Failed to load compute pipeline: %@
Failed to load user palette '%@', error: %@
class %@ is not the correct type, its superclass should be %@
Unable to serialize finalized composition: %{public}@
Error executing command buffer '%{public}@': %{public}@
CPV rendering failure, returned status %d
Could not apply PIPortraitVideoProcessor: %@
Failed to prewarm portrait renderer: %{public}@
Failed to obtain video properties: %{public}@
Using recipe directory at '%{public}@'
Failed to load style recipe for identifier '%@', error: %@
Missing configuration file '%{private}@'
PARALLAX CLOCK: luminance is %f - %@
ColorBGStandard: suggested %@ for background luminance of %.2f
ColorBG color: %@ -> neutral: %@ lowKey: %0.3f highKey: %0.3f
Couldn't find a single candidate style for category %{public}@, falling back to Original
Median luminance: %f
Percent above chroma min: %0.0f%%, max hues: %ld
Found %ld dominant hues: %@
Found %ld dominant grays: %@
facerect yiq = %.5f, %.5f, %.5f
Choosing gray world instead of gray edge
Brightness is %@, greenChannelPercentage is %f, Sushi: %@
aperture=%@, shutterSpeed=%@, iso=%@
Buffer sizes are not the same: %{public}@ vs %{public}@
Buffer size is %@, bytes per row is %ld, dark_thr=%f, sat_thr=%f, noise_thr=%f
all done summing, np_gw is %d, np_ge is %d
wp_ge {%f, %f, %f, %f} wp_gw {%f, %f, %f, %f}, Sushi? %@
RGB = %f, %f, %f
YIQ = %f, %f, %f from %f, %f, %f
Failed to export image to %@: %@
Failed to export image to data: %@
Failed to prepare video metadata: %@
Unexpected Live Photo export format pairing. Video codec (%{public}@) and image export format (%{public}@)
Export Composition: exportImage: %{public}@ / exportVideoComplement: %{public}@ / exportVideo: %{public}@ / exportVideoPosterFrame: %{public}@
Export Composition: exporting image
Export Composition: exporting image complete
Export Composition: exporting video complement
Export Composition: exporting video complement complete
Export Composition: exporting video
Export Composition: exporting video complete
Export Composition: exporting video poster frame
Export Composition: exporting video poster frame complete
Export Composition: exporting overcapture image
Export Composition: exporting overcapture image complete
Export Composition: exporting overcapture video
Export Composition: exporting overcapture video complete
Export Composition: waiting on notify
Export Composition: notify triggered
Export Composition: calling completion with error: %{public}@
Export Composition: callling completion with result: %{public}@
failed to render auxiliary image data: %@
Skipping applyVideoOrientationAsMetadata. Bounces and Autoloops are not supported. Falling back to burned-in orientation.
Skipping applyVideoOrientationAsMetadata. Flipped orientations are not supported. Falling back to burned-in orientation.
Failed to export video: %@
invalid format version: %@
Failed to update layout: %{public}@
Failed to compute clock overlap: %{public}@
Failed to compute clock material: %{public}@
Failed to render layer stack, error: %{public}@
Pixel-based headroom zoom final range %@,%@: %@,%@
Unable to calculate a new inactiveRect; falling back to visible frame
PIParallaxLegacyPosterStyle.localLight
PIPortraitVideoMetadataSample: unexpected number of items for identifier %@: %@
PIPortraitVideoMetadataSample: item %@ is not of expected class %@
Error unarchiving cameraInfo metadata: %@
CINE: allocating new PT renderer: %@
CINE: recycling unused PT renderer: %@ (%lu remaining)
CINE: need new render state due to rendering version mismatch
CINE: need new render state due to sensor mismatch
CINE: allocating new renderState with metadata: %p
Requesting forced cleanup of Vision caches
Unsupported filter parameter: %{public}@
Parameter %{public}@ is not a valid color value: %{public}@
Parameter %{public}@ is not a valid number value: %{public}@
Error evaluating filter definition: %@, error: %@
CPV asset editability: onAppleSilicon: %s, hasMetalDeviceForPortrait: %s, oniOS: %s
CPV assets aren't editable because there is no metal device with support for portrait rendering
CPV assets aren't editable because the device has no ANE
CPV assets are editable
HDR asset not editable because this device doesn't support 10-bit HEVC encoding
HDR asset not editable because this device doesn't support HDR
HDR asset not editable because editing not supported on DolbyVision 5
CPV asset checking for editability
CPV asset not editable because it's in a legacy, unsupported format
CPV asset editable due to override
CPV asset not editable because CPV assets are not editable on this device
CPV asset not editable because this asset is not playable on this device
CPV asset not editable because this device doesn't support HDR
CPV asset is editable
validation issue: recipe is blank on the autoloop adjustment
validation issue: no flavor specified on the autoloop adjustment
validation issue: Missing inputCorrectionInfo on a redeye adjustment
validation issue: Missing depthInfo on a depth adjustment
validation issue: Missing portraitInfo on a portrait adjustment
validation issue: invalid trim startTime or endTime
Can PLPhotoEditPFDataConverter interpret identifier %{public}@? %@. Version %{public}@? %@
Failed to load filter named '%@'
PFSizeGetAspectRatio produced an undefined aspect ratio from size %@. Returning %f. Use PFSizeGetAspectRatioWithDefault() to provide a value for this case.
PARALLAX CLOCK: execution time %.2f ms
waitUntilReadyForMoreData: waited for %0.1fms
Writing long-exposure image to %{public}@
Writing long-exposure motion mask to %{public}@
Still image node:
Still image geometry:
Still image:
clap=%@, crop=%@, fullSize=%@, videoScale=%@ => guide rect: %@
High-resolution image registration failure : %@
Unable to load scoring ranges dictionary from %@, error %@
Unable to load scoring plist, using fallback
Failed to load scoring configuration: %@
MediaAnalysis not available
currentSystemCanRenderAsset: error retrieving rendering version from asset: %@
Failed to render %{public}@, error: %{public}@
Failed to allocate pixel buffer for render cache entry (size=%ldx%ld, format=%{public}@)
Cache miss for image: %{public}@ cost: %lu digest: %llx
Loading long-exposure fusion tuning parameters from file: %@
Failed to load fusion tuning parameters.
Using fusion tuning parameters: {kNCCBlurHalfSize=%d, kNCCEdge0=%f, kNCCEdge1=%f, kBlendMaskThreshold0=%f, kBlendMaskThreshold1=%f}
Missing inputImage
Missing inputMaskImage
Missing inputStillImage
Missing inputRenderScale
Missing inputVideoScale
Missing inputAlignmentExtent
Malformed inputAlignmentExtent vector
Missing inputAlignmentTransform
Malformed inputAlignmentTransform vector
Writing intermediate long exposure fusion images to : %@
Evaluating pipeline as HDR input
PISegmentationLoader.memory.warmUp
Ensuring segmentation resources with counter %ld
Freeing segmentation loader resources with counter %ld
PISegmentationLoader.memory.purge
PISegmentationLoader.item
Warning: PISegmentationLoader layout configuration unspecified! Using override layout configuration '%{public}@'
Warning: Override layout configuration '%{public}@' not found, using generic fallback
Warning: PISegmentationLoader layout configuration unspecified! Using the layout configuration matching this device
PISegmentationLoader.proxy
PISegmentationLoader.properties
PISegmentationLoader.fullSize
Failed to compute proxy image size, error: %{public}@
Cancelling segmentation loader
Triggering non-foreground user initiated download for asset with local identifier: %{public}@
Loading resource %ld for asset %{public}@, allow download? %d
Successfully loaded resource %ld for asset %{public}@ in %0.3fs
Failed to load resource %ld for asset %{public}@ after %0.3fs, error: %{public}@
Cancelled loading resource %ld for asset %{public}@ after %0.3fs
PISegmentationLoader.segment
PISegmentationLoader.regions
PISegmentationLoader.infill
PISegmentationLoader.layout
PISegmentationLoader.colorAnalysis
PISegmentationLoader.localLightData
Image segmentation response: %@
Image segmentation failed: %{public}@
Full Image color analysis response: %@
Full Image color analysis failed: %{public}@
Foreground color analysis response: %@
Foreground color analysis failed: %{public}@
Background color analysis response: %@
Background color analysis failed: %{public}@
Parallax infill response: %@
Parallax infill failed: %{public}@
MAD pets results: %@, pets face results: %@, error: %@
Failed to load pets regions: %{public}@
Vision detection response: %@
Failed to run face/saliency detection: %{public}@
Parallax layout response: %{public}@
Failed to layout item: %{public}@
Local light data response: %{public}@
Failed to compute local light data: %{public}@
Failed to deserialize segmentation adjustment data: %{public}@
Failed to read orientation for image file: %@, error: %{public}@
PISegmentationLoader.data.read
Failed to read cached segmentation data from: %{public}@, error: %{public}@
Cached segmentation version mismatch: got %ld, expected %ld
Cached segmentation source mode mismatch: got %ld, expected %ld
Cached segmentation disabled flag mismatch: got %d, expected %d
Cached segmentation infill algorithm mismatch: got %ld, expected %ld
Cached segmentation layout configuration mismatch: got %{public}@, expected %{public}@
Cached segmentation classification mismatch: got %{public}@, expected %{public}@
PISegmentationLoader.data.write
Failed to save segmentation data for asset: %{public}@, error:%{public}@
PISegmentationLoader.archive.write
PISegmentationLoader.archive.read
PISegmentationLoader.layerStack.render
PISegmentationLoader.wallpaper.write
Failed to save segmentation item and layer stack to wallpaper URL: %{public}@
Failed to create wallpaper directory: %{public}@
Failed to export segmentation item: %{public}@
PISegmentationLoader.layerStack.write
Failed to export layer stack: %{public}@
Failed to load segmentation item from wallpaper: %{public}@
PISegmentationLoader.layerStack.read
Failed to load layer stack from wallpaper: %{public}@
Failed to render layer stack: %{public}@
Failed to reload segmentation item from wallpaper: %{public}@
Beginning finalization for candidates: %@
No overcapture source found in composition, removing any candidate bits for reframing
Finalizer result contains roll angle degrees: %f pitch angle degrees: %f yaw angle degrees: %f
Finalizer result did not contain a change
Received error while finalizing: %@
Starting horizon auto calculator
Finished horizon auto calculator: %@
Starting perspective auto calculator
Finished perspective auto calculator: %@
Could not write PICropAutoCalculator debug diagnostics. %@
Failed to compute overcapture rect %@
Can't find enabled video track in asset: %@
Asset does not contain Live Photo metadata track
Track does not contain V3 metadata
Error creating asset reader: %@
Can't add metadata output for asset
Failed to start reading asset
Starting metadata extraction
Finished metadata extraction
PISmartToneAutoCalculator submitSynchronous isHDRComposition: %s
PISmartToneAutoCalculator smartTone request submitting: %{public}@
PISmartToneAutoCalculator smartTone result: %{public}@
PISmartToneAutoCalculator localLight request submitting: %{public}@
PISmartToneAutoCalculator localLight result: %{public}@
PISmartToneAutoCalculator submit isHDRComposition: %s
PISmartToneAutoCalculator global result: %{public}@
PISmartToneAutoCalculator going to commit and wait
PISmartToneAutoCalculator committed
PISmartToneAutoCalculator completed
PISmartToneAutoCalculator error: %{public}@
PISmartToneAutoCalculator coalesced
Failed to deserialize layout configuration: %{public}@, error: %{public}@
Can load cold asset? %{public}@ => %{public}@
Failed to deserialize style from dictionary: %{public}@, error: %{public}@
Unknown style option, ignored: %{public}@
Upgrading wallpaper at %{public}@ to %{public}@, options: %{public}@
Failed to export refreshed wallpaper at %{public}@ to %{public}@, error: %{public}@
Successfully exported refreshed wallpaper at %{public}@ to %{public}@
Failed to load poster configuration from: '%{public}@', error: %{public}@
Failed to upgrade poster configuration from: '%{public}@' to: '%{public}@', error: %{public}@
Successfully upgraded poster configuration from: '%{public}@' to: '%{public}@'
Upgrading poster media: SETTING .hasInactiveContent
Upgrading poster media: CLEARING .hasInactiveContent
Upgrading poster media: %{public}@
Successfully upgraded poster media: %{public}@
Failed to upgraded poster media: %{public}@, error: %{public}@
Failed to upgrade %lu poster media
Successfully upgraded %lu poster media
Successfully upgraded poster configuration from '%{public}@' to '%{public}@'
Failed to save poster configuration to '%{public}@', error: %{public}@
Failed to apply red eye repair. error: %{public}@
PIPortraitAutoCalculator getMinMaxSimulatedAperture returned error:%i (minAperture:%f maxAperture:%f version:%d)
Cant get focusRect - Metadata dictionary missing fullSizeWith or fullSizeHeight:
Cant get focusRect - Metadata dictionary missing exif aux dictionary:
Cant get focusRect - Exif aux dictionary missing MWG region dictionary:
Cant get focusRect - MWG region dictionary missing region list:
Cant get focusRect - Region list does not contain a focus rect:
Cant get focusRect - Malformed focus rect dictionary:
Invalid focus rect: {%g,%g,%g,%g}
Couldn't deserialize face observations from metadata: %@, assuming empty, error: %@.
Invalid face indexes from metadata: %@, ignored. Error: %@
Couldn't deserialize face observations from metadata: %@
s-curve black: %f
s-curve point %d: (%f, %f)
s-curve white: %f
red curve: blackPoint = %f, whitePoint = %f
green curve: blackPoint = %f, whitePoint = %f
blue curve: blackPoint = %f, whitePoint = %f
Initializer not available: -[%@ %@], use designated initializer instead.
-[PIParallaxInfillJob initWithRequest:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Parallax/PIParallaxInfillRequest.m
Missing matte image
Invalid matte image size
Invalid matte image
Output image must not be nil
-[PIParallaxInfillJob render:]
Matte image must not be nil
Failed to generate background infill image
Failed to allocate buffer from pool
failed to render
Invalid parameter not satisfying: %s
-[PITapToTrackRenderJob prepare:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Autocalculators/PITapToTrackRequest.m
No metal device for request
Failed to find output video asset
Failed to make asset reader for video
Failed to read first frame for video
Failed to find rect to track at initial point
Failed to add detection and start tracking
Failed to get finalized track from tracking session
kernel vec4 _hdrOffsetPosBW(sampler image) { vec4 im = sample(image, samplerCoord(image)); float offset = (im.r + im.g + im.b) / 3.0f; offset = max(0.0f, offset - 1.0f); return vec4(offset, offset, offset, 0.0f); }
kernel vec4 _hdrOffsetPos(sampler image) { vec4 im = sample(image, samplerCoord(image)); float r = max(im.r - 1.0f, 0.0f); float g = max(im.g - 1.0f, 0.0f); float b = max(im.b - 1.0f, 0.0f); return vec4(r, g, b, 0.0f); }
s_hdrOffsetPos kernel is nil
+[PIPhotoEffectHDR kernel]_block_invoke
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Adjustments/HDR/PIPhotoEffectHDR.m
inputImage cannot be nil
-[PIPhotoEffectHDR outputImage]
inputImage
CIAdditionCompositing
inputBackgroundImage
s_hdrOffsetPosBlackAndWhite kernel is nil
+[PIPhotoEffectBlackAndWhiteHDR kernelBlackAndWhite]_block_invoke
-[PIPhotoEffectBlackAndWhiteHDR outputImage]
+[PIPhotoEffect3DHDR kernel]_block_invoke
-[PIPhotoEffect3DHDR outputImage]
inputThreshold
inputDepthMap
+[PIPhotoEffect3DBlackAndWhiteHDR kernelBlackAndWhite]_block_invoke
-[PIPhotoEffect3DBlackAndWhiteHDR outputImage]
PhotoImaging
PIColorNormalizationFilter
PITempTintFilter
temperature
inputTemperature
tint
inputTint
PIHighKey
CISmartToneFilter
CISmartColorFilter
normalization != nil
+[PIColorNormalizationFilter colorCubeForNormalization:dimension:targetColorSpace:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Adjustments/PIColorNormalizationFilter.m
colorSpace != NULL
PIColorNormalization
candidateForStill
candidateForVideo
candidateForReframe
candidateForPerspective
candidateForHorizon
addRule exception : %@
v24@?0@"NSString"8@?<v@?@"NURuleSystem">16
v24@?0@"NSString"8@"NSString"16
isStitched
validSubjects
hasHorizonLine
hasBuilding
state.stitched == true OR state.perfectlyStitched == true
state.stitchConfidence < $StitchConfidenceThreshold
state.buildingCount > 0
state.buildingConfidence > $BuildingConfidenceThreshold
state.subjectCount >= $SubjectCountMinThreshold
state.allSubjectsInsidePrimaryBounds == true
(state.subjectCount - state.intersectingSubjectCount) > $MaxDisparateSubjects
state.largeSubjectFace == true
state.largestSubjectArea <= $MinimumSubjectSize
state.horizonLinePresent == true
state.horizonLineAngleInDegrees < -$HorizonAngleInDegreesMaxThreshold OR state.horizonLineAngleInDegrees > $HorizonAngleInDegreesMaxThreshold
state.horizonLineAngleInDegrees > -$HorizonAngleInDegreesMinThreshold AND state.horizonLineAngleInDegrees < $HorizonAngleInDegreesMinThreshold
state.video == true
state.videoDuration < $VideoDurationLowerBound
state.videoDuration > $VideoDurationUpperBound
(state.videoDuration * $VideoDurationWithSubjectsRatio) > state.videoDurationWithSubjects
state.videoQualityScore < $VideoQualityScoreThreshold
(state.videoDuration * $VideoDurationBelowMinZoomRatio) < state.videoDurationBelowMinZoom
facts.isStitched == true AND facts.validSubjects == true
facts.isStitched == true AND facts.hasBuilding == true
facts.isStitched == true AND facts.hasHorizonLine == true
state.largeSubject == true
state.backFacing == false
state.deviceIsStationary == true
state.video == true AND state.livePhoto == true
state.pano == true
state.reframingAllowed == false
state.hasAdjustments == true
facts.candidateForStill == true
facts.candidateForVideo == true
platedFood
sunriseSunset
genericLandscape
intensity
sceneLabel
sceneConfidence
faceBoundingBoxes
boundingBoxes
-[PIDisparitySampleJob render:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Autocalculators/PIDisparitySampleRequest.m
Failed to read video frame
Incompatible disparity buffer
inputStrength
inputScale
vec3 localContrast(vec3 im, vec3 shc, float amt) { float midAmt = amt; vec3 neg = min(im, 0.0); vec3 pos = max(im, 1.0)-1.0; im = clamp(im, 0.0, 1.0); float y = dot(im, vec3(0.3333)); y = sqrt(y); y = y*(1.0-y); im = sqrt(im); float pivot = sqrt(shc.g); float a = midAmt*y; float b = -pivot*a; vec3 pix = im.r * vec3(0.299*a) + im.g * vec3(0.587*a) + im.b * vec3(0.114*a) + im + vec3(b); im = mix(im, vec3(pivot), -y*midAmt); im = mix(im, pix, 0.8); im = max(im, 0.0); im *= im; im = im + neg + pos; return im; } vec3 localContrastHLG(vec3 im, vec3 shc, float hlg_scale, float amt) { return localContrast(im.rgb/hlg_scale, shc.rgb/hlg_scale, amt).rgb * hlg_scale; } kernel vec4 _localContrastHDR(__sample im, __sample shc, float hlg_scale, float amt) { float max_comp = max(im.r,max(im.g,im.b)); float threshold = 0.75 * hlg_scale; if (max_comp <= 1.0) { im.rgb = localContrast(im.rgb, shc.rgb, amt); } else if (max_comp < threshold) { vec3 retSDR = localContrast(im.rgb, shc.rgb, amt); vec3 retHDR = localContrastHLG(im.rgb, shc.rgb, hlg_scale, amt); float lerp_t = (max_comp - 1.0) / (threshold - 1.0); im.rgb = mix(retSDR, retHDR, lerp_t); } else { im.rgb = localContrastHLG(im.rgb, shc.rgb, hlg_scale, amt); } return im; }
CIColorMatrix
inputRVector
inputGVector
inputBVector
inputAVector
inputBiasVector
CIColorClamp
inputMinComponents
inputMaxComponents
CIEdgePreserveUpsampleFilter
inputSmallImage
inputSpatialSigma
inputLumaSigma
intermediate.visibleRect.size.width >= 1
-[PIParallaxLayoutHelper intermediateWithZoomStrategy:intermediate:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Parallax/PISegmentationLayout.m
overlapStrategy != PFParallaxUtilityOverlapForceMetadataAvoid
-[PIParallaxLayoutHelper intermediateWithOverlapStrategy:intermediate:]
<%@:%p accept=%@ pref=%@ faces=%@ pets=%@>
regions != nil
+[PISegmentationLayoutRegions dictionaryFromRegions:]
@"NSDictionary"40@?0{CGRect={CGPoint=dd}{CGSize=dd}}8
@"NSArray"16@?0@"NSArray"8
acceptable
preferred
faces
pets
+[PISegmentationLayoutRegions regionsFromDictionary:error:]
B24@?0@8^{CGRect={CGPoint=dd}{CGSize=dd}}16
Expected a rect value
B24@?0@"NSArray"8@"NSMutableArray"16
Expected an array of rect values
layoutConfiguration != nil
+[PISegmentationLayout generateLayoutForLayoutConfiguration:imageSize:regions:parallaxClassification:context:matte:layoutScore:cropScore:clockOverlapAcceptable:]
layoutConfiguration.screenSize.height > 0
imageSize.height > 0
<%@:%p layerOrder:%@ intersectsForeground:%@>
-[_PIParallaxClockLayoutJob initWithRequest:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Parallax/PIParallaxClockLayoutRequest.m
Missing segmentation item
-[_PIParallaxClockLayoutJob prepare:]
Missing parallax layout
Missing renderer
-[_PIParallaxClockLayoutJob render:]
This is an abstract method! Subclass '%@' should provide concrete implementation
-[PIParallaxClockLayoutRequest initWithComposition:]
-[PIParallaxClockLayoutRequest initWithSegmentationItem:]
kernel vec4 highKey(__sample im, float str)
vec3 neg = min(im.rgb, 0.0);
vec3 pos = max(im.rgb, 1.0) - 1.0;
im = clamp(im, 0.0, 1.0);
vec4 im2 = 1.0-((im-1.0)*(im-1.0));
im2 = sqrt(im2);
float y = dot(im.rgb, vec3(.333333));
float y2 = sqrt(1.0-(y-1.0)*(y-1.0));
y2 = mix(y2, smoothstep(0.0, 1.0, y2), 0.5);
vec4 im3 = (y>0) ? im*y2/y : vec4(0.0, 0.0, 0.0, 1.0);
im3 = mix(im3, im2, .7*sqrt(y2));
im3 = mix(im, im3, sqrt(y));
im.rgb = mix(im.rgb, im3.rgb, str) + pos + neg;
return im;
high key kernel is nil
+[PIHighKey kernel]_block_invoke
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Adjustments/PIHighKey.m
-[PIHighKey outputImage]
inputStrength cannot be nil
/System/Library/Frameworks/CoreImage.framework/inp_gen_eds2_00_q16.espresso.weights
CIInpaintingFilter
inputInpaintingMode
Metal unavailable
inputTexture != nil
+[PIParallaxInwardFillKernel fillSourceTexture:intoDestinationTexture:withCommandBuffer:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Parallax/PISegmentationInfillFilter.m
outputTexture != nil
commandBuffer != nil
Missing input texture
-[PIParallaxInwardFillKernel encodeToCommandBuffer:destinationTexture:]
Failed to allocate intermediate texture
pi::inward_fill_down
pi::inward_fill_up
LumaHueChroma
HueTone
v24@?0Q8^B16
version
mode
primaryColors
secondaryColors
suggestionIndices
url != nil
+[PIParallaxColorPalette loadPaletteFromURL:error:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Parallax/PIParallaxColorPalette.m
Failed to parse color palette plist data
+[PIParallaxColorPalette _paletteWithConfigurationDictionary:error:]
Invalid version number
Unsupported palette version
Invalid mode value
Invalid primary color values
Invalid secondary color values
Invalid suggestion indices
Invalid suggestion index
Missing suggestion indices
Unknown color mode: %@
+[PIParallaxColorPalette _serializeColors:mode:]
colorValues != nil
+[PIParallaxColorPalette _loadColorsFromValues:mode:error:]
Invalid color values
primaryColors != nil
-[PIParallaxColorPalette initWithPrimaryColors:secondaryColors:suggestionIndices:]
secondaryColors != nil
suggestionIndices != nil
suggestionIndices.lastIndex < primaryColors.count
Secondary color palette should be empty or equal in size to the primary palette
ColorBGPalette
ColorWashSinglePalette
ColorWashDuotonePalette
plist
Failed to load color palette '%@', error: %@
+[PIParallaxColorPalette loadPaletteWithName:]
Tonal colors: %@
B16@?0Q8
The palette can't be empty
-[PIParallaxColorPalette _lookupColor:withPredicate:]
Failed to find a nearest color
smartTone
smartColor
smartBlackAndWhite
grain
autoEnhance
whiteBalance
cropStraighten
redEye
apertureRedEye
depthEffect
livePhotoKeyFrame
videoPosterFrame
trim
slomo
portraitEffect
orientation
autoLoop
highResFusion
retouch
vignette
sharpen
noiseReduction
definition
curves
levels
selectiveColor
effect
effect3D
mute
rawNoiseReduction
source
videoReframe
debug
sourceSelect
sourceSpatialOvercapture
portraitVideo
videoStabilize
videoCrossfadeLoop
semanticEnhance
enabled
value
primary
PICompositionController(%p): %@
v16@?0@"PISmartToneAdjustmentController"8
com.apple.photo
adjustmentFormatIdentifier
adjustmentFormatVersion
adjustmentData
PICompositionSerializer.m
Invalid parameter not satisfying: %@
adjustments
metadata
file://dummy.jpg
dummy
identifier
PICompositionSerializerDomain
Missing identifierName
Missing definition in conversion map for the adjustment key: 
settings
Unsupported adjustment: 
Configuration does not support Aperature adjustments.
Configuration does not support CTM adjustments.
nil identifierName
inputKeys
omitIfDisabled
Orientation
PICompositionSerializer-geometry
Invalid adjustment stack for needsGeometry=NO
masterWidth
masterHeight
Serialization map has no entry for %@
+[PICompositionSerializer serializeComposition:versionInfo:serializerMetadata:error:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Controllers/Conversion/PICompositionSerializer.m
Serialization map does not contain identifierName
current
auto
formatIdentifier
formatVersion
versionInfo
Missing required key: %@
Value for key %@ has type %@; expected type %@
v32@?0@"NSString"8#16^B24
PICompositionSerializer
exception
dictionary
data is not an NSData: %@: %@
Adjustment for %@ is identity
+[PICompositionSerializer _sanitizeComposition:]
composition
CFBundleVersion
platform
buildNumber
appVersion
schemaRevision
kernel vec4 levelsHDR (sampler src, sampler LUT) { vec4 p,r; vec2 c1,c2; float f; p = sample(src, samplerCoord(src)); vec3 neg = min(p.rgb, 0.0); vec3 pos = max(p.rgb, 1.0)-1.0; p.rgb = clamp(p.rgb, 0.0, 1.0); f = p.r * 1024; c1 = vec2(floor(f)+0.5, 0.5); c2 = vec2(ceil(f)+0.5, 0.5); r = mix(sample(LUT,samplerTransform(LUT,c1)), sample(LUT,samplerTransform(LUT,c2)), fract(f)); p.r = r.r; f = p.g * 1024; c1 = vec2(floor(f)+0.5, 0.5); c2 = vec2(ceil(f)+0.5, 0.5); r = mix(sample(LUT,samplerTransform(LUT,c1)), sample(LUT,samplerTransform(LUT,c2)), fract(f)); p.g = r.g; f = p.b * 1024; c1 = vec2(floor(f)+0.5, 0.5); c2 = vec2(ceil(f)+0.5, 0.5); r = mix(sample(LUT,samplerTransform(LUT,c1)), sample(LUT,samplerTransform(LUT,c2)), fract(f)); p.b = r.b; p.rgb = max(p.rgb, 0.0); p.rgb = p.rgb + pos + neg; return p; }
inputBlackSrc
inputBlackDst
inputShadowSrc
inputShadowDst
inputMidSrc
inputMidDst
inputHilightSrc
inputHilightDst
inputWhiteSrc
inputWhiteDst
inputBlackSrcRGB
inputBlackDstRGB
inputShadowSrcRGB
inputShadowDstRGB
inputMidSrcRGB
inputMidDstRGB
inputHilightSrcRGB
inputHilightDstRGB
inputWhiteSrcRGB
inputWhiteDstRGB
inputBlackSrcRed
inputBlackDstRed
inputShadowSrcRed
inputShadowDstRed
inputMidSrcRed
inputMidDstRed
inputHilightSrcRed
inputHilightDstRed
inputWhiteSrcRed
inputWhiteDstRed
inputBlackSrcGreen
inputBlackDstGreen
inputShadowSrcGreen
inputShadowDstGreen
inputMidSrcGreen
inputMidDstGreen
inputHilightSrcGreen
inputHilightDstGreen
inputWhiteSrcGreen
inputWhiteDstGreen
inputBlackSrcBlue
inputBlackDstBlue
inputShadowSrcBlue
inputShadowDstBlue
inputMidSrcBlue
inputMidDstBlue
inputHilightSrcBlue
inputHilightDstBlue
inputWhiteSrcBlue
inputWhiteDstBlue
Green
Blue
Failed converting data to RGBAh: %ld
-[PILevelsFilterHDR _LUTImage]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Adjustments/HDR/PILevelsFilterHDR.m
score
/Master/Source
data != nil
+[PIColorNormalizationAutoCalculator autoCalculatorWithImageData:orientation:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Autocalculators/PIColorNormalizationAutoCalculator.m
-[PIColorNormalizationAutoCalculator submit:]
Feature Unavailable
No color normalization available
kernel vec4 _falseColorHDRDebug(sampler image, float cutoff){   vec4 im = sample(image, samplerCoord(image));   if (im.x < 0.0 && im.y < 0.0 && im.z < 0.0) {        return vec4(0.0 , 0.6 , 0.2 , 1.0);   }   if (im.x > cutoff && im.y > cutoff && im.z > cutoff) {        return vec4(1.0 , 0.0 , 1.0 , 1.0);   }   if (im.x > cutoff || im.y > cutoff || im.z > cutoff) {        return vec4(0.6 , 0.0 , 0.6 , 1.0);   }   return im;}
s_falseColorHDRDebugKernelSource kernel is nil
+[PIFalseColorHDRDebug kernel]_block_invoke
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Adjustments/HDR/PIFalseColorHDRDebug.m
-[PIFalseColorHDRDebug outputImage]
-[PIAutoLoopExportRequest initWithRequest:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/AutoLoop/PIAutoLoopRequests.m
-[PIAutoLoopExportRequest initWithComposition:destinationURL:]
Unexpected input pixel format
+[PIPortraitVideoProcessor _configureRGBColorTexture:format:isHDR:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Render/PIPortraitVideoFilter.m
unexpected inputs
+[PIPortraitVideoProcessor processWithInputs:arguments:output:error:]
expected a device
expected a texture
aperture
focusDistance
quality
isHDR
globalMetadata
timedMetadata
Expected an input color texture
Expected an input disparity texture
colorPixelBuffer
missing direct source color pixel buffer
disparityPixelBuffer
missing direct source disparity pixel buffer
v16@?0@"<MTLCommandBuffer>"8
Invalid index
+[PIPortraitVideoProcessor formatForInputAtIndex:]
imageExtents
+[PIPortraitVideoProcessor applyWithInputImage:disparityImage:inputPixelBuffer:disparityPixelBuffer:globalMetadata:timedMetadata:aperture:focusedDisparity:quality:debugMode:isHDR:error:]
inputImage != nil || inputPixelBuffer != nil
disparityImage != nil || disparityPixelBuffer != nil
PIPortrait: cinematic renderer supports RGB10Packed
NormStabilizeInstructions
stabCropRect
Width
Height
flavor
recipe
-[PIVideoCrossfadeLoopNode initWithSettings:inputs:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Video Stabilization/PIVideoCrossfadeLoopNode.m
input != nil
-[PIVideoCrossfadeLoopNode initWithInput:timeRange:crossfadeDuration:startTime:]
CMTIME_IS_VALID(crossfadeDuration)
CMTIMERANGE_IS_VALID(loopTimeRange)
crossfadeDuration
startTime
loopTimeRange
-[PIVideoCrossfadeLoopNode nodeByReplayingAgainstCache:pipelineState:error:]
Missing primary video frame
video
secondary
Missing secondary video frame
CIDissolveTransition
-[PIVideoCrossfadeLoopNode _evaluateVideo:]
[[AVMutableComposition alloc] init] failed.
No input video track found
Failed to update video track when inserting the pre-crossfade content with source range %@ from track %@ to time %@ in track %@.
Failed to update audio track when inserting the pre-crossfade content with source range %@ from track %@ to time %@ in track %@.
Failed to update video track when inserting the crossfade content with source range %@ from track %@ to time %@ in track %@.
Failed to update audio track when inserting the crossfade content with source range %@ from track %@ to time %@ in track %@.
Failed to update video track when inserting the post-crossfade content with source range %@ from track %@ to time %@ in track %@.
Failed to update audio track when inserting the post-crossfade content with source range %@ from track %@ to time %@ in track %@.
-[PIVideoCrossfadeLoopNode _evaluateVideoComposition:]
Unsupported video configuration
-[PIVideoCrossfadeLoopNode _evaluateAudioMix:]
time
scale
radius
falloff
Adjustment empty
-[PIAdjustmentController displayName]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Controllers/PIAdjustmentController.m
-[PIAdjustmentController inputKeys]
Adjustment does not have an enabled setting
-[PIAdjustmentController enabled]
-[PIAdjustmentController setEnabled:]
Adjustment does not have an auto setting
-[PIAdjustmentController isAuto]
-[PIAdjustmentController setIsAuto:]
<%@: %p; adjustment = %@>
-[PIParallaxFilter outputForegroundImage]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Parallax/PIParallaxFilter.m
-[PIParallaxFilter outputBackgroundImage]
-[PIParallaxFilter outputMatteImage]
-[_PIParallaxClockMaterialJob render:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Parallax/PIParallaxClockMaterialRequest.m
Request needs either a layerStack or a segmentationItem
-[PIParallaxClockMaterialRequest initWithComposition:]
__dominantInputSettingsKey
debugMode
disparityKeyframes
apertureKeyframes
NUScaleIsValid(scale)
-[PIPortraitVideoRenderNode _targetScaleForScale:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Render/PIPortraitVideoRenderNode.m
-[PIPortraitVideoRenderNode nodeByReplayingAgainstCache:pipelineState:error:]
state != nil
-[PIPortraitVideoRenderNode _prewarmPortraitRendererWithPipelineState:error:]
No device specified for prewarm
portraitVideoMetadata
PIPortraitVideoRenderNode: expected a valid portraitVideoMetadata sample
PIPortrait: passing YUV source buffers directly to the cinematic renderer
renderTime
renderQuality
sourceTransferFunction
useSourceBuffersDirectly
Source image isn't backed by a CVPixelBuffer
Could not get the input image
Could not get the disparityImage
Could not get the timed metadata
Could not get the source transfer function
Could not get the render time
PIFaceObservationCache
PIFaceObservationCache-newRequest
Original
StudioBright
StudioDark
ColorBGStandard
BlackWhiteHighKey
BlackWhiteStage
BlackWhiteMono
ColorWashSingle
ColorWashDuotone
Original-Inactive
StudioBright-Inactive
StudioDark-Inactive
ColorBGStandard-Inactive
BlackWhiteHighKey-Inactive
BlackWhiteStage-Inactive
BlackWhiteMono-Inactive
ColorWashSingle-Inactive
ColorWashDuotone-Inactive
The bundle should contain recipes for all known identifiers
+[PIParallaxStyleRecipeRegistry recipeForIdentifier:]_block_invoke
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Parallax/PIParallaxStyleRecipeRegistry.m
-[PIParallaxStyleUserURLProvider init]
spatialOvercapture
spatialOvercaptureFused
Unspecified Key
Low Key
Neutral Key
High Key
SF Soft Time
SF Rounded Time
New York Time
ADT Slab Time
SF Stencil Time
SF Rail Time
+[PIParallaxStyle styleWithColorAnalysis:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Parallax/PIParallaxStyle.m
clockColor != nil
-[PIParallaxStyle initWithClockColor:colorSuggestions:]
suggestions != nil
Unknown style kind: %@
+[PIParallaxStyle defaultStyleForKind:colorAnalysis:]
+[PIParallaxStyle styleWithParameters:colorSuggestions:]
+[PIParallaxStyle styleWithBakedStyle:]
<%@: %p; parameters = %@>
-[PIParallaxStyle parameters]
-[PIParallaxStyle kind]
-[PIParallaxStyle recipeIdentifier]
-Inactive
BLACK
WHITE
B16@?0@"PFParallaxPaletteSuggestion"8
-[PIParallaxStyle configureForCategory:]
parameters != nil
+[PIParallaxOriginalStyle styleWithParameters:colorSuggestions:]
+[PIParallaxStudioStyle styleWithParameters:colorSuggestions:]
@"PFParallaxPaletteSuggestion"16@?0@"PFParallaxColor"8
backgroundColor != nil
-[PIParallaxColorBGStandardStyle initWithBackgroundColor:clockColor:colorSuggestions:]
+[PIParallaxBlackWhiteMonoStyle styleWithParameters:colorSuggestions:]
@"PFParallaxColor"24@?0@"PFParallaxColor"8@"PIParallaxColorAnalysis"16
@"PFParallaxColor"16@?0@"PFParallaxColor"8
+[PIParallaxColorWashSingleStyle styleWithParameters:colorSuggestions:]
-[PIParallaxColorWashSingleStyle initWithColor:clockColor:suggestions:]
Failed to retrieve secondary color from palette
+[PIParallaxColorWashDuotoneStyle styleWithColorAnalysis:]
+[PIParallaxColorWashDuotoneStyle styleWithParameters:colorSuggestions:]
primaryColor != nil
-[PIParallaxColorWashDuotoneStyle initWithPrimaryColor:secondaryColor:clockColor:colorSuggestions:]
secondaryColor != nil
-[PIParallaxRecipeStyle initWithIdentifier:recipe:]
<%@: %p; identifier = %@, recipe = %@>
timeValue
timeScale
%@: t=%@, v=%f
<%@ %p %@ %@ id=%lu conf=%.2f>
  bounds=%@
  expandedBounds=%@
  edgeBleed=%@
type
confidence
edgeBleed
bounds
expandedBounds
Human Face
Human Body
Cat Body
Cat Head
Dog Body
Dog Head
Salient Object
Unknown
(%.4f, %.4f, %.4f, %.4f)
minX 
minY 
maxX 
maxY 
B16@?0@"PFParallaxLayerStyle"8
<%@:%p class=%@ matte=%@ conf=%@ infill=%@ layout=%@ resource=%@ composition=%@>
archiveURL != nil
-[PIParallaxSegmentationItem saveToURL:error:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Parallax/PISegmentationItem.m
Failed to create archive directory
asset.resource
Failed to archive asset resource
segmentation.data.aar
Failed to archive segmentation data
-[PIParallaxSegmentationItem loadFromURL:error:]
Failed to load asset resource
Failed to load segmentation data
-[PIParallaxSegmentationItem saveAssetResourceToURL:error:]
Missing asset resource
-[PIParallaxSegmentationItem saveSegmentationDataToURL:error:]
Failed to write segmentation archive
Failed to serialize contents plist
contents.plist
Failed to archive contents plist data
Failed to write segmentation matte
matte.heic
Failed to archive segmentation matte data
Failed to write segmentation background
background.heic
Failed to archive segmentation background data
Failed to close archive file
-[PIParallaxSegmentationItem loadSegmentationDataFromURL:error:]
Failed to read segmentation archive
Failed to decode contents plist data
Expected contents.plist data
Failed to deserialize contents plist
Invalid contents plist
Failed to load contents dictionary
Failed to decode matte image data
Expected matte.heic data
Failed to read matte image data
Failed to decode background image data
Expected background.heic data
Failed to read background image data
Missing context info!
-[PIParallaxSegmentationItem contentsDictionary]
classification
hasMatte
hasBackground
regions
layout
scores
colorAnalysis
localLightData
systemVersion
sourceMode
infillAlgorithm
layoutConfiguration
segmentationDisabled
contents != nil
-[PIParallaxSegmentationItem loadContentsFromDictionary:hasMatte:hasBackground:error:]
Missing version info
Invalid version info
Unsupported version
Invalid system version info
Invalid system name
Invalid system version
Invalid system build version
Invalid source mode
Invalid segmentation disabled flag
Invalid infill algorithm
Invalid layout configuration
Failed to deserialize layout configuration
Missing classification info
Expected classification string
Missing matte image info
Expected boolean
Missing background image info
Expected regions dictionary
Failed to deserialize regions info
Expected layout dictionary
Failed to deserialize layout info
Expected score dictionary
Invalid score key
Invalid score value
Invalid color analysis info
Failed to deserialize color analysis info
styles
Expected styles array
Invalid style value
Invalid style dictionary
Unsupported style kind
Invalid local light data
+[PIParallaxSegmentationItem writeImageBuffer:toURL:error:]
Failed to encode pixel buffer
+[PIParallaxSegmentationItem readImageBufferFromURL:error:]
Failed to decode pixel buffer
+[PIParallaxSegmentationItem dataForImageBuffer:error:]
imageData != nil
+[PIParallaxSegmentationItem imageBufferFromData:error:]
cinematographyState
renderingVersionAtCapture
-[PIParallaxColorSuggester init]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Parallax/PIParallaxColorSuggester.m
analysis != nil
-[PIParallaxColorSuggester initWithColorAnalysis:]
color != nil
-[PIParallaxColorSuggester suggestedColorForColor:]
inputColor
outputColor
hueMin <= hueMax
-[PIParallaxColorSuggester addRuleWithHueMin:hueMax:suggestion:]
B16@?0@"NURuleSystem"8
v16@?0@"NURuleSystem"8
colors != nil
-[PIParallaxColorSuggester suggestedColorsForColors:fromColorPalette:]
B16@?0@"PFParallaxColor"8
homography
<%@:%p time:%@>
kernel vec4 ipt_hue_chroma_scale_hue(__sample ihc, vec2 hso) {
float luma = ihc.r;
float hue = ihc.g;
float chroma = ihc.b;
float alpha = ihc.a;
float hueScale = hso.x;
float hueOffset = hso.y;
hue = hueScale * hue + hueOffset;
return vec4(luma, hue, chroma, alpha);
kernel vec4 ipt_hue_chroma_filter_hue(__sample ihc, vec4 hcr) {
float luma = ihc.r;
float hue = ihc.g;
float chroma = ihc.b;
float alpha = ihc.a;
float hueTarget = hcr.x;
float hueRange = hcr.y;
float hueModulo = hcr.z;
float chromaMin = hcr.w;
float chromaFactor = step(chromaMin, chroma);
float hueDelta = min(abs(hue - hueTarget), min(abs(hue + hueModulo - hueTarget), abs(hue - hueModulo - hueTarget)));
float hueFactor = 1.0 - smoothstep(0.0, hueRange, hueDelta);
alpha *= hueFactor * chromaFactor;
return vec4(luma, hue, chroma, alpha);
kernel vec4 ipt_hue_chroma_filter_luma(__sample ihc, vec3 hcr) {
float luma = ihc.r;
float hue = ihc.g;
float chroma = ihc.b;
float alpha = ihc.a;
float lumaTarget = hcr.x;
float lumaRange = hcr.y;
float chromaMax = hcr.z;
float chromaFactor = 1.0 - step(chromaMax, chroma);
float lumaDelta = abs(luma - lumaTarget);
float lumaFactor = 1.0 - smoothstep(0.0, lumaRange, lumaDelta);
alpha *= lumaFactor * chromaFactor;
return vec4(luma, hue, chroma, alpha);
<%@: %p; lum = %.3f colors = %@>
-[_PIParallaxColorAnalysisJob initWithRequest:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Parallax/PIParallaxColorAnalysisRequest.m
-[_PIParallaxColorAnalysisJob prepare:]
CISRGBtoIPT
Failed to load hue/chroma filter
inputReturnHueChroma
Hue/chroma filter failed
-[_PIParallaxColorAnalysisJob render:]
No storage pool
hue-%ld
gray-%ld
-[_PIParallaxColorAnalysisJob _beginRenderingImage:colorSpace:format:error:]
No storage allocated
hueChromaImage != nil
-[_PIParallaxColorAnalysisJob _beginRenderNormalizedHueChromaImage:targetHue:hueRange:chromaMin:error:]
PIIPTHueChromaColorFilter
inputHueTarget
inputHueRange
inputChromaMin
inputHueIsNormalized
Failed to produce filtered Hue/Chroma image
CIIPTtoSRGB
inputIsHueChroma
-[_PIParallaxColorAnalysisJob _beginRenderNormalizedHueChromaImage:targetGray:grayRange:chromaMax:error:]
PIIPTHueChromaGrayFilter
inputLumaTarget
inputLumaRange
inputChromaMax
-[_PIParallaxColorAnalysisJob _waitForRenderResources:]
Failed to render image
Failed to compute histogram
-[_PIParallaxColorAnalysisJob complete:]
q24@?0@"_PIParallaxRenderResource"8@"_PIParallaxRenderResource"16
ipt_hue_chroma_scale_hue
+[PIIPTHueChromaFilter normalizeHueChromaImage:]
+[PIIPTHueChromaFilter denormalizeHueChromaImage:]
ipt_hue_chroma_filter_hue
ipt_hue_chroma_filter_luma
luminance
foregroundLuminance
backgroundLuminance
@"NSArray"16@?0@"PFParallaxColor"8
colors
foregroundColors
backgroundColors
+[PIParallaxColorAnalysis loadFromContentsDictionary:error:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Parallax/PIParallaxColorAnalysis.m
Incompatible color analysis version
Invalid luminance value
Invalid color array
Failed to deserialize color values
PILongExposureFusionAutoCalculator-videoProperties
pre-AutoLoop
-[PILongExposureFusionAutoCalculator submit:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Autocalculators/PILongExposureFusionAutoCalculator.m
AutoLoop not supported
kind
-[PIAutoLoopAutoCalculator submit:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Autocalculators/PIAutoLoopAutoCalculator.m
B32@?0@"AVMetadataItem"8Q16^B24
-[PITempTintFilter outputImage]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Adjustments/PITempTintFilter.m
-[PIAutoLoopAnalysisJob prepare:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/AutoLoop/PIAutoLoopJob+Analysis.m
unable to find video source node
-[PIAutoLoopAnalysisJob render:]
AutoLoop is not supported
-[PIVideoStabilizeRenderJob prepare:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Autocalculators/PIVideoStabilizeRequest.m
Stabilize request was cancelled
frameInstructions
rawTime
void CreateKeyframesFromHomographyDictionary(NSDictionary *__strong, NUPixelSize, NSMutableArray<PIReframeKeyframe *> *__strong, NUPixelRect *)
PIVideoStabilizeRequest.m
unexpected homography
Failure in ICSynthesizeAnalysis
Failure in ICAnalyzeInputMotion
No available analysis types were allowed
Failure in ICCalcCinematicL1Corrections
neutralGray
faceBalance
tempTint
warmTint
PIFaceBalanceAutoCalculator-imageProperties
+[PIFaceBalanceAutoCalculator calculateWithRequest:completion:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Autocalculators/PIWhiteBalanceAutoCalculators.m
CIFaceBalance
PIFaceBalanceAutoCalculator-calculate
WarmTemp
WarmTint
warmTemp
+[PIFaceBalanceAutoCalculator calculateRAWWithRequest:completion:]
PIFaceBalanceAutoCalculator-RAW-face-request
+[PIFaceBalanceAutoCalculator faceBalanceResultFromFaceObservations:request:error:]
PIWhiteBalanceAutoCalculator-face-balance
PIRAWFaceBalanceAutoCalculator.responseQueue
Failure in rendering image
OrigI
OrigQ
Warmth
Strength
WarmFace
unsupported format - incorrect white balance
+[PIFaceBalanceAutoCalculator faceBalanceFromFaceImage:forFaceRect:]
{?={?=[4d]}{?=[4d]}d}
{?=[4d]}
colorType
grayColor
-[PIWhiteBalanceAutoCalculator submit:]
PIWhiteBalanceAutoCalculator-imageProperties
PIFaceBalanceAutoCalculator.responseQueue
faceI
faceQ
faceWarmth
faceStrength
PIWhiteBalanceAutoCalculator
color
v16@?0@"<NUBuffer>"8
CIAreaAverage
PIWhiteColorCalculator-computeGreenPercentage
PIWhiteColorCalculator-grayWorld
PIWhiteColorCalculator-grayEdges
return Filter('CIEdges', { 'inputImage' : input, 'inputIntensity' : 1.0 });
Expected non-NULL pixels passed in
void customRGBtoYIQ(const CGFloat *, double *, double, double, double)
vec4 meaningBlur(vec4 im, vec4 b)
vec4 result = im;
float thresh = 0.1;
float g1 = max(max(im.r, im.g), im.b);
float g2 = dot(b.rgb, vec3(1.0/3.0));
float diff = max(g2-g1, -1.0);
diff = smoothstep(0.1-thresh, 0.1+thresh, diff);
result.rgb = mix(im.rgb, b.rgb, diff+0.5);
return result;
vec4 clarityNew(vec4 s, vec4 b, float intensity)
float sl = (s.r + s.g + s.b);
float bl = (b.r + b.g + b.b);
float dl = sl + (sl - bl) * intensity;
float mult = dl / max(sl, 0.0001);
mult = 1.571 * (mult - 1.0);
mult = mult / (1.0 + abs(mult));
mult += 1.0;
mult = clamp(mult, 1.0 - 0.5 * abs(intensity), 1.0 + 1.0 * abs(intensity));
s.rgb = s.rgb * mult;
return s;
kernel vec4 definition(sampler image, sampler blur, float intensity)
vec4 imgSample = sample(image, samplerCoord(image));
vec4 blurSample = sample(blur, samplerCoord(blur));
vec4 meaning = meaningBlur(imgSample, blurSample);
vec4 clarity = clarityNew(imgSample, meaning, intensity);
return clarity;
strength
neutral
tone
inputNeutralGamma
inputTone
inputGrain
inputHue
inputSeed
<%@:%p - priority: %@, color space: %@, scale policy: %@>
<%@:%p - priority: %@, color space: %@, scale policy: %@, video codec type: %@, bypass settings: %@, orientation as metadata: %@, hardware encoder: %@>
Export URL UTI (%@) does not match expected export format (%@)
-[PICompositionExporterImageOptions imageExportFormatForURL:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Util/PICompositionExporter.m
metadataConverter must be set
-[PICompositionExporter init]
PICompositionExporter-videoProperties
Expecting HEIF/HEVC or JPEG/H264 when exporting Live Photo still and video complement
Unexpected video codec when attempting to export Live Photo
Unexpected image export format when attempting to export Live Photo
_companion
unable to prepare image properties
PICompositionExporter-image
PICompositionExporter-auxPropertiesRequest
PICompositionExporter-%@
PICompositionExporter.imageProperties.transaction
Regions
-[PICompositionExporter addVideoProperties:composition:options:error:]
Unknown autoloop flavor
PICompositionExporter-video
float luminanceWeight(vec4 pix, vec4 center, float slope)
return max(1.0 + (slope * length(pix.rgb - center.rgb)), 0.0);
vec4 bilateralRow_1(sampler src, float slope, vec2 offset1, float weight1)
float diff1;
vec2 coord;
vec4 pix1, center;
coord = destCoord();
center = sample(src, samplerCoord(src));
pix1 = sample(src, samplerTransform(src, coord + offset1));
diff1 = luminanceWeight(pix1, center, slope) * weight1;
return vec4(diff1 * pix1.rgb, diff1);
kernel vec4 bilateralAdd_1(sampler src, sampler sums, float slope, vec2 offset1, float weight1, float sumWeight)
vec4 sum;
sum = sample(sums, samplerCoord(sums))*sumWeight;
return sum + bilateralRow_1(src, slope, offset1, weight1);
vec4 bilateralRow_2(sampler src, float slope, vec2 offset1, vec2 offset2, vec2 weight1)
float diff1, diff2;
vec2 coord;
vec4 pix1, pix2, center;
coord = destCoord();
center = sample(src, samplerCoord(src));
pix1 = sample(src, samplerTransform(src, coord + offset1));
pix2 = sample(src, samplerTransform(src, coord + offset2));
diff1 = luminanceWeight(pix1, center, slope) * weight1.x;
diff2 = luminanceWeight(pix2, center, slope) * weight1.y;
return vec4(diff1 * pix1.rgb + diff2 * pix2.rgb, diff1 + diff2);
kernel vec4 bilateralAdd_2(sampler src, sampler sums, float slope, vec2 offset1, vec2 offset2, vec2 weight1, float sumWeight)
vec4 sum;
sum = sample(sums, samplerCoord(sums))*sumWeight;
return sum + bilateralRow_2(src, slope, offset1, offset2, weight1);
vec4 bilateralRow_3(sampler src, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec3 weight1)
float diff1, diff2, diff3;
vec2 coord;
vec4 pix1, pix2, pix3, center;
coord = destCoord();
center = sample(src, samplerCoord(src));
pix1 = sample(src, samplerTransform(src, coord + offset1));
pix2 = sample(src, samplerTransform(src, coord + offset2));
pix3 = sample(src, samplerTransform(src, coord + offset3));
diff1 = luminanceWeight(pix1, center, slope) * weight1.x;
diff2 = luminanceWeight(pix2, center, slope) * weight1.y;
diff3 = luminanceWeight(pix3, center, slope) * weight1.z;
return vec4(diff1 * pix1.rgb + diff2 * pix2.rgb + diff3 * pix3.rgb, diff1 + diff2 + diff3);
kernel vec4 bilateralAdd_3(sampler src, sampler sums, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec3 weight1, float sumWeight)
vec4 sum;
sum = sample(sums, samplerCoord(sums))*sumWeight;
return sum + bilateralRow_3(src, slope, offset1, offset2, offset3, weight1);
vec4 bilateralRow_4(sampler src, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec2 offset4, vec4 weight1)
float diff1, diff2, diff3, diff4;
vec2 coord;
vec4 pix1, pix2, pix3, pix4, center;
coord = destCoord();
center = sample(src, samplerCoord(src));
pix1 = sample(src, samplerTransform(src, coord + offset1));
pix2 = sample(src, samplerTransform(src, coord + offset2));
pix3 = sample(src, samplerTransform(src, coord + offset3));
pix4 = sample(src, samplerTransform(src, coord + offset4));
diff1 = luminanceWeight(pix1, center, slope) * weight1.x;
diff2 = luminanceWeight(pix2, center, slope) * weight1.y;
diff3 = luminanceWeight(pix3, center, slope) * weight1.z;
diff4 = luminanceWeight(pix4, center, slope) * weight1.w;
return vec4(diff1 * pix1.rgb + diff2 * pix2.rgb + diff3 * pix3.rgb + diff4 * pix4.rgb,
diff1 + diff2 + diff3 + diff4);
kernel vec4 bilateralAdd_4(sampler src, sampler sums, float slope, vec2 offset1, vec2 offset2, vec2 offset3,
vec2 offset4, vec4 weight1, float sumWeight)
vec4 sum;
sum = sample(sums, samplerCoord(sums))*sumWeight;
return sum + bilateralRow_4(src, slope, offset1, offset2, offset3, offset4, weight1);
vec4 bilateralRow_5(sampler src, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec2 offset4, vec2 offset5,
vec4 weight1, float weight2)
float diff1, diff2, diff3, diff4, diff5;
vec2 coord;
vec4 pix1, pix2, pix3, pix4, pix5, center;
coord = destCoord();
center = sample(src, samplerCoord(src));
pix1 = sample(src, samplerTransform(src, coord + offset1));
pix2 = sample(src, samplerTransform(src, coord + offset2));
pix3 = sample(src, samplerTransform(src, coord + offset3));
pix4 = sample(src, samplerTransform(src, coord + offset4));
pix5 = sample(src, samplerTransform(src, coord + offset5));
diff1 = luminanceWeight(pix1, center, slope) * weight1.x;
diff2 = luminanceWeight(pix2, center, slope) * weight1.y;
diff3 = luminanceWeight(pix3, center, slope) * weight1.z;
diff4 = luminanceWeight(pix4, center, slope) * weight1.w;
diff5 = luminanceWeight(pix5, center, slope) * weight2;
return vec4(diff1 * pix1.rgb + diff2 * pix2.rgb + diff3 * pix3.rgb + diff4 * pix4.rgb + diff5 * pix5.rgb,
diff1 + diff2 + diff3 + diff4 + diff5);
kernel vec4 bilateralAdd_5(sampler src, sampler sums, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec2 offset4,
vec2 offset5, vec4 weight1, float weight2, float sumWeight)
vec4 sum;
sum = sample(sums, samplerCoord(sums))*sumWeight;
return sum + bilateralRow_5(src, slope, offset1, offset2, offset3, offset4, offset5, weight1, weight2);
vec4 bilateralRow_6(sampler src, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec2 offset4, vec2 offset5,
vec2 offset6, vec4 weight1, vec2 weight2)
float diff1, diff2, diff3, diff4, diff5, diff6;
vec2 coord;
vec4 pix1, pix2, pix3, pix4, pix5, pix6, center;
coord = destCoord();
center = sample(src, samplerCoord(src));
pix1 = sample(src, samplerTransform(src, coord + offset1));
pix2 = sample(src, samplerTransform(src, coord + offset2));
pix3 = sample(src, samplerTransform(src, coord + offset3));
pix4 = sample(src, samplerTransform(src, coord + offset4));
pix5 = sample(src, samplerTransform(src, coord + offset5));
pix6 = sample(src, samplerTransform(src, coord + offset6));
diff1 = luminanceWeight(pix1, center, slope) * weight1.x;
diff2 = luminanceWeight(pix2, center, slope) * weight1.y;
diff3 = luminanceWeight(pix3, center, slope) * weight1.z;
diff4 = luminanceWeight(pix4, center, slope) * weight1.w;
diff5 = luminanceWeight(pix5, center, slope) * weight2.x;
diff6 = luminanceWeight(pix6, center, slope) * weight2.y;
return vec4(diff1 * pix1.rgb + diff2 * pix2.rgb + diff3 * pix3.rgb + diff4 * pix4.rgb + diff5 * pix5.rgb + diff6 * pix6.rgb,
diff1 + diff2 + diff3 + diff4 + diff5 + diff6);
kernel vec4 bilateralAdd_6(sampler src, sampler sums, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec2 offset4,
vec2 offset5, vec2 offset6, vec4 weight1, vec2 weight2, float sumWeight)
vec4 sum;
sum = sample(sums, samplerCoord(sums))*sumWeight;
return sum + bilateralRow_6(src, slope, offset1, offset2, offset3, offset4, offset5, offset6, weight1, weight2);
vec4 bilateralRow_7(sampler src, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec2 offset4, vec2 offset5,
vec2 offset6, vec2 offset7, vec4 weight1, vec3 weight2)
float diff1, diff2, diff3, diff4, diff5, diff6, diff7;
vec2 coord;
vec4 pix1, pix2, pix3, pix4, pix5, pix6, pix7, center;
coord = destCoord();
center = sample(src, samplerCoord(src));
pix1 = sample(src, samplerTransform(src, coord + offset1));
pix2 = sample(src, samplerTransform(src, coord + offset2));
pix3 = sample(src, samplerTransform(src, coord + offset3));
pix4 = sample(src, samplerTransform(src, coord + offset4));
pix5 = sample(src, samplerTransform(src, coord + offset5));
pix6 = sample(src, samplerTransform(src, coord + offset6));
pix7 = sample(src, samplerTransform(src, coord + offset7));
diff1 = luminanceWeight(pix1, center, slope) * weight1.x;
diff2 = luminanceWeight(pix2, center, slope) * weight1.y;
diff3 = luminanceWeight(pix3, center, slope) * weight1.z;
diff4 = luminanceWeight(pix4, center, slope) * weight1.w;
diff5 = luminanceWeight(pix5, center, slope) * weight2.x;
diff6 = luminanceWeight(pix6, center, slope) * weight2.y;
diff7 = luminanceWeight(pix7, center, slope) * weight2.z;
return vec4(diff1 * pix1.rgb + diff2 * pix2.rgb + diff3 * pix3.rgb + diff4 * pix4.rgb + diff5 * pix5.rgb + diff6 * pix6.rgb + diff7 * pix7.rgb,
diff1 + diff2 + diff3 + diff4 + diff5 + diff6 + diff7);
kernel vec4 bilateralAdd_7(sampler src, sampler sums, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec2 offset4,
vec2 offset5, vec2 offset6, vec2 offset7, vec4 weight1, vec3 weight2, float sumWeight)
vec4 sum;
sum = sample(sums, samplerCoord(sums))*sumWeight;
return sum + bilateralRow_7(src, slope, offset1, offset2, offset3, offset4, offset5, offset6, offset7, weight1, weight2);
vec4 bilateralRow_8(sampler src, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec2 offset4, vec2 offset5, vec2 offset6,
vec2 offset7, vec2 offset8, vec4 weight1, vec4 weight2)
float diff1, diff2, diff3, diff4, diff5, diff6, diff7, diff8;
vec2 coord;
vec4 pix1, pix2, pix3, pix4, pix5, pix6, pix7, pix8, center;
coord = destCoord();
center = sample(src, samplerCoord(src));
pix1 = sample(src, samplerTransform(src, coord + offset1));
pix2 = sample(src, samplerTransform(src, coord + offset2));
pix3 = sample(src, samplerTransform(src, coord + offset3));
pix4 = sample(src, samplerTransform(src, coord + offset4));
pix5 = sample(src, samplerTransform(src, coord + offset5));
pix6 = sample(src, samplerTransform(src, coord + offset6));
pix7 = sample(src, samplerTransform(src, coord + offset7));
pix8 = sample(src, samplerTransform(src, coord + offset8));
diff1 = luminanceWeight(pix1, center, slope) * weight1.x;
diff2 = luminanceWeight(pix2, center, slope) * weight1.y;
diff3 = luminanceWeight(pix3, center, slope) * weight1.z;
diff4 = luminanceWeight(pix4, center, slope) * weight1.w;
diff5 = luminanceWeight(pix5, center, slope) * weight2.x;
diff6 = luminanceWeight(pix6, center, slope) * weight2.y;
diff7 = luminanceWeight(pix7, center, slope) * weight2.z;
diff8 = luminanceWeight(pix8, center, slope) * weight2.w;
return vec4(diff1 * pix1.rgb + diff2 * pix2.rgb + diff3 * pix3.rgb + diff4 * pix4.rgb + diff5 * pix5.rgb + diff6 * pix6.rgb + diff7 * pix7.rgb + diff8 * pix8.rgb,
diff1 + diff2 + diff3 + diff4 + diff5 + diff6 + diff7 + diff8);
kernel vec4 bilateralAdd_8(sampler src, sampler sums, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec2 offset4, vec2 offset5,
vec2 offset6, vec2 offset7, vec2 offset8, vec4 weight1, vec4 weight2, float sumWeight)
vec4 sum;
sum = sample(sums, samplerCoord(sums))*sumWeight;
return sum + bilateralRow_8(src, slope, offset1, offset2, offset3, offset4, offset5, offset6, offset7, offset8, weight1, weight2);
vec4 bilateralRow_9(sampler src, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec2 offset4, vec2 offset5, vec2 offset6, vec2 offset7,
vec2 offset8, vec2 offset9, vec4 weight1, vec4 weight2, float weight3)
float diff1, diff2, diff3, diff4, diff5, diff6, diff7, diff8, diff9;
vec2 coord;
vec4 pix1, pix2, pix3, pix4, pix5, pix6, pix7, pix8, pix9, center;
coord = destCoord();
center = sample(src, samplerCoord(src));
pix1 = sample(src, samplerTransform(src, coord + offset1));
pix2 = sample(src, samplerTransform(src, coord + offset2));
pix3 = sample(src, samplerTransform(src, coord + offset3));
pix4 = sample(src, samplerTransform(src, coord + offset4));
pix5 = sample(src, samplerTransform(src, coord + offset5));
pix6 = sample(src, samplerTransform(src, coord + offset6));
pix7 = sample(src, samplerTransform(src, coord + offset7));
pix8 = sample(src, samplerTransform(src, coord + offset8));
pix9 = sample(src, samplerTransform(src, coord + offset9));
diff1 = luminanceWeight(pix1, center, slope) * weight1.x;
diff2 = luminanceWeight(pix2, center, slope) * weight1.y;
diff3 = luminanceWeight(pix3, center, slope) * weight1.z;
diff4 = luminanceWeight(pix4, center, slope) * weight1.w;
diff5 = luminanceWeight(pix5, center, slope) * weight2.x;
diff6 = luminanceWeight(pix6, center, slope) * weight2.y;
diff7 = luminanceWeight(pix7, center, slope) * weight2.z;
diff8 = luminanceWeight(pix8, center, slope) * weight2.w;
diff9 = luminanceWeight(pix9, center, slope) * weight3;
return vec4(diff1 * pix1.rgb + diff2 * pix2.rgb + diff3 * pix3.rgb + diff4 * pix4.rgb + diff5 * pix5.rgb + diff6 * pix6.rgb + diff7 * pix7.rgb + diff8 * pix8.rgb + diff9 * pix9.rgb,
diff1 + diff2 + diff3 + diff4 + diff5 + diff6 + diff7 + diff8 + diff9);
kernel vec4 bilateralAdd_9(sampler src, sampler sums, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec2 offset4, vec2 offset5,
vec2 offset6, vec2 offset7, vec2 offset8, vec2 offset9, vec4 weight1, vec4 weight2, float weight3, float sumWeight)
vec4 sum;
sum = sample(sums, samplerCoord(sums))*sumWeight;
return sum + bilateralRow_9(src, slope, offset1, offset2, offset3, offset4, offset5, offset6, offset7, offset8, offset9,
weight1, weight2, weight3);
kernel vec4 bilateralFinalize(sampler sums)
vec4 sum;
sum = sample(sums, samplerCoord(sums));
return vec4(sum.rgb / max(sum.a, 0.001), 1.0);
kernel vec4 bilateralLoop2(sampler src, float slope, vec2 weights12)
vec2 coord;
vec4 center = sample(src, samplerCoord(src));
vec4 sum;
vec4 pix0, pix1, pix2;
vec4 pix3,       pix5;
vec4 pix6, pix7, pix8;
vec2 dx = samplerTransform(src, vec2( 1.0, 0.0)) - samplerTransform(src, vec2(0.0, 0.0));
vec2 dy = samplerTransform(src, vec2(-2.0, 1.0)) - samplerTransform(src, vec2(0.0, 0.0));
coord = samplerTransform(src, destCoord() + vec2(-1.0, -1.0));
pix0 = sample(src, coord);
coord = coord + dx;
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dy;
pix3 = sample(src, coord);
coord = coord + dx;
coord = coord + dx;
pix5 = sample(src, coord);
coord = coord + dy;
pix6 = sample(src, coord);
coord = coord + dx;
pix7 = sample(src, coord);
coord = coord + dx;
pix8 = sample(src, coord);
sum = (1.0 - min(1.0, (slope * length(pix0 - center)))) * (pix0 * weights12.y);
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights12.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights12.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights12.x) + sum;
sum =                                        center                            + sum;
sum = (1.0 - min(1.0, (slope * length(pix5 - center)))) * (pix5 * weights12.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix6 - center)))) * (pix6 * weights12.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix7 - center)))) * (pix7 * weights12.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix8 - center)))) * (pix8 * weights12.y) + sum;
return vec4(sum.rgb / sum.a, 1.0);
kernel vec4 bilateralLoop5(sampler src, float slope, vec4 weights1245)
vec2  coord;
vec4  center = sample(src, samplerCoord(src));
vec4  sum;
vec4  pix0, pix1, pix2, pix3, pix4;
vec2 dx = samplerTransform(src, vec2( 1.0, 0.0)) - samplerTransform(src, vec2(0.0, 0.0));
vec2 dy = samplerTransform(src, vec2(-4.0, 1.0)) - samplerTransform(src, vec2(0.0, 0.0));
coord = samplerTransform(src, destCoord() + vec2(-1.0, -2.0));
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dx;
pix3 = sample(src, coord);
coord = coord + dx + dy;
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights1245.w);
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights1245.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights1245.w) + sum;
pix0 = sample(src, coord);
coord = coord + dx;
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dx;
pix3 = sample(src, coord);
coord = coord + dx;
pix4 = sample(src, coord);
coord = coord + dy;
sum = (1.0 - min(1.0, (slope * length(pix0 - center)))) * (pix0 * weights1245.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights1245.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights1245.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights1245.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix4 - center)))) * (pix4 * weights1245.w) + sum;
pix0 = sample(src, coord);
coord = coord + dx;
pix1 = sample(src, coord);
coord = coord + dx + dx;
pix3 = sample(src, coord);
coord = coord + dx;
pix4 = sample(src, coord);
coord = coord + dy;
sum = (1.0 - min(1.0, (slope * length(pix0 - center)))) * (pix0 * weights1245.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights1245.x) + sum;
sum =                                        center                              + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights1245.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix4 - center)))) * (pix4 * weights1245.z) + sum;
pix0 = sample(src, coord);
coord = coord + dx;
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dx;
pix3 = sample(src, coord);
coord = coord + dx;
pix4 = sample(src, coord);
coord = coord + dx + dy;
sum = (1.0 - min(1.0, (slope * length(pix0 - center)))) * (pix0 * weights1245.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights1245.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights1245.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights1245.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix4 - center)))) * (pix4 * weights1245.w) + sum;
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dx;
pix3 = sample(src, coord);
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights1245.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights1245.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights1245.w) + sum;
return vec4(sum.rgb / sum.a, 1.0);
kernel vec4 bilateralLoop11(sampler src, float slope, vec4 weights1245, vec4 weights89AD)
vec2  coord;
vec4  center = sample(src, samplerCoord(src));
vec4  sum;
vec4  pix0, pix1, pix2, pix3, pix4, pix5, pix6;
vec2 dx = samplerTransform(src, vec2( 1.0, 0.0)) - samplerTransform(src, vec2(0.0, 0.0));
vec2 dy = samplerTransform(src, vec2(-6.0, 1.0)) - samplerTransform(src, vec2(0.0, 0.0));
coord = samplerTransform(src, destCoord() + vec2(-2.0, -3.0));
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dx;
pix3 = sample(src, coord);
coord = coord + dx;
pix4 = sample(src, coord);
coord = coord + dx;
pix5 = sample(src, coord);
coord = coord + dx + dy;
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights89AD.w);
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights89AD.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights89AD.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix4 - center)))) * (pix4 * weights89AD.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix5 - center)))) * (pix5 * weights89AD.w) + sum;
pix0 = sample(src, coord);
coord = coord + dx;
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dx;
pix3 = sample(src, coord);
coord = coord + dx;
pix4 = sample(src, coord);
coord = coord + dx;
pix5 = sample(src, coord);
coord = coord + dx;
pix6 = sample(src, coord);
coord = coord + dy;
sum = (1.0 - min(1.0, (slope * length(pix0 - center)))) * (pix0 * weights89AD.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights89AD.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights1245.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights1245.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix4 - center)))) * (pix4 * weights1245.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix5 - center)))) * (pix5 * weights89AD.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix6 - center)))) * (pix6 * weights89AD.w) + sum;
pix0 = sample(src, coord);
coord = coord + dx;
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dx;
pix3 = sample(src, coord);
coord = coord + dx;
pix4 = sample(src, coord);
coord = coord + dx;
pix5 = sample(src, coord);
coord = coord + dx;
pix6 = sample(src, coord);
coord = coord + dy;
sum = (1.0 - min(1.0, (slope * length(pix0 - center)))) * (pix0 * weights89AD.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights1245.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights1245.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights1245.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix4 - center)))) * (pix4 * weights1245.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix5 - center)))) * (pix5 * weights1245.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix6 - center)))) * (pix6 * weights89AD.z) + sum;
pix0 = sample(src, coord);
coord = coord + dx;
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dx + dx;
pix4 = sample(src, coord);
coord = coord + dx;
pix5 = sample(src, coord);
coord = coord + dx;
pix6 = sample(src, coord);
coord = coord + dy;
sum = (1.0 - min(1.0, (slope * length(pix0 - center)))) * (pix0 * weights89AD.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights1245.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights1245.x) + sum;
sum =                                        center                              + sum;
sum = (1.0 - min(1.0, (slope * length(pix4 - center)))) * (pix4 * weights1245.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix5 - center)))) * (pix5 * weights1245.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix6 - center)))) * (pix6 * weights89AD.y) + sum;
pix0 = sample(src, coord);
coord = coord + dx;
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dx;
pix3 = sample(src, coord);
coord = coord + dx;
pix4 = sample(src, coord);
coord = coord + dx;
pix5 = sample(src, coord);
coord = coord + dx;
pix6 = sample(src, coord);
coord = coord + dy;
sum = (1.0 - min(1.0, (slope * length(pix0 - center)))) * (pix0 * weights89AD.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights1245.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights1245.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights1245.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix4 - center)))) * (pix4 * weights1245.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix5 - center)))) * (pix5 * weights1245.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix6 - center)))) * (pix6 * weights89AD.z) + sum;
pix0 = sample(src, coord);
coord = coord + dx;
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dx;
pix3 = sample(src, coord);
coord = coord + dx;
pix4 = sample(src, coord);
coord = coord + dx;
pix5 = sample(src, coord);
coord = coord + dx;
pix6 = sample(src, coord);
coord = coord + dx + dy;
sum = (1.0 - min(1.0, (slope * length(pix0 - center)))) * (pix0 * weights89AD.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights89AD.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights1245.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights1245.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix4 - center)))) * (pix4 * weights1245.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix5 - center)))) * (pix5 * weights89AD.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix6 - center)))) * (pix6 * weights89AD.w) + sum;
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dx;
pix3 = sample(src, coord);
coord = coord + dx;
pix4 = sample(src, coord);
coord = coord + dx;
pix5 = sample(src, coord);
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights89AD.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights89AD.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights89AD.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix4 - center)))) * (pix4 * weights89AD.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix5 - center)))) * (pix5 * weights89AD.w) + sum;
return vec4(sum.rgb / sum.a, 1.0);
kernel vec4 convertFromRGBToLab(sampler src)
vec3 f;
vec4 pix, color;
pix = unpremultiply(sample(src, samplerCoord(src)));
color.xyz = pix.r * vec3(0.449695,  0.316251, 0.18452  )
+ pix.g * vec3(0.244634,  0.672034, 0.0833318)
+ pix.b * vec3(0.0251829, 0.141184, 0.922602 );
color.xyz *= vec3(1.052111, 1.0, 0.918417);
f = compare(color.xyz - 0.00885645, 7.787037 * color.xyz + 0.137931, pow(color.xyz, vec3(0.333333)));
color.r = 116.0 * f.y - 16.0;
color.g = 500.0 * (f.x - f.y);
color.b = 200.0 * (f.y - f.z);
color.rgb *= 0.005;
color.a = 1.0;
return color;
kernel vec4 convertFromLabToRGB(sampler src, sampler original)
vec3 f, cie;
vec4 color, pix, opix;
pix = sample(src, samplerCoord(src));
opix = sample(original, samplerCoord(original));
pix.rgb *= 200.0;
f.y = (pix.r + 16.0) / 116.0;
f.x = f.y + pix.g * 0.002;
f.z = f.y - pix.b * 0.005;
color.xyz = f * f * f;
cie = compare(color.xyz - 0.00885645, (f.xyz - 0.137931) / 7.787037, color.xyz);
cie *= vec3(0.95047, 1.0, 1.08883);
color.rgb = cie.x * vec3(2.95176,   -1.28951, -0.47388  )
+ cie.y * vec3(-1.0851,    1.99084,  0.0372023)
+ cie.z * vec3(0.0854804, -0.269456, 1.09113  );
color.a = opix.a;
return premultiply(color);
bilateralAdd_1
bilateralAdd_2
bilateralAdd_3
bilateralAdd_4
bilateralAdd_5
bilateralAdd_6
bilateralAdd_7
bilateralAdd_8
bilateralAdd_9
bilateralFinalize
convertFromRGBToLab
convertFromLabToRGB
points.count > 0
-[GUBilateralConvolution boundsForPointArray:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Adjustments/PIBilateralFilter.m
bilateralLoop2
bilateralLoop5
bilateralLoop11
inputEdgeDetail
ridiculously large radius for bilateral filter
-[PIBilateralFilter outputImage]
inputPoints
inputWeights
unable to allocate convolution table in bilateral filter
com.apple.livephoto
com.apple.video
com.apple.video.slomo
.%lu
%lu.%lu%@%@
satPercentile75
satPercentile98
satPercentileG98
satAutoValue
inputVibrancy
inputCast
kernel vec4 _smartcolor_contrast_HDR (__sample im, float amt) 
 vec3 diff = im.rgb-dot(im.rgb, vec3(.0, .3, .5)); 
 float dist = distance(diff, vec3(0.0)); 
 dist = smoothstep(0.0, 1.0, dist); 
 float strength = 5.0*dist*amt; 
 vec3 pos = max(im.rgb, 1.0)-1.0 + min(im.rgb, 0.0); 
 im.rgb = clamp(im.rgb, 0.0, 1.0); 
 strength *= (im.b-im.g); 
 strength = max(strength, -0.35); 
 vec4 result; 
 result.rgb = im.rgb/(strength + 1.0 - (im.rgb*strength)) + pos; 
 result.a = im.a; 
 return result; 
kernel vec4 _smartcolor_contrast_darken_HDR (__sample im, float amt) 
 vec3 diff = im.rgb-dot(im.rgb, vec3(.0, .3, .5)); 
 float dist = distance(diff, vec3(0.0)); 
 dist = smoothstep(0.0, 1.0, dist); 
 float strength = 5.0*dist*amt; 
 vec3 pos = max(im.rgb, 1.0)-1.0 + min(im.rgb, 0.0); 
 im.rgb = clamp(im.rgb, 0.0, 1.0); 
 strength *= (im.b-im.g); 
 float gray = 1.0-min(dot(im.rgb, vec3(0.5, 0.7, -0.20)), 1.0); 
 vec4 result; 
 result.rgb = strength < 0.0 ? pow(im.rgb, vec3(1.0-strength*gray)) : im.rgb/(strength+1.0-(im.rgb*strength)); 
 result.rgb += pos; result.a = im.a; 
 return result; 
kernel vec4 _smartcolor_vibrancy_gt1_HDR (__sample im, float amt) 
 float gray = dot(clamp(im.rgb, 0.0, 1.0), vec3(.3, .5, .2)); 
 float y = dot(clamp(im.rgb, 0.0, 1.0), vec3(.4, .2, .1)); 
 float damp = 1.0-4.0*y*(1.0-y); 
 float s = 1.0/(im.r+im.g+im.b); 
 float r = im.r*s; 
 float b = im.b*s; 
 float d = 1.0-.8*smoothstep(0.2, 0.4, r-b); 
 damp *= d; 
 damp = amt > 2.5 ? min(damp+(amt-2.5)/5.0, 1.0) : damp; 
 float sat = min(amt, 3.0); 
 vec4 result; 
 result.rgb = (im.rgb - gray)*sat + gray; 
 result.rgb = mix(im.rgb, result.rgb, damp); 
 result.a = im.a; 
 return result; 
kernel vec4 _smartcolor_vibrancy_lt1_HDR (__sample im, float amt) 
 float gray = dot(im.rgb, vec3(0.333333)); 
 im.rgb = mix(vec3(gray), im.rgb, amt); 
 return im; 
kernel vec4 _smartcolor_cast_HDR (__sample im, float lum, float grayI, float grayQ, float strength) 
 vec4 pix = clamp(im, 0.0, 1.0); 
 pix.rgb = pow(pix.rgb, vec3(.25)); 
 pix.rgb = pix.r * vec3(0.299, 0.595716, 0.211456) + 
 pix.g * vec3(0.587, -0.274453, -0.522591) + 
 pix.b * vec3(0.114, -0.321263, 0.311135); 
 vec2 grayOffset = vec2(grayI, grayQ) ; 
 vec3 result = pix.rgb; 
 float newStrength = 1.0 + (strength-1.0)*(1.0-pix.r) ; 
 result.gb = pix.gb + newStrength*grayOffset ; 
 float damp = max(min(1.0, pix.r/(lum+0.00001)),0.0) ; 
 result.rgb = mix(pix.rgb, result.rgb, damp) ; 
 pix.rgb = result.r * vec3(1.0) + 
 result.g * vec3(0.956296, -0.272122, -1.10699) + 
 result.b * vec3(0.621024, -0.647381, 1.70461); 
 pix.rgb = clamp(pix.rgb, 0.0, 1.0); 
 pix.rgb *= pix.rgb*pix.rgb*pix.rgb; 
 pix.rgb += min(im.rgb, 0.0) + max(im.rgb,1.0) -1.0; 
 return pix; 
CIVibrance
inputAmount
PISmartColorHDR-sat-histogram
Aperture
Photos
ColorNorm
inputAlgorithm
Auto-Enhance input: %llX, target: %llX
%@-%llX
*** failed to compute color normalization, ignored
PIAutoEnhanceFilter
CILocalLight
*** failed to compute smart tone statistics: %@
CISmartTone
autoValue
localAutoValue
CISmartColor
*** failed to compute smart color statistics: %@
*** failed to compute face balance statistics: %@
inputOrigI
inputOrigQ
inputWarmth
inputShadows
CILocalLightFilter
inputLocalLight
inputSmartShadows
inputLightMap
lightMap
inputLightMapWidth
lightMapWidth
inputLightMapHeight
lightMapHeight
CIHighKey
inputBrightness
inputExposure
inputContrast
inputHighlights
inputBlack
inputRawHighlights
-[PIParallaxCompoundLayerStackRequest initWithSegmentationItem:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Parallax/PIParallaxCompoundLayerStackRequest.m
-[PIParallaxCompoundLayerStackRequest initWithComposition:]
-[PIParallaxCompoundLayerStackRequest newRenderJob]
v16@?0@"PFParallaxLayerStack"8
v24@?0@"NSString"8Q16
v16@?0@"PFParallaxLayout"8
@"<NURenderStatistics>"16@?0@"<NURenderResult>"8
crossfadeDurationValue
crossfadeDurationTimescale
startTimeValue
startTimeTimescale
timeRangeStartValue
timeRangeStartTimescale
timeRangeDurationValue
timeRangeDurationTimescale
CIConstantColorGenerator
Failed to produce histogram for matte image
+[PISegmentationHelper histogramForSegmentationMatteImage:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Parallax/PISegmentationHelper.m
q24@?0^q8q16
CIColorThreshold
CIColorInvert
segmentationMatte != nil
+[PISegmentationHelper computeAvoidOverlapYOffsetWithVisibleFrame:imageSize:segmentationMatte:layoutConfiguration:outputUnsafeOverlap:context:]
+[PISegmentationHelper _computeAvoidingRect:imageSize:maxYMotion:segmentationMatte:layoutConfiguration:context:]
saliency preferred
saliency acceptable
padded
inactive
inTime
visible
device %d x %d
9:41
Semibold
bg lum
bg col %ld
fg lum
fg col %ld
Helvetica
CICircleGenerator
portraitInfo
spillMatteAllowed
legacyPosterStyle
+[PIParallaxLegacyPosterStyle applyInactiveStyleToImage:error:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Parallax/PIParallaxLegacyPosterStyle.m
Inactive filter is not available
Failed to produce background image with filter
@"NUAdjustment"16@?0@"NUAdjustment"8
grayI
grayQ
grayWarmth
grayY
warmFace
-[PIOrientationAdjustmentController setOrientation:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Controllers/PIOrientationAdjustmentController.m
+[PIParallaxStyleRecipeArchiver writeRecipe:toURL:error:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Parallax/PIParallaxStyleRecipeArchiver.m
Failed to open recipe destination file
+[PIParallaxStyleRecipeArchiver serializeRecipe:]
parameters
foreground
background
matte
+[PIParallaxStyleRecipeArchiver unarchivedStyleRecipeWithURL:error:]
Failed to open recipe source file
+[PIParallaxStyleRecipeArchiver deserializeRecipe:error:]
dictionary != nil
Missing recipe parameters
Invalid recipe parameters
Failed to deserialize recipe parameters
Missing foreground filters
Invalid foreground filters dictionary
Failed to deserialize foreground filters
Missing background filters
Invalid background filters dictionary
Failed to deserialize background filters
Missing matte filters
Invalid matte filters dictionary
Failed to deserialize matte filters
unit
Unknown parameter type: %@
+[PIParallaxStyleRecipeArchiver _serializeParameter:]
+[PIParallaxStyleRecipeArchiver _deserializeParameters:version:error:]
Invalid parameter name
Invalid parameter dictionary
v32@?0@"NSString"8@"NSDictionary"16^B24
+[PIParallaxStyleRecipeArchiver _deserializeParameter:version:error:]
Missing parameter type
Invalid parameter type
number
Missing number value
Expected a number value
Invalid unit value
Unknown unit value
Missing color values
Expected color values
Expected 4 color values
Expected 4 numbers
Expected 3 color values
Expected 3 numbers
point
Missing point values
Expected point values
Expected 2 point values
Expected 2 numbers
Missing mode value
Expected mode value
binding
Missing binding value
Expected binding value
Unrecognized parameter type
v32@?0@"PIParallaxStyleDefinition"8Q16^B24
filter
name
filters
Unknown definition type: %@
+[PIParallaxStyleRecipeArchiver _serializeDefinition:]
serializedFilters != nil
+[PIParallaxStyleRecipeArchiver _deserializeFilterDefinitions:version:error:]
Invalid definition dictionary
v32@?0@"NSDictionary"8Q16^B24
+[PIParallaxStyleRecipeArchiver _deserializeFilterDefinition:version:error:]
Missing definition type
Invalid filter definition type
filterName
Missing filter name
Invalid filter name
Missing filter parameters
Invalid filter parameters
Filed to deserialize filter parameters
Missing stack name
Invalid stack name
Missing stack filters
Invalid stack filters
Failed to deserialize stack filters
Unrecognized definition type
mdta/com.apple.quicktime.cinematic-video.rendering
mdta/com.apple.quicktime.disparity-float
mdta/com.apple.quicktime.aperture-float
mdta/com.apple.quicktime.camera-dictionary
PIPortraitVideoMetadata.m
metadataGroup != nil
outError != nil
Could not decode timed rendering data
Missing disparity rendering metadata
Missing aperture rendering metadata
v24@?0@"PTRenderPipeline"8@"<PTRenderState>"16
device != nil
-[PIPortraitVideoRenderer initWithDevice:colorSize:disparitySize:quality:debugMode:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Render/PIPortraitVideoRenderer.m
!NUPixelSizeIsEmpty(colorSize)
!NUPixelSizeIsEmpty(disparitySize)
quality >= PTQualityPreviewLow && quality <= PTQualityExportProfessional
<%@:%p device:%@ color=%ldx%ld disparity=%ldx%ld quality=%d debug=%ld>
PIPortraitVideoRenderer.pool
B32@?0@"PIPortraitVideoRenderer"8Q16^B24
Expected _renderPipeline and _renderState to be allocated at the same time
-[PIPortraitVideoRenderer prepareToRenderWithMetadata:]
kernel vec4 _highKeyHDR(__sample im, float str) 
 vec3 neg = min(im.rgb, 0.0); 
 vec3 pos = max(im.rgb, 1.0) - 1.0; 
 im = clamp(im, 0.0, 1.0); 
 vec4 im2 = 1.0-((im-1.0)*(im-1.0)); 
 im2 = sqrt(im2); 
 float y = dot(im.rgb, vec3(.333333)); 
 float y2 = sqrt(1.0-(y-1.0)*(y-1.0)); 
 y2 = mix(y2, smoothstep(0.0, 1.0, y2), 0.5); 
 vec4 im3 = (y>0) ? im*y2/y : vec4(0.0, 0.0, 0.0, 1.0) ; 
 im3 = mix(im3, im2, .7*sqrt(y2)); 
 im3 = mix(im, im3, sqrt(y)) ; 
 im.rgb = mix(im.rgb, im3.rgb, str) + pos + neg; 
 return im; } 
None
UnexpectedRuntimeError
NoBuildingDetected
PanoAndFFCNotSupported
FacesTooLarge
ConfidenceTooLow
LimitExceeded
NotEnoughLines
CorrectedAreaIsSmall
LessThanMinimumCorrection
-[PIPerspectiveAutoCalculator initWithComposition:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Autocalculators/PIPerspectiveAutoCalculator.m
%@.%@
%@.error
%@.underlyingError
pitchError
yawError
pitchError.underlyingError
yawError.underlyingError
%@PerspectiveEvaluation-txt.DBG
%@PerspectiveLineDetection-png.DBG
Edit
PIPerspectiveAutoCalculator-debug
faces != nil
-[PIPerspectiveAutoCalculator getSizeOfAllFaces:]
-[PIPerspectiveAutoCalculator passesFaceCheck:]
PIPerspectiveAutoCalculator-faceCheck
faceSize
maxFaceSize
passesFaceCheck
Apple
front
camera
-[PIPerspectiveAutoCalculator passesImagePropertiesCheck:]
PIPerspectiveAutoCalculator-selfieAndAspectRatioCheck
aspectRatio
isSelfieCam
passesImagePropertiesCheck
-[PIPerspectiveAutoCalculator passesBuildingCheck:]
PIPerspectiveAutoCalculator-classify
minConfidence
passesBuildingCheck
-[PIPerspectiveAutoCalculator submit:]
pitch
angle
-[PIPerspectiveAutoCalculator overcaptureImageProperties:]
No overcapture
PIPerspectiveAutoCalculator-overcaptureImageProperties
-[PIPerspectiveAutoCalculator primaryImageProperties:]
PIPerspectiveAutoCalculator-primaryImageProperties
-[PIPerspectiveAutoCalculator canGenerateNewCropRect:]
preseedRoll
canGenerateNewCropRect
setToPrimary
xOrigin
yOrigin
width
height
pitch != nil
+[PIPerspectiveAutoCalculator undoOrientation:forPitch:yaw:angle:]
yaw != nil
angle != nil
-[PIPerspectiveAutoCalculator passesConfidenceCheck:error:]
passesConfidenceCheck
supported
Not supported by Core Image. Default: YES
-[PIPerspectiveAutoCalculator passesMinimumCorrectionCheck:error:]
pitchExpandTopDegrees
yawExpandLeftDegrees
rollAngleInDegreesCW
passesMinimumCorrectionCheck
minimumPitchCorrection
minimumYawCorrection
minimumAngleCorrection
-[PIPerspectiveAutoCalculator submitVerified:]
focalLength
pitchLimit
yawLimit
rollLimit
minimumPitchCorrectionArea
minimumYawCorrectionArea
CIAutoPerspective
submitVerified
debugImage
pitchCorrectionAreaCoverage
yawCorrectionAreaCoverage
ciPitchError
ciYawError
PIPerspectiveAutoCalculator-getPrimaryOrientation
Flash
FNumber
ApertureValue
ShutterSpeedValue
ExposureTime
ISOSpeedRatings
cannot set empty crop rect
-[PICropAdjustmentController setCropRect:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Controllers/PICropAdjustmentController.m
constraintWidth
constraintHeight
smart
originalCrop
inputDecoderVersion
v32@?0@"NSString"8@16^B24
definitions != nil
-[PIParallaxRecipeFilter _evaluateImageWithFilterDefinitions:inputImage:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Parallax/PIParallaxRecipeFilter.m
Failed to produce an image
inputLight
offsetBlack
offsetBrightness
offsetContrast
offsetExposure
offsetHighlights
offsetLocalLight
offsetShadows
statistics
overcaptureStatistics
kernel vec4 alphaCompositing(__sample src, __sample dst, float a)
return mix(dst, src, a);
kernel vec4 dynamismMap(sampler imageMax, sampler imageMin, float logisticGain, float logisticMid) __attribute__((outputFormat(kCIFormatRGBAh)))
const float MAX_VAL = 1.74;
const float THRESHOLD = 0.05;
vec2 dc = destCoord();
vec2 sc;
sc = dc + vec2(-1,-1);
vec4 pMax00 = sample(imageMax, samplerTransform(imageMax, sc));
vec4 pMin00 = sample(imageMin, samplerTransform(imageMin, sc));
sc = dc + vec2(-1,0);
vec4 pMax01 = sample(imageMax, samplerTransform(imageMax, sc));
vec4 pMin01 = sample(imageMin, samplerTransform(imageMin, sc));
sc = dc + vec2(-1,1);
vec4 pMax02 = sample(imageMax, samplerTransform(imageMax, sc));
vec4 pMin02 = sample(imageMin, samplerTransform(imageMin, sc));
sc = dc + vec2(0,-1);
vec4 pMax10 = sample(imageMax, samplerTransform(imageMax, sc));
vec4 pMin10 = sample(imageMin, samplerTransform(imageMin, sc));
vec4 pMax11 = sample(imageMax, samplerTransform(imageMax, dc));
vec4 pMin11 = sample(imageMin, samplerTransform(imageMin, dc));
sc = dc + vec2(0,1);
vec4 pMax12 = sample(imageMax, samplerTransform(imageMax, sc));
vec4 pMin12 = sample(imageMin, samplerTransform(imageMin, sc));
sc = dc + vec2(1,-1);
vec4 pMax20 = sample(imageMax, samplerTransform(imageMax, sc));
vec4 pMin20 = sample(imageMin, samplerTransform(imageMin, sc));
sc = dc + vec2(1,0);
vec4 pMax21 = sample(imageMax, samplerTransform(imageMax, sc));
vec4 pMin21 = sample(imageMin, samplerTransform(imageMin, sc));
sc = dc + vec2(1,1);
vec4 pMax22 = sample(imageMax, samplerTransform(imageMax, sc));
vec4 pMin22 = sample(imageMin, samplerTransform(imageMin, sc));
float minDevCMaxNMin = distance(pMax11, pMin00);
minDevCMaxNMin = min( minDevCMaxNMin , distance(pMax11, pMin01) );
minDevCMaxNMin = min( minDevCMaxNMin , distance(pMax11, pMin02) );
minDevCMaxNMin = min( minDevCMaxNMin , distance(pMax11, pMin10) );
minDevCMaxNMin = min( minDevCMaxNMin , distance(pMax11, pMin11) );
minDevCMaxNMin = min( minDevCMaxNMin , distance(pMax11, pMin12) );
minDevCMaxNMin = min( minDevCMaxNMin , distance(pMax11, pMin20) );
minDevCMaxNMin = min( minDevCMaxNMin , distance(pMax11, pMin21) );
minDevCMaxNMin = min( minDevCMaxNMin , distance(pMax11, pMin22) );
float minDevCMinNMax = distance(pMin11, pMax00);
minDevCMinNMax = min( minDevCMinNMax , distance(pMin11, pMax01) );
minDevCMinNMax = min( minDevCMinNMax , distance(pMin11, pMax02) );
minDevCMinNMax = min( minDevCMinNMax , distance(pMin11, pMax10) );
minDevCMinNMax = min( minDevCMinNMax , distance(pMin11, pMax11) );
minDevCMinNMax = min( minDevCMinNMax , distance(pMin11, pMax12) );
minDevCMinNMax = min( minDevCMinNMax , distance(pMin11, pMax20) );
minDevCMinNMax = min( minDevCMinNMax , distance(pMin11, pMax21) );
minDevCMinNMax = min( minDevCMinNMax , distance(pMin11, pMax22) );
float outVal = min(minDevCMaxNMin , minDevCMinNMax) / MAX_VAL;
outVal = 1.0f / (1.0f + exp(-logisticGain*(outVal - logisticMid)));
vec4 outPixel = vec4(outVal, outVal, outVal, 1.0);
return outPixel;
kernel vec4 jointbilateralfilter(sampler mask, sampler guide, vec2 rad, vec3 sig) __attribute__((outputFormat(kCIFormatRh)))
vec2 coord = destCoord();
float rh = rad.x;
float rv = rad.y;
vec4 p0 = sample(mask, samplerTransform(mask, coord));
vec4 g0 = sample(guide, samplerTransform(guide, coord));
vec4 result = vec4(0.0f);
float w_sum = 0.0f;
for(float yo=-rad.y;  yo<=rad.y;  yo++) {
for(float xo=-rad.x;  xo<=rad.x;  xo++) {
vec2 dc = vec2(xo, yo);
vec4 p = sample(mask, samplerTransform(mask, coord + dc));
vec4 g = sample(guide, samplerTransform(guide, coord + dc));
float d = dot(dc, dc);
float pdif = dot(p-p0,p-p0);
float gdif = dot(g-g0,g-g0);
float w = exp(dot(sig, vec3(d,pdif,gdif)));
result += w*p;
w_sum += w;
result /= w_sum;
return vec4(result.rgb, 1.0);
kernel vec4 rgba_to_luma(__sample rgb)
const vec4 rgb2y = vec4(0.2999,0.5870,0.1140,0.0);
float luma = dot(rgb, rgb2y);
return vec4(luma, luma, luma, 1.0);
kernel vec2 warp_homography(vec3 h012, vec3 h345, vec3 h678)
vec3 coord = vec3(destCoord(), 1.f);
float norm = 1.f / dot(h678, coord);
float x = norm * dot(h012, coord);
float y = norm * dot(h345, coord);
return vec2(x,y);
kernel vec4 ncc_compute(sampler img1, sampler img2) __attribute__((outputFormat(kCIFormatR8)))
vec2 coord = destCoord();
float sum1   = float(0.0);
float sum2   = float(0.0);
float sumSq1 = float(0.0);
float sumSq2 = float(0.0);
float sumCrs = float(0.0);
float ncc;
float p1, p2;
const int halfKernel = 3;
const vec4 meanOp = vec4(1.0/3.0, 1.0/3.0, 1.0/3.0, 0.0);
float count = 0.0;
for(int y=-halfKernel; y<=halfKernel; y++) {
for(int x=-halfKernel; x<=halfKernel; x++) {
p1 = dot(meanOp, sample(img1, samplerTransform(img1, coord + vec2(x, y))));
p2 = dot(meanOp, sample(img2, samplerTransform(img2, coord + vec2(x, y))));
sum1 += p1;
sum2 += p2;
sumSq1 += p1*p1;
sumSq2 += p2*p2;
sumCrs += p1*p2;
count += 1.0;
float denom = (sumSq1-sum1*sum1/count) * (sumSq2-sum2*sum2/count);
ncc = compare(denom, 0.0, ((sumCrs-(sum1*sum2/count)))/sqrt(denom));
ncc = max(ncc, 0.0);
return vec4(ncc, ncc, ncc, 1.0);
kernel vec4 ncc_coarse_compute(__sample ncc, vec2 edge) __attribute__((outputFormat(kCIFormatR8)))
float value = smoothstep(edge.x, edge.y, ncc.r);
return vec4(value, value, value, 1.0);
kernel vec4 blur_image_compute_3x3(sampler img)
const int halfKernel = 1;
vec2 coord = destCoord();
vec4 sum = vec4(0.0);
float count = 0.0;
for(int y=-halfKernel; y<=halfKernel; y++) {
for(int x=-halfKernel; x<=halfKernel; x++) {
sum += sample(img, samplerTransform(img, coord + vec2(x, y)));
count += 1.0;
sum = sum/(count);
return sum;
kernel vec4 blur_image_compute_5x5(sampler img)
const int halfKernel = 2;
vec2 coord = destCoord();
vec4 sum = vec4(0.0);
float count = 0.0;
for(int y=-halfKernel; y<=halfKernel; y++) {
for(int x=-halfKernel; x<=halfKernel; x++) {
sum += sample(img, samplerTransform(img, coord + vec2(x, y)));
count += 1.0;
sum = sum/(count);
return sum;
kernel vec4 blur_image_compute_7x7(sampler img)
const int halfKernel = 3;
vec2 coord = destCoord();
vec4 sum = vec4(0.0);
float count = 0.0;
for(int y=-halfKernel; y<=halfKernel; y++) {
for(int x=-halfKernel; x<=halfKernel; x++) {
sum += sample(img, samplerTransform(img, coord + vec2(x, y)));
count += 1.0;
sum = sum/(count);
return sum;
kernel vec4 fuse_image_compute(sampler refImg, sampler guideImg, sampler blurImg, sampler weightImg, sampler maskImg, vec2 threshold)
vec4 ref       = sample(refImg, samplerCoord(refImg));
vec4 refBlur   = ref;
vec4 guide     = sample(guideImg, samplerCoord(guideImg));
vec4 guideBlur = sample(blurImg, samplerCoord(blurImg));
float weight   = sample(weightImg, samplerCoord(weightImg)).r;
float mask     = sample(maskImg, samplerCoord(maskImg)).r;
vec3 CbCoef = vec3(-0.1726, -0.3391, 0.5117);
vec3 CrCoef = vec3(0.5115, -0.4282, -0.0830);
float CbRef   = dot(CbCoef, ref.rgb);
float CbGuide = dot(CbCoef, guideBlur.rgb);
float CrRef   = dot(CrCoef, ref.rgb);
float CrGuide = dot(CrCoef, guideBlur.rgb);
float crDiff = smoothstep(0.0, 0.2, abs(CbRef-CbGuide));
float cbDiff = smoothstep(0.0, 0.2, abs(CrRef-CrGuide));
float chDiff = smoothstep(0.0,0.3,crDiff+cbDiff);
float weight1 = (1.0-smoothstep(threshold.x,threshold.y,mask));
weight *= weight1 * (1.0-chDiff);
vec4 value = mix(ref, (0.9*(guide-guideBlur)+refBlur), weight);
return value;
jointbilateralfilter
rgba_to_luma
warp_homography
ncc_compute
ncc_coarse_compute
blur_image_compute_7x7
blur_image_compute_5x5
blur_image_compute_3x3
fuse_image_compute
CIPhotoEffectMono
Mono
CIPhotoEffectTonal
Tonal
CIPhotoEffectNoir
Noir
CIPhotoEffectFade
Fade
CIPhotoEffectChrome
Chrome
CIPhotoEffectProcess
Process
CIPhotoEffectTransfer
Transfer
CIPhotoEffectInstant
Instant
CIPhotoEffect3DVivid
3DVivid
CIPhotoEffect3DVividWarm
3DVividWarm
CIPhotoEffect3DVividCool
3DVividCool
CIPhotoEffect3DDramatic
3DDramatic
CIPhotoEffect3DDramaticWarm
3DDramaticWarm
CIPhotoEffect3DDramaticCool
3DDramaticCool
CIPhotoEffect3DSilverplate
3DSilverplate
CIPhotoEffect3DNoir
3DNoir
CIPortraitEffectBlack
Black
CIPortraitEffectBlackoutMono
BlackoutMono
CIPortraitEffectStageV2
StageV2
CIPortraitEffectStageMonoV2
StageMonoV2
CIPortraitEffectStageWhite
StageWhite
CIPortraitEffectLight
Light
CIPortraitEffectCommercial
Commercial
CIPortraitEffectContour
Contour
CIPortraitEffectStudioV2
StudioV2
CIPortraitEffectContourV2
ContourV2
%@ (preview) %a
%@ %a
+[PIPhotoEditHelper imageSourceWithURL:type:useEmbeddedPreview:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Util/PIPhotoEditHelper.m
+[PIPhotoEditHelper imageSourceWithURL:type:proxyImage:orientation:useEmbeddedPreview:]
+[PIPhotoEditHelper imageSourceWithCIImage:orientation:]
+[PIPhotoEditHelper videoSourceWithURL:]
photoSource != nil
+[PIPhotoEditHelper livePhotoSourceWithPhotoSource:videoSource:]
videoSource != nil
%@+%@
name != nil
+[PIPhotoEditHelper newAdjustmentWithName:]
+[PIPhotoEditHelper newAdjustmentWithIdentifier:]
+[PIPhotoEditHelper newImageRenderClientWithName:]
+[PIPhotoEditHelper geometryRequestWithComposition:]
+[PIPhotoEditHelper imagePropertiesRequestWithComposition:]
+[PIPhotoEditHelper videoPropertiesRequestWithComposition:]
targetSize.width > 0 && targetSize.height > 0
+[PIPhotoEditHelper imageRenderRequestWithComposition:fitInSize:wideGamut:]
PIPhotoEditHelper-targetSize-image
+[PIPhotoEditHelper imageRenderRequestWithComposition:fillInSize:wideGamut:]
PIPhotoEditHelper-fillSquare-image
+[PIPhotoEditHelper _imageRenderRequestWithComposition:wideGamut:]
PIPhotoEditHelper-basic-image
+[PIPhotoEditHelper videoRenderRequestWithComposition:fitInSize:]
v16@?0q8
overrideVideoEditability
v32@?0@"NSString"8@"NSString"16^B24
var image = input;
ResetTagInput('/pre-VideoReframe', image);
image = GetTag('VideoReframe');
ResetTagInput('/pre-Geometry', image);
return GetTag('/post-Geometry');
/pre-VideoReframe
VideoReframe
/pre-Geometry
/post-Geometry
/ShowOriginalSource
ResetTagInput('/pre-Geometry', input);
return GetTag('/post-Geometry');
PIPhotoEditHelper.m
inputCorrectionInfo
depthInfo
start
startScale
endScale
SkipShaderPrewarm
boostVersion
boostParams
gainMapVersion
gainMapParameters
kernel vec4 forwardBoostWithBoost(sampler image, float boost) {
vec4 im = sample(image, samplerCoord(image));
vec4 orig = im;
float n = 1.8;
float k = 0.8;
vec3 pos = max(im.rgb, k) - k;
vec3 neg = min(im.rgb, 0.0);
neg *= 2.8;
im.rgb = clamp(im.rgb, 0.0, k);
im.rgb = ((1.0+n)*im.rgb)/(1.0+(n*im.rgb)) + neg;
im.r = pos.r > 0.0 ? (.91803 + .470304*pos.r) : im.r;
im.g = pos.g > 0.0 ? (.91803 + .470304*pos.g) : im.g;
im.b = pos.b > 0.0 ? (.91803 + .470304*pos.b) : im.b;
im.rgb = mix(orig.rgb, im.rgb, boost);
return im;
kernel vec4 inverseBoostWithBoost(sampler image, float boost){
vec4 im = sample(image, samplerCoord(image));
vec4 orig = im;
float n = 1.8;
float kinv = 0.91803;
vec3 pos = max(im.rgb, kinv);
vec3 neg = min(im.rgb, 0.0);
im.rgb = clamp(im.rgb, 0.0, kinv);
neg *= .35714286;
im.rgb = im.rgb/(n+1.0-(im.rgb*n)) + neg;
im.r = pos.r > kinv ? 0.8 + 2.126286*(pos.r-kinv) : im.r;
im.g = pos.g > kinv ? 0.8 + 2.126286*(pos.g-kinv) : im.g;
im.b = pos.b > kinv ? 0.8 + 2.126286*(pos.b-kinv) : im.b;
im.rgb = mix(orig.rgb, im.rgb, boost);
return im;
kernel vec4 forwardBoost3(__sample im, float boost, vec4 abcd, vec3 p0, vec3 p1) {
float a = abcd.x;
float b = abcd.y;
float c = abcd.z;
float d = abcd.w;
vec3 x = im.rgb;
vec3 f = (a * x + b) / (c * x + d);
float x0 = p0.x;
float y0 = p0.y;
float s0 = p0.z;
float a0 = y0*y0;
float c0 = y0 - s0*x0;
float d0 = s0*x0*x0;
vec3 f0 = (a0 * x) / (c0 * x + d0);
vec3 l0 = (a0 / d0) * x;
f0 = compare(x, l0, f0);
float x1 = p1.x;
float y1 = p1.y;
float s1 = p1.z;
float a1 = (1.f-y1)*(1.f-y1);
float c1 = (1.f-y1) - s1*(1.f-x1);
float d1 = s1 *(1.f-x1)*(1.f-x1);
vec3 f1 = 1.f - (a1 * (1.f - x)) / (c1 * (1.f - x) + d1);
vec3 l1 = 1.f - (a1 / d1) * (1.f - x);
f1 = compare(1.f - x, l1, f1);
f = compare(x - p0.x, f0, compare(x - p1.x, f, f1));
im.rgb = mix(im.rgb, f, boost);
return im;
kernel vec4 inverseBoost3(__sample im, float boost, vec4 abcd, vec3 p0, vec3 p1) {
float a = abcd.x;
float b = abcd.y;
float c = abcd.z;
float d = abcd.w;
vec3 y = im.rgb;
vec3 g = (d * y - b) / (a - c * y);
float x0 = p0.x;
float y0 = p0.y;
float s0 = p0.z;
float a0 = x0*x0;
float c0 = x0 - y0/s0;
float d0 = y0*y0/s0;
vec3 g0 = (a0 * y) / (c0 * y + d0);
vec3 l0 = (a0 / d0) * y;
g0 = compare(y, l0, g0);
float x1 = p1.x;
float y1 = p1.y;
float s1 = p1.z;
float a1 = (1.f-x1)*(1.f-x1);
float c1 = (1.f-x1) - (1.f-y1)/s1;
float d1 = (1.f-y1)*(1.f-y1)/s1;
vec3 g1 = 1.f - (a1 * (1.f - y)) / (c1 * (1.f - y) + d1);
vec3 l1 = 1.f - (a1 / d1) * (1.f - y);
g1 = compare(1.f - y, l1, g1);
g = compare(y - p0.y, g0, compare(y - p1.y, g, g1));
im.rgb = mix(im.rgb, g, boost);
return im;
+[PIFakeBoost kernelFB0]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Adjustments/PIFakeBoost.m
+[PIFakeBoost kernelFB3]
properties != nil
+[PIFakeBoost boostParametersFromRawProperties:]
-[PIFakeBoost outputImageFB0]
-[PIFakeBoost outputImageFB3]
Invalid boost parameters
Invalid points parameters
boost kernel is nil
+[PIForwardFakeBoost kernelFB0]_block_invoke
+[PIForwardFakeBoost kernelFB3]_block_invoke
inverse boost kernel is nil
+[PIInverseFakeBoost kernelFB0]_block_invoke
+[PIInverseFakeBoost kernelFB3]_block_invoke
PIAttributeTypeMode
PIAttributeAvailableModes
default
metallib
Failed to load metal lib data: %@
+[PICoreImageUtilities metalLibraryData]_block_invoke
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Adjustments/PICoreImageUtilities.m
AutoEnhance
AutoLoop
Crop
DepthEffect
Effect
Effect3D
Grain
HighResolutionFusion
LivePhotoKeyFrame
Mute
PortraitEffect
PortraitVideo
RedEyeBB
SelectiveColor
SemanticEnhance
SlowMotion
SmartBlackAndWhite
SmartColor
SmartTone
SourceSelect
Trim
VideoCrossfadeLoop
VideoPosterFrame
VideoStabilize
WhiteBalance
compositionName
identifierName
custom
customSerialization
customCompositionName
customIdentifierName
requiresEnabled
offsetBlackPoint
v24@?0@"NSDictionary"8@"NUAdjustment"16
v32@?0@8@16^B24
offsetSaturation
offsetCast
offsetStrength
offsetGrain
offsetTone
inputBlackAndWhite
offsetNeutralGamma
amount
seed
DGVignetteEffectOperation
Vignette
inputIntensity
inputRadius
inputFalloff
DGDefinition2Operation
Definition
RKNoiseReductionOperation
NoiseReduction
edgeDetail
RKProSharpenOperation
Sharpen
inputEdgeScale
edges
inputSharpness
@"NSString"16@?0@"NSDictionary"8
effectName
@"NSString"24@?0@"NSDictionary"8@"NUAdjustment"16
effectVersion
effectIntensity
CropStraighten
straightenAngle
DGRAWReduceNoiseOperation
RawNoiseReduction
inputDetailAmount
detail
inputCNRAmount
inputLNRAmount
RKRawDecodeOperation
inputMethodVersion
colorComponents
slope
bias
RKLevelsOperation
Levels
colorSpace
inputColorSpace
blackSrcRGB
blackDstRGB
shadowSrcRGB
shadowDstRGB
midSrcRGB
midDstRGB
hilightSrcRGB
hilightDstRGB
whiteSrcRGB
whiteDstRGB
blackSrcRed
blackDstRed
shadowSrcRed
shadowDstRed
midSrcRed
midDstRed
hilightSrcRed
hilightDstRed
whiteSrcRed
whiteDstRed
blackSrcGreen
blackDstGreen
shadowSrcGreen
shadowDstGreen
midSrcGreen
midDstGreen
hilightSrcGreen
hilightDstGreen
whiteSrcGreen
whiteDstGreen
blackSrcBlue
blackDstBlue
shadowSrcBlue
shadowDstBlue
midSrcBlue
midDstBlue
hilightSrcBlue
hilightDstBlue
whiteSrcBlue
whiteDstBlue
Generic P3
Display P3
Adobe RGB
sRGB
RKCurvesOperation
Curves
RGBCurvePoints
pointsL
redCurvePoints
pointsR
greenCurvePoints
pointsG
blueCurvePoints
pointsB
RKSelectiveColorOperation
corrections
RKRetouchOperation
Retouch
B16@?0@"NSDictionary"8
inputStrokes
hasSource
brushStroke
softness
opacity
repairEdges
sourceOffset
data
xLocation
yLocation
pressure
points
clipRect
detectedFaces
RKRedEyeOperation
ApertureRedEye
inputSpots
pointX
center
pointY
sensitivity
RedEye
allCorrections
redEyeCorrections
autoRedEyeCorrections
endTime
timeRange
rate
timescale
alignment
glassesMatteAllowed
portraitEffectFilterName
keyframes
cropFraction
analysisType
v24@?0@"NSMutableDictionary"8@"NUAdjustment"16
face
warm
inputDetectedFaces
flags
epoch
lightMapAvg
vec4 _curve_sample_HDR(float x, sampler2D table, vec2 domain, vec2 normalizer) { x = (x - domain.x) / (domain.y - domain.x); x = clamp(x, 0.0001, 0.9999); x = normalizer.x * x + normalizer.y; return texture2D(table, vec2(x, 0.5)); } kernel vec4 _curves_rgb_lum_HDR(__sample color, sampler2D table, vec2 domain, vec2 normalizer) { vec4 pixel = color; pixel.r = _curve_sample_HDR(pixel.r, table, domain, normalizer).r; pixel.g = _curve_sample_HDR(pixel.g, table, domain, normalizer).g; pixel.b = _curve_sample_HDR(pixel.b, table, domain, normalizer).b; float lum0 = dot(pixel.rgb, vec3(0.3, 0.59, 0.11)); float lum1 = _curve_sample_HDR(lum0, table, domain, normalizer).a; float lum1c = clamp(lum1, -8.0 * abs(lum0), 8.0 * abs(lum0)); float lum_scale = (lum0 == 0.0 ? 0.0 : lum1c / lum0); float lum_offset = lum1 - lum1c; pixel.rgb = lum_scale * pixel.rgb + lum_offset; pixel.rgb = min(pixel.rgb, 1.0); return pixel; }
-[_PIParallaxLayoutJob initWithRequest:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Parallax/PIParallaxLayoutRequest.m
Missing layout configuration
Missing layout regions
Invalid segmentation matte size
Invalid segmentation classification
Invalid segmentation confidence map size
-[_PIParallaxLayoutJob render:]
Missing output geometry
Failed to generate segmentation layout
-[_PIParallaxLayoutJob complete:]
layer stack has no background layer
none
pixels
degrees
count
logic
stack
clockFont
clockColor
<%@:%p parameters: %@
foreground:%@
 background: %@
 matte: %@>
-[PIParallaxStyleDefinition type]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Parallax/PIParallaxStyleRecipe.m
-[PIParallaxStyleDefinition isEqualToParallaxStyleDefinition:]
-[PIParallaxStyleDefinition evaluateWithContext:error:]
-[PIParallaxStyleFilterDefinition init]
context != nil
-[PIParallaxStyleFilterDefinition evaluateWithContext:error:]
Unknown filter
Failed to set filter parameter value
Unknown filter parameter
v32@?0@"NSString"8@"PIParallaxStyleParameter"16^B24
Failed to evaluate filter parameters
Local
inputLightMapImage
inputTargetBackgroundImage
filter produced invalid image
CILocalContrast
LocalContrast
<%@:%p filter:%@ parameters: %@>
-[PIParallaxStyleFilterStackDefinition init]
-[PIParallaxStyleFilterStackDefinition evaluateWithContext:error:]
<%@:%p stack:%@ filters:%@>
-[PIParallaxStyleParameter type]
-[PIParallaxStyleParameter evaluateWithContext:error:]
-[PIParallaxStyleParameter isEqualToParallaxStyleParameter:]
(%@, unit: %@) 
(R:%@, G:%@, B:%@, A:%@)
(X:%@, Y:%@, unit: %@)
(>%@)
-[PIParallaxStyleBindingParameter evaluateWithContext:error:]
Unable to find source for variable bound to '%@'
($%@)
_PIDynamicLocalLightMapPrepare
inputGuideImage
PILongExposureAccumulator.state
PILongExposureAccumulator.accum
Accumulation was cancelled
PILongExposureAccumulator-main-j%lld
failed to init accumulator
B16@?0@"CIRenderDestination"8
frame != nil
-[PILongExposureAccumulator accumulate:error:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/AutoLoop/PIAutoLoopJob+LongExposure.m
-[PILongExposureAccumulator _accumulate:error:]
PILongExposureAccumulator-average-j%lld
failed to render average image
v16@?0@"CIImage"8
CIBoxBlur
PILongExposureAccumulator-minimum-j%lld
failed to render minimum image
PILongExposureAccumulator-maximum-j%lld
failed to render maximum image
-[PILongExposureAccumulator writeLongExposureImage:UTI:colorSpace:error:]
-[PILongExposureAccumulator writeMaskImage:UTI:error:]
-[PILongExposureAccumulator _exportOutputImage:format:colorSpace:toURL:uti:error:]
Failed to finalize image destination
Failed to create CGImageDestinationRef
Failed to create CGImageRef
/AutoLoop/LongExposure
-[PILongExposureRegistrationJob prepare:]
return Source(composition.source, {'skipOrientation':true});
Malformed AutoLoop recipe : crop
-[PILongExposureRegistrationJob render:]
Failed to allocate intermediate pixel buffer
PILongExposureRegistrationJob-render-j%lld
PILongExposureRegistrationJob-reference-j%lld
Failed to render luma
Image registration failure (expected 1 observation)
/RAW/SushiLevel1
/Master%@
CIRedEyeCorrection
inputCameraModel
CILanczosScaleTransform
1.5.1
1.9.1
1.10
computed
overcaptureComputed
dynamic
@"NSString"16@?0@"NUAdjustment"8
SDOFRenderingVersion
convexHull
/private/var/mobile/Media/PhotoData/CaptureDebug/
Repair
Clone
RepairML
<%@: %p; min = %.2f, max = %.2f, manualMin = %.2f, manualMax = %.2f, depthMin = %.2f, depthMax = %.2f>
manualMin
manualMax
depthMin
depthMax
SegmentationScoreRanges
monochrome
assetURL
-[PIPortraitVideoDebugDetectionsRenderNode resolvedNodeWithCachedInputs:settings:pipelineState:error:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Render/PIPortraitVideoDebugDetectionsRenderNode.m
v20@?0B8@"NSError"12
%@-labelImageCache
-[PIPortraitVideoDebugDetectionsRenderNode _evaluateImage:]
Face
Head
Torso
Sports Ball
AutoFocus
%@ %@
%@ G:%@
kernel vec4 ipt_hue_chroma_color_wash(__sample s, vec3 c) {
float luma = s.x;
float hue = c.y;
float chroma = 0.5*(s.z+c.z);
return vec4(luma, hue, chroma, s.a);
kernel vec4 ipt_hue_chroma_color_wash_fixed(__sample s, vec3 c) {
float luma = (s.x <= 0.5) ? mix(0.0, c.x, 2.0*s.x) : mix(c.x, 1.0, 2.0*(s.x-0.5));
float hue = c.y;
float chroma = 0.5*(s.z+c.z);
return vec4(luma, hue, chroma, s.a);
kernel vec4 ipt_hue_chroma_color_wash_variable(__sample s, vec3 c) {
float x0 = 0.5;
float y0 = 0.5 + 0.75 * (c.x - 0.5);
float a = (y0 - x0) / (x0 * (1.0 - y0));
float luma = (s.x * (a + 1.0)) / (s.x * a + 1.0);
float hue = c.y;
float chroma = 0.5*(s.z+c.z);
return vec4(luma, hue, chroma, s.a);
kernel vec4 rgb_color_wash_variable(__sample s, __color c) {
float l0 = dot(c.rgb, vec3(0.299, 0.587, 0.114));
float l = dot(s.rgb, vec3(0.299, 0.587, 0.114));
vec3 cw;
if (l <= l0) {
cw = mix(vec3(0), c.rgb, l/l0);
} else {
cw = mix(c.rgb, vec3(1), (l-l0)/(1-l0));
return vec4(cw, 1.0);
kernel vec4 rgb_color_wash_variable_smooth(__sample s, __color c) {
float x0 = dot(c.rgb, vec3(0.299, 0.587, 0.114));
vec3 y0 = clamp(c.rgb, 0.05, 0.95);
float x = dot(s.rgb, vec3(0.299, 0.587, 0.114));
vec3 a = (y0 - x0) / (x0 * (1.0 - y0));
vec3 y = (x * (a + 1.0)) / (x * a + 1.0);
return vec4(y, 1.0);
kernel vec4 rgb_color_wash_fixed(__sample s, __color c) {
float l = dot(s.rgb, vec3(0.299, 0.587, 0.114));
vec3 cw;
if (l <= 0.5) {
cw = mix(vec3(0), c.rgb, 2*l);
} else {
cw = mix(c.rgb, vec3(1), 2*(l-0.5));
return vec4(cw, 1.0);
kernel vec4 rgb_color_wash_fixed_smooth(__sample s, __color c) {
float x0 = 0.5;
vec3 y0 = clamp(c.rgb, 0.05, 0.95);
float x = dot(s.rgb, vec3(0.299, 0.587, 0.114));
vec3 a = (y0 - x0) / (x0 * (1.0 - y0));
vec3 y = (x * (a + 1.0)) / (x * a + 1.0);
return vec4(y, 1.0);
kernel vec4 ipt_color_wash_duo(__sample s, vec3 c0, vec3 c1) {
vec3 cw = mix(c0, c1, s.x);
return vec4(cw, s.a);
kernel vec4 ipt_color_wash_duo_fixed(__sample s, vec3 c0, vec3 c1) {
vec3 cw;
cw.x = (s.x <= 0.2) ? mix(0.0, c0.x, s.x/0.2) : (s.x <= 0.8) ? mix(c0.x, c1.x, (s.x-0.2)/(0.8-0.2)) : mix(c1.x, 1.0, (s.x-0.8)/(1.0-0.8));
float x = clamp((s.x-0.2)/(0.8-0.2), 0.0, 1.0);
cw.yz = mix(c0.yz, c1.yz, x);
return vec4(cw, s.a);
kernel vec4 ipt_color_wash_duo_variable(__sample s, vec3 c0, vec3 c1) {
vec3 cw;
cw.x = (s.x <= c0.x) ? mix(0.0, c0.x, s.x/c0.x) : (s.x <= c1.x) ? mix(c0.x, c1.x, (s.x-c0.x)/(c1.x-c0.x)) : mix(c1.x, 1.0, (s.x-c1.x)/(1.0-c1.x));
float x = clamp((s.x-c0.x)/(c1.x-c0.x), 0.0, 1.0);
cw.yz = mix(c0.yz, c1.yz, x);
return vec4(cw, s.a);
kernel vec4 ipt_hue_chroma_color_wash_duo(__sample s, vec3 c0, vec3 c1) {
vec3 lhc = mix(c0, c1, s.x);
lhc.z = 0.5*(s.z+lhc.z);
return vec4(lhc, s.a);
kernel vec4 ipt_hue_chroma_color_wash_duo_fixed(__sample s, vec3 c0, vec3 c1) {
float l = (s.x <= 0.8) ? mix(c0.x, c1.x, s.x/0.8) : mix(c1.x, 1.0, (s.x-0.8)/(1.0-0.8));
float x = clamp(s.x/0.8, 0.0, 1.0);
vec2 hc = mix(c0.yz, c1.yz, x);
hc.y = 0.5*(s.z+hc.y);
return vec4(l, hc.x, hc.y, s.a);
kernel vec4 ipt_hue_chroma_color_wash_duo_variable(__sample s, vec3 c0, vec3 c1) {
float l = (s.x <= c1.x) ? mix(c0.x, c1.x, s.x/c1.x) : mix(c1.x, 1.0, (s.x-c1.x)/(1.0-c1.x));
float x = clamp(s.x/c1.x, 0.0, 1.0);
vec2 hc = mix(c0.yz, c1.yz, x);
hc.y = 0.5*(s.z+hc.y);
return vec4(l, hc.x, hc.y, s.a);
kernel vec4 rgb_color_wash_duo(__sample s, __color c0, __color c1) {
float l = dot(s.rgb, vec3(0.299, 0.587, 0.114));
vec3 cw = mix(c0.rgb, c1.rgb, l);
return vec4(cw, s.a);
kernel vec4 rgb_color_wash_duo_fixed(__sample s, __color c0, __color c1) {
float l = dot(s.rgb, vec3(0.299, 0.587, 0.114));
vec3 cw;
if (l <= 0.75) {
cw = mix(c0.rgb, c1.rgb, l/0.75);
} else {
cw = mix(c1.rgb, vec3(1), 4*(l-0.75));
return vec4(cw, s.a);
kernel vec4 rgb_color_wash_duo_variable(__sample s, __color c0, __color c1) {
float l = dot(s.rgb, vec3(0.299, 0.587, 0.114));
float l0 = dot(c0.rgb, vec3(0.299, 0.587, 0.114));
float l1 = dot(c1.rgb, vec3(0.299, 0.587, 0.114));
vec3 cw;
if (l <= l1) {
cw = mix(c0.rgb, c1.rgb, l/l1);
} else {
cw = mix(c1.rgb, vec3(1), (l-l1)/(1-l1));
return vec4(cw, s.a);
ipt_hue_chroma_color_wash
ipt_hue_chroma_color_wash_fixed
ipt_hue_chroma_color_wash_variable
rgb_color_wash_fixed
rgb_color_wash_variable
rgb_color_wash_fixed_smooth
rgb_color_wash_variable_smooth
inputMode
HueChroma
HueChromaFixed
HueChromaVariable
RGBFixed
RGBFixedSmooth
RGBVariable
RGBVariableSmooth
MonoLight
CISoftLightBlendMode
ipt_color_wash_duo
ipt_color_wash_duo_fixed
ipt_color_wash_duo_variable
ipt_hue_chroma_color_wash_duo
ipt_hue_chroma_color_wash_duo_fixed
ipt_hue_chroma_color_wash_duo_variable
rgb_color_wash_duo
rgb_color_wash_duo_fixed
rgb_color_wash_duo_variable
inputShadowColor
inputHighlightColor
IPTFixed
IPTVariable
outputExposure
falseColorHDR
inputRAWGamutMapMax
Buffer must be RGBA16 type for red eye repairs
id<NUBuffer> computeRepairMask(__strong id<NUBuffer>, uint16_t *)
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Adjustments/ApertureRedEye/PIApertureRedEye.mm
reddestx out of bounds
void seedFill(__strong id<NUMutableBuffer>, __strong id<NUBuffer>, const uint32_t, const NSInteger, const NSInteger)
reddesty out of bounds
void applyRepairMask(__strong id<NUMutableBuffer>, __strong id<NUBuffer>, double, const uint32_t)
repairBuffer != nil
kernel vec4 facebalance(__sample src, vec2 gamma, vec3 matchMinusOrigStrength)
vec4 pix = src;
pix.rgb = pow(max(pix.rgb, 0.0), vec3(gamma.x));
pix.rgb = pix.r * vec3(0.299,  0.595716,  0.211456) +
pix.g * vec3(0.587, -0.274453, -0.522591) +
pix.b * vec3(0.114, -0.321263,  0.311135);
vec4 orig = pix ;
float chroma = min(1.0, 2.0*sqrt(pix.g*pix.g + pix.b*pix.b) );
pix.gb +=  matchMinusOrigStrength.rg;
float strength = matchMinusOrigStrength.z*pow(chroma, .4) ;
pix.gb = mix(orig.gb, pix.gb, strength) ;
pix.rgb = pix.rrr +
pix.g * vec3(0.956296, -0.272122, -1.10699) +
pix.b * vec3(0.621024, -0.647381, 1.70461);
pix.rgb = max(pix.rgb, vec3(0.0));
pix.rgb = pow(pix.rgb, vec3(gamma.y));
return pix;
facebalance
vec4 curve_sample(float x, sampler2D table, vec2 domain, vec2 normalizer)
x = (x - domain.x) / (domain.y - domain.x);
x = clamp(x, 0.0001, 0.9999);
x = normalizer.x * x + normalizer.y;
return texture2D(table, vec2(x, 0.5));
kernel vec4 curves_rgb_lum(__sample color, sampler2D table, vec2 domain, vec2 normalizer)
vec4 pixel = color;
pixel.r = curve_sample(pixel.r, table, domain, normalizer).r;
pixel.g = curve_sample(pixel.g, table, domain, normalizer).g;
pixel.b = curve_sample(pixel.b, table, domain, normalizer).b;
float lum0 = dot(pixel.rgb, vec3(0.3, 0.59, 0.11));
float lum1 = curve_sample(lum0, table, domain, normalizer).a;
float lum1c = clamp(lum1, -8.0 * abs(lum0), 8.0 * abs(lum0));
float lum_scale = (lum0 == 0.0 ? 0.0 : lum1c / lum0);
float lum_offset = lum1 - lum1c;
pixel.rgb = lum_scale * pixel.rgb + lum_offset;
return pixel;
tableR != nil
+[PICurvesLUTFilter tableImageFromRed:green:blue:luminance:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Adjustments/PICurvesFilter.mm
tableG != nil
tableB != nil
tableL != nil
kernel vec4 gHDRtoPP(sampler image)
vec4 pix ;
vec3 pix2;
pix = sample(image, samplerCoord(image));
pix2 = pix.r * vec3(0.615429622407401,   0.114831839141528,   0.011544126697221) +
pix.g * vec3(0.367479646665836,   0.797943554457996,   0.064077744191180) +
pix.b * vec3(  0.016956659608091,   0.087783443422360,   0.924405601458102);
return vec4(pix2, pix.a);
kernel vec4 PPtogHDR(sampler image)
vec4 pix ;
vec3 pix2;
pix = sample(image, samplerCoord(image));
pix2 = pix.r * vec3(1.777445503202045,  -0.255296595099306,  -0.004500433755654) +
pix.g * vec3( -0.822224875430495,   1.380948853784730,  -0.085456231694984) +
pix.b * vec3(0.045475917061484,  -0.126454737973025,   1.089973874037625);
return vec4(pix2, pix.a);
kernel vec4 colorBalance(__sample image, float colorI, float colorQ, float str, vec2 gamma)
vec4 pix = image;
vec3 neg = min(pix.rgb, 0.0);
vec3 pos = max(pix.rgb, 1.0) - 1.0;
pix.rgb = pow(clamp(pix.rgb, 0.0, 1.0), vec3(gamma.x));
pix.rgb = pix.r * vec3(0.299,  0.595716,  0.211456) +
pix.g * vec3(0.587, -0.274453, -0.522591) +
pix.b * vec3(0.114, -0.321263,  0.311135);
vec4 orig = pix ;
float chroma = min(1.0, 2.0*sqrt(pix.g*pix.g + pix.b*pix.b) );
pix.gb += vec2(colorI,colorQ);
float strength = str*pow(chroma, .4) ;
pix.gb = mix(orig.gb, pix.gb, strength) ;
pix.rgb = pix.rrr +
pix.g * vec3(0.956296, -0.272122, -1.10699) +
pix.b * vec3(0.621024, -0.647381, 1.70461);
pix.rgb = pow(max(pix.rgb, 0.0), vec3(gamma.y));
pix.rgb += pos + neg;
return pix;
inputWarmTemp
inputWarmTint
inputHasFace
gHDRtoPP
PPtogHDR
colorBalance
-[PIColorBalanceFilter outputImage]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Adjustments/PIColorBalanceFilter.m
@"NSString"16@?0@"NSString"8
CIMix
CIGammaAdjust
CIExposureAdjust
CIProSharpenEdges
PILocalContrastHDR
CILocalLightMapPrepare
CIPhotoGrain
CISmartBlackAndWhite
CIVignetteEffect
PIFalseColorHDRDebug
PIColorBalanceFilter
PIRAWFaceBalance
PINeutralGrayWhiteBalanceFilter
PIBilateralFilter
PICurvesLUTFilter
PICurvesFilter
PISelectiveColorFilter
PILevelsFilter
PIGlobalSettings
IPXRootSettings
PXSettingsArchiveKey
editSettings
PURootSettings
photoEditingSettings
PI_SEGMENT_ROUND_TRIP_PROXY
PI_SEGMENT_DISABLE_SEGMENTATION
PI_SEGMENT_INFILL_ALGO
PI_SEGMENT_TINT_LAYERS
PI_SEGMENT_DISABLE_CACHE
PI_SEGMENT_PREVIEW_DISABLE_CLOCK
PI_SEGMENT_PREVIEW_HIGH_QUALITY
PI_SEGMENT_MANUAL_GATING_LENIENCE
PI_STYLE_RECIPE_DIR_PATH
PI_USE_STYLE_RECIPE_CONFIG
PI_PARALLAX_LAYOUT_CONFIG
PI_PARALLAX_DISABLE_UPGRADE
PI_CINEMATIC_ALLOW_YUV_SOURCE
PI_CINEMATIC_ALLOW_RGB10_PACKED
PI_AUTOLOOP_EXPORT_USE_METAL
B8@?0
-[PIAutoLoopExportJob initWithVideoExportRequest:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/AutoLoop/PIAutoLoopJob+Export.m
@"NUJSRenderNode"24@?0@"NUJSRenderNode"8@"JSValue"16
Invalid data type for adjustmentValue
Invalid data type for keyframes
Invalid data type for stabCropRect
input to VideoReframe cannot be nil
Unable to unwrap the input node!
-[PIJSRenderPipeline setUpContext:]_block_invoke
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Pipeline/PIPhotosPipeline.m
Missing duration for crossfade
input to VideoCrossfadeLoop cannot be nil
{CGRect={CGPoint=dd}{CGSize=dd}}64@?0{CGSize=dd}8{CGSize=dd}24{CGSize=dd}40@"JSValue"56
OvercaptureRectForAutoCrop
Couldn't find bundle for class %@
+[NUJSRenderPipeline(PhotosPipeline) newPhotosPipeline:]
PhotosPipeline
Couldn't find the specified Pipeline
JSContext
Class getJSContextClass(void)_block_invoke
JavaScriptCoreSoftLinking.h
Unable to find class %s
void *JavaScriptCoreLibrary(void)
-[PIApertureRedEyeAutoCalculator submit:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Autocalculators/PIApertureRedEyeAutoCalculator.m
PIApertureRedEyeAutoCalculator-faceDetection
pre-Adjustments
redEyeSpots
PILocalLightHDR-stats
NSNumber
kernel vec4 _shadowKernelHDR(__sample im, __sample adj, float str) { adj.r = 3.4*adj.r-1.2; vec3 neg = min(im.rgb, 0.0); vec3 pos = max(im.rgb, 1.0)-1.0; im.rgb = clamp(im.rgb, 0.0, 1.0); vec4 orig = im; float y = sqrt(dot(im.rgb, vec3(.33333))); float s = mix(0.0, adj.r, str); vec3 gain = s > 0.0 ? vec3(0.0) : vec3(s*s) * vec3(-2.75, -2.75, -2.5); im.rgb = im.rgb*im.rgb*gain + im.rgb*(1.0-gain); float m = 1.0 + 1.85*s*(max(0.1-y, 0.0)) ; im.rgb = (clamp(m*im.rgb, 0.0, 1.0)); float midAmt = s < 0.0 ? min(s*s,1.0) : 0.0; y = y*(1.0-y); im.rgb = sqrt(im.rgb); float pivot = .4; float a = midAmt*y; float b = -pivot*a; vec3 pix = im.r * vec3(0.299*a) + 
 im.g * vec3(0.587*a) + 
 im.b * vec3(0.114*a) + 
 im.rgb + vec3(b); im.rgb = mix(im.rgb, vec3(pivot), -y*midAmt); im.rgb = mix(im.rgb, pix, 0.8); im.rgb = max(im.rgb, 0.0); im.rgb *= im.rgb; im.rgb = clamp(im.rgb, 0.0,1.0)+pos+neg; return im; }
kernel vec4 _polyKernelHDR(__sample im, __sample adj, float str) { adj.r = 3.4*adj.r-1.2; vec3 neg = min(im.rgb, 0.0); vec3 pos = max(im.rgb, 1.0)-1.0; im.rgb = clamp(im.rgb, 0.0, 1.0); vec4 orig = im; float y = sqrt(dot(im.rgb, vec3(.33333))); float s = mix(0.0, adj.r, str); vec3 gain = s > 0.0 ? vec3(1.5*s) : vec3(1.75*s, 1.75*s, 1.55*s); im.rgb = im.rgb*im.rgb*gain + im.rgb*(1.0-gain); im.rgb = (clamp(im.rgb, 0.0, 1.0)); float midAmt = min(str, .5); y = y*(1.0-y); im.rgb = sqrt(im.rgb); float pivot = max(adj.g, 0.5); float a = midAmt*y; float b = -pivot*a; vec3 pix = im.r * vec3(0.299*a) + 
 im.g * vec3(0.587*a) + 
 im.b * vec3(0.114*a) + 
 im.rgb + vec3(b); 
 im.rgb = mix(im.rgb, vec3(pivot), -y*midAmt); im.rgb = mix(im.rgb, pix, 0.8); im.rgb = max(im.rgb, 0.0); im.rgb *= im.rgb; im.rgb = clamp(im.rgb, 0.0,1.0)+pos+neg; return im; }
-[PILocalLightFilterHDR outputImage]
PILocalLightHDR.m
CGRectEqualToRect([guideImage extent], [inputImage extent])
inputImage != nil
guideImage != nil
inputLocalLight != nil
CGRectEqualToRect([lightMapImage extent], [inputImage extent])
PILocalLightHDR
_lightMapImageFromData_block_invoke
x == 0
y == 0
width == lmWidth
height == lmHeight
CIEdgePreserveUpsampleRGFilter
-[PIParallaxAsset initWithFileURL:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Parallax/PIParallaxAsset.m
<%@: %p; fileURL = %@>
Asset is not local
proxy.jpg
Failed to load thumbnail image
Failed to load image properties
PIParallaxAsset.m
Unsupported
Failed to read image file
MediaAnalysis not available
v16@?0@"<NUMutableBuffer>"8
v24@?0@"<NUBufferTile>"8^B16
-[PISmartBlackAndWhiteAutoCalculator submit:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Autocalculators/PISmartBlackAndWhiteAutoCalculator.mm
PISmartBlackAndWhiteAutoCalculator-renderInputImage
Produced invalid BlackAndWhite settings, using defaults
void calculate_bw_auto_matrix_lum_contrast(__strong id<NUBuffer>, CGColorSpaceRef, MsgBlackAndWhiteSettings *)
num == w
void MsgMatrix<double>::AppendRow(const NumberType *, uint) [MatrixFloatType = double, NumberType = double]
xi < w && yi < h
MatrixFloatType &MsgMatrix<double>::operator()(size_t, size_t) [MatrixFloatType = double]
index < data.size()
inputScaleFactor
kernel vec4 _smartBlackAndWhiteHDR(__sample imageHDR, sampler2D hueImage, vec4 rgbWeights, vec4 normalizer) { float hueTableScaleFactor = rgbWeights.w; float hueImageWidth = normalizer.x; float huePixelCenter = normalizer.y; float neutralGamma = normalizer.z; float phototone = normalizer.w; float bw = dot(imageHDR.rgb / 12.0, rgbWeights.rgb); bw = clamp(bw, 0.0, 1.0); vec3 lms; lms.x = dot(imageHDR.rgb, vec3(0.3139902162, 0.6395129383, 0.0464975462)); lms.y = dot(imageHDR.rgb, vec3(0.155372406, 0.7578944616, 0.0867014186)); lms.z = dot(imageHDR.rgb, vec3(0.017752387, 0.109442094, 0.8725692246)); lms = pow(lms, vec3(0.43)); float i = dot(lms, vec3(0.4,0.4,0.2)); float p = dot(lms, vec3(4.4550,-4.8510,0.3960)); float t = dot(lms, vec3(0.8056,0.3572,-1.1628)); float chroma = sqrt(p*p+t*t); float hue = 0.5 + (atan(t, p) / 6.28318530718); vec2 huePt = vec2(hue * hueImageWidth + huePixelCenter, 0.5); float hueGamma = hueTableScaleFactor * texture2D(hueImage, huePt).a; float cd = 0.06 + 0.53 * abs(i-0.5); float lowSaturationDamp = smoothstep(0.0, 1.0, (chroma)/cd); float intensityDamp = smoothstep(0.0, 1.0, 1.0 - i); float lowLuminosityDamp = smoothstep(0.0, 1.0, 25.0 * i); float hWeight = lowSaturationDamp * intensityDamp * lowLuminosityDamp; hueGamma -= 1; hueGamma *= hWeight; hueGamma += 1; bw = pow(bw, hueGamma); float bwSDR = clamp(bw * 12.0, 0.0, 1.0); float midLumWeight = bwSDR*(1.0 - bwSDR); float grayWeight = 1.0 - smoothstep(0.0, 1.0, chroma * 10.0); float nWeight = midLumWeight * grayWeight; neutralGamma -= 1; neutralGamma *= nWeight; neutralGamma *= -2; neutralGamma += 1; bw = pow(bw, neutralGamma); bw = bw * 12.0; bw = clamp(bw, 0.0, 12.0); float df0 = 0.812379; float result; if (bw < df0) { result = 1.8031*bw*bw*bw - 2.1972*bw*bw + 1.3823*bw; } else { float scale = 12.0 - df0; float x = (bw - df0) / scale; result = 1.8031*x*x*x - 2.1972*x*x + 1.3823*x; result = result * scale + df0; result -= 0.158305860; } bw = mix(bw, result,-phototone); return vec4(bw,bw,bw,imageHDR.a); }
PIPhotoGrainHDR
inputISO
There is no need to call smartBlackAndWhiteStatistics.
Just use [CIFilter filterWithName:@"PISmartBlackAndWhiteHDR"] instead.
There is no need to call smartBlackAndWhiteAdjustmentsForValue:andStatistics:.
PICinematicVideoUtilities.m
Can't find global cinematic metadata in asset
Unexpected global rendering metadata class
-[_PIParallaxRenderCacheEntry initWithImage:format:colorSpace:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Parallax/PIParallaxLayerStackRequest.m
format != nil
space != nil
Invalid image extent
renderer != nil
-[_PIParallaxRenderCacheEntry render:error:]
layout != nil
-[_PIParallaxLayerStackScalePolicy initWithLayout:]
-[_PIParallaxLayerStackJob initWithRequest:]
-[_PIParallaxLayerStackJob effectiveLayout]
-[_PIParallaxLayerStackJob backfillScalePolicy]
-[_PIParallaxLayerStackJob prepare:]
Missing output image
MatteImage
Failed to render background layer
Failed to render foreground layer
buffer != nil
-[_PIParallaxLayerStackJob layerForBuffer:image:zPosition:identifier:]
identifier != nil
Failed to create pixel buffer
Failed to create renderDestination
-[_PIParallaxLayerStackJob cacheImage:key:format:colorSpace:]
key != nil
-[PIParallaxLayerStackRequest initWithSegmentationItem:]
-[PIParallaxLayerStackRequest initWithComposition:]
debugInput
debugMatte
debugMatteCrop
debugLocalConfidence
debugConfidenceMap
debugInfill
debugLayout
debugPreview
debugColorAnalysis
debugLayoutIntermediate_%d
-[PILevelsAutoCalculator submit:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Autocalculators/PILevelsAutoCalculator.m
PILevelsAutoCalculator-histogram
pre-Levels
-[PILevelsAutoCalculator calculateSettingsForImageHistogram:]
blackSrc
blackDst
shadowSrc
shadowDst
midSrc
midDst
hilightSrc
hilightDst
whiteSrc
whiteDst
Adjustment controller for key %@ is of class: %@, but was expected to be %@
-[PICompositionController(AdjustmentExtensions) _adjustmentControllerForKey:creatingIfNecessary:expectedClass:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Controllers/PICompositionController+AdjustmentExtensions.m
<%@:%p> [(%.3f, %.3f), %s]
editable
not editable
Warning smartToneStatistics will soon need [receiver properties] be non-nil so flash-fired state can be determined.
tonalRange
highKey
blackPoint
whitePoint
kernel vec4 _smarttone_brightness_neg_HDR (__sample c, float gamma) 
 vec3 neg = min(c.rgb, 0.0); 
 c.rgb = max(c.rgb, 0.0); 
 vec3 pix = pow(c.rgb, vec3(gamma)); 
 float lum = dot(c.rgb, vec3(0.39, .5, .11)); 
 vec3 pix2 = lum>0.0 ? c.rgb*pow(lum, gamma)/lum : vec3(0.0); 
 pix = mix(pix, pix2, 0.8) + neg; 
 return vec4(pix, c.a); 
kernel vec4 _smarttone_brightness_pos_HDR (__sample c, float gamma) 
 vec3 neg = min(c.rgb, 0.0); 
 vec3 pos = max(c.rgb, 1.0)-1.0; 
 c.rgb = clamp(c.rgb, 0.0, 1.0); 
 vec3 m = 1.0-c.rgb; 
 float a = 0.6; 
 vec4 result = c; 
 result.rgb = 1.0 - (pow(m, vec3(gamma))+a*( ((gamma-1.0)*m*(1.0-m*m))/(gamma*gamma))); 
 c.rgb = pow(c.rgb, vec3(1.0-((min(gamma, 2.95)-1.0)/2.6))); 
 result.rgb = mix(c.rgb, result.rgb, .85); 
 result.rgb = result.rgb+neg+pos; 
 return result; 
kernel vec4 _smarttone_contrast_HDR (__sample im, float midAmt) 
 vec3 neg = min(im.rgb, 0.0); 
 vec3 pos = max(im.rgb, 1.0)-1.0; 
 im.rgb = clamp(im.rgb, 0.0, 1.0); 
 float y = dot(im.rgb, vec3(0.3333)); 
 y = sqrt(y); 
 float sat = (im.r-y)*(im.r-y)+(im.g-y)*(im.g-y)+(im.b-y)*(im.b-y); 
 y = y*(1.0-y); 
 im.rgb = sqrt(im.rgb); 
 float a = midAmt*y; 
 float b = -0.5*a; 
 vec3 pix = im.r * vec3(0.299*a) + 
 im.g * vec3(0.587*a) + 
 im.b * vec3(0.114*a) + 
 im.rgb + vec3(b); 
 im.rgb = mix(im.rgb, vec3(0.5), -y*midAmt); 
 im.rgb = mix(im.rgb, pix, 0.8+sat); 
 im.rgb = max(im.rgb, 0.0); 
 im.rgb *= im.rgb; 
 im.rgb = im.rgb + neg + pos; 
 return im; 
kernel vec4 _smarttone_contrast_HDR (__sample im, float midAmt, float maxVal) 
 midAmt = midAmt / maxVal; 
 vec3 neg = min(im.rgb, 0.0); 
 vec3 pos = max(im.rgb, maxVal) - maxVal; 
 im.rgb = clamp(im.rgb, 0.0, maxVal); 
 float y = dot(im.rgb, vec3(0.3333)); 
 y = sqrt(y); 
 im.rgb = sqrt(im.rgb); 
 float sat = dot(im.rgb - vec3(y), im.rgb - vec3(y)) / sqrt(maxVal); 
 y = y*(sqrt(maxVal)-y); 
 float a = midAmt*y; 
 float b = -0.5*a; 
 vec3 pix = im.r * vec3(0.299*a) + 
 im.g * vec3(0.587*a) + 
 im.b * vec3(0.114*a) + 
 im.rgb + vec3(b); 
 im.rgb = mix(im.rgb, vec3(0.5), -a); 
 im.rgb = mix(im.rgb, pix, clamp(0.8+sat, 0.0, 1.0)); 
 im.rgb = max(im.rgb, 0.0); 
 im.rgb *= im.rgb; 
 im.rgb = im.rgb + neg + pos; 
 return im; 
kernel vec4 _smarttone_highlightcontrast_HDR (__sample pix, float highAmt, float sat) 
 float maxVal = 1.0; vec3 neg = min(pix.rgb, 0.0); 
 vec3 pos = max(pix.rgb, maxVal) - maxVal; 
 pix.rgb = clamp(pix.rgb, 0.0, maxVal); 
 float lum = clamp(dot(pix.rgb, vec3(.3333)),0.0,1.0); 
 vec3 high = pow(pix.rgb, vec3(3.0 - 2.0*highAmt)); 
 float pivot = 0.8; 
 vec3 pix1 = (high - pivot)*(4.0 - 3.0*highAmt) + pivot; 
 float h = highAmt*highAmt*highAmt*highAmt; 
 float a = (4.0 - 3.0*h); 
 vec3 pix2 = (lum-pivot)*a+pivot + high.rgb -lum; 
 high = mix(pix2, pix1, sat); 
 pix.rgb = mix(pix.rgb, high, lum*lum); 
 pix.rgb = pix.rgb + neg + pos; 
 return pix; 
kernel vec4 _rawHighlightsHDR(__sample pix, float gain) 
 vec3 high = gain*pix.rgb; 
 float lum = clamp(dot(pix.rgb, vec3(.3333)), 0.0, 1.0); 
 vec3 neg = min(high, 0.0); 
 high.rgb = mix(max(pix.rgb, 0.0), high.rgb, lum*lum) + neg; 
 return vec4(high, pix.a); 
CIHighlightShadowAdjust
inputShadowAmount
inputHighlightAmount
PISmartToneFilterHDR-histogram
componentAdd
componentMultiply
componentMin
componentMax
clear
destination
sourceOver
destinationOver
sourceIn
destinationIn
sourceOut
destinationOut
sourceAtop
destinationAtop
exclusiveOr
multiply
screen
overlay
darken
lighten
colorDodge
colorBurn
hardLight
softLight
difference
exclusion
saturation
luminosity
subtract
divide
linearBurn
linearDodge
vividLight
linearLight
pinLight
hardMix
darkerColor
lighterColor
*** Couldn't find blend kernel for blend mode '%@'
inputBlendMode
PI_LONG_EXPOSURE_FUSION_PARAMS
@"NSString"8@?0
kNCCBlurHalfSize
kNCCEdge0
kNCCEdge1
kBlendMaskThreshold0
kBlendMaskThreshold1
PI_LONG_EXPOSURE_DUMP_INTERMEDIATES
long-exp-input-image.tiff
long-exp-mask-image.tiff
long-exp-still-image.tiff
long-exp-guide-image.tiff
long-exp-ncc-map-image.tiff
long-exp-refined-mask-image.tiff
long-exp-fusion-image.tiff
-[PIModernPhotosPipeline _processedRenderNodeForComposition:input:pipelineState:error:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Pipeline/PIModernPhotosPipeline.m
pipelineState != nil
LongExposure
inputSushiLevel
kCGImageSourceShouldExtendRaw
inputGamutMapMax
inputNoiseReductionDetailAmount
inputUIColorNoiseReductionAmount
inputUILuminanceNoiseReductionAmount
sharpness
inputNoiseReductionSharpnessAmount
contrast
inputNoiseReductionContrastAmount
inputNeutralTemperature
inputNeutralTint
Video
mediaComponentType
hardCropCleanAperture
defaultFrameTime
skipOrientation
Master
unsupported sourceSelect adjustment
RAW/Linear
Source
ShowOriginalSource
PIForwardFakeBoost
inputBoost
inputVersion
inputParams
Intermediate
keepCacheWhenAtOneToOne
Disparity
Image
Source does not contain depth
PortraitEffectsMatte
HairSegmentationMatte
SkinSegmentationMatte
TeethSegmentationMatte
GlassesSegmentationMatte
MeteorPlusGainMap
focusRect
inputAperture
inputFocusRect
leftEyeX
leftEyeY
rightEyeX
rightEyeY
noseX
noseY
chinX
chinY
inputLeftEyePosition
inputRightEyePosition
inputFaceMidPoint
inputChinPosition
Missing required depth settings
inputShiftmapImage
inputCalibrationData
inputAuxDataMetadata
inputOriginalSize
CIDepthEffectMakeBlurMap
inputMatteImage
inputHairImage
inputGlassesImage
inputPower
inputEV
faceLandmarks
inputFaceLandmarkArray
inputDisparity
inputMatte
inputBlurMap
inputFaceMask
inputHairMask
inputTeethMask
CIPortraitEffectV2
inputGenerateSpillMatte
inputFullSizeImage
CIPortraitEffect%@
PortraitV2-zeroStrength
PortraitV2
pre-PortraitVideo
auxiliaryImageType
nativeScale
post-PortraitVideo
inputLumaNoiseScale
lumaNoiseScale
shape
inputShape
CIDepthEffectApplyBlurMap
inputGainMap
masterSpace
pre-WB
inputIsRaw
warmth
pre-Mute
pre-LivePhotoKeyFrame
pre-Trim
pre-SloMo
SloMo
inputConfidence
inputFaceBoxArray
CIDynamicFood
inputBoundingBoxArray
sunsetOrSunrise
CIDynamicRender
Unknown sceneLabel when rendering semantic enhance adjustment
inputTargetImage
inputTime
inputSaturation
pre-Curves
inputPointsR
inputPointsG
inputPointsB
inputPointsL
inputTableImage
green
blue
spread
hueShift
inputCorrections
CIPhotoEffect%@
filterVersion
pre-VideoReframe
pre-VideoStabilize
pre-VideoCrossfadeLoop
pre-Geometry
pre-Crop
perspectiveStraighten
resetCleanAperture
{?={?=qq}{?=qq}}
projectUsingCleanAperture
{?=qq}
projectUsingOriginalSize
com.apple.quicktime.live-photo.vitality-disabled
projectUsingEstimatedCleanAperture
pre-Orientation
post-Geometry
inputCenter
post-Adjustments
PIInverseFakeBoost
Master/RAW/Linear
inputCutoff
kernel vec4 levelsNewGammaForP3 (sampler src, sampler LUT)
vec4
p,r;
vec2
c1,c2;
float
p  = sample(src, samplerCoord(src));
vec3 neg = min(p.rgb, 0.0);
vec3 pos = max(p.rgb, 1.0)-1.0;
p.rgb = clamp(p.rgb, 0.0, 1.0);
f = p.r * 255.0 + 256.0;
c1 = vec2(floor(f)+0.5, 0.5);
c2 = vec2(ceil(f)+0.5, 0.5);
r = mix(sample(LUT,samplerTransform(LUT,c1)), sample(LUT,samplerTransform(LUT,c2)), fract(f));
p.r = r.r * 4.0 - 1.0;
f = p.g * 255.0 + 256.0;
c1 = vec2(floor(f)+0.5, 0.5);
c2 = vec2(ceil(f)+0.5, 0.5);
r = mix(sample(LUT,samplerTransform(LUT,c1)), sample(LUT,samplerTransform(LUT,c2)), fract(f));
p.g = r.g * 4.0 - 1.0;
f = p.b * 255.0 + 256.0;
c1 = vec2(floor(f)+0.5, 0.5);
c2 = vec2(ceil(f)+0.5, 0.5);
r = mix(sample(LUT,samplerTransform(LUT,c1)), sample(LUT,samplerTransform(LUT,c2)), fract(f));
p.b = r.b * 4.0 - 1.0;
p.rgb = max(p.rgb, 0.0);
p.rgb = p.rgb + pos + neg;
return p;
-[PILevelsFilter _LUTImage]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Adjustments/PILevelsFilter.m
com.apple.photos.PhotoImaging
segmentation
Unbalanced call to ensureResources detected! (%ld)
+[PISegmentationLoader ensureResources]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Parallax/PISegmentationLoader.m
Unbalanced call to freeResources detected! (%ld)
+[PISegmentationLoader freeResources]
-[PISegmentationLoader initWithParallaxAsset:]
PISegmentationItemLoader.state
PISegmentationItemLoader.loading
item != nil
-[PISegmentationLoader initWithSegmentationItem:parallaxAsset:]
v16@?0@"PIParallaxSegmentationItem"8
v24@?0@"PFParallaxAssetResource"8@"NSError"16
B24@?0@"NSString"8@"NSString"16
Missing composition
-[PISegmentationLoader _segment:completion:]
B32@?0@"<NUImageBuffer>"8@"<NUImageBuffer>"16Q24
v24@?0@"<NUImageBuffer>"8@"<NUImageBuffer>"16
composition != nil
-[PISegmentationLoader _performSegmentation:type:completion:]
-[PISegmentationLoader _analyzeColors:completion:]
Missing original layout
-[PISegmentationLoader _loadBackground:completion:]
-[PISegmentationLoader _loadRegions:completion:]
v32@?0@"NSArray"8@"NSArray"16@"NSError"24
-[PISegmentationLoader _performLayout:completion:]
item.composition != nil
-[PISegmentationLoader _loadLocalLightData:completion:]
resource != nil
+[PISegmentationLoader segmentationCompositionForAssetResource:]
proxyImage != NULL
+[PISegmentationLoader segmentationCompositionForProxyImage:orientation:]
imageURL != nil
+[PISegmentationLoader segmentationCompositionForImageURL:fileUTI:orientation:proxyImage:]
v16@?0@"PIOrientationAdjustmentController"8
+[PISegmentationLoader segmentationSourceForImageURL:fileUTI:orientation:proxyImage:]
currentInfo != nil
-[PISegmentationLoader _tryLoadSegmentationItemFromCache:]
B16@?0@"NSURL"8
+[PISegmentationLoader saveSegmentationItem:toURL:error:]
Invalid segmentation item: %@
+[PISegmentationLoader loadSegmentationItemFromURL:error:]
Segmentation item is incomplete
+[PISegmentationLoader saveSegmentationItem:layerStackOptions:configuration:style:layout:toWallpaperURL:completion:]
v24@?0@"PFParallaxLayerStack"8@"NSError"16
+[PISegmentationLoader generateLayerStackForItem:style:layout:options:completion:]
+[PISegmentationLoader saveSegmentationItem:layerStack:toWallpaperURL:error:]
Failed to create wallpaper directory
input.segmentation
Failed to export segmentation item
output.layerStack
Failed to export layer stack
+[PISegmentationLoader loadSegmentationItemFromWallpaperURL:error:]
Segmentation item from wallpaper is incomplete
Failed to load segmentation item from wallpaper
+[PISegmentationLoader loadLayerStackFromWallpaperURL:options:error:]
Failed to load layer stack from wallpaper
+[PISegmentationLoader renderPreviewLayerStackFromWallpaperURL:styleCategory:completion:]
+[PISegmentationLoader reloadSegmentationItemFromWallpaperURL:asset:completion:]
Null
Not available
<%@:%p %@>
-[_PIParallaxLayoutInactiveFrameJob initWithRequest:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Parallax/PIParallaxLayoutInactiveFrameRequest.m
-[_PIParallaxLayoutInactiveFrameJob prepare:]
-[_PIParallaxLayoutInactiveFrameJob render:]
-[_PIParallaxLayoutInactiveFrameJob complete:]
-[PIParallaxLayoutInactiveFrameRequest initWithComposition:]
segmentationItem != nil
-[PIParallaxLayoutInactiveFrameRequest initWithSegmentationItem:]
Mirror
Adjustment
SourceSelect~1.0
enum
values
Failed to register schema %@: %@
+[PISchema sourceSelectSchema]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Pipeline/PISchema.m
RAW~1.0
opaque
required
+[PISchema rawSchema]
RawNoiseReduction~1.0
bool
minimum
maximum
ui_minimum
ui_maximum
+[PISchema rawNoiseReductionSchema]
SmartTone~1.0
+[PISchema smartToneSchema]
SmartColor~1.0
+[PISchema smartColorSchema]
SmartBlackAndWhite~1.0
+[PISchema smartBlackAndWhiteSchema]
Grain~1.0
+[PISchema grainSchema]
Sharpen~1.0
+[PISchema sharpenSchema]
CropStraighten~1.0
+[PISchema cropSchema]
Trim~1.0
+[PISchema trimSchema]
SlowMotion~1.0
+[PISchema slomoSchema]
LivePhotoKeyFrame~1.0
+[PISchema livePhotoKeyFrameSchema]
Mute~1.0
+[PISchema muteSchema]
VideoPosterFrame~1.0
+[PISchema videoPosterFrameSchema]
AutoLoop~1.0
+[PISchema autoLoopSchema]
HighResolutionFusion~1.0
+[PISchema highResFusionSchema]
DepthEffect~1.0
+[PISchema depthEffectSchema]
Effect3D~1.0
+[PISchema effect3DSchema]
PortraitEffect~1.0
+[PISchema portraitEffectSchema]
+[PISchema effectSchema]
iPhone
+[PISchema redEyeSchema]
array
content
compound
properties
+[PISchema apertureRedEyeSchema]
+[PISchema retouchSchema]
+[PISchema vignetteSchema]
Orientation~1.0
+[PISchema orientationSchema]
+[PISchema definitionSchema]
+[PISchema noiseReductionSchema]
grayStrength
identity
+[PISchema whiteBalanceSchema]
+[PISchema levelsSchema]
+[PISchema curvesSchema]
+[PISchema selectiveColorSchema]
VideoReframe~1.0
+[PISchema videoReframeSchema]
VideoStabilize~1.0
pixel
gyro
+[PISchema videoStabilizeSchema]
VideoCrossfadeLoop~1.0
+[PISchema videoCrossfadeLoopSchema]
Debug
+[PISchema debugSchema]
SemanticEnhance~1.0
+[PISchema semanticEnhance]
PortraitVideo~1.0
+[PISchema portraitVideoSchema]
Composition
PhotosComposition
contents
com.apple.photo:Source~1.0
reference
Retouch~1.0
WhiteBalance~1.0
RedEye~1.0
ApertureRedEye~1.0
Definition~1.0
Effect~1.0
Vignette~1.0
NoiseReduction~1.0
Levels~1.0
Curves~1.0
SelectiveColor~1.0
Debug~1.0
+[PISchema photosCompositionSchema]
failed to register %@: %@
+[PISchema registerPhotosSchema]
com.apple.mobileslideshow.reframe.photo.eligible-pass
com.apple.mobileslideshow.reframe.photo.eligible-fail
com.apple.mobileslideshow.reframe.video.eligible-pass
com.apple.mobileslideshow.reframe.video.eligible-fail
com.apple.mobileslideshow.reframe.type.photo.horizon
com.apple.mobileslideshow.reframe.type.photo.perspective
PICompositionFinalizer-primaryImageProperties
PICompositionFinalizer-overcaptureImageProperties
v16@?0@"PIAdjustmentController"8
maxCameraAutoStraightenCorrection
Capture
maxCameraVerticalPerspectiveCorrection
maxCameraHorizontalPerspectiveCorrection
maxCameraRotatePerspectiveCorrection
minHorizontalCameraPerspectiveCorrection
minVerticalCameraPerspectiveCorrection
minRotateCameraPerspectiveCorrection
enable_perspective_correction
reframe
perpsective
horizon
PICropAutoCalculator-imageProperties
-[PICropAutoCalculator undoExifOrientation:error:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Autocalculators/PICropAutoCalculator.m
Source geometry has 0 size
-[PICropAutoCalculator submit:]
PICropAutoCalculator-faceDetection
{CGRect={CGPoint=dd}{CGSize=dd}}
maxAutoStraighten
filterOptions
CIAutoStraighten
straightenAngleInDegreesCCW
limitExceeded
belowMinimum
autoCrop
result
error
%@AutoCropEvaluation-txt.DBG
v16@?0@"PISourceSelectAdjustmentController"8
v16@?0@"PIAutoLoopAdjustmentController"8
PICropAutoCalculator-stitchedOvercaptureRect
+[PICropAutoCalculator(Utilities) updateCropAdjustment:after:error:]
PICropAutoCalculator-getBeforeGeometry
PICropAutoCalculator-getAfterGeometry
v16@?0@"PICropAdjustmentController"8
kernel vec4 iptLumHueSatTable(sampler image, __table sampler hueSatLumTable)
vec4 im = sample(image, samplerCoord(image)) ;
vec4 result = im;
float hueIdx = 359.0 * 0.5 * (atan(im.b, im.g)/3.1416 + 1.0);
hueIdx = clamp(hueIdx, 0.0, 359.0) + 0.5;
float hueChange = (sample(hueSatLumTable, samplerTransform(hueSatLumTable, vec2(hueIdx,0.5))).r);
float satChange = (sample(hueSatLumTable, samplerTransform(hueSatLumTable, vec2(hueIdx,0.5))).g);
float lumChange = (sample(hueSatLumTable, samplerTransform(hueSatLumTable, vec2(hueIdx,0.5))).b);
float chroma = sqrt(im.g*im.g+im.b*im.b) ;
chroma *= satChange ;
float hue = atan(im.b, im.g) + hueChange ;
vec3 adjustIm = im.rgb;
float hueAngle = hue  ;
lumChange = mix(1.0, lumChange, clamp(chroma,-0.7,0.7));
adjustIm.r *= lumChange;
adjustIm.g = chroma * cos(hueAngle) ;
adjustIm.b = chroma * sin(hueAngle) ;
result.rgb = adjustIm.rgb;
result.a = im.a ;
return result ;
kernel vec4 srgbToIPT(sampler image){
vec4 im = sample(image, samplerCoord(image));
vec3 lms, ipt;
lms = im.r * vec3(0.3139902162, 0.155372406, 0.017752387) +
im.g * vec3(0.6395129383, 0.7578944616, 0.109442094) +
im.b * vec3(0.0464975462, 0.0867014186, 0.8725692246);
lms = sign(lms)*pow(abs(lms), vec3(0.43, 0.43, 0.43));
ipt = lms.r * vec3(0.4, 4.455, 0.8056) +
lms.g * vec3(0.4, -4.851, 0.3572) +
lms.b * vec3(0.2, 0.3960, -1.1628);
return vec4(ipt, im.a);
kernel vec4 iptToSRGB(sampler image){
vec4 im = sample(image, samplerCoord(image));
vec3 lms, rgb;
lms = im.rrr +
im.g * vec3(0.09756893,-0.11387649,0.03261511) +
im.b * vec3(0.20522644, 0.13321716,  -0.67688718);
lms = sign(lms)*pow(abs(lms), vec3(1.0/.43));
rgb = lms.r * vec3( 5.472212058380287,  -1.125241895533569,   0.029801651173470) +
lms.g * vec3(-4.641960098354470, 2.293170938060623, -0.193180728257140) +
lms.b * vec3(0.169637076827974,  -0.167895202223709, 1.163647892783812);
return vec4(rgb, im.a);
kernel vec4 add_gaussian(sampler srcTable, float tableSize, float hueAmplitude, float satAmplitude, float lumAmplitude, float gaussX, float gaussSigmaSquared) {
vec2 d = destCoord();
vec4 src = sample(srcTable, samplerCoord(srcTable));
float x = d.x / (tableSize - 1.0);
float dist = min(min(abs(x - gaussX), abs(x - 1.0 - gaussX)), abs(x + 1.0 - gaussX));
float p = -((dist * dist) / (2.0 * gaussSigmaSquared));
float ep = exp(p);
float hue = hueAmplitude * ep;
float sat = satAmplitude * ep;
float lum = lumAmplitude * ep;
float h = clamp(src.r + hue, -1.0, 1.0);
float s = clamp(src.g + sat, -1.0, 1.0);
float l = clamp(src.b + lum, -1.0, 1.0);
return vec4(h,s,l,1.0);
srgbToIPT
iptToSRGB
add_gaussian
iptLumHueSatTable
PIVideoReframeNode.m
invalid crop rect
pipelineState
-[PIVideoReframeNode initWithSettings:inputs:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Render/PIVideoReframeNode.m
-[PIVideoReframeNode _evaluateVideoProperties:]
error != nil
-[PIVideoReframeNode _evaluateImageGeometry:]
Failed to get input geometry
Could not get the input geometry
Could not get the input image properties
CIPerspectiveTransform
inputTopLeft
inputTopRight
inputBottomLeft
inputBottomRight
showStabilizationWatermark
init is not a valid initializer
Invalid plist at path: %@
Time
Subjects
Type
HumanBody
HumanFace
CatBody
DogBody
-[PIVideoReframeMetadataExtractor overwriteTrackingMetadataWithPlist:]
PIVideoReframeMetadataExtractor.mm
type >= 0
Confidence
-[PIVideoReframeMetadataExtractor motionBlurVectorFromMetadata:]
-[PIVideoReframeMetadataExtractor extractMetadata]
mdataGroup.items.count == 1
0 && "unexpected metadata identifier"
0 && "unexpected metadata data type"
err == noErr
kernel vec4 convertFromRGBToYIQ(sampler src, float gamma)
vec4 pix ;
vec3 pix2;
pix = sample(src, samplerCoord(src));
pix.rgb = sign(pix.rgb)*pow(abs(pix.rgb), vec3(1.0/gamma)) ;
pix2.rgb = pix.r * vec3(0.299,  0.595716,  0.211456) +
pix.g * vec3(0.587, -0.274453, -0.522591) +
pix.b * vec3(0.114, -0.321263,  0.311135);
return vec4(pix2, pix.a);
kernel vec4 convertFromYIQToRGB(sampler src, float gamma)
vec4 color, pix;
pix = sample(src, samplerCoord(src));
color.rgb = pix.rrr +
pix.g * vec3(0.956296, -0.272122, -1.10699) +
pix.b * vec3(0.621024, -0.647381, 1.70461);
color.rgb = sign(color.rgb)*pow(abs(color.rgb), vec3(gamma, gamma, gamma) );
color.a = pix.a;
return color;
kernel vec4 whiteBalance(sampler image, float grayY, float grayI, float grayQ, float strength)
vec4 im = sample(image, samplerCoord(image)) ;
vec2 grayOffset = vec2(grayI, grayQ) ;
vec4 result ;
float newStrength = 1.0 + (strength-1.0)*(1.0-im.r) ;
result.r = im.r ;
result.gb = im.gb + newStrength*grayOffset ;
float damp = max(min(1.0, im.r/(grayY+0.00001)),0.0) ;
result.rgb = mix(im.rgb, result.rgb, damp) ;
result.a = im.a ;
return result ;
kernel vec4 gHDRtoPP(sampler image)
vec4 pix ;
vec3 pix2;
pix = sample(image, samplerCoord(image));
pix2 = pix.r * vec3(0.615429622407401,   0.114831839141528,   0.011544126697221) +
pix.g * vec3(0.367479646665836,   0.797943554457996,   0.064077744191180) +
pix.b * vec3(  0.016956659608091,   0.087783443422360,   0.924405601458102);
return vec4(pix2, pix.a);
kernel vec4 PPtogHDR(sampler image)
vec4 pix ;
vec3 pix2;
pix = sample(image, samplerCoord(image));
pix2 = pix.r * vec3(1.777445503202045,  -0.255296595099306,  -0.004500433755654) +
pix.g * vec3( -0.822224875430495,   1.380948853784730,  -0.085456231694984) +
pix.b * vec3(0.045475917061484,  -0.126454737973025,   1.089973874037625);
return vec4(pix2, pix.a);
convertFromRGBToYIQ
convertFromYIQToRGB
-[PINeutralGrayWhiteBalanceFilter outputImage]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Adjustments/PINeutralGrayWhiteBalanceFilter.m
PISmartToneAutoCalculator
-[PISmartToneAutoCalculator submit:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Autocalculators/PIAutoCalculators.mm
-[PISmartColorAutoCalculator submit:]
-[PIRedEyeAutoCalculator submit:]
PIRedEyeAutoCalculator-getLensInfo
locationX
locationY
touchDiameter
inputRect
MPSImageReduce unavailable
/masterSpace
kernel vec4 _blendGrainsHDR(__sample isoImages, float log10iso)
  vec4 c = isoImages; 
  float mix10_50    = mix(c.r, c.g, log10iso*1.43067655809 
                                           - 1.43067655809); 
  float mix50_400   = mix(c.g, c.b, log10iso*1.10730936496 
                                           - 1.88128539659); 
  float mix400_3200 = mix(c.b, c.a, log10iso*1.10730936496 
                                           - 2.88128539659); 
  float v = compare(log10iso - 1.69897000434,                     mix10_50,                     compare(log10iso - 2.60205999133,                             mix50_400,                             mix400_3200)); 
  return vec4(v,v,v,1.0);
kernel vec4 _grainBlendAndMixHDR(__sample img, __sample grainImage, float contrast, float mixAmount)
  vec3 rgb = img.rgb;
  float luminance = clamp(dot(rgb, vec3(.333333)), 0.0, 1.0); 
  float gamma = 4.01 - 2.0*luminance;
  rgb = sign(rgb) * pow(abs(rgb), vec3(1.0/gamma));
  float grain = grainImage.r - 0.5;
  float mult = contrast * grain;
  rgb += (max(luminance, 0.5) * mult * (1.0-luminance));
  rgb = sign(rgb) * pow(abs(rgb), vec3(gamma));
  rgb = min(rgb, 12.0);
  return mix(img, vec4(rgb,img.a), mixAmount);
kernel vec2 _paddedTile2(vec4 k) { return fract(destCoord() * k.zw) * k.xy + vec2(1.0); }
{CGRect={CGPoint=dd}{CGSize=dd}}44@?0i8{CGRect={CGPoint=dd}{CGSize=dd}}12
1536 x 1536 noise grain image prov
v56@?0^v8Q16Q24Q32Q40Q48
generateNoiseImage_block_invoke
PIPhotoGrainHDR.m
width==512*3
height==512*3
x==0
y==0
kernel vec4 _grainGenCombineHDR (__sample r, __sample g, __sample b, __sample a)
{ return vec4(r.x, g.x, b.x, a.x); }
CIUnsharpMask
Classification
LayoutConfiguration
LowResolution
DisableDownload
DisableSegmentation
DisableRendering
PetsRegions
Priority
PetsFaceRegions
Style
LayerStackOptions
OutOfProcess
WallpaperUpgradeMode
asset != nil
+[PISegmentation computeSegmentationScoresForAsset:options:completion:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Parallax/PISegmentation.m
options != nil
v24@?0@"<PISegmentationItem>"8@"NSError"16
+[PISegmentation loadSegmentationItemForAsset:options:completion:]
Invalid classification option: %@
+[PISegmentation cancelSegmentationForAsset:]
NO (timeout)
+[PISegmentation exportWallpaperForAsset:toURL:options:completion:]
wallpaperURL != nil
Segmentation failure
Invalid additionalLayerOptions: %@
+[PISegmentation _layerStackOptionsFromOptions:]
sourceURL != nil
+[PISegmentation upgradeWallpaperAtURL:exportToURL:options:completion:]
destinationURL != nil
v24@?0@"PFPosterEditConfiguration"8@"NSError"16
Not implemented yet
v16@?0@"NSError"8
Failed to load poster configuration from source URL
Failed to upgrade poster configuration from source URL
v24@?0@"PFPosterConfiguration"8@"NSError"16
PISegmentation.upgrade
Failed to upgrade some poster media
Failed to save poster configuration
v8@?0
+[PIImageIO writeImage:fileURL:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Util/PIImageIO.m
cgImage != NULL
+[PIImageIO writeCGImage:fileURL:]
fileURL != nil
Unhandled bit depth: %ld
+[PIImageIO writeCGImage:fileURL:options:]
Successfully wrote image to file %@
Failed to write image to file %@
%@-%d-%ld.tiff
GU %@ %@
Sensitivity
Bad float to fixed 16 conversion
+[PIApertureRedEyeProcessorKernel convertFloat:toFixed16:count:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Adjustments/ApertureRedEye/PIApertureRedEyeFilter.mm
Bad fixed 16 to float conversion
+[PIApertureRedEyeProcessorKernel convertFixed16:toFloat:count:]
+[PIApertureRedEyeProcessorKernel processWithInputs:arguments:output:error:]
PIApertureRedEyeFilter.mm
output.format == kCIFormatRGBAf
inputSensitivity
portraitStrength
capturedPortraitMajorVersion
capturedPortraitMinorVersion
portraitDisableStage
+[PIValuesAtCapture valuesAtCaptureFromImageProperties:error:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Autocalculators/PIPortraitAutoCalculator.m
Portrait was previously applied.
Failed to load depth data
Unfiltered depth data is not supported
Low quality depth data is not supported
Missing camera calibration data
Failed to load auxiliary data
Missing auxiliary metadata
Depth data version mismatch, asset has %@ but we can only handle %@
We only know up to sDOF version %d. Found: %d
<%@:%p aperture:%f minimumAperture:%@ maximumAperture:%@ portraitStrength:%f SDOFRenderingVersion:%lu portraitMajorVersion:%lu portraitMinorVersion:%lu>
-[PIPortraitAutoCalculator submit:]
PIPortraitAutoCalculator-getValuesAtCapture-imageProperties
capturedAperture
capturedPortraitStrength
v16@?0@"NUResponse"8
PIPortraitAutoCalculator-faceDetect
faceObservations != nil
+[PIPortraitAutoCalculator portraitInfoDictionaryFromFaceObservations:metadata:orientation:valuesAtCapture:]
metadata != nil
NUOrientationIsValid(orientation)
Insufficient number of landmark points
+[PIPortraitAutoCalculator depthEffectInfoDictionaryFromFaceObservations:focus:valuesAtCapture:lumaNoiseScale:orientation:]
minimumAperture
maximumAperture
+[PIPortraitAutoCalculator portraitEffectInfoDictionaryFromFaceObservations:orientation:valuesAtCapture:]
faceBoundingBox
faceJunkinessIndex
faceOrientationIndex
roll
allPoints
faceContour
leftEye
rightEye
leftEyebrow
rightEyebrow
nose
noseCrest
medianLine
outerLips
innerLips
leftPupil
rightPupil
+[PIPortraitAutoCalculator portraitInfoDictionaryFromCameraMetadata:]
-[PIDepthEffectApertureAutoCalculator submit:]
PIDepthEffectApertureAutoCalculator-getValuesAtCapture-imageProperties
depthData:DepthDataVersion
var tagNode = GetTag('/pre-Adjustments');
orientation = tagNode.geometry.orientation;
var node = Orient(tagNode, orientation);
return node;
@"NURenderNode"40@?0@"NURenderPipelineHelper"8@"NUComposition"16@"NURenderNode"24^@32
/pre-Adjustments
var source = GetTag('/pre-Adjustments');
ResetTagInput('/pre-Crop', source);
source = GetTag('/Crop');
orientation = source.geometry.orientation;
source = Orient(source, orientation);
return source;
/pre-Crop
/Crop
var sourceSettings = { 'skipOrientation': true };
var rawAdjustment = composition.raw
if (rawAdjustment) {
    sourceSettings['inputDecoderVersion'] = rawAdjustment.inputDecoderVersion
var image = Source(composition.source, sourceSettings)
var rawImage = GetTagInput('RAW/Linear')
if (rawImage) {
    image = rawImage
    image = Filter('PIForwardFakeBoost', {'inputImage' : image, 'inputBoost' : 1.0,})
var orientation = composition.orientation
if (orientation) { image = Orient(image, orientation.value) }
return image
Raw/Linear
var sourceSettings = { }; 
var raw = composition.raw; 
if (raw) { 
  sourceSettings['inputDecoderVersion'] = raw.inputDecoderVersion; 
  var sushiLevel = raw.inputSushiLevel;
  if (sushiLevel) {
    sourceSettings['kCGImageSourceShouldExtendRaw'] = sushiLevel;
} else { throw "expected RAW in rawSourceFilterIncludingOrientation"; }
return Source(composition.source, sourceSettings); 
expected RAW in rawSourceFilterIncludingOrientation
+[PIPipelineFilters rawSourceFilterIncludingOrientation]_block_invoke
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Pipeline/PIPipelineFilters.m
var sourceSettings = { 'skipOrientation' : true }; 
var raw = composition.raw; 
if (raw) { 
  sourceSettings['inputDecoderVersion'] = raw.inputDecoderVersion; 
  var sushiLevel = raw.inputSushiLevel;
  if (sushiLevel) {
    sourceSettings['kCGImageSourceShouldExtendRaw'] = sushiLevel;
  return Source(composition.source, sourceSettings); 
} else {
  return GetTag("/ShowOriginalSource");
var source = GetTag('/pre-RedEye');ResetTagInput('/post-RedEye', source);
return GetTag('/post-RedEye');
/pre-RedEye
/post-RedEye
Could not construct noRedEye filter from inline source
+[PIPipelineFilters noRedEyeFilter]
var output = input
var preTrim = GetTag('/pre-Trim');ResetTagInput('/SloMo', preTrim);
let slomo = composition.slomo
if (slomo) {
    var startTime = TimeMake(slomo.start, slomo.startScale)
    var endTime = TimeMake(slomo.end, slomo.endScale)
    output = SloMo(input, startTime, endTime, slomo.rate)
return output;
/pre-Trim
/SloMo
Could not construct noTrimFilter filter from inline source
+[PIPipelineFilters noTrimFilter]
var preMute = GetTag('pre-Mute');ResetTagInput('Mute', preMute);
return GetTag('Image');
Could not construct noMuteFilter filter from inline source
+[PIPipelineFilters noMuteFilter]
var preCrop = GetTag('/pre-Crop');ResetTagInput('/pre-Orientation', preCrop);
return input;
/pre-Orientation
Could not construct noCropFilter filter from inline source
+[PIPipelineFilters noCropFilter]
let preGeometry = GetTag('/pre-Geometry');ResetTagInput('/post-Adjustments', preGeometry);
return input;
/post-Adjustments
Could not construct noGeometry filter from inline source
+[PIPipelineFilters iosCropToolFilter]_block_invoke
var image = GetTag('pre-Trim');ResetTagInput('Trim', image);
image = GetTag('pre-SloMo');ResetTagInput('SloMo', image);
return input;
Could not construct stripAllTimeAdjustmentsFilter filter from inline source
+[PIPipelineFilters stripAllTimeAdjustmentsFilter]
let preGeometry = GetTag('/pre-Geometry');ResetTagInput('/post-Geometry', preGeometry);
return input;
+[PIPipelineFilters noGeometryFilter]
var preOrientation = GetTag('/pre-Orientation');ResetTagInput('/Orientation', preOrientation);
return input;
/Orientation
Could not construct noOrientationFilter filter from inline source
+[PIPipelineFilters noOrientationFilter]
return null;
Could not construct pipeline filter from source: %@
+[PIPipelineFilters orientationAsMetaDataFilter]
var preCrop = GetTag('/perspectiveStraighten');if (preCrop) {   ResetTagInput('/Crop', preCrop);
}return input;
/perspectiveStraighten
Could not construct perspectiveStraightenWithoutCropFilter filter from inline source
+[PIPipelineFilters perspectiveStraightenWithoutCropFilter]
var output = input;
var prePortraitVideoNode = GetTag('pre-PortraitVideo');
if (prePortraitVideoNode) {
    ResetTagInput('/post-PortraitVideo', prePortraitVideoNode);
return output;
/post-PortraitVideo
Could not construct histogramOptimizationFilter from inline source
+[PIPipelineFilters histogramOptimizationFilter]
var tagNode = GetTag('%@');
if (tagNode) {
    ResetTagInput('/pre-Geometry', tagNode);
return GetTag('/post-Geometry');
Could not construct stopAtTagIncludeGeometryFilter from inline source
+[PIPipelineFilters stopAtTagIncludeGeometryFilter:]
var tagNode = GetTag('%@');
if (tagNode) {
    ResetTagInput('/pre-Orientation', tagNode);
return GetTag('/Orientation');
Could not construct stopAtTagIncludeOrientationFilter from inline source
+[PIPipelineFilters stopAtTagIncludeOrientationFilter:]
var output = input;
var orientation = composition.orientation;
if (orientation) {
    output = Orient(input, orientation.value);
return output;
+[PIPipelineFilters applyOrientationFilter]
ResetTagInput('/AutoLoop/Output', GetTag('/AutoLoop/StabilizedVideo'));
return input;
/AutoLoop/Output
/AutoLoop/StabilizedVideo
Could not construct autoloopStabilizedVideoFilter filter from inline source
+[PIPipelineFilters autoloopStabilizedVideoFilter]
return Source(composition.sourceSpatialOvercapture, { 'skipOrientation' : true })
Could not construct overcaptureSourceFilter filter from inline source
+[PIPipelineFilters overcaptureSourceFilter]
return Source(composition.source, { 'skipOrientation' : true })
Could not construct primarySourceFilter filter from inline source
+[PIPipelineFilters primarySourceFilter]
return Source(composition.sourceSpatialOvercapture, {  })
+[PIPipelineFilters spatialOvercaptureVideoSourceFilter]
return input;
/PortraitV2-zeroStrength
/PortraitV2
Could not construct oneShotPortraitV2ExportFilter filter from inline source
+[PIPipelineFilters oneShotPortraitV2ExportFilter]
let output = input;
var image = GetTag('pre-VideoReframe');
if (image) {
    var videoReframe = composition.videoReframe;
    if (videoReframe && videoReframe.enabled) {
        image = VideoReframe(image, videoReframe);
        image = Filter(`CIColorMatrix`, {'inputImage' : image, 'inputBVector' : CIVectorMake(1,1,1,0)});
        ResetTagInput('VideoReframe', image);
    }
else { // image or live-photo { 
    var crop = composition.cropStraighten;
    var image = GetTag('pre-Crop');
    if (crop && crop.smart == 1 && image) {
        if (crop.pitch || crop.yaw) {
            var perspectiveTransform = PerspectiveTransformMake(crop.angle, crop.pitch, crop.yaw, image.geometry);
            image = Transform(image, perspectiveTransform);
        } else {
            var straightenTransform = StraightenTransformMake(crop.angle, image.geometry);
            image = Transform(image, straightenTransform);
        }
        // Apply the crop rect to the incoming image;
        image = Crop(image, crop.xOrigin, crop.yOrigin, crop.width, crop.height);
        image = Filter(`CIColorMatrix`, {'inputImage' : image, 'inputBVector' : CIVectorMake(1,1,1,0)});
        ResetTagInput('Crop', image);
    }
return output;
+[PIPipelineFilters(Debug) socPseudoColorFilter]_block_invoke
repairExtent != nil
+[PIRepairUtilities calcExtentsForStrokeRadius:offset:inputImageExtent:maskExtent:repairExtent:textureExtent:sourceExtent:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Adjustments/Retouch/PIRepair.mm
textureExtent != nil
sourceExtent != nil
Bad half float to float conversion
+[PIRepairUtilities extractRGBAfPixelsFromImage:fromROI:]
PIRepair expects the incoming image to be RGBAh, not %@
inputMaskImage
inputMaskBoundingBox
inputFaceBoundingBoxes
PIRepair-applyRepairMLStrokeToMutableBuffer
Bad fixed float to half float conversion
+[PIRepairUtilities applyRepairStrokeToMutableBuffer:brushStroke:sourceOffset:repairEdges:]
allocator<T>::allocate(size_t n) 'n' exceeds maximum supported size
error != NULL
-[NURenderPipelineHelper(PI) videoReframe:reframes:error:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Pipeline/PIPhotosPipelineHelper.m
-[NURenderPipelineHelper(PI) videoCrossfadeLoop:crossfadeAdjustment:error:]
-[NURenderPipelineHelper(PI) portraitVideo:disparityInput:disparityKeyframes:apertureKeyframes:debugMode:error:]
@"NSMutableArray"16@?0@"NSArray"8
-[NURenderPipelineHelper(PI) portraitVideoDebugDetectedObjects:source:cinematographyState:monochrome:error:]
Unexpected source type
extent
transform
AutoLoop/LongExposureMotion
PILongExposureFusion
inputStillImage
inputVideoScale
inputRenderScale
inputAlignmentExtent
inputAlignmentTransform
PIApertureRedEyeFilter
PIRedEye
inputDestinationImage
image != nil
+[PIHDRUtilities newHLGPixelBufferFromSDRImage:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Util/PIHDRUtilities.m
PIHDRInverseHLGFilter
Failed to start rendering %@ to: %@, error: %@
Succesfully rendered HLG buffer: %@
Failed to allocate pixel buffer
kernel vec4 hlg_luma_blending_inv(__sample sdr, float scale) 
  const vec3 lum_weights = vec3(0.2627, 0.6780, 0.0593); 
  float Ys = dot(lum_weights, sdr.rgb); 
  float Ymax = max(sdr.r, max(sdr.g, sdr.b)); 
  float Yb = 0.5*(Ys+Ymax); 
  const float gamma1 = 0.845906630893; 
  float absY = max(abs(Yb), 0.00001); 
  float gainInv = scale * pow(absY, 1.0/gamma1 - 1.0); 
  float3 hdr = gainInv * sdr.rgb; 
  return vec4(hdr.rgb, 1.0); 
recipe != nil
NSDictionary * _Nonnull PIAutoLoopRecipeForFlavor(NSDictionary *__strong _Nonnull, PIAutoLoopFlavor)
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/AutoLoop/PIAutoLoop.m
CGRect PIAutoLoopRecipeGetCropRect(NSDictionary *__strong _Nonnull)
origin_x
origin_y
BOOL PIAutoLoopRecipeHasGoodStabilization(NSDictionary *__strong _Nonnull)
frameTransform != nil
CMTime PIAutoLoopRecipeFrameTransformGetTime(NSDictionary *__strong)
frameTransform_rawTime
matrix_float3x3 PIAutoLoopRecipeFrameTransformGetHomography(NSDictionary *__strong)
frameTransform_homography
NSDictionary * _Nonnull PIAutoLoopRecipeGetInstructionAtTime(NSDictionary *__strong _Nonnull, CMTime)
CMTIME_IS_VALID(time)
loopRecipe_frameInstructions
loopFrameData_presTime
q24@?0@"NSDictionary"8@"NSDictionary"16
CMTime PIAutoLoopRecipeGetFrameDuration(NSDictionary *__strong _Nonnull)
NSDictionary * _Nullable PIAutoLoopRecipeGetFlavorParameters(NSDictionary *__strong _Nonnull, PIAutoLoopFlavor)
CMTimeRange PIAutoLoopRecipeGetTimeRangeForFlavor(NSDictionary *__strong _Nonnull, PIAutoLoopFlavor)
completion != nil
-[PICurvesAutoCalculator submit:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Autocalculators/PICurvesAutoCalculator.m
-[PICurvesAutoCalculator computeCurvesForImageHistogram:]
softlink:r:path:/System/Library/Frameworks/JavaScriptCore.framework/JavaScriptCore
_PIParallaxInfillResult
PIParallaxInfillResult
NURenderResult
NSObject
PIParallaxInfillJob
PIParallaxInfillRequest
_PITapToTrackRenderResult
PITapToTrackResult
PITapToTrackRenderJob
PITapToTrackRequest
PIPhotoEffectHDR
PIPhotoEffectBlackAndWhiteHDR
PIPhotoEffectNoirHDR
PIPhotoEffectChromeHDR
PIPhotoEffectFadeHDR
PIPhotoEffectInstantHDR
PIPhotoEffectMonoHDR
PIPhotoEffectProcessHDR
PIPhotoEffectTonalHDR
PIPhotoEffectTransferHDR
PIPhotoEffectStageMonoHDR
PIPhotoEffect3DHDR
PIPhotoEffect3DBlackAndWhiteHDR
PIPhotoEffect3DVividHDR
PIPhotoEffect3DVividWarmHDR
PIPhotoEffect3DVividCoolHDR
PIPhotoEffect3DDramaticHDR
PIPhotoEffect3DDramaticCoolHDR
PIPhotoEffect3DDramaticWarmHDR
PIPhotoEffect3DSilverplateHDR
PIPhotoEffect3DNoirHDR
PIColorNormalizationFilter
PIReframeRules
PISemanticEnhanceAdjustmentController
_PIDisparitySampleResult
PIDisparitySampleResult
PIDisparitySampleJob
PIDisparitySampleRequest
PILocalContrastHDR
PIParallaxLayoutHelper
PISegmentationLayoutRegions
PFParallaxAssetRegions
PISegmentationLayout
_PIParallaxClockLayoutResult
PIParallaxClockLayoutResult
_PIParallaxClockLayoutJob
PIParallaxClockLayoutRequest
PIHighKey
PISegmentationInfillFilter
PISegmentationInwardFillProcessor
PIParallaxInwardFillKernel
PIParallaxColorPalette
PICompositionController
NSCopying
PICompositionSerializer
PICompositionSerializationResult
PICompositionSerializerMetadata
PILevelsFilterHDR
PIColorNormalizationAutoCalculator
NUTimeBased
PIFalseColorHDRDebug
PIAutoCalculatorUtils
PIAutoLoopExportRequest
PIAutoLoopExportClient
PIAutoLoopAnalysisRequest
PIAutoLoopAnalysisClient
PILongExposureRegistrationRequest
PIPortraitVideoProcessor
PIRGB10PortraitVideoProcessor
PIPortraitVideoFilter
PIAutoLoopAdjustmentController
PIVideoCrossfadeLoopNode
PILivePhotoKeyFrameAdjustmentController
PIVignetteAdjustmentController
PIAdjustmentController
PIParallaxFilter
_PIParallaxClockMaterialResult
PIParallaxClockMaterialResult
_PIParallaxClockMaterialJob
PIParallaxClockMaterialRequest
NUDigest
PIPortraitVideoRenderNode
PIFaceObservationCache
PIParallaxStyleRecipeRegistry
PIParallaxStyleBundleURLProvider
PIParallaxStyleURLProvider
PIParallaxStyleUserURLProvider
PISourceSelectAdjustmentController
PIParallaxStyle
PIParallaxOriginalStyle
PIParallaxStudioStyle
PIParallaxTonalityModeStyle
PIParallaxColorBGStandardStyle
PIParallaxColorParameterStyle
PIParallaxBlackWhiteStudioStyle
PIParallaxBlackWhiteMonoStyle
PIParallaxColorWashSingleStyle
PIParallaxColorWashDuotoneStyle
PIParallaxRecipeStyle
PIScalarKeyframe
PIDictionaryRepresentable
PIReframeSubject
NSSecureCoding
NSCoding
PIParallaxSegmentationItem
PISegmentationItem
PISegmentationContextInfo
PIPortraitVideoAdjustmentController
PIParallaxColorSuggester
PIReframeKeyframe
PIReframeKeyframeSequence
_PIParallaxColorAnalysisResult
PIParallaxColorAnalysisResult
_PIParallaxRenderResource
_PIParallaxColorAnalysisJob
PIParallaxColorAnalysisRequest
PIIPTHueChromaFilter
PIIPTHueChromaColorFilter
PIIPTHueChromaGrayFilter
PIParallaxColorAnalysis
PILongExposureFusionAutoCalculator
PIEffectAdjustmentController
PIAutoLoopAutoCalculator
PIMakerNoteUtilities
PITempTintFilter
PITimeVaryingPipelineStateSetting
_PIAutoLoopAnalysisResult
PIAutoLoopAnalysisResult
PIAutoLoopAnalysisJob
_PIVideoStabilizeFlowControl
ICFlowControl
_PIVideoStabilizeResult
PIVideoStabilizeResult
PIVideoStabilizeRenderJob
PIVideoStabilizeRequest
PIFaceBalanceAutoCalculator
GrayColorResult
RGBResult
PIWhiteBalanceAutoCalculator
_PIWhiteColorCalculator
PIDefinitionFilter
PISmartBlackAndWhiteAdjustmentController
PICompositionExporterDefaultMetadataConverter
PICompositionExporterMetadataConverter
PICompositionExportImagePrepareResult
PICompositionExportAuxiliaryResult
PICompositionExportResult
PICompositionExportDataResult
PICompositionExporterOptions
PICompositionExporterVideoOptions
PICompositionExporterImageOptions
PICompositionExporterAuxiliaryOptions
PICompositionExporter
GUBilateralConvolution
GUBWBilateralConvolution
PIBilateralFilter
PIPhotoEditAdjustmentsVersion
PINoiseReductionAdjustmentController
PrivateSmartColorHDR
PISmartColorFilterHDR
PIAutoEnhanceFilter
_PIParallaxCompoundLayerStackResult
PIParallaxLayerStackResult
PIParallaxCompoundLayerStackRequest
PIVideoCrossfadeLoopAdjustmentController
PISegmentationHelper
PIPortraitAdjustmentController
PIParallaxLegacyPosterStyle
PICompositionNoOpRemoval
PIOrientationAdjustmentController
PIParallaxStyleRecipeArchiver
PIPortraitVideoMetadataSample
PIPortraitVideoRenderer
PIDefinitionAdjustmentController
PIHighKeyHDR
PIPerspectiveAutoCalculator
PIFaceObservingAutoCalculator
PIRawNoiseReductionAdjustmentController
PICropAdjustmentController
PIRawAdjustmentController
PIParallaxRecipeFilter
PISmartToneAdjustmentController
PIAutoLoopKernels
PIPhotoEditHelper
PIAdjustmentConstants
PIFakeBoost
PIForwardFakeBoost
PIInverseFakeBoost
PICoreImageUtilities
PICompositionSerializerConstants
PICurvesFilterHDR
_PIParallaxLayoutResult
PIParallaxLayoutResult
_PIParallaxLayoutJob
PIParallaxLayoutRequest
PIParallaxLuminanceCalculator
PIParallaxStyleRecipe
PIParallaxStyleDefinition
PIParallaxStyleFilterDefinition
PIParallaxStyleFilterStackDefinition
PIParallaxStyleParameter
PIParallaxStyleNumberParameter
PIParallaxStyleColorParameter
PIParallaxStylePointParameter
PIParallaxStyleModeParameter
PIParallaxStyleBindingParameter
PIParallaxStyleEvaluationContext
PILongExposureAccumulator
_PILongExposureRegistrationResult
PILongExposureRegistrationResult
PILongExposureRegistrationJob
PIRAWTempTintSampler
PITagColorSampler
PIRedEye
PIVideoPosterFrameAdjustmentController
PICompositionSerializerFormatVersion
PICaptureDebugUtilities
PISlomoAdjustmentController
PISegmentationGatingRange
PISegmentationGatingRanges
PISegmentationGating
PIPortraitVideoDebugDetectionsRenderNode
PIColorWashFilter
PIColorWashDuoFilter
PIDebugAdjustmentController
PIGainMap
PIRAWFaceBalance
PICurvesLUTFilter
PICurvesFilter
PIColorBalanceFilter
PIPhotosPipelineHDRFilters
PIHighResFusionAdjustmentController
PIGlobalSettings
PIAutoLoopExportJob
PIJSRenderPipeline
PhotosPipeline
PIApertureRedEyeAutoCalculator
PrivateLocalLightHDR
PILocalLightMapPrepareHDR
PILocalLightFilterHDR
PIParallaxAsset
PFParallaxAsset
PFParallaxSegmentationResourceCaching
PIMsgImageBuffer
PISmartBlackAndWhiteAutoCalculator
PISmartBlackAndWhiteHDR
PrivateSmartBlackAndWhite
PITrimAdjustmentController
PIRedEyeAdjustmentController
PICinematicVideoUtilities
_PIParallaxRenderBuffer
NUImageBuffer
_PIParallaxRenderCacheEntry
_PIParallaxLayerStackResult
_PIParallaxLayerStackScalePolicy
NUScalePolicy
_PIParallaxLayerStackJob
_PIParallaxLayerStackRenderer
PIParallaxFilterCache
PIParallaxLayerStackRequest
_PIParallaxLayerStackDebugImageCollector
PILevelsAutoCalculator
PILevelsLuminanceAutoCalculator
PILevelsRGBAutoCalculator
AdjustmentExtensions
PICurveControlPoint
PrivateSmartToneHDR
PISmartToneFilterHDR
PICompositingFilter
PILongExposureFusion
PIModernPhotosPipeline
PILevelsFilter
PISegmentationLoader
PISegmentationLoading
_PISegmentationNullAsset
_PIParallaxLayoutInactiveFrameResult
PIParallaxLayoutInactiveFrameResult
_PIParallaxLayoutInactiveFrameJob
PIParallaxLayoutInactiveFrameRequest
PISchema
PICompositionFinalizer
PICompositionFinalizerResult
PIVideoStabilizeAdjustmentController
PICropAutoCalculator
Utilities
PISmartColorAdjustmentController
PISelectiveColorFilter
PIVideoReframeNode
PIVideoReframe
PIVideoReframeTimedMetadata
PIVideoReframeMetadataExtractor
PINeutralGrayWhiteBalanceFilter
PISmartToneAutoCalculator
PISmartColorAutoCalculator
PIRedEyeAutoCalculator
PIManualRedEyeAutoCalculator
PISegmentationCropFilter
PISegmentationMPSReduceProcessor
PISourceSampler
PIPhotoGrainHDR
PISegmentation
PIImageIO
PISharpenAdjustmentController
PIApertureRedEyeProcessorKernel
PIApertureRedEyeFilter
PIValuesAtCapture
PIPortraitAutoCalculator
PIDepthEffectApertureAutoCalculator
PIPipelineFilters
Debug
PIWhiteBalanceAdjustmentController
PIRepairUtilities
PIDepthAdjustmentController
PIVideoReframeAdjustmentController
PIHDRUtilities
PIHDRInverseHLGFilter
PICurvesAutoCalculator
PICurvesLuminanceAutoCalculator
PICurvesRGBAutoCalculator
isEqual:
class
self
performSelector:
performSelector:withObject:
performSelector:withObject:withObject:
isProxy
isKindOfClass:
isMemberOfClass:
conformsToProtocol:
respondsToSelector:
retain
release
autorelease
retainCount
zone
hash
superclass
description
debugDescription
TQ,R
T#,R
T@"NSString",R,C
statistics
T@"<NURenderStatistics>",R
infilledImage
T@"<NUImageBuffer>",R,N
setInfilledImage:
.cxx_destruct
_infilledImage
T@"<NUImageBuffer>",&,N,V_infilledImage
stringWithFormat:
callStackSymbols
componentsJoinedByString:
initWithRequest:
request
infillRequest
scalePolicy
prepare:
segmentationMatte
missingError:object:
imageWithNUImageBuffer:
extent
invalidError:object:
shouldInfillForeground
invertImage:
infillMaskForSegmentationMatte:
outputImage
setMatteImage:
imageByApplyingTransform:
matteImage
globalSettings
segmentationInfillAlgorithm
setInfillAlgorithm:
setInputImage:
setInputMatteImage:
failureError:object:
pixelFormat
colorSpace
imageSize
regionWithRect:
sharedFactory
surfaceStoragePool
imageBufferWithSize:format:fromPool:
storage
renderImage:into:colorSpace:roi:imageSize:error:
nu_waitUntilCompletedAndReturnError:
errorWithCode:reason:object:underlyingError:
initWithParallaxInfillRequest:
wantsRenderStage
wantsOutputImage
wantsOutputGeometry
render:
complete:
result
cleanUp
_infilledImageBuffer
_renderTask
_matteImage
T@"PIParallaxInfillRequest",R,N
T@"CIImage",&,N,V_matteImage
initWithComposition:
oneToOneScalePolicy
BGRA8
sRGBColorSpace
copyWithZone:
submitGeneric:
newRenderJob
mediaComponentType
submit:
setSegmentationMatte:
setScalePolicy:
setPixelFormat:
setColorSpace:
setShouldInfillForeground:
_shouldInfillForeground
_segmentationMatte
_scalePolicy
_pixelFormat
_colorSpace
T@"<NUImageBuffer>",&,N,V_segmentationMatte
T@"<NUScalePolicy>",&,N,V_scalePolicy
T@"NUPixelFormat",&,N,V_pixelFormat
T@"NUColorSpace",&,N,V_colorSpace
TB,N,V_shouldInfillForeground
init
completedTrack
T@"PTCinematographyTrack",R,N
initWithCompletedTrack:
_completedTrack
T@"PTCinematographyTrack",R,N,V_completedTrack
device
currentPlatform
mainDevice
metalDevice
newCommandQueue
initWithCommandQueue:
outputVideo
initWithAsset:
startTime
startReadingFrames:atTime:error:
nextFrame
normalizedImagePoint
colorBuffer
getRectForPoint:colorBuffer:
rect
time
disparityBuffer
addDetectionAndStartTrackingRect:time:colorBuffer:disparityBuffer:
confidence
_reportProgressAtTime:rect:confidence:
clientRequestedStop
addDetectionForNextFrameAt:colorBuffer:disparityBuffer:
stopReadingFrames
finalizeTrack
setCompletedTrack:
progressHandler
responseQueue
setClientRequestedStop:
wantsCompleteStage
wantsOutputVideo
setStartTime:
setNormalizedImagePoint:
setProgressHandler:
_clientRequestedStop
_progressHandler
_normalizedImagePoint
_startTime
T{?=qiIq},N,V_startTime
T{CGPoint=dd},N,V_normalizedImagePoint
T@?,C,N,V_progressHandler
T@"PTCinematographyTrack",&,N,V_completedTrack
TB,V_clientRequestedStop
sourceFilterNoOrientation
arrayWithObjects:count:
setPipelineFilters:
initWithComposition:startTime:pointToTrack:
kernelWithString:
stringByReplacingOccurrencesOfString:withString:
kernel
applyWithExtent:arguments:
photoEffectName
filterWithName:
setValue:forKey:
inputImage
_inputImage
T@"CIImage",&,V_inputImage
kernelBlackAndWhite
numberWithFloat:
inputDepthMap
setInputDepthMap:
inputThreshold
setInputThreshold:
_inputThreshold
_inputDepthMap
T@"CIImage",&,V_inputDepthMap
Tf,V_inputThreshold
inputNormalization
tempTintProperties
objectForKeyedSubscript:
dictionaryWithObjects:forKeys:count:
imageByApplyingFilter:withInputParameters:
highKeyProperties
smartToneProperties
smartColorProperties
logger
initWithCIImage:options:
setRevision:
performRequests:error:
results
firstObject
adjustmentKeys
countByEnumeratingWithState:objects:count:
adjustmentValuesForKey:
setObject:forKeyedSubscript:
initWithAnalysisData:
setInputNormalization:
contextWithOptions:
pi_createColorCubeDataForFilters:dimension:colorSpace:
isAnalysisAvailable
colorCubeForNormalization:dimension:targetColorSpace:
analysisAvailable
TB,R,N,GisAnalysisAvailable
outputNormalization
T@"CIImage",&,N,VinputImage
T@"PFStoryRecipeDisplayAssetNormalization",&,N,VinputNormalization
T@"PFStoryRecipeDisplayAssetNormalization",R,N
initWithCapacity:
predicateWithFormat:
ruleWithPredicate:action:
addObject:
ruleWithPredicate:assertingFact:grade:
ruleWithPredicate:retractingFact:grade:
factCandidateForReframe
factCandidateForStill
factCandidateForVideo
factCandidateForPerspective
factCandidateForHorizon
retractFact:
initWithArray:
setEnableLogging:
setConstants:
sharedPregateRules
addRulesFromArray:
gradeForFact:
pregateRulesSystemWithConstants:
isCandidateForReframe
isCandidateForPerspective
isCandidateForHorizon
TB,R
adjustment
intensityKey
isEqualToString:
doubleValue
sceneLabelKey
boundingBoxesKey
isEqualToArray:
isSettingEqual:forKey:
numberWithDouble:
floatValue
platedFoodSceneLabel
sunriseSunsetSceneLabel
genericLandscapeSceneLabel
sceneConfidenceKey
copy
count
arrayWithCapacity:
faceBoundingBoxesKey
setIntensity:
intensity
scene
sceneConfidence
boundingBoxes
setScene:confidence:
setBoundingBoxesFromObservations:orientation:
setFaceBoundingBoxesFromObservations:orientation:
Td,N
Tq,R,N
Td,R,N
T@"NSArray",R,C,N
currentHandler
stringWithUTF8String:
handleFailureInFunction:file:lineNumber:description:
boundingBox
sampledDisparityValue
Tf,R,N
initWithDisparityValue:
_sampledDisparityValue
Tf,R,N,V_sampledDisparityValue
sampleTime
sampleRect
setSampledDisparityValue:
setSampleTime:
setSampleRect:
_sampleTime
_sampleRect
T{?=qiIq},N,V_sampleTime
T{CGRect={CGPoint=dd}{CGSize=dd}},N,V_sampleRect
Tf,N,V_sampledDisparityValue
initWithComposition:time:sampleRect:
vectorWithX:Y:Z:W:
HLGOpticalScale
imageByClampingToExtent
imageByApplyingGaussianBlurWithSigma:
imageByCroppingToRect:
_kernelLocalContrast
customAttributes
inputStrength
inputScale
initWithParallaxClassification:initialRect:imageSize:effectiveAcceptableRect:effectivePreferredRect:layoutConfiguration:
initWithCIContext:matte:parallaxClassification:initialRect:imageSize:effectiveAcceptableRect:effectivePreferredRect:layoutConfiguration:
visibleRect
intermediateWithZoomStrategy:intermediate:
pixelEffectiveAcceptable
layoutConfiguration
computeHeadroomZoomFactorWithVisibleFrame:zoomTowardsTop:matte:layoutConfiguration:context:
targetZoomFactorLimit
inactiveRect
overlapStrategy
parallaxStrategy
inactiveStrategy
initWithVisibleRect:inactiveRect:zoomStrategy:overlapStrategy:parallaxStrategy:inactiveStrategy:
zoomStrategy
intermediateWithOverlapStrategy:intermediate:
computeAvoidOverlapYOffsetWithVisibleFrame:imageSize:segmentationMatte:layoutConfiguration:outputUnsafeOverlap:context:
classification
pixelBasedIntermediateWithOverlapStrategy:intermediate:translationY:
computeTargetOverlapYOffsetWithVisibleFrame:imageSize:segmentationMatte:layoutConfiguration:context:
computeInactiveFrameWithVisibleFrame:imageSize:canUpdateVisibleRect:segmentationMatte:layoutConfiguration:context:
computeCropScoreForIntermediate:
unsafeAreaInImageSpaceWithVisibleFrame:
computeClockLayerOrderWithVisibleFrame:segmentationMatte:layoutConfiguration:context:interactive:
computeMatteCoverageWithRect:segmentationMatte:context:
canInflate
inflatePersonFaceRect:
scoreAdjustmentWithUnscoredIntermediate:unsafeAreaOverlap:timeBottomOverlap:timeTopOverlap:
initWithVisibleRect:inactiveRect:zoomStrategy:overlapStrategy:parallaxStrategy:inactiveStrategy:cropScore:layoutScore:timeBottomOverlap:timeTopOverlap:unsafeAreaOverlap:uninflatedUnsafeAreaOverlap:
intermediateWithInactiveStrategy:intermediate:
scoreIntermediate:
_context
_matte
initWithAcceptableRect:preferredRect:faces:pets:
acceptableCropRect
valueWithBytes:objCType:
preferredCropRect
faceRegions
petRegions
rectValue
mismatchError:object:
dictionaryFromRegions:
regionsFromDictionary:error:
T{CGRect={CGPoint=dd}{CGSize=dd}},R,N
T@"NSArray",R,N
_faceRegions
_petRegions
_acceptableCropRect
_preferredCropRect
T{CGRect={CGPoint=dd}{CGSize=dd}},R,N,V_acceptableCropRect
T{CGRect={CGPoint=dd}{CGSize=dd}},R,N,V_preferredCropRect
T@"NSArray",R,N,V_faceRegions
T@"NSArray",R,N,V_petRegions
screenSize
effectiveAcceptableRectForClassification:havePetFaces:sourcePreferredCropRectNormalized:sourceAcceptableCropRectNormalized:sourceFaceAreaRectNormalized:
effectivePreferredRectForClassification:havePetFaces:sourcePreferredCropRectNormalized:sourceAcceptableCropRectNormalized:sourceFaceAreaRectNormalized:
bestCropRectV2ForAspectRatio:sourcePixelWidth:sourcePixelHeight:sourceEssentialAreaRect:sourceSecondaryEssentialAreaRect:outputCropScore:
bestCropRectV2ForAspectRatio:withFocusRegion:sourcePixelWidth:sourcePixelHeight:sourcePreferredCropRectNormalized:sourceAcceptableCropRectNormalized:sourceFaceAreaRectNormalized:outputCropScore:
computeLayoutsWithHelper:
bestLayout:
arrayByAddingObject:
layoutScore
cropScore
clockOverlapAcceptableForLayoutConfiguration:
parallaxPaddingPct
timeRect
initWithImageSize:deviceResolution:parallaxPadding:visibleFrame:inactiveFrame:timeFrame:clockLayerOrder:clockIntersection:debugLayouts:
generateLayoutForLayoutConfiguration:imageSize:regions:parallaxClassification:context:matte:layoutScore:cropScore:clockOverlapAcceptable:
clockLayerOrder
clockIntersection
T@"NSString",R,N
TQ,R,N
setClockLayerOrder:
setClockIntersection:
_clockLayerOrder
_clockIntersection
T@"NSString",&,N,V_clockLayerOrder
TQ,N,V_clockIntersection
initWithTargetPixelCount:
clockLayoutRequest
segmentationItem
layout
originalLayout
deviceConfiguration
setLayout:
setLayoutConfiguration:
renderer:
visibleFrame
context
initWithParallaxClockLayoutRequest:
_layout
_layoutConfiguration
T@"PIParallaxClockLayoutRequest",R,N
T@"PFParallaxLayout",&,N,V_layout
T@"<PFParallaxLayoutConfiguration>",&,N,V_layoutConfiguration
composition
initWithSegmentationItem:
_segmentationItem
T@"<PISegmentationItem>",R,N,V_segmentationItem
setInputStrength:
T@"NSNumber",&,N,VinputStrength
defaultManager
fileExistsAtPath:
inputMatteImage
infillAlgorithm
fastInfillImage:matte:
watchInfillImage:matte:
inpaintingInfillImage:matte:
blackInfillImage:matte:
blendWithMaskFilter
blackColor
imageWithColor:
setBackgroundImage:
setMaskImage:
applyWithExtent:inputs:arguments:error:
imageByApplyingTransform:highQualityDownsample:
createCGImage:fromRect:
newWatchInfillFromImage:mask:
imageWithCGImage:
foregroundForImage:matte:
imageByCompositingOverImage:
filterWithName:withInputParameters:
isMLInpaintingAvailable
_infillAlgorithm
_inputMatteImage
Tq,N,V_infillAlgorithm
T@"CIImage",&,N,V_inputImage
T@"CIImage",&,N,V_inputMatteImage
objectAtIndexedSubscript:
metalCommandBuffer
metalTexture
fillSourceTexture:intoDestinationTexture:withCommandBuffer:
formatForInputAtIndex:
outputIsOpaque
synchronizeInputs
processWithInputs:arguments:output:error:
initWithDevice:
setSourceTexture:
encodeToCommandBuffer:destinationTexture:
sourceTexture
width
height
texture2DDescriptorWithPixelFormat:width:height:mipmapped:
setUsage:
setStorageMode:
newTextureWithDescriptor:
mipmapLevelCount
pipelineStateForFunctionWithName:device:error:
textureType
newTextureViewWithPixelFormat:textureType:levels:slices:
computeCommandEncoder
pushDebugGroup:
setComputePipelineState:
setTexture:atIndex:
depth
groupSizeForImageSize:pipelineState:
gridSizeForThreadGroupSize:imageSize:
dispatchThreadgroups:threadsPerThreadgroup:
popDebugGroup
endEncoding
_sourceTexture
T@"<MTLTexture>",&,N,V_sourceTexture
suggestionIndices
numberWithUnsignedInteger:
enumerateIndexesUsingBlock:
primaryColors
_serializeColors:mode:
secondaryColors
writeToURL:error:
dictionaryWithContentsOfURL:error:
_paletteWithConfigurationDictionary:error:
integerValue
unsupportedError:object:
containsObject:
_loadColorsFromValues:mode:error:
initWithIndexesInRange:
unsignedIntegerValue
addIndex:
initWithPrimaryColors:secondaryColors:suggestionIndices:
tone
luma
chroma
green
blue
initWithHue:tone:
initWithLuma:hue:chroma:
initWithRed:green:blue:
initWithPrimaryColors:secondaryColors:
lastIndex
loadPaletteWithName:
useStyleRecipeConfigDirectory
styleRecipeConfigDirectoryPath
fileURLWithPath:
URLByAppendingPathComponent:
URLByAppendingPathExtension:
loadPaletteFromURL:error:
bundleForClass:
URLForResource:withExtension:
addObjectsFromArray:
containsIndex:
_lookupColor:withPredicate:
suggestionAtIndex:
distanceToColor:
indexOfColor:
initWithPrimaryColor:secondaryColor:
colorBGPalette
colorWashSinglePalette
colorWashDuotonePalette
customPalette
writeToURL:mode:error:
initWithColors:
suggestionForColor:
paletteColorForColor:
_primaryColors
_secondaryColors
_suggestionIndices
T@"NSArray",R,C,N,V_primaryColors
T@"NSArray",R,C,N,V_secondaryColors
T@"NSIndexSet",R,C,N,V_suggestionIndices
imageOrientation
setImageOrientation:
sourceSelectAdjustmentController
sourceSelection
setSourceSelection:
compositionController:didAddAdjustment:
compositionController:didRemoveAdjustment:
compositionController:didUpdateAdjustment:
compositionController:didUpdateAdjustments:
compositionController:adjustmentControllerClassForKey:
schema
contents
allKeys
compositionKeys
_keyToIdentifierMap
initWithIdentifier:
reset
_adjustmentControllerClassForKey:
initWithAdjustment:
setIdentifier:
differingAdjustmentsWithComposition:
changeDelegate
isEqual:forKeys:visualChangesOnly:
availableKeys
settings
boolValue
isEqual:visualChangesOnly:
sharedRegistry
schemaWithIdentifier:
schemaForKey:
orientationAdjustmentControllerCreatingIfNecessary:
orientation
removeAdjustmentWithKey:
modifyAdjustmentWithKey:modificationBlock:
smartToneAdjustmentControllerCreatingIfNecessary:
registeredPhotosSchemaIdentifier
photosSchema
adjustmentControllerClassForKey:
isSubclassOfClass:
setMediaType:
mediaType
settingForAdjustmentKey:settingKey:
setChangeDelegate:
addAdjustmentWithKey:
replaceAdjustment:withKey:
adjustmentControllerForKey:
applyChangesFromCompositionController:
isEqual:forKeys:comparisonBlock:
userOrientation
setOvercaptureSource:
overcaptureSource
setSource:mediaType:
source
_composition
_delegateFlags
_identifierMap
_changeDelegate
_imageOrientation
T@"NUComposition",R,C,N
T@"<PICompositionControllerDelegate>",W,N,V_changeDelegate
Tq,N
Tq,N,V_imageOrientation
setWithObjects:
locallySupportedFormatVersions
setWithArray:
handleFailureInMethod:object:file:lineNumber:description:
deserializeDictionaryFromData:error:
validateAdjustmentsEnvelope:error:
deserializeCompositionFromAdjustments:metadata:formatIdentifier:formatVersion:error:
_addDummySourceToCompositionIfNeeded:
validateComposition:error:
URLWithString:
imageSourceWithURL:type:useEmbeddedPreview:
conversionMap
newComposition
initWithDomain:code:userInfo:
stringByAppendingString:
newAdjustmentWithName:
enumerateKeysAndObjectsUsingBlock:
initWithName:
_sanitizeComposition:
validateCompositionWithMissingSource:error:
_serializeComposition:versionInfo:needsGeometry:error:
geometryRequestWithComposition:
setName:
submitSynchronous:
geometry
size
setWidth:
setHeight:
setOrientation:
geometryBasedAdjustmentIdentifiers
serializeComposition:versionInfo:serializerMetadata:error:
numberWithInteger:
array
copyOfCompositionRemovingNoOps:
mapForSerialization
hasInputKey:
isAuto
serializeDictionary:error:
setFormatIdentifier:
adjustmentDataFormatVersionForComposition:
setFormatVersion:
setData:
_validateValueTypesForKeys:requiredKeys:inDictionary:error:
dataWithJSONObject:options:error:
errorWithDomain:code:userInfo:
compressData:options:error:
decompressData:options:error:
JSONObjectWithData:options:error:
_isDefault
adjustmentInformationForComposition:needsGeometry:error:
mainBundle
infoDictionary
objectForKey:
dictionary
data
formatIdentifier
formatVersion
initialize
disableApertureEffects:
canInterpretDataWithFormatIdentifier:formatVersion:
deserializeCompositionFromData:formatIdentifier:formatVersion:error:
serializeComposition:versionInfo:error:
adjustmentInformationForComposition:error:
_data
_formatIdentifier
_formatVersion
T@"NSData",&,N,V_data
T@"NSString",&,N,V_formatIdentifier
T@"NSString",&,N,V_formatVersion
_width
_height
_orientation
Tq,N,V_width
Tq,N,V_height
Tq,N,V_orientation
containsString:
defaultValueForKey:
dictionaryWithObjectsAndKeys:
_customAttributesForKey:
valueForKey:
floatValueForKey:defaultValue:clearIfNotDefault:
dataWithBytesNoCopy:length:freeWhenDone:
imageWithBitmapData:bytesPerRow:size:format:colorSpace:
_LUTImage
samplerWithImage:keysAndValues:
P3KernelHDR
itur2100HLGColorSpace
CGColorSpace
imageByColorMatchingWorkingSpaceToColorSpace:
imageByUnpremultiplyingAlpha
samplerWithImage:
definition
applyWithExtent:roiCallback:arguments:options:
imageByColorMatchingColorSpaceToWorkingSpace:
imageByPremultiplyingAlpha
setDefaults
inputBlackSrcRGB
setInputBlackSrcRGB:
inputBlackDstRGB
setInputBlackDstRGB:
inputShadowSrcRGB
setInputShadowSrcRGB:
inputShadowDstRGB
setInputShadowDstRGB:
inputMidSrcRGB
setInputMidSrcRGB:
inputMidDstRGB
setInputMidDstRGB:
inputHilightSrcRGB
setInputHilightSrcRGB:
inputHilightDstRGB
setInputHilightDstRGB:
inputWhiteSrcRGB
setInputWhiteSrcRGB:
inputWhiteDstRGB
setInputWhiteDstRGB:
inputBlackSrcRed
setInputBlackSrcRed:
inputBlackDstRed
setInputBlackDstRed:
inputShadowSrcRed
setInputShadowSrcRed:
inputShadowDstRed
setInputShadowDstRed:
inputMidSrcRed
setInputMidSrcRed:
inputMidDstRed
setInputMidDstRed:
inputHilightSrcRed
setInputHilightSrcRed:
inputHilightDstRed
setInputHilightDstRed:
inputWhiteSrcRed
setInputWhiteSrcRed:
inputWhiteDstRed
setInputWhiteDstRed:
inputBlackSrcGreen
setInputBlackSrcGreen:
inputBlackDstGreen
setInputBlackDstGreen:
inputShadowSrcGreen
setInputShadowSrcGreen:
inputShadowDstGreen
setInputShadowDstGreen:
inputMidSrcGreen
setInputMidSrcGreen:
inputMidDstGreen
setInputMidDstGreen:
inputHilightSrcGreen
setInputHilightSrcGreen:
inputHilightDstGreen
setInputHilightDstGreen:
inputWhiteSrcGreen
setInputWhiteSrcGreen:
inputWhiteDstGreen
setInputWhiteDstGreen:
inputBlackSrcBlue
setInputBlackSrcBlue:
inputBlackDstBlue
setInputBlackDstBlue:
inputShadowSrcBlue
setInputShadowSrcBlue:
inputShadowDstBlue
setInputShadowDstBlue:
inputMidSrcBlue
setInputMidSrcBlue:
inputMidDstBlue
setInputMidDstBlue:
inputHilightSrcBlue
setInputHilightSrcBlue:
inputHilightDstBlue
setInputHilightDstBlue:
inputWhiteSrcBlue
setInputWhiteSrcBlue:
inputWhiteDstBlue
setInputWhiteDstBlue:
inputColorSpace
setInputColorSpace:
_inputBlackSrcRGB
_inputBlackDstRGB
_inputShadowSrcRGB
_inputShadowDstRGB
_inputMidSrcRGB
_inputMidDstRGB
_inputHilightSrcRGB
_inputHilightDstRGB
_inputWhiteSrcRGB
_inputWhiteDstRGB
_inputBlackSrcRed
_inputBlackDstRed
_inputShadowSrcRed
_inputShadowDstRed
_inputMidSrcRed
_inputMidDstRed
_inputHilightSrcRed
_inputHilightDstRed
_inputWhiteSrcRed
_inputWhiteDstRed
_inputBlackSrcGreen
_inputBlackDstGreen
_inputShadowSrcGreen
_inputShadowDstGreen
_inputMidSrcGreen
_inputMidDstGreen
_inputHilightSrcGreen
_inputHilightDstGreen
_inputWhiteSrcGreen
_inputWhiteDstGreen
_inputBlackSrcBlue
_inputBlackDstBlue
_inputShadowSrcBlue
_inputShadowDstBlue
_inputMidSrcBlue
_inputMidDstBlue
_inputHilightSrcBlue
_inputHilightDstBlue
_inputWhiteSrcBlue
_inputWhiteDstBlue
_inputColorSpace
T@"NSNumber",&,N,V_inputBlackSrcRGB
T@"NSNumber",&,N,V_inputBlackDstRGB
T@"NSNumber",&,N,V_inputShadowSrcRGB
T@"NSNumber",&,N,V_inputShadowDstRGB
T@"NSNumber",&,N,V_inputMidSrcRGB
T@"NSNumber",&,N,V_inputMidDstRGB
T@"NSNumber",&,N,V_inputHilightSrcRGB
T@"NSNumber",&,N,V_inputHilightDstRGB
T@"NSNumber",&,N,V_inputWhiteSrcRGB
T@"NSNumber",&,N,V_inputWhiteDstRGB
T@"NSNumber",&,N,V_inputBlackSrcRed
T@"NSNumber",&,N,V_inputBlackDstRed
T@"NSNumber",&,N,V_inputShadowSrcRed
T@"NSNumber",&,N,V_inputShadowDstRed
T@"NSNumber",&,N,V_inputMidSrcRed
T@"NSNumber",&,N,V_inputMidDstRed
T@"NSNumber",&,N,V_inputHilightSrcRed
T@"NSNumber",&,N,V_inputHilightDstRed
T@"NSNumber",&,N,V_inputWhiteSrcRed
T@"NSNumber",&,N,V_inputWhiteDstRed
T@"NSNumber",&,N,V_inputBlackSrcGreen
T@"NSNumber",&,N,V_inputBlackDstGreen
T@"NSNumber",&,N,V_inputShadowSrcGreen
T@"NSNumber",&,N,V_inputShadowDstGreen
T@"NSNumber",&,N,V_inputMidSrcGreen
T@"NSNumber",&,N,V_inputMidDstGreen
T@"NSNumber",&,N,V_inputHilightSrcGreen
T@"NSNumber",&,N,V_inputHilightDstGreen
T@"NSNumber",&,N,V_inputWhiteSrcGreen
T@"NSNumber",&,N,V_inputWhiteDstGreen
T@"NSNumber",&,N,V_inputBlackSrcBlue
T@"NSNumber",&,N,V_inputBlackDstBlue
T@"NSNumber",&,N,V_inputShadowSrcBlue
T@"NSNumber",&,N,V_inputShadowDstBlue
T@"NSNumber",&,N,V_inputMidSrcBlue
T@"NSNumber",&,N,V_inputMidDstBlue
T@"NSNumber",&,N,V_inputHilightSrcBlue
T@"NSNumber",&,N,V_inputHilightDstBlue
T@"NSNumber",&,N,V_inputWhiteSrcBlue
T@"NSNumber",&,N,V_inputWhiteDstBlue
T@"NSString",&,N,V_inputColorSpace
stopAtTagIncludeOrientationFilter:
setSampleMode:
setTime:
imageWithData:
imageSourceWithCIImage:orientation:
newCompositionControllerWithComposition:
initWithError:
setVisionRequests:
result:
observations
requestRevision
initWithResult:
isAvailable
autoCalculatorWithImageData:orientation:
T{?=qiIq},N
available
TB,R,N,GisAvailable
T{?=qiIq},N,Vtime
inputCutoff
setInputCutoff:
_inputCutoff
Td,V_inputCutoff
convertFacePoint:toImagePointWithFaceRect:orientation:
averagePoints:pointCount:
averageCGPoints:pointCount:
initWithComposition:destinationURL:
initWithAutoLoopExportRequest:
outputSettings
colorSpaceFromVideoColorProperties:
initWithComposition:stabilizedVideoURL:longExposureDestinationURL:maskDestinationURL:destinationUTI:
outputColorSpace
destinationUTI
destinationLongExposureURL
destinationMaskURL
_destinationUTI
_destinationLongExposureURL
_destinationMaskURL
T@"NSString",R,V_destinationUTI
T@"NSURL",R,V_destinationLongExposureURL
T@"NSURL",R,V_destinationMaskURL
T@"NUColorSpace",R
setGenericCompletionBlock:
submitGenericRequest:
setCompletionBlock:
submitRequest:
flavor
setFlavor:
_flavor
Tq,N,V_flavor
recipe
setRecipe:
cleanAperture
setCleanAperture:
_recipe
_cleanAperture
T@"NSDictionary",C,N,V_recipe
T{?={?=qq}{?=qq}},N,V_cleanAperture
setTransferFunction:
createRGBA:
format
_configureRGBColorTexture:format:isHDR:
region
CVPixelBuffer
pixelBufferToLumaChroma:pixelBuffer:outLuma:outChroma:read:write:
createYUV420:chroma:
setColorPrimaries:
setYCbCrMatrix:
setYCbCrColorDepth:
setYCbCrFullRange:
getMTLTextureFromPixelBuffer:device:
setSourceColor:
setSourceDisparity:
setDestinationColor:
applyToRenderRequest:
setAperture:
setFocusDistance:
intValue
setRenderState:
encodeRenderTo:withRenderRequest:
status
label
error
addCompletedHandler:
renderOnDevice:colorSize:disparitySize:quality:debugMode:globalRenderingMetadata:usingBlock:
setTotalSensorCrop:
setRawSensorWidth:
setRawSensorHeight:
setFocalLenIn35mmFilm:
setConversionGain:
setReadNoise_1x:
setReadNoise_8x:
CGRectValue
vectorWithCGRect:
numberWithBool:
_updateRenderState:withLegacyCameraInfo:
outputFormat
allowPartialOutputRegion
allowCompressedInputsAndOutputs
roiForInput:arguments:outputRect:
applyWithInputImage:disparityImage:inputPixelBuffer:disparityPixelBuffer:globalMetadata:timedMetadata:aperture:focusedDisparity:quality:debugMode:isHDR:error:
inputGlobalRenderingMetadata
renderingVersion
cinematicAllowRGB10Packed
inputIsHDR
rec709ColorSpace
inputDisparityImage
inputColorPixelBuffer
inputDisparityPixelBuffer
inputTimedRenderingMetadata
inputAperture
inputFocusedDisparity
inputRenderQuality
inputRenderDebugMode
setInputDisparityImage:
setInputColorPixelBuffer:
setInputDisparityPixelBuffer:
setInputRenderQuality:
setInputRenderDebugMode:
setInputIsHDR:
setInputGlobalRenderingMetadata:
setInputTimedRenderingMetadata:
setInputAperture:
setInputFocusedDisparity:
_inputIsHDR
_inputDisparityImage
_inputColorPixelBuffer
_inputDisparityPixelBuffer
_inputRenderQuality
_inputRenderDebugMode
_inputGlobalRenderingMetadata
_inputTimedRenderingMetadata
_inputAperture
_inputFocusedDisparity
T@"CIImage",&,N,V_inputDisparityImage
T@"NUCVPixelBuffer",&,N,V_inputColorPixelBuffer
T@"NUCVPixelBuffer",&,N,V_inputDisparityPixelBuffer
T@"NSNumber",&,N,V_inputRenderQuality
T@"NSNumber",&,N,V_inputRenderDebugMode
TB,N,V_inputIsHDR
T@"PTGlobalRenderingMetadata",&,N,V_inputGlobalRenderingMetadata
T@"PIPortraitVideoMetadataSample",&,N,V_inputTimedRenderingMetadata
T@"NSNumber",&,N,V_inputAperture
T@"NSNumber",&,N,V_inputFocusedDisparity
recipeKey
flavorKey
stabilizedCropRect
pasteKeysForMediaType:
T@"NSDictionary",C,N
T@"NSString",C,N
initWithSettings:inputs:
inputs
resolvedNodeWithCachedInputs:settings:pipelineState:error:
evaluationMode
nodeByReplayingAgainstCache:pipelineState:error:
crossfadeDuration
loopTimeRange
input
outputVideo:
videoFrames
setVideoFrames:
setRawTime:
initWithFilterName:settings:inputs:
nodeFromCache:cache:
setEvaluatedForMode:
errorWithCode:reason:object:
firstEnabledVideoTrackInAsset:error:
tracksWithMediaType:
addMutableTrackWithMediaType:preferredTrackID:
insertTimeRange:ofTrack:atTime:error:
debugDescriptionOfAssetTrack:
timeRange
conformRange:inRange:
outputVideoComposition:
instructions
setTimeRange:
trackID
numberWithInt:
setRequiredSourceTrackIDs:
setSourceIdentifier:forTrackID:
frameDuration
setFrameDuration:
renderSize
setRenderSize:
setInstructions:
setSourceTrackIDForFrameTiming:
audioMix
audioMixInputParametersWithTrack:
setVolume:atTime:
setVolumeRampFromStartVolume:toEndVolume:timeRange:
setInputParameters:
initWithInput:timeRange:crossfadeDuration:startTime:
shouldCacheNodeForPipelineState:
_evaluateVideo:
requiresVideoComposition
_evaluateVideoComposition:
requiresAudioMix
_evaluateAudioMix:
_crossfadeDuration
_loopTimeRange
T{?=qiIq},R,N,V_startTime
T{?={?=qiIq}{?=qiIq}},R,N,V_loopTimeRange
T{?=qiIq},R,N,V_crossfadeDuration
timeKey
longLongValue
scaleKey
numberWithLongLong:
keyFrameTime
setKeyFrameTime:
radiusKey
falloffKey
radius
setRadius:
falloff
setFalloff:
identifier
name
inputKeys
settingForKey:
canBeEnabled
enabledKey
autoKey
hasAutoKeyInSchema
canHaveAuto
_setPrimitiveValue:forKey:
setValue:forUndefinedKey:
_primitiveValueForKey:
values
setFromAdjustment:
isEqualToAdjustmentController:
visualInputKeys
isEqual:forKeys:
defaultValue
autoKeysForPaste
displayName
displayInputKeys
enabled
setEnabled:
setIsAuto:
valueForUndefinedKey:
valuesForArrayInputKey:
interpolateFromStart:toEnd:progress:
timeFromInputKey:timescaleKey:
pasteAdjustment:forMediaType:
_changes
_identifier
_adjustment
T@"NSDictionary",R,N
TB,N
T@"NUIdentifier",&,N,V_identifier
T@"NUAdjustment",R,N,V_adjustment
TB,R,N
outputForegroundImage
outputBackgroundImage
outputMatteImage
setVisibleFrame:
renderScale
setRenderScale:
localLightData
setLocalLightData:
cache
setCache:
inputForegroundImage
setInputForegroundImage:
inputBackgroundImage
setInputBackgroundImage:
inputGuideImage
setInputGuideImage:
_renderScale
_localLightData
_cache
_inputForegroundImage
_inputBackgroundImage
_inputGuideImage
_visibleFrame
T{CGRect={CGPoint=dd}{CGSize=dd}},N,V_visibleFrame
Td,N,V_renderScale
T@"NSDictionary",C,N,V_localLightData
T@"<PIParallaxFilterCache>",&,N,V_cache
T@"CIImage",&,N,V_inputForegroundImage
T@"CIImage",&,N,V_inputBackgroundImage
T@"CIImage",&,N,V_inputGuideImage
T@"CIImage",R,N
clockAreaLuminance
initWithClockAreaLuminance:
_clockAreaLuminance
Td,R,N,V_clockAreaLuminance
clockMaterialRequest
layerStack
calculateClockLuminanceValuesForLayerStack:renderer:error:
setLuminanceValue:
luminanceValue
_luminanceValue
T@"NSNumber",&,N,V_luminanceValue
whiteColor
initWithLayerStack:
setLayerStack:
_layerStack
T@"PFParallaxLayerStack",&,N,V_layerStack
majorVersion
minorVersion
addBytes:length:
nu_digest
nu_updateDigest:
CIImageProcessorDigestObject
inputForKey:
_prewarmPortraitRendererWithPipelineState:error:
scale
_targetScaleForScale:
setScale:
sampleMode
initWithTargetScale:effectiveScale:sampleMode:input:
outputImageGeometry:
initWithExtent:renderScale:orientation:
_portraitQualityForRenderScale:
scaledSize
debugMode
globalMetadata
prewarmRendererForDevice:colorSize:disparitySize:quality:debugMode:globalRenderingMetadata:
videoProperties:
metadata
value
deserializeMetadataWithType:fromGlobalMetadata:error:
videoMetadataSamples
renderingMetadataIdentifier
outputTimedMetadataSampleWithIdentifier:atTime:error:
metadataGroup
initWithMetadataGroup:majorVersion:minorVersion:error:
cinematicAllowYUVSourceInput
mutableCopy
colorProperties
pixelBuffer
outputImage:
timedMetadata
sourceTransferFunction
renderTime
disparityKeyframes
keyframeInArray:closestToTime:
apertureKeyframes
renderQuality
useSourceBuffersDirectly
_sourceBufferFromInput:error:
initWithCVPixelBuffer:
initWithInput:disparityInput:disparityKeyframes:apertureKeyframes:debugMode:
uniqueInputNode
_evaluateImage:
T{?=qiIq},R,N
Ti,R,N
T@"PTGlobalRenderingMetadata",R,N
T@"PIPortraitVideoMetadataSample",R,N
applyOrientationFilter
submitGenericSynchronous:
faceRequestWithRequest:
submit:response:
submitSynchronous:error:
_group
_queue
_result
supportedIdentifiers
_recipesForIdentifiers:withURLProvider:
initWithBaseURL:
urlForIdentifier:
path
isReadableFileAtPath:
unarchivedStyleRecipeWithURL:error:
recipeForIdentifier:
_bundle
baseURL
setBaseURL:
_baseURL
T@"NSURL",&,N,V_baseURL
sourceSelectionKey
sourceSelectionForString:
stringForSourceSelection:
initWithClockColor:colorSuggestions:
initWithRecipe:
styleWithColorAnalysis:
kind
parameters
colorSuggestions
styleWithParameters:colorSuggestions:
bakedStyle
initWithKind:parameters:colorSuggestions:
recipeIdentifier
isSegmentedStyle:
_filterForRecipeIdentifier:
setParameters:
inactiveRecipeIdentifier
primaryColor
isWarm
isCool
colors
defaultStyleForKind:colorAnalysis:
styleWithBakedStyle:
colorPaletteWithStyleKind:
clockFont
updateClockPropertiesWithClockAreaLuminance:
isSegmented
hasTonalityMode
hasColorParameter
filter
inactiveFilter
colorSuggestionForCategory:
configureForCategory:
defaultDominantColorWithAnalysis:
clockColor
setClockColor:
setColorSuggestions:
_colorSuggestions
_clockColor
T@"NSArray",&,N,V_colorSuggestions
T@"NSString",R,C,N
T@"PIParallaxStyleRecipe",R,N
T@"PFParallaxColor",&,N,V_clockColor
backgroundLuminance
setTonality:
tonality
Tq,N,Vtonality
parallaxStyleKeyLevelOverride
initWithColorAnalysis:
backgroundColors
suggestedColorsForColors:fromColorPalette:
initWithBackgroundColor:clockColor:colorSuggestions:
color
setColor:
lowKeyTone
neutralTone
highKeyTone
T@"PFParallaxColor",&,N
Td,R
_color
T@"PFParallaxColor",&,N,V_color
luminance
foregroundLuminance
addRuleWithHueMin:hueMax:suggestion:
parallaxStyleAvoidColorWashBrownOverride
initWithColor:clockColor:suggestions:
secondaryColor
initWithPrimaryColor:secondaryColor:clockColor:colorSuggestions:
setPrimaryColor:
setSecondaryColor:
_primaryColor
_secondaryColor
T@"PFParallaxColor",&,N,V_primaryColor
T@"PFParallaxColor",&,N,V_secondaryColor
type
redValue
greenValue
blueValue
modeValue
initWithIdentifier:recipe:
initWithTime:value:
initWithDictionaryRepresentation:
dictionaryRepresentation
_value
_time
labels
appendString:
appendFormat:
allocWithZone:
initWithType:source:identifier:confidence:
encodeInteger:forKey:
encodeDouble:forKey:
bounds
encodeObject:forKey:
expandedBounds
decodeIntegerForKey:
decodeDoubleForKey:
decodeObjectForKey:
stringValue
supportsSecureCoding
encodeWithCoder:
initWithCoder:
setBounds:
isHuman
isAnimal
setExpandedBounds:
edgeBleed
setEdgeBleed:
_type
_confidence
_source
_edgeBleed
_bounds
_expandedBounds
Tq,R,N,V_type
Tq,R,N,V_identifier
Td,R,N,V_confidence
Tq,R,N,V_source
T{CGRect={CGPoint=dd}{CGSize=dd}},N,V_bounds
T{CGRect={CGPoint=dd}{CGSize=dd}},N,V_expandedBounds
Tq,N,V_edgeBleed
componentsSeparatedByString:
resource
setResource:
setComposition:
setClassification:
segmentationConfidenceMap
setSegmentationConfidenceMap:
segmentationBackground
setSegmentationBackground:
regions
setRegions:
setOriginalLayout:
defaultLayout
setDefaultLayout:
scores
setScores:
colorAnalysis
setColorAnalysis:
_availableStyles
_populateAvailableStyles
gatingResultForSegmentationScores:
set_availableStyles:
contextInfo
availableStyles
_defaultStyles
_populateDefaultStyles
set_defaultStyles:
originalStyle
createDirectoryAtURL:withIntermediateDirectories:attributes:error:
saveAssetResourceToURL:error:
saveSegmentationDataToURL:error:
loadFromArchiveURL:error:
loadSegmentationDataFromURL:error:
setFileURL:
archiveURL
copyItemAtURL:toURL:error:
saveToArchiveURL:error:
segmentationDataURL
initWithArchiveURL:
setCompression:
openForWriting:
contentsDictionary
dataWithPropertyList:format:options:error:
encodeData:filename:error:
dataForImageBuffer:error:
close:
setSegmentationDataURL:
openForReading:
decodeData:filename:error:
propertyListWithData:options:format:error:
loadContentsFromDictionary:hasMatte:hasBackground:error:
imageBufferFromData:error:
version
dictionaryWithLayout:
systemName
systemVersion
systemBuildVersion
sourceMode
dictionaryFromLayoutConfiguration:
segmentationDisabled
setVersion:
setSystemName:
setSystemVersion:
setSystemBuildVersion:
setSourceMode:
setSegmentationDisabled:
layoutConfigurationFromDictionary:error:
layoutWithDictionary:
loadFromContentsDictionary:error:
styleWithDictionary:error:
setContextInfo:
writeImageBuffer:toURL:error:
readImageBufferFromURL:error:
availableStyleOfKind:
suggestedStyleForCategory:
defaultStyleOfKind:
defaultStyles
fileURL
supportsManualClockIntersectionTolerance
T@"NUComposition",R,N
T@"<PFParallaxAssetRegions>",R,N
T@"PFParallaxLayout",R,N
T@"<PFParallaxLayoutConfiguration>",R,N
T@"NSDictionary",R,C,N
T@"PIParallaxColorAnalysis",R,N
T@"PFParallaxLayerStyle",R,N
T@"NSURL",R,N
isComplete
saveToURL:error:
loadFromURL:error:
loadingState
setLoadingState:
_resource
_classification
_segmentationConfidenceMap
_segmentationBackground
_regions
_defaultLayout
_originalLayout
_scores
_colorAnalysis
_loadingState
__availableStyles
__defaultStyles
_fileURL
_segmentationDataURL
_contextInfo
T@"NSArray",C,N,V__availableStyles
T@"NSArray",C,N,V__defaultStyles
T@"NSURL",&,N,V_fileURL
T@"NSURL",&,N,V_segmentationDataURL
T@"PISegmentationContextInfo",&,N,V_contextInfo
T@"PFParallaxAssetResource",&,N,V_resource
T@"NUComposition",&,N,V_composition
TQ,N,V_classification
T@"<NUImageBuffer>",&,N,V_segmentationConfidenceMap
T@"<NUImageBuffer>",&,N,V_segmentationBackground
T@"<PFParallaxAssetRegions>",&,N,V_regions
T@"PFParallaxLayout",&,N,V_defaultLayout
T@"PFParallaxLayout",&,N,V_originalLayout
T@"NSDictionary",C,N,V_scores
T@"PIParallaxColorAnalysis",&,N,V_colorAnalysis
TQ,N,V_loadingState
currentContextInfo
_segmentationDisabled
_systemName
_systemVersion
_systemBuildVersion
_version
_sourceMode
T@"NSString",C,N,V_systemName
T@"NSString",C,N,V_systemVersion
T@"NSString",C,N,V_systemBuildVersion
TQ,N,V_version
Tq,N,V_sourceMode
TB,N,V_segmentationDisabled
_keyframesForKey:class:
_setKeyframes:forKey:
unsignedIntValue
removeObject:
cinematographyState
changesDictionaryTrimmedByTimeRange:
setCinematographyState:
setDisparityKeyframes:
aperture
setDebugMode:
renderingVersionAtCapture
setRenderingVersionAtCapture:
trimToTimeRange:usingScript:
T@"NSArray",C,N
T@"NSNumber",&,N
T@"NSDictionary",&,N
TQ,N
resetAndEvaluateWithInitialState:
state
constants
setStateObject:forKey:
ruleWithBlockPredicate:action:
addRule:
suggestedColorForColor:
warmColor
coolColor
system
_system
T@"NURuleSystem",R,N,V_system
initWithTime:homography:
keyframesFromDictionaryRepresentations:
homography
_homography
T{?=qiIq},R,N,V_time
T{?=[3]},R,N,V_homography
initWithCount:times:values:
interpolation
sampleAtTime:
initWithKeyframeArray:
homographyAtTime:
sparseSequence
_homographySequence
medianLuminance
dominantColors
setMedianLuminance:
setDominantColors:
_medianLuminance
_dominantColors
Td,N,V_medianLuminance
T@"NSArray",C,N,V_dominantColors
imageHistogram
median
smoothWithFunction:windowSize:sampleMode:
mode
medianColor
dominantColor
destination
setDestination:
image
setImage:
task
setTask:
setImageHistogram:
luminanceWeights
setLuminanceWeights:
luminanceThresholds
setLuminanceThresholds:
_destination
_image
_task
_imageHistogram
_luminanceWeights
_luminanceThresholds
T@"NSString",&,N,V_identifier
T@"<NUPurgeableStorage>",&,N,V_destination
T@"CIImage",&,N,V_image
T@"CIRenderTask",&,N,V_task
T@"NUImageHistogram",&,N,V_imageHistogram
T{?=ffff},N,V_luminanceWeights
T{?=ffff},N,V_luminanceThresholds
T@"PFParallaxColor",R,N
colorAnalysisRequest
analyzeBackground
backgroundForImage:matte:
normalizedClipRect
setImageRect:
loadFilterWithName:
normalizeHueChromaImage:
setHueChromaImage:
sRGBLinearColorSpace
chromaThreshold
hueChromaImage
_beginRenderingImage:colorSpace:format:error:
_waitForRenderResources:
_computeAllHistograms:
sampleCount
setAlphaCount:
maxDominantColors
threshold:
modalityAnalysisWithLimit:sampleMode:
setDominantHues:
dominantHues
setDominantGrays:
dominantGrays
_purgeRenderResources
_beginRenderNormalizedHueChromaImage:targetHue:hueRange:chromaMin:error:
_beginRenderNormalizedHueChromaImage:targetGray:grayRange:chromaMax:error:
imageRect
newStorageWithSize:format:
renderImage:into:colorSpace:roi:imageSize:alpha:error:
denormalizeHueChromaImage:
computeHistogramFromBuffer:error:
readBufferInRegion:block:
sortUsingComparator:
alphaCount
dominanceThreshold
returnStorage:
removeAllObjects
initWithParallaxColorAnalysisRequest:
_storagePool
_renderResources
_hueChromaImage
_alphaCount
_dominantHues
_dominantGrays
_imageRect
T@"PIParallaxColorAnalysisRequest",R,N
T{?={?=qq}{?=qq}},N,V_imageRect
T@"CIImage",&,N,V_hueChromaImage
Tq,N,V_alphaCount
T@"NSArray",C,N,V_dominantHues
T@"NSArray",C,N,V_dominantGrays
setMaxDominantColors:
setDominanceThreshold:
setChromaThreshold:
setAnalyzeBackground:
setNormalizedClipRect:
_analyzeBackground
_maxDominantColors
_dominanceThreshold
_chromaThreshold
_normalizedClipRect
TB,N,V_analyzeBackground
T{CGRect={CGPoint=dd}{CGSize=dd}},N,V_normalizedClipRect
Tq,N,V_maxDominantColors
Td,N,V_dominanceThreshold
Td,N,V_chromaThreshold
kernelsDictionaryWithString:
hueChromaKernels
scaleHueKernel
vectorWithX:Y:
inputHueTarget
inputHueRange
inputChromaMin
inputHueIsNormalized
filterHueKernel
setInputHueTarget:
setInputHueRange:
setInputChromaMin:
setInputHueIsNormalized:
_inputHueTarget
_inputHueRange
_inputChromaMin
_inputHueIsNormalized
T@"NSNumber",&,N,V_inputHueTarget
T@"NSNumber",&,N,V_inputHueRange
T@"NSNumber",&,N,V_inputChromaMin
T@"NSNumber",&,N,V_inputHueIsNormalized
inputLumaTarget
inputLumaRange
inputChromaMax
vectorWithX:Y:Z:
filterLumaKernel
setInputLumaTarget:
setInputLumaRange:
setInputChromaMax:
_inputLumaTarget
_inputLumaRange
_inputChromaMax
T@"NSNumber",&,N,V_inputLumaTarget
T@"NSNumber",&,N,V_inputLumaRange
T@"NSNumber",&,N,V_inputChromaMax
RGBValues
foregroundColors
setForegroundLuminance:
setBackgroundLuminance:
colorsFromDictionary:key:error:
setForegroundColors:
setBackgroundColors:
setLuminance:
setColors:
colorWithRGBValues:error:
currentVersion
_luminance
_foregroundLuminance
_backgroundLuminance
_colors
_foregroundColors
_backgroundColors
Tq,N,V_version
Td,N,V_luminance
Td,N,V_foregroundLuminance
Td,N,V_backgroundLuminance
T@"NSArray",C,N,V_colors
T@"NSArray",C,N,V_foregroundColors
T@"NSArray",C,N,V_backgroundColors
stopAtTagFilter:
properties
_computeCleanAperture:
kindKey
versionKey
setKind:
removeAssetIdentifierFromMetadataArray:
metadataItem
setValue:
indexOfObjectPassingTest:
removeObjectAtIndex:
addAssetIdentifier:toMetadataDictionary:
addAssetIdentifier:toMetadataArray:
inputTemperature
setInputTemperature:
inputTint
setInputTint:
setInputVectorsForFilter:
temperature
setTemperature:
tint
setTint:
_inputTemperature
_inputTint
Td,N,V_inputTemperature
Td,N,V_inputTint
rawTime
nu_evaluateWithPipelineState:error:
_sampleMode
_rawTime
T{?=qiIq},N,V_time
T{?=qiIq},N,V_rawTime
Tq,N,V_sampleMode
T@"NSDictionary",R
T@"NSDictionary",&,V_recipe
renderNode
finalize
asset:
cacheKey
analysisRequest
videoSource
setVideoSource:
_videoSource
T@"AVAsset",&,N,V_videoSource
T@"NSDictionary",&,N,V_recipe
shouldCancelHandler
ICShouldBeCanceled
ICReportProgress:
rangeMin
setRangeMin:
rangeMax
setRangeMax:
setShouldCancelHandler:
_rangeMin
_rangeMax
_shouldCancelHandler
Td,N,V_rangeMin
Td,N,V_rangeMax
T@?,C,N,V_shouldCancelHandler
keyframes
stabCropRect
analysisType
rawHomographies
T{?={?=qq}{?=qq}},R,N
initWithKeyframes:stabCropRect:analysisType:rawHomographies:
_keyframes
_analysisType
_rawHomographies
_stabCropRect
T@"NSArray",R,C,N,V_keyframes
T{?={?=qq}{?=qq}},R,N,V_stabCropRect
TQ,R,N,V_analysisType
T@"NSDictionary",R,N,V_rawHomographies
cleanApertureOfTrack:oriented:
isCanceled
allowedAnalysisTypes
canceledError:object:
nominalFrameRate
allowedCropFraction
setAllowedAnalysisTypes:
setAllowedCropFraction:
_allowedAnalysisTypes
_allowedCropFraction
TQ,N,V_allowedAnalysisTypes
Td,N,V_allowedCropFraction
canProvideMetadataForAVAsset:
canPerformGyroBasedStabilizationForAsset:
initWithAVAsset:
timedMetadataArray
trajectoryHomography
calculateWithRequest:completion:
calculateRAWWithRequest:completion:
rawProperties
initWithRequest:dataExtractor:options:
initWithDictionary:
minimumValue
maximumValue
faceBalanceResultFromFaceObservations:request:error:
faces
addRect:
rawFaceBalanceFilter
initWithRect:
setRegionPolicy:
setResponseQueue:
faceRectFromNormalizedFaceRet:forImageExtent:scaleX:scaleY:
faceBalanceFromFaceImage:forFaceRect:
ARGB8
isEqualToPixelFormat:
RGBA8
validRegion
buffer
frameRect
bytesAtPoint:
readBufferRegion:withBlock:
initWithRequest:isRAW:
rawState
_rawState
Tq,R,V_rawState
getValue:
pi_grayColorResultValue
pi_valueWithGrayColorResult:
T{?={?=[4d]}{?=[4d]}d},R
RGBResultValue
valueWithRGBResult:
T{?=[4d]},R
initWithComposition:useSushi:
_chooseNeutralGrayForNonSushi:
calculateColorWithProperties:completion:
_chooseTempTintForSushi:RAWProperties:brightness:
_correctedRGBResultFromResult:
_useTempTint:
inputNeutralXYFromRGB:
initWithName:responseQueue:
bufferFactory
RGBAf
bytes
rowBytes
writeBufferInRegion:block:
_brightnessMultiplierFromImageProperties:
begin
_computeGreenPercentage:
_submitGERenderRequest:
_submitGWRenderRequest:
rawCameraSpaceProperties
_whitePointColorFromGrayEdgesImage:grayWorldImage:greenChannelPercentage:RAWCameraSpaceProperties:
commitAndNotifyOnQueue:withBlock:
whiteValue
whiteFactor
_computeWhitePointColorWithGrayEdgesBuffer:grayWorldBuffer:greenChannelPercentage:RAWCameraSpaceProperties:
readBufferFromImage:withRGBAfBufferBlock:
RGBAh
genericRGBLinearColorSpace
setTileSize:
rawSourceFilterIncludingOrientation
sushiLevel1Filter
initWithComposition:dataExtractor:options:
_configureRequest:
submitRequest:completion:
initWithSource:
initWithScript:
_bufferRenderClient
_imageDataClient
_useSushi
definitionKernel
inputBlurImage
setInputBlurImage:
inputIntensity
setInputIntensity:
_inputBlurImage
_inputIntensity
T@"CIImage",&,V_inputBlurImage
T@"NSNumber",&,V_inputIntensity
strengthKey
neutralKey
toneKey
grainKey
hueKey
inputStrengthKey
inputNeutralGammaKey
inputToneKey
inputHueKey
inputGrainKey
inputSeedKey
attributeStrengthKey
attributeNeutralGammaKey
attributeToneKey
attributeHueKey
attributeGrainKey
setStrength:
strength
setNeutral:
neutral
setTone:
setGrain:
grain
setHue:
metadataItemsWithMetadataType:value:error:
writeMetadataType:value:toCGImageProperties:error:
readMetadataType:fromCGImageProperties:value:error:
videoMetadataForVariation:error:
setImageVariation:properties:error:
photoProcessingFlagsFromProperties:error:
setPhotoProcessingFlags:properties:error:
photoFeatureFlags:error:
setPhotoFeatureFlags:properties:error:
setRequest:
inputSize
setInputSize:
_request
_inputSize
T@"NUImageExportRequest",&,V_request
T{?=qq},V_inputSize
companionImageData
setCompanionImageData:
companionVideoURL
setCompanionVideoURL:
auxiliaryImages
setAuxiliaryImages:
canPropagateOriginalAuxiliaryData
setCanPropagateOriginalAuxiliaryData:
setProperties:
_canPropagateOriginalAuxiliaryData
_companionImageData
_companionVideoURL
_auxiliaryImages
_properties
T@"NSDictionary",&,V_auxiliaryImages
TB,V_canPropagateOriginalAuxiliaryData
T@"NSDictionary",C,V_properties
T{?=qq},D
T@"NSData",&,V_companionImageData
T@"NSURL",&,V_companionVideoURL
setGeometry:
digest
setDigest:
_geometry
_digest
T@"NUImageGeometry",&,V_geometry
T@"NSString",C,V_digest
T@"NSData",&,V_data
initWithLevel:
displayP3ColorSpace
priority
setPriority:
pairingIdentifier
setPairingIdentifier:
_priority
_pairingIdentifier
T@"NUPriority",&,V_priority
T@"NUColorSpace",&,V_colorSpace
T@"NSString",C,V_pairingIdentifier
T@"<NUScalePolicy>",&,V_scalePolicy
videoCodecType
bypassOutputSettingsIfNoComposition
applyVideoOrientationAsMetadata
requireHardwareEncoder
metadataProcessor
setMetadataProcessor:
increaseBitRateIfNecessary
setIncreaseBitRateIfNecessary:
setVideoCodecType:
preserveSourceColorSpace
setPreserveSourceColorSpace:
setBypassOutputSettingsIfNoComposition:
setApplyVideoOrientationAsMetadata:
setRequireHardwareEncoder:
includeCinematicVideoTracks
setIncludeCinematicVideoTracks:
computeDigest
setComputeDigest:
_increaseBitRateIfNecessary
_preserveSourceColorSpace
_bypassOutputSettingsIfNoComposition
_applyVideoOrientationAsMetadata
_requireHardwareEncoder
_includeCinematicVideoTracks
_computeDigest
_metadataProcessor
_videoCodecType
T@?,C,V_metadataProcessor
TB,N,V_increaseBitRateIfNecessary
T@"NSString",C,N,V_videoCodecType
TB,N,V_preserveSourceColorSpace
TB,N,V_bypassOutputSettingsIfNoComposition
TB,N,V_applyVideoOrientationAsMetadata
TB,N,V_requireHardwareEncoder
TB,N,V_includeCinematicVideoTracks
TB,N,V_computeDigest
pathExtension
typeWithFilenameExtension:
fileType
defaultFormatForURL:
JPEGCompressionQuality
setCompressionQuality:
imageExportFormatForURL:
imageExportFormat
setImageExportFormat:
setJPEGCompressionQuality:
optimizeForSharing
setOptimizeForSharing:
applyImageOrientationAsMetadata
setApplyImageOrientationAsMetadata:
optimizeForBackgroundProcessing
setOptimizeForBackgroundProcessing:
_optimizeForSharing
_applyImageOrientationAsMetadata
_optimizeForBackgroundProcessing
_imageExportFormat
_JPEGCompressionQuality
T@"NUImageExportFormat",C,V_imageExportFormat
Td,V_JPEGCompressionQuality
TB,V_optimizeForSharing
TB,V_applyImageOrientationAsMetadata
TB,V_optimizeForBackgroundProcessing
primaryURL
setPrimaryURL:
videoComplementURL
setVideoComplementURL:
videoPosterFrameURL
setVideoPosterFrameURL:
renderCompanionResources
setRenderCompanionResources:
reframeCropAdjustment
setReframeCropAdjustment:
reframeVideoAdjustment
setReframeVideoAdjustment:
_renderCompanionResources
_primaryURL
_videoComplementURL
_videoPosterFrameURL
_reframeCropAdjustment
_reframeVideoAdjustment
T@"NSURL",&,V_primaryURL
T@"NSURL",&,V_videoComplementURL
T@"NSURL",&,V_videoPosterFrameURL
TB,V_renderCompanionResources
T@"NUAdjustment",&,V_reframeCropAdjustment
T@"NUAdjustment",&,V_reframeVideoAdjustment
TB,V_applyVideoOrientationAsMetadata
oneShotPortraitV2ExportFilter
setApplyOrientationAsMetadata:
_lowMemoryModeSupportedForComposition:
setLowMemoryModeEnabled:
setDestinationURL:
setFormat:
prepareImageExportRequest:options:completion:
setRenderToData:
orientationAsMetaDataFilter
destinationData
discreteProgressWithTotalUnitCount:
addVideoProperties:composition:options:error:
initWithProperties:
setMetadata:
_exportVideoToURL:composition:options:properties:progress:completion:
exportComposition:options:completionQueue:completion:
UUID
UUIDString
conformsToType:
exportImageToURL:composition:options:completion:
exportVideoToURL:composition:options:completion:
adjustmentConstants
PICropAdjustmentKey
PIVideoReframeAdjustmentKey
exportImageToDataWithComposition:options:completion:
URLForDirectory:inDomain:appropriateForURL:create:error:
lastPathComponent
stringByDeletingPathExtension
stringByAppendingPathExtension:
URLByAppendingPathComponent:isDirectory:
resetImageProperties:preserveRegions:
addImageProperties:composition:options:error:
unknownError:object:
disableIOSurfacePortaitExport
setImageProperties:
setAuxImages:
setRenderWithIOSurface:
prepareAuxiliaryImagesFetchProperties:options:completion:
auxiliaryImagesProperties
setAuxiliaryImageType:
auxiliaryImage
setObject:forKey:
removeObjectForKey:
autoLoopAdjustmentController
metadataConverter
variationForFlavor:
depthAdjustmentController
semanticEnhanceAdjustmentController
livePhotoKeyFrameAdjustmentController
portraitAdjustmentController
orientationAdjustmentController
cropAdjustmentController
isOriginalCrop
setBitRateMultiplicationFactor:
setOutputSettings:
submitWithProgress:completion:
setMetadataConverter:
T@"<PICompositionExporterMetadataConverter>",&
exportComposition:toPrimaryURL:videoComplementURL:videoPosterFrameURL:priority:completionQueue:completion:
bilateralKernels
RGBToLabKernels
objectAtIndex:
boundsForPointArray:
enlargedBounds:withPoints:
shapeWithRect:
unionWith:
bilateralAddROI:destRect:userInfo:
bilateralAdd1Kernel
applyWithExtent:roiCallback:arguments:
bilateralAdd2Kernel
bilateralAdd3Kernel
bilateralAdd4Kernel
bilateralAdd5Kernel
bilateralAdd6Kernel
bilateralAdd7Kernel
bilateralAdd8Kernel
bilateralAdd9Kernel
samplesPerPass
samplerWithImage:options:
RGBToLabKernel
subarrayWithRange:
doBilateralPass:points:weights:sums:slope:
bilateralFinalizeKernel
LabToRGBKernel
inputPoints
setInputPoints:
inputWeights
setInputWeights:
inputEdgeDetail
setInputEdgeDetail:
inputVersion
setInputVersion:
_inputPoints
_inputWeights
_inputEdgeDetail
_inputVersion
T@"NSArray",&,V_inputPoints
T@"NSArray",&,V_inputWeights
T@"NSNumber",&,V_inputEdgeDetail
T@"NSNumber",&,V_inputVersion
BWBilateralKernels
bilateralROI:destRect:userInfo:
bilateralLoop2Kernel
bilateralLoop5Kernel
bilateralLoop11Kernel
doBilateralLoop:points:weights:slope:
inputBorder
setInputBorder:
_inputBorder
T@"NSNumber",&,V_inputBorder
inputRadius
setInputRadius:
_inputRadius
T@"NSNumber",&,V_inputRadius
initWithMajor:minor:subMinor:
initWithMajor:minor:subMinor:platform:
decimalDigitCharacterSet
invertedSet
length
rangeOfCharacterFromSet:
asOrderedInteger
string
isEqualToAdjustmentVersion:
caseInsensitiveCompare:
stringByAppendingFormat:
versionWithMajor:minor:subMinor:platform:
versionFromString:
compare:
subMinorVersion
platform
_majorVersion
_minorVersion
_subMinorVersion
_platform
T@"NSString",R,W,N
TQ,R,N,V_majorVersion
TQ,R,N,V_minorVersion
TQ,R,N,V_subMinorVersion
T@"NSString",R,C,N,V_platform
amountKey
smartColorHDRStatistics
_isIdentity
_kernelV_lt1
_kernelV_gt1
_kernelCPos
_kernelCNeg
_kernelCast
inputVibrancy
setInputVibrancy:
inputContrast
setInputContrast:
inputCast
setInputCast:
T@"NSNumber",&,N,VinputVibrancy
T@"NSNumber",&,N,VinputContrast
T@"NSNumber",&,N,VinputCast
dataWithLength:
mutableBytes
render:toBitmap:rowBytes:bounds:format:colorSpace:
setCountLimit:
inputTargetImage
inputAlgorithm
autoEnhanceCache
autoEnhanceFiltersForImage:algorithm:
apertureAutoEnhanceFiltersForImage:
photosAutoEnhanceFiltersForImage:
colorNormalizationFiltersForImage:
autoAdjustmentFiltersWithOptions:
extractDataToDictionary:dataExtractor:options:context:colorSpace:error:
smartToneAdjustmentsForValue:localLightAutoValue:andStatistics:
smartColorAdjustmentsForValue:andStatistics:
setInputTargetImage:
setInputAlgorithm:
_inputTargetImage
_inputAlgorithm
T@"CIImage",&,N,V_inputTargetImage
T@"NSString",&,N,V_inputAlgorithm
T@"PFParallaxLayerStack",R,N
style
layerStackOptions
updateInactiveFrame
updateClockZPosition
updateClockAreaLuminance
_submit:
layoutByUpdatingClockLayerOrder:
layoutByUpdatingClockIntersection:
_responseWithLayerStack:
_submitClockMaterialRequestWithLayerStack:completion:
_submitLayerStackRequestsWithLayout:completion:
_submitClockOverlapRequestWithLayout:completion:
_submitInactiveLayoutRequest:
layers
layerStackByUpdatingLayers:
_submitLayerStackRequestForMode:layout:completion:
_recordResult:
layoutByUpdatingInactiveFrame:
_recordError:
layerStackByUpdatingClockAreaLuminance:
setStyle:
setLayerStackMode:
renderContext
cancelAllRequests
aggregateStatistics:
setStatistics:
cancel
setLayerStackOptions:
setUpdateInactiveFrame:
setUpdateClockZPosition:
setUpdateClockAreaLuminance:
_requests
_results
_error
_updateInactiveFrame
_updateClockZPosition
_updateClockAreaLuminance
_style
_layerStackOptions
T@"PIParallaxStyle",&,N,V_style
TQ,N,V_layerStackOptions
TB,N,V_updateInactiveFrame
TB,N,V_updateClockZPosition
TB,N,V_updateClockAreaLuminance
crossfadeDurationValueKey
crossfadeDurationTimescaleKey
startTimeValueKey
startTimeTimescaleKey
loopTimeRangeStartValueKey
loopTimeRangeStartTimescaleKey
loopTimeRangeDurationValueKey
loopTimeRangeDurationTimescaleKey
setLoopTimeRange:
setCrossfadeDuration:
T{?={?=qiIq}{?=qiIq}},N
thresholdImage:withThreshold:
dilateMask:withRadius:
colorThresholdFilter
setThreshold:
morphologyMaximumFilter
morphologyMinimumFilter
colorInvertFilter
upsampleMatteImage:guideImage:
upsampleBackgroundImage:toSize:
histogramForSegmentationMatteImage:
areaHistogramFilter
setExtent:
setCount:
initWithLength:
initWithBinCount:range:
write:
areaAverageFilter
imageByApplyingFilter:
multiplyCompositingFilter
openMask:withRadius:
timeOverlapCheckBottom
timeOverlapCheckTop
computeAlphaCoverageWithRect:foregroundImage:context:
timeOverlapCheckThresholdForTopRect:isInteractive:
clockIntersectionFromTopRectMatteCoverage:bottomRectMatteCoverage:
unsafeRect
scaleRect:scaleFactor:scaleCenter:
_computeHeadroomZoomFactorWithVisibleFrame:scaleCenter:initialOverlap:matte:layoutConfiguration:context:
_computeAvoidingRect:imageSize:maxYMotion:segmentationMatte:layoutConfiguration:context:
colorWithRed:green:blue:
configuration
bestCropRectV2ForParallaxClassification:layoutConfiguration:sourcePixelWidth:sourcePixelHeight:sourcePreferredCropRectNormalized:sourceAcceptableCropRectNormalized:sourceFaceAreaRectNormalized:outputCropScore:outputLayoutScore:outputClockOverlapAcceptable:
parallaxPadding
inactiveFrame
timeFrame
deviceResolution
segmentationDebugPreviewDisableClock
CGColor
colorWithCGColor:
imageWithColor:extent:
erodeMask:withRadius:
backgroundForImage:matte:infill:
tightCropFrameFromMatteImage:
histogramForSegmentationMatte:
matteHistogramIndicatesSubjectDetected:
bimodalScoreForHistogram:
localConfidenceScoreForLocalConfidenceImage:extent:context:
localConfidenceImage:
groundedScoreForSegmentationMatte:context:
computeClockLayerOrderWithVisibleFrame:foregroundImage:layoutConfiguration:context:
debugImageWithInputImage:layout:faceRects:saliencyPreferrectRect:saliencyAcceptableRect:
debugImageWithInputImage:finalLayout:intermediateLayout:faceRects:saliencyPreferrectRect:saliencyAcceptableRect:
debugPreviewRenderWithBackground:foreground:layout:style:
debugImageForColorAnalysis:inputImage:visibleFrame:
rowAverageFilter
framedRectImageWithCGRect:color:borderWidth:
textImageGeneratorFilter
setText:
setFontName:
setFontSize:
roundedRectangleGeneratorFilter
setWithObject:
rangeOfString:options:
setScaleFactor:
colorWithRed:green:blue:alpha:
vectorWithCGPoint:
portraitInfoKey
portraitInfo
spillMatteAllowedKey
setPortraitInfo:
canRenderPortraitEffect
defaultStrength
setSpillMatteAllowed:
spillMatteAllowed
T@"NSNumber",C,N
resourceUnavailableError:object:
_localLightDataForImage:
scaleForImageSize:
localLightStatisticsNoProxy
applyInactiveStyleToImage:error:
_noOpRemovalFunctions
noOpRemovalFunctions
copyOfAdjustmentRemovingNoOps:identifier:
valueKey
serializeRecipe:
outputStreamWithURL:append:
open
writePropertyList:toStream:format:options:error:
close
_serializeParameters:
foregroundFilters
_serializeFilters:
backgroundFilters
matteFilters
inputStreamWithURL:
propertyListWithStream:options:format:error:
deserializeRecipe:error:
_deserializeParameters:version:error:
_deserializeFilterDefinitions:version:error:
initWithParameters:foregroundFilters:backgroundFilters:matteFilters:
_serializeParameter:
numberValue
unit
alphaValue
xValue
yValue
variableName
_deserializeParameter:version:error:
initWithNumber:unit:
initWithRed:green:blue:alpha:
initWithX:Y:unit:
initWithMode:
initWithVariableName:
_serializeDefinition:
enumerateObjectsUsingBlock:
filterName
stackName
filters
_deserializeFilterDefinition:version:error:
initWithFilterName:parameters:
initWithStackName:filters:
writeRecipe:toURL:error:
valueWithIdentifier:inGroup:ofClass:
objectFromData:withMajorVersion:minorVersion:
setTimedMetadata:
_cameraInfoFromMetadataGroup:
items
metadataItemsFromArray:filteredByIdentifier:
unarchivedObjectOfClasses:fromData:error:
cameraInfo
focusedDisparity
_cameraInfo
_timedMetadata
_focusedDisparity
_aperture
T@"PTTimedRenderingMetadata",&,N,V_timedMetadata
Td,R,N,V_focusedDisparity
Td,R,N,V_aperture
T@"NSDictionary",R,N,V_cameraInfo
colorSize
disparitySize
quality
isInUse
initWithDevice:colorSize:disparitySize:quality:debugMode:
setInUse:
prepareToRenderWithMetadata:
setLastUseTime:
lastUseTime
isEqualToDate:
removeObjectIdenticalTo:
sensorID
hwModelID
initWithDevice:version:colorSize:disparitySize:
setDebugRendering:
setUseRGBA:
initWithDescriptor:
renderState:matchesMetadata:
createRenderStateWithQuality:
applyToRenderState:
_renderPipeline
_renderState
_inUse
_quality
_device
_debugMode
_lastUseTime
_colorSize
_disparitySize
T@"<MTLDevice>",R,N,V_device
T{?=qq},R,N,V_colorSize
T{?=qq},R,N,V_disparitySize
Ti,R,N,V_quality
Tq,R,N,V_debugMode
inUse
TB,N,GisInUse,V_inUse
T@"NSDate",&,N,V_lastUseTime
_highKeyHDR
debugDiagnostics
nonLocalizedFailureReason
userInfo
debugFilesEnabled
captureDebugDirectoryForComposition:
debugFilesPrefix
debugLineDetectionImage
PNGRepresentationOfImage:format:colorSpace:options:
writeToURL:atomically:
faceObservationCache
wrapAsUnexpectedError:
getSizeOfAllFaces:
maxFaceSize
addMethodDiagnostics:details:
lowercaseString
hasFrontFacingCameraDimentions:
disableOnPanos
disableOnFrontFacingCameraImages
isFrontFacingCameraImage:pixelSize:
passesImagePropertiesCheck:
addMethodResultToDiagnostics:error:setYawPitchError:
passesFaceCheck:
shouldRunBuildingCheck
passesBuildingCheck:
submitVerified:
writeDebugDiagnosticsToDisk
overcaptureSourceFilter
primarySourceFilter
primaryImageProperties:
overcaptureImageProperties:
isFusedOvercapture
initWithMasterImageSize:stitchedImageSize:
initWithMasterImageSize:
setCropRect:
angleSeedDegreesCCW
setRollAngle:constrainCropRectWithTargetArea:
cropRect
setAngle:
null
minimumConfidence
minimumPitchCorrection
minimumYawCorrection
minimumAngleCorrection
canGenerateNewCropRect:
defaultFocalLength
maxAutoPitch
maxAutoYaw
maxAutoAngle
minimumPitchCorrectionArea
minimumYawCorrectionArea
exifOrientationAndCropStraightenOnly
passesConfidenceCheck:error:
passesMinimumCorrectionCheck:error:
setDebugLineDetectionImage:
perspectiveErrorFromCoreImage:
undoOrientation:forPitch:yaw:angle:
globalSession
releaseCachedResources
requestVisionCleanUp
setFaceObservationCache:
T@"PIFaceObservationCache",&,N
setMaxAutoYaw:
setMaxAutoPitch:
setMaxAutoAngle:
setMinimumPitchCorrection:
setMinimumYawCorrection:
setMinimumAngleCorrection:
setMinimumConfidence:
setMaxFaceSize:
setMinimumPitchCorrectionArea:
setMinimumYawCorrectionArea:
setDisableOnPanos:
setDisableOnFrontFacingCameraImages:
setShouldRunBuildingCheck:
setAngleSeedDegreesCCW:
setDebugFilesEnabled:
setDebugFilesPrefix:
_disableOnPanos
_disableOnFrontFacingCameraImages
_shouldRunBuildingCheck
_debugFilesEnabled
_faceObservationCache
_maxAutoYaw
_maxAutoPitch
_maxAutoAngle
_minimumPitchCorrection
_minimumYawCorrection
_minimumAngleCorrection
_minimumConfidence
_maxFaceSize
_minimumPitchCorrectionArea
_minimumYawCorrectionArea
_angleSeedDegreesCCW
_debugFilesPrefix
_debugDiagnostics
_debugLineDetectionImage
T@"CIImage",&,N,V_debugLineDetectionImage
T@"NSNumber",C,V_maxAutoYaw
T@"NSNumber",C,V_maxAutoPitch
T@"NSNumber",C,V_maxAutoAngle
Td,V_minimumPitchCorrection
Td,V_minimumYawCorrection
Td,V_minimumAngleCorrection
Td,V_minimumConfidence
Td,V_maxFaceSize
Td,V_minimumPitchCorrectionArea
Td,V_minimumYawCorrectionArea
TB,V_disableOnPanos
TB,V_disableOnFrontFacingCameraImages
TB,V_shouldRunBuildingCheck
Td,V_angleSeedDegreesCCW
TB,V_debugFilesEnabled
T@"NSString",C,V_debugFilesPrefix
T@"NSMutableDictionary",R,V_debugDiagnostics
T@"PIFaceObservationCache",&,N,V_faceObservationCache
luminanceKey
shortValue
angleKey
pitchKey
yawKey
xOriginKey
yOriginKey
widthKey
heightKey
isCropIdentityForImageSize:
isCropConstrained
constraintWidthKey
constraintHeightKey
angle
pitch
smartKey
originalCropKey
setPitch:
setYaw:
isGeometryIdentityForImageSize:
constraintWidth
constraintHeight
angleRadians
pitchRadians
yawRadians
autoCropped
isSmart
setConstraintWidth:
setConstraintHeight:
setAngleRadians:
setPitchRadians:
setYawRadians:
setAutoCropped:
setSmart:
setOriginalCrop:
T{CGRect={CGPoint=dd}{CGSize=dd}},N
smart
TB,N,GisSmart
originalCrop
TB,N,GisOriginalCrop
setInputDecoderVersion:
inputDecoderVersion
_evaluateImageWithFilterDefinitions:inputImage:
initWithNumber:
resolvedParameters
setGuideImage:
setOutputImage:
setVisibleRect:
evaluateWithContext:error:
_parameters
T@"PIParallaxStyleRecipe",&,N,V_recipe
T@"NSDictionary",C,N,V_parameters
_updateSettingsWithInputLight:
overcaptureStatistics
inputExposureKey
inputContrastKey
inputBrightnessKey
inputShadowsKey
inputHighlightsKey
inputBlackKey
inputLocalLightKey
inputRawHighlightsKey
inputLightKey
offsetExposureKey
offsetBrightnessKey
offsetContrastKey
offsetShadowsKey
offsetHighlightsKey
offsetBlackKey
offsetLocalLightKey
statisticsKey
inputLight
overcaptureStatisticsKey
attributeBrightnessKey
attributeContrastKey
attributeExposureKey
attributeHighlightsKey
attributeShadowsKey
attributeBlackPointKey
attributeLocalLightKey
attributeLightMapKey
attributeLightMapWidthKey
attributeLightMapHeightKey
computedSettings
inputLightDefault
setInputLight:
setInputExposure:
inputExposure
setInputBrightness:
inputBrightness
setInputShadows:
inputShadows
setInputHighlights:
inputHighlights
setInputBlack:
inputBlack
setInputLocalLight:
inputLocalLight
setInputRawHighlights:
inputRawHighlights
setOvercaptureStatistics:
offsetBlack
setOffsetBlack:
offsetBrightness
setOffsetBrightness:
offsetContrast
setOffsetContrast:
offsetExposure
setOffsetExposure:
offsetHighlights
setOffsetHighlights:
offsetLocalLight
setOffsetLocalLight:
offsetShadows
setOffsetShadows:
_smartSettings
longExposureFusionKernels
alphaCompositingKernel
dynamismMapKernel
dynamismMapRefineKernel
rgbToLumaKernel
homographyKernel
nccKernel
nccCoarseKernel
blur7x7Kernel
blur5x5Kernel
blur3x3Kernel
fusionKernel
addEntriesFromDictionary:
dictionaryWithDictionary:
getResourceValue:forKey:error:
timeIntervalSinceReferenceDate
typeWithIdentifier:
absoluteString
assetIdentifierForURL:type:useEmbeddedPreview:
setAssetIdentifier:
initWithURL:UTI:
setUseEmbeddedPreview:
initWithSourceDefinitions:
setResolvedSourceDefinition:
initWithCIImage:orientation:
assetIdentifier
resolvedSourceDefinition
initWithImageSourceDefinition:videoSourceDefinition:
_imageRenderRequestWithComposition:wideGamut:
initWithTargetSize:
setExtentPolicy:
newCGImageFromBufferImage:
allValues
isMetalDeviceSupported:
supportsANE
isReadable
isPlayable
isExportable
videoAssetIsHighDynamicRange:
deviceSupportsHardware10BitHEVCEncoding
deviceSupportsHighDynamicRangeVideo
isAVAssetDolbyProfile5:error:
assetIsCinematicVideo:
isAssetUnsupportedLegacyPortraitVideo:
editSettings
areCPVAssetsEditable
capabilitiesForCurrentDevice
decodingSupportForAVAsset:
is3DEffect:
isPortraitEffect:
iosCropToolFilter
noGeometryFilter
pipelineFiltersForShowingOriginalWithGeometry
noCropFilter
resetTag:input:
getTagWithPath:error:
initWithScript:block:
forceGlassesMatteOff
setGlassesMatteAllowed:
forceSpillMatteOff
allowSpillMatteOnOlderPortraitV2Captures
PIAutoLoopAdjustmentKey
PIRedEyeAdjustmentKey
PIDepthAdjustmentKey
PIPortraitAdjustmentKey
PITrimAdjustmentKey
PIMuteAdjustmentKey
newAdjustmentWithIdentifier:
handlePIGlobalSettings:
PIOrientationAdjustmentKey
PIRetouchAdjustmentKey
PIHighResFusionAdjustmentKey
knownFormatsVersionsMap
updateCropAdjustment:after:error:
standardUserDefaults
boolForKey:
prepareForPerformingRequests:error:
availableDecoderVersions
lastObject
boostParametersFromRawProperties:
gainMapParametersFromRawProperties:
imageSourceWithURL:type:proxyImage:orientation:useEmbeddedPreview:
videoSourceWithURL:
livePhotoSourceWithPhotoSource:videoSource:
compositionByRemovingVideoAndLivePhotoAdjustments:
newImageRenderClientWithName:
imagePropertiesRequestWithComposition:
videoPropertiesRequestWithComposition:
imageRenderRequestWithComposition:fitInSize:wideGamut:
imageRenderRequestWithComposition:fillInSize:wideGamut:
priorityWithLevel:
videoRenderRequestWithComposition:fitInSize:
isPortraitStageEffect:
isAVAssetEditable:reason:
effectNameForFilterName:
filterNameForEffectName:
pipelineFiltersForCropping
pipelineFiltersForOriginalGeometry
pipelineFiltersForShowingOriginal
pipelineFiltersForRAWShowingOriginalWithGeometry
validatedCompositionCopyForComposition:mediaType:
updateCropAdjustmentController:after:error:
preheatEditDependencies
rawAdjustmentWithRawImageProperties:
allAdjustmentTypes
nonVisualAdjustmentTypes
PISmartToneAdjustmentKey
PISmartColorAdjustmentKey
PISmartBWAdjustmentKey
PIGrainAdjustmentKey
PIAutoEnhanceAdjustmentKey
PIWhiteBalanceAdjustmentKey
PIApertureRedEyeAdjustmentKey
PIVignetteAdjustmentKey
PISharpenAdjustmentKey
PINoiseReductionAdjustmentKey
PIDefinitionAdjustmentKey
PICurvesAdjustmentKey
PILevelsAdjustmentKey
PISelectiveColorAdjustmentKey
PIEffectAdjustmentKey
PIEffect3DAdjustmentKey
PISlomoAdjustmentKey
PILivePhotoKeyFrameAdjustmentKey
PIVideoPosterFrameAdjustmentKey
PIRawAdjustmentKey
PIRawNoiseReductionAdjustmentKey
PISemanticEnhanceAdjustmentKey
PISourceSelectAdjustmentKey
PIVideoStabilizeAdjustmentKey
PIVideoCrossfadeLoopAdjustmentKey
PIPortraitVideoAdjustmentKey
PISourceAdjustmentKey
PIOvercaptureSourceAdjustmentKey
_PISmartToneAdjustmentKey
_PISmartColorAdjustmentKey
_PISmartBWAdjustmentKey
_PIGrainAdjustmentKey
_PIAutoEnhanceAdjustmentKey
_PIWhiteBalanceAdjustmentKey
_PIRedEyeAdjustmentKey
_PIApertureRedEyeAdjustmentKey
_PIRetouchAdjustmentKey
_PIVignetteAdjustmentKey
_PISharpenAdjustmentKey
_PINoiseReductionAdjustmentKey
_PIDefinitionAdjustmentKey
_PICurvesAdjustmentKey
_PILevelsAdjustmentKey
_PISelectiveColorAdjustmentKey
_PIEffectAdjustmentKey
_PIEffect3DAdjustmentKey
_PICropAdjustmentKey
_PITrimAdjustmentKey
_PISlomoAdjustmentKey
_PILivePhotoKeyFrameAdjustmentKey
_PIVideoPosterFrameAdjustmentKey
_PIAutoLoopAdjustmentKey
_PIHighResFusionAdjustmentKey
_PIMuteAdjustmentKey
_PIDepthAdjustmentKey
_PIPortraitAdjustmentKey
_PIOrientationAdjustmentKey
_PIRawAdjustmentKey
_PIRawNoiseReductionAdjustmentKey
_PISemanticEnhanceAdjustmentKey
_PIVideoReframeAdjustmentKey
_PISourceSelectAdjustmentKey
_PIVideoStabilizeAdjustmentKey
_PIVideoCrossfadeLoopAdjustmentKey
_PIPortraitVideoAdjustmentKey
_PISourceAdjustmentKey
_PIOvercaptureSourceAdjustmentKey
T@"NSString",R,N,V_PISmartToneAdjustmentKey
T@"NSString",R,N,V_PISmartColorAdjustmentKey
T@"NSString",R,N,V_PISmartBWAdjustmentKey
T@"NSString",R,N,V_PIGrainAdjustmentKey
T@"NSString",R,N,V_PIAutoEnhanceAdjustmentKey
T@"NSString",R,N,V_PIWhiteBalanceAdjustmentKey
T@"NSString",R,N,V_PIRedEyeAdjustmentKey
T@"NSString",R,N,V_PIApertureRedEyeAdjustmentKey
T@"NSString",R,N,V_PIRetouchAdjustmentKey
T@"NSString",R,N,V_PIVignetteAdjustmentKey
T@"NSString",R,N,V_PISharpenAdjustmentKey
T@"NSString",R,N,V_PINoiseReductionAdjustmentKey
T@"NSString",R,N,V_PIDefinitionAdjustmentKey
T@"NSString",R,N,V_PICurvesAdjustmentKey
T@"NSString",R,N,V_PILevelsAdjustmentKey
T@"NSString",R,N,V_PISelectiveColorAdjustmentKey
T@"NSString",R,N,V_PIEffectAdjustmentKey
T@"NSString",R,N,V_PIEffect3DAdjustmentKey
T@"NSString",R,N,V_PICropAdjustmentKey
T@"NSString",R,N,V_PITrimAdjustmentKey
T@"NSString",R,N,V_PISlomoAdjustmentKey
T@"NSString",R,N,V_PILivePhotoKeyFrameAdjustmentKey
T@"NSString",R,N,V_PIVideoPosterFrameAdjustmentKey
T@"NSString",R,N,V_PIAutoLoopAdjustmentKey
T@"NSString",R,N,V_PIHighResFusionAdjustmentKey
T@"NSString",R,N,V_PIMuteAdjustmentKey
T@"NSString",R,N,V_PIDepthAdjustmentKey
T@"NSString",R,N,V_PIPortraitAdjustmentKey
T@"NSString",R,N,V_PIOrientationAdjustmentKey
T@"NSString",R,N,V_PIRawAdjustmentKey
T@"NSString",R,N,V_PIRawNoiseReductionAdjustmentKey
T@"NSString",R,N,V_PISemanticEnhanceAdjustmentKey
T@"NSString",R,N,V_PIVideoReframeAdjustmentKey
T@"NSString",R,N,V_PISourceSelectAdjustmentKey
T@"NSString",R,N,V_PIVideoStabilizeAdjustmentKey
T@"NSString",R,N,V_PIVideoCrossfadeLoopAdjustmentKey
T@"NSString",R,N,V_PIPortraitVideoAdjustmentKey
T@"NSString",R,N,V_PISourceAdjustmentKey
T@"NSString",R,N,V_PIOvercaptureSourceAdjustmentKey
rawToneCurveProperties
boostCurveValueAt:
outputImageFB3
outputImageFB0
kernelFB0
kernelFB3
T@"NSString",R
inputBoost
setInputBoost:
inputParams
setInputParams:
_inputBoost
_inputParams
Td,V_inputBoost
T@"NSString",C,V_inputVersion
T@"NSArray",C,V_inputParams
initWithContentsOfURL:options:error:
metalLibraryData
kernelWithFunctionName:fromMetalLibraryData:error:
sourceOutCompositingFilter
metalKernelWithFunctionName:error:
kernelsWithString:
stringForColorType:
arrayWithArray:
colorTypeForString:
initWithBase64EncodedString:options:
base64EncodedStringWithOptions:
inputTableImage
curvesKernel
setInputTableImage:
_inputTableImage
T@"CIImage",&,V_inputTableImage
layoutRequest
layoutRegions
setLayoutRegions:
segmentationClassification
setSegmentationClassification:
setSegmentationMatteImage:
setSegmentationConfidenceMapImage:
outputGeometry
segmentationMatteImage
segmentationConfidenceMapImage
facePositionAcceptable:imageAspect:
setCropScore:
setLayoutScore:
setNFaces:
setSegmentationScore:
setLocalConfidenceScore:
setGroundedScore:
setConfidenceMapScore:
setMattePureBackground:
setMattePureForeground:
setConfidencePureBackground:
setConfidencePureForeground:
setClockOverlapAcceptable:
setResolutionRatio:
setFaceSize:
setFaceLocalConfidence:
setFacePositionAcceptable:
setMetadataClockOverlapAcceptable:
setParallaxScore:
nFaces
segmentationScore
localConfidenceScore
groundedScore
confidenceMapScore
parallaxScore
mattePureBackground
mattePureForeground
confidencePureBackground
confidencePureForeground
clockOverlapAcceptable
resolutionRatio
faceSize
faceLocalConfidence
facePositionAcceptable
metadataClockOverlapAcceptable
initWithParallaxLayoutRequest:
_clockOverlapAcceptable
_facePositionAcceptable
_metadataClockOverlapAcceptable
_cropScore
_layoutScore
_nFaces
_segmentationScore
_localConfidenceScore
_groundedScore
_confidenceMapScore
_parallaxScore
_mattePureBackground
_mattePureForeground
_confidencePureBackground
_confidencePureForeground
_resolutionRatio
_faceSize
_faceLocalConfidence
_layoutRegions
_segmentationMatteImage
_segmentationConfidenceMapImage
_segmentationClassification
T@"PIParallaxLayoutRequest",R,N
T@"<PFParallaxAssetRegions>",&,N,V_layoutRegions
T@"CIImage",&,N,V_segmentationMatteImage
T@"CIImage",&,N,V_segmentationConfidenceMapImage
TQ,N,V_segmentationClassification
Tf,N,V_cropScore
Tf,N,V_layoutScore
Tf,N,V_nFaces
Tf,N,V_segmentationScore
Tf,N,V_localConfidenceScore
Tf,N,V_groundedScore
Tf,N,V_confidenceMapScore
Tf,N,V_parallaxScore
Tf,N,V_mattePureBackground
Tf,N,V_mattePureForeground
Tf,N,V_confidencePureBackground
Tf,N,V_confidencePureForeground
TB,N,V_clockOverlapAcceptable
Tf,N,V_resolutionRatio
Tf,N,V_faceSize
Tf,N,V_faceLocalConfidence
TB,N,V_facePositionAcceptable
TB,N,V_metadataClockOverlapAcceptable
backgroundLayer
imageWithCVPixelBuffer:
foregroundLayer
calculateLuminanceValuesForImage:renderer:error:
isEqualToParallaxStyleRecipe:
isEqualToDictionary:
_foregroundFilters
_backgroundFilters
_matteFilters
T@"NSDictionary",R,C,N,V_parameters
T@"NSArray",R,C,N,V_foregroundFilters
T@"NSArray",R,C,N,V_backgroundFilters
T@"NSArray",R,C,N,V_matteFilters
isEqualToParallaxStyleDefinition:
isEqualToParallaxStyleFilterDefinition:
guideImage
lightMapImage
backgroundImage
cachedImage:forKey:
_filterName
T@"NSString",R,C,N,V_filterName
isEqualToParallaxStyleFilterStackDefinition:
_stackName
_filters
T@"NSString",R,C,N,V_stackName
T@"NSArray",R,C,N,V_filters
isEqualToParallaxStyleParameter:
isEqualToNumber:
_numberValue
_unit
T@"NSNumber",R,N,V_numberValue
T@"NSString",R,N,V_unit
initWithRed:green:blue:alpha:colorSpace:
_redValue
_greenValue
_blueValue
_alphaValue
T@"NSNumber",R,N,V_redValue
T@"NSNumber",R,N,V_greenValue
T@"NSNumber",R,N,V_blueValue
T@"NSNumber",R,N,V_alphaValue
initWithX:Y:
_xValue
_yValue
T@"NSNumber",R,N,V_xValue
T@"NSNumber",R,N,V_yValue
_modeValue
T@"NSString",R,C,N,V_modeValue
_variableName
T@"NSString",R,C,N,V_variableName
_outputImage
_guideImage
_backgroundImage
_visibleRect
T@"CIImage",&,N,V_outputImage
T@"CIImage",&,N,V_guideImage
T@"CIImage",&,N,V_backgroundImage
T{CGRect={CGPoint=dd}{CGSize=dd}},N,V_visibleRect
workingColorSpace
dealloc
set_accumError:
_start
setLabel:
renderImage:rect:toDestination:atPoint:error:
useAsCIRenderDestinationWithRenderer:block:
_isReadyForMoreData
_markAsFinished
markAsFinished
_accumError
_appendInputFrame:
_nextInputFrame
_initializeAccumulation
nextInputFrame
_accumulate:
_initializeAccumulation:
colorWithRed:green:blue:colorSpace:
_initializeStorage:image:error:
_accumulate:error:
imageWithCVPixelBuffer:options:
componentMax
componentMin
useAsCIImageWithOptions:renderer:block:
waitUntilDone
_exportOutputImage:format:colorSpace:toURL:uti:error:
_dynamismMapWithMinImage:maxImage:extent:
createCGImage:fromRect:format:colorSpace:deferred:
initWithSize:renderer:jobNumber:
start:
isReadyForMoreData
accumulate:error:
writeLongExposureImage:UTI:colorSpace:error:
writeMaskImage:UTI:error:
_pixelSize
_renderer
_temporaryDestinationStorage
_averageAccumulationStorage
_minimumAccumulationStorage
_maximumAccumulationStorage
_frameCount
_jobNumber
_stateQueue
_accumQueue
_accumSemaphore
_readySemaphore
_doneGroup
_inputFrames
_finished
_imageOptions
__accumError
T@"NSError",&,V__accumError
observation
T@"VNImageHomographicAlignmentObservation",R,C
setObservation:
_observation
_extent
T@"VNImageHomographicAlignmentObservation",C,N,V_observation
T{?={?=qq}{?=qq}},N,V_extent
newRenderPipelineStateForEvaluationMode:
_shouldWaitForDependentJobs
prepareNodeWithPipelineState:error:
nodeByReplayingAgainstCache:error:
setStillImage:
stillImage
prepareNode
registrationRequest
setGuideExtent:
guideExtent
newPixelBufferOfSize:format:
initWithPixelBuffer:
jobNumber
initWithTargetedCVPixelBuffer:options:
initWithCVPixelBuffer:options:
wantsRenderScaleClampedToNativeScale
_stillImage
_guideExtent
T{?={?=qq}{?=qq}},N,V_guideExtent
T@"CIImage",&,N,V_stillImage
T@"VNImageHomographicAlignmentObservation",&,N,V_observation
initWithComposition:tag:responseQueue:
initWithComposition:responseQueue:
_pipelineFilters
inputDestinationImage
setInputDestinationImage:
inputCorrectionInfo
setInputCorrectionInfo:
inputCameraModel
setInputCameraModel:
_inputDestinationImage
_inputCorrectionInfo
_inputCameraModel
T@"CIImage",&,N,V_inputDestinationImage
T@"NSArray",&,N,V_inputCorrectionInfo
T@"NSString",&,N,V_inputCameraModel
posterFrameTime
setPosterFrameTime:
adjustmentHasPerspective:settings:
adjustmentHasCTM:settings:
_versionRules
versionRules
currentFormatVersion
indexOfObject:
formatVersionForAdjustment:identifier:
sourceDefinitions
startKey
startScaleKey
endKey
endScaleKey
rateKey
endTime
setEndTime:
rate
setRate:
initWithMin:max:manualMin:manualMax:depthMin:depthMax:
manualMin
manualMax
depthMin
depthMax
withLenience:
_min
_max
_manualMin
_manualMax
_depthMin
_depthMax
Td,R,N,V_min
Td,R,N,V_max
Td,R,N,V_manualMin
Td,R,N,V_manualMax
Td,R,N,V_depthMin
Td,R,N,V_depthMax
initWithRanges:
loadFromURL:
ranges
_ranges
T@"NSDictionary",R,C,N,V_ranges
cropScoreThresholdForClassification:
segmentationScoreRanges
segmentationManualGatingLenience
isValidSegmentationScoreForDepth:
cinematographyScript
assetWithURL:
setCinematographyScript:
loadWithAsset:changesDictionary:completion:
labelImageCache
setLabelImageCache:
setRenderTime:
frameNearestTime:
focusDetection
allDetections
detectionType
trackIdentifier
_imageByAddingDetection:toImage:isPrimary:canvasSize:inverseOrientation:
initWithInput:assetURL:cinematographyState:monochrome:
_cinematographyScript
_labelImageCache
_renderTime
T@"PTCinematographyScript",&,N,V_cinematographyScript
T{?=qiIq},N,V_renderTime
T@"NSCache",&,N,V_labelImageCache
groupIdentifier
colorWashKernels
magentaColor
inputColor
inputMode
hasPrefix:
cgColor
hueChromaFixedColorWashKernel
hueChromaVariableColorWashKernel
hueChromaColorWashKernel
colorWashFixedKernel
colorWashVariableKernel
colorWashFixedSmoothKernel
colorWashVariableSmoothKernel
setInputColor:
setInputMode:
_inputColor
_inputMode
T@"CIColor",&,N,V_inputColor
T@"NSString",&,N,V_inputMode
blueColor
yellowColor
inputShadowColor
inputHighlightColor
iptColorWashDuoFixedKernel
iptColorWashDuoVariableKernel
iptColorWashDuoKernel
hueChromaColorWashDuoFixedKernel
hueChromaColorWashDuoVariableKernel
hueChromaColorWashDuoKernel
rgbColorWashDuoKernel
rgbColorWashDuoFixedKernel
rgbColorWashDuoVariableKernel
setInputShadowColor:
setInputHighlightColor:
_inputShadowColor
_inputHighlightColor
T@"CIColor",&,N,V_inputShadowColor
T@"CIColor",&,N,V_inputHighlightColor
outputExposureKey
falseColorHDRKey
inputRAWGamutMapMaxKey
bytesPerPixel
initWithSize:format:
mutableBytesAtPoint:
RG16
isAppleProRaw
shouldUseGainMapExposureCompensationForRawProperties:
inverseAggregatedCurveValueAt:
meteorGainMapExposureCompensationMode
isSensorRawCapture
isUnifiedBracketingHDRCapture
dataWithBytes:length:
newLinearWideGamutColorSpace
inputWarmth
inputOrigI
inputOrigQ
linearWideGamutColorSpace
faceBalanceKernels
setInputOrigI:
setInputOrigQ:
setInputWarmth:
_inputOrigI
_inputOrigQ
_inputStrength
_inputWarmth
Td,N,V_inputOrigI
Td,N,V_inputOrigQ
Td,N,V_inputStrength
Td,N,V_inputWarmth
initWithImageProvider:width:height:format:colorSpace:options:
inputPointsR
curvePointsFromDictionaries:
inputPointsG
inputPointsB
inputPointsL
calculateCurveTable:
tableImageFromRed:green:blue:luminance:
setInputPointsR:
setInputPointsG:
setInputPointsB:
setInputPointsL:
_inputPointsR
_inputPointsG
_inputPointsB
_inputPointsL
T@"NSArray",&,V_inputPointsR
T@"NSArray",&,V_inputPointsG
T@"NSArray",&,V_inputPointsB
T@"NSArray",&,V_inputPointsL
colorBalanceKernels
gHDRtoPPKernel
PPtogHDRKernel
applyInputConversion:
colorBalanceKernel
applyOutputConversion:
inputWarmTemp
setInputWarmTemp:
inputWarmTint
setInputWarmTint:
inputHasFace
setInputHasFace:
inputIsRaw
setInputIsRaw:
_inputWarmTemp
_inputWarmTint
_inputHasFace
_inputIsRaw
T@"NSNumber",&,N,V_inputWarmTemp
T@"NSNumber",&,N,V_inputWarmTint
T@"NSNumber",&,N,V_inputStrength
T@"NSNumber",&,N,V_inputHasFace
T@"NSNumber",&,N,V_inputIsRaw
substringFromIndex:
_map
getMap
HDRFilterForSDRFilter:
alignmentKey
alignment
setAlignment:
PUEditSettings
setBool:forKey:
integerForKey:
setInteger:forKey:
doubleForKey:
setDouble:forKey:
stringForKey:
IPXEditSettings
falseColorHDR
setFalseColorHDR:
segmentationDebugRoundTripProxyImage
setSegmentationDebugRoundTripProxyImage:
disableSegmentation
setDisableSegmentation:
setSegmentationInfillAlgorithm:
segmentationDebugTintLayers
setSegmentationDebugTintLayers:
segmentationDisableCaching
setSegmentationDisableCaching:
setSegmentationDebugPreviewDisableClock:
segmentationDebugPreviewHighQuality
setSegmentationDebugPreviewHighQuality:
setSegmentationManualGatingLenience:
setStyleRecipeConfigDirectoryPath:
setUseStyleRecipeConfigDirectory:
parallaxLayoutConfigurationOverride
setParallaxLayoutConfigurationOverride:
parallaxWallpaperDisableUpgrade
setParallaxWallpaperDisableUpgrade:
setCinematicAllowYUVSourceInput:
setCinematicAllowRGB10Packed:
setParallaxStyleKeyLevelOverride:
setParallaxStyleAvoidColorWashBrownOverride:
setForceGlassesMatteOff:
setForceSpillMatteOff:
setAllowSpillMatteOnOlderPortraitV2Captures:
_forceGlassesMatteOff
_forceSpillMatteOff
_allowSpillMatteOnOlderPortraitV2Captures
TB,N,V_forceGlassesMatteOff
TB,N,V_forceSpillMatteOff
TB,N,V_allowSpillMatteOnOlderPortraitV2Captures
Tq,N,VparallaxStyleKeyLevelOverride
TB,N,VparallaxStyleAvoidColorWashBrownOverride
boolSettingForKey:defaultValue:
shouldUseMetalRenderer
initWithMetalDevice:options:
initWithVideoExportRequest:
metalRenderer
autoLoopExportRequest
setUpContext:
currentContext
contextForContext:
isObject
setError:
toDictionary
node
initWithKeyframes:stabCropRect:input:
initWithNode:context:
jsContext
toRect
overcaptureRectForStitchedOvercaptureSize:overcaptureVideoComplementSize:primarySize:autoLoopStabilizedCropRect:
newPhotosPipelineAtSourceURL:error:
initWithURL:
newPhotosPipeline:
apertureRedEyeResultFromFaceObservations:imageSize:
landmarks
leftEye
rightEye
pointCount
normalizedPoints
_faceRequest
localLightHDRStatisticsNoProxy
inputLightMap
inputLightMapWidth
inputLightMapHeight
_polyKernelHDR
_shadowKernelHDR
inputLightMapImage
inputSmartShadows
isInCloud
networkAccessAllowed
removeItemAtURL:error:
setType:
setProxyImage:
setImageFileURL:
setFileType:
cacheURL
updateSegmentationResource:
supportsSegmentationResourceCaching
segmentationResourceURL
loadParallaxResource:options:resultHandler:
cancelParallaxResourceRequest:
loadPetsRegions:
cancelPetsRegionsRequest:
localIdentifier
initWithFileURL:
setCacheURL:
setIsInCloud:
_acceptableRect
_preferredRect
_isInCloud
_cacheURL
T@"NSURL",R,N,V_fileURL
T@"NSURL",&,N,V_cacheURL
TB,N,V_isInCloud
T{CGRect={CGPoint=dd}{CGSize=dd}},R,N,V_acceptableRect
T{CGRect={CGPoint=dd}{CGSize=dd}},R,N,V_preferredRect
initWithData:width:height:bytesPerRow:bytesPerComponent:format:colorspace:
initWithBytes:width:height:bytesPerRow:bytesPerComponent:format:colorspace:
elementByteSize
rowElements
bufferColorspace
T@"NSMutableData",&,Vdata
TQ,R,VelementByteSize
TQ,R,VrowElements
TQ,R,Vwidth
TQ,R,Vheight
Ti,R,Vformat
initWithTargetPixelSize:
_calculateBlackAndWhiteSettingsFromBufferImage:
CIFormat
getNonNormalizedSettings:
createHueArray
imageWithBitmapData:bytesPerRow:size:format:options:
hueArrayImage:
smartBlackWhiteKernel
inputNeutralGamma
setInputNeutralGamma:
inputTone
setInputTone:
inputHue
setInputHue:
inputGrain
setInputGrain:
inputSeed
setInputSeed:
inputScaleFactor
setInputScaleFactor:
T@"NSNumber",C,N,VinputStrength
T@"NSNumber",C,N,VinputNeutralGamma
T@"NSNumber",C,N,VinputTone
T@"NSNumber",C,N,VinputHue
T@"NSNumber",C,N,VinputGrain
T@"NSNumber",C,N,VinputSeed
T@"NSNumber",C,N,VinputScaleFactor
smartBlackAndWhiteStatistics
smartBlackAndWhiteAdjustmentsForValue:andStatistics:
inputCorrectionInfoKey
hasCorrections
cinematicMetadataFromAsset:
renderingVersionFromAsset:error:
isRenderVersionSupported:
latestVersion
currentSystemCanRenderAsset:
currentSystemRenderingVersion
waitUntilCompletedAndReturnError:
T{?=qq},R,N
T@"NUPixelFormat",R,N
T@"NUColorSpace",R,N
T^{__CVBuffer=},R,N
initWithPixelBuffer:renderTask:
renderTask
renderInfo
_pixelBuffer
_renderInfo
T@"<NUImageBuffer>",R,N,V_pixelBuffer
T@"CIRenderTask",R,N,V_renderTask
T@"CIRenderInfo",R,N,V_renderInfo
CVPixelFormat
initWithImage:format:colorSpace:
sizeInBytes
render:error:
waitForRender:
cachedImage
_cachedImage
T@"CIImage",R,N,V_image
T@"NUPixelFormat",R,N,V_pixelFormat
T@"NUColorSpace",R,N,V_colorSpace
T@"CIImage",R,N,V_cachedImage
initWithLayout:
T@"PFParallaxLayout",R,N,V_layout
layerStackRequest
layerStackMode
requestLayout
effectiveLayout
backfillScalePolicy
deviceScalePolicy
setDisableIntermediateCaching:
layoutByUpdatingImageSize:
linearGrayColorSpace
imageByCachingImage:format:colorSpace:key:
parallaxVisibleFrame
parallaxInactiveFrame
scaledExtent
debugTintedImage:isBackfill:
setForegroundImage:
setDebugImageCollector:
debugImageCollector
prepareImagesForItem:renderer:layout:style:inputImage:matteImage:infillImage:foregroundImage:backgroundImage:
foregroundImage
newRenderedPixelBufferFromImage:hasAlpha:error:
setForegroundBuffer:
setBackgroundBuffer:
renderImagesWithRenderer:
backgroundBuffer
layerForBuffer:image:zPosition:identifier:
foregroundBuffer
resultLayersWithRenderer:
setLayers:
fullSizeGeometry
initWithLayers:size:layout:depthEnabled:parallaxDisabled:clockAreaLuminance:
imageLayer:frame:zPosition:identifier:
applyAttachmentsToCVPixelBuffer:
setAlphaMode:
cacheImage:key:format:colorSpace:
setObject:forKey:cost:
initWithParallaxLayerStackRequest:
_cachedImageEntries
_foregroundImage
_foregroundBuffer
_backgroundBuffer
_debugImageCollector
_layers
T@"<PISegmentationItem>",R,N
T@"PIParallaxStyle",R,N
T@"CIImage",&,N,V_foregroundImage
T@"<NUImageBuffer>",&,N,V_foregroundBuffer
T@"<NUImageBuffer>",&,N,V_backgroundBuffer
T@"_PIParallaxLayerStackDebugImageCollector",&,N,V_debugImageCollector
T@"NSArray",C,N,V_layers
_layerStackMode
Tq,N,V_layerStackMode
T@"NSCache",&,N,V_cache
setFlattenedBackgroundForDebugPreview:
flattenedBackgroundForDebugPreview
setFlattenedForegroundForDebugPreview:
flattenedForegroundForDebugPreview
debugLayouts
blackImage
setDebugInputImage:
setDebugMatteImage:
setDebugMatteCropImage:
setDebugLocalConfidenceImage:
setDebugConfidenceMapImage:
setDebugInfillImage:
setDebugLayoutImage:
setDebugPreviewImage:
setDebugColorAnalysisImage:
setDebugIntermediateLayoutImages:
debugInputImage
setDebugInputBuffer:
debugMatteImage
setDebugMatteBuffer:
debugMatteCropImage
setDebugMatteCropBuffer:
debugLocalConfidenceImage
setDebugLocalConfidenceBuffer:
debugConfidenceMapImage
setDebugConfidenceMapBuffer:
debugInfillImage
setDebugInfillBuffer:
debugLayoutImage
setDebugLayoutBuffer:
debugPreviewImage
setDebugPreviewBuffer:
debugColorAnalysisImage
setDebugColorAnalysisBuffer:
debugIntermediateLayoutImages
setDebugIntermediateLayoutBuffers:
debugInputBuffer
debugMatteBuffer
debugMatteCropBuffer
debugLocalConfidenceBuffer
debugConfidenceMapBuffer
debugInfillBuffer
debugLayoutBuffer
debugPreviewBuffer
debugColorAnalysisBuffer
debugIntermediateLayoutBuffers
_debugInputImage
_debugMatteImage
_debugMatteCropImage
_debugLocalConfidenceImage
_debugConfidenceMapImage
_debugInfillImage
_debugLayoutImage
_debugIntermediateLayoutImages
_debugPreviewImage
_debugColorAnalysisImage
_debugInputBuffer
_debugMatteBuffer
_debugMatteCropBuffer
_debugLocalConfidenceBuffer
_debugConfidenceMapBuffer
_debugInfillBuffer
_debugLayoutBuffer
_debugIntermediateLayoutBuffers
_debugPreviewBuffer
_debugColorAnalysisBuffer
_flattenedBackgroundForDebugPreview
_flattenedForegroundForDebugPreview
T@"CIImage",&,N,V_debugInputImage
T@"CIImage",&,N,V_debugMatteImage
T@"CIImage",&,N,V_debugMatteCropImage
T@"CIImage",&,N,V_debugLocalConfidenceImage
T@"CIImage",&,N,V_debugConfidenceMapImage
T@"CIImage",&,N,V_debugInfillImage
T@"CIImage",&,N,V_debugLayoutImage
T@"NSArray",&,N,V_debugIntermediateLayoutImages
T@"CIImage",&,N,V_debugPreviewImage
T@"CIImage",&,N,V_debugColorAnalysisImage
T@"<NUImageBuffer>",&,N,V_debugInputBuffer
T@"<NUImageBuffer>",&,N,V_debugMatteBuffer
T@"<NUImageBuffer>",&,N,V_debugMatteCropBuffer
T@"<NUImageBuffer>",&,N,V_debugLocalConfidenceBuffer
T@"<NUImageBuffer>",&,N,V_debugConfidenceMapBuffer
T@"<NUImageBuffer>",&,N,V_debugInfillBuffer
T@"<NUImageBuffer>",&,N,V_debugLayoutBuffer
T@"NSArray",&,N,V_debugIntermediateLayoutBuffers
T@"<NUImageBuffer>",&,N,V_debugPreviewBuffer
T@"<NUImageBuffer>",&,N,V_debugColorAnalysisBuffer
T@"<NUImageBuffer>",&,N,V_flattenedBackgroundForDebugPreview
T@"<NUImageBuffer>",&,N,V_flattenedForegroundForDebugPreview
stopAtTagIncludeGeometryFilter:
histogramOptimizationFilter
histogramCalculationColorSpace
setHistogramCalculationColorSpace:
histogram
calculateSettingsForImageHistogram:
percentile:
calculateSettingsForSingleChannelHistogram:suffix:
_adjustmentControllerForKey:creatingIfNecessary:expectedClass:
smartColorAdjustmentControllerCreatingIfNecessary:
smartBWAdjustmentControllerCreatingIfNecessary:
cropAdjustmentControllerCreatingIfNecessary:
redEyeAdjustmentControllerCreatingIfNecessary:
livePhotoKeyFrameAdjustmentControllerCreatingIfNecessary:
videoPosterFrameAdjustmentControllerCreatingIfNecessary:
depthAdjustmentControllerCreatingIfNecessary:
trimAdjustmentControllerCreatingIfNecessary:
slomoAdjustmentControllerCreatingIfNecessary:
effectAdjustmentControllerCreatingIfNecessary:
effect3DAdjustmentControllerCreatingIfNecessary:
portraitAdjustmentControllerCreatingIfNecessary:
autoLoopAdjustmentControllerCreatingIfNecessary:
highResFusionAdjustmentControllerCreatingIfNecessary:
rawAdjustmentControllerCreatingIfNecessary:
rawNoiseReductionAdjustmentControllerCreatingIfNecessary:
sharpenAdjustmentControllerCreatingIfNecessary:
whiteBalanceAdjustmentControllerCreatingIfNecessary:
noiseReductionAdjustmentControllerCreatingIfNecessary:
definitionAdjustmentControllerCreatingIfNecessary:
vignetteAdjustmentControllerCreatingIfNecessary:
videoReframeAdjustmentControllerCreatingIfNecessary:
sourceSelectAdjustmentControllerCreatingIfNecessary:
videoStabilizeAdjustmentControllerCreatingIfNecessary:
videoCrossfadeLoopAdjustmentControllerCreatingIfNecessary:
semanticEnhanceAdjustmentControllerCreatingIfNecessary:
portraitVideoAdjustmentControllerCreatingIfNecessary:
smartToneAdjustmentController
smartColorAdjustmentController
smartBWAdjustmentController
redEyeAdjustmentController
videoPosterFrameAdjustmentController
trimAdjustmentController
slomoAdjustmentController
effectAdjustmentController
effect3DAdjustmentController
highResFusionAdjustmentController
rawAdjustmentController
rawNoiseReductionAdjustmentController
sharpenAdjustmentController
whiteBalanceAdjustmentController
noiseReductionAdjustmentController
definitionAdjustmentController
vignetteAdjustmentController
videoReframeAdjustmentController
videoStabilizeAdjustmentController
videoCrossfadeLoopAdjustmentController
portraitVideoAdjustmentController
T@"PIAdjustmentConstants",R,N
isEditable
initWithX:y:editable:
_editable
Td,R,N,V_x
Td,R,N,V_y
editable
TB,R,N,GisEditable,V_editable
smartToneHDRStatistics
_kernelBneg
_kernelBpos
_kernelRH
_kernelH
_kernelC_hdr
_kernelC
T@"NSNumber",&,N,VinputExposure
T@"NSNumber",&,N,VinputBrightness
T@"NSNumber",&,N,VinputShadows
T@"NSNumber",&,N,VinputHighlights
T@"NSNumber",&,N,VinputBlack
T@"NSNumber",&,N,VinputRawHighlights
availableBlendModes
inputBlendMode
blendKernelForBlendMode:
applyWithForeground:background:
setInputBlendMode:
_inputBlendMode
T@"NSNumber",&,N,V_inputIntensity
T@"NSString",&,N,V_inputBlendMode
loadFusionTuningParameters
stringSettingForKey:defaultValue:
dictionaryWithContentsOfFile:
_debugDumpIntermediateImages
inputMaskImage
inputStillImage
inputRenderScale
inputVideoScale
inputAlignmentExtent
inputAlignmentTransform
valueAtIndex:
alignImage:transform:extent:
_computeNCCMapFromImage:toImage:scale:
_refineMaskImage:guideImage:scale:
_fuseImage:withGuideImage:weightImage:maskImage:
debugDumpIntermediateImages
currentDirectoryPath
writeToTIFF:
applyWithExtent:roiCallback:inputImage:arguments:
setInputStillImage:
setInputMaskImage:
setInputRenderScale:
setInputVideoScale:
setInputAlignmentExtent:
setInputAlignmentTransform:
_inputStillImage
_inputMaskImage
_inputRenderScale
_inputVideoScale
_inputAlignmentExtent
_inputAlignmentTransform
T@"CIImage",&,N,V_inputStillImage
T@"CIImage",&,N,V_inputMaskImage
T@"NSNumber",&,N,V_inputRenderScale
T@"NSNumber",&,N,V_inputVideoScale
T@"CIVector",&,N,V_inputAlignmentExtent
T@"CIVector",&,N,V_inputAlignmentTransform
isEnabled
evaluate:input:pipelineState:error:
initWithPipelineState:
mediaTypeForComposition:
hasStaticTime
valueWithCMTime:
beginGroupWithName:error:
renderNodeFromSource:settings:error:
initWithAffineTransform:
transformNodeWithInput:transform:error:
inputForPath:error:
addTagWithName:inputNode:error:
endGroupWithName:error:
enableHDRSupport
auxiliaryImageType
allAssetsCanUseHDRPipeline
imageProperties:
initWithCGColorSpace:
isHDR
cacheNode:type:settings:error:
performLongExposureFusionForComposition:longExposureImage:useHDRFilter:error:
scaleNode:scale:error:
overcaptureRectForAutoCrop:overcaptureVideoComplementSize:primarySize:CGRect:
nodeByApplyingFilterWithName:useHDRFilter:settingsAndInputs:
performApertureRedeyeOnImage:useHDRFilter:redEyeAdjustment:
performRedeyeOnImage:useHDRFilter:redEyeAdjustment:
auxiliaryImageFromComposition:type:mediaComponentType:error:
versionForPortraitEffect:
vectorWithFloats:
auxiliaryDataInfoMetadata
depthCameraCalibrationData
scaleMultiplyOfScalar:
isCIFilterAvailable:propertyName:
initWithInput:scale:
remapPortraitV2Strength:portraitEffectKind:
isSourceAvailable:sourceSettings:
portraitVideo:disparityInput:disparityKeyframes:apertureKeyframes:debugMode:error:
portraitVideoDebugDetectedObjects:source:cinematographyState:monochrome:error:
cropNode:cropRect:cropSettings:
initWithInput:
livePhotoKeyFrameMetadataFromNode:time:error:
trimInput:startTime:endTime:error:
createSloMoWithInput:startTime:endTime:rate:error:
grainInputSeedFromFrameTime
sharpnessWithIntensity:
videoReframe:reframes:error:
videoCrossfadeLoop:crossfadeAdjustment:error:
perspectiveTransformWithPitch:yaw:roll:imageRect:
straightenTransformWithAngle:extent:
originalCleanAperture
originalSize
orientedNode:withOrientation:
scaledVector:
debugNodeByApplyingFilterWithName:useHDRFilter:settingsAndInputs:
_processedRenderNodeForComposition:input:pipelineState:error:
P3Kernel
warmUp
_ensureResources
_freeResources
freeAllResources:
initWithParallaxAsset:
_didLoad:completion:
_load:
loadingHandlerQueue
configurationWithDeviceName:
genericConfiguration
_tryLoadSegmentationItemFromCache:
_loadItem:completion:
_handlePartialItem:loadingState:
_loadSegmentationData:completion:
proxyOnly
_abort:
_loadFullSizeResource:completion:
_loadProxyResource:completion:
loadingHandler
proxyImage
segmentationCompositionForAssetResource:
_loadAssetResourceForProxy:completion:
isProxyOnly
setImageSize:
setCanHandleAdjustmentData:
setResultHandlerQueue:
setNetworkAccessAllowed:
disableDownload
downloadProgressHandler
setDownloadProgressHandler:
asset
_loadAssetResource:options:completion:
timeIntervalSinceDate:
_loadRegions:completion:
_loadBackground:completion:
_segment:completion:
_analyzeColors:completion:
_performLayout:completion:
_loadLocalLightData:completion:
_cacheSegmentationDataForItem:
disableRendering
_isValidSegmentationMatteHistogramForDepth:
_performSegmentation:type:completion:
setSegmentationType:
setVisionSegmentationPolicy:
proxyScalePolicy
setRenderContext:
renderPriorityForResourcePriority:
setProduceConfidenceMap:
matteImageBuffer
confidenceMapBuffer
salientObjects
segmentationCompositionForProxyImage:orientation:
imageFileURL
segmentationCompositionForImageURL:fileUTI:orientation:proxyImage:
adjustmentData
adjustmentFormat
adjustmentVersion
segmentationSourceForImageURL:fileUTI:orientation:proxyImage:
imageWithCGImage:options:
isEqualToLayoutConfiguration:
_loadSegmentationItemFromURL:error:
normalizedVisibleFrame
layoutByUpdatingNormalizedVisibleFrame:
isDepthEnabled
layerStackByUpdatingDepthEnabled:
isPerspectiveZoomEnabled
layerStackByUpdatingParallaxDisabled:
_saveSegmentationItem:layerStack:toWallpaperURL:completion:
generateLayerStackForItem:style:layout:options:completion:
saveSegmentationItem:layerStack:toWallpaperURL:error:
saveSegmentationItem:toURL:error:
saveLayerStack:toURL:options:error:
_loadSegmentationItemFromWallpaperURL:error:
loadLayerStackFromURL:options:error:
reloadSegmentationItemFromWallpaperURL:asset:completion:
initWithSegmentationItem:parallaxAsset:
setProxyOnly:
loadSegmentationItemWithCompletion:
warmUpResources
ensureResources
freeResources
loadSegmentationItemFromURL:error:
saveSegmentationItem:layerStackOptions:configuration:style:layout:toWallpaperURL:completion:
loadSegmentationItemFromWallpaperURL:error:
loadLayerStackFromWallpaperURL:options:error:
renderPreviewLayerStackFromWallpaperURL:styleCategory:completion:
setDisableRendering:
setDisableDownload:
petsRegions
setPetsRegions:
petsFaceRegions
setPetsFaceRegions:
setLoadingHandler:
setLoadingHandlerQueue:
_signpost
_isLoading
_isCancelled
_loadRequestID
_petsRequestID
_loadingError
_renderContext
_item
_disableSegmentation
_disableRendering
_proxyOnly
_disableDownload
_asset
_petsRegions
_petsFaceRegions
_downloadProgressHandler
_loadingHandler
_loadingHandlerQueue
T@"<PFParallaxAsset>",R,N,V_asset
TB,N,V_disableSegmentation
TB,N,V_disableRendering
TB,N,V_proxyOnly
TB,N,V_disableDownload
Tq,N,V_priority
T@"NSArray",C,N,V_petsRegions
T@"NSArray",C,N,V_petsFaceRegions
T@?,C,N,V_downloadProgressHandler
T@?,C,N,V_loadingHandler
T@"NSObject<OS_dispatch_queue>",&,N,V_loadingHandlerQueue
T{CGRect={CGPoint=dd}{CGSize=dd}},R,N,VacceptableCropRect
T{CGRect={CGPoint=dd}{CGSize=dd}},R,N,VpreferredCropRect
T@"NSArray",R,N,VfaceRegions
T@"NSArray",R,N,VpetRegions
setInactiveRect:
_inactiveRect
T{CGRect={CGPoint=dd}{CGSize=dd}},N,V_inactiveRect
layoutInactiveFrameRequest
computeInactiveAvoidingRectForVisibleRect:acceptableFrame:unsafeRect:imageSize:newVisibleRect:
initWithParallaxLayoutInactiveFrameRequest:
T@"PIParallaxLayoutInactiveFrameRequest",R,N
deserializeFromDictionary:error:
sourceSelectSchema
photosCompositionSchema
rawSchema
rawNoiseReductionSchema
retouchSchema
smartToneSchema
smartColorSchema
whiteBalanceSchema
cropSchema
trimSchema
slomoSchema
livePhotoKeyFrameSchema
muteSchema
videoPosterFrameSchema
autoLoopSchema
orientationSchema
effectSchema
redEyeSchema
apertureRedEyeSchema
smartBlackAndWhiteSchema
grainSchema
sharpenSchema
definitionSchema
noiseReductionSchema
vignetteSchema
levelsSchema
curvesSchema
selectiveColorSchema
depthEffectSchema
effect3DSchema
portraitEffectSchema
highResFusionSchema
videoReframeSchema
videoStabilizeSchema
videoCrossfadeLoopSchema
debugSchema
semanticEnhance
portraitVideoSchema
renderPipelineForIdentifier:
registerPhotosSchema
registerSchemas:error:
registerRenderPipeline:forIdentifier:
T@"NUIdentifier",R
candidacy
descriptionForCandidacy:
setCandidacy:
shouldAllowPerspectiveCorrection
finalizerError
rollAngleDegrees
pitchAngleDegrees
yawAngleDegrees
getCropRectThatCompletelyContainsMasterImageForPitch:yaw:roll:
initWithDisposition:composition:
performNextActionWithCompletion:
shouldPerformAction:
markActionAsPerformed:
processHorizonResult:
setFinalizerError:
performHorizonCorrectionWithCompletion:
processPerspectiveResult:
performPerspectiveCorrectionWithCompletion:
hasPerformedAction:
performedActions
setPerformedActions:
floatForKey:
setShouldPerformAutoCrop:
setShouldPerformAutoStraighten:
setShouldUseAutoStraightenVerticalDetector:
setMaxAutoStraighten:
setMinAutoStraighten:
setAutoStraightenDominantAngleDiffThreshold:
setAutoStraightenVerticalAngleThreshold:
setRollAngleDegrees:
setPitchAngleDegrees:
setYawAngleDegrees:
_candidacy
_finalizerError
_performedActions
_rollAngleDegrees
_pitchAngleDegrees
_yawAngleDegrees
T@"NSError",&,N,V_finalizerError
TQ,N,V_performedActions
Td,N,V_rollAngleDegrees
Td,N,V_pitchAngleDegrees
Td,N,V_yawAngleDegrees
TQ,N,V_candidacy
disposition
_disposition
Tq,R,V_disposition
T@"NUComposition",R,C,V_composition
cropFraction
setCropFraction:
setAnalysisType:
shouldPerformAutoStraighten
shouldPerformAutoCrop
shouldUseAutoStraightenVerticalDetector
maxAutoStraighten
autoCropFilter
minAutoStraighten
undoExifOrientation:error:
autoStraightenVerticalAngleThreshold
autoStraightenDominantAngleDiffThreshold
_shouldPerformAutoCrop
_shouldPerformAutoStraighten
_shouldUseAutoStraightenVerticalDetector
_autoStraightenVerticalAngleThreshold
_autoStraightenDominantAngleDiffThreshold
_maxAutoStraighten
_minAutoStraighten
TB,V_shouldPerformAutoCrop
TB,V_shouldPerformAutoStraighten
TB,V_shouldUseAutoStraightenVerticalDetector
T@"NSNumber",C,V_autoStraightenVerticalAngleThreshold
T@"NSNumber",C,V_autoStraightenDominantAngleDiffThreshold
Td,V_maxAutoStraighten
Td,V_minAutoStraighten
inputToCropFilter
stitchedOvercaptureRect:primaryRect:forComposition:error:
initWithMasterImageRect:stitchedImageRect:
setRollRadians:
integralCropRect:
inputColorKey
_updateSettingsWithInputColor:
inputSaturationKey
inputCastKey
offsetSaturationKey
offsetCastKey
attributeVibrancyKey
attributeCastKey
setInputSaturation:
inputSaturation
setOffsetCast:
offsetCast
setOffsetSaturation:
offsetSaturation
_stats
iptFromLinearInto:fromRed:green:blue:
hueAngleFrom:
selectiveColorKernels
colorWithRed:green:blue:alpha:colorSpace:
iptHueAngleFromRed:green:blue:
convertToIPT:
hueSatLumTable
convertFromIPT:
inputCorrections
setInputCorrections:
_inputCorrections
T@"NSArray",&,N,V_inputCorrections
setKeyframeSequence:
setStabCropRect:
setShouldApplyWatermark:
setInputVideoProperties:
setSize:
inputVideoProperties
keyframeSequence
_stabilizeImage:cleanRect:cropRect:transform:geometry:
pi_imageByApplyingStabilizationWatermark
_evaluateVideoProperties:
_evaluateImageGeometry:
canPropagateOriginalLivePhotoMetadataTrack
shouldApplyWatermark
_shouldApplyWatermark
_keyframeSequence
_inputVideoProperties
_frameDuration
T@"PIReframeKeyframeSequence",&,N,V_keyframeSequence
T{?={?=qq}{?=qq}},N,V_stabCropRect
T@"<NUVideoProperties>",&,N,V_inputVideoProperties
T{?=qiIq},N,V_frameDuration
TB,N,V_shouldApplyWatermark
dictionaryForKey:
formatDescriptions
setSubjects:
setEstimatedCenterMotion:
setEstimatedMotionBlur:
setTrajectoryHomography:
subjects
estimatedCenterMotion
estimatedMotionBlur
_subjects
_estimatedCenterMotion
_estimatedMotionBlur
_trajectoryHomography
T{?=qiIq},R,V_time
T@"NSArray",R,V_subjects
T{CGVector=dd},R,V_estimatedCenterMotion
T{CGVector=dd},R,V_estimatedMotionBlur
T{?=[3]},R,V_trajectoryHomography
exceptionWithName:reason:userInfo:
tracks
encodedPixelSizeOfTrack:oriented:
extractMetadata
initWithContentsOfFile:
assetReaderWithAsset:error:
initWithTrack:outputSettings:
canAddOutput:
addOutput:
initWithAssetReaderTrackOutput:
startReading
nextTimedMetadataGroup
dataType
dataValue
subjectsFromMetadata:
centerMotionVectorFromMetadata:
motionBlurVectorFromMetadata:
trajectoryeHomographyFromMetadata:containsV3Metadata:
overwriteTrackingMetadataWithPlist:
_videoTrack
_mdataTrack
ndcMetadataTransform
pxlMetadataTransform
T@"NSArray",R,N,VtimedMetadataArray
vectorWithX:
RGBToYIQKernel
YIQToRGBKernel
isDefaultWarmth:
whiteBalanceKernel
warmth
setWarmth:
setY:
setI:
setQ:
_strength
_warmth
T@"NSNumber",&,N,V_strength
T@"NSNumber",&,N,V_warmth
T@"NSNumber",&,N,V_y
T@"NSNumber",&,N,V_i
T@"NSNumber",&,N,V_q
isHDRComposition:
setDataExtractor:
force
_options
initWithRequest:options:
internalComposition
options
setOptions:
setForce:
_force
TB,V_force
initWithComposition:location:touchDiameter:
_location
_touchDiameter
outputCropRect
Tf,N,V_inputThreshold
setPerservesAlpha:
_interpolateGrainKernel
_paddedTileKernel
_grainBlendAndMixKernel
inputISO
setInputISO:
inputAmount
setInputAmount:
T@"NSNumber",C,N,VinputISO
T@"NSNumber",C,N,VinputAmount
loadSegmentationItemForAsset:options:completion:
setSegmentationLoader:forAsset:
segmentationLoaderForAsset:
strongToStrongObjectsMapTable
_layerStackOptionsFromOptions:
_styleFromOptions:item:
_upgradeWallpaperAtURL:exportToURL:options:completion:
_upgradeFullPosterAtURL:exportToURL:options:completion:
layoutByUpgradingToConfiguration:
setNormalizedVisibleFrame:
depthEnabled
setIsDepthEnabled:
parallaxDisabled
setIsPerspectiveZoomEnabled:
editConfiguration
upgradePosterConfiguration:atURL:exportTo:options:completion:
dictionaryWithStyle:
media
assetUUID
subpath
configurationType
setEditConfiguration:
numberWithUnsignedLong:
setMedia:
computeSegmentationScoresForAsset:options:completion:
cancelSegmentationForAsset:
curatedSegmentationGatingDecisionForSegmentationScores:
manualSegmentationGatingDecisionForSegmentationScores:
layoutGatingDecisionForSegmentationScores:
tryLoadSegmentationForColdAsset:
exportWallpaperForAsset:toURL:options:completion:
upgradeWallpaperAtURL:exportToURL:options:completion:
writeCGImage:fileURL:options:
writeImage:toDirectoryAtPath:withBasename:
stringByAppendingPathComponent:
writeImage:fileURL:
writeCGImage:fileURL:
writeImage:toTemporaryDirectoryWithBasename:
currentSoftwareVersion
buildNumber
edgesKey
baseAddress
bytesPerRow
convertFloat:toFixed16:count:
RGBA16
initWithSize:format:rowBytes:mutableBytes:
convertFixed16:toFloat:count:
initWithSize:format:rowBytes:bytes:
initWithBuffer:colorSpace:validRegion:
initWithMutableBuffer:colorSpace:validRegion:
copyPixelsFromImage:srcRect:destImage:destOrigin:
ROIForCenterPoint:radius:
pointValue
inputSpots
auxiliaryImage:
underlyingAVDepthData
isDepthDataFiltered
depthDataQuality
cameraCalibrationData
auxiliaryCoreGraphicsInfoDictionary:
depthBlurEffectRenderingParameters
getMinMaxSimulatedApertureFrom:minValue:maxValue:version:
maxSDOFRenderingVersionSupported
depthBlurEffectSimulatedAperture
portraitLightingEffectStrength
initFromMinAperture:maxAperture:aperture:portraitStrength:SDOFRenderingVersion:depthVersionInfo:
minimumAperture
maximumAperture
portraitStrength
SDOFRenderingVersion
portraitMajorVersion
portraitMinorVersion
valuesAtCaptureFromImageProperties:error:
setPortraitStrength:
setMinimumAperture:
setMaximumAperture:
setSDOFRenderingVersion:
setPortraitMajorVersion:
setPortraitMinorVersion:
depthVersionInfo
setDepthVersionInfo:
_portraitStrength
_minimumAperture
_maximumAperture
_SDOFRenderingVersion
_portraitMajorVersion
_portraitMinorVersion
_depthVersionInfo
Tf,N,V_aperture
Tf,N,V_portraitStrength
T@"NSNumber",&,N,V_minimumAperture
T@"NSNumber",&,N,V_maximumAperture
TQ,N,V_SDOFRenderingVersion
TQ,N,V_portraitMajorVersion
TQ,N,V_portraitMinorVersion
T{?=ii},N,V_depthVersionInfo
focusRectDictionaryFromRect:
isStillImageDisparity:
_calculateWithImageProperties:valuesAtCapture:completion:
portraitInfoDictionaryFromFaceObservations:metadata:orientation:valuesAtCapture:
canApplyPortraitEffectsWithMetadata:
portraitEffectInfoDictionaryFromFaceObservations:orientation:valuesAtCapture:
depthEffectInfoDictionaryFromFaceObservations:metadata:orientation:valuesAtCapture:
focusRectDictionaryFromMetadata:
depthEffectInfoDictionaryFromFaceObservations:focus:valuesAtCapture:lumaNoiseScale:orientation:
nose
allPoints
faceJunkinessIndex
faceOrientationIndex
roll
faceObservationsData
indexesOfShallowDepthOfFieldObservations
objectsAtIndexes:
faceOrientation
focusRectangle
minimumApertureFocalRatio
maximumApertureFocalRatio
apertureFocalRatio
luminanceNoiseAmplitude
portraitInfoDictionaryFromCameraMetadata:
resetTag:input:error:
noRedEyeFilter
noTrimFilter
noMuteFilter
stripAllTimeAdjustmentsFilter
noOrientationFilter
perspectiveStraightenWithoutCropFilter
preGeometryFilter
postGeometryFilter
autoloopStabilizedVideoFilter
spatialOvercaptureVideoSourceFilter
socPseudoColorFilter
colorTypeKey
faceStrengthKey
faceWarmthKey
faceIKey
faceQKey
grayStrengthKey
grayWarmthKey
grayYKey
grayIKey
grayQKey
temperatureKey
tintKey
warmTempKey
warmFace
warmTintKey
warmFaceKey
colorType
setColorType:
faceStrength
setFaceStrength:
faceWarmth
setFaceWarmth:
faceI
setFaceI:
faceQ
setFaceQ:
grayStrength
setGrayStrength:
grayWarmth
setGrayWarmth:
grayY
setGrayY:
grayI
setGrayI:
grayQ
setGrayQ:
warmTemp
setWarmTemp:
warmTint
setWarmTint:
setWarmFace:
initWithRadius:softness:opacity:clipRect:pressureMode:
appendPoints:pointCount:
copyPixelsFromImage:rect:destPtr:destPtrRowBytes:
newCIImageFromBufferImage:
ciImageTiled:closed:pressureMode:
clipRect
prepareForCIFilterWithFaces:cropRect:
alignedRowBytesForWidth:
initWithBitmapData:width:height:bytesPerRow:format:
startTaskToRender:toDestination:error:
copyPixelsToImage:rect:srcPtr:srcPtrRowBytes:
calcExtentsForStrokeRadius:offset:inputImageExtent:maskExtent:repairExtent:textureExtent:sourceExtent:
extractRGBAfPixelsFromImage:fromROI:
rasterizeBrushStroke:atPoint:toBuffer:
copyPixelsToImage:atPoint:fromBuffer:inRect:
brushStrokeFromRetouchStrokeDictionary:
applyRepairMLStrokeToMutableBuffer:brushStroke:detectedFaces:context:error:
applyRepairStrokeToMutableBuffer:brushStroke:sourceOffset:repairEdges:
sourceDefinition:
redEyeSpotsWithCorrectionInfo:
depthInfo
depthInfoKey
apertureKey
glassesMatteAllowedKey
canRenderDepth
canAdjustApertureValue
setDepthInfo:
capturedAperture
glassesMatteAllowed
keyframesKey
stabCropRectKey
setKeyframes:
copyKeyframesTrimmingToTimeRange:
T{?={?=qq}{?=qq}},N
_newHLGPixelBufferOfSize:
_renderImage:toPixelBuffer:
recycleHLGPixelBuffer:
newHLGPixelBufferFromSDRImage:
colorSpaceFromColorPrimaries:transferFunction:yccMatrix:
inverseHLGLumaBlendingKernel
indexOfObject:inSortedRange:options:usingComparator:
setColorMatrix:
computeCurvesForImageHistogram:
_defaultCurveArray
replaceObjectAtIndex:withObject:
dictionariesFromPoints:
autoValuesForBlackPoint:whitePoint:
curvePointAtIndex:blackPoint:whitePoint:histogram:
insertObject:atIndex:
B24@0:8@16
#16@0:8
@16@0:8
@24@0:8:16
@32@0:8:16@24
@40@0:8:16@24@32
B16@0:8
B24@0:8#16
B24@0:8:16
Vv16@0:8
Q16@0:8
^{_NSZone=}16@0:8
B24@0:8@"Protocol"16
@"NSString"16@0:8
@"<NURenderStatistics>"16@0:8
@"<NUImageBuffer>"16@0:8
v24@0:8@16
v16@0:8
@"<NUImageBuffer>"
@24@0:8@16
B24@0:8o^@16
@"NUStorageImageBuffer"
@"CIRenderTask"
@"CIImage"
@24@0:8^{_NSZone=}16
q16@0:8
v24@0:8@?16
v20@0:8B16
@"<NUScalePolicy>"
@"NUPixelFormat"
@"NUColorSpace"
@"PTCinematographyTrack"16@0:8
@"PTCinematographyTrack"
v76@0:8{?=qiIq}16{CGRect={CGPoint=dd}{CGSize=dd}}40f72
{?=qiIq}16@0:8
v40@0:8{?=qiIq}16
{CGPoint=dd}16@0:8
v32@0:8{CGPoint=dd}16
@?16@0:8
{CGPoint="x"d"y"d}
{?="value"q"timescale"i"flags"I"epoch"q}
@64@0:8@16{?=qiIq}24{CGPoint=dd}48
f16@0:8
v20@0:8f16
@40@0:8@16q24^{CGColorSpace=}32
@"PFStoryRecipeDisplayAssetNormalization"
B32@0:8@16@24
v24@0:8d16
d16@0:8
v32@0:8q16d24
v32@0:8@16q24
@20@0:8f16
{CGRect={CGPoint=dd}{CGSize=dd}}16@0:8
v48@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16
{CGRect="origin"{CGPoint="x"d"y"d}"size"{CGSize="width"d"height"d}}
@80@0:8@16{?=qiIq}24{CGRect={CGPoint=dd}{CGSize=dd}}48
@"NSNumber"
@160@0:8@16@24Q32{CGRect={CGPoint=dd}{CGSize=dd}}40{CGSize=dd}72{CGRect={CGPoint=dd}{CGSize=dd}}88{CGRect={CGPoint=dd}{CGSize=dd}}120@152
@144@0:8Q16{CGRect={CGPoint=dd}{CGSize=dd}}24{CGSize=dd}56{CGRect={CGPoint=dd}{CGSize=dd}}72{CGRect={CGPoint=dd}{CGSize=dd}}104@136
@32@0:8Q16@24
@40@0:8Q16@24d32
@"CIContext"
@32@0:8@16o^@24
@"NSArray"16@0:8
@96@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{CGRect={CGPoint=dd}{CGSize=dd}}48@80@88
@"NSArray"
@96@0:8@16{CGSize=dd}24@40Q48@56@64^d72^d80^B88
v24@0:8Q16
@"NSString"
@"PFParallaxLayout"
@"<PFParallaxLayoutConfiguration>"
@"<PISegmentationItem>"
@32@0:8@16@24
v24@0:8q16
i20@0:8i16
B48@0:8@16@24@32^@40
v40@0:8@16@24@32
v32@0:8@16@24
@"<MTLTexture>"
@40@0:8@16@24o^@32
B40@0:8@16@24o^@32
@40@0:8@16@24@32
Q32@0:8@16@?24
Q24@0:8@16
@24@0:8Q16
@"NSIndexSet"
#24@0:8@16
v32@0:8@16@?24
B28@0:8@16B24
B36@0:8@16@24B32
B40@0:8@16@24@?32
@"NUComposition"
{?="hasDidAdd"B"hasDidRemove"B"hasDidUpdate"B"hasDidUpdateMultiple"B"hasClassForController"B}
@"NSDictionary"
@"<PICompositionControllerDelegate>"
@48@0:8@16@24@32o^@40
B32@0:8@16o^@24
@56@0:8@16@24@32@40o^@48
@44@0:8@16@24B32o^@36
B48@0:8@16@24@32o^@40
@36@0:8@16B24o^@28
@"NSData"
d40@0:8@16d24^B32
@32@0:8@16q24
{CGPoint=dd}72@0:8{CGPoint=dd}16{CGRect={CGPoint=dd}{CGSize=dd}}32q64
{CGPoint=dd}32@0:8r^{CGPoint=dd}16Q24
@56@0:8@16@24@32@40@48
@"NSURL"
{?={?=qq}{?=qq}}16@0:8
v48@0:8{?={?=qq}{?=qq}}16
{?="origin"{?="x"q"y"q}"size"{?="width"q"height"q}}
v32@0:8@16i24B28
i16@0:8
{CGRect={CGPoint=dd}{CGSize=dd}}60@0:8i16@20{CGRect={CGPoint=dd}{CGSize=dd}}28
@108@0:8@16@24@32@40@48@56@64@72@80@88B96o^@100
@"NUCVPixelBuffer"
@"PTGlobalRenderingMetadata"
@"PIPortraitVideoMetadataSample"
@24@0:8q16
@120@0:8@16{?={?=qiIq}{?=qiIq}}24{?=qiIq}72{?=qiIq}96
@24@0:8o^@16
{?={?=qiIq}{?=qiIq}}16@0:8
{?="start"{?="value"q"timescale"i"flags"I"epoch"q}"duration"{?="value"q"timescale"i"flags"I"epoch"q}}
v40@0:8@16@24d32
{?=qiIq}32@0:8@16@24
@"NSMutableDictionary"
@"NUIdentifier"
@"NUAdjustment"
@"<PIParallaxFilterCache>"
@24@0:8d16
@"PFParallaxLayerStack"
@56@0:8@16@24@32@40q48
{?=qq}32@0:8{?=qq}16
i32@0:8{?=qq}16
^{__CVBuffer=}32@0:8@16o^@24
@"NSObject<OS_dispatch_group>"
@"NSObject<OS_dispatch_queue>"
@"<NUFaceDetectionResult>"
@"NSURL"24@0:8@"NSString"16
@"NSBundle"
q24@0:8@16
@"PFParallaxColor"
@"PFParallaxColor"16@0:8
v24@0:8@"PFParallaxColor"16
@48@0:8@16@24@32@40
@"PIParallaxStyleRecipe"
@48@0:8@16{?=qiIq}24
@24@0:8@"NSDictionary"16
@"NSDictionary"16@0:8
@48@0:8{?=qiIq}16d40
v24@0:8@"NSCoder"16
@24@0:8@"NSCoder"16
@48@0:8q16q24q32d40
@"PFParallaxLayerStyle"24@0:8@"NSString"16
@"NUComposition"16@0:8
@"<PFParallaxAssetRegions>"16@0:8
@"PFParallaxLayout"16@0:8
@"<PFParallaxLayoutConfiguration>"16@0:8
@"PIParallaxColorAnalysis"16@0:8
@"PFParallaxLayerStyle"16@0:8
@"NSURL"16@0:8
B48@0:8@16^B24^B32o^@40
@"PFParallaxAssetResource"
@"<PFParallaxAssetRegions>"
@"PIParallaxColorAnalysis"
@"PISegmentationContextInfo"
@32@0:8@16#24
v72@0:8{?={?=qiIq}{?=qiIq}}16@64
v40@0:8d16d24@?32
@"NURuleSystem"
@88@0:8{?=qiIq}16{?=[3]}40
{?=[3]}16@0:8
{?="columns"[3]}
{?=[3]}40@0:8{?=qiIq}16
@"NUKeyframeSequenceMatrixFloat33"
{?=ffff}16@0:8
v32@0:8{?=ffff}16
@"<NUPurgeableStorage>"
@"NUImageHistogram"
{?="r"f"g"f"b"f"a"f}
@56@0:8@16d24d32d40o^@48
@"NUPurgeableStoragePool"
@"NSMutableArray"
@"AVAsset"
@72@0:8@16{?={?=qq}{?=qq}}24Q56@64
{?={?=qq}{?=qq}}96@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{?={?=qq}{?=qq}}48d80d88
{?=ddd}56@0:8@16{?={?=qq}{?=qq}}24
@28@0:8@16B24
{?={?=[4d]}{?=[4d]}d}16@0:8
@88@0:8{?={?=[4d]}{?=[4d]}d}16
{?=[4d]}16@0:8
@48@0:8{?=[4d]}16
B48@0:8{?=[4d]}16
{?=[4d]}48@0:8{?=[4d]}16
{?=[4d]}88@0:8{?={?=[4d]}{?=[4d]}d}16
{?=dd}104@0:8{?={?=[4d]}{?=[4d]}d}16@88d96
{?={?=[4d]}{?=[4d]}d}48@0:8@16@24d32@40
@"NUBufferRenderClient"
@"NUImageDataClient"
@32@0:8@16^@24
B40@0:8@16@24^@32
@"NSArray"32@0:8@16^@24
B40@0:8@16@"NSMutableDictionary"24^@32
@"NSNumber"32@0:8@"NSDictionary"16^@24
B40@0:8@"NSNumber"16@"NSMutableDictionary"24^@32
{?=qq}16@0:8
v32@0:8{?=qq}16
@"NUImageExportRequest"
{?="width"q"height"q}
@"NUImageGeometry"
@"NUPriority"
@"NUImageExportFormat"
v48@0:8@16@24@32@?40
v40@0:8@16@24@?32
@48@0:8@16@24@32@?40
@72@0:8@16@24@32@40@48@56@?64
@48@0:8@16@24@32^@40
v64@0:8@16@24@32@40@48@?56
{CGRect={CGPoint=dd}{CGSize=dd}}24@0:8@16
{CGRect={CGPoint=dd}{CGSize=dd}}56@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16@48
{CGRect={CGPoint=dd}{CGSize=dd}}64@0:8q16{CGRect={CGPoint=dd}{CGSize=dd}}24@56
@48@0:8Q16Q24Q32@40
@40@0:8Q16Q24Q32
@"PFParallaxLayerStack"16@0:8
v40@0:8q16@24@?32
@"NSError"
@"PIParallaxStyle"
v64@0:8{?={?=qiIq}{?=qiIq}}16
@56@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24
@32@0:8@16d24
@40@0:8@16{CGSize=dd}24
{PISegmentationBimodalScore=fff}24@0:8@16
f64@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24@56
f32@0:8@16@24
{PISegmentationClockOverlapResult=@Qdd}72@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16@48@56@64
{PISegmentationClockOverlapResult=@Qdd}76@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16@48@56@64B72
d64@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16@48@56
d88@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{CGSize=dd}48@64@72@80
d96@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{CGSize=dd}48@64@72^d80@88
{CGRect={CGPoint=dd}{CGSize=dd}}72@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16d48{CGPoint=dd}56
d120@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{CGPoint=dd}48{PISegmentationClockOverlapResult=@Qdd}64@96@104@112
d80@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16^B48@56@64@72
{CGRect={CGPoint=dd}{CGSize=dd}}96@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{CGSize=dd}48d64@72@80@88
{PISegmentationInactiveResult={CGRect={CGPoint=dd}{CGSize=dd}}{CGRect={CGPoint=dd}{CGSize=dd}}}92@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{CGSize=dd}48B64@68@76@84
@104@0:8@16@24@32{CGRect={CGPoint=dd}{CGSize=dd}}40{CGRect={CGPoint=dd}{CGSize=dd}}72
@112@0:8@16@24@32@40{CGRect={CGPoint=dd}{CGSize=dd}}48{CGRect={CGPoint=dd}{CGSize=dd}}80
@64@0:8@16@24{CGRect={CGPoint=dd}{CGSize=dd}}32
@40@0:8@16Q24o^@32
@40@0:8@16I24I28o^@32
@40@0:8@16@24#32
@"PTTimedRenderingMetadata"
v84@0:8@16{?=qq}24{?=qq}40i56q60@68@?76
v76@0:8@16{?=qq}24{?=qq}40i56q60@68
@68@0:8@16{?=qq}24{?=qq}40i56q60
@"PTRenderPipeline"
@"<PTRenderState>"
@"<MTLDevice>"
@"NSDate"
v48@0:8q16^d24^d32^d40
@"PIFaceObservationCache"16@0:8
v24@0:8@"PIFaceObservationCache"16
v36@0:8@16@24B32
f24@0:8@16
B32@0:8{?=qq}16
B40@0:8@16{?=qq}24
@"PIFaceObservationCache"
B32@0:8{CGSize=dd}16
{?="exposure"d"contrast"d"brightness"d"shadows"d"highlights"d"black"d"rawHighlights"d"localLight"d}
@36@0:8@16@24B32
@52@0:8@16@24@32q40B48
@44@0:8@16{CGSize=dd}24B40
^{CGImage=}24@0:8@16
B32@0:8@16^q24
@40@0:8@16Q24^{CGColorSpace=}32
@64@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16@48d56
@48@0:8{?=qq}16@32Q40
@64@0:8@16@24{?={?=qq}{?=qq}}32
B60@0:8@16i24^{CGColorSpace=}28@36@44o^@52
@"<NURenderer>"
@"<NUSurfaceStorage>"
@"NSObject<OS_dispatch_semaphore>"
@"VNImageHomographicAlignmentObservation"16@0:8
@"VNImageHomographicAlignmentObservation"
@64@0:8d16d24d32d40d48d56
B28@0:8{PISegmentationBimodalScore=fff}16
@44@0:8@16@24@32B40
@60@0:8@16@24B32{CGSize=dd}36q52
@"PTCinematographyScript"
@"NSCache"
@"CIColor"
^{CGColorSpace=}16@0:8
{vector<float, std::allocator<float>>=^f^f{__compressed_pair<float *, std::allocator<float>>=^f}}24@0:8@16
@48@0:8r^f16r^f24r^f32r^f40
@40@0:8@16{?=qq}24
@"NUFaceDetectionRequest"
v24@0:8@?<B@?@"NSURL">16
i40@0:8q16@24@?32
v20@0:8i16
i24@0:8@?16
i40@0:8q16@"PFParallaxAssetResourceOptions"24@?<v@?@"PFParallaxAssetResource"@"NSError">32
i24@0:8@?<v@?@"NSArray"@"NSArray"@"NSError">16
@68@0:8^v16Q24Q32q40Q48i56^{CGColorSpace=}60
@68@0:8@16Q24Q32q40Q48i56^{CGColorSpace=}60
^v16@0:8
^{CGColorSpace=}
@"NSMutableData"
v24@0:8^{?=Bffff[3f]}16
^f16@0:8
@24@0:8^f16
@32@0:8d16@24
Q32@0:8@16o^@24
^{__CVBuffer=}16@0:8
@"NUPixelFormat"16@0:8
@"NUColorSpace"16@0:8
@"CIRenderInfo"
@48@0:8@16@24d32@40
@"<NUImageBuffer>"36@0:8@"CIImage"16B24o^@28
@"PFParallaxLayer"48@0:8@"<NUImageBuffer>"16@"CIImage"24d32@"NSString"40
@"CIImage"32@0:8@"CIImage"16@"NSString"24
@"_PIParallaxLayerStackDebugImageCollector"
v88@0:8@16@24@32@40@48@56@64@72@80
@36@0:8@16B24#28
@20@0:8B16
@36@0:8d16d24B32
@104@0:8@16{?=[3]}24{CGRect={CGPoint=dd}{CGSize=dd}}72
@40@0:8@16@24d32
@"CIVector"
@32@0:8^{CGImage=}16q24
@48@0:8@16@24q32^{CGImage=}40
@72@0:8@16Q24@32@40@48@56@?64
@56@0:8@16@24@32Q40@?48
@40@0:8@16Q24^@32
@40@0:8@16@24@?32
v32@0:8@16Q24
v28@0:8B16@?20
v40@0:8@16q24@?32
@"NURenderContext"
@"PIParallaxSegmentationItem"
@"<PFParallaxAsset>"
B24@0:8Q16
@32@0:8q16@24
B32@0:8^{?={?=qq}{?=qq}}16o^@24
B48@0:8^{CGRect={CGPoint=dd}{CGSize=dd}}16^{CGRect={CGPoint=dd}{CGSize=dd}}24@32o^@40
{CGRect={CGPoint=dd}{CGSize=dd}}96@0:8{?=qq}16{?=qq}32{?=qq}48{CGRect={CGPoint=dd}{CGSize=dd}}64
{?="p75"d"p98"d"autoValue"d"g98"d}
{?="sat"d"contrast"d"cast"d}
v36@0:8^f16f24f28f32
f24@0:8r^f16
d40@0:8d16d24d32
@64@0:8@16{?={?=qq}{?=qq}}24@56
@144@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24{CGRect={CGPoint=dd}{CGSize=dd}}56{?=[3]}88@136
@"PIReframeKeyframeSequence"
@"<NUVideoProperties>"
v32@0:8{CGVector=dd}16
v64@0:8{?=[3]}16
{CGVector=dd}16@0:8
{CGVector="dx"d"dy"d}
@24@0:8r^{FigLivePhotoMetadata=I{FigLivePhotoMetadataV1Struct=fqffffffccSI[0{FigLivePhotoDetectedFaceV1Struct=qffffisS}]}}16
{CGVector=dd}24@0:8r^{FigLivePhotoMetadata=I{FigLivePhotoMetadataV1Struct=fqffffffccSI[0{FigLivePhotoDetectedFaceV1Struct=qffffisS}]}}16
{?=[3]}32@0:8r^{FigLivePhotoMetadata=I{FigLivePhotoMetadataV1Struct=fqffffffccSI[0{FigLivePhotoDetectedFaceV1Struct=qffffisS}]}}16^B24
@"AVAssetTrack"
{CGAffineTransform="a"d"b"d"c"d"d"d"tx"d"ty"d}
B24@0:8d16
@48@0:8@16{CGPoint=dd}24d40
v56@0:8@16@24@32@40@?48
B32@0:8^{CGImage=}16@24
B40@0:8^{CGImage=}16@24@32
{?={?=qq}{?=qq}}40@0:8{CGPoint=dd}16d32
v40@0:8r^f16^S24Q32
v40@0:8r^S16^f24Q32
@48@0:8f16f20f24f28Q32{?=ii}40
{?=ii}16@0:8
v24@0:8{?=ii}16
{?="major"i"minor"i}
@48@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16
@48@0:8@16@24q32@40
@52@0:8@16@24@32f40q44
@40@0:8@16q24@32
v128@0:8d16{?=qq}24{?={?=qq}{?=qq}}40{?={?=qq}{?=qq}}72^{?={?=qq}{?=qq}}104^{?={?=qq}{?=qq}}112^{?={?=qq}{?=qq}}120
@56@0:8@16{?={?=qq}{?=qq}}24
B56@0:8@16@24@32@40o^@48
v52@0:8@16@24{CGPoint=dd}32B48
^{CGImage=}32@0:8^{CGImage=}16^{CGImage=}24
@64@0:8@16@24@32@40q48o^@56
@52@0:8@16@24@32B40o^@44
@36@0:8@16B24@28
@64@0:8{?={?=qiIq}{?=qiIq}}16
^{__CVBuffer=}24@0:8^{CGImage=}16
B32@0:8@16^{__CVBuffer=}24
^{__CVBuffer=}32@0:8{CGSize=dd}16
v24@0:8^{__CVBuffer=}16
@32@0:8d16d24
{CGPoint=dd}44@0:8i16d20d28@36
ffffff
[@q=
433333
?ffffff
?YAH
pCh?
@<,_
@+~h
?ffffff
?333333
?(D#$w
333333
333333
?UUUUUU
?______
?TTTTTT
zt?:
Ww'&l
MbP?
333333
?ffffff
?ffffff
?333333
333333
ffffff
?333333
333333
@UUUUUU
Q@333333
?ffffff
z>UUUUUU
UUU?
]?~0d>
@(#)PROGRAM:PhotoImaging  PROJECT:Neutrino-512.0.160
0Xr
?g|_\
ADBE
mntrRGB XYZ 
;acspAPPL
none
-ADBE
cprt
2desc
kwtpt
bkpt
rTRC
gTRC
bTRC
rXYZ
gXYZ
bXYZ
text
Copyright 2000 Adobe Systems Incorporated
desc
Adobe WGG linear
XYZ 
XYZ 
curv
XYZ 
XYZ 
iXYZ 
#?6t>=
<Xa<
<R>)<
A><QMI<
N-=y#3=*
OE=&
N,>;
2>F(6>
<>&T@>u
dN>;
Gl>#.p>
?Jy!?
V*?9%,?w
1?l{3?!X5?
77?"
NF=O
 >Me!>
">LP#>
)>k'*>"
^+>w
,>o+->D
->O\.>
1>uv2>N
T5>y
5>pw6>
7>t'8>M
D9>F
`:> 
z;>%
=>34>>Y
>>=H?>
A> zB>
E>>"F>]
F>9*G>
0H>K
;K>o
<L>l
;M>Z
M>;:N>
O>U2P>
P>k,Q>
Q>1%R>0
gW>i
HY>#
Y>&7Z>)
$[>$
]>B[^>
Mc>r
0d>h
f>cCg>
ni>r
Jj> 
%k>G
l>oFm>
n>gao>
q>!wq>L
q>4Kr>
+u>`
u>Acv>
x>35y>
z>U3{>
{>(c|>
,}>h
?%"?
+?Y5?
>?|H?
n?Bx?
< ?(D ?8L ?GT ?W\ ?gd ?vl ?
!?F%!?E-!?D5!?C=!?AE!?@M!?PU!?O]!?Me!?Lm!?Ku!?9}!?8
,"?v4"?u<"?cD"?QL"??T"?-\"?
9#?lA#?ZI#?7Q#?
x#?g
<$?kD$?8L$?
c$?{k$?Gs$?
%?B&%?
=%?RE%?
\%?Rd%?
{%?A
&?c%&?
<&?1D&?
S&?B[&?
j&?Tr&?
"'?9*'?
1'?~9'?)A'?
H'?nP'?
_'?Mg'?
v'?,~'?
(?l%(?
4(?)<(?
C(?^K(?
i(?.q(?
x(?R
)?R&)?
-)?T5)?
<)?gD)?
K)?iS)?
Z)?kb)?
i)?~q)?
*?1%*?
,*?"4*?
C*?{J*?
Q*?mY*?
`*?^h*?
o*?>w*?
+?)"+?
0+?a8+?
?+?0G+?
V+?h]+?
d+?7l+?
{+?o
+,?a3,?
B,?fI,?
X,?l_,?
n,?qu,?
-?$%-?|,-?
;-?_B-?
P-?CX-?
n-?\u-?
+.?<2.?
G.?3O.?zV.?
d.?+l.?`s.?
6/?K=/?
Y/?5a/?jh/?
?0?AF0?fM0?
q0?!x0?E
?1?,F1?@M1?TT1?h[1?|b1?
=2?1D2?4K2?7R2?KY2?N`2?Qg2?Tn2?Xu2?[|2?^
3?t$3?w+3?j23?m93?_@3?cG3?UN3?XU3?K\3?=c3?@j3?2q3?6x3?(
V4?t]4?gd4?Hk4?;r4?
35?`:5?BA5?$H5?
c5?{j5?\q5?>x5? 
#6?o*6?@16?
L6?eS6?GZ6?
u6?[|6?,
7?|&7?<-7?
A7?oH7?/O7?
c7?Qj7?
~7?b
8?E
'8?E.8?
;8?tB8?4I8?
V8?d]8?$d8?
q8?Sx8?
&9?w-9?&49?
A9?4H9?
U9?B\9?
i9?Pp9?
}9?^
:?5$:?
1:?"8:?
>:?oE:?
R:?\Y:?
f:?Hm:?
z:?$
;?$;?
3;?X:;?
G;?#N;?
T;?O[;?
a;?{h;?
u;?5|;?
<?f
-<?+4<?
:<?FA<?
G<?QN<?
T<?l[<?
u<? |<?
%=?I,=?
2=?T9=?
?=?NF=?
L=?XS=?
Y=?R`=?
f=?Lm=?
s=?Wz=?
">?A)>?
/>?*6>?
<>?$C>?
P>?zV>?
\>?cc>?
i>?Lp>?
v>?5}>?
*??]1??
7??6>??
K??zQ??
W??B^??
w??N~??
*@?H1@?
=@?[D@?
J@?"Q@?~W@?
]@?4d@?
p@?Ww@?
)A?o/A?
<A?pBA?
OA?aUA?
bA?bhA?
uA?S{A?
B?kB?
+B?+2B?v8B?
EB?UKB?
WB?5^B?odB?
qB?OwB?
C?u C?
,C?33C?m9C?
EC?+LC?eRC?
^C?#eC?]kC?
~C?U
&D?M,D?v2D?
ED?MKD?
]D?$dD?MjD?
0E??6E?h<E?
UE?6[E?_aE?
zE?,
,F?+2F?T8F?l>F?
]F?0cF?IiF?aoF?yuF?
G?S G?l&G?
PG?WG?%]G?=cG?DiG?]oG?uuG?|{G?
H?)%H?A+H?H1H?P7H?W=H?pCH?wIH?
I?S"I?[(I?Q.I?Y4I?`:I?W@I?_FI?ULI?]RI?SXI?[^I?cdI?YjI?apI?WvI?N|I?V
SJ?}YJ?t_J?keJ?akJ?XqJ?OwJ?F}J?,
)K?w/K?n5K?T;K?JAK?0GK?'MK?
vK?x|K?o
3L?i9L?>?L?$EL?
bL?whL?]nL?CtL?
M?:$M? *M?
AM?ZGM?/MM?
dM?{jM?PpM?%vM?
0N?~6N?S<N?
SN?\YN?1_N?
pN?dvN?9|N?
O?1$O?
/O?~5O?T;O?
LO?eRO?*XO?
cO?wiO?;oO?
!P?t'P?8-P?
8P?u>P?(DP?
OP?eUP?)[P?
fP?UlP?
}P?E
Q?E
Q?p#Q?$)Q?
4Q??:Q?
EQ?ZKQ?
VQ?t\Q?(bQ?
mQ?CsQ?
~Q?^
R?s
R?}#R?0)R?
4R?*:R?
ER?4KR?
VR?-\R?
gR?&mR?
rR?}xR? ~R?
'S?<-S?
8S?%>S?
CS?kIS?
TS?SZS?
eS?+kS?
pS?qvS?
/T?H5T?
:T?}@T?
KT?DQT?
VT?y\T?
gT?@mT?
rT?uxT?
U?h%U?
;U?BAU?
FU?ULU?
QU?yWU?
bU?0hU?
mU?CsU?
xU?g~U?
$V?=*V?
/V?P5V?
:V?d@V?
EV?wKV?
VV?\V?
lV?"rV?
wV?5}V?
W?}"W?
SW?yYW?
^W?|dW?
iW?moW?
tW?pzW?
X?>$X?
)X?0/X?
4X?!:X?
OX?eUX?
ZX?W`X?
eX?8kX?
pX?)vX?
Y?l$Y?
)Y?</Y?
DY?oJY?
OY??UY?
ZY? `Y?
kY?apY?
uY?B{Y?
#Z?g(Z?
-Z?73Z?
>Z?gCZ?
HZ?8NZ?
XZ?h^Z?
cZ?'iZ?
sZ?WyZ?
[?! [?p%[?
*[?/0[?
:[?O@[?
K[?]P[?
[[?|`[?
e[?+k[?
u[?J{[?
!\?g&\?
1\?e6\?
A\?cF\?
Q\?`V\?
a\?^f\?
p\?\v\?
]?m ]?
+]?Y0]?
:]?F@]?
J]?"P]?qU]?
`]?^e]?
o]?:u]?
(^?,.^?j3^?
=^?FC^?
S^?OX^?
h^?Ym^?
w^?$}^?s
_?>%_?|*_?
4_?6:_?u?_?
I_?/O_?\T_?
d_?Ui_?
y_?=~_?{
 `?[%`?
4`?2:`?`?`?
O`?7T`?dY`?
i`?;n`?is`?
#a?$)a?Q.a?
Ca?4Ha?bMa?
ba?Ega?rla?
b?|b?
!b?>&b?Z+b?
Eb?IJb?fOb?
cb?'ib?Dnb?qsb?
,c?<1c?Y6c?u;c?
Uc?2Zc?N_c?kdc?
d?:!d?F&d?c+d?
Td?2Yd?>^d?Zcd?fhd?
$e?5)e?A.e?L3e?X8e?u=e?
ze? 
f?[f?f
g?xg?s
g?u g?
%g?|*g?w/g?
4g?~9g?y>g?
Hg?{Mg?
Wg?}\g?xag?
kg?zpg?uug?
zg?|
sh?uxh?p}h?k
Mi?~Ri?iWi?d\i?Nai?8fi?3ki?
&j?k+j?U0j??5j?;:j?%?j?
aj?ffj?Qkj?;pj?%uj?
%k?|*k?f/k?P4k?*9k?
Vk?`[k?J`k?4ek?
l?S#l?,(l?
;l?}@l?VEl?0Jl?
bl?Zgl?3ll?
m?=m?
m?l$m?E)m?
<m?tAm?NFm?
Ym?l^m?Ecm?
vm?t{m?=
n?%$n?
2n?X7n?2<n?
Jn?fOn?.Tn?
bn?sgn?<ln?
zn?o
o?l"o?5'o?
5o?X:o?
Ho?jMo?3Ro?
`o?Veo?
so?hxo?1}o?
p?Ap?
-p?U2p?
@p?VEp?
Sp?XXp? ]p?
fp?Ykp?"pp?
yp?Z~p?#
q?Z
q?J$q?
-q?r2q?*7q?
@q?cEq?
Sq?SXq?
aq?{fq?3kq?
tq?[yq?
,r?@1r?
:r?W?r?
Hr?nMr?&Rr?
[r?=`r?
ir?Tnr?sr?
wr?l|r?$
 s?>%s?
.s?D3s?
<s?JAs?
Js?QOs?
Xs?W]s?
fs?]ks?
ts?dys?
t?K!t?
*t?@/t?
8t?6=t?
Ft?+Kt?
Ot?zTt?!Yt?
]t?obt?
kt?ept?ut?
yt?J~t?
 u?F%u?
.u?+3u?
7u?h<u?
Eu?MJu?
Su?2Xu?
\u?pau?
ju?Tou?
xu?(}u?
v?9v?
v?:#v?
'v?w,v?
5v?K:v?
>v?xCv?
Lv?LQv?
Uv?yZv?
cv?Mhv?
lv?zqv?
zv?N
w?t$w?
-w?82w?
6w?T;w?
Mw?DRw?
Vw?`[w?
dw?#iw?
mw??rw?
vw?l{w?
x?O$x?
(x?k-x?
6x?;x?
?x?(Dx?
Hx?DMx?
Qx?`Vx?
Zx?|_x?
qx?:vx?
zx?V
"y?='y?
+y?Y0y?
4y?e9y?
=y?pBy?
Fy?{Ky?
]y?(by?
fy?3ky?
oy??ty?
xy?J}y?
z?W$z?
(z?b-z?
1z?m6z?
:z?h?z?
Cz?sHz?
Lz?nQz?
Uz?hZz?
^z?scz?
gz?nlz?
pz?yuz?
yz?t~z?
{?! {?
L{?zQ{?
U{?dZ{?
^{?^c{?
g{?Yl{?
p{?Cu{?
y{?=~{?
0|?i5|?
9|?S>|?
B|?=G|?
K|?'P|?
a|?of|?
j|?Xo|?
s|?Bx|?
||?,
}?%!}?
)}?s.}?
2}?]7}?
;}?5@}?
Q}?\V}?
Z}?5_}?
p}?\u}?
y}?F~}?
!~?>&~?
/~?{3~?
7~?S<~?
@~?,E~?
M~?YR~?
V~?1[~?
d~?nh~?
l~?7q~?
z~?t~~?
333333
?333333
?ffffff
?YAH
?YAH
sU?gDi?
z?-C
Fail: %{public}@
job: %{public}@
Trace:
%{public}@
Trace:
%{public}@
Tap-to-track: client requested stop at %@
Tap-to-track: tracking lost at %@
PIColorNormalizationAnalysis
Tried to set sceneLabel to unsupported value: %ld
Setting bounding boxes based on orientation (%@) and observations: %@
Setting face bounding boxes based on orientation (%@) and observations: %@
Continue: %{public}@
Failed to load compute pipeline: %@
Failed to load user palette '%@', error: %@
class %@ is not the correct type, its superclass should be %@
Unable to serialize finalized composition: %{public}@
Error executing command buffer '%{public}@': %{public}@
CPV rendering failure, returned status %d
Could not apply PIPortraitVideoProcessor: %@
Failed to prewarm portrait renderer: %{public}@
Failed to obtain video properties: %{public}@
Using recipe directory at '%{public}@'
Failed to load style recipe for identifier '%@', error: %@
Missing configuration file '%{private}@'
PARALLAX CLOCK: luminance is %f - %@
ColorBGStandard: suggested %@ for background luminance of %.2f
ColorBG color: %@ -> neutral: %@ lowKey: %0.3f highKey: %0.3f
Couldn't find a single candidate style for category %{public}@, falling back to Original
Median luminance: %f
Percent above chroma min: %0.0f%%, max hues: %ld
Found %ld dominant hues: %@
Found %ld dominant grays: %@
facerect yiq = %.5f, %.5f, %.5f
Choosing gray world instead of gray edge
Brightness is %@, greenChannelPercentage is %f, Sushi: %@
aperture=%@, shutterSpeed=%@, iso=%@
Buffer sizes are not the same: %{public}@ vs %{public}@
Buffer size is %@, bytes per row is %ld, dark_thr=%f, sat_thr=%f, noise_thr=%f
all done summing, np_gw is %d, np_ge is %d
wp_ge {%f, %f, %f, %f} wp_gw {%f, %f, %f, %f}, Sushi? %@
RGB = %f, %f, %f
YIQ = %f, %f, %f from %f, %f, %f
Failed to export image to %@: %@
Failed to export image to data: %@
Failed to prepare video metadata: %@
Unexpected Live Photo export format pairing. Video codec (%{public}@) and image export format (%{public}@)
Export Composition: exportImage: %{public}@ / exportVideoComplement: %{public}@ / exportVideo: %{public}@ / exportVideoPosterFrame: %{public}@
Export Composition: exporting image
Export Composition: exporting image complete
Export Composition: exporting video complement
Export Composition: exporting video complement complete
Export Composition: exporting video
Export Composition: exporting video complete
Export Composition: exporting video poster frame
Export Composition: exporting video poster frame complete
Export Composition: exporting overcapture image
Export Composition: exporting overcapture image complete
Export Composition: exporting overcapture video
Export Composition: exporting overcapture video complete
Export Composition: waiting on notify
Export Composition: notify triggered
Export Composition: calling completion with error: %{public}@
Export Composition: callling completion with result: %{public}@
failed to render auxiliary image data: %@
Skipping applyVideoOrientationAsMetadata. Bounces and Autoloops are not supported. Falling back to burned-in orientation.
Skipping applyVideoOrientationAsMetadata. Flipped orientations are not supported. Falling back to burned-in orientation.
Failed to export video: %@
invalid format version: %@
Failed to update layout: %{public}@
Failed to compute clock overlap: %{public}@
Failed to compute clock material: %{public}@
Failed to render layer stack, error: %{public}@
Pixel-based headroom zoom final range %@,%@: %@,%@
Unable to calculate a new inactiveRect; falling back to visible frame
PIParallaxLegacyPosterStyle.localLight
PIPortraitVideoMetadataSample: unexpected number of items for identifier %@: %@
PIPortraitVideoMetadataSample: item %@ is not of expected class %@
Error unarchiving cameraInfo metadata: %@
CINE: allocating new PT renderer: %@
CINE: recycling unused PT renderer: %@ (%lu remaining)
CINE: need new render state due to rendering version mismatch
CINE: need new render state due to sensor mismatch
CINE: allocating new renderState with metadata: %p
Requesting forced cleanup of Vision caches
Unsupported filter parameter: %{public}@
Parameter %{public}@ is not a valid color value: %{public}@
Parameter %{public}@ is not a valid number value: %{public}@
Error evaluating filter definition: %@, error: %@
CPV asset editability: onAppleSilicon: %s, hasMetalDeviceForPortrait: %s, oniOS: %s
CPV assets aren't editable because there is no metal device with support for portrait rendering
CPV assets aren't editable because the device has no ANE
CPV assets are editable
HDR asset not editable because this device doesn't support 10-bit HEVC encoding
HDR asset not editable because this device doesn't support HDR
HDR asset not editable because editing not supported on DolbyVision 5
CPV asset checking for editability
CPV asset not editable because it's in a legacy, unsupported format
CPV asset editable due to override
CPV asset not editable because CPV assets are not editable on this device
CPV asset not editable because this asset is not playable on this device
CPV asset not editable because this device doesn't support HDR
CPV asset is editable
validation issue: recipe is blank on the autoloop adjustment
validation issue: no flavor specified on the autoloop adjustment
validation issue: Missing inputCorrectionInfo on a redeye adjustment
validation issue: Missing depthInfo on a depth adjustment
validation issue: Missing portraitInfo on a portrait adjustment
validation issue: invalid trim startTime or endTime
Can PLPhotoEditPFDataConverter interpret identifier %{public}@? %@. Version %{public}@? %@
Failed to load filter named '%@'
PFSizeGetAspectRatio produced an undefined aspect ratio from size %@. Returning %f. Use PFSizeGetAspectRatioWithDefault() to provide a value for this case.
PARALLAX CLOCK: execution time %.2f ms
waitUntilReadyForMoreData: waited for %0.1fms
Writing long-exposure image to %{public}@
Writing long-exposure motion mask to %{public}@
Still image node:
Still image geometry:
Still image:
clap=%@, crop=%@, fullSize=%@, videoScale=%@ => guide rect: %@
High-resolution image registration failure : %@
Unable to load scoring ranges dictionary from %@, error %@
Unable to load scoring plist, using fallback
Failed to load scoring configuration: %@
MediaAnalysis not available
currentSystemCanRenderAsset: error retrieving rendering version from asset: %@
Failed to render %{public}@, error: %{public}@
Failed to allocate pixel buffer for render cache entry (size=%ldx%ld, format=%{public}@)
Cache miss for image: %{public}@ cost: %lu digest: %llx
Loading long-exposure fusion tuning parameters from file: %@
Failed to load fusion tuning parameters.
Using fusion tuning parameters: {kNCCBlurHalfSize=%d, kNCCEdge0=%f, kNCCEdge1=%f, kBlendMaskThreshold0=%f, kBlendMaskThreshold1=%f}
Missing inputImage
Missing inputMaskImage
Missing inputStillImage
Missing inputRenderScale
Missing inputVideoScale
Missing inputAlignmentExtent
Malformed inputAlignmentExtent vector
Missing inputAlignmentTransform
Malformed inputAlignmentTransform vector
Writing intermediate long exposure fusion images to : %@
Evaluating pipeline as HDR input
PISegmentationLoader.memory.warmUp
Ensuring segmentation resources with counter %ld
Freeing segmentation loader resources with counter %ld
PISegmentationLoader.memory.purge
PISegmentationLoader.item
Warning: PISegmentationLoader layout configuration unspecified! Using override layout configuration '%{public}@'
Warning: Override layout configuration '%{public}@' not found, using generic fallback
Warning: PISegmentationLoader layout configuration unspecified! Using the layout configuration matching this device
PISegmentationLoader.proxy
PISegmentationLoader.properties
PISegmentationLoader.fullSize
Failed to compute proxy image size, error: %{public}@
Cancelling segmentation loader
Triggering non-foreground user initiated download for asset with local identifier: %{public}@
Loading resource %ld for asset %{public}@, allow download? %d
Successfully loaded resource %ld for asset %{public}@ in %0.3fs
Failed to load resource %ld for asset %{public}@ after %0.3fs, error: %{public}@
Cancelled loading resource %ld for asset %{public}@ after %0.3fs
PISegmentationLoader.segment
PISegmentationLoader.regions
PISegmentationLoader.infill
PISegmentationLoader.layout
PISegmentationLoader.colorAnalysis
PISegmentationLoader.localLightData
Image segmentation response: %@
Image segmentation failed: %{public}@
Full Image color analysis response: %@
Full Image color analysis failed: %{public}@
Foreground color analysis response: %@
Foreground color analysis failed: %{public}@
Background color analysis response: %@
Background color analysis failed: %{public}@
Parallax infill response: %@
Parallax infill failed: %{public}@
MAD pets results: %@, pets face results: %@, error: %@
Failed to load pets regions: %{public}@
Vision detection response: %@
Failed to run face/saliency detection: %{public}@
Parallax layout response: %{public}@
Failed to layout item: %{public}@
Local light data response: %{public}@
Failed to compute local light data: %{public}@
Failed to deserialize segmentation adjustment data: %{public}@
Failed to read orientation for image file: %@, error: %{public}@
PISegmentationLoader.data.read
Failed to read cached segmentation data from: %{public}@, error: %{public}@
Cached segmentation version mismatch: got %ld, expected %ld
Cached segmentation source mode mismatch: got %ld, expected %ld
Cached segmentation disabled flag mismatch: got %d, expected %d
Cached segmentation infill algorithm mismatch: got %ld, expected %ld
Cached segmentation layout configuration mismatch: got %{public}@, expected %{public}@
Cached segmentation classification mismatch: got %{public}@, expected %{public}@
PISegmentationLoader.data.write
Failed to save segmentation data for asset: %{public}@, error:%{public}@
PISegmentationLoader.archive.write
PISegmentationLoader.archive.read
PISegmentationLoader.layerStack.render
PISegmentationLoader.wallpaper.write
Failed to save segmentation item and layer stack to wallpaper URL: %{public}@
Failed to create wallpaper directory: %{public}@
Failed to export segmentation item: %{public}@
PISegmentationLoader.layerStack.write
Failed to export layer stack: %{public}@
Failed to load segmentation item from wallpaper: %{public}@
PISegmentationLoader.layerStack.read
Failed to load layer stack from wallpaper: %{public}@
Failed to render layer stack: %{public}@
Failed to reload segmentation item from wallpaper: %{public}@
Beginning finalization for candidates: %@
No overcapture source found in composition, removing any candidate bits for reframing
Finalizer result contains roll angle degrees: %f pitch angle degrees: %f yaw angle degrees: %f
Finalizer result did not contain a change
Received error while finalizing: %@
Starting horizon auto calculator
Finished horizon auto calculator: %@
Starting perspective auto calculator
Finished perspective auto calculator: %@
Could not write PICropAutoCalculator debug diagnostics. %@
Failed to compute overcapture rect %@
Can't find enabled video track in asset: %@
Asset does not contain Live Photo metadata track
Track does not contain V3 metadata
Error creating asset reader: %@
Can't add metadata output for asset
Failed to start reading asset
Starting metadata extraction
Finished metadata extraction
PISmartToneAutoCalculator submitSynchronous isHDRComposition: %s
PISmartToneAutoCalculator smartTone request submitting: %{public}@
PISmartToneAutoCalculator smartTone result: %{public}@
PISmartToneAutoCalculator localLight request submitting: %{public}@
PISmartToneAutoCalculator localLight result: %{public}@
PISmartToneAutoCalculator submit isHDRComposition: %s
PISmartToneAutoCalculator global result: %{public}@
PISmartToneAutoCalculator going to commit and wait
PISmartToneAutoCalculator committed
PISmartToneAutoCalculator completed
PISmartToneAutoCalculator error: %{public}@
PISmartToneAutoCalculator coalesced
Failed to deserialize layout configuration: %{public}@, error: %{public}@
Can load cold asset? %{public}@ => %{public}@
Failed to deserialize style from dictionary: %{public}@, error: %{public}@
Unknown style option, ignored: %{public}@
Upgrading wallpaper at %{public}@ to %{public}@, options: %{public}@
Failed to export refreshed wallpaper at %{public}@ to %{public}@, error: %{public}@
Successfully exported refreshed wallpaper at %{public}@ to %{public}@
Failed to load poster configuration from: '%{public}@', error: %{public}@
Failed to upgrade poster configuration from: '%{public}@' to: '%{public}@', error: %{public}@
Successfully upgraded poster configuration from: '%{public}@' to: '%{public}@'
Upgrading poster media: SETTING .hasInactiveContent
Upgrading poster media: CLEARING .hasInactiveContent
Upgrading poster media: %{public}@
Successfully upgraded poster media: %{public}@
Failed to upgraded poster media: %{public}@, error: %{public}@
Failed to upgrade %lu poster media
Successfully upgraded %lu poster media
Successfully upgraded poster configuration from '%{public}@' to '%{public}@'
Failed to save poster configuration to '%{public}@', error: %{public}@
Failed to apply red eye repair. error: %{public}@
PIPortraitAutoCalculator getMinMaxSimulatedAperture returned error:%i (minAperture:%f maxAperture:%f version:%d)
Cant get focusRect - Metadata dictionary missing fullSizeWith or fullSizeHeight:
Cant get focusRect - Metadata dictionary missing exif aux dictionary:
Cant get focusRect - Exif aux dictionary missing MWG region dictionary:
Cant get focusRect - MWG region dictionary missing region list:
Cant get focusRect - Region list does not contain a focus rect:
Cant get focusRect - Malformed focus rect dictionary:
Invalid focus rect: {%g,%g,%g,%g}
Couldn't deserialize face observations from metadata: %@, assuming empty, error: %@.
Invalid face indexes from metadata: %@, ignored. Error: %@
Couldn't deserialize face observations from metadata: %@
s-curve black: %f
s-curve point %d: (%f, %f)
s-curve white: %f
red curve: blackPoint = %f, whitePoint = %f
green curve: blackPoint = %f, whitePoint = %f
blue curve: blackPoint = %f, whitePoint = %f
Initializer not available: -[%@ %@], use designated initializer instead.
-[PIParallaxInfillJob initWithRequest:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Parallax/PIParallaxInfillRequest.m
Missing matte image
Invalid matte image size
Invalid matte image
Output image must not be nil
-[PIParallaxInfillJob render:]
Matte image must not be nil
Failed to generate background infill image
Failed to allocate buffer from pool
failed to render
Invalid parameter not satisfying: %s
-[PITapToTrackRenderJob prepare:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Autocalculators/PITapToTrackRequest.m
No metal device for request
Failed to find output video asset
Failed to make asset reader for video
Failed to read first frame for video
Failed to find rect to track at initial point
Failed to add detection and start tracking
Failed to get finalized track from tracking session
kernel vec4 _hdrOffsetPosBW(sampler image) { vec4 im = sample(image, samplerCoord(image)); float offset = (im.r + im.g + im.b) / 3.0f; offset = max(0.0f, offset - 1.0f); return vec4(offset, offset, offset, 0.0f); }
kernel vec4 _hdrOffsetPos(sampler image) { vec4 im = sample(image, samplerCoord(image)); float r = max(im.r - 1.0f, 0.0f); float g = max(im.g - 1.0f, 0.0f); float b = max(im.b - 1.0f, 0.0f); return vec4(r, g, b, 0.0f); }
s_hdrOffsetPos kernel is nil
+[PIPhotoEffectHDR kernel]_block_invoke
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Adjustments/HDR/PIPhotoEffectHDR.m
inputImage cannot be nil
-[PIPhotoEffectHDR outputImage]
inputImage
CIAdditionCompositing
inputBackgroundImage
s_hdrOffsetPosBlackAndWhite kernel is nil
+[PIPhotoEffectBlackAndWhiteHDR kernelBlackAndWhite]_block_invoke
-[PIPhotoEffectBlackAndWhiteHDR outputImage]
+[PIPhotoEffect3DHDR kernel]_block_invoke
-[PIPhotoEffect3DHDR outputImage]
inputThreshold
inputDepthMap
+[PIPhotoEffect3DBlackAndWhiteHDR kernelBlackAndWhite]_block_invoke
-[PIPhotoEffect3DBlackAndWhiteHDR outputImage]
PhotoImaging
PIColorNormalizationFilter
PITempTintFilter
temperature
inputTemperature
tint
inputTint
PIHighKey
CISmartToneFilter
CISmartColorFilter
normalization != nil
+[PIColorNormalizationFilter colorCubeForNormalization:dimension:targetColorSpace:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Adjustments/PIColorNormalizationFilter.m
colorSpace != NULL
PIColorNormalization
candidateForStill
candidateForVideo
candidateForReframe
candidateForPerspective
candidateForHorizon
addRule exception : %@
v24@?0@"NSString"8@?<v@?@"NURuleSystem">16
v24@?0@"NSString"8@"NSString"16
isStitched
validSubjects
hasHorizonLine
hasBuilding
state.stitched == true OR state.perfectlyStitched == true
state.stitchConfidence < $StitchConfidenceThreshold
state.buildingCount > 0
state.buildingConfidence > $BuildingConfidenceThreshold
state.subjectCount >= $SubjectCountMinThreshold
state.allSubjectsInsidePrimaryBounds == true
(state.subjectCount - state.intersectingSubjectCount) > $MaxDisparateSubjects
state.largeSubjectFace == true
state.largestSubjectArea <= $MinimumSubjectSize
state.horizonLinePresent == true
state.horizonLineAngleInDegrees < -$HorizonAngleInDegreesMaxThreshold OR state.horizonLineAngleInDegrees > $HorizonAngleInDegreesMaxThreshold
state.horizonLineAngleInDegrees > -$HorizonAngleInDegreesMinThreshold AND state.horizonLineAngleInDegrees < $HorizonAngleInDegreesMinThreshold
state.video == true
state.videoDuration < $VideoDurationLowerBound
state.videoDuration > $VideoDurationUpperBound
(state.videoDuration * $VideoDurationWithSubjectsRatio) > state.videoDurationWithSubjects
state.videoQualityScore < $VideoQualityScoreThreshold
(state.videoDuration * $VideoDurationBelowMinZoomRatio) < state.videoDurationBelowMinZoom
facts.isStitched == true AND facts.validSubjects == true
facts.isStitched == true AND facts.hasBuilding == true
facts.isStitched == true AND facts.hasHorizonLine == true
state.largeSubject == true
state.backFacing == false
state.deviceIsStationary == true
state.video == true AND state.livePhoto == true
state.pano == true
state.reframingAllowed == false
state.hasAdjustments == true
facts.candidateForStill == true
facts.candidateForVideo == true
platedFood
sunriseSunset
genericLandscape
intensity
sceneLabel
sceneConfidence
faceBoundingBoxes
boundingBoxes
-[PIDisparitySampleJob render:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Autocalculators/PIDisparitySampleRequest.m
Failed to read video frame
Incompatible disparity buffer
inputStrength
inputScale
vec3 localContrast(vec3 im, vec3 shc, float amt) { float midAmt = amt; vec3 neg = min(im, 0.0); vec3 pos = max(im, 1.0)-1.0; im = clamp(im, 0.0, 1.0); float y = dot(im, vec3(0.3333)); y = sqrt(y); y = y*(1.0-y); im = sqrt(im); float pivot = sqrt(shc.g); float a = midAmt*y; float b = -pivot*a; vec3 pix = im.r * vec3(0.299*a) + im.g * vec3(0.587*a) + im.b * vec3(0.114*a) + im + vec3(b); im = mix(im, vec3(pivot), -y*midAmt); im = mix(im, pix, 0.8); im = max(im, 0.0); im *= im; im = im + neg + pos; return im; } vec3 localContrastHLG(vec3 im, vec3 shc, float hlg_scale, float amt) { return localContrast(im.rgb/hlg_scale, shc.rgb/hlg_scale, amt).rgb * hlg_scale; } kernel vec4 _localContrastHDR(__sample im, __sample shc, float hlg_scale, float amt) { float max_comp = max(im.r,max(im.g,im.b)); float threshold = 0.75 * hlg_scale; if (max_comp <= 1.0) { im.rgb = localContrast(im.rgb, shc.rgb, amt); } else if (max_comp < threshold) { vec3 retSDR = localContrast(im.rgb, shc.rgb, amt); vec3 retHDR = localContrastHLG(im.rgb, shc.rgb, hlg_scale, amt); float lerp_t = (max_comp - 1.0) / (threshold - 1.0); im.rgb = mix(retSDR, retHDR, lerp_t); } else { im.rgb = localContrastHLG(im.rgb, shc.rgb, hlg_scale, amt); } return im; }
CIColorMatrix
inputRVector
inputGVector
inputBVector
inputAVector
inputBiasVector
CIColorClamp
inputMinComponents
inputMaxComponents
CIEdgePreserveUpsampleFilter
inputSmallImage
inputSpatialSigma
inputLumaSigma
intermediate.visibleRect.size.width >= 1
-[PIParallaxLayoutHelper intermediateWithZoomStrategy:intermediate:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Parallax/PISegmentationLayout.m
overlapStrategy != PFParallaxUtilityOverlapForceMetadataAvoid
-[PIParallaxLayoutHelper intermediateWithOverlapStrategy:intermediate:]
<%@:%p accept=%@ pref=%@ faces=%@ pets=%@>
regions != nil
+[PISegmentationLayoutRegions dictionaryFromRegions:]
@"NSDictionary"40@?0{CGRect={CGPoint=dd}{CGSize=dd}}8
@"NSArray"16@?0@"NSArray"8
acceptable
preferred
faces
pets
+[PISegmentationLayoutRegions regionsFromDictionary:error:]
B24@?0@8^{CGRect={CGPoint=dd}{CGSize=dd}}16
Expected a rect value
B24@?0@"NSArray"8@"NSMutableArray"16
Expected an array of rect values
layoutConfiguration != nil
+[PISegmentationLayout generateLayoutForLayoutConfiguration:imageSize:regions:parallaxClassification:context:matte:layoutScore:cropScore:clockOverlapAcceptable:]
layoutConfiguration.screenSize.height > 0
imageSize.height > 0
<%@:%p layerOrder:%@ intersectsForeground:%@>
-[_PIParallaxClockLayoutJob initWithRequest:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Parallax/PIParallaxClockLayoutRequest.m
Missing segmentation item
-[_PIParallaxClockLayoutJob prepare:]
Missing parallax layout
Missing renderer
-[_PIParallaxClockLayoutJob render:]
This is an abstract method! Subclass '%@' should provide concrete implementation
-[PIParallaxClockLayoutRequest initWithComposition:]
-[PIParallaxClockLayoutRequest initWithSegmentationItem:]
kernel vec4 highKey(__sample im, float str)
vec3 neg = min(im.rgb, 0.0);
vec3 pos = max(im.rgb, 1.0) - 1.0;
im = clamp(im, 0.0, 1.0);
vec4 im2 = 1.0-((im-1.0)*(im-1.0));
im2 = sqrt(im2);
float y = dot(im.rgb, vec3(.333333));
float y2 = sqrt(1.0-(y-1.0)*(y-1.0));
y2 = mix(y2, smoothstep(0.0, 1.0, y2), 0.5);
vec4 im3 = (y>0) ? im*y2/y : vec4(0.0, 0.0, 0.0, 1.0);
im3 = mix(im3, im2, .7*sqrt(y2));
im3 = mix(im, im3, sqrt(y));
im.rgb = mix(im.rgb, im3.rgb, str) + pos + neg;
return im;
high key kernel is nil
+[PIHighKey kernel]_block_invoke
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Adjustments/PIHighKey.m
-[PIHighKey outputImage]
inputStrength cannot be nil
/System/Library/Frameworks/CoreImage.framework/inp_gen_eds2_00_q16.espresso.weights
CIInpaintingFilter
inputInpaintingMode
Metal unavailable
inputTexture != nil
+[PIParallaxInwardFillKernel fillSourceTexture:intoDestinationTexture:withCommandBuffer:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Parallax/PISegmentationInfillFilter.m
outputTexture != nil
commandBuffer != nil
Missing input texture
-[PIParallaxInwardFillKernel encodeToCommandBuffer:destinationTexture:]
Failed to allocate intermediate texture
pi::inward_fill_down
pi::inward_fill_up
LumaHueChroma
HueTone
v24@?0Q8^B16
version
mode
primaryColors
secondaryColors
suggestionIndices
url != nil
+[PIParallaxColorPalette loadPaletteFromURL:error:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Parallax/PIParallaxColorPalette.m
Failed to parse color palette plist data
+[PIParallaxColorPalette _paletteWithConfigurationDictionary:error:]
Invalid version number
Unsupported palette version
Invalid mode value
Invalid primary color values
Invalid secondary color values
Invalid suggestion indices
Invalid suggestion index
Missing suggestion indices
Unknown color mode: %@
+[PIParallaxColorPalette _serializeColors:mode:]
colorValues != nil
+[PIParallaxColorPalette _loadColorsFromValues:mode:error:]
Invalid color values
primaryColors != nil
-[PIParallaxColorPalette initWithPrimaryColors:secondaryColors:suggestionIndices:]
secondaryColors != nil
suggestionIndices != nil
suggestionIndices.lastIndex < primaryColors.count
Secondary color palette should be empty or equal in size to the primary palette
ColorBGPalette
ColorWashSinglePalette
ColorWashDuotonePalette
plist
Failed to load color palette '%@', error: %@
+[PIParallaxColorPalette loadPaletteWithName:]
Tonal colors: %@
B16@?0Q8
The palette can't be empty
-[PIParallaxColorPalette _lookupColor:withPredicate:]
Failed to find a nearest color
smartTone
smartColor
smartBlackAndWhite
grain
autoEnhance
whiteBalance
cropStraighten
redEye
apertureRedEye
depthEffect
livePhotoKeyFrame
videoPosterFrame
trim
slomo
portraitEffect
orientation
autoLoop
highResFusion
retouch
vignette
sharpen
noiseReduction
definition
curves
levels
selectiveColor
effect
effect3D
mute
rawNoiseReduction
source
videoReframe
debug
sourceSelect
sourceSpatialOvercapture
portraitVideo
videoStabilize
videoCrossfadeLoop
semanticEnhance
enabled
value
primary
PICompositionController(%p): %@
v16@?0@"PISmartToneAdjustmentController"8
com.apple.photo
adjustmentFormatIdentifier
adjustmentFormatVersion
adjustmentData
PICompositionSerializer.m
Invalid parameter not satisfying: %@
adjustments
metadata
file://dummy.jpg
dummy
identifier
PICompositionSerializerDomain
Missing identifierName
Missing definition in conversion map for the adjustment key: 
settings
Unsupported adjustment: 
Configuration does not support Aperature adjustments.
Configuration does not support CTM adjustments.
nil identifierName
inputKeys
omitIfDisabled
Orientation
PICompositionSerializer-geometry
Invalid adjustment stack for needsGeometry=NO
masterWidth
masterHeight
Serialization map has no entry for %@
+[PICompositionSerializer serializeComposition:versionInfo:serializerMetadata:error:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Controllers/Conversion/PICompositionSerializer.m
Serialization map does not contain identifierName
current
auto
formatIdentifier
formatVersion
versionInfo
Missing required key: %@
Value for key %@ has type %@; expected type %@
v32@?0@"NSString"8#16^B24
PICompositionSerializer
exception
dictionary
data is not an NSData: %@: %@
Adjustment for %@ is identity
+[PICompositionSerializer _sanitizeComposition:]
composition
CFBundleVersion
platform
buildNumber
appVersion
schemaRevision
kernel vec4 levelsHDR (sampler src, sampler LUT) { vec4 p,r; vec2 c1,c2; float f; p = sample(src, samplerCoord(src)); vec3 neg = min(p.rgb, 0.0); vec3 pos = max(p.rgb, 1.0)-1.0; p.rgb = clamp(p.rgb, 0.0, 1.0); f = p.r * 1024; c1 = vec2(floor(f)+0.5, 0.5); c2 = vec2(ceil(f)+0.5, 0.5); r = mix(sample(LUT,samplerTransform(LUT,c1)), sample(LUT,samplerTransform(LUT,c2)), fract(f)); p.r = r.r; f = p.g * 1024; c1 = vec2(floor(f)+0.5, 0.5); c2 = vec2(ceil(f)+0.5, 0.5); r = mix(sample(LUT,samplerTransform(LUT,c1)), sample(LUT,samplerTransform(LUT,c2)), fract(f)); p.g = r.g; f = p.b * 1024; c1 = vec2(floor(f)+0.5, 0.5); c2 = vec2(ceil(f)+0.5, 0.5); r = mix(sample(LUT,samplerTransform(LUT,c1)), sample(LUT,samplerTransform(LUT,c2)), fract(f)); p.b = r.b; p.rgb = max(p.rgb, 0.0); p.rgb = p.rgb + pos + neg; return p; }
inputBlackSrc
inputBlackDst
inputShadowSrc
inputShadowDst
inputMidSrc
inputMidDst
inputHilightSrc
inputHilightDst
inputWhiteSrc
inputWhiteDst
inputBlackSrcRGB
inputBlackDstRGB
inputShadowSrcRGB
inputShadowDstRGB
inputMidSrcRGB
inputMidDstRGB
inputHilightSrcRGB
inputHilightDstRGB
inputWhiteSrcRGB
inputWhiteDstRGB
inputBlackSrcRed
inputBlackDstRed
inputShadowSrcRed
inputShadowDstRed
inputMidSrcRed
inputMidDstRed
inputHilightSrcRed
inputHilightDstRed
inputWhiteSrcRed
inputWhiteDstRed
inputBlackSrcGreen
inputBlackDstGreen
inputShadowSrcGreen
inputShadowDstGreen
inputMidSrcGreen
inputMidDstGreen
inputHilightSrcGreen
inputHilightDstGreen
inputWhiteSrcGreen
inputWhiteDstGreen
inputBlackSrcBlue
inputBlackDstBlue
inputShadowSrcBlue
inputShadowDstBlue
inputMidSrcBlue
inputMidDstBlue
inputHilightSrcBlue
inputHilightDstBlue
inputWhiteSrcBlue
inputWhiteDstBlue
Green
Blue
Failed converting data to RGBAh: %ld
-[PILevelsFilterHDR _LUTImage]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Adjustments/HDR/PILevelsFilterHDR.m
score
/Master/Source
data != nil
+[PIColorNormalizationAutoCalculator autoCalculatorWithImageData:orientation:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Autocalculators/PIColorNormalizationAutoCalculator.m
-[PIColorNormalizationAutoCalculator submit:]
Feature Unavailable
No color normalization available
kernel vec4 _falseColorHDRDebug(sampler image, float cutoff){   vec4 im = sample(image, samplerCoord(image));   if (im.x < 0.0 && im.y < 0.0 && im.z < 0.0) {        return vec4(0.0 , 0.6 , 0.2 , 1.0);   }   if (im.x > cutoff && im.y > cutoff && im.z > cutoff) {        return vec4(1.0 , 0.0 , 1.0 , 1.0);   }   if (im.x > cutoff || im.y > cutoff || im.z > cutoff) {        return vec4(0.6 , 0.0 , 0.6 , 1.0);   }   return im;}
s_falseColorHDRDebugKernelSource kernel is nil
+[PIFalseColorHDRDebug kernel]_block_invoke
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Adjustments/HDR/PIFalseColorHDRDebug.m
-[PIFalseColorHDRDebug outputImage]
-[PIAutoLoopExportRequest initWithRequest:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/AutoLoop/PIAutoLoopRequests.m
-[PIAutoLoopExportRequest initWithComposition:destinationURL:]
Unexpected input pixel format
+[PIPortraitVideoProcessor _configureRGBColorTexture:format:isHDR:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Render/PIPortraitVideoFilter.m
unexpected inputs
+[PIPortraitVideoProcessor processWithInputs:arguments:output:error:]
expected a device
expected a texture
aperture
focusDistance
quality
isHDR
globalMetadata
timedMetadata
Expected an input color texture
Expected an input disparity texture
colorPixelBuffer
missing direct source color pixel buffer
disparityPixelBuffer
missing direct source disparity pixel buffer
v16@?0@"<MTLCommandBuffer>"8
Invalid index
+[PIPortraitVideoProcessor formatForInputAtIndex:]
imageExtents
+[PIPortraitVideoProcessor applyWithInputImage:disparityImage:inputPixelBuffer:disparityPixelBuffer:globalMetadata:timedMetadata:aperture:focusedDisparity:quality:debugMode:isHDR:error:]
inputImage != nil || inputPixelBuffer != nil
disparityImage != nil || disparityPixelBuffer != nil
PIPortrait: cinematic renderer supports RGB10Packed
NormStabilizeInstructions
stabCropRect
Width
Height
flavor
recipe
-[PIVideoCrossfadeLoopNode initWithSettings:inputs:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Video Stabilization/PIVideoCrossfadeLoopNode.m
input != nil
-[PIVideoCrossfadeLoopNode initWithInput:timeRange:crossfadeDuration:startTime:]
CMTIME_IS_VALID(crossfadeDuration)
CMTIMERANGE_IS_VALID(loopTimeRange)
crossfadeDuration
startTime
loopTimeRange
-[PIVideoCrossfadeLoopNode nodeByReplayingAgainstCache:pipelineState:error:]
Missing primary video frame
video
secondary
Missing secondary video frame
CIDissolveTransition
-[PIVideoCrossfadeLoopNode _evaluateVideo:]
[[AVMutableComposition alloc] init] failed.
No input video track found
Failed to update video track when inserting the pre-crossfade content with source range %@ from track %@ to time %@ in track %@.
Failed to update audio track when inserting the pre-crossfade content with source range %@ from track %@ to time %@ in track %@.
Failed to update video track when inserting the crossfade content with source range %@ from track %@ to time %@ in track %@.
Failed to update audio track when inserting the crossfade content with source range %@ from track %@ to time %@ in track %@.
Failed to update video track when inserting the post-crossfade content with source range %@ from track %@ to time %@ in track %@.
Failed to update audio track when inserting the post-crossfade content with source range %@ from track %@ to time %@ in track %@.
-[PIVideoCrossfadeLoopNode _evaluateVideoComposition:]
Unsupported video configuration
-[PIVideoCrossfadeLoopNode _evaluateAudioMix:]
time
scale
radius
falloff
Adjustment empty
-[PIAdjustmentController displayName]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Controllers/PIAdjustmentController.m
-[PIAdjustmentController inputKeys]
Adjustment does not have an enabled setting
-[PIAdjustmentController enabled]
-[PIAdjustmentController setEnabled:]
Adjustment does not have an auto setting
-[PIAdjustmentController isAuto]
-[PIAdjustmentController setIsAuto:]
<%@: %p; adjustment = %@>
-[PIParallaxFilter outputForegroundImage]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Parallax/PIParallaxFilter.m
-[PIParallaxFilter outputBackgroundImage]
-[PIParallaxFilter outputMatteImage]
-[_PIParallaxClockMaterialJob render:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Parallax/PIParallaxClockMaterialRequest.m
Request needs either a layerStack or a segmentationItem
-[PIParallaxClockMaterialRequest initWithComposition:]
__dominantInputSettingsKey
debugMode
disparityKeyframes
apertureKeyframes
NUScaleIsValid(scale)
-[PIPortraitVideoRenderNode _targetScaleForScale:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Render/PIPortraitVideoRenderNode.m
-[PIPortraitVideoRenderNode nodeByReplayingAgainstCache:pipelineState:error:]
state != nil
-[PIPortraitVideoRenderNode _prewarmPortraitRendererWithPipelineState:error:]
No device specified for prewarm
portraitVideoMetadata
PIPortraitVideoRenderNode: expected a valid portraitVideoMetadata sample
PIPortrait: passing YUV source buffers directly to the cinematic renderer
renderTime
renderQuality
sourceTransferFunction
useSourceBuffersDirectly
Source image isn't backed by a CVPixelBuffer
Could not get the input image
Could not get the disparityImage
Could not get the timed metadata
Could not get the source transfer function
Could not get the render time
PIFaceObservationCache
PIFaceObservationCache-newRequest
Original
StudioBright
StudioDark
ColorBGStandard
BlackWhiteHighKey
BlackWhiteStage
BlackWhiteMono
ColorWashSingle
ColorWashDuotone
Original-Inactive
StudioBright-Inactive
StudioDark-Inactive
ColorBGStandard-Inactive
BlackWhiteHighKey-Inactive
BlackWhiteStage-Inactive
BlackWhiteMono-Inactive
ColorWashSingle-Inactive
ColorWashDuotone-Inactive
The bundle should contain recipes for all known identifiers
+[PIParallaxStyleRecipeRegistry recipeForIdentifier:]_block_invoke
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Parallax/PIParallaxStyleRecipeRegistry.m
-[PIParallaxStyleUserURLProvider init]
spatialOvercapture
spatialOvercaptureFused
Unspecified Key
Low Key
Neutral Key
High Key
SF Soft Time
SF Rounded Time
New York Time
ADT Slab Time
SF Stencil Time
SF Rail Time
+[PIParallaxStyle styleWithColorAnalysis:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Parallax/PIParallaxStyle.m
clockColor != nil
-[PIParallaxStyle initWithClockColor:colorSuggestions:]
suggestions != nil
Unknown style kind: %@
+[PIParallaxStyle defaultStyleForKind:colorAnalysis:]
+[PIParallaxStyle styleWithParameters:colorSuggestions:]
+[PIParallaxStyle styleWithBakedStyle:]
<%@: %p; parameters = %@>
-[PIParallaxStyle parameters]
-[PIParallaxStyle kind]
-[PIParallaxStyle recipeIdentifier]
-Inactive
BLACK
WHITE
B16@?0@"PFParallaxPaletteSuggestion"8
-[PIParallaxStyle configureForCategory:]
parameters != nil
+[PIParallaxOriginalStyle styleWithParameters:colorSuggestions:]
+[PIParallaxStudioStyle styleWithParameters:colorSuggestions:]
@"PFParallaxPaletteSuggestion"16@?0@"PFParallaxColor"8
backgroundColor != nil
-[PIParallaxColorBGStandardStyle initWithBackgroundColor:clockColor:colorSuggestions:]
+[PIParallaxBlackWhiteMonoStyle styleWithParameters:colorSuggestions:]
@"PFParallaxColor"24@?0@"PFParallaxColor"8@"PIParallaxColorAnalysis"16
@"PFParallaxColor"16@?0@"PFParallaxColor"8
+[PIParallaxColorWashSingleStyle styleWithParameters:colorSuggestions:]
-[PIParallaxColorWashSingleStyle initWithColor:clockColor:suggestions:]
Failed to retrieve secondary color from palette
+[PIParallaxColorWashDuotoneStyle styleWithColorAnalysis:]
+[PIParallaxColorWashDuotoneStyle styleWithParameters:colorSuggestions:]
primaryColor != nil
-[PIParallaxColorWashDuotoneStyle initWithPrimaryColor:secondaryColor:clockColor:colorSuggestions:]
secondaryColor != nil
-[PIParallaxRecipeStyle initWithIdentifier:recipe:]
<%@: %p; identifier = %@, recipe = %@>
timeValue
timeScale
%@: t=%@, v=%f
<%@ %p %@ %@ id=%lu conf=%.2f>
  bounds=%@
  expandedBounds=%@
  edgeBleed=%@
type
confidence
edgeBleed
bounds
expandedBounds
Human Face
Human Body
Cat Body
Cat Head
Dog Body
Dog Head
Salient Object
Unknown
(%.4f, %.4f, %.4f, %.4f)
minX 
minY 
maxX 
maxY 
B16@?0@"PFParallaxLayerStyle"8
<%@:%p class=%@ matte=%@ conf=%@ infill=%@ layout=%@ resource=%@ composition=%@>
archiveURL != nil
-[PIParallaxSegmentationItem saveToURL:error:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Parallax/PISegmentationItem.m
Failed to create archive directory
asset.resource
Failed to archive asset resource
segmentation.data.aar
Failed to archive segmentation data
-[PIParallaxSegmentationItem loadFromURL:error:]
Failed to load asset resource
Failed to load segmentation data
-[PIParallaxSegmentationItem saveAssetResourceToURL:error:]
Missing asset resource
-[PIParallaxSegmentationItem saveSegmentationDataToURL:error:]
Failed to write segmentation archive
Failed to serialize contents plist
contents.plist
Failed to archive contents plist data
Failed to write segmentation matte
matte.heic
Failed to archive segmentation matte data
Failed to write segmentation background
background.heic
Failed to archive segmentation background data
Failed to close archive file
-[PIParallaxSegmentationItem loadSegmentationDataFromURL:error:]
Failed to read segmentation archive
Failed to decode contents plist data
Expected contents.plist data
Failed to deserialize contents plist
Invalid contents plist
Failed to load contents dictionary
Failed to decode matte image data
Expected matte.heic data
Failed to read matte image data
Failed to decode background image data
Expected background.heic data
Failed to read background image data
Missing context info!
-[PIParallaxSegmentationItem contentsDictionary]
classification
hasMatte
hasBackground
regions
layout
scores
colorAnalysis
localLightData
systemVersion
sourceMode
infillAlgorithm
layoutConfiguration
segmentationDisabled
contents != nil
-[PIParallaxSegmentationItem loadContentsFromDictionary:hasMatte:hasBackground:error:]
Missing version info
Invalid version info
Unsupported version
Invalid system version info
Invalid system name
Invalid system version
Invalid system build version
Invalid source mode
Invalid segmentation disabled flag
Invalid infill algorithm
Invalid layout configuration
Failed to deserialize layout configuration
Missing classification info
Expected classification string
Missing matte image info
Expected boolean
Missing background image info
Expected regions dictionary
Failed to deserialize regions info
Expected layout dictionary
Failed to deserialize layout info
Expected score dictionary
Invalid score key
Invalid score value
Invalid color analysis info
Failed to deserialize color analysis info
styles
Expected styles array
Invalid style value
Invalid style dictionary
Unsupported style kind
Invalid local light data
+[PIParallaxSegmentationItem writeImageBuffer:toURL:error:]
Failed to encode pixel buffer
+[PIParallaxSegmentationItem readImageBufferFromURL:error:]
Failed to decode pixel buffer
+[PIParallaxSegmentationItem dataForImageBuffer:error:]
imageData != nil
+[PIParallaxSegmentationItem imageBufferFromData:error:]
cinematographyState
renderingVersionAtCapture
-[PIParallaxColorSuggester init]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Parallax/PIParallaxColorSuggester.m
analysis != nil
-[PIParallaxColorSuggester initWithColorAnalysis:]
color != nil
-[PIParallaxColorSuggester suggestedColorForColor:]
inputColor
outputColor
hueMin <= hueMax
-[PIParallaxColorSuggester addRuleWithHueMin:hueMax:suggestion:]
B16@?0@"NURuleSystem"8
v16@?0@"NURuleSystem"8
colors != nil
-[PIParallaxColorSuggester suggestedColorsForColors:fromColorPalette:]
B16@?0@"PFParallaxColor"8
homography
<%@:%p time:%@>
kernel vec4 ipt_hue_chroma_scale_hue(__sample ihc, vec2 hso) {
float luma = ihc.r;
float hue = ihc.g;
float chroma = ihc.b;
float alpha = ihc.a;
float hueScale = hso.x;
float hueOffset = hso.y;
hue = hueScale * hue + hueOffset;
return vec4(luma, hue, chroma, alpha);
kernel vec4 ipt_hue_chroma_filter_hue(__sample ihc, vec4 hcr) {
float luma = ihc.r;
float hue = ihc.g;
float chroma = ihc.b;
float alpha = ihc.a;
float hueTarget = hcr.x;
float hueRange = hcr.y;
float hueModulo = hcr.z;
float chromaMin = hcr.w;
float chromaFactor = step(chromaMin, chroma);
float hueDelta = min(abs(hue - hueTarget), min(abs(hue + hueModulo - hueTarget), abs(hue - hueModulo - hueTarget)));
float hueFactor = 1.0 - smoothstep(0.0, hueRange, hueDelta);
alpha *= hueFactor * chromaFactor;
return vec4(luma, hue, chroma, alpha);
kernel vec4 ipt_hue_chroma_filter_luma(__sample ihc, vec3 hcr) {
float luma = ihc.r;
float hue = ihc.g;
float chroma = ihc.b;
float alpha = ihc.a;
float lumaTarget = hcr.x;
float lumaRange = hcr.y;
float chromaMax = hcr.z;
float chromaFactor = 1.0 - step(chromaMax, chroma);
float lumaDelta = abs(luma - lumaTarget);
float lumaFactor = 1.0 - smoothstep(0.0, lumaRange, lumaDelta);
alpha *= lumaFactor * chromaFactor;
return vec4(luma, hue, chroma, alpha);
<%@: %p; lum = %.3f colors = %@>
-[_PIParallaxColorAnalysisJob initWithRequest:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Parallax/PIParallaxColorAnalysisRequest.m
-[_PIParallaxColorAnalysisJob prepare:]
CISRGBtoIPT
Failed to load hue/chroma filter
inputReturnHueChroma
Hue/chroma filter failed
-[_PIParallaxColorAnalysisJob render:]
No storage pool
hue-%ld
gray-%ld
-[_PIParallaxColorAnalysisJob _beginRenderingImage:colorSpace:format:error:]
No storage allocated
hueChromaImage != nil
-[_PIParallaxColorAnalysisJob _beginRenderNormalizedHueChromaImage:targetHue:hueRange:chromaMin:error:]
PIIPTHueChromaColorFilter
inputHueTarget
inputHueRange
inputChromaMin
inputHueIsNormalized
Failed to produce filtered Hue/Chroma image
CIIPTtoSRGB
inputIsHueChroma
-[_PIParallaxColorAnalysisJob _beginRenderNormalizedHueChromaImage:targetGray:grayRange:chromaMax:error:]
PIIPTHueChromaGrayFilter
inputLumaTarget
inputLumaRange
inputChromaMax
-[_PIParallaxColorAnalysisJob _waitForRenderResources:]
Failed to render image
Failed to compute histogram
-[_PIParallaxColorAnalysisJob complete:]
q24@?0@"_PIParallaxRenderResource"8@"_PIParallaxRenderResource"16
ipt_hue_chroma_scale_hue
+[PIIPTHueChromaFilter normalizeHueChromaImage:]
+[PIIPTHueChromaFilter denormalizeHueChromaImage:]
ipt_hue_chroma_filter_hue
ipt_hue_chroma_filter_luma
luminance
foregroundLuminance
backgroundLuminance
@"NSArray"16@?0@"PFParallaxColor"8
colors
foregroundColors
backgroundColors
+[PIParallaxColorAnalysis loadFromContentsDictionary:error:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Parallax/PIParallaxColorAnalysis.m
Incompatible color analysis version
Invalid luminance value
Invalid color array
Failed to deserialize color values
PILongExposureFusionAutoCalculator-videoProperties
pre-AutoLoop
-[PILongExposureFusionAutoCalculator submit:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Autocalculators/PILongExposureFusionAutoCalculator.m
AutoLoop not supported
kind
-[PIAutoLoopAutoCalculator submit:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Autocalculators/PIAutoLoopAutoCalculator.m
B32@?0@"AVMetadataItem"8Q16^B24
-[PITempTintFilter outputImage]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Adjustments/PITempTintFilter.m
-[PIAutoLoopAnalysisJob prepare:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/AutoLoop/PIAutoLoopJob+Analysis.m
unable to find video source node
-[PIAutoLoopAnalysisJob render:]
AutoLoop is not supported
-[PIVideoStabilizeRenderJob prepare:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Autocalculators/PIVideoStabilizeRequest.m
Stabilize request was cancelled
frameInstructions
rawTime
void CreateKeyframesFromHomographyDictionary(NSDictionary *__strong, NUPixelSize, NSMutableArray<PIReframeKeyframe *> *__strong, NUPixelRect *)
PIVideoStabilizeRequest.m
unexpected homography
Failure in ICSynthesizeAnalysis
Failure in ICAnalyzeInputMotion
No available analysis types were allowed
Failure in ICCalcCinematicL1Corrections
neutralGray
faceBalance
tempTint
warmTint
PIFaceBalanceAutoCalculator-imageProperties
+[PIFaceBalanceAutoCalculator calculateWithRequest:completion:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Autocalculators/PIWhiteBalanceAutoCalculators.m
CIFaceBalance
PIFaceBalanceAutoCalculator-calculate
WarmTemp
WarmTint
warmTemp
+[PIFaceBalanceAutoCalculator calculateRAWWithRequest:completion:]
PIFaceBalanceAutoCalculator-RAW-face-request
+[PIFaceBalanceAutoCalculator faceBalanceResultFromFaceObservations:request:error:]
PIWhiteBalanceAutoCalculator-face-balance
PIRAWFaceBalanceAutoCalculator.responseQueue
Failure in rendering image
OrigI
OrigQ
Warmth
Strength
WarmFace
unsupported format - incorrect white balance
+[PIFaceBalanceAutoCalculator faceBalanceFromFaceImage:forFaceRect:]
{?={?=[4d]}{?=[4d]}d}
{?=[4d]}
colorType
grayColor
-[PIWhiteBalanceAutoCalculator submit:]
PIWhiteBalanceAutoCalculator-imageProperties
PIFaceBalanceAutoCalculator.responseQueue
faceI
faceQ
faceWarmth
faceStrength
PIWhiteBalanceAutoCalculator
color
v16@?0@"<NUBuffer>"8
CIAreaAverage
PIWhiteColorCalculator-computeGreenPercentage
PIWhiteColorCalculator-grayWorld
PIWhiteColorCalculator-grayEdges
return Filter('CIEdges', { 'inputImage' : input, 'inputIntensity' : 1.0 });
Expected non-NULL pixels passed in
void customRGBtoYIQ(const CGFloat *, double *, double, double, double)
vec4 meaningBlur(vec4 im, vec4 b)
vec4 result = im;
float thresh = 0.1;
float g1 = max(max(im.r, im.g), im.b);
float g2 = dot(b.rgb, vec3(1.0/3.0));
float diff = max(g2-g1, -1.0);
diff = smoothstep(0.1-thresh, 0.1+thresh, diff);
result.rgb = mix(im.rgb, b.rgb, diff+0.5);
return result;
vec4 clarityNew(vec4 s, vec4 b, float intensity)
float sl = (s.r + s.g + s.b);
float bl = (b.r + b.g + b.b);
float dl = sl + (sl - bl) * intensity;
float mult = dl / max(sl, 0.0001);
mult = 1.571 * (mult - 1.0);
mult = mult / (1.0 + abs(mult));
mult += 1.0;
mult = clamp(mult, 1.0 - 0.5 * abs(intensity), 1.0 + 1.0 * abs(intensity));
s.rgb = s.rgb * mult;
return s;
kernel vec4 definition(sampler image, sampler blur, float intensity)
vec4 imgSample = sample(image, samplerCoord(image));
vec4 blurSample = sample(blur, samplerCoord(blur));
vec4 meaning = meaningBlur(imgSample, blurSample);
vec4 clarity = clarityNew(imgSample, meaning, intensity);
return clarity;
strength
neutral
tone
inputNeutralGamma
inputTone
inputGrain
inputHue
inputSeed
<%@:%p - priority: %@, color space: %@, scale policy: %@>
<%@:%p - priority: %@, color space: %@, scale policy: %@, video codec type: %@, bypass settings: %@, orientation as metadata: %@, hardware encoder: %@>
Export URL UTI (%@) does not match expected export format (%@)
-[PICompositionExporterImageOptions imageExportFormatForURL:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Util/PICompositionExporter.m
metadataConverter must be set
-[PICompositionExporter init]
PICompositionExporter-videoProperties
Expecting HEIF/HEVC or JPEG/H264 when exporting Live Photo still and video complement
Unexpected video codec when attempting to export Live Photo
Unexpected image export format when attempting to export Live Photo
_companion
unable to prepare image properties
PICompositionExporter-image
PICompositionExporter-auxPropertiesRequest
PICompositionExporter-%@
PICompositionExporter.imageProperties.transaction
Regions
-[PICompositionExporter addVideoProperties:composition:options:error:]
Unknown autoloop flavor
PICompositionExporter-video
float luminanceWeight(vec4 pix, vec4 center, float slope)
return max(1.0 + (slope * length(pix.rgb - center.rgb)), 0.0);
vec4 bilateralRow_1(sampler src, float slope, vec2 offset1, float weight1)
float diff1;
vec2 coord;
vec4 pix1, center;
coord = destCoord();
center = sample(src, samplerCoord(src));
pix1 = sample(src, samplerTransform(src, coord + offset1));
diff1 = luminanceWeight(pix1, center, slope) * weight1;
return vec4(diff1 * pix1.rgb, diff1);
kernel vec4 bilateralAdd_1(sampler src, sampler sums, float slope, vec2 offset1, float weight1, float sumWeight)
vec4 sum;
sum = sample(sums, samplerCoord(sums))*sumWeight;
return sum + bilateralRow_1(src, slope, offset1, weight1);
vec4 bilateralRow_2(sampler src, float slope, vec2 offset1, vec2 offset2, vec2 weight1)
float diff1, diff2;
vec2 coord;
vec4 pix1, pix2, center;
coord = destCoord();
center = sample(src, samplerCoord(src));
pix1 = sample(src, samplerTransform(src, coord + offset1));
pix2 = sample(src, samplerTransform(src, coord + offset2));
diff1 = luminanceWeight(pix1, center, slope) * weight1.x;
diff2 = luminanceWeight(pix2, center, slope) * weight1.y;
return vec4(diff1 * pix1.rgb + diff2 * pix2.rgb, diff1 + diff2);
kernel vec4 bilateralAdd_2(sampler src, sampler sums, float slope, vec2 offset1, vec2 offset2, vec2 weight1, float sumWeight)
vec4 sum;
sum = sample(sums, samplerCoord(sums))*sumWeight;
return sum + bilateralRow_2(src, slope, offset1, offset2, weight1);
vec4 bilateralRow_3(sampler src, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec3 weight1)
float diff1, diff2, diff3;
vec2 coord;
vec4 pix1, pix2, pix3, center;
coord = destCoord();
center = sample(src, samplerCoord(src));
pix1 = sample(src, samplerTransform(src, coord + offset1));
pix2 = sample(src, samplerTransform(src, coord + offset2));
pix3 = sample(src, samplerTransform(src, coord + offset3));
diff1 = luminanceWeight(pix1, center, slope) * weight1.x;
diff2 = luminanceWeight(pix2, center, slope) * weight1.y;
diff3 = luminanceWeight(pix3, center, slope) * weight1.z;
return vec4(diff1 * pix1.rgb + diff2 * pix2.rgb + diff3 * pix3.rgb, diff1 + diff2 + diff3);
kernel vec4 bilateralAdd_3(sampler src, sampler sums, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec3 weight1, float sumWeight)
vec4 sum;
sum = sample(sums, samplerCoord(sums))*sumWeight;
return sum + bilateralRow_3(src, slope, offset1, offset2, offset3, weight1);
vec4 bilateralRow_4(sampler src, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec2 offset4, vec4 weight1)
float diff1, diff2, diff3, diff4;
vec2 coord;
vec4 pix1, pix2, pix3, pix4, center;
coord = destCoord();
center = sample(src, samplerCoord(src));
pix1 = sample(src, samplerTransform(src, coord + offset1));
pix2 = sample(src, samplerTransform(src, coord + offset2));
pix3 = sample(src, samplerTransform(src, coord + offset3));
pix4 = sample(src, samplerTransform(src, coord + offset4));
diff1 = luminanceWeight(pix1, center, slope) * weight1.x;
diff2 = luminanceWeight(pix2, center, slope) * weight1.y;
diff3 = luminanceWeight(pix3, center, slope) * weight1.z;
diff4 = luminanceWeight(pix4, center, slope) * weight1.w;
return vec4(diff1 * pix1.rgb + diff2 * pix2.rgb + diff3 * pix3.rgb + diff4 * pix4.rgb,
diff1 + diff2 + diff3 + diff4);
kernel vec4 bilateralAdd_4(sampler src, sampler sums, float slope, vec2 offset1, vec2 offset2, vec2 offset3,
vec2 offset4, vec4 weight1, float sumWeight)
vec4 sum;
sum = sample(sums, samplerCoord(sums))*sumWeight;
return sum + bilateralRow_4(src, slope, offset1, offset2, offset3, offset4, weight1);
vec4 bilateralRow_5(sampler src, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec2 offset4, vec2 offset5,
vec4 weight1, float weight2)
float diff1, diff2, diff3, diff4, diff5;
vec2 coord;
vec4 pix1, pix2, pix3, pix4, pix5, center;
coord = destCoord();
center = sample(src, samplerCoord(src));
pix1 = sample(src, samplerTransform(src, coord + offset1));
pix2 = sample(src, samplerTransform(src, coord + offset2));
pix3 = sample(src, samplerTransform(src, coord + offset3));
pix4 = sample(src, samplerTransform(src, coord + offset4));
pix5 = sample(src, samplerTransform(src, coord + offset5));
diff1 = luminanceWeight(pix1, center, slope) * weight1.x;
diff2 = luminanceWeight(pix2, center, slope) * weight1.y;
diff3 = luminanceWeight(pix3, center, slope) * weight1.z;
diff4 = luminanceWeight(pix4, center, slope) * weight1.w;
diff5 = luminanceWeight(pix5, center, slope) * weight2;
return vec4(diff1 * pix1.rgb + diff2 * pix2.rgb + diff3 * pix3.rgb + diff4 * pix4.rgb + diff5 * pix5.rgb,
diff1 + diff2 + diff3 + diff4 + diff5);
kernel vec4 bilateralAdd_5(sampler src, sampler sums, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec2 offset4,
vec2 offset5, vec4 weight1, float weight2, float sumWeight)
vec4 sum;
sum = sample(sums, samplerCoord(sums))*sumWeight;
return sum + bilateralRow_5(src, slope, offset1, offset2, offset3, offset4, offset5, weight1, weight2);
vec4 bilateralRow_6(sampler src, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec2 offset4, vec2 offset5,
vec2 offset6, vec4 weight1, vec2 weight2)
float diff1, diff2, diff3, diff4, diff5, diff6;
vec2 coord;
vec4 pix1, pix2, pix3, pix4, pix5, pix6, center;
coord = destCoord();
center = sample(src, samplerCoord(src));
pix1 = sample(src, samplerTransform(src, coord + offset1));
pix2 = sample(src, samplerTransform(src, coord + offset2));
pix3 = sample(src, samplerTransform(src, coord + offset3));
pix4 = sample(src, samplerTransform(src, coord + offset4));
pix5 = sample(src, samplerTransform(src, coord + offset5));
pix6 = sample(src, samplerTransform(src, coord + offset6));
diff1 = luminanceWeight(pix1, center, slope) * weight1.x;
diff2 = luminanceWeight(pix2, center, slope) * weight1.y;
diff3 = luminanceWeight(pix3, center, slope) * weight1.z;
diff4 = luminanceWeight(pix4, center, slope) * weight1.w;
diff5 = luminanceWeight(pix5, center, slope) * weight2.x;
diff6 = luminanceWeight(pix6, center, slope) * weight2.y;
return vec4(diff1 * pix1.rgb + diff2 * pix2.rgb + diff3 * pix3.rgb + diff4 * pix4.rgb + diff5 * pix5.rgb + diff6 * pix6.rgb,
diff1 + diff2 + diff3 + diff4 + diff5 + diff6);
kernel vec4 bilateralAdd_6(sampler src, sampler sums, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec2 offset4,
vec2 offset5, vec2 offset6, vec4 weight1, vec2 weight2, float sumWeight)
vec4 sum;
sum = sample(sums, samplerCoord(sums))*sumWeight;
return sum + bilateralRow_6(src, slope, offset1, offset2, offset3, offset4, offset5, offset6, weight1, weight2);
vec4 bilateralRow_7(sampler src, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec2 offset4, vec2 offset5,
vec2 offset6, vec2 offset7, vec4 weight1, vec3 weight2)
float diff1, diff2, diff3, diff4, diff5, diff6, diff7;
vec2 coord;
vec4 pix1, pix2, pix3, pix4, pix5, pix6, pix7, center;
coord = destCoord();
center = sample(src, samplerCoord(src));
pix1 = sample(src, samplerTransform(src, coord + offset1));
pix2 = sample(src, samplerTransform(src, coord + offset2));
pix3 = sample(src, samplerTransform(src, coord + offset3));
pix4 = sample(src, samplerTransform(src, coord + offset4));
pix5 = sample(src, samplerTransform(src, coord + offset5));
pix6 = sample(src, samplerTransform(src, coord + offset6));
pix7 = sample(src, samplerTransform(src, coord + offset7));
diff1 = luminanceWeight(pix1, center, slope) * weight1.x;
diff2 = luminanceWeight(pix2, center, slope) * weight1.y;
diff3 = luminanceWeight(pix3, center, slope) * weight1.z;
diff4 = luminanceWeight(pix4, center, slope) * weight1.w;
diff5 = luminanceWeight(pix5, center, slope) * weight2.x;
diff6 = luminanceWeight(pix6, center, slope) * weight2.y;
diff7 = luminanceWeight(pix7, center, slope) * weight2.z;
return vec4(diff1 * pix1.rgb + diff2 * pix2.rgb + diff3 * pix3.rgb + diff4 * pix4.rgb + diff5 * pix5.rgb + diff6 * pix6.rgb + diff7 * pix7.rgb,
diff1 + diff2 + diff3 + diff4 + diff5 + diff6 + diff7);
kernel vec4 bilateralAdd_7(sampler src, sampler sums, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec2 offset4,
vec2 offset5, vec2 offset6, vec2 offset7, vec4 weight1, vec3 weight2, float sumWeight)
vec4 sum;
sum = sample(sums, samplerCoord(sums))*sumWeight;
return sum + bilateralRow_7(src, slope, offset1, offset2, offset3, offset4, offset5, offset6, offset7, weight1, weight2);
vec4 bilateralRow_8(sampler src, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec2 offset4, vec2 offset5, vec2 offset6,
vec2 offset7, vec2 offset8, vec4 weight1, vec4 weight2)
float diff1, diff2, diff3, diff4, diff5, diff6, diff7, diff8;
vec2 coord;
vec4 pix1, pix2, pix3, pix4, pix5, pix6, pix7, pix8, center;
coord = destCoord();
center = sample(src, samplerCoord(src));
pix1 = sample(src, samplerTransform(src, coord + offset1));
pix2 = sample(src, samplerTransform(src, coord + offset2));
pix3 = sample(src, samplerTransform(src, coord + offset3));
pix4 = sample(src, samplerTransform(src, coord + offset4));
pix5 = sample(src, samplerTransform(src, coord + offset5));
pix6 = sample(src, samplerTransform(src, coord + offset6));
pix7 = sample(src, samplerTransform(src, coord + offset7));
pix8 = sample(src, samplerTransform(src, coord + offset8));
diff1 = luminanceWeight(pix1, center, slope) * weight1.x;
diff2 = luminanceWeight(pix2, center, slope) * weight1.y;
diff3 = luminanceWeight(pix3, center, slope) * weight1.z;
diff4 = luminanceWeight(pix4, center, slope) * weight1.w;
diff5 = luminanceWeight(pix5, center, slope) * weight2.x;
diff6 = luminanceWeight(pix6, center, slope) * weight2.y;
diff7 = luminanceWeight(pix7, center, slope) * weight2.z;
diff8 = luminanceWeight(pix8, center, slope) * weight2.w;
return vec4(diff1 * pix1.rgb + diff2 * pix2.rgb + diff3 * pix3.rgb + diff4 * pix4.rgb + diff5 * pix5.rgb + diff6 * pix6.rgb + diff7 * pix7.rgb + diff8 * pix8.rgb,
diff1 + diff2 + diff3 + diff4 + diff5 + diff6 + diff7 + diff8);
kernel vec4 bilateralAdd_8(sampler src, sampler sums, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec2 offset4, vec2 offset5,
vec2 offset6, vec2 offset7, vec2 offset8, vec4 weight1, vec4 weight2, float sumWeight)
vec4 sum;
sum = sample(sums, samplerCoord(sums))*sumWeight;
return sum + bilateralRow_8(src, slope, offset1, offset2, offset3, offset4, offset5, offset6, offset7, offset8, weight1, weight2);
vec4 bilateralRow_9(sampler src, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec2 offset4, vec2 offset5, vec2 offset6, vec2 offset7,
vec2 offset8, vec2 offset9, vec4 weight1, vec4 weight2, float weight3)
float diff1, diff2, diff3, diff4, diff5, diff6, diff7, diff8, diff9;
vec2 coord;
vec4 pix1, pix2, pix3, pix4, pix5, pix6, pix7, pix8, pix9, center;
coord = destCoord();
center = sample(src, samplerCoord(src));
pix1 = sample(src, samplerTransform(src, coord + offset1));
pix2 = sample(src, samplerTransform(src, coord + offset2));
pix3 = sample(src, samplerTransform(src, coord + offset3));
pix4 = sample(src, samplerTransform(src, coord + offset4));
pix5 = sample(src, samplerTransform(src, coord + offset5));
pix6 = sample(src, samplerTransform(src, coord + offset6));
pix7 = sample(src, samplerTransform(src, coord + offset7));
pix8 = sample(src, samplerTransform(src, coord + offset8));
pix9 = sample(src, samplerTransform(src, coord + offset9));
diff1 = luminanceWeight(pix1, center, slope) * weight1.x;
diff2 = luminanceWeight(pix2, center, slope) * weight1.y;
diff3 = luminanceWeight(pix3, center, slope) * weight1.z;
diff4 = luminanceWeight(pix4, center, slope) * weight1.w;
diff5 = luminanceWeight(pix5, center, slope) * weight2.x;
diff6 = luminanceWeight(pix6, center, slope) * weight2.y;
diff7 = luminanceWeight(pix7, center, slope) * weight2.z;
diff8 = luminanceWeight(pix8, center, slope) * weight2.w;
diff9 = luminanceWeight(pix9, center, slope) * weight3;
return vec4(diff1 * pix1.rgb + diff2 * pix2.rgb + diff3 * pix3.rgb + diff4 * pix4.rgb + diff5 * pix5.rgb + diff6 * pix6.rgb + diff7 * pix7.rgb + diff8 * pix8.rgb + diff9 * pix9.rgb,
diff1 + diff2 + diff3 + diff4 + diff5 + diff6 + diff7 + diff8 + diff9);
kernel vec4 bilateralAdd_9(sampler src, sampler sums, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec2 offset4, vec2 offset5,
vec2 offset6, vec2 offset7, vec2 offset8, vec2 offset9, vec4 weight1, vec4 weight2, float weight3, float sumWeight)
vec4 sum;
sum = sample(sums, samplerCoord(sums))*sumWeight;
return sum + bilateralRow_9(src, slope, offset1, offset2, offset3, offset4, offset5, offset6, offset7, offset8, offset9,
weight1, weight2, weight3);
kernel vec4 bilateralFinalize(sampler sums)
vec4 sum;
sum = sample(sums, samplerCoord(sums));
return vec4(sum.rgb / max(sum.a, 0.001), 1.0);
kernel vec4 bilateralLoop2(sampler src, float slope, vec2 weights12)
vec2 coord;
vec4 center = sample(src, samplerCoord(src));
vec4 sum;
vec4 pix0, pix1, pix2;
vec4 pix3,       pix5;
vec4 pix6, pix7, pix8;
vec2 dx = samplerTransform(src, vec2( 1.0, 0.0)) - samplerTransform(src, vec2(0.0, 0.0));
vec2 dy = samplerTransform(src, vec2(-2.0, 1.0)) - samplerTransform(src, vec2(0.0, 0.0));
coord = samplerTransform(src, destCoord() + vec2(-1.0, -1.0));
pix0 = sample(src, coord);
coord = coord + dx;
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dy;
pix3 = sample(src, coord);
coord = coord + dx;
coord = coord + dx;
pix5 = sample(src, coord);
coord = coord + dy;
pix6 = sample(src, coord);
coord = coord + dx;
pix7 = sample(src, coord);
coord = coord + dx;
pix8 = sample(src, coord);
sum = (1.0 - min(1.0, (slope * length(pix0 - center)))) * (pix0 * weights12.y);
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights12.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights12.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights12.x) + sum;
sum =                                        center                            + sum;
sum = (1.0 - min(1.0, (slope * length(pix5 - center)))) * (pix5 * weights12.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix6 - center)))) * (pix6 * weights12.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix7 - center)))) * (pix7 * weights12.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix8 - center)))) * (pix8 * weights12.y) + sum;
return vec4(sum.rgb / sum.a, 1.0);
kernel vec4 bilateralLoop5(sampler src, float slope, vec4 weights1245)
vec2  coord;
vec4  center = sample(src, samplerCoord(src));
vec4  sum;
vec4  pix0, pix1, pix2, pix3, pix4;
vec2 dx = samplerTransform(src, vec2( 1.0, 0.0)) - samplerTransform(src, vec2(0.0, 0.0));
vec2 dy = samplerTransform(src, vec2(-4.0, 1.0)) - samplerTransform(src, vec2(0.0, 0.0));
coord = samplerTransform(src, destCoord() + vec2(-1.0, -2.0));
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dx;
pix3 = sample(src, coord);
coord = coord + dx + dy;
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights1245.w);
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights1245.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights1245.w) + sum;
pix0 = sample(src, coord);
coord = coord + dx;
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dx;
pix3 = sample(src, coord);
coord = coord + dx;
pix4 = sample(src, coord);
coord = coord + dy;
sum = (1.0 - min(1.0, (slope * length(pix0 - center)))) * (pix0 * weights1245.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights1245.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights1245.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights1245.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix4 - center)))) * (pix4 * weights1245.w) + sum;
pix0 = sample(src, coord);
coord = coord + dx;
pix1 = sample(src, coord);
coord = coord + dx + dx;
pix3 = sample(src, coord);
coord = coord + dx;
pix4 = sample(src, coord);
coord = coord + dy;
sum = (1.0 - min(1.0, (slope * length(pix0 - center)))) * (pix0 * weights1245.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights1245.x) + sum;
sum =                                        center                              + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights1245.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix4 - center)))) * (pix4 * weights1245.z) + sum;
pix0 = sample(src, coord);
coord = coord + dx;
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dx;
pix3 = sample(src, coord);
coord = coord + dx;
pix4 = sample(src, coord);
coord = coord + dx + dy;
sum = (1.0 - min(1.0, (slope * length(pix0 - center)))) * (pix0 * weights1245.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights1245.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights1245.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights1245.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix4 - center)))) * (pix4 * weights1245.w) + sum;
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dx;
pix3 = sample(src, coord);
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights1245.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights1245.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights1245.w) + sum;
return vec4(sum.rgb / sum.a, 1.0);
kernel vec4 bilateralLoop11(sampler src, float slope, vec4 weights1245, vec4 weights89AD)
vec2  coord;
vec4  center = sample(src, samplerCoord(src));
vec4  sum;
vec4  pix0, pix1, pix2, pix3, pix4, pix5, pix6;
vec2 dx = samplerTransform(src, vec2( 1.0, 0.0)) - samplerTransform(src, vec2(0.0, 0.0));
vec2 dy = samplerTransform(src, vec2(-6.0, 1.0)) - samplerTransform(src, vec2(0.0, 0.0));
coord = samplerTransform(src, destCoord() + vec2(-2.0, -3.0));
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dx;
pix3 = sample(src, coord);
coord = coord + dx;
pix4 = sample(src, coord);
coord = coord + dx;
pix5 = sample(src, coord);
coord = coord + dx + dy;
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights89AD.w);
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights89AD.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights89AD.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix4 - center)))) * (pix4 * weights89AD.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix5 - center)))) * (pix5 * weights89AD.w) + sum;
pix0 = sample(src, coord);
coord = coord + dx;
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dx;
pix3 = sample(src, coord);
coord = coord + dx;
pix4 = sample(src, coord);
coord = coord + dx;
pix5 = sample(src, coord);
coord = coord + dx;
pix6 = sample(src, coord);
coord = coord + dy;
sum = (1.0 - min(1.0, (slope * length(pix0 - center)))) * (pix0 * weights89AD.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights89AD.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights1245.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights1245.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix4 - center)))) * (pix4 * weights1245.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix5 - center)))) * (pix5 * weights89AD.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix6 - center)))) * (pix6 * weights89AD.w) + sum;
pix0 = sample(src, coord);
coord = coord + dx;
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dx;
pix3 = sample(src, coord);
coord = coord + dx;
pix4 = sample(src, coord);
coord = coord + dx;
pix5 = sample(src, coord);
coord = coord + dx;
pix6 = sample(src, coord);
coord = coord + dy;
sum = (1.0 - min(1.0, (slope * length(pix0 - center)))) * (pix0 * weights89AD.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights1245.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights1245.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights1245.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix4 - center)))) * (pix4 * weights1245.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix5 - center)))) * (pix5 * weights1245.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix6 - center)))) * (pix6 * weights89AD.z) + sum;
pix0 = sample(src, coord);
coord = coord + dx;
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dx + dx;
pix4 = sample(src, coord);
coord = coord + dx;
pix5 = sample(src, coord);
coord = coord + dx;
pix6 = sample(src, coord);
coord = coord + dy;
sum = (1.0 - min(1.0, (slope * length(pix0 - center)))) * (pix0 * weights89AD.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights1245.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights1245.x) + sum;
sum =                                        center                              + sum;
sum = (1.0 - min(1.0, (slope * length(pix4 - center)))) * (pix4 * weights1245.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix5 - center)))) * (pix5 * weights1245.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix6 - center)))) * (pix6 * weights89AD.y) + sum;
pix0 = sample(src, coord);
coord = coord + dx;
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dx;
pix3 = sample(src, coord);
coord = coord + dx;
pix4 = sample(src, coord);
coord = coord + dx;
pix5 = sample(src, coord);
coord = coord + dx;
pix6 = sample(src, coord);
coord = coord + dy;
sum = (1.0 - min(1.0, (slope * length(pix0 - center)))) * (pix0 * weights89AD.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights1245.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights1245.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights1245.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix4 - center)))) * (pix4 * weights1245.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix5 - center)))) * (pix5 * weights1245.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix6 - center)))) * (pix6 * weights89AD.z) + sum;
pix0 = sample(src, coord);
coord = coord + dx;
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dx;
pix3 = sample(src, coord);
coord = coord + dx;
pix4 = sample(src, coord);
coord = coord + dx;
pix5 = sample(src, coord);
coord = coord + dx;
pix6 = sample(src, coord);
coord = coord + dx + dy;
sum = (1.0 - min(1.0, (slope * length(pix0 - center)))) * (pix0 * weights89AD.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights89AD.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights1245.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights1245.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix4 - center)))) * (pix4 * weights1245.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix5 - center)))) * (pix5 * weights89AD.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix6 - center)))) * (pix6 * weights89AD.w) + sum;
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dx;
pix3 = sample(src, coord);
coord = coord + dx;
pix4 = sample(src, coord);
coord = coord + dx;
pix5 = sample(src, coord);
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights89AD.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights89AD.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights89AD.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix4 - center)))) * (pix4 * weights89AD.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix5 - center)))) * (pix5 * weights89AD.w) + sum;
return vec4(sum.rgb / sum.a, 1.0);
kernel vec4 convertFromRGBToLab(sampler src)
vec3 f;
vec4 pix, color;
pix = unpremultiply(sample(src, samplerCoord(src)));
color.xyz = pix.r * vec3(0.449695,  0.316251, 0.18452  )
+ pix.g * vec3(0.244634,  0.672034, 0.0833318)
+ pix.b * vec3(0.0251829, 0.141184, 0.922602 );
color.xyz *= vec3(1.052111, 1.0, 0.918417);
f = compare(color.xyz - 0.00885645, 7.787037 * color.xyz + 0.137931, pow(color.xyz, vec3(0.333333)));
color.r = 116.0 * f.y - 16.0;
color.g = 500.0 * (f.x - f.y);
color.b = 200.0 * (f.y - f.z);
color.rgb *= 0.005;
color.a = 1.0;
return color;
kernel vec4 convertFromLabToRGB(sampler src, sampler original)
vec3 f, cie;
vec4 color, pix, opix;
pix = sample(src, samplerCoord(src));
opix = sample(original, samplerCoord(original));
pix.rgb *= 200.0;
f.y = (pix.r + 16.0) / 116.0;
f.x = f.y + pix.g * 0.002;
f.z = f.y - pix.b * 0.005;
color.xyz = f * f * f;
cie = compare(color.xyz - 0.00885645, (f.xyz - 0.137931) / 7.787037, color.xyz);
cie *= vec3(0.95047, 1.0, 1.08883);
color.rgb = cie.x * vec3(2.95176,   -1.28951, -0.47388  )
+ cie.y * vec3(-1.0851,    1.99084,  0.0372023)
+ cie.z * vec3(0.0854804, -0.269456, 1.09113  );
color.a = opix.a;
return premultiply(color);
bilateralAdd_1
bilateralAdd_2
bilateralAdd_3
bilateralAdd_4
bilateralAdd_5
bilateralAdd_6
bilateralAdd_7
bilateralAdd_8
bilateralAdd_9
bilateralFinalize
convertFromRGBToLab
convertFromLabToRGB
points.count > 0
-[GUBilateralConvolution boundsForPointArray:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Adjustments/PIBilateralFilter.m
bilateralLoop2
bilateralLoop5
bilateralLoop11
inputEdgeDetail
ridiculously large radius for bilateral filter
-[PIBilateralFilter outputImage]
inputPoints
inputWeights
unable to allocate convolution table in bilateral filter
com.apple.livephoto
com.apple.video
com.apple.video.slomo
.%lu
%lu.%lu%@%@
satPercentile75
satPercentile98
satPercentileG98
satAutoValue
inputVibrancy
inputCast
kernel vec4 _smartcolor_contrast_HDR (__sample im, float amt) 
 vec3 diff = im.rgb-dot(im.rgb, vec3(.0, .3, .5)); 
 float dist = distance(diff, vec3(0.0)); 
 dist = smoothstep(0.0, 1.0, dist); 
 float strength = 5.0*dist*amt; 
 vec3 pos = max(im.rgb, 1.0)-1.0 + min(im.rgb, 0.0); 
 im.rgb = clamp(im.rgb, 0.0, 1.0); 
 strength *= (im.b-im.g); 
 strength = max(strength, -0.35); 
 vec4 result; 
 result.rgb = im.rgb/(strength + 1.0 - (im.rgb*strength)) + pos; 
 result.a = im.a; 
 return result; 
kernel vec4 _smartcolor_contrast_darken_HDR (__sample im, float amt) 
 vec3 diff = im.rgb-dot(im.rgb, vec3(.0, .3, .5)); 
 float dist = distance(diff, vec3(0.0)); 
 dist = smoothstep(0.0, 1.0, dist); 
 float strength = 5.0*dist*amt; 
 vec3 pos = max(im.rgb, 1.0)-1.0 + min(im.rgb, 0.0); 
 im.rgb = clamp(im.rgb, 0.0, 1.0); 
 strength *= (im.b-im.g); 
 float gray = 1.0-min(dot(im.rgb, vec3(0.5, 0.7, -0.20)), 1.0); 
 vec4 result; 
 result.rgb = strength < 0.0 ? pow(im.rgb, vec3(1.0-strength*gray)) : im.rgb/(strength+1.0-(im.rgb*strength)); 
 result.rgb += pos; result.a = im.a; 
 return result; 
kernel vec4 _smartcolor_vibrancy_gt1_HDR (__sample im, float amt) 
 float gray = dot(clamp(im.rgb, 0.0, 1.0), vec3(.3, .5, .2)); 
 float y = dot(clamp(im.rgb, 0.0, 1.0), vec3(.4, .2, .1)); 
 float damp = 1.0-4.0*y*(1.0-y); 
 float s = 1.0/(im.r+im.g+im.b); 
 float r = im.r*s; 
 float b = im.b*s; 
 float d = 1.0-.8*smoothstep(0.2, 0.4, r-b); 
 damp *= d; 
 damp = amt > 2.5 ? min(damp+(amt-2.5)/5.0, 1.0) : damp; 
 float sat = min(amt, 3.0); 
 vec4 result; 
 result.rgb = (im.rgb - gray)*sat + gray; 
 result.rgb = mix(im.rgb, result.rgb, damp); 
 result.a = im.a; 
 return result; 
kernel vec4 _smartcolor_vibrancy_lt1_HDR (__sample im, float amt) 
 float gray = dot(im.rgb, vec3(0.333333)); 
 im.rgb = mix(vec3(gray), im.rgb, amt); 
 return im; 
kernel vec4 _smartcolor_cast_HDR (__sample im, float lum, float grayI, float grayQ, float strength) 
 vec4 pix = clamp(im, 0.0, 1.0); 
 pix.rgb = pow(pix.rgb, vec3(.25)); 
 pix.rgb = pix.r * vec3(0.299, 0.595716, 0.211456) + 
 pix.g * vec3(0.587, -0.274453, -0.522591) + 
 pix.b * vec3(0.114, -0.321263, 0.311135); 
 vec2 grayOffset = vec2(grayI, grayQ) ; 
 vec3 result = pix.rgb; 
 float newStrength = 1.0 + (strength-1.0)*(1.0-pix.r) ; 
 result.gb = pix.gb + newStrength*grayOffset ; 
 float damp = max(min(1.0, pix.r/(lum+0.00001)),0.0) ; 
 result.rgb = mix(pix.rgb, result.rgb, damp) ; 
 pix.rgb = result.r * vec3(1.0) + 
 result.g * vec3(0.956296, -0.272122, -1.10699) + 
 result.b * vec3(0.621024, -0.647381, 1.70461); 
 pix.rgb = clamp(pix.rgb, 0.0, 1.0); 
 pix.rgb *= pix.rgb*pix.rgb*pix.rgb; 
 pix.rgb += min(im.rgb, 0.0) + max(im.rgb,1.0) -1.0; 
 return pix; 
CIVibrance
inputAmount
PISmartColorHDR-sat-histogram
Aperture
Photos
ColorNorm
inputAlgorithm
Auto-Enhance input: %llX, target: %llX
%@-%llX
*** failed to compute color normalization, ignored
PIAutoEnhanceFilter
CILocalLight
*** failed to compute smart tone statistics: %@
CISmartTone
autoValue
localAutoValue
CISmartColor
*** failed to compute smart color statistics: %@
*** failed to compute face balance statistics: %@
inputOrigI
inputOrigQ
inputWarmth
inputShadows
CILocalLightFilter
inputLocalLight
inputSmartShadows
inputLightMap
lightMap
inputLightMapWidth
lightMapWidth
inputLightMapHeight
lightMapHeight
CIHighKey
inputBrightness
inputExposure
inputContrast
inputHighlights
inputBlack
inputRawHighlights
-[PIParallaxCompoundLayerStackRequest initWithSegmentationItem:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Parallax/PIParallaxCompoundLayerStackRequest.m
-[PIParallaxCompoundLayerStackRequest initWithComposition:]
-[PIParallaxCompoundLayerStackRequest newRenderJob]
v16@?0@"PFParallaxLayerStack"8
v24@?0@"NSString"8Q16
v16@?0@"PFParallaxLayout"8
@"<NURenderStatistics>"16@?0@"<NURenderResult>"8
crossfadeDurationValue
crossfadeDurationTimescale
startTimeValue
startTimeTimescale
timeRangeStartValue
timeRangeStartTimescale
timeRangeDurationValue
timeRangeDurationTimescale
CIConstantColorGenerator
Failed to produce histogram for matte image
+[PISegmentationHelper histogramForSegmentationMatteImage:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Parallax/PISegmentationHelper.m
q24@?0^q8q16
CIColorThreshold
CIColorInvert
segmentationMatte != nil
+[PISegmentationHelper computeAvoidOverlapYOffsetWithVisibleFrame:imageSize:segmentationMatte:layoutConfiguration:outputUnsafeOverlap:context:]
+[PISegmentationHelper _computeAvoidingRect:imageSize:maxYMotion:segmentationMatte:layoutConfiguration:context:]
saliency preferred
saliency acceptable
padded
inactive
inTime
visible
device %d x %d
9:41
Semibold
bg lum
bg col %ld
fg lum
fg col %ld
Helvetica
CICircleGenerator
portraitInfo
spillMatteAllowed
legacyPosterStyle
+[PIParallaxLegacyPosterStyle applyInactiveStyleToImage:error:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Parallax/PIParallaxLegacyPosterStyle.m
Inactive filter is not available
Failed to produce background image with filter
@"NUAdjustment"16@?0@"NUAdjustment"8
grayI
grayQ
grayWarmth
grayY
warmFace
-[PIOrientationAdjustmentController setOrientation:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Controllers/PIOrientationAdjustmentController.m
+[PIParallaxStyleRecipeArchiver writeRecipe:toURL:error:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Parallax/PIParallaxStyleRecipeArchiver.m
Failed to open recipe destination file
+[PIParallaxStyleRecipeArchiver serializeRecipe:]
parameters
foreground
background
matte
+[PIParallaxStyleRecipeArchiver unarchivedStyleRecipeWithURL:error:]
Failed to open recipe source file
+[PIParallaxStyleRecipeArchiver deserializeRecipe:error:]
dictionary != nil
Missing recipe parameters
Invalid recipe parameters
Failed to deserialize recipe parameters
Missing foreground filters
Invalid foreground filters dictionary
Failed to deserialize foreground filters
Missing background filters
Invalid background filters dictionary
Failed to deserialize background filters
Missing matte filters
Invalid matte filters dictionary
Failed to deserialize matte filters
unit
Unknown parameter type: %@
+[PIParallaxStyleRecipeArchiver _serializeParameter:]
+[PIParallaxStyleRecipeArchiver _deserializeParameters:version:error:]
Invalid parameter name
Invalid parameter dictionary
v32@?0@"NSString"8@"NSDictionary"16^B24
+[PIParallaxStyleRecipeArchiver _deserializeParameter:version:error:]
Missing parameter type
Invalid parameter type
number
Missing number value
Expected a number value
Invalid unit value
Unknown unit value
Missing color values
Expected color values
Expected 4 color values
Expected 4 numbers
Expected 3 color values
Expected 3 numbers
point
Missing point values
Expected point values
Expected 2 point values
Expected 2 numbers
Missing mode value
Expected mode value
binding
Missing binding value
Expected binding value
Unrecognized parameter type
v32@?0@"PIParallaxStyleDefinition"8Q16^B24
filter
name
filters
Unknown definition type: %@
+[PIParallaxStyleRecipeArchiver _serializeDefinition:]
serializedFilters != nil
+[PIParallaxStyleRecipeArchiver _deserializeFilterDefinitions:version:error:]
Invalid definition dictionary
v32@?0@"NSDictionary"8Q16^B24
+[PIParallaxStyleRecipeArchiver _deserializeFilterDefinition:version:error:]
Missing definition type
Invalid filter definition type
filterName
Missing filter name
Invalid filter name
Missing filter parameters
Invalid filter parameters
Filed to deserialize filter parameters
Missing stack name
Invalid stack name
Missing stack filters
Invalid stack filters
Failed to deserialize stack filters
Unrecognized definition type
mdta/com.apple.quicktime.cinematic-video.rendering
mdta/com.apple.quicktime.disparity-float
mdta/com.apple.quicktime.aperture-float
mdta/com.apple.quicktime.camera-dictionary
PIPortraitVideoMetadata.m
metadataGroup != nil
outError != nil
Could not decode timed rendering data
Missing disparity rendering metadata
Missing aperture rendering metadata
v24@?0@"PTRenderPipeline"8@"<PTRenderState>"16
device != nil
-[PIPortraitVideoRenderer initWithDevice:colorSize:disparitySize:quality:debugMode:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Render/PIPortraitVideoRenderer.m
!NUPixelSizeIsEmpty(colorSize)
!NUPixelSizeIsEmpty(disparitySize)
quality >= PTQualityPreviewLow && quality <= PTQualityExportProfessional
<%@:%p device:%@ color=%ldx%ld disparity=%ldx%ld quality=%d debug=%ld>
PIPortraitVideoRenderer.pool
B32@?0@"PIPortraitVideoRenderer"8Q16^B24
Expected _renderPipeline and _renderState to be allocated at the same time
-[PIPortraitVideoRenderer prepareToRenderWithMetadata:]
kernel vec4 _highKeyHDR(__sample im, float str) 
 vec3 neg = min(im.rgb, 0.0); 
 vec3 pos = max(im.rgb, 1.0) - 1.0; 
 im = clamp(im, 0.0, 1.0); 
 vec4 im2 = 1.0-((im-1.0)*(im-1.0)); 
 im2 = sqrt(im2); 
 float y = dot(im.rgb, vec3(.333333)); 
 float y2 = sqrt(1.0-(y-1.0)*(y-1.0)); 
 y2 = mix(y2, smoothstep(0.0, 1.0, y2), 0.5); 
 vec4 im3 = (y>0) ? im*y2/y : vec4(0.0, 0.0, 0.0, 1.0) ; 
 im3 = mix(im3, im2, .7*sqrt(y2)); 
 im3 = mix(im, im3, sqrt(y)) ; 
 im.rgb = mix(im.rgb, im3.rgb, str) + pos + neg; 
 return im; } 
None
UnexpectedRuntimeError
NoBuildingDetected
PanoAndFFCNotSupported
FacesTooLarge
ConfidenceTooLow
LimitExceeded
NotEnoughLines
CorrectedAreaIsSmall
LessThanMinimumCorrection
-[PIPerspectiveAutoCalculator initWithComposition:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Autocalculators/PIPerspectiveAutoCalculator.m
%@.%@
%@.error
%@.underlyingError
pitchError
yawError
pitchError.underlyingError
yawError.underlyingError
%@PerspectiveEvaluation-txt.DBG
%@PerspectiveLineDetection-png.DBG
Edit
PIPerspectiveAutoCalculator-debug
faces != nil
-[PIPerspectiveAutoCalculator getSizeOfAllFaces:]
-[PIPerspectiveAutoCalculator passesFaceCheck:]
PIPerspectiveAutoCalculator-faceCheck
faceSize
maxFaceSize
passesFaceCheck
Apple
front
camera
-[PIPerspectiveAutoCalculator passesImagePropertiesCheck:]
PIPerspectiveAutoCalculator-selfieAndAspectRatioCheck
aspectRatio
isSelfieCam
passesImagePropertiesCheck
-[PIPerspectiveAutoCalculator passesBuildingCheck:]
PIPerspectiveAutoCalculator-classify
minConfidence
passesBuildingCheck
-[PIPerspectiveAutoCalculator submit:]
pitch
angle
-[PIPerspectiveAutoCalculator overcaptureImageProperties:]
No overcapture
PIPerspectiveAutoCalculator-overcaptureImageProperties
-[PIPerspectiveAutoCalculator primaryImageProperties:]
PIPerspectiveAutoCalculator-primaryImageProperties
-[PIPerspectiveAutoCalculator canGenerateNewCropRect:]
preseedRoll
canGenerateNewCropRect
setToPrimary
xOrigin
yOrigin
width
height
pitch != nil
+[PIPerspectiveAutoCalculator undoOrientation:forPitch:yaw:angle:]
yaw != nil
angle != nil
-[PIPerspectiveAutoCalculator passesConfidenceCheck:error:]
passesConfidenceCheck
supported
Not supported by Core Image. Default: YES
-[PIPerspectiveAutoCalculator passesMinimumCorrectionCheck:error:]
pitchExpandTopDegrees
yawExpandLeftDegrees
rollAngleInDegreesCW
passesMinimumCorrectionCheck
minimumPitchCorrection
minimumYawCorrection
minimumAngleCorrection
-[PIPerspectiveAutoCalculator submitVerified:]
focalLength
pitchLimit
yawLimit
rollLimit
minimumPitchCorrectionArea
minimumYawCorrectionArea
CIAutoPerspective
submitVerified
debugImage
pitchCorrectionAreaCoverage
yawCorrectionAreaCoverage
ciPitchError
ciYawError
PIPerspectiveAutoCalculator-getPrimaryOrientation
Flash
FNumber
ApertureValue
ShutterSpeedValue
ExposureTime
ISOSpeedRatings
cannot set empty crop rect
-[PICropAdjustmentController setCropRect:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Controllers/PICropAdjustmentController.m
constraintWidth
constraintHeight
smart
originalCrop
inputDecoderVersion
v32@?0@"NSString"8@16^B24
definitions != nil
-[PIParallaxRecipeFilter _evaluateImageWithFilterDefinitions:inputImage:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Parallax/PIParallaxRecipeFilter.m
Failed to produce an image
inputLight
offsetBlack
offsetBrightness
offsetContrast
offsetExposure
offsetHighlights
offsetLocalLight
offsetShadows
statistics
overcaptureStatistics
kernel vec4 alphaCompositing(__sample src, __sample dst, float a)
return mix(dst, src, a);
kernel vec4 dynamismMap(sampler imageMax, sampler imageMin, float logisticGain, float logisticMid) __attribute__((outputFormat(kCIFormatRGBAh)))
const float MAX_VAL = 1.74;
const float THRESHOLD = 0.05;
vec2 dc = destCoord();
vec2 sc;
sc = dc + vec2(-1,-1);
vec4 pMax00 = sample(imageMax, samplerTransform(imageMax, sc));
vec4 pMin00 = sample(imageMin, samplerTransform(imageMin, sc));
sc = dc + vec2(-1,0);
vec4 pMax01 = sample(imageMax, samplerTransform(imageMax, sc));
vec4 pMin01 = sample(imageMin, samplerTransform(imageMin, sc));
sc = dc + vec2(-1,1);
vec4 pMax02 = sample(imageMax, samplerTransform(imageMax, sc));
vec4 pMin02 = sample(imageMin, samplerTransform(imageMin, sc));
sc = dc + vec2(0,-1);
vec4 pMax10 = sample(imageMax, samplerTransform(imageMax, sc));
vec4 pMin10 = sample(imageMin, samplerTransform(imageMin, sc));
vec4 pMax11 = sample(imageMax, samplerTransform(imageMax, dc));
vec4 pMin11 = sample(imageMin, samplerTransform(imageMin, dc));
sc = dc + vec2(0,1);
vec4 pMax12 = sample(imageMax, samplerTransform(imageMax, sc));
vec4 pMin12 = sample(imageMin, samplerTransform(imageMin, sc));
sc = dc + vec2(1,-1);
vec4 pMax20 = sample(imageMax, samplerTransform(imageMax, sc));
vec4 pMin20 = sample(imageMin, samplerTransform(imageMin, sc));
sc = dc + vec2(1,0);
vec4 pMax21 = sample(imageMax, samplerTransform(imageMax, sc));
vec4 pMin21 = sample(imageMin, samplerTransform(imageMin, sc));
sc = dc + vec2(1,1);
vec4 pMax22 = sample(imageMax, samplerTransform(imageMax, sc));
vec4 pMin22 = sample(imageMin, samplerTransform(imageMin, sc));
float minDevCMaxNMin = distance(pMax11, pMin00);
minDevCMaxNMin = min( minDevCMaxNMin , distance(pMax11, pMin01) );
minDevCMaxNMin = min( minDevCMaxNMin , distance(pMax11, pMin02) );
minDevCMaxNMin = min( minDevCMaxNMin , distance(pMax11, pMin10) );
minDevCMaxNMin = min( minDevCMaxNMin , distance(pMax11, pMin11) );
minDevCMaxNMin = min( minDevCMaxNMin , distance(pMax11, pMin12) );
minDevCMaxNMin = min( minDevCMaxNMin , distance(pMax11, pMin20) );
minDevCMaxNMin = min( minDevCMaxNMin , distance(pMax11, pMin21) );
minDevCMaxNMin = min( minDevCMaxNMin , distance(pMax11, pMin22) );
float minDevCMinNMax = distance(pMin11, pMax00);
minDevCMinNMax = min( minDevCMinNMax , distance(pMin11, pMax01) );
minDevCMinNMax = min( minDevCMinNMax , distance(pMin11, pMax02) );
minDevCMinNMax = min( minDevCMinNMax , distance(pMin11, pMax10) );
minDevCMinNMax = min( minDevCMinNMax , distance(pMin11, pMax11) );
minDevCMinNMax = min( minDevCMinNMax , distance(pMin11, pMax12) );
minDevCMinNMax = min( minDevCMinNMax , distance(pMin11, pMax20) );
minDevCMinNMax = min( minDevCMinNMax , distance(pMin11, pMax21) );
minDevCMinNMax = min( minDevCMinNMax , distance(pMin11, pMax22) );
float outVal = min(minDevCMaxNMin , minDevCMinNMax) / MAX_VAL;
outVal = 1.0f / (1.0f + exp(-logisticGain*(outVal - logisticMid)));
vec4 outPixel = vec4(outVal, outVal, outVal, 1.0);
return outPixel;
kernel vec4 jointbilateralfilter(sampler mask, sampler guide, vec2 rad, vec3 sig) __attribute__((outputFormat(kCIFormatRh)))
vec2 coord = destCoord();
float rh = rad.x;
float rv = rad.y;
vec4 p0 = sample(mask, samplerTransform(mask, coord));
vec4 g0 = sample(guide, samplerTransform(guide, coord));
vec4 result = vec4(0.0f);
float w_sum = 0.0f;
for(float yo=-rad.y;  yo<=rad.y;  yo++) {
for(float xo=-rad.x;  xo<=rad.x;  xo++) {
vec2 dc = vec2(xo, yo);
vec4 p = sample(mask, samplerTransform(mask, coord + dc));
vec4 g = sample(guide, samplerTransform(guide, coord + dc));
float d = dot(dc, dc);
float pdif = dot(p-p0,p-p0);
float gdif = dot(g-g0,g-g0);
float w = exp(dot(sig, vec3(d,pdif,gdif)));
result += w*p;
w_sum += w;
result /= w_sum;
return vec4(result.rgb, 1.0);
kernel vec4 rgba_to_luma(__sample rgb)
const vec4 rgb2y = vec4(0.2999,0.5870,0.1140,0.0);
float luma = dot(rgb, rgb2y);
return vec4(luma, luma, luma, 1.0);
kernel vec2 warp_homography(vec3 h012, vec3 h345, vec3 h678)
vec3 coord = vec3(destCoord(), 1.f);
float norm = 1.f / dot(h678, coord);
float x = norm * dot(h012, coord);
float y = norm * dot(h345, coord);
return vec2(x,y);
kernel vec4 ncc_compute(sampler img1, sampler img2) __attribute__((outputFormat(kCIFormatR8)))
vec2 coord = destCoord();
float sum1   = float(0.0);
float sum2   = float(0.0);
float sumSq1 = float(0.0);
float sumSq2 = float(0.0);
float sumCrs = float(0.0);
float ncc;
float p1, p2;
const int halfKernel = 3;
const vec4 meanOp = vec4(1.0/3.0, 1.0/3.0, 1.0/3.0, 0.0);
float count = 0.0;
for(int y=-halfKernel; y<=halfKernel; y++) {
for(int x=-halfKernel; x<=halfKernel; x++) {
p1 = dot(meanOp, sample(img1, samplerTransform(img1, coord + vec2(x, y))));
p2 = dot(meanOp, sample(img2, samplerTransform(img2, coord + vec2(x, y))));
sum1 += p1;
sum2 += p2;
sumSq1 += p1*p1;
sumSq2 += p2*p2;
sumCrs += p1*p2;
count += 1.0;
float denom = (sumSq1-sum1*sum1/count) * (sumSq2-sum2*sum2/count);
ncc = compare(denom, 0.0, ((sumCrs-(sum1*sum2/count)))/sqrt(denom));
ncc = max(ncc, 0.0);
return vec4(ncc, ncc, ncc, 1.0);
kernel vec4 ncc_coarse_compute(__sample ncc, vec2 edge) __attribute__((outputFormat(kCIFormatR8)))
float value = smoothstep(edge.x, edge.y, ncc.r);
return vec4(value, value, value, 1.0);
kernel vec4 blur_image_compute_3x3(sampler img)
const int halfKernel = 1;
vec2 coord = destCoord();
vec4 sum = vec4(0.0);
float count = 0.0;
for(int y=-halfKernel; y<=halfKernel; y++) {
for(int x=-halfKernel; x<=halfKernel; x++) {
sum += sample(img, samplerTransform(img, coord + vec2(x, y)));
count += 1.0;
sum = sum/(count);
return sum;
kernel vec4 blur_image_compute_5x5(sampler img)
const int halfKernel = 2;
vec2 coord = destCoord();
vec4 sum = vec4(0.0);
float count = 0.0;
for(int y=-halfKernel; y<=halfKernel; y++) {
for(int x=-halfKernel; x<=halfKernel; x++) {
sum += sample(img, samplerTransform(img, coord + vec2(x, y)));
count += 1.0;
sum = sum/(count);
return sum;
kernel vec4 blur_image_compute_7x7(sampler img)
const int halfKernel = 3;
vec2 coord = destCoord();
vec4 sum = vec4(0.0);
float count = 0.0;
for(int y=-halfKernel; y<=halfKernel; y++) {
for(int x=-halfKernel; x<=halfKernel; x++) {
sum += sample(img, samplerTransform(img, coord + vec2(x, y)));
count += 1.0;
sum = sum/(count);
return sum;
kernel vec4 fuse_image_compute(sampler refImg, sampler guideImg, sampler blurImg, sampler weightImg, sampler maskImg, vec2 threshold)
vec4 ref       = sample(refImg, samplerCoord(refImg));
vec4 refBlur   = ref;
vec4 guide     = sample(guideImg, samplerCoord(guideImg));
vec4 guideBlur = sample(blurImg, samplerCoord(blurImg));
float weight   = sample(weightImg, samplerCoord(weightImg)).r;
float mask     = sample(maskImg, samplerCoord(maskImg)).r;
vec3 CbCoef = vec3(-0.1726, -0.3391, 0.5117);
vec3 CrCoef = vec3(0.5115, -0.4282, -0.0830);
float CbRef   = dot(CbCoef, ref.rgb);
float CbGuide = dot(CbCoef, guideBlur.rgb);
float CrRef   = dot(CrCoef, ref.rgb);
float CrGuide = dot(CrCoef, guideBlur.rgb);
float crDiff = smoothstep(0.0, 0.2, abs(CbRef-CbGuide));
float cbDiff = smoothstep(0.0, 0.2, abs(CrRef-CrGuide));
float chDiff = smoothstep(0.0,0.3,crDiff+cbDiff);
float weight1 = (1.0-smoothstep(threshold.x,threshold.y,mask));
weight *= weight1 * (1.0-chDiff);
vec4 value = mix(ref, (0.9*(guide-guideBlur)+refBlur), weight);
return value;
jointbilateralfilter
rgba_to_luma
warp_homography
ncc_compute
ncc_coarse_compute
blur_image_compute_7x7
blur_image_compute_5x5
blur_image_compute_3x3
fuse_image_compute
CIPhotoEffectMono
Mono
CIPhotoEffectTonal
Tonal
CIPhotoEffectNoir
Noir
CIPhotoEffectFade
Fade
CIPhotoEffectChrome
Chrome
CIPhotoEffectProcess
Process
CIPhotoEffectTransfer
Transfer
CIPhotoEffectInstant
Instant
CIPhotoEffect3DVivid
3DVivid
CIPhotoEffect3DVividWarm
3DVividWarm
CIPhotoEffect3DVividCool
3DVividCool
CIPhotoEffect3DDramatic
3DDramatic
CIPhotoEffect3DDramaticWarm
3DDramaticWarm
CIPhotoEffect3DDramaticCool
3DDramaticCool
CIPhotoEffect3DSilverplate
3DSilverplate
CIPhotoEffect3DNoir
3DNoir
CIPortraitEffectBlack
Black
CIPortraitEffectBlackoutMono
BlackoutMono
CIPortraitEffectStageV2
StageV2
CIPortraitEffectStageMonoV2
StageMonoV2
CIPortraitEffectStageWhite
StageWhite
CIPortraitEffectLight
Light
CIPortraitEffectCommercial
Commercial
CIPortraitEffectContour
Contour
CIPortraitEffectStudioV2
StudioV2
CIPortraitEffectContourV2
ContourV2
%@ (preview) %a
%@ %a
+[PIPhotoEditHelper imageSourceWithURL:type:useEmbeddedPreview:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Util/PIPhotoEditHelper.m
+[PIPhotoEditHelper imageSourceWithURL:type:proxyImage:orientation:useEmbeddedPreview:]
+[PIPhotoEditHelper imageSourceWithCIImage:orientation:]
+[PIPhotoEditHelper videoSourceWithURL:]
photoSource != nil
+[PIPhotoEditHelper livePhotoSourceWithPhotoSource:videoSource:]
videoSource != nil
%@+%@
name != nil
+[PIPhotoEditHelper newAdjustmentWithName:]
+[PIPhotoEditHelper newAdjustmentWithIdentifier:]
+[PIPhotoEditHelper newImageRenderClientWithName:]
+[PIPhotoEditHelper geometryRequestWithComposition:]
+[PIPhotoEditHelper imagePropertiesRequestWithComposition:]
+[PIPhotoEditHelper videoPropertiesRequestWithComposition:]
targetSize.width > 0 && targetSize.height > 0
+[PIPhotoEditHelper imageRenderRequestWithComposition:fitInSize:wideGamut:]
PIPhotoEditHelper-targetSize-image
+[PIPhotoEditHelper imageRenderRequestWithComposition:fillInSize:wideGamut:]
PIPhotoEditHelper-fillSquare-image
+[PIPhotoEditHelper _imageRenderRequestWithComposition:wideGamut:]
PIPhotoEditHelper-basic-image
+[PIPhotoEditHelper videoRenderRequestWithComposition:fitInSize:]
v16@?0q8
overrideVideoEditability
v32@?0@"NSString"8@"NSString"16^B24
var image = input;
ResetTagInput('/pre-VideoReframe', image);
image = GetTag('VideoReframe');
ResetTagInput('/pre-Geometry', image);
return GetTag('/post-Geometry');
/pre-VideoReframe
VideoReframe
/pre-Geometry
/post-Geometry
/ShowOriginalSource
ResetTagInput('/pre-Geometry', input);
return GetTag('/post-Geometry');
PIPhotoEditHelper.m
inputCorrectionInfo
depthInfo
start
startScale
endScale
SkipShaderPrewarm
boostVersion
boostParams
gainMapVersion
gainMapParameters
kernel vec4 forwardBoostWithBoost(sampler image, float boost) {
vec4 im = sample(image, samplerCoord(image));
vec4 orig = im;
float n = 1.8;
float k = 0.8;
vec3 pos = max(im.rgb, k) - k;
vec3 neg = min(im.rgb, 0.0);
neg *= 2.8;
im.rgb = clamp(im.rgb, 0.0, k);
im.rgb = ((1.0+n)*im.rgb)/(1.0+(n*im.rgb)) + neg;
im.r = pos.r > 0.0 ? (.91803 + .470304*pos.r) : im.r;
im.g = pos.g > 0.0 ? (.91803 + .470304*pos.g) : im.g;
im.b = pos.b > 0.0 ? (.91803 + .470304*pos.b) : im.b;
im.rgb = mix(orig.rgb, im.rgb, boost);
return im;
kernel vec4 inverseBoostWithBoost(sampler image, float boost){
vec4 im = sample(image, samplerCoord(image));
vec4 orig = im;
float n = 1.8;
float kinv = 0.91803;
vec3 pos = max(im.rgb, kinv);
vec3 neg = min(im.rgb, 0.0);
im.rgb = clamp(im.rgb, 0.0, kinv);
neg *= .35714286;
im.rgb = im.rgb/(n+1.0-(im.rgb*n)) + neg;
im.r = pos.r > kinv ? 0.8 + 2.126286*(pos.r-kinv) : im.r;
im.g = pos.g > kinv ? 0.8 + 2.126286*(pos.g-kinv) : im.g;
im.b = pos.b > kinv ? 0.8 + 2.126286*(pos.b-kinv) : im.b;
im.rgb = mix(orig.rgb, im.rgb, boost);
return im;
kernel vec4 forwardBoost3(__sample im, float boost, vec4 abcd, vec3 p0, vec3 p1) {
float a = abcd.x;
float b = abcd.y;
float c = abcd.z;
float d = abcd.w;
vec3 x = im.rgb;
vec3 f = (a * x + b) / (c * x + d);
float x0 = p0.x;
float y0 = p0.y;
float s0 = p0.z;
float a0 = y0*y0;
float c0 = y0 - s0*x0;
float d0 = s0*x0*x0;
vec3 f0 = (a0 * x) / (c0 * x + d0);
vec3 l0 = (a0 / d0) * x;
f0 = compare(x, l0, f0);
float x1 = p1.x;
float y1 = p1.y;
float s1 = p1.z;
float a1 = (1.f-y1)*(1.f-y1);
float c1 = (1.f-y1) - s1*(1.f-x1);
float d1 = s1 *(1.f-x1)*(1.f-x1);
vec3 f1 = 1.f - (a1 * (1.f - x)) / (c1 * (1.f - x) + d1);
vec3 l1 = 1.f - (a1 / d1) * (1.f - x);
f1 = compare(1.f - x, l1, f1);
f = compare(x - p0.x, f0, compare(x - p1.x, f, f1));
im.rgb = mix(im.rgb, f, boost);
return im;
kernel vec4 inverseBoost3(__sample im, float boost, vec4 abcd, vec3 p0, vec3 p1) {
float a = abcd.x;
float b = abcd.y;
float c = abcd.z;
float d = abcd.w;
vec3 y = im.rgb;
vec3 g = (d * y - b) / (a - c * y);
float x0 = p0.x;
float y0 = p0.y;
float s0 = p0.z;
float a0 = x0*x0;
float c0 = x0 - y0/s0;
float d0 = y0*y0/s0;
vec3 g0 = (a0 * y) / (c0 * y + d0);
vec3 l0 = (a0 / d0) * y;
g0 = compare(y, l0, g0);
float x1 = p1.x;
float y1 = p1.y;
float s1 = p1.z;
float a1 = (1.f-x1)*(1.f-x1);
float c1 = (1.f-x1) - (1.f-y1)/s1;
float d1 = (1.f-y1)*(1.f-y1)/s1;
vec3 g1 = 1.f - (a1 * (1.f - y)) / (c1 * (1.f - y) + d1);
vec3 l1 = 1.f - (a1 / d1) * (1.f - y);
g1 = compare(1.f - y, l1, g1);
g = compare(y - p0.y, g0, compare(y - p1.y, g, g1));
im.rgb = mix(im.rgb, g, boost);
return im;
+[PIFakeBoost kernelFB0]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Adjustments/PIFakeBoost.m
+[PIFakeBoost kernelFB3]
properties != nil
+[PIFakeBoost boostParametersFromRawProperties:]
-[PIFakeBoost outputImageFB0]
-[PIFakeBoost outputImageFB3]
Invalid boost parameters
Invalid points parameters
boost kernel is nil
+[PIForwardFakeBoost kernelFB0]_block_invoke
+[PIForwardFakeBoost kernelFB3]_block_invoke
inverse boost kernel is nil
+[PIInverseFakeBoost kernelFB0]_block_invoke
+[PIInverseFakeBoost kernelFB3]_block_invoke
PIAttributeTypeMode
PIAttributeAvailableModes
default
metallib
Failed to load metal lib data: %@
+[PICoreImageUtilities metalLibraryData]_block_invoke
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Adjustments/PICoreImageUtilities.m
AutoEnhance
AutoLoop
Crop
DepthEffect
Effect
Effect3D
Grain
HighResolutionFusion
LivePhotoKeyFrame
Mute
PortraitEffect
PortraitVideo
RedEyeBB
SelectiveColor
SemanticEnhance
SlowMotion
SmartBlackAndWhite
SmartColor
SmartTone
SourceSelect
Trim
VideoCrossfadeLoop
VideoPosterFrame
VideoStabilize
WhiteBalance
compositionName
identifierName
custom
customSerialization
customCompositionName
customIdentifierName
requiresEnabled
offsetBlackPoint
v24@?0@"NSDictionary"8@"NUAdjustment"16
v32@?0@8@16^B24
offsetSaturation
offsetCast
offsetStrength
offsetGrain
offsetTone
inputBlackAndWhite
offsetNeutralGamma
amount
seed
DGVignetteEffectOperation
Vignette
inputIntensity
inputRadius
inputFalloff
DGDefinition2Operation
Definition
RKNoiseReductionOperation
NoiseReduction
edgeDetail
RKProSharpenOperation
Sharpen
inputEdgeScale
edges
inputSharpness
@"NSString"16@?0@"NSDictionary"8
effectName
@"NSString"24@?0@"NSDictionary"8@"NUAdjustment"16
effectVersion
effectIntensity
CropStraighten
straightenAngle
DGRAWReduceNoiseOperation
RawNoiseReduction
inputDetailAmount
detail
inputCNRAmount
inputLNRAmount
RKRawDecodeOperation
inputMethodVersion
colorComponents
slope
bias
RKLevelsOperation
Levels
colorSpace
inputColorSpace
blackSrcRGB
blackDstRGB
shadowSrcRGB
shadowDstRGB
midSrcRGB
midDstRGB
hilightSrcRGB
hilightDstRGB
whiteSrcRGB
whiteDstRGB
blackSrcRed
blackDstRed
shadowSrcRed
shadowDstRed
midSrcRed
midDstRed
hilightSrcRed
hilightDstRed
whiteSrcRed
whiteDstRed
blackSrcGreen
blackDstGreen
shadowSrcGreen
shadowDstGreen
midSrcGreen
midDstGreen
hilightSrcGreen
hilightDstGreen
whiteSrcGreen
whiteDstGreen
blackSrcBlue
blackDstBlue
shadowSrcBlue
shadowDstBlue
midSrcBlue
midDstBlue
hilightSrcBlue
hilightDstBlue
whiteSrcBlue
whiteDstBlue
Generic P3
Display P3
Adobe RGB
sRGB
RKCurvesOperation
Curves
RGBCurvePoints
pointsL
redCurvePoints
pointsR
greenCurvePoints
pointsG
blueCurvePoints
pointsB
RKSelectiveColorOperation
corrections
RKRetouchOperation
Retouch
B16@?0@"NSDictionary"8
inputStrokes
hasSource
brushStroke
softness
opacity
repairEdges
sourceOffset
data
xLocation
yLocation
pressure
points
clipRect
detectedFaces
RKRedEyeOperation
ApertureRedEye
inputSpots
pointX
center
pointY
sensitivity
RedEye
allCorrections
redEyeCorrections
autoRedEyeCorrections
endTime
timeRange
rate
timescale
alignment
glassesMatteAllowed
portraitEffectFilterName
keyframes
cropFraction
analysisType
v24@?0@"NSMutableDictionary"8@"NUAdjustment"16
face
warm
inputDetectedFaces
flags
epoch
lightMapAvg
vec4 _curve_sample_HDR(float x, sampler2D table, vec2 domain, vec2 normalizer) { x = (x - domain.x) / (domain.y - domain.x); x = clamp(x, 0.0001, 0.9999); x = normalizer.x * x + normalizer.y; return texture2D(table, vec2(x, 0.5)); } kernel vec4 _curves_rgb_lum_HDR(__sample color, sampler2D table, vec2 domain, vec2 normalizer) { vec4 pixel = color; pixel.r = _curve_sample_HDR(pixel.r, table, domain, normalizer).r; pixel.g = _curve_sample_HDR(pixel.g, table, domain, normalizer).g; pixel.b = _curve_sample_HDR(pixel.b, table, domain, normalizer).b; float lum0 = dot(pixel.rgb, vec3(0.3, 0.59, 0.11)); float lum1 = _curve_sample_HDR(lum0, table, domain, normalizer).a; float lum1c = clamp(lum1, -8.0 * abs(lum0), 8.0 * abs(lum0)); float lum_scale = (lum0 == 0.0 ? 0.0 : lum1c / lum0); float lum_offset = lum1 - lum1c; pixel.rgb = lum_scale * pixel.rgb + lum_offset; pixel.rgb = min(pixel.rgb, 1.0); return pixel; }
-[_PIParallaxLayoutJob initWithRequest:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Parallax/PIParallaxLayoutRequest.m
Missing layout configuration
Missing layout regions
Invalid segmentation matte size
Invalid segmentation classification
Invalid segmentation confidence map size
-[_PIParallaxLayoutJob render:]
Missing output geometry
Failed to generate segmentation layout
-[_PIParallaxLayoutJob complete:]
layer stack has no background layer
none
pixels
degrees
count
logic
stack
clockFont
clockColor
<%@:%p parameters: %@
foreground:%@
 background: %@
 matte: %@>
-[PIParallaxStyleDefinition type]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Parallax/PIParallaxStyleRecipe.m
-[PIParallaxStyleDefinition isEqualToParallaxStyleDefinition:]
-[PIParallaxStyleDefinition evaluateWithContext:error:]
-[PIParallaxStyleFilterDefinition init]
context != nil
-[PIParallaxStyleFilterDefinition evaluateWithContext:error:]
Unknown filter
Failed to set filter parameter value
Unknown filter parameter
v32@?0@"NSString"8@"PIParallaxStyleParameter"16^B24
Failed to evaluate filter parameters
Local
inputLightMapImage
inputTargetBackgroundImage
filter produced invalid image
CILocalContrast
LocalContrast
<%@:%p filter:%@ parameters: %@>
-[PIParallaxStyleFilterStackDefinition init]
-[PIParallaxStyleFilterStackDefinition evaluateWithContext:error:]
<%@:%p stack:%@ filters:%@>
-[PIParallaxStyleParameter type]
-[PIParallaxStyleParameter evaluateWithContext:error:]
-[PIParallaxStyleParameter isEqualToParallaxStyleParameter:]
(%@, unit: %@) 
(R:%@, G:%@, B:%@, A:%@)
(X:%@, Y:%@, unit: %@)
(>%@)
-[PIParallaxStyleBindingParameter evaluateWithContext:error:]
Unable to find source for variable bound to '%@'
($%@)
_PIDynamicLocalLightMapPrepare
inputGuideImage
PILongExposureAccumulator.state
PILongExposureAccumulator.accum
Accumulation was cancelled
PILongExposureAccumulator-main-j%lld
failed to init accumulator
B16@?0@"CIRenderDestination"8
frame != nil
-[PILongExposureAccumulator accumulate:error:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/AutoLoop/PIAutoLoopJob+LongExposure.m
-[PILongExposureAccumulator _accumulate:error:]
PILongExposureAccumulator-average-j%lld
failed to render average image
v16@?0@"CIImage"8
CIBoxBlur
PILongExposureAccumulator-minimum-j%lld
failed to render minimum image
PILongExposureAccumulator-maximum-j%lld
failed to render maximum image
-[PILongExposureAccumulator writeLongExposureImage:UTI:colorSpace:error:]
-[PILongExposureAccumulator writeMaskImage:UTI:error:]
-[PILongExposureAccumulator _exportOutputImage:format:colorSpace:toURL:uti:error:]
Failed to finalize image destination
Failed to create CGImageDestinationRef
Failed to create CGImageRef
/AutoLoop/LongExposure
-[PILongExposureRegistrationJob prepare:]
return Source(composition.source, {'skipOrientation':true});
Malformed AutoLoop recipe : crop
-[PILongExposureRegistrationJob render:]
Failed to allocate intermediate pixel buffer
PILongExposureRegistrationJob-render-j%lld
PILongExposureRegistrationJob-reference-j%lld
Failed to render luma
Image registration failure (expected 1 observation)
/RAW/SushiLevel1
/Master%@
CIRedEyeCorrection
inputCameraModel
CILanczosScaleTransform
1.5.1
1.9.1
1.10
computed
overcaptureComputed
dynamic
@"NSString"16@?0@"NUAdjustment"8
SDOFRenderingVersion
convexHull
/private/var/mobile/Media/PhotoData/CaptureDebug/
Repair
Clone
RepairML
<%@: %p; min = %.2f, max = %.2f, manualMin = %.2f, manualMax = %.2f, depthMin = %.2f, depthMax = %.2f>
manualMin
manualMax
depthMin
depthMax
SegmentationScoreRanges
monochrome
assetURL
-[PIPortraitVideoDebugDetectionsRenderNode resolvedNodeWithCachedInputs:settings:pipelineState:error:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Render/PIPortraitVideoDebugDetectionsRenderNode.m
v20@?0B8@"NSError"12
%@-labelImageCache
-[PIPortraitVideoDebugDetectionsRenderNode _evaluateImage:]
Face
Head
Torso
Sports Ball
AutoFocus
%@ %@
%@ G:%@
kernel vec4 ipt_hue_chroma_color_wash(__sample s, vec3 c) {
float luma = s.x;
float hue = c.y;
float chroma = 0.5*(s.z+c.z);
return vec4(luma, hue, chroma, s.a);
kernel vec4 ipt_hue_chroma_color_wash_fixed(__sample s, vec3 c) {
float luma = (s.x <= 0.5) ? mix(0.0, c.x, 2.0*s.x) : mix(c.x, 1.0, 2.0*(s.x-0.5));
float hue = c.y;
float chroma = 0.5*(s.z+c.z);
return vec4(luma, hue, chroma, s.a);
kernel vec4 ipt_hue_chroma_color_wash_variable(__sample s, vec3 c) {
float x0 = 0.5;
float y0 = 0.5 + 0.75 * (c.x - 0.5);
float a = (y0 - x0) / (x0 * (1.0 - y0));
float luma = (s.x * (a + 1.0)) / (s.x * a + 1.0);
float hue = c.y;
float chroma = 0.5*(s.z+c.z);
return vec4(luma, hue, chroma, s.a);
kernel vec4 rgb_color_wash_variable(__sample s, __color c) {
float l0 = dot(c.rgb, vec3(0.299, 0.587, 0.114));
float l = dot(s.rgb, vec3(0.299, 0.587, 0.114));
vec3 cw;
if (l <= l0) {
cw = mix(vec3(0), c.rgb, l/l0);
} else {
cw = mix(c.rgb, vec3(1), (l-l0)/(1-l0));
return vec4(cw, 1.0);
kernel vec4 rgb_color_wash_variable_smooth(__sample s, __color c) {
float x0 = dot(c.rgb, vec3(0.299, 0.587, 0.114));
vec3 y0 = clamp(c.rgb, 0.05, 0.95);
float x = dot(s.rgb, vec3(0.299, 0.587, 0.114));
vec3 a = (y0 - x0) / (x0 * (1.0 - y0));
vec3 y = (x * (a + 1.0)) / (x * a + 1.0);
return vec4(y, 1.0);
kernel vec4 rgb_color_wash_fixed(__sample s, __color c) {
float l = dot(s.rgb, vec3(0.299, 0.587, 0.114));
vec3 cw;
if (l <= 0.5) {
cw = mix(vec3(0), c.rgb, 2*l);
} else {
cw = mix(c.rgb, vec3(1), 2*(l-0.5));
return vec4(cw, 1.0);
kernel vec4 rgb_color_wash_fixed_smooth(__sample s, __color c) {
float x0 = 0.5;
vec3 y0 = clamp(c.rgb, 0.05, 0.95);
float x = dot(s.rgb, vec3(0.299, 0.587, 0.114));
vec3 a = (y0 - x0) / (x0 * (1.0 - y0));
vec3 y = (x * (a + 1.0)) / (x * a + 1.0);
return vec4(y, 1.0);
kernel vec4 ipt_color_wash_duo(__sample s, vec3 c0, vec3 c1) {
vec3 cw = mix(c0, c1, s.x);
return vec4(cw, s.a);
kernel vec4 ipt_color_wash_duo_fixed(__sample s, vec3 c0, vec3 c1) {
vec3 cw;
cw.x = (s.x <= 0.2) ? mix(0.0, c0.x, s.x/0.2) : (s.x <= 0.8) ? mix(c0.x, c1.x, (s.x-0.2)/(0.8-0.2)) : mix(c1.x, 1.0, (s.x-0.8)/(1.0-0.8));
float x = clamp((s.x-0.2)/(0.8-0.2), 0.0, 1.0);
cw.yz = mix(c0.yz, c1.yz, x);
return vec4(cw, s.a);
kernel vec4 ipt_color_wash_duo_variable(__sample s, vec3 c0, vec3 c1) {
vec3 cw;
cw.x = (s.x <= c0.x) ? mix(0.0, c0.x, s.x/c0.x) : (s.x <= c1.x) ? mix(c0.x, c1.x, (s.x-c0.x)/(c1.x-c0.x)) : mix(c1.x, 1.0, (s.x-c1.x)/(1.0-c1.x));
float x = clamp((s.x-c0.x)/(c1.x-c0.x), 0.0, 1.0);
cw.yz = mix(c0.yz, c1.yz, x);
return vec4(cw, s.a);
kernel vec4 ipt_hue_chroma_color_wash_duo(__sample s, vec3 c0, vec3 c1) {
vec3 lhc = mix(c0, c1, s.x);
lhc.z = 0.5*(s.z+lhc.z);
return vec4(lhc, s.a);
kernel vec4 ipt_hue_chroma_color_wash_duo_fixed(__sample s, vec3 c0, vec3 c1) {
float l = (s.x <= 0.8) ? mix(c0.x, c1.x, s.x/0.8) : mix(c1.x, 1.0, (s.x-0.8)/(1.0-0.8));
float x = clamp(s.x/0.8, 0.0, 1.0);
vec2 hc = mix(c0.yz, c1.yz, x);
hc.y = 0.5*(s.z+hc.y);
return vec4(l, hc.x, hc.y, s.a);
kernel vec4 ipt_hue_chroma_color_wash_duo_variable(__sample s, vec3 c0, vec3 c1) {
float l = (s.x <= c1.x) ? mix(c0.x, c1.x, s.x/c1.x) : mix(c1.x, 1.0, (s.x-c1.x)/(1.0-c1.x));
float x = clamp(s.x/c1.x, 0.0, 1.0);
vec2 hc = mix(c0.yz, c1.yz, x);
hc.y = 0.5*(s.z+hc.y);
return vec4(l, hc.x, hc.y, s.a);
kernel vec4 rgb_color_wash_duo(__sample s, __color c0, __color c1) {
float l = dot(s.rgb, vec3(0.299, 0.587, 0.114));
vec3 cw = mix(c0.rgb, c1.rgb, l);
return vec4(cw, s.a);
kernel vec4 rgb_color_wash_duo_fixed(__sample s, __color c0, __color c1) {
float l = dot(s.rgb, vec3(0.299, 0.587, 0.114));
vec3 cw;
if (l <= 0.75) {
cw = mix(c0.rgb, c1.rgb, l/0.75);
} else {
cw = mix(c1.rgb, vec3(1), 4*(l-0.75));
return vec4(cw, s.a);
kernel vec4 rgb_color_wash_duo_variable(__sample s, __color c0, __color c1) {
float l = dot(s.rgb, vec3(0.299, 0.587, 0.114));
float l0 = dot(c0.rgb, vec3(0.299, 0.587, 0.114));
float l1 = dot(c1.rgb, vec3(0.299, 0.587, 0.114));
vec3 cw;
if (l <= l1) {
cw = mix(c0.rgb, c1.rgb, l/l1);
} else {
cw = mix(c1.rgb, vec3(1), (l-l1)/(1-l1));
return vec4(cw, s.a);
ipt_hue_chroma_color_wash
ipt_hue_chroma_color_wash_fixed
ipt_hue_chroma_color_wash_variable
rgb_color_wash_fixed
rgb_color_wash_variable
rgb_color_wash_fixed_smooth
rgb_color_wash_variable_smooth
inputMode
HueChroma
HueChromaFixed
HueChromaVariable
RGBFixed
RGBFixedSmooth
RGBVariable
RGBVariableSmooth
MonoLight
CISoftLightBlendMode
ipt_color_wash_duo
ipt_color_wash_duo_fixed
ipt_color_wash_duo_variable
ipt_hue_chroma_color_wash_duo
ipt_hue_chroma_color_wash_duo_fixed
ipt_hue_chroma_color_wash_duo_variable
rgb_color_wash_duo
rgb_color_wash_duo_fixed
rgb_color_wash_duo_variable
inputShadowColor
inputHighlightColor
IPTFixed
IPTVariable
outputExposure
falseColorHDR
inputRAWGamutMapMax
Buffer must be RGBA16 type for red eye repairs
id<NUBuffer> computeRepairMask(__strong id<NUBuffer>, uint16_t *)
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Adjustments/ApertureRedEye/PIApertureRedEye.mm
reddestx out of bounds
void seedFill(__strong id<NUMutableBuffer>, __strong id<NUBuffer>, const uint32_t, const NSInteger, const NSInteger)
reddesty out of bounds
void applyRepairMask(__strong id<NUMutableBuffer>, __strong id<NUBuffer>, double, const uint32_t)
repairBuffer != nil
kernel vec4 facebalance(__sample src, vec2 gamma, vec3 matchMinusOrigStrength)
vec4 pix = src;
pix.rgb = pow(max(pix.rgb, 0.0), vec3(gamma.x));
pix.rgb = pix.r * vec3(0.299,  0.595716,  0.211456) +
pix.g * vec3(0.587, -0.274453, -0.522591) +
pix.b * vec3(0.114, -0.321263,  0.311135);
vec4 orig = pix ;
float chroma = min(1.0, 2.0*sqrt(pix.g*pix.g + pix.b*pix.b) );
pix.gb +=  matchMinusOrigStrength.rg;
float strength = matchMinusOrigStrength.z*pow(chroma, .4) ;
pix.gb = mix(orig.gb, pix.gb, strength) ;
pix.rgb = pix.rrr +
pix.g * vec3(0.956296, -0.272122, -1.10699) +
pix.b * vec3(0.621024, -0.647381, 1.70461);
pix.rgb = max(pix.rgb, vec3(0.0));
pix.rgb = pow(pix.rgb, vec3(gamma.y));
return pix;
facebalance
vec4 curve_sample(float x, sampler2D table, vec2 domain, vec2 normalizer)
x = (x - domain.x) / (domain.y - domain.x);
x = clamp(x, 0.0001, 0.9999);
x = normalizer.x * x + normalizer.y;
return texture2D(table, vec2(x, 0.5));
kernel vec4 curves_rgb_lum(__sample color, sampler2D table, vec2 domain, vec2 normalizer)
vec4 pixel = color;
pixel.r = curve_sample(pixel.r, table, domain, normalizer).r;
pixel.g = curve_sample(pixel.g, table, domain, normalizer).g;
pixel.b = curve_sample(pixel.b, table, domain, normalizer).b;
float lum0 = dot(pixel.rgb, vec3(0.3, 0.59, 0.11));
float lum1 = curve_sample(lum0, table, domain, normalizer).a;
float lum1c = clamp(lum1, -8.0 * abs(lum0), 8.0 * abs(lum0));
float lum_scale = (lum0 == 0.0 ? 0.0 : lum1c / lum0);
float lum_offset = lum1 - lum1c;
pixel.rgb = lum_scale * pixel.rgb + lum_offset;
return pixel;
tableR != nil
+[PICurvesLUTFilter tableImageFromRed:green:blue:luminance:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Adjustments/PICurvesFilter.mm
tableG != nil
tableB != nil
tableL != nil
kernel vec4 gHDRtoPP(sampler image)
vec4 pix ;
vec3 pix2;
pix = sample(image, samplerCoord(image));
pix2 = pix.r * vec3(0.615429622407401,   0.114831839141528,   0.011544126697221) +
pix.g * vec3(0.367479646665836,   0.797943554457996,   0.064077744191180) +
pix.b * vec3(  0.016956659608091,   0.087783443422360,   0.924405601458102);
return vec4(pix2, pix.a);
kernel vec4 PPtogHDR(sampler image)
vec4 pix ;
vec3 pix2;
pix = sample(image, samplerCoord(image));
pix2 = pix.r * vec3(1.777445503202045,  -0.255296595099306,  -0.004500433755654) +
pix.g * vec3( -0.822224875430495,   1.380948853784730,  -0.085456231694984) +
pix.b * vec3(0.045475917061484,  -0.126454737973025,   1.089973874037625);
return vec4(pix2, pix.a);
kernel vec4 colorBalance(__sample image, float colorI, float colorQ, float str, vec2 gamma)
vec4 pix = image;
vec3 neg = min(pix.rgb, 0.0);
vec3 pos = max(pix.rgb, 1.0) - 1.0;
pix.rgb = pow(clamp(pix.rgb, 0.0, 1.0), vec3(gamma.x));
pix.rgb = pix.r * vec3(0.299,  0.595716,  0.211456) +
pix.g * vec3(0.587, -0.274453, -0.522591) +
pix.b * vec3(0.114, -0.321263,  0.311135);
vec4 orig = pix ;
float chroma = min(1.0, 2.0*sqrt(pix.g*pix.g + pix.b*pix.b) );
pix.gb += vec2(colorI,colorQ);
float strength = str*pow(chroma, .4) ;
pix.gb = mix(orig.gb, pix.gb, strength) ;
pix.rgb = pix.rrr +
pix.g * vec3(0.956296, -0.272122, -1.10699) +
pix.b * vec3(0.621024, -0.647381, 1.70461);
pix.rgb = pow(max(pix.rgb, 0.0), vec3(gamma.y));
pix.rgb += pos + neg;
return pix;
inputWarmTemp
inputWarmTint
inputHasFace
gHDRtoPP
PPtogHDR
colorBalance
-[PIColorBalanceFilter outputImage]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Adjustments/PIColorBalanceFilter.m
@"NSString"16@?0@"NSString"8
CIMix
CIGammaAdjust
CIExposureAdjust
CIProSharpenEdges
PILocalContrastHDR
CILocalLightMapPrepare
CIPhotoGrain
CISmartBlackAndWhite
CIVignetteEffect
PIFalseColorHDRDebug
PIColorBalanceFilter
PIRAWFaceBalance
PINeutralGrayWhiteBalanceFilter
PIBilateralFilter
PICurvesLUTFilter
PICurvesFilter
PISelectiveColorFilter
PILevelsFilter
PIGlobalSettings
IPXRootSettings
PXSettingsArchiveKey
editSettings
PURootSettings
photoEditingSettings
PI_SEGMENT_ROUND_TRIP_PROXY
PI_SEGMENT_DISABLE_SEGMENTATION
PI_SEGMENT_INFILL_ALGO
PI_SEGMENT_TINT_LAYERS
PI_SEGMENT_DISABLE_CACHE
PI_SEGMENT_PREVIEW_DISABLE_CLOCK
PI_SEGMENT_PREVIEW_HIGH_QUALITY
PI_SEGMENT_MANUAL_GATING_LENIENCE
PI_STYLE_RECIPE_DIR_PATH
PI_USE_STYLE_RECIPE_CONFIG
PI_PARALLAX_LAYOUT_CONFIG
PI_PARALLAX_DISABLE_UPGRADE
PI_CINEMATIC_ALLOW_YUV_SOURCE
PI_CINEMATIC_ALLOW_RGB10_PACKED
PI_AUTOLOOP_EXPORT_USE_METAL
B8@?0
-[PIAutoLoopExportJob initWithVideoExportRequest:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/AutoLoop/PIAutoLoopJob+Export.m
@"NUJSRenderNode"24@?0@"NUJSRenderNode"8@"JSValue"16
Invalid data type for adjustmentValue
Invalid data type for keyframes
Invalid data type for stabCropRect
input to VideoReframe cannot be nil
Unable to unwrap the input node!
-[PIJSRenderPipeline setUpContext:]_block_invoke
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Pipeline/PIPhotosPipeline.m
Missing duration for crossfade
input to VideoCrossfadeLoop cannot be nil
{CGRect={CGPoint=dd}{CGSize=dd}}64@?0{CGSize=dd}8{CGSize=dd}24{CGSize=dd}40@"JSValue"56
OvercaptureRectForAutoCrop
Couldn't find bundle for class %@
+[NUJSRenderPipeline(PhotosPipeline) newPhotosPipeline:]
PhotosPipeline
Couldn't find the specified Pipeline
JSContext
Class getJSContextClass(void)_block_invoke
JavaScriptCoreSoftLinking.h
Unable to find class %s
void *JavaScriptCoreLibrary(void)
-[PIApertureRedEyeAutoCalculator submit:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Autocalculators/PIApertureRedEyeAutoCalculator.m
PIApertureRedEyeAutoCalculator-faceDetection
pre-Adjustments
redEyeSpots
PILocalLightHDR-stats
NSNumber
kernel vec4 _shadowKernelHDR(__sample im, __sample adj, float str) { adj.r = 3.4*adj.r-1.2; vec3 neg = min(im.rgb, 0.0); vec3 pos = max(im.rgb, 1.0)-1.0; im.rgb = clamp(im.rgb, 0.0, 1.0); vec4 orig = im; float y = sqrt(dot(im.rgb, vec3(.33333))); float s = mix(0.0, adj.r, str); vec3 gain = s > 0.0 ? vec3(0.0) : vec3(s*s) * vec3(-2.75, -2.75, -2.5); im.rgb = im.rgb*im.rgb*gain + im.rgb*(1.0-gain); float m = 1.0 + 1.85*s*(max(0.1-y, 0.0)) ; im.rgb = (clamp(m*im.rgb, 0.0, 1.0)); float midAmt = s < 0.0 ? min(s*s,1.0) : 0.0; y = y*(1.0-y); im.rgb = sqrt(im.rgb); float pivot = .4; float a = midAmt*y; float b = -pivot*a; vec3 pix = im.r * vec3(0.299*a) + 
 im.g * vec3(0.587*a) + 
 im.b * vec3(0.114*a) + 
 im.rgb + vec3(b); im.rgb = mix(im.rgb, vec3(pivot), -y*midAmt); im.rgb = mix(im.rgb, pix, 0.8); im.rgb = max(im.rgb, 0.0); im.rgb *= im.rgb; im.rgb = clamp(im.rgb, 0.0,1.0)+pos+neg; return im; }
kernel vec4 _polyKernelHDR(__sample im, __sample adj, float str) { adj.r = 3.4*adj.r-1.2; vec3 neg = min(im.rgb, 0.0); vec3 pos = max(im.rgb, 1.0)-1.0; im.rgb = clamp(im.rgb, 0.0, 1.0); vec4 orig = im; float y = sqrt(dot(im.rgb, vec3(.33333))); float s = mix(0.0, adj.r, str); vec3 gain = s > 0.0 ? vec3(1.5*s) : vec3(1.75*s, 1.75*s, 1.55*s); im.rgb = im.rgb*im.rgb*gain + im.rgb*(1.0-gain); im.rgb = (clamp(im.rgb, 0.0, 1.0)); float midAmt = min(str, .5); y = y*(1.0-y); im.rgb = sqrt(im.rgb); float pivot = max(adj.g, 0.5); float a = midAmt*y; float b = -pivot*a; vec3 pix = im.r * vec3(0.299*a) + 
 im.g * vec3(0.587*a) + 
 im.b * vec3(0.114*a) + 
 im.rgb + vec3(b); 
 im.rgb = mix(im.rgb, vec3(pivot), -y*midAmt); im.rgb = mix(im.rgb, pix, 0.8); im.rgb = max(im.rgb, 0.0); im.rgb *= im.rgb; im.rgb = clamp(im.rgb, 0.0,1.0)+pos+neg; return im; }
-[PILocalLightFilterHDR outputImage]
PILocalLightHDR.m
CGRectEqualToRect([guideImage extent], [inputImage extent])
inputImage != nil
guideImage != nil
inputLocalLight != nil
CGRectEqualToRect([lightMapImage extent], [inputImage extent])
PILocalLightHDR
_lightMapImageFromData_block_invoke
x == 0
y == 0
width == lmWidth
height == lmHeight
CIEdgePreserveUpsampleRGFilter
-[PIParallaxAsset initWithFileURL:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Parallax/PIParallaxAsset.m
<%@: %p; fileURL = %@>
Asset is not local
proxy.jpg
Failed to load thumbnail image
Failed to load image properties
PIParallaxAsset.m
Unsupported
Failed to read image file
MediaAnalysis not available
v16@?0@"<NUMutableBuffer>"8
v24@?0@"<NUBufferTile>"8^B16
-[PISmartBlackAndWhiteAutoCalculator submit:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Autocalculators/PISmartBlackAndWhiteAutoCalculator.mm
PISmartBlackAndWhiteAutoCalculator-renderInputImage
Produced invalid BlackAndWhite settings, using defaults
void calculate_bw_auto_matrix_lum_contrast(__strong id<NUBuffer>, CGColorSpaceRef, MsgBlackAndWhiteSettings *)
num == w
void MsgMatrix<double>::AppendRow(const NumberType *, uint) [MatrixFloatType = double, NumberType = double]
xi < w && yi < h
MatrixFloatType &MsgMatrix<double>::operator()(size_t, size_t) [MatrixFloatType = double]
index < data.size()
inputScaleFactor
kernel vec4 _smartBlackAndWhiteHDR(__sample imageHDR, sampler2D hueImage, vec4 rgbWeights, vec4 normalizer) { float hueTableScaleFactor = rgbWeights.w; float hueImageWidth = normalizer.x; float huePixelCenter = normalizer.y; float neutralGamma = normalizer.z; float phototone = normalizer.w; float bw = dot(imageHDR.rgb / 12.0, rgbWeights.rgb); bw = clamp(bw, 0.0, 1.0); vec3 lms; lms.x = dot(imageHDR.rgb, vec3(0.3139902162, 0.6395129383, 0.0464975462)); lms.y = dot(imageHDR.rgb, vec3(0.155372406, 0.7578944616, 0.0867014186)); lms.z = dot(imageHDR.rgb, vec3(0.017752387, 0.109442094, 0.8725692246)); lms = pow(lms, vec3(0.43)); float i = dot(lms, vec3(0.4,0.4,0.2)); float p = dot(lms, vec3(4.4550,-4.8510,0.3960)); float t = dot(lms, vec3(0.8056,0.3572,-1.1628)); float chroma = sqrt(p*p+t*t); float hue = 0.5 + (atan(t, p) / 6.28318530718); vec2 huePt = vec2(hue * hueImageWidth + huePixelCenter, 0.5); float hueGamma = hueTableScaleFactor * texture2D(hueImage, huePt).a; float cd = 0.06 + 0.53 * abs(i-0.5); float lowSaturationDamp = smoothstep(0.0, 1.0, (chroma)/cd); float intensityDamp = smoothstep(0.0, 1.0, 1.0 - i); float lowLuminosityDamp = smoothstep(0.0, 1.0, 25.0 * i); float hWeight = lowSaturationDamp * intensityDamp * lowLuminosityDamp; hueGamma -= 1; hueGamma *= hWeight; hueGamma += 1; bw = pow(bw, hueGamma); float bwSDR = clamp(bw * 12.0, 0.0, 1.0); float midLumWeight = bwSDR*(1.0 - bwSDR); float grayWeight = 1.0 - smoothstep(0.0, 1.0, chroma * 10.0); float nWeight = midLumWeight * grayWeight; neutralGamma -= 1; neutralGamma *= nWeight; neutralGamma *= -2; neutralGamma += 1; bw = pow(bw, neutralGamma); bw = bw * 12.0; bw = clamp(bw, 0.0, 12.0); float df0 = 0.812379; float result; if (bw < df0) { result = 1.8031*bw*bw*bw - 2.1972*bw*bw + 1.3823*bw; } else { float scale = 12.0 - df0; float x = (bw - df0) / scale; result = 1.8031*x*x*x - 2.1972*x*x + 1.3823*x; result = result * scale + df0; result -= 0.158305860; } bw = mix(bw, result,-phototone); return vec4(bw,bw,bw,imageHDR.a); }
PIPhotoGrainHDR
inputISO
There is no need to call smartBlackAndWhiteStatistics.
Just use [CIFilter filterWithName:@"PISmartBlackAndWhiteHDR"] instead.
There is no need to call smartBlackAndWhiteAdjustmentsForValue:andStatistics:.
PICinematicVideoUtilities.m
Can't find global cinematic metadata in asset
Unexpected global rendering metadata class
-[_PIParallaxRenderCacheEntry initWithImage:format:colorSpace:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Parallax/PIParallaxLayerStackRequest.m
format != nil
space != nil
Invalid image extent
renderer != nil
-[_PIParallaxRenderCacheEntry render:error:]
layout != nil
-[_PIParallaxLayerStackScalePolicy initWithLayout:]
-[_PIParallaxLayerStackJob initWithRequest:]
-[_PIParallaxLayerStackJob effectiveLayout]
-[_PIParallaxLayerStackJob backfillScalePolicy]
-[_PIParallaxLayerStackJob prepare:]
Missing output image
MatteImage
Failed to render background layer
Failed to render foreground layer
buffer != nil
-[_PIParallaxLayerStackJob layerForBuffer:image:zPosition:identifier:]
identifier != nil
Failed to create pixel buffer
Failed to create renderDestination
-[_PIParallaxLayerStackJob cacheImage:key:format:colorSpace:]
key != nil
-[PIParallaxLayerStackRequest initWithSegmentationItem:]
-[PIParallaxLayerStackRequest initWithComposition:]
debugInput
debugMatte
debugMatteCrop
debugLocalConfidence
debugConfidenceMap
debugInfill
debugLayout
debugPreview
debugColorAnalysis
debugLayoutIntermediate_%d
-[PILevelsAutoCalculator submit:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Autocalculators/PILevelsAutoCalculator.m
PILevelsAutoCalculator-histogram
pre-Levels
-[PILevelsAutoCalculator calculateSettingsForImageHistogram:]
blackSrc
blackDst
shadowSrc
shadowDst
midSrc
midDst
hilightSrc
hilightDst
whiteSrc
whiteDst
Adjustment controller for key %@ is of class: %@, but was expected to be %@
-[PICompositionController(AdjustmentExtensions) _adjustmentControllerForKey:creatingIfNecessary:expectedClass:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Controllers/PICompositionController+AdjustmentExtensions.m
<%@:%p> [(%.3f, %.3f), %s]
editable
not editable
Warning smartToneStatistics will soon need [receiver properties] be non-nil so flash-fired state can be determined.
tonalRange
highKey
blackPoint
whitePoint
kernel vec4 _smarttone_brightness_neg_HDR (__sample c, float gamma) 
 vec3 neg = min(c.rgb, 0.0); 
 c.rgb = max(c.rgb, 0.0); 
 vec3 pix = pow(c.rgb, vec3(gamma)); 
 float lum = dot(c.rgb, vec3(0.39, .5, .11)); 
 vec3 pix2 = lum>0.0 ? c.rgb*pow(lum, gamma)/lum : vec3(0.0); 
 pix = mix(pix, pix2, 0.8) + neg; 
 return vec4(pix, c.a); 
kernel vec4 _smarttone_brightness_pos_HDR (__sample c, float gamma) 
 vec3 neg = min(c.rgb, 0.0); 
 vec3 pos = max(c.rgb, 1.0)-1.0; 
 c.rgb = clamp(c.rgb, 0.0, 1.0); 
 vec3 m = 1.0-c.rgb; 
 float a = 0.6; 
 vec4 result = c; 
 result.rgb = 1.0 - (pow(m, vec3(gamma))+a*( ((gamma-1.0)*m*(1.0-m*m))/(gamma*gamma))); 
 c.rgb = pow(c.rgb, vec3(1.0-((min(gamma, 2.95)-1.0)/2.6))); 
 result.rgb = mix(c.rgb, result.rgb, .85); 
 result.rgb = result.rgb+neg+pos; 
 return result; 
kernel vec4 _smarttone_contrast_HDR (__sample im, float midAmt) 
 vec3 neg = min(im.rgb, 0.0); 
 vec3 pos = max(im.rgb, 1.0)-1.0; 
 im.rgb = clamp(im.rgb, 0.0, 1.0); 
 float y = dot(im.rgb, vec3(0.3333)); 
 y = sqrt(y); 
 float sat = (im.r-y)*(im.r-y)+(im.g-y)*(im.g-y)+(im.b-y)*(im.b-y); 
 y = y*(1.0-y); 
 im.rgb = sqrt(im.rgb); 
 float a = midAmt*y; 
 float b = -0.5*a; 
 vec3 pix = im.r * vec3(0.299*a) + 
 im.g * vec3(0.587*a) + 
 im.b * vec3(0.114*a) + 
 im.rgb + vec3(b); 
 im.rgb = mix(im.rgb, vec3(0.5), -y*midAmt); 
 im.rgb = mix(im.rgb, pix, 0.8+sat); 
 im.rgb = max(im.rgb, 0.0); 
 im.rgb *= im.rgb; 
 im.rgb = im.rgb + neg + pos; 
 return im; 
kernel vec4 _smarttone_contrast_HDR (__sample im, float midAmt, float maxVal) 
 midAmt = midAmt / maxVal; 
 vec3 neg = min(im.rgb, 0.0); 
 vec3 pos = max(im.rgb, maxVal) - maxVal; 
 im.rgb = clamp(im.rgb, 0.0, maxVal); 
 float y = dot(im.rgb, vec3(0.3333)); 
 y = sqrt(y); 
 im.rgb = sqrt(im.rgb); 
 float sat = dot(im.rgb - vec3(y), im.rgb - vec3(y)) / sqrt(maxVal); 
 y = y*(sqrt(maxVal)-y); 
 float a = midAmt*y; 
 float b = -0.5*a; 
 vec3 pix = im.r * vec3(0.299*a) + 
 im.g * vec3(0.587*a) + 
 im.b * vec3(0.114*a) + 
 im.rgb + vec3(b); 
 im.rgb = mix(im.rgb, vec3(0.5), -a); 
 im.rgb = mix(im.rgb, pix, clamp(0.8+sat, 0.0, 1.0)); 
 im.rgb = max(im.rgb, 0.0); 
 im.rgb *= im.rgb; 
 im.rgb = im.rgb + neg + pos; 
 return im; 
kernel vec4 _smarttone_highlightcontrast_HDR (__sample pix, float highAmt, float sat) 
 float maxVal = 1.0; vec3 neg = min(pix.rgb, 0.0); 
 vec3 pos = max(pix.rgb, maxVal) - maxVal; 
 pix.rgb = clamp(pix.rgb, 0.0, maxVal); 
 float lum = clamp(dot(pix.rgb, vec3(.3333)),0.0,1.0); 
 vec3 high = pow(pix.rgb, vec3(3.0 - 2.0*highAmt)); 
 float pivot = 0.8; 
 vec3 pix1 = (high - pivot)*(4.0 - 3.0*highAmt) + pivot; 
 float h = highAmt*highAmt*highAmt*highAmt; 
 float a = (4.0 - 3.0*h); 
 vec3 pix2 = (lum-pivot)*a+pivot + high.rgb -lum; 
 high = mix(pix2, pix1, sat); 
 pix.rgb = mix(pix.rgb, high, lum*lum); 
 pix.rgb = pix.rgb + neg + pos; 
 return pix; 
kernel vec4 _rawHighlightsHDR(__sample pix, float gain) 
 vec3 high = gain*pix.rgb; 
 float lum = clamp(dot(pix.rgb, vec3(.3333)), 0.0, 1.0); 
 vec3 neg = min(high, 0.0); 
 high.rgb = mix(max(pix.rgb, 0.0), high.rgb, lum*lum) + neg; 
 return vec4(high, pix.a); 
CIHighlightShadowAdjust
inputShadowAmount
inputHighlightAmount
PISmartToneFilterHDR-histogram
componentAdd
componentMultiply
componentMin
componentMax
clear
destination
sourceOver
destinationOver
sourceIn
destinationIn
sourceOut
destinationOut
sourceAtop
destinationAtop
exclusiveOr
multiply
screen
overlay
darken
lighten
colorDodge
colorBurn
hardLight
softLight
difference
exclusion
saturation
luminosity
subtract
divide
linearBurn
linearDodge
vividLight
linearLight
pinLight
hardMix
darkerColor
lighterColor
*** Couldn't find blend kernel for blend mode '%@'
inputBlendMode
PI_LONG_EXPOSURE_FUSION_PARAMS
@"NSString"8@?0
kNCCBlurHalfSize
kNCCEdge0
kNCCEdge1
kBlendMaskThreshold0
kBlendMaskThreshold1
PI_LONG_EXPOSURE_DUMP_INTERMEDIATES
long-exp-input-image.tiff
long-exp-mask-image.tiff
long-exp-still-image.tiff
long-exp-guide-image.tiff
long-exp-ncc-map-image.tiff
long-exp-refined-mask-image.tiff
long-exp-fusion-image.tiff
-[PIModernPhotosPipeline _processedRenderNodeForComposition:input:pipelineState:error:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Pipeline/PIModernPhotosPipeline.m
pipelineState != nil
LongExposure
inputSushiLevel
kCGImageSourceShouldExtendRaw
inputGamutMapMax
inputNoiseReductionDetailAmount
inputUIColorNoiseReductionAmount
inputUILuminanceNoiseReductionAmount
sharpness
inputNoiseReductionSharpnessAmount
contrast
inputNoiseReductionContrastAmount
inputNeutralTemperature
inputNeutralTint
Video
mediaComponentType
hardCropCleanAperture
defaultFrameTime
skipOrientation
Master
unsupported sourceSelect adjustment
RAW/Linear
Source
ShowOriginalSource
PIForwardFakeBoost
inputBoost
inputVersion
inputParams
Intermediate
keepCacheWhenAtOneToOne
Disparity
Image
Source does not contain depth
PortraitEffectsMatte
HairSegmentationMatte
SkinSegmentationMatte
TeethSegmentationMatte
GlassesSegmentationMatte
MeteorPlusGainMap
focusRect
inputAperture
inputFocusRect
leftEyeX
leftEyeY
rightEyeX
rightEyeY
noseX
noseY
chinX
chinY
inputLeftEyePosition
inputRightEyePosition
inputFaceMidPoint
inputChinPosition
Missing required depth settings
inputShiftmapImage
inputCalibrationData
inputAuxDataMetadata
inputOriginalSize
CIDepthEffectMakeBlurMap
inputMatteImage
inputHairImage
inputGlassesImage
inputPower
inputEV
faceLandmarks
inputFaceLandmarkArray
inputDisparity
inputMatte
inputBlurMap
inputFaceMask
inputHairMask
inputTeethMask
CIPortraitEffectV2
inputGenerateSpillMatte
inputFullSizeImage
CIPortraitEffect%@
PortraitV2-zeroStrength
PortraitV2
pre-PortraitVideo
auxiliaryImageType
nativeScale
post-PortraitVideo
inputLumaNoiseScale
lumaNoiseScale
shape
inputShape
CIDepthEffectApplyBlurMap
inputGainMap
masterSpace
pre-WB
inputIsRaw
warmth
pre-Mute
pre-LivePhotoKeyFrame
pre-Trim
pre-SloMo
SloMo
inputConfidence
inputFaceBoxArray
CIDynamicFood
inputBoundingBoxArray
sunsetOrSunrise
CIDynamicRender
Unknown sceneLabel when rendering semantic enhance adjustment
inputTargetImage
inputTime
inputSaturation
pre-Curves
inputPointsR
inputPointsG
inputPointsB
inputPointsL
inputTableImage
green
blue
spread
hueShift
inputCorrections
CIPhotoEffect%@
filterVersion
pre-VideoReframe
pre-VideoStabilize
pre-VideoCrossfadeLoop
pre-Geometry
pre-Crop
perspectiveStraighten
resetCleanAperture
{?={?=qq}{?=qq}}
projectUsingCleanAperture
{?=qq}
projectUsingOriginalSize
com.apple.quicktime.live-photo.vitality-disabled
projectUsingEstimatedCleanAperture
pre-Orientation
post-Geometry
inputCenter
post-Adjustments
PIInverseFakeBoost
Master/RAW/Linear
inputCutoff
kernel vec4 levelsNewGammaForP3 (sampler src, sampler LUT)
vec4
p,r;
vec2
c1,c2;
float
p  = sample(src, samplerCoord(src));
vec3 neg = min(p.rgb, 0.0);
vec3 pos = max(p.rgb, 1.0)-1.0;
p.rgb = clamp(p.rgb, 0.0, 1.0);
f = p.r * 255.0 + 256.0;
c1 = vec2(floor(f)+0.5, 0.5);
c2 = vec2(ceil(f)+0.5, 0.5);
r = mix(sample(LUT,samplerTransform(LUT,c1)), sample(LUT,samplerTransform(LUT,c2)), fract(f));
p.r = r.r * 4.0 - 1.0;
f = p.g * 255.0 + 256.0;
c1 = vec2(floor(f)+0.5, 0.5);
c2 = vec2(ceil(f)+0.5, 0.5);
r = mix(sample(LUT,samplerTransform(LUT,c1)), sample(LUT,samplerTransform(LUT,c2)), fract(f));
p.g = r.g * 4.0 - 1.0;
f = p.b * 255.0 + 256.0;
c1 = vec2(floor(f)+0.5, 0.5);
c2 = vec2(ceil(f)+0.5, 0.5);
r = mix(sample(LUT,samplerTransform(LUT,c1)), sample(LUT,samplerTransform(LUT,c2)), fract(f));
p.b = r.b * 4.0 - 1.0;
p.rgb = max(p.rgb, 0.0);
p.rgb = p.rgb + pos + neg;
return p;
-[PILevelsFilter _LUTImage]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Adjustments/PILevelsFilter.m
com.apple.photos.PhotoImaging
segmentation
Unbalanced call to ensureResources detected! (%ld)
+[PISegmentationLoader ensureResources]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Parallax/PISegmentationLoader.m
Unbalanced call to freeResources detected! (%ld)
+[PISegmentationLoader freeResources]
-[PISegmentationLoader initWithParallaxAsset:]
PISegmentationItemLoader.state
PISegmentationItemLoader.loading
item != nil
-[PISegmentationLoader initWithSegmentationItem:parallaxAsset:]
v16@?0@"PIParallaxSegmentationItem"8
v24@?0@"PFParallaxAssetResource"8@"NSError"16
B24@?0@"NSString"8@"NSString"16
Missing composition
-[PISegmentationLoader _segment:completion:]
B32@?0@"<NUImageBuffer>"8@"<NUImageBuffer>"16Q24
v24@?0@"<NUImageBuffer>"8@"<NUImageBuffer>"16
composition != nil
-[PISegmentationLoader _performSegmentation:type:completion:]
-[PISegmentationLoader _analyzeColors:completion:]
Missing original layout
-[PISegmentationLoader _loadBackground:completion:]
-[PISegmentationLoader _loadRegions:completion:]
v32@?0@"NSArray"8@"NSArray"16@"NSError"24
-[PISegmentationLoader _performLayout:completion:]
item.composition != nil
-[PISegmentationLoader _loadLocalLightData:completion:]
resource != nil
+[PISegmentationLoader segmentationCompositionForAssetResource:]
proxyImage != NULL
+[PISegmentationLoader segmentationCompositionForProxyImage:orientation:]
imageURL != nil
+[PISegmentationLoader segmentationCompositionForImageURL:fileUTI:orientation:proxyImage:]
v16@?0@"PIOrientationAdjustmentController"8
+[PISegmentationLoader segmentationSourceForImageURL:fileUTI:orientation:proxyImage:]
currentInfo != nil
-[PISegmentationLoader _tryLoadSegmentationItemFromCache:]
B16@?0@"NSURL"8
+[PISegmentationLoader saveSegmentationItem:toURL:error:]
Invalid segmentation item: %@
+[PISegmentationLoader loadSegmentationItemFromURL:error:]
Segmentation item is incomplete
+[PISegmentationLoader saveSegmentationItem:layerStackOptions:configuration:style:layout:toWallpaperURL:completion:]
v24@?0@"PFParallaxLayerStack"8@"NSError"16
+[PISegmentationLoader generateLayerStackForItem:style:layout:options:completion:]
+[PISegmentationLoader saveSegmentationItem:layerStack:toWallpaperURL:error:]
Failed to create wallpaper directory
input.segmentation
Failed to export segmentation item
output.layerStack
Failed to export layer stack
+[PISegmentationLoader loadSegmentationItemFromWallpaperURL:error:]
Segmentation item from wallpaper is incomplete
Failed to load segmentation item from wallpaper
+[PISegmentationLoader loadLayerStackFromWallpaperURL:options:error:]
Failed to load layer stack from wallpaper
+[PISegmentationLoader renderPreviewLayerStackFromWallpaperURL:styleCategory:completion:]
+[PISegmentationLoader reloadSegmentationItemFromWallpaperURL:asset:completion:]
Null
Not available
<%@:%p %@>
-[_PIParallaxLayoutInactiveFrameJob initWithRequest:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Parallax/PIParallaxLayoutInactiveFrameRequest.m
-[_PIParallaxLayoutInactiveFrameJob prepare:]
-[_PIParallaxLayoutInactiveFrameJob render:]
-[_PIParallaxLayoutInactiveFrameJob complete:]
-[PIParallaxLayoutInactiveFrameRequest initWithComposition:]
segmentationItem != nil
-[PIParallaxLayoutInactiveFrameRequest initWithSegmentationItem:]
Mirror
Adjustment
SourceSelect~1.0
enum
values
Failed to register schema %@: %@
+[PISchema sourceSelectSchema]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Pipeline/PISchema.m
RAW~1.0
opaque
required
+[PISchema rawSchema]
RawNoiseReduction~1.0
bool
minimum
maximum
ui_minimum
ui_maximum
+[PISchema rawNoiseReductionSchema]
SmartTone~1.0
+[PISchema smartToneSchema]
SmartColor~1.0
+[PISchema smartColorSchema]
SmartBlackAndWhite~1.0
+[PISchema smartBlackAndWhiteSchema]
Grain~1.0
+[PISchema grainSchema]
Sharpen~1.0
+[PISchema sharpenSchema]
CropStraighten~1.0
+[PISchema cropSchema]
Trim~1.0
+[PISchema trimSchema]
SlowMotion~1.0
+[PISchema slomoSchema]
LivePhotoKeyFrame~1.0
+[PISchema livePhotoKeyFrameSchema]
Mute~1.0
+[PISchema muteSchema]
VideoPosterFrame~1.0
+[PISchema videoPosterFrameSchema]
AutoLoop~1.0
+[PISchema autoLoopSchema]
HighResolutionFusion~1.0
+[PISchema highResFusionSchema]
DepthEffect~1.0
+[PISchema depthEffectSchema]
Effect3D~1.0
+[PISchema effect3DSchema]
PortraitEffect~1.0
+[PISchema portraitEffectSchema]
+[PISchema effectSchema]
iPhone
+[PISchema redEyeSchema]
array
content
compound
properties
+[PISchema apertureRedEyeSchema]
+[PISchema retouchSchema]
+[PISchema vignetteSchema]
Orientation~1.0
+[PISchema orientationSchema]
+[PISchema definitionSchema]
+[PISchema noiseReductionSchema]
grayStrength
identity
+[PISchema whiteBalanceSchema]
+[PISchema levelsSchema]
+[PISchema curvesSchema]
+[PISchema selectiveColorSchema]
VideoReframe~1.0
+[PISchema videoReframeSchema]
VideoStabilize~1.0
pixel
gyro
+[PISchema videoStabilizeSchema]
VideoCrossfadeLoop~1.0
+[PISchema videoCrossfadeLoopSchema]
Debug
+[PISchema debugSchema]
SemanticEnhance~1.0
+[PISchema semanticEnhance]
PortraitVideo~1.0
+[PISchema portraitVideoSchema]
Composition
PhotosComposition
contents
com.apple.photo:Source~1.0
reference
Retouch~1.0
WhiteBalance~1.0
RedEye~1.0
ApertureRedEye~1.0
Definition~1.0
Effect~1.0
Vignette~1.0
NoiseReduction~1.0
Levels~1.0
Curves~1.0
SelectiveColor~1.0
Debug~1.0
+[PISchema photosCompositionSchema]
failed to register %@: %@
+[PISchema registerPhotosSchema]
com.apple.mobileslideshow.reframe.photo.eligible-pass
com.apple.mobileslideshow.reframe.photo.eligible-fail
com.apple.mobileslideshow.reframe.video.eligible-pass
com.apple.mobileslideshow.reframe.video.eligible-fail
com.apple.mobileslideshow.reframe.type.photo.horizon
com.apple.mobileslideshow.reframe.type.photo.perspective
PICompositionFinalizer-primaryImageProperties
PICompositionFinalizer-overcaptureImageProperties
v16@?0@"PIAdjustmentController"8
maxCameraAutoStraightenCorrection
Capture
maxCameraVerticalPerspectiveCorrection
maxCameraHorizontalPerspectiveCorrection
maxCameraRotatePerspectiveCorrection
minHorizontalCameraPerspectiveCorrection
minVerticalCameraPerspectiveCorrection
minRotateCameraPerspectiveCorrection
enable_perspective_correction
reframe
perpsective
horizon
PICropAutoCalculator-imageProperties
-[PICropAutoCalculator undoExifOrientation:error:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Autocalculators/PICropAutoCalculator.m
Source geometry has 0 size
-[PICropAutoCalculator submit:]
PICropAutoCalculator-faceDetection
{CGRect={CGPoint=dd}{CGSize=dd}}
maxAutoStraighten
filterOptions
CIAutoStraighten
straightenAngleInDegreesCCW
limitExceeded
belowMinimum
autoCrop
result
error
%@AutoCropEvaluation-txt.DBG
v16@?0@"PISourceSelectAdjustmentController"8
v16@?0@"PIAutoLoopAdjustmentController"8
PICropAutoCalculator-stitchedOvercaptureRect
+[PICropAutoCalculator(Utilities) updateCropAdjustment:after:error:]
PICropAutoCalculator-getBeforeGeometry
PICropAutoCalculator-getAfterGeometry
v16@?0@"PICropAdjustmentController"8
kernel vec4 iptLumHueSatTable(sampler image, __table sampler hueSatLumTable)
vec4 im = sample(image, samplerCoord(image)) ;
vec4 result = im;
float hueIdx = 359.0 * 0.5 * (atan(im.b, im.g)/3.1416 + 1.0);
hueIdx = clamp(hueIdx, 0.0, 359.0) + 0.5;
float hueChange = (sample(hueSatLumTable, samplerTransform(hueSatLumTable, vec2(hueIdx,0.5))).r);
float satChange = (sample(hueSatLumTable, samplerTransform(hueSatLumTable, vec2(hueIdx,0.5))).g);
float lumChange = (sample(hueSatLumTable, samplerTransform(hueSatLumTable, vec2(hueIdx,0.5))).b);
float chroma = sqrt(im.g*im.g+im.b*im.b) ;
chroma *= satChange ;
float hue = atan(im.b, im.g) + hueChange ;
vec3 adjustIm = im.rgb;
float hueAngle = hue  ;
lumChange = mix(1.0, lumChange, clamp(chroma,-0.7,0.7));
adjustIm.r *= lumChange;
adjustIm.g = chroma * cos(hueAngle) ;
adjustIm.b = chroma * sin(hueAngle) ;
result.rgb = adjustIm.rgb;
result.a = im.a ;
return result ;
kernel vec4 srgbToIPT(sampler image){
vec4 im = sample(image, samplerCoord(image));
vec3 lms, ipt;
lms = im.r * vec3(0.3139902162, 0.155372406, 0.017752387) +
im.g * vec3(0.6395129383, 0.7578944616, 0.109442094) +
im.b * vec3(0.0464975462, 0.0867014186, 0.8725692246);
lms = sign(lms)*pow(abs(lms), vec3(0.43, 0.43, 0.43));
ipt = lms.r * vec3(0.4, 4.455, 0.8056) +
lms.g * vec3(0.4, -4.851, 0.3572) +
lms.b * vec3(0.2, 0.3960, -1.1628);
return vec4(ipt, im.a);
kernel vec4 iptToSRGB(sampler image){
vec4 im = sample(image, samplerCoord(image));
vec3 lms, rgb;
lms = im.rrr +
im.g * vec3(0.09756893,-0.11387649,0.03261511) +
im.b * vec3(0.20522644, 0.13321716,  -0.67688718);
lms = sign(lms)*pow(abs(lms), vec3(1.0/.43));
rgb = lms.r * vec3( 5.472212058380287,  -1.125241895533569,   0.029801651173470) +
lms.g * vec3(-4.641960098354470, 2.293170938060623, -0.193180728257140) +
lms.b * vec3(0.169637076827974,  -0.167895202223709, 1.163647892783812);
return vec4(rgb, im.a);
kernel vec4 add_gaussian(sampler srcTable, float tableSize, float hueAmplitude, float satAmplitude, float lumAmplitude, float gaussX, float gaussSigmaSquared) {
vec2 d = destCoord();
vec4 src = sample(srcTable, samplerCoord(srcTable));
float x = d.x / (tableSize - 1.0);
float dist = min(min(abs(x - gaussX), abs(x - 1.0 - gaussX)), abs(x + 1.0 - gaussX));
float p = -((dist * dist) / (2.0 * gaussSigmaSquared));
float ep = exp(p);
float hue = hueAmplitude * ep;
float sat = satAmplitude * ep;
float lum = lumAmplitude * ep;
float h = clamp(src.r + hue, -1.0, 1.0);
float s = clamp(src.g + sat, -1.0, 1.0);
float l = clamp(src.b + lum, -1.0, 1.0);
return vec4(h,s,l,1.0);
srgbToIPT
iptToSRGB
add_gaussian
iptLumHueSatTable
PIVideoReframeNode.m
invalid crop rect
pipelineState
-[PIVideoReframeNode initWithSettings:inputs:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Render/PIVideoReframeNode.m
-[PIVideoReframeNode _evaluateVideoProperties:]
error != nil
-[PIVideoReframeNode _evaluateImageGeometry:]
Failed to get input geometry
Could not get the input geometry
Could not get the input image properties
CIPerspectiveTransform
inputTopLeft
inputTopRight
inputBottomLeft
inputBottomRight
showStabilizationWatermark
init is not a valid initializer
Invalid plist at path: %@
Time
Subjects
Type
HumanBody
HumanFace
CatBody
DogBody
-[PIVideoReframeMetadataExtractor overwriteTrackingMetadataWithPlist:]
PIVideoReframeMetadataExtractor.mm
type >= 0
Confidence
-[PIVideoReframeMetadataExtractor motionBlurVectorFromMetadata:]
-[PIVideoReframeMetadataExtractor extractMetadata]
mdataGroup.items.count == 1
0 && "unexpected metadata identifier"
0 && "unexpected metadata data type"
err == noErr
kernel vec4 convertFromRGBToYIQ(sampler src, float gamma)
vec4 pix ;
vec3 pix2;
pix = sample(src, samplerCoord(src));
pix.rgb = sign(pix.rgb)*pow(abs(pix.rgb), vec3(1.0/gamma)) ;
pix2.rgb = pix.r * vec3(0.299,  0.595716,  0.211456) +
pix.g * vec3(0.587, -0.274453, -0.522591) +
pix.b * vec3(0.114, -0.321263,  0.311135);
return vec4(pix2, pix.a);
kernel vec4 convertFromYIQToRGB(sampler src, float gamma)
vec4 color, pix;
pix = sample(src, samplerCoord(src));
color.rgb = pix.rrr +
pix.g * vec3(0.956296, -0.272122, -1.10699) +
pix.b * vec3(0.621024, -0.647381, 1.70461);
color.rgb = sign(color.rgb)*pow(abs(color.rgb), vec3(gamma, gamma, gamma) );
color.a = pix.a;
return color;
kernel vec4 whiteBalance(sampler image, float grayY, float grayI, float grayQ, float strength)
vec4 im = sample(image, samplerCoord(image)) ;
vec2 grayOffset = vec2(grayI, grayQ) ;
vec4 result ;
float newStrength = 1.0 + (strength-1.0)*(1.0-im.r) ;
result.r = im.r ;
result.gb = im.gb + newStrength*grayOffset ;
float damp = max(min(1.0, im.r/(grayY+0.00001)),0.0) ;
result.rgb = mix(im.rgb, result.rgb, damp) ;
result.a = im.a ;
return result ;
kernel vec4 gHDRtoPP(sampler image)
vec4 pix ;
vec3 pix2;
pix = sample(image, samplerCoord(image));
pix2 = pix.r * vec3(0.615429622407401,   0.114831839141528,   0.011544126697221) +
pix.g * vec3(0.367479646665836,   0.797943554457996,   0.064077744191180) +
pix.b * vec3(  0.016956659608091,   0.087783443422360,   0.924405601458102);
return vec4(pix2, pix.a);
kernel vec4 PPtogHDR(sampler image)
vec4 pix ;
vec3 pix2;
pix = sample(image, samplerCoord(image));
pix2 = pix.r * vec3(1.777445503202045,  -0.255296595099306,  -0.004500433755654) +
pix.g * vec3( -0.822224875430495,   1.380948853784730,  -0.085456231694984) +
pix.b * vec3(0.045475917061484,  -0.126454737973025,   1.089973874037625);
return vec4(pix2, pix.a);
convertFromRGBToYIQ
convertFromYIQToRGB
-[PINeutralGrayWhiteBalanceFilter outputImage]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Adjustments/PINeutralGrayWhiteBalanceFilter.m
PISmartToneAutoCalculator
-[PISmartToneAutoCalculator submit:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Autocalculators/PIAutoCalculators.mm
-[PISmartColorAutoCalculator submit:]
-[PIRedEyeAutoCalculator submit:]
PIRedEyeAutoCalculator-getLensInfo
locationX
locationY
touchDiameter
inputRect
MPSImageReduce unavailable
/masterSpace
kernel vec4 _blendGrainsHDR(__sample isoImages, float log10iso)
  vec4 c = isoImages; 
  float mix10_50    = mix(c.r, c.g, log10iso*1.43067655809 
                                           - 1.43067655809); 
  float mix50_400   = mix(c.g, c.b, log10iso*1.10730936496 
                                           - 1.88128539659); 
  float mix400_3200 = mix(c.b, c.a, log10iso*1.10730936496 
                                           - 2.88128539659); 
  float v = compare(log10iso - 1.69897000434,                     mix10_50,                     compare(log10iso - 2.60205999133,                             mix50_400,                             mix400_3200)); 
  return vec4(v,v,v,1.0);
kernel vec4 _grainBlendAndMixHDR(__sample img, __sample grainImage, float contrast, float mixAmount)
  vec3 rgb = img.rgb;
  float luminance = clamp(dot(rgb, vec3(.333333)), 0.0, 1.0); 
  float gamma = 4.01 - 2.0*luminance;
  rgb = sign(rgb) * pow(abs(rgb), vec3(1.0/gamma));
  float grain = grainImage.r - 0.5;
  float mult = contrast * grain;
  rgb += (max(luminance, 0.5) * mult * (1.0-luminance));
  rgb = sign(rgb) * pow(abs(rgb), vec3(gamma));
  rgb = min(rgb, 12.0);
  return mix(img, vec4(rgb,img.a), mixAmount);
kernel vec2 _paddedTile2(vec4 k) { return fract(destCoord() * k.zw) * k.xy + vec2(1.0); }
{CGRect={CGPoint=dd}{CGSize=dd}}44@?0i8{CGRect={CGPoint=dd}{CGSize=dd}}12
1536 x 1536 noise grain image prov
v56@?0^v8Q16Q24Q32Q40Q48
generateNoiseImage_block_invoke
PIPhotoGrainHDR.m
width==512*3
height==512*3
x==0
y==0
kernel vec4 _grainGenCombineHDR (__sample r, __sample g, __sample b, __sample a)
{ return vec4(r.x, g.x, b.x, a.x); }
CIUnsharpMask
Classification
LayoutConfiguration
LowResolution
DisableDownload
DisableSegmentation
DisableRendering
PetsRegions
Priority
PetsFaceRegions
Style
LayerStackOptions
OutOfProcess
WallpaperUpgradeMode
asset != nil
+[PISegmentation computeSegmentationScoresForAsset:options:completion:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Parallax/PISegmentation.m
options != nil
v24@?0@"<PISegmentationItem>"8@"NSError"16
+[PISegmentation loadSegmentationItemForAsset:options:completion:]
Invalid classification option: %@
+[PISegmentation cancelSegmentationForAsset:]
NO (timeout)
+[PISegmentation exportWallpaperForAsset:toURL:options:completion:]
wallpaperURL != nil
Segmentation failure
Invalid additionalLayerOptions: %@
+[PISegmentation _layerStackOptionsFromOptions:]
sourceURL != nil
+[PISegmentation upgradeWallpaperAtURL:exportToURL:options:completion:]
destinationURL != nil
v24@?0@"PFPosterEditConfiguration"8@"NSError"16
Not implemented yet
v16@?0@"NSError"8
Failed to load poster configuration from source URL
Failed to upgrade poster configuration from source URL
v24@?0@"PFPosterConfiguration"8@"NSError"16
PISegmentation.upgrade
Failed to upgrade some poster media
Failed to save poster configuration
v8@?0
+[PIImageIO writeImage:fileURL:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Util/PIImageIO.m
cgImage != NULL
+[PIImageIO writeCGImage:fileURL:]
fileURL != nil
Unhandled bit depth: %ld
+[PIImageIO writeCGImage:fileURL:options:]
Successfully wrote image to file %@
Failed to write image to file %@
%@-%d-%ld.tiff
GU %@ %@
Sensitivity
Bad float to fixed 16 conversion
+[PIApertureRedEyeProcessorKernel convertFloat:toFixed16:count:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Adjustments/ApertureRedEye/PIApertureRedEyeFilter.mm
Bad fixed 16 to float conversion
+[PIApertureRedEyeProcessorKernel convertFixed16:toFloat:count:]
+[PIApertureRedEyeProcessorKernel processWithInputs:arguments:output:error:]
PIApertureRedEyeFilter.mm
output.format == kCIFormatRGBAf
inputSensitivity
portraitStrength
capturedPortraitMajorVersion
capturedPortraitMinorVersion
portraitDisableStage
+[PIValuesAtCapture valuesAtCaptureFromImageProperties:error:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Autocalculators/PIPortraitAutoCalculator.m
Portrait was previously applied.
Failed to load depth data
Unfiltered depth data is not supported
Low quality depth data is not supported
Missing camera calibration data
Failed to load auxiliary data
Missing auxiliary metadata
Depth data version mismatch, asset has %@ but we can only handle %@
We only know up to sDOF version %d. Found: %d
<%@:%p aperture:%f minimumAperture:%@ maximumAperture:%@ portraitStrength:%f SDOFRenderingVersion:%lu portraitMajorVersion:%lu portraitMinorVersion:%lu>
-[PIPortraitAutoCalculator submit:]
PIPortraitAutoCalculator-getValuesAtCapture-imageProperties
capturedAperture
capturedPortraitStrength
v16@?0@"NUResponse"8
PIPortraitAutoCalculator-faceDetect
faceObservations != nil
+[PIPortraitAutoCalculator portraitInfoDictionaryFromFaceObservations:metadata:orientation:valuesAtCapture:]
metadata != nil
NUOrientationIsValid(orientation)
Insufficient number of landmark points
+[PIPortraitAutoCalculator depthEffectInfoDictionaryFromFaceObservations:focus:valuesAtCapture:lumaNoiseScale:orientation:]
minimumAperture
maximumAperture
+[PIPortraitAutoCalculator portraitEffectInfoDictionaryFromFaceObservations:orientation:valuesAtCapture:]
faceBoundingBox
faceJunkinessIndex
faceOrientationIndex
roll
allPoints
faceContour
leftEye
rightEye
leftEyebrow
rightEyebrow
nose
noseCrest
medianLine
outerLips
innerLips
leftPupil
rightPupil
+[PIPortraitAutoCalculator portraitInfoDictionaryFromCameraMetadata:]
-[PIDepthEffectApertureAutoCalculator submit:]
PIDepthEffectApertureAutoCalculator-getValuesAtCapture-imageProperties
depthData:DepthDataVersion
var tagNode = GetTag('/pre-Adjustments');
orientation = tagNode.geometry.orientation;
var node = Orient(tagNode, orientation);
return node;
@"NURenderNode"40@?0@"NURenderPipelineHelper"8@"NUComposition"16@"NURenderNode"24^@32
/pre-Adjustments
var source = GetTag('/pre-Adjustments');
ResetTagInput('/pre-Crop', source);
source = GetTag('/Crop');
orientation = source.geometry.orientation;
source = Orient(source, orientation);
return source;
/pre-Crop
/Crop
var sourceSettings = { 'skipOrientation': true };
var rawAdjustment = composition.raw
if (rawAdjustment) {
    sourceSettings['inputDecoderVersion'] = rawAdjustment.inputDecoderVersion
var image = Source(composition.source, sourceSettings)
var rawImage = GetTagInput('RAW/Linear')
if (rawImage) {
    image = rawImage
    image = Filter('PIForwardFakeBoost', {'inputImage' : image, 'inputBoost' : 1.0,})
var orientation = composition.orientation
if (orientation) { image = Orient(image, orientation.value) }
return image
Raw/Linear
var sourceSettings = { }; 
var raw = composition.raw; 
if (raw) { 
  sourceSettings['inputDecoderVersion'] = raw.inputDecoderVersion; 
  var sushiLevel = raw.inputSushiLevel;
  if (sushiLevel) {
    sourceSettings['kCGImageSourceShouldExtendRaw'] = sushiLevel;
} else { throw "expected RAW in rawSourceFilterIncludingOrientation"; }
return Source(composition.source, sourceSettings); 
expected RAW in rawSourceFilterIncludingOrientation
+[PIPipelineFilters rawSourceFilterIncludingOrientation]_block_invoke
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Pipeline/PIPipelineFilters.m
var sourceSettings = { 'skipOrientation' : true }; 
var raw = composition.raw; 
if (raw) { 
  sourceSettings['inputDecoderVersion'] = raw.inputDecoderVersion; 
  var sushiLevel = raw.inputSushiLevel;
  if (sushiLevel) {
    sourceSettings['kCGImageSourceShouldExtendRaw'] = sushiLevel;
  return Source(composition.source, sourceSettings); 
} else {
  return GetTag("/ShowOriginalSource");
var source = GetTag('/pre-RedEye');ResetTagInput('/post-RedEye', source);
return GetTag('/post-RedEye');
/pre-RedEye
/post-RedEye
Could not construct noRedEye filter from inline source
+[PIPipelineFilters noRedEyeFilter]
var output = input
var preTrim = GetTag('/pre-Trim');ResetTagInput('/SloMo', preTrim);
let slomo = composition.slomo
if (slomo) {
    var startTime = TimeMake(slomo.start, slomo.startScale)
    var endTime = TimeMake(slomo.end, slomo.endScale)
    output = SloMo(input, startTime, endTime, slomo.rate)
return output;
/pre-Trim
/SloMo
Could not construct noTrimFilter filter from inline source
+[PIPipelineFilters noTrimFilter]
var preMute = GetTag('pre-Mute');ResetTagInput('Mute', preMute);
return GetTag('Image');
Could not construct noMuteFilter filter from inline source
+[PIPipelineFilters noMuteFilter]
var preCrop = GetTag('/pre-Crop');ResetTagInput('/pre-Orientation', preCrop);
return input;
/pre-Orientation
Could not construct noCropFilter filter from inline source
+[PIPipelineFilters noCropFilter]
let preGeometry = GetTag('/pre-Geometry');ResetTagInput('/post-Adjustments', preGeometry);
return input;
/post-Adjustments
Could not construct noGeometry filter from inline source
+[PIPipelineFilters iosCropToolFilter]_block_invoke
var image = GetTag('pre-Trim');ResetTagInput('Trim', image);
image = GetTag('pre-SloMo');ResetTagInput('SloMo', image);
return input;
Could not construct stripAllTimeAdjustmentsFilter filter from inline source
+[PIPipelineFilters stripAllTimeAdjustmentsFilter]
let preGeometry = GetTag('/pre-Geometry');ResetTagInput('/post-Geometry', preGeometry);
return input;
+[PIPipelineFilters noGeometryFilter]
var preOrientation = GetTag('/pre-Orientation');ResetTagInput('/Orientation', preOrientation);
return input;
/Orientation
Could not construct noOrientationFilter filter from inline source
+[PIPipelineFilters noOrientationFilter]
return null;
Could not construct pipeline filter from source: %@
+[PIPipelineFilters orientationAsMetaDataFilter]
var preCrop = GetTag('/perspectiveStraighten');if (preCrop) {   ResetTagInput('/Crop', preCrop);
}return input;
/perspectiveStraighten
Could not construct perspectiveStraightenWithoutCropFilter filter from inline source
+[PIPipelineFilters perspectiveStraightenWithoutCropFilter]
var output = input;
var prePortraitVideoNode = GetTag('pre-PortraitVideo');
if (prePortraitVideoNode) {
    ResetTagInput('/post-PortraitVideo', prePortraitVideoNode);
return output;
/post-PortraitVideo
Could not construct histogramOptimizationFilter from inline source
+[PIPipelineFilters histogramOptimizationFilter]
var tagNode = GetTag('%@');
if (tagNode) {
    ResetTagInput('/pre-Geometry', tagNode);
return GetTag('/post-Geometry');
Could not construct stopAtTagIncludeGeometryFilter from inline source
+[PIPipelineFilters stopAtTagIncludeGeometryFilter:]
var tagNode = GetTag('%@');
if (tagNode) {
    ResetTagInput('/pre-Orientation', tagNode);
return GetTag('/Orientation');
Could not construct stopAtTagIncludeOrientationFilter from inline source
+[PIPipelineFilters stopAtTagIncludeOrientationFilter:]
var output = input;
var orientation = composition.orientation;
if (orientation) {
    output = Orient(input, orientation.value);
return output;
+[PIPipelineFilters applyOrientationFilter]
ResetTagInput('/AutoLoop/Output', GetTag('/AutoLoop/StabilizedVideo'));
return input;
/AutoLoop/Output
/AutoLoop/StabilizedVideo
Could not construct autoloopStabilizedVideoFilter filter from inline source
+[PIPipelineFilters autoloopStabilizedVideoFilter]
return Source(composition.sourceSpatialOvercapture, { 'skipOrientation' : true })
Could not construct overcaptureSourceFilter filter from inline source
+[PIPipelineFilters overcaptureSourceFilter]
return Source(composition.source, { 'skipOrientation' : true })
Could not construct primarySourceFilter filter from inline source
+[PIPipelineFilters primarySourceFilter]
return Source(composition.sourceSpatialOvercapture, {  })
+[PIPipelineFilters spatialOvercaptureVideoSourceFilter]
return input;
/PortraitV2-zeroStrength
/PortraitV2
Could not construct oneShotPortraitV2ExportFilter filter from inline source
+[PIPipelineFilters oneShotPortraitV2ExportFilter]
let output = input;
var image = GetTag('pre-VideoReframe');
if (image) {
    var videoReframe = composition.videoReframe;
    if (videoReframe && videoReframe.enabled) {
        image = VideoReframe(image, videoReframe);
        image = Filter(`CIColorMatrix`, {'inputImage' : image, 'inputBVector' : CIVectorMake(1,1,1,0)});
        ResetTagInput('VideoReframe', image);
    }
else { // image or live-photo { 
    var crop = composition.cropStraighten;
    var image = GetTag('pre-Crop');
    if (crop && crop.smart == 1 && image) {
        if (crop.pitch || crop.yaw) {
            var perspectiveTransform = PerspectiveTransformMake(crop.angle, crop.pitch, crop.yaw, image.geometry);
            image = Transform(image, perspectiveTransform);
        } else {
            var straightenTransform = StraightenTransformMake(crop.angle, image.geometry);
            image = Transform(image, straightenTransform);
        }
        // Apply the crop rect to the incoming image;
        image = Crop(image, crop.xOrigin, crop.yOrigin, crop.width, crop.height);
        image = Filter(`CIColorMatrix`, {'inputImage' : image, 'inputBVector' : CIVectorMake(1,1,1,0)});
        ResetTagInput('Crop', image);
    }
return output;
+[PIPipelineFilters(Debug) socPseudoColorFilter]_block_invoke
repairExtent != nil
+[PIRepairUtilities calcExtentsForStrokeRadius:offset:inputImageExtent:maskExtent:repairExtent:textureExtent:sourceExtent:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Adjustments/Retouch/PIRepair.mm
textureExtent != nil
sourceExtent != nil
Bad half float to float conversion
+[PIRepairUtilities extractRGBAfPixelsFromImage:fromROI:]
PIRepair expects the incoming image to be RGBAh, not %@
inputMaskImage
inputMaskBoundingBox
inputFaceBoundingBoxes
PIRepair-applyRepairMLStrokeToMutableBuffer
Bad fixed float to half float conversion
+[PIRepairUtilities applyRepairStrokeToMutableBuffer:brushStroke:sourceOffset:repairEdges:]
allocator<T>::allocate(size_t n) 'n' exceeds maximum supported size
error != NULL
-[NURenderPipelineHelper(PI) videoReframe:reframes:error:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Pipeline/PIPhotosPipelineHelper.m
-[NURenderPipelineHelper(PI) videoCrossfadeLoop:crossfadeAdjustment:error:]
-[NURenderPipelineHelper(PI) portraitVideo:disparityInput:disparityKeyframes:apertureKeyframes:debugMode:error:]
@"NSMutableArray"16@?0@"NSArray"8
-[NURenderPipelineHelper(PI) portraitVideoDebugDetectedObjects:source:cinematographyState:monochrome:error:]
Unexpected source type
extent
transform
AutoLoop/LongExposureMotion
PILongExposureFusion
inputStillImage
inputVideoScale
inputRenderScale
inputAlignmentExtent
inputAlignmentTransform
PIApertureRedEyeFilter
PIRedEye
inputDestinationImage
image != nil
+[PIHDRUtilities newHLGPixelBufferFromSDRImage:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Util/PIHDRUtilities.m
PIHDRInverseHLGFilter
Failed to start rendering %@ to: %@, error: %@
Succesfully rendered HLG buffer: %@
Failed to allocate pixel buffer
kernel vec4 hlg_luma_blending_inv(__sample sdr, float scale) 
  const vec3 lum_weights = vec3(0.2627, 0.6780, 0.0593); 
  float Ys = dot(lum_weights, sdr.rgb); 
  float Ymax = max(sdr.r, max(sdr.g, sdr.b)); 
  float Yb = 0.5*(Ys+Ymax); 
  const float gamma1 = 0.845906630893; 
  float absY = max(abs(Yb), 0.00001); 
  float gainInv = scale * pow(absY, 1.0/gamma1 - 1.0); 
  float3 hdr = gainInv * sdr.rgb; 
  return vec4(hdr.rgb, 1.0); 
recipe != nil
NSDictionary * _Nonnull PIAutoLoopRecipeForFlavor(NSDictionary *__strong _Nonnull, PIAutoLoopFlavor)
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/AutoLoop/PIAutoLoop.m
CGRect PIAutoLoopRecipeGetCropRect(NSDictionary *__strong _Nonnull)
origin_x
origin_y
BOOL PIAutoLoopRecipeHasGoodStabilization(NSDictionary *__strong _Nonnull)
frameTransform != nil
CMTime PIAutoLoopRecipeFrameTransformGetTime(NSDictionary *__strong)
frameTransform_rawTime
matrix_float3x3 PIAutoLoopRecipeFrameTransformGetHomography(NSDictionary *__strong)
frameTransform_homography
NSDictionary * _Nonnull PIAutoLoopRecipeGetInstructionAtTime(NSDictionary *__strong _Nonnull, CMTime)
CMTIME_IS_VALID(time)
loopRecipe_frameInstructions
loopFrameData_presTime
q24@?0@"NSDictionary"8@"NSDictionary"16
CMTime PIAutoLoopRecipeGetFrameDuration(NSDictionary *__strong _Nonnull)
NSDictionary * _Nullable PIAutoLoopRecipeGetFlavorParameters(NSDictionary *__strong _Nonnull, PIAutoLoopFlavor)
CMTimeRange PIAutoLoopRecipeGetTimeRangeForFlavor(NSDictionary *__strong _Nonnull, PIAutoLoopFlavor)
completion != nil
-[PICurvesAutoCalculator submit:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/workspaces/neutrino/PhotoImaging/Autocalculators/PICurvesAutoCalculator.m
-[PICurvesAutoCalculator computeCurvesForImageHistogram:]
softlink:r:path:/System/Library/Frameworks/JavaScriptCore.framework/JavaScriptCore
_PIParallaxInfillResult
PIParallaxInfillResult
NURenderResult
NSObject
PIParallaxInfillJob
PIParallaxInfillRequest
_PITapToTrackRenderResult
PITapToTrackResult
PITapToTrackRenderJob
PITapToTrackRequest
PIPhotoEffectHDR
PIPhotoEffectBlackAndWhiteHDR
PIPhotoEffectNoirHDR
PIPhotoEffectChromeHDR
PIPhotoEffectFadeHDR
PIPhotoEffectInstantHDR
PIPhotoEffectMonoHDR
PIPhotoEffectProcessHDR
PIPhotoEffectTonalHDR
PIPhotoEffectTransferHDR
PIPhotoEffectStageMonoHDR
PIPhotoEffect3DHDR
PIPhotoEffect3DBlackAndWhiteHDR
PIPhotoEffect3DVividHDR
PIPhotoEffect3DVividWarmHDR
PIPhotoEffect3DVividCoolHDR
PIPhotoEffect3DDramaticHDR
PIPhotoEffect3DDramaticCoolHDR
PIPhotoEffect3DDramaticWarmHDR
PIPhotoEffect3DSilverplateHDR
PIPhotoEffect3DNoirHDR
PIColorNormalizationFilter
PIReframeRules
PISemanticEnhanceAdjustmentController
_PIDisparitySampleResult
PIDisparitySampleResult
PIDisparitySampleJob
PIDisparitySampleRequest
PILocalContrastHDR
PIParallaxLayoutHelper
PISegmentationLayoutRegions
PFParallaxAssetRegions
PISegmentationLayout
_PIParallaxClockLayoutResult
PIParallaxClockLayoutResult
_PIParallaxClockLayoutJob
PIParallaxClockLayoutRequest
PIHighKey
PISegmentationInfillFilter
PISegmentationInwardFillProcessor
PIParallaxInwardFillKernel
PIParallaxColorPalette
PICompositionController
NSCopying
PICompositionSerializer
PICompositionSerializationResult
PICompositionSerializerMetadata
PILevelsFilterHDR
PIColorNormalizationAutoCalculator
NUTimeBased
PIFalseColorHDRDebug
PIAutoCalculatorUtils
PIAutoLoopExportRequest
PIAutoLoopExportClient
PIAutoLoopAnalysisRequest
PIAutoLoopAnalysisClient
PILongExposureRegistrationRequest
PIPortraitVideoProcessor
PIRGB10PortraitVideoProcessor
PIPortraitVideoFilter
PIAutoLoopAdjustmentController
PIVideoCrossfadeLoopNode
PILivePhotoKeyFrameAdjustmentController
PIVignetteAdjustmentController
PIAdjustmentController
PIParallaxFilter
_PIParallaxClockMaterialResult
PIParallaxClockMaterialResult
_PIParallaxClockMaterialJob
PIParallaxClockMaterialRequest
NUDigest
PIPortraitVideoRenderNode
PIFaceObservationCache
PIParallaxStyleRecipeRegistry
PIParallaxStyleBundleURLProvider
PIParallaxStyleURLProvider
PIParallaxStyleUserURLProvider
PISourceSelectAdjustmentController
PIParallaxStyle
PIParallaxOriginalStyle
PIParallaxStudioStyle
PIParallaxTonalityModeStyle
PIParallaxColorBGStandardStyle
PIParallaxColorParameterStyle
PIParallaxBlackWhiteStudioStyle
PIParallaxBlackWhiteMonoStyle
PIParallaxColorWashSingleStyle
PIParallaxColorWashDuotoneStyle
PIParallaxRecipeStyle
PIScalarKeyframe
PIDictionaryRepresentable
PIReframeSubject
NSSecureCoding
NSCoding
PIParallaxSegmentationItem
PISegmentationItem
PISegmentationContextInfo
PIPortraitVideoAdjustmentController
PIParallaxColorSuggester
PIReframeKeyframe
PIReframeKeyframeSequence
_PIParallaxColorAnalysisResult
PIParallaxColorAnalysisResult
_PIParallaxRenderResource
_PIParallaxColorAnalysisJob
PIParallaxColorAnalysisRequest
PIIPTHueChromaFilter
PIIPTHueChromaColorFilter
PIIPTHueChromaGrayFilter
PIParallaxColorAnalysis
PILongExposureFusionAutoCalculator
PIEffectAdjustmentController
PIAutoLoopAutoCalculator
PIMakerNoteUtilities
PITempTintFilter
PITimeVaryingPipelineStateSetting
_PIAutoLoopAnalysisResult
PIAutoLoopAnalysisResult
PIAutoLoopAnalysisJob
_PIVideoStabilizeFlowControl
ICFlowControl
_PIVideoStabilizeResult
PIVideoStabilizeResult
PIVideoStabilizeRenderJob
PIVideoStabilizeRequest
PIFaceBalanceAutoCalculator
GrayColorResult
RGBResult
PIWhiteBalanceAutoCalculator
_PIWhiteColorCalculator
PIDefinitionFilter
PISmartBlackAndWhiteAdjustmentController
PICompositionExporterDefaultMetadataConverter
PICompositionExporterMetadataConverter
PICompositionExportImagePrepareResult
PICompositionExportAuxiliaryResult
PICompositionExportResult
PICompositionExportDataResult
PICompositionExporterOptions
PICompositionExporterVideoOptions
PICompositionExporterImageOptions
PICompositionExporterAuxiliaryOptions
PICompositionExporter
GUBilateralConvolution
GUBWBilateralConvolution
PIBilateralFilter
PIPhotoEditAdjustmentsVersion
PINoiseReductionAdjustmentController
PrivateSmartColorHDR
PISmartColorFilterHDR
PIAutoEnhanceFilter
_PIParallaxCompoundLayerStackResult
PIParallaxLayerStackResult
PIParallaxCompoundLayerStackRequest
PIVideoCrossfadeLoopAdjustmentController
PISegmentationHelper
PIPortraitAdjustmentController
PIParallaxLegacyPosterStyle
PICompositionNoOpRemoval
PIOrientationAdjustmentController
PIParallaxStyleRecipeArchiver
PIPortraitVideoMetadataSample
PIPortraitVideoRenderer
PIDefinitionAdjustmentController
PIHighKeyHDR
PIPerspectiveAutoCalculator
PIFaceObservingAutoCalculator
PIRawNoiseReductionAdjustmentController
PICropAdjustmentController
PIRawAdjustmentController
PIParallaxRecipeFilter
PISmartToneAdjustmentController
PIAutoLoopKernels
PIPhotoEditHelper
PIAdjustmentConstants
PIFakeBoost
PIForwardFakeBoost
PIInverseFakeBoost
PICoreImageUtilities
PICompositionSerializerConstants
PICurvesFilterHDR
_PIParallaxLayoutResult
PIParallaxLayoutResult
_PIParallaxLayoutJob
PIParallaxLayoutRequest
PIParallaxLuminanceCalculator
PIParallaxStyleRecipe
PIParallaxStyleDefinition
PIParallaxStyleFilterDefinition
PIParallaxStyleFilterStackDefinition
PIParallaxStyleParameter
PIParallaxStyleNumberParameter
PIParallaxStyleColorParameter
PIParallaxStylePointParameter
PIParallaxStyleModeParameter
PIParallaxStyleBindingParameter
PIParallaxStyleEvaluationContext
PILongExposureAccumulator
_PILongExposureRegistrationResult
PILongExposureRegistrationResult
PILongExposureRegistrationJob
PIRAWTempTintSampler
PITagColorSampler
PIRedEye
PIVideoPosterFrameAdjustmentController
PICompositionSerializerFormatVersion
PICaptureDebugUtilities
PISlomoAdjustmentController
PISegmentationGatingRange
PISegmentationGatingRanges
PISegmentationGating
PIPortraitVideoDebugDetectionsRenderNode
PIColorWashFilter
PIColorWashDuoFilter
PIDebugAdjustmentController
PIGainMap
PIRAWFaceBalance
PICurvesLUTFilter
PICurvesFilter
PIColorBalanceFilter
PIPhotosPipelineHDRFilters
PIHighResFusionAdjustmentController
PIGlobalSettings
PIAutoLoopExportJob
PIJSRenderPipeline
PhotosPipeline
PIApertureRedEyeAutoCalculator
PrivateLocalLightHDR
PILocalLightMapPrepareHDR
PILocalLightFilterHDR
PIParallaxAsset
PFParallaxAsset
PFParallaxSegmentationResourceCaching
PIMsgImageBuffer
PISmartBlackAndWhiteAutoCalculator
PISmartBlackAndWhiteHDR
PrivateSmartBlackAndWhite
PITrimAdjustmentController
PIRedEyeAdjustmentController
PICinematicVideoUtilities
_PIParallaxRenderBuffer
NUImageBuffer
_PIParallaxRenderCacheEntry
_PIParallaxLayerStackResult
_PIParallaxLayerStackScalePolicy
NUScalePolicy
_PIParallaxLayerStackJob
_PIParallaxLayerStackRenderer
PIParallaxFilterCache
PIParallaxLayerStackRequest
_PIParallaxLayerStackDebugImageCollector
PILevelsAutoCalculator
PILevelsLuminanceAutoCalculator
PILevelsRGBAutoCalculator
AdjustmentExtensions
PICurveControlPoint
PrivateSmartToneHDR
PISmartToneFilterHDR
PICompositingFilter
PILongExposureFusion
PIModernPhotosPipeline
PILevelsFilter
PISegmentationLoader
PISegmentationLoading
_PISegmentationNullAsset
_PIParallaxLayoutInactiveFrameResult
PIParallaxLayoutInactiveFrameResult
_PIParallaxLayoutInactiveFrameJob
PIParallaxLayoutInactiveFrameRequest
PISchema
PICompositionFinalizer
PICompositionFinalizerResult
PIVideoStabilizeAdjustmentController
PICropAutoCalculator
Utilities
PISmartColorAdjustmentController
PISelectiveColorFilter
PIVideoReframeNode
PIVideoReframe
PIVideoReframeTimedMetadata
PIVideoReframeMetadataExtractor
PINeutralGrayWhiteBalanceFilter
PISmartToneAutoCalculator
PISmartColorAutoCalculator
PIRedEyeAutoCalculator
PIManualRedEyeAutoCalculator
PISegmentationCropFilter
PISegmentationMPSReduceProcessor
PISourceSampler
PIPhotoGrainHDR
PISegmentation
PIImageIO
PISharpenAdjustmentController
PIApertureRedEyeProcessorKernel
PIApertureRedEyeFilter
PIValuesAtCapture
PIPortraitAutoCalculator
PIDepthEffectApertureAutoCalculator
PIPipelineFilters
Debug
PIWhiteBalanceAdjustmentController
PIRepairUtilities
PIDepthAdjustmentController
PIVideoReframeAdjustmentController
PIHDRUtilities
PIHDRInverseHLGFilter
PICurvesAutoCalculator
PICurvesLuminanceAutoCalculator
PICurvesRGBAutoCalculator
initWithTargetPixelCount:
smartToneAdjustmentsForValue:localLightAutoValue:andStatistics:
appendPoints:pointCount:
dictionaryWithContentsOfFile:
setStorageMode:
dictionaryWithContentsOfURL:error:
imageByUnpremultiplyingAlpha
sensorID
initWithTargetPixelSize:
rawToneCurveProperties
smartToneProperties
isSubclassOfClass:
faceJunkinessIndex
valueAtIndex:
configuration
mipmapLevelCount
valueForKey:
getCropRectThatCompletelyContainsMasterImageForPitch:yaw:roll:
getMTLTextureFromPixelBuffer:device:
initWithTargetScale:effectiveScale:sampleMode:input:
dictionaryWithDictionary:
applyAttachmentsToCVPixelBuffer:
loadWithAsset:changesDictionary:completion:
mismatchError:object:
dictionaryWithLayout:
readBufferInRegion:block:
isUnifiedBracketingHDRCapture
initWithTargetSize:
initWithDomain:code:userInfo:
smoothWithFunction:windowSize:sampleMode:
configurationType
dictionaryWithObjects:forKeys:count:
missingError:object:
valueWithBytes:objCType:
cleanApertureOfTrack:oriented:
configurationWithDeviceName:
readBufferRegion:withBlock:
supportsANE
initWithError:
imageFileURL
initWithTargetedCVPixelBuffer:options:
isWarm
faceObservationsData
sortUsingComparator:
dictionaryWithObjectsAndKeys:
modalityAnalysisWithLimit:sampleMode:
getMinMaxSimulatedApertureFrom:minValue:maxValue:version:
readMetadataType:fromCGImageProperties:value:error:
rightEye
conformRange:inRange:
initWithExtent:renderScale:orientation:
items
valueWithCMTime:
rec709ColorSpace
faceOrientation
yellowColor
imageLayer:frame:zPosition:identifier:
getRectForPoint:colorBuffer:
surfaceStoragePool
roll
faceOrientationIndex
oneToOneScalePolicy
zoomStrategy
dictionaryWithStyle:
itur2100HLGColorSpace
sourceDefinition:
clipRect
conformsToType:
getResourceValue:forKey:error:
sourceDefinitions
initWithFilterName:settings:inputs:
setInputParameters:
constants
roundedRectangleGeneratorFilter
open
values
applyToRenderState:
initWithTrack:outputSettings:
setText:
pipelineStateForFunctionWithName:device:error:
facePositionAcceptable:imageAspect:
setDebugRendering:
jobNumber
initWithHue:tone:
openForReading:
setAlphaMode:
rowAverageFilter
setTexture:atIndex:
inputs
setProduceConfidenceMap:
jsContext
morphologyMaximumFilter
getTagWithPath:error:
applyWithExtent:arguments:
initWithIdentifier:
initWithURL:
openForWriting:
imageSize
rowBytes
setThreshold:
morphologyMinimumFilter
getValue:
sourceOutCompositingFilter
applyWithExtent:inputs:arguments:error:
containsIndex:
initWithURL:UTI:
insertObject:atIndex:
rect
setDestinationColor:
insertTimeRange:ofTrack:atTime:error:
setTileSize:
blackColor
localLightStatisticsNoProxy
disableIOSurfacePortaitExport
clockIntersectionFromTopRectMatteCoverage:bottomRectMatteCoverage:
containsObject:
ruleWithBlockPredicate:action:
applyWithExtent:roiCallback:arguments:
setProxyImage:
rectValue
PNGRepresentationOfImage:format:colorSpace:options:
instructions
setDestinationURL:
multiplyCompositingFilter
initWithVisibleRect:inactiveRect:zoomStrategy:overlapStrategy:parallaxStrategy:inactiveStrategy:
targetZoomFactorLimit
initWithImageProvider:width:height:format:colorSpace:options:
vectorWithCGPoint:
blackImage
ruleWithPredicate:action:
initWithImageSize:deviceResolution:parallaxPadding:visibleFrame:inactiveFrame:timeFrame:clockLayerOrder:clockIntersection:debugLayouts:
containsString:
debugLayouts
applyWithExtent:roiCallback:arguments:options:
mutableBytes
intValue
vectorWithCGRect:
setTimeRange:
initWithVisibleRect:inactiveRect:zoomStrategy:overlapStrategy:parallaxStrategy:inactiveStrategy:cropScore:layoutScore:timeBottomOverlap:timeTopOverlap:unsafeAreaOverlap:uninflatedUnsafeAreaOverlap:
setApplyOrientationAsMetadata:
ruleWithPredicate:assertingFact:grade:
contents
initWithImageSourceDefinition:videoSourceDefinition:
kernelWithFunctionName:fromMetalLibraryData:error:
mutableBytesAtPoint:
globalSession
imageWithBitmapData:bytesPerRow:size:format:colorSpace:
applyWithExtent:roiCallback:inputImage:arguments:
tempTintProperties
options
integerForKey:
ARGB8
vectorWithFloats:
ruleWithPredicate:retractingFact:grade:
kernelWithString:
initWithIndexesInRange:
integerValue
mutableCopy
imageWithBitmapData:bytesPerRow:size:format:options:
gradeForFact:
blendWithMaskFilter
pixelBufferToLumaChroma:pixelBuffer:outLuma:outChroma:read:write:
applyWithForeground:background:
setDisableIntermediateCaching:
longLongValue
sRGBColorSpace
context
setAssetIdentifier:
vectorWithX:
faces
clockOverlapAcceptableForLayoutConfiguration:
imageWithCGImage:
setTotalSensorCrop:
vectorWithX:Y:
discreteProgressWithTotalUnitCount:
pixelEffectiveAcceptable
blue
setRawSensorHeight:
initWithInput:
textImageGeneratorFilter
RG16
integralCropRect:
setInstructions:
grainInputSeedFromFrameTime
archiveURL
disparityBuffer
vectorWithX:Y:Z:
sRGBLinearColorSpace
close
kernelsWithString:
texture2DDescriptorWithPixelFormat:width:height:mipmapped:
contextForContext:
imageWithCGImage:options:
setRawSensorWidth:
blueColor
redEyeSpotsWithCorrectionInfo:
vectorWithX:Y:Z:W:
setTransferFunction:
name
setInteger:forKey:
salientObjects
close:
textureType
RGBA16
absoluteString
setAuxImages:
contextWithOptions:
sampleAtTime:
imageWithCVPixelBuffer:
RGBA8
internalComposition
areaAverageFilter
setReadNoise_1x:
threshold:
setAuxiliaryImageType:
initWithInput:scale:
imageWithCVPixelBuffer:options:
setType:
sampleCount
RGBAf
invalidError:object:
dispatchThreadgroups:threadsPerThreadgroup:
areaHistogramFilter
imageWithColor:
setReadNoise_8x:
orientedNode:withOrientation:
setIsDepthEnabled:
array
pointCount
RGBAh
originalCleanAperture
imageWithData:
region
displayP3ColorSpace
CGColorSpace
setIsPerspectiveZoomEnabled:
lowercaseString
inverseAggregatedCurveValueAt:
arrayByAddingObject:
networkAccessAllowed
failureError:object:
imageWithNUImageBuffer:
regionWithRect:
distanceToColor:
pointValue
popDebugGroup
luma
setRegionPolicy:
timeFrame
boolForKey:
arrayWithArray:
setUsage:
addBytes:length:
setLabel:
arrayWithCapacity:
initWithKind:parameters:colorSuggestions:
boolSettingForKey:defaultValue:
standardUserDefaults
setDouble:forKey:
addCompletedHandler:
timeIntervalSinceDate:
originalSize
invertedSet
inactiveFrame
green
boolValue
setUseEmbeddedPreview:
arrayWithObjects:count:
samplerWithImage:
timeIntervalSinceReferenceDate
RGBValues
colorBuffer
addDetectionAndStartTrackingRect:time:colorBuffer:disparityBuffer:
setUseRGBA:
luminanceNoiseAmplitude
videoAssetIsHighDynamicRange:
setBitRateMultiplicationFactor:
boostCurveValueAt:
fileExistsAtPath:
samplerWithImage:keysAndValues:
coolColor
setRenderContext:
setEditConfiguration:
addDetectionForNextFrameAt:colorBuffer:disparityBuffer:
setEnableLogging:
setValue:
initWithLayers:size:layout:depthEnabled:parallaxDisabled:clockAreaLuminance:
gridSizeForThreadGroupSize:imageSize:
timeOverlapCheckBottom
startReading
colorInvertFilter
copy
startReadingFrames:atTime:error:
fileType
registerRenderPipeline:forIdentifier:
setBool:forKey:
samplerWithImage:options:
newCIImageFromBufferImage:
setValue:forKey:
inactiveStrategy
newCommandQueue
boundingBox
setRenderSize:
decimalDigitCharacterSet
timeOverlapCheckThresholdForTopRect:isInteractive:
addEntriesFromDictionary:
initWithLength:
URLByAppendingPathComponent:
fileURLWithPath:
label
registerSchemas:error:
copyItemAtURL:toURL:error:
colorProperties
asset:
setRenderState:
decodeData:filename:error:
timeOverlapCheckTop
initWithLevel:
setError:
URLByAppendingPathComponent:isDirectory:
magentaColor
assetIdentifier
addIndex:
outputGeometry
mainBundle
timeRange
doubleForKey:
decodeDoubleForKey:
startTaskToRender:toDestination:error:
labels
initWithLuma:hue:chroma:
URLByAppendingPathExtension:
doubleValue
groupIdentifier
releaseCachedResources
mainDevice
timeRect
groupSizeForImageSize:pipelineState:
decodeIntegerForKey:
setRenderToData:
saveLayerStack:toURL:options:error:
outputImage:
CGColor
URLForDirectory:inDomain:appropriateForURL:create:error:
setEvaluatedForMode:
setVideoFrames:
videoFrames
setCanHandleAdjustmentData:
indexOfObject:
landmarks
decodeObjectForKey:
portraitLightingEffectStrength
buffer
copyPixelsFromImage:rect:destPtr:destPtrRowBytes:
setRenderWithIOSurface:
lastIndex
URLForResource:withExtension:
colorSpaceFromColorPrimaries:transferFunction:yccMatrix:
indexOfObject:inSortedRange:options:usingComparator:
colorSpaceFromVideoColorProperties:
addMutableTrackWithMediaType:preferredTrackID:
assetReaderWithAsset:error:
isAVAssetDolbyProfile5:error:
bufferFactory
copyPixelsFromImage:srcRect:destImage:destOrigin:
decodingSupportForAVAsset:
lastObject
URLWithString:
state
newPixelBufferOfSize:format:
addObject:
videoMetadataSamples
initWithMasterImageRect:stitchedImageRect:
indexOfObjectPassingTest:
decompressData:options:error:
handleFailureInFunction:file:lineNumber:description:
assetUUID
setExtentPolicy:
copyPixelsToImage:atPoint:fromBuffer:inRect:
filterWithName:
outputImageGeometry:
lastPathComponent
isAppleProRaw
UUID
buildNumber
initWithMasterImageSize:
addObjectsFromArray:
removeAllObjects
assetWithURL:
isAssetUnsupportedLegacyPortraitVideo:
bundleForClass:
handleFailureInMethod:object:file:lineNumber:description:
setRequiredSourceTrackIDs:
copyPixelsToImage:rect:srcPtr:srcPtrRowBytes:
defaultFocalLength
filterWithName:withInputParameters:
audioMix
editConfiguration
toDictionary
saveToArchiveURL:error:
UUIDString
indexesOfShallowDepthOfFieldObservations
setVisionRequests:
initWithMasterImageSize:stitchedImageSize:
newStorageWithSize:format:
colorThresholdFilter
defaultFormatForURL:
countByEnumeratingWithState:objects:count:
status
audioMixInputParametersWithTrack:
toRect
addOutput:
newTextureViewWithPixelFormat:textureType:levels:slices:
addRect:
latestVersion
removeItemAtURL:error:
setResolvedSourceDefinition:
createCGImage:fromRect:
outputSettings
scale
isCIFilterAvailable:propertyName:
finalize
setLowMemoryModeEnabled:
setVisionSegmentationPolicy:
newTextureWithDescriptor:
setVolume:atTime:
autoAdjustmentFiltersWithOptions:
videoProperties:
removeObject:
addRule:
bytesAtPoint:
defaultManager
initWithMetalDevice:options:
isCanceled
setFileType:
finalizeTrack
outputStreamWithURL:append:
createCGImage:fromRect:format:colorSpace:deferred:
setVolumeRampFromStartVolume:toEndVolume:timeRange:
removeObjectAtIndex:
bytesPerPixel
predicateWithFormat:
outputTimedMetadataSampleWithIdentifier:atTime:error:
createDirectoryAtURL:withIntermediateDirectories:attributes:error:
setResponseQueue:
stopAtTagFilter:
nextFrame
addRulesFromArray:
inflatePersonFaceRect:
isCool
layerStackByUpdatingClockAreaLuminance:
removeObjectForKey:
trackID
defaultValue
inputForKey:
outputVideo
firstEnabledVideoTrackInAsset:error:
setResultHandlerQueue:
setWithArray:
infoDictionary
removeObjectIdenticalTo:
bytesPerRow
setRevision:
layerStackByUpdatingDepthEnabled:
addTagWithName:inputNode:error:
initWithMutableBuffer:colorSpace:validRegion:
firstObject
initWithName:
trackIdentifier
createRGBA:
outputVideo:
hasPrefix:
effectiveAcceptableRectForClassification:havePetFaces:sourcePreferredCropRectNormalized:sourceAcceptableCropRectNormalized:sourceFaceAreaRectNormalized:
inputForPath:error:
setWithObject:
layerStackByUpdatingLayers:
matteImageBuffer
definition
setRollAngle:constrainCropRectWithTargetArea:
scaleMultiplyOfScalar:
stopReadingFrames
setMaskImage:
outputVideoComposition:
initWithName:responseQueue:
hasStaticTime
nextTimedMetadataGroup
createRenderStateWithQuality:
setWithObjects:
setFocalLenIn35mmFilm:
tracks
render:toBitmap:rowBytes:bounds:format:colorSpace:
scaleNode:scale:error:
tracksWithMediaType:
layerStackByUpdatingParallaxDisabled:
prepareForPerformingRequests:error:
storage
cacheNode:type:settings:error:
initWithNode:context:
setFocusDistance:
setYCbCrColorDepth:
effectivePreferredRectForClassification:havePetFaces:sourcePreferredCropRectNormalized:sourceAcceptableCropRectNormalized:sourceFaceAreaRectNormalized:
createSloMoWithInput:startTime:endTime:rate:error:
createYUV420:chroma:
isDepthDataFiltered
setYCbCrFullRange:
renderContext
setFontName:
setRollRadians:
straightenTransformWithAngle:extent:
enableHDRSupport
isDepthEnabled
setYCbCrMatrix:
renderImage:into:colorSpace:roi:imageSize:alpha:error:
setColorMatrix:
highKeyProperties
setColorPrimaries:
scaledExtent
setFontSize:
colorWithCGColor:
scaledSize
node
prepareNode
renderImage:into:colorSpace:roi:imageSize:error:
transformNodeWithInput:transform:error:
colorWithRGBValues:error:
depth
floatForKey:
initWithAffineTransform:
prepareNodeWithPipelineState:error:
floatValue
adjustmentData
stringByAppendingFormat:
scaledVector:
initWithAnalysisData:
cropNode:cropRect:cropSettings:
encodeData:filename:error:
colorWithRed:green:blue:
renderImage:rect:toDestination:atPoint:error:
nodeByReplayingAgainstCache:error:
auxiliaryCoreGraphicsInfoDictionary:
layoutByUpdatingClockIntersection:
encodeDouble:forKey:
trimInput:startTime:endTime:error:
initWithArchiveURL:
auxiliaryDataInfoMetadata
colorWithRed:green:blue:alpha:
setScale:
stringByAppendingPathComponent:
overlapStrategy
adjustmentFormat
histogram
stringByAppendingPathExtension:
auxiliaryImage
initWithArray:
nodeFromCache:cache:
layoutByUpdatingClockLayerOrder:
encodeInteger:forKey:
renderNode
focusDetection
setScaleFactor:
waitUntilCompletedAndReturnError:
schema
stringByAppendingString:
histogramCalculationColorSpace
colorWithRed:green:blue:alpha:colorSpace:
initWithAsset:
encodeObject:forKey:
layoutByUpdatingImageSize:
depthBlurEffectRenderingParameters
colorWithRed:green:blue:colorSpace:
auxiliaryImage:
typeWithFilenameExtension:
stringByDeletingPathExtension
layoutByUpdatingInactiveFrame:
renderNodeFromSource:settings:error:
cropScoreThresholdForClassification:
encodeRenderTo:withRenderRequest:
depthBlurEffectSimulatedAperture
auxiliaryImageFromComposition:type:mediaComponentType:error:
maxSDOFRenderingVersionSupported
setMedia:
warmColor
setFormat:
stringByReplacingOccurrencesOfString:withString:
schemaWithIdentifier:
typeWithIdentifier:
depthCameraCalibrationData
initWithAssetReaderTrackOutput:
auxiliaryImageType
layoutByUpdatingNormalizedVisibleFrame:
nominalFrameRate
unarchivedObjectOfClasses:fromData:error:
parallaxDisabled
focusRectangle
scoreAdjustmentWithUnscoredIntermediate:unsafeAreaOverlap:timeBottomOverlap:timeTopOverlap:
renderPipelineForIdentifier:
adjustmentValuesForKey:
isEqualToArray:
depthDataQuality
parallaxInactiveFrame
maximumApertureFocalRatio
commitAndNotifyOnQueue:withBlock:
nonLocalizedFailureReason
setCompression:
layoutByUpgradingToConfiguration:
maximumValue
setMetadata:
encodedPixelSizeOfTrack:oriented:
auxiliaryImagesProperties
endEncoding
stringForKey:
isEqualToDate:
setCompressionQuality:
initWithBase64EncodedString:options:
screenSize
layoutConfigurationFromDictionary:error:
adjustmentVersion
parallaxPadding
media
underlyingAVDepthData
currentContext
shapeWithRect:
aggregateStatistics:
callStackSymbols
isEqualToDictionary:
componentMax
initWithPipelineState:
parallaxPaddingPct
availableDecoderVersions
endGroupWithName:error:
initWithPixelBuffer:
stringSettingForKey:defaultValue:
sharedFactory
setGenericCompletionBlock:
renderSize
initWithBinCount:range:
componentMin
setComputePipelineState:
isEqualToLayoutConfiguration:
warmUp
componentsJoinedByString:
depthEnabled
isEqualToNumber:
cameraCalibrationData
unionWith:
currentDirectoryPath
normalizedPoints
mediaTypeForComposition:
stringWithFormat:
parallaxStrategy
initWithPrimaryColor:secondaryColor:
initWithBitmapData:width:height:bytesPerRow:format:
alignedRowBytesForWidth:
sharedRegistry
canAddOutput:
componentsSeparatedByString:
median
currentHandler
normalizedVisibleFrame
initWithBuffer:colorSpace:validRegion:
stringWithUTF8String:
allAssetsCanUseHDRPipeline
currentPlatform
layoutWithDictionary:
nose
enumerateIndexesUsingBlock:
setConstants:
strongToStrongObjectsMapTable
allDetections
propertyListWithData:options:format:error:
initWithCGColorSpace:
unknownError:object:
foregroundLayer
leftEye
allKeys
parallaxVisibleFrame
enumerateKeysAndObjectsUsingBlock:
renderingVersion
nu_digest
unsafeAreaInImageSpaceWithVisibleFrame:
metadata
currentSoftwareVersion
sharpnessWithIntensity:
propertyListWithStream:options:format:error:
initWithProperties:
length
shortValue
initWithCIImage:options:
whiteColor
canInflate
setConversionGain:
enumerateObjectsUsingBlock:
unsafeRect
setName:
allPoints
nu_waitUntilCompletedAndReturnError:
error
isEqualToPixelFormat:
whiteFactor
proxyImage
unsignedIntValue
initWithRadius:softness:opacity:clipRect:pressureMode:
initWithCIImage:orientation:
setCount:
allValues
formatDescriptions
setHistogramCalculationColorSpace:
setSegmentationType:
errorWithCode:reason:object:
initWithCVPixelBuffer:
allocWithZone:
isEqualToString:
metadataGroup
unsignedIntegerValue
inputNeutralXYFromRGB:
whiteValue
setNetworkAccessAllowed:
null
setCountLimit:
styleWithDictionary:error:
replaceObjectAtIndex:withObject:
errorWithCode:reason:object:underlyingError:
unsupportedError:object:
linearGrayColorSpace
metadataItem
isExportable
initWithRect:
initWithCVPixelBuffer:options:
initWithCapacity:
errorWithDomain:code:userInfo:
compressData:options:error:
numberWithBool:
pushDebugGroup:
metadataItemsFromArray:filteredByIdentifier:
initWithRed:green:blue:
subarrayWithRange:
numberWithDouble:
requestRevision
isFusedOvercapture
metadataItemsWithMetadataType:value:error:
setNormalizedVisibleFrame:
cancelAllRequests
setObject:forKey:
numberWithFloat:
deserializeFromDictionary:error:
backgroundLayer
deserializeMetadataWithType:fromGlobalMetadata:error:
setImageFileURL:
path
isHDR
initWithRed:green:blue:alpha:colorSpace:
frameNearestTime:
reset
evaluationMode
setDataExtractor:
numberWithInt:
setObject:forKey:cost:
metalCommandBuffer
livePhotoKeyFrameMetadataFromNode:time:error:
hwModelID
dataType
frameRect
pathExtension
isHDRComposition:
computeCommandEncoder
numberWithInteger:
resetAndEvaluateWithInitialState:
write:
metalDevice
dataValue
canceledError:object:
submitGeneric:
initWithRequest:dataExtractor:options:
exceptionWithName:reason:userInfo:
base64EncodedStringWithOptions:
initWithCommandQueue:
computeCropScoreForIntermediate:
numberWithLongLong:
writeBufferInRegion:block:
dataWithBytes:length:
setSize:
setImageProperties:
destinationData
percentile:
rangeOfCharacterFromSet:
dataWithBytesNoCopy:length:freeWhenDone:
submitGenericRequest:
numberWithUnsignedInteger:
freeAllResources:
resetTag:input:
setOptions:
isMetalDeviceSupported:
rangeOfString:options:
baseAddress
dataWithJSONObject:options:error:
capabilitiesForCurrentDevice
submitGenericSynchronous:
detectionType
loadFromArchiveURL:error:
imageBufferWithSize:format:fromPool:
metalTexture
initWithRequest:options:
isObject
setSourceColor:
setImageSize:
initWithResult:
numberWithUnsignedLong:
begin
resetTag:input:error:
dataWithLength:
objectAtIndex:
initWithComposition:dataExtractor:options:
fullSizeGeometry
deviceConfiguration
setSourceDisparity:
caseInsensitiveCompare:
imageByApplyingFilter:
dataWithPropertyList:format:options:error:
submitRequest:completion:
initWithScript:
meteorGainMapExposureCompensationMode
objectAtIndexedSubscript:
CGRectValue
computeHistogramFromBuffer:error:
rasterizeBrushStroke:atPoint:toBuffer:
imageByApplyingFilter:withInputParameters:
setSourceIdentifier:forTrackID:
beginGroupWithName:error:
deviceResolution
bestCropRectV2ForAspectRatio:sourcePixelWidth:sourcePixelHeight:sourceEssentialAreaRect:sourceSecondaryEssentialAreaRect:outputCropScore:
isPerspectiveZoomEnabled
setOutputSettings:
initWithScript:block:
objectForKey:
writeMetadataType:value:toCGImageProperties:error:
resolvedSourceDefinition
imageByApplyingGaussianBlurWithSigma:
isPlayable
useAsCIImageWithOptions:renderer:block:
cgColor
CIFormat
bestCropRectV2ForAspectRatio:withFocusRegion:sourcePixelWidth:sourcePixelHeight:sourcePreferredCropRectNormalized:sourceAcceptableCropRectNormalized:sourceFaceAreaRectNormalized:outputCropScore:
computeInactiveAvoidingRectForVisibleRect:acceptableFrame:unsafeRect:imageSize:newVisibleRect:
imageByApplyingTransform:
writePropertyList:toStream:format:options:error:
rawCameraSpaceProperties
performRequests:error:
bestCropRectV2ForParallaxClassification:layoutConfiguration:sourcePixelWidth:sourcePixelHeight:sourcePreferredCropRectNormalized:sourceAcceptableCropRectNormalized:sourceFaceAreaRectNormalized:outputCropScore:outputLayoutScore:outputClockOverlapAcceptable:
deviceSupportsHardware10BitHEVCEncoding
useAsCIRenderDestinationWithRenderer:block:
initWithContentsOfFile:
computeLayoutsWithHelper:
extractDataToDictionary:dataExtractor:options:context:colorSpace:error:
resourceUnavailableError:object:
objectFromData:withMajorVersion:minorVersion:
imageByApplyingTransform:highQualityDownsample:
loadLayerStackFromURL:options:error:
initWithContentsOfURL:options:error:
changesDictionaryTrimmedByTimeRange:
CVPixelFormat
BGRA8
bestLayout:
writeToTIFF:
deviceSupportsHighDynamicRangeVideo
submitWithProgress:completion:
isProxyOnly
responseQueue
initWithCount:times:values:
writeToURL:atomically:
isReadable
chroma
setSourceTrackIDForFrameTiming:
_shouldWaitForDependentJobs
initWithSize:format:
apertureFocalRatio
HLGOpticalScale
writeToURL:error:
minimumApertureFocalRatio
result:
objectsAtIndexes:
isReadableFileAtPath:
imageByClampingToExtent
dictionary
userInfo
subpath
debugDescriptionOfAssetTrack:
substringFromIndex:
perspectiveTransformWithPitch:yaw:roll:imageRect:
observations
genericConfiguration
smartColorAdjustmentsForValue:andStatistics:
imageByColorMatchingColorSpaceToWorkingSpace:
validRegion
inputStreamWithURL:
setPerservesAlpha:
initWithDescriptor:
initWithSize:format:rowBytes:bytes:
dictionaryForKey:
isRenderVersionSupported:
smartColorProperties
results
initWithDevice:
JSONObjectWithData:options:error:
ciImageTiled:closed:pressureMode:
imageByColorMatchingWorkingSpaceToColorSpace:
isSegmentedStyle:
dictionaryFromLayoutConfiguration:
initWithSize:format:rowBytes:mutableBytes:
rawProperties
genericRGBLinearColorSpace
initWithSource:
imageByCompositingOverImage:
validateComposition:error:
isSensorRawCapture
setStateObject:forKey:
confidenceMapBuffer
minimumValue
retractFact:
initWithSourceDefinitions:
appendFormat:
initWithDevice:version:colorSize:disparitySize:
imageByCroppingToRect:
returnStorage:
value
imageByPremultiplyingAlpha
setPipelineFilters:
isSourceAvailable:sourceSettings:
appendString:
isEqual:
class
self
performSelector:
performSelector:withObject:
performSelector:withObject:withObject:
isProxy
isKindOfClass:
isMemberOfClass:
conformsToProtocol:
respondsToSelector:
retain
release
autorelease
retainCount
zone
hash
superclass
description
debugDescription
TQ,R
T#,R
T@"NSString",R,C
statistics
T@"<NURenderStatistics>",R
infilledImage
T@"<NUImageBuffer>",R,N
setInfilledImage:
.cxx_destruct
_infilledImage
T@"<NUImageBuffer>",&,N,V_infilledImage
initWithRequest:
prepare:
initWithParallaxInfillRequest:
infillRequest
wantsRenderStage
wantsOutputImage
wantsOutputGeometry
scalePolicy
render:
complete:
result
cleanUp
matteImage
setMatteImage:
_infilledImageBuffer
_renderTask
_matteImage
T@"PIParallaxInfillRequest",R,N
T@"CIImage",&,N,V_matteImage
initWithComposition:
copyWithZone:
newRenderJob
mediaComponentType
submit:
segmentationMatte
setSegmentationMatte:
setScalePolicy:
pixelFormat
setPixelFormat:
colorSpace
setColorSpace:
shouldInfillForeground
setShouldInfillForeground:
_shouldInfillForeground
_segmentationMatte
_scalePolicy
_pixelFormat
_colorSpace
T@"<NUImageBuffer>",&,N,V_segmentationMatte
T@"<NUScalePolicy>",&,N,V_scalePolicy
T@"NUPixelFormat",&,N,V_pixelFormat
T@"NUColorSpace",&,N,V_colorSpace
TB,N,V_shouldInfillForeground
init
completedTrack
T@"PTCinematographyTrack",R,N
initWithCompletedTrack:
_completedTrack
T@"PTCinematographyTrack",R,N,V_completedTrack
wantsCompleteStage
wantsOutputVideo
_reportProgressAtTime:rect:confidence:
startTime
setStartTime:
normalizedImagePoint
setNormalizedImagePoint:
progressHandler
setProgressHandler:
setCompletedTrack:
clientRequestedStop
setClientRequestedStop:
_clientRequestedStop
_progressHandler
_normalizedImagePoint
_startTime
T{?=qiIq},N,V_startTime
T{CGPoint=dd},N,V_normalizedImagePoint
T@?,C,N,V_progressHandler
T@"PTCinematographyTrack",&,N,V_completedTrack
TB,V_clientRequestedStop
initWithComposition:startTime:pointToTrack:
kernel
photoEffectName
outputImage
inputImage
setInputImage:
_inputImage
T@"CIImage",&,V_inputImage
kernelBlackAndWhite
inputDepthMap
setInputDepthMap:
inputThreshold
setInputThreshold:
_inputThreshold
_inputDepthMap
T@"CIImage",&,V_inputDepthMap
Tf,V_inputThreshold
logger
isAnalysisAvailable
colorCubeForNormalization:dimension:targetColorSpace:
analysisAvailable
TB,R,N,GisAnalysisAvailable
outputNormalization
inputNormalization
setInputNormalization:
T@"CIImage",&,N,VinputImage
T@"PFStoryRecipeDisplayAssetNormalization",&,N,VinputNormalization
T@"PFStoryRecipeDisplayAssetNormalization",R,N
factCandidateForStill
factCandidateForVideo
factCandidateForReframe
factCandidateForPerspective
factCandidateForHorizon
sharedPregateRules
pregateRulesSystemWithConstants:
isCandidateForReframe
isCandidateForPerspective
isCandidateForHorizon
TB,R
isSettingEqual:forKey:
platedFoodSceneLabel
sunriseSunsetSceneLabel
genericLandscapeSceneLabel
intensityKey
sceneLabelKey
sceneConfidenceKey
faceBoundingBoxesKey
boundingBoxesKey
setIntensity:
intensity
scene
sceneConfidence
boundingBoxes
setScene:confidence:
setBoundingBoxesFromObservations:orientation:
setFaceBoundingBoxesFromObservations:orientation:
Td,N
Tq,R,N
Td,R,N
T@"NSArray",R,C,N
sampledDisparityValue
Tf,R,N
initWithDisparityValue:
_sampledDisparityValue
Tf,R,N,V_sampledDisparityValue
sampleTime
setSampleTime:
sampleRect
setSampleRect:
setSampledDisparityValue:
_sampleTime
_sampleRect
T{?=qiIq},N,V_sampleTime
T{CGRect={CGPoint=dd}{CGSize=dd}},N,V_sampleRect
Tf,N,V_sampledDisparityValue
initWithComposition:time:sampleRect:
customAttributes
_kernelLocalContrast
inputStrength
inputScale
initWithParallaxClassification:initialRect:imageSize:effectiveAcceptableRect:effectivePreferredRect:layoutConfiguration:
intermediateWithZoomStrategy:intermediate:
intermediateWithOverlapStrategy:intermediate:
initWithCIContext:matte:parallaxClassification:initialRect:imageSize:effectiveAcceptableRect:effectivePreferredRect:layoutConfiguration:
pixelBasedIntermediateWithOverlapStrategy:intermediate:translationY:
intermediateWithInactiveStrategy:intermediate:
scoreIntermediate:
_context
_matte
dictionaryFromRegions:
regionsFromDictionary:error:
acceptableCropRect
preferredCropRect
faceRegions
petRegions
T{CGRect={CGPoint=dd}{CGSize=dd}},R,N
T@"NSArray",R,N
initWithAcceptableRect:preferredRect:faces:pets:
_faceRegions
_petRegions
_acceptableCropRect
_preferredCropRect
T{CGRect={CGPoint=dd}{CGSize=dd}},R,N,V_acceptableCropRect
T{CGRect={CGPoint=dd}{CGSize=dd}},R,N,V_preferredCropRect
T@"NSArray",R,N,V_faceRegions
T@"NSArray",R,N,V_petRegions
generateLayoutForLayoutConfiguration:imageSize:regions:parallaxClassification:context:matte:layoutScore:cropScore:clockOverlapAcceptable:
clockLayerOrder
clockIntersection
T@"NSString",R,N
TQ,R,N
setClockLayerOrder:
setClockIntersection:
_clockLayerOrder
_clockIntersection
T@"NSString",&,N,V_clockLayerOrder
TQ,N,V_clockIntersection
initWithParallaxClockLayoutRequest:
clockLayoutRequest
layout
setLayout:
layoutConfiguration
setLayoutConfiguration:
_layout
_layoutConfiguration
T@"PIParallaxClockLayoutRequest",R,N
T@"PFParallaxLayout",&,N,V_layout
T@"<PFParallaxLayoutConfiguration>",&,N,V_layoutConfiguration
initWithSegmentationItem:
segmentationItem
_segmentationItem
T@"<PISegmentationItem>",R,N,V_segmentationItem
setInputStrength:
T@"NSNumber",&,N,VinputStrength
isMLInpaintingAvailable
blackInfillImage:matte:
fastInfillImage:matte:
watchInfillImage:matte:
inpaintingInfillImage:matte:
infillAlgorithm
setInfillAlgorithm:
inputMatteImage
setInputMatteImage:
_infillAlgorithm
_inputMatteImage
Tq,N,V_infillAlgorithm
T@"CIImage",&,N,V_inputImage
T@"CIImage",&,N,V_inputMatteImage
formatForInputAtIndex:
outputIsOpaque
synchronizeInputs
processWithInputs:arguments:output:error:
fillSourceTexture:intoDestinationTexture:withCommandBuffer:
encodeToCommandBuffer:destinationTexture:
sourceTexture
setSourceTexture:
_sourceTexture
T@"<MTLTexture>",&,N,V_sourceTexture
loadPaletteFromURL:error:
_paletteWithConfigurationDictionary:error:
_serializeColors:mode:
_loadColorsFromValues:mode:error:
colorBGPalette
colorWashSinglePalette
colorWashDuotonePalette
loadPaletteWithName:
customPalette
writeToURL:mode:error:
initWithColors:
initWithPrimaryColors:secondaryColors:
initWithPrimaryColors:secondaryColors:suggestionIndices:
suggestionForColor:
_lookupColor:withPredicate:
paletteColorForColor:
indexOfColor:
suggestionAtIndex:
primaryColors
secondaryColors
suggestionIndices
_primaryColors
_secondaryColors
_suggestionIndices
T@"NSArray",R,C,N,V_primaryColors
T@"NSArray",R,C,N,V_secondaryColors
T@"NSIndexSet",R,C,N,V_suggestionIndices
compositionController:didAddAdjustment:
compositionController:didRemoveAdjustment:
compositionController:didUpdateAdjustment:
compositionController:didUpdateAdjustments:
compositionController:adjustmentControllerClassForKey:
schemaForKey:
settingForAdjustmentKey:settingKey:
photosSchema
_keyToIdentifierMap
adjustmentControllerClassForKey:
composition
setChangeDelegate:
compositionKeys
adjustmentKeys
availableKeys
addAdjustmentWithKey:
replaceAdjustment:withKey:
removeAdjustmentWithKey:
adjustmentControllerForKey:
modifyAdjustmentWithKey:modificationBlock:
applyChangesFromCompositionController:
isEqual:visualChangesOnly:
isEqual:forKeys:visualChangesOnly:
isEqual:forKeys:comparisonBlock:
differingAdjustmentsWithComposition:
userOrientation
sourceSelection
setSourceSelection:
setOvercaptureSource:
overcaptureSource
_adjustmentControllerClassForKey:
setSource:mediaType:
source
setMediaType:
mediaType
changeDelegate
imageOrientation
setImageOrientation:
_composition
_delegateFlags
_identifierMap
_changeDelegate
_imageOrientation
T@"NUComposition",R,C,N
T@"<PICompositionControllerDelegate>",W,N,V_changeDelegate
Tq,N
Tq,N,V_imageOrientation
initialize
disableApertureEffects:
canInterpretDataWithFormatIdentifier:formatVersion:
deserializeCompositionFromData:formatIdentifier:formatVersion:error:
validateCompositionWithMissingSource:error:
_addDummySourceToCompositionIfNeeded:
deserializeCompositionFromAdjustments:metadata:formatIdentifier:formatVersion:error:
serializeComposition:versionInfo:error:
_serializeComposition:versionInfo:needsGeometry:error:
serializeComposition:versionInfo:serializerMetadata:error:
validateAdjustmentsEnvelope:error:
_validateValueTypesForKeys:requiredKeys:inDictionary:error:
serializeDictionary:error:
deserializeDictionaryFromData:error:
_sanitizeComposition:
adjustmentInformationForComposition:error:
adjustmentInformationForComposition:needsGeometry:error:
data
setData:
formatIdentifier
setFormatIdentifier:
formatVersion
setFormatVersion:
_data
_formatIdentifier
_formatVersion
T@"NSData",&,N,V_data
T@"NSString",&,N,V_formatIdentifier
T@"NSString",&,N,V_formatVersion
width
setWidth:
height
setHeight:
orientation
setOrientation:
_width
_height
_orientation
Tq,N,V_width
Tq,N,V_height
Tq,N,V_orientation
defaultValueForKey:
_customAttributesForKey:
P3KernelHDR
setDefaults
floatValueForKey:defaultValue:clearIfNotDefault:
_LUTImage
inputBlackSrcRGB
setInputBlackSrcRGB:
inputBlackDstRGB
setInputBlackDstRGB:
inputShadowSrcRGB
setInputShadowSrcRGB:
inputShadowDstRGB
setInputShadowDstRGB:
inputMidSrcRGB
setInputMidSrcRGB:
inputMidDstRGB
setInputMidDstRGB:
inputHilightSrcRGB
setInputHilightSrcRGB:
inputHilightDstRGB
setInputHilightDstRGB:
inputWhiteSrcRGB
setInputWhiteSrcRGB:
inputWhiteDstRGB
setInputWhiteDstRGB:
inputBlackSrcRed
setInputBlackSrcRed:
inputBlackDstRed
setInputBlackDstRed:
inputShadowSrcRed
setInputShadowSrcRed:
inputShadowDstRed
setInputShadowDstRed:
inputMidSrcRed
setInputMidSrcRed:
inputMidDstRed
setInputMidDstRed:
inputHilightSrcRed
setInputHilightSrcRed:
inputHilightDstRed
setInputHilightDstRed:
inputWhiteSrcRed
setInputWhiteSrcRed:
inputWhiteDstRed
setInputWhiteDstRed:
inputBlackSrcGreen
setInputBlackSrcGreen:
inputBlackDstGreen
setInputBlackDstGreen:
inputShadowSrcGreen
setInputShadowSrcGreen:
inputShadowDstGreen
setInputShadowDstGreen:
inputMidSrcGreen
setInputMidSrcGreen:
inputMidDstGreen
setInputMidDstGreen:
inputHilightSrcGreen
setInputHilightSrcGreen:
inputHilightDstGreen
setInputHilightDstGreen:
inputWhiteSrcGreen
setInputWhiteSrcGreen:
inputWhiteDstGreen
setInputWhiteDstGreen:
inputBlackSrcBlue
setInputBlackSrcBlue:
inputBlackDstBlue
setInputBlackDstBlue:
inputShadowSrcBlue
setInputShadowSrcBlue:
inputShadowDstBlue
setInputShadowDstBlue:
inputMidSrcBlue
setInputMidSrcBlue:
inputMidDstBlue
setInputMidDstBlue:
inputHilightSrcBlue
setInputHilightSrcBlue:
inputHilightDstBlue
setInputHilightDstBlue:
inputWhiteSrcBlue
setInputWhiteSrcBlue:
inputWhiteDstBlue
setInputWhiteDstBlue:
inputColorSpace
setInputColorSpace:
_inputBlackSrcRGB
_inputBlackDstRGB
_inputShadowSrcRGB
_inputShadowDstRGB
_inputMidSrcRGB
_inputMidDstRGB
_inputHilightSrcRGB
_inputHilightDstRGB
_inputWhiteSrcRGB
_inputWhiteDstRGB
_inputBlackSrcRed
_inputBlackDstRed
_inputShadowSrcRed
_inputShadowDstRed
_inputMidSrcRed
_inputMidDstRed
_inputHilightSrcRed
_inputHilightDstRed
_inputWhiteSrcRed
_inputWhiteDstRed
_inputBlackSrcGreen
_inputBlackDstGreen
_inputShadowSrcGreen
_inputShadowDstGreen
_inputMidSrcGreen
_inputMidDstGreen
_inputHilightSrcGreen
_inputHilightDstGreen
_inputWhiteSrcGreen
_inputWhiteDstGreen
_inputBlackSrcBlue
_inputBlackDstBlue
_inputShadowSrcBlue
_inputShadowDstBlue
_inputMidSrcBlue
_inputMidDstBlue
_inputHilightSrcBlue
_inputHilightDstBlue
_inputWhiteSrcBlue
_inputWhiteDstBlue
_inputColorSpace
T@"NSNumber",&,N,V_inputBlackSrcRGB
T@"NSNumber",&,N,V_inputBlackDstRGB
T@"NSNumber",&,N,V_inputShadowSrcRGB
T@"NSNumber",&,N,V_inputShadowDstRGB
T@"NSNumber",&,N,V_inputMidSrcRGB
T@"NSNumber",&,N,V_inputMidDstRGB
T@"NSNumber",&,N,V_inputHilightSrcRGB
T@"NSNumber",&,N,V_inputHilightDstRGB
T@"NSNumber",&,N,V_inputWhiteSrcRGB
T@"NSNumber",&,N,V_inputWhiteDstRGB
T@"NSNumber",&,N,V_inputBlackSrcRed
T@"NSNumber",&,N,V_inputBlackDstRed
T@"NSNumber",&,N,V_inputShadowSrcRed
T@"NSNumber",&,N,V_inputShadowDstRed
T@"NSNumber",&,N,V_inputMidSrcRed
T@"NSNumber",&,N,V_inputMidDstRed
T@"NSNumber",&,N,V_inputHilightSrcRed
T@"NSNumber",&,N,V_inputHilightDstRed
T@"NSNumber",&,N,V_inputWhiteSrcRed
T@"NSNumber",&,N,V_inputWhiteDstRed
T@"NSNumber",&,N,V_inputBlackSrcGreen
T@"NSNumber",&,N,V_inputBlackDstGreen
T@"NSNumber",&,N,V_inputShadowSrcGreen
T@"NSNumber",&,N,V_inputShadowDstGreen
T@"NSNumber",&,N,V_inputMidSrcGreen
T@"NSNumber",&,N,V_inputMidDstGreen
T@"NSNumber",&,N,V_inputHilightSrcGreen
T@"NSNumber",&,N,V_inputHilightDstGreen
T@"NSNumber",&,N,V_inputWhiteSrcGreen
T@"NSNumber",&,N,V_inputWhiteDstGreen
T@"NSNumber",&,N,V_inputBlackSrcBlue
T@"NSNumber",&,N,V_inputBlackDstBlue
T@"NSNumber",&,N,V_inputShadowSrcBlue
T@"NSNumber",&,N,V_inputShadowDstBlue
T@"NSNumber",&,N,V_inputMidSrcBlue
T@"NSNumber",&,N,V_inputMidDstBlue
T@"NSNumber",&,N,V_inputHilightSrcBlue
T@"NSNumber",&,N,V_inputHilightDstBlue
T@"NSNumber",&,N,V_inputWhiteSrcBlue
T@"NSNumber",&,N,V_inputWhiteDstBlue
T@"NSString",&,N,V_inputColorSpace
isAvailable
autoCalculatorWithImageData:orientation:
time
setTime:
T{?=qiIq},N
available
TB,R,N,GisAvailable
T{?=qiIq},N,Vtime
inputCutoff
setInputCutoff:
_inputCutoff
Td,V_inputCutoff
convertFacePoint:toImagePointWithFaceRect:orientation:
averagePoints:pointCount:
averageCGPoints:pointCount:
initWithComposition:destinationURL:
initWithComposition:stabilizedVideoURL:longExposureDestinationURL:maskDestinationURL:destinationUTI:
outputColorSpace
destinationUTI
destinationLongExposureURL
destinationMaskURL
_destinationUTI
_destinationLongExposureURL
_destinationMaskURL
T@"NSString",R,V_destinationUTI
T@"NSURL",R,V_destinationLongExposureURL
T@"NSURL",R,V_destinationMaskURL
T@"NUColorSpace",R
setCompletionBlock:
submitRequest:
flavor
setFlavor:
_flavor
Tq,N,V_flavor
recipe
setRecipe:
cleanAperture
setCleanAperture:
_recipe
_cleanAperture
T@"NSDictionary",C,N,V_recipe
T{?={?=qq}{?=qq}},N,V_cleanAperture
_configureRGBColorTexture:format:isHDR:
_updateRenderState:withLegacyCameraInfo:
outputFormat
allowPartialOutputRegion
allowCompressedInputsAndOutputs
roiForInput:arguments:outputRect:
applyWithInputImage:disparityImage:inputPixelBuffer:disparityPixelBuffer:globalMetadata:timedMetadata:aperture:focusedDisparity:quality:debugMode:isHDR:error:
inputDisparityImage
setInputDisparityImage:
inputColorPixelBuffer
setInputColorPixelBuffer:
inputDisparityPixelBuffer
setInputDisparityPixelBuffer:
inputRenderQuality
setInputRenderQuality:
inputRenderDebugMode
setInputRenderDebugMode:
inputIsHDR
setInputIsHDR:
inputGlobalRenderingMetadata
setInputGlobalRenderingMetadata:
inputTimedRenderingMetadata
setInputTimedRenderingMetadata:
inputAperture
setInputAperture:
inputFocusedDisparity
setInputFocusedDisparity:
_inputIsHDR
_inputDisparityImage
_inputColorPixelBuffer
_inputDisparityPixelBuffer
_inputRenderQuality
_inputRenderDebugMode
_inputGlobalRenderingMetadata
_inputTimedRenderingMetadata
_inputAperture
_inputFocusedDisparity
T@"CIImage",&,N,V_inputDisparityImage
T@"NUCVPixelBuffer",&,N,V_inputColorPixelBuffer
T@"NUCVPixelBuffer",&,N,V_inputDisparityPixelBuffer
T@"NSNumber",&,N,V_inputRenderQuality
T@"NSNumber",&,N,V_inputRenderDebugMode
TB,N,V_inputIsHDR
T@"PTGlobalRenderingMetadata",&,N,V_inputGlobalRenderingMetadata
T@"PIPortraitVideoMetadataSample",&,N,V_inputTimedRenderingMetadata
T@"NSNumber",&,N,V_inputAperture
T@"NSNumber",&,N,V_inputFocusedDisparity
flavorKey
recipeKey
stabilizedCropRect
pasteKeysForMediaType:
T@"NSDictionary",C,N
T@"NSString",C,N
initWithSettings:inputs:
resolvedNodeWithCachedInputs:settings:pipelineState:error:
nodeByReplayingAgainstCache:pipelineState:error:
initWithInput:timeRange:crossfadeDuration:startTime:
input
shouldCacheNodeForPipelineState:
_evaluateVideo:
requiresVideoComposition
_evaluateVideoComposition:
requiresAudioMix
_evaluateAudioMix:
loopTimeRange
crossfadeDuration
_crossfadeDuration
_loopTimeRange
T{?=qiIq},R,N,V_startTime
T{?={?=qiIq}{?=qiIq}},R,N,V_loopTimeRange
T{?=qiIq},R,N,V_crossfadeDuration
initWithAdjustment:
timeKey
scaleKey
keyFrameTime
setKeyFrameTime:
radiusKey
falloffKey
radius
setRadius:
falloff
setFalloff:
setValue:forUndefinedKey:
autoKey
enabledKey
displayName
displayInputKeys
inputKeys
settingForKey:
hasInputKey:
enabled
setEnabled:
canBeEnabled
canHaveAuto
hasAutoKeyInSchema
isAuto
setIsAuto:
objectForKeyedSubscript:
setObject:forKeyedSubscript:
valueForUndefinedKey:
valuesForArrayInputKey:
setFromAdjustment:
interpolateFromStart:toEnd:progress:
timeFromInputKey:timescaleKey:
visualInputKeys
isEqualToAdjustmentController:
isEqual:forKeys:
_setPrimitiveValue:forKey:
_primitiveValueForKey:
settings
_isDefault
pasteAdjustment:forMediaType:
autoKeysForPaste
identifier
setIdentifier:
adjustment
_changes
_identifier
_adjustment
T@"NSDictionary",R,N
TB,N
T@"NUIdentifier",&,N,V_identifier
T@"NUAdjustment",R,N,V_adjustment
TB,R,N
outputForegroundImage
outputBackgroundImage
outputMatteImage
visibleFrame
setVisibleFrame:
renderScale
setRenderScale:
localLightData
setLocalLightData:
cache
setCache:
inputForegroundImage
setInputForegroundImage:
inputBackgroundImage
setInputBackgroundImage:
inputGuideImage
setInputGuideImage:
_renderScale
_localLightData
_cache
_inputForegroundImage
_inputBackgroundImage
_inputGuideImage
_visibleFrame
T{CGRect={CGPoint=dd}{CGSize=dd}},N,V_visibleFrame
Td,N,V_renderScale
T@"NSDictionary",C,N,V_localLightData
T@"<PIParallaxFilterCache>",&,N,V_cache
T@"CIImage",&,N,V_inputForegroundImage
T@"CIImage",&,N,V_inputBackgroundImage
T@"CIImage",&,N,V_inputGuideImage
T@"CIImage",R,N
clockAreaLuminance
initWithClockAreaLuminance:
_clockAreaLuminance
Td,R,N,V_clockAreaLuminance
clockMaterialRequest
luminanceValue
setLuminanceValue:
_luminanceValue
T@"NSNumber",&,N,V_luminanceValue
initWithLayerStack:
layerStack
setLayerStack:
_layerStack
T@"PFParallaxLayerStack",&,N,V_layerStack
nu_updateDigest:
CIImageProcessorDigestObject
initWithInput:disparityInput:disparityKeyframes:apertureKeyframes:debugMode:
disparityKeyframes
apertureKeyframes
debugMode
uniqueInputNode
_targetScaleForScale:
_prewarmPortraitRendererWithPipelineState:error:
renderTime
renderQuality
globalMetadata
timedMetadata
sourceTransferFunction
useSourceBuffersDirectly
_portraitQualityForRenderScale:
_sourceBufferFromInput:error:
_evaluateImage:
T{?=qiIq},R,N
Ti,R,N
T@"PTGlobalRenderingMetadata",R,N
T@"PIPortraitVideoMetadataSample",R,N
faceRequestWithRequest:
submit:response:
submitSynchronous:error:
_group
_queue
_result
supportedIdentifiers
recipeForIdentifier:
_recipesForIdentifiers:withURLProvider:
urlForIdentifier:
_bundle
initWithBaseURL:
baseURL
setBaseURL:
_baseURL
T@"NSURL",&,N,V_baseURL
sourceSelectionForString:
stringForSourceSelection:
sourceSelectionKey
styleWithColorAnalysis:
_filterForRecipeIdentifier:
defaultStyleForKind:colorAnalysis:
styleWithParameters:colorSuggestions:
styleWithBakedStyle:
colorPaletteWithStyleKind:
initWithClockColor:colorSuggestions:
bakedStyle
parameters
kind
recipeIdentifier
inactiveRecipeIdentifier
clockFont
updateClockPropertiesWithClockAreaLuminance:
isSegmented
hasTonalityMode
hasColorParameter
filter
inactiveFilter
colorSuggestionForCategory:
configureForCategory:
defaultDominantColorWithAnalysis:
clockColor
setClockColor:
colorSuggestions
setColorSuggestions:
_colorSuggestions
_clockColor
T@"NSArray",&,N,V_colorSuggestions
T@"NSString",R,C,N
T@"PIParallaxStyleRecipe",R,N
T@"PFParallaxColor",&,N,V_clockColor
tonality
setTonality:
Tq,N,Vtonality
lowKeyTone
neutralTone
highKeyTone
color
setColor:
T@"PFParallaxColor",&,N
Td,R
initWithBackgroundColor:clockColor:colorSuggestions:
_color
T@"PFParallaxColor",&,N,V_color
initWithColor:clockColor:suggestions:
initWithPrimaryColor:secondaryColor:clockColor:colorSuggestions:
primaryColor
setPrimaryColor:
secondaryColor
setSecondaryColor:
_primaryColor
_secondaryColor
T@"PFParallaxColor",&,N,V_primaryColor
T@"PFParallaxColor",&,N,V_secondaryColor
initWithIdentifier:recipe:
keyframeInArray:closestToTime:
initWithDictionaryRepresentation:
dictionaryRepresentation
initWithTime:value:
_value
_time
stringValue
supportsSecureCoding
encodeWithCoder:
initWithCoder:
initWithType:source:identifier:confidence:
setBounds:
isHuman
isAnimal
type
confidence
bounds
expandedBounds
setExpandedBounds:
edgeBleed
setEdgeBleed:
_type
_confidence
_source
_edgeBleed
_bounds
_expandedBounds
Tq,R,N,V_type
Tq,R,N,V_identifier
Td,R,N,V_confidence
Tq,R,N,V_source
T{CGRect={CGPoint=dd}{CGSize=dd}},N,V_bounds
T{CGRect={CGPoint=dd}{CGSize=dd}},N,V_expandedBounds
Tq,N,V_edgeBleed
writeImageBuffer:toURL:error:
readImageBufferFromURL:error:
dataForImageBuffer:error:
imageBufferFromData:error:
availableStyleOfKind:
suggestedStyleForCategory:
defaultStyleOfKind:
classification
segmentationConfidenceMap
segmentationBackground
regions
originalLayout
scores
colorAnalysis
availableStyles
defaultStyles
originalStyle
fileURL
segmentationDataURL
version
supportsManualClockIntersectionTolerance
T@"NUComposition",R,N
T@"<PFParallaxAssetRegions>",R,N
T@"PFParallaxLayout",R,N
T@"<PFParallaxLayoutConfiguration>",R,N
T@"NSDictionary",R,C,N
T@"PIParallaxColorAnalysis",R,N
T@"PFParallaxLayerStyle",R,N
T@"NSURL",R,N
isComplete
_populateAvailableStyles
_populateDefaultStyles
saveToURL:error:
loadFromURL:error:
saveAssetResourceToURL:error:
saveSegmentationDataToURL:error:
loadSegmentationDataFromURL:error:
contentsDictionary
loadContentsFromDictionary:hasMatte:hasBackground:error:
resource
setResource:
setComposition:
setClassification:
setSegmentationConfidenceMap:
setSegmentationBackground:
setRegions:
defaultLayout
setDefaultLayout:
setOriginalLayout:
setScores:
setColorAnalysis:
loadingState
setLoadingState:
_availableStyles
set_availableStyles:
_defaultStyles
set_defaultStyles:
setFileURL:
setSegmentationDataURL:
contextInfo
setContextInfo:
_resource
_classification
_segmentationConfidenceMap
_segmentationBackground
_regions
_defaultLayout
_originalLayout
_scores
_colorAnalysis
_loadingState
__availableStyles
__defaultStyles
_fileURL
_segmentationDataURL
_contextInfo
T@"NSArray",C,N,V__availableStyles
T@"NSArray",C,N,V__defaultStyles
T@"NSURL",&,N,V_fileURL
T@"NSURL",&,N,V_segmentationDataURL
T@"PISegmentationContextInfo",&,N,V_contextInfo
T@"PFParallaxAssetResource",&,N,V_resource
T@"NUComposition",&,N,V_composition
TQ,N,V_classification
T@"<NUImageBuffer>",&,N,V_segmentationConfidenceMap
T@"<NUImageBuffer>",&,N,V_segmentationBackground
T@"<PFParallaxAssetRegions>",&,N,V_regions
T@"PFParallaxLayout",&,N,V_defaultLayout
T@"PFParallaxLayout",&,N,V_originalLayout
T@"NSDictionary",C,N,V_scores
T@"PIParallaxColorAnalysis",&,N,V_colorAnalysis
TQ,N,V_loadingState
currentContextInfo
systemName
setSystemName:
systemVersion
setSystemVersion:
systemBuildVersion
setSystemBuildVersion:
setVersion:
sourceMode
setSourceMode:
segmentationDisabled
setSegmentationDisabled:
_segmentationDisabled
_systemName
_systemVersion
_systemBuildVersion
_version
_sourceMode
T@"NSString",C,N,V_systemName
T@"NSString",C,N,V_systemVersion
T@"NSString",C,N,V_systemBuildVersion
TQ,N,V_version
Tq,N,V_sourceMode
TB,N,V_segmentationDisabled
_keyframesForKey:class:
_setKeyframes:forKey:
setDisparityKeyframes:
aperture
setAperture:
cinematographyState
setCinematographyState:
setDebugMode:
renderingVersionAtCapture
setRenderingVersionAtCapture:
trimToTimeRange:usingScript:
T@"NSArray",C,N
T@"NSNumber",&,N
T@"NSDictionary",&,N
TQ,N
initWithColorAnalysis:
suggestedColorForColor:
addRuleWithHueMin:hueMax:suggestion:
suggestedColorsForColors:fromColorPalette:
system
_system
T@"NURuleSystem",R,N,V_system
keyframesFromDictionaryRepresentations:
initWithTime:homography:
homography
_homography
T{?=qiIq},R,N,V_time
T{?=[3]},R,N,V_homography
initWithKeyframeArray:
count
interpolation
homographyAtTime:
sparseSequence
_homographySequence
medianLuminance
dominantColors
setMedianLuminance:
setDominantColors:
_medianLuminance
_dominantColors
Td,N,V_medianLuminance
T@"NSArray",C,N,V_dominantColors
medianColor
dominantColor
destination
setDestination:
image
setImage:
task
setTask:
imageHistogram
setImageHistogram:
luminanceWeights
setLuminanceWeights:
luminanceThresholds
setLuminanceThresholds:
_destination
_image
_task
_imageHistogram
_luminanceWeights
_luminanceThresholds
T@"NSString",&,N,V_identifier
T@"<NUPurgeableStorage>",&,N,V_destination
T@"CIImage",&,N,V_image
T@"CIRenderTask",&,N,V_task
T@"NUImageHistogram",&,N,V_imageHistogram
T{?=ffff},N,V_luminanceWeights
T{?=ffff},N,V_luminanceThresholds
T@"PFParallaxColor",R,N
initWithParallaxColorAnalysisRequest:
colorAnalysisRequest
_beginRenderingImage:colorSpace:format:error:
_beginRenderNormalizedHueChromaImage:targetHue:hueRange:chromaMin:error:
_beginRenderNormalizedHueChromaImage:targetGray:grayRange:chromaMax:error:
_waitForRenderResources:
_computeAllHistograms:
_purgeRenderResources
imageRect
setImageRect:
hueChromaImage
setHueChromaImage:
alphaCount
setAlphaCount:
dominantHues
setDominantHues:
dominantGrays
setDominantGrays:
_storagePool
_renderResources
_hueChromaImage
_alphaCount
_dominantHues
_dominantGrays
_imageRect
T@"PIParallaxColorAnalysisRequest",R,N
T{?={?=qq}{?=qq}},N,V_imageRect
T@"CIImage",&,N,V_hueChromaImage
Tq,N,V_alphaCount
T@"NSArray",C,N,V_dominantHues
T@"NSArray",C,N,V_dominantGrays
analyzeBackground
setAnalyzeBackground:
normalizedClipRect
setNormalizedClipRect:
maxDominantColors
setMaxDominantColors:
dominanceThreshold
setDominanceThreshold:
chromaThreshold
setChromaThreshold:
_analyzeBackground
_maxDominantColors
_dominanceThreshold
_chromaThreshold
_normalizedClipRect
TB,N,V_analyzeBackground
T{CGRect={CGPoint=dd}{CGSize=dd}},N,V_normalizedClipRect
Tq,N,V_maxDominantColors
Td,N,V_dominanceThreshold
Td,N,V_chromaThreshold
hueChromaKernels
scaleHueKernel
normalizeHueChromaImage:
denormalizeHueChromaImage:
filterHueKernel
inputHueTarget
setInputHueTarget:
inputHueRange
setInputHueRange:
inputChromaMin
setInputChromaMin:
inputHueIsNormalized
setInputHueIsNormalized:
_inputHueTarget
_inputHueRange
_inputChromaMin
_inputHueIsNormalized
T@"NSNumber",&,N,V_inputHueTarget
T@"NSNumber",&,N,V_inputHueRange
T@"NSNumber",&,N,V_inputChromaMin
T@"NSNumber",&,N,V_inputHueIsNormalized
filterLumaKernel
inputLumaTarget
setInputLumaTarget:
inputLumaRange
setInputLumaRange:
inputChromaMax
setInputChromaMax:
_inputLumaTarget
_inputLumaRange
_inputChromaMax
T@"NSNumber",&,N,V_inputLumaTarget
T@"NSNumber",&,N,V_inputLumaRange
T@"NSNumber",&,N,V_inputChromaMax
currentVersion
loadFromContentsDictionary:error:
colorsFromDictionary:key:error:
luminance
setLuminance:
foregroundLuminance
setForegroundLuminance:
backgroundLuminance
setBackgroundLuminance:
colors
setColors:
foregroundColors
setForegroundColors:
backgroundColors
setBackgroundColors:
_luminance
_foregroundLuminance
_backgroundLuminance
_colors
_foregroundColors
_backgroundColors
Tq,N,V_version
Td,N,V_luminance
Td,N,V_foregroundLuminance
Td,N,V_backgroundLuminance
T@"NSArray",C,N,V_colors
T@"NSArray",C,N,V_foregroundColors
T@"NSArray",C,N,V_backgroundColors
_computeCleanAperture:
kindKey
versionKey
setKind:
addAssetIdentifier:toMetadataDictionary:
addAssetIdentifier:toMetadataArray:
removeAssetIdentifierFromMetadataArray:
temperature
setTemperature:
tint
setTint:
setInputVectorsForFilter:
inputTemperature
setInputTemperature:
inputTint
setInputTint:
_inputTemperature
_inputTint
Td,N,V_inputTemperature
Td,N,V_inputTint
nu_evaluateWithPipelineState:error:
rawTime
setRawTime:
sampleMode
setSampleMode:
_sampleMode
_rawTime
T{?=qiIq},N,V_time
T{?=qiIq},N,V_rawTime
Tq,N,V_sampleMode
T@"NSDictionary",R
T@"NSDictionary",&,V_recipe
cacheKey
analysisRequest
videoSource
setVideoSource:
_videoSource
T@"AVAsset",&,N,V_videoSource
T@"NSDictionary",&,N,V_recipe
ICShouldBeCanceled
ICReportProgress:
rangeMin
setRangeMin:
rangeMax
setRangeMax:
shouldCancelHandler
setShouldCancelHandler:
_rangeMin
_rangeMax
_shouldCancelHandler
Td,N,V_rangeMin
Td,N,V_rangeMax
T@?,C,N,V_shouldCancelHandler
keyframes
stabCropRect
analysisType
rawHomographies
T{?={?=qq}{?=qq}},R,N
initWithKeyframes:stabCropRect:analysisType:rawHomographies:
_keyframes
_analysisType
_rawHomographies
_stabCropRect
T@"NSArray",R,C,N,V_keyframes
T{?={?=qq}{?=qq}},R,N,V_stabCropRect
TQ,R,N,V_analysisType
T@"NSDictionary",R,N,V_rawHomographies
allowedAnalysisTypes
setAllowedAnalysisTypes:
allowedCropFraction
setAllowedCropFraction:
_allowedAnalysisTypes
_allowedCropFraction
TQ,N,V_allowedAnalysisTypes
Td,N,V_allowedCropFraction
canPerformGyroBasedStabilizationForAsset:
calculateWithRequest:completion:
calculateRAWWithRequest:completion:
faceBalanceResultFromFaceObservations:request:error:
faceRectFromNormalizedFaceRet:forImageExtent:scaleX:scaleY:
faceBalanceFromFaceImage:forFaceRect:
initWithRequest:isRAW:
rawState
_rawState
Tq,R,V_rawState
pi_grayColorResultValue
pi_valueWithGrayColorResult:
T{?={?=[4d]}{?=[4d]}d},R
RGBResultValue
valueWithRGBResult:
T{?=[4d]},R
_useTempTint:
_correctedRGBResultFromResult:
_chooseNeutralGrayForNonSushi:
_chooseTempTintForSushi:RAWProperties:brightness:
initWithComposition:useSushi:
readBufferFromImage:withRGBAfBufferBlock:
calculateColorWithProperties:completion:
_brightnessMultiplierFromImageProperties:
_computeWhitePointColorWithGrayEdgesBuffer:grayWorldBuffer:greenChannelPercentage:RAWCameraSpaceProperties:
_whitePointColorFromGrayEdgesImage:grayWorldImage:greenChannelPercentage:RAWCameraSpaceProperties:
_configureRequest:
_computeGreenPercentage:
_submitGWRenderRequest:
_submitGERenderRequest:
_bufferRenderClient
_imageDataClient
_useSushi
definitionKernel
inputBlurImage
setInputBlurImage:
inputIntensity
setInputIntensity:
_inputBlurImage
_inputIntensity
T@"CIImage",&,V_inputBlurImage
T@"NSNumber",&,V_inputIntensity
strengthKey
neutralKey
toneKey
grainKey
hueKey
inputStrengthKey
inputNeutralGammaKey
inputToneKey
inputGrainKey
inputHueKey
inputSeedKey
attributeStrengthKey
attributeNeutralGammaKey
attributeToneKey
attributeHueKey
attributeGrainKey
setStrength:
strength
setNeutral:
neutral
setTone:
tone
setGrain:
grain
setHue:
videoMetadataForVariation:error:
setImageVariation:properties:error:
photoProcessingFlagsFromProperties:error:
setPhotoProcessingFlags:properties:error:
photoFeatureFlags:error:
setPhotoFeatureFlags:properties:error:
request
setRequest:
inputSize
setInputSize:
_request
_inputSize
T@"NUImageExportRequest",&,V_request
T{?=qq},V_inputSize
companionImageData
setCompanionImageData:
companionVideoURL
setCompanionVideoURL:
auxiliaryImages
setAuxiliaryImages:
canPropagateOriginalAuxiliaryData
setCanPropagateOriginalAuxiliaryData:
properties
setProperties:
_canPropagateOriginalAuxiliaryData
_companionImageData
_companionVideoURL
_auxiliaryImages
_properties
T@"NSDictionary",&,V_auxiliaryImages
TB,V_canPropagateOriginalAuxiliaryData
T@"NSDictionary",C,V_properties
T{?=qq},D
T@"NSData",&,V_companionImageData
T@"NSURL",&,V_companionVideoURL
geometry
setGeometry:
digest
setDigest:
_geometry
_digest
T@"NUImageGeometry",&,V_geometry
T@"NSString",C,V_digest
T@"NSData",&,V_data
priority
setPriority:
pairingIdentifier
setPairingIdentifier:
_priority
_pairingIdentifier
T@"NUPriority",&,V_priority
T@"NUColorSpace",&,V_colorSpace
T@"NSString",C,V_pairingIdentifier
T@"<NUScalePolicy>",&,V_scalePolicy
metadataProcessor
setMetadataProcessor:
increaseBitRateIfNecessary
setIncreaseBitRateIfNecessary:
videoCodecType
setVideoCodecType:
preserveSourceColorSpace
setPreserveSourceColorSpace:
bypassOutputSettingsIfNoComposition
setBypassOutputSettingsIfNoComposition:
applyVideoOrientationAsMetadata
setApplyVideoOrientationAsMetadata:
requireHardwareEncoder
setRequireHardwareEncoder:
includeCinematicVideoTracks
setIncludeCinematicVideoTracks:
computeDigest
setComputeDigest:
_increaseBitRateIfNecessary
_preserveSourceColorSpace
_bypassOutputSettingsIfNoComposition
_applyVideoOrientationAsMetadata
_requireHardwareEncoder
_includeCinematicVideoTracks
_computeDigest
_metadataProcessor
_videoCodecType
T@?,C,V_metadataProcessor
TB,N,V_increaseBitRateIfNecessary
T@"NSString",C,N,V_videoCodecType
TB,N,V_preserveSourceColorSpace
TB,N,V_bypassOutputSettingsIfNoComposition
TB,N,V_applyVideoOrientationAsMetadata
TB,N,V_requireHardwareEncoder
TB,N,V_includeCinematicVideoTracks
TB,N,V_computeDigest
imageExportFormatForURL:
imageExportFormat
setImageExportFormat:
JPEGCompressionQuality
setJPEGCompressionQuality:
optimizeForSharing
setOptimizeForSharing:
applyImageOrientationAsMetadata
setApplyImageOrientationAsMetadata:
optimizeForBackgroundProcessing
setOptimizeForBackgroundProcessing:
_optimizeForSharing
_applyImageOrientationAsMetadata
_optimizeForBackgroundProcessing
_imageExportFormat
_JPEGCompressionQuality
T@"NUImageExportFormat",C,V_imageExportFormat
Td,V_JPEGCompressionQuality
TB,V_optimizeForSharing
TB,V_applyImageOrientationAsMetadata
TB,V_optimizeForBackgroundProcessing
primaryURL
setPrimaryURL:
videoComplementURL
setVideoComplementURL:
videoPosterFrameURL
setVideoPosterFrameURL:
renderCompanionResources
setRenderCompanionResources:
reframeCropAdjustment
setReframeCropAdjustment:
reframeVideoAdjustment
setReframeVideoAdjustment:
_renderCompanionResources
_primaryURL
_videoComplementURL
_videoPosterFrameURL
_reframeCropAdjustment
_reframeVideoAdjustment
T@"NSURL",&,V_primaryURL
T@"NSURL",&,V_videoComplementURL
T@"NSURL",&,V_videoPosterFrameURL
TB,V_renderCompanionResources
T@"NUAdjustment",&,V_reframeCropAdjustment
T@"NUAdjustment",&,V_reframeVideoAdjustment
TB,V_applyVideoOrientationAsMetadata
setMetadataConverter:
metadataConverter
_lowMemoryModeSupportedForComposition:
resetImageProperties:preserveRegions:
T@"<PICompositionExporterMetadataConverter>",&
exportImageToURL:composition:options:completion:
exportImageToDataWithComposition:options:completion:
exportVideoToURL:composition:options:completion:
exportComposition:toPrimaryURL:videoComplementURL:videoPosterFrameURL:priority:completionQueue:completion:
exportComposition:options:completionQueue:completion:
variationForFlavor:
prepareImageExportRequest:options:completion:
prepareAuxiliaryImagesFetchProperties:options:completion:
addImageProperties:composition:options:error:
addVideoProperties:composition:options:error:
_exportVideoToURL:composition:options:properties:progress:completion:
bilateralKernels
RGBToLabKernels
bilateralAdd1Kernel
bilateralAdd2Kernel
bilateralAdd3Kernel
bilateralAdd4Kernel
bilateralAdd5Kernel
bilateralAdd6Kernel
bilateralAdd7Kernel
bilateralAdd8Kernel
bilateralAdd9Kernel
bilateralFinalizeKernel
RGBToLabKernel
LabToRGBKernel
samplesPerPass
boundsForPointArray:
enlargedBounds:withPoints:
bilateralAddROI:destRect:userInfo:
doBilateralPass:points:weights:sums:slope:
inputPoints
setInputPoints:
inputWeights
setInputWeights:
inputEdgeDetail
setInputEdgeDetail:
inputVersion
setInputVersion:
_inputPoints
_inputWeights
_inputEdgeDetail
_inputVersion
T@"NSArray",&,V_inputPoints
T@"NSArray",&,V_inputWeights
T@"NSNumber",&,V_inputEdgeDetail
T@"NSNumber",&,V_inputVersion
BWBilateralKernels
bilateralLoop2Kernel
bilateralLoop5Kernel
bilateralLoop11Kernel
bilateralROI:destRect:userInfo:
doBilateralLoop:points:weights:slope:
inputBorder
setInputBorder:
_inputBorder
T@"NSNumber",&,V_inputBorder
inputRadius
setInputRadius:
_inputRadius
T@"NSNumber",&,V_inputRadius
versionWithMajor:minor:subMinor:platform:
versionFromString:
initWithMajor:minor:subMinor:platform:
initWithMajor:minor:subMinor:
string
asOrderedInteger
compare:
isEqualToAdjustmentVersion:
majorVersion
minorVersion
subMinorVersion
platform
_majorVersion
_minorVersion
_subMinorVersion
_platform
T@"NSString",R,W,N
TQ,R,N,V_majorVersion
TQ,R,N,V_minorVersion
TQ,R,N,V_subMinorVersion
T@"NSString",R,C,N,V_platform
amountKey
smartColorHDRStatistics
_isIdentity
_kernelCPos
_kernelCNeg
_kernelV_gt1
_kernelV_lt1
_kernelCast
inputVibrancy
setInputVibrancy:
inputContrast
setInputContrast:
inputCast
setInputCast:
T@"NSNumber",&,N,VinputVibrancy
T@"NSNumber",&,N,VinputContrast
T@"NSNumber",&,N,VinputCast
autoEnhanceCache
autoEnhanceFiltersForImage:algorithm:
apertureAutoEnhanceFiltersForImage:
colorNormalizationFiltersForImage:
photosAutoEnhanceFiltersForImage:
inputTargetImage
setInputTargetImage:
inputAlgorithm
setInputAlgorithm:
_inputTargetImage
_inputAlgorithm
T@"CIImage",&,N,V_inputTargetImage
T@"NSString",&,N,V_inputAlgorithm
T@"PFParallaxLayerStack",R,N
_submit:
_submitLayerStackRequestsWithLayout:completion:
_submitInactiveLayoutRequest:
_submitClockOverlapRequestWithLayout:completion:
_submitClockMaterialRequestWithLayerStack:completion:
_submitLayerStackRequestForMode:layout:completion:
_recordResult:
_recordError:
_responseWithLayerStack:
cancel
style
setStyle:
layerStackOptions
setLayerStackOptions:
updateInactiveFrame
setUpdateInactiveFrame:
updateClockZPosition
setUpdateClockZPosition:
updateClockAreaLuminance
setUpdateClockAreaLuminance:
_requests
_results
_error
_updateInactiveFrame
_updateClockZPosition
_updateClockAreaLuminance
_style
_layerStackOptions
T@"PIParallaxStyle",&,N,V_style
TQ,N,V_layerStackOptions
TB,N,V_updateInactiveFrame
TB,N,V_updateClockZPosition
TB,N,V_updateClockAreaLuminance
crossfadeDurationValueKey
crossfadeDurationTimescaleKey
startTimeValueKey
startTimeTimescaleKey
loopTimeRangeStartValueKey
loopTimeRangeStartTimescaleKey
loopTimeRangeDurationValueKey
loopTimeRangeDurationTimescaleKey
setLoopTimeRange:
setCrossfadeDuration:
T{?={?=qiIq}{?=qiIq}},N
infillMaskForSegmentationMatte:
imageWithColor:extent:
thresholdImage:withThreshold:
dilateMask:withRadius:
erodeMask:withRadius:
openMask:withRadius:
invertImage:
foregroundForImage:matte:
backgroundForImage:matte:
backgroundForImage:matte:infill:
upsampleMatteImage:guideImage:
upsampleBackgroundImage:toSize:
tightCropFrameFromMatteImage:
histogramForSegmentationMatte:
histogramForSegmentationMatteImage:
matteHistogramIndicatesSubjectDetected:
bimodalScoreForHistogram:
localConfidenceScoreForLocalConfidenceImage:extent:context:
localConfidenceImage:
groundedScoreForSegmentationMatte:context:
computeClockLayerOrderWithVisibleFrame:foregroundImage:layoutConfiguration:context:
computeClockLayerOrderWithVisibleFrame:segmentationMatte:layoutConfiguration:context:interactive:
computeAlphaCoverageWithRect:foregroundImage:context:
computeMatteCoverageWithRect:segmentationMatte:context:
computeTargetOverlapYOffsetWithVisibleFrame:imageSize:segmentationMatte:layoutConfiguration:context:
computeAvoidOverlapYOffsetWithVisibleFrame:imageSize:segmentationMatte:layoutConfiguration:outputUnsafeOverlap:context:
scaleRect:scaleFactor:scaleCenter:
_computeHeadroomZoomFactorWithVisibleFrame:scaleCenter:initialOverlap:matte:layoutConfiguration:context:
computeHeadroomZoomFactorWithVisibleFrame:zoomTowardsTop:matte:layoutConfiguration:context:
_computeAvoidingRect:imageSize:maxYMotion:segmentationMatte:layoutConfiguration:context:
computeInactiveFrameWithVisibleFrame:imageSize:canUpdateVisibleRect:segmentationMatte:layoutConfiguration:context:
debugImageWithInputImage:layout:faceRects:saliencyPreferrectRect:saliencyAcceptableRect:
debugImageWithInputImage:finalLayout:intermediateLayout:faceRects:saliencyPreferrectRect:saliencyAcceptableRect:
debugPreviewRenderWithBackground:foreground:layout:style:
debugImageForColorAnalysis:inputImage:visibleFrame:
portraitInfoKey
spillMatteAllowedKey
setPortraitInfo:
portraitInfo
canRenderPortraitEffect
defaultStrength
setSpillMatteAllowed:
spillMatteAllowed
T@"NSNumber",C,N
applyInactiveStyleToImage:error:
_localLightDataForImage:
_noOpRemovalFunctions
noOpRemovalFunctions
copyOfAdjustmentRemovingNoOps:identifier:
copyOfCompositionRemovingNoOps:
valueKey
writeRecipe:toURL:error:
serializeRecipe:
unarchivedStyleRecipeWithURL:error:
deserializeRecipe:error:
_serializeParameters:
_serializeParameter:
_deserializeParameters:version:error:
_deserializeParameter:version:error:
_serializeFilters:
_serializeDefinition:
_deserializeFilterDefinitions:version:error:
_deserializeFilterDefinition:version:error:
renderingMetadataIdentifier
initWithMetadataGroup:majorVersion:minorVersion:error:
applyToRenderRequest:
valueWithIdentifier:inGroup:ofClass:
_cameraInfoFromMetadataGroup:
cameraInfo
setTimedMetadata:
focusedDisparity
_cameraInfo
_timedMetadata
_focusedDisparity
_aperture
T@"PTTimedRenderingMetadata",&,N,V_timedMetadata
Td,R,N,V_focusedDisparity
Td,R,N,V_aperture
T@"NSDictionary",R,N,V_cameraInfo
renderOnDevice:colorSize:disparitySize:quality:debugMode:globalRenderingMetadata:usingBlock:
prewarmRendererForDevice:colorSize:disparitySize:quality:debugMode:globalRenderingMetadata:
initWithDevice:colorSize:disparitySize:quality:debugMode:
renderState:matchesMetadata:
prepareToRenderWithMetadata:
device
colorSize
disparitySize
quality
isInUse
setInUse:
lastUseTime
setLastUseTime:
_renderPipeline
_renderState
_inUse
_quality
_device
_debugMode
_lastUseTime
_colorSize
_disparitySize
T@"<MTLDevice>",R,N,V_device
T{?=qq},R,N,V_colorSize
T{?=qq},R,N,V_disparitySize
Ti,R,N,V_quality
Tq,R,N,V_debugMode
inUse
TB,N,GisInUse,V_inUse
T@"NSDate",&,N,V_lastUseTime
_highKeyHDR
undoOrientation:forPitch:yaw:angle:
requestVisionCleanUp
faceObservationCache
setFaceObservationCache:
T@"PIFaceObservationCache",&,N
perspectiveErrorFromCoreImage:
addMethodDiagnostics:details:
addMethodResultToDiagnostics:error:setYawPitchError:
wrapAsUnexpectedError:
writeDebugDiagnosticsToDisk
getSizeOfAllFaces:
passesFaceCheck:
hasFrontFacingCameraDimentions:
isFrontFacingCameraImage:pixelSize:
passesImagePropertiesCheck:
passesBuildingCheck:
overcaptureImageProperties:
primaryImageProperties:
canGenerateNewCropRect:
passesConfidenceCheck:error:
passesMinimumCorrectionCheck:error:
submitVerified:
maxAutoYaw
setMaxAutoYaw:
maxAutoPitch
setMaxAutoPitch:
maxAutoAngle
setMaxAutoAngle:
minimumPitchCorrection
setMinimumPitchCorrection:
minimumYawCorrection
setMinimumYawCorrection:
minimumAngleCorrection
setMinimumAngleCorrection:
minimumConfidence
setMinimumConfidence:
maxFaceSize
setMaxFaceSize:
minimumPitchCorrectionArea
setMinimumPitchCorrectionArea:
minimumYawCorrectionArea
setMinimumYawCorrectionArea:
disableOnPanos
setDisableOnPanos:
disableOnFrontFacingCameraImages
setDisableOnFrontFacingCameraImages:
shouldRunBuildingCheck
setShouldRunBuildingCheck:
angleSeedDegreesCCW
setAngleSeedDegreesCCW:
debugFilesEnabled
setDebugFilesEnabled:
debugFilesPrefix
setDebugFilesPrefix:
debugDiagnostics
debugLineDetectionImage
setDebugLineDetectionImage:
_disableOnPanos
_disableOnFrontFacingCameraImages
_shouldRunBuildingCheck
_debugFilesEnabled
_faceObservationCache
_maxAutoYaw
_maxAutoPitch
_maxAutoAngle
_minimumPitchCorrection
_minimumYawCorrection
_minimumAngleCorrection
_minimumConfidence
_maxFaceSize
_minimumPitchCorrectionArea
_minimumYawCorrectionArea
_angleSeedDegreesCCW
_debugFilesPrefix
_debugDiagnostics
_debugLineDetectionImage
T@"CIImage",&,N,V_debugLineDetectionImage
T@"NSNumber",C,V_maxAutoYaw
T@"NSNumber",C,V_maxAutoPitch
T@"NSNumber",C,V_maxAutoAngle
Td,V_minimumPitchCorrection
Td,V_minimumYawCorrection
Td,V_minimumAngleCorrection
Td,V_minimumConfidence
Td,V_maxFaceSize
Td,V_minimumPitchCorrectionArea
Td,V_minimumYawCorrectionArea
TB,V_disableOnPanos
TB,V_disableOnFrontFacingCameraImages
TB,V_shouldRunBuildingCheck
Td,V_angleSeedDegreesCCW
TB,V_debugFilesEnabled
T@"NSString",C,V_debugFilesPrefix
T@"NSMutableDictionary",R,V_debugDiagnostics
T@"PIFaceObservationCache",&,N,V_faceObservationCache
luminanceKey
xOriginKey
yOriginKey
widthKey
heightKey
constraintWidthKey
constraintHeightKey
angleKey
pitchKey
yawKey
smartKey
originalCropKey
isGeometryIdentityForImageSize:
isCropConstrained
isCropIdentityForImageSize:
cropRect
constraintWidth
constraintHeight
angle
angleRadians
pitch
pitchRadians
yawRadians
autoCropped
isSmart
isOriginalCrop
setCropRect:
setConstraintWidth:
setConstraintHeight:
setAngle:
setAngleRadians:
setPitch:
setPitchRadians:
setYaw:
setYawRadians:
setAutoCropped:
setSmart:
setOriginalCrop:
T{CGRect={CGPoint=dd}{CGSize=dd}},N
smart
TB,N,GisSmart
originalCrop
TB,N,GisOriginalCrop
setInputDecoderVersion:
inputDecoderVersion
initWithRecipe:
resolvedParameters
_evaluateImageWithFilterDefinitions:inputImage:
setParameters:
_parameters
T@"PIParallaxStyleRecipe",&,N,V_recipe
T@"NSDictionary",C,N,V_parameters
inputLightKey
attributeBrightnessKey
attributeContrastKey
attributeExposureKey
attributeHighlightsKey
attributeShadowsKey
attributeBlackPointKey
attributeLocalLightKey
attributeLightMapKey
attributeLightMapWidthKey
attributeLightMapHeightKey
offsetBlackKey
offsetBrightnessKey
offsetContrastKey
offsetExposureKey
offsetHighlightsKey
offsetLocalLightKey
offsetShadowsKey
inputExposureKey
inputContrastKey
inputBrightnessKey
inputShadowsKey
inputHighlightsKey
inputBlackKey
inputLocalLightKey
inputRawHighlightsKey
statisticsKey
overcaptureStatisticsKey
_updateSettingsWithInputLight:
computedSettings
inputLightDefault
setInputLight:
inputLight
setInputExposure:
inputExposure
setInputBrightness:
inputBrightness
setInputShadows:
inputShadows
setInputHighlights:
inputHighlights
setInputBlack:
inputBlack
setInputLocalLight:
inputLocalLight
setInputRawHighlights:
inputRawHighlights
setStatistics:
setOvercaptureStatistics:
overcaptureStatistics
offsetBlack
setOffsetBlack:
offsetBrightness
setOffsetBrightness:
offsetContrast
setOffsetContrast:
offsetExposure
setOffsetExposure:
offsetHighlights
setOffsetHighlights:
offsetLocalLight
setOffsetLocalLight:
offsetShadows
setOffsetShadows:
_smartSettings
alphaCompositingKernel
dynamismMapKernel
longExposureFusionKernels
dynamismMapRefineKernel
rgbToLumaKernel
homographyKernel
nccKernel
nccCoarseKernel
blur7x7Kernel
blur5x5Kernel
blur3x3Kernel
fusionKernel
assetIdentifierForURL:type:useEmbeddedPreview:
imageSourceWithURL:type:useEmbeddedPreview:
imageSourceWithURL:type:proxyImage:orientation:useEmbeddedPreview:
imageSourceWithCIImage:orientation:
videoSourceWithURL:
livePhotoSourceWithPhotoSource:videoSource:
newComposition
compositionByRemovingVideoAndLivePhotoAdjustments:
newAdjustmentWithName:
newAdjustmentWithIdentifier:
newImageRenderClientWithName:
geometryRequestWithComposition:
imagePropertiesRequestWithComposition:
videoPropertiesRequestWithComposition:
imageRenderRequestWithComposition:fitInSize:wideGamut:
imageRenderRequestWithComposition:fillInSize:wideGamut:
_imageRenderRequestWithComposition:wideGamut:
newCGImageFromBufferImage:
priorityWithLevel:
videoRenderRequestWithComposition:fitInSize:
is3DEffect:
isPortraitEffect:
isPortraitStageEffect:
areCPVAssetsEditable
isAVAssetEditable:reason:
effectNameForFilterName:
filterNameForEffectName:
pipelineFiltersForCropping
pipelineFiltersForOriginalGeometry
pipelineFiltersForShowingOriginal
pipelineFiltersForShowingOriginalWithGeometry
pipelineFiltersForRAWShowingOriginalWithGeometry
newCompositionControllerWithComposition:
adjustmentConstants
handlePIGlobalSettings:
validatedCompositionCopyForComposition:mediaType:
geometryBasedAdjustmentIdentifiers
knownFormatsVersionsMap
updateCropAdjustmentController:after:error:
preheatEditDependencies
rawAdjustmentWithRawImageProperties:
allAdjustmentTypes
nonVisualAdjustmentTypes
PISmartToneAdjustmentKey
PISmartColorAdjustmentKey
PISmartBWAdjustmentKey
PIGrainAdjustmentKey
PIAutoEnhanceAdjustmentKey
PIWhiteBalanceAdjustmentKey
PIRedEyeAdjustmentKey
PIApertureRedEyeAdjustmentKey
PIRetouchAdjustmentKey
PIVignetteAdjustmentKey
PISharpenAdjustmentKey
PINoiseReductionAdjustmentKey
PIDefinitionAdjustmentKey
PICurvesAdjustmentKey
PILevelsAdjustmentKey
PISelectiveColorAdjustmentKey
PIEffectAdjustmentKey
PIEffect3DAdjustmentKey
PICropAdjustmentKey
PITrimAdjustmentKey
PISlomoAdjustmentKey
PILivePhotoKeyFrameAdjustmentKey
PIVideoPosterFrameAdjustmentKey
PIAutoLoopAdjustmentKey
PIHighResFusionAdjustmentKey
PIMuteAdjustmentKey
PIDepthAdjustmentKey
PIPortraitAdjustmentKey
PIOrientationAdjustmentKey
PIRawAdjustmentKey
PIRawNoiseReductionAdjustmentKey
PISemanticEnhanceAdjustmentKey
PIVideoReframeAdjustmentKey
PISourceSelectAdjustmentKey
PIVideoStabilizeAdjustmentKey
PIVideoCrossfadeLoopAdjustmentKey
PIPortraitVideoAdjustmentKey
PISourceAdjustmentKey
PIOvercaptureSourceAdjustmentKey
_PISmartToneAdjustmentKey
_PISmartColorAdjustmentKey
_PISmartBWAdjustmentKey
_PIGrainAdjustmentKey
_PIAutoEnhanceAdjustmentKey
_PIWhiteBalanceAdjustmentKey
_PIRedEyeAdjustmentKey
_PIApertureRedEyeAdjustmentKey
_PIRetouchAdjustmentKey
_PIVignetteAdjustmentKey
_PISharpenAdjustmentKey
_PINoiseReductionAdjustmentKey
_PIDefinitionAdjustmentKey
_PICurvesAdjustmentKey
_PILevelsAdjustmentKey
_PISelectiveColorAdjustmentKey
_PIEffectAdjustmentKey
_PIEffect3DAdjustmentKey
_PICropAdjustmentKey
_PITrimAdjustmentKey
_PISlomoAdjustmentKey
_PILivePhotoKeyFrameAdjustmentKey
_PIVideoPosterFrameAdjustmentKey
_PIAutoLoopAdjustmentKey
_PIHighResFusionAdjustmentKey
_PIMuteAdjustmentKey
_PIDepthAdjustmentKey
_PIPortraitAdjustmentKey
_PIOrientationAdjustmentKey
_PIRawAdjustmentKey
_PIRawNoiseReductionAdjustmentKey
_PISemanticEnhanceAdjustmentKey
_PIVideoReframeAdjustmentKey
_PISourceSelectAdjustmentKey
_PIVideoStabilizeAdjustmentKey
_PIVideoCrossfadeLoopAdjustmentKey
_PIPortraitVideoAdjustmentKey
_PISourceAdjustmentKey
_PIOvercaptureSourceAdjustmentKey
T@"NSString",R,N,V_PISmartToneAdjustmentKey
T@"NSString",R,N,V_PISmartColorAdjustmentKey
T@"NSString",R,N,V_PISmartBWAdjustmentKey
T@"NSString",R,N,V_PIGrainAdjustmentKey
T@"NSString",R,N,V_PIAutoEnhanceAdjustmentKey
T@"NSString",R,N,V_PIWhiteBalanceAdjustmentKey
T@"NSString",R,N,V_PIRedEyeAdjustmentKey
T@"NSString",R,N,V_PIApertureRedEyeAdjustmentKey
T@"NSString",R,N,V_PIRetouchAdjustmentKey
T@"NSString",R,N,V_PIVignetteAdjustmentKey
T@"NSString",R,N,V_PISharpenAdjustmentKey
T@"NSString",R,N,V_PINoiseReductionAdjustmentKey
T@"NSString",R,N,V_PIDefinitionAdjustmentKey
T@"NSString",R,N,V_PICurvesAdjustmentKey
T@"NSString",R,N,V_PILevelsAdjustmentKey
T@"NSString",R,N,V_PISelectiveColorAdjustmentKey
T@"NSString",R,N,V_PIEffectAdjustmentKey
T@"NSString",R,N,V_PIEffect3DAdjustmentKey
T@"NSString",R,N,V_PICropAdjustmentKey
T@"NSString",R,N,V_PITrimAdjustmentKey
T@"NSString",R,N,V_PISlomoAdjustmentKey
T@"NSString",R,N,V_PILivePhotoKeyFrameAdjustmentKey
T@"NSString",R,N,V_PIVideoPosterFrameAdjustmentKey
T@"NSString",R,N,V_PIAutoLoopAdjustmentKey
T@"NSString",R,N,V_PIHighResFusionAdjustmentKey
T@"NSString",R,N,V_PIMuteAdjustmentKey
T@"NSString",R,N,V_PIDepthAdjustmentKey
T@"NSString",R,N,V_PIPortraitAdjustmentKey
T@"NSString",R,N,V_PIOrientationAdjustmentKey
T@"NSString",R,N,V_PIRawAdjustmentKey
T@"NSString",R,N,V_PIRawNoiseReductionAdjustmentKey
T@"NSString",R,N,V_PISemanticEnhanceAdjustmentKey
T@"NSString",R,N,V_PIVideoReframeAdjustmentKey
T@"NSString",R,N,V_PISourceSelectAdjustmentKey
T@"NSString",R,N,V_PIVideoStabilizeAdjustmentKey
T@"NSString",R,N,V_PIVideoCrossfadeLoopAdjustmentKey
T@"NSString",R,N,V_PIPortraitVideoAdjustmentKey
T@"NSString",R,N,V_PISourceAdjustmentKey
T@"NSString",R,N,V_PIOvercaptureSourceAdjustmentKey
kernelFB0
kernelFB3
boostParametersFromRawProperties:
T@"NSString",R
outputImageFB0
outputImageFB3
inputBoost
setInputBoost:
inputParams
setInputParams:
_inputBoost
_inputParams
Td,V_inputBoost
T@"NSString",C,V_inputVersion
T@"NSArray",C,V_inputParams
kernelsDictionaryWithString:
pi_createColorCubeDataForFilters:dimension:colorSpace:
metalLibraryData
metalKernelWithFunctionName:error:
framedRectImageWithCGRect:color:borderWidth:
loadFilterWithName:
conversionMap
mapForSerialization
curvesKernel
inputTableImage
setInputTableImage:
_inputTableImage
T@"CIImage",&,V_inputTableImage
initWithParallaxLayoutRequest:
layoutRequest
layoutRegions
setLayoutRegions:
segmentationMatteImage
setSegmentationMatteImage:
segmentationConfidenceMapImage
setSegmentationConfidenceMapImage:
segmentationClassification
setSegmentationClassification:
cropScore
setCropScore:
layoutScore
setLayoutScore:
nFaces
setNFaces:
segmentationScore
setSegmentationScore:
localConfidenceScore
setLocalConfidenceScore:
groundedScore
setGroundedScore:
confidenceMapScore
setConfidenceMapScore:
parallaxScore
setParallaxScore:
mattePureBackground
setMattePureBackground:
mattePureForeground
setMattePureForeground:
confidencePureBackground
setConfidencePureBackground:
confidencePureForeground
setConfidencePureForeground:
clockOverlapAcceptable
setClockOverlapAcceptable:
resolutionRatio
setResolutionRatio:
faceSize
setFaceSize:
faceLocalConfidence
setFaceLocalConfidence:
facePositionAcceptable
setFacePositionAcceptable:
metadataClockOverlapAcceptable
setMetadataClockOverlapAcceptable:
_clockOverlapAcceptable
_facePositionAcceptable
_metadataClockOverlapAcceptable
_cropScore
_layoutScore
_nFaces
_segmentationScore
_localConfidenceScore
_groundedScore
_confidenceMapScore
_parallaxScore
_mattePureBackground
_mattePureForeground
_confidencePureBackground
_confidencePureForeground
_resolutionRatio
_faceSize
_faceLocalConfidence
_layoutRegions
_segmentationMatteImage
_segmentationConfidenceMapImage
_segmentationClassification
T@"PIParallaxLayoutRequest",R,N
T@"<PFParallaxAssetRegions>",&,N,V_layoutRegions
T@"CIImage",&,N,V_segmentationMatteImage
T@"CIImage",&,N,V_segmentationConfidenceMapImage
TQ,N,V_segmentationClassification
Tf,N,V_cropScore
Tf,N,V_layoutScore
Tf,N,V_nFaces
Tf,N,V_segmentationScore
Tf,N,V_localConfidenceScore
Tf,N,V_groundedScore
Tf,N,V_confidenceMapScore
Tf,N,V_parallaxScore
Tf,N,V_mattePureBackground
Tf,N,V_mattePureForeground
Tf,N,V_confidencePureBackground
Tf,N,V_confidencePureForeground
TB,N,V_clockOverlapAcceptable
Tf,N,V_resolutionRatio
Tf,N,V_faceSize
Tf,N,V_faceLocalConfidence
TB,N,V_facePositionAcceptable
TB,N,V_metadataClockOverlapAcceptable
calculateLuminanceValuesForImage:renderer:error:
calculateClockLuminanceValuesForLayerStack:renderer:error:
initWithParameters:foregroundFilters:backgroundFilters:matteFilters:
isEqualToParallaxStyleRecipe:
foregroundFilters
backgroundFilters
matteFilters
_foregroundFilters
_backgroundFilters
_matteFilters
T@"NSDictionary",R,C,N,V_parameters
T@"NSArray",R,C,N,V_foregroundFilters
T@"NSArray",R,C,N,V_backgroundFilters
T@"NSArray",R,C,N,V_matteFilters
isEqualToParallaxStyleDefinition:
evaluateWithContext:error:
initWithFilterName:parameters:
isEqualToParallaxStyleFilterDefinition:
filterName
_filterName
T@"NSString",R,C,N,V_filterName
initWithStackName:filters:
isEqualToParallaxStyleFilterStackDefinition:
stackName
filters
_stackName
_filters
T@"NSString",R,C,N,V_stackName
T@"NSArray",R,C,N,V_filters
isEqualToParallaxStyleParameter:
initWithNumber:
initWithNumber:unit:
numberValue
unit
_numberValue
_unit
T@"NSNumber",R,N,V_numberValue
T@"NSString",R,N,V_unit
initWithRed:green:blue:alpha:
redValue
greenValue
blueValue
alphaValue
_redValue
_greenValue
_blueValue
_alphaValue
T@"NSNumber",R,N,V_redValue
T@"NSNumber",R,N,V_greenValue
T@"NSNumber",R,N,V_blueValue
T@"NSNumber",R,N,V_alphaValue
initWithX:Y:
initWithX:Y:unit:
xValue
yValue
_xValue
_yValue
T@"NSNumber",R,N,V_xValue
T@"NSNumber",R,N,V_yValue
initWithMode:
modeValue
_modeValue
T@"NSString",R,C,N,V_modeValue
initWithVariableName:
variableName
_variableName
T@"NSString",R,C,N,V_variableName
lightMapImage
setOutputImage:
guideImage
setGuideImage:
backgroundImage
setBackgroundImage:
visibleRect
setVisibleRect:
_outputImage
_guideImage
_backgroundImage
_visibleRect
T@"CIImage",&,N,V_outputImage
T@"CIImage",&,N,V_guideImage
T@"CIImage",&,N,V_backgroundImage
T{CGRect={CGPoint=dd}{CGSize=dd}},N,V_visibleRect
dealloc
initWithSize:renderer:jobNumber:
workingColorSpace
start:
_initializeStorage:image:error:
isReadyForMoreData
_isReadyForMoreData
markAsFinished
_markAsFinished
waitUntilDone
accumulate:error:
_appendInputFrame:
nextInputFrame
_nextInputFrame
_start
_initializeAccumulation
_initializeAccumulation:
_accumulate:
_accumulate:error:
writeLongExposureImage:UTI:colorSpace:error:
writeMaskImage:UTI:error:
_dynamismMapWithMinImage:maxImage:extent:
_exportOutputImage:format:colorSpace:toURL:uti:error:
_accumError
set_accumError:
_pixelSize
_renderer
_temporaryDestinationStorage
_averageAccumulationStorage
_minimumAccumulationStorage
_maximumAccumulationStorage
_frameCount
_jobNumber
_stateQueue
_accumQueue
_accumSemaphore
_readySemaphore
_doneGroup
_inputFrames
_finished
_imageOptions
__accumError
T@"NSError",&,V__accumError
observation
extent
T@"VNImageHomographicAlignmentObservation",R,C
setObservation:
setExtent:
_observation
_extent
T@"VNImageHomographicAlignmentObservation",C,N,V_observation
T{?={?=qq}{?=qq}},N,V_extent
newRenderPipelineStateForEvaluationMode:
wantsRenderScaleClampedToNativeScale
registrationRequest
guideExtent
setGuideExtent:
stillImage
setStillImage:
_stillImage
_guideExtent
T{?={?=qq}{?=qq}},N,V_guideExtent
T@"CIImage",&,N,V_stillImage
T@"VNImageHomographicAlignmentObservation",&,N,V_observation
initWithComposition:tag:responseQueue:
initWithComposition:responseQueue:
_pipelineFilters
inputDestinationImage
setInputDestinationImage:
inputCorrectionInfo
setInputCorrectionInfo:
inputCameraModel
setInputCameraModel:
_inputDestinationImage
_inputCorrectionInfo
_inputCameraModel
T@"CIImage",&,N,V_inputDestinationImage
T@"NSArray",&,N,V_inputCorrectionInfo
T@"NSString",&,N,V_inputCameraModel
posterFrameTime
setPosterFrameTime:
locallySupportedFormatVersions
currentFormatVersion
adjustmentHasPerspective:settings:
adjustmentHasCTM:settings:
_versionRules
versionRules
formatVersionForAdjustment:identifier:
adjustmentDataFormatVersionForComposition:
captureDebugDirectoryForComposition:
startKey
startScaleKey
endKey
endScaleKey
rateKey
endTime
setEndTime:
rate
setRate:
initWithMin:max:manualMin:manualMax:depthMin:depthMax:
withLenience:
manualMin
manualMax
depthMin
depthMax
_min
_max
_manualMin
_manualMax
_depthMin
_depthMax
Td,R,N,V_min
Td,R,N,V_max
Td,R,N,V_manualMin
Td,R,N,V_manualMax
Td,R,N,V_depthMin
Td,R,N,V_depthMax
loadFromURL:
initWithRanges:
ranges
_ranges
T@"NSDictionary",R,C,N,V_ranges
gatingResultForSegmentationScores:
isValidSegmentationScoreForDepth:
segmentationScoreRanges
initWithInput:assetURL:cinematographyState:monochrome:
_imageByAddingDetection:toImage:isPrimary:canvasSize:inverseOrientation:
cinematographyScript
setCinematographyScript:
setRenderTime:
labelImageCache
setLabelImageCache:
_cinematographyScript
_labelImageCache
_renderTime
T@"PTCinematographyScript",&,N,V_cinematographyScript
T{?=qiIq},N,V_renderTime
T@"NSCache",&,N,V_labelImageCache
colorWashKernels
hueChromaColorWashKernel
hueChromaFixedColorWashKernel
hueChromaVariableColorWashKernel
colorWashFixedKernel
colorWashVariableKernel
colorWashFixedSmoothKernel
colorWashVariableSmoothKernel
inputColor
setInputColor:
inputMode
setInputMode:
_inputColor
_inputMode
T@"CIColor",&,N,V_inputColor
T@"NSString",&,N,V_inputMode
iptColorWashDuoKernel
iptColorWashDuoFixedKernel
iptColorWashDuoVariableKernel
hueChromaColorWashDuoKernel
hueChromaColorWashDuoFixedKernel
hueChromaColorWashDuoVariableKernel
rgbColorWashDuoKernel
rgbColorWashDuoFixedKernel
rgbColorWashDuoVariableKernel
inputShadowColor
setInputShadowColor:
inputHighlightColor
setInputHighlightColor:
_inputShadowColor
_inputHighlightColor
T@"CIColor",&,N,V_inputShadowColor
T@"CIColor",&,N,V_inputHighlightColor
outputExposureKey
falseColorHDRKey
inputRAWGamutMapMaxKey
gainMapParametersFromRawProperties:
shouldUseGainMapExposureCompensationForRawProperties:
faceBalanceKernels
newLinearWideGamutColorSpace
linearWideGamutColorSpace
inputOrigI
setInputOrigI:
inputOrigQ
setInputOrigQ:
inputWarmth
setInputWarmth:
_inputOrigI
_inputOrigQ
_inputStrength
_inputWarmth
Td,N,V_inputOrigI
Td,N,V_inputOrigQ
Td,N,V_inputStrength
Td,N,V_inputWarmth
calculateCurveTable:
curvePointsFromDictionaries:
tableImageFromRed:green:blue:luminance:
inputPointsR
setInputPointsR:
inputPointsG
setInputPointsG:
inputPointsB
setInputPointsB:
inputPointsL
setInputPointsL:
_inputPointsR
_inputPointsG
_inputPointsB
_inputPointsL
T@"NSArray",&,V_inputPointsR
T@"NSArray",&,V_inputPointsG
T@"NSArray",&,V_inputPointsB
T@"NSArray",&,V_inputPointsL
colorBalanceKernels
gHDRtoPPKernel
PPtogHDRKernel
colorBalanceKernel
applyInputConversion:
applyOutputConversion:
inputWarmTemp
setInputWarmTemp:
inputWarmTint
setInputWarmTint:
inputHasFace
setInputHasFace:
inputIsRaw
setInputIsRaw:
_inputWarmTemp
_inputWarmTint
_inputHasFace
_inputIsRaw
T@"NSNumber",&,N,V_inputWarmTemp
T@"NSNumber",&,N,V_inputWarmTint
T@"NSNumber",&,N,V_inputStrength
T@"NSNumber",&,N,V_inputHasFace
T@"NSNumber",&,N,V_inputIsRaw
_map
getMap
HDRFilterForSDRFilter:
alignmentKey
alignment
setAlignment:
globalSettings
IPXEditSettings
PUEditSettings
editSettings
falseColorHDR
setFalseColorHDR:
segmentationDebugRoundTripProxyImage
setSegmentationDebugRoundTripProxyImage:
disableSegmentation
setDisableSegmentation:
segmentationInfillAlgorithm
setSegmentationInfillAlgorithm:
segmentationDebugTintLayers
setSegmentationDebugTintLayers:
segmentationDisableCaching
setSegmentationDisableCaching:
segmentationDebugPreviewDisableClock
setSegmentationDebugPreviewDisableClock:
segmentationDebugPreviewHighQuality
setSegmentationDebugPreviewHighQuality:
segmentationManualGatingLenience
setSegmentationManualGatingLenience:
styleRecipeConfigDirectoryPath
setStyleRecipeConfigDirectoryPath:
useStyleRecipeConfigDirectory
setUseStyleRecipeConfigDirectory:
parallaxLayoutConfigurationOverride
setParallaxLayoutConfigurationOverride:
parallaxWallpaperDisableUpgrade
setParallaxWallpaperDisableUpgrade:
cinematicAllowYUVSourceInput
setCinematicAllowYUVSourceInput:
cinematicAllowRGB10Packed
setCinematicAllowRGB10Packed:
parallaxStyleKeyLevelOverride
setParallaxStyleKeyLevelOverride:
parallaxStyleAvoidColorWashBrownOverride
setParallaxStyleAvoidColorWashBrownOverride:
forceGlassesMatteOff
setForceGlassesMatteOff:
forceSpillMatteOff
setForceSpillMatteOff:
allowSpillMatteOnOlderPortraitV2Captures
setAllowSpillMatteOnOlderPortraitV2Captures:
_forceGlassesMatteOff
_forceSpillMatteOff
_allowSpillMatteOnOlderPortraitV2Captures
TB,N,V_forceGlassesMatteOff
TB,N,V_forceSpillMatteOff
TB,N,V_allowSpillMatteOnOlderPortraitV2Captures
Tq,N,VparallaxStyleKeyLevelOverride
TB,N,VparallaxStyleAvoidColorWashBrownOverride
initWithVideoExportRequest:
renderer:
shouldUseMetalRenderer
metalRenderer
initWithAutoLoopExportRequest:
autoLoopExportRequest
setUpContext:
newPhotosPipeline:
newPhotosPipelineAtSourceURL:error:
apertureRedEyeResultFromFaceObservations:imageSize:
_faceRequest
localLightHDRStatisticsNoProxy
inputLightMap
inputLightMapWidth
inputLightMapHeight
_shadowKernelHDR
_polyKernelHDR
inputLightMapImage
inputSmartShadows
updateSegmentationResource:
supportsSegmentationResourceCaching
segmentationResourceURL
loadParallaxResource:options:resultHandler:
cancelParallaxResourceRequest:
loadPetsRegions:
cancelPetsRegionsRequest:
localIdentifier
initWithFileURL:
cacheURL
setCacheURL:
isInCloud
setIsInCloud:
_acceptableRect
_preferredRect
_isInCloud
_cacheURL
T@"NSURL",R,N,V_fileURL
T@"NSURL",&,N,V_cacheURL
TB,N,V_isInCloud
T{CGRect={CGPoint=dd}{CGSize=dd}},R,N,V_acceptableRect
T{CGRect={CGPoint=dd}{CGSize=dd}},R,N,V_preferredRect
initWithBytes:width:height:bytesPerRow:bytesPerComponent:format:colorspace:
initWithData:width:height:bytesPerRow:bytesPerComponent:format:colorspace:
bytes
elementByteSize
rowElements
format
bufferColorspace
T@"NSMutableData",&,Vdata
TQ,R,VelementByteSize
TQ,R,VrowElements
TQ,R,Vwidth
TQ,R,Vheight
Ti,R,Vformat
_calculateBlackAndWhiteSettingsFromBufferImage:
getNonNormalizedSettings:
createHueArray
hueArrayImage:
smartBlackWhiteKernel
inputNeutralGamma
setInputNeutralGamma:
inputTone
setInputTone:
inputHue
setInputHue:
inputGrain
setInputGrain:
inputSeed
setInputSeed:
inputScaleFactor
setInputScaleFactor:
T@"NSNumber",C,N,VinputStrength
T@"NSNumber",C,N,VinputNeutralGamma
T@"NSNumber",C,N,VinputTone
T@"NSNumber",C,N,VinputHue
T@"NSNumber",C,N,VinputGrain
T@"NSNumber",C,N,VinputSeed
T@"NSNumber",C,N,VinputScaleFactor
smartBlackAndWhiteStatistics
smartBlackAndWhiteAdjustmentsForValue:andStatistics:
inputCorrectionInfoKey
hasCorrections
assetIsCinematicVideo:
currentSystemCanRenderAsset:
currentSystemRenderingVersion
cinematicMetadataFromAsset:
renderingVersionFromAsset:error:
size
CVPixelBuffer
T{?=qq},R,N
T@"NUPixelFormat",R,N
T@"NUColorSpace",R,N
T^{__CVBuffer=},R,N
initWithPixelBuffer:renderTask:
pixelBuffer
renderTask
renderInfo
_pixelBuffer
_renderInfo
T@"<NUImageBuffer>",R,N,V_pixelBuffer
T@"CIRenderTask",R,N,V_renderTask
T@"CIRenderInfo",R,N,V_renderInfo
initWithImage:format:colorSpace:
sizeInBytes
render:error:
waitForRender:
cachedImage
_cachedImage
T@"CIImage",R,N,V_image
T@"NUPixelFormat",R,N,V_pixelFormat
T@"NUColorSpace",R,N,V_colorSpace
T@"CIImage",R,N,V_cachedImage
scaleForImageSize:
initWithLayout:
T@"PFParallaxLayout",R,N,V_layout
newRenderedPixelBufferFromImage:hasAlpha:error:
layerForBuffer:image:zPosition:identifier:
cachedImage:forKey:
initWithParallaxLayerStackRequest:
layerStackRequest
requestLayout
mode
effectiveLayout
backfillScalePolicy
deviceScalePolicy
debugTintedImage:isBackfill:
imageByCachingImage:format:colorSpace:key:
cacheImage:key:format:colorSpace:
foregroundImage
setForegroundImage:
foregroundBuffer
setForegroundBuffer:
backgroundBuffer
setBackgroundBuffer:
debugImageCollector
setDebugImageCollector:
layers
setLayers:
_cachedImageEntries
_foregroundImage
_foregroundBuffer
_backgroundBuffer
_debugImageCollector
_layers
T@"<PISegmentationItem>",R,N
T@"PIParallaxStyle",R,N
T@"CIImage",&,N,V_foregroundImage
T@"<NUImageBuffer>",&,N,V_foregroundBuffer
T@"<NUImageBuffer>",&,N,V_backgroundBuffer
T@"_PIParallaxLayerStackDebugImageCollector",&,N,V_debugImageCollector
T@"NSArray",C,N,V_layers
layerStackMode
setLayerStackMode:
_layerStackMode
Tq,N,V_layerStackMode
T@"NSCache",&,N,V_cache
prepareImagesForItem:renderer:layout:style:inputImage:matteImage:infillImage:foregroundImage:backgroundImage:
renderImagesWithRenderer:
resultLayersWithRenderer:
debugInputImage
setDebugInputImage:
debugMatteImage
setDebugMatteImage:
debugMatteCropImage
setDebugMatteCropImage:
debugLocalConfidenceImage
setDebugLocalConfidenceImage:
debugConfidenceMapImage
setDebugConfidenceMapImage:
debugInfillImage
setDebugInfillImage:
debugLayoutImage
setDebugLayoutImage:
debugIntermediateLayoutImages
setDebugIntermediateLayoutImages:
debugPreviewImage
setDebugPreviewImage:
debugColorAnalysisImage
setDebugColorAnalysisImage:
debugInputBuffer
setDebugInputBuffer:
debugMatteBuffer
setDebugMatteBuffer:
debugMatteCropBuffer
setDebugMatteCropBuffer:
debugLocalConfidenceBuffer
setDebugLocalConfidenceBuffer:
debugConfidenceMapBuffer
setDebugConfidenceMapBuffer:
debugInfillBuffer
setDebugInfillBuffer:
debugLayoutBuffer
setDebugLayoutBuffer:
debugIntermediateLayoutBuffers
setDebugIntermediateLayoutBuffers:
debugPreviewBuffer
setDebugPreviewBuffer:
debugColorAnalysisBuffer
setDebugColorAnalysisBuffer:
flattenedBackgroundForDebugPreview
setFlattenedBackgroundForDebugPreview:
flattenedForegroundForDebugPreview
setFlattenedForegroundForDebugPreview:
_debugInputImage
_debugMatteImage
_debugMatteCropImage
_debugLocalConfidenceImage
_debugConfidenceMapImage
_debugInfillImage
_debugLayoutImage
_debugIntermediateLayoutImages
_debugPreviewImage
_debugColorAnalysisImage
_debugInputBuffer
_debugMatteBuffer
_debugMatteCropBuffer
_debugLocalConfidenceBuffer
_debugConfidenceMapBuffer
_debugInfillBuffer
_debugLayoutBuffer
_debugIntermediateLayoutBuffers
_debugPreviewBuffer
_debugColorAnalysisBuffer
_flattenedBackgroundForDebugPreview
_flattenedForegroundForDebugPreview
T@"CIImage",&,N,V_debugInputImage
T@"CIImage",&,N,V_debugMatteImage
T@"CIImage",&,N,V_debugMatteCropImage
T@"CIImage",&,N,V_debugLocalConfidenceImage
T@"CIImage",&,N,V_debugConfidenceMapImage
T@"CIImage",&,N,V_debugInfillImage
T@"CIImage",&,N,V_debugLayoutImage
T@"NSArray",&,N,V_debugIntermediateLayoutImages
T@"CIImage",&,N,V_debugPreviewImage
T@"CIImage",&,N,V_debugColorAnalysisImage
T@"<NUImageBuffer>",&,N,V_debugInputBuffer
T@"<NUImageBuffer>",&,N,V_debugMatteBuffer
T@"<NUImageBuffer>",&,N,V_debugMatteCropBuffer
T@"<NUImageBuffer>",&,N,V_debugLocalConfidenceBuffer
T@"<NUImageBuffer>",&,N,V_debugConfidenceMapBuffer
T@"<NUImageBuffer>",&,N,V_debugInfillBuffer
T@"<NUImageBuffer>",&,N,V_debugLayoutBuffer
T@"NSArray",&,N,V_debugIntermediateLayoutBuffers
T@"<NUImageBuffer>",&,N,V_debugPreviewBuffer
T@"<NUImageBuffer>",&,N,V_debugColorAnalysisBuffer
T@"<NUImageBuffer>",&,N,V_flattenedBackgroundForDebugPreview
T@"<NUImageBuffer>",&,N,V_flattenedForegroundForDebugPreview
calculateSettingsForImageHistogram:
calculateSettingsForSingleChannelHistogram:suffix:
_adjustmentControllerForKey:creatingIfNecessary:expectedClass:
smartToneAdjustmentController
smartToneAdjustmentControllerCreatingIfNecessary:
smartColorAdjustmentController
smartColorAdjustmentControllerCreatingIfNecessary:
smartBWAdjustmentController
smartBWAdjustmentControllerCreatingIfNecessary:
cropAdjustmentController
cropAdjustmentControllerCreatingIfNecessary:
redEyeAdjustmentController
redEyeAdjustmentControllerCreatingIfNecessary:
livePhotoKeyFrameAdjustmentController
livePhotoKeyFrameAdjustmentControllerCreatingIfNecessary:
videoPosterFrameAdjustmentController
videoPosterFrameAdjustmentControllerCreatingIfNecessary:
depthAdjustmentController
depthAdjustmentControllerCreatingIfNecessary:
trimAdjustmentController
trimAdjustmentControllerCreatingIfNecessary:
slomoAdjustmentController
slomoAdjustmentControllerCreatingIfNecessary:
effectAdjustmentController
effectAdjustmentControllerCreatingIfNecessary:
effect3DAdjustmentController
effect3DAdjustmentControllerCreatingIfNecessary:
portraitAdjustmentController
portraitAdjustmentControllerCreatingIfNecessary:
orientationAdjustmentController
orientationAdjustmentControllerCreatingIfNecessary:
autoLoopAdjustmentController
autoLoopAdjustmentControllerCreatingIfNecessary:
highResFusionAdjustmentController
highResFusionAdjustmentControllerCreatingIfNecessary:
rawAdjustmentController
rawAdjustmentControllerCreatingIfNecessary:
rawNoiseReductionAdjustmentController
rawNoiseReductionAdjustmentControllerCreatingIfNecessary:
sharpenAdjustmentController
sharpenAdjustmentControllerCreatingIfNecessary:
whiteBalanceAdjustmentController
whiteBalanceAdjustmentControllerCreatingIfNecessary:
noiseReductionAdjustmentController
noiseReductionAdjustmentControllerCreatingIfNecessary:
definitionAdjustmentController
definitionAdjustmentControllerCreatingIfNecessary:
vignetteAdjustmentController
vignetteAdjustmentControllerCreatingIfNecessary:
videoReframeAdjustmentController
videoReframeAdjustmentControllerCreatingIfNecessary:
sourceSelectAdjustmentController
sourceSelectAdjustmentControllerCreatingIfNecessary:
videoStabilizeAdjustmentController
videoStabilizeAdjustmentControllerCreatingIfNecessary:
videoCrossfadeLoopAdjustmentController
videoCrossfadeLoopAdjustmentControllerCreatingIfNecessary:
semanticEnhanceAdjustmentController
semanticEnhanceAdjustmentControllerCreatingIfNecessary:
portraitVideoAdjustmentController
portraitVideoAdjustmentControllerCreatingIfNecessary:
T@"PIAdjustmentConstants",R,N
initWithX:y:editable:
initWithDictionary:
isEditable
_editable
Td,R,N,V_x
Td,R,N,V_y
editable
TB,R,N,GisEditable,V_editable
smartToneHDRStatistics
_kernelBneg
_kernelBpos
_kernelC
_kernelC_hdr
_kernelH
_kernelRH
T@"NSNumber",&,N,VinputExposure
T@"NSNumber",&,N,VinputBrightness
T@"NSNumber",&,N,VinputShadows
T@"NSNumber",&,N,VinputHighlights
T@"NSNumber",&,N,VinputBlack
T@"NSNumber",&,N,VinputRawHighlights
availableBlendModes
blendKernelForBlendMode:
inputBlendMode
setInputBlendMode:
_inputBlendMode
T@"NSNumber",&,N,V_inputIntensity
T@"NSString",&,N,V_inputBlendMode
loadFusionTuningParameters
debugDumpIntermediateImages
_debugDumpIntermediateImages
alignImage:transform:extent:
_computeNCCMapFromImage:toImage:scale:
_refineMaskImage:guideImage:scale:
_fuseImage:withGuideImage:weightImage:maskImage:
inputStillImage
setInputStillImage:
inputMaskImage
setInputMaskImage:
inputRenderScale
setInputRenderScale:
inputVideoScale
setInputVideoScale:
inputAlignmentExtent
setInputAlignmentExtent:
inputAlignmentTransform
setInputAlignmentTransform:
_inputStillImage
_inputMaskImage
_inputRenderScale
_inputVideoScale
_inputAlignmentExtent
_inputAlignmentTransform
T@"CIImage",&,N,V_inputStillImage
T@"CIImage",&,N,V_inputMaskImage
T@"NSNumber",&,N,V_inputRenderScale
T@"NSNumber",&,N,V_inputVideoScale
T@"CIVector",&,N,V_inputAlignmentExtent
T@"CIVector",&,N,V_inputAlignmentTransform
isEnabled
_processedRenderNodeForComposition:input:pipelineState:error:
evaluate:input:pipelineState:error:
P3Kernel
warmUpResources
ensureResources
freeResources
_ensureResources
_freeResources
proxyScalePolicy
renderPriorityForResourcePriority:
segmentationCompositionForAssetResource:
segmentationCompositionForProxyImage:orientation:
segmentationCompositionForImageURL:fileUTI:orientation:proxyImage:
segmentationSourceForImageURL:fileUTI:orientation:proxyImage:
saveSegmentationItem:toURL:error:
loadSegmentationItemFromURL:error:
_loadSegmentationItemFromURL:error:
saveSegmentationItem:layerStackOptions:configuration:style:layout:toWallpaperURL:completion:
generateLayerStackForItem:style:layout:options:completion:
_saveSegmentationItem:layerStack:toWallpaperURL:completion:
saveSegmentationItem:layerStack:toWallpaperURL:error:
loadSegmentationItemFromWallpaperURL:error:
_loadSegmentationItemFromWallpaperURL:error:
loadLayerStackFromWallpaperURL:options:error:
renderPreviewLayerStackFromWallpaperURL:styleCategory:completion:
reloadSegmentationItemFromWallpaperURL:asset:completion:
initWithParallaxAsset:
initWithSegmentationItem:parallaxAsset:
loadSegmentationItemWithCompletion:
_didLoad:completion:
_load:
_loadItem:completion:
_handlePartialItem:loadingState:
_abort:
_loadProxyResource:completion:
_loadFullSizeResource:completion:
_loadAssetResourceForProxy:completion:
_loadAssetResource:options:completion:
_loadSegmentationData:completion:
_segment:completion:
_performSegmentation:type:completion:
_isValidSegmentationMatteHistogramForDepth:
_analyzeColors:completion:
_loadBackground:completion:
_loadRegions:completion:
_performLayout:completion:
_loadLocalLightData:completion:
_tryLoadSegmentationItemFromCache:
_cacheSegmentationDataForItem:
asset
disableRendering
setDisableRendering:
proxyOnly
setProxyOnly:
disableDownload
setDisableDownload:
petsRegions
setPetsRegions:
petsFaceRegions
setPetsFaceRegions:
downloadProgressHandler
setDownloadProgressHandler:
loadingHandler
setLoadingHandler:
loadingHandlerQueue
setLoadingHandlerQueue:
_signpost
_isLoading
_isCancelled
_loadRequestID
_petsRequestID
_loadingError
_renderContext
_item
_disableSegmentation
_disableRendering
_proxyOnly
_disableDownload
_asset
_petsRegions
_petsFaceRegions
_downloadProgressHandler
_loadingHandler
_loadingHandlerQueue
T@"<PFParallaxAsset>",R,N,V_asset
TB,N,V_disableSegmentation
TB,N,V_disableRendering
TB,N,V_proxyOnly
TB,N,V_disableDownload
Tq,N,V_priority
T@"NSArray",C,N,V_petsRegions
T@"NSArray",C,N,V_petsFaceRegions
T@?,C,N,V_downloadProgressHandler
T@?,C,N,V_loadingHandler
T@"NSObject<OS_dispatch_queue>",&,N,V_loadingHandlerQueue
T{CGRect={CGPoint=dd}{CGSize=dd}},R,N,VacceptableCropRect
T{CGRect={CGPoint=dd}{CGSize=dd}},R,N,VpreferredCropRect
T@"NSArray",R,N,VfaceRegions
T@"NSArray",R,N,VpetRegions
inactiveRect
setInactiveRect:
_inactiveRect
T{CGRect={CGPoint=dd}{CGSize=dd}},N,V_inactiveRect
initWithParallaxLayoutInactiveFrameRequest:
layoutInactiveFrameRequest
T@"PIParallaxLayoutInactiveFrameRequest",R,N
sourceSelectSchema
rawSchema
rawNoiseReductionSchema
smartToneSchema
smartColorSchema
smartBlackAndWhiteSchema
grainSchema
sharpenSchema
cropSchema
trimSchema
slomoSchema
livePhotoKeyFrameSchema
muteSchema
videoPosterFrameSchema
autoLoopSchema
highResFusionSchema
depthEffectSchema
effect3DSchema
portraitEffectSchema
effectSchema
redEyeSchema
apertureRedEyeSchema
retouchSchema
vignetteSchema
orientationSchema
definitionSchema
noiseReductionSchema
whiteBalanceSchema
levelsSchema
curvesSchema
selectiveColorSchema
videoReframeSchema
videoStabilizeSchema
videoCrossfadeLoopSchema
debugSchema
semanticEnhance
portraitVideoSchema
photosCompositionSchema
registeredPhotosSchemaIdentifier
registerPhotosSchema
T@"NUIdentifier",R
descriptionForCandidacy:
performNextActionWithCompletion:
shouldPerformAction:
hasPerformedAction:
markActionAsPerformed:
performHorizonCorrectionWithCompletion:
performPerspectiveCorrectionWithCompletion:
shouldAllowPerspectiveCorrection
processHorizonResult:
processPerspectiveResult:
candidacy
setCandidacy:
finalizerError
setFinalizerError:
performedActions
setPerformedActions:
rollAngleDegrees
setRollAngleDegrees:
pitchAngleDegrees
setPitchAngleDegrees:
yawAngleDegrees
setYawAngleDegrees:
_candidacy
_finalizerError
_performedActions
_rollAngleDegrees
_pitchAngleDegrees
_yawAngleDegrees
T@"NSError",&,N,V_finalizerError
TQ,N,V_performedActions
Td,N,V_rollAngleDegrees
Td,N,V_pitchAngleDegrees
Td,N,V_yawAngleDegrees
TQ,N,V_candidacy
initWithDisposition:composition:
disposition
_disposition
Tq,R,V_disposition
T@"NUComposition",R,C,V_composition
cropFraction
setCropFraction:
setAnalysisType:
imageProperties:
undoExifOrientation:error:
shouldPerformAutoCrop
setShouldPerformAutoCrop:
shouldPerformAutoStraighten
setShouldPerformAutoStraighten:
shouldUseAutoStraightenVerticalDetector
setShouldUseAutoStraightenVerticalDetector:
autoStraightenVerticalAngleThreshold
setAutoStraightenVerticalAngleThreshold:
autoStraightenDominantAngleDiffThreshold
setAutoStraightenDominantAngleDiffThreshold:
maxAutoStraighten
setMaxAutoStraighten:
minAutoStraighten
setMinAutoStraighten:
_shouldPerformAutoCrop
_shouldPerformAutoStraighten
_shouldUseAutoStraightenVerticalDetector
_autoStraightenVerticalAngleThreshold
_autoStraightenDominantAngleDiffThreshold
_maxAutoStraighten
_minAutoStraighten
TB,V_shouldPerformAutoCrop
TB,V_shouldPerformAutoStraighten
TB,V_shouldUseAutoStraightenVerticalDetector
T@"NSNumber",C,V_autoStraightenVerticalAngleThreshold
T@"NSNumber",C,V_autoStraightenDominantAngleDiffThreshold
Td,V_maxAutoStraighten
Td,V_minAutoStraighten
stitchedOvercaptureRect:primaryRect:forComposition:error:
overcaptureRectForStitchedOvercaptureSize:overcaptureVideoComplementSize:primarySize:autoLoopStabilizedCropRect:
updateCropAdjustment:after:error:
inputColorKey
inputSaturationKey
inputCastKey
offsetSaturationKey
offsetCastKey
attributeVibrancyKey
attributeCastKey
_updateSettingsWithInputColor:
setInputSaturation:
inputSaturation
setOffsetCast:
offsetCast
setOffsetSaturation:
offsetSaturation
_stats
iptFromLinearInto:fromRed:green:blue:
hueAngleFrom:
iptHueAngleFromRed:green:blue:
selectiveColorKernels
convertToIPT:
convertFromIPT:
hueSatLumTable
inputCorrections
setInputCorrections:
_inputCorrections
T@"NSArray",&,N,V_inputCorrections
initWithKeyframes:stabCropRect:input:
_evaluateVideoProperties:
_evaluateImageGeometry:
canPropagateOriginalLivePhotoMetadataTrack
_stabilizeImage:cleanRect:cropRect:transform:geometry:
keyframeSequence
setKeyframeSequence:
setStabCropRect:
inputVideoProperties
setInputVideoProperties:
frameDuration
setFrameDuration:
shouldApplyWatermark
setShouldApplyWatermark:
_shouldApplyWatermark
_keyframeSequence
_inputVideoProperties
_frameDuration
T@"PIReframeKeyframeSequence",&,N,V_keyframeSequence
T{?={?=qq}{?=qq}},N,V_stabCropRect
T@"<NUVideoProperties>",&,N,V_inputVideoProperties
T{?=qiIq},N,V_frameDuration
TB,N,V_shouldApplyWatermark
pi_imageByApplyingStabilizationWatermark
setSubjects:
setEstimatedCenterMotion:
setEstimatedMotionBlur:
setTrajectoryHomography:
subjects
estimatedCenterMotion
estimatedMotionBlur
trajectoryHomography
_subjects
_estimatedCenterMotion
_estimatedMotionBlur
_trajectoryHomography
T{?=qiIq},R,V_time
T@"NSArray",R,V_subjects
T{CGVector=dd},R,V_estimatedCenterMotion
T{CGVector=dd},R,V_estimatedMotionBlur
T{?=[3]},R,V_trajectoryHomography
canProvideMetadataForAVAsset:
initWithAVAsset:
overwriteTrackingMetadataWithPlist:
subjectsFromMetadata:
centerMotionVectorFromMetadata:
motionBlurVectorFromMetadata:
trajectoryeHomographyFromMetadata:containsV3Metadata:
extractMetadata
timedMetadataArray
_videoTrack
_mdataTrack
ndcMetadataTransform
pxlMetadataTransform
T@"NSArray",R,N,VtimedMetadataArray
RGBToYIQKernel
YIQToRGBKernel
whiteBalanceKernel
isDefaultWarmth:
warmth
setWarmth:
setY:
setI:
setQ:
_strength
_warmth
T@"NSNumber",&,N,V_strength
T@"NSNumber",&,N,V_warmth
T@"NSNumber",&,N,V_y
T@"NSNumber",&,N,V_i
T@"NSNumber",&,N,V_q
submitSynchronous:
_options
force
setForce:
_force
TB,V_force
initWithComposition:location:touchDiameter:
_location
_touchDiameter
outputCropRect
Tf,N,V_inputThreshold
_interpolateGrainKernel
_grainBlendAndMixKernel
_paddedTileKernel
inputISO
setInputISO:
inputAmount
setInputAmount:
T@"NSNumber",C,N,VinputISO
T@"NSNumber",C,N,VinputAmount
computeSegmentationScoresForAsset:options:completion:
loadSegmentationItemForAsset:options:completion:
cancelSegmentationForAsset:
curatedSegmentationGatingDecisionForSegmentationScores:
manualSegmentationGatingDecisionForSegmentationScores:
layoutGatingDecisionForSegmentationScores:
tryLoadSegmentationForColdAsset:
setSegmentationLoader:forAsset:
segmentationLoaderForAsset:
exportWallpaperForAsset:toURL:options:completion:
_layerStackOptionsFromOptions:
_styleFromOptions:item:
upgradeWallpaperAtURL:exportToURL:options:completion:
_upgradeWallpaperAtURL:exportToURL:options:completion:
_upgradeFullPosterAtURL:exportToURL:options:completion:
upgradePosterConfiguration:atURL:exportTo:options:completion:
writeImage:fileURL:
writeCGImage:fileURL:
writeCGImage:fileURL:options:
writeImage:toTemporaryDirectoryWithBasename:
writeImage:toDirectoryAtPath:withBasename:
edgesKey
ROIForCenterPoint:radius:
convertFloat:toFixed16:count:
convertFixed16:toFloat:count:
inputSpots
valuesAtCaptureFromImageProperties:error:
initFromMinAperture:maxAperture:aperture:portraitStrength:SDOFRenderingVersion:depthVersionInfo:
portraitStrength
setPortraitStrength:
minimumAperture
setMinimumAperture:
maximumAperture
setMaximumAperture:
SDOFRenderingVersion
setSDOFRenderingVersion:
portraitMajorVersion
setPortraitMajorVersion:
portraitMinorVersion
setPortraitMinorVersion:
depthVersionInfo
setDepthVersionInfo:
_portraitStrength
_minimumAperture
_maximumAperture
_SDOFRenderingVersion
_portraitMajorVersion
_portraitMinorVersion
_depthVersionInfo
Tf,N,V_aperture
Tf,N,V_portraitStrength
T@"NSNumber",&,N,V_minimumAperture
T@"NSNumber",&,N,V_maximumAperture
TQ,N,V_SDOFRenderingVersion
TQ,N,V_portraitMajorVersion
TQ,N,V_portraitMinorVersion
T{?=ii},N,V_depthVersionInfo
focusRectDictionaryFromMetadata:
focusRectDictionaryFromRect:
isStillImageDisparity:
canApplyPortraitEffectsWithMetadata:
portraitInfoDictionaryFromFaceObservations:metadata:orientation:valuesAtCapture:
depthEffectInfoDictionaryFromFaceObservations:metadata:orientation:valuesAtCapture:
depthEffectInfoDictionaryFromFaceObservations:focus:valuesAtCapture:lumaNoiseScale:orientation:
portraitEffectInfoDictionaryFromFaceObservations:orientation:valuesAtCapture:
portraitInfoDictionaryFromCameraMetadata:
_calculateWithImageProperties:valuesAtCapture:completion:
autoCropFilter
exifOrientationAndCropStraightenOnly
rawFaceBalanceFilter
rawSourceFilterIncludingOrientation
sourceFilterNoOrientation
sushiLevel1Filter
noRedEyeFilter
noTrimFilter
noMuteFilter
noCropFilter
iosCropToolFilter
stripAllTimeAdjustmentsFilter
noGeometryFilter
noOrientationFilter
orientationAsMetaDataFilter
perspectiveStraightenWithoutCropFilter
preGeometryFilter
postGeometryFilter
inputToCropFilter
histogramOptimizationFilter
stopAtTagIncludeGeometryFilter:
stopAtTagIncludeOrientationFilter:
applyOrientationFilter
autoloopStabilizedVideoFilter
overcaptureSourceFilter
primarySourceFilter
spatialOvercaptureVideoSourceFilter
oneShotPortraitV2ExportFilter
socPseudoColorFilter
colorTypeForString:
stringForColorType:
colorTypeKey
faceStrengthKey
faceWarmthKey
faceIKey
faceQKey
grayStrengthKey
grayWarmthKey
grayYKey
grayIKey
grayQKey
temperatureKey
tintKey
warmTempKey
warmTintKey
warmFaceKey
colorType
setColorType:
faceStrength
setFaceStrength:
faceWarmth
setFaceWarmth:
faceI
setFaceI:
faceQ
setFaceQ:
grayStrength
setGrayStrength:
grayWarmth
setGrayWarmth:
grayY
setGrayY:
grayI
setGrayI:
grayQ
setGrayQ:
warmTemp
setWarmTemp:
warmTint
setWarmTint:
warmFace
setWarmFace:
calcExtentsForStrokeRadius:offset:inputImageExtent:maskExtent:repairExtent:textureExtent:sourceExtent:
brushStrokeFromRetouchStrokeDictionary:
extractRGBAfPixelsFromImage:fromROI:
prepareForCIFilterWithFaces:cropRect:
applyRepairMLStrokeToMutableBuffer:brushStroke:detectedFaces:context:error:
applyRepairStrokeToMutableBuffer:brushStroke:sourceOffset:repairEdges:
newWatchInfillFromImage:mask:
overcaptureRectForAutoCrop:overcaptureVideoComplementSize:primarySize:CGRect:
remapPortraitV2Strength:portraitEffectKind:
videoReframe:reframes:error:
videoCrossfadeLoop:crossfadeAdjustment:error:
portraitVideo:disparityInput:disparityKeyframes:apertureKeyframes:debugMode:error:
portraitVideoDebugDetectedObjects:source:cinematographyState:monochrome:error:
versionForPortraitEffect:
performLongExposureFusionForComposition:longExposureImage:useHDRFilter:error:
performApertureRedeyeOnImage:useHDRFilter:redEyeAdjustment:
performRedeyeOnImage:useHDRFilter:redEyeAdjustment:
nodeByApplyingFilterWithName:useHDRFilter:settingsAndInputs:
debugNodeByApplyingFilterWithName:useHDRFilter:settingsAndInputs:
depthInfoKey
apertureKey
glassesMatteAllowedKey
canRenderDepth
canAdjustApertureValue
setDepthInfo:
depthInfo
capturedAperture
setGlassesMatteAllowed:
glassesMatteAllowed
keyframesKey
stabCropRectKey
setKeyframes:
copyKeyframesTrimmingToTimeRange:
T{?={?=qq}{?=qq}},N
newHLGPixelBufferFromSDRImage:
_renderImage:toPixelBuffer:
_newHLGPixelBufferOfSize:
recycleHLGPixelBuffer:
inverseHLGLumaBlendingKernel
dictionariesFromPoints:
_defaultCurveArray
autoValuesForBlackPoint:whitePoint:
computeCurvesForImageHistogram:
curvePointAtIndex:blackPoint:whitePoint:histogram:
B24@0:8@16
#16@0:8
@16@0:8
@24@0:8:16
@32@0:8:16@24
@40@0:8:16@24@32
B16@0:8
B24@0:8#16
B24@0:8:16
Vv16@0:8
Q16@0:8
^{_NSZone=}16@0:8
B24@0:8@"Protocol"16
@"NSString"16@0:8
@"<NURenderStatistics>"16@0:8
@"<NUImageBuffer>"16@0:8
v24@0:8@16
v16@0:8
@"<NUImageBuffer>"
@24@0:8@16
B24@0:8o^@16
@"NUStorageImageBuffer"
@"CIRenderTask"
@"CIImage"
@24@0:8^{_NSZone=}16
q16@0:8
v24@0:8@?16
v20@0:8B16
@"<NUScalePolicy>"
@"NUPixelFormat"
@"NUColorSpace"
@"PTCinematographyTrack"16@0:8
@"PTCinematographyTrack"
v76@0:8{?=qiIq}16{CGRect={CGPoint=dd}{CGSize=dd}}40f72
{?=qiIq}16@0:8
v40@0:8{?=qiIq}16
{CGPoint=dd}16@0:8
v32@0:8{CGPoint=dd}16
@?16@0:8
{CGPoint="x"d"y"d}
{?="value"q"timescale"i"flags"I"epoch"q}
@64@0:8@16{?=qiIq}24{CGPoint=dd}48
f16@0:8
v20@0:8f16
@40@0:8@16q24^{CGColorSpace=}32
@"PFStoryRecipeDisplayAssetNormalization"
B32@0:8@16@24
v24@0:8d16
d16@0:8
v32@0:8q16d24
v32@0:8@16q24
@20@0:8f16
{CGRect={CGPoint=dd}{CGSize=dd}}16@0:8
v48@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16
{CGRect="origin"{CGPoint="x"d"y"d}"size"{CGSize="width"d"height"d}}
@80@0:8@16{?=qiIq}24{CGRect={CGPoint=dd}{CGSize=dd}}48
@"NSNumber"
@160@0:8@16@24Q32{CGRect={CGPoint=dd}{CGSize=dd}}40{CGSize=dd}72{CGRect={CGPoint=dd}{CGSize=dd}}88{CGRect={CGPoint=dd}{CGSize=dd}}120@152
@144@0:8Q16{CGRect={CGPoint=dd}{CGSize=dd}}24{CGSize=dd}56{CGRect={CGPoint=dd}{CGSize=dd}}72{CGRect={CGPoint=dd}{CGSize=dd}}104@136
@32@0:8Q16@24
@40@0:8Q16@24d32
@"CIContext"
@32@0:8@16o^@24
@"NSArray"16@0:8
@96@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{CGRect={CGPoint=dd}{CGSize=dd}}48@80@88
@"NSArray"
@96@0:8@16{CGSize=dd}24@40Q48@56@64^d72^d80^B88
v24@0:8Q16
@"NSString"
@"PFParallaxLayout"
@"<PFParallaxLayoutConfiguration>"
@"<PISegmentationItem>"
@32@0:8@16@24
v24@0:8q16
i20@0:8i16
B48@0:8@16@24@32^@40
v40@0:8@16@24@32
v32@0:8@16@24
@"<MTLTexture>"
@40@0:8@16@24o^@32
B40@0:8@16@24o^@32
@40@0:8@16@24@32
Q32@0:8@16@?24
Q24@0:8@16
@24@0:8Q16
@"NSIndexSet"
#24@0:8@16
v32@0:8@16@?24
B28@0:8@16B24
B36@0:8@16@24B32
B40@0:8@16@24@?32
@"NUComposition"
{?="hasDidAdd"B"hasDidRemove"B"hasDidUpdate"B"hasDidUpdateMultiple"B"hasClassForController"B}
@"NSDictionary"
@"<PICompositionControllerDelegate>"
@48@0:8@16@24@32o^@40
B32@0:8@16o^@24
@56@0:8@16@24@32@40o^@48
@44@0:8@16@24B32o^@36
B48@0:8@16@24@32o^@40
@36@0:8@16B24o^@28
@"NSData"
d40@0:8@16d24^B32
@32@0:8@16q24
{CGPoint=dd}72@0:8{CGPoint=dd}16{CGRect={CGPoint=dd}{CGSize=dd}}32q64
{CGPoint=dd}32@0:8r^{CGPoint=dd}16Q24
@56@0:8@16@24@32@40@48
@"NSURL"
{?={?=qq}{?=qq}}16@0:8
v48@0:8{?={?=qq}{?=qq}}16
{?="origin"{?="x"q"y"q}"size"{?="width"q"height"q}}
v32@0:8@16i24B28
i16@0:8
{CGRect={CGPoint=dd}{CGSize=dd}}60@0:8i16@20{CGRect={CGPoint=dd}{CGSize=dd}}28
@108@0:8@16@24@32@40@48@56@64@72@80@88B96o^@100
@"NUCVPixelBuffer"
@"PTGlobalRenderingMetadata"
@"PIPortraitVideoMetadataSample"
@24@0:8q16
@120@0:8@16{?={?=qiIq}{?=qiIq}}24{?=qiIq}72{?=qiIq}96
@24@0:8o^@16
{?={?=qiIq}{?=qiIq}}16@0:8
{?="start"{?="value"q"timescale"i"flags"I"epoch"q}"duration"{?="value"q"timescale"i"flags"I"epoch"q}}
v40@0:8@16@24d32
{?=qiIq}32@0:8@16@24
@"NSMutableDictionary"
@"NUIdentifier"
@"NUAdjustment"
@"<PIParallaxFilterCache>"
@24@0:8d16
@"PFParallaxLayerStack"
@56@0:8@16@24@32@40q48
{?=qq}32@0:8{?=qq}16
i32@0:8{?=qq}16
^{__CVBuffer=}32@0:8@16o^@24
@"NSObject<OS_dispatch_group>"
@"NSObject<OS_dispatch_queue>"
@"<NUFaceDetectionResult>"
@"NSURL"24@0:8@"NSString"16
@"NSBundle"
q24@0:8@16
@"PFParallaxColor"
@"PFParallaxColor"16@0:8
v24@0:8@"PFParallaxColor"16
@48@0:8@16@24@32@40
@"PIParallaxStyleRecipe"
@48@0:8@16{?=qiIq}24
@24@0:8@"NSDictionary"16
@"NSDictionary"16@0:8
@48@0:8{?=qiIq}16d40
v24@0:8@"NSCoder"16
@24@0:8@"NSCoder"16
@48@0:8q16q24q32d40
@"PFParallaxLayerStyle"24@0:8@"NSString"16
@"NUComposition"16@0:8
@"<PFParallaxAssetRegions>"16@0:8
@"PFParallaxLayout"16@0:8
@"<PFParallaxLayoutConfiguration>"16@0:8
@"PIParallaxColorAnalysis"16@0:8
@"PFParallaxLayerStyle"16@0:8
@"NSURL"16@0:8
B48@0:8@16^B24^B32o^@40
@"PFParallaxAssetResource"
@"<PFParallaxAssetRegions>"
@"PIParallaxColorAnalysis"
@"PISegmentationContextInfo"
@32@0:8@16#24
v72@0:8{?={?=qiIq}{?=qiIq}}16@64
v40@0:8d16d24@?32
@"NURuleSystem"
@88@0:8{?=qiIq}16{?=[3]}40
{?=[3]}16@0:8
{?="columns"[3]}
{?=[3]}40@0:8{?=qiIq}16
@"NUKeyframeSequenceMatrixFloat33"
{?=ffff}16@0:8
v32@0:8{?=ffff}16
@"<NUPurgeableStorage>"
@"NUImageHistogram"
{?="r"f"g"f"b"f"a"f}
@56@0:8@16d24d32d40o^@48
@"NUPurgeableStoragePool"
@"NSMutableArray"
@"AVAsset"
@72@0:8@16{?={?=qq}{?=qq}}24Q56@64
{?={?=qq}{?=qq}}96@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{?={?=qq}{?=qq}}48d80d88
{?=ddd}56@0:8@16{?={?=qq}{?=qq}}24
@28@0:8@16B24
{?={?=[4d]}{?=[4d]}d}16@0:8
@88@0:8{?={?=[4d]}{?=[4d]}d}16
{?=[4d]}16@0:8
@48@0:8{?=[4d]}16
B48@0:8{?=[4d]}16
{?=[4d]}48@0:8{?=[4d]}16
{?=[4d]}88@0:8{?={?=[4d]}{?=[4d]}d}16
{?=dd}104@0:8{?={?=[4d]}{?=[4d]}d}16@88d96
{?={?=[4d]}{?=[4d]}d}48@0:8@16@24d32@40
@"NUBufferRenderClient"
@"NUImageDataClient"
@32@0:8@16^@24
B40@0:8@16@24^@32
@"NSArray"32@0:8@16^@24
B40@0:8@16@"NSMutableDictionary"24^@32
@"NSNumber"32@0:8@"NSDictionary"16^@24
B40@0:8@"NSNumber"16@"NSMutableDictionary"24^@32
{?=qq}16@0:8
v32@0:8{?=qq}16
@"NUImageExportRequest"
{?="width"q"height"q}
@"NUImageGeometry"
@"NUPriority"
@"NUImageExportFormat"
v48@0:8@16@24@32@?40
v40@0:8@16@24@?32
@48@0:8@16@24@32@?40
@72@0:8@16@24@32@40@48@56@?64
@48@0:8@16@24@32^@40
v64@0:8@16@24@32@40@48@?56
{CGRect={CGPoint=dd}{CGSize=dd}}24@0:8@16
{CGRect={CGPoint=dd}{CGSize=dd}}56@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16@48
{CGRect={CGPoint=dd}{CGSize=dd}}64@0:8q16{CGRect={CGPoint=dd}{CGSize=dd}}24@56
@48@0:8Q16Q24Q32@40
@40@0:8Q16Q24Q32
@"PFParallaxLayerStack"16@0:8
v40@0:8q16@24@?32
@"NSError"
@"PIParallaxStyle"
v64@0:8{?={?=qiIq}{?=qiIq}}16
@56@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24
@32@0:8@16d24
@40@0:8@16{CGSize=dd}24
{PISegmentationBimodalScore=fff}24@0:8@16
f64@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24@56
f32@0:8@16@24
{PISegmentationClockOverlapResult=@Qdd}72@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16@48@56@64
{PISegmentationClockOverlapResult=@Qdd}76@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16@48@56@64B72
d64@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16@48@56
d88@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{CGSize=dd}48@64@72@80
d96@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{CGSize=dd}48@64@72^d80@88
{CGRect={CGPoint=dd}{CGSize=dd}}72@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16d48{CGPoint=dd}56
d120@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{CGPoint=dd}48{PISegmentationClockOverlapResult=@Qdd}64@96@104@112
d80@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16^B48@56@64@72
{CGRect={CGPoint=dd}{CGSize=dd}}96@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{CGSize=dd}48d64@72@80@88
{PISegmentationInactiveResult={CGRect={CGPoint=dd}{CGSize=dd}}{CGRect={CGPoint=dd}{CGSize=dd}}}92@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{CGSize=dd}48B64@68@76@84
@104@0:8@16@24@32{CGRect={CGPoint=dd}{CGSize=dd}}40{CGRect={CGPoint=dd}{CGSize=dd}}72
@112@0:8@16@24@32@40{CGRect={CGPoint=dd}{CGSize=dd}}48{CGRect={CGPoint=dd}{CGSize=dd}}80
@64@0:8@16@24{CGRect={CGPoint=dd}{CGSize=dd}}32
@40@0:8@16Q24o^@32
@40@0:8@16I24I28o^@32
@40@0:8@16@24#32
@"PTTimedRenderingMetadata"
v84@0:8@16{?=qq}24{?=qq}40i56q60@68@?76
v76@0:8@16{?=qq}24{?=qq}40i56q60@68
@68@0:8@16{?=qq}24{?=qq}40i56q60
@"PTRenderPipeline"
@"<PTRenderState>"
@"<MTLDevice>"
@"NSDate"
v48@0:8q16^d24^d32^d40
@"PIFaceObservationCache"16@0:8
v24@0:8@"PIFaceObservationCache"16
v36@0:8@16@24B32
f24@0:8@16
B32@0:8{?=qq}16
B40@0:8@16{?=qq}24
@"PIFaceObservationCache"
B32@0:8{CGSize=dd}16
{?="exposure"d"contrast"d"brightness"d"shadows"d"highlights"d"black"d"rawHighlights"d"localLight"d}
@36@0:8@16@24B32
@52@0:8@16@24@32q40B48
@44@0:8@16{CGSize=dd}24B40
^{CGImage=}24@0:8@16
B32@0:8@16^q24
@40@0:8@16Q24^{CGColorSpace=}32
@64@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16@48d56
@48@0:8{?=qq}16@32Q40
@64@0:8@16@24{?={?=qq}{?=qq}}32
B60@0:8@16i24^{CGColorSpace=}28@36@44o^@52
@"<NURenderer>"
@"<NUSurfaceStorage>"
@"NSObject<OS_dispatch_semaphore>"
@"VNImageHomographicAlignmentObservation"16@0:8
@"VNImageHomographicAlignmentObservation"
@64@0:8d16d24d32d40d48d56
B28@0:8{PISegmentationBimodalScore=fff}16
@44@0:8@16@24@32B40
@60@0:8@16@24B32{CGSize=dd}36q52
@"PTCinematographyScript"
@"NSCache"
@"CIColor"
^{CGColorSpace=}16@0:8
{vector<float, std::allocator<float>>=^f^f{__compressed_pair<float *, std::allocator<float>>=^f}}24@0:8@16
@48@0:8r^f16r^f24r^f32r^f40
@40@0:8@16{?=qq}24
@"NUFaceDetectionRequest"
v24@0:8@?<B@?@"NSURL">16
i40@0:8q16@24@?32
v20@0:8i16
i24@0:8@?16
i40@0:8q16@"PFParallaxAssetResourceOptions"24@?<v@?@"PFParallaxAssetResource"@"NSError">32
i24@0:8@?<v@?@"NSArray"@"NSArray"@"NSError">16
@68@0:8^v16Q24Q32q40Q48i56^{CGColorSpace=}60
@68@0:8@16Q24Q32q40Q48i56^{CGColorSpace=}60
^v16@0:8
^{CGColorSpace=}
@"NSMutableData"
v24@0:8^{?=Bffff[3f]}16
^f16@0:8
@24@0:8^f16
@32@0:8d16@24
Q32@0:8@16o^@24
^{__CVBuffer=}16@0:8
@"NUPixelFormat"16@0:8
@"NUColorSpace"16@0:8
@"CIRenderInfo"
@48@0:8@16@24d32@40
@"<NUImageBuffer>"36@0:8@"CIImage"16B24o^@28
@"PFParallaxLayer"48@0:8@"<NUImageBuffer>"16@"CIImage"24d32@"NSString"40
@"CIImage"32@0:8@"CIImage"16@"NSString"24
@"_PIParallaxLayerStackDebugImageCollector"
v88@0:8@16@24@32@40@48@56@64@72@80
@36@0:8@16B24#28
@20@0:8B16
@36@0:8d16d24B32
@104@0:8@16{?=[3]}24{CGRect={CGPoint=dd}{CGSize=dd}}72
@40@0:8@16@24d32
@"CIVector"
@32@0:8^{CGImage=}16q24
@48@0:8@16@24q32^{CGImage=}40
@72@0:8@16Q24@32@40@48@56@?64
@56@0:8@16@24@32Q40@?48
@40@0:8@16Q24^@32
@40@0:8@16@24@?32
v32@0:8@16Q24
v28@0:8B16@?20
v40@0:8@16q24@?32
@"NURenderContext"
@"PIParallaxSegmentationItem"
@"<PFParallaxAsset>"
B24@0:8Q16
@32@0:8q16@24
B32@0:8^{?={?=qq}{?=qq}}16o^@24
B48@0:8^{CGRect={CGPoint=dd}{CGSize=dd}}16^{CGRect={CGPoint=dd}{CGSize=dd}}24@32o^@40
{CGRect={CGPoint=dd}{CGSize=dd}}96@0:8{?=qq}16{?=qq}32{?=qq}48{CGRect={CGPoint=dd}{CGSize=dd}}64
{?="p75"d"p98"d"autoValue"d"g98"d}
{?="sat"d"contrast"d"cast"d}
v36@0:8^f16f24f28f32
f24@0:8r^f16
d40@0:8d16d24d32
@64@0:8@16{?={?=qq}{?=qq}}24@56
@144@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24{CGRect={CGPoint=dd}{CGSize=dd}}56{?=[3]}88@136
@"PIReframeKeyframeSequence"
@"<NUVideoProperties>"
v32@0:8{CGVector=dd}16
v64@0:8{?=[3]}16
{CGVector=dd}16@0:8
{CGVector="dx"d"dy"d}
@24@0:8r^{FigLivePhotoMetadata=I{FigLivePhotoMetadataV1Struct=fqffffffccSI[0{FigLivePhotoDetectedFaceV1Struct=qffffisS}]}}16
{CGVector=dd}24@0:8r^{FigLivePhotoMetadata=I{FigLivePhotoMetadataV1Struct=fqffffffccSI[0{FigLivePhotoDetectedFaceV1Struct=qffffisS}]}}16
{?=[3]}32@0:8r^{FigLivePhotoMetadata=I{FigLivePhotoMetadataV1Struct=fqffffffccSI[0{FigLivePhotoDetectedFaceV1Struct=qffffisS}]}}16^B24
@"AVAssetTrack"
{CGAffineTransform="a"d"b"d"c"d"d"d"tx"d"ty"d}
B24@0:8d16
@48@0:8@16{CGPoint=dd}24d40
v56@0:8@16@24@32@40@?48
B32@0:8^{CGImage=}16@24
B40@0:8^{CGImage=}16@24@32
{?={?=qq}{?=qq}}40@0:8{CGPoint=dd}16d32
v40@0:8r^f16^S24Q32
v40@0:8r^S16^f24Q32
@48@0:8f16f20f24f28Q32{?=ii}40
{?=ii}16@0:8
v24@0:8{?=ii}16
{?="major"i"minor"i}
@48@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16
@48@0:8@16@24q32@40
@52@0:8@16@24@32f40q44
@40@0:8@16q24@32
v128@0:8d16{?=qq}24{?={?=qq}{?=qq}}40{?={?=qq}{?=qq}}72^{?={?=qq}{?=qq}}104^{?={?=qq}{?=qq}}112^{?={?=qq}{?=qq}}120
@56@0:8@16{?={?=qq}{?=qq}}24
B56@0:8@16@24@32@40o^@48
v52@0:8@16@24{CGPoint=dd}32B48
^{CGImage=}32@0:8^{CGImage=}16^{CGImage=}24
@64@0:8@16@24@32@40q48o^@56
@52@0:8@16@24@32B40o^@44
@36@0:8@16B24@28
@64@0:8{?={?=qiIq}{?=qiIq}}16
^{__CVBuffer=}24@0:8^{CGImage=}16
B32@0:8@16^{__CVBuffer=}24
^{__CVBuffer=}32@0:8{CGSize=dd}16
v24@0:8^{__CVBuffer=}16
@32@0:8d16d24
{CGPoint=dd}44@0:8i16d20d28@36
ffffff
