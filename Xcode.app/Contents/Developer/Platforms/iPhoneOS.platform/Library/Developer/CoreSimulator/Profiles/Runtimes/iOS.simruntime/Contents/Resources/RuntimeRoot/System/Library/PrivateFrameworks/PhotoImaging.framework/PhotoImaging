?F]k
?YAH
?ffffff
@<,_
?ffffff
>333333
?UUUUUU
?______
?TTTTTT
?333333
yE>)\
Ww'&l
MbP?{
333333
?333333
?ffffff
?ffffff
?333333
333333
ffffff
333333
333333
@UUUUUU
@333333
?ffffff
z>UUUUUU
?YAH
?YAH
pCh?
zt?{
p?33
UUUUUU
?UUUUUU
4@333333
sU?gDi?
z?-C
Mb@?
UUU?
?~0d>
@(#)PROGRAM:PhotoImaging  PROJECT:Neutrino-440.0.140
0Xr
?g|_\
ADBE
mntrRGB XYZ 
;acspAPPL
none
-ADBE
cprt
2desc
kwtpt
bkpt
rTRC
gTRC
bTRC
rXYZ
gXYZ
bXYZ
text
Copyright 2000 Adobe Systems Incorporated
desc
Adobe WGG linear
XYZ 
XYZ 
curv
XYZ 
XYZ 
iXYZ 
#?6t>=
333333
?333333
?ffffff
Fail: %{public}@
job: %{public}@
Trace:
%{public}@
Trace:
%{public}@
Tap-to-track: client requested stop at %@
Tap-to-track: tracking lost at %@
PIColorNormalizationAnalysis
Tried to set sceneLabel to unsupported value: %ld
Setting bounding boxes based on orientation (%@) and observations: %@
Setting face bounding boxes based on orientation (%@) and observations: %@
class %@ is not the correct type, its superclass should be %@
Continue: %{public}@
Unable to serialize finalized composition: %{public}@
Failed to deserialize global rendering metadata: %{public}@
Failed to deserialize timed rendering metadata: %{public}@
Missing timed rendering metadata
Error executing command buffer '%{public}@': %{public}@
CPV rendering failure, returned status %d
Could not apply PIPortraitVideoProcessor: %@
Failed to prewarm portrait renderer: %{public}@
Failed to obtain video properties: %{public}@
facerect yiq = %.5f, %.5f, %.5f
Choosing gray world instead of gray edge
Brightness is %@, greenChannelPercentage is %f, Sushi: %@
aperture=%@, shutterSpeed=%@, iso=%@
Buffer sizes are not the same: %{public}@ vs %{public}@
Buffer size is %@, bytes per row is %ld, dark_thr=%f, sat_thr=%f, noise_thr=%f
all done summing, np_gw is %d, np_ge is %d
wp_ge {%f, %f, %f, %f} wp_gw {%f, %f, %f, %f}, Sushi? %@
RGB = %f, %f, %f
YIQ = %f, %f, %f from %f, %f, %f
Failed to export image to %@: %@
Failed to export image to data: %@
Failed to prepare video metadata: %@
Unexpected Live Photo export format pairing. Video codec (%{public}@) and image export format (%{public}@)
Export Composition: exportImage: %{public}@ / exportVideoComplement: %{public}@ / exportVideo: %{public}@ / exportVideoPosterFrame: %{public}@
Export Composition: exporting image
Export Composition: exporting image complete
Export Composition: exporting video complement
Export Composition: exporting video complement complete
Export Composition: exporting video
Export Composition: exporting video complete
Export Composition: exporting video poster frame
Export Composition: exporting video poster frame complete
Export Composition: exporting overcapture image
Export Composition: exporting overcapture image complete
Export Composition: exporting overcapture video
Export Composition: exporting overcapture video complete
Export Composition: waiting on notify
Export Composition: notify triggered
Export Composition: calling completion with error: %{public}@
Export Composition: callling completion with result: %{public}@
failed to render auxiliary image data: %@
[PICompositionExporter shouldTryVideoRotationFastPath] failed. Error:%@
NUVideoRotationExportRequest failed. Error:%@
Skipping applyVideoOrientationAsMetadata. Bounces and Autoloops are not supported. Falling back to burned-in orientation.
Skipping applyVideoOrientationAsMetadata. Flipped orientations are not supported. Falling back to burned-in orientation.
Failed to export video: %@
invalid format version: %@
PIPortraitVideoMetadataSample: unexpected number of items for identifier %@: %@
PIPortraitVideoMetadataSample: item %@ is not of expected class %@
Error unarchiving cameraInfo metadata: %@
CINE: allocating new PT renderer: %@
CINE: recycling unused PT renderer: %@ (%lu remaining)
Requesting forced cleanup of Vision caches
CPV asset editability: onAppleSilicon: %s, hasMetalDeviceForPortrait: %s, oniOS: %s
CPV assets aren't editable because there is no metal device with support for portrait rendering
CPV assets aren't editable because the device has no ANE
CPV assets are editable
HDR asset not editable because this device doesn't support 10-bit HEVC encoding
HDR asset not editable because this device doesn't support HDR
HDR asset not editable because editing not supported on DolbyVision 5
CPV asset checking for editability
CPV asset not editable because it's in a legacy, unsupported format
CPV asset editable due to override
CPV asset not editable because CPV assets are not editable on this device
CPV asset not editable because this asset is not playable on this device
CPV asset not editable because this device doesn't support HDR
CPV asset is editable
validation issue: recipe is blank on the autoloop adjustment
validation issue: no flavor specified on the autoloop adjustment
validation issue: Missing inputCorrectionInfo on a redeye adjustment
validation issue: Missing depthInfo on a depth adjustment
validation issue: Missing portraitInfo on a portrait adjustment
validation issue: invalid trim startTime or endTime
Can PLPhotoEditPFDataConverter interpret identifier %{public}@? %@. Version %{public}@? %@
waitUntilReadyForMoreData: waited for %0.1fms
Writing long-exposure image to %{public}@
Writing long-exposure motion mask to %{public}@
Still image node:
Still image geometry:
Still image:
clap=%@, crop=%@, fullSize=%@, videoScale=%@ => guide rect: %@
High-resolution image registration failure : %@
Loading long-exposure fusion tuning parameters from file: %@
Failed to load fusion tuning parameters.
Using fusion tuning parameters: {kNCCBlurHalfSize=%d, kNCCEdge0=%f, kNCCEdge1=%f, kBlendMaskThreshold0=%f, kBlendMaskThreshold1=%f}
Missing inputImage
Missing inputMaskImage
Missing inputStillImage
Missing inputRenderScale
Missing inputVideoScale
Missing inputAlignmentExtent
Malformed inputAlignmentExtent vector
Missing inputAlignmentTransform
Malformed inputAlignmentTransform vector
Writing intermediate long exposure fusion images to : %@
Evaluating pipeline as HDR input
Beginning finalization for candidates: %@
No overcapture source found in composition, removing any candidate bits for reframing
Finalizer result contains roll angle degrees: %f pitch angle degrees: %f yaw angle degrees: %f
Finalizer result did not contain a change
Received error while finalizing: %@
Starting horizon auto calculator
Finished horizon auto calculator: %@
Starting perspective auto calculator
Finished perspective auto calculator: %@
Could not write PICropAutoCalculator debug diagnostics. %@
Failed to compute overcapture rect %@
Can't find enabled video track in asset: %@
Asset does not contain Live Photo metadata track
Track does not contain V3 metadata
Error creating asset reader: %@
Can't add metadata output for asset
Failed to start reading asset
Starting metadata extraction
Finished metadata extraction
PISmartToneAutoCalculator submitSynchronous isHDRComposition: %s
PISmartToneAutoCalculator smartTone request submitting: %{public}@
PISmartToneAutoCalculator smartTone result: %{public}@
PISmartToneAutoCalculator localLight request submitting: %{public}@
PISmartToneAutoCalculator localLight result: %{public}@
PISmartToneAutoCalculator submit isHDRComposition: %s
PISmartToneAutoCalculator global result: %{public}@
PISmartToneAutoCalculator going to commit and wait
PISmartToneAutoCalculator committed
PISmartToneAutoCalculator completed
PISmartToneAutoCalculator error: %{public}@
PISmartToneAutoCalculator coalesced
Failed to apply red eye repair. error: %{public}@
PIPortraitAutoCalculator getMinMaxSimulatedAperture returned error:%i (minAperture:%f maxAperture:%f version:%d)
Cant get focusRect - Metadata dictionary missing fullSizeWith or fullSizeHeight:
Cant get focusRect - Metadata dictionary missing exif aux dictionary:
Cant get focusRect - Exif aux dictionary missing MWG region dictionary:
Cant get focusRect - MWG region dictionary missing region list:
Cant get focusRect - Region list does not contain a focus rect:
Cant get focusRect - Malformed focus rect dictionary:
Invalid focus rect: {%g,%g,%g,%g}
Depth effects not supported: %@
Couldn't deserialize face observations from metadata: %@, assuming empty, error: %@.
Invalid face indexes from metadata: %@, ignored. Error: %@
Couldn't deserialize face observations from metadata: %@
s-curve black: %f
s-curve point %d: (%f, %f)
s-curve white: %f
red curve: blackPoint = %f, whitePoint = %f
green curve: blackPoint = %f, whitePoint = %f
blue curve: blackPoint = %f, whitePoint = %f
Invalid parameter not satisfying: %s
-[PITapToTrackRenderJob prepare:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Autocalculators/PITapToTrackRequest.m
No metal device for request
Failed to find output video asset
Failed to make asset reader for video
Failed to read first frame for video
Failed to find rect to track at initial point
Failed to add detection and start tracking
Failed to get finalized track from tracking session
kernel vec4 _hdrOffsetPosBW(sampler image) { vec4 im = sample(image, samplerCoord(image)); float offset = (im.r + im.g + im.b) / 3.0f; offset = max(0.0f, offset - 1.0f); return vec4(offset, offset, offset, 0.0f); }
kernel vec4 _hdrOffsetPos(sampler image) { vec4 im = sample(image, samplerCoord(image)); float r = max(im.r - 1.0f, 0.0f); float g = max(im.g - 1.0f, 0.0f); float b = max(im.b - 1.0f, 0.0f); return vec4(r, g, b, 0.0f); }
s_hdrOffsetPos kernel is nil
+[PIPhotoEffectHDR kernel]_block_invoke
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Adjustments/HDR/PIPhotoEffectHDR.m
inputImage cannot be nil
-[PIPhotoEffectHDR outputImage]
inputImage
CIAdditionCompositing
inputBackgroundImage
s_hdrOffsetPosBlackAndWhite kernel is nil
+[PIPhotoEffectBlackAndWhiteHDR kernelBlackAndWhite]_block_invoke
-[PIPhotoEffectBlackAndWhiteHDR outputImage]
+[PIPhotoEffect3DHDR kernel]_block_invoke
-[PIPhotoEffect3DHDR outputImage]
inputThreshold
inputDepthMap
+[PIPhotoEffect3DBlackAndWhiteHDR kernelBlackAndWhite]_block_invoke
-[PIPhotoEffect3DBlackAndWhiteHDR outputImage]
PhotoImaging
PIColorNormalizationFilter
PITempTintFilter
temperature
inputTemperature
tint
inputTint
PIHighKey
CISmartToneFilter
CISmartColorFilter
normalization != nil
+[PIColorNormalizationFilter colorCubeForNormalization:dimension:targetColorSpace:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Adjustments/PIColorNormalizationFilter.m
colorSpace != NULL
PIColorNormalization
candidateForStill
candidateForVideo
candidateForReframe
candidateForPerspective
candidateForHorizon
addRule exception : %@
v24@?0@"NSString"8@?<v@?@"NURuleSystem">16
v24@?0@"NSString"8@"NSString"16
isStitched
validSubjects
hasHorizonLine
hasBuilding
state.stitched == true OR state.perfectlyStitched == true
state.stitchConfidence < $StitchConfidenceThreshold
state.buildingCount > 0
state.buildingConfidence > $BuildingConfidenceThreshold
state.subjectCount >= $SubjectCountMinThreshold
state.allSubjectsInsidePrimaryBounds == true
(state.subjectCount - state.intersectingSubjectCount) > $MaxDisparateSubjects
state.largeSubjectFace == true
state.largestSubjectArea <= $MinimumSubjectSize
state.horizonLinePresent == true
state.horizonLineAngleInDegrees < -$HorizonAngleInDegreesMaxThreshold OR state.horizonLineAngleInDegrees > $HorizonAngleInDegreesMaxThreshold
state.horizonLineAngleInDegrees > -$HorizonAngleInDegreesMinThreshold AND state.horizonLineAngleInDegrees < $HorizonAngleInDegreesMinThreshold
state.video == true
state.videoDuration < $VideoDurationLowerBound
state.videoDuration > $VideoDurationUpperBound
(state.videoDuration * $VideoDurationWithSubjectsRatio) > state.videoDurationWithSubjects
state.videoQualityScore < $VideoQualityScoreThreshold
(state.videoDuration * $VideoDurationBelowMinZoomRatio) < state.videoDurationBelowMinZoom
facts.isStitched == true AND facts.validSubjects == true
facts.isStitched == true AND facts.hasBuilding == true
facts.isStitched == true AND facts.hasHorizonLine == true
state.largeSubject == true
v16@?0@"NURuleSystem"8
state.backFacing == false
state.deviceIsStationary == true
state.video == true AND state.livePhoto == true
state.pano == true
state.reframingAllowed == false
state.hasAdjustments == true
facts.candidateForStill == true
facts.candidateForVideo == true
platedFood
sunriseSunset
genericLandscape
intensity
sceneLabel
sceneConfidence
faceBoundingBoxes
boundingBoxes
-[PIDisparitySampleJob render:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Autocalculators/PIDisparitySampleRequest.m
Failed to read video frame
Incompatible disparity buffer
inputStrength
inputScale
vec3 localContrast(vec3 im, vec3 shc, float amt) { float midAmt = amt; vec3 neg = min(im, 0.0); vec3 pos = max(im, 1.0)-1.0; im = clamp(im, 0.0, 1.0); float y = dot(im, vec3(0.3333)); y = sqrt(y); y = y*(1.0-y); im = sqrt(im); float pivot = sqrt(shc.g); float a = midAmt*y; float b = -pivot*a; vec3 pix = im.r * vec3(0.299*a) + im.g * vec3(0.587*a) + im.b * vec3(0.114*a) + im + vec3(b); im = mix(im, vec3(pivot), -y*midAmt); im = mix(im, pix, 0.8); im = max(im, 0.0); im *= im; im = im + neg + pos; return im; } vec3 localContrastHLG(vec3 im, vec3 shc, float hlg_scale, float amt) { return localContrast(im.rgb/hlg_scale, shc.rgb/hlg_scale, amt).rgb * hlg_scale; } kernel vec4 _localContrastHDR(__sample im, __sample shc, float hlg_scale, float amt) { float max_comp = max(im.r,max(im.g,im.b)); float threshold = 0.75 * hlg_scale; if (max_comp <= 1.0) { im.rgb = localContrast(im.rgb, shc.rgb, amt); } else if (max_comp < threshold) { vec3 retSDR = localContrast(im.rgb, shc.rgb, amt); vec3 retHDR = localContrastHLG(im.rgb, shc.rgb, hlg_scale, amt); float lerp_t = (max_comp - 1.0) / (threshold - 1.0); im.rgb = mix(retSDR, retHDR, lerp_t); } else { im.rgb = localContrastHLG(im.rgb, shc.rgb, hlg_scale, amt); } return im; }
CIColorMatrix
inputRVector
inputGVector
inputBVector
inputAVector
inputBiasVector
CIColorClamp
inputMinComponents
inputMaxComponents
CIEdgePreserveUpsampleFilter
inputSmallImage
inputSpatialSigma
inputLumaSigma
kernel vec4 highKey(__sample im, float str)
vec3 neg = min(im.rgb, 0.0);
vec3 pos = max(im.rgb, 1.0) - 1.0;
im = clamp(im, 0.0, 1.0);
vec4 im2 = 1.0-((im-1.0)*(im-1.0));
im2 = sqrt(im2);
float y = dot(im.rgb, vec3(.333333));
float y2 = sqrt(1.0-(y-1.0)*(y-1.0));
y2 = mix(y2, smoothstep(0.0, 1.0, y2), 0.5);
vec4 im3 = (y>0) ? im*y2/y : vec4(0.0, 0.0, 0.0, 1.0);
im3 = mix(im3, im2, .7*sqrt(y2));
im3 = mix(im, im3, sqrt(y));
im.rgb = mix(im.rgb, im3.rgb, str) + pos + neg;
return im;
high key kernel is nil
+[PIHighKey kernel]_block_invoke
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Adjustments/PIHighKey.m
-[PIHighKey outputImage]
inputStrength cannot be nil
smartTone
smartColor
smartBlackAndWhite
grain
autoEnhance
whiteBalance
cropStraighten
redEye
apertureRedEye
depthEffect
livePhotoKeyFrame
videoPosterFrame
trim
slomo
portraitEffect
orientation
autoLoop
highResFusion
retouch
vignette
sharpen
noiseReduction
definition
curves
levels
selectiveColor
effect
effect3D
mute
rawNoiseReduction
source
videoReframe
debug
sourceSelect
sourceSpatialOvercapture
portraitVideo
videoStabilize
videoCrossfadeLoop
semanticEnhance
enabled
value
primary
PICompositionController(0x%X): %@
v16@?0@"PISmartToneAdjustmentController"8
com.apple.photo
adjustmentFormatIdentifier
adjustmentFormatVersion
adjustmentData
PICompositionSerializer.m
Invalid parameter not satisfying: %@
adjustments
metadata
file://dummy.jpg
dummy
identifier
PICompositionSerializerDomain
Missing identifierName
Missing definition in conversion map for the adjustment key: 
settings
Unsupported adjustment: 
Configuration does not support Aperature adjustments.
Configuration does not support CTM adjustments.
nil identifierName
inputKeys
omitIfDisabled
Orientation
PICompositionSerializer-geometry
masterWidth
masterHeight
Serialization map has no entry for %@
+[PICompositionSerializer serializeComposition:versionInfo:serializerMetadata:error:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Controllers/Conversion/PICompositionSerializer.m
Serialization map does not contain identifierName
current
auto
formatIdentifier
formatVersion
versionInfo
Missing required key: %@
Value for key %@ has type %@; expected type %@
v32@?0@"NSString"8#16^B24
PICompositionSerializer
exception
dictionary
data is not an NSData: %@: %@
Adjustment for %@ is identity
+[PICompositionSerializer _sanitizeComposition:]
composition
CFBundleVersion
platform
buildNumber
appVersion
schemaRevision
kernel vec4 levelsHDR (sampler src, sampler LUT) { vec4 p,r; vec2 c1,c2; float f; p = sample(src, samplerCoord(src)); vec3 neg = min(p.rgb, 0.0); vec3 pos = max(p.rgb, 1.0)-1.0; p.rgb = clamp(p.rgb, 0.0, 1.0); f = p.r * 1024; c1 = vec2(floor(f)+0.5, 0.5); c2 = vec2(ceil(f)+0.5, 0.5); r = mix(sample(LUT,samplerTransform(LUT,c1)), sample(LUT,samplerTransform(LUT,c2)), fract(f)); p.r = r.r; f = p.g * 1024; c1 = vec2(floor(f)+0.5, 0.5); c2 = vec2(ceil(f)+0.5, 0.5); r = mix(sample(LUT,samplerTransform(LUT,c1)), sample(LUT,samplerTransform(LUT,c2)), fract(f)); p.g = r.g; f = p.b * 1024; c1 = vec2(floor(f)+0.5, 0.5); c2 = vec2(ceil(f)+0.5, 0.5); r = mix(sample(LUT,samplerTransform(LUT,c1)), sample(LUT,samplerTransform(LUT,c2)), fract(f)); p.b = r.b; p.rgb = max(p.rgb, 0.0); p.rgb = p.rgb + pos + neg; return p; }
inputBlackSrc
inputBlackDst
inputShadowSrc
inputShadowDst
inputMidSrc
inputMidDst
inputHilightSrc
inputHilightDst
inputWhiteSrc
inputWhiteDst
inputBlackSrcRGB
inputBlackDstRGB
inputShadowSrcRGB
inputShadowDstRGB
inputMidSrcRGB
inputMidDstRGB
inputHilightSrcRGB
inputHilightDstRGB
inputWhiteSrcRGB
inputWhiteDstRGB
inputBlackSrcRed
inputBlackDstRed
inputShadowSrcRed
inputShadowDstRed
inputMidSrcRed
inputMidDstRed
inputHilightSrcRed
inputHilightDstRed
inputWhiteSrcRed
inputWhiteDstRed
inputBlackSrcGreen
inputBlackDstGreen
inputShadowSrcGreen
inputShadowDstGreen
inputMidSrcGreen
inputMidDstGreen
inputHilightSrcGreen
inputHilightDstGreen
inputWhiteSrcGreen
inputWhiteDstGreen
inputBlackSrcBlue
inputBlackDstBlue
inputShadowSrcBlue
inputShadowDstBlue
inputMidSrcBlue
inputMidDstBlue
inputHilightSrcBlue
inputHilightDstBlue
inputWhiteSrcBlue
inputWhiteDstBlue
Green
Blue
Failed converting data to RGBAh: %ld
-[PILevelsFilterHDR _LUTImage]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Adjustments/HDR/PILevelsFilterHDR.m
version
score
/Master/Source
data != nil
+[PIColorNormalizationAutoCalculator autoCalculatorWithImageData:orientation:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Autocalculators/PIColorNormalizationAutoCalculator.m
-[PIColorNormalizationAutoCalculator submit:]
Feature Unavailable
No color normalization available
kernel vec4 _falseColorHDRDebug(sampler image, float cutoff){   vec4 im = sample(image, samplerCoord(image));   if (im.x < 0.0 && im.y < 0.0 && im.z < 0.0) {        return vec4(0.0 , 0.6 , 0.2 , 1.0);   }   if (im.x > cutoff && im.y > cutoff && im.z > cutoff) {        return vec4(1.0 , 0.0 , 1.0 , 1.0);   }   if (im.x > cutoff || im.y > cutoff || im.z > cutoff) {        return vec4(0.6 , 0.0 , 0.6 , 1.0);   }   return im;}
s_falseColorHDRDebugKernelSource kernel is nil
+[PIFalseColorHDRDebug kernel]_block_invoke
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Adjustments/HDR/PIFalseColorHDRDebug.m
-[PIFalseColorHDRDebug outputImage]
Initializer not available: -[%@ %@], use designated initializer instead.
-[PIAutoLoopExportRequest initWithRequest:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/AutoLoop/PIAutoLoopRequests.m
-[PIAutoLoopExportRequest initWithComposition:destinationURL:]
unexpected inputs
+[PIPortraitVideoProcessor processWithInputs:arguments:output:error:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Render/PIPortraitVideoFilter.m
expected a device
expected a texture
aperture
focusDistance
quality
isHDR
globalMetadata
Failed to deserialize global rendering metadata: %{public}@
timedMetadata
Failed to deserialize timed rendering metadata: %{public}@
Missing timed rendering metadata
v16@?0@"<MTLCommandBuffer>"8
Invalid index
+[PIPortraitVideoProcessor formatForInputAtIndex:]
imageExtents
+[PIPortraitVideoProcessor applyWithInputImage:disparityImage:globalMetadata:timedMetadata:aperture:focusedDisparity:quality:debugMode:isHDR:error:]
disparityImage != nil
NormStabilizeInstructions
stabCropRect
Width
Height
flavor
recipe
-[PIVideoCrossfadeLoopNode initWithSettings:inputs:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Video Stabilization/PIVideoCrossfadeLoopNode.m
input != nil
-[PIVideoCrossfadeLoopNode initWithInput:timeRange:crossfadeDuration:startTime:]
CMTIME_IS_VALID(crossfadeDuration)
CMTIMERANGE_IS_VALID(loopTimeRange)
crossfadeDuration
startTime
loopTimeRange
-[PIVideoCrossfadeLoopNode nodeByReplayingAgainstCache:pipelineState:error:]
Missing primary video frame
video
secondary
Missing secondary video frame
CIDissolveTransition
-[PIVideoCrossfadeLoopNode _evaluateVideo:]
[[AVMutableComposition alloc] init] failed.
No input video track found
Failed to update video track when inserting the pre-crossfade content with source range %@ from track %@ to time %@ in track %@.
Failed to update audio track when inserting the pre-crossfade content with source range %@ from track %@ to time %@ in track %@.
Failed to update video track when inserting the crossfade content with source range %@ from track %@ to time %@ in track %@.
Failed to update audio track when inserting the crossfade content with source range %@ from track %@ to time %@ in track %@.
Failed to update video track when inserting the post-crossfade content with source range %@ from track %@ to time %@ in track %@.
Failed to update audio track when inserting the post-crossfade content with source range %@ from track %@ to time %@ in track %@.
-[PIVideoCrossfadeLoopNode _evaluateVideoComposition:]
Unsupported video configuration
-[PIVideoCrossfadeLoopNode _evaluateAudioMix:]
time
scale
radius
falloff
Adjustment empty
-[PIAdjustmentController displayName]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Controllers/PIAdjustmentController.m
-[PIAdjustmentController inputKeys]
Adjustment does not have an enabled setting
-[PIAdjustmentController enabled]
-[PIAdjustmentController setEnabled:]
Adjustment does not have an auto setting
-[PIAdjustmentController isAuto]
-[PIAdjustmentController setIsAuto:]
PIAdjustmentController(0x%X): %@
__dominantInputSettingsKey
debugMode
disparityKeyframes
apertureKeyframes
NUScaleIsValid(scale)
-[PIPortraitVideoRenderNode _targetScaleForScale:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Render/PIPortraitVideoRenderNode.m
-[PIPortraitVideoRenderNode nodeByReplayingAgainstCache:pipelineState:error:]
state != nil
-[PIPortraitVideoRenderNode _prewarmPortraitRendererWithPipelineState:error:]
No device specified for prewarm
portraitVideoMetadata
mdta/com.apple.quicktime.disparity-float
PIPortraitVideoRenderNode: expected a non-nil portraitVideoMetadata sample
renderTime
renderQuality
sourceTransferFunction
Could not get the input image
Could not get the disparityImage
Could not get the timed metadata
Could not get the source transfer function
Could not get the render time
PIFaceObservationCache
PIFaceObservationCache-newRequest
spatialOvercapture
spatialOvercaptureFused
timeValue
timeScale
%@: t=%@, v=%f
<%@ %p %@ %@ id=%lu conf=%.2f>
  bounds=%@
  expandedBounds=%@
  edgeBleed=%@
type
confidence
edgeBleed
bounds
expandedBounds
Human Face
Human Body
Cat Body
Cat Head
Dog Body
Dog Head
Salient Object
Unknown
(%.4f, %.4f, %.4f, %.4f)
minX 
minY 
maxX 
maxY 
cinematographyState
homography
<%@:%p time:%@>
PILongExposureFusionAutoCalculator-videoProperties
pre-AutoLoop
-[PILongExposureFusionAutoCalculator submit:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Autocalculators/PILongExposureFusionAutoCalculator.m
AutoLoop not supported
kind
-[PIAutoLoopAutoCalculator submit:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Autocalculators/PIAutoLoopAutoCalculator.m
B32@?0@"AVMetadataItem"8Q16^B24
-[PITempTintFilter outputImage]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Adjustments/PITempTintFilter.m
-[PIAutoLoopAnalysisJob prepare:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/AutoLoop/PIAutoLoopJob+Analysis.m
unable to find video source node
-[PIAutoLoopAnalysisJob render:]
AutoLoop is not supported
-[PIVideoStabilizeRenderJob prepare:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Autocalculators/PIVideoStabilizeRequest.m
Stabilize request was cancelled
frameInstructions
rawTime
void CreateKeyframesFromHomographyDictionary(NSDictionary *__strong, NUPixelSize, NSMutableArray<PIReframeKeyframe *> *__strong, NUPixelRect *)
PIVideoStabilizeRequest.m
unexpected homography
Failure in ICSynthesizeAnalysis
Failure in ICAnalyzeInputMotion
No available analysis types were allowed
Failure in ICCalcCinematicL1Corrections
neutralGray
faceBalance
tempTint
warmTint
PIFaceBalanceAutoCalculator-imageProperties
+[PIFaceBalanceAutoCalculator calculateWithRequest:completion:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Autocalculators/PIWhiteBalanceAutoCalculators.m
CIFaceBalance
PIFaceBalanceAutoCalculator-calculate
WarmTemp
WarmTint
warmTemp
+[PIFaceBalanceAutoCalculator calculateRAWWithRequest:completion:]
PIFaceBalanceAutoCalculator-RAW-face-request
+[PIFaceBalanceAutoCalculator faceBalanceResultFromFaceObservations:request:error:]
PIWhiteBalanceAutoCalculator-face-balance
PIRAWFaceBalanceAutoCalculator.responseQueue
Failure in rendering image
OrigI
OrigQ
Warmth
Strength
WarmFace
unsupported format - incorrect white balance
+[PIFaceBalanceAutoCalculator faceBalanceFromFaceImage:forFaceRect:]
{?={?=[4d]}{?=[4d]}d}
{?=[4d]}
colorType
grayColor
-[PIWhiteBalanceAutoCalculator submit:]
PIWhiteBalanceAutoCalculator-imageProperties
PIFaceBalanceAutoCalculator.responseQueue
faceI
faceQ
faceWarmth
faceStrength
PIWhiteBalanceAutoCalculator
color
v16@?0@"<NUBuffer>"8
CIAreaAverage
PIWhiteColorCalculator-computeGreenPercentage
PIWhiteColorCalculator-grayWorld
PIWhiteColorCalculator-grayEdges
return Filter('CIEdges', { 'inputImage' : input, 'inputIntensity' : 1.0 });
Expected non-NULL pixels passed in
void customRGBtoYIQ(const CGFloat *, double *, double, double, double)
vec4 meaningBlur(vec4 im, vec4 b)
vec4 result = im;
float thresh = 0.1;
float g1 = max(max(im.r, im.g), im.b);
float g2 = dot(b.rgb, vec3(1.0/3.0));
float diff = max(g2-g1, -1.0);
diff = smoothstep(0.1-thresh, 0.1+thresh, diff);
result.rgb = mix(im.rgb, b.rgb, diff+0.5);
return result;
vec4 clarityNew(vec4 s, vec4 b, float intensity)
float sl = (s.r + s.g + s.b);
float bl = (b.r + b.g + b.b);
float dl = sl + (sl - bl) * intensity;
float mult = dl / max(sl, 0.0001);
mult = 1.571 * (mult - 1.0);
mult = mult / (1.0 + abs(mult));
mult += 1.0;
mult = clamp(mult, 1.0 - 0.5 * abs(intensity), 1.0 + 1.0 * abs(intensity));
s.rgb = s.rgb * mult;
return s;
kernel vec4 definition(sampler image, sampler blur, float intensity)
vec4 imgSample = sample(image, samplerCoord(image));
vec4 blurSample = sample(blur, samplerCoord(blur));
vec4 meaning = meaningBlur(imgSample, blurSample);
vec4 clarity = clarityNew(imgSample, meaning, intensity);
return clarity;
strength
neutral
tone
inputNeutralGamma
inputTone
inputGrain
inputHue
inputSeed
<%@:%p - priority: %@, color space: %@, scale policy: %@>
<%@:%p - priority: %@, color space: %@, scale policy: %@, video codec type: %@, bypass settings: %@, orientation as metadata: %@, hardware encoder: %@>
Export URL UTI (%@) does not match expected export format (%@)
-[PICompositionExporterImageOptions imageExportFormatForURL:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Util/PICompositionExporter.m
metadataConverter must be set
-[PICompositionExporter init]
PICompositionExporter-videoProperties
Expecting HEIF/HEVC or JPEG/H264 when exporting Live Photo still and video complement
Unexpected video codec when attempting to export Live Photo
Unexpected image export format when attempting to export Live Photo
_companion
unable to prepare image properties
PICompositionExporter-image
PICompositionExporter-auxPropertiesRequest
PICompositionExporter-%@
PICompositionExporter.imageProperties.transaction
Regions
-[PICompositionExporter addVideoProperties:composition:options:error:]
Unknown autoloop flavor
reference
PICompositionExporter-shouldTryVideoRotationFastPath-geometry
pitch
roll
xOrigin
yOrigin
width
height
PICompositionExporter-video
PICompositionExporter-noOrientation-videoProperties
float luminanceWeight(vec4 pix, vec4 center, float slope)
return max(1.0 + (slope * length(pix.rgb - center.rgb)), 0.0);
vec4 bilateralRow_1(sampler src, float slope, vec2 offset1, float weight1)
float diff1;
vec2 coord;
vec4 pix1, center;
coord = destCoord();
center = sample(src, samplerCoord(src));
pix1 = sample(src, samplerTransform(src, coord + offset1));
diff1 = luminanceWeight(pix1, center, slope) * weight1;
return vec4(diff1 * pix1.rgb, diff1);
kernel vec4 bilateralAdd_1(sampler src, sampler sums, float slope, vec2 offset1, float weight1, float sumWeight)
vec4 sum;
sum = sample(sums, samplerCoord(sums))*sumWeight;
return sum + bilateralRow_1(src, slope, offset1, weight1);
vec4 bilateralRow_2(sampler src, float slope, vec2 offset1, vec2 offset2, vec2 weight1)
float diff1, diff2;
vec2 coord;
vec4 pix1, pix2, center;
coord = destCoord();
center = sample(src, samplerCoord(src));
pix1 = sample(src, samplerTransform(src, coord + offset1));
pix2 = sample(src, samplerTransform(src, coord + offset2));
diff1 = luminanceWeight(pix1, center, slope) * weight1.x;
diff2 = luminanceWeight(pix2, center, slope) * weight1.y;
return vec4(diff1 * pix1.rgb + diff2 * pix2.rgb, diff1 + diff2);
kernel vec4 bilateralAdd_2(sampler src, sampler sums, float slope, vec2 offset1, vec2 offset2, vec2 weight1, float sumWeight)
vec4 sum;
sum = sample(sums, samplerCoord(sums))*sumWeight;
return sum + bilateralRow_2(src, slope, offset1, offset2, weight1);
vec4 bilateralRow_3(sampler src, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec3 weight1)
float diff1, diff2, diff3;
vec2 coord;
vec4 pix1, pix2, pix3, center;
coord = destCoord();
center = sample(src, samplerCoord(src));
pix1 = sample(src, samplerTransform(src, coord + offset1));
pix2 = sample(src, samplerTransform(src, coord + offset2));
pix3 = sample(src, samplerTransform(src, coord + offset3));
diff1 = luminanceWeight(pix1, center, slope) * weight1.x;
diff2 = luminanceWeight(pix2, center, slope) * weight1.y;
diff3 = luminanceWeight(pix3, center, slope) * weight1.z;
return vec4(diff1 * pix1.rgb + diff2 * pix2.rgb + diff3 * pix3.rgb, diff1 + diff2 + diff3);
kernel vec4 bilateralAdd_3(sampler src, sampler sums, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec3 weight1, float sumWeight)
vec4 sum;
sum = sample(sums, samplerCoord(sums))*sumWeight;
return sum + bilateralRow_3(src, slope, offset1, offset2, offset3, weight1);
vec4 bilateralRow_4(sampler src, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec2 offset4, vec4 weight1)
float diff1, diff2, diff3, diff4;
vec2 coord;
vec4 pix1, pix2, pix3, pix4, center;
coord = destCoord();
center = sample(src, samplerCoord(src));
pix1 = sample(src, samplerTransform(src, coord + offset1));
pix2 = sample(src, samplerTransform(src, coord + offset2));
pix3 = sample(src, samplerTransform(src, coord + offset3));
pix4 = sample(src, samplerTransform(src, coord + offset4));
diff1 = luminanceWeight(pix1, center, slope) * weight1.x;
diff2 = luminanceWeight(pix2, center, slope) * weight1.y;
diff3 = luminanceWeight(pix3, center, slope) * weight1.z;
diff4 = luminanceWeight(pix4, center, slope) * weight1.w;
return vec4(diff1 * pix1.rgb + diff2 * pix2.rgb + diff3 * pix3.rgb + diff4 * pix4.rgb,
diff1 + diff2 + diff3 + diff4);
kernel vec4 bilateralAdd_4(sampler src, sampler sums, float slope, vec2 offset1, vec2 offset2, vec2 offset3,
vec2 offset4, vec4 weight1, float sumWeight)
vec4 sum;
sum = sample(sums, samplerCoord(sums))*sumWeight;
return sum + bilateralRow_4(src, slope, offset1, offset2, offset3, offset4, weight1);
vec4 bilateralRow_5(sampler src, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec2 offset4, vec2 offset5,
vec4 weight1, float weight2)
float diff1, diff2, diff3, diff4, diff5;
vec2 coord;
vec4 pix1, pix2, pix3, pix4, pix5, center;
coord = destCoord();
center = sample(src, samplerCoord(src));
pix1 = sample(src, samplerTransform(src, coord + offset1));
pix2 = sample(src, samplerTransform(src, coord + offset2));
pix3 = sample(src, samplerTransform(src, coord + offset3));
pix4 = sample(src, samplerTransform(src, coord + offset4));
pix5 = sample(src, samplerTransform(src, coord + offset5));
diff1 = luminanceWeight(pix1, center, slope) * weight1.x;
diff2 = luminanceWeight(pix2, center, slope) * weight1.y;
diff3 = luminanceWeight(pix3, center, slope) * weight1.z;
diff4 = luminanceWeight(pix4, center, slope) * weight1.w;
diff5 = luminanceWeight(pix5, center, slope) * weight2;
return vec4(diff1 * pix1.rgb + diff2 * pix2.rgb + diff3 * pix3.rgb + diff4 * pix4.rgb + diff5 * pix5.rgb,
diff1 + diff2 + diff3 + diff4 + diff5);
kernel vec4 bilateralAdd_5(sampler src, sampler sums, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec2 offset4,
vec2 offset5, vec4 weight1, float weight2, float sumWeight)
vec4 sum;
sum = sample(sums, samplerCoord(sums))*sumWeight;
return sum + bilateralRow_5(src, slope, offset1, offset2, offset3, offset4, offset5, weight1, weight2);
vec4 bilateralRow_6(sampler src, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec2 offset4, vec2 offset5,
vec2 offset6, vec4 weight1, vec2 weight2)
float diff1, diff2, diff3, diff4, diff5, diff6;
vec2 coord;
vec4 pix1, pix2, pix3, pix4, pix5, pix6, center;
coord = destCoord();
center = sample(src, samplerCoord(src));
pix1 = sample(src, samplerTransform(src, coord + offset1));
pix2 = sample(src, samplerTransform(src, coord + offset2));
pix3 = sample(src, samplerTransform(src, coord + offset3));
pix4 = sample(src, samplerTransform(src, coord + offset4));
pix5 = sample(src, samplerTransform(src, coord + offset5));
pix6 = sample(src, samplerTransform(src, coord + offset6));
diff1 = luminanceWeight(pix1, center, slope) * weight1.x;
diff2 = luminanceWeight(pix2, center, slope) * weight1.y;
diff3 = luminanceWeight(pix3, center, slope) * weight1.z;
diff4 = luminanceWeight(pix4, center, slope) * weight1.w;
diff5 = luminanceWeight(pix5, center, slope) * weight2.x;
diff6 = luminanceWeight(pix6, center, slope) * weight2.y;
return vec4(diff1 * pix1.rgb + diff2 * pix2.rgb + diff3 * pix3.rgb + diff4 * pix4.rgb + diff5 * pix5.rgb + diff6 * pix6.rgb,
diff1 + diff2 + diff3 + diff4 + diff5 + diff6);
kernel vec4 bilateralAdd_6(sampler src, sampler sums, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec2 offset4,
vec2 offset5, vec2 offset6, vec4 weight1, vec2 weight2, float sumWeight)
vec4 sum;
sum = sample(sums, samplerCoord(sums))*sumWeight;
return sum + bilateralRow_6(src, slope, offset1, offset2, offset3, offset4, offset5, offset6, weight1, weight2);
vec4 bilateralRow_7(sampler src, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec2 offset4, vec2 offset5,
vec2 offset6, vec2 offset7, vec4 weight1, vec3 weight2)
float diff1, diff2, diff3, diff4, diff5, diff6, diff7;
vec2 coord;
vec4 pix1, pix2, pix3, pix4, pix5, pix6, pix7, center;
coord = destCoord();
center = sample(src, samplerCoord(src));
pix1 = sample(src, samplerTransform(src, coord + offset1));
pix2 = sample(src, samplerTransform(src, coord + offset2));
pix3 = sample(src, samplerTransform(src, coord + offset3));
pix4 = sample(src, samplerTransform(src, coord + offset4));
pix5 = sample(src, samplerTransform(src, coord + offset5));
pix6 = sample(src, samplerTransform(src, coord + offset6));
pix7 = sample(src, samplerTransform(src, coord + offset7));
diff1 = luminanceWeight(pix1, center, slope) * weight1.x;
diff2 = luminanceWeight(pix2, center, slope) * weight1.y;
diff3 = luminanceWeight(pix3, center, slope) * weight1.z;
diff4 = luminanceWeight(pix4, center, slope) * weight1.w;
diff5 = luminanceWeight(pix5, center, slope) * weight2.x;
diff6 = luminanceWeight(pix6, center, slope) * weight2.y;
diff7 = luminanceWeight(pix7, center, slope) * weight2.z;
return vec4(diff1 * pix1.rgb + diff2 * pix2.rgb + diff3 * pix3.rgb + diff4 * pix4.rgb + diff5 * pix5.rgb + diff6 * pix6.rgb + diff7 * pix7.rgb,
diff1 + diff2 + diff3 + diff4 + diff5 + diff6 + diff7);
kernel vec4 bilateralAdd_7(sampler src, sampler sums, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec2 offset4,
vec2 offset5, vec2 offset6, vec2 offset7, vec4 weight1, vec3 weight2, float sumWeight)
vec4 sum;
sum = sample(sums, samplerCoord(sums))*sumWeight;
return sum + bilateralRow_7(src, slope, offset1, offset2, offset3, offset4, offset5, offset6, offset7, weight1, weight2);
vec4 bilateralRow_8(sampler src, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec2 offset4, vec2 offset5, vec2 offset6,
vec2 offset7, vec2 offset8, vec4 weight1, vec4 weight2)
float diff1, diff2, diff3, diff4, diff5, diff6, diff7, diff8;
vec2 coord;
vec4 pix1, pix2, pix3, pix4, pix5, pix6, pix7, pix8, center;
coord = destCoord();
center = sample(src, samplerCoord(src));
pix1 = sample(src, samplerTransform(src, coord + offset1));
pix2 = sample(src, samplerTransform(src, coord + offset2));
pix3 = sample(src, samplerTransform(src, coord + offset3));
pix4 = sample(src, samplerTransform(src, coord + offset4));
pix5 = sample(src, samplerTransform(src, coord + offset5));
pix6 = sample(src, samplerTransform(src, coord + offset6));
pix7 = sample(src, samplerTransform(src, coord + offset7));
pix8 = sample(src, samplerTransform(src, coord + offset8));
diff1 = luminanceWeight(pix1, center, slope) * weight1.x;
diff2 = luminanceWeight(pix2, center, slope) * weight1.y;
diff3 = luminanceWeight(pix3, center, slope) * weight1.z;
diff4 = luminanceWeight(pix4, center, slope) * weight1.w;
diff5 = luminanceWeight(pix5, center, slope) * weight2.x;
diff6 = luminanceWeight(pix6, center, slope) * weight2.y;
diff7 = luminanceWeight(pix7, center, slope) * weight2.z;
diff8 = luminanceWeight(pix8, center, slope) * weight2.w;
return vec4(diff1 * pix1.rgb + diff2 * pix2.rgb + diff3 * pix3.rgb + diff4 * pix4.rgb + diff5 * pix5.rgb + diff6 * pix6.rgb + diff7 * pix7.rgb + diff8 * pix8.rgb,
diff1 + diff2 + diff3 + diff4 + diff5 + diff6 + diff7 + diff8);
kernel vec4 bilateralAdd_8(sampler src, sampler sums, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec2 offset4, vec2 offset5,
vec2 offset6, vec2 offset7, vec2 offset8, vec4 weight1, vec4 weight2, float sumWeight)
vec4 sum;
sum = sample(sums, samplerCoord(sums))*sumWeight;
return sum + bilateralRow_8(src, slope, offset1, offset2, offset3, offset4, offset5, offset6, offset7, offset8, weight1, weight2);
vec4 bilateralRow_9(sampler src, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec2 offset4, vec2 offset5, vec2 offset6, vec2 offset7,
vec2 offset8, vec2 offset9, vec4 weight1, vec4 weight2, float weight3)
float diff1, diff2, diff3, diff4, diff5, diff6, diff7, diff8, diff9;
vec2 coord;
vec4 pix1, pix2, pix3, pix4, pix5, pix6, pix7, pix8, pix9, center;
coord = destCoord();
center = sample(src, samplerCoord(src));
pix1 = sample(src, samplerTransform(src, coord + offset1));
pix2 = sample(src, samplerTransform(src, coord + offset2));
pix3 = sample(src, samplerTransform(src, coord + offset3));
pix4 = sample(src, samplerTransform(src, coord + offset4));
pix5 = sample(src, samplerTransform(src, coord + offset5));
pix6 = sample(src, samplerTransform(src, coord + offset6));
pix7 = sample(src, samplerTransform(src, coord + offset7));
pix8 = sample(src, samplerTransform(src, coord + offset8));
pix9 = sample(src, samplerTransform(src, coord + offset9));
diff1 = luminanceWeight(pix1, center, slope) * weight1.x;
diff2 = luminanceWeight(pix2, center, slope) * weight1.y;
diff3 = luminanceWeight(pix3, center, slope) * weight1.z;
diff4 = luminanceWeight(pix4, center, slope) * weight1.w;
diff5 = luminanceWeight(pix5, center, slope) * weight2.x;
diff6 = luminanceWeight(pix6, center, slope) * weight2.y;
diff7 = luminanceWeight(pix7, center, slope) * weight2.z;
diff8 = luminanceWeight(pix8, center, slope) * weight2.w;
diff9 = luminanceWeight(pix9, center, slope) * weight3;
return vec4(diff1 * pix1.rgb + diff2 * pix2.rgb + diff3 * pix3.rgb + diff4 * pix4.rgb + diff5 * pix5.rgb + diff6 * pix6.rgb + diff7 * pix7.rgb + diff8 * pix8.rgb + diff9 * pix9.rgb,
diff1 + diff2 + diff3 + diff4 + diff5 + diff6 + diff7 + diff8 + diff9);
kernel vec4 bilateralAdd_9(sampler src, sampler sums, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec2 offset4, vec2 offset5,
vec2 offset6, vec2 offset7, vec2 offset8, vec2 offset9, vec4 weight1, vec4 weight2, float weight3, float sumWeight)
vec4 sum;
sum = sample(sums, samplerCoord(sums))*sumWeight;
return sum + bilateralRow_9(src, slope, offset1, offset2, offset3, offset4, offset5, offset6, offset7, offset8, offset9,
weight1, weight2, weight3);
kernel vec4 bilateralFinalize(sampler sums)
vec4 sum;
sum = sample(sums, samplerCoord(sums));
return vec4(sum.rgb / max(sum.a, 0.001), 1.0);
kernel vec4 bilateralLoop2(sampler src, float slope, vec2 weights12)
vec2 coord;
vec4 center = sample(src, samplerCoord(src));
vec4 sum;
vec4 pix0, pix1, pix2;
vec4 pix3,       pix5;
vec4 pix6, pix7, pix8;
vec2 dx = samplerTransform(src, vec2( 1.0, 0.0)) - samplerTransform(src, vec2(0.0, 0.0));
vec2 dy = samplerTransform(src, vec2(-2.0, 1.0)) - samplerTransform(src, vec2(0.0, 0.0));
coord = samplerTransform(src, destCoord() + vec2(-1.0, -1.0));
pix0 = sample(src, coord);
coord = coord + dx;
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dy;
pix3 = sample(src, coord);
coord = coord + dx;
coord = coord + dx;
pix5 = sample(src, coord);
coord = coord + dy;
pix6 = sample(src, coord);
coord = coord + dx;
pix7 = sample(src, coord);
coord = coord + dx;
pix8 = sample(src, coord);
sum = (1.0 - min(1.0, (slope * length(pix0 - center)))) * (pix0 * weights12.y);
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights12.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights12.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights12.x) + sum;
sum =                                        center                            + sum;
sum = (1.0 - min(1.0, (slope * length(pix5 - center)))) * (pix5 * weights12.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix6 - center)))) * (pix6 * weights12.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix7 - center)))) * (pix7 * weights12.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix8 - center)))) * (pix8 * weights12.y) + sum;
return vec4(sum.rgb / sum.a, 1.0);
kernel vec4 bilateralLoop5(sampler src, float slope, vec4 weights1245)
vec2  coord;
vec4  center = sample(src, samplerCoord(src));
vec4  sum;
vec4  pix0, pix1, pix2, pix3, pix4;
vec2 dx = samplerTransform(src, vec2( 1.0, 0.0)) - samplerTransform(src, vec2(0.0, 0.0));
vec2 dy = samplerTransform(src, vec2(-4.0, 1.0)) - samplerTransform(src, vec2(0.0, 0.0));
coord = samplerTransform(src, destCoord() + vec2(-1.0, -2.0));
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dx;
pix3 = sample(src, coord);
coord = coord + dx + dy;
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights1245.w);
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights1245.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights1245.w) + sum;
pix0 = sample(src, coord);
coord = coord + dx;
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dx;
pix3 = sample(src, coord);
coord = coord + dx;
pix4 = sample(src, coord);
coord = coord + dy;
sum = (1.0 - min(1.0, (slope * length(pix0 - center)))) * (pix0 * weights1245.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights1245.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights1245.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights1245.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix4 - center)))) * (pix4 * weights1245.w) + sum;
pix0 = sample(src, coord);
coord = coord + dx;
pix1 = sample(src, coord);
coord = coord + dx + dx;
pix3 = sample(src, coord);
coord = coord + dx;
pix4 = sample(src, coord);
coord = coord + dy;
sum = (1.0 - min(1.0, (slope * length(pix0 - center)))) * (pix0 * weights1245.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights1245.x) + sum;
sum =                                        center                              + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights1245.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix4 - center)))) * (pix4 * weights1245.z) + sum;
pix0 = sample(src, coord);
coord = coord + dx;
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dx;
pix3 = sample(src, coord);
coord = coord + dx;
pix4 = sample(src, coord);
coord = coord + dx + dy;
sum = (1.0 - min(1.0, (slope * length(pix0 - center)))) * (pix0 * weights1245.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights1245.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights1245.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights1245.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix4 - center)))) * (pix4 * weights1245.w) + sum;
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dx;
pix3 = sample(src, coord);
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights1245.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights1245.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights1245.w) + sum;
return vec4(sum.rgb / sum.a, 1.0);
kernel vec4 bilateralLoop11(sampler src, float slope, vec4 weights1245, vec4 weights89AD)
vec2  coord;
vec4  center = sample(src, samplerCoord(src));
vec4  sum;
vec4  pix0, pix1, pix2, pix3, pix4, pix5, pix6;
vec2 dx = samplerTransform(src, vec2( 1.0, 0.0)) - samplerTransform(src, vec2(0.0, 0.0));
vec2 dy = samplerTransform(src, vec2(-6.0, 1.0)) - samplerTransform(src, vec2(0.0, 0.0));
coord = samplerTransform(src, destCoord() + vec2(-2.0, -3.0));
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dx;
pix3 = sample(src, coord);
coord = coord + dx;
pix4 = sample(src, coord);
coord = coord + dx;
pix5 = sample(src, coord);
coord = coord + dx + dy;
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights89AD.w);
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights89AD.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights89AD.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix4 - center)))) * (pix4 * weights89AD.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix5 - center)))) * (pix5 * weights89AD.w) + sum;
pix0 = sample(src, coord);
coord = coord + dx;
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dx;
pix3 = sample(src, coord);
coord = coord + dx;
pix4 = sample(src, coord);
coord = coord + dx;
pix5 = sample(src, coord);
coord = coord + dx;
pix6 = sample(src, coord);
coord = coord + dy;
sum = (1.0 - min(1.0, (slope * length(pix0 - center)))) * (pix0 * weights89AD.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights89AD.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights1245.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights1245.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix4 - center)))) * (pix4 * weights1245.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix5 - center)))) * (pix5 * weights89AD.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix6 - center)))) * (pix6 * weights89AD.w) + sum;
pix0 = sample(src, coord);
coord = coord + dx;
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dx;
pix3 = sample(src, coord);
coord = coord + dx;
pix4 = sample(src, coord);
coord = coord + dx;
pix5 = sample(src, coord);
coord = coord + dx;
pix6 = sample(src, coord);
coord = coord + dy;
sum = (1.0 - min(1.0, (slope * length(pix0 - center)))) * (pix0 * weights89AD.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights1245.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights1245.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights1245.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix4 - center)))) * (pix4 * weights1245.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix5 - center)))) * (pix5 * weights1245.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix6 - center)))) * (pix6 * weights89AD.z) + sum;
pix0 = sample(src, coord);
coord = coord + dx;
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dx + dx;
pix4 = sample(src, coord);
coord = coord + dx;
pix5 = sample(src, coord);
coord = coord + dx;
pix6 = sample(src, coord);
coord = coord + dy;
sum = (1.0 - min(1.0, (slope * length(pix0 - center)))) * (pix0 * weights89AD.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights1245.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights1245.x) + sum;
sum =                                        center                              + sum;
sum = (1.0 - min(1.0, (slope * length(pix4 - center)))) * (pix4 * weights1245.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix5 - center)))) * (pix5 * weights1245.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix6 - center)))) * (pix6 * weights89AD.y) + sum;
pix0 = sample(src, coord);
coord = coord + dx;
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dx;
pix3 = sample(src, coord);
coord = coord + dx;
pix4 = sample(src, coord);
coord = coord + dx;
pix5 = sample(src, coord);
coord = coord + dx;
pix6 = sample(src, coord);
coord = coord + dy;
sum = (1.0 - min(1.0, (slope * length(pix0 - center)))) * (pix0 * weights89AD.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights1245.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights1245.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights1245.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix4 - center)))) * (pix4 * weights1245.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix5 - center)))) * (pix5 * weights1245.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix6 - center)))) * (pix6 * weights89AD.z) + sum;
pix0 = sample(src, coord);
coord = coord + dx;
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dx;
pix3 = sample(src, coord);
coord = coord + dx;
pix4 = sample(src, coord);
coord = coord + dx;
pix5 = sample(src, coord);
coord = coord + dx;
pix6 = sample(src, coord);
coord = coord + dx + dy;
sum = (1.0 - min(1.0, (slope * length(pix0 - center)))) * (pix0 * weights89AD.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights89AD.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights1245.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights1245.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix4 - center)))) * (pix4 * weights1245.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix5 - center)))) * (pix5 * weights89AD.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix6 - center)))) * (pix6 * weights89AD.w) + sum;
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dx;
pix3 = sample(src, coord);
coord = coord + dx;
pix4 = sample(src, coord);
coord = coord + dx;
pix5 = sample(src, coord);
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights89AD.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights89AD.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights89AD.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix4 - center)))) * (pix4 * weights89AD.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix5 - center)))) * (pix5 * weights89AD.w) + sum;
return vec4(sum.rgb / sum.a, 1.0);
kernel vec4 convertFromRGBToLab(sampler src)
vec3 f;
vec4 pix, color;
pix = unpremultiply(sample(src, samplerCoord(src)));
color.xyz = pix.r * vec3(0.449695,  0.316251, 0.18452  )
+ pix.g * vec3(0.244634,  0.672034, 0.0833318)
+ pix.b * vec3(0.0251829, 0.141184, 0.922602 );
color.xyz *= vec3(1.052111, 1.0, 0.918417);
f = compare(color.xyz - 0.00885645, 7.787037 * color.xyz + 0.137931, pow(color.xyz, vec3(0.333333)));
color.r = 116.0 * f.y - 16.0;
color.g = 500.0 * (f.x - f.y);
color.b = 200.0 * (f.y - f.z);
color.rgb *= 0.005;
color.a = 1.0;
return color;
kernel vec4 convertFromLabToRGB(sampler src, sampler original)
vec3 f, cie;
vec4 color, pix, opix;
pix = sample(src, samplerCoord(src));
opix = sample(original, samplerCoord(original));
pix.rgb *= 200.0;
f.y = (pix.r + 16.0) / 116.0;
f.x = f.y + pix.g * 0.002;
f.z = f.y - pix.b * 0.005;
color.xyz = f * f * f;
cie = compare(color.xyz - 0.00885645, (f.xyz - 0.137931) / 7.787037, color.xyz);
cie *= vec3(0.95047, 1.0, 1.08883);
color.rgb = cie.x * vec3(2.95176,   -1.28951, -0.47388  )
+ cie.y * vec3(-1.0851,    1.99084,  0.0372023)
+ cie.z * vec3(0.0854804, -0.269456, 1.09113  );
color.a = opix.a;
return premultiply(color);
bilateralAdd_1
bilateralAdd_2
bilateralAdd_3
bilateralAdd_4
bilateralAdd_5
bilateralAdd_6
bilateralAdd_7
bilateralAdd_8
bilateralAdd_9
bilateralFinalize
convertFromRGBToLab
convertFromLabToRGB
points.count > 0
-[GUBilateralConvolution boundsForPointArray:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Adjustments/PIBilateralFilter.m
bilateralLoop2
bilateralLoop5
bilateralLoop11
inputEdgeDetail
ridiculously large radius for bilateral filter
-[PIBilateralFilter outputImage]
inputPoints
inputWeights
unable to allocate convolution table in bilateral filter
com.apple.livephoto
com.apple.video
com.apple.video.slomo
.%lu
%lu.%lu%@%@
satPercentile75
satPercentile98
satPercentileG98
satAutoValue
inputVibrancy
inputCast
kernel vec4 _smartcolor_contrast_HDR (__sample im, float amt) 
 vec3 diff = im.rgb-dot(im.rgb, vec3(.0, .3, .5)); 
 float dist = distance(diff, vec3(0.0)); 
 dist = smoothstep(0.0, 1.0, dist); 
 float strength = 5.0*dist*amt; 
 vec3 pos = max(im.rgb, 1.0)-1.0 + min(im.rgb, 0.0); 
 im.rgb = clamp(im.rgb, 0.0, 1.0); 
 strength *= (im.b-im.g); 
 strength = max(strength, -0.35); 
 vec4 result; 
 result.rgb = im.rgb/(strength + 1.0 - (im.rgb*strength)) + pos; 
 result.a = im.a; 
 return result; 
kernel vec4 _smartcolor_contrast_darken_HDR (__sample im, float amt) 
 vec3 diff = im.rgb-dot(im.rgb, vec3(.0, .3, .5)); 
 float dist = distance(diff, vec3(0.0)); 
 dist = smoothstep(0.0, 1.0, dist); 
 float strength = 5.0*dist*amt; 
 vec3 pos = max(im.rgb, 1.0)-1.0 + min(im.rgb, 0.0); 
 im.rgb = clamp(im.rgb, 0.0, 1.0); 
 strength *= (im.b-im.g); 
 float gray = 1.0-min(dot(im.rgb, vec3(0.5, 0.7, -0.20)), 1.0); 
 vec4 result; 
 result.rgb = strength < 0.0 ? pow(im.rgb, vec3(1.0-strength*gray)) : im.rgb/(strength+1.0-(im.rgb*strength)); 
 result.rgb += pos; result.a = im.a; 
 return result; 
kernel vec4 _smartcolor_vibrancy_gt1_HDR (__sample im, float amt) 
 float gray = dot(clamp(im.rgb, 0.0, 1.0), vec3(.3, .5, .2)); 
 float y = dot(clamp(im.rgb, 0.0, 1.0), vec3(.4, .2, .1)); 
 float damp = 1.0-4.0*y*(1.0-y); 
 float s = 1.0/(im.r+im.g+im.b); 
 float r = im.r*s; 
 float b = im.b*s; 
 float d = 1.0-.8*smoothstep(0.2, 0.4, r-b); 
 damp *= d; 
 damp = amt > 2.5 ? min(damp+(amt-2.5)/5.0, 1.0) : damp; 
 float sat = min(amt, 3.0); 
 vec4 result; 
 result.rgb = (im.rgb - gray)*sat + gray; 
 result.rgb = mix(im.rgb, result.rgb, damp); 
 result.a = im.a; 
 return result; 
kernel vec4 _smartcolor_vibrancy_lt1_HDR (__sample im, float amt) 
 float gray = dot(im.rgb, vec3(0.333333)); 
 im.rgb = mix(vec3(gray), im.rgb, amt); 
 return im; 
kernel vec4 _smartcolor_cast_HDR (__sample im, float lum, float grayI, float grayQ, float strength) 
 vec4 pix = clamp(im, 0.0, 1.0); 
 pix.rgb = pow(pix.rgb, vec3(.25)); 
 pix.rgb = pix.r * vec3(0.299, 0.595716, 0.211456) + 
 pix.g * vec3(0.587, -0.274453, -0.522591) + 
 pix.b * vec3(0.114, -0.321263, 0.311135); 
 vec2 grayOffset = vec2(grayI, grayQ) ; 
 vec3 result = pix.rgb; 
 float newStrength = 1.0 + (strength-1.0)*(1.0-pix.r) ; 
 result.gb = pix.gb + newStrength*grayOffset ; 
 float damp = max(min(1.0, pix.r/(lum+0.00001)),0.0) ; 
 result.rgb = mix(pix.rgb, result.rgb, damp) ; 
 pix.rgb = result.r * vec3(1.0) + 
 result.g * vec3(0.956296, -0.272122, -1.10699) + 
 result.b * vec3(0.621024, -0.647381, 1.70461); 
 pix.rgb = clamp(pix.rgb, 0.0, 1.0); 
 pix.rgb *= pix.rgb*pix.rgb*pix.rgb; 
 pix.rgb += min(im.rgb, 0.0) + max(im.rgb,1.0) -1.0; 
 return pix; 
CIVibrance
inputAmount
PISmartColorHDR-sat-histogram
crossfadeDurationValue
crossfadeDurationTimescale
startTimeValue
startTimeTimescale
timeRangeStartValue
timeRangeStartTimescale
timeRangeDurationValue
timeRangeDurationTimescale
portraitInfo
spillMatteAllowed
@"NUAdjustment"16@?0@"NUAdjustment"8
grayI
grayQ
grayWarmth
grayY
warmFace
-[PIOrientationAdjustmentController setOrientation:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Controllers/PIOrientationAdjustmentController.m
mdta/com.apple.quicktime.cinematic-video.rendering
mdta/com.apple.quicktime.aperture-float
mdta/com.apple.quicktime.camera-dictionary
PIPortraitVideoMetadata.m
metadataGroup != nil
outError != nil
Could not decode timed rendering data
Missing disparity rendering metadata
Missing aperture rendering metadata
v24@?0@"PTRenderPipeline"8@"<PTRenderState>"16
device != nil
-[PIPortraitVideoRenderer initWithDevice:colorSize:disparitySize:quality:debugMode:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Render/PIPortraitVideoRenderer.m
!NUPixelSizeIsEmpty(colorSize)
!NUPixelSizeIsEmpty(disparitySize)
quality >= PTQualityPreviewLow && quality <= PTQualityExportProfessional
<%@:%p device:%@ color=%ldx%ld disparity=%ldx%ld quality=%d debug=%ld>
PIPortraitVideoRenderer.pool
B32@?0@"PIPortraitVideoRenderer"8Q16^B24
kernel vec4 _highKeyHDR(__sample im, float str) 
 vec3 neg = min(im.rgb, 0.0); 
 vec3 pos = max(im.rgb, 1.0) - 1.0; 
 im = clamp(im, 0.0, 1.0); 
 vec4 im2 = 1.0-((im-1.0)*(im-1.0)); 
 im2 = sqrt(im2); 
 float y = dot(im.rgb, vec3(.333333)); 
 float y2 = sqrt(1.0-(y-1.0)*(y-1.0)); 
 y2 = mix(y2, smoothstep(0.0, 1.0, y2), 0.5); 
 vec4 im3 = (y>0) ? im*y2/y : vec4(0.0, 0.0, 0.0, 1.0) ; 
 im3 = mix(im3, im2, .7*sqrt(y2)); 
 im3 = mix(im, im3, sqrt(y)) ; 
 im.rgb = mix(im.rgb, im3.rgb, str) + pos + neg; 
 return im; } 
None
UnexpectedRuntimeError
NoBuildingDetected
PanoAndFFCNotSupported
FacesTooLarge
ConfidenceTooLow
LimitExceeded
NotEnoughLines
CorrectedAreaIsSmall
LessThanMinimumCorrection
-[PIPerspectiveAutoCalculator initWithComposition:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Autocalculators/PIPerspectiveAutoCalculator.m
%@.%@
%@.error
%@.underlyingError
pitchError
yawError
pitchError.underlyingError
yawError.underlyingError
%@PerspectiveEvaluation-txt.DBG
%@PerspectiveLineDetection-png.DBG
Edit
PIPerspectiveAutoCalculator-debug
faces != nil
-[PIPerspectiveAutoCalculator getSizeOfAllFaces:]
-[PIPerspectiveAutoCalculator passesFaceCheck:]
PIPerspectiveAutoCalculator-faceCheck
faceSize
maxFaceSize
passesFaceCheck
Apple
front
camera
-[PIPerspectiveAutoCalculator passesImagePropertiesCheck:]
PIPerspectiveAutoCalculator-selfieAndAspectRatioCheck
aspectRatio
isSelfieCam
passesImagePropertiesCheck
-[PIPerspectiveAutoCalculator passesBuildingCheck:]
PIPerspectiveAutoCalculator-classify
minConfidence
passesBuildingCheck
-[PIPerspectiveAutoCalculator submit:]
angle
-[PIPerspectiveAutoCalculator overcaptureImageProperties:]
No overcapture
PIPerspectiveAutoCalculator-overcaptureImageProperties
-[PIPerspectiveAutoCalculator primaryImageProperties:]
PIPerspectiveAutoCalculator-primaryImageProperties
-[PIPerspectiveAutoCalculator canGenerateNewCropRect:]
preseedRoll
canGenerateNewCropRect
setToPrimary
pitch != nil
+[PIPerspectiveAutoCalculator undoOrientation:forPitch:yaw:angle:]
yaw != nil
angle != nil
-[PIPerspectiveAutoCalculator passesConfidenceCheck:error:]
passesConfidenceCheck
supported
Not supported by Core Image. Default: YES
-[PIPerspectiveAutoCalculator passesMinimumCorrectionCheck:error:]
pitchExpandTopDegrees
yawExpandLeftDegrees
rollAngleInDegreesCW
passesMinimumCorrectionCheck
minimumPitchCorrection
minimumYawCorrection
minimumAngleCorrection
-[PIPerspectiveAutoCalculator submitVerified:]
focalLength
pitchLimit
yawLimit
rollLimit
minimumPitchCorrectionArea
minimumYawCorrectionArea
CIAutoPerspective
submitVerified
debugImage
pitchCorrectionAreaCoverage
yawCorrectionAreaCoverage
ciPitchError
ciYawError
PIPerspectiveAutoCalculator-getPrimaryOrientation
luminance
Flash
FNumber
ApertureValue
ShutterSpeedValue
ExposureTime
ISOSpeedRatings
cannot set empty crop rect
-[PICropAdjustmentController setCropRect:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Controllers/PICropAdjustmentController.m
constraintWidth
constraintHeight
smart
originalCrop
inputDecoderVersion
inputLight
localAutoValue
inputBrightness
inputContrast
inputExposure
inputHighlights
inputShadows
inputBlack
inputLocalLight
inputLightMap
inputLightMapWidth
inputLightMapHeight
offsetBlack
offsetBrightness
offsetContrast
offsetExposure
offsetHighlights
offsetLocalLight
offsetShadows
inputRawHighlights
statistics
overcaptureStatistics
kernel vec4 alphaCompositing(__sample src, __sample dst, float a)
return mix(dst, src, a);
kernel vec4 dynamismMap(sampler imageMax, sampler imageMin, float logisticGain, float logisticMid) __attribute__((outputFormat(kCIFormatRGBAh)))
const float MAX_VAL = 1.74;
const float THRESHOLD = 0.05;
vec2 dc = destCoord();
vec2 sc;
sc = dc + vec2(-1,-1);
vec4 pMax00 = sample(imageMax, samplerTransform(imageMax, sc));
vec4 pMin00 = sample(imageMin, samplerTransform(imageMin, sc));
sc = dc + vec2(-1,0);
vec4 pMax01 = sample(imageMax, samplerTransform(imageMax, sc));
vec4 pMin01 = sample(imageMin, samplerTransform(imageMin, sc));
sc = dc + vec2(-1,1);
vec4 pMax02 = sample(imageMax, samplerTransform(imageMax, sc));
vec4 pMin02 = sample(imageMin, samplerTransform(imageMin, sc));
sc = dc + vec2(0,-1);
vec4 pMax10 = sample(imageMax, samplerTransform(imageMax, sc));
vec4 pMin10 = sample(imageMin, samplerTransform(imageMin, sc));
vec4 pMax11 = sample(imageMax, samplerTransform(imageMax, dc));
vec4 pMin11 = sample(imageMin, samplerTransform(imageMin, dc));
sc = dc + vec2(0,1);
vec4 pMax12 = sample(imageMax, samplerTransform(imageMax, sc));
vec4 pMin12 = sample(imageMin, samplerTransform(imageMin, sc));
sc = dc + vec2(1,-1);
vec4 pMax20 = sample(imageMax, samplerTransform(imageMax, sc));
vec4 pMin20 = sample(imageMin, samplerTransform(imageMin, sc));
sc = dc + vec2(1,0);
vec4 pMax21 = sample(imageMax, samplerTransform(imageMax, sc));
vec4 pMin21 = sample(imageMin, samplerTransform(imageMin, sc));
sc = dc + vec2(1,1);
vec4 pMax22 = sample(imageMax, samplerTransform(imageMax, sc));
vec4 pMin22 = sample(imageMin, samplerTransform(imageMin, sc));
float minDevCMaxNMin = distance(pMax11, pMin00);
minDevCMaxNMin = min( minDevCMaxNMin , distance(pMax11, pMin01) );
minDevCMaxNMin = min( minDevCMaxNMin , distance(pMax11, pMin02) );
minDevCMaxNMin = min( minDevCMaxNMin , distance(pMax11, pMin10) );
minDevCMaxNMin = min( minDevCMaxNMin , distance(pMax11, pMin11) );
minDevCMaxNMin = min( minDevCMaxNMin , distance(pMax11, pMin12) );
minDevCMaxNMin = min( minDevCMaxNMin , distance(pMax11, pMin20) );
minDevCMaxNMin = min( minDevCMaxNMin , distance(pMax11, pMin21) );
minDevCMaxNMin = min( minDevCMaxNMin , distance(pMax11, pMin22) );
float minDevCMinNMax = distance(pMin11, pMax00);
minDevCMinNMax = min( minDevCMinNMax , distance(pMin11, pMax01) );
minDevCMinNMax = min( minDevCMinNMax , distance(pMin11, pMax02) );
minDevCMinNMax = min( minDevCMinNMax , distance(pMin11, pMax10) );
minDevCMinNMax = min( minDevCMinNMax , distance(pMin11, pMax11) );
minDevCMinNMax = min( minDevCMinNMax , distance(pMin11, pMax12) );
minDevCMinNMax = min( minDevCMinNMax , distance(pMin11, pMax20) );
minDevCMinNMax = min( minDevCMinNMax , distance(pMin11, pMax21) );
minDevCMinNMax = min( minDevCMinNMax , distance(pMin11, pMax22) );
float outVal = min(minDevCMaxNMin , minDevCMinNMax) / MAX_VAL;
outVal = 1.0f / (1.0f + exp(-logisticGain*(outVal - logisticMid)));
vec4 outPixel = vec4(outVal, outVal, outVal, 1.0);
return outPixel;
kernel vec4 jointbilateralfilter(sampler mask, sampler guide, vec2 rad, vec3 sig) __attribute__((outputFormat(kCIFormatRh)))
vec2 coord = destCoord();
float rh = rad.x;
float rv = rad.y;
vec4 p0 = sample(mask, samplerTransform(mask, coord));
vec4 g0 = sample(guide, samplerTransform(guide, coord));
vec4 result = vec4(0.0f);
float w_sum = 0.0f;
for(float yo=-rad.y;  yo<=rad.y;  yo++) {
for(float xo=-rad.x;  xo<=rad.x;  xo++) {
vec2 dc = vec2(xo, yo);
vec4 p = sample(mask, samplerTransform(mask, coord + dc));
vec4 g = sample(guide, samplerTransform(guide, coord + dc));
float d = dot(dc, dc);
float pdif = dot(p-p0,p-p0);
float gdif = dot(g-g0,g-g0);
float w = exp(dot(sig, vec3(d,pdif,gdif)));
result += w*p;
w_sum += w;
result /= w_sum;
return vec4(result.rgb, 1.0);
kernel vec4 rgba_to_luma(__sample rgb)
const vec4 rgb2y = vec4(0.2999,0.5870,0.1140,0.0);
float luma = dot(rgb, rgb2y);
return vec4(luma, luma, luma, 1.0);
kernel vec2 warp_homography(vec3 h012, vec3 h345, vec3 h678)
vec3 coord = vec3(destCoord(), 1.f);
float norm = 1.f / dot(h678, coord);
float x = norm * dot(h012, coord);
float y = norm * dot(h345, coord);
return vec2(x,y);
kernel vec4 ncc_compute(sampler img1, sampler img2) __attribute__((outputFormat(kCIFormatR8)))
vec2 coord = destCoord();
float sum1   = float(0.0);
float sum2   = float(0.0);
float sumSq1 = float(0.0);
float sumSq2 = float(0.0);
float sumCrs = float(0.0);
float ncc;
float p1, p2;
const int halfKernel = 3;
const vec4 meanOp = vec4(1.0/3.0, 1.0/3.0, 1.0/3.0, 0.0);
float count = 0.0;
for(int y=-halfKernel; y<=halfKernel; y++) {
for(int x=-halfKernel; x<=halfKernel; x++) {
p1 = dot(meanOp, sample(img1, samplerTransform(img1, coord + vec2(x, y))));
p2 = dot(meanOp, sample(img2, samplerTransform(img2, coord + vec2(x, y))));
sum1 += p1;
sum2 += p2;
sumSq1 += p1*p1;
sumSq2 += p2*p2;
sumCrs += p1*p2;
count += 1.0;
float denom = (sumSq1-sum1*sum1/count) * (sumSq2-sum2*sum2/count);
ncc = compare(denom, 0.0, ((sumCrs-(sum1*sum2/count)))/sqrt(denom));
ncc = max(ncc, 0.0);
return vec4(ncc, ncc, ncc, 1.0);
kernel vec4 ncc_coarse_compute(__sample ncc, vec2 edge) __attribute__((outputFormat(kCIFormatR8)))
float value = smoothstep(edge.x, edge.y, ncc.r);
return vec4(value, value, value, 1.0);
kernel vec4 blur_image_compute_3x3(sampler img)
const int halfKernel = 1;
vec2 coord = destCoord();
vec4 sum = vec4(0.0);
float count = 0.0;
for(int y=-halfKernel; y<=halfKernel; y++) {
for(int x=-halfKernel; x<=halfKernel; x++) {
sum += sample(img, samplerTransform(img, coord + vec2(x, y)));
count += 1.0;
sum = sum/(count);
return sum;
kernel vec4 blur_image_compute_5x5(sampler img)
const int halfKernel = 2;
vec2 coord = destCoord();
vec4 sum = vec4(0.0);
float count = 0.0;
for(int y=-halfKernel; y<=halfKernel; y++) {
for(int x=-halfKernel; x<=halfKernel; x++) {
sum += sample(img, samplerTransform(img, coord + vec2(x, y)));
count += 1.0;
sum = sum/(count);
return sum;
kernel vec4 blur_image_compute_7x7(sampler img)
const int halfKernel = 3;
vec2 coord = destCoord();
vec4 sum = vec4(0.0);
float count = 0.0;
for(int y=-halfKernel; y<=halfKernel; y++) {
for(int x=-halfKernel; x<=halfKernel; x++) {
sum += sample(img, samplerTransform(img, coord + vec2(x, y)));
count += 1.0;
sum = sum/(count);
return sum;
kernel vec4 fuse_image_compute(sampler refImg, sampler guideImg, sampler blurImg, sampler weightImg, sampler maskImg, vec2 threshold)
vec4 ref       = sample(refImg, samplerCoord(refImg));
vec4 refBlur   = ref;
vec4 guide     = sample(guideImg, samplerCoord(guideImg));
vec4 guideBlur = sample(blurImg, samplerCoord(blurImg));
float weight   = sample(weightImg, samplerCoord(weightImg)).r;
float mask     = sample(maskImg, samplerCoord(maskImg)).r;
vec3 CbCoef = vec3(-0.1726, -0.3391, 0.5117);
vec3 CrCoef = vec3(0.5115, -0.4282, -0.0830);
float CbRef   = dot(CbCoef, ref.rgb);
float CbGuide = dot(CbCoef, guideBlur.rgb);
float CrRef   = dot(CrCoef, ref.rgb);
float CrGuide = dot(CrCoef, guideBlur.rgb);
float crDiff = smoothstep(0.0, 0.2, abs(CbRef-CbGuide));
float cbDiff = smoothstep(0.0, 0.2, abs(CrRef-CrGuide));
float chDiff = smoothstep(0.0,0.3,crDiff+cbDiff);
float weight1 = (1.0-smoothstep(threshold.x,threshold.y,mask));
weight *= weight1 * (1.0-chDiff);
vec4 value = mix(ref, (0.9*(guide-guideBlur)+refBlur), weight);
return value;
jointbilateralfilter
rgba_to_luma
warp_homography
ncc_compute
ncc_coarse_compute
blur_image_compute_7x7
blur_image_compute_5x5
blur_image_compute_3x3
fuse_image_compute
CIPhotoEffectMono
Mono
CIPhotoEffectTonal
Tonal
CIPhotoEffectNoir
Noir
CIPhotoEffectFade
Fade
CIPhotoEffectChrome
Chrome
CIPhotoEffectProcess
Process
CIPhotoEffectTransfer
Transfer
CIPhotoEffectInstant
Instant
CIPhotoEffect3DVivid
3DVivid
CIPhotoEffect3DVividWarm
3DVividWarm
CIPhotoEffect3DVividCool
3DVividCool
CIPhotoEffect3DDramatic
3DDramatic
CIPhotoEffect3DDramaticWarm
3DDramaticWarm
CIPhotoEffect3DDramaticCool
3DDramaticCool
CIPhotoEffect3DSilverplate
3DSilverplate
CIPhotoEffect3DNoir
3DNoir
CIPortraitEffectBlack
Black
CIPortraitEffectBlackoutMono
BlackoutMono
CIPortraitEffectStageV2
StageV2
CIPortraitEffectStageMonoV2
StageMonoV2
CIPortraitEffectStageWhite
StageWhite
CIPortraitEffectLight
Light
CIPortraitEffectCommercial
Commercial
CIPortraitEffectContour
Contour
CIPortraitEffectStudioV2
StudioV2
CIPortraitEffectContourV2
ContourV2
%@ (preview) %a
%@ %a
+[PIPhotoEditHelper imageSourceWithURL:type:useEmbeddedPreview:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Util/PIPhotoEditHelper.m
+[PIPhotoEditHelper imageSourceWithURL:type:proxyImage:orientation:useEmbeddedPreview:]
+[PIPhotoEditHelper imageSourceWithCIImage:orientation:]
+[PIPhotoEditHelper videoSourceWithURL:]
photoSource != nil
+[PIPhotoEditHelper livePhotoSourceWithPhotoSource:videoSource:]
videoSource != nil
%@+%@
name != nil
+[PIPhotoEditHelper newAdjustmentWithName:]
identifier != nil
+[PIPhotoEditHelper newAdjustmentWithIdentifier:]
+[PIPhotoEditHelper newImageRenderClientWithName:]
+[PIPhotoEditHelper geometryRequestWithComposition:]
+[PIPhotoEditHelper imagePropertiesRequestWithComposition:]
+[PIPhotoEditHelper videoPropertiesRequestWithComposition:]
targetSize.width > 0 && targetSize.height > 0
+[PIPhotoEditHelper imageRenderRequestWithComposition:fitInSize:wideGamut:]
PIPhotoEditHelper-targetSize-image
+[PIPhotoEditHelper imageRenderRequestWithComposition:fillInSize:wideGamut:]
PIPhotoEditHelper-fillSquare-image
+[PIPhotoEditHelper _imageRenderRequestWithComposition:wideGamut:]
PIPhotoEditHelper-basic-image
+[PIPhotoEditHelper videoRenderRequestWithComposition:fitInSize:]
overrideVideoEditability
v32@?0@"NSString"8@"NSString"16^B24
/ShowOriginalSource
var image = input;
ResetTagInput('/pre-VideoReframe', image);
image = GetTag('VideoReframe');
ResetTagInput('/pre-Geometry', image);
return GetTag('/post-Geometry');
/pre-VideoReframe
VideoReframe
/pre-Geometry
/post-Geometry
ResetTagInput('/pre-Geometry', input);
return GetTag('/post-Geometry');
PIPhotoEditHelper.m
inputCorrectionInfo
depthInfo
start
startScale
endScale
SkipShaderPrewarm
boostVersion
boostParams
kernel vec4 forwardBoostWithBoost(sampler image, float boost) {
vec4 im = sample(image, samplerCoord(image));
vec4 orig = im;
float n = 1.8;
float k = 0.8;
vec3 pos = max(im.rgb, k) - k;
vec3 neg = min(im.rgb, 0.0);
neg *= 2.8;
im.rgb = clamp(im.rgb, 0.0, k);
im.rgb = ((1.0+n)*im.rgb)/(1.0+(n*im.rgb)) + neg;
im.r = pos.r > 0.0 ? (.91803 + .470304*pos.r) : im.r;
im.g = pos.g > 0.0 ? (.91803 + .470304*pos.g) : im.g;
im.b = pos.b > 0.0 ? (.91803 + .470304*pos.b) : im.b;
im.rgb = mix(orig.rgb, im.rgb, boost);
return im;
kernel vec4 inverseBoostWithBoost(sampler image, float boost){
vec4 im = sample(image, samplerCoord(image));
vec4 orig = im;
float n = 1.8;
float kinv = 0.91803;
vec3 pos = max(im.rgb, kinv);
vec3 neg = min(im.rgb, 0.0);
im.rgb = clamp(im.rgb, 0.0, kinv);
neg *= .35714286;
im.rgb = im.rgb/(n+1.0-(im.rgb*n)) + neg;
im.r = pos.r > kinv ? 0.8 + 2.126286*(pos.r-kinv) : im.r;
im.g = pos.g > kinv ? 0.8 + 2.126286*(pos.g-kinv) : im.g;
im.b = pos.b > kinv ? 0.8 + 2.126286*(pos.b-kinv) : im.b;
im.rgb = mix(orig.rgb, im.rgb, boost);
return im;
kernel vec4 forwardBoost3(__sample im, float boost, vec4 abcd, vec3 p0, vec3 p1) {
float a = abcd.x;
float b = abcd.y;
float c = abcd.z;
float d = abcd.w;
vec3 x = im.rgb;
vec3 f = (a * x + b) / (c * x + d);
float x0 = p0.x;
float y0 = p0.y;
float s0 = p0.z;
float a0 = y0*y0;
float c0 = y0 - s0*x0;
float d0 = s0*x0*x0;
vec3 f0 = (a0 * x) / (c0 * x + d0);
vec3 l0 = (a0 / d0) * x;
f0 = compare(x, l0, f0);
float x1 = p1.x;
float y1 = p1.y;
float s1 = p1.z;
float a1 = (1.f-y1)*(1.f-y1);
float c1 = (1.f-y1) - s1*(1.f-x1);
float d1 = s1 *(1.f-x1)*(1.f-x1);
vec3 f1 = 1.f - (a1 * (1.f - x)) / (c1 * (1.f - x) + d1);
vec3 l1 = 1.f - (a1 / d1) * (1.f - x);
f1 = compare(1.f - x, l1, f1);
f = compare(x - p0.x, f0, compare(x - p1.x, f, f1));
im.rgb = mix(im.rgb, f, boost);
return im;
kernel vec4 inverseBoost3(__sample im, float boost, vec4 abcd, vec3 p0, vec3 p1) {
float a = abcd.x;
float b = abcd.y;
float c = abcd.z;
float d = abcd.w;
vec3 y = im.rgb;
vec3 g = (d * y - b) / (a - c * y);
float x0 = p0.x;
float y0 = p0.y;
float s0 = p0.z;
float a0 = x0*x0;
float c0 = x0 - y0/s0;
float d0 = y0*y0/s0;
vec3 g0 = (a0 * y) / (c0 * y + d0);
vec3 l0 = (a0 / d0) * y;
g0 = compare(y, l0, g0);
float x1 = p1.x;
float y1 = p1.y;
float s1 = p1.z;
float a1 = (1.f-x1)*(1.f-x1);
float c1 = (1.f-x1) - (1.f-y1)/s1;
float d1 = (1.f-y1)*(1.f-y1)/s1;
vec3 g1 = 1.f - (a1 * (1.f - y)) / (c1 * (1.f - y) + d1);
vec3 l1 = 1.f - (a1 / d1) * (1.f - y);
g1 = compare(1.f - y, l1, g1);
g = compare(y - p0.y, g0, compare(y - p1.y, g, g1));
im.rgb = mix(im.rgb, g, boost);
return im;
This is an abstract method! Subclass '%@' should provide concrete implementation
+[PIFakeBoost kernelFB0]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Adjustments/PIFakeBoost.m
+[PIFakeBoost kernelFB3]
properties != nil
+[PIFakeBoost boostParametersFromRawProperties:]
-[PIFakeBoost outputImageFB0]
-[PIFakeBoost outputImageFB3]
Invalid boost parameters
Invalid points parameters
boost kernel is nil
+[PIForwardFakeBoost kernelFB0]_block_invoke
+[PIForwardFakeBoost kernelFB3]_block_invoke
inverse boost kernel is nil
+[PIInverseFakeBoost kernelFB0]_block_invoke
+[PIInverseFakeBoost kernelFB3]_block_invoke
AutoEnhance
AutoLoop
Crop
DepthEffect
Effect
Effect3D
Grain
HighResolutionFusion
LivePhotoKeyFrame
Mute
PortraitEffect
PortraitVideo
RedEyeBB
SelectiveColor
SemanticEnhance
SlowMotion
SmartBlackAndWhite
SmartColor
SmartTone
SourceSelect
Trim
VideoCrossfadeLoop
VideoPosterFrame
VideoStabilize
WhiteBalance
compositionName
identifierName
custom
customSerialization
customCompositionName
customIdentifierName
autoValue
requiresEnabled
offsetBlackPoint
v24@?0@"NSDictionary"8@"NUAdjustment"16
v32@?0@8@16^B24
inputColor
offsetSaturation
offsetCast
offsetStrength
offsetGrain
offsetTone
inputBlackAndWhite
offsetNeutralGamma
amount
seed
DGVignetteEffectOperation
Vignette
inputIntensity
inputRadius
inputFalloff
DGDefinition2Operation
Definition
RKNoiseReductionOperation
NoiseReduction
edgeDetail
RKProSharpenOperation
Sharpen
inputEdgeScale
edges
inputSharpness
@"NSString"16@?0@"NSDictionary"8
effectName
@"NSString"24@?0@"NSDictionary"8@"NUAdjustment"16
effectVersion
effectIntensity
CropStraighten
straightenAngle
DGRAWReduceNoiseOperation
RawNoiseReduction
inputDetailAmount
detail
inputCNRAmount
inputLNRAmount
RKRawDecodeOperation
inputMethodVersion
colorComponents
slope
bias
RKLevelsOperation
Levels
colorSpace
inputColorSpace
blackSrcRGB
blackDstRGB
shadowSrcRGB
shadowDstRGB
midSrcRGB
midDstRGB
hilightSrcRGB
hilightDstRGB
whiteSrcRGB
whiteDstRGB
blackSrcRed
blackDstRed
shadowSrcRed
shadowDstRed
midSrcRed
midDstRed
hilightSrcRed
hilightDstRed
whiteSrcRed
whiteDstRed
blackSrcGreen
blackDstGreen
shadowSrcGreen
shadowDstGreen
midSrcGreen
midDstGreen
hilightSrcGreen
hilightDstGreen
whiteSrcGreen
whiteDstGreen
blackSrcBlue
blackDstBlue
shadowSrcBlue
shadowDstBlue
midSrcBlue
midDstBlue
hilightSrcBlue
hilightDstBlue
whiteSrcBlue
whiteDstBlue
Generic P3
Display P3
Adobe RGB
sRGB
RKCurvesOperation
Curves
RGBCurvePoints
pointsL
redCurvePoints
pointsR
greenCurvePoints
pointsG
blueCurvePoints
pointsB
RKSelectiveColorOperation
B16@?0@"NSDictionary"8
corrections
RKRetouchOperation
Retouch
inputStrokes
hasSource
mode
brushStroke
softness
opacity
repairEdges
sourceOffset
data
xLocation
yLocation
pressure
points
clipRect
detectedFaces
RKRedEyeOperation
ApertureRedEye
inputSpots
pointX
center
pointY
sensitivity
RedEye
allCorrections
redEyeCorrections
autoRedEyeCorrections
endTime
regions
timeRange
rate
timescale
alignment
glassesMatteAllowed
portraitEffectFilterName
keyframes
cropFraction
analysisType
v24@?0@"NSMutableDictionary"8@"NUAdjustment"16
face
warm
inputDetectedFaces
flags
epoch
lightMap
lightMapAvg
vec4 _curve_sample_HDR(float x, sampler2D table, vec2 domain, vec2 normalizer) { x = (x - domain.x) / (domain.y - domain.x); x = clamp(x, 0.0001, 0.9999); x = normalizer.x * x + normalizer.y; return texture2D(table, vec2(x, 0.5)); } kernel vec4 _curves_rgb_lum_HDR(__sample color, sampler2D table, vec2 domain, vec2 normalizer) { vec4 pixel = color; pixel.r = _curve_sample_HDR(pixel.r, table, domain, normalizer).r; pixel.g = _curve_sample_HDR(pixel.g, table, domain, normalizer).g; pixel.b = _curve_sample_HDR(pixel.b, table, domain, normalizer).b; float lum0 = dot(pixel.rgb, vec3(0.3, 0.59, 0.11)); float lum1 = _curve_sample_HDR(lum0, table, domain, normalizer).a; float lum1c = clamp(lum1, -8.0 * abs(lum0), 8.0 * abs(lum0)); float lum_scale = (lum0 == 0.0 ? 0.0 : lum1c / lum0); float lum_offset = lum1 - lum1c; pixel.rgb = lum_scale * pixel.rgb + lum_offset; pixel.rgb = min(pixel.rgb, 1.0); return pixel; }
PILongExposureAccumulator.state
PILongExposureAccumulator.accum
Accumulation was cancelled
PILongExposureAccumulator-main-j%lld
failed to init accumulator
B16@?0@"CIRenderDestination"8
frame != nil
-[PILongExposureAccumulator accumulate:error:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/AutoLoop/PIAutoLoopJob+LongExposure.m
-[PILongExposureAccumulator _accumulate:error:]
PILongExposureAccumulator-average-j%lld
failed to render average image
v16@?0@"CIImage"8
CIBoxBlur
PILongExposureAccumulator-minimum-j%lld
failed to render minimum image
PILongExposureAccumulator-maximum-j%lld
failed to render maximum image
-[PILongExposureAccumulator writeLongExposureImage:UTI:colorSpace:error:]
-[PILongExposureAccumulator writeMaskImage:UTI:error:]
-[PILongExposureAccumulator _exportOutputImage:format:colorSpace:toURL:uti:error:]
destinationURL != nil
Failed to finalize image destination
Failed to create CGImageDestinationRef
Failed to create CGImageRef
/AutoLoop/LongExposure
-[PILongExposureRegistrationJob prepare:]
return Source(composition.source, {'skipOrientation':true});
Malformed AutoLoop recipe : crop
-[PILongExposureRegistrationJob render:]
Failed to allocate intermediate pixel buffer
PILongExposureRegistrationJob-render-j%lld
PILongExposureRegistrationJob-reference-j%lld
Failed to render luma
Image registration failure (expected 1 observation)
/RAW/SushiLevel1
/Master%@
CIRedEyeCorrection
inputCameraModel
CILanczosScaleTransform
1.5.1
1.9.1
computed
overcaptureComputed
dynamic
@"NSString"16@?0@"NUAdjustment"8
SDOFRenderingVersion
convexHull
/private/var/mobile/Media/PhotoData/CaptureDebug/
Repair
Clone
RepairML
monochrome
assetURL
-[PIPortraitVideoDebugDetectionsRenderNode resolvedNodeWithCachedInputs:settings:pipelineState:error:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Render/PIPortraitVideoDebugDetectionsRenderNode.m
v20@?0B8@"NSError"12
%@-labelImageCache
-[PIPortraitVideoDebugDetectionsRenderNode _evaluateImage:]
Face
Head
Torso
Sports Ball
AutoFocus
%@ %@
%@ G:%@
Helvetica
outputExposure
falseColorHDR
inputRAWGamutMapMax
Buffer must be RGBA16 type for red eye repairs
id<NUBuffer> computeRepairMask(__strong id<NUBuffer>, uint16_t *)
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Adjustments/ApertureRedEye/PIApertureRedEye.mm
reddestx out of bounds
void seedFill(__strong id<NUMutableBuffer>, __strong id<NUBuffer>, const uint32_t, const NSInteger, const NSInteger)
reddesty out of bounds
void applyRepairMask(__strong id<NUMutableBuffer>, __strong id<NUBuffer>, double, const uint32_t)
repairBuffer != nil
kernel vec4 facebalance(__sample src, vec2 gamma, vec3 matchMinusOrigStrength)
vec4 pix = src;
pix.rgb = pow(max(pix.rgb, 0.0), vec3(gamma.x));
pix.rgb = pix.r * vec3(0.299,  0.595716,  0.211456) +
pix.g * vec3(0.587, -0.274453, -0.522591) +
pix.b * vec3(0.114, -0.321263,  0.311135);
vec4 orig = pix ;
float chroma = min(1.0, 2.0*sqrt(pix.g*pix.g + pix.b*pix.b) );
pix.gb +=  matchMinusOrigStrength.rg;
float strength = matchMinusOrigStrength.z*pow(chroma, .4) ;
pix.gb = mix(orig.gb, pix.gb, strength) ;
pix.rgb = pix.rrr +
pix.g * vec3(0.956296, -0.272122, -1.10699) +
pix.b * vec3(0.621024, -0.647381, 1.70461);
pix.rgb = max(pix.rgb, vec3(0.0));
pix.rgb = pow(pix.rgb, vec3(gamma.y));
return pix;
facebalance
vec4 curve_sample(float x, sampler2D table, vec2 domain, vec2 normalizer)
x = (x - domain.x) / (domain.y - domain.x);
x = clamp(x, 0.0001, 0.9999);
x = normalizer.x * x + normalizer.y;
return texture2D(table, vec2(x, 0.5));
kernel vec4 curves_rgb_lum(__sample color, sampler2D table, vec2 domain, vec2 normalizer)
vec4 pixel = color;
pixel.r = curve_sample(pixel.r, table, domain, normalizer).r;
pixel.g = curve_sample(pixel.g, table, domain, normalizer).g;
pixel.b = curve_sample(pixel.b, table, domain, normalizer).b;
float lum0 = dot(pixel.rgb, vec3(0.3, 0.59, 0.11));
float lum1 = curve_sample(lum0, table, domain, normalizer).a;
float lum1c = clamp(lum1, -8.0 * abs(lum0), 8.0 * abs(lum0));
float lum_scale = (lum0 == 0.0 ? 0.0 : lum1c / lum0);
float lum_offset = lum1 - lum1c;
pixel.rgb = lum_scale * pixel.rgb + lum_offset;
return pixel;
tableR != nil
+[PICurvesLUTFilter tableImageFromRed:green:blue:luminance:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Adjustments/PICurvesFilter.mm
tableG != nil
tableB != nil
tableL != nil
kernel vec4 gHDRtoPP(sampler image)
vec4 pix ;
vec3 pix2;
pix = sample(image, samplerCoord(image));
pix2 = pix.r * vec3(0.615429622407401,   0.114831839141528,   0.011544126697221) +
pix.g * vec3(0.367479646665836,   0.797943554457996,   0.064077744191180) +
pix.b * vec3(  0.016956659608091,   0.087783443422360,   0.924405601458102);
return vec4(pix2, pix.a);
kernel vec4 PPtogHDR(sampler image)
vec4 pix ;
vec3 pix2;
pix = sample(image, samplerCoord(image));
pix2 = pix.r * vec3(1.777445503202045,  -0.255296595099306,  -0.004500433755654) +
pix.g * vec3( -0.822224875430495,   1.380948853784730,  -0.085456231694984) +
pix.b * vec3(0.045475917061484,  -0.126454737973025,   1.089973874037625);
return vec4(pix2, pix.a);
kernel vec4 colorBalance(__sample image, float colorI, float colorQ, float str, vec2 gamma)
vec4 pix = image;
vec3 neg = min(pix.rgb, 0.0);
vec3 pos = max(pix.rgb, 1.0) - 1.0;
pix.rgb = pow(clamp(pix.rgb, 0.0, 1.0), vec3(gamma.x));
pix.rgb = pix.r * vec3(0.299,  0.595716,  0.211456) +
pix.g * vec3(0.587, -0.274453, -0.522591) +
pix.b * vec3(0.114, -0.321263,  0.311135);
vec4 orig = pix ;
float chroma = min(1.0, 2.0*sqrt(pix.g*pix.g + pix.b*pix.b) );
pix.gb += vec2(colorI,colorQ);
float strength = str*pow(chroma, .4) ;
pix.gb = mix(orig.gb, pix.gb, strength) ;
pix.rgb = pix.rrr +
pix.g * vec3(0.956296, -0.272122, -1.10699) +
pix.b * vec3(0.621024, -0.647381, 1.70461);
pix.rgb = pow(max(pix.rgb, 0.0), vec3(gamma.y));
pix.rgb += pos + neg;
return pix;
gHDRtoPP
PPtogHDR
colorBalance
-[PIColorBalanceFilter outputImage]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Adjustments/PIColorBalanceFilter.m
@"NSString"16@?0@"NSString"8
CIMix
CIGammaAdjust
CIExposureAdjust
CIProSharpenEdges
CIHighKey
CILocalContrast
PILocalContrastHDR
CILocalLightFilter
CILocalLightMapPrepare
CIPhotoGrain
CISmartBlackAndWhite
CIVignetteEffect
CILocalLight
CISmartColor
CISmartTone
PIFalseColorHDRDebug
PIColorBalanceFilter
PIRAWFaceBalance
PINeutralGrayWhiteBalanceFilter
PIBilateralFilter
PICurvesLUTFilter
PICurvesFilter
PISelectiveColorFilter
PILevelsFilter
PIGlobalSettings
IPXRootSettings
PXSettingsArchiveKey
editSettings
PURootSettings
photoEditingSettings
debugDecoratorFiltersEnabled
PI_AUTOLOOP_EXPORT_USE_METAL
B8@?0
-[PIAutoLoopExportJob initWithVideoExportRequest:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/AutoLoop/PIAutoLoopJob+Export.m
@"NUJSRenderNode"24@?0@"NUJSRenderNode"8@"JSValue"16
Invalid data type for adjustmentValue
Invalid data type for keyframes
Invalid data type for stabCropRect
input to VideoReframe cannot be nil
Unable to unwrap the input node!
-[PIJSRenderPipeline setUpContext:]_block_invoke
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Pipeline/PIPhotosPipeline.m
Missing duration for crossfade
input to VideoCrossfadeLoop cannot be nil
{CGRect={CGPoint=dd}{CGSize=dd}}64@?0{CGSize=dd}8{CGSize=dd}24{CGSize=dd}40@"JSValue"56
OvercaptureRectForAutoCrop
Couldn't find bundle for class %@
+[NUJSRenderPipeline(PhotosPipeline) newPhotosPipeline:]
PhotosPipeline
Couldn't find the specified Pipeline
JSContext
Class getJSContextClass(void)_block_invoke
JavaScriptCoreSoftLinking.h
Unable to find class %s
void *JavaScriptCoreLibrary(void)
/System/Library/Frameworks/JavaScriptCore.framework/JavaScriptCore
/System/Library/Frameworks/JavaScriptCore.framework/Contents/MacOS/JavaScriptCore
-[PIApertureRedEyeAutoCalculator submit:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Autocalculators/PIApertureRedEyeAutoCalculator.m
PIApertureRedEyeAutoCalculator-faceDetection
pre-Adjustments
redEyeSpots
PILocalLightHDR-stats
lightMapWidth
lightMapHeight
NSNumber
inputSmartShadows
kernel vec4 _shadowKernelHDR(__sample im, __sample adj, float str) { adj.r = 3.4*adj.r-1.2; vec3 neg = min(im.rgb, 0.0); vec3 pos = max(im.rgb, 1.0)-1.0; im.rgb = clamp(im.rgb, 0.0, 1.0); vec4 orig = im; float y = sqrt(dot(im.rgb, vec3(.33333))); float s = mix(0.0, adj.r, str); vec3 gain = s > 0.0 ? vec3(0.0) : vec3(s*s) * vec3(-2.75, -2.75, -2.5); im.rgb = im.rgb*im.rgb*gain + im.rgb*(1.0-gain); float m = 1.0 + 1.85*s*(max(0.1-y, 0.0)) ; im.rgb = (clamp(m*im.rgb, 0.0, 1.0)); float midAmt = s < 0.0 ? min(s*s,1.0) : 0.0; y = y*(1.0-y); im.rgb = sqrt(im.rgb); float pivot = .4; float a = midAmt*y; float b = -pivot*a; vec3 pix = im.r * vec3(0.299*a) + 
 im.g * vec3(0.587*a) + 
 im.b * vec3(0.114*a) + 
 im.rgb + vec3(b); im.rgb = mix(im.rgb, vec3(pivot), -y*midAmt); im.rgb = mix(im.rgb, pix, 0.8); im.rgb = max(im.rgb, 0.0); im.rgb *= im.rgb; im.rgb = clamp(im.rgb, 0.0,1.0)+pos+neg; return im; }
kernel vec4 _polyKernelHDR(__sample im, __sample adj, float str) { adj.r = 3.4*adj.r-1.2; vec3 neg = min(im.rgb, 0.0); vec3 pos = max(im.rgb, 1.0)-1.0; im.rgb = clamp(im.rgb, 0.0, 1.0); vec4 orig = im; float y = sqrt(dot(im.rgb, vec3(.33333))); float s = mix(0.0, adj.r, str); vec3 gain = s > 0.0 ? vec3(1.5*s) : vec3(1.75*s, 1.75*s, 1.55*s); im.rgb = im.rgb*im.rgb*gain + im.rgb*(1.0-gain); im.rgb = (clamp(im.rgb, 0.0, 1.0)); float midAmt = min(str, .5); y = y*(1.0-y); im.rgb = sqrt(im.rgb); float pivot = max(adj.g, 0.5); float a = midAmt*y; float b = -pivot*a; vec3 pix = im.r * vec3(0.299*a) + 
 im.g * vec3(0.587*a) + 
 im.b * vec3(0.114*a) + 
 im.rgb + vec3(b); 
 im.rgb = mix(im.rgb, vec3(pivot), -y*midAmt); im.rgb = mix(im.rgb, pix, 0.8); im.rgb = max(im.rgb, 0.0); im.rgb *= im.rgb; im.rgb = clamp(im.rgb, 0.0,1.0)+pos+neg; return im; }
-[PILocalLightFilterHDR outputImage]
PILocalLightHDR.m
CGRectEqualToRect([guideImage extent], [inputImage extent])
inputImage != nil
guideImage != nil
inputLocalLight != nil
CGRectEqualToRect([lightMapImage extent], [inputImage extent])
PILocalLightHDR
_lightMapImageFromData_block_invoke
x == 0
y == 0
width == lmWidth
height == lmHeight
CIEdgePreserveUpsampleRGFilter
v16@?0@"<NUMutableBuffer>"8
v24@?0@"<NUBufferTile>"8^B16
-[PISmartBlackAndWhiteAutoCalculator submit:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Autocalculators/PISmartBlackAndWhiteAutoCalculator.mm
PISmartBlackAndWhiteAutoCalculator-renderInputImage
Produced invalid BlackAndWhite settings, using defaults
void calculate_bw_auto_matrix_lum_contrast(__strong id<NUBuffer>, CGColorSpaceRef, MsgBlackAndWhiteSettings *)
allocator<T>::allocate(size_t n) 'n' exceeds maximum supported size
num == w
void MsgMatrix<double>::AppendRow(const NumberType *, uint) [MatrixFloatType = double, NumberType = double]
xi < w && yi < h
MatrixFloatType &MsgMatrix<double>::operator()(size_t, size_t) [MatrixFloatType = double]
index < data.size()
inputScaleFactor
kernel vec4 _smartBlackAndWhiteHDR(__sample imageHDR, sampler2D hueImage, vec4 rgbWeights, vec4 normalizer) { float hueTableScaleFactor = rgbWeights.w; float hueImageWidth = normalizer.x; float huePixelCenter = normalizer.y; float neutralGamma = normalizer.z; float phototone = normalizer.w; float bw = dot(imageHDR.rgb / 12.0, rgbWeights.rgb); bw = clamp(bw, 0.0, 1.0); vec3 lms; lms.x = dot(imageHDR.rgb, vec3(0.3139902162, 0.6395129383, 0.0464975462)); lms.y = dot(imageHDR.rgb, vec3(0.155372406, 0.7578944616, 0.0867014186)); lms.z = dot(imageHDR.rgb, vec3(0.017752387, 0.109442094, 0.8725692246)); lms = pow(lms, vec3(0.43)); float i = dot(lms, vec3(0.4,0.4,0.2)); float p = dot(lms, vec3(4.4550,-4.8510,0.3960)); float t = dot(lms, vec3(0.8056,0.3572,-1.1628)); float chroma = sqrt(p*p+t*t); float hue = 0.5 + (atan(t, p) / 6.28318530718); vec2 huePt = vec2(hue * hueImageWidth + huePixelCenter, 0.5); float hueGamma = hueTableScaleFactor * texture2D(hueImage, huePt).a; float cd = 0.06 + 0.53 * abs(i-0.5); float lowSaturationDamp = smoothstep(0.0, 1.0, (chroma)/cd); float intensityDamp = smoothstep(0.0, 1.0, 1.0 - i); float lowLuminosityDamp = smoothstep(0.0, 1.0, 25.0 * i); float hWeight = lowSaturationDamp * intensityDamp * lowLuminosityDamp; hueGamma -= 1; hueGamma *= hWeight; hueGamma += 1; bw = pow(bw, hueGamma); float bwSDR = clamp(bw * 12.0, 0.0, 1.0); float midLumWeight = bwSDR*(1.0 - bwSDR); float grayWeight = 1.0 - smoothstep(0.0, 1.0, chroma * 10.0); float nWeight = midLumWeight * grayWeight; neutralGamma -= 1; neutralGamma *= nWeight; neutralGamma *= -2; neutralGamma += 1; bw = pow(bw, neutralGamma); bw = bw * 12.0; bw = clamp(bw, 0.0, 12.0); float df0 = 0.812379; float result; if (bw < df0) { result = 1.8031*bw*bw*bw - 2.1972*bw*bw + 1.3823*bw; } else { float scale = 12.0 - df0; float x = (bw - df0) / scale; result = 1.8031*x*x*x - 2.1972*x*x + 1.3823*x; result = result * scale + df0; result -= 0.158305860; } bw = mix(bw, result,-phototone); return vec4(bw,bw,bw,imageHDR.a); }
PIPhotoGrainHDR
inputISO
There is no need to call smartBlackAndWhiteStatistics.
Just use [CIFilter filterWithName:@"PISmartBlackAndWhiteHDR"] instead.
There is no need to call smartBlackAndWhiteAdjustmentsForValue:andStatistics:.
-[PILevelsAutoCalculator submit:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Autocalculators/PILevelsAutoCalculator.m
PILevelsAutoCalculator-histogram
pre-Levels
-[PILevelsAutoCalculator calculateSettingsForImageHistogram:]
blackSrc
blackDst
shadowSrc
shadowDst
midSrc
midDst
hilightSrc
hilightDst
whiteSrc
whiteDst
Adjustment controller for key %@ is of class: %@, but was expected to be %@
-[PICompositionController(AdjustmentExtensions) _adjustmentControllerForKey:creatingIfNecessary:expectedClass:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Controllers/PICompositionController+AdjustmentExtensions.m
<%@:%p> [(%.3f, %.3f), %s]
editable
not editable
Warning smartToneStatistics will soon need [receiver properties] be non-nil so flash-fired state can be determined.
tonalRange
highKey
blackPoint
whitePoint
kernel vec4 _smarttone_brightness_neg_HDR (__sample c, float gamma) 
 vec3 neg = min(c.rgb, 0.0); 
 c.rgb = max(c.rgb, 0.0); 
 vec3 pix = pow(c.rgb, vec3(gamma)); 
 float lum = dot(c.rgb, vec3(0.39, .5, .11)); 
 vec3 pix2 = lum>0.0 ? c.rgb*pow(lum, gamma)/lum : vec3(0.0); 
 pix = mix(pix, pix2, 0.8) + neg; 
 return vec4(pix, c.a); 
kernel vec4 _smarttone_brightness_pos_HDR (__sample c, float gamma) 
 vec3 neg = min(c.rgb, 0.0); 
 vec3 pos = max(c.rgb, 1.0)-1.0; 
 c.rgb = clamp(c.rgb, 0.0, 1.0); 
 vec3 m = 1.0-c.rgb; 
 float a = 0.6; 
 vec4 result = c; 
 result.rgb = 1.0 - (pow(m, vec3(gamma))+a*( ((gamma-1.0)*m*(1.0-m*m))/(gamma*gamma))); 
 c.rgb = pow(c.rgb, vec3(1.0-((min(gamma, 2.95)-1.0)/2.6))); 
 result.rgb = mix(c.rgb, result.rgb, .85); 
 result.rgb = result.rgb+neg+pos; 
 return result; 
kernel vec4 _smarttone_contrast_HDR (__sample im, float midAmt) 
 vec3 neg = min(im.rgb, 0.0); 
 vec3 pos = max(im.rgb, 1.0)-1.0; 
 im.rgb = clamp(im.rgb, 0.0, 1.0); 
 float y = dot(im.rgb, vec3(0.3333)); 
 y = sqrt(y); 
 float sat = (im.r-y)*(im.r-y)+(im.g-y)*(im.g-y)+(im.b-y)*(im.b-y); 
 y = y*(1.0-y); 
 im.rgb = sqrt(im.rgb); 
 float a = midAmt*y; 
 float b = -0.5*a; 
 vec3 pix = im.r * vec3(0.299*a) + 
 im.g * vec3(0.587*a) + 
 im.b * vec3(0.114*a) + 
 im.rgb + vec3(b); 
 im.rgb = mix(im.rgb, vec3(0.5), -y*midAmt); 
 im.rgb = mix(im.rgb, pix, 0.8+sat); 
 im.rgb = max(im.rgb, 0.0); 
 im.rgb *= im.rgb; 
 im.rgb = im.rgb + neg + pos; 
 return im; 
kernel vec4 _smarttone_contrast_HDR (__sample im, float midAmt, float maxVal) 
 midAmt = midAmt / maxVal; 
 vec3 neg = min(im.rgb, 0.0); 
 vec3 pos = max(im.rgb, maxVal) - maxVal; 
 im.rgb = clamp(im.rgb, 0.0, maxVal); 
 float y = dot(im.rgb, vec3(0.3333)); 
 y = sqrt(y); 
 im.rgb = sqrt(im.rgb); 
 float sat = dot(im.rgb - vec3(y), im.rgb - vec3(y)) / sqrt(maxVal); 
 y = y*(sqrt(maxVal)-y); 
 float a = midAmt*y; 
 float b = -0.5*a; 
 vec3 pix = im.r * vec3(0.299*a) + 
 im.g * vec3(0.587*a) + 
 im.b * vec3(0.114*a) + 
 im.rgb + vec3(b); 
 im.rgb = mix(im.rgb, vec3(0.5), -a); 
 im.rgb = mix(im.rgb, pix, clamp(0.8+sat, 0.0, 1.0)); 
 im.rgb = max(im.rgb, 0.0); 
 im.rgb *= im.rgb; 
 im.rgb = im.rgb + neg + pos; 
 return im; 
kernel vec4 _smarttone_highlightcontrast_HDR (__sample pix, float highAmt, float sat) 
 float maxVal = 1.0; vec3 neg = min(pix.rgb, 0.0); 
 vec3 pos = max(pix.rgb, maxVal) - maxVal; 
 pix.rgb = clamp(pix.rgb, 0.0, maxVal); 
 float lum = clamp(dot(pix.rgb, vec3(.3333)),0.0,1.0); 
 vec3 high = pow(pix.rgb, vec3(3.0 - 2.0*highAmt)); 
 float pivot = 0.8; 
 vec3 pix1 = (high - pivot)*(4.0 - 3.0*highAmt) + pivot; 
 float h = highAmt*highAmt*highAmt*highAmt; 
 float a = (4.0 - 3.0*h); 
 vec3 pix2 = (lum-pivot)*a+pivot + high.rgb -lum; 
 high = mix(pix2, pix1, sat); 
 pix.rgb = mix(pix.rgb, high, lum*lum); 
 pix.rgb = pix.rgb + neg + pos; 
 return pix; 
kernel vec4 _rawHighlightsHDR(__sample pix, float gain) 
 vec3 high = gain*pix.rgb; 
 float lum = clamp(dot(pix.rgb, vec3(.3333)), 0.0, 1.0); 
 vec3 neg = min(high, 0.0); 
 high.rgb = mix(max(pix.rgb, 0.0), high.rgb, lum*lum) + neg; 
 return vec4(high, pix.a); 
CIHighlightShadowAdjust
inputShadowAmount
inputHighlightAmount
PISmartToneFilterHDR-histogram
PI_LONG_EXPOSURE_FUSION_PARAMS
@"NSString"8@?0
kNCCBlurHalfSize
kNCCEdge0
kNCCEdge1
kBlendMaskThreshold0
kBlendMaskThreshold1
PI_LONG_EXPOSURE_DUMP_INTERMEDIATES
long-exp-input-image.tiff
long-exp-mask-image.tiff
long-exp-still-image.tiff
long-exp-guide-image.tiff
long-exp-ncc-map-image.tiff
long-exp-refined-mask-image.tiff
long-exp-fusion-image.tiff
composition != nil
-[PIModernPhotosPipeline _processedRenderNodeForComposition:input:pipelineState:error:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Pipeline/PIModernPhotosPipeline.m
pipelineState != nil
LongExposure
inputSushiLevel
kCGImageSourceShouldExtendRaw
inputGamutMapMax
inputNoiseReductionDetailAmount
inputUIColorNoiseReductionAmount
inputUILuminanceNoiseReductionAmount
sharpness
inputNoiseReductionSharpnessAmount
contrast
inputNoiseReductionContrastAmount
inputNeutralTemperature
inputNeutralTint
Video
mediaComponentType
hardCropCleanAperture
defaultFrameTime
skipOrientation
Master
unsupported sourceSelect adjustment
RAW/Linear
Source
ShowOriginalSource
PIForwardFakeBoost
inputBoost
inputVersion
inputParams
Intermediate
keepCacheWhenAtOneToOne
Disparity
Image
Source does not contain depth
PortraitEffectsMatte
HairSegmentationMatte
SkinSegmentationMatte
TeethSegmentationMatte
GlassesSegmentationMatte
MeteorPlusGainMap
focusRect
faces
inputAperture
inputFocusRect
leftEyeX
leftEyeY
rightEyeX
rightEyeY
noseX
noseY
chinX
chinY
inputLeftEyePosition
inputRightEyePosition
inputFaceMidPoint
inputChinPosition
Missing required depth settings
inputShiftmapImage
inputCalibrationData
inputAuxDataMetadata
inputOriginalSize
CIDepthEffectMakeBlurMap
inputMatteImage
inputHairImage
inputGlassesImage
inputPower
inputEV
faceLandmarks
inputFaceLandmarkArray
inputDisparity
inputMatte
inputBlurMap
inputFaceMask
inputHairMask
inputTeethMask
CIPortraitEffectV2
inputGenerateSpillMatte
inputFullSizeImage
CIPortraitEffect%@
PortraitV2-zeroStrength
PortraitV2
pre-PortraitVideo
auxiliaryImageType
nativeScale
post-PortraitVideo
inputLumaNoiseScale
lumaNoiseScale
shape
inputShape
CIDepthEffectApplyBlurMap
inputGainMap
masterSpace
pre-WB
inputWarmTemp
inputWarmTint
inputHasFace
inputIsRaw
inputOrigI
inputOrigQ
inputWarmth
warmth
pre-Mute
pre-LivePhotoKeyFrame
pre-Trim
pre-SloMo
SloMo
inputConfidence
inputFaceBoxArray
CIDynamicFood
inputBoundingBoxArray
sunsetOrSunrise
CIDynamicRender
Unknown sceneLabel when rendering semantic enhance adjustment
inputTargetImage
inputTime
inputGuideImage
inputLightMapImage
inputSaturation
pre-Curves
inputPointsR
inputPointsG
inputPointsB
inputPointsL
inputTableImage
green
blue
spread
hueShift
saturation
inputCorrections
CIPhotoEffect%@
filterVersion
pre-VideoReframe
pre-VideoStabilize
pre-VideoCrossfadeLoop
pre-Geometry
pre-Crop
perspectiveStraighten
resetCleanAperture
{?={?=qq}{?=qq}}
projectUsingCleanAperture
{?=qq}
projectUsingOriginalSize
com.apple.quicktime.live-photo.vitality-disabled
projectUsingEstimatedCleanAperture
pre-Orientation
post-Geometry
inputCenter
post-Adjustments
PIInverseFakeBoost
Master/RAW/Linear
inputCutoff
kernel vec4 levelsNewGammaForP3 (sampler src, sampler LUT)
vec4
p,r;
vec2
c1,c2;
float
p  = sample(src, samplerCoord(src));
vec3 neg = min(p.rgb, 0.0);
vec3 pos = max(p.rgb, 1.0)-1.0;
p.rgb = clamp(p.rgb, 0.0, 1.0);
f = p.r * 255.0 + 256.0;
c1 = vec2(floor(f)+0.5, 0.5);
c2 = vec2(ceil(f)+0.5, 0.5);
r = mix(sample(LUT,samplerTransform(LUT,c1)), sample(LUT,samplerTransform(LUT,c2)), fract(f));
p.r = r.r * 4.0 - 1.0;
f = p.g * 255.0 + 256.0;
c1 = vec2(floor(f)+0.5, 0.5);
c2 = vec2(ceil(f)+0.5, 0.5);
r = mix(sample(LUT,samplerTransform(LUT,c1)), sample(LUT,samplerTransform(LUT,c2)), fract(f));
p.g = r.g * 4.0 - 1.0;
f = p.b * 255.0 + 256.0;
c1 = vec2(floor(f)+0.5, 0.5);
c2 = vec2(ceil(f)+0.5, 0.5);
r = mix(sample(LUT,samplerTransform(LUT,c1)), sample(LUT,samplerTransform(LUT,c2)), fract(f));
p.b = r.b * 4.0 - 1.0;
p.rgb = max(p.rgb, 0.0);
p.rgb = p.rgb + pos + neg;
return p;
-[PILevelsFilter _LUTImage]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Adjustments/PILevelsFilter.m
Mirror
Adjustment
SourceSelect~1.0
enum
values
Failed to register schema %@: %@
+[PISchema sourceSelectSchema]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Pipeline/PISchema.m
RAW~1.0
opaque
required
+[PISchema rawSchema]
RawNoiseReduction~1.0
bool
default
number
minimum
maximum
ui_minimum
ui_maximum
+[PISchema rawNoiseReductionSchema]
SmartTone~1.0
+[PISchema smartToneSchema]
SmartColor~1.0
+[PISchema smartColorSchema]
SmartBlackAndWhite~1.0
+[PISchema smartBlackAndWhiteSchema]
Grain~1.0
+[PISchema grainSchema]
Sharpen~1.0
+[PISchema sharpenSchema]
CropStraighten~1.0
+[PISchema cropSchema]
Trim~1.0
+[PISchema trimSchema]
SlowMotion~1.0
+[PISchema slomoSchema]
LivePhotoKeyFrame~1.0
+[PISchema livePhotoKeyFrameSchema]
Mute~1.0
+[PISchema muteSchema]
VideoPosterFrame~1.0
+[PISchema videoPosterFrameSchema]
AutoLoop~1.0
+[PISchema autoLoopSchema]
HighResolutionFusion~1.0
+[PISchema highResFusionSchema]
DepthEffect~1.0
+[PISchema depthEffectSchema]
Effect3D~1.0
+[PISchema effect3DSchema]
PortraitEffect~1.0
+[PISchema portraitEffectSchema]
+[PISchema effectSchema]
iPhone
+[PISchema redEyeSchema]
array
content
compound
properties
+[PISchema apertureRedEyeSchema]
+[PISchema retouchSchema]
+[PISchema vignetteSchema]
Orientation~1.0
+[PISchema orientationSchema]
+[PISchema definitionSchema]
+[PISchema noiseReductionSchema]
grayStrength
identity
+[PISchema whiteBalanceSchema]
+[PISchema levelsSchema]
+[PISchema curvesSchema]
+[PISchema selectiveColorSchema]
VideoReframe~1.0
+[PISchema videoReframeSchema]
VideoStabilize~1.0
pixel
gyro
+[PISchema videoStabilizeSchema]
VideoCrossfadeLoop~1.0
+[PISchema videoCrossfadeLoopSchema]
Debug
+[PISchema debugSchema]
SemanticEnhance~1.0
+[PISchema semanticEnhance]
PortraitVideo~1.0
+[PISchema portraitVideoSchema]
Composition
PhotosComposition
contents
com.apple.photo:Source~1.0
Retouch~1.0
WhiteBalance~1.0
RedEye~1.0
ApertureRedEye~1.0
Definition~1.0
Effect~1.0
Vignette~1.0
NoiseReduction~1.0
Levels~1.0
Curves~1.0
SelectiveColor~1.0
Debug~1.0
+[PISchema photosCompositionSchema]
failed to register %@: %@
+[PISchema registerPhotosSchema]
com.apple.mobileslideshow.reframe.photo.eligible-pass
com.apple.mobileslideshow.reframe.photo.eligible-fail
com.apple.mobileslideshow.reframe.video.eligible-pass
com.apple.mobileslideshow.reframe.video.eligible-fail
com.apple.mobileslideshow.reframe.type.photo.horizon
com.apple.mobileslideshow.reframe.type.photo.perspective
PICompositionFinalizer-primaryImageProperties
PICompositionFinalizer-overcaptureImageProperties
v16@?0@"PIAdjustmentController"8
maxCameraAutoStraightenCorrection
Capture
maxCameraVerticalPerspectiveCorrection
maxCameraHorizontalPerspectiveCorrection
maxCameraRotatePerspectiveCorrection
minHorizontalCameraPerspectiveCorrection
minVerticalCameraPerspectiveCorrection
minRotateCameraPerspectiveCorrection
enable_perspective_correction
reframe
perpsective
horizon
PICropAutoCalculator-imageProperties
-[PICropAutoCalculator undoExifOrientation:error:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Autocalculators/PICropAutoCalculator.m
Source geometry has 0 size
-[PICropAutoCalculator submit:]
PICropAutoCalculator-faceDetection
{CGRect={CGPoint=dd}{CGSize=dd}}
maxAutoStraighten
filterOptions
CIAutoStraighten
straightenAngleInDegreesCCW
limitExceeded
belowMinimum
autoCrop
result
error
%@AutoCropEvaluation-txt.DBG
v16@?0@"PISourceSelectAdjustmentController"8
v16@?0@"PIAutoLoopAdjustmentController"8
PICropAutoCalculator-stitchedOvercaptureRect
+[PICropAutoCalculator(Utilities) updateCropAdjustment:after:error:]
PICropAutoCalculator-getBeforeGeometry
PICropAutoCalculator-getAfterGeometry
v16@?0@"PICropAdjustmentController"8
kernel vec4 iptLumHueSatTable(sampler image, __table sampler hueSatLumTable)
vec4 im = sample(image, samplerCoord(image)) ;
vec4 result = im;
float hueIdx = 359.0 * 0.5 * (atan(im.b, im.g)/3.1416 + 1.0);
hueIdx = clamp(hueIdx, 0.0, 359.0) + 0.5;
float hueChange = (sample(hueSatLumTable, samplerTransform(hueSatLumTable, vec2(hueIdx,0.5))).r);
float satChange = (sample(hueSatLumTable, samplerTransform(hueSatLumTable, vec2(hueIdx,0.5))).g);
float lumChange = (sample(hueSatLumTable, samplerTransform(hueSatLumTable, vec2(hueIdx,0.5))).b);
float chroma = sqrt(im.g*im.g+im.b*im.b) ;
chroma *= satChange ;
float hue = atan(im.b, im.g) + hueChange ;
vec3 adjustIm = im.rgb;
float hueAngle = hue  ;
lumChange = mix(1.0, lumChange, clamp(chroma,-0.7,0.7));
adjustIm.r *= lumChange;
adjustIm.g = chroma * cos(hueAngle) ;
adjustIm.b = chroma * sin(hueAngle) ;
result.rgb = adjustIm.rgb;
result.a = im.a ;
return result ;
kernel vec4 srgbToIPT(sampler image){
vec4 im = sample(image, samplerCoord(image));
vec3 lms, ipt;
lms = im.r * vec3(0.3139902162, 0.155372406, 0.017752387) +
im.g * vec3(0.6395129383, 0.7578944616, 0.109442094) +
im.b * vec3(0.0464975462, 0.0867014186, 0.8725692246);
lms = sign(lms)*pow(abs(lms), vec3(0.43, 0.43, 0.43));
ipt = lms.r * vec3(0.4, 4.455, 0.8056) +
lms.g * vec3(0.4, -4.851, 0.3572) +
lms.b * vec3(0.2, 0.3960, -1.1628);
return vec4(ipt, im.a);
kernel vec4 iptToSRGB(sampler image){
vec4 im = sample(image, samplerCoord(image));
vec3 lms, rgb;
lms = im.rrr +
im.g * vec3(0.09756893,-0.11387649,0.03261511) +
im.b * vec3(0.20522644, 0.13321716,  -0.67688718);
lms = sign(lms)*pow(abs(lms), vec3(1.0/.43));
rgb = lms.r * vec3( 5.472212058380287,  -1.125241895533569,   0.029801651173470) +
lms.g * vec3(-4.641960098354470, 2.293170938060623, -0.193180728257140) +
lms.b * vec3(0.169637076827974,  -0.167895202223709, 1.163647892783812);
return vec4(rgb, im.a);
kernel vec4 add_gaussian(sampler srcTable, float tableSize, float hueAmplitude, float satAmplitude, float lumAmplitude, float gaussX, float gaussSigmaSquared) {
vec2 d = destCoord();
vec4 src = sample(srcTable, samplerCoord(srcTable));
float x = d.x / (tableSize - 1.0);
float dist = min(min(abs(x - gaussX), abs(x - 1.0 - gaussX)), abs(x + 1.0 - gaussX));
float p = -((dist * dist) / (2.0 * gaussSigmaSquared));
float ep = exp(p);
float hue = hueAmplitude * ep;
float sat = satAmplitude * ep;
float lum = lumAmplitude * ep;
float h = clamp(src.r + hue, -1.0, 1.0);
float s = clamp(src.g + sat, -1.0, 1.0);
float l = clamp(src.b + lum, -1.0, 1.0);
return vec4(h,s,l,1.0);
srgbToIPT
iptToSRGB
CIConstantColorGenerator
add_gaussian
iptLumHueSatTable
PIVideoReframeNode.m
invalid crop rect
pipelineState
-[PIVideoReframeNode initWithSettings:inputs:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Render/PIVideoReframeNode.m
-[PIVideoReframeNode _evaluateVideoProperties:]
error != nil
-[PIVideoReframeNode _evaluateImageGeometry:]
Failed to get input geometry
Could not get the input geometry
Could not get the input image properties
CIPerspectiveTransform
inputTopLeft
inputTopRight
inputBottomLeft
inputBottomRight
showStabilizationWatermark
init is not a valid initializer
Invalid plist at path: %@
Time
Subjects
Type
HumanBody
HumanFace
CatBody
DogBody
-[PIVideoReframeMetadataExtractor overwriteTrackingMetadataWithPlist:]
PIVideoReframeMetadataExtractor.mm
type >= 0
Confidence
-[PIVideoReframeMetadataExtractor motionBlurVectorFromMetadata:]
-[PIVideoReframeMetadataExtractor extractMetadata]
mdataGroup.items.count == 1
0 && "unexpected metadata identifier"
0 && "unexpected metadata data type"
err == noErr
kernel vec4 convertFromRGBToYIQ(sampler src, float gamma)
vec4 pix ;
vec3 pix2;
pix = sample(src, samplerCoord(src));
pix.rgb = sign(pix.rgb)*pow(abs(pix.rgb), vec3(1.0/gamma)) ;
pix2.rgb = pix.r * vec3(0.299,  0.595716,  0.211456) +
pix.g * vec3(0.587, -0.274453, -0.522591) +
pix.b * vec3(0.114, -0.321263,  0.311135);
return vec4(pix2, pix.a);
kernel vec4 convertFromYIQToRGB(sampler src, float gamma)
vec4 color, pix;
pix = sample(src, samplerCoord(src));
color.rgb = pix.rrr +
pix.g * vec3(0.956296, -0.272122, -1.10699) +
pix.b * vec3(0.621024, -0.647381, 1.70461);
color.rgb = sign(color.rgb)*pow(abs(color.rgb), vec3(gamma, gamma, gamma) );
color.a = pix.a;
return color;
kernel vec4 whiteBalance(sampler image, float grayY, float grayI, float grayQ, float strength)
vec4 im = sample(image, samplerCoord(image)) ;
vec2 grayOffset = vec2(grayI, grayQ) ;
vec4 result ;
float newStrength = 1.0 + (strength-1.0)*(1.0-im.r) ;
result.r = im.r ;
result.gb = im.gb + newStrength*grayOffset ;
float damp = max(min(1.0, im.r/(grayY+0.00001)),0.0) ;
result.rgb = mix(im.rgb, result.rgb, damp) ;
result.a = im.a ;
return result ;
kernel vec4 gHDRtoPP(sampler image)
vec4 pix ;
vec3 pix2;
pix = sample(image, samplerCoord(image));
pix2 = pix.r * vec3(0.615429622407401,   0.114831839141528,   0.011544126697221) +
pix.g * vec3(0.367479646665836,   0.797943554457996,   0.064077744191180) +
pix.b * vec3(  0.016956659608091,   0.087783443422360,   0.924405601458102);
return vec4(pix2, pix.a);
kernel vec4 PPtogHDR(sampler image)
vec4 pix ;
vec3 pix2;
pix = sample(image, samplerCoord(image));
pix2 = pix.r * vec3(1.777445503202045,  -0.255296595099306,  -0.004500433755654) +
pix.g * vec3( -0.822224875430495,   1.380948853784730,  -0.085456231694984) +
pix.b * vec3(0.045475917061484,  -0.126454737973025,   1.089973874037625);
return vec4(pix2, pix.a);
convertFromRGBToYIQ
convertFromYIQToRGB
-[PINeutralGrayWhiteBalanceFilter outputImage]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Adjustments/PINeutralGrayWhiteBalanceFilter.m
PISmartToneAutoCalculator
-[PISmartToneAutoCalculator submit:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Autocalculators/PIAutoCalculators.mm
v8@?0
-[PISmartColorAutoCalculator submit:]
-[PIRedEyeAutoCalculator submit:]
PIRedEyeAutoCalculator-getLensInfo
locationX
locationY
touchDiameter
/masterSpace
kernel vec4 _blendGrainsHDR(__sample isoImages, float log10iso)
  vec4 c = isoImages; 
  float mix10_50    = mix(c.r, c.g, log10iso*1.43067655809 
                                           - 1.43067655809); 
  float mix50_400   = mix(c.g, c.b, log10iso*1.10730936496 
                                           - 1.88128539659); 
  float mix400_3200 = mix(c.b, c.a, log10iso*1.10730936496 
                                           - 2.88128539659); 
  float v = compare(log10iso - 1.69897000434,                     mix10_50,                     compare(log10iso - 2.60205999133,                             mix50_400,                             mix400_3200)); 
  return vec4(v,v,v,1.0);
kernel vec4 _grainBlendAndMixHDR(__sample img, __sample grainImage, float contrast, float mixAmount)
  vec3 rgb = img.rgb;
  float luminance = clamp(dot(rgb, vec3(.333333)), 0.0, 1.0); 
  float gamma = 4.01 - 2.0*luminance;
  rgb = sign(rgb) * pow(abs(rgb), vec3(1.0/gamma));
  float grain = grainImage.r - 0.5;
  float mult = contrast * grain;
  rgb += (max(luminance, 0.5) * mult * (1.0-luminance));
  rgb = sign(rgb) * pow(abs(rgb), vec3(gamma));
  rgb = min(rgb, 12.0);
  return mix(img, vec4(rgb,img.a), mixAmount);
kernel vec2 _paddedTile2(vec4 k) { return fract(destCoord() * k.zw) * k.xy + vec2(1.0); }
{CGRect={CGPoint=dd}{CGSize=dd}}44@?0i8{CGRect={CGPoint=dd}{CGSize=dd}}12
1536 x 1536 noise grain image prov
v56@?0^v8Q16Q24Q32Q40Q48
generateNoiseImage_block_invoke
PIPhotoGrainHDR.m
width==512*3
height==512*3
x==0
y==0
kernel vec4 _grainGenCombineHDR (__sample r, __sample g, __sample b, __sample a)
{ return vec4(r.x, g.x, b.x, a.x); }
CIUnsharpMask
image != nil
+[PIImageIO writeImage:fileURL:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Util/PIImageIO.m
cgImage != NULL
+[PIImageIO writeCGImage:fileURL:]
fileURL != nil
Unhandled bit depth: %ld
+[PIImageIO writeCGImage:fileURL:options:]
Successfully wrote image to file %@
Failed to write image to file %@
%@-%d-%ld.tiff
GU %@ %@
Sensitivity
Bad float to fixed 16 conversion
+[PIApertureRedEyeProcessorKernel convertFloat:toFixed16:count:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Adjustments/ApertureRedEye/PIApertureRedEyeFilter.mm
Bad fixed 16 to float conversion
+[PIApertureRedEyeProcessorKernel convertFixed16:toFloat:count:]
+[PIApertureRedEyeProcessorKernel processWithInputs:arguments:output:error:]
PIApertureRedEyeFilter.mm
output.format == kCIFormatRGBAf
inputSensitivity
portraitStrength
capturedPortraitMajorVersion
capturedPortraitMinorVersion
portraitDisableStage
+[PIValuesAtCapture valuesAtCaptureFromImageProperties:error:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Autocalculators/PIPortraitAutoCalculator.m
Portrait was previously applied.
Failed to load depth data
Unfiltered depth data is not supported
Low quality depth data is not supported
Missing camera calibration data
Failed to load auxiliary data
Missing auxiliary metadata
Depth data version mismatch, asset has %@ but we can only handle %@
maxSDOFRenderingVersionSupported
We only know up to sDOF version %d. Found: %d
<%@:%p aperture:%f minimumAperture:%@ maximumAperture:%@ portraitStrength:%f SDOFRenderingVersion:%lu portraitMajorVersion:%lu portraitMinorVersion:%lu>
-[PIPortraitAutoCalculator submit:]
PIPortraitAutoCalculator-getValuesAtCapture-imageProperties
capturedAperture
capturedPortraitStrength
v16@?0@"NUResponse"8
PIPortraitAutoCalculator-faceDetect
faceObservations != nil
+[PIPortraitAutoCalculator portraitInfoDictionaryFromFaceObservations:metadata:orientation:valuesAtCapture:]
metadata != nil
NUOrientationIsValid(orientation)
Insufficient number of landmark points
+[PIPortraitAutoCalculator depthEffectInfoDictionaryFromFaceObservations:focus:valuesAtCapture:lumaNoiseScale:orientation:]
minimumAperture
maximumAperture
+[PIPortraitAutoCalculator portraitEffectInfoDictionaryFromFaceObservations:orientation:valuesAtCapture:]
faceBoundingBox
faceJunkinessIndex
faceOrientationIndex
allPoints
faceContour
leftEye
rightEye
leftEyebrow
rightEyebrow
nose
noseCrest
medianLine
outerLips
innerLips
leftPupil
rightPupil
+[PIPortraitAutoCalculator portraitInfoDictionaryFromCameraMetadata:]
-[PIDepthEffectApertureAutoCalculator submit:]
PIDepthEffectApertureAutoCalculator-getValuesAtCapture-imageProperties
depthData:DepthDataVersion
var tagNode = GetTag('/pre-Adjustments');
orientation = tagNode.geometry.orientation;
var node = Orient(tagNode, orientation);
return node;
@"NURenderNode"40@?0@"NURenderPipelineHelper"8@"NUComposition"16@"NURenderNode"24^@32
/pre-Adjustments
var source = GetTag('/pre-Adjustments');
ResetTagInput('/pre-Crop', source);
source = GetTag('/Crop');
orientation = source.geometry.orientation;
source = Orient(source, orientation);
return source;
/pre-Crop
/Crop
var sourceSettings = { 'skipOrientation': true };
var rawAdjustment = composition.raw
if (rawAdjustment) {
    sourceSettings['inputDecoderVersion'] = rawAdjustment.inputDecoderVersion
var image = Source(composition.source, sourceSettings)
var rawImage = GetTagInput('RAW/Linear')
if (rawImage) {
    image = rawImage
    image = Filter('PIForwardFakeBoost', {'inputImage' : image, 'inputBoost' : 1.0,})
var orientation = composition.orientation
if (orientation) { image = Orient(image, orientation.value) }
return image
Raw/Linear
var sourceSettings = { }; 
var raw = composition.raw; 
if (raw) { 
  sourceSettings['inputDecoderVersion'] = raw.inputDecoderVersion; 
  var sushiLevel = raw.inputSushiLevel;
  if (sushiLevel) {
    sourceSettings['kCGImageSourceShouldExtendRaw'] = sushiLevel;
} else { throw "expected RAW in rawSourceFilterIncludingOrientation"; }
return Source(composition.source, sourceSettings); 
expected RAW in rawSourceFilterIncludingOrientation
+[PIPipelineFilters rawSourceFilterIncludingOrientation]_block_invoke
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Pipeline/PIPipelineFilters.m
var sourceSettings = { 'skipOrientation' : true }; 
var raw = composition.raw; 
if (raw) { 
  sourceSettings['inputDecoderVersion'] = raw.inputDecoderVersion; 
  var sushiLevel = raw.inputSushiLevel;
  if (sushiLevel) {
    sourceSettings['kCGImageSourceShouldExtendRaw'] = sushiLevel;
  return Source(composition.source, sourceSettings); 
} else {
  return GetTag("/ShowOriginalSource");
var source = GetTag('/pre-RedEye');ResetTagInput('/post-RedEye', source);
return GetTag('/post-RedEye');
/pre-RedEye
/post-RedEye
Could not construct noRedEye filter from inline source
+[PIPipelineFilters noRedEyeFilter]
var output = input
var preTrim = GetTag('/pre-Trim');ResetTagInput('/SloMo', preTrim);
let slomo = composition.slomo
if (slomo) {
    var startTime = TimeMake(slomo.start, slomo.startScale)
    var endTime = TimeMake(slomo.end, slomo.endScale)
    output = SloMo(input, startTime, endTime, slomo.rate)
return output;
/pre-Trim
/SloMo
Could not construct noTrimFilter filter from inline source
+[PIPipelineFilters noTrimFilter]
var preMute = GetTag('pre-Mute');ResetTagInput('Mute', preMute);
return GetTag('Image');
Could not construct noMuteFilter filter from inline source
+[PIPipelineFilters noMuteFilter]
var preCrop = GetTag('/pre-Crop');ResetTagInput('/pre-Orientation', preCrop);
return input;
/pre-Orientation
Could not construct noCropFilter filter from inline source
+[PIPipelineFilters noCropFilter]
let preGeometry = GetTag('/pre-Geometry');ResetTagInput('/post-Adjustments', preGeometry);
return input;
/post-Adjustments
Could not construct noGeometry filter from inline source
+[PIPipelineFilters iosCropToolFilter]_block_invoke
var image = GetTag('pre-Trim');ResetTagInput('Trim', image);
image = GetTag('pre-SloMo');ResetTagInput('SloMo', image);
return input;
Could not construct stripAllTimeAdjustmentsFilter filter from inline source
+[PIPipelineFilters stripAllTimeAdjustmentsFilter]
let preGeometry = GetTag('/pre-Geometry');ResetTagInput('/post-Geometry', preGeometry);
return input;
+[PIPipelineFilters noGeometryFilter]
var preOrientation = GetTag('/pre-Orientation');ResetTagInput('/Orientation', preOrientation);
return input;
/Orientation
Could not construct noOrientationFilter filter from inline source
+[PIPipelineFilters noOrientationFilter]
return null;
Could not construct pipeline filter from source: %@
+[PIPipelineFilters orientationAsMetaDataFilter]
var preCrop = GetTag('/perspectiveStraighten');if (preCrop) {   ResetTagInput('/Crop', preCrop);
}return input;
/perspectiveStraighten
Could not construct perspectiveStraightenWithoutCropFilter filter from inline source
+[PIPipelineFilters perspectiveStraightenWithoutCropFilter]
var output = input;
var prePortraitVideoNode = GetTag('pre-PortraitVideo');
if (prePortraitVideoNode) {
    ResetTagInput('/post-PortraitVideo', prePortraitVideoNode);
return output;
/post-PortraitVideo
Could not construct histogramOptimizationFilter from inline source
+[PIPipelineFilters histogramOptimizationFilter]
var tagNode = GetTag('%@');
if (tagNode) {
    ResetTagInput('/pre-Geometry', tagNode);
return GetTag('/post-Geometry');
Could not construct stopAtTagIncludeGeometryFilter from inline source
+[PIPipelineFilters stopAtTagIncludeGeometryFilter:]
var tagNode = GetTag('%@');
if (tagNode) {
    ResetTagInput('/pre-Orientation', tagNode);
return GetTag('/Orientation');
Could not construct stopAtTagIncludeOrientationFilter from inline source
+[PIPipelineFilters stopAtTagIncludeOrientationFilter:]
var output = input;
var orientation = composition.orientation;
if (orientation) {
    output = Orient(input, orientation.value);
return output;
+[PIPipelineFilters applyOrientationFilter]
ResetTagInput('/AutoLoop/Output', GetTag('/AutoLoop/StabilizedVideo'));
return input;
/AutoLoop/Output
/AutoLoop/StabilizedVideo
Could not construct autoloopStabilizedVideoFilter filter from inline source
+[PIPipelineFilters autoloopStabilizedVideoFilter]
return Source(composition.sourceSpatialOvercapture, { 'skipOrientation' : true })
Could not construct overcaptureSourceFilter filter from inline source
+[PIPipelineFilters overcaptureSourceFilter]
return Source(composition.source, { 'skipOrientation' : true })
Could not construct primarySourceFilter filter from inline source
+[PIPipelineFilters primarySourceFilter]
return Source(composition.sourceSpatialOvercapture, {  })
+[PIPipelineFilters spatialOvercaptureVideoSourceFilter]
return input;
/PortraitV2-zeroStrength
/PortraitV2
Could not construct oneShotPortraitV2ExportFilter filter from inline source
+[PIPipelineFilters oneShotPortraitV2ExportFilter]
let output = input;
var image = GetTag('pre-VideoReframe');
if (image) {
    var videoReframe = composition.videoReframe;
    if (videoReframe && videoReframe.enabled) {
        image = VideoReframe(image, videoReframe);
        image = Filter(`CIColorMatrix`, {'inputImage' : image, 'inputBVector' : CIVectorMake(1,1,1,0)});
        ResetTagInput('VideoReframe', image);
    }
else { // image or live-photo { 
    var crop = composition.cropStraighten;
    var image = GetTag('pre-Crop');
    if (crop && crop.smart == 1 && image) {
        if (crop.pitch || crop.yaw) {
            var perspectiveTransform = PerspectiveTransformMake(crop.angle, crop.pitch, crop.yaw, image.geometry);
            image = Transform(image, perspectiveTransform);
        } else {
            var straightenTransform = StraightenTransformMake(crop.angle, image.geometry);
            image = Transform(image, straightenTransform);
        }
        // Apply the crop rect to the incoming image;
        image = Crop(image, crop.xOrigin, crop.yOrigin, crop.width, crop.height);
        image = Filter(`CIColorMatrix`, {'inputImage' : image, 'inputBVector' : CIVectorMake(1,1,1,0)});
        ResetTagInput('Crop', image);
    }
return output;
+[PIPipelineFilters(Debug) socPseudoColorFilter]_block_invoke
none
error != NULL
-[NURenderPipelineHelper(PI) videoReframe:reframes:error:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Pipeline/PIPhotosPipelineHelper.m
-[NURenderPipelineHelper(PI) videoCrossfadeLoop:crossfadeAdjustment:error:]
-[NURenderPipelineHelper(PI) portraitVideo:disparityInput:disparityKeyframes:apertureKeyframes:debugMode:error:]
@"NSMutableArray"16@?0@"NSArray"8
-[NURenderPipelineHelper(PI) portraitVideoDebugDetectedObjects:source:cinematographyState:monochrome:error:]
Unexpected source type
extent
transform
AutoLoop/LongExposureMotion
PILongExposureFusion
inputStillImage
inputVideoScale
inputRenderScale
inputAlignmentExtent
inputAlignmentTransform
inputMaskImage
PIApertureRedEyeFilter
PIRedEye
inputDestinationImage
recipe != nil
NSDictionary * _Nonnull PIAutoLoopRecipeForFlavor(NSDictionary *__strong _Nonnull, PIAutoLoopFlavor)
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/AutoLoop/PIAutoLoop.m
CGRect PIAutoLoopRecipeGetCropRect(NSDictionary *__strong _Nonnull)
origin_x
origin_y
BOOL PIAutoLoopRecipeHasGoodStabilization(NSDictionary *__strong _Nonnull)
frameTransform != nil
CMTime PIAutoLoopRecipeFrameTransformGetTime(NSDictionary *__strong)
frameTransform_rawTime
matrix_float3x3 PIAutoLoopRecipeFrameTransformGetHomography(NSDictionary *__strong)
frameTransform_homography
NSDictionary * _Nonnull PIAutoLoopRecipeGetInstructionAtTime(NSDictionary *__strong _Nonnull, CMTime)
CMTIME_IS_VALID(time)
loopRecipe_frameInstructions
loopFrameData_presTime
q24@?0@"NSDictionary"8@"NSDictionary"16
CMTime PIAutoLoopRecipeGetFrameDuration(NSDictionary *__strong _Nonnull)
NSDictionary * _Nullable PIAutoLoopRecipeGetFlavorParameters(NSDictionary *__strong _Nonnull, PIAutoLoopFlavor)
CMTimeRange PIAutoLoopRecipeGetTimeRangeForFlavor(NSDictionary *__strong _Nonnull, PIAutoLoopFlavor)
completion != nil
-[PICurvesAutoCalculator submit:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Autocalculators/PICurvesAutoCalculator.m
-[PICurvesAutoCalculator computeCurvesForImageHistogram:]
softlink:r:path:/System/Library/Frameworks/JavaScriptCore.framework/JavaScriptCore
_PITapToTrackRenderResult
PITapToTrackResult
NURenderResult
NSObject
PITapToTrackRenderJob
PITapToTrackRequest
PIPhotoEffectHDR
PIPhotoEffectBlackAndWhiteHDR
PIPhotoEffectNoirHDR
PIPhotoEffectChromeHDR
PIPhotoEffectFadeHDR
PIPhotoEffectInstantHDR
PIPhotoEffectMonoHDR
PIPhotoEffectProcessHDR
PIPhotoEffectTonalHDR
PIPhotoEffectTransferHDR
PIPhotoEffectStageMonoHDR
PIPhotoEffect3DHDR
PIPhotoEffect3DBlackAndWhiteHDR
PIPhotoEffect3DVividHDR
PIPhotoEffect3DVividWarmHDR
PIPhotoEffect3DVividCoolHDR
PIPhotoEffect3DDramaticHDR
PIPhotoEffect3DDramaticCoolHDR
PIPhotoEffect3DDramaticWarmHDR
PIPhotoEffect3DSilverplateHDR
PIPhotoEffect3DNoirHDR
PIColorNormalizationFilter
PIReframeRules
PISemanticEnhanceAdjustmentController
_PIDisparitySampleResult
PIDisparitySampleResult
PIDisparitySampleJob
PIDisparitySampleRequest
PILocalContrastHDR
PIHighKey
PICompositionController
NSCopying
PICompositionSerializer
PICompositionSerializationResult
PICompositionSerializerMetadata
PILevelsFilterHDR
PIColorNormalizationAutoCalculator
NUTimeBased
PIFalseColorHDRDebug
PIAutoCalculatorUtils
PIAutoLoopExportRequest
PIAutoLoopExportClient
PIAutoLoopAnalysisRequest
PIAutoLoopAnalysisClient
PILongExposureRegistrationRequest
1 1@
PIPortraitVideoProcessor
PIPortraitVideoFilter
PIAutoLoopAdjustmentController
PIVideoCrossfadeLoopNode
PILivePhotoKeyFrameAdjustmentController
PIVignetteAdjustmentController
PIAdjustmentController
PIPortraitVideoRenderNode
PIFaceObservationCache
PISourceSelectAdjustmentController
PIScalarKeyframe
PIDictionaryRepresentable
PIReframeSubject
NSSecureCoding
NSCoding
PIPortraitVideoAdjustmentController
PIReframeKeyframe
PIReframeKeyframeSequence
PILongExposureFusionAutoCalculator
PIEffectAdjustmentController
PIAutoLoopAutoCalculator
PIMakerNoteUtilities
PITempTintFilter
PITimeVaryingPipelineStateSetting
_PIAutoLoopAnalysisResult
PIAutoLoopAnalysisResult
PIAutoLoopAnalysisJob
_PIVideoStabilizeFlowControl
ICFlowControl
_PIVideoStabilizeResult
PIVideoStabilizeResult
PIVideoStabilizeRenderJob
PIVideoStabilizeRequest
PIFaceBalanceAutoCalculator
GrayColorResult
RGBResult
PIWhiteBalanceAutoCalculator
_PIWhiteColorCalculator
PIDefinitionFilter
PISmartBlackAndWhiteAdjustmentController
PICompositionExporterDefaultMetadataConverter
PICompositionExporterMetadataConverter
PICompositionExportImagePrepareResult
PICompositionExportAuxiliaryResult
PICompositionExportResult
PICompositionExportDataResult
PICompositionExporterOptions
PICompositionExporterVideoOptions
PICompositionExporterImageOptions
PICompositionExporterAuxiliaryOptions
PICompositionExporter
GUBilateralConvolution
GUBWBilateralConvolution
PIBilateralFilter
PIPhotoEditAdjustmentsVersion
PINoiseReductionAdjustmentController
PrivateSmartColorHDR
PISmartColorFilterHDR
PIVideoCrossfadeLoopAdjustmentController
PIPortraitAdjustmentController
PICompositionNoOpRemoval
PIOrientationAdjustmentController
PIPortraitVideoMetadataSample
PIPortraitVideoRenderer
PIDefinitionAdjustmentController
PIHighKeyHDR
PIPerspectiveAutoCalculator
PIFaceObservingAutoCalculator
PIRawNoiseReductionAdjustmentController
PICropAdjustmentController
PIRawAdjustmentController
PISmartToneAdjustmentController
PIAutoLoopKernels
PIPhotoEditHelper
PIAdjustmentConstants
PIFakeBoost
PIForwardFakeBoost
PIInverseFakeBoost
PICoreImageUtilities
PICompositionSerializerConstants
PICurvesFilterHDR
PILongExposureAccumulator
_PILongExposureRegistrationResult
PILongExposureRegistrationResult
PILongExposureRegistrationJob
PIRAWTempTintSampler
PITagColorSampler
PIRedEye
PIVideoPosterFrameAdjustmentController
PICompositionSerializerFormatVersion
PICaptureDebugUtilities
PISlomoAdjustmentController
PIPortraitVideoDebugDetectionsRenderNode
PIDebugAdjustmentController
PIRAWFaceBalance
PICurvesLUTFilter
PICurvesFilter
PIColorBalanceFilter
PIPhotosPipelineHDRFilters
PIHighResFusionAdjustmentController
PIGlobalSettings
PIAutoLoopExportJob
PIJSRenderPipeline
PhotosPipeline
PIApertureRedEyeAutoCalculator
PrivateLocalLightHDR
PILocalLightMapPrepareHDR
PILocalLightFilterHDR
PIMsgImageBuffer
PISmartBlackAndWhiteAutoCalculator
PISmartBlackAndWhiteHDR
PrivateSmartBlackAndWhite
PITrimAdjustmentController
PIRedEyeAdjustmentController
PILevelsAutoCalculator
PILevelsLuminanceAutoCalculator
PILevelsRGBAutoCalculator
AdjustmentExtensions
PICurveControlPoint
PrivateSmartToneHDR
PISmartToneFilterHDR
PILongExposureFusion
PIModernPhotosPipeline
PILevelsFilter
PISchema
PICompositionFinalizer
PICompositionFinalizerResult
PIVideoStabilizeAdjustmentController
PICropAutoCalculator
Utilities
PISmartColorAdjustmentController
PISelectiveColorFilter
PIVideoReframeNode
PIVideoReframe
PIVideoReframeTimedMetadata
PIVideoReframeMetadataExtractor
PINeutralGrayWhiteBalanceFilter
PISmartToneAutoCalculator
PISmartColorAutoCalculator
PIRedEyeAutoCalculator
PIManualRedEyeAutoCalculator
PISourceSampler
PIPhotoGrainHDR
PIImageIO
PISharpenAdjustmentController
PIApertureRedEyeProcessorKernel
PIApertureRedEyeFilter
PIValuesAtCapture
PIPortraitAutoCalculator
PIDepthEffectApertureAutoCalculator
PIPipelineFilters
Debug
PIWhiteBalanceAdjustmentController
PIDepthAdjustmentController
PIVideoReframeAdjustmentController
PICurvesAutoCalculator
PICurvesLuminanceAutoCalculator
PICurvesRGBAutoCalculator
init
isEqual:
class
self
performSelector:
performSelector:withObject:
performSelector:withObject:withObject:
isProxy
isKindOfClass:
isMemberOfClass:
conformsToProtocol:
respondsToSelector:
retain
release
autorelease
retainCount
zone
hash
superclass
description
debugDescription
TQ,R
T#,R
T@"NSString",R,C
statistics
T@"<NURenderStatistics>",R
completedTrack
T@"PTCinematographyTrack",R,N
initWithCompletedTrack:
.cxx_destruct
_completedTrack
T@"PTCinematographyTrack",R,N,V_completedTrack
oneToOneScalePolicy
stringWithFormat:
callStackSymbols
componentsJoinedByString:
prepare:
request
device
currentPlatform
mainDevice
metalDevice
missingError:object:
newCommandQueue
initWithCommandQueue:
outputVideo
initWithAsset:
startTime
startReadingFrames:atTime:error:
nextFrame
normalizedImagePoint
colorBuffer
getRectForPoint:colorBuffer:
rect
time
disparityBuffer
addDetectionAndStartTrackingRect:time:colorBuffer:disparityBuffer:
confidence
_reportProgressAtTime:rect:confidence:
clientRequestedStop
addDetectionForNextFrameAt:colorBuffer:disparityBuffer:
stopReadingFrames
finalizeTrack
setCompletedTrack:
progressHandler
responseQueue
setClientRequestedStop:
wantsRenderStage
wantsCompleteStage
wantsOutputVideo
scalePolicy
result
setStartTime:
setNormalizedImagePoint:
setProgressHandler:
_clientRequestedStop
_progressHandler
_normalizedImagePoint
_startTime
T{?=qiIq},N,V_startTime
T{CGPoint=dd},N,V_normalizedImagePoint
T@?,C,N,V_progressHandler
T@"PTCinematographyTrack",&,N,V_completedTrack
TB,V_clientRequestedStop
initWithComposition:
sourceFilterNoOrientation
arrayWithObjects:count:
setPipelineFilters:
copyWithZone:
initWithRequest:
submitGeneric:
initWithComposition:startTime:pointToTrack:
newRenderJob
mediaComponentType
submit:
kernelWithString:
stringByReplacingOccurrencesOfString:withString:
kernel
extent
applyWithExtent:arguments:
photoEffectName
filterWithName:
setValue:forKey:
outputImage
inputImage
setInputImage:
_inputImage
T@"CIImage",&,V_inputImage
kernelBlackAndWhite
numberWithFloat:
inputDepthMap
setInputDepthMap:
inputThreshold
setInputThreshold:
_inputThreshold
_inputDepthMap
T@"CIImage",&,V_inputDepthMap
Tf,V_inputThreshold
inputNormalization
tempTintProperties
objectForKeyedSubscript:
dictionaryWithObjects:forKeys:count:
imageByApplyingFilter:withInputParameters:
highKeyProperties
smartToneProperties
smartColorProperties
logger
initWithCIImage:options:
setRevision:
performRequests:error:
results
firstObject
adjustmentKeys
countByEnumeratingWithState:objects:count:
adjustmentValuesForKey:
setObject:forKeyedSubscript:
initWithAnalysisData:
setInputNormalization:
contextWithOptions:
pi_createColorCubeDataForFilters:dimension:colorSpace:
isAnalysisAvailable
colorCubeForNormalization:dimension:targetColorSpace:
analysisAvailable
TB,R,N,GisAnalysisAvailable
outputNormalization
T@"CIImage",&,N,VinputImage
T@"PFStoryRecipeDisplayAssetNormalization",&,N,VinputNormalization
T@"PFStoryRecipeDisplayAssetNormalization",R,N
initWithCapacity:
predicateWithFormat:
ruleWithPredicate:action:
addObject:
ruleWithPredicate:assertingFact:grade:
ruleWithPredicate:retractingFact:grade:
factCandidateForReframe
factCandidateForStill
factCandidateForVideo
factCandidateForPerspective
factCandidateForHorizon
retractFact:
initWithArray:
setEnableLogging:
setConstants:
sharedPregateRules
addRulesFromArray:
gradeForFact:
pregateRulesSystemWithConstants:
isCandidateForReframe
isCandidateForPerspective
isCandidateForHorizon
TB,R
adjustment
intensityKey
isEqualToString:
doubleValue
sceneLabelKey
boundingBoxesKey
isEqualToArray:
isSettingEqual:forKey:
numberWithDouble:
floatValue
platedFoodSceneLabel
sunriseSunsetSceneLabel
genericLandscapeSceneLabel
sceneConfidenceKey
copy
count
arrayWithCapacity:
faceBoundingBoxesKey
setIntensity:
intensity
scene
sceneConfidence
boundingBoxes
setScene:confidence:
setBoundingBoxesFromObservations:orientation:
setFaceBoundingBoxesFromObservations:orientation:
Td,N
Tq,R,N
Td,R,N
T@"NSArray",R,C,N
currentHandler
stringWithUTF8String:
handleFailureInFunction:file:lineNumber:description:
boundingBox
sampledDisparityValue
Tf,R,N
initWithDisparityValue:
_sampledDisparityValue
Tf,R,N,V_sampledDisparityValue
failureError:object:
sampleTime
invalidError:object:
sampleRect
setSampledDisparityValue:
render:
setSampleTime:
setSampleRect:
_sampleTime
_sampleRect
T{?=qiIq},N,V_sampleTime
T{CGRect={CGPoint=dd}{CGSize=dd}},N,V_sampleRect
Tf,N,V_sampledDisparityValue
initWithComposition:time:sampleRect:
vectorWithX:Y:Z:W:
HLGOpticalScale
imageByClampingToExtent
imageByApplyingTransform:
imageByApplyingGaussianBlurWithSigma:
imageByCroppingToRect:
_kernelLocalContrast
customAttributes
inputStrength
inputScale
setInputStrength:
T@"NSNumber",&,N,VinputStrength
imageOrientation
setImageOrientation:
sourceSelection
setSourceSelection:
compositionController:didAddAdjustment:
compositionController:didRemoveAdjustment:
compositionController:didUpdateAdjustment:
compositionController:didUpdateAdjustments:
compositionController:adjustmentControllerClassForKey:
schema
contents
allKeys
compositionKeys
addObjectsFromArray:
_keyToIdentifierMap
initWithIdentifier:
reset
_adjustmentControllerClassForKey:
initWithAdjustment:
setIdentifier:
composition
differingAdjustmentsWithComposition:
changeDelegate
isEqual:forKeys:visualChangesOnly:
availableKeys
settings
containsObject:
boolValue
integerValue
isEqual:visualChangesOnly:
sharedRegistry
schemaWithIdentifier:
schemaForKey:
orientationAdjustmentControllerCreatingIfNecessary:
orientation
sourceSelectAdjustmentController
removeAdjustmentWithKey:
modifyAdjustmentWithKey:modificationBlock:
smartToneAdjustmentControllerCreatingIfNecessary:
registeredPhotosSchemaIdentifier
photosSchema
adjustmentControllerClassForKey:
isSubclassOfClass:
setMediaType:
mediaType
settingForAdjustmentKey:settingKey:
setChangeDelegate:
addAdjustmentWithKey:
replaceAdjustment:withKey:
adjustmentControllerForKey:
applyChangesFromCompositionController:
isEqual:forKeys:comparisonBlock:
userOrientation
setOvercaptureSource:
overcaptureSource
setSource:mediaType:
source
_composition
_delegateFlags
_identifierMap
_changeDelegate
_imageOrientation
T@"NUComposition",R,C,N
T@"<PICompositionControllerDelegate>",W,N,V_changeDelegate
Tq,N
Tq,N,V_imageOrientation
setWithObjects:
locallySupportedFormatVersions
setWithArray:
handleFailureInMethod:object:file:lineNumber:description:
deserializeDictionaryFromData:error:
validateAdjustmentsEnvelope:error:
deserializeCompositionFromAdjustments:metadata:formatIdentifier:formatVersion:error:
URLWithString:
imageSourceWithURL:type:useEmbeddedPreview:
validateComposition:error:
conversionMap
newComposition
initWithDomain:code:userInfo:
stringByAppendingString:
newAdjustmentWithName:
enumerateKeysAndObjectsUsingBlock:
initWithName:
_sanitizeComposition:
validateCompositionWithMissingSource:error:
geometryRequestWithComposition:
setName:
submitSynchronous:
geometry
size
setWidth:
setHeight:
setOrientation:
serializeComposition:versionInfo:serializerMetadata:error:
width
numberWithInteger:
height
array
copyOfCompositionRemovingNoOps:
mapForSerialization
hasInputKey:
isAuto
serializeDictionary:error:
setFormatIdentifier:
adjustmentDataFormatVersionForComposition:
setFormatVersion:
setData:
_validateValueTypesForKeys:requiredKeys:inDictionary:error:
dataWithJSONObject:options:error:
errorWithDomain:code:userInfo:
compressData:options:error:
decompressData:options:error:
JSONObjectWithData:options:error:
_isDefault
mainBundle
infoDictionary
objectForKey:
serializeComposition:versionInfo:error:
dictionary
data
formatIdentifier
formatVersion
initialize
disableApertureEffects:
canInterpretDataWithFormatIdentifier:formatVersion:
deserializeCompositionFromData:formatIdentifier:formatVersion:error:
adjustmentInformationForComposition:error:
_data
_formatIdentifier
_formatVersion
T@"NSData",&,N,V_data
T@"NSString",&,N,V_formatIdentifier
T@"NSString",&,N,V_formatVersion
_width
_height
_orientation
Tq,N,V_width
Tq,N,V_height
Tq,N,V_orientation
containsString:
defaultValueForKey:
dictionaryWithObjectsAndKeys:
_customAttributesForKey:
valueForKey:
floatValueForKey:defaultValue:clearIfNotDefault:
dataWithBytesNoCopy:length:freeWhenDone:
imageWithBitmapData:bytesPerRow:size:format:colorSpace:
_LUTImage
samplerWithImage:keysAndValues:
P3KernelHDR
itur2100HLGColorSpace
CGColorSpace
imageByColorMatchingWorkingSpaceToColorSpace:
imageByUnpremultiplyingAlpha
samplerWithImage:
definition
applyWithExtent:roiCallback:arguments:options:
imageByColorMatchingColorSpaceToWorkingSpace:
imageByPremultiplyingAlpha
setDefaults
inputBlackSrcRGB
setInputBlackSrcRGB:
inputBlackDstRGB
setInputBlackDstRGB:
inputShadowSrcRGB
setInputShadowSrcRGB:
inputShadowDstRGB
setInputShadowDstRGB:
inputMidSrcRGB
setInputMidSrcRGB:
inputMidDstRGB
setInputMidDstRGB:
inputHilightSrcRGB
setInputHilightSrcRGB:
inputHilightDstRGB
setInputHilightDstRGB:
inputWhiteSrcRGB
setInputWhiteSrcRGB:
inputWhiteDstRGB
setInputWhiteDstRGB:
inputBlackSrcRed
setInputBlackSrcRed:
inputBlackDstRed
setInputBlackDstRed:
inputShadowSrcRed
setInputShadowSrcRed:
inputShadowDstRed
setInputShadowDstRed:
inputMidSrcRed
setInputMidSrcRed:
inputMidDstRed
setInputMidDstRed:
inputHilightSrcRed
setInputHilightSrcRed:
inputHilightDstRed
setInputHilightDstRed:
inputWhiteSrcRed
setInputWhiteSrcRed:
inputWhiteDstRed
setInputWhiteDstRed:
inputBlackSrcGreen
setInputBlackSrcGreen:
inputBlackDstGreen
setInputBlackDstGreen:
inputShadowSrcGreen
setInputShadowSrcGreen:
inputShadowDstGreen
setInputShadowDstGreen:
inputMidSrcGreen
setInputMidSrcGreen:
inputMidDstGreen
setInputMidDstGreen:
inputHilightSrcGreen
setInputHilightSrcGreen:
inputHilightDstGreen
setInputHilightDstGreen:
inputWhiteSrcGreen
setInputWhiteSrcGreen:
inputWhiteDstGreen
setInputWhiteDstGreen:
inputBlackSrcBlue
setInputBlackSrcBlue:
inputBlackDstBlue
setInputBlackDstBlue:
inputShadowSrcBlue
setInputShadowSrcBlue:
inputShadowDstBlue
setInputShadowDstBlue:
inputMidSrcBlue
setInputMidSrcBlue:
inputMidDstBlue
setInputMidDstBlue:
inputHilightSrcBlue
setInputHilightSrcBlue:
inputHilightDstBlue
setInputHilightDstBlue:
inputWhiteSrcBlue
setInputWhiteSrcBlue:
inputWhiteDstBlue
setInputWhiteDstBlue:
inputColorSpace
setInputColorSpace:
_inputBlackSrcRGB
_inputBlackDstRGB
_inputShadowSrcRGB
_inputShadowDstRGB
_inputMidSrcRGB
_inputMidDstRGB
_inputHilightSrcRGB
_inputHilightDstRGB
_inputWhiteSrcRGB
_inputWhiteDstRGB
_inputBlackSrcRed
_inputBlackDstRed
_inputShadowSrcRed
_inputShadowDstRed
_inputMidSrcRed
_inputMidDstRed
_inputHilightSrcRed
_inputHilightDstRed
_inputWhiteSrcRed
_inputWhiteDstRed
_inputBlackSrcGreen
_inputBlackDstGreen
_inputShadowSrcGreen
_inputShadowDstGreen
_inputMidSrcGreen
_inputMidDstGreen
_inputHilightSrcGreen
_inputHilightDstGreen
_inputWhiteSrcGreen
_inputWhiteDstGreen
_inputBlackSrcBlue
_inputBlackDstBlue
_inputShadowSrcBlue
_inputShadowDstBlue
_inputMidSrcBlue
_inputMidDstBlue
_inputHilightSrcBlue
_inputHilightDstBlue
_inputWhiteSrcBlue
_inputWhiteDstBlue
_inputColorSpace
T@"CIImage",&,N,V_inputImage
T@"NSNumber",&,N,V_inputBlackSrcRGB
T@"NSNumber",&,N,V_inputBlackDstRGB
T@"NSNumber",&,N,V_inputShadowSrcRGB
T@"NSNumber",&,N,V_inputShadowDstRGB
T@"NSNumber",&,N,V_inputMidSrcRGB
T@"NSNumber",&,N,V_inputMidDstRGB
T@"NSNumber",&,N,V_inputHilightSrcRGB
T@"NSNumber",&,N,V_inputHilightDstRGB
T@"NSNumber",&,N,V_inputWhiteSrcRGB
T@"NSNumber",&,N,V_inputWhiteDstRGB
T@"NSNumber",&,N,V_inputBlackSrcRed
T@"NSNumber",&,N,V_inputBlackDstRed
T@"NSNumber",&,N,V_inputShadowSrcRed
T@"NSNumber",&,N,V_inputShadowDstRed
T@"NSNumber",&,N,V_inputMidSrcRed
T@"NSNumber",&,N,V_inputMidDstRed
T@"NSNumber",&,N,V_inputHilightSrcRed
T@"NSNumber",&,N,V_inputHilightDstRed
T@"NSNumber",&,N,V_inputWhiteSrcRed
T@"NSNumber",&,N,V_inputWhiteDstRed
T@"NSNumber",&,N,V_inputBlackSrcGreen
T@"NSNumber",&,N,V_inputBlackDstGreen
T@"NSNumber",&,N,V_inputShadowSrcGreen
T@"NSNumber",&,N,V_inputShadowDstGreen
T@"NSNumber",&,N,V_inputMidSrcGreen
T@"NSNumber",&,N,V_inputMidDstGreen
T@"NSNumber",&,N,V_inputHilightSrcGreen
T@"NSNumber",&,N,V_inputHilightDstGreen
T@"NSNumber",&,N,V_inputWhiteSrcGreen
T@"NSNumber",&,N,V_inputWhiteDstGreen
T@"NSNumber",&,N,V_inputBlackSrcBlue
T@"NSNumber",&,N,V_inputBlackDstBlue
T@"NSNumber",&,N,V_inputShadowSrcBlue
T@"NSNumber",&,N,V_inputShadowDstBlue
T@"NSNumber",&,N,V_inputMidSrcBlue
T@"NSNumber",&,N,V_inputMidDstBlue
T@"NSNumber",&,N,V_inputHilightSrcBlue
T@"NSNumber",&,N,V_inputHilightDstBlue
T@"NSNumber",&,N,V_inputWhiteSrcBlue
T@"NSNumber",&,N,V_inputWhiteDstBlue
T@"NSString",&,N,V_inputColorSpace
stopAtTagIncludeOrientationFilter:
setSampleMode:
setTime:
imageWithData:
imageSourceWithCIImage:orientation:
newCompositionControllerWithComposition:
unsupportedError:object:
initWithError:
setVisionRequests:
initWithTargetPixelCount:
setScalePolicy:
result:
observations
requestRevision
numberWithUnsignedInteger:
initWithResult:
isAvailable
autoCalculatorWithImageData:orientation:
T{?=qiIq},N
available
TB,R,N,GisAvailable
T{?=qiIq},N,Vtime
inputCutoff
setInputCutoff:
_inputCutoff
Td,V_inputCutoff
convertFacePoint:toImagePointWithFaceRect:orientation:
averagePoints:pointCount:
averageCGPoints:pointCount:
initWithComposition:destinationURL:
initWithAutoLoopExportRequest:
outputSettings
colorSpaceFromVideoColorProperties:
sRGBColorSpace
initWithComposition:stabilizedVideoURL:longExposureDestinationURL:maskDestinationURL:destinationUTI:
outputColorSpace
destinationUTI
destinationLongExposureURL
destinationMaskURL
_destinationUTI
_destinationLongExposureURL
_destinationMaskURL
T@"NSString",R,V_destinationUTI
T@"NSURL",R,V_destinationLongExposureURL
T@"NSURL",R,V_destinationMaskURL
T@"NUColorSpace",R
setGenericCompletionBlock:
submitGenericRequest:
setCompletionBlock:
submitRequest:
flavor
setFlavor:
_flavor
Tq,N,V_flavor
recipe
setRecipe:
cleanAperture
setCleanAperture:
_recipe
_cleanAperture
T@"NSDictionary",C,N,V_recipe
T{?={?=qq}{?=qq}},N,V_cleanAperture
metalCommandBuffer
objectAtIndexedSubscript:
metalTexture
value
deserializeMetadataWithType:fromGlobalMetadata:error:
errorWithCode:reason:object:underlyingError:
majorVersion
minorVersion
initWithMetadataGroup:majorVersion:minorVersion:error:
createRGBA:
setTransferFunction:
setSourceColor:
setSourceDisparity:
setDestinationColor:
applyToRenderRequest:
setAperture:
setFocusDistance:
intValue
applyToRenderState:
cameraInfo
_updateRenderState:withLegacyCameraInfo:
setRenderState:
encodeRenderTo:withRenderRequest:
status
label
error
addCompletedHandler:
renderOnDevice:colorSize:disparitySize:quality:debugMode:usingBlock:
setTotalSensorCrop:
setRawSensorWidth:
setRawSensorHeight:
setFocalLenIn35mmFilm:
setConversionGain:
setReadNoise_1x:
setReadNoise_8x:
CGRectValue
vectorWithCGRect:
numberWithBool:
applyWithExtent:inputs:arguments:error:
processWithInputs:arguments:output:error:
formatForInputAtIndex:
outputFormat
outputIsOpaque
synchronizeInputs
allowPartialOutputRegion
roiForInput:arguments:outputRect:
applyWithInputImage:disparityImage:globalMetadata:timedMetadata:aperture:focusedDisparity:quality:debugMode:isHDR:error:
inputDisparityImage
inputGlobalRenderingMetadata
inputTimedRenderingMetadata
inputAperture
inputFocusedDisparity
inputRenderQuality
inputRenderDebugMode
inputIsHDR
setInputDisparityImage:
setInputRenderQuality:
setInputRenderDebugMode:
setInputIsHDR:
setInputGlobalRenderingMetadata:
setInputTimedRenderingMetadata:
setInputAperture:
setInputFocusedDisparity:
_inputIsHDR
_inputDisparityImage
_inputRenderQuality
_inputRenderDebugMode
_inputGlobalRenderingMetadata
_inputTimedRenderingMetadata
_inputAperture
_inputFocusedDisparity
T@"CIImage",&,N,V_inputDisparityImage
T@"NSNumber",&,N,V_inputRenderQuality
T@"NSNumber",&,N,V_inputRenderDebugMode
TB,N,V_inputIsHDR
T@"AVMetadataItem",&,N,V_inputGlobalRenderingMetadata
T@"AVTimedMetadataGroup",&,N,V_inputTimedRenderingMetadata
T@"NSNumber",&,N,V_inputAperture
T@"NSNumber",&,N,V_inputFocusedDisparity
recipeKey
flavorKey
stabilizedCropRect
T@"NSDictionary",C,N
T@"NSString",C,N
T{CGRect={CGPoint=dd}{CGSize=dd}},R,N
initWithSettings:inputs:
inputs
resolvedNodeWithCachedInputs:settings:pipelineState:error:
evaluationMode
nodeByReplayingAgainstCache:pipelineState:error:
crossfadeDuration
loopTimeRange
input
outputVideo:
videoFrames
setVideoFrames:
setRawTime:
initWithFilterName:settings:inputs:
nodeFromCache:cache:
setEvaluatedForMode:
errorWithCode:reason:object:
firstEnabledVideoTrackInAsset:error:
tracksWithMediaType:
addMutableTrackWithMediaType:preferredTrackID:
insertTimeRange:ofTrack:atTime:error:
debugDescriptionOfAssetTrack:
timeRange
conformRange:inRange:
outputVideoComposition:
instructions
setTimeRange:
trackID
numberWithInt:
setRequiredSourceTrackIDs:
setSourceIdentifier:forTrackID:
frameDuration
setFrameDuration:
renderSize
setRenderSize:
setInstructions:
setSourceTrackIDForFrameTiming:
audioMix
audioMixInputParametersWithTrack:
setVolume:atTime:
setVolumeRampFromStartVolume:toEndVolume:timeRange:
setInputParameters:
initWithInput:timeRange:crossfadeDuration:startTime:
shouldCacheNodeForPipelineState:
_evaluateVideo:
requiresVideoComposition
_evaluateVideoComposition:
requiresAudioMix
_evaluateAudioMix:
_crossfadeDuration
_loopTimeRange
T{?=qiIq},R,N,V_startTime
T{?={?=qiIq}{?=qiIq}},R,N,V_loopTimeRange
T{?=qiIq},R,N,V_crossfadeDuration
timeKey
longLongValue
scaleKey
numberWithLongLong:
keyFrameTime
setKeyFrameTime:
radiusKey
falloffKey
T@"NSString",R,N
radius
setRadius:
falloff
setFalloff:
identifier
name
inputKeys
settingForKey:
canBeEnabled
enabledKey
autoKey
hasAutoKeyInSchema
canHaveAuto
_setPrimitiveValue:forKey:
setValue:forUndefinedKey:
_primitiveValueForKey:
values
setFromAdjustment:
isEqualToAdjustmentController:
visualInputKeys
isEqual:forKeys:
defaultValue
displayName
displayInputKeys
enabled
setEnabled:
setIsAuto:
valueForUndefinedKey:
valuesForArrayInputKey:
interpolateFromStart:toEnd:progress:
timeFromInputKey:timescaleKey:
_changes
_identifier
_adjustment
T@"NSDictionary",R,N
TB,N
T@"NUIdentifier",&,N,V_identifier
T@"NUAdjustment",R,N,V_adjustment
T@"NSArray",R,N
TB,R,N
inputForKey:
_prewarmPortraitRendererWithPipelineState:error:
scale
_targetScaleForScale:
setScale:
sampleMode
initWithTargetScale:effectiveScale:sampleMode:input:
outputImageGeometry:
initWithExtent:renderScale:orientation:
_portraitQualityForRenderScale:
scaledSize
debugMode
prewarmRendererForDevice:colorSize:disparitySize:quality:debugMode:
videoProperties:
metadata
videoMetadataSamples
outputTimedMetadataSampleWithIdentifier:atTime:error:
mutableCopy
metadataGroup
colorProperties
outputImage:
globalMetadata
timedMetadata
sourceTransferFunction
renderTime
disparityKeyframes
keyframeInArray:closestToTime:
apertureKeyframes
imageByCompositingOverImage:
renderQuality
initWithInput:disparityInput:disparityKeyframes:apertureKeyframes:debugMode:
uniqueInputNode
_evaluateImage:
T{?=qiIq},R,N
Ti,R,N
T@"AVTimedMetadataGroup",R,N
T@"AVMetadataItem",R,N
applyOrientationFilter
submitGenericSynchronous:
faceRequestWithRequest:
submit:response:
submitSynchronous:error:
_group
_queue
_result
sourceSelectionKey
sourceSelectionForString:
stringForSourceSelection:
initWithTime:value:
addBytes:length:
initWithDictionaryRepresentation:
dictionaryRepresentation
nu_updateDigest:
_value
_time
labels
appendString:
appendFormat:
allocWithZone:
initWithType:source:identifier:confidence:
encodeInteger:forKey:
encodeDouble:forKey:
bounds
encodeObject:forKey:
expandedBounds
decodeIntegerForKey:
decodeDoubleForKey:
decodeObjectForKey:
stringValue
type
supportsSecureCoding
encodeWithCoder:
initWithCoder:
setBounds:
isHuman
isAnimal
setExpandedBounds:
edgeBleed
setEdgeBleed:
_type
_confidence
_source
_edgeBleed
_bounds
_expandedBounds
Tq,R,N,V_type
Tq,R,N,V_identifier
Td,R,N,V_confidence
Tq,R,N,V_source
T{CGRect={CGPoint=dd}{CGSize=dd}},N,V_bounds
T{CGRect={CGPoint=dd}{CGSize=dd}},N,V_expandedBounds
Tq,N,V_edgeBleed
componentsSeparatedByString:
_keyframesForKey:class:
_setKeyframes:forKey:
unsignedIntValue
removeObject:
cinematographyState
changesDictionaryTrimmedByTimeRange:
setCinematographyState:
setDisparityKeyframes:
aperture
setDebugMode:
trimToTimeRange:usingScript:
T@"NSArray",C,N
T@"NSNumber",&,N
T@"NSDictionary",&,N
initWithTime:homography:
keyframesFromDictionaryRepresentations:
homography
_homography
T{?=qiIq},R,N,V_time
T{?=[3]},R,N,V_homography
T@"NSDictionary",R,C,N
initWithCount:times:values:
interpolation
sampleAtTime:
initWithKeyframeArray:
homographyAtTime:
sparseSequence
_homographySequence
TQ,R,N
stopAtTagFilter:
properties
_computeCleanAperture:
kindKey
versionKey
setKind:
kind
setVersion:
version
removeAssetIdentifierFromMetadataArray:
metadataItem
setValue:
indexOfObjectPassingTest:
removeObjectAtIndex:
addAssetIdentifier:toMetadataDictionary:
addAssetIdentifier:toMetadataArray:
setInputVectorsForFilter:
temperature
setTemperature:
tint
setTint:
_temperature
_tint
Td,N,V_temperature
Td,N,V_tint
rawTime
nu_evaluateWithPipelineState:error:
_sampleMode
_rawTime
T{?=qiIq},N,V_time
T{?=qiIq},N,V_rawTime
Tq,N,V_sampleMode
T@"NSDictionary",R
T@"NSDictionary",&,V_recipe
renderNode
finalize
asset:
wantsOutputGeometry
cacheKey
analysisRequest
videoSource
setVideoSource:
_videoSource
T@"AVAsset",&,N,V_videoSource
T@"NSDictionary",&,N,V_recipe
shouldCancelHandler
ICShouldBeCanceled
ICReportProgress:
rangeMin
setRangeMin:
rangeMax
setRangeMax:
setShouldCancelHandler:
_rangeMin
_rangeMax
_shouldCancelHandler
Td,N,V_rangeMin
Td,N,V_rangeMax
T@?,C,N,V_shouldCancelHandler
keyframes
stabCropRect
analysisType
rawHomographies
T{?={?=qq}{?=qq}},R,N
initWithKeyframes:stabCropRect:analysisType:rawHomographies:
_keyframes
_analysisType
_rawHomographies
_stabCropRect
T@"NSArray",R,C,N,V_keyframes
T{?={?=qq}{?=qq}},R,N,V_stabCropRect
TQ,R,N,V_analysisType
T@"NSDictionary",R,N,V_rawHomographies
cleanApertureOfTrack:oriented:
isCanceled
allowedAnalysisTypes
canceledError:object:
nominalFrameRate
allowedCropFraction
setAllowedAnalysisTypes:
setAllowedCropFraction:
_allowedAnalysisTypes
_allowedCropFraction
TQ,N,V_allowedAnalysisTypes
Td,N,V_allowedCropFraction
canProvideMetadataForAVAsset:
canPerformGyroBasedStabilizationForAsset:
initWithAVAsset:
timedMetadataArray
trajectoryHomography
calculateWithRequest:completion:
calculateRAWWithRequest:completion:
rawProperties
initWithRequest:dataExtractor:options:
initWithDictionary:
minimumValue
maximumValue
faceBalanceResultFromFaceObservations:request:error:
faces
imageSize
addRect:
rawFaceBalanceFilter
initWithRect:
setRegionPolicy:
setResponseQueue:
image
faceRectFromNormalizedFaceRet:forImageExtent:scaleX:scaleY:
faceBalanceFromFaceImage:forFaceRect:
format
ARGB8
isEqualToPixelFormat:
RGBA8
BGRA8
validRegion
buffer
frameRect
bytesAtPoint:
readBufferRegion:withBlock:
initWithRequest:isRAW:
rawState
_rawState
Tq,R,V_rawState
valueWithBytes:objCType:
getValue:
pi_grayColorResultValue
pi_valueWithGrayColorResult:
T{?={?=[4d]}{?=[4d]}d},R
RGBResultValue
valueWithRGBResult:
T{?=[4d]},R
initWithComposition:useSushi:
_chooseNeutralGrayForNonSushi:
calculateColorWithProperties:completion:
_chooseTempTintForSushi:RAWProperties:brightness:
_correctedRGBResultFromResult:
_useTempTint:
inputNeutralXYFromRGB:
initWithName:responseQueue:
sharedFactory
bufferFactory
RGBAf
newStorageWithSize:format:
regionWithRect:
bytes
rowBytes
writeBufferInRegion:block:
returnStorage:
_brightnessMultiplierFromImageProperties:
begin
_computeGreenPercentage:
_submitGERenderRequest:
_submitGWRenderRequest:
rawCameraSpaceProperties
_whitePointColorFromGrayEdgesImage:grayWorldImage:greenChannelPercentage:RAWCameraSpaceProperties:
commitAndNotifyOnQueue:withBlock:
whiteValue
whiteFactor
_computeWhitePointColorWithGrayEdgesBuffer:grayWorldBuffer:greenChannelPercentage:RAWCameraSpaceProperties:
readBufferFromImage:withRGBAfBufferBlock:
RGBAh
setPixelFormat:
genericRGBLinearColorSpace
setColorSpace:
setTileSize:
rawSourceFilterIncludingOrientation
sushiLevel1Filter
initWithComposition:dataExtractor:options:
_configureRequest:
submitRequest:completion:
initWithSource:
initWithScript:
_bufferRenderClient
_imageDataClient
_useSushi
definitionKernel
inputBlurImage
setInputBlurImage:
inputIntensity
setInputIntensity:
_inputBlurImage
_inputIntensity
T@"CIImage",&,V_inputBlurImage
T@"NSNumber",&,V_inputIntensity
strengthKey
neutralKey
toneKey
grainKey
hueKey
inputStrengthKey
inputNeutralGammaKey
inputToneKey
inputHueKey
inputGrainKey
inputSeedKey
attributeStrengthKey
attributeNeutralGammaKey
attributeToneKey
attributeHueKey
attributeGrainKey
setStrength:
strength
setNeutral:
neutral
setTone:
tone
setGrain:
grain
setHue:
metadataItemsWithMetadataType:value:error:
writeMetadataType:value:toCGImageProperties:error:
readMetadataType:fromCGImageProperties:value:error:
videoMetadataForVariation:error:
setImageVariation:properties:error:
photoProcessingFlagsFromProperties:error:
setPhotoProcessingFlags:properties:error:
photoFeatureFlags:error:
setPhotoFeatureFlags:properties:error:
setRequest:
inputSize
setInputSize:
_request
_inputSize
T@"NUImageExportRequest",&,V_request
T{?=qq},V_inputSize
companionImageData
setCompanionImageData:
companionVideoURL
setCompanionVideoURL:
auxiliaryImages
setAuxiliaryImages:
canPropagateOriginalAuxiliaryData
setCanPropagateOriginalAuxiliaryData:
setProperties:
_canPropagateOriginalAuxiliaryData
_companionImageData
_companionVideoURL
_auxiliaryImages
_properties
T@"NSDictionary",&,V_auxiliaryImages
TB,V_canPropagateOriginalAuxiliaryData
T@"NSDictionary",C,V_properties
T{?=qq},D
T@"NSData",&,V_companionImageData
T@"NSURL",&,V_companionVideoURL
setGeometry:
_geometry
T@"NUImageGeometry",&,V_geometry
T@"NSData",&,V_data
initWithLevel:
displayP3ColorSpace
priority
setPriority:
colorSpace
pairingIdentifier
setPairingIdentifier:
_priority
_colorSpace
_pairingIdentifier
_scalePolicy
T@"NUPriority",&,V_priority
T@"NUColorSpace",&,V_colorSpace
T@"NSString",C,V_pairingIdentifier
T@"<NUScalePolicy>",&,V_scalePolicy
videoCodecType
bypassOutputSettingsIfNoComposition
applyVideoOrientationAsMetadata
requireHardwareEncoder
metadataProcessor
setMetadataProcessor:
increaseBitRateIfNecessary
setIncreaseBitRateIfNecessary:
setVideoCodecType:
preserveSourceColorSpace
setPreserveSourceColorSpace:
setBypassOutputSettingsIfNoComposition:
setApplyVideoOrientationAsMetadata:
setRequireHardwareEncoder:
includeCinematicVideoTracks
setIncludeCinematicVideoTracks:
_increaseBitRateIfNecessary
_preserveSourceColorSpace
_bypassOutputSettingsIfNoComposition
_applyVideoOrientationAsMetadata
_requireHardwareEncoder
_includeCinematicVideoTracks
_metadataProcessor
_videoCodecType
T@?,C,V_metadataProcessor
TB,N,V_increaseBitRateIfNecessary
T@"NSString",C,N,V_videoCodecType
TB,N,V_preserveSourceColorSpace
TB,N,V_bypassOutputSettingsIfNoComposition
TB,N,V_applyVideoOrientationAsMetadata
TB,N,V_requireHardwareEncoder
TB,N,V_includeCinematicVideoTracks
pathExtension
typeWithFilenameExtension:
fileType
defaultFormatForURL:
JPEGCompressionQuality
setCompressionQuality:
imageExportFormatForURL:
imageExportFormat
setImageExportFormat:
setJPEGCompressionQuality:
optimizeForSharing
setOptimizeForSharing:
applyImageOrientationAsMetadata
setApplyImageOrientationAsMetadata:
_optimizeForSharing
_applyImageOrientationAsMetadata
_imageExportFormat
_JPEGCompressionQuality
T@"NUImageExportFormat",C,V_imageExportFormat
Td,V_JPEGCompressionQuality
TB,V_optimizeForSharing
TB,V_applyImageOrientationAsMetadata
primaryURL
setPrimaryURL:
videoComplementURL
setVideoComplementURL:
videoPosterFrameURL
setVideoPosterFrameURL:
renderCompanionResources
setRenderCompanionResources:
reframeCropAdjustment
setReframeCropAdjustment:
reframeVideoAdjustment
setReframeVideoAdjustment:
_renderCompanionResources
_primaryURL
_videoComplementURL
_videoPosterFrameURL
_reframeCropAdjustment
_reframeVideoAdjustment
T@"NSURL",&,V_primaryURL
T@"NSURL",&,V_videoComplementURL
T@"NSURL",&,V_videoPosterFrameURL
TB,V_renderCompanionResources
T@"NUAdjustment",&,V_reframeCropAdjustment
T@"NUAdjustment",&,V_reframeVideoAdjustment
TB,V_applyVideoOrientationAsMetadata
globalSettings
decoratorRenderFiltersForImages
oneShotPortraitV2ExportFilter
orientationAsMetaDataFilter
setDestinationURL:
setFormat:
prepareImageExportRequest:options:completion:
setRenderToData:
destinationData
discreteProgressWithTotalUnitCount:
addVideoProperties:composition:options:error:
initWithProperties:
setMetadata:
_exportVideoToURL:composition:options:properties:progress:completion:
exportComposition:options:completionQueue:completion:
UUID
UUIDString
mismatchError:object:
defaultExportCodecForComposition:
conformsToType:
exportImageToURL:composition:options:completion:
exportVideoToURL:composition:options:completion:
adjustmentConstants
PICropAdjustmentKey
PIVideoReframeAdjustmentKey
exportImageToDataWithComposition:options:completion:
defaultManager
URLForDirectory:inDomain:appropriateForURL:create:error:
lastPathComponent
stringByDeletingPathExtension
stringByAppendingPathExtension:
URLByAppendingPathComponent:isDirectory:
resetImageProperties:preserveRegions:
addImageProperties:composition:options:error:
unknownError:object:
disableIOSurfacePortaitExport
setImageProperties:
setAuxImages:
setRenderWithIOSurface:
prepareAuxiliaryImagesFetchProperties:options:completion:
auxiliaryImagesProperties
setAuxiliaryImageType:
auxiliaryImage
setObject:forKey:
removeObjectForKey:
autoLoopAdjustmentController
metadataConverter
variationForFlavor:
depthAdjustmentController
semanticEnhanceAdjustmentController
livePhotoKeyFrameAdjustmentController
portraitAdjustmentController
unsignedIntegerValue
orientationAdjustmentController
noCropFilter
shouldTryVideoRotationFastPath:options:
_exportVideoToURLFull:composition:options:properties:progress:completion:
originalSize
preferredTransformFromOrientation:size:
initWithCGAffineTransform:
setPreferredTransform:
cropAdjustmentController
isOriginalCrop
setBitRateMultiplicationFactor:
setOutputSettings:
submitWithProgress:completion:
setMetadataConverter:
T@"<PICompositionExporterMetadataConverter>",&
exportComposition:toPrimaryURL:videoComplementURL:videoPosterFrameURL:priority:completionQueue:completion:
kernelsDictionaryWithString:
bilateralKernels
RGBToLabKernels
objectAtIndex:
boundsForPointArray:
enlargedBounds:withPoints:
shapeWithRect:
unionWith:
bilateralAddROI:destRect:userInfo:
bilateralAdd1Kernel
applyWithExtent:roiCallback:arguments:
vectorWithX:Y:
bilateralAdd2Kernel
vectorWithX:Y:Z:
bilateralAdd3Kernel
bilateralAdd4Kernel
bilateralAdd5Kernel
bilateralAdd6Kernel
bilateralAdd7Kernel
bilateralAdd8Kernel
bilateralAdd9Kernel
samplesPerPass
samplerWithImage:options:
RGBToLabKernel
subarrayWithRange:
doBilateralPass:points:weights:sums:slope:
bilateralFinalizeKernel
LabToRGBKernel
inputPoints
setInputPoints:
inputWeights
setInputWeights:
inputEdgeDetail
setInputEdgeDetail:
inputVersion
setInputVersion:
_inputPoints
_inputWeights
_inputEdgeDetail
_inputVersion
T@"NSArray",&,V_inputPoints
T@"NSArray",&,V_inputWeights
T@"NSNumber",&,V_inputEdgeDetail
T@"NSNumber",&,V_inputVersion
BWBilateralKernels
bilateralROI:destRect:userInfo:
bilateralLoop2Kernel
bilateralLoop5Kernel
bilateralLoop11Kernel
doBilateralLoop:points:weights:slope:
inputBorder
setInputBorder:
_inputBorder
T@"NSNumber",&,V_inputBorder
inputRadius
setInputRadius:
_inputRadius
T@"NSNumber",&,V_inputRadius
initWithMajor:minor:subMinor:
initWithMajor:minor:subMinor:platform:
decimalDigitCharacterSet
invertedSet
length
rangeOfCharacterFromSet:
asOrderedInteger
string
isEqualToAdjustmentVersion:
caseInsensitiveCompare:
stringByAppendingFormat:
versionWithMajor:minor:subMinor:platform:
versionFromString:
compare:
subMinorVersion
platform
_majorVersion
_minorVersion
_subMinorVersion
_platform
T@"NSString",R,W,N
TQ,R,N,V_majorVersion
TQ,R,N,V_minorVersion
TQ,R,N,V_subMinorVersion
T@"NSString",R,C,N,V_platform
amountKey
smartColorHDRStatistics
_isIdentity
_kernelV_lt1
_kernelV_gt1
_kernelCPos
_kernelCNeg
_kernelCast
inputVibrancy
setInputVibrancy:
inputContrast
setInputContrast:
inputCast
setInputCast:
T@"NSNumber",&,N,VinputVibrancy
T@"NSNumber",&,N,VinputContrast
T@"NSNumber",&,N,VinputCast
dataWithLength:
mutableBytes
render:toBitmap:rowBytes:bounds:format:colorSpace:
crossfadeDurationValueKey
crossfadeDurationTimescaleKey
startTimeValueKey
startTimeTimescaleKey
loopTimeRangeStartValueKey
loopTimeRangeStartTimescaleKey
loopTimeRangeDurationValueKey
loopTimeRangeDurationTimescaleKey
setLoopTimeRange:
setCrossfadeDuration:
T{?={?=qiIq}{?=qiIq}},N
portraitInfoKey
portraitInfo
spillMatteAllowedKey
setPortraitInfo:
canRenderPortraitEffect
setSpillMatteAllowed:
spillMatteAllowed
_version
Tq,N,V_version
T@"NSNumber",C,N
_noOpRemovalFunctions
noOpRemovalFunctions
copyOfAdjustmentRemovingNoOps:identifier:
valueKey
valueWithIdentifier:inGroup:ofClass:
objectFromData:withMajorVersion:minorVersion:
setTimedMetadata:
_cameraInfoFromMetadataGroup:
items
metadataItemsFromArray:filteredByIdentifier:
unarchivedObjectOfClasses:fromData:error:
focusedDisparity
_cameraInfo
_timedMetadata
_focusedDisparity
_aperture
T@"PTTimedRenderingMetadata",&,N,V_timedMetadata
Td,R,N,V_focusedDisparity
Td,R,N,V_aperture
T@"NSDictionary",R,N,V_cameraInfo
colorSize
disparitySize
quality
isInUse
initWithDevice:colorSize:disparitySize:quality:debugMode:
setInUse:
renderUsingBlock:
setLastUseTime:
lastUseTime
isEqualToDate:
removeObjectIdenticalTo:
initWithDevice:version:colorSize:disparitySize:
setDebugRendering:
setUseRGBA:
initWithDescriptor:
createRenderStateWithQuality:
_renderPipeline
_renderState
_inUse
_quality
_device
_debugMode
_lastUseTime
_colorSize
_disparitySize
inUse
TB,N,GisInUse,V_inUse
T@"NSDate",&,N,V_lastUseTime
T@"<MTLDevice>",R,N,V_device
T{?=qq},R,N,V_colorSize
T{?=qq},R,N,V_disparitySize
Ti,R,N,V_quality
Tq,R,N,V_debugMode
_highKeyHDR
debugDiagnostics
nonLocalizedFailureReason
userInfo
debugFilesEnabled
captureDebugDirectoryForComposition:
debugFilesPrefix
URLByAppendingPathComponent:
createDirectoryAtURL:withIntermediateDirectories:attributes:error:
debugLineDetectionImage
PNGRepresentationOfImage:format:colorSpace:options:
writeToURL:atomically:
faceObservationCache
wrapAsUnexpectedError:
getSizeOfAllFaces:
maxFaceSize
addMethodDiagnostics:details:
lowercaseString
hasFrontFacingCameraDimentions:
disableOnPanos
disableOnFrontFacingCameraImages
isFrontFacingCameraImage:pixelSize:
passesImagePropertiesCheck:
addMethodResultToDiagnostics:error:setYawPitchError:
passesFaceCheck:
shouldRunBuildingCheck
passesBuildingCheck:
submitVerified:
writeDebugDiagnosticsToDisk
overcaptureSourceFilter
primarySourceFilter
setComposition:
primaryImageProperties:
overcaptureImageProperties:
isFusedOvercapture
initWithMasterImageSize:stitchedImageSize:
initWithMasterImageSize:
setCropRect:
angleSeedDegreesCCW
setRollAngle:constrainCropRectWithTargetArea:
cropRect
setAngle:
null
minimumConfidence
minimumPitchCorrection
minimumYawCorrection
minimumAngleCorrection
canGenerateNewCropRect:
defaultFocalLength
maxAutoPitch
maxAutoYaw
maxAutoAngle
minimumPitchCorrectionArea
minimumYawCorrectionArea
exifOrientationAndCropStraightenOnly
passesConfidenceCheck:error:
passesMinimumCorrectionCheck:error:
setDebugLineDetectionImage:
perspectiveErrorFromCoreImage:
undoOrientation:forPitch:yaw:angle:
globalSession
releaseCachedResources
requestVisionCleanUp
setFaceObservationCache:
T@"PIFaceObservationCache",&,N
setMaxAutoYaw:
setMaxAutoPitch:
setMaxAutoAngle:
setMinimumPitchCorrection:
setMinimumYawCorrection:
setMinimumAngleCorrection:
setMinimumConfidence:
setMaxFaceSize:
setMinimumPitchCorrectionArea:
setMinimumYawCorrectionArea:
setDisableOnPanos:
setDisableOnFrontFacingCameraImages:
setShouldRunBuildingCheck:
setAngleSeedDegreesCCW:
setDebugFilesEnabled:
setDebugFilesPrefix:
_disableOnPanos
_disableOnFrontFacingCameraImages
_shouldRunBuildingCheck
_debugFilesEnabled
_faceObservationCache
_maxAutoYaw
_maxAutoPitch
_maxAutoAngle
_minimumPitchCorrection
_minimumYawCorrection
_minimumAngleCorrection
_minimumConfidence
_maxFaceSize
_minimumPitchCorrectionArea
_minimumYawCorrectionArea
_angleSeedDegreesCCW
_debugFilesPrefix
_debugDiagnostics
_debugLineDetectionImage
T@"CIImage",&,N,V_debugLineDetectionImage
T@"NSNumber",C,V_maxAutoYaw
T@"NSNumber",C,V_maxAutoPitch
T@"NSNumber",C,V_maxAutoAngle
Td,V_minimumPitchCorrection
Td,V_minimumYawCorrection
Td,V_minimumAngleCorrection
Td,V_minimumConfidence
Td,V_maxFaceSize
Td,V_minimumPitchCorrectionArea
Td,V_minimumYawCorrectionArea
TB,V_disableOnPanos
TB,V_disableOnFrontFacingCameraImages
TB,V_shouldRunBuildingCheck
Td,V_angleSeedDegreesCCW
TB,V_debugFilesEnabled
T@"NSString",C,V_debugFilesPrefix
T@"NSMutableDictionary",R,V_debugDiagnostics
T@"PIFaceObservationCache",&,N,V_faceObservationCache
luminanceKey
shortValue
angleKey
pitchKey
yawKey
xOriginKey
yOriginKey
widthKey
heightKey
constraintWidthKey
constraintHeightKey
isCropIdentityForImageSize:
isCropConstrained
angle
pitch
smartKey
originalCropKey
setPitch:
setYaw:
isGeometryIdentityForImageSize:
constraintWidth
constraintHeight
angleRadians
pitchRadians
yawRadians
autoCropped
isSmart
setConstraintWidth:
setConstraintHeight:
setAngleRadians:
setPitchRadians:
setYawRadians:
setAutoCropped:
setSmart:
setOriginalCrop:
T{CGRect={CGPoint=dd}{CGSize=dd}},N
smart
TB,N,GisSmart
originalCrop
TB,N,GisOriginalCrop
setInputDecoderVersion:
inputDecoderVersion
_updateSettingsWithInputLight:
overcaptureStatistics
smartToneAdjustmentsForValue:localLightAutoValue:andStatistics:
inputExposureKey
inputContrastKey
inputBrightnessKey
inputShadowsKey
inputHighlightsKey
inputBlackKey
inputLocalLightKey
inputRawHighlightsKey
inputLightKey
offsetExposureKey
offsetBrightnessKey
offsetContrastKey
offsetShadowsKey
offsetHighlightsKey
offsetBlackKey
offsetLocalLightKey
statisticsKey
inputLight
overcaptureStatisticsKey
attributeBrightnessKey
attributeContrastKey
attributeExposureKey
attributeHighlightsKey
attributeShadowsKey
attributeBlackPointKey
attributeLocalLightKey
attributeLightMapKey
attributeLightMapWidthKey
attributeLightMapHeightKey
computedSettings
setInputLight:
setInputExposure:
inputExposure
setInputBrightness:
inputBrightness
setInputShadows:
inputShadows
setInputHighlights:
inputHighlights
setInputBlack:
inputBlack
setInputLocalLight:
inputLocalLight
setInputRawHighlights:
inputRawHighlights
setStatistics:
setOvercaptureStatistics:
offsetBlack
setOffsetBlack:
offsetBrightness
setOffsetBrightness:
offsetContrast
setOffsetContrast:
offsetExposure
setOffsetExposure:
offsetHighlights
setOffsetHighlights:
offsetLocalLight
setOffsetLocalLight:
offsetShadows
setOffsetShadows:
_smartSettings
longExposureFusionKernels
alphaCompositingKernel
dynamismMapKernel
dynamismMapRefineKernel
rgbToLumaKernel
homographyKernel
nccKernel
nccCoarseKernel
blur7x7Kernel
blur5x5Kernel
blur3x3Kernel
fusionKernel
addEntriesFromDictionary:
dictionaryWithDictionary:
getResourceValue:forKey:error:
timeIntervalSinceReferenceDate
typeWithIdentifier:
absoluteString
assetIdentifierForURL:type:useEmbeddedPreview:
setAssetIdentifier:
initWithURL:UTI:
setUseEmbeddedPreview:
initWithSourceDefinitions:
setResolvedSourceDefinition:
initWithCIImage:orientation:
assetIdentifier
resolvedSourceDefinition
initWithImageSourceDefinition:videoSourceDefinition:
_imageRenderRequestWithComposition:wideGamut:
initWithTargetSize:
setExtentPolicy:
newCGImageFromBufferImage:
allValues
isMetalDeviceSupported:
supportsANE
isReadable
isPlayable
isExportable
videoAssetIsHighDynamicRange:
deviceSupportsHardware10BitHEVCEncoding
deviceSupportsHighDynamicRangeVideo
isAVAssetDolbyProfile5:error:
metadataTrackWithPortraitVideoDataInAsset:
isAssetUnsupportedLegacyPortraitVideo:
editSettings
areCPVAssetsEditable
capabilitiesForCurrentDevice
decodingSupportForAVAsset:
is3DEffect:
isPortraitEffect:
iosCropToolFilter
pipelineFiltersForShowingOriginalWithGeometry
resetTag:input:
getTagWithPath:error:
initWithScript:block:
forceGlassesMatteOff
setGlassesMatteAllowed:
forceSpillMatteOff
allowSpillMatteOnOlderPortraitV2Captures
PIAutoLoopAdjustmentKey
PIRedEyeAdjustmentKey
PIDepthAdjustmentKey
PIPortraitAdjustmentKey
PITrimAdjustmentKey
PIMuteAdjustmentKey
newAdjustmentWithIdentifier:
handlePIGlobalSettings:
knownFormatsVersionsMap
updateCropAdjustment:after:error:
standardUserDefaults
boolForKey:
prepareForPerformingRequests:error:
availableDecoderVersions
lastObject
boostParametersFromRawProperties:
currentVersion
imageSourceWithURL:type:proxyImage:orientation:useEmbeddedPreview:
videoSourceWithURL:
livePhotoSourceWithPhotoSource:videoSource:
compositionByRemovingVideoAndLivePhotoAdjustments:
newImageRenderClientWithName:
imagePropertiesRequestWithComposition:
videoPropertiesRequestWithComposition:
imageRenderRequestWithComposition:fitInSize:wideGamut:
imageRenderRequestWithComposition:fillInSize:wideGamut:
priorityWithLevel:
videoRenderRequestWithComposition:fitInSize:
isPortraitStageEffect:
isAVAssetEditable:
effectNameForFilterName:
filterNameForEffectName:
pipelineFiltersForCropping
pipelineFiltersForOriginalGeometry
pipelineFiltersForShowingOriginal
pipelineFiltersForRAWShowingOriginalWithGeometry
validatedCompositionCopyForComposition:mediaType:
updateCropAdjustmentController:after:error:
preheatEditDependencies
rawAdjustmentWithRawImageProperties:
allAdjustmentTypes
nonVisualAdjustmentTypes
PISmartToneAdjustmentKey
PISmartColorAdjustmentKey
PISmartBWAdjustmentKey
PIGrainAdjustmentKey
PIAutoEnhanceAdjustmentKey
PIWhiteBalanceAdjustmentKey
PIApertureRedEyeAdjustmentKey
PIRetouchAdjustmentKey
PIVignetteAdjustmentKey
PISharpenAdjustmentKey
PINoiseReductionAdjustmentKey
PIDefinitionAdjustmentKey
PICurvesAdjustmentKey
PILevelsAdjustmentKey
PISelectiveColorAdjustmentKey
PIEffectAdjustmentKey
PIEffect3DAdjustmentKey
PISlomoAdjustmentKey
PILivePhotoKeyFrameAdjustmentKey
PIVideoPosterFrameAdjustmentKey
PIHighResFusionAdjustmentKey
PIOrientationAdjustmentKey
PIRawAdjustmentKey
PIRawNoiseReductionAdjustmentKey
PISemanticEnhanceAdjustmentKey
PISourceSelectAdjustmentKey
PIVideoStabilizeAdjustmentKey
PIVideoCrossfadeLoopAdjustmentKey
PIPortraitVideoAdjustmentKey
PISourceAdjustmentKey
PIOvercaptureSourceAdjustmentKey
_PISmartToneAdjustmentKey
_PISmartColorAdjustmentKey
_PISmartBWAdjustmentKey
_PIGrainAdjustmentKey
_PIAutoEnhanceAdjustmentKey
_PIWhiteBalanceAdjustmentKey
_PIRedEyeAdjustmentKey
_PIApertureRedEyeAdjustmentKey
_PIRetouchAdjustmentKey
_PIVignetteAdjustmentKey
_PISharpenAdjustmentKey
_PINoiseReductionAdjustmentKey
_PIDefinitionAdjustmentKey
_PICurvesAdjustmentKey
_PILevelsAdjustmentKey
_PISelectiveColorAdjustmentKey
_PIEffectAdjustmentKey
_PIEffect3DAdjustmentKey
_PICropAdjustmentKey
_PITrimAdjustmentKey
_PISlomoAdjustmentKey
_PILivePhotoKeyFrameAdjustmentKey
_PIVideoPosterFrameAdjustmentKey
_PIAutoLoopAdjustmentKey
_PIHighResFusionAdjustmentKey
_PIMuteAdjustmentKey
_PIDepthAdjustmentKey
_PIPortraitAdjustmentKey
_PIOrientationAdjustmentKey
_PIRawAdjustmentKey
_PIRawNoiseReductionAdjustmentKey
_PISemanticEnhanceAdjustmentKey
_PIVideoReframeAdjustmentKey
_PISourceSelectAdjustmentKey
_PIVideoStabilizeAdjustmentKey
_PIVideoCrossfadeLoopAdjustmentKey
_PIPortraitVideoAdjustmentKey
_PISourceAdjustmentKey
_PIOvercaptureSourceAdjustmentKey
T@"NSString",R,N,V_PISmartToneAdjustmentKey
T@"NSString",R,N,V_PISmartColorAdjustmentKey
T@"NSString",R,N,V_PISmartBWAdjustmentKey
T@"NSString",R,N,V_PIGrainAdjustmentKey
T@"NSString",R,N,V_PIAutoEnhanceAdjustmentKey
T@"NSString",R,N,V_PIWhiteBalanceAdjustmentKey
T@"NSString",R,N,V_PIRedEyeAdjustmentKey
T@"NSString",R,N,V_PIApertureRedEyeAdjustmentKey
T@"NSString",R,N,V_PIRetouchAdjustmentKey
T@"NSString",R,N,V_PIVignetteAdjustmentKey
T@"NSString",R,N,V_PISharpenAdjustmentKey
T@"NSString",R,N,V_PINoiseReductionAdjustmentKey
T@"NSString",R,N,V_PIDefinitionAdjustmentKey
T@"NSString",R,N,V_PICurvesAdjustmentKey
T@"NSString",R,N,V_PILevelsAdjustmentKey
T@"NSString",R,N,V_PISelectiveColorAdjustmentKey
T@"NSString",R,N,V_PIEffectAdjustmentKey
T@"NSString",R,N,V_PIEffect3DAdjustmentKey
T@"NSString",R,N,V_PICropAdjustmentKey
T@"NSString",R,N,V_PITrimAdjustmentKey
T@"NSString",R,N,V_PISlomoAdjustmentKey
T@"NSString",R,N,V_PILivePhotoKeyFrameAdjustmentKey
T@"NSString",R,N,V_PIVideoPosterFrameAdjustmentKey
T@"NSString",R,N,V_PIAutoLoopAdjustmentKey
T@"NSString",R,N,V_PIHighResFusionAdjustmentKey
T@"NSString",R,N,V_PIMuteAdjustmentKey
T@"NSString",R,N,V_PIDepthAdjustmentKey
T@"NSString",R,N,V_PIPortraitAdjustmentKey
T@"NSString",R,N,V_PIOrientationAdjustmentKey
T@"NSString",R,N,V_PIRawAdjustmentKey
T@"NSString",R,N,V_PIRawNoiseReductionAdjustmentKey
T@"NSString",R,N,V_PISemanticEnhanceAdjustmentKey
T@"NSString",R,N,V_PIVideoReframeAdjustmentKey
T@"NSString",R,N,V_PISourceSelectAdjustmentKey
T@"NSString",R,N,V_PIVideoStabilizeAdjustmentKey
T@"NSString",R,N,V_PIVideoCrossfadeLoopAdjustmentKey
T@"NSString",R,N,V_PIPortraitVideoAdjustmentKey
T@"NSString",R,N,V_PISourceAdjustmentKey
T@"NSString",R,N,V_PIOvercaptureSourceAdjustmentKey
rawToneCurveProperties
curveValueAt:
outputImageFB3
outputImageFB0
kernelFB0
kernelFB3
T@"NSString",R
inputBoost
setInputBoost:
inputParams
setInputParams:
_inputBoost
_inputParams
Td,V_inputBoost
T@"NSString",C,V_inputVersion
T@"NSArray",C,V_inputParams
kernelsWithString:
stringForColorType:
arrayWithArray:
colorTypeForString:
initWithBase64EncodedString:options:
base64EncodedStringWithOptions:
inputTableImage
curvesKernel
setInputTableImage:
_inputTableImage
T@"CIImage",&,V_inputTableImage
surfaceStoragePool
workingColorSpace
dealloc
sRGBLinearColorSpace
set_accumError:
_start
setLabel:
renderImage:rect:toDestination:atPoint:error:
useAsCIRenderDestinationWithRenderer:block:
_isReadyForMoreData
_markAsFinished
markAsFinished
_accumError
_appendInputFrame:
_nextInputFrame
_initializeAccumulation
nextInputFrame
_accumulate:
_initializeAccumulation:
colorWithRed:green:blue:colorSpace:
imageWithColor:
_initializeStorage:image:error:
_accumulate:error:
CVPixelBuffer
imageWithCVPixelBuffer:options:
componentMax
componentMin
useAsCIImageWithOptions:renderer:block:
waitUntilDone
_exportOutputImage:format:colorSpace:toURL:uti:error:
_dynamismMapWithMinImage:maxImage:extent:
context
createCGImage:fromRect:format:colorSpace:deferred:
initWithSize:renderer:jobNumber:
cancel
start:
isReadyForMoreData
accumulate:error:
writeLongExposureImage:UTI:colorSpace:error:
writeMaskImage:UTI:error:
_pixelSize
_renderer
_temporaryDestinationStorage
_averageAccumulationStorage
_minimumAccumulationStorage
_maximumAccumulationStorage
_frameCount
_jobNumber
_stateQueue
_accumQueue
_accumSemaphore
_readySemaphore
_doneGroup
_inputFrames
_finished
_imageOptions
__accumError
T@"NSError",&,V__accumError
observation
T@"VNImageHomographicAlignmentObservation",R,C
setObservation:
setExtent:
_observation
_extent
T@"VNImageHomographicAlignmentObservation",C,N,V_observation
T{?={?=qq}{?=qq}},N,V_extent
newRenderPipelineStateForEvaluationMode:
_shouldWaitForDependentJobs
prepareNodeWithPipelineState:error:
nodeByReplayingAgainstCache:error:
setStillImage:
stillImage
prepareNode
registrationRequest
outputGeometry
renderScale
setGuideExtent:
renderer:
guideExtent
newPixelBufferOfSize:format:
initWithPixelBuffer:
jobNumber
waitUntilCompletedAndReturnError:
initWithTargetedCVPixelBuffer:options:
initWithCVPixelBuffer:options:
wantsOutputImage
wantsRenderScaleClampedToNativeScale
_stillImage
_guideExtent
T{?={?=qq}{?=qq}},N,V_guideExtent
T@"CIImage",&,N,V_stillImage
T@"VNImageHomographicAlignmentObservation",&,N,V_observation
initWithComposition:tag:responseQueue:
initWithComposition:responseQueue:
_pipelineFilters
inputDestinationImage
setInputDestinationImage:
inputCorrectionInfo
setInputCorrectionInfo:
inputCameraModel
setInputCameraModel:
_inputDestinationImage
_inputCorrectionInfo
_inputCameraModel
T@"CIImage",&,N,V_inputDestinationImage
T@"NSArray",&,N,V_inputCorrectionInfo
T@"NSString",&,N,V_inputCameraModel
posterFrameTime
setPosterFrameTime:
adjustmentHasPerspective:settings:
adjustmentHasCTM:settings:
_versionRules
versionRules
currentFormatVersion
indexOfObject:
formatVersionForAdjustment:identifier:
sourceDefinitions
fileURLWithPath:
startKey
startScaleKey
endKey
endScaleKey
rateKey
endTime
setEndTime:
rate
setRate:
cinematographyScript
assetWithURL:
setCinematographyScript:
loadWithAsset:changesDictionary:completion:
labelImageCache
setLabelImageCache:
setRenderTime:
roundedRectangleGeneratorFilter
setColor:
frameNearestTime:
focusDetection
allDetections
detectionType
trackIdentifier
_imageByAddingDetection:toImage:isPrimary:canvasSize:inverseOrientation:
initWithInput:assetURL:cinematographyState:monochrome:
_cinematographyScript
_labelImageCache
_renderTime
T@"PTCinematographyScript",&,N,V_cinematographyScript
T{?=qiIq},N,V_renderTime
T@"NSCache",&,N,V_labelImageCache
colorWithRed:green:blue:
groupIdentifier
blackColor
sourceOutCompositingFilter
setBackgroundImage:
textImageGeneratorFilter
setText:
setFontName:
setFontSize:
outputExposureKey
falseColorHDRKey
inputRAWGamutMapMaxKey
bytesPerPixel
initWithSize:format:
mutableBytesAtPoint:
RG16
dataWithBytes:length:
newLinearWideGamutColorSpace
inputWarmth
inputOrigI
inputOrigQ
linearWideGamutColorSpace
faceBalanceKernels
setInputOrigI:
setInputOrigQ:
setInputWarmth:
_inputOrigI
_inputOrigQ
_inputStrength
_inputWarmth
Td,N,V_inputOrigI
Td,N,V_inputOrigQ
Td,N,V_inputStrength
Td,N,V_inputWarmth
initWithLength:
initWithImageProvider:width:height:format:colorSpace:options:
inputPointsR
curvePointsFromDictionaries:
inputPointsG
inputPointsB
inputPointsL
calculateCurveTable:
tableImageFromRed:green:blue:luminance:
setInputPointsR:
setInputPointsG:
setInputPointsB:
setInputPointsL:
_inputPointsR
_inputPointsG
_inputPointsB
_inputPointsL
T@"NSArray",&,V_inputPointsR
T@"NSArray",&,V_inputPointsG
T@"NSArray",&,V_inputPointsB
T@"NSArray",&,V_inputPointsL
colorBalanceKernels
gHDRtoPPKernel
PPtogHDRKernel
applyInputConversion:
colorBalanceKernel
applyOutputConversion:
inputWarmTemp
setInputWarmTemp:
inputWarmTint
setInputWarmTint:
inputHasFace
setInputHasFace:
inputIsRaw
setInputIsRaw:
_inputWarmTemp
_inputWarmTint
_inputHasFace
_inputIsRaw
T@"NSNumber",&,N,V_inputWarmTemp
T@"NSNumber",&,N,V_inputWarmTint
T@"NSNumber",&,N,V_inputStrength
T@"NSNumber",&,N,V_inputHasFace
T@"NSNumber",&,N,V_inputIsRaw
substringFromIndex:
_map
getMap
HDRFilterForSDRFilter:
alignmentKey
alignment
setAlignment:
PUEditSettings
socPseudoColorFilter
IPXEditSettings
falseColorHDR
setFalseColorHDR:
decoratorRenderFiltersForVideos
setForceGlassesMatteOff:
setForceSpillMatteOff:
setAllowSpillMatteOnOlderPortraitV2Captures:
_settings
_forceGlassesMatteOff
_forceSpillMatteOff
_allowSpillMatteOnOlderPortraitV2Captures
TB,N,V_forceGlassesMatteOff
TB,N,V_forceSpillMatteOff
TB,N,V_allowSpillMatteOnOlderPortraitV2Captures
boolSettingForKey:defaultValue:
shouldUseMetalRenderer
initWithMetalDevice:options:
initWithVideoExportRequest:
metalRenderer
autoLoopExportRequest
setUpContext:
currentContext
contextForContext:
isObject
setError:
toDictionary
node
initWithKeyframes:stabCropRect:input:
initWithNode:context:
jsContext
toRect
overcaptureRectForStitchedOvercaptureSize:overcaptureVideoComplementSize:primarySize:autoLoopStabilizedCropRect:
bundleForClass:
URLForResource:withExtension:
newPhotosPipelineAtSourceURL:error:
initWithURL:
newPhotosPipeline:
renderContext
cancelAllRequests
apertureRedEyeResultFromFaceObservations:imageSize:
landmarks
leftEye
rightEye
pointCount
normalizedPoints
_faceRequest
localLightHDRStatisticsNoProxy
inputLightMap
inputLightMapWidth
inputLightMapHeight
inputGuideImage
_polyKernelHDR
_shadowKernelHDR
inputLightMapImage
inputSmartShadows
initWithData:width:height:bytesPerRow:bytesPerComponent:format:colorspace:
initWithBytes:width:height:bytesPerRow:bytesPerComponent:format:colorspace:
elementByteSize
rowElements
bufferColorspace
T@"NSMutableData",&,Vdata
TQ,R,VelementByteSize
TQ,R,VrowElements
TQ,R,Vwidth
TQ,R,Vheight
Ti,R,Vformat
initWithTargetPixelSize:
_calculateBlackAndWhiteSettingsFromBufferImage:
CIFormat
getNonNormalizedSettings:
createHueArray
imageWithBitmapData:bytesPerRow:size:format:options:
hueArrayImage:
smartBlackWhiteKernel
inputNeutralGamma
setInputNeutralGamma:
inputTone
setInputTone:
inputHue
setInputHue:
inputGrain
setInputGrain:
inputSeed
setInputSeed:
inputScaleFactor
setInputScaleFactor:
T@"NSNumber",C,N,VinputStrength
T@"NSNumber",C,N,VinputNeutralGamma
T@"NSNumber",C,N,VinputTone
T@"NSNumber",C,N,VinputHue
T@"NSNumber",C,N,VinputGrain
T@"NSNumber",C,N,VinputSeed
T@"NSNumber",C,N,VinputScaleFactor
smartBlackAndWhiteStatistics
smartBlackAndWhiteAdjustmentsForValue:andStatistics:
inputCorrectionInfoKey
hasCorrections
stopAtTagIncludeGeometryFilter:
histogramOptimizationFilter
histogramCalculationColorSpace
setHistogramCalculationColorSpace:
histogram
calculateSettingsForImageHistogram:
percentile:
calculateSettingsForSingleChannelHistogram:suffix:
luminance
green
blue
_adjustmentControllerForKey:creatingIfNecessary:expectedClass:
smartColorAdjustmentControllerCreatingIfNecessary:
smartBWAdjustmentControllerCreatingIfNecessary:
cropAdjustmentControllerCreatingIfNecessary:
redEyeAdjustmentControllerCreatingIfNecessary:
livePhotoKeyFrameAdjustmentControllerCreatingIfNecessary:
videoPosterFrameAdjustmentControllerCreatingIfNecessary:
depthAdjustmentControllerCreatingIfNecessary:
trimAdjustmentControllerCreatingIfNecessary:
slomoAdjustmentControllerCreatingIfNecessary:
effectAdjustmentControllerCreatingIfNecessary:
effect3DAdjustmentControllerCreatingIfNecessary:
portraitAdjustmentControllerCreatingIfNecessary:
autoLoopAdjustmentControllerCreatingIfNecessary:
highResFusionAdjustmentControllerCreatingIfNecessary:
rawAdjustmentControllerCreatingIfNecessary:
rawNoiseReductionAdjustmentControllerCreatingIfNecessary:
sharpenAdjustmentControllerCreatingIfNecessary:
whiteBalanceAdjustmentControllerCreatingIfNecessary:
noiseReductionAdjustmentControllerCreatingIfNecessary:
definitionAdjustmentControllerCreatingIfNecessary:
vignetteAdjustmentControllerCreatingIfNecessary:
videoReframeAdjustmentControllerCreatingIfNecessary:
sourceSelectAdjustmentControllerCreatingIfNecessary:
videoStabilizeAdjustmentControllerCreatingIfNecessary:
videoCrossfadeLoopAdjustmentControllerCreatingIfNecessary:
semanticEnhanceAdjustmentControllerCreatingIfNecessary:
portraitVideoAdjustmentControllerCreatingIfNecessary:
smartToneAdjustmentController
smartColorAdjustmentController
smartBWAdjustmentController
redEyeAdjustmentController
videoPosterFrameAdjustmentController
trimAdjustmentController
slomoAdjustmentController
effectAdjustmentController
effect3DAdjustmentController
highResFusionAdjustmentController
rawAdjustmentController
rawNoiseReductionAdjustmentController
sharpenAdjustmentController
whiteBalanceAdjustmentController
noiseReductionAdjustmentController
definitionAdjustmentController
vignetteAdjustmentController
videoReframeAdjustmentController
videoStabilizeAdjustmentController
videoCrossfadeLoopAdjustmentController
portraitVideoAdjustmentController
T@"PIAdjustmentConstants",R,N
isEditable
initWithX:y:editable:
_editable
Td,R,N,V_x
Td,R,N,V_y
editable
TB,R,N,GisEditable,V_editable
smartToneHDRStatistics
_kernelBneg
_kernelBpos
_kernelRH
_kernelH
_kernelC_hdr
_kernelC
T@"NSNumber",&,N,VinputExposure
T@"NSNumber",&,N,VinputBrightness
T@"NSNumber",&,N,VinputShadows
T@"NSNumber",&,N,VinputHighlights
T@"NSNumber",&,N,VinputBlack
T@"NSNumber",&,N,VinputRawHighlights
loadFusionTuningParameters
stringSettingForKey:defaultValue:
dictionaryWithContentsOfFile:
_debugDumpIntermediateImages
inputMaskImage
inputStillImage
inputRenderScale
inputVideoScale
inputAlignmentExtent
inputAlignmentTransform
valueAtIndex:
alignImage:transform:extent:
imageByApplyingTransform:highQualityDownsample:
_computeNCCMapFromImage:toImage:scale:
_refineMaskImage:guideImage:scale:
_fuseImage:withGuideImage:weightImage:maskImage:
debugDumpIntermediateImages
currentDirectoryPath
writeToTIFF:
applyWithExtent:roiCallback:inputImage:arguments:
setInputStillImage:
setInputMaskImage:
setInputRenderScale:
setInputVideoScale:
setInputAlignmentExtent:
setInputAlignmentTransform:
_inputStillImage
_inputMaskImage
_inputRenderScale
_inputVideoScale
_inputAlignmentExtent
_inputAlignmentTransform
T@"CIImage",&,N,V_inputStillImage
T@"CIImage",&,N,V_inputMaskImage
T@"NSNumber",&,N,V_inputRenderScale
T@"NSNumber",&,N,V_inputVideoScale
T@"CIVector",&,N,V_inputAlignmentExtent
T@"CIVector",&,N,V_inputAlignmentTransform
isEnabled
evaluate:input:pipelineState:error:
initWithPipelineState:
mediaTypeForComposition:
hasStaticTime
valueWithCMTime:
beginGroupWithName:error:
renderNodeFromSource:settings:error:
initWithAffineTransform:
transformNodeWithInput:transform:error:
inputForPath:error:
addTagWithName:inputNode:error:
endGroupWithName:error:
enableHDRSupport
auxiliaryImageType
allAssetsCanUseHDRPipeline
imageProperties:
initWithCGColorSpace:
isHDR
cacheNode:type:settings:error:
performLongExposureFusionForComposition:longExposureImage:useHDRFilter:error:
scaleNode:scale:error:
overcaptureRectForAutoCrop:overcaptureVideoComplementSize:primarySize:CGRect:
nodeByApplyingFilterWithName:useHDRFilter:settingsAndInputs:
performApertureRedeyeOnImage:useHDRFilter:redEyeAdjustment:
performRedeyeOnImage:useHDRFilter:redEyeAdjustment:
auxiliaryImageFromComposition:type:mediaComponentType:error:
versionForPortraitEffect:
vectorWithFloats:
auxiliaryDataInfoMetadata
depthCameraCalibrationData
scaleMultiplyOfScalar:
isCIFilterAvailable:propertyName:
initWithInput:scale:
remapPortraitV2Strength:portraitEffectKind:
isSourceAvailable:sourceSettings:
portraitVideo:disparityInput:disparityKeyframes:apertureKeyframes:debugMode:error:
portraitVideoDebugDetectedObjects:source:cinematographyState:monochrome:error:
cropNode:cropRect:cropSettings:
isEqualToNumber:
initWithInput:
livePhotoKeyFrameMetadataFromNode:time:error:
trimInput:startTime:endTime:error:
createSloMoWithInput:startTime:endTime:rate:error:
grainInputSeedFromFrameTime
sharpnessWithIntensity:
videoReframe:reframes:error:
videoCrossfadeLoop:crossfadeAdjustment:error:
perspectiveTransformWithPitch:yaw:roll:imageRect:
straightenTransformWithAngle:extent:
originalCleanAperture
orientedNode:withOrientation:
scaledVector:
debugNodeByApplyingFilterWithName:useHDRFilter:settingsAndInputs:
_processedRenderNodeForComposition:input:pipelineState:error:
P3Kernel
deserializeFromDictionary:error:
sourceSelectSchema
photosCompositionSchema
rawSchema
rawNoiseReductionSchema
retouchSchema
smartToneSchema
smartColorSchema
whiteBalanceSchema
cropSchema
trimSchema
slomoSchema
livePhotoKeyFrameSchema
muteSchema
videoPosterFrameSchema
autoLoopSchema
orientationSchema
effectSchema
redEyeSchema
apertureRedEyeSchema
smartBlackAndWhiteSchema
grainSchema
sharpenSchema
definitionSchema
noiseReductionSchema
vignetteSchema
levelsSchema
curvesSchema
selectiveColorSchema
depthEffectSchema
effect3DSchema
portraitEffectSchema
highResFusionSchema
videoReframeSchema
videoStabilizeSchema
videoCrossfadeLoopSchema
debugSchema
semanticEnhance
portraitVideoSchema
renderPipelineForIdentifier:
registerPhotosSchema
registerSchemas:error:
registerRenderPipeline:forIdentifier:
T@"NUIdentifier",R
candidacy
descriptionForCandidacy:
setCandidacy:
shouldAllowPerspectiveCorrection
finalizerError
rollAngleDegrees
pitchAngleDegrees
yawAngleDegrees
getCropRectThatCompletelyContainsMasterImageForPitch:yaw:roll:
initWithDisposition:composition:
performNextActionWithCompletion:
shouldPerformAction:
markActionAsPerformed:
processHorizonResult:
setFinalizerError:
performHorizonCorrectionWithCompletion:
processPerspectiveResult:
performPerspectiveCorrectionWithCompletion:
hasPerformedAction:
performedActions
setPerformedActions:
floatForKey:
setShouldPerformAutoCrop:
setShouldPerformAutoStraighten:
setShouldUseAutoStraightenVerticalDetector:
setMaxAutoStraighten:
setMinAutoStraighten:
setAutoStraightenDominantAngleDiffThreshold:
setAutoStraightenVerticalAngleThreshold:
setRollAngleDegrees:
setPitchAngleDegrees:
setYawAngleDegrees:
_candidacy
_finalizerError
_performedActions
_rollAngleDegrees
_pitchAngleDegrees
_yawAngleDegrees
T@"NSError",&,N,V_finalizerError
TQ,N,V_performedActions
Td,N,V_rollAngleDegrees
Td,N,V_pitchAngleDegrees
Td,N,V_yawAngleDegrees
TQ,N,V_candidacy
disposition
_disposition
Tq,R,V_disposition
T@"NUComposition",R,C,V_composition
cropFraction
setCropFraction:
setAnalysisType:
TQ,N
shouldPerformAutoStraighten
shouldPerformAutoCrop
shouldUseAutoStraightenVerticalDetector
maxAutoStraighten
autoCropFilter
minAutoStraighten
undoExifOrientation:error:
autoStraightenVerticalAngleThreshold
autoStraightenDominantAngleDiffThreshold
_shouldPerformAutoCrop
_shouldPerformAutoStraighten
_shouldUseAutoStraightenVerticalDetector
_autoStraightenVerticalAngleThreshold
_autoStraightenDominantAngleDiffThreshold
_maxAutoStraighten
_minAutoStraighten
TB,V_shouldPerformAutoCrop
TB,V_shouldPerformAutoStraighten
TB,V_shouldUseAutoStraightenVerticalDetector
T@"NSNumber",C,V_autoStraightenVerticalAngleThreshold
T@"NSNumber",C,V_autoStraightenDominantAngleDiffThreshold
Td,V_maxAutoStraighten
Td,V_minAutoStraighten
noGeometryFilter
inputToCropFilter
stitchedOvercaptureRect:primaryRect:forComposition:error:
initWithMasterImageRect:stitchedImageRect:
setRollRadians:
integralCropRect:
inputColorKey
_updateSettingsWithInputColor:
inputSaturationKey
inputCastKey
offsetSaturationKey
offsetCastKey
attributeVibrancyKey
attributeCastKey
setInputColor:
inputColor
setInputSaturation:
inputSaturation
setOffsetCast:
offsetCast
setOffsetSaturation:
offsetSaturation
_stats
iptFromLinearInto:fromRed:green:blue:
hueAngleFrom:
selectiveColorKernels
colorWithRed:green:blue:alpha:colorSpace:
iptHueAngleFromRed:green:blue:
convertToIPT:
hueSatLumTable
convertFromIPT:
inputCorrections
setInputCorrections:
_inputCorrections
T@"NSArray",&,N,V_inputCorrections
setKeyframeSequence:
setStabCropRect:
setShouldApplyWatermark:
setInputVideoProperties:
setSize:
inputVideoProperties
keyframeSequence
_stabilizeImage:cleanRect:cropRect:transform:geometry:
vectorWithCGPoint:
pi_imageByApplyingStabilizationWatermark
_evaluateVideoProperties:
_evaluateImageGeometry:
canPropagateOriginalLivePhotoMetadataTrack
shouldApplyWatermark
_shouldApplyWatermark
_keyframeSequence
_inputVideoProperties
_frameDuration
T@"PIReframeKeyframeSequence",&,N,V_keyframeSequence
T{?={?=qq}{?=qq}},N,V_stabCropRect
T@"<NUVideoProperties>",&,N,V_inputVideoProperties
T{?=qiIq},N,V_frameDuration
TB,N,V_shouldApplyWatermark
imageWithCGImage:
dictionaryForKey:
formatDescriptions
setSubjects:
setEstimatedCenterMotion:
setEstimatedMotionBlur:
setTrajectoryHomography:
subjects
estimatedCenterMotion
estimatedMotionBlur
_subjects
_estimatedCenterMotion
_estimatedMotionBlur
_trajectoryHomography
T{?=qiIq},R,V_time
T@"NSArray",R,V_subjects
T{CGVector=dd},R,V_estimatedCenterMotion
T{CGVector=dd},R,V_estimatedMotionBlur
T{?=[3]},R,V_trajectoryHomography
exceptionWithName:reason:userInfo:
tracks
encodedPixelSizeOfTrack:oriented:
extractMetadata
initWithContentsOfFile:
assetReaderWithAsset:error:
initWithTrack:outputSettings:
canAddOutput:
addOutput:
initWithAssetReaderTrackOutput:
startReading
nextTimedMetadataGroup
dataType
dataValue
subjectsFromMetadata:
centerMotionVectorFromMetadata:
motionBlurVectorFromMetadata:
trajectoryeHomographyFromMetadata:containsV3Metadata:
overwriteTrackingMetadataWithPlist:
_asset
_videoTrack
_mdataTrack
ndcMetadataTransform
pxlMetadataTransform
T@"NSArray",R,N,VtimedMetadataArray
vectorWithX:
RGBToYIQKernel
YIQToRGBKernel
isDefaultWarmth:
whiteBalanceKernel
warmth
setWarmth:
setY:
setI:
setQ:
_strength
_warmth
T@"NSNumber",&,N,V_strength
T@"NSNumber",&,N,V_warmth
T@"NSNumber",&,N,V_y
T@"NSNumber",&,N,V_i
T@"NSNumber",&,N,V_q
isHDRComposition:
setDataExtractor:
force
_options
initWithRequest:options:
internalComposition
options
setOptions:
setForce:
_force
TB,V_force
initWithComposition:location:touchDiameter:
_location
_touchDiameter
setPerservesAlpha:
_interpolateGrainKernel
_paddedTileKernel
_grainBlendAndMixKernel
inputISO
setInputISO:
inputAmount
setInputAmount:
T@"NSNumber",C,N,VinputISO
T@"NSNumber",C,N,VinputAmount
writeCGImage:fileURL:options:
path
writeImage:toDirectoryAtPath:withBasename:
stringByAppendingPathComponent:
writeImage:fileURL:
writeCGImage:fileURL:
writeImage:toTemporaryDirectoryWithBasename:
currentSoftwareVersion
buildNumber
edgesKey
baseAddress
bytesPerRow
region
convertFloat:toFixed16:count:
RGBA16
initWithSize:format:rowBytes:mutableBytes:
convertFixed16:toFloat:count:
initWithSize:format:rowBytes:bytes:
initWithBuffer:colorSpace:validRegion:
initWithMutableBuffer:colorSpace:validRegion:
copyPixelsFromImage:srcRect:destImage:destOrigin:
ROIForCenterPoint:radius:
pointValue
inputSpots
auxiliaryImage:
underlyingAVDepthData
isDepthDataFiltered
depthDataQuality
cameraCalibrationData
auxiliaryCoreGraphicsInfoDictionary:
depthBlurEffectRenderingParameters
getMinMaxSimulatedApertureFrom:minValue:maxValue:version:
methodForSelector:
depthBlurEffectSimulatedAperture
portraitLightingEffectStrength
initFromMinAperture:maxAperture:aperture:portraitStrength:SDOFRenderingVerion:depthVersionInfo:
minimumAperture
maximumAperture
portraitStrength
SDOFRenderingVersion
portraitMajorVersion
portraitMinorVersion
valuesAtCaptureFromImageProperties:error:
setPortraitStrength:
setMinimumAperture:
setMaximumAperture:
setSDOFRenderingVersion:
setPortraitMajorVersion:
setPortraitMinorVersion:
depthVersionInfo
setDepthVersionInfo:
_portraitStrength
_minimumAperture
_maximumAperture
_SDOFRenderingVersion
_portraitMajorVersion
_portraitMinorVersion
_depthVersionInfo
Tf,N,V_aperture
Tf,N,V_portraitStrength
T@"NSNumber",&,N,V_minimumAperture
T@"NSNumber",&,N,V_maximumAperture
TQ,N,V_SDOFRenderingVersion
TQ,N,V_portraitMajorVersion
TQ,N,V_portraitMinorVersion
T{?=ii},N,V_depthVersionInfo
focusRectDictionaryFromRect:
isStillImageDisparity:
_calculateWithImageProperties:valuesAtCapture:completion:
portraitInfoDictionaryFromFaceObservations:metadata:orientation:valuesAtCapture:
canApplyPortraitEffectsWithMetadata:
portraitEffectInfoDictionaryFromFaceObservations:orientation:valuesAtCapture:
depthEffectInfoDictionaryFromFaceObservations:metadata:orientation:valuesAtCapture:
focusRectDictionaryFromMetadata:
depthEffectInfoDictionaryFromFaceObservations:focus:valuesAtCapture:lumaNoiseScale:orientation:
nose
allPoints
faceJunkinessIndex
faceOrientationIndex
roll
faceObservationsData
indexesOfShallowDepthOfFieldObservations
objectsAtIndexes:
faceOrientation
focusRectangle
minimumApertureFocalRatio
maximumApertureFocalRatio
apertureFocalRatio
luminanceNoiseAmplitude
portraitInfoDictionaryFromCameraMetadata:
resetTag:input:error:
noRedEyeFilter
noTrimFilter
noMuteFilter
stripAllTimeAdjustmentsFilter
noOrientationFilter
perspectiveStraightenWithoutCropFilter
preGeometryFilter
postGeometryFilter
autoloopStabilizedVideoFilter
spatialOvercaptureVideoSourceFilter
colorTypeKey
faceStrengthKey
faceWarmthKey
faceIKey
faceQKey
grayStrengthKey
grayWarmthKey
grayYKey
grayIKey
grayQKey
temperatureKey
tintKey
warmTempKey
warmFace
warmTintKey
warmFaceKey
colorType
setColorType:
faceStrength
setFaceStrength:
faceWarmth
setFaceWarmth:
faceI
setFaceI:
faceQ
setFaceQ:
grayStrength
setGrayStrength:
grayWarmth
setGrayWarmth:
grayY
setGrayY:
grayI
setGrayI:
grayQ
setGrayQ:
warmTemp
setWarmTemp:
warmTint
setWarmTint:
setWarmFace:
sourceDefinition:
redEyeSpotsWithCorrectionInfo:
depthInfo
depthInfoKey
apertureKey
glassesMatteAllowedKey
canRenderDepth
setDepthInfo:
capturedAperture
glassesMatteAllowed
keyframesKey
stabCropRectKey
setKeyframes:
copyKeyframesTrimmingToTimeRange:
T{?={?=qq}{?=qq}},N
indexOfObject:inSortedRange:options:usingComparator:
setColorMatrix:
setParameters:
computeCurvesForImageHistogram:
_defaultCurveArray
replaceObjectAtIndex:withObject:
dictionariesFromPoints:
autoValuesForBlackPoint:whitePoint:
curvePointAtIndex:blackPoint:whitePoint:histogram:
insertObject:atIndex:
B24@0:8@16
#16@0:8
@16@0:8
@24@0:8:16
@32@0:8:16@24
@40@0:8:16@24@32
B16@0:8
B24@0:8#16
B24@0:8:16
Vv16@0:8
Q16@0:8
^{_NSZone=}16@0:8
B24@0:8@"Protocol"16
@"NSString"16@0:8
@"<NURenderStatistics>"16@0:8
@"PTCinematographyTrack"16@0:8
@24@0:8@16
v16@0:8
@"PTCinematographyTrack"
B24@0:8o^@16
v76@0:8{?=qiIq}16{CGRect={CGPoint=dd}{CGSize=dd}}40f72
{?=qiIq}16@0:8
v40@0:8{?=qiIq}16
{CGPoint=dd}16@0:8
v32@0:8{CGPoint=dd}16
@?16@0:8
v24@0:8@?16
v24@0:8@16
v20@0:8B16
{CGPoint="x"d"y"d}
{?="value"q"timescale"i"flags"I"epoch"q}
@64@0:8@16{?=qiIq}24{CGPoint=dd}48
@24@0:8^{_NSZone=}16
q16@0:8
@"CIImage"
f16@0:8
v20@0:8f16
@40@0:8@16q24^{CGColorSpace=}32
@"PFStoryRecipeDisplayAssetNormalization"
B32@0:8@16@24
v24@0:8d16
d16@0:8
v32@0:8q16d24
v32@0:8@16q24
@20@0:8f16
{CGRect={CGPoint=dd}{CGSize=dd}}16@0:8
v48@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16
{CGRect="origin"{CGPoint="x"d"y"d}"size"{CGSize="width"d"height"d}}
@80@0:8@16{?=qiIq}24{CGRect={CGPoint=dd}{CGSize=dd}}48
@"NSNumber"
@32@0:8@16@24
#24@0:8@16
v32@0:8@16@24
v32@0:8@16@?24
B28@0:8@16B24
B36@0:8@16@24B32
B40@0:8@16@24@?32
v24@0:8q16
@"NUComposition"
{?="hasDidAdd"B"hasDidRemove"B"hasDidUpdate"B"hasDidUpdateMultiple"B"hasClassForController"B}
@"NSDictionary"
@"<PICompositionControllerDelegate>"
@48@0:8@16@24@32o^@40
B32@0:8@16o^@24
@56@0:8@16@24@32@40o^@48
@40@0:8@16@24o^@32
B48@0:8@16@24@32o^@40
@32@0:8@16o^@24
@"NSData"
@"NSString"
d40@0:8@16d24^B32
@32@0:8@16q24
{CGPoint=dd}72@0:8{CGPoint=dd}16{CGRect={CGPoint=dd}{CGSize=dd}}32q64
{CGPoint=dd}32@0:8r^{CGPoint=dd}16Q24
@56@0:8@16@24@32@40@48
@"NSURL"
{?={?=qq}{?=qq}}16@0:8
v48@0:8{?={?=qq}{?=qq}}16
{?="origin"{?="x"q"y"q}"size"{?="width"q"height"q}}
B48@0:8@16@24@32^@40
i20@0:8i16
i16@0:8
{CGRect={CGPoint=dd}{CGSize=dd}}60@0:8i16@20{CGRect={CGPoint=dd}{CGSize=dd}}28
@92@0:8@16@24@32@40@48@56@64@72B80o^@84
@"AVMetadataItem"
@"AVTimedMetadataGroup"
@120@0:8@16{?={?=qiIq}{?=qiIq}}24{?=qiIq}72{?=qiIq}96
@24@0:8o^@16
{?={?=qiIq}{?=qiIq}}16@0:8
{?="start"{?="value"q"timescale"i"flags"I"epoch"q}"duration"{?="value"q"timescale"i"flags"I"epoch"q}}
v40@0:8@16@24d32
{?=qiIq}32@0:8@16@24
@"NSMutableDictionary"
@"NUIdentifier"
@"NUAdjustment"
@56@0:8@16@24@32@40q48
{?=qq}32@0:8{?=qq}16
i32@0:8{?=qq}16
@"NSObject<OS_dispatch_group>"
@"NSObject<OS_dispatch_queue>"
@"<NUFaceDetectionResult>"
q24@0:8@16
@24@0:8q16
@48@0:8@16{?=qiIq}24
@24@0:8@"NSDictionary"16
@"NSDictionary"16@0:8
@48@0:8{?=qiIq}16d40
v24@0:8@"NSCoder"16
@24@0:8@"NSCoder"16
@48@0:8q16q24q32d40
@32@0:8@16#24
v72@0:8{?={?=qiIq}{?=qiIq}}16@64
@88@0:8{?=qiIq}16{?=[3]}40
{?=[3]}16@0:8
{?="columns"[3]}
{?=[3]}40@0:8{?=qiIq}16
@"NUKeyframeSequenceMatrixFloat33"
@"AVAsset"
@"NSArray"16@0:8
@72@0:8@16{?={?=qq}{?=qq}}24Q56@64
@"NSArray"
v24@0:8Q16
{?={?=qq}{?=qq}}96@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{?={?=qq}{?=qq}}48d80d88
{?=ddd}56@0:8@16{?={?=qq}{?=qq}}24
@28@0:8@16B24
{?={?=[4d]}{?=[4d]}d}16@0:8
@88@0:8{?={?=[4d]}{?=[4d]}d}16
{?=[4d]}16@0:8
@48@0:8{?=[4d]}16
B48@0:8{?=[4d]}16
{?=[4d]}48@0:8{?=[4d]}16
{?=[4d]}88@0:8{?={?=[4d]}{?=[4d]}d}16
{?=dd}104@0:8{?={?=[4d]}{?=[4d]}d}16@88d96
{?={?=[4d]}{?=[4d]}d}48@0:8@16@24d32@40
@"NUBufferRenderClient"
@"NUImageDataClient"
@32@0:8@16^@24
B40@0:8@16@24^@32
@"NSArray"32@0:8@16^@24
B40@0:8@16@"NSMutableDictionary"24^@32
@"NSNumber"32@0:8@"NSDictionary"16^@24
B40@0:8@"NSNumber"16@"NSMutableDictionary"24^@32
{?=qq}16@0:8
v32@0:8{?=qq}16
@"NUImageExportRequest"
{?="width"q"height"q}
@"NUImageGeometry"
@"NUPriority"
@"NUColorSpace"
@"<NUScalePolicy>"
@"NUImageExportFormat"
v48@0:8@16@24@32@?40
v40@0:8@16@24@?32
@48@0:8@16@24@32@?40
@72@0:8@16@24@32@40@48@56@?64
@48@0:8@16@24@32^@40
v64@0:8@16@24@32@40@48@?56
{CGRect={CGPoint=dd}{CGSize=dd}}24@0:8@16
{CGRect={CGPoint=dd}{CGSize=dd}}56@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16@48
{CGRect={CGPoint=dd}{CGSize=dd}}64@0:8q16{CGRect={CGPoint=dd}{CGSize=dd}}24@56
@48@0:8@16@24@32@40
@48@0:8Q16Q24Q32@40
@40@0:8Q16Q24Q32
v64@0:8{?={?=qiIq}{?=qiIq}}16
@40@0:8@16I24I28o^@32
@40@0:8@16@24#32
@"PTTimedRenderingMetadata"
v76@0:8@16{?=qq}24{?=qq}40i56q60@?68
v68@0:8@16{?=qq}24{?=qq}40i56q60
@68@0:8@16{?=qq}24{?=qq}40i56q60
@"PTRenderPipeline"
@"<PTRenderState>"
@"<MTLDevice>"
@"NSDate"
v48@0:8q16^d24^d32^d40
@"PIFaceObservationCache"16@0:8
v24@0:8@"PIFaceObservationCache"16
v36@0:8@16@24B32
f24@0:8@16
B32@0:8{?=qq}16
B40@0:8@16{?=qq}24
@"PIFaceObservationCache"
B32@0:8{CGSize=dd}16
{?="exposure"d"contrast"d"brightness"d"shadows"d"highlights"d"black"d"rawHighlights"d"localLight"d}
@36@0:8@16@24B32
@52@0:8@16@24@32q40B48
@44@0:8@16{CGSize=dd}24B40
^{CGImage=}24@0:8@16
@40@0:8@16{CGSize=dd}24
@40@0:8@16Q24^{CGColorSpace=}32
@48@0:8{?=qq}16@32Q40
B40@0:8@16@24o^@32
@64@0:8@16@24{?={?=qq}{?=qq}}32
B60@0:8@16i24^{CGColorSpace=}28@36@44o^@52
@"<NURenderer>"
@"<NUSurfaceStorage>"
@"NSObject<OS_dispatch_semaphore>"
@"NSMutableArray"
@"NSError"
@"VNImageHomographicAlignmentObservation"16@0:8
@"VNImageHomographicAlignmentObservation"
@44@0:8@16@24@32B40
@60@0:8@16@24B32{CGSize=dd}36q52
@"PTCinematographyScript"
@"NSCache"
^{CGColorSpace=}16@0:8
{vector<float, std::allocator<float>>=^f^f{__compressed_pair<float *, std::allocator<float>>=^f}}24@0:8@16
@48@0:8r^f16r^f24r^f32r^f40
@40@0:8@16{?=qq}24
@"NUFaceDetectionRequest"
@68@0:8^v16Q24Q32q40Q48i56^{CGColorSpace=}60
@68@0:8@16Q24Q32q40Q48i56^{CGColorSpace=}60
^v16@0:8
^{CGColorSpace=}
@"NSMutableData"
v24@0:8^{?=Bffff[3f]}16
^f16@0:8
@24@0:8^f16
@32@0:8d16@24
@36@0:8@16B24#28
@20@0:8B16
@36@0:8d16d24B32
@104@0:8@16{?=[3]}24{CGRect={CGPoint=dd}{CGSize=dd}}72
@40@0:8@16@24d32
@"CIVector"
@24@0:8Q16
B24@0:8Q16
@32@0:8q16@24
B32@0:8^{?={?=qq}{?=qq}}16o^@24
B48@0:8^{CGRect={CGPoint=dd}{CGSize=dd}}16^{CGRect={CGPoint=dd}{CGSize=dd}}24@32o^@40
{CGRect={CGPoint=dd}{CGSize=dd}}96@0:8{?=qq}16{?=qq}32{?=qq}48{CGRect={CGPoint=dd}{CGSize=dd}}64
{?="p75"d"p98"d"autoValue"d"g98"d}
{?="sat"d"contrast"d"cast"d}
v36@0:8^f16f24f28f32
f24@0:8r^f16
d40@0:8d16d24d32
@64@0:8@16{?={?=qq}{?=qq}}24@56
@144@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24{CGRect={CGPoint=dd}{CGSize=dd}}56{?=[3]}88@136
@"PIReframeKeyframeSequence"
@"<NUVideoProperties>"
v32@0:8{CGVector=dd}16
v64@0:8{?=[3]}16
{CGVector=dd}16@0:8
{CGVector="dx"d"dy"d}
@24@0:8r^{FigLivePhotoMetadata=I{FigLivePhotoMetadataV1Struct=fqffffffccSI[0{FigLivePhotoDetectedFaceV1Struct=qffffisS}]}}16
{CGVector=dd}24@0:8r^{FigLivePhotoMetadata=I{FigLivePhotoMetadataV1Struct=fqffffffccSI[0{FigLivePhotoDetectedFaceV1Struct=qffffisS}]}}16
{?=[3]}32@0:8r^{FigLivePhotoMetadata=I{FigLivePhotoMetadataV1Struct=fqffffffccSI[0{FigLivePhotoDetectedFaceV1Struct=qffffisS}]}}16^B24
@"AVAssetTrack"
{CGAffineTransform="a"d"b"d"c"d"d"d"tx"d"ty"d}
B24@0:8d16
@48@0:8@16{CGPoint=dd}24d40
B32@0:8^{CGImage=}16@24
B40@0:8^{CGImage=}16@24@32
@40@0:8@16@24@32
{?={?=qq}{?=qq}}40@0:8{CGPoint=dd}16d32
v40@0:8r^f16^S24Q32
v40@0:8r^S16^f24Q32
@48@0:8f16f20f24f28Q32{?=ii}40
{?=ii}16@0:8
v24@0:8{?=ii}16
{?="major"i"minor"i}
@48@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16
@48@0:8@16@24q32@40
@52@0:8@16@24@32f40q44
@40@0:8@16q24@32
@64@0:8@16@24@32@40q48o^@56
@52@0:8@16@24@32B40o^@44
@44@0:8@16@24B32o^@36
@36@0:8@16B24@28
@64@0:8{?={?=qiIq}{?=qiIq}}16
@32@0:8d16d24
{CGPoint=dd}44@0:8i16d20d28@36
ffffff
?YAH
?ffffff
pCh?
@<,_
@+~h
?ffffff
?(D#$w
>333333
?UUUUUU
?______
?TTTTTT
?333333
zt?:
Ww'&l
MbP?{
333333
?333333
?ffffff
?ffffff
?333333
333333
ffffff
?333333
333333
@UUUUUU
Q@333333
?ffffff
z>UUUUUU
UUU?
]?~0d>
?YAH
?YAH
sU?gDi?
z?-C
Mb@?
@(#)PROGRAM:PhotoImaging  PROJECT:Neutrino-440.0.140
0Xr
?g|_\
ADBE
mntrRGB XYZ 
;acspAPPL
none
-ADBE
cprt
2desc
kwtpt
bkpt
rTRC
gTRC
bTRC
rXYZ
gXYZ
bXYZ
text
Copyright 2000 Adobe Systems Incorporated
desc
Adobe WGG linear
XYZ 
XYZ 
curv
XYZ 
XYZ 
iXYZ 
#?6t>=
333333
?333333
?ffffff
Fail: %{public}@
job: %{public}@
Trace:
%{public}@
Trace:
%{public}@
Tap-to-track: client requested stop at %@
Tap-to-track: tracking lost at %@
PIColorNormalizationAnalysis
Tried to set sceneLabel to unsupported value: %ld
Setting bounding boxes based on orientation (%@) and observations: %@
Setting face bounding boxes based on orientation (%@) and observations: %@
class %@ is not the correct type, its superclass should be %@
Continue: %{public}@
Unable to serialize finalized composition: %{public}@
Failed to deserialize global rendering metadata: %{public}@
Failed to deserialize timed rendering metadata: %{public}@
Missing timed rendering metadata
Error executing command buffer '%{public}@': %{public}@
CPV rendering failure, returned status %d
Could not apply PIPortraitVideoProcessor: %@
Failed to prewarm portrait renderer: %{public}@
Failed to obtain video properties: %{public}@
facerect yiq = %.5f, %.5f, %.5f
Choosing gray world instead of gray edge
Brightness is %@, greenChannelPercentage is %f, Sushi: %@
aperture=%@, shutterSpeed=%@, iso=%@
Buffer sizes are not the same: %{public}@ vs %{public}@
Buffer size is %@, bytes per row is %ld, dark_thr=%f, sat_thr=%f, noise_thr=%f
all done summing, np_gw is %d, np_ge is %d
wp_ge {%f, %f, %f, %f} wp_gw {%f, %f, %f, %f}, Sushi? %@
RGB = %f, %f, %f
YIQ = %f, %f, %f from %f, %f, %f
Failed to export image to %@: %@
Failed to export image to data: %@
Failed to prepare video metadata: %@
Unexpected Live Photo export format pairing. Video codec (%{public}@) and image export format (%{public}@)
Export Composition: exportImage: %{public}@ / exportVideoComplement: %{public}@ / exportVideo: %{public}@ / exportVideoPosterFrame: %{public}@
Export Composition: exporting image
Export Composition: exporting image complete
Export Composition: exporting video complement
Export Composition: exporting video complement complete
Export Composition: exporting video
Export Composition: exporting video complete
Export Composition: exporting video poster frame
Export Composition: exporting video poster frame complete
Export Composition: exporting overcapture image
Export Composition: exporting overcapture image complete
Export Composition: exporting overcapture video
Export Composition: exporting overcapture video complete
Export Composition: waiting on notify
Export Composition: notify triggered
Export Composition: calling completion with error: %{public}@
Export Composition: callling completion with result: %{public}@
failed to render auxiliary image data: %@
[PICompositionExporter shouldTryVideoRotationFastPath] failed. Error:%@
NUVideoRotationExportRequest failed. Error:%@
Skipping applyVideoOrientationAsMetadata. Bounces and Autoloops are not supported. Falling back to burned-in orientation.
Skipping applyVideoOrientationAsMetadata. Flipped orientations are not supported. Falling back to burned-in orientation.
Failed to export video: %@
invalid format version: %@
PIPortraitVideoMetadataSample: unexpected number of items for identifier %@: %@
PIPortraitVideoMetadataSample: item %@ is not of expected class %@
Error unarchiving cameraInfo metadata: %@
CINE: allocating new PT renderer: %@
CINE: recycling unused PT renderer: %@ (%lu remaining)
Requesting forced cleanup of Vision caches
CPV asset editability: onAppleSilicon: %s, hasMetalDeviceForPortrait: %s, oniOS: %s
CPV assets aren't editable because there is no metal device with support for portrait rendering
CPV assets aren't editable because the device has no ANE
CPV assets are editable
HDR asset not editable because this device doesn't support 10-bit HEVC encoding
HDR asset not editable because this device doesn't support HDR
HDR asset not editable because editing not supported on DolbyVision 5
CPV asset checking for editability
CPV asset not editable because it's in a legacy, unsupported format
CPV asset editable due to override
CPV asset not editable because CPV assets are not editable on this device
CPV asset not editable because this asset is not playable on this device
CPV asset not editable because this device doesn't support HDR
CPV asset is editable
validation issue: recipe is blank on the autoloop adjustment
validation issue: no flavor specified on the autoloop adjustment
validation issue: Missing inputCorrectionInfo on a redeye adjustment
validation issue: Missing depthInfo on a depth adjustment
validation issue: Missing portraitInfo on a portrait adjustment
validation issue: invalid trim startTime or endTime
Can PLPhotoEditPFDataConverter interpret identifier %{public}@? %@. Version %{public}@? %@
waitUntilReadyForMoreData: waited for %0.1fms
Writing long-exposure image to %{public}@
Writing long-exposure motion mask to %{public}@
Still image node:
Still image geometry:
Still image:
clap=%@, crop=%@, fullSize=%@, videoScale=%@ => guide rect: %@
High-resolution image registration failure : %@
Loading long-exposure fusion tuning parameters from file: %@
Failed to load fusion tuning parameters.
Using fusion tuning parameters: {kNCCBlurHalfSize=%d, kNCCEdge0=%f, kNCCEdge1=%f, kBlendMaskThreshold0=%f, kBlendMaskThreshold1=%f}
Missing inputImage
Missing inputMaskImage
Missing inputStillImage
Missing inputRenderScale
Missing inputVideoScale
Missing inputAlignmentExtent
Malformed inputAlignmentExtent vector
Missing inputAlignmentTransform
Malformed inputAlignmentTransform vector
Writing intermediate long exposure fusion images to : %@
Evaluating pipeline as HDR input
Beginning finalization for candidates: %@
No overcapture source found in composition, removing any candidate bits for reframing
Finalizer result contains roll angle degrees: %f pitch angle degrees: %f yaw angle degrees: %f
Finalizer result did not contain a change
Received error while finalizing: %@
Starting horizon auto calculator
Finished horizon auto calculator: %@
Starting perspective auto calculator
Finished perspective auto calculator: %@
Could not write PICropAutoCalculator debug diagnostics. %@
Failed to compute overcapture rect %@
Can't find enabled video track in asset: %@
Asset does not contain Live Photo metadata track
Track does not contain V3 metadata
Error creating asset reader: %@
Can't add metadata output for asset
Failed to start reading asset
Starting metadata extraction
Finished metadata extraction
PISmartToneAutoCalculator submitSynchronous isHDRComposition: %s
PISmartToneAutoCalculator smartTone request submitting: %{public}@
PISmartToneAutoCalculator smartTone result: %{public}@
PISmartToneAutoCalculator localLight request submitting: %{public}@
PISmartToneAutoCalculator localLight result: %{public}@
PISmartToneAutoCalculator submit isHDRComposition: %s
PISmartToneAutoCalculator global result: %{public}@
PISmartToneAutoCalculator going to commit and wait
PISmartToneAutoCalculator committed
PISmartToneAutoCalculator completed
PISmartToneAutoCalculator error: %{public}@
PISmartToneAutoCalculator coalesced
Failed to apply red eye repair. error: %{public}@
PIPortraitAutoCalculator getMinMaxSimulatedAperture returned error:%i (minAperture:%f maxAperture:%f version:%d)
Cant get focusRect - Metadata dictionary missing fullSizeWith or fullSizeHeight:
Cant get focusRect - Metadata dictionary missing exif aux dictionary:
Cant get focusRect - Exif aux dictionary missing MWG region dictionary:
Cant get focusRect - MWG region dictionary missing region list:
Cant get focusRect - Region list does not contain a focus rect:
Cant get focusRect - Malformed focus rect dictionary:
Invalid focus rect: {%g,%g,%g,%g}
Depth effects not supported: %@
Couldn't deserialize face observations from metadata: %@, assuming empty, error: %@.
Invalid face indexes from metadata: %@, ignored. Error: %@
Couldn't deserialize face observations from metadata: %@
s-curve black: %f
s-curve point %d: (%f, %f)
s-curve white: %f
red curve: blackPoint = %f, whitePoint = %f
green curve: blackPoint = %f, whitePoint = %f
blue curve: blackPoint = %f, whitePoint = %f
Invalid parameter not satisfying: %s
-[PITapToTrackRenderJob prepare:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Autocalculators/PITapToTrackRequest.m
No metal device for request
Failed to find output video asset
Failed to make asset reader for video
Failed to read first frame for video
Failed to find rect to track at initial point
Failed to add detection and start tracking
Failed to get finalized track from tracking session
kernel vec4 _hdrOffsetPosBW(sampler image) { vec4 im = sample(image, samplerCoord(image)); float offset = (im.r + im.g + im.b) / 3.0f; offset = max(0.0f, offset - 1.0f); return vec4(offset, offset, offset, 0.0f); }
kernel vec4 _hdrOffsetPos(sampler image) { vec4 im = sample(image, samplerCoord(image)); float r = max(im.r - 1.0f, 0.0f); float g = max(im.g - 1.0f, 0.0f); float b = max(im.b - 1.0f, 0.0f); return vec4(r, g, b, 0.0f); }
s_hdrOffsetPos kernel is nil
+[PIPhotoEffectHDR kernel]_block_invoke
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Adjustments/HDR/PIPhotoEffectHDR.m
inputImage cannot be nil
-[PIPhotoEffectHDR outputImage]
inputImage
CIAdditionCompositing
inputBackgroundImage
s_hdrOffsetPosBlackAndWhite kernel is nil
+[PIPhotoEffectBlackAndWhiteHDR kernelBlackAndWhite]_block_invoke
-[PIPhotoEffectBlackAndWhiteHDR outputImage]
+[PIPhotoEffect3DHDR kernel]_block_invoke
-[PIPhotoEffect3DHDR outputImage]
inputThreshold
inputDepthMap
+[PIPhotoEffect3DBlackAndWhiteHDR kernelBlackAndWhite]_block_invoke
-[PIPhotoEffect3DBlackAndWhiteHDR outputImage]
PhotoImaging
PIColorNormalizationFilter
PITempTintFilter
temperature
inputTemperature
tint
inputTint
PIHighKey
CISmartToneFilter
CISmartColorFilter
normalization != nil
+[PIColorNormalizationFilter colorCubeForNormalization:dimension:targetColorSpace:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Adjustments/PIColorNormalizationFilter.m
colorSpace != NULL
PIColorNormalization
candidateForStill
candidateForVideo
candidateForReframe
candidateForPerspective
candidateForHorizon
addRule exception : %@
v24@?0@"NSString"8@?<v@?@"NURuleSystem">16
v24@?0@"NSString"8@"NSString"16
isStitched
validSubjects
hasHorizonLine
hasBuilding
state.stitched == true OR state.perfectlyStitched == true
state.stitchConfidence < $StitchConfidenceThreshold
state.buildingCount > 0
state.buildingConfidence > $BuildingConfidenceThreshold
state.subjectCount >= $SubjectCountMinThreshold
state.allSubjectsInsidePrimaryBounds == true
(state.subjectCount - state.intersectingSubjectCount) > $MaxDisparateSubjects
state.largeSubjectFace == true
state.largestSubjectArea <= $MinimumSubjectSize
state.horizonLinePresent == true
state.horizonLineAngleInDegrees < -$HorizonAngleInDegreesMaxThreshold OR state.horizonLineAngleInDegrees > $HorizonAngleInDegreesMaxThreshold
state.horizonLineAngleInDegrees > -$HorizonAngleInDegreesMinThreshold AND state.horizonLineAngleInDegrees < $HorizonAngleInDegreesMinThreshold
state.video == true
state.videoDuration < $VideoDurationLowerBound
state.videoDuration > $VideoDurationUpperBound
(state.videoDuration * $VideoDurationWithSubjectsRatio) > state.videoDurationWithSubjects
state.videoQualityScore < $VideoQualityScoreThreshold
(state.videoDuration * $VideoDurationBelowMinZoomRatio) < state.videoDurationBelowMinZoom
facts.isStitched == true AND facts.validSubjects == true
facts.isStitched == true AND facts.hasBuilding == true
facts.isStitched == true AND facts.hasHorizonLine == true
state.largeSubject == true
v16@?0@"NURuleSystem"8
state.backFacing == false
state.deviceIsStationary == true
state.video == true AND state.livePhoto == true
state.pano == true
state.reframingAllowed == false
state.hasAdjustments == true
facts.candidateForStill == true
facts.candidateForVideo == true
platedFood
sunriseSunset
genericLandscape
intensity
sceneLabel
sceneConfidence
faceBoundingBoxes
boundingBoxes
-[PIDisparitySampleJob render:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Autocalculators/PIDisparitySampleRequest.m
Failed to read video frame
Incompatible disparity buffer
inputStrength
inputScale
vec3 localContrast(vec3 im, vec3 shc, float amt) { float midAmt = amt; vec3 neg = min(im, 0.0); vec3 pos = max(im, 1.0)-1.0; im = clamp(im, 0.0, 1.0); float y = dot(im, vec3(0.3333)); y = sqrt(y); y = y*(1.0-y); im = sqrt(im); float pivot = sqrt(shc.g); float a = midAmt*y; float b = -pivot*a; vec3 pix = im.r * vec3(0.299*a) + im.g * vec3(0.587*a) + im.b * vec3(0.114*a) + im + vec3(b); im = mix(im, vec3(pivot), -y*midAmt); im = mix(im, pix, 0.8); im = max(im, 0.0); im *= im; im = im + neg + pos; return im; } vec3 localContrastHLG(vec3 im, vec3 shc, float hlg_scale, float amt) { return localContrast(im.rgb/hlg_scale, shc.rgb/hlg_scale, amt).rgb * hlg_scale; } kernel vec4 _localContrastHDR(__sample im, __sample shc, float hlg_scale, float amt) { float max_comp = max(im.r,max(im.g,im.b)); float threshold = 0.75 * hlg_scale; if (max_comp <= 1.0) { im.rgb = localContrast(im.rgb, shc.rgb, amt); } else if (max_comp < threshold) { vec3 retSDR = localContrast(im.rgb, shc.rgb, amt); vec3 retHDR = localContrastHLG(im.rgb, shc.rgb, hlg_scale, amt); float lerp_t = (max_comp - 1.0) / (threshold - 1.0); im.rgb = mix(retSDR, retHDR, lerp_t); } else { im.rgb = localContrastHLG(im.rgb, shc.rgb, hlg_scale, amt); } return im; }
CIColorMatrix
inputRVector
inputGVector
inputBVector
inputAVector
inputBiasVector
CIColorClamp
inputMinComponents
inputMaxComponents
CIEdgePreserveUpsampleFilter
inputSmallImage
inputSpatialSigma
inputLumaSigma
kernel vec4 highKey(__sample im, float str)
vec3 neg = min(im.rgb, 0.0);
vec3 pos = max(im.rgb, 1.0) - 1.0;
im = clamp(im, 0.0, 1.0);
vec4 im2 = 1.0-((im-1.0)*(im-1.0));
im2 = sqrt(im2);
float y = dot(im.rgb, vec3(.333333));
float y2 = sqrt(1.0-(y-1.0)*(y-1.0));
y2 = mix(y2, smoothstep(0.0, 1.0, y2), 0.5);
vec4 im3 = (y>0) ? im*y2/y : vec4(0.0, 0.0, 0.0, 1.0);
im3 = mix(im3, im2, .7*sqrt(y2));
im3 = mix(im, im3, sqrt(y));
im.rgb = mix(im.rgb, im3.rgb, str) + pos + neg;
return im;
high key kernel is nil
+[PIHighKey kernel]_block_invoke
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Adjustments/PIHighKey.m
-[PIHighKey outputImage]
inputStrength cannot be nil
smartTone
smartColor
smartBlackAndWhite
grain
autoEnhance
whiteBalance
cropStraighten
redEye
apertureRedEye
depthEffect
livePhotoKeyFrame
videoPosterFrame
trim
slomo
portraitEffect
orientation
autoLoop
highResFusion
retouch
vignette
sharpen
noiseReduction
definition
curves
levels
selectiveColor
effect
effect3D
mute
rawNoiseReduction
source
videoReframe
debug
sourceSelect
sourceSpatialOvercapture
portraitVideo
videoStabilize
videoCrossfadeLoop
semanticEnhance
enabled
value
primary
PICompositionController(0x%X): %@
v16@?0@"PISmartToneAdjustmentController"8
com.apple.photo
adjustmentFormatIdentifier
adjustmentFormatVersion
adjustmentData
PICompositionSerializer.m
Invalid parameter not satisfying: %@
adjustments
metadata
file://dummy.jpg
dummy
identifier
PICompositionSerializerDomain
Missing identifierName
Missing definition in conversion map for the adjustment key: 
settings
Unsupported adjustment: 
Configuration does not support Aperature adjustments.
Configuration does not support CTM adjustments.
nil identifierName
inputKeys
omitIfDisabled
Orientation
PICompositionSerializer-geometry
masterWidth
masterHeight
Serialization map has no entry for %@
+[PICompositionSerializer serializeComposition:versionInfo:serializerMetadata:error:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Controllers/Conversion/PICompositionSerializer.m
Serialization map does not contain identifierName
current
auto
formatIdentifier
formatVersion
versionInfo
Missing required key: %@
Value for key %@ has type %@; expected type %@
v32@?0@"NSString"8#16^B24
PICompositionSerializer
exception
dictionary
data is not an NSData: %@: %@
Adjustment for %@ is identity
+[PICompositionSerializer _sanitizeComposition:]
composition
CFBundleVersion
platform
buildNumber
appVersion
schemaRevision
kernel vec4 levelsHDR (sampler src, sampler LUT) { vec4 p,r; vec2 c1,c2; float f; p = sample(src, samplerCoord(src)); vec3 neg = min(p.rgb, 0.0); vec3 pos = max(p.rgb, 1.0)-1.0; p.rgb = clamp(p.rgb, 0.0, 1.0); f = p.r * 1024; c1 = vec2(floor(f)+0.5, 0.5); c2 = vec2(ceil(f)+0.5, 0.5); r = mix(sample(LUT,samplerTransform(LUT,c1)), sample(LUT,samplerTransform(LUT,c2)), fract(f)); p.r = r.r; f = p.g * 1024; c1 = vec2(floor(f)+0.5, 0.5); c2 = vec2(ceil(f)+0.5, 0.5); r = mix(sample(LUT,samplerTransform(LUT,c1)), sample(LUT,samplerTransform(LUT,c2)), fract(f)); p.g = r.g; f = p.b * 1024; c1 = vec2(floor(f)+0.5, 0.5); c2 = vec2(ceil(f)+0.5, 0.5); r = mix(sample(LUT,samplerTransform(LUT,c1)), sample(LUT,samplerTransform(LUT,c2)), fract(f)); p.b = r.b; p.rgb = max(p.rgb, 0.0); p.rgb = p.rgb + pos + neg; return p; }
inputBlackSrc
inputBlackDst
inputShadowSrc
inputShadowDst
inputMidSrc
inputMidDst
inputHilightSrc
inputHilightDst
inputWhiteSrc
inputWhiteDst
inputBlackSrcRGB
inputBlackDstRGB
inputShadowSrcRGB
inputShadowDstRGB
inputMidSrcRGB
inputMidDstRGB
inputHilightSrcRGB
inputHilightDstRGB
inputWhiteSrcRGB
inputWhiteDstRGB
inputBlackSrcRed
inputBlackDstRed
inputShadowSrcRed
inputShadowDstRed
inputMidSrcRed
inputMidDstRed
inputHilightSrcRed
inputHilightDstRed
inputWhiteSrcRed
inputWhiteDstRed
inputBlackSrcGreen
inputBlackDstGreen
inputShadowSrcGreen
inputShadowDstGreen
inputMidSrcGreen
inputMidDstGreen
inputHilightSrcGreen
inputHilightDstGreen
inputWhiteSrcGreen
inputWhiteDstGreen
inputBlackSrcBlue
inputBlackDstBlue
inputShadowSrcBlue
inputShadowDstBlue
inputMidSrcBlue
inputMidDstBlue
inputHilightSrcBlue
inputHilightDstBlue
inputWhiteSrcBlue
inputWhiteDstBlue
Green
Blue
Failed converting data to RGBAh: %ld
-[PILevelsFilterHDR _LUTImage]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Adjustments/HDR/PILevelsFilterHDR.m
version
score
/Master/Source
data != nil
+[PIColorNormalizationAutoCalculator autoCalculatorWithImageData:orientation:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Autocalculators/PIColorNormalizationAutoCalculator.m
-[PIColorNormalizationAutoCalculator submit:]
Feature Unavailable
No color normalization available
kernel vec4 _falseColorHDRDebug(sampler image, float cutoff){   vec4 im = sample(image, samplerCoord(image));   if (im.x < 0.0 && im.y < 0.0 && im.z < 0.0) {        return vec4(0.0 , 0.6 , 0.2 , 1.0);   }   if (im.x > cutoff && im.y > cutoff && im.z > cutoff) {        return vec4(1.0 , 0.0 , 1.0 , 1.0);   }   if (im.x > cutoff || im.y > cutoff || im.z > cutoff) {        return vec4(0.6 , 0.0 , 0.6 , 1.0);   }   return im;}
s_falseColorHDRDebugKernelSource kernel is nil
+[PIFalseColorHDRDebug kernel]_block_invoke
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Adjustments/HDR/PIFalseColorHDRDebug.m
-[PIFalseColorHDRDebug outputImage]
Initializer not available: -[%@ %@], use designated initializer instead.
-[PIAutoLoopExportRequest initWithRequest:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/AutoLoop/PIAutoLoopRequests.m
-[PIAutoLoopExportRequest initWithComposition:destinationURL:]
unexpected inputs
+[PIPortraitVideoProcessor processWithInputs:arguments:output:error:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Render/PIPortraitVideoFilter.m
expected a device
expected a texture
aperture
focusDistance
quality
isHDR
globalMetadata
Failed to deserialize global rendering metadata: %{public}@
timedMetadata
Failed to deserialize timed rendering metadata: %{public}@
Missing timed rendering metadata
v16@?0@"<MTLCommandBuffer>"8
Invalid index
+[PIPortraitVideoProcessor formatForInputAtIndex:]
imageExtents
+[PIPortraitVideoProcessor applyWithInputImage:disparityImage:globalMetadata:timedMetadata:aperture:focusedDisparity:quality:debugMode:isHDR:error:]
disparityImage != nil
NormStabilizeInstructions
stabCropRect
Width
Height
flavor
recipe
-[PIVideoCrossfadeLoopNode initWithSettings:inputs:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Video Stabilization/PIVideoCrossfadeLoopNode.m
input != nil
-[PIVideoCrossfadeLoopNode initWithInput:timeRange:crossfadeDuration:startTime:]
CMTIME_IS_VALID(crossfadeDuration)
CMTIMERANGE_IS_VALID(loopTimeRange)
crossfadeDuration
startTime
loopTimeRange
-[PIVideoCrossfadeLoopNode nodeByReplayingAgainstCache:pipelineState:error:]
Missing primary video frame
video
secondary
Missing secondary video frame
CIDissolveTransition
-[PIVideoCrossfadeLoopNode _evaluateVideo:]
[[AVMutableComposition alloc] init] failed.
No input video track found
Failed to update video track when inserting the pre-crossfade content with source range %@ from track %@ to time %@ in track %@.
Failed to update audio track when inserting the pre-crossfade content with source range %@ from track %@ to time %@ in track %@.
Failed to update video track when inserting the crossfade content with source range %@ from track %@ to time %@ in track %@.
Failed to update audio track when inserting the crossfade content with source range %@ from track %@ to time %@ in track %@.
Failed to update video track when inserting the post-crossfade content with source range %@ from track %@ to time %@ in track %@.
Failed to update audio track when inserting the post-crossfade content with source range %@ from track %@ to time %@ in track %@.
-[PIVideoCrossfadeLoopNode _evaluateVideoComposition:]
Unsupported video configuration
-[PIVideoCrossfadeLoopNode _evaluateAudioMix:]
time
scale
radius
falloff
Adjustment empty
-[PIAdjustmentController displayName]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Controllers/PIAdjustmentController.m
-[PIAdjustmentController inputKeys]
Adjustment does not have an enabled setting
-[PIAdjustmentController enabled]
-[PIAdjustmentController setEnabled:]
Adjustment does not have an auto setting
-[PIAdjustmentController isAuto]
-[PIAdjustmentController setIsAuto:]
PIAdjustmentController(0x%X): %@
__dominantInputSettingsKey
debugMode
disparityKeyframes
apertureKeyframes
NUScaleIsValid(scale)
-[PIPortraitVideoRenderNode _targetScaleForScale:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Render/PIPortraitVideoRenderNode.m
-[PIPortraitVideoRenderNode nodeByReplayingAgainstCache:pipelineState:error:]
state != nil
-[PIPortraitVideoRenderNode _prewarmPortraitRendererWithPipelineState:error:]
No device specified for prewarm
portraitVideoMetadata
mdta/com.apple.quicktime.disparity-float
PIPortraitVideoRenderNode: expected a non-nil portraitVideoMetadata sample
renderTime
renderQuality
sourceTransferFunction
Could not get the input image
Could not get the disparityImage
Could not get the timed metadata
Could not get the source transfer function
Could not get the render time
PIFaceObservationCache
PIFaceObservationCache-newRequest
spatialOvercapture
spatialOvercaptureFused
timeValue
timeScale
%@: t=%@, v=%f
<%@ %p %@ %@ id=%lu conf=%.2f>
  bounds=%@
  expandedBounds=%@
  edgeBleed=%@
type
confidence
edgeBleed
bounds
expandedBounds
Human Face
Human Body
Cat Body
Cat Head
Dog Body
Dog Head
Salient Object
Unknown
(%.4f, %.4f, %.4f, %.4f)
minX 
minY 
maxX 
maxY 
cinematographyState
homography
<%@:%p time:%@>
PILongExposureFusionAutoCalculator-videoProperties
pre-AutoLoop
-[PILongExposureFusionAutoCalculator submit:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Autocalculators/PILongExposureFusionAutoCalculator.m
AutoLoop not supported
kind
-[PIAutoLoopAutoCalculator submit:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Autocalculators/PIAutoLoopAutoCalculator.m
B32@?0@"AVMetadataItem"8Q16^B24
-[PITempTintFilter outputImage]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Adjustments/PITempTintFilter.m
-[PIAutoLoopAnalysisJob prepare:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/AutoLoop/PIAutoLoopJob+Analysis.m
unable to find video source node
-[PIAutoLoopAnalysisJob render:]
AutoLoop is not supported
-[PIVideoStabilizeRenderJob prepare:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Autocalculators/PIVideoStabilizeRequest.m
Stabilize request was cancelled
frameInstructions
rawTime
void CreateKeyframesFromHomographyDictionary(NSDictionary *__strong, NUPixelSize, NSMutableArray<PIReframeKeyframe *> *__strong, NUPixelRect *)
PIVideoStabilizeRequest.m
unexpected homography
Failure in ICSynthesizeAnalysis
Failure in ICAnalyzeInputMotion
No available analysis types were allowed
Failure in ICCalcCinematicL1Corrections
neutralGray
faceBalance
tempTint
warmTint
PIFaceBalanceAutoCalculator-imageProperties
+[PIFaceBalanceAutoCalculator calculateWithRequest:completion:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Autocalculators/PIWhiteBalanceAutoCalculators.m
CIFaceBalance
PIFaceBalanceAutoCalculator-calculate
WarmTemp
WarmTint
warmTemp
+[PIFaceBalanceAutoCalculator calculateRAWWithRequest:completion:]
PIFaceBalanceAutoCalculator-RAW-face-request
+[PIFaceBalanceAutoCalculator faceBalanceResultFromFaceObservations:request:error:]
PIWhiteBalanceAutoCalculator-face-balance
PIRAWFaceBalanceAutoCalculator.responseQueue
Failure in rendering image
OrigI
OrigQ
Warmth
Strength
WarmFace
unsupported format - incorrect white balance
+[PIFaceBalanceAutoCalculator faceBalanceFromFaceImage:forFaceRect:]
{?={?=[4d]}{?=[4d]}d}
{?=[4d]}
colorType
grayColor
-[PIWhiteBalanceAutoCalculator submit:]
PIWhiteBalanceAutoCalculator-imageProperties
PIFaceBalanceAutoCalculator.responseQueue
faceI
faceQ
faceWarmth
faceStrength
PIWhiteBalanceAutoCalculator
color
v16@?0@"<NUBuffer>"8
CIAreaAverage
PIWhiteColorCalculator-computeGreenPercentage
PIWhiteColorCalculator-grayWorld
PIWhiteColorCalculator-grayEdges
return Filter('CIEdges', { 'inputImage' : input, 'inputIntensity' : 1.0 });
Expected non-NULL pixels passed in
void customRGBtoYIQ(const CGFloat *, double *, double, double, double)
vec4 meaningBlur(vec4 im, vec4 b)
vec4 result = im;
float thresh = 0.1;
float g1 = max(max(im.r, im.g), im.b);
float g2 = dot(b.rgb, vec3(1.0/3.0));
float diff = max(g2-g1, -1.0);
diff = smoothstep(0.1-thresh, 0.1+thresh, diff);
result.rgb = mix(im.rgb, b.rgb, diff+0.5);
return result;
vec4 clarityNew(vec4 s, vec4 b, float intensity)
float sl = (s.r + s.g + s.b);
float bl = (b.r + b.g + b.b);
float dl = sl + (sl - bl) * intensity;
float mult = dl / max(sl, 0.0001);
mult = 1.571 * (mult - 1.0);
mult = mult / (1.0 + abs(mult));
mult += 1.0;
mult = clamp(mult, 1.0 - 0.5 * abs(intensity), 1.0 + 1.0 * abs(intensity));
s.rgb = s.rgb * mult;
return s;
kernel vec4 definition(sampler image, sampler blur, float intensity)
vec4 imgSample = sample(image, samplerCoord(image));
vec4 blurSample = sample(blur, samplerCoord(blur));
vec4 meaning = meaningBlur(imgSample, blurSample);
vec4 clarity = clarityNew(imgSample, meaning, intensity);
return clarity;
strength
neutral
tone
inputNeutralGamma
inputTone
inputGrain
inputHue
inputSeed
<%@:%p - priority: %@, color space: %@, scale policy: %@>
<%@:%p - priority: %@, color space: %@, scale policy: %@, video codec type: %@, bypass settings: %@, orientation as metadata: %@, hardware encoder: %@>
Export URL UTI (%@) does not match expected export format (%@)
-[PICompositionExporterImageOptions imageExportFormatForURL:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Util/PICompositionExporter.m
metadataConverter must be set
-[PICompositionExporter init]
PICompositionExporter-videoProperties
Expecting HEIF/HEVC or JPEG/H264 when exporting Live Photo still and video complement
Unexpected video codec when attempting to export Live Photo
Unexpected image export format when attempting to export Live Photo
_companion
unable to prepare image properties
PICompositionExporter-image
PICompositionExporter-auxPropertiesRequest
PICompositionExporter-%@
PICompositionExporter.imageProperties.transaction
Regions
-[PICompositionExporter addVideoProperties:composition:options:error:]
Unknown autoloop flavor
reference
PICompositionExporter-shouldTryVideoRotationFastPath-geometry
pitch
roll
xOrigin
yOrigin
width
height
PICompositionExporter-video
PICompositionExporter-noOrientation-videoProperties
float luminanceWeight(vec4 pix, vec4 center, float slope)
return max(1.0 + (slope * length(pix.rgb - center.rgb)), 0.0);
vec4 bilateralRow_1(sampler src, float slope, vec2 offset1, float weight1)
float diff1;
vec2 coord;
vec4 pix1, center;
coord = destCoord();
center = sample(src, samplerCoord(src));
pix1 = sample(src, samplerTransform(src, coord + offset1));
diff1 = luminanceWeight(pix1, center, slope) * weight1;
return vec4(diff1 * pix1.rgb, diff1);
kernel vec4 bilateralAdd_1(sampler src, sampler sums, float slope, vec2 offset1, float weight1, float sumWeight)
vec4 sum;
sum = sample(sums, samplerCoord(sums))*sumWeight;
return sum + bilateralRow_1(src, slope, offset1, weight1);
vec4 bilateralRow_2(sampler src, float slope, vec2 offset1, vec2 offset2, vec2 weight1)
float diff1, diff2;
vec2 coord;
vec4 pix1, pix2, center;
coord = destCoord();
center = sample(src, samplerCoord(src));
pix1 = sample(src, samplerTransform(src, coord + offset1));
pix2 = sample(src, samplerTransform(src, coord + offset2));
diff1 = luminanceWeight(pix1, center, slope) * weight1.x;
diff2 = luminanceWeight(pix2, center, slope) * weight1.y;
return vec4(diff1 * pix1.rgb + diff2 * pix2.rgb, diff1 + diff2);
kernel vec4 bilateralAdd_2(sampler src, sampler sums, float slope, vec2 offset1, vec2 offset2, vec2 weight1, float sumWeight)
vec4 sum;
sum = sample(sums, samplerCoord(sums))*sumWeight;
return sum + bilateralRow_2(src, slope, offset1, offset2, weight1);
vec4 bilateralRow_3(sampler src, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec3 weight1)
float diff1, diff2, diff3;
vec2 coord;
vec4 pix1, pix2, pix3, center;
coord = destCoord();
center = sample(src, samplerCoord(src));
pix1 = sample(src, samplerTransform(src, coord + offset1));
pix2 = sample(src, samplerTransform(src, coord + offset2));
pix3 = sample(src, samplerTransform(src, coord + offset3));
diff1 = luminanceWeight(pix1, center, slope) * weight1.x;
diff2 = luminanceWeight(pix2, center, slope) * weight1.y;
diff3 = luminanceWeight(pix3, center, slope) * weight1.z;
return vec4(diff1 * pix1.rgb + diff2 * pix2.rgb + diff3 * pix3.rgb, diff1 + diff2 + diff3);
kernel vec4 bilateralAdd_3(sampler src, sampler sums, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec3 weight1, float sumWeight)
vec4 sum;
sum = sample(sums, samplerCoord(sums))*sumWeight;
return sum + bilateralRow_3(src, slope, offset1, offset2, offset3, weight1);
vec4 bilateralRow_4(sampler src, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec2 offset4, vec4 weight1)
float diff1, diff2, diff3, diff4;
vec2 coord;
vec4 pix1, pix2, pix3, pix4, center;
coord = destCoord();
center = sample(src, samplerCoord(src));
pix1 = sample(src, samplerTransform(src, coord + offset1));
pix2 = sample(src, samplerTransform(src, coord + offset2));
pix3 = sample(src, samplerTransform(src, coord + offset3));
pix4 = sample(src, samplerTransform(src, coord + offset4));
diff1 = luminanceWeight(pix1, center, slope) * weight1.x;
diff2 = luminanceWeight(pix2, center, slope) * weight1.y;
diff3 = luminanceWeight(pix3, center, slope) * weight1.z;
diff4 = luminanceWeight(pix4, center, slope) * weight1.w;
return vec4(diff1 * pix1.rgb + diff2 * pix2.rgb + diff3 * pix3.rgb + diff4 * pix4.rgb,
diff1 + diff2 + diff3 + diff4);
kernel vec4 bilateralAdd_4(sampler src, sampler sums, float slope, vec2 offset1, vec2 offset2, vec2 offset3,
vec2 offset4, vec4 weight1, float sumWeight)
vec4 sum;
sum = sample(sums, samplerCoord(sums))*sumWeight;
return sum + bilateralRow_4(src, slope, offset1, offset2, offset3, offset4, weight1);
vec4 bilateralRow_5(sampler src, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec2 offset4, vec2 offset5,
vec4 weight1, float weight2)
float diff1, diff2, diff3, diff4, diff5;
vec2 coord;
vec4 pix1, pix2, pix3, pix4, pix5, center;
coord = destCoord();
center = sample(src, samplerCoord(src));
pix1 = sample(src, samplerTransform(src, coord + offset1));
pix2 = sample(src, samplerTransform(src, coord + offset2));
pix3 = sample(src, samplerTransform(src, coord + offset3));
pix4 = sample(src, samplerTransform(src, coord + offset4));
pix5 = sample(src, samplerTransform(src, coord + offset5));
diff1 = luminanceWeight(pix1, center, slope) * weight1.x;
diff2 = luminanceWeight(pix2, center, slope) * weight1.y;
diff3 = luminanceWeight(pix3, center, slope) * weight1.z;
diff4 = luminanceWeight(pix4, center, slope) * weight1.w;
diff5 = luminanceWeight(pix5, center, slope) * weight2;
return vec4(diff1 * pix1.rgb + diff2 * pix2.rgb + diff3 * pix3.rgb + diff4 * pix4.rgb + diff5 * pix5.rgb,
diff1 + diff2 + diff3 + diff4 + diff5);
kernel vec4 bilateralAdd_5(sampler src, sampler sums, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec2 offset4,
vec2 offset5, vec4 weight1, float weight2, float sumWeight)
vec4 sum;
sum = sample(sums, samplerCoord(sums))*sumWeight;
return sum + bilateralRow_5(src, slope, offset1, offset2, offset3, offset4, offset5, weight1, weight2);
vec4 bilateralRow_6(sampler src, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec2 offset4, vec2 offset5,
vec2 offset6, vec4 weight1, vec2 weight2)
float diff1, diff2, diff3, diff4, diff5, diff6;
vec2 coord;
vec4 pix1, pix2, pix3, pix4, pix5, pix6, center;
coord = destCoord();
center = sample(src, samplerCoord(src));
pix1 = sample(src, samplerTransform(src, coord + offset1));
pix2 = sample(src, samplerTransform(src, coord + offset2));
pix3 = sample(src, samplerTransform(src, coord + offset3));
pix4 = sample(src, samplerTransform(src, coord + offset4));
pix5 = sample(src, samplerTransform(src, coord + offset5));
pix6 = sample(src, samplerTransform(src, coord + offset6));
diff1 = luminanceWeight(pix1, center, slope) * weight1.x;
diff2 = luminanceWeight(pix2, center, slope) * weight1.y;
diff3 = luminanceWeight(pix3, center, slope) * weight1.z;
diff4 = luminanceWeight(pix4, center, slope) * weight1.w;
diff5 = luminanceWeight(pix5, center, slope) * weight2.x;
diff6 = luminanceWeight(pix6, center, slope) * weight2.y;
return vec4(diff1 * pix1.rgb + diff2 * pix2.rgb + diff3 * pix3.rgb + diff4 * pix4.rgb + diff5 * pix5.rgb + diff6 * pix6.rgb,
diff1 + diff2 + diff3 + diff4 + diff5 + diff6);
kernel vec4 bilateralAdd_6(sampler src, sampler sums, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec2 offset4,
vec2 offset5, vec2 offset6, vec4 weight1, vec2 weight2, float sumWeight)
vec4 sum;
sum = sample(sums, samplerCoord(sums))*sumWeight;
return sum + bilateralRow_6(src, slope, offset1, offset2, offset3, offset4, offset5, offset6, weight1, weight2);
vec4 bilateralRow_7(sampler src, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec2 offset4, vec2 offset5,
vec2 offset6, vec2 offset7, vec4 weight1, vec3 weight2)
float diff1, diff2, diff3, diff4, diff5, diff6, diff7;
vec2 coord;
vec4 pix1, pix2, pix3, pix4, pix5, pix6, pix7, center;
coord = destCoord();
center = sample(src, samplerCoord(src));
pix1 = sample(src, samplerTransform(src, coord + offset1));
pix2 = sample(src, samplerTransform(src, coord + offset2));
pix3 = sample(src, samplerTransform(src, coord + offset3));
pix4 = sample(src, samplerTransform(src, coord + offset4));
pix5 = sample(src, samplerTransform(src, coord + offset5));
pix6 = sample(src, samplerTransform(src, coord + offset6));
pix7 = sample(src, samplerTransform(src, coord + offset7));
diff1 = luminanceWeight(pix1, center, slope) * weight1.x;
diff2 = luminanceWeight(pix2, center, slope) * weight1.y;
diff3 = luminanceWeight(pix3, center, slope) * weight1.z;
diff4 = luminanceWeight(pix4, center, slope) * weight1.w;
diff5 = luminanceWeight(pix5, center, slope) * weight2.x;
diff6 = luminanceWeight(pix6, center, slope) * weight2.y;
diff7 = luminanceWeight(pix7, center, slope) * weight2.z;
return vec4(diff1 * pix1.rgb + diff2 * pix2.rgb + diff3 * pix3.rgb + diff4 * pix4.rgb + diff5 * pix5.rgb + diff6 * pix6.rgb + diff7 * pix7.rgb,
diff1 + diff2 + diff3 + diff4 + diff5 + diff6 + diff7);
kernel vec4 bilateralAdd_7(sampler src, sampler sums, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec2 offset4,
vec2 offset5, vec2 offset6, vec2 offset7, vec4 weight1, vec3 weight2, float sumWeight)
vec4 sum;
sum = sample(sums, samplerCoord(sums))*sumWeight;
return sum + bilateralRow_7(src, slope, offset1, offset2, offset3, offset4, offset5, offset6, offset7, weight1, weight2);
vec4 bilateralRow_8(sampler src, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec2 offset4, vec2 offset5, vec2 offset6,
vec2 offset7, vec2 offset8, vec4 weight1, vec4 weight2)
float diff1, diff2, diff3, diff4, diff5, diff6, diff7, diff8;
vec2 coord;
vec4 pix1, pix2, pix3, pix4, pix5, pix6, pix7, pix8, center;
coord = destCoord();
center = sample(src, samplerCoord(src));
pix1 = sample(src, samplerTransform(src, coord + offset1));
pix2 = sample(src, samplerTransform(src, coord + offset2));
pix3 = sample(src, samplerTransform(src, coord + offset3));
pix4 = sample(src, samplerTransform(src, coord + offset4));
pix5 = sample(src, samplerTransform(src, coord + offset5));
pix6 = sample(src, samplerTransform(src, coord + offset6));
pix7 = sample(src, samplerTransform(src, coord + offset7));
pix8 = sample(src, samplerTransform(src, coord + offset8));
diff1 = luminanceWeight(pix1, center, slope) * weight1.x;
diff2 = luminanceWeight(pix2, center, slope) * weight1.y;
diff3 = luminanceWeight(pix3, center, slope) * weight1.z;
diff4 = luminanceWeight(pix4, center, slope) * weight1.w;
diff5 = luminanceWeight(pix5, center, slope) * weight2.x;
diff6 = luminanceWeight(pix6, center, slope) * weight2.y;
diff7 = luminanceWeight(pix7, center, slope) * weight2.z;
diff8 = luminanceWeight(pix8, center, slope) * weight2.w;
return vec4(diff1 * pix1.rgb + diff2 * pix2.rgb + diff3 * pix3.rgb + diff4 * pix4.rgb + diff5 * pix5.rgb + diff6 * pix6.rgb + diff7 * pix7.rgb + diff8 * pix8.rgb,
diff1 + diff2 + diff3 + diff4 + diff5 + diff6 + diff7 + diff8);
kernel vec4 bilateralAdd_8(sampler src, sampler sums, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec2 offset4, vec2 offset5,
vec2 offset6, vec2 offset7, vec2 offset8, vec4 weight1, vec4 weight2, float sumWeight)
vec4 sum;
sum = sample(sums, samplerCoord(sums))*sumWeight;
return sum + bilateralRow_8(src, slope, offset1, offset2, offset3, offset4, offset5, offset6, offset7, offset8, weight1, weight2);
vec4 bilateralRow_9(sampler src, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec2 offset4, vec2 offset5, vec2 offset6, vec2 offset7,
vec2 offset8, vec2 offset9, vec4 weight1, vec4 weight2, float weight3)
float diff1, diff2, diff3, diff4, diff5, diff6, diff7, diff8, diff9;
vec2 coord;
vec4 pix1, pix2, pix3, pix4, pix5, pix6, pix7, pix8, pix9, center;
coord = destCoord();
center = sample(src, samplerCoord(src));
pix1 = sample(src, samplerTransform(src, coord + offset1));
pix2 = sample(src, samplerTransform(src, coord + offset2));
pix3 = sample(src, samplerTransform(src, coord + offset3));
pix4 = sample(src, samplerTransform(src, coord + offset4));
pix5 = sample(src, samplerTransform(src, coord + offset5));
pix6 = sample(src, samplerTransform(src, coord + offset6));
pix7 = sample(src, samplerTransform(src, coord + offset7));
pix8 = sample(src, samplerTransform(src, coord + offset8));
pix9 = sample(src, samplerTransform(src, coord + offset9));
diff1 = luminanceWeight(pix1, center, slope) * weight1.x;
diff2 = luminanceWeight(pix2, center, slope) * weight1.y;
diff3 = luminanceWeight(pix3, center, slope) * weight1.z;
diff4 = luminanceWeight(pix4, center, slope) * weight1.w;
diff5 = luminanceWeight(pix5, center, slope) * weight2.x;
diff6 = luminanceWeight(pix6, center, slope) * weight2.y;
diff7 = luminanceWeight(pix7, center, slope) * weight2.z;
diff8 = luminanceWeight(pix8, center, slope) * weight2.w;
diff9 = luminanceWeight(pix9, center, slope) * weight3;
return vec4(diff1 * pix1.rgb + diff2 * pix2.rgb + diff3 * pix3.rgb + diff4 * pix4.rgb + diff5 * pix5.rgb + diff6 * pix6.rgb + diff7 * pix7.rgb + diff8 * pix8.rgb + diff9 * pix9.rgb,
diff1 + diff2 + diff3 + diff4 + diff5 + diff6 + diff7 + diff8 + diff9);
kernel vec4 bilateralAdd_9(sampler src, sampler sums, float slope, vec2 offset1, vec2 offset2, vec2 offset3, vec2 offset4, vec2 offset5,
vec2 offset6, vec2 offset7, vec2 offset8, vec2 offset9, vec4 weight1, vec4 weight2, float weight3, float sumWeight)
vec4 sum;
sum = sample(sums, samplerCoord(sums))*sumWeight;
return sum + bilateralRow_9(src, slope, offset1, offset2, offset3, offset4, offset5, offset6, offset7, offset8, offset9,
weight1, weight2, weight3);
kernel vec4 bilateralFinalize(sampler sums)
vec4 sum;
sum = sample(sums, samplerCoord(sums));
return vec4(sum.rgb / max(sum.a, 0.001), 1.0);
kernel vec4 bilateralLoop2(sampler src, float slope, vec2 weights12)
vec2 coord;
vec4 center = sample(src, samplerCoord(src));
vec4 sum;
vec4 pix0, pix1, pix2;
vec4 pix3,       pix5;
vec4 pix6, pix7, pix8;
vec2 dx = samplerTransform(src, vec2( 1.0, 0.0)) - samplerTransform(src, vec2(0.0, 0.0));
vec2 dy = samplerTransform(src, vec2(-2.0, 1.0)) - samplerTransform(src, vec2(0.0, 0.0));
coord = samplerTransform(src, destCoord() + vec2(-1.0, -1.0));
pix0 = sample(src, coord);
coord = coord + dx;
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dy;
pix3 = sample(src, coord);
coord = coord + dx;
coord = coord + dx;
pix5 = sample(src, coord);
coord = coord + dy;
pix6 = sample(src, coord);
coord = coord + dx;
pix7 = sample(src, coord);
coord = coord + dx;
pix8 = sample(src, coord);
sum = (1.0 - min(1.0, (slope * length(pix0 - center)))) * (pix0 * weights12.y);
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights12.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights12.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights12.x) + sum;
sum =                                        center                            + sum;
sum = (1.0 - min(1.0, (slope * length(pix5 - center)))) * (pix5 * weights12.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix6 - center)))) * (pix6 * weights12.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix7 - center)))) * (pix7 * weights12.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix8 - center)))) * (pix8 * weights12.y) + sum;
return vec4(sum.rgb / sum.a, 1.0);
kernel vec4 bilateralLoop5(sampler src, float slope, vec4 weights1245)
vec2  coord;
vec4  center = sample(src, samplerCoord(src));
vec4  sum;
vec4  pix0, pix1, pix2, pix3, pix4;
vec2 dx = samplerTransform(src, vec2( 1.0, 0.0)) - samplerTransform(src, vec2(0.0, 0.0));
vec2 dy = samplerTransform(src, vec2(-4.0, 1.0)) - samplerTransform(src, vec2(0.0, 0.0));
coord = samplerTransform(src, destCoord() + vec2(-1.0, -2.0));
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dx;
pix3 = sample(src, coord);
coord = coord + dx + dy;
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights1245.w);
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights1245.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights1245.w) + sum;
pix0 = sample(src, coord);
coord = coord + dx;
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dx;
pix3 = sample(src, coord);
coord = coord + dx;
pix4 = sample(src, coord);
coord = coord + dy;
sum = (1.0 - min(1.0, (slope * length(pix0 - center)))) * (pix0 * weights1245.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights1245.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights1245.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights1245.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix4 - center)))) * (pix4 * weights1245.w) + sum;
pix0 = sample(src, coord);
coord = coord + dx;
pix1 = sample(src, coord);
coord = coord + dx + dx;
pix3 = sample(src, coord);
coord = coord + dx;
pix4 = sample(src, coord);
coord = coord + dy;
sum = (1.0 - min(1.0, (slope * length(pix0 - center)))) * (pix0 * weights1245.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights1245.x) + sum;
sum =                                        center                              + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights1245.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix4 - center)))) * (pix4 * weights1245.z) + sum;
pix0 = sample(src, coord);
coord = coord + dx;
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dx;
pix3 = sample(src, coord);
coord = coord + dx;
pix4 = sample(src, coord);
coord = coord + dx + dy;
sum = (1.0 - min(1.0, (slope * length(pix0 - center)))) * (pix0 * weights1245.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights1245.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights1245.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights1245.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix4 - center)))) * (pix4 * weights1245.w) + sum;
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dx;
pix3 = sample(src, coord);
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights1245.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights1245.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights1245.w) + sum;
return vec4(sum.rgb / sum.a, 1.0);
kernel vec4 bilateralLoop11(sampler src, float slope, vec4 weights1245, vec4 weights89AD)
vec2  coord;
vec4  center = sample(src, samplerCoord(src));
vec4  sum;
vec4  pix0, pix1, pix2, pix3, pix4, pix5, pix6;
vec2 dx = samplerTransform(src, vec2( 1.0, 0.0)) - samplerTransform(src, vec2(0.0, 0.0));
vec2 dy = samplerTransform(src, vec2(-6.0, 1.0)) - samplerTransform(src, vec2(0.0, 0.0));
coord = samplerTransform(src, destCoord() + vec2(-2.0, -3.0));
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dx;
pix3 = sample(src, coord);
coord = coord + dx;
pix4 = sample(src, coord);
coord = coord + dx;
pix5 = sample(src, coord);
coord = coord + dx + dy;
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights89AD.w);
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights89AD.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights89AD.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix4 - center)))) * (pix4 * weights89AD.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix5 - center)))) * (pix5 * weights89AD.w) + sum;
pix0 = sample(src, coord);
coord = coord + dx;
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dx;
pix3 = sample(src, coord);
coord = coord + dx;
pix4 = sample(src, coord);
coord = coord + dx;
pix5 = sample(src, coord);
coord = coord + dx;
pix6 = sample(src, coord);
coord = coord + dy;
sum = (1.0 - min(1.0, (slope * length(pix0 - center)))) * (pix0 * weights89AD.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights89AD.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights1245.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights1245.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix4 - center)))) * (pix4 * weights1245.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix5 - center)))) * (pix5 * weights89AD.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix6 - center)))) * (pix6 * weights89AD.w) + sum;
pix0 = sample(src, coord);
coord = coord + dx;
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dx;
pix3 = sample(src, coord);
coord = coord + dx;
pix4 = sample(src, coord);
coord = coord + dx;
pix5 = sample(src, coord);
coord = coord + dx;
pix6 = sample(src, coord);
coord = coord + dy;
sum = (1.0 - min(1.0, (slope * length(pix0 - center)))) * (pix0 * weights89AD.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights1245.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights1245.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights1245.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix4 - center)))) * (pix4 * weights1245.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix5 - center)))) * (pix5 * weights1245.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix6 - center)))) * (pix6 * weights89AD.z) + sum;
pix0 = sample(src, coord);
coord = coord + dx;
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dx + dx;
pix4 = sample(src, coord);
coord = coord + dx;
pix5 = sample(src, coord);
coord = coord + dx;
pix6 = sample(src, coord);
coord = coord + dy;
sum = (1.0 - min(1.0, (slope * length(pix0 - center)))) * (pix0 * weights89AD.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights1245.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights1245.x) + sum;
sum =                                        center                              + sum;
sum = (1.0 - min(1.0, (slope * length(pix4 - center)))) * (pix4 * weights1245.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix5 - center)))) * (pix5 * weights1245.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix6 - center)))) * (pix6 * weights89AD.y) + sum;
pix0 = sample(src, coord);
coord = coord + dx;
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dx;
pix3 = sample(src, coord);
coord = coord + dx;
pix4 = sample(src, coord);
coord = coord + dx;
pix5 = sample(src, coord);
coord = coord + dx;
pix6 = sample(src, coord);
coord = coord + dy;
sum = (1.0 - min(1.0, (slope * length(pix0 - center)))) * (pix0 * weights89AD.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights1245.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights1245.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights1245.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix4 - center)))) * (pix4 * weights1245.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix5 - center)))) * (pix5 * weights1245.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix6 - center)))) * (pix6 * weights89AD.z) + sum;
pix0 = sample(src, coord);
coord = coord + dx;
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dx;
pix3 = sample(src, coord);
coord = coord + dx;
pix4 = sample(src, coord);
coord = coord + dx;
pix5 = sample(src, coord);
coord = coord + dx;
pix6 = sample(src, coord);
coord = coord + dx + dy;
sum = (1.0 - min(1.0, (slope * length(pix0 - center)))) * (pix0 * weights89AD.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights89AD.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights1245.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights1245.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix4 - center)))) * (pix4 * weights1245.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix5 - center)))) * (pix5 * weights89AD.x) + sum;
sum = (1.0 - min(1.0, (slope * length(pix6 - center)))) * (pix6 * weights89AD.w) + sum;
pix1 = sample(src, coord);
coord = coord + dx;
pix2 = sample(src, coord);
coord = coord + dx;
pix3 = sample(src, coord);
coord = coord + dx;
pix4 = sample(src, coord);
coord = coord + dx;
pix5 = sample(src, coord);
sum = (1.0 - min(1.0, (slope * length(pix1 - center)))) * (pix1 * weights89AD.w) + sum;
sum = (1.0 - min(1.0, (slope * length(pix2 - center)))) * (pix2 * weights89AD.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix3 - center)))) * (pix3 * weights89AD.y) + sum;
sum = (1.0 - min(1.0, (slope * length(pix4 - center)))) * (pix4 * weights89AD.z) + sum;
sum = (1.0 - min(1.0, (slope * length(pix5 - center)))) * (pix5 * weights89AD.w) + sum;
return vec4(sum.rgb / sum.a, 1.0);
kernel vec4 convertFromRGBToLab(sampler src)
vec3 f;
vec4 pix, color;
pix = unpremultiply(sample(src, samplerCoord(src)));
color.xyz = pix.r * vec3(0.449695,  0.316251, 0.18452  )
+ pix.g * vec3(0.244634,  0.672034, 0.0833318)
+ pix.b * vec3(0.0251829, 0.141184, 0.922602 );
color.xyz *= vec3(1.052111, 1.0, 0.918417);
f = compare(color.xyz - 0.00885645, 7.787037 * color.xyz + 0.137931, pow(color.xyz, vec3(0.333333)));
color.r = 116.0 * f.y - 16.0;
color.g = 500.0 * (f.x - f.y);
color.b = 200.0 * (f.y - f.z);
color.rgb *= 0.005;
color.a = 1.0;
return color;
kernel vec4 convertFromLabToRGB(sampler src, sampler original)
vec3 f, cie;
vec4 color, pix, opix;
pix = sample(src, samplerCoord(src));
opix = sample(original, samplerCoord(original));
pix.rgb *= 200.0;
f.y = (pix.r + 16.0) / 116.0;
f.x = f.y + pix.g * 0.002;
f.z = f.y - pix.b * 0.005;
color.xyz = f * f * f;
cie = compare(color.xyz - 0.00885645, (f.xyz - 0.137931) / 7.787037, color.xyz);
cie *= vec3(0.95047, 1.0, 1.08883);
color.rgb = cie.x * vec3(2.95176,   -1.28951, -0.47388  )
+ cie.y * vec3(-1.0851,    1.99084,  0.0372023)
+ cie.z * vec3(0.0854804, -0.269456, 1.09113  );
color.a = opix.a;
return premultiply(color);
bilateralAdd_1
bilateralAdd_2
bilateralAdd_3
bilateralAdd_4
bilateralAdd_5
bilateralAdd_6
bilateralAdd_7
bilateralAdd_8
bilateralAdd_9
bilateralFinalize
convertFromRGBToLab
convertFromLabToRGB
points.count > 0
-[GUBilateralConvolution boundsForPointArray:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Adjustments/PIBilateralFilter.m
bilateralLoop2
bilateralLoop5
bilateralLoop11
inputEdgeDetail
ridiculously large radius for bilateral filter
-[PIBilateralFilter outputImage]
inputPoints
inputWeights
unable to allocate convolution table in bilateral filter
com.apple.livephoto
com.apple.video
com.apple.video.slomo
.%lu
%lu.%lu%@%@
satPercentile75
satPercentile98
satPercentileG98
satAutoValue
inputVibrancy
inputCast
kernel vec4 _smartcolor_contrast_HDR (__sample im, float amt) 
 vec3 diff = im.rgb-dot(im.rgb, vec3(.0, .3, .5)); 
 float dist = distance(diff, vec3(0.0)); 
 dist = smoothstep(0.0, 1.0, dist); 
 float strength = 5.0*dist*amt; 
 vec3 pos = max(im.rgb, 1.0)-1.0 + min(im.rgb, 0.0); 
 im.rgb = clamp(im.rgb, 0.0, 1.0); 
 strength *= (im.b-im.g); 
 strength = max(strength, -0.35); 
 vec4 result; 
 result.rgb = im.rgb/(strength + 1.0 - (im.rgb*strength)) + pos; 
 result.a = im.a; 
 return result; 
kernel vec4 _smartcolor_contrast_darken_HDR (__sample im, float amt) 
 vec3 diff = im.rgb-dot(im.rgb, vec3(.0, .3, .5)); 
 float dist = distance(diff, vec3(0.0)); 
 dist = smoothstep(0.0, 1.0, dist); 
 float strength = 5.0*dist*amt; 
 vec3 pos = max(im.rgb, 1.0)-1.0 + min(im.rgb, 0.0); 
 im.rgb = clamp(im.rgb, 0.0, 1.0); 
 strength *= (im.b-im.g); 
 float gray = 1.0-min(dot(im.rgb, vec3(0.5, 0.7, -0.20)), 1.0); 
 vec4 result; 
 result.rgb = strength < 0.0 ? pow(im.rgb, vec3(1.0-strength*gray)) : im.rgb/(strength+1.0-(im.rgb*strength)); 
 result.rgb += pos; result.a = im.a; 
 return result; 
kernel vec4 _smartcolor_vibrancy_gt1_HDR (__sample im, float amt) 
 float gray = dot(clamp(im.rgb, 0.0, 1.0), vec3(.3, .5, .2)); 
 float y = dot(clamp(im.rgb, 0.0, 1.0), vec3(.4, .2, .1)); 
 float damp = 1.0-4.0*y*(1.0-y); 
 float s = 1.0/(im.r+im.g+im.b); 
 float r = im.r*s; 
 float b = im.b*s; 
 float d = 1.0-.8*smoothstep(0.2, 0.4, r-b); 
 damp *= d; 
 damp = amt > 2.5 ? min(damp+(amt-2.5)/5.0, 1.0) : damp; 
 float sat = min(amt, 3.0); 
 vec4 result; 
 result.rgb = (im.rgb - gray)*sat + gray; 
 result.rgb = mix(im.rgb, result.rgb, damp); 
 result.a = im.a; 
 return result; 
kernel vec4 _smartcolor_vibrancy_lt1_HDR (__sample im, float amt) 
 float gray = dot(im.rgb, vec3(0.333333)); 
 im.rgb = mix(vec3(gray), im.rgb, amt); 
 return im; 
kernel vec4 _smartcolor_cast_HDR (__sample im, float lum, float grayI, float grayQ, float strength) 
 vec4 pix = clamp(im, 0.0, 1.0); 
 pix.rgb = pow(pix.rgb, vec3(.25)); 
 pix.rgb = pix.r * vec3(0.299, 0.595716, 0.211456) + 
 pix.g * vec3(0.587, -0.274453, -0.522591) + 
 pix.b * vec3(0.114, -0.321263, 0.311135); 
 vec2 grayOffset = vec2(grayI, grayQ) ; 
 vec3 result = pix.rgb; 
 float newStrength = 1.0 + (strength-1.0)*(1.0-pix.r) ; 
 result.gb = pix.gb + newStrength*grayOffset ; 
 float damp = max(min(1.0, pix.r/(lum+0.00001)),0.0) ; 
 result.rgb = mix(pix.rgb, result.rgb, damp) ; 
 pix.rgb = result.r * vec3(1.0) + 
 result.g * vec3(0.956296, -0.272122, -1.10699) + 
 result.b * vec3(0.621024, -0.647381, 1.70461); 
 pix.rgb = clamp(pix.rgb, 0.0, 1.0); 
 pix.rgb *= pix.rgb*pix.rgb*pix.rgb; 
 pix.rgb += min(im.rgb, 0.0) + max(im.rgb,1.0) -1.0; 
 return pix; 
CIVibrance
inputAmount
PISmartColorHDR-sat-histogram
crossfadeDurationValue
crossfadeDurationTimescale
startTimeValue
startTimeTimescale
timeRangeStartValue
timeRangeStartTimescale
timeRangeDurationValue
timeRangeDurationTimescale
portraitInfo
spillMatteAllowed
@"NUAdjustment"16@?0@"NUAdjustment"8
grayI
grayQ
grayWarmth
grayY
warmFace
-[PIOrientationAdjustmentController setOrientation:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Controllers/PIOrientationAdjustmentController.m
mdta/com.apple.quicktime.cinematic-video.rendering
mdta/com.apple.quicktime.aperture-float
mdta/com.apple.quicktime.camera-dictionary
PIPortraitVideoMetadata.m
metadataGroup != nil
outError != nil
Could not decode timed rendering data
Missing disparity rendering metadata
Missing aperture rendering metadata
v24@?0@"PTRenderPipeline"8@"<PTRenderState>"16
device != nil
-[PIPortraitVideoRenderer initWithDevice:colorSize:disparitySize:quality:debugMode:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Render/PIPortraitVideoRenderer.m
!NUPixelSizeIsEmpty(colorSize)
!NUPixelSizeIsEmpty(disparitySize)
quality >= PTQualityPreviewLow && quality <= PTQualityExportProfessional
<%@:%p device:%@ color=%ldx%ld disparity=%ldx%ld quality=%d debug=%ld>
PIPortraitVideoRenderer.pool
B32@?0@"PIPortraitVideoRenderer"8Q16^B24
kernel vec4 _highKeyHDR(__sample im, float str) 
 vec3 neg = min(im.rgb, 0.0); 
 vec3 pos = max(im.rgb, 1.0) - 1.0; 
 im = clamp(im, 0.0, 1.0); 
 vec4 im2 = 1.0-((im-1.0)*(im-1.0)); 
 im2 = sqrt(im2); 
 float y = dot(im.rgb, vec3(.333333)); 
 float y2 = sqrt(1.0-(y-1.0)*(y-1.0)); 
 y2 = mix(y2, smoothstep(0.0, 1.0, y2), 0.5); 
 vec4 im3 = (y>0) ? im*y2/y : vec4(0.0, 0.0, 0.0, 1.0) ; 
 im3 = mix(im3, im2, .7*sqrt(y2)); 
 im3 = mix(im, im3, sqrt(y)) ; 
 im.rgb = mix(im.rgb, im3.rgb, str) + pos + neg; 
 return im; } 
None
UnexpectedRuntimeError
NoBuildingDetected
PanoAndFFCNotSupported
FacesTooLarge
ConfidenceTooLow
LimitExceeded
NotEnoughLines
CorrectedAreaIsSmall
LessThanMinimumCorrection
-[PIPerspectiveAutoCalculator initWithComposition:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Autocalculators/PIPerspectiveAutoCalculator.m
%@.%@
%@.error
%@.underlyingError
pitchError
yawError
pitchError.underlyingError
yawError.underlyingError
%@PerspectiveEvaluation-txt.DBG
%@PerspectiveLineDetection-png.DBG
Edit
PIPerspectiveAutoCalculator-debug
faces != nil
-[PIPerspectiveAutoCalculator getSizeOfAllFaces:]
-[PIPerspectiveAutoCalculator passesFaceCheck:]
PIPerspectiveAutoCalculator-faceCheck
faceSize
maxFaceSize
passesFaceCheck
Apple
front
camera
-[PIPerspectiveAutoCalculator passesImagePropertiesCheck:]
PIPerspectiveAutoCalculator-selfieAndAspectRatioCheck
aspectRatio
isSelfieCam
passesImagePropertiesCheck
-[PIPerspectiveAutoCalculator passesBuildingCheck:]
PIPerspectiveAutoCalculator-classify
minConfidence
passesBuildingCheck
-[PIPerspectiveAutoCalculator submit:]
angle
-[PIPerspectiveAutoCalculator overcaptureImageProperties:]
No overcapture
PIPerspectiveAutoCalculator-overcaptureImageProperties
-[PIPerspectiveAutoCalculator primaryImageProperties:]
PIPerspectiveAutoCalculator-primaryImageProperties
-[PIPerspectiveAutoCalculator canGenerateNewCropRect:]
preseedRoll
canGenerateNewCropRect
setToPrimary
pitch != nil
+[PIPerspectiveAutoCalculator undoOrientation:forPitch:yaw:angle:]
yaw != nil
angle != nil
-[PIPerspectiveAutoCalculator passesConfidenceCheck:error:]
passesConfidenceCheck
supported
Not supported by Core Image. Default: YES
-[PIPerspectiveAutoCalculator passesMinimumCorrectionCheck:error:]
pitchExpandTopDegrees
yawExpandLeftDegrees
rollAngleInDegreesCW
passesMinimumCorrectionCheck
minimumPitchCorrection
minimumYawCorrection
minimumAngleCorrection
-[PIPerspectiveAutoCalculator submitVerified:]
focalLength
pitchLimit
yawLimit
rollLimit
minimumPitchCorrectionArea
minimumYawCorrectionArea
CIAutoPerspective
submitVerified
debugImage
pitchCorrectionAreaCoverage
yawCorrectionAreaCoverage
ciPitchError
ciYawError
PIPerspectiveAutoCalculator-getPrimaryOrientation
luminance
Flash
FNumber
ApertureValue
ShutterSpeedValue
ExposureTime
ISOSpeedRatings
cannot set empty crop rect
-[PICropAdjustmentController setCropRect:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Controllers/PICropAdjustmentController.m
constraintWidth
constraintHeight
smart
originalCrop
inputDecoderVersion
inputLight
localAutoValue
inputBrightness
inputContrast
inputExposure
inputHighlights
inputShadows
inputBlack
inputLocalLight
inputLightMap
inputLightMapWidth
inputLightMapHeight
offsetBlack
offsetBrightness
offsetContrast
offsetExposure
offsetHighlights
offsetLocalLight
offsetShadows
inputRawHighlights
statistics
overcaptureStatistics
kernel vec4 alphaCompositing(__sample src, __sample dst, float a)
return mix(dst, src, a);
kernel vec4 dynamismMap(sampler imageMax, sampler imageMin, float logisticGain, float logisticMid) __attribute__((outputFormat(kCIFormatRGBAh)))
const float MAX_VAL = 1.74;
const float THRESHOLD = 0.05;
vec2 dc = destCoord();
vec2 sc;
sc = dc + vec2(-1,-1);
vec4 pMax00 = sample(imageMax, samplerTransform(imageMax, sc));
vec4 pMin00 = sample(imageMin, samplerTransform(imageMin, sc));
sc = dc + vec2(-1,0);
vec4 pMax01 = sample(imageMax, samplerTransform(imageMax, sc));
vec4 pMin01 = sample(imageMin, samplerTransform(imageMin, sc));
sc = dc + vec2(-1,1);
vec4 pMax02 = sample(imageMax, samplerTransform(imageMax, sc));
vec4 pMin02 = sample(imageMin, samplerTransform(imageMin, sc));
sc = dc + vec2(0,-1);
vec4 pMax10 = sample(imageMax, samplerTransform(imageMax, sc));
vec4 pMin10 = sample(imageMin, samplerTransform(imageMin, sc));
vec4 pMax11 = sample(imageMax, samplerTransform(imageMax, dc));
vec4 pMin11 = sample(imageMin, samplerTransform(imageMin, dc));
sc = dc + vec2(0,1);
vec4 pMax12 = sample(imageMax, samplerTransform(imageMax, sc));
vec4 pMin12 = sample(imageMin, samplerTransform(imageMin, sc));
sc = dc + vec2(1,-1);
vec4 pMax20 = sample(imageMax, samplerTransform(imageMax, sc));
vec4 pMin20 = sample(imageMin, samplerTransform(imageMin, sc));
sc = dc + vec2(1,0);
vec4 pMax21 = sample(imageMax, samplerTransform(imageMax, sc));
vec4 pMin21 = sample(imageMin, samplerTransform(imageMin, sc));
sc = dc + vec2(1,1);
vec4 pMax22 = sample(imageMax, samplerTransform(imageMax, sc));
vec4 pMin22 = sample(imageMin, samplerTransform(imageMin, sc));
float minDevCMaxNMin = distance(pMax11, pMin00);
minDevCMaxNMin = min( minDevCMaxNMin , distance(pMax11, pMin01) );
minDevCMaxNMin = min( minDevCMaxNMin , distance(pMax11, pMin02) );
minDevCMaxNMin = min( minDevCMaxNMin , distance(pMax11, pMin10) );
minDevCMaxNMin = min( minDevCMaxNMin , distance(pMax11, pMin11) );
minDevCMaxNMin = min( minDevCMaxNMin , distance(pMax11, pMin12) );
minDevCMaxNMin = min( minDevCMaxNMin , distance(pMax11, pMin20) );
minDevCMaxNMin = min( minDevCMaxNMin , distance(pMax11, pMin21) );
minDevCMaxNMin = min( minDevCMaxNMin , distance(pMax11, pMin22) );
float minDevCMinNMax = distance(pMin11, pMax00);
minDevCMinNMax = min( minDevCMinNMax , distance(pMin11, pMax01) );
minDevCMinNMax = min( minDevCMinNMax , distance(pMin11, pMax02) );
minDevCMinNMax = min( minDevCMinNMax , distance(pMin11, pMax10) );
minDevCMinNMax = min( minDevCMinNMax , distance(pMin11, pMax11) );
minDevCMinNMax = min( minDevCMinNMax , distance(pMin11, pMax12) );
minDevCMinNMax = min( minDevCMinNMax , distance(pMin11, pMax20) );
minDevCMinNMax = min( minDevCMinNMax , distance(pMin11, pMax21) );
minDevCMinNMax = min( minDevCMinNMax , distance(pMin11, pMax22) );
float outVal = min(minDevCMaxNMin , minDevCMinNMax) / MAX_VAL;
outVal = 1.0f / (1.0f + exp(-logisticGain*(outVal - logisticMid)));
vec4 outPixel = vec4(outVal, outVal, outVal, 1.0);
return outPixel;
kernel vec4 jointbilateralfilter(sampler mask, sampler guide, vec2 rad, vec3 sig) __attribute__((outputFormat(kCIFormatRh)))
vec2 coord = destCoord();
float rh = rad.x;
float rv = rad.y;
vec4 p0 = sample(mask, samplerTransform(mask, coord));
vec4 g0 = sample(guide, samplerTransform(guide, coord));
vec4 result = vec4(0.0f);
float w_sum = 0.0f;
for(float yo=-rad.y;  yo<=rad.y;  yo++) {
for(float xo=-rad.x;  xo<=rad.x;  xo++) {
vec2 dc = vec2(xo, yo);
vec4 p = sample(mask, samplerTransform(mask, coord + dc));
vec4 g = sample(guide, samplerTransform(guide, coord + dc));
float d = dot(dc, dc);
float pdif = dot(p-p0,p-p0);
float gdif = dot(g-g0,g-g0);
float w = exp(dot(sig, vec3(d,pdif,gdif)));
result += w*p;
w_sum += w;
result /= w_sum;
return vec4(result.rgb, 1.0);
kernel vec4 rgba_to_luma(__sample rgb)
const vec4 rgb2y = vec4(0.2999,0.5870,0.1140,0.0);
float luma = dot(rgb, rgb2y);
return vec4(luma, luma, luma, 1.0);
kernel vec2 warp_homography(vec3 h012, vec3 h345, vec3 h678)
vec3 coord = vec3(destCoord(), 1.f);
float norm = 1.f / dot(h678, coord);
float x = norm * dot(h012, coord);
float y = norm * dot(h345, coord);
return vec2(x,y);
kernel vec4 ncc_compute(sampler img1, sampler img2) __attribute__((outputFormat(kCIFormatR8)))
vec2 coord = destCoord();
float sum1   = float(0.0);
float sum2   = float(0.0);
float sumSq1 = float(0.0);
float sumSq2 = float(0.0);
float sumCrs = float(0.0);
float ncc;
float p1, p2;
const int halfKernel = 3;
const vec4 meanOp = vec4(1.0/3.0, 1.0/3.0, 1.0/3.0, 0.0);
float count = 0.0;
for(int y=-halfKernel; y<=halfKernel; y++) {
for(int x=-halfKernel; x<=halfKernel; x++) {
p1 = dot(meanOp, sample(img1, samplerTransform(img1, coord + vec2(x, y))));
p2 = dot(meanOp, sample(img2, samplerTransform(img2, coord + vec2(x, y))));
sum1 += p1;
sum2 += p2;
sumSq1 += p1*p1;
sumSq2 += p2*p2;
sumCrs += p1*p2;
count += 1.0;
float denom = (sumSq1-sum1*sum1/count) * (sumSq2-sum2*sum2/count);
ncc = compare(denom, 0.0, ((sumCrs-(sum1*sum2/count)))/sqrt(denom));
ncc = max(ncc, 0.0);
return vec4(ncc, ncc, ncc, 1.0);
kernel vec4 ncc_coarse_compute(__sample ncc, vec2 edge) __attribute__((outputFormat(kCIFormatR8)))
float value = smoothstep(edge.x, edge.y, ncc.r);
return vec4(value, value, value, 1.0);
kernel vec4 blur_image_compute_3x3(sampler img)
const int halfKernel = 1;
vec2 coord = destCoord();
vec4 sum = vec4(0.0);
float count = 0.0;
for(int y=-halfKernel; y<=halfKernel; y++) {
for(int x=-halfKernel; x<=halfKernel; x++) {
sum += sample(img, samplerTransform(img, coord + vec2(x, y)));
count += 1.0;
sum = sum/(count);
return sum;
kernel vec4 blur_image_compute_5x5(sampler img)
const int halfKernel = 2;
vec2 coord = destCoord();
vec4 sum = vec4(0.0);
float count = 0.0;
for(int y=-halfKernel; y<=halfKernel; y++) {
for(int x=-halfKernel; x<=halfKernel; x++) {
sum += sample(img, samplerTransform(img, coord + vec2(x, y)));
count += 1.0;
sum = sum/(count);
return sum;
kernel vec4 blur_image_compute_7x7(sampler img)
const int halfKernel = 3;
vec2 coord = destCoord();
vec4 sum = vec4(0.0);
float count = 0.0;
for(int y=-halfKernel; y<=halfKernel; y++) {
for(int x=-halfKernel; x<=halfKernel; x++) {
sum += sample(img, samplerTransform(img, coord + vec2(x, y)));
count += 1.0;
sum = sum/(count);
return sum;
kernel vec4 fuse_image_compute(sampler refImg, sampler guideImg, sampler blurImg, sampler weightImg, sampler maskImg, vec2 threshold)
vec4 ref       = sample(refImg, samplerCoord(refImg));
vec4 refBlur   = ref;
vec4 guide     = sample(guideImg, samplerCoord(guideImg));
vec4 guideBlur = sample(blurImg, samplerCoord(blurImg));
float weight   = sample(weightImg, samplerCoord(weightImg)).r;
float mask     = sample(maskImg, samplerCoord(maskImg)).r;
vec3 CbCoef = vec3(-0.1726, -0.3391, 0.5117);
vec3 CrCoef = vec3(0.5115, -0.4282, -0.0830);
float CbRef   = dot(CbCoef, ref.rgb);
float CbGuide = dot(CbCoef, guideBlur.rgb);
float CrRef   = dot(CrCoef, ref.rgb);
float CrGuide = dot(CrCoef, guideBlur.rgb);
float crDiff = smoothstep(0.0, 0.2, abs(CbRef-CbGuide));
float cbDiff = smoothstep(0.0, 0.2, abs(CrRef-CrGuide));
float chDiff = smoothstep(0.0,0.3,crDiff+cbDiff);
float weight1 = (1.0-smoothstep(threshold.x,threshold.y,mask));
weight *= weight1 * (1.0-chDiff);
vec4 value = mix(ref, (0.9*(guide-guideBlur)+refBlur), weight);
return value;
jointbilateralfilter
rgba_to_luma
warp_homography
ncc_compute
ncc_coarse_compute
blur_image_compute_7x7
blur_image_compute_5x5
blur_image_compute_3x3
fuse_image_compute
CIPhotoEffectMono
Mono
CIPhotoEffectTonal
Tonal
CIPhotoEffectNoir
Noir
CIPhotoEffectFade
Fade
CIPhotoEffectChrome
Chrome
CIPhotoEffectProcess
Process
CIPhotoEffectTransfer
Transfer
CIPhotoEffectInstant
Instant
CIPhotoEffect3DVivid
3DVivid
CIPhotoEffect3DVividWarm
3DVividWarm
CIPhotoEffect3DVividCool
3DVividCool
CIPhotoEffect3DDramatic
3DDramatic
CIPhotoEffect3DDramaticWarm
3DDramaticWarm
CIPhotoEffect3DDramaticCool
3DDramaticCool
CIPhotoEffect3DSilverplate
3DSilverplate
CIPhotoEffect3DNoir
3DNoir
CIPortraitEffectBlack
Black
CIPortraitEffectBlackoutMono
BlackoutMono
CIPortraitEffectStageV2
StageV2
CIPortraitEffectStageMonoV2
StageMonoV2
CIPortraitEffectStageWhite
StageWhite
CIPortraitEffectLight
Light
CIPortraitEffectCommercial
Commercial
CIPortraitEffectContour
Contour
CIPortraitEffectStudioV2
StudioV2
CIPortraitEffectContourV2
ContourV2
%@ (preview) %a
%@ %a
+[PIPhotoEditHelper imageSourceWithURL:type:useEmbeddedPreview:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Util/PIPhotoEditHelper.m
+[PIPhotoEditHelper imageSourceWithURL:type:proxyImage:orientation:useEmbeddedPreview:]
+[PIPhotoEditHelper imageSourceWithCIImage:orientation:]
+[PIPhotoEditHelper videoSourceWithURL:]
photoSource != nil
+[PIPhotoEditHelper livePhotoSourceWithPhotoSource:videoSource:]
videoSource != nil
%@+%@
name != nil
+[PIPhotoEditHelper newAdjustmentWithName:]
identifier != nil
+[PIPhotoEditHelper newAdjustmentWithIdentifier:]
+[PIPhotoEditHelper newImageRenderClientWithName:]
+[PIPhotoEditHelper geometryRequestWithComposition:]
+[PIPhotoEditHelper imagePropertiesRequestWithComposition:]
+[PIPhotoEditHelper videoPropertiesRequestWithComposition:]
targetSize.width > 0 && targetSize.height > 0
+[PIPhotoEditHelper imageRenderRequestWithComposition:fitInSize:wideGamut:]
PIPhotoEditHelper-targetSize-image
+[PIPhotoEditHelper imageRenderRequestWithComposition:fillInSize:wideGamut:]
PIPhotoEditHelper-fillSquare-image
+[PIPhotoEditHelper _imageRenderRequestWithComposition:wideGamut:]
PIPhotoEditHelper-basic-image
+[PIPhotoEditHelper videoRenderRequestWithComposition:fitInSize:]
overrideVideoEditability
v32@?0@"NSString"8@"NSString"16^B24
/ShowOriginalSource
var image = input;
ResetTagInput('/pre-VideoReframe', image);
image = GetTag('VideoReframe');
ResetTagInput('/pre-Geometry', image);
return GetTag('/post-Geometry');
/pre-VideoReframe
VideoReframe
/pre-Geometry
/post-Geometry
ResetTagInput('/pre-Geometry', input);
return GetTag('/post-Geometry');
PIPhotoEditHelper.m
inputCorrectionInfo
depthInfo
start
startScale
endScale
SkipShaderPrewarm
boostVersion
boostParams
kernel vec4 forwardBoostWithBoost(sampler image, float boost) {
vec4 im = sample(image, samplerCoord(image));
vec4 orig = im;
float n = 1.8;
float k = 0.8;
vec3 pos = max(im.rgb, k) - k;
vec3 neg = min(im.rgb, 0.0);
neg *= 2.8;
im.rgb = clamp(im.rgb, 0.0, k);
im.rgb = ((1.0+n)*im.rgb)/(1.0+(n*im.rgb)) + neg;
im.r = pos.r > 0.0 ? (.91803 + .470304*pos.r) : im.r;
im.g = pos.g > 0.0 ? (.91803 + .470304*pos.g) : im.g;
im.b = pos.b > 0.0 ? (.91803 + .470304*pos.b) : im.b;
im.rgb = mix(orig.rgb, im.rgb, boost);
return im;
kernel vec4 inverseBoostWithBoost(sampler image, float boost){
vec4 im = sample(image, samplerCoord(image));
vec4 orig = im;
float n = 1.8;
float kinv = 0.91803;
vec3 pos = max(im.rgb, kinv);
vec3 neg = min(im.rgb, 0.0);
im.rgb = clamp(im.rgb, 0.0, kinv);
neg *= .35714286;
im.rgb = im.rgb/(n+1.0-(im.rgb*n)) + neg;
im.r = pos.r > kinv ? 0.8 + 2.126286*(pos.r-kinv) : im.r;
im.g = pos.g > kinv ? 0.8 + 2.126286*(pos.g-kinv) : im.g;
im.b = pos.b > kinv ? 0.8 + 2.126286*(pos.b-kinv) : im.b;
im.rgb = mix(orig.rgb, im.rgb, boost);
return im;
kernel vec4 forwardBoost3(__sample im, float boost, vec4 abcd, vec3 p0, vec3 p1) {
float a = abcd.x;
float b = abcd.y;
float c = abcd.z;
float d = abcd.w;
vec3 x = im.rgb;
vec3 f = (a * x + b) / (c * x + d);
float x0 = p0.x;
float y0 = p0.y;
float s0 = p0.z;
float a0 = y0*y0;
float c0 = y0 - s0*x0;
float d0 = s0*x0*x0;
vec3 f0 = (a0 * x) / (c0 * x + d0);
vec3 l0 = (a0 / d0) * x;
f0 = compare(x, l0, f0);
float x1 = p1.x;
float y1 = p1.y;
float s1 = p1.z;
float a1 = (1.f-y1)*(1.f-y1);
float c1 = (1.f-y1) - s1*(1.f-x1);
float d1 = s1 *(1.f-x1)*(1.f-x1);
vec3 f1 = 1.f - (a1 * (1.f - x)) / (c1 * (1.f - x) + d1);
vec3 l1 = 1.f - (a1 / d1) * (1.f - x);
f1 = compare(1.f - x, l1, f1);
f = compare(x - p0.x, f0, compare(x - p1.x, f, f1));
im.rgb = mix(im.rgb, f, boost);
return im;
kernel vec4 inverseBoost3(__sample im, float boost, vec4 abcd, vec3 p0, vec3 p1) {
float a = abcd.x;
float b = abcd.y;
float c = abcd.z;
float d = abcd.w;
vec3 y = im.rgb;
vec3 g = (d * y - b) / (a - c * y);
float x0 = p0.x;
float y0 = p0.y;
float s0 = p0.z;
float a0 = x0*x0;
float c0 = x0 - y0/s0;
float d0 = y0*y0/s0;
vec3 g0 = (a0 * y) / (c0 * y + d0);
vec3 l0 = (a0 / d0) * y;
g0 = compare(y, l0, g0);
float x1 = p1.x;
float y1 = p1.y;
float s1 = p1.z;
float a1 = (1.f-x1)*(1.f-x1);
float c1 = (1.f-x1) - (1.f-y1)/s1;
float d1 = (1.f-y1)*(1.f-y1)/s1;
vec3 g1 = 1.f - (a1 * (1.f - y)) / (c1 * (1.f - y) + d1);
vec3 l1 = 1.f - (a1 / d1) * (1.f - y);
g1 = compare(1.f - y, l1, g1);
g = compare(y - p0.y, g0, compare(y - p1.y, g, g1));
im.rgb = mix(im.rgb, g, boost);
return im;
This is an abstract method! Subclass '%@' should provide concrete implementation
+[PIFakeBoost kernelFB0]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Adjustments/PIFakeBoost.m
+[PIFakeBoost kernelFB3]
properties != nil
+[PIFakeBoost boostParametersFromRawProperties:]
-[PIFakeBoost outputImageFB0]
-[PIFakeBoost outputImageFB3]
Invalid boost parameters
Invalid points parameters
boost kernel is nil
+[PIForwardFakeBoost kernelFB0]_block_invoke
+[PIForwardFakeBoost kernelFB3]_block_invoke
inverse boost kernel is nil
+[PIInverseFakeBoost kernelFB0]_block_invoke
+[PIInverseFakeBoost kernelFB3]_block_invoke
AutoEnhance
AutoLoop
Crop
DepthEffect
Effect
Effect3D
Grain
HighResolutionFusion
LivePhotoKeyFrame
Mute
PortraitEffect
PortraitVideo
RedEyeBB
SelectiveColor
SemanticEnhance
SlowMotion
SmartBlackAndWhite
SmartColor
SmartTone
SourceSelect
Trim
VideoCrossfadeLoop
VideoPosterFrame
VideoStabilize
WhiteBalance
compositionName
identifierName
custom
customSerialization
customCompositionName
customIdentifierName
autoValue
requiresEnabled
offsetBlackPoint
v24@?0@"NSDictionary"8@"NUAdjustment"16
v32@?0@8@16^B24
inputColor
offsetSaturation
offsetCast
offsetStrength
offsetGrain
offsetTone
inputBlackAndWhite
offsetNeutralGamma
amount
seed
DGVignetteEffectOperation
Vignette
inputIntensity
inputRadius
inputFalloff
DGDefinition2Operation
Definition
RKNoiseReductionOperation
NoiseReduction
edgeDetail
RKProSharpenOperation
Sharpen
inputEdgeScale
edges
inputSharpness
@"NSString"16@?0@"NSDictionary"8
effectName
@"NSString"24@?0@"NSDictionary"8@"NUAdjustment"16
effectVersion
effectIntensity
CropStraighten
straightenAngle
DGRAWReduceNoiseOperation
RawNoiseReduction
inputDetailAmount
detail
inputCNRAmount
inputLNRAmount
RKRawDecodeOperation
inputMethodVersion
colorComponents
slope
bias
RKLevelsOperation
Levels
colorSpace
inputColorSpace
blackSrcRGB
blackDstRGB
shadowSrcRGB
shadowDstRGB
midSrcRGB
midDstRGB
hilightSrcRGB
hilightDstRGB
whiteSrcRGB
whiteDstRGB
blackSrcRed
blackDstRed
shadowSrcRed
shadowDstRed
midSrcRed
midDstRed
hilightSrcRed
hilightDstRed
whiteSrcRed
whiteDstRed
blackSrcGreen
blackDstGreen
shadowSrcGreen
shadowDstGreen
midSrcGreen
midDstGreen
hilightSrcGreen
hilightDstGreen
whiteSrcGreen
whiteDstGreen
blackSrcBlue
blackDstBlue
shadowSrcBlue
shadowDstBlue
midSrcBlue
midDstBlue
hilightSrcBlue
hilightDstBlue
whiteSrcBlue
whiteDstBlue
Generic P3
Display P3
Adobe RGB
sRGB
RKCurvesOperation
Curves
RGBCurvePoints
pointsL
redCurvePoints
pointsR
greenCurvePoints
pointsG
blueCurvePoints
pointsB
RKSelectiveColorOperation
B16@?0@"NSDictionary"8
corrections
RKRetouchOperation
Retouch
inputStrokes
hasSource
mode
brushStroke
softness
opacity
repairEdges
sourceOffset
data
xLocation
yLocation
pressure
points
clipRect
detectedFaces
RKRedEyeOperation
ApertureRedEye
inputSpots
pointX
center
pointY
sensitivity
RedEye
allCorrections
redEyeCorrections
autoRedEyeCorrections
endTime
regions
timeRange
rate
timescale
alignment
glassesMatteAllowed
portraitEffectFilterName
keyframes
cropFraction
analysisType
v24@?0@"NSMutableDictionary"8@"NUAdjustment"16
face
warm
inputDetectedFaces
flags
epoch
lightMap
lightMapAvg
vec4 _curve_sample_HDR(float x, sampler2D table, vec2 domain, vec2 normalizer) { x = (x - domain.x) / (domain.y - domain.x); x = clamp(x, 0.0001, 0.9999); x = normalizer.x * x + normalizer.y; return texture2D(table, vec2(x, 0.5)); } kernel vec4 _curves_rgb_lum_HDR(__sample color, sampler2D table, vec2 domain, vec2 normalizer) { vec4 pixel = color; pixel.r = _curve_sample_HDR(pixel.r, table, domain, normalizer).r; pixel.g = _curve_sample_HDR(pixel.g, table, domain, normalizer).g; pixel.b = _curve_sample_HDR(pixel.b, table, domain, normalizer).b; float lum0 = dot(pixel.rgb, vec3(0.3, 0.59, 0.11)); float lum1 = _curve_sample_HDR(lum0, table, domain, normalizer).a; float lum1c = clamp(lum1, -8.0 * abs(lum0), 8.0 * abs(lum0)); float lum_scale = (lum0 == 0.0 ? 0.0 : lum1c / lum0); float lum_offset = lum1 - lum1c; pixel.rgb = lum_scale * pixel.rgb + lum_offset; pixel.rgb = min(pixel.rgb, 1.0); return pixel; }
PILongExposureAccumulator.state
PILongExposureAccumulator.accum
Accumulation was cancelled
PILongExposureAccumulator-main-j%lld
failed to init accumulator
B16@?0@"CIRenderDestination"8
frame != nil
-[PILongExposureAccumulator accumulate:error:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/AutoLoop/PIAutoLoopJob+LongExposure.m
-[PILongExposureAccumulator _accumulate:error:]
PILongExposureAccumulator-average-j%lld
failed to render average image
v16@?0@"CIImage"8
CIBoxBlur
PILongExposureAccumulator-minimum-j%lld
failed to render minimum image
PILongExposureAccumulator-maximum-j%lld
failed to render maximum image
-[PILongExposureAccumulator writeLongExposureImage:UTI:colorSpace:error:]
-[PILongExposureAccumulator writeMaskImage:UTI:error:]
-[PILongExposureAccumulator _exportOutputImage:format:colorSpace:toURL:uti:error:]
destinationURL != nil
Failed to finalize image destination
Failed to create CGImageDestinationRef
Failed to create CGImageRef
/AutoLoop/LongExposure
-[PILongExposureRegistrationJob prepare:]
return Source(composition.source, {'skipOrientation':true});
Malformed AutoLoop recipe : crop
-[PILongExposureRegistrationJob render:]
Failed to allocate intermediate pixel buffer
PILongExposureRegistrationJob-render-j%lld
PILongExposureRegistrationJob-reference-j%lld
Failed to render luma
Image registration failure (expected 1 observation)
/RAW/SushiLevel1
/Master%@
CIRedEyeCorrection
inputCameraModel
CILanczosScaleTransform
1.5.1
1.9.1
computed
overcaptureComputed
dynamic
@"NSString"16@?0@"NUAdjustment"8
SDOFRenderingVersion
convexHull
/private/var/mobile/Media/PhotoData/CaptureDebug/
Repair
Clone
RepairML
monochrome
assetURL
-[PIPortraitVideoDebugDetectionsRenderNode resolvedNodeWithCachedInputs:settings:pipelineState:error:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Render/PIPortraitVideoDebugDetectionsRenderNode.m
v20@?0B8@"NSError"12
%@-labelImageCache
-[PIPortraitVideoDebugDetectionsRenderNode _evaluateImage:]
Face
Head
Torso
Sports Ball
AutoFocus
%@ %@
%@ G:%@
Helvetica
outputExposure
falseColorHDR
inputRAWGamutMapMax
Buffer must be RGBA16 type for red eye repairs
id<NUBuffer> computeRepairMask(__strong id<NUBuffer>, uint16_t *)
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Adjustments/ApertureRedEye/PIApertureRedEye.mm
reddestx out of bounds
void seedFill(__strong id<NUMutableBuffer>, __strong id<NUBuffer>, const uint32_t, const NSInteger, const NSInteger)
reddesty out of bounds
void applyRepairMask(__strong id<NUMutableBuffer>, __strong id<NUBuffer>, double, const uint32_t)
repairBuffer != nil
kernel vec4 facebalance(__sample src, vec2 gamma, vec3 matchMinusOrigStrength)
vec4 pix = src;
pix.rgb = pow(max(pix.rgb, 0.0), vec3(gamma.x));
pix.rgb = pix.r * vec3(0.299,  0.595716,  0.211456) +
pix.g * vec3(0.587, -0.274453, -0.522591) +
pix.b * vec3(0.114, -0.321263,  0.311135);
vec4 orig = pix ;
float chroma = min(1.0, 2.0*sqrt(pix.g*pix.g + pix.b*pix.b) );
pix.gb +=  matchMinusOrigStrength.rg;
float strength = matchMinusOrigStrength.z*pow(chroma, .4) ;
pix.gb = mix(orig.gb, pix.gb, strength) ;
pix.rgb = pix.rrr +
pix.g * vec3(0.956296, -0.272122, -1.10699) +
pix.b * vec3(0.621024, -0.647381, 1.70461);
pix.rgb = max(pix.rgb, vec3(0.0));
pix.rgb = pow(pix.rgb, vec3(gamma.y));
return pix;
facebalance
vec4 curve_sample(float x, sampler2D table, vec2 domain, vec2 normalizer)
x = (x - domain.x) / (domain.y - domain.x);
x = clamp(x, 0.0001, 0.9999);
x = normalizer.x * x + normalizer.y;
return texture2D(table, vec2(x, 0.5));
kernel vec4 curves_rgb_lum(__sample color, sampler2D table, vec2 domain, vec2 normalizer)
vec4 pixel = color;
pixel.r = curve_sample(pixel.r, table, domain, normalizer).r;
pixel.g = curve_sample(pixel.g, table, domain, normalizer).g;
pixel.b = curve_sample(pixel.b, table, domain, normalizer).b;
float lum0 = dot(pixel.rgb, vec3(0.3, 0.59, 0.11));
float lum1 = curve_sample(lum0, table, domain, normalizer).a;
float lum1c = clamp(lum1, -8.0 * abs(lum0), 8.0 * abs(lum0));
float lum_scale = (lum0 == 0.0 ? 0.0 : lum1c / lum0);
float lum_offset = lum1 - lum1c;
pixel.rgb = lum_scale * pixel.rgb + lum_offset;
return pixel;
tableR != nil
+[PICurvesLUTFilter tableImageFromRed:green:blue:luminance:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Adjustments/PICurvesFilter.mm
tableG != nil
tableB != nil
tableL != nil
kernel vec4 gHDRtoPP(sampler image)
vec4 pix ;
vec3 pix2;
pix = sample(image, samplerCoord(image));
pix2 = pix.r * vec3(0.615429622407401,   0.114831839141528,   0.011544126697221) +
pix.g * vec3(0.367479646665836,   0.797943554457996,   0.064077744191180) +
pix.b * vec3(  0.016956659608091,   0.087783443422360,   0.924405601458102);
return vec4(pix2, pix.a);
kernel vec4 PPtogHDR(sampler image)
vec4 pix ;
vec3 pix2;
pix = sample(image, samplerCoord(image));
pix2 = pix.r * vec3(1.777445503202045,  -0.255296595099306,  -0.004500433755654) +
pix.g * vec3( -0.822224875430495,   1.380948853784730,  -0.085456231694984) +
pix.b * vec3(0.045475917061484,  -0.126454737973025,   1.089973874037625);
return vec4(pix2, pix.a);
kernel vec4 colorBalance(__sample image, float colorI, float colorQ, float str, vec2 gamma)
vec4 pix = image;
vec3 neg = min(pix.rgb, 0.0);
vec3 pos = max(pix.rgb, 1.0) - 1.0;
pix.rgb = pow(clamp(pix.rgb, 0.0, 1.0), vec3(gamma.x));
pix.rgb = pix.r * vec3(0.299,  0.595716,  0.211456) +
pix.g * vec3(0.587, -0.274453, -0.522591) +
pix.b * vec3(0.114, -0.321263,  0.311135);
vec4 orig = pix ;
float chroma = min(1.0, 2.0*sqrt(pix.g*pix.g + pix.b*pix.b) );
pix.gb += vec2(colorI,colorQ);
float strength = str*pow(chroma, .4) ;
pix.gb = mix(orig.gb, pix.gb, strength) ;
pix.rgb = pix.rrr +
pix.g * vec3(0.956296, -0.272122, -1.10699) +
pix.b * vec3(0.621024, -0.647381, 1.70461);
pix.rgb = pow(max(pix.rgb, 0.0), vec3(gamma.y));
pix.rgb += pos + neg;
return pix;
gHDRtoPP
PPtogHDR
colorBalance
-[PIColorBalanceFilter outputImage]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Adjustments/PIColorBalanceFilter.m
@"NSString"16@?0@"NSString"8
CIMix
CIGammaAdjust
CIExposureAdjust
CIProSharpenEdges
CIHighKey
CILocalContrast
PILocalContrastHDR
CILocalLightFilter
CILocalLightMapPrepare
CIPhotoGrain
CISmartBlackAndWhite
CIVignetteEffect
CILocalLight
CISmartColor
CISmartTone
PIFalseColorHDRDebug
PIColorBalanceFilter
PIRAWFaceBalance
PINeutralGrayWhiteBalanceFilter
PIBilateralFilter
PICurvesLUTFilter
PICurvesFilter
PISelectiveColorFilter
PILevelsFilter
PIGlobalSettings
IPXRootSettings
PXSettingsArchiveKey
editSettings
PURootSettings
photoEditingSettings
debugDecoratorFiltersEnabled
PI_AUTOLOOP_EXPORT_USE_METAL
B8@?0
-[PIAutoLoopExportJob initWithVideoExportRequest:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/AutoLoop/PIAutoLoopJob+Export.m
@"NUJSRenderNode"24@?0@"NUJSRenderNode"8@"JSValue"16
Invalid data type for adjustmentValue
Invalid data type for keyframes
Invalid data type for stabCropRect
input to VideoReframe cannot be nil
Unable to unwrap the input node!
-[PIJSRenderPipeline setUpContext:]_block_invoke
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Pipeline/PIPhotosPipeline.m
Missing duration for crossfade
input to VideoCrossfadeLoop cannot be nil
{CGRect={CGPoint=dd}{CGSize=dd}}64@?0{CGSize=dd}8{CGSize=dd}24{CGSize=dd}40@"JSValue"56
OvercaptureRectForAutoCrop
Couldn't find bundle for class %@
+[NUJSRenderPipeline(PhotosPipeline) newPhotosPipeline:]
PhotosPipeline
Couldn't find the specified Pipeline
JSContext
Class getJSContextClass(void)_block_invoke
JavaScriptCoreSoftLinking.h
Unable to find class %s
void *JavaScriptCoreLibrary(void)
/System/Library/Frameworks/JavaScriptCore.framework/JavaScriptCore
/System/Library/Frameworks/JavaScriptCore.framework/Contents/MacOS/JavaScriptCore
-[PIApertureRedEyeAutoCalculator submit:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Autocalculators/PIApertureRedEyeAutoCalculator.m
PIApertureRedEyeAutoCalculator-faceDetection
pre-Adjustments
redEyeSpots
PILocalLightHDR-stats
lightMapWidth
lightMapHeight
NSNumber
inputSmartShadows
kernel vec4 _shadowKernelHDR(__sample im, __sample adj, float str) { adj.r = 3.4*adj.r-1.2; vec3 neg = min(im.rgb, 0.0); vec3 pos = max(im.rgb, 1.0)-1.0; im.rgb = clamp(im.rgb, 0.0, 1.0); vec4 orig = im; float y = sqrt(dot(im.rgb, vec3(.33333))); float s = mix(0.0, adj.r, str); vec3 gain = s > 0.0 ? vec3(0.0) : vec3(s*s) * vec3(-2.75, -2.75, -2.5); im.rgb = im.rgb*im.rgb*gain + im.rgb*(1.0-gain); float m = 1.0 + 1.85*s*(max(0.1-y, 0.0)) ; im.rgb = (clamp(m*im.rgb, 0.0, 1.0)); float midAmt = s < 0.0 ? min(s*s,1.0) : 0.0; y = y*(1.0-y); im.rgb = sqrt(im.rgb); float pivot = .4; float a = midAmt*y; float b = -pivot*a; vec3 pix = im.r * vec3(0.299*a) + 
 im.g * vec3(0.587*a) + 
 im.b * vec3(0.114*a) + 
 im.rgb + vec3(b); im.rgb = mix(im.rgb, vec3(pivot), -y*midAmt); im.rgb = mix(im.rgb, pix, 0.8); im.rgb = max(im.rgb, 0.0); im.rgb *= im.rgb; im.rgb = clamp(im.rgb, 0.0,1.0)+pos+neg; return im; }
kernel vec4 _polyKernelHDR(__sample im, __sample adj, float str) { adj.r = 3.4*adj.r-1.2; vec3 neg = min(im.rgb, 0.0); vec3 pos = max(im.rgb, 1.0)-1.0; im.rgb = clamp(im.rgb, 0.0, 1.0); vec4 orig = im; float y = sqrt(dot(im.rgb, vec3(.33333))); float s = mix(0.0, adj.r, str); vec3 gain = s > 0.0 ? vec3(1.5*s) : vec3(1.75*s, 1.75*s, 1.55*s); im.rgb = im.rgb*im.rgb*gain + im.rgb*(1.0-gain); im.rgb = (clamp(im.rgb, 0.0, 1.0)); float midAmt = min(str, .5); y = y*(1.0-y); im.rgb = sqrt(im.rgb); float pivot = max(adj.g, 0.5); float a = midAmt*y; float b = -pivot*a; vec3 pix = im.r * vec3(0.299*a) + 
 im.g * vec3(0.587*a) + 
 im.b * vec3(0.114*a) + 
 im.rgb + vec3(b); 
 im.rgb = mix(im.rgb, vec3(pivot), -y*midAmt); im.rgb = mix(im.rgb, pix, 0.8); im.rgb = max(im.rgb, 0.0); im.rgb *= im.rgb; im.rgb = clamp(im.rgb, 0.0,1.0)+pos+neg; return im; }
-[PILocalLightFilterHDR outputImage]
PILocalLightHDR.m
CGRectEqualToRect([guideImage extent], [inputImage extent])
inputImage != nil
guideImage != nil
inputLocalLight != nil
CGRectEqualToRect([lightMapImage extent], [inputImage extent])
PILocalLightHDR
_lightMapImageFromData_block_invoke
x == 0
y == 0
width == lmWidth
height == lmHeight
CIEdgePreserveUpsampleRGFilter
v16@?0@"<NUMutableBuffer>"8
v24@?0@"<NUBufferTile>"8^B16
-[PISmartBlackAndWhiteAutoCalculator submit:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Autocalculators/PISmartBlackAndWhiteAutoCalculator.mm
PISmartBlackAndWhiteAutoCalculator-renderInputImage
Produced invalid BlackAndWhite settings, using defaults
void calculate_bw_auto_matrix_lum_contrast(__strong id<NUBuffer>, CGColorSpaceRef, MsgBlackAndWhiteSettings *)
allocator<T>::allocate(size_t n) 'n' exceeds maximum supported size
num == w
void MsgMatrix<double>::AppendRow(const NumberType *, uint) [MatrixFloatType = double, NumberType = double]
xi < w && yi < h
MatrixFloatType &MsgMatrix<double>::operator()(size_t, size_t) [MatrixFloatType = double]
index < data.size()
inputScaleFactor
kernel vec4 _smartBlackAndWhiteHDR(__sample imageHDR, sampler2D hueImage, vec4 rgbWeights, vec4 normalizer) { float hueTableScaleFactor = rgbWeights.w; float hueImageWidth = normalizer.x; float huePixelCenter = normalizer.y; float neutralGamma = normalizer.z; float phototone = normalizer.w; float bw = dot(imageHDR.rgb / 12.0, rgbWeights.rgb); bw = clamp(bw, 0.0, 1.0); vec3 lms; lms.x = dot(imageHDR.rgb, vec3(0.3139902162, 0.6395129383, 0.0464975462)); lms.y = dot(imageHDR.rgb, vec3(0.155372406, 0.7578944616, 0.0867014186)); lms.z = dot(imageHDR.rgb, vec3(0.017752387, 0.109442094, 0.8725692246)); lms = pow(lms, vec3(0.43)); float i = dot(lms, vec3(0.4,0.4,0.2)); float p = dot(lms, vec3(4.4550,-4.8510,0.3960)); float t = dot(lms, vec3(0.8056,0.3572,-1.1628)); float chroma = sqrt(p*p+t*t); float hue = 0.5 + (atan(t, p) / 6.28318530718); vec2 huePt = vec2(hue * hueImageWidth + huePixelCenter, 0.5); float hueGamma = hueTableScaleFactor * texture2D(hueImage, huePt).a; float cd = 0.06 + 0.53 * abs(i-0.5); float lowSaturationDamp = smoothstep(0.0, 1.0, (chroma)/cd); float intensityDamp = smoothstep(0.0, 1.0, 1.0 - i); float lowLuminosityDamp = smoothstep(0.0, 1.0, 25.0 * i); float hWeight = lowSaturationDamp * intensityDamp * lowLuminosityDamp; hueGamma -= 1; hueGamma *= hWeight; hueGamma += 1; bw = pow(bw, hueGamma); float bwSDR = clamp(bw * 12.0, 0.0, 1.0); float midLumWeight = bwSDR*(1.0 - bwSDR); float grayWeight = 1.0 - smoothstep(0.0, 1.0, chroma * 10.0); float nWeight = midLumWeight * grayWeight; neutralGamma -= 1; neutralGamma *= nWeight; neutralGamma *= -2; neutralGamma += 1; bw = pow(bw, neutralGamma); bw = bw * 12.0; bw = clamp(bw, 0.0, 12.0); float df0 = 0.812379; float result; if (bw < df0) { result = 1.8031*bw*bw*bw - 2.1972*bw*bw + 1.3823*bw; } else { float scale = 12.0 - df0; float x = (bw - df0) / scale; result = 1.8031*x*x*x - 2.1972*x*x + 1.3823*x; result = result * scale + df0; result -= 0.158305860; } bw = mix(bw, result,-phototone); return vec4(bw,bw,bw,imageHDR.a); }
PIPhotoGrainHDR
inputISO
There is no need to call smartBlackAndWhiteStatistics.
Just use [CIFilter filterWithName:@"PISmartBlackAndWhiteHDR"] instead.
There is no need to call smartBlackAndWhiteAdjustmentsForValue:andStatistics:.
-[PILevelsAutoCalculator submit:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Autocalculators/PILevelsAutoCalculator.m
PILevelsAutoCalculator-histogram
pre-Levels
-[PILevelsAutoCalculator calculateSettingsForImageHistogram:]
blackSrc
blackDst
shadowSrc
shadowDst
midSrc
midDst
hilightSrc
hilightDst
whiteSrc
whiteDst
Adjustment controller for key %@ is of class: %@, but was expected to be %@
-[PICompositionController(AdjustmentExtensions) _adjustmentControllerForKey:creatingIfNecessary:expectedClass:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Controllers/PICompositionController+AdjustmentExtensions.m
<%@:%p> [(%.3f, %.3f), %s]
editable
not editable
Warning smartToneStatistics will soon need [receiver properties] be non-nil so flash-fired state can be determined.
tonalRange
highKey
blackPoint
whitePoint
kernel vec4 _smarttone_brightness_neg_HDR (__sample c, float gamma) 
 vec3 neg = min(c.rgb, 0.0); 
 c.rgb = max(c.rgb, 0.0); 
 vec3 pix = pow(c.rgb, vec3(gamma)); 
 float lum = dot(c.rgb, vec3(0.39, .5, .11)); 
 vec3 pix2 = lum>0.0 ? c.rgb*pow(lum, gamma)/lum : vec3(0.0); 
 pix = mix(pix, pix2, 0.8) + neg; 
 return vec4(pix, c.a); 
kernel vec4 _smarttone_brightness_pos_HDR (__sample c, float gamma) 
 vec3 neg = min(c.rgb, 0.0); 
 vec3 pos = max(c.rgb, 1.0)-1.0; 
 c.rgb = clamp(c.rgb, 0.0, 1.0); 
 vec3 m = 1.0-c.rgb; 
 float a = 0.6; 
 vec4 result = c; 
 result.rgb = 1.0 - (pow(m, vec3(gamma))+a*( ((gamma-1.0)*m*(1.0-m*m))/(gamma*gamma))); 
 c.rgb = pow(c.rgb, vec3(1.0-((min(gamma, 2.95)-1.0)/2.6))); 
 result.rgb = mix(c.rgb, result.rgb, .85); 
 result.rgb = result.rgb+neg+pos; 
 return result; 
kernel vec4 _smarttone_contrast_HDR (__sample im, float midAmt) 
 vec3 neg = min(im.rgb, 0.0); 
 vec3 pos = max(im.rgb, 1.0)-1.0; 
 im.rgb = clamp(im.rgb, 0.0, 1.0); 
 float y = dot(im.rgb, vec3(0.3333)); 
 y = sqrt(y); 
 float sat = (im.r-y)*(im.r-y)+(im.g-y)*(im.g-y)+(im.b-y)*(im.b-y); 
 y = y*(1.0-y); 
 im.rgb = sqrt(im.rgb); 
 float a = midAmt*y; 
 float b = -0.5*a; 
 vec3 pix = im.r * vec3(0.299*a) + 
 im.g * vec3(0.587*a) + 
 im.b * vec3(0.114*a) + 
 im.rgb + vec3(b); 
 im.rgb = mix(im.rgb, vec3(0.5), -y*midAmt); 
 im.rgb = mix(im.rgb, pix, 0.8+sat); 
 im.rgb = max(im.rgb, 0.0); 
 im.rgb *= im.rgb; 
 im.rgb = im.rgb + neg + pos; 
 return im; 
kernel vec4 _smarttone_contrast_HDR (__sample im, float midAmt, float maxVal) 
 midAmt = midAmt / maxVal; 
 vec3 neg = min(im.rgb, 0.0); 
 vec3 pos = max(im.rgb, maxVal) - maxVal; 
 im.rgb = clamp(im.rgb, 0.0, maxVal); 
 float y = dot(im.rgb, vec3(0.3333)); 
 y = sqrt(y); 
 im.rgb = sqrt(im.rgb); 
 float sat = dot(im.rgb - vec3(y), im.rgb - vec3(y)) / sqrt(maxVal); 
 y = y*(sqrt(maxVal)-y); 
 float a = midAmt*y; 
 float b = -0.5*a; 
 vec3 pix = im.r * vec3(0.299*a) + 
 im.g * vec3(0.587*a) + 
 im.b * vec3(0.114*a) + 
 im.rgb + vec3(b); 
 im.rgb = mix(im.rgb, vec3(0.5), -a); 
 im.rgb = mix(im.rgb, pix, clamp(0.8+sat, 0.0, 1.0)); 
 im.rgb = max(im.rgb, 0.0); 
 im.rgb *= im.rgb; 
 im.rgb = im.rgb + neg + pos; 
 return im; 
kernel vec4 _smarttone_highlightcontrast_HDR (__sample pix, float highAmt, float sat) 
 float maxVal = 1.0; vec3 neg = min(pix.rgb, 0.0); 
 vec3 pos = max(pix.rgb, maxVal) - maxVal; 
 pix.rgb = clamp(pix.rgb, 0.0, maxVal); 
 float lum = clamp(dot(pix.rgb, vec3(.3333)),0.0,1.0); 
 vec3 high = pow(pix.rgb, vec3(3.0 - 2.0*highAmt)); 
 float pivot = 0.8; 
 vec3 pix1 = (high - pivot)*(4.0 - 3.0*highAmt) + pivot; 
 float h = highAmt*highAmt*highAmt*highAmt; 
 float a = (4.0 - 3.0*h); 
 vec3 pix2 = (lum-pivot)*a+pivot + high.rgb -lum; 
 high = mix(pix2, pix1, sat); 
 pix.rgb = mix(pix.rgb, high, lum*lum); 
 pix.rgb = pix.rgb + neg + pos; 
 return pix; 
kernel vec4 _rawHighlightsHDR(__sample pix, float gain) 
 vec3 high = gain*pix.rgb; 
 float lum = clamp(dot(pix.rgb, vec3(.3333)), 0.0, 1.0); 
 vec3 neg = min(high, 0.0); 
 high.rgb = mix(max(pix.rgb, 0.0), high.rgb, lum*lum) + neg; 
 return vec4(high, pix.a); 
CIHighlightShadowAdjust
inputShadowAmount
inputHighlightAmount
PISmartToneFilterHDR-histogram
PI_LONG_EXPOSURE_FUSION_PARAMS
@"NSString"8@?0
kNCCBlurHalfSize
kNCCEdge0
kNCCEdge1
kBlendMaskThreshold0
kBlendMaskThreshold1
PI_LONG_EXPOSURE_DUMP_INTERMEDIATES
long-exp-input-image.tiff
long-exp-mask-image.tiff
long-exp-still-image.tiff
long-exp-guide-image.tiff
long-exp-ncc-map-image.tiff
long-exp-refined-mask-image.tiff
long-exp-fusion-image.tiff
composition != nil
-[PIModernPhotosPipeline _processedRenderNodeForComposition:input:pipelineState:error:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Pipeline/PIModernPhotosPipeline.m
pipelineState != nil
LongExposure
inputSushiLevel
kCGImageSourceShouldExtendRaw
inputGamutMapMax
inputNoiseReductionDetailAmount
inputUIColorNoiseReductionAmount
inputUILuminanceNoiseReductionAmount
sharpness
inputNoiseReductionSharpnessAmount
contrast
inputNoiseReductionContrastAmount
inputNeutralTemperature
inputNeutralTint
Video
mediaComponentType
hardCropCleanAperture
defaultFrameTime
skipOrientation
Master
unsupported sourceSelect adjustment
RAW/Linear
Source
ShowOriginalSource
PIForwardFakeBoost
inputBoost
inputVersion
inputParams
Intermediate
keepCacheWhenAtOneToOne
Disparity
Image
Source does not contain depth
PortraitEffectsMatte
HairSegmentationMatte
SkinSegmentationMatte
TeethSegmentationMatte
GlassesSegmentationMatte
MeteorPlusGainMap
focusRect
faces
inputAperture
inputFocusRect
leftEyeX
leftEyeY
rightEyeX
rightEyeY
noseX
noseY
chinX
chinY
inputLeftEyePosition
inputRightEyePosition
inputFaceMidPoint
inputChinPosition
Missing required depth settings
inputShiftmapImage
inputCalibrationData
inputAuxDataMetadata
inputOriginalSize
CIDepthEffectMakeBlurMap
inputMatteImage
inputHairImage
inputGlassesImage
inputPower
inputEV
faceLandmarks
inputFaceLandmarkArray
inputDisparity
inputMatte
inputBlurMap
inputFaceMask
inputHairMask
inputTeethMask
CIPortraitEffectV2
inputGenerateSpillMatte
inputFullSizeImage
CIPortraitEffect%@
PortraitV2-zeroStrength
PortraitV2
pre-PortraitVideo
auxiliaryImageType
nativeScale
post-PortraitVideo
inputLumaNoiseScale
lumaNoiseScale
shape
inputShape
CIDepthEffectApplyBlurMap
inputGainMap
masterSpace
pre-WB
inputWarmTemp
inputWarmTint
inputHasFace
inputIsRaw
inputOrigI
inputOrigQ
inputWarmth
warmth
pre-Mute
pre-LivePhotoKeyFrame
pre-Trim
pre-SloMo
SloMo
inputConfidence
inputFaceBoxArray
CIDynamicFood
inputBoundingBoxArray
sunsetOrSunrise
CIDynamicRender
Unknown sceneLabel when rendering semantic enhance adjustment
inputTargetImage
inputTime
inputGuideImage
inputLightMapImage
inputSaturation
pre-Curves
inputPointsR
inputPointsG
inputPointsB
inputPointsL
inputTableImage
green
blue
spread
hueShift
saturation
inputCorrections
CIPhotoEffect%@
filterVersion
pre-VideoReframe
pre-VideoStabilize
pre-VideoCrossfadeLoop
pre-Geometry
pre-Crop
perspectiveStraighten
resetCleanAperture
{?={?=qq}{?=qq}}
projectUsingCleanAperture
{?=qq}
projectUsingOriginalSize
com.apple.quicktime.live-photo.vitality-disabled
projectUsingEstimatedCleanAperture
pre-Orientation
post-Geometry
inputCenter
post-Adjustments
PIInverseFakeBoost
Master/RAW/Linear
inputCutoff
kernel vec4 levelsNewGammaForP3 (sampler src, sampler LUT)
vec4
p,r;
vec2
c1,c2;
float
p  = sample(src, samplerCoord(src));
vec3 neg = min(p.rgb, 0.0);
vec3 pos = max(p.rgb, 1.0)-1.0;
p.rgb = clamp(p.rgb, 0.0, 1.0);
f = p.r * 255.0 + 256.0;
c1 = vec2(floor(f)+0.5, 0.5);
c2 = vec2(ceil(f)+0.5, 0.5);
r = mix(sample(LUT,samplerTransform(LUT,c1)), sample(LUT,samplerTransform(LUT,c2)), fract(f));
p.r = r.r * 4.0 - 1.0;
f = p.g * 255.0 + 256.0;
c1 = vec2(floor(f)+0.5, 0.5);
c2 = vec2(ceil(f)+0.5, 0.5);
r = mix(sample(LUT,samplerTransform(LUT,c1)), sample(LUT,samplerTransform(LUT,c2)), fract(f));
p.g = r.g * 4.0 - 1.0;
f = p.b * 255.0 + 256.0;
c1 = vec2(floor(f)+0.5, 0.5);
c2 = vec2(ceil(f)+0.5, 0.5);
r = mix(sample(LUT,samplerTransform(LUT,c1)), sample(LUT,samplerTransform(LUT,c2)), fract(f));
p.b = r.b * 4.0 - 1.0;
p.rgb = max(p.rgb, 0.0);
p.rgb = p.rgb + pos + neg;
return p;
-[PILevelsFilter _LUTImage]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Adjustments/PILevelsFilter.m
Mirror
Adjustment
SourceSelect~1.0
enum
values
Failed to register schema %@: %@
+[PISchema sourceSelectSchema]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Pipeline/PISchema.m
RAW~1.0
opaque
required
+[PISchema rawSchema]
RawNoiseReduction~1.0
bool
default
number
minimum
maximum
ui_minimum
ui_maximum
+[PISchema rawNoiseReductionSchema]
SmartTone~1.0
+[PISchema smartToneSchema]
SmartColor~1.0
+[PISchema smartColorSchema]
SmartBlackAndWhite~1.0
+[PISchema smartBlackAndWhiteSchema]
Grain~1.0
+[PISchema grainSchema]
Sharpen~1.0
+[PISchema sharpenSchema]
CropStraighten~1.0
+[PISchema cropSchema]
Trim~1.0
+[PISchema trimSchema]
SlowMotion~1.0
+[PISchema slomoSchema]
LivePhotoKeyFrame~1.0
+[PISchema livePhotoKeyFrameSchema]
Mute~1.0
+[PISchema muteSchema]
VideoPosterFrame~1.0
+[PISchema videoPosterFrameSchema]
AutoLoop~1.0
+[PISchema autoLoopSchema]
HighResolutionFusion~1.0
+[PISchema highResFusionSchema]
DepthEffect~1.0
+[PISchema depthEffectSchema]
Effect3D~1.0
+[PISchema effect3DSchema]
PortraitEffect~1.0
+[PISchema portraitEffectSchema]
+[PISchema effectSchema]
iPhone
+[PISchema redEyeSchema]
array
content
compound
properties
+[PISchema apertureRedEyeSchema]
+[PISchema retouchSchema]
+[PISchema vignetteSchema]
Orientation~1.0
+[PISchema orientationSchema]
+[PISchema definitionSchema]
+[PISchema noiseReductionSchema]
grayStrength
identity
+[PISchema whiteBalanceSchema]
+[PISchema levelsSchema]
+[PISchema curvesSchema]
+[PISchema selectiveColorSchema]
VideoReframe~1.0
+[PISchema videoReframeSchema]
VideoStabilize~1.0
pixel
gyro
+[PISchema videoStabilizeSchema]
VideoCrossfadeLoop~1.0
+[PISchema videoCrossfadeLoopSchema]
Debug
+[PISchema debugSchema]
SemanticEnhance~1.0
+[PISchema semanticEnhance]
PortraitVideo~1.0
+[PISchema portraitVideoSchema]
Composition
PhotosComposition
contents
com.apple.photo:Source~1.0
Retouch~1.0
WhiteBalance~1.0
RedEye~1.0
ApertureRedEye~1.0
Definition~1.0
Effect~1.0
Vignette~1.0
NoiseReduction~1.0
Levels~1.0
Curves~1.0
SelectiveColor~1.0
Debug~1.0
+[PISchema photosCompositionSchema]
failed to register %@: %@
+[PISchema registerPhotosSchema]
com.apple.mobileslideshow.reframe.photo.eligible-pass
com.apple.mobileslideshow.reframe.photo.eligible-fail
com.apple.mobileslideshow.reframe.video.eligible-pass
com.apple.mobileslideshow.reframe.video.eligible-fail
com.apple.mobileslideshow.reframe.type.photo.horizon
com.apple.mobileslideshow.reframe.type.photo.perspective
PICompositionFinalizer-primaryImageProperties
PICompositionFinalizer-overcaptureImageProperties
v16@?0@"PIAdjustmentController"8
maxCameraAutoStraightenCorrection
Capture
maxCameraVerticalPerspectiveCorrection
maxCameraHorizontalPerspectiveCorrection
maxCameraRotatePerspectiveCorrection
minHorizontalCameraPerspectiveCorrection
minVerticalCameraPerspectiveCorrection
minRotateCameraPerspectiveCorrection
enable_perspective_correction
reframe
perpsective
horizon
PICropAutoCalculator-imageProperties
-[PICropAutoCalculator undoExifOrientation:error:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Autocalculators/PICropAutoCalculator.m
Source geometry has 0 size
-[PICropAutoCalculator submit:]
PICropAutoCalculator-faceDetection
{CGRect={CGPoint=dd}{CGSize=dd}}
maxAutoStraighten
filterOptions
CIAutoStraighten
straightenAngleInDegreesCCW
limitExceeded
belowMinimum
autoCrop
result
error
%@AutoCropEvaluation-txt.DBG
v16@?0@"PISourceSelectAdjustmentController"8
v16@?0@"PIAutoLoopAdjustmentController"8
PICropAutoCalculator-stitchedOvercaptureRect
+[PICropAutoCalculator(Utilities) updateCropAdjustment:after:error:]
PICropAutoCalculator-getBeforeGeometry
PICropAutoCalculator-getAfterGeometry
v16@?0@"PICropAdjustmentController"8
kernel vec4 iptLumHueSatTable(sampler image, __table sampler hueSatLumTable)
vec4 im = sample(image, samplerCoord(image)) ;
vec4 result = im;
float hueIdx = 359.0 * 0.5 * (atan(im.b, im.g)/3.1416 + 1.0);
hueIdx = clamp(hueIdx, 0.0, 359.0) + 0.5;
float hueChange = (sample(hueSatLumTable, samplerTransform(hueSatLumTable, vec2(hueIdx,0.5))).r);
float satChange = (sample(hueSatLumTable, samplerTransform(hueSatLumTable, vec2(hueIdx,0.5))).g);
float lumChange = (sample(hueSatLumTable, samplerTransform(hueSatLumTable, vec2(hueIdx,0.5))).b);
float chroma = sqrt(im.g*im.g+im.b*im.b) ;
chroma *= satChange ;
float hue = atan(im.b, im.g) + hueChange ;
vec3 adjustIm = im.rgb;
float hueAngle = hue  ;
lumChange = mix(1.0, lumChange, clamp(chroma,-0.7,0.7));
adjustIm.r *= lumChange;
adjustIm.g = chroma * cos(hueAngle) ;
adjustIm.b = chroma * sin(hueAngle) ;
result.rgb = adjustIm.rgb;
result.a = im.a ;
return result ;
kernel vec4 srgbToIPT(sampler image){
vec4 im = sample(image, samplerCoord(image));
vec3 lms, ipt;
lms = im.r * vec3(0.3139902162, 0.155372406, 0.017752387) +
im.g * vec3(0.6395129383, 0.7578944616, 0.109442094) +
im.b * vec3(0.0464975462, 0.0867014186, 0.8725692246);
lms = sign(lms)*pow(abs(lms), vec3(0.43, 0.43, 0.43));
ipt = lms.r * vec3(0.4, 4.455, 0.8056) +
lms.g * vec3(0.4, -4.851, 0.3572) +
lms.b * vec3(0.2, 0.3960, -1.1628);
return vec4(ipt, im.a);
kernel vec4 iptToSRGB(sampler image){
vec4 im = sample(image, samplerCoord(image));
vec3 lms, rgb;
lms = im.rrr +
im.g * vec3(0.09756893,-0.11387649,0.03261511) +
im.b * vec3(0.20522644, 0.13321716,  -0.67688718);
lms = sign(lms)*pow(abs(lms), vec3(1.0/.43));
rgb = lms.r * vec3( 5.472212058380287,  -1.125241895533569,   0.029801651173470) +
lms.g * vec3(-4.641960098354470, 2.293170938060623, -0.193180728257140) +
lms.b * vec3(0.169637076827974,  -0.167895202223709, 1.163647892783812);
return vec4(rgb, im.a);
kernel vec4 add_gaussian(sampler srcTable, float tableSize, float hueAmplitude, float satAmplitude, float lumAmplitude, float gaussX, float gaussSigmaSquared) {
vec2 d = destCoord();
vec4 src = sample(srcTable, samplerCoord(srcTable));
float x = d.x / (tableSize - 1.0);
float dist = min(min(abs(x - gaussX), abs(x - 1.0 - gaussX)), abs(x + 1.0 - gaussX));
float p = -((dist * dist) / (2.0 * gaussSigmaSquared));
float ep = exp(p);
float hue = hueAmplitude * ep;
float sat = satAmplitude * ep;
float lum = lumAmplitude * ep;
float h = clamp(src.r + hue, -1.0, 1.0);
float s = clamp(src.g + sat, -1.0, 1.0);
float l = clamp(src.b + lum, -1.0, 1.0);
return vec4(h,s,l,1.0);
srgbToIPT
iptToSRGB
CIConstantColorGenerator
add_gaussian
iptLumHueSatTable
PIVideoReframeNode.m
invalid crop rect
pipelineState
-[PIVideoReframeNode initWithSettings:inputs:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Render/PIVideoReframeNode.m
-[PIVideoReframeNode _evaluateVideoProperties:]
error != nil
-[PIVideoReframeNode _evaluateImageGeometry:]
Failed to get input geometry
Could not get the input geometry
Could not get the input image properties
CIPerspectiveTransform
inputTopLeft
inputTopRight
inputBottomLeft
inputBottomRight
showStabilizationWatermark
init is not a valid initializer
Invalid plist at path: %@
Time
Subjects
Type
HumanBody
HumanFace
CatBody
DogBody
-[PIVideoReframeMetadataExtractor overwriteTrackingMetadataWithPlist:]
PIVideoReframeMetadataExtractor.mm
type >= 0
Confidence
-[PIVideoReframeMetadataExtractor motionBlurVectorFromMetadata:]
-[PIVideoReframeMetadataExtractor extractMetadata]
mdataGroup.items.count == 1
0 && "unexpected metadata identifier"
0 && "unexpected metadata data type"
err == noErr
kernel vec4 convertFromRGBToYIQ(sampler src, float gamma)
vec4 pix ;
vec3 pix2;
pix = sample(src, samplerCoord(src));
pix.rgb = sign(pix.rgb)*pow(abs(pix.rgb), vec3(1.0/gamma)) ;
pix2.rgb = pix.r * vec3(0.299,  0.595716,  0.211456) +
pix.g * vec3(0.587, -0.274453, -0.522591) +
pix.b * vec3(0.114, -0.321263,  0.311135);
return vec4(pix2, pix.a);
kernel vec4 convertFromYIQToRGB(sampler src, float gamma)
vec4 color, pix;
pix = sample(src, samplerCoord(src));
color.rgb = pix.rrr +
pix.g * vec3(0.956296, -0.272122, -1.10699) +
pix.b * vec3(0.621024, -0.647381, 1.70461);
color.rgb = sign(color.rgb)*pow(abs(color.rgb), vec3(gamma, gamma, gamma) );
color.a = pix.a;
return color;
kernel vec4 whiteBalance(sampler image, float grayY, float grayI, float grayQ, float strength)
vec4 im = sample(image, samplerCoord(image)) ;
vec2 grayOffset = vec2(grayI, grayQ) ;
vec4 result ;
float newStrength = 1.0 + (strength-1.0)*(1.0-im.r) ;
result.r = im.r ;
result.gb = im.gb + newStrength*grayOffset ;
float damp = max(min(1.0, im.r/(grayY+0.00001)),0.0) ;
result.rgb = mix(im.rgb, result.rgb, damp) ;
result.a = im.a ;
return result ;
kernel vec4 gHDRtoPP(sampler image)
vec4 pix ;
vec3 pix2;
pix = sample(image, samplerCoord(image));
pix2 = pix.r * vec3(0.615429622407401,   0.114831839141528,   0.011544126697221) +
pix.g * vec3(0.367479646665836,   0.797943554457996,   0.064077744191180) +
pix.b * vec3(  0.016956659608091,   0.087783443422360,   0.924405601458102);
return vec4(pix2, pix.a);
kernel vec4 PPtogHDR(sampler image)
vec4 pix ;
vec3 pix2;
pix = sample(image, samplerCoord(image));
pix2 = pix.r * vec3(1.777445503202045,  -0.255296595099306,  -0.004500433755654) +
pix.g * vec3( -0.822224875430495,   1.380948853784730,  -0.085456231694984) +
pix.b * vec3(0.045475917061484,  -0.126454737973025,   1.089973874037625);
return vec4(pix2, pix.a);
convertFromRGBToYIQ
convertFromYIQToRGB
-[PINeutralGrayWhiteBalanceFilter outputImage]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Adjustments/PINeutralGrayWhiteBalanceFilter.m
PISmartToneAutoCalculator
-[PISmartToneAutoCalculator submit:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Autocalculators/PIAutoCalculators.mm
v8@?0
-[PISmartColorAutoCalculator submit:]
-[PIRedEyeAutoCalculator submit:]
PIRedEyeAutoCalculator-getLensInfo
locationX
locationY
touchDiameter
/masterSpace
kernel vec4 _blendGrainsHDR(__sample isoImages, float log10iso)
  vec4 c = isoImages; 
  float mix10_50    = mix(c.r, c.g, log10iso*1.43067655809 
                                           - 1.43067655809); 
  float mix50_400   = mix(c.g, c.b, log10iso*1.10730936496 
                                           - 1.88128539659); 
  float mix400_3200 = mix(c.b, c.a, log10iso*1.10730936496 
                                           - 2.88128539659); 
  float v = compare(log10iso - 1.69897000434,                     mix10_50,                     compare(log10iso - 2.60205999133,                             mix50_400,                             mix400_3200)); 
  return vec4(v,v,v,1.0);
kernel vec4 _grainBlendAndMixHDR(__sample img, __sample grainImage, float contrast, float mixAmount)
  vec3 rgb = img.rgb;
  float luminance = clamp(dot(rgb, vec3(.333333)), 0.0, 1.0); 
  float gamma = 4.01 - 2.0*luminance;
  rgb = sign(rgb) * pow(abs(rgb), vec3(1.0/gamma));
  float grain = grainImage.r - 0.5;
  float mult = contrast * grain;
  rgb += (max(luminance, 0.5) * mult * (1.0-luminance));
  rgb = sign(rgb) * pow(abs(rgb), vec3(gamma));
  rgb = min(rgb, 12.0);
  return mix(img, vec4(rgb,img.a), mixAmount);
kernel vec2 _paddedTile2(vec4 k) { return fract(destCoord() * k.zw) * k.xy + vec2(1.0); }
{CGRect={CGPoint=dd}{CGSize=dd}}44@?0i8{CGRect={CGPoint=dd}{CGSize=dd}}12
1536 x 1536 noise grain image prov
v56@?0^v8Q16Q24Q32Q40Q48
generateNoiseImage_block_invoke
PIPhotoGrainHDR.m
width==512*3
height==512*3
x==0
y==0
kernel vec4 _grainGenCombineHDR (__sample r, __sample g, __sample b, __sample a)
{ return vec4(r.x, g.x, b.x, a.x); }
CIUnsharpMask
image != nil
+[PIImageIO writeImage:fileURL:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Util/PIImageIO.m
cgImage != NULL
+[PIImageIO writeCGImage:fileURL:]
fileURL != nil
Unhandled bit depth: %ld
+[PIImageIO writeCGImage:fileURL:options:]
Successfully wrote image to file %@
Failed to write image to file %@
%@-%d-%ld.tiff
GU %@ %@
Sensitivity
Bad float to fixed 16 conversion
+[PIApertureRedEyeProcessorKernel convertFloat:toFixed16:count:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Adjustments/ApertureRedEye/PIApertureRedEyeFilter.mm
Bad fixed 16 to float conversion
+[PIApertureRedEyeProcessorKernel convertFixed16:toFloat:count:]
+[PIApertureRedEyeProcessorKernel processWithInputs:arguments:output:error:]
PIApertureRedEyeFilter.mm
output.format == kCIFormatRGBAf
inputSensitivity
portraitStrength
capturedPortraitMajorVersion
capturedPortraitMinorVersion
portraitDisableStage
+[PIValuesAtCapture valuesAtCaptureFromImageProperties:error:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Autocalculators/PIPortraitAutoCalculator.m
Portrait was previously applied.
Failed to load depth data
Unfiltered depth data is not supported
Low quality depth data is not supported
Missing camera calibration data
Failed to load auxiliary data
Missing auxiliary metadata
Depth data version mismatch, asset has %@ but we can only handle %@
maxSDOFRenderingVersionSupported
We only know up to sDOF version %d. Found: %d
<%@:%p aperture:%f minimumAperture:%@ maximumAperture:%@ portraitStrength:%f SDOFRenderingVersion:%lu portraitMajorVersion:%lu portraitMinorVersion:%lu>
-[PIPortraitAutoCalculator submit:]
PIPortraitAutoCalculator-getValuesAtCapture-imageProperties
capturedAperture
capturedPortraitStrength
v16@?0@"NUResponse"8
PIPortraitAutoCalculator-faceDetect
faceObservations != nil
+[PIPortraitAutoCalculator portraitInfoDictionaryFromFaceObservations:metadata:orientation:valuesAtCapture:]
metadata != nil
NUOrientationIsValid(orientation)
Insufficient number of landmark points
+[PIPortraitAutoCalculator depthEffectInfoDictionaryFromFaceObservations:focus:valuesAtCapture:lumaNoiseScale:orientation:]
minimumAperture
maximumAperture
+[PIPortraitAutoCalculator portraitEffectInfoDictionaryFromFaceObservations:orientation:valuesAtCapture:]
faceBoundingBox
faceJunkinessIndex
faceOrientationIndex
allPoints
faceContour
leftEye
rightEye
leftEyebrow
rightEyebrow
nose
noseCrest
medianLine
outerLips
innerLips
leftPupil
rightPupil
+[PIPortraitAutoCalculator portraitInfoDictionaryFromCameraMetadata:]
-[PIDepthEffectApertureAutoCalculator submit:]
PIDepthEffectApertureAutoCalculator-getValuesAtCapture-imageProperties
depthData:DepthDataVersion
var tagNode = GetTag('/pre-Adjustments');
orientation = tagNode.geometry.orientation;
var node = Orient(tagNode, orientation);
return node;
@"NURenderNode"40@?0@"NURenderPipelineHelper"8@"NUComposition"16@"NURenderNode"24^@32
/pre-Adjustments
var source = GetTag('/pre-Adjustments');
ResetTagInput('/pre-Crop', source);
source = GetTag('/Crop');
orientation = source.geometry.orientation;
source = Orient(source, orientation);
return source;
/pre-Crop
/Crop
var sourceSettings = { 'skipOrientation': true };
var rawAdjustment = composition.raw
if (rawAdjustment) {
    sourceSettings['inputDecoderVersion'] = rawAdjustment.inputDecoderVersion
var image = Source(composition.source, sourceSettings)
var rawImage = GetTagInput('RAW/Linear')
if (rawImage) {
    image = rawImage
    image = Filter('PIForwardFakeBoost', {'inputImage' : image, 'inputBoost' : 1.0,})
var orientation = composition.orientation
if (orientation) { image = Orient(image, orientation.value) }
return image
Raw/Linear
var sourceSettings = { }; 
var raw = composition.raw; 
if (raw) { 
  sourceSettings['inputDecoderVersion'] = raw.inputDecoderVersion; 
  var sushiLevel = raw.inputSushiLevel;
  if (sushiLevel) {
    sourceSettings['kCGImageSourceShouldExtendRaw'] = sushiLevel;
} else { throw "expected RAW in rawSourceFilterIncludingOrientation"; }
return Source(composition.source, sourceSettings); 
expected RAW in rawSourceFilterIncludingOrientation
+[PIPipelineFilters rawSourceFilterIncludingOrientation]_block_invoke
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Pipeline/PIPipelineFilters.m
var sourceSettings = { 'skipOrientation' : true }; 
var raw = composition.raw; 
if (raw) { 
  sourceSettings['inputDecoderVersion'] = raw.inputDecoderVersion; 
  var sushiLevel = raw.inputSushiLevel;
  if (sushiLevel) {
    sourceSettings['kCGImageSourceShouldExtendRaw'] = sushiLevel;
  return Source(composition.source, sourceSettings); 
} else {
  return GetTag("/ShowOriginalSource");
var source = GetTag('/pre-RedEye');ResetTagInput('/post-RedEye', source);
return GetTag('/post-RedEye');
/pre-RedEye
/post-RedEye
Could not construct noRedEye filter from inline source
+[PIPipelineFilters noRedEyeFilter]
var output = input
var preTrim = GetTag('/pre-Trim');ResetTagInput('/SloMo', preTrim);
let slomo = composition.slomo
if (slomo) {
    var startTime = TimeMake(slomo.start, slomo.startScale)
    var endTime = TimeMake(slomo.end, slomo.endScale)
    output = SloMo(input, startTime, endTime, slomo.rate)
return output;
/pre-Trim
/SloMo
Could not construct noTrimFilter filter from inline source
+[PIPipelineFilters noTrimFilter]
var preMute = GetTag('pre-Mute');ResetTagInput('Mute', preMute);
return GetTag('Image');
Could not construct noMuteFilter filter from inline source
+[PIPipelineFilters noMuteFilter]
var preCrop = GetTag('/pre-Crop');ResetTagInput('/pre-Orientation', preCrop);
return input;
/pre-Orientation
Could not construct noCropFilter filter from inline source
+[PIPipelineFilters noCropFilter]
let preGeometry = GetTag('/pre-Geometry');ResetTagInput('/post-Adjustments', preGeometry);
return input;
/post-Adjustments
Could not construct noGeometry filter from inline source
+[PIPipelineFilters iosCropToolFilter]_block_invoke
var image = GetTag('pre-Trim');ResetTagInput('Trim', image);
image = GetTag('pre-SloMo');ResetTagInput('SloMo', image);
return input;
Could not construct stripAllTimeAdjustmentsFilter filter from inline source
+[PIPipelineFilters stripAllTimeAdjustmentsFilter]
let preGeometry = GetTag('/pre-Geometry');ResetTagInput('/post-Geometry', preGeometry);
return input;
+[PIPipelineFilters noGeometryFilter]
var preOrientation = GetTag('/pre-Orientation');ResetTagInput('/Orientation', preOrientation);
return input;
/Orientation
Could not construct noOrientationFilter filter from inline source
+[PIPipelineFilters noOrientationFilter]
return null;
Could not construct pipeline filter from source: %@
+[PIPipelineFilters orientationAsMetaDataFilter]
var preCrop = GetTag('/perspectiveStraighten');if (preCrop) {   ResetTagInput('/Crop', preCrop);
}return input;
/perspectiveStraighten
Could not construct perspectiveStraightenWithoutCropFilter filter from inline source
+[PIPipelineFilters perspectiveStraightenWithoutCropFilter]
var output = input;
var prePortraitVideoNode = GetTag('pre-PortraitVideo');
if (prePortraitVideoNode) {
    ResetTagInput('/post-PortraitVideo', prePortraitVideoNode);
return output;
/post-PortraitVideo
Could not construct histogramOptimizationFilter from inline source
+[PIPipelineFilters histogramOptimizationFilter]
var tagNode = GetTag('%@');
if (tagNode) {
    ResetTagInput('/pre-Geometry', tagNode);
return GetTag('/post-Geometry');
Could not construct stopAtTagIncludeGeometryFilter from inline source
+[PIPipelineFilters stopAtTagIncludeGeometryFilter:]
var tagNode = GetTag('%@');
if (tagNode) {
    ResetTagInput('/pre-Orientation', tagNode);
return GetTag('/Orientation');
Could not construct stopAtTagIncludeOrientationFilter from inline source
+[PIPipelineFilters stopAtTagIncludeOrientationFilter:]
var output = input;
var orientation = composition.orientation;
if (orientation) {
    output = Orient(input, orientation.value);
return output;
+[PIPipelineFilters applyOrientationFilter]
ResetTagInput('/AutoLoop/Output', GetTag('/AutoLoop/StabilizedVideo'));
return input;
/AutoLoop/Output
/AutoLoop/StabilizedVideo
Could not construct autoloopStabilizedVideoFilter filter from inline source
+[PIPipelineFilters autoloopStabilizedVideoFilter]
return Source(composition.sourceSpatialOvercapture, { 'skipOrientation' : true })
Could not construct overcaptureSourceFilter filter from inline source
+[PIPipelineFilters overcaptureSourceFilter]
return Source(composition.source, { 'skipOrientation' : true })
Could not construct primarySourceFilter filter from inline source
+[PIPipelineFilters primarySourceFilter]
return Source(composition.sourceSpatialOvercapture, {  })
+[PIPipelineFilters spatialOvercaptureVideoSourceFilter]
return input;
/PortraitV2-zeroStrength
/PortraitV2
Could not construct oneShotPortraitV2ExportFilter filter from inline source
+[PIPipelineFilters oneShotPortraitV2ExportFilter]
let output = input;
var image = GetTag('pre-VideoReframe');
if (image) {
    var videoReframe = composition.videoReframe;
    if (videoReframe && videoReframe.enabled) {
        image = VideoReframe(image, videoReframe);
        image = Filter(`CIColorMatrix`, {'inputImage' : image, 'inputBVector' : CIVectorMake(1,1,1,0)});
        ResetTagInput('VideoReframe', image);
    }
else { // image or live-photo { 
    var crop = composition.cropStraighten;
    var image = GetTag('pre-Crop');
    if (crop && crop.smart == 1 && image) {
        if (crop.pitch || crop.yaw) {
            var perspectiveTransform = PerspectiveTransformMake(crop.angle, crop.pitch, crop.yaw, image.geometry);
            image = Transform(image, perspectiveTransform);
        } else {
            var straightenTransform = StraightenTransformMake(crop.angle, image.geometry);
            image = Transform(image, straightenTransform);
        }
        // Apply the crop rect to the incoming image;
        image = Crop(image, crop.xOrigin, crop.yOrigin, crop.width, crop.height);
        image = Filter(`CIColorMatrix`, {'inputImage' : image, 'inputBVector' : CIVectorMake(1,1,1,0)});
        ResetTagInput('Crop', image);
    }
return output;
+[PIPipelineFilters(Debug) socPseudoColorFilter]_block_invoke
none
error != NULL
-[NURenderPipelineHelper(PI) videoReframe:reframes:error:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Pipeline/PIPhotosPipelineHelper.m
-[NURenderPipelineHelper(PI) videoCrossfadeLoop:crossfadeAdjustment:error:]
-[NURenderPipelineHelper(PI) portraitVideo:disparityInput:disparityKeyframes:apertureKeyframes:debugMode:error:]
@"NSMutableArray"16@?0@"NSArray"8
-[NURenderPipelineHelper(PI) portraitVideoDebugDetectedObjects:source:cinematographyState:monochrome:error:]
Unexpected source type
extent
transform
AutoLoop/LongExposureMotion
PILongExposureFusion
inputStillImage
inputVideoScale
inputRenderScale
inputAlignmentExtent
inputAlignmentTransform
inputMaskImage
PIApertureRedEyeFilter
PIRedEye
inputDestinationImage
recipe != nil
NSDictionary * _Nonnull PIAutoLoopRecipeForFlavor(NSDictionary *__strong _Nonnull, PIAutoLoopFlavor)
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/AutoLoop/PIAutoLoop.m
CGRect PIAutoLoopRecipeGetCropRect(NSDictionary *__strong _Nonnull)
origin_x
origin_y
BOOL PIAutoLoopRecipeHasGoodStabilization(NSDictionary *__strong _Nonnull)
frameTransform != nil
CMTime PIAutoLoopRecipeFrameTransformGetTime(NSDictionary *__strong)
frameTransform_rawTime
matrix_float3x3 PIAutoLoopRecipeFrameTransformGetHomography(NSDictionary *__strong)
frameTransform_homography
NSDictionary * _Nonnull PIAutoLoopRecipeGetInstructionAtTime(NSDictionary *__strong _Nonnull, CMTime)
CMTIME_IS_VALID(time)
loopRecipe_frameInstructions
loopFrameData_presTime
q24@?0@"NSDictionary"8@"NSDictionary"16
CMTime PIAutoLoopRecipeGetFrameDuration(NSDictionary *__strong _Nonnull)
NSDictionary * _Nullable PIAutoLoopRecipeGetFlavorParameters(NSDictionary *__strong _Nonnull, PIAutoLoopFlavor)
CMTimeRange PIAutoLoopRecipeGetTimeRangeForFlavor(NSDictionary *__strong _Nonnull, PIAutoLoopFlavor)
completion != nil
-[PICurvesAutoCalculator submit:]
/Library/Caches/com.apple.xbs/Sources/Photos_Sim/Photos-440.0.140/workspaces/neutrino/PhotoImaging/Autocalculators/PICurvesAutoCalculator.m
-[PICurvesAutoCalculator computeCurvesForImageHistogram:]
softlink:r:path:/System/Library/Frameworks/JavaScriptCore.framework/JavaScriptCore
_PITapToTrackRenderResult
PITapToTrackResult
NURenderResult
NSObject
PITapToTrackRenderJob
PITapToTrackRequest
PIPhotoEffectHDR
PIPhotoEffectBlackAndWhiteHDR
PIPhotoEffectNoirHDR
PIPhotoEffectChromeHDR
PIPhotoEffectFadeHDR
PIPhotoEffectInstantHDR
PIPhotoEffectMonoHDR
PIPhotoEffectProcessHDR
PIPhotoEffectTonalHDR
PIPhotoEffectTransferHDR
PIPhotoEffectStageMonoHDR
PIPhotoEffect3DHDR
PIPhotoEffect3DBlackAndWhiteHDR
PIPhotoEffect3DVividHDR
PIPhotoEffect3DVividWarmHDR
PIPhotoEffect3DVividCoolHDR
PIPhotoEffect3DDramaticHDR
PIPhotoEffect3DDramaticCoolHDR
PIPhotoEffect3DDramaticWarmHDR
PIPhotoEffect3DSilverplateHDR
PIPhotoEffect3DNoirHDR
PIColorNormalizationFilter
PIReframeRules
PISemanticEnhanceAdjustmentController
_PIDisparitySampleResult
PIDisparitySampleResult
PIDisparitySampleJob
PIDisparitySampleRequest
PILocalContrastHDR
PIHighKey
PICompositionController
NSCopying
PICompositionSerializer
PICompositionSerializationResult
PICompositionSerializerMetadata
PILevelsFilterHDR
PIColorNormalizationAutoCalculator
NUTimeBased
PIFalseColorHDRDebug
PIAutoCalculatorUtils
PIAutoLoopExportRequest
PIAutoLoopExportClient
PIAutoLoopAnalysisRequest
PIAutoLoopAnalysisClient
PILongExposureRegistrationRequest
1 1@
PIPortraitVideoProcessor
PIPortraitVideoFilter
PIAutoLoopAdjustmentController
PIVideoCrossfadeLoopNode
PILivePhotoKeyFrameAdjustmentController
PIVignetteAdjustmentController
PIAdjustmentController
PIPortraitVideoRenderNode
PIFaceObservationCache
PISourceSelectAdjustmentController
PIScalarKeyframe
PIDictionaryRepresentable
PIReframeSubject
NSSecureCoding
NSCoding
PIPortraitVideoAdjustmentController
PIReframeKeyframe
PIReframeKeyframeSequence
PILongExposureFusionAutoCalculator
PIEffectAdjustmentController
PIAutoLoopAutoCalculator
PIMakerNoteUtilities
PITempTintFilter
PITimeVaryingPipelineStateSetting
_PIAutoLoopAnalysisResult
PIAutoLoopAnalysisResult
PIAutoLoopAnalysisJob
_PIVideoStabilizeFlowControl
ICFlowControl
_PIVideoStabilizeResult
PIVideoStabilizeResult
PIVideoStabilizeRenderJob
PIVideoStabilizeRequest
PIFaceBalanceAutoCalculator
GrayColorResult
RGBResult
PIWhiteBalanceAutoCalculator
_PIWhiteColorCalculator
PIDefinitionFilter
PISmartBlackAndWhiteAdjustmentController
PICompositionExporterDefaultMetadataConverter
PICompositionExporterMetadataConverter
PICompositionExportImagePrepareResult
PICompositionExportAuxiliaryResult
PICompositionExportResult
PICompositionExportDataResult
PICompositionExporterOptions
PICompositionExporterVideoOptions
PICompositionExporterImageOptions
PICompositionExporterAuxiliaryOptions
PICompositionExporter
GUBilateralConvolution
GUBWBilateralConvolution
PIBilateralFilter
PIPhotoEditAdjustmentsVersion
PINoiseReductionAdjustmentController
PrivateSmartColorHDR
PISmartColorFilterHDR
PIVideoCrossfadeLoopAdjustmentController
PIPortraitAdjustmentController
PICompositionNoOpRemoval
PIOrientationAdjustmentController
PIPortraitVideoMetadataSample
PIPortraitVideoRenderer
PIDefinitionAdjustmentController
PIHighKeyHDR
PIPerspectiveAutoCalculator
PIFaceObservingAutoCalculator
PIRawNoiseReductionAdjustmentController
PICropAdjustmentController
PIRawAdjustmentController
PISmartToneAdjustmentController
PIAutoLoopKernels
PIPhotoEditHelper
PIAdjustmentConstants
PIFakeBoost
PIForwardFakeBoost
PIInverseFakeBoost
PICoreImageUtilities
PICompositionSerializerConstants
PICurvesFilterHDR
PILongExposureAccumulator
_PILongExposureRegistrationResult
PILongExposureRegistrationResult
PILongExposureRegistrationJob
PIRAWTempTintSampler
PITagColorSampler
PIRedEye
PIVideoPosterFrameAdjustmentController
PICompositionSerializerFormatVersion
PICaptureDebugUtilities
PISlomoAdjustmentController
PIPortraitVideoDebugDetectionsRenderNode
PIDebugAdjustmentController
PIRAWFaceBalance
PICurvesLUTFilter
PICurvesFilter
PIColorBalanceFilter
PIPhotosPipelineHDRFilters
PIHighResFusionAdjustmentController
PIGlobalSettings
PIAutoLoopExportJob
PIJSRenderPipeline
PhotosPipeline
PIApertureRedEyeAutoCalculator
PrivateLocalLightHDR
PILocalLightMapPrepareHDR
PILocalLightFilterHDR
PIMsgImageBuffer
PISmartBlackAndWhiteAutoCalculator
PISmartBlackAndWhiteHDR
PrivateSmartBlackAndWhite
PITrimAdjustmentController
PIRedEyeAdjustmentController
PILevelsAutoCalculator
PILevelsLuminanceAutoCalculator
PILevelsRGBAutoCalculator
AdjustmentExtensions
PICurveControlPoint
PrivateSmartToneHDR
PISmartToneFilterHDR
PILongExposureFusion
PIModernPhotosPipeline
PILevelsFilter
PISchema
PICompositionFinalizer
PICompositionFinalizerResult
PIVideoStabilizeAdjustmentController
PICropAutoCalculator
Utilities
PISmartColorAdjustmentController
PISelectiveColorFilter
PIVideoReframeNode
PIVideoReframe
PIVideoReframeTimedMetadata
PIVideoReframeMetadataExtractor
PINeutralGrayWhiteBalanceFilter
PISmartToneAutoCalculator
PISmartColorAutoCalculator
PIRedEyeAutoCalculator
PIManualRedEyeAutoCalculator
PISourceSampler
PIPhotoGrainHDR
PIImageIO
PISharpenAdjustmentController
PIApertureRedEyeProcessorKernel
PIApertureRedEyeFilter
PIValuesAtCapture
PIPortraitAutoCalculator
PIDepthEffectApertureAutoCalculator
PIPipelineFilters
Debug
PIWhiteBalanceAdjustmentController
PIDepthAdjustmentController
PIVideoReframeAdjustmentController
PICurvesAutoCalculator
PICurvesLuminanceAutoCalculator
PICurvesRGBAutoCalculator
init
isEqual:
class
self
performSelector:
performSelector:withObject:
performSelector:withObject:withObject:
isProxy
isKindOfClass:
isMemberOfClass:
conformsToProtocol:
respondsToSelector:
retain
release
autorelease
retainCount
zone
hash
superclass
description
debugDescription
TQ,R
T#,R
T@"NSString",R,C
statistics
T@"<NURenderStatistics>",R
completedTrack
T@"PTCinematographyTrack",R,N
initWithCompletedTrack:
.cxx_destruct
_completedTrack
T@"PTCinematographyTrack",R,N,V_completedTrack
oneToOneScalePolicy
stringWithFormat:
callStackSymbols
componentsJoinedByString:
prepare:
request
device
currentPlatform
mainDevice
metalDevice
missingError:object:
newCommandQueue
initWithCommandQueue:
outputVideo
initWithAsset:
startTime
startReadingFrames:atTime:error:
nextFrame
normalizedImagePoint
colorBuffer
getRectForPoint:colorBuffer:
rect
time
disparityBuffer
addDetectionAndStartTrackingRect:time:colorBuffer:disparityBuffer:
confidence
_reportProgressAtTime:rect:confidence:
clientRequestedStop
addDetectionForNextFrameAt:colorBuffer:disparityBuffer:
stopReadingFrames
finalizeTrack
setCompletedTrack:
progressHandler
responseQueue
setClientRequestedStop:
wantsRenderStage
wantsCompleteStage
wantsOutputVideo
scalePolicy
result
setStartTime:
setNormalizedImagePoint:
setProgressHandler:
_clientRequestedStop
_progressHandler
_normalizedImagePoint
_startTime
T{?=qiIq},N,V_startTime
T{CGPoint=dd},N,V_normalizedImagePoint
T@?,C,N,V_progressHandler
T@"PTCinematographyTrack",&,N,V_completedTrack
TB,V_clientRequestedStop
initWithComposition:
sourceFilterNoOrientation
arrayWithObjects:count:
setPipelineFilters:
copyWithZone:
initWithRequest:
submitGeneric:
initWithComposition:startTime:pointToTrack:
newRenderJob
mediaComponentType
submit:
kernelWithString:
stringByReplacingOccurrencesOfString:withString:
kernel
extent
applyWithExtent:arguments:
photoEffectName
filterWithName:
setValue:forKey:
outputImage
inputImage
setInputImage:
_inputImage
T@"CIImage",&,V_inputImage
kernelBlackAndWhite
numberWithFloat:
inputDepthMap
setInputDepthMap:
inputThreshold
setInputThreshold:
_inputThreshold
_inputDepthMap
T@"CIImage",&,V_inputDepthMap
Tf,V_inputThreshold
inputNormalization
tempTintProperties
objectForKeyedSubscript:
dictionaryWithObjects:forKeys:count:
imageByApplyingFilter:withInputParameters:
highKeyProperties
smartToneProperties
smartColorProperties
logger
initWithCIImage:options:
setRevision:
performRequests:error:
results
firstObject
adjustmentKeys
countByEnumeratingWithState:objects:count:
adjustmentValuesForKey:
setObject:forKeyedSubscript:
initWithAnalysisData:
setInputNormalization:
contextWithOptions:
pi_createColorCubeDataForFilters:dimension:colorSpace:
isAnalysisAvailable
colorCubeForNormalization:dimension:targetColorSpace:
analysisAvailable
TB,R,N,GisAnalysisAvailable
outputNormalization
T@"CIImage",&,N,VinputImage
T@"PFStoryRecipeDisplayAssetNormalization",&,N,VinputNormalization
T@"PFStoryRecipeDisplayAssetNormalization",R,N
initWithCapacity:
predicateWithFormat:
ruleWithPredicate:action:
addObject:
ruleWithPredicate:assertingFact:grade:
ruleWithPredicate:retractingFact:grade:
factCandidateForReframe
factCandidateForStill
factCandidateForVideo
factCandidateForPerspective
factCandidateForHorizon
retractFact:
initWithArray:
setEnableLogging:
setConstants:
sharedPregateRules
addRulesFromArray:
gradeForFact:
pregateRulesSystemWithConstants:
isCandidateForReframe
isCandidateForPerspective
isCandidateForHorizon
TB,R
adjustment
intensityKey
isEqualToString:
doubleValue
sceneLabelKey
boundingBoxesKey
isEqualToArray:
isSettingEqual:forKey:
numberWithDouble:
floatValue
platedFoodSceneLabel
sunriseSunsetSceneLabel
genericLandscapeSceneLabel
sceneConfidenceKey
copy
count
arrayWithCapacity:
faceBoundingBoxesKey
setIntensity:
intensity
scene
sceneConfidence
boundingBoxes
setScene:confidence:
setBoundingBoxesFromObservations:orientation:
setFaceBoundingBoxesFromObservations:orientation:
Td,N
Tq,R,N
Td,R,N
T@"NSArray",R,C,N
currentHandler
stringWithUTF8String:
handleFailureInFunction:file:lineNumber:description:
boundingBox
sampledDisparityValue
Tf,R,N
initWithDisparityValue:
_sampledDisparityValue
Tf,R,N,V_sampledDisparityValue
failureError:object:
sampleTime
invalidError:object:
sampleRect
setSampledDisparityValue:
render:
setSampleTime:
setSampleRect:
_sampleTime
_sampleRect
T{?=qiIq},N,V_sampleTime
T{CGRect={CGPoint=dd}{CGSize=dd}},N,V_sampleRect
Tf,N,V_sampledDisparityValue
initWithComposition:time:sampleRect:
vectorWithX:Y:Z:W:
HLGOpticalScale
imageByClampingToExtent
imageByApplyingTransform:
imageByApplyingGaussianBlurWithSigma:
imageByCroppingToRect:
_kernelLocalContrast
customAttributes
inputStrength
inputScale
setInputStrength:
T@"NSNumber",&,N,VinputStrength
imageOrientation
setImageOrientation:
sourceSelection
setSourceSelection:
compositionController:didAddAdjustment:
compositionController:didRemoveAdjustment:
compositionController:didUpdateAdjustment:
compositionController:didUpdateAdjustments:
compositionController:adjustmentControllerClassForKey:
schema
contents
allKeys
compositionKeys
addObjectsFromArray:
_keyToIdentifierMap
initWithIdentifier:
reset
_adjustmentControllerClassForKey:
initWithAdjustment:
setIdentifier:
composition
differingAdjustmentsWithComposition:
changeDelegate
isEqual:forKeys:visualChangesOnly:
availableKeys
settings
containsObject:
boolValue
integerValue
isEqual:visualChangesOnly:
sharedRegistry
schemaWithIdentifier:
schemaForKey:
orientationAdjustmentControllerCreatingIfNecessary:
orientation
sourceSelectAdjustmentController
removeAdjustmentWithKey:
modifyAdjustmentWithKey:modificationBlock:
smartToneAdjustmentControllerCreatingIfNecessary:
registeredPhotosSchemaIdentifier
photosSchema
adjustmentControllerClassForKey:
isSubclassOfClass:
setMediaType:
mediaType
settingForAdjustmentKey:settingKey:
setChangeDelegate:
addAdjustmentWithKey:
replaceAdjustment:withKey:
adjustmentControllerForKey:
applyChangesFromCompositionController:
isEqual:forKeys:comparisonBlock:
userOrientation
setOvercaptureSource:
overcaptureSource
setSource:mediaType:
source
_composition
_delegateFlags
_identifierMap
_changeDelegate
_imageOrientation
T@"NUComposition",R,C,N
T@"<PICompositionControllerDelegate>",W,N,V_changeDelegate
Tq,N
Tq,N,V_imageOrientation
setWithObjects:
locallySupportedFormatVersions
setWithArray:
handleFailureInMethod:object:file:lineNumber:description:
deserializeDictionaryFromData:error:
validateAdjustmentsEnvelope:error:
deserializeCompositionFromAdjustments:metadata:formatIdentifier:formatVersion:error:
URLWithString:
imageSourceWithURL:type:useEmbeddedPreview:
validateComposition:error:
conversionMap
newComposition
initWithDomain:code:userInfo:
stringByAppendingString:
newAdjustmentWithName:
enumerateKeysAndObjectsUsingBlock:
initWithName:
_sanitizeComposition:
validateCompositionWithMissingSource:error:
geometryRequestWithComposition:
setName:
submitSynchronous:
geometry
size
setWidth:
setHeight:
setOrientation:
serializeComposition:versionInfo:serializerMetadata:error:
width
numberWithInteger:
height
array
copyOfCompositionRemovingNoOps:
mapForSerialization
hasInputKey:
isAuto
serializeDictionary:error:
setFormatIdentifier:
adjustmentDataFormatVersionForComposition:
setFormatVersion:
setData:
_validateValueTypesForKeys:requiredKeys:inDictionary:error:
dataWithJSONObject:options:error:
errorWithDomain:code:userInfo:
compressData:options:error:
decompressData:options:error:
JSONObjectWithData:options:error:
_isDefault
mainBundle
infoDictionary
objectForKey:
serializeComposition:versionInfo:error:
dictionary
data
formatIdentifier
formatVersion
initialize
disableApertureEffects:
canInterpretDataWithFormatIdentifier:formatVersion:
deserializeCompositionFromData:formatIdentifier:formatVersion:error:
adjustmentInformationForComposition:error:
_data
_formatIdentifier
_formatVersion
T@"NSData",&,N,V_data
T@"NSString",&,N,V_formatIdentifier
T@"NSString",&,N,V_formatVersion
_width
_height
_orientation
Tq,N,V_width
Tq,N,V_height
Tq,N,V_orientation
containsString:
defaultValueForKey:
dictionaryWithObjectsAndKeys:
_customAttributesForKey:
valueForKey:
floatValueForKey:defaultValue:clearIfNotDefault:
dataWithBytesNoCopy:length:freeWhenDone:
imageWithBitmapData:bytesPerRow:size:format:colorSpace:
_LUTImage
samplerWithImage:keysAndValues:
P3KernelHDR
itur2100HLGColorSpace
CGColorSpace
imageByColorMatchingWorkingSpaceToColorSpace:
imageByUnpremultiplyingAlpha
samplerWithImage:
definition
applyWithExtent:roiCallback:arguments:options:
imageByColorMatchingColorSpaceToWorkingSpace:
imageByPremultiplyingAlpha
setDefaults
inputBlackSrcRGB
setInputBlackSrcRGB:
inputBlackDstRGB
setInputBlackDstRGB:
inputShadowSrcRGB
setInputShadowSrcRGB:
inputShadowDstRGB
setInputShadowDstRGB:
inputMidSrcRGB
setInputMidSrcRGB:
inputMidDstRGB
setInputMidDstRGB:
inputHilightSrcRGB
setInputHilightSrcRGB:
inputHilightDstRGB
setInputHilightDstRGB:
inputWhiteSrcRGB
setInputWhiteSrcRGB:
inputWhiteDstRGB
setInputWhiteDstRGB:
inputBlackSrcRed
setInputBlackSrcRed:
inputBlackDstRed
setInputBlackDstRed:
inputShadowSrcRed
setInputShadowSrcRed:
inputShadowDstRed
setInputShadowDstRed:
inputMidSrcRed
setInputMidSrcRed:
inputMidDstRed
setInputMidDstRed:
inputHilightSrcRed
setInputHilightSrcRed:
inputHilightDstRed
setInputHilightDstRed:
inputWhiteSrcRed
setInputWhiteSrcRed:
inputWhiteDstRed
setInputWhiteDstRed:
inputBlackSrcGreen
setInputBlackSrcGreen:
inputBlackDstGreen
setInputBlackDstGreen:
inputShadowSrcGreen
setInputShadowSrcGreen:
inputShadowDstGreen
setInputShadowDstGreen:
inputMidSrcGreen
setInputMidSrcGreen:
inputMidDstGreen
setInputMidDstGreen:
inputHilightSrcGreen
setInputHilightSrcGreen:
inputHilightDstGreen
setInputHilightDstGreen:
inputWhiteSrcGreen
setInputWhiteSrcGreen:
inputWhiteDstGreen
setInputWhiteDstGreen:
inputBlackSrcBlue
setInputBlackSrcBlue:
inputBlackDstBlue
setInputBlackDstBlue:
inputShadowSrcBlue
setInputShadowSrcBlue:
inputShadowDstBlue
setInputShadowDstBlue:
inputMidSrcBlue
setInputMidSrcBlue:
inputMidDstBlue
setInputMidDstBlue:
inputHilightSrcBlue
setInputHilightSrcBlue:
inputHilightDstBlue
setInputHilightDstBlue:
inputWhiteSrcBlue
setInputWhiteSrcBlue:
inputWhiteDstBlue
setInputWhiteDstBlue:
inputColorSpace
setInputColorSpace:
_inputBlackSrcRGB
_inputBlackDstRGB
_inputShadowSrcRGB
_inputShadowDstRGB
_inputMidSrcRGB
_inputMidDstRGB
_inputHilightSrcRGB
_inputHilightDstRGB
_inputWhiteSrcRGB
_inputWhiteDstRGB
_inputBlackSrcRed
_inputBlackDstRed
_inputShadowSrcRed
_inputShadowDstRed
_inputMidSrcRed
_inputMidDstRed
_inputHilightSrcRed
_inputHilightDstRed
_inputWhiteSrcRed
_inputWhiteDstRed
_inputBlackSrcGreen
_inputBlackDstGreen
_inputShadowSrcGreen
_inputShadowDstGreen
_inputMidSrcGreen
_inputMidDstGreen
_inputHilightSrcGreen
_inputHilightDstGreen
_inputWhiteSrcGreen
_inputWhiteDstGreen
_inputBlackSrcBlue
_inputBlackDstBlue
_inputShadowSrcBlue
_inputShadowDstBlue
_inputMidSrcBlue
_inputMidDstBlue
_inputHilightSrcBlue
_inputHilightDstBlue
_inputWhiteSrcBlue
_inputWhiteDstBlue
_inputColorSpace
T@"CIImage",&,N,V_inputImage
T@"NSNumber",&,N,V_inputBlackSrcRGB
T@"NSNumber",&,N,V_inputBlackDstRGB
T@"NSNumber",&,N,V_inputShadowSrcRGB
T@"NSNumber",&,N,V_inputShadowDstRGB
T@"NSNumber",&,N,V_inputMidSrcRGB
T@"NSNumber",&,N,V_inputMidDstRGB
T@"NSNumber",&,N,V_inputHilightSrcRGB
T@"NSNumber",&,N,V_inputHilightDstRGB
T@"NSNumber",&,N,V_inputWhiteSrcRGB
T@"NSNumber",&,N,V_inputWhiteDstRGB
T@"NSNumber",&,N,V_inputBlackSrcRed
T@"NSNumber",&,N,V_inputBlackDstRed
T@"NSNumber",&,N,V_inputShadowSrcRed
T@"NSNumber",&,N,V_inputShadowDstRed
T@"NSNumber",&,N,V_inputMidSrcRed
T@"NSNumber",&,N,V_inputMidDstRed
T@"NSNumber",&,N,V_inputHilightSrcRed
T@"NSNumber",&,N,V_inputHilightDstRed
T@"NSNumber",&,N,V_inputWhiteSrcRed
T@"NSNumber",&,N,V_inputWhiteDstRed
T@"NSNumber",&,N,V_inputBlackSrcGreen
T@"NSNumber",&,N,V_inputBlackDstGreen
T@"NSNumber",&,N,V_inputShadowSrcGreen
T@"NSNumber",&,N,V_inputShadowDstGreen
T@"NSNumber",&,N,V_inputMidSrcGreen
T@"NSNumber",&,N,V_inputMidDstGreen
T@"NSNumber",&,N,V_inputHilightSrcGreen
T@"NSNumber",&,N,V_inputHilightDstGreen
T@"NSNumber",&,N,V_inputWhiteSrcGreen
T@"NSNumber",&,N,V_inputWhiteDstGreen
T@"NSNumber",&,N,V_inputBlackSrcBlue
T@"NSNumber",&,N,V_inputBlackDstBlue
T@"NSNumber",&,N,V_inputShadowSrcBlue
T@"NSNumber",&,N,V_inputShadowDstBlue
T@"NSNumber",&,N,V_inputMidSrcBlue
T@"NSNumber",&,N,V_inputMidDstBlue
T@"NSNumber",&,N,V_inputHilightSrcBlue
T@"NSNumber",&,N,V_inputHilightDstBlue
T@"NSNumber",&,N,V_inputWhiteSrcBlue
T@"NSNumber",&,N,V_inputWhiteDstBlue
T@"NSString",&,N,V_inputColorSpace
stopAtTagIncludeOrientationFilter:
setSampleMode:
setTime:
imageWithData:
imageSourceWithCIImage:orientation:
newCompositionControllerWithComposition:
unsupportedError:object:
initWithError:
setVisionRequests:
initWithTargetPixelCount:
setScalePolicy:
result:
observations
requestRevision
numberWithUnsignedInteger:
initWithResult:
isAvailable
autoCalculatorWithImageData:orientation:
T{?=qiIq},N
available
TB,R,N,GisAvailable
T{?=qiIq},N,Vtime
inputCutoff
setInputCutoff:
_inputCutoff
Td,V_inputCutoff
convertFacePoint:toImagePointWithFaceRect:orientation:
averagePoints:pointCount:
averageCGPoints:pointCount:
initWithComposition:destinationURL:
initWithAutoLoopExportRequest:
outputSettings
colorSpaceFromVideoColorProperties:
sRGBColorSpace
initWithComposition:stabilizedVideoURL:longExposureDestinationURL:maskDestinationURL:destinationUTI:
outputColorSpace
destinationUTI
destinationLongExposureURL
destinationMaskURL
_destinationUTI
_destinationLongExposureURL
_destinationMaskURL
T@"NSString",R,V_destinationUTI
T@"NSURL",R,V_destinationLongExposureURL
T@"NSURL",R,V_destinationMaskURL
T@"NUColorSpace",R
setGenericCompletionBlock:
submitGenericRequest:
setCompletionBlock:
submitRequest:
flavor
setFlavor:
_flavor
Tq,N,V_flavor
recipe
setRecipe:
cleanAperture
setCleanAperture:
_recipe
_cleanAperture
T@"NSDictionary",C,N,V_recipe
T{?={?=qq}{?=qq}},N,V_cleanAperture
metalCommandBuffer
objectAtIndexedSubscript:
metalTexture
value
deserializeMetadataWithType:fromGlobalMetadata:error:
errorWithCode:reason:object:underlyingError:
majorVersion
minorVersion
initWithMetadataGroup:majorVersion:minorVersion:error:
createRGBA:
setTransferFunction:
setSourceColor:
setSourceDisparity:
setDestinationColor:
applyToRenderRequest:
setAperture:
setFocusDistance:
intValue
applyToRenderState:
cameraInfo
_updateRenderState:withLegacyCameraInfo:
setRenderState:
encodeRenderTo:withRenderRequest:
status
label
error
addCompletedHandler:
renderOnDevice:colorSize:disparitySize:quality:debugMode:usingBlock:
setTotalSensorCrop:
setRawSensorWidth:
setRawSensorHeight:
setFocalLenIn35mmFilm:
setConversionGain:
setReadNoise_1x:
setReadNoise_8x:
CGRectValue
vectorWithCGRect:
numberWithBool:
applyWithExtent:inputs:arguments:error:
processWithInputs:arguments:output:error:
formatForInputAtIndex:
outputFormat
outputIsOpaque
synchronizeInputs
allowPartialOutputRegion
roiForInput:arguments:outputRect:
applyWithInputImage:disparityImage:globalMetadata:timedMetadata:aperture:focusedDisparity:quality:debugMode:isHDR:error:
inputDisparityImage
inputGlobalRenderingMetadata
inputTimedRenderingMetadata
inputAperture
inputFocusedDisparity
inputRenderQuality
inputRenderDebugMode
inputIsHDR
setInputDisparityImage:
setInputRenderQuality:
setInputRenderDebugMode:
setInputIsHDR:
setInputGlobalRenderingMetadata:
setInputTimedRenderingMetadata:
setInputAperture:
setInputFocusedDisparity:
_inputIsHDR
_inputDisparityImage
_inputRenderQuality
_inputRenderDebugMode
_inputGlobalRenderingMetadata
_inputTimedRenderingMetadata
_inputAperture
_inputFocusedDisparity
T@"CIImage",&,N,V_inputDisparityImage
T@"NSNumber",&,N,V_inputRenderQuality
T@"NSNumber",&,N,V_inputRenderDebugMode
TB,N,V_inputIsHDR
T@"AVMetadataItem",&,N,V_inputGlobalRenderingMetadata
T@"AVTimedMetadataGroup",&,N,V_inputTimedRenderingMetadata
T@"NSNumber",&,N,V_inputAperture
T@"NSNumber",&,N,V_inputFocusedDisparity
recipeKey
flavorKey
stabilizedCropRect
T@"NSDictionary",C,N
T@"NSString",C,N
T{CGRect={CGPoint=dd}{CGSize=dd}},R,N
initWithSettings:inputs:
inputs
resolvedNodeWithCachedInputs:settings:pipelineState:error:
evaluationMode
nodeByReplayingAgainstCache:pipelineState:error:
crossfadeDuration
loopTimeRange
input
outputVideo:
videoFrames
setVideoFrames:
setRawTime:
initWithFilterName:settings:inputs:
nodeFromCache:cache:
setEvaluatedForMode:
errorWithCode:reason:object:
firstEnabledVideoTrackInAsset:error:
tracksWithMediaType:
addMutableTrackWithMediaType:preferredTrackID:
insertTimeRange:ofTrack:atTime:error:
debugDescriptionOfAssetTrack:
timeRange
conformRange:inRange:
outputVideoComposition:
instructions
setTimeRange:
trackID
numberWithInt:
setRequiredSourceTrackIDs:
setSourceIdentifier:forTrackID:
frameDuration
setFrameDuration:
renderSize
setRenderSize:
setInstructions:
setSourceTrackIDForFrameTiming:
audioMix
audioMixInputParametersWithTrack:
setVolume:atTime:
setVolumeRampFromStartVolume:toEndVolume:timeRange:
setInputParameters:
initWithInput:timeRange:crossfadeDuration:startTime:
shouldCacheNodeForPipelineState:
_evaluateVideo:
requiresVideoComposition
_evaluateVideoComposition:
requiresAudioMix
_evaluateAudioMix:
_crossfadeDuration
_loopTimeRange
T{?=qiIq},R,N,V_startTime
T{?={?=qiIq}{?=qiIq}},R,N,V_loopTimeRange
T{?=qiIq},R,N,V_crossfadeDuration
timeKey
longLongValue
scaleKey
numberWithLongLong:
keyFrameTime
setKeyFrameTime:
radiusKey
falloffKey
T@"NSString",R,N
radius
setRadius:
falloff
setFalloff:
identifier
name
inputKeys
settingForKey:
canBeEnabled
enabledKey
autoKey
hasAutoKeyInSchema
canHaveAuto
_setPrimitiveValue:forKey:
setValue:forUndefinedKey:
_primitiveValueForKey:
values
setFromAdjustment:
isEqualToAdjustmentController:
visualInputKeys
isEqual:forKeys:
defaultValue
displayName
displayInputKeys
enabled
setEnabled:
setIsAuto:
valueForUndefinedKey:
valuesForArrayInputKey:
interpolateFromStart:toEnd:progress:
timeFromInputKey:timescaleKey:
_changes
_identifier
_adjustment
T@"NSDictionary",R,N
TB,N
T@"NUIdentifier",&,N,V_identifier
T@"NUAdjustment",R,N,V_adjustment
T@"NSArray",R,N
TB,R,N
inputForKey:
_prewarmPortraitRendererWithPipelineState:error:
scale
_targetScaleForScale:
setScale:
sampleMode
initWithTargetScale:effectiveScale:sampleMode:input:
outputImageGeometry:
initWithExtent:renderScale:orientation:
_portraitQualityForRenderScale:
scaledSize
debugMode
prewarmRendererForDevice:colorSize:disparitySize:quality:debugMode:
videoProperties:
metadata
videoMetadataSamples
outputTimedMetadataSampleWithIdentifier:atTime:error:
mutableCopy
metadataGroup
colorProperties
outputImage:
globalMetadata
timedMetadata
sourceTransferFunction
renderTime
disparityKeyframes
keyframeInArray:closestToTime:
apertureKeyframes
imageByCompositingOverImage:
renderQuality
initWithInput:disparityInput:disparityKeyframes:apertureKeyframes:debugMode:
uniqueInputNode
_evaluateImage:
T{?=qiIq},R,N
Ti,R,N
T@"AVTimedMetadataGroup",R,N
T@"AVMetadataItem",R,N
applyOrientationFilter
submitGenericSynchronous:
faceRequestWithRequest:
submit:response:
submitSynchronous:error:
_group
_queue
_result
sourceSelectionKey
sourceSelectionForString:
stringForSourceSelection:
initWithTime:value:
addBytes:length:
initWithDictionaryRepresentation:
dictionaryRepresentation
nu_updateDigest:
_value
_time
labels
appendString:
appendFormat:
allocWithZone:
initWithType:source:identifier:confidence:
encodeInteger:forKey:
encodeDouble:forKey:
bounds
encodeObject:forKey:
expandedBounds
decodeIntegerForKey:
decodeDoubleForKey:
decodeObjectForKey:
stringValue
type
supportsSecureCoding
encodeWithCoder:
initWithCoder:
setBounds:
isHuman
isAnimal
setExpandedBounds:
edgeBleed
setEdgeBleed:
_type
_confidence
_source
_edgeBleed
_bounds
_expandedBounds
Tq,R,N,V_type
Tq,R,N,V_identifier
Td,R,N,V_confidence
Tq,R,N,V_source
T{CGRect={CGPoint=dd}{CGSize=dd}},N,V_bounds
T{CGRect={CGPoint=dd}{CGSize=dd}},N,V_expandedBounds
Tq,N,V_edgeBleed
componentsSeparatedByString:
_keyframesForKey:class:
_setKeyframes:forKey:
unsignedIntValue
removeObject:
cinematographyState
changesDictionaryTrimmedByTimeRange:
setCinematographyState:
setDisparityKeyframes:
aperture
setDebugMode:
trimToTimeRange:usingScript:
T@"NSArray",C,N
T@"NSNumber",&,N
T@"NSDictionary",&,N
initWithTime:homography:
keyframesFromDictionaryRepresentations:
homography
_homography
T{?=qiIq},R,N,V_time
T{?=[3]},R,N,V_homography
T@"NSDictionary",R,C,N
initWithCount:times:values:
interpolation
sampleAtTime:
initWithKeyframeArray:
homographyAtTime:
sparseSequence
_homographySequence
TQ,R,N
stopAtTagFilter:
properties
_computeCleanAperture:
kindKey
versionKey
setKind:
kind
setVersion:
version
removeAssetIdentifierFromMetadataArray:
metadataItem
setValue:
indexOfObjectPassingTest:
removeObjectAtIndex:
addAssetIdentifier:toMetadataDictionary:
addAssetIdentifier:toMetadataArray:
setInputVectorsForFilter:
temperature
setTemperature:
tint
setTint:
_temperature
_tint
Td,N,V_temperature
Td,N,V_tint
rawTime
nu_evaluateWithPipelineState:error:
_sampleMode
_rawTime
T{?=qiIq},N,V_time
T{?=qiIq},N,V_rawTime
Tq,N,V_sampleMode
T@"NSDictionary",R
T@"NSDictionary",&,V_recipe
renderNode
finalize
asset:
wantsOutputGeometry
cacheKey
analysisRequest
videoSource
setVideoSource:
_videoSource
T@"AVAsset",&,N,V_videoSource
T@"NSDictionary",&,N,V_recipe
shouldCancelHandler
ICShouldBeCanceled
ICReportProgress:
rangeMin
setRangeMin:
rangeMax
setRangeMax:
setShouldCancelHandler:
_rangeMin
_rangeMax
_shouldCancelHandler
Td,N,V_rangeMin
Td,N,V_rangeMax
T@?,C,N,V_shouldCancelHandler
keyframes
stabCropRect
analysisType
rawHomographies
T{?={?=qq}{?=qq}},R,N
initWithKeyframes:stabCropRect:analysisType:rawHomographies:
_keyframes
_analysisType
_rawHomographies
_stabCropRect
T@"NSArray",R,C,N,V_keyframes
T{?={?=qq}{?=qq}},R,N,V_stabCropRect
TQ,R,N,V_analysisType
T@"NSDictionary",R,N,V_rawHomographies
cleanApertureOfTrack:oriented:
isCanceled
allowedAnalysisTypes
canceledError:object:
nominalFrameRate
allowedCropFraction
setAllowedAnalysisTypes:
setAllowedCropFraction:
_allowedAnalysisTypes
_allowedCropFraction
TQ,N,V_allowedAnalysisTypes
Td,N,V_allowedCropFraction
canProvideMetadataForAVAsset:
canPerformGyroBasedStabilizationForAsset:
initWithAVAsset:
timedMetadataArray
trajectoryHomography
calculateWithRequest:completion:
calculateRAWWithRequest:completion:
rawProperties
initWithRequest:dataExtractor:options:
initWithDictionary:
minimumValue
maximumValue
faceBalanceResultFromFaceObservations:request:error:
faces
imageSize
addRect:
rawFaceBalanceFilter
initWithRect:
setRegionPolicy:
setResponseQueue:
image
faceRectFromNormalizedFaceRet:forImageExtent:scaleX:scaleY:
faceBalanceFromFaceImage:forFaceRect:
format
ARGB8
isEqualToPixelFormat:
RGBA8
BGRA8
validRegion
buffer
frameRect
bytesAtPoint:
readBufferRegion:withBlock:
initWithRequest:isRAW:
rawState
_rawState
Tq,R,V_rawState
valueWithBytes:objCType:
getValue:
pi_grayColorResultValue
pi_valueWithGrayColorResult:
T{?={?=[4d]}{?=[4d]}d},R
RGBResultValue
valueWithRGBResult:
T{?=[4d]},R
initWithComposition:useSushi:
_chooseNeutralGrayForNonSushi:
calculateColorWithProperties:completion:
_chooseTempTintForSushi:RAWProperties:brightness:
_correctedRGBResultFromResult:
_useTempTint:
inputNeutralXYFromRGB:
initWithName:responseQueue:
sharedFactory
bufferFactory
RGBAf
newStorageWithSize:format:
regionWithRect:
bytes
rowBytes
writeBufferInRegion:block:
returnStorage:
_brightnessMultiplierFromImageProperties:
begin
_computeGreenPercentage:
_submitGERenderRequest:
_submitGWRenderRequest:
rawCameraSpaceProperties
_whitePointColorFromGrayEdgesImage:grayWorldImage:greenChannelPercentage:RAWCameraSpaceProperties:
commitAndNotifyOnQueue:withBlock:
whiteValue
whiteFactor
_computeWhitePointColorWithGrayEdgesBuffer:grayWorldBuffer:greenChannelPercentage:RAWCameraSpaceProperties:
readBufferFromImage:withRGBAfBufferBlock:
RGBAh
setPixelFormat:
genericRGBLinearColorSpace
setColorSpace:
setTileSize:
rawSourceFilterIncludingOrientation
sushiLevel1Filter
initWithComposition:dataExtractor:options:
_configureRequest:
submitRequest:completion:
initWithSource:
initWithScript:
_bufferRenderClient
_imageDataClient
_useSushi
definitionKernel
inputBlurImage
setInputBlurImage:
inputIntensity
setInputIntensity:
_inputBlurImage
_inputIntensity
T@"CIImage",&,V_inputBlurImage
T@"NSNumber",&,V_inputIntensity
strengthKey
neutralKey
toneKey
grainKey
hueKey
inputStrengthKey
inputNeutralGammaKey
inputToneKey
inputHueKey
inputGrainKey
inputSeedKey
attributeStrengthKey
attributeNeutralGammaKey
attributeToneKey
attributeHueKey
attributeGrainKey
setStrength:
strength
setNeutral:
neutral
setTone:
tone
setGrain:
grain
setHue:
metadataItemsWithMetadataType:value:error:
writeMetadataType:value:toCGImageProperties:error:
readMetadataType:fromCGImageProperties:value:error:
videoMetadataForVariation:error:
setImageVariation:properties:error:
photoProcessingFlagsFromProperties:error:
setPhotoProcessingFlags:properties:error:
photoFeatureFlags:error:
setPhotoFeatureFlags:properties:error:
setRequest:
inputSize
setInputSize:
_request
_inputSize
T@"NUImageExportRequest",&,V_request
T{?=qq},V_inputSize
companionImageData
setCompanionImageData:
companionVideoURL
setCompanionVideoURL:
auxiliaryImages
setAuxiliaryImages:
canPropagateOriginalAuxiliaryData
setCanPropagateOriginalAuxiliaryData:
setProperties:
_canPropagateOriginalAuxiliaryData
_companionImageData
_companionVideoURL
_auxiliaryImages
_properties
T@"NSDictionary",&,V_auxiliaryImages
TB,V_canPropagateOriginalAuxiliaryData
T@"NSDictionary",C,V_properties
T{?=qq},D
T@"NSData",&,V_companionImageData
T@"NSURL",&,V_companionVideoURL
setGeometry:
_geometry
T@"NUImageGeometry",&,V_geometry
T@"NSData",&,V_data
initWithLevel:
displayP3ColorSpace
priority
setPriority:
colorSpace
pairingIdentifier
setPairingIdentifier:
_priority
_colorSpace
_pairingIdentifier
_scalePolicy
T@"NUPriority",&,V_priority
T@"NUColorSpace",&,V_colorSpace
T@"NSString",C,V_pairingIdentifier
T@"<NUScalePolicy>",&,V_scalePolicy
videoCodecType
bypassOutputSettingsIfNoComposition
applyVideoOrientationAsMetadata
requireHardwareEncoder
metadataProcessor
setMetadataProcessor:
increaseBitRateIfNecessary
setIncreaseBitRateIfNecessary:
setVideoCodecType:
preserveSourceColorSpace
setPreserveSourceColorSpace:
setBypassOutputSettingsIfNoComposition:
setApplyVideoOrientationAsMetadata:
setRequireHardwareEncoder:
includeCinematicVideoTracks
setIncludeCinematicVideoTracks:
_increaseBitRateIfNecessary
_preserveSourceColorSpace
_bypassOutputSettingsIfNoComposition
_applyVideoOrientationAsMetadata
_requireHardwareEncoder
_includeCinematicVideoTracks
_metadataProcessor
_videoCodecType
T@?,C,V_metadataProcessor
TB,N,V_increaseBitRateIfNecessary
T@"NSString",C,N,V_videoCodecType
TB,N,V_preserveSourceColorSpace
TB,N,V_bypassOutputSettingsIfNoComposition
TB,N,V_applyVideoOrientationAsMetadata
TB,N,V_requireHardwareEncoder
TB,N,V_includeCinematicVideoTracks
pathExtension
typeWithFilenameExtension:
fileType
defaultFormatForURL:
JPEGCompressionQuality
setCompressionQuality:
imageExportFormatForURL:
imageExportFormat
setImageExportFormat:
setJPEGCompressionQuality:
optimizeForSharing
setOptimizeForSharing:
applyImageOrientationAsMetadata
setApplyImageOrientationAsMetadata:
_optimizeForSharing
_applyImageOrientationAsMetadata
_imageExportFormat
_JPEGCompressionQuality
T@"NUImageExportFormat",C,V_imageExportFormat
Td,V_JPEGCompressionQuality
TB,V_optimizeForSharing
TB,V_applyImageOrientationAsMetadata
primaryURL
setPrimaryURL:
videoComplementURL
setVideoComplementURL:
videoPosterFrameURL
setVideoPosterFrameURL:
renderCompanionResources
setRenderCompanionResources:
reframeCropAdjustment
setReframeCropAdjustment:
reframeVideoAdjustment
setReframeVideoAdjustment:
_renderCompanionResources
_primaryURL
_videoComplementURL
_videoPosterFrameURL
_reframeCropAdjustment
_reframeVideoAdjustment
T@"NSURL",&,V_primaryURL
T@"NSURL",&,V_videoComplementURL
T@"NSURL",&,V_videoPosterFrameURL
TB,V_renderCompanionResources
T@"NUAdjustment",&,V_reframeCropAdjustment
T@"NUAdjustment",&,V_reframeVideoAdjustment
TB,V_applyVideoOrientationAsMetadata
globalSettings
decoratorRenderFiltersForImages
oneShotPortraitV2ExportFilter
orientationAsMetaDataFilter
setDestinationURL:
setFormat:
prepareImageExportRequest:options:completion:
setRenderToData:
destinationData
discreteProgressWithTotalUnitCount:
addVideoProperties:composition:options:error:
initWithProperties:
setMetadata:
_exportVideoToURL:composition:options:properties:progress:completion:
exportComposition:options:completionQueue:completion:
UUID
UUIDString
mismatchError:object:
defaultExportCodecForComposition:
conformsToType:
exportImageToURL:composition:options:completion:
exportVideoToURL:composition:options:completion:
adjustmentConstants
PICropAdjustmentKey
PIVideoReframeAdjustmentKey
exportImageToDataWithComposition:options:completion:
defaultManager
URLForDirectory:inDomain:appropriateForURL:create:error:
lastPathComponent
stringByDeletingPathExtension
stringByAppendingPathExtension:
URLByAppendingPathComponent:isDirectory:
resetImageProperties:preserveRegions:
addImageProperties:composition:options:error:
unknownError:object:
disableIOSurfacePortaitExport
setImageProperties:
setAuxImages:
setRenderWithIOSurface:
prepareAuxiliaryImagesFetchProperties:options:completion:
auxiliaryImagesProperties
setAuxiliaryImageType:
auxiliaryImage
setObject:forKey:
removeObjectForKey:
autoLoopAdjustmentController
metadataConverter
variationForFlavor:
depthAdjustmentController
semanticEnhanceAdjustmentController
livePhotoKeyFrameAdjustmentController
portraitAdjustmentController
unsignedIntegerValue
orientationAdjustmentController
noCropFilter
shouldTryVideoRotationFastPath:options:
_exportVideoToURLFull:composition:options:properties:progress:completion:
originalSize
preferredTransformFromOrientation:size:
initWithCGAffineTransform:
setPreferredTransform:
cropAdjustmentController
isOriginalCrop
setBitRateMultiplicationFactor:
setOutputSettings:
submitWithProgress:completion:
setMetadataConverter:
T@"<PICompositionExporterMetadataConverter>",&
exportComposition:toPrimaryURL:videoComplementURL:videoPosterFrameURL:priority:completionQueue:completion:
kernelsDictionaryWithString:
bilateralKernels
RGBToLabKernels
objectAtIndex:
boundsForPointArray:
enlargedBounds:withPoints:
shapeWithRect:
unionWith:
bilateralAddROI:destRect:userInfo:
bilateralAdd1Kernel
applyWithExtent:roiCallback:arguments:
vectorWithX:Y:
bilateralAdd2Kernel
vectorWithX:Y:Z:
bilateralAdd3Kernel
bilateralAdd4Kernel
bilateralAdd5Kernel
bilateralAdd6Kernel
bilateralAdd7Kernel
bilateralAdd8Kernel
bilateralAdd9Kernel
samplesPerPass
samplerWithImage:options:
RGBToLabKernel
subarrayWithRange:
doBilateralPass:points:weights:sums:slope:
bilateralFinalizeKernel
LabToRGBKernel
inputPoints
setInputPoints:
inputWeights
setInputWeights:
inputEdgeDetail
setInputEdgeDetail:
inputVersion
setInputVersion:
_inputPoints
_inputWeights
_inputEdgeDetail
_inputVersion
T@"NSArray",&,V_inputPoints
T@"NSArray",&,V_inputWeights
T@"NSNumber",&,V_inputEdgeDetail
T@"NSNumber",&,V_inputVersion
BWBilateralKernels
bilateralROI:destRect:userInfo:
bilateralLoop2Kernel
bilateralLoop5Kernel
bilateralLoop11Kernel
doBilateralLoop:points:weights:slope:
inputBorder
setInputBorder:
_inputBorder
T@"NSNumber",&,V_inputBorder
inputRadius
setInputRadius:
_inputRadius
T@"NSNumber",&,V_inputRadius
initWithMajor:minor:subMinor:
initWithMajor:minor:subMinor:platform:
decimalDigitCharacterSet
invertedSet
length
rangeOfCharacterFromSet:
asOrderedInteger
string
isEqualToAdjustmentVersion:
caseInsensitiveCompare:
stringByAppendingFormat:
versionWithMajor:minor:subMinor:platform:
versionFromString:
compare:
subMinorVersion
platform
_majorVersion
_minorVersion
_subMinorVersion
_platform
T@"NSString",R,W,N
TQ,R,N,V_majorVersion
TQ,R,N,V_minorVersion
TQ,R,N,V_subMinorVersion
T@"NSString",R,C,N,V_platform
amountKey
smartColorHDRStatistics
_isIdentity
_kernelV_lt1
_kernelV_gt1
_kernelCPos
_kernelCNeg
_kernelCast
inputVibrancy
setInputVibrancy:
inputContrast
setInputContrast:
inputCast
setInputCast:
T@"NSNumber",&,N,VinputVibrancy
T@"NSNumber",&,N,VinputContrast
T@"NSNumber",&,N,VinputCast
dataWithLength:
mutableBytes
render:toBitmap:rowBytes:bounds:format:colorSpace:
crossfadeDurationValueKey
crossfadeDurationTimescaleKey
startTimeValueKey
startTimeTimescaleKey
loopTimeRangeStartValueKey
loopTimeRangeStartTimescaleKey
loopTimeRangeDurationValueKey
loopTimeRangeDurationTimescaleKey
setLoopTimeRange:
setCrossfadeDuration:
T{?={?=qiIq}{?=qiIq}},N
portraitInfoKey
portraitInfo
spillMatteAllowedKey
setPortraitInfo:
canRenderPortraitEffect
setSpillMatteAllowed:
spillMatteAllowed
_version
Tq,N,V_version
T@"NSNumber",C,N
_noOpRemovalFunctions
noOpRemovalFunctions
copyOfAdjustmentRemovingNoOps:identifier:
valueKey
valueWithIdentifier:inGroup:ofClass:
objectFromData:withMajorVersion:minorVersion:
setTimedMetadata:
_cameraInfoFromMetadataGroup:
items
metadataItemsFromArray:filteredByIdentifier:
unarchivedObjectOfClasses:fromData:error:
focusedDisparity
_cameraInfo
_timedMetadata
_focusedDisparity
_aperture
T@"PTTimedRenderingMetadata",&,N,V_timedMetadata
Td,R,N,V_focusedDisparity
Td,R,N,V_aperture
T@"NSDictionary",R,N,V_cameraInfo
colorSize
disparitySize
quality
isInUse
initWithDevice:colorSize:disparitySize:quality:debugMode:
setInUse:
renderUsingBlock:
setLastUseTime:
lastUseTime
isEqualToDate:
removeObjectIdenticalTo:
initWithDevice:version:colorSize:disparitySize:
setDebugRendering:
setUseRGBA:
initWithDescriptor:
createRenderStateWithQuality:
_renderPipeline
_renderState
_inUse
_quality
_device
_debugMode
_lastUseTime
_colorSize
_disparitySize
inUse
TB,N,GisInUse,V_inUse
T@"NSDate",&,N,V_lastUseTime
T@"<MTLDevice>",R,N,V_device
T{?=qq},R,N,V_colorSize
T{?=qq},R,N,V_disparitySize
Ti,R,N,V_quality
Tq,R,N,V_debugMode
_highKeyHDR
debugDiagnostics
nonLocalizedFailureReason
userInfo
debugFilesEnabled
captureDebugDirectoryForComposition:
debugFilesPrefix
URLByAppendingPathComponent:
createDirectoryAtURL:withIntermediateDirectories:attributes:error:
debugLineDetectionImage
PNGRepresentationOfImage:format:colorSpace:options:
writeToURL:atomically:
faceObservationCache
wrapAsUnexpectedError:
getSizeOfAllFaces:
maxFaceSize
addMethodDiagnostics:details:
lowercaseString
hasFrontFacingCameraDimentions:
disableOnPanos
disableOnFrontFacingCameraImages
isFrontFacingCameraImage:pixelSize:
passesImagePropertiesCheck:
addMethodResultToDiagnostics:error:setYawPitchError:
passesFaceCheck:
shouldRunBuildingCheck
passesBuildingCheck:
submitVerified:
writeDebugDiagnosticsToDisk
overcaptureSourceFilter
primarySourceFilter
setComposition:
primaryImageProperties:
overcaptureImageProperties:
isFusedOvercapture
initWithMasterImageSize:stitchedImageSize:
initWithMasterImageSize:
setCropRect:
angleSeedDegreesCCW
setRollAngle:constrainCropRectWithTargetArea:
cropRect
setAngle:
null
minimumConfidence
minimumPitchCorrection
minimumYawCorrection
minimumAngleCorrection
canGenerateNewCropRect:
defaultFocalLength
maxAutoPitch
maxAutoYaw
maxAutoAngle
minimumPitchCorrectionArea
minimumYawCorrectionArea
exifOrientationAndCropStraightenOnly
passesConfidenceCheck:error:
passesMinimumCorrectionCheck:error:
setDebugLineDetectionImage:
perspectiveErrorFromCoreImage:
undoOrientation:forPitch:yaw:angle:
globalSession
releaseCachedResources
requestVisionCleanUp
setFaceObservationCache:
T@"PIFaceObservationCache",&,N
setMaxAutoYaw:
setMaxAutoPitch:
setMaxAutoAngle:
setMinimumPitchCorrection:
setMinimumYawCorrection:
setMinimumAngleCorrection:
setMinimumConfidence:
setMaxFaceSize:
setMinimumPitchCorrectionArea:
setMinimumYawCorrectionArea:
setDisableOnPanos:
setDisableOnFrontFacingCameraImages:
setShouldRunBuildingCheck:
setAngleSeedDegreesCCW:
setDebugFilesEnabled:
setDebugFilesPrefix:
_disableOnPanos
_disableOnFrontFacingCameraImages
_shouldRunBuildingCheck
_debugFilesEnabled
_faceObservationCache
_maxAutoYaw
_maxAutoPitch
_maxAutoAngle
_minimumPitchCorrection
_minimumYawCorrection
_minimumAngleCorrection
_minimumConfidence
_maxFaceSize
_minimumPitchCorrectionArea
_minimumYawCorrectionArea
_angleSeedDegreesCCW
_debugFilesPrefix
_debugDiagnostics
_debugLineDetectionImage
T@"CIImage",&,N,V_debugLineDetectionImage
T@"NSNumber",C,V_maxAutoYaw
T@"NSNumber",C,V_maxAutoPitch
T@"NSNumber",C,V_maxAutoAngle
Td,V_minimumPitchCorrection
Td,V_minimumYawCorrection
Td,V_minimumAngleCorrection
Td,V_minimumConfidence
Td,V_maxFaceSize
Td,V_minimumPitchCorrectionArea
Td,V_minimumYawCorrectionArea
TB,V_disableOnPanos
TB,V_disableOnFrontFacingCameraImages
TB,V_shouldRunBuildingCheck
Td,V_angleSeedDegreesCCW
TB,V_debugFilesEnabled
T@"NSString",C,V_debugFilesPrefix
T@"NSMutableDictionary",R,V_debugDiagnostics
T@"PIFaceObservationCache",&,N,V_faceObservationCache
luminanceKey
shortValue
angleKey
pitchKey
yawKey
xOriginKey
yOriginKey
widthKey
heightKey
constraintWidthKey
constraintHeightKey
isCropIdentityForImageSize:
isCropConstrained
angle
pitch
smartKey
originalCropKey
setPitch:
setYaw:
isGeometryIdentityForImageSize:
constraintWidth
constraintHeight
angleRadians
pitchRadians
yawRadians
autoCropped
isSmart
setConstraintWidth:
setConstraintHeight:
setAngleRadians:
setPitchRadians:
setYawRadians:
setAutoCropped:
setSmart:
setOriginalCrop:
T{CGRect={CGPoint=dd}{CGSize=dd}},N
smart
TB,N,GisSmart
originalCrop
TB,N,GisOriginalCrop
setInputDecoderVersion:
inputDecoderVersion
_updateSettingsWithInputLight:
overcaptureStatistics
smartToneAdjustmentsForValue:localLightAutoValue:andStatistics:
inputExposureKey
inputContrastKey
inputBrightnessKey
inputShadowsKey
inputHighlightsKey
inputBlackKey
inputLocalLightKey
inputRawHighlightsKey
inputLightKey
offsetExposureKey
offsetBrightnessKey
offsetContrastKey
offsetShadowsKey
offsetHighlightsKey
offsetBlackKey
offsetLocalLightKey
statisticsKey
inputLight
overcaptureStatisticsKey
attributeBrightnessKey
attributeContrastKey
attributeExposureKey
attributeHighlightsKey
attributeShadowsKey
attributeBlackPointKey
attributeLocalLightKey
attributeLightMapKey
attributeLightMapWidthKey
attributeLightMapHeightKey
computedSettings
setInputLight:
setInputExposure:
inputExposure
setInputBrightness:
inputBrightness
setInputShadows:
inputShadows
setInputHighlights:
inputHighlights
setInputBlack:
inputBlack
setInputLocalLight:
inputLocalLight
setInputRawHighlights:
inputRawHighlights
setStatistics:
setOvercaptureStatistics:
offsetBlack
setOffsetBlack:
offsetBrightness
setOffsetBrightness:
offsetContrast
setOffsetContrast:
offsetExposure
setOffsetExposure:
offsetHighlights
setOffsetHighlights:
offsetLocalLight
setOffsetLocalLight:
offsetShadows
setOffsetShadows:
_smartSettings
longExposureFusionKernels
alphaCompositingKernel
dynamismMapKernel
dynamismMapRefineKernel
rgbToLumaKernel
homographyKernel
nccKernel
nccCoarseKernel
blur7x7Kernel
blur5x5Kernel
blur3x3Kernel
fusionKernel
addEntriesFromDictionary:
dictionaryWithDictionary:
getResourceValue:forKey:error:
timeIntervalSinceReferenceDate
typeWithIdentifier:
absoluteString
assetIdentifierForURL:type:useEmbeddedPreview:
setAssetIdentifier:
initWithURL:UTI:
setUseEmbeddedPreview:
initWithSourceDefinitions:
setResolvedSourceDefinition:
initWithCIImage:orientation:
assetIdentifier
resolvedSourceDefinition
initWithImageSourceDefinition:videoSourceDefinition:
_imageRenderRequestWithComposition:wideGamut:
initWithTargetSize:
setExtentPolicy:
newCGImageFromBufferImage:
allValues
isMetalDeviceSupported:
supportsANE
isReadable
isPlayable
isExportable
videoAssetIsHighDynamicRange:
deviceSupportsHardware10BitHEVCEncoding
deviceSupportsHighDynamicRangeVideo
isAVAssetDolbyProfile5:error:
metadataTrackWithPortraitVideoDataInAsset:
isAssetUnsupportedLegacyPortraitVideo:
editSettings
areCPVAssetsEditable
capabilitiesForCurrentDevice
decodingSupportForAVAsset:
is3DEffect:
isPortraitEffect:
iosCropToolFilter
pipelineFiltersForShowingOriginalWithGeometry
resetTag:input:
getTagWithPath:error:
initWithScript:block:
forceGlassesMatteOff
setGlassesMatteAllowed:
forceSpillMatteOff
allowSpillMatteOnOlderPortraitV2Captures
PIAutoLoopAdjustmentKey
PIRedEyeAdjustmentKey
PIDepthAdjustmentKey
PIPortraitAdjustmentKey
PITrimAdjustmentKey
PIMuteAdjustmentKey
newAdjustmentWithIdentifier:
handlePIGlobalSettings:
knownFormatsVersionsMap
updateCropAdjustment:after:error:
standardUserDefaults
boolForKey:
prepareForPerformingRequests:error:
availableDecoderVersions
lastObject
boostParametersFromRawProperties:
currentVersion
imageSourceWithURL:type:proxyImage:orientation:useEmbeddedPreview:
videoSourceWithURL:
livePhotoSourceWithPhotoSource:videoSource:
compositionByRemovingVideoAndLivePhotoAdjustments:
newImageRenderClientWithName:
imagePropertiesRequestWithComposition:
videoPropertiesRequestWithComposition:
imageRenderRequestWithComposition:fitInSize:wideGamut:
imageRenderRequestWithComposition:fillInSize:wideGamut:
priorityWithLevel:
videoRenderRequestWithComposition:fitInSize:
isPortraitStageEffect:
isAVAssetEditable:
effectNameForFilterName:
filterNameForEffectName:
pipelineFiltersForCropping
pipelineFiltersForOriginalGeometry
pipelineFiltersForShowingOriginal
pipelineFiltersForRAWShowingOriginalWithGeometry
validatedCompositionCopyForComposition:mediaType:
updateCropAdjustmentController:after:error:
preheatEditDependencies
rawAdjustmentWithRawImageProperties:
allAdjustmentTypes
nonVisualAdjustmentTypes
PISmartToneAdjustmentKey
PISmartColorAdjustmentKey
PISmartBWAdjustmentKey
PIGrainAdjustmentKey
PIAutoEnhanceAdjustmentKey
PIWhiteBalanceAdjustmentKey
PIApertureRedEyeAdjustmentKey
PIRetouchAdjustmentKey
PIVignetteAdjustmentKey
PISharpenAdjustmentKey
PINoiseReductionAdjustmentKey
PIDefinitionAdjustmentKey
PICurvesAdjustmentKey
PILevelsAdjustmentKey
PISelectiveColorAdjustmentKey
PIEffectAdjustmentKey
PIEffect3DAdjustmentKey
PISlomoAdjustmentKey
PILivePhotoKeyFrameAdjustmentKey
PIVideoPosterFrameAdjustmentKey
PIHighResFusionAdjustmentKey
PIOrientationAdjustmentKey
PIRawAdjustmentKey
PIRawNoiseReductionAdjustmentKey
PISemanticEnhanceAdjustmentKey
PISourceSelectAdjustmentKey
PIVideoStabilizeAdjustmentKey
PIVideoCrossfadeLoopAdjustmentKey
PIPortraitVideoAdjustmentKey
PISourceAdjustmentKey
PIOvercaptureSourceAdjustmentKey
_PISmartToneAdjustmentKey
_PISmartColorAdjustmentKey
_PISmartBWAdjustmentKey
_PIGrainAdjustmentKey
_PIAutoEnhanceAdjustmentKey
_PIWhiteBalanceAdjustmentKey
_PIRedEyeAdjustmentKey
_PIApertureRedEyeAdjustmentKey
_PIRetouchAdjustmentKey
_PIVignetteAdjustmentKey
_PISharpenAdjustmentKey
_PINoiseReductionAdjustmentKey
_PIDefinitionAdjustmentKey
_PICurvesAdjustmentKey
_PILevelsAdjustmentKey
_PISelectiveColorAdjustmentKey
_PIEffectAdjustmentKey
_PIEffect3DAdjustmentKey
_PICropAdjustmentKey
_PITrimAdjustmentKey
_PISlomoAdjustmentKey
_PILivePhotoKeyFrameAdjustmentKey
_PIVideoPosterFrameAdjustmentKey
_PIAutoLoopAdjustmentKey
_PIHighResFusionAdjustmentKey
_PIMuteAdjustmentKey
_PIDepthAdjustmentKey
_PIPortraitAdjustmentKey
_PIOrientationAdjustmentKey
_PIRawAdjustmentKey
_PIRawNoiseReductionAdjustmentKey
_PISemanticEnhanceAdjustmentKey
_PIVideoReframeAdjustmentKey
_PISourceSelectAdjustmentKey
_PIVideoStabilizeAdjustmentKey
_PIVideoCrossfadeLoopAdjustmentKey
_PIPortraitVideoAdjustmentKey
_PISourceAdjustmentKey
_PIOvercaptureSourceAdjustmentKey
T@"NSString",R,N,V_PISmartToneAdjustmentKey
T@"NSString",R,N,V_PISmartColorAdjustmentKey
T@"NSString",R,N,V_PISmartBWAdjustmentKey
T@"NSString",R,N,V_PIGrainAdjustmentKey
T@"NSString",R,N,V_PIAutoEnhanceAdjustmentKey
T@"NSString",R,N,V_PIWhiteBalanceAdjustmentKey
T@"NSString",R,N,V_PIRedEyeAdjustmentKey
T@"NSString",R,N,V_PIApertureRedEyeAdjustmentKey
T@"NSString",R,N,V_PIRetouchAdjustmentKey
T@"NSString",R,N,V_PIVignetteAdjustmentKey
T@"NSString",R,N,V_PISharpenAdjustmentKey
T@"NSString",R,N,V_PINoiseReductionAdjustmentKey
T@"NSString",R,N,V_PIDefinitionAdjustmentKey
T@"NSString",R,N,V_PICurvesAdjustmentKey
T@"NSString",R,N,V_PILevelsAdjustmentKey
T@"NSString",R,N,V_PISelectiveColorAdjustmentKey
T@"NSString",R,N,V_PIEffectAdjustmentKey
T@"NSString",R,N,V_PIEffect3DAdjustmentKey
T@"NSString",R,N,V_PICropAdjustmentKey
T@"NSString",R,N,V_PITrimAdjustmentKey
T@"NSString",R,N,V_PISlomoAdjustmentKey
T@"NSString",R,N,V_PILivePhotoKeyFrameAdjustmentKey
T@"NSString",R,N,V_PIVideoPosterFrameAdjustmentKey
T@"NSString",R,N,V_PIAutoLoopAdjustmentKey
T@"NSString",R,N,V_PIHighResFusionAdjustmentKey
T@"NSString",R,N,V_PIMuteAdjustmentKey
T@"NSString",R,N,V_PIDepthAdjustmentKey
T@"NSString",R,N,V_PIPortraitAdjustmentKey
T@"NSString",R,N,V_PIOrientationAdjustmentKey
T@"NSString",R,N,V_PIRawAdjustmentKey
T@"NSString",R,N,V_PIRawNoiseReductionAdjustmentKey
T@"NSString",R,N,V_PISemanticEnhanceAdjustmentKey
T@"NSString",R,N,V_PIVideoReframeAdjustmentKey
T@"NSString",R,N,V_PISourceSelectAdjustmentKey
T@"NSString",R,N,V_PIVideoStabilizeAdjustmentKey
T@"NSString",R,N,V_PIVideoCrossfadeLoopAdjustmentKey
T@"NSString",R,N,V_PIPortraitVideoAdjustmentKey
T@"NSString",R,N,V_PISourceAdjustmentKey
T@"NSString",R,N,V_PIOvercaptureSourceAdjustmentKey
rawToneCurveProperties
curveValueAt:
outputImageFB3
outputImageFB0
kernelFB0
kernelFB3
T@"NSString",R
inputBoost
setInputBoost:
inputParams
setInputParams:
_inputBoost
_inputParams
Td,V_inputBoost
T@"NSString",C,V_inputVersion
T@"NSArray",C,V_inputParams
kernelsWithString:
stringForColorType:
arrayWithArray:
colorTypeForString:
initWithBase64EncodedString:options:
base64EncodedStringWithOptions:
inputTableImage
curvesKernel
setInputTableImage:
_inputTableImage
T@"CIImage",&,V_inputTableImage
surfaceStoragePool
workingColorSpace
dealloc
sRGBLinearColorSpace
set_accumError:
_start
setLabel:
renderImage:rect:toDestination:atPoint:error:
useAsCIRenderDestinationWithRenderer:block:
_isReadyForMoreData
_markAsFinished
markAsFinished
_accumError
_appendInputFrame:
_nextInputFrame
_initializeAccumulation
nextInputFrame
_accumulate:
_initializeAccumulation:
colorWithRed:green:blue:colorSpace:
imageWithColor:
_initializeStorage:image:error:
_accumulate:error:
CVPixelBuffer
imageWithCVPixelBuffer:options:
componentMax
componentMin
useAsCIImageWithOptions:renderer:block:
waitUntilDone
_exportOutputImage:format:colorSpace:toURL:uti:error:
_dynamismMapWithMinImage:maxImage:extent:
context
createCGImage:fromRect:format:colorSpace:deferred:
initWithSize:renderer:jobNumber:
cancel
start:
isReadyForMoreData
accumulate:error:
writeLongExposureImage:UTI:colorSpace:error:
writeMaskImage:UTI:error:
_pixelSize
_renderer
_temporaryDestinationStorage
_averageAccumulationStorage
_minimumAccumulationStorage
_maximumAccumulationStorage
_frameCount
_jobNumber
_stateQueue
_accumQueue
_accumSemaphore
_readySemaphore
_doneGroup
_inputFrames
_finished
_imageOptions
__accumError
T@"NSError",&,V__accumError
observation
T@"VNImageHomographicAlignmentObservation",R,C
setObservation:
setExtent:
_observation
_extent
T@"VNImageHomographicAlignmentObservation",C,N,V_observation
T{?={?=qq}{?=qq}},N,V_extent
newRenderPipelineStateForEvaluationMode:
_shouldWaitForDependentJobs
prepareNodeWithPipelineState:error:
nodeByReplayingAgainstCache:error:
setStillImage:
stillImage
prepareNode
registrationRequest
outputGeometry
renderScale
setGuideExtent:
renderer:
guideExtent
newPixelBufferOfSize:format:
initWithPixelBuffer:
jobNumber
waitUntilCompletedAndReturnError:
initWithTargetedCVPixelBuffer:options:
initWithCVPixelBuffer:options:
wantsOutputImage
wantsRenderScaleClampedToNativeScale
_stillImage
_guideExtent
T{?={?=qq}{?=qq}},N,V_guideExtent
T@"CIImage",&,N,V_stillImage
T@"VNImageHomographicAlignmentObservation",&,N,V_observation
initWithComposition:tag:responseQueue:
initWithComposition:responseQueue:
_pipelineFilters
inputDestinationImage
setInputDestinationImage:
inputCorrectionInfo
setInputCorrectionInfo:
inputCameraModel
setInputCameraModel:
_inputDestinationImage
_inputCorrectionInfo
_inputCameraModel
T@"CIImage",&,N,V_inputDestinationImage
T@"NSArray",&,N,V_inputCorrectionInfo
T@"NSString",&,N,V_inputCameraModel
posterFrameTime
setPosterFrameTime:
adjustmentHasPerspective:settings:
adjustmentHasCTM:settings:
_versionRules
versionRules
currentFormatVersion
indexOfObject:
formatVersionForAdjustment:identifier:
sourceDefinitions
fileURLWithPath:
startKey
startScaleKey
endKey
endScaleKey
rateKey
endTime
setEndTime:
rate
setRate:
cinematographyScript
assetWithURL:
setCinematographyScript:
loadWithAsset:changesDictionary:completion:
labelImageCache
setLabelImageCache:
setRenderTime:
roundedRectangleGeneratorFilter
setColor:
frameNearestTime:
focusDetection
allDetections
detectionType
trackIdentifier
_imageByAddingDetection:toImage:isPrimary:canvasSize:inverseOrientation:
initWithInput:assetURL:cinematographyState:monochrome:
_cinematographyScript
_labelImageCache
_renderTime
T@"PTCinematographyScript",&,N,V_cinematographyScript
T{?=qiIq},N,V_renderTime
T@"NSCache",&,N,V_labelImageCache
colorWithRed:green:blue:
groupIdentifier
blackColor
sourceOutCompositingFilter
setBackgroundImage:
textImageGeneratorFilter
setText:
setFontName:
setFontSize:
outputExposureKey
falseColorHDRKey
inputRAWGamutMapMaxKey
bytesPerPixel
initWithSize:format:
mutableBytesAtPoint:
RG16
dataWithBytes:length:
newLinearWideGamutColorSpace
inputWarmth
inputOrigI
inputOrigQ
linearWideGamutColorSpace
faceBalanceKernels
setInputOrigI:
setInputOrigQ:
setInputWarmth:
_inputOrigI
_inputOrigQ
_inputStrength
_inputWarmth
Td,N,V_inputOrigI
Td,N,V_inputOrigQ
Td,N,V_inputStrength
Td,N,V_inputWarmth
initWithLength:
initWithImageProvider:width:height:format:colorSpace:options:
inputPointsR
curvePointsFromDictionaries:
inputPointsG
inputPointsB
inputPointsL
calculateCurveTable:
tableImageFromRed:green:blue:luminance:
setInputPointsR:
setInputPointsG:
setInputPointsB:
setInputPointsL:
_inputPointsR
_inputPointsG
_inputPointsB
_inputPointsL
T@"NSArray",&,V_inputPointsR
T@"NSArray",&,V_inputPointsG
T@"NSArray",&,V_inputPointsB
T@"NSArray",&,V_inputPointsL
colorBalanceKernels
gHDRtoPPKernel
PPtogHDRKernel
applyInputConversion:
colorBalanceKernel
applyOutputConversion:
inputWarmTemp
setInputWarmTemp:
inputWarmTint
setInputWarmTint:
inputHasFace
setInputHasFace:
inputIsRaw
setInputIsRaw:
_inputWarmTemp
_inputWarmTint
_inputHasFace
_inputIsRaw
T@"NSNumber",&,N,V_inputWarmTemp
T@"NSNumber",&,N,V_inputWarmTint
T@"NSNumber",&,N,V_inputStrength
T@"NSNumber",&,N,V_inputHasFace
T@"NSNumber",&,N,V_inputIsRaw
substringFromIndex:
_map
getMap
HDRFilterForSDRFilter:
alignmentKey
alignment
setAlignment:
PUEditSettings
socPseudoColorFilter
IPXEditSettings
falseColorHDR
setFalseColorHDR:
decoratorRenderFiltersForVideos
setForceGlassesMatteOff:
setForceSpillMatteOff:
setAllowSpillMatteOnOlderPortraitV2Captures:
_settings
_forceGlassesMatteOff
_forceSpillMatteOff
_allowSpillMatteOnOlderPortraitV2Captures
TB,N,V_forceGlassesMatteOff
TB,N,V_forceSpillMatteOff
TB,N,V_allowSpillMatteOnOlderPortraitV2Captures
boolSettingForKey:defaultValue:
shouldUseMetalRenderer
initWithMetalDevice:options:
initWithVideoExportRequest:
metalRenderer
autoLoopExportRequest
setUpContext:
currentContext
contextForContext:
isObject
setError:
toDictionary
node
initWithKeyframes:stabCropRect:input:
initWithNode:context:
jsContext
toRect
overcaptureRectForStitchedOvercaptureSize:overcaptureVideoComplementSize:primarySize:autoLoopStabilizedCropRect:
bundleForClass:
URLForResource:withExtension:
newPhotosPipelineAtSourceURL:error:
initWithURL:
newPhotosPipeline:
renderContext
cancelAllRequests
apertureRedEyeResultFromFaceObservations:imageSize:
landmarks
leftEye
rightEye
pointCount
normalizedPoints
_faceRequest
localLightHDRStatisticsNoProxy
inputLightMap
inputLightMapWidth
inputLightMapHeight
inputGuideImage
_polyKernelHDR
_shadowKernelHDR
inputLightMapImage
inputSmartShadows
initWithData:width:height:bytesPerRow:bytesPerComponent:format:colorspace:
initWithBytes:width:height:bytesPerRow:bytesPerComponent:format:colorspace:
elementByteSize
rowElements
bufferColorspace
T@"NSMutableData",&,Vdata
TQ,R,VelementByteSize
TQ,R,VrowElements
TQ,R,Vwidth
TQ,R,Vheight
Ti,R,Vformat
initWithTargetPixelSize:
_calculateBlackAndWhiteSettingsFromBufferImage:
CIFormat
getNonNormalizedSettings:
createHueArray
imageWithBitmapData:bytesPerRow:size:format:options:
hueArrayImage:
smartBlackWhiteKernel
inputNeutralGamma
setInputNeutralGamma:
inputTone
setInputTone:
inputHue
setInputHue:
inputGrain
setInputGrain:
inputSeed
setInputSeed:
inputScaleFactor
setInputScaleFactor:
T@"NSNumber",C,N,VinputStrength
T@"NSNumber",C,N,VinputNeutralGamma
T@"NSNumber",C,N,VinputTone
T@"NSNumber",C,N,VinputHue
T@"NSNumber",C,N,VinputGrain
T@"NSNumber",C,N,VinputSeed
T@"NSNumber",C,N,VinputScaleFactor
smartBlackAndWhiteStatistics
smartBlackAndWhiteAdjustmentsForValue:andStatistics:
inputCorrectionInfoKey
hasCorrections
stopAtTagIncludeGeometryFilter:
histogramOptimizationFilter
histogramCalculationColorSpace
setHistogramCalculationColorSpace:
histogram
calculateSettingsForImageHistogram:
percentile:
calculateSettingsForSingleChannelHistogram:suffix:
luminance
green
blue
_adjustmentControllerForKey:creatingIfNecessary:expectedClass:
smartColorAdjustmentControllerCreatingIfNecessary:
smartBWAdjustmentControllerCreatingIfNecessary:
cropAdjustmentControllerCreatingIfNecessary:
redEyeAdjustmentControllerCreatingIfNecessary:
livePhotoKeyFrameAdjustmentControllerCreatingIfNecessary:
videoPosterFrameAdjustmentControllerCreatingIfNecessary:
depthAdjustmentControllerCreatingIfNecessary:
trimAdjustmentControllerCreatingIfNecessary:
slomoAdjustmentControllerCreatingIfNecessary:
effectAdjustmentControllerCreatingIfNecessary:
effect3DAdjustmentControllerCreatingIfNecessary:
portraitAdjustmentControllerCreatingIfNecessary:
autoLoopAdjustmentControllerCreatingIfNecessary:
highResFusionAdjustmentControllerCreatingIfNecessary:
rawAdjustmentControllerCreatingIfNecessary:
rawNoiseReductionAdjustmentControllerCreatingIfNecessary:
sharpenAdjustmentControllerCreatingIfNecessary:
whiteBalanceAdjustmentControllerCreatingIfNecessary:
noiseReductionAdjustmentControllerCreatingIfNecessary:
definitionAdjustmentControllerCreatingIfNecessary:
vignetteAdjustmentControllerCreatingIfNecessary:
videoReframeAdjustmentControllerCreatingIfNecessary:
sourceSelectAdjustmentControllerCreatingIfNecessary:
videoStabilizeAdjustmentControllerCreatingIfNecessary:
videoCrossfadeLoopAdjustmentControllerCreatingIfNecessary:
semanticEnhanceAdjustmentControllerCreatingIfNecessary:
portraitVideoAdjustmentControllerCreatingIfNecessary:
smartToneAdjustmentController
smartColorAdjustmentController
smartBWAdjustmentController
redEyeAdjustmentController
videoPosterFrameAdjustmentController
trimAdjustmentController
slomoAdjustmentController
effectAdjustmentController
effect3DAdjustmentController
highResFusionAdjustmentController
rawAdjustmentController
rawNoiseReductionAdjustmentController
sharpenAdjustmentController
whiteBalanceAdjustmentController
noiseReductionAdjustmentController
definitionAdjustmentController
vignetteAdjustmentController
videoReframeAdjustmentController
videoStabilizeAdjustmentController
videoCrossfadeLoopAdjustmentController
portraitVideoAdjustmentController
T@"PIAdjustmentConstants",R,N
isEditable
initWithX:y:editable:
_editable
Td,R,N,V_x
Td,R,N,V_y
editable
TB,R,N,GisEditable,V_editable
smartToneHDRStatistics
_kernelBneg
_kernelBpos
_kernelRH
_kernelH
_kernelC_hdr
_kernelC
T@"NSNumber",&,N,VinputExposure
T@"NSNumber",&,N,VinputBrightness
T@"NSNumber",&,N,VinputShadows
T@"NSNumber",&,N,VinputHighlights
T@"NSNumber",&,N,VinputBlack
T@"NSNumber",&,N,VinputRawHighlights
loadFusionTuningParameters
stringSettingForKey:defaultValue:
dictionaryWithContentsOfFile:
_debugDumpIntermediateImages
inputMaskImage
inputStillImage
inputRenderScale
inputVideoScale
inputAlignmentExtent
inputAlignmentTransform
valueAtIndex:
alignImage:transform:extent:
imageByApplyingTransform:highQualityDownsample:
_computeNCCMapFromImage:toImage:scale:
_refineMaskImage:guideImage:scale:
_fuseImage:withGuideImage:weightImage:maskImage:
debugDumpIntermediateImages
currentDirectoryPath
writeToTIFF:
applyWithExtent:roiCallback:inputImage:arguments:
setInputStillImage:
setInputMaskImage:
setInputRenderScale:
setInputVideoScale:
setInputAlignmentExtent:
setInputAlignmentTransform:
_inputStillImage
_inputMaskImage
_inputRenderScale
_inputVideoScale
_inputAlignmentExtent
_inputAlignmentTransform
T@"CIImage",&,N,V_inputStillImage
T@"CIImage",&,N,V_inputMaskImage
T@"NSNumber",&,N,V_inputRenderScale
T@"NSNumber",&,N,V_inputVideoScale
T@"CIVector",&,N,V_inputAlignmentExtent
T@"CIVector",&,N,V_inputAlignmentTransform
isEnabled
evaluate:input:pipelineState:error:
initWithPipelineState:
mediaTypeForComposition:
hasStaticTime
valueWithCMTime:
beginGroupWithName:error:
renderNodeFromSource:settings:error:
initWithAffineTransform:
transformNodeWithInput:transform:error:
inputForPath:error:
addTagWithName:inputNode:error:
endGroupWithName:error:
enableHDRSupport
auxiliaryImageType
allAssetsCanUseHDRPipeline
imageProperties:
initWithCGColorSpace:
isHDR
cacheNode:type:settings:error:
performLongExposureFusionForComposition:longExposureImage:useHDRFilter:error:
scaleNode:scale:error:
overcaptureRectForAutoCrop:overcaptureVideoComplementSize:primarySize:CGRect:
nodeByApplyingFilterWithName:useHDRFilter:settingsAndInputs:
performApertureRedeyeOnImage:useHDRFilter:redEyeAdjustment:
performRedeyeOnImage:useHDRFilter:redEyeAdjustment:
auxiliaryImageFromComposition:type:mediaComponentType:error:
versionForPortraitEffect:
vectorWithFloats:
auxiliaryDataInfoMetadata
depthCameraCalibrationData
scaleMultiplyOfScalar:
isCIFilterAvailable:propertyName:
initWithInput:scale:
remapPortraitV2Strength:portraitEffectKind:
isSourceAvailable:sourceSettings:
portraitVideo:disparityInput:disparityKeyframes:apertureKeyframes:debugMode:error:
portraitVideoDebugDetectedObjects:source:cinematographyState:monochrome:error:
cropNode:cropRect:cropSettings:
isEqualToNumber:
initWithInput:
livePhotoKeyFrameMetadataFromNode:time:error:
trimInput:startTime:endTime:error:
createSloMoWithInput:startTime:endTime:rate:error:
grainInputSeedFromFrameTime
sharpnessWithIntensity:
videoReframe:reframes:error:
videoCrossfadeLoop:crossfadeAdjustment:error:
perspectiveTransformWithPitch:yaw:roll:imageRect:
straightenTransformWithAngle:extent:
originalCleanAperture
orientedNode:withOrientation:
scaledVector:
debugNodeByApplyingFilterWithName:useHDRFilter:settingsAndInputs:
_processedRenderNodeForComposition:input:pipelineState:error:
P3Kernel
deserializeFromDictionary:error:
sourceSelectSchema
photosCompositionSchema
rawSchema
rawNoiseReductionSchema
retouchSchema
smartToneSchema
smartColorSchema
whiteBalanceSchema
cropSchema
trimSchema
slomoSchema
livePhotoKeyFrameSchema
muteSchema
videoPosterFrameSchema
autoLoopSchema
orientationSchema
effectSchema
redEyeSchema
apertureRedEyeSchema
smartBlackAndWhiteSchema
grainSchema
sharpenSchema
definitionSchema
noiseReductionSchema
vignetteSchema
levelsSchema
curvesSchema
selectiveColorSchema
depthEffectSchema
effect3DSchema
portraitEffectSchema
highResFusionSchema
videoReframeSchema
videoStabilizeSchema
videoCrossfadeLoopSchema
debugSchema
semanticEnhance
portraitVideoSchema
renderPipelineForIdentifier:
registerPhotosSchema
registerSchemas:error:
registerRenderPipeline:forIdentifier:
T@"NUIdentifier",R
candidacy
descriptionForCandidacy:
setCandidacy:
shouldAllowPerspectiveCorrection
finalizerError
rollAngleDegrees
pitchAngleDegrees
yawAngleDegrees
getCropRectThatCompletelyContainsMasterImageForPitch:yaw:roll:
initWithDisposition:composition:
performNextActionWithCompletion:
shouldPerformAction:
markActionAsPerformed:
processHorizonResult:
setFinalizerError:
performHorizonCorrectionWithCompletion:
processPerspectiveResult:
performPerspectiveCorrectionWithCompletion:
hasPerformedAction:
performedActions
setPerformedActions:
floatForKey:
setShouldPerformAutoCrop:
setShouldPerformAutoStraighten:
setShouldUseAutoStraightenVerticalDetector:
setMaxAutoStraighten:
setMinAutoStraighten:
setAutoStraightenDominantAngleDiffThreshold:
setAutoStraightenVerticalAngleThreshold:
setRollAngleDegrees:
setPitchAngleDegrees:
setYawAngleDegrees:
_candidacy
_finalizerError
_performedActions
_rollAngleDegrees
_pitchAngleDegrees
_yawAngleDegrees
T@"NSError",&,N,V_finalizerError
TQ,N,V_performedActions
Td,N,V_rollAngleDegrees
Td,N,V_pitchAngleDegrees
Td,N,V_yawAngleDegrees
TQ,N,V_candidacy
disposition
_disposition
Tq,R,V_disposition
T@"NUComposition",R,C,V_composition
cropFraction
setCropFraction:
setAnalysisType:
TQ,N
shouldPerformAutoStraighten
shouldPerformAutoCrop
shouldUseAutoStraightenVerticalDetector
maxAutoStraighten
autoCropFilter
minAutoStraighten
undoExifOrientation:error:
autoStraightenVerticalAngleThreshold
autoStraightenDominantAngleDiffThreshold
_shouldPerformAutoCrop
_shouldPerformAutoStraighten
_shouldUseAutoStraightenVerticalDetector
_autoStraightenVerticalAngleThreshold
_autoStraightenDominantAngleDiffThreshold
_maxAutoStraighten
_minAutoStraighten
TB,V_shouldPerformAutoCrop
TB,V_shouldPerformAutoStraighten
TB,V_shouldUseAutoStraightenVerticalDetector
T@"NSNumber",C,V_autoStraightenVerticalAngleThreshold
T@"NSNumber",C,V_autoStraightenDominantAngleDiffThreshold
Td,V_maxAutoStraighten
Td,V_minAutoStraighten
noGeometryFilter
inputToCropFilter
stitchedOvercaptureRect:primaryRect:forComposition:error:
initWithMasterImageRect:stitchedImageRect:
setRollRadians:
integralCropRect:
inputColorKey
_updateSettingsWithInputColor:
inputSaturationKey
inputCastKey
offsetSaturationKey
offsetCastKey
attributeVibrancyKey
attributeCastKey
setInputColor:
inputColor
setInputSaturation:
inputSaturation
setOffsetCast:
offsetCast
setOffsetSaturation:
offsetSaturation
_stats
iptFromLinearInto:fromRed:green:blue:
hueAngleFrom:
selectiveColorKernels
colorWithRed:green:blue:alpha:colorSpace:
iptHueAngleFromRed:green:blue:
convertToIPT:
hueSatLumTable
convertFromIPT:
inputCorrections
setInputCorrections:
_inputCorrections
T@"NSArray",&,N,V_inputCorrections
setKeyframeSequence:
setStabCropRect:
setShouldApplyWatermark:
setInputVideoProperties:
setSize:
inputVideoProperties
keyframeSequence
_stabilizeImage:cleanRect:cropRect:transform:geometry:
vectorWithCGPoint:
pi_imageByApplyingStabilizationWatermark
_evaluateVideoProperties:
_evaluateImageGeometry:
canPropagateOriginalLivePhotoMetadataTrack
shouldApplyWatermark
_shouldApplyWatermark
_keyframeSequence
_inputVideoProperties
_frameDuration
T@"PIReframeKeyframeSequence",&,N,V_keyframeSequence
T{?={?=qq}{?=qq}},N,V_stabCropRect
T@"<NUVideoProperties>",&,N,V_inputVideoProperties
T{?=qiIq},N,V_frameDuration
TB,N,V_shouldApplyWatermark
imageWithCGImage:
dictionaryForKey:
formatDescriptions
setSubjects:
setEstimatedCenterMotion:
setEstimatedMotionBlur:
setTrajectoryHomography:
subjects
estimatedCenterMotion
estimatedMotionBlur
_subjects
_estimatedCenterMotion
_estimatedMotionBlur
_trajectoryHomography
T{?=qiIq},R,V_time
T@"NSArray",R,V_subjects
T{CGVector=dd},R,V_estimatedCenterMotion
T{CGVector=dd},R,V_estimatedMotionBlur
T{?=[3]},R,V_trajectoryHomography
exceptionWithName:reason:userInfo:
tracks
encodedPixelSizeOfTrack:oriented:
extractMetadata
initWithContentsOfFile:
assetReaderWithAsset:error:
initWithTrack:outputSettings:
canAddOutput:
addOutput:
initWithAssetReaderTrackOutput:
startReading
nextTimedMetadataGroup
dataType
dataValue
subjectsFromMetadata:
centerMotionVectorFromMetadata:
motionBlurVectorFromMetadata:
trajectoryeHomographyFromMetadata:containsV3Metadata:
overwriteTrackingMetadataWithPlist:
_asset
_videoTrack
_mdataTrack
ndcMetadataTransform
pxlMetadataTransform
T@"NSArray",R,N,VtimedMetadataArray
vectorWithX:
RGBToYIQKernel
YIQToRGBKernel
isDefaultWarmth:
whiteBalanceKernel
warmth
setWarmth:
setY:
setI:
setQ:
_strength
_warmth
T@"NSNumber",&,N,V_strength
T@"NSNumber",&,N,V_warmth
T@"NSNumber",&,N,V_y
T@"NSNumber",&,N,V_i
T@"NSNumber",&,N,V_q
isHDRComposition:
setDataExtractor:
force
_options
initWithRequest:options:
internalComposition
options
setOptions:
setForce:
_force
TB,V_force
initWithComposition:location:touchDiameter:
_location
_touchDiameter
setPerservesAlpha:
_interpolateGrainKernel
_paddedTileKernel
_grainBlendAndMixKernel
inputISO
setInputISO:
inputAmount
setInputAmount:
T@"NSNumber",C,N,VinputISO
T@"NSNumber",C,N,VinputAmount
writeCGImage:fileURL:options:
path
writeImage:toDirectoryAtPath:withBasename:
stringByAppendingPathComponent:
writeImage:fileURL:
writeCGImage:fileURL:
writeImage:toTemporaryDirectoryWithBasename:
currentSoftwareVersion
buildNumber
edgesKey
baseAddress
bytesPerRow
region
convertFloat:toFixed16:count:
RGBA16
initWithSize:format:rowBytes:mutableBytes:
convertFixed16:toFloat:count:
initWithSize:format:rowBytes:bytes:
initWithBuffer:colorSpace:validRegion:
initWithMutableBuffer:colorSpace:validRegion:
copyPixelsFromImage:srcRect:destImage:destOrigin:
ROIForCenterPoint:radius:
pointValue
inputSpots
auxiliaryImage:
underlyingAVDepthData
isDepthDataFiltered
depthDataQuality
cameraCalibrationData
auxiliaryCoreGraphicsInfoDictionary:
depthBlurEffectRenderingParameters
getMinMaxSimulatedApertureFrom:minValue:maxValue:version:
methodForSelector:
depthBlurEffectSimulatedAperture
portraitLightingEffectStrength
initFromMinAperture:maxAperture:aperture:portraitStrength:SDOFRenderingVerion:depthVersionInfo:
minimumAperture
maximumAperture
portraitStrength
SDOFRenderingVersion
portraitMajorVersion
portraitMinorVersion
valuesAtCaptureFromImageProperties:error:
setPortraitStrength:
setMinimumAperture:
setMaximumAperture:
setSDOFRenderingVersion:
setPortraitMajorVersion:
setPortraitMinorVersion:
depthVersionInfo
setDepthVersionInfo:
_portraitStrength
_minimumAperture
_maximumAperture
_SDOFRenderingVersion
_portraitMajorVersion
_portraitMinorVersion
_depthVersionInfo
Tf,N,V_aperture
Tf,N,V_portraitStrength
T@"NSNumber",&,N,V_minimumAperture
T@"NSNumber",&,N,V_maximumAperture
TQ,N,V_SDOFRenderingVersion
TQ,N,V_portraitMajorVersion
TQ,N,V_portraitMinorVersion
T{?=ii},N,V_depthVersionInfo
focusRectDictionaryFromRect:
isStillImageDisparity:
_calculateWithImageProperties:valuesAtCapture:completion:
portraitInfoDictionaryFromFaceObservations:metadata:orientation:valuesAtCapture:
canApplyPortraitEffectsWithMetadata:
portraitEffectInfoDictionaryFromFaceObservations:orientation:valuesAtCapture:
depthEffectInfoDictionaryFromFaceObservations:metadata:orientation:valuesAtCapture:
focusRectDictionaryFromMetadata:
depthEffectInfoDictionaryFromFaceObservations:focus:valuesAtCapture:lumaNoiseScale:orientation:
nose
allPoints
faceJunkinessIndex
faceOrientationIndex
roll
faceObservationsData
indexesOfShallowDepthOfFieldObservations
objectsAtIndexes:
faceOrientation
focusRectangle
minimumApertureFocalRatio
maximumApertureFocalRatio
apertureFocalRatio
luminanceNoiseAmplitude
portraitInfoDictionaryFromCameraMetadata:
resetTag:input:error:
noRedEyeFilter
noTrimFilter
noMuteFilter
stripAllTimeAdjustmentsFilter
noOrientationFilter
perspectiveStraightenWithoutCropFilter
preGeometryFilter
postGeometryFilter
autoloopStabilizedVideoFilter
spatialOvercaptureVideoSourceFilter
colorTypeKey
faceStrengthKey
faceWarmthKey
faceIKey
faceQKey
grayStrengthKey
grayWarmthKey
grayYKey
grayIKey
grayQKey
temperatureKey
tintKey
warmTempKey
warmFace
warmTintKey
warmFaceKey
colorType
setColorType:
faceStrength
setFaceStrength:
faceWarmth
setFaceWarmth:
faceI
setFaceI:
faceQ
setFaceQ:
grayStrength
setGrayStrength:
grayWarmth
setGrayWarmth:
grayY
setGrayY:
grayI
setGrayI:
grayQ
setGrayQ:
warmTemp
setWarmTemp:
warmTint
setWarmTint:
setWarmFace:
sourceDefinition:
redEyeSpotsWithCorrectionInfo:
depthInfo
depthInfoKey
apertureKey
glassesMatteAllowedKey
canRenderDepth
setDepthInfo:
capturedAperture
glassesMatteAllowed
keyframesKey
stabCropRectKey
setKeyframes:
copyKeyframesTrimmingToTimeRange:
T{?={?=qq}{?=qq}},N
indexOfObject:inSortedRange:options:usingComparator:
setColorMatrix:
setParameters:
computeCurvesForImageHistogram:
_defaultCurveArray
replaceObjectAtIndex:withObject:
dictionariesFromPoints:
autoValuesForBlackPoint:whitePoint:
curvePointAtIndex:blackPoint:whitePoint:histogram:
insertObject:atIndex:
B24@0:8@16
#16@0:8
@16@0:8
@24@0:8:16
@32@0:8:16@24
@40@0:8:16@24@32
B16@0:8
B24@0:8#16
B24@0:8:16
Vv16@0:8
Q16@0:8
^{_NSZone=}16@0:8
B24@0:8@"Protocol"16
@"NSString"16@0:8
@"<NURenderStatistics>"16@0:8
@"PTCinematographyTrack"16@0:8
@24@0:8@16
v16@0:8
@"PTCinematographyTrack"
B24@0:8o^@16
v76@0:8{?=qiIq}16{CGRect={CGPoint=dd}{CGSize=dd}}40f72
{?=qiIq}16@0:8
v40@0:8{?=qiIq}16
{CGPoint=dd}16@0:8
v32@0:8{CGPoint=dd}16
@?16@0:8
v24@0:8@?16
v24@0:8@16
v20@0:8B16
{CGPoint="x"d"y"d}
{?="value"q"timescale"i"flags"I"epoch"q}
@64@0:8@16{?=qiIq}24{CGPoint=dd}48
@24@0:8^{_NSZone=}16
q16@0:8
@"CIImage"
f16@0:8
v20@0:8f16
@40@0:8@16q24^{CGColorSpace=}32
@"PFStoryRecipeDisplayAssetNormalization"
B32@0:8@16@24
v24@0:8d16
d16@0:8
v32@0:8q16d24
v32@0:8@16q24
@20@0:8f16
{CGRect={CGPoint=dd}{CGSize=dd}}16@0:8
v48@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16
{CGRect="origin"{CGPoint="x"d"y"d}"size"{CGSize="width"d"height"d}}
@80@0:8@16{?=qiIq}24{CGRect={CGPoint=dd}{CGSize=dd}}48
@"NSNumber"
@32@0:8@16@24
#24@0:8@16
v32@0:8@16@24
v32@0:8@16@?24
B28@0:8@16B24
B36@0:8@16@24B32
B40@0:8@16@24@?32
v24@0:8q16
@"NUComposition"
{?="hasDidAdd"B"hasDidRemove"B"hasDidUpdate"B"hasDidUpdateMultiple"B"hasClassForController"B}
@"NSDictionary"
@"<PICompositionControllerDelegate>"
@48@0:8@16@24@32o^@40
B32@0:8@16o^@24
@56@0:8@16@24@32@40o^@48
@40@0:8@16@24o^@32
B48@0:8@16@24@32o^@40
@32@0:8@16o^@24
@"NSData"
@"NSString"
d40@0:8@16d24^B32
@32@0:8@16q24
{CGPoint=dd}72@0:8{CGPoint=dd}16{CGRect={CGPoint=dd}{CGSize=dd}}32q64
{CGPoint=dd}32@0:8r^{CGPoint=dd}16Q24
@56@0:8@16@24@32@40@48
@"NSURL"
{?={?=qq}{?=qq}}16@0:8
v48@0:8{?={?=qq}{?=qq}}16
{?="origin"{?="x"q"y"q}"size"{?="width"q"height"q}}
B48@0:8@16@24@32^@40
i20@0:8i16
i16@0:8
{CGRect={CGPoint=dd}{CGSize=dd}}60@0:8i16@20{CGRect={CGPoint=dd}{CGSize=dd}}28
@92@0:8@16@24@32@40@48@56@64@72B80o^@84
@"AVMetadataItem"
@"AVTimedMetadataGroup"
@120@0:8@16{?={?=qiIq}{?=qiIq}}24{?=qiIq}72{?=qiIq}96
@24@0:8o^@16
{?={?=qiIq}{?=qiIq}}16@0:8
{?="start"{?="value"q"timescale"i"flags"I"epoch"q}"duration"{?="value"q"timescale"i"flags"I"epoch"q}}
v40@0:8@16@24d32
{?=qiIq}32@0:8@16@24
@"NSMutableDictionary"
@"NUIdentifier"
@"NUAdjustment"
@56@0:8@16@24@32@40q48
{?=qq}32@0:8{?=qq}16
i32@0:8{?=qq}16
@"NSObject<OS_dispatch_group>"
@"NSObject<OS_dispatch_queue>"
@"<NUFaceDetectionResult>"
q24@0:8@16
@24@0:8q16
@48@0:8@16{?=qiIq}24
@24@0:8@"NSDictionary"16
@"NSDictionary"16@0:8
@48@0:8{?=qiIq}16d40
v24@0:8@"NSCoder"16
@24@0:8@"NSCoder"16
@48@0:8q16q24q32d40
@32@0:8@16#24
v72@0:8{?={?=qiIq}{?=qiIq}}16@64
@88@0:8{?=qiIq}16{?=[3]}40
{?=[3]}16@0:8
{?="columns"[3]}
{?=[3]}40@0:8{?=qiIq}16
@"NUKeyframeSequenceMatrixFloat33"
@"AVAsset"
@"NSArray"16@0:8
@72@0:8@16{?={?=qq}{?=qq}}24Q56@64
@"NSArray"
v24@0:8Q16
{?={?=qq}{?=qq}}96@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{?={?=qq}{?=qq}}48d80d88
{?=ddd}56@0:8@16{?={?=qq}{?=qq}}24
@28@0:8@16B24
{?={?=[4d]}{?=[4d]}d}16@0:8
@88@0:8{?={?=[4d]}{?=[4d]}d}16
{?=[4d]}16@0:8
@48@0:8{?=[4d]}16
B48@0:8{?=[4d]}16
{?=[4d]}48@0:8{?=[4d]}16
{?=[4d]}88@0:8{?={?=[4d]}{?=[4d]}d}16
{?=dd}104@0:8{?={?=[4d]}{?=[4d]}d}16@88d96
{?={?=[4d]}{?=[4d]}d}48@0:8@16@24d32@40
@"NUBufferRenderClient"
@"NUImageDataClient"
@32@0:8@16^@24
B40@0:8@16@24^@32
@"NSArray"32@0:8@16^@24
B40@0:8@16@"NSMutableDictionary"24^@32
@"NSNumber"32@0:8@"NSDictionary"16^@24
B40@0:8@"NSNumber"16@"NSMutableDictionary"24^@32
{?=qq}16@0:8
v32@0:8{?=qq}16
@"NUImageExportRequest"
{?="width"q"height"q}
@"NUImageGeometry"
@"NUPriority"
@"NUColorSpace"
@"<NUScalePolicy>"
@"NUImageExportFormat"
v48@0:8@16@24@32@?40
v40@0:8@16@24@?32
@48@0:8@16@24@32@?40
@72@0:8@16@24@32@40@48@56@?64
@48@0:8@16@24@32^@40
v64@0:8@16@24@32@40@48@?56
{CGRect={CGPoint=dd}{CGSize=dd}}24@0:8@16
{CGRect={CGPoint=dd}{CGSize=dd}}56@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16@48
{CGRect={CGPoint=dd}{CGSize=dd}}64@0:8q16{CGRect={CGPoint=dd}{CGSize=dd}}24@56
@48@0:8@16@24@32@40
@48@0:8Q16Q24Q32@40
@40@0:8Q16Q24Q32
v64@0:8{?={?=qiIq}{?=qiIq}}16
@40@0:8@16I24I28o^@32
@40@0:8@16@24#32
@"PTTimedRenderingMetadata"
v76@0:8@16{?=qq}24{?=qq}40i56q60@?68
v68@0:8@16{?=qq}24{?=qq}40i56q60
@68@0:8@16{?=qq}24{?=qq}40i56q60
@"PTRenderPipeline"
@"<PTRenderState>"
@"<MTLDevice>"
@"NSDate"
v48@0:8q16^d24^d32^d40
@"PIFaceObservationCache"16@0:8
v24@0:8@"PIFaceObservationCache"16
v36@0:8@16@24B32
f24@0:8@16
B32@0:8{?=qq}16
B40@0:8@16{?=qq}24
@"PIFaceObservationCache"
B32@0:8{CGSize=dd}16
{?="exposure"d"contrast"d"brightness"d"shadows"d"highlights"d"black"d"rawHighlights"d"localLight"d}
@36@0:8@16@24B32
@52@0:8@16@24@32q40B48
@44@0:8@16{CGSize=dd}24B40
^{CGImage=}24@0:8@16
@40@0:8@16{CGSize=dd}24
@40@0:8@16Q24^{CGColorSpace=}32
@48@0:8{?=qq}16@32Q40
B40@0:8@16@24o^@32
@64@0:8@16@24{?={?=qq}{?=qq}}32
B60@0:8@16i24^{CGColorSpace=}28@36@44o^@52
@"<NURenderer>"
@"<NUSurfaceStorage>"
@"NSObject<OS_dispatch_semaphore>"
@"NSMutableArray"
@"NSError"
@"VNImageHomographicAlignmentObservation"16@0:8
@"VNImageHomographicAlignmentObservation"
@44@0:8@16@24@32B40
@60@0:8@16@24B32{CGSize=dd}36q52
@"PTCinematographyScript"
@"NSCache"
^{CGColorSpace=}16@0:8
{vector<float, std::allocator<float>>=^f^f{__compressed_pair<float *, std::allocator<float>>=^f}}24@0:8@16
@48@0:8r^f16r^f24r^f32r^f40
@40@0:8@16{?=qq}24
@"NUFaceDetectionRequest"
@68@0:8^v16Q24Q32q40Q48i56^{CGColorSpace=}60
@68@0:8@16Q24Q32q40Q48i56^{CGColorSpace=}60
^v16@0:8
^{CGColorSpace=}
@"NSMutableData"
v24@0:8^{?=Bffff[3f]}16
^f16@0:8
@24@0:8^f16
@32@0:8d16@24
@36@0:8@16B24#28
@20@0:8B16
@36@0:8d16d24B32
@104@0:8@16{?=[3]}24{CGRect={CGPoint=dd}{CGSize=dd}}72
@40@0:8@16@24d32
@"CIVector"
@24@0:8Q16
B24@0:8Q16
@32@0:8q16@24
B32@0:8^{?={?=qq}{?=qq}}16o^@24
B48@0:8^{CGRect={CGPoint=dd}{CGSize=dd}}16^{CGRect={CGPoint=dd}{CGSize=dd}}24@32o^@40
{CGRect={CGPoint=dd}{CGSize=dd}}96@0:8{?=qq}16{?=qq}32{?=qq}48{CGRect={CGPoint=dd}{CGSize=dd}}64
{?="p75"d"p98"d"autoValue"d"g98"d}
{?="sat"d"contrast"d"cast"d}
v36@0:8^f16f24f28f32
f24@0:8r^f16
d40@0:8d16d24d32
@64@0:8@16{?={?=qq}{?=qq}}24@56
@144@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24{CGRect={CGPoint=dd}{CGSize=dd}}56{?=[3]}88@136
@"PIReframeKeyframeSequence"
@"<NUVideoProperties>"
v32@0:8{CGVector=dd}16
v64@0:8{?=[3]}16
{CGVector=dd}16@0:8
{CGVector="dx"d"dy"d}
@24@0:8r^{FigLivePhotoMetadata=I{FigLivePhotoMetadataV1Struct=fqffffffccSI[0{FigLivePhotoDetectedFaceV1Struct=qffffisS}]}}16
{CGVector=dd}24@0:8r^{FigLivePhotoMetadata=I{FigLivePhotoMetadataV1Struct=fqffffffccSI[0{FigLivePhotoDetectedFaceV1Struct=qffffisS}]}}16
{?=[3]}32@0:8r^{FigLivePhotoMetadata=I{FigLivePhotoMetadataV1Struct=fqffffffccSI[0{FigLivePhotoDetectedFaceV1Struct=qffffisS}]}}16^B24
@"AVAssetTrack"
{CGAffineTransform="a"d"b"d"c"d"d"d"tx"d"ty"d}
B24@0:8d16
@48@0:8@16{CGPoint=dd}24d40
B32@0:8^{CGImage=}16@24
B40@0:8^{CGImage=}16@24@32
@40@0:8@16@24@32
{?={?=qq}{?=qq}}40@0:8{CGPoint=dd}16d32
v40@0:8r^f16^S24Q32
v40@0:8r^S16^f24Q32
@48@0:8f16f20f24f28Q32{?=ii}40
{?=ii}16@0:8
v24@0:8{?=ii}16
{?="major"i"minor"i}
@48@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16
@48@0:8@16@24q32@40
@52@0:8@16@24@32f40q44
@40@0:8@16q24@32
@64@0:8@16@24@32@40q48o^@56
@52@0:8@16@24@32B40o^@44
@44@0:8@16@24B32o^@36
@36@0:8@16B24@28
@64@0:8{?={?=qiIq}{?=qiIq}}16
@32@0:8d16d24
{CGPoint=dd}44@0:8i16d20d28@36
ffffff
