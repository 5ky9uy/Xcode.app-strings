@(#)PROGRAM:HomeAI  PROJECT:HomeAI-231.5.1
Mb@?
UsesIOSurface
HardwareAcceleratedTransfer
HighSpeedTransfer
HMIPixelBufferTransferMultiStageResize
?a0cTa1cTa2cTa3cTa0hTa1hTa2hTa3hT
L?_p
r@ffffff
.>333?
N6homeai3mod28ImageDescriptorBufferFloat32E
HBN6homeai10clustering15GreedyClustererE
NSt3__120__shared_ptr_emplaceINS_6vectorIxNS_9allocatorIxEEEENS2_IS4_EEEE
NSt3__120__shared_ptr_emplaceIN6homeai10clustering15GreedyClusterer9cluster_tENS_9allocatorIS4_EEEE
NSt3__120__shared_ptr_emplaceINS_6vectorINS_5tupleIJxxfEEENS_9allocatorIS3_EEEENS4_IS6_EEEE
N6homeai3mod29ImageDescriptorBufferAbstractE
?ffffff
333333
>NSt3__120__shared_ptr_emplaceIN6homeai10clustering15GreedyClustererENS_9allocatorIS3_EEEE
MbP?
Motion
Person
Vehicle
Package
?fff?
N2cv13BaseRowFilterE
N2cv16BaseColumnFilterE
N2cv10BaseFilterE
N2cv12FilterEngineE
N2cv18SymmRowSmallFilterIhiNS_21SymmRowSmallVec_8u32sEEE
N2cv9RowFilterIhiNS_21SymmRowSmallVec_8u32sEEE
N2cv18SymmRowSmallFilterIffNS_19SymmRowSmallVec_32fEEE
N2cv9RowFilterIffNS_19SymmRowSmallVec_32fEEE
N2cv9RowFilterIhiNS_12RowVec_8u32sEEE
N2cv9RowFilterIhfNS_8RowNoVecEEE
N2cv9RowFilterIhdNS_8RowNoVecEEE
N2cv9RowFilterItfNS_8RowNoVecEEE
N2cv9RowFilterItdNS_8RowNoVecEEE
N2cv9RowFilterIsfNS_13RowVec_16s32fEEE
N2cv9RowFilterIsdNS_8RowNoVecEEE
N2cv9RowFilterIffNS_10RowVec_32fEEE
N2cv9RowFilterIfdNS_8RowNoVecEEE
N2cv9RowFilterIddNS_8RowNoVecEEE
N2cv12ColumnFilterINS_13FixedPtCastExIihEENS_11ColumnNoVecEEE
N2cv12ColumnFilterINS_4CastIfhEENS_11ColumnNoVecEEE
N2cv12ColumnFilterINS_4CastIdhEENS_11ColumnNoVecEEE
N2cv12ColumnFilterINS_4CastIftEENS_11ColumnNoVecEEE
N2cv12ColumnFilterINS_4CastIdtEENS_11ColumnNoVecEEE
N2cv12ColumnFilterINS_4CastIfsEENS_11ColumnNoVecEEE
N2cv12ColumnFilterINS_4CastIdsEENS_11ColumnNoVecEEE
N2cv12ColumnFilterINS_4CastIffEENS_11ColumnNoVecEEE
N2cv12ColumnFilterINS_4CastIddEENS_11ColumnNoVecEEE
N2cv21SymmColumnSmallFilterINS_13FixedPtCastExIihEENS_19SymmColumnVec_32s8uEEE
N2cv16SymmColumnFilterINS_13FixedPtCastExIihEENS_19SymmColumnVec_32s8uEEE
N2cv12ColumnFilterINS_13FixedPtCastExIihEENS_19SymmColumnVec_32s8uEEE
N2cv21SymmColumnSmallFilterINS_4CastIisEENS_25SymmColumnSmallVec_32s16sEEE
N2cv16SymmColumnFilterINS_4CastIisEENS_25SymmColumnSmallVec_32s16sEEE
N2cv12ColumnFilterINS_4CastIisEENS_25SymmColumnSmallVec_32s16sEEE
N2cv21SymmColumnSmallFilterINS_4CastIffEENS_22SymmColumnSmallVec_32fEEE
N2cv16SymmColumnFilterINS_4CastIffEENS_22SymmColumnSmallVec_32fEEE
N2cv12ColumnFilterINS_4CastIffEENS_22SymmColumnSmallVec_32fEEE
N2cv16SymmColumnFilterINS_4CastIfhEENS_11ColumnNoVecEEE
N2cv16SymmColumnFilterINS_4CastIdhEENS_11ColumnNoVecEEE
N2cv16SymmColumnFilterINS_4CastIftEENS_11ColumnNoVecEEE
N2cv16SymmColumnFilterINS_4CastIdtEENS_11ColumnNoVecEEE
N2cv16SymmColumnFilterINS_4CastIisEENS_11ColumnNoVecEEE
N2cv12ColumnFilterINS_4CastIisEENS_11ColumnNoVecEEE
N2cv16SymmColumnFilterINS_4CastIfsEENS_20SymmColumnVec_32f16sEEE
N2cv12ColumnFilterINS_4CastIfsEENS_20SymmColumnVec_32f16sEEE
N2cv16SymmColumnFilterINS_4CastIdsEENS_11ColumnNoVecEEE
N2cv16SymmColumnFilterINS_4CastIffEENS_17SymmColumnVec_32fEEE
N2cv12ColumnFilterINS_4CastIffEENS_17SymmColumnVec_32fEEE
N2cv16SymmColumnFilterINS_4CastIddEENS_11ColumnNoVecEEE
N2cv8Filter2DIhNS_4CastIfhEENS_12FilterVec_8uEEE
N2cv8Filter2DIhNS_4CastIftEENS_11FilterNoVecEEE
N2cv8Filter2DIhNS_4CastIfsEENS_15FilterVec_8u16sEEE
N2cv8Filter2DIhNS_4CastIffEENS_11FilterNoVecEEE
N2cv8Filter2DIhNS_4CastIddEENS_11FilterNoVecEEE
N2cv8Filter2DItNS_4CastIftEENS_11FilterNoVecEEE
N2cv8Filter2DItNS_4CastIffEENS_11FilterNoVecEEE
N2cv8Filter2DItNS_4CastIddEENS_11FilterNoVecEEE
N2cv8Filter2DIsNS_4CastIfsEENS_11FilterNoVecEEE
N2cv8Filter2DIsNS_4CastIffEENS_11FilterNoVecEEE
N2cv8Filter2DIsNS_4CastIddEENS_11FilterNoVecEEE
N2cv8Filter2DIfNS_4CastIffEENS_13FilterVec_32fEEE
N2cv8Filter2DIdNS_4CastIddEENS_11FilterNoVecEEE
 !"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\]^_`abcdefghijklmnopqrstuvwxyz{|}~
ucwsifdr
?qxs?qxs
9D)5:
@`|-A
PF SdF
 [@W:[
D]eV
"x@
z4I5z
?~my
!Y?4
!I?u
!)?gX
?g^FT
?$"LT
?$"LT
N2cv15ThresholdRunnerE
4N2cv6detail16LKTrackerInvokerE
N2cv9ColumnSumIihEE
N2cv9ColumnSumIitEE
N2cv9ColumnSumIisEE
N2cv6RowSumIhiEE
N2cv6RowSumIhdEE
N2cv6RowSumItiEE
N2cv6RowSumItdEE
N2cv6RowSumIsiEE
N2cv6RowSumIiiEE
N2cv6RowSumIsdEE
N2cv6RowSumIfdEE
N2cv6RowSumIddEE
N2cv9ColumnSumIdhEE
N2cv9ColumnSumIdtEE
N2cv9ColumnSumIdsEE
N2cv9ColumnSumIiiEE
N2cv9ColumnSumIifEE
N2cv9ColumnSumIdfEE
N2cv9ColumnSumIidEE
N2cv9ColumnSumIddEE
 !"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\]^_`abcdefghijklmnopqrstuvwxyz{|}~
N2cv11_InputArrayE
N2cv12_OutputArrayE
N2cv9ExceptionE
14EmptyFuncTable
12GpuFuncTable
N2cv16ParallelLoopBodyE
GN2cv16MorphologyRunnerE
N2cv14MorphRowFilterINS_5MinOpIhEENS_12MorphRowIVecINS_6VMin8uEEEEE
N2cv14MorphRowFilterINS_5MinOpItEENS_12MorphRowIVecINS_7VMin16uEEEEE
N2cv14MorphRowFilterINS_5MinOpIsEENS_12MorphRowIVecINS_7VMin16sEEEEE
N2cv14MorphRowFilterINS_5MinOpIfEENS_12MorphRowFVecINS_7VMin32fEEEEE
N2cv14MorphRowFilterINS_5MinOpIdEENS_13MorphRowNoVecEEE
N2cv14MorphRowFilterINS_5MaxOpIhEENS_12MorphRowIVecINS_6VMax8uEEEEE
N2cv14MorphRowFilterINS_5MaxOpItEENS_12MorphRowIVecINS_7VMax16uEEEEE
N2cv14MorphRowFilterINS_5MaxOpIsEENS_12MorphRowIVecINS_7VMax16sEEEEE
N2cv14MorphRowFilterINS_5MaxOpIfEENS_12MorphRowFVecINS_7VMax32fEEEEE
N2cv14MorphRowFilterINS_5MaxOpIdEENS_13MorphRowNoVecEEE
N2cv17MorphColumnFilterINS_5MinOpIhEENS_15MorphColumnIVecINS_6VMin8uEEEEE
N2cv17MorphColumnFilterINS_5MinOpItEENS_15MorphColumnIVecINS_7VMin16uEEEEE
N2cv17MorphColumnFilterINS_5MinOpIsEENS_15MorphColumnIVecINS_7VMin16sEEEEE
N2cv17MorphColumnFilterINS_5MinOpIfEENS_15MorphColumnFVecINS_7VMin32fEEEEE
N2cv17MorphColumnFilterINS_5MinOpIdEENS_16MorphColumnNoVecEEE
N2cv17MorphColumnFilterINS_5MaxOpIhEENS_15MorphColumnIVecINS_6VMax8uEEEEE
N2cv17MorphColumnFilterINS_5MaxOpItEENS_15MorphColumnIVecINS_7VMax16uEEEEE
N2cv17MorphColumnFilterINS_5MaxOpIsEENS_15MorphColumnIVecINS_7VMax16sEEEEE
N2cv17MorphColumnFilterINS_5MaxOpIfEENS_15MorphColumnFVecINS_7VMax32fEEEEE
N2cv17MorphColumnFilterINS_5MaxOpIdEENS_16MorphColumnNoVecEEE
N2cv11MorphFilterINS_5MinOpIhEENS_9MorphIVecINS_6VMin8uEEEEE
N2cv11MorphFilterINS_5MinOpItEENS_9MorphIVecINS_7VMin16uEEEEE
N2cv11MorphFilterINS_5MinOpIsEENS_9MorphIVecINS_7VMin16sEEEEE
N2cv11MorphFilterINS_5MinOpIfEENS_9MorphFVecINS_7VMin32fEEEEE
N2cv11MorphFilterINS_5MinOpIdEENS_10MorphNoVecEEE
N2cv11MorphFilterINS_5MaxOpIhEENS_9MorphIVecINS_6VMax8uEEEEE
N2cv11MorphFilterINS_5MaxOpItEENS_9MorphIVecINS_7VMax16uEEEEE
N2cv11MorphFilterINS_5MaxOpIsEENS_9MorphIVecINS_7VMax16sEEEEE
N2cv11MorphFilterINS_5MaxOpIfEENS_9MorphFVecINS_7VMax32fEEEEE
N2cv11MorphFilterINS_5MaxOpIdEENS_10MorphNoVecEEE
?N2cv5MatOpE
N2cv14MatOp_IdentityE
N2cv11MatOp_AddExE
N2cv9MatOp_BinE
N2cv9MatOp_CmpE
N2cv10MatOp_GEMME
N2cv12MatOp_InvertE
N2cv7MatOp_TE
N2cv11MatOp_SolveE
N2cv17MatOp_InitializerE
Error creating directory for person:%@
version
UUID
displayName
person.json
Error saving metadata to disk for person:%@
v8@?0
Error creating directory for person face crop:%@
faceBoundingBox
dateCreated
facecrop.json
Error saving metadata to disk for person face crop:%@
facecrop.jpeg
Error saving face crop image to disk for person face crop:%@
#ffffff
#000000
</body>
</html>
Visualizer saved (%@)
HMIHTMLReport
<html>
<head><title>%@</title></head>
<style>
</style>
<body text='%@' bgcolor='%@'>
<script>
</script>
%@<br>
border:%dpx;
border:%dpx solid %@;
outline:%dpx;
outline:%dpx solid %@;
<div class='image'>
<img width='%d' height='%d' src='data:image/jpeg;base64,%@' style='%@'/>
<div class='rect' style='top:%dpx; left:%dpx; width:%dpx; height:%dpx; border-color:%@; opacity:%.1f' threshold='%.3f'>%@</div>
v32@?0@"HMIHTMLReportBox"8Q16^B24
<div class="text">%@</div>
</div>
%.3fs
v16@?0@"HMIVideoAnalyzerFrameResult"8
@"NSValue"16@?0@"HMIVideoAnalyzerFrameResult"8
[%lu/%lu] %@ (%.2fs)
v32@?0@"HMIVideoFrame"8Q16^B24
v16@?0@"NSArray"8
v24@?0@"HMIVideoAnalyzerEvent"8^B16
confidence.value
v16@?0@"HMIVideoAnalyzerEvent"8
%.3f %@
%.3f
%.3f %@ %.2f %@
#ffff00
HMISystemResourceUsageLevelLow
HMISystemResourceUsageLevelMed
HMISystemResourceUsageLevelHigh
HMISystemResourceUsageLevelUnknown
HMISystemResourceUsageLevelUndefined
FaceFilteredState
HMIVAEF.fr
HMIVAEF.ta
HMIVAEF.ya
HMIVAEF.ro
None
QualitySVMKnown
QualitySVMUnknown
QualityANFRScore
NoPredictions
HasFaceMask
Face Recognition
Torso Annotation
Face Yaw
Face Roll
%@@(%@,%@)
v16@?0@"NSError"8
%c%c%c%c
Type: %@, DTS: %@, PTS: %@, DUR: %@, NUM: %ld [%@]
Sync
%c%02d:%02d:%02d.%03d
%*lld/%-6d %@
%*lld %@
POSITIVE INFINITY
NEGATIVE INFINITY
   INDEFINITE    
  INVALID TIME   
%@ DTS %@ PTS %@ dur %@%@
PTS: %.2f
en_US_POSIX
yyyy-MM-dd'T'HH:mm:ss
%.3f x %.3f
%.3f, %.3f %@
%.2f
%02x
q24@?0@"<HMIVideoEvent>"8@"<HMIVideoEvent>"16
B32@?0@"<HMIVideoEvent>"8Q16^B24
@16@?0@"<HMIVideoEvent>"8
Time: %@, Date: %@
HMIVAEM.ms
Motion score
%@@(%@)
Failed to set request revision
failed to perform image request
Expected 1 torsoprint, but got %lu torsoprints
torsoprint is nil
HMICoreAnalyticsVIPModelReportTime
v24@?0@"HMIHomePersonManagerSettings"8@"NSError"16
v24@?0@"NSDictionary"8@"NSError"16
B16@?0@"HMIFaceClassification"8
UUID:%@ HomeUUID:%@
Frame %lu @ %@
Logi Circle 2
%#{flags}
transferPixelBuffer
VTPixelTransferSessionCreate failed. Error %d
VTSessionSetProperty failed. Error %d
VTPixelTransferSessionTransferImage failed. Error %d
JPEGDataFromPixelBuffer
Invalid crop for affine transform
Error in pixelbuffer format for affine transform
Error generating pixelbuffer for affine transform
Error applying affine transform
q24@?0@"HMIPairwiseMatch"8@"HMIPairwiseMatch"16
v12@?0i8
/private/var
OutOfMemory
Reached high water mark.
PrimaryUsagePage
PrimaryUsage
LocationID
image/Placeholder
HomeSSD/box0_offset0
HomeSSD/box0_offset1
HomeSSD/box0_offset2
HomeSSD/box0_offset3
HomeSSD/box0_offset4
HomeSSD/box1_offset0
HomeSSD/box1_offset1
HomeSSD/box1_offset2
HomeSSD/box1_offset3
HomeSSD/box1_offset4
HomeSSD/class_prob0
HomeSSD/class_prob1
HomeSSD/class_prob2
HomeSSD/class_prob3
HomeSSD/class_prob4
HomeSSD/object_yaw0
HomeSSD/object_yaw1
HomeSSD/object_yaw2
HomeSSD/object_yaw3
HomeSSD/object_yaw4
HomeSSD/object_roll0
HomeSSD/object_roll1
HomeSSD/object_roll2
HomeSSD/object_roll3
HomeSSD/object_roll4
SignificantActivityFcosDetector
CoreML output nil or not of type MLFeatureTypeMultiArray
SignificantActivityFcos
mlmodelc
name
VeryLongTrackDuration
discussion
Track %@ has an unexpectedly long track duration %@.
v16@?0@"AVAssetTrack"8
WidelyDifferingTrackDurations
Track durations vary widely, this is usually caused by a corrupt video / audio sample duration.
v16@?0@"NSDictionary"8
Sanitized Data
Asset Check
HMIHomeKitClient HomeKit Delegate Queue
B16@?0@"HMHome"8
B16@?0@"HMResidentDevice"8
B16@?0@"HMCameraProfile"8
v16@?0@"HMHomeManager"8
Not supported error
General error
Espresso error
incorrect binserializer key
small sparsity error
feature extraction error
initialization error
no saved state to revert
nominal distance not changed
batch size violation
computation kill request was issued
too few IDs to build VIP model
video error
error with projection computation
missing positional parameter
inconsistent state error
warping error
OpenGL error
invalid format
out of bounds
singular point configuration error
division by zero
LAPACK error
platform endianess not supported
hash already in use
invalid ID
invalid data type
data inconsistency error
I/O error
unknown option
invalid option
missing option
delegate error
vImage related error
memory allocation error
invalid parameter
unexpected null pointer
internal error
not implemented error
CVML_status module %d error %lld
BinSerializer
Face3D
FaceDescriptor
FaceFrontalizer
FaceWarper
Geometry2D
Geometry3D
ImageGrouping
ImageQuality
LandmarkDetector
MomentProcessor
FaceboxAligner
ImageDescriptor
ImageClassifier
ImageProcessing
VIPIdentification
ImageRegistration
SimilarityMatrix
Clustering
HumanDetector
FaceRegionMap
ObjectDetector
ObjectTracker
SRCClassifier
Kmeans
SparseCoding
FaceID
BoostedClassifier
FaceSegmenter
ImageAnalyzer
FaceAttributes
FaceprintAndAttributes
FaceQuality
Generic
ImageTools
VideoTools
ImageWarper
ThirdParty
BinSerializerProcessor
AppleNetParser
FaceProcessorCLI
ImageClassifierCLI
MPCmdlineClientCLI
ClusteringCLI
ImageProcessorCLI
PhotosProcessorCLI
CVMLEngine
CVML Module %lld
%@ (%@)
input
transformed_features
classProbability
FaceRecognizabilityFilterSVM
FaceRecognizabilityFilterSVMDataScaler
FaceAestheticQualityFilterSVM
FaceAestheticQualityFilterSVMDataScaler
You must override %@ in a subclass
@16@?0@"HMPerson"8
v24@?0@"NSSet"8@"NSError"16
@16@?0@"HMPersonFaceCrop"8
@16@?0@"HMFaceprint"8
@16@?0@"HMIFaceprint"8
com.apple.cvml.%@
CVML module = %@
HMIFC.ck.pu
HMIFC.ck.so
Unknown
PhotoLibrary
User
ImpureClusteringCleanup
Unknown source: %ld
dataRepresentation
personUUID
Person UUID
Source
frame
Frame
Events
Region of Interest
B16@?0@"HMIVideoAnalyzerEvent"8
@16@?0#8
q24@?0@"HMIVideoAnalyzerFrameResult"8@"HMIVideoAnalyzerFrameResult"16
Track:%.2f-%.2f @ %@ (%@)
CVPixelBufferGetPixelFormatType(pixelBuffer) == kCVPixelFormatType_420YpCbCr8BiPlanarVideoRange || CVPixelBufferGetPixelFormatType(pixelBuffer) == kCVPixelFormatType_420YpCbCr8BiPlanarFullRange
v16@?0@"HMIForegroundTrack"8
B16@?0@"HMIForegroundTrack"8
{CGRect={CGPoint=dd}{CGSize=dd}}
B16@?0@"NSValue"8
@16@?0@"HMIForegroundTrack"8
B16@?0@"HMIForegroundBlob"8
v32@?0@"HMIForegroundBlob"8Q16^B24
v32@?0@"HMIForegroundTrack"8Q16^B24
v16@?0@"HMIPairwiseMatch"8
v24@?0Q8^B16
v16@?0@"NSValue"8
CVPixelBufferGetPixelFormatType(pixelBuffer) == kCVPixelFormatType_420YpCbCr8BiPlanarFullRange
@16@?0@"NSValue"8
%.2f Result
%.2f Mean
%.2f Std
%.2f Diff
@"NSSet"16@?0@"HMIForegroundTrack"8
%.2f Assign
allocator<T>::allocate(size_t n) 'n' exceeds maximum supported size
hmi://in-memory
HMIMemoryAVAsset
tracks
Invalid parameter for windowSize: %lu, windowSize must be > 0
v16@?0@"MovingAverageEntry"8
@16@?0@"HMIVideoAnalyzerEvent"8
class
timestamp
uuid
v32@?0@"NSArray"8Q16^B24
@16@?0@"NSDictionary"8
v16@?0@"HMIVideoAnnotationParserTrack"8
{CGRect={CGPoint=dd}{CGSize=dd}}80@?0{CGRect={CGPoint=dd}{CGSize=dd}}8{CGRect={CGPoint=dd}{CGSize=dd}}40f72f76
@16@?0@"HMIVideoAnnotationParserTrack"8
@16@?0@"HMIPersonsModelPrediction"8
detections
bounds
label
confidence
filters
v16@?0@8
com.apple.HomeAIDESPlugin
v24@?0@"NSUUID"8@"NSError"16
bias_b
bias_g
bias_r
is_network_bgr
scale
Label:%d Confidence:%f X:%f Y:%f Width:%f Height:%f Yaw:%@}
q24@?0@"HMIObjectDetection"8@"HMIObjectDetection"16
@16@?0@"HMIObjectDetection"8
@"HMIPersonBlob"16@?0@"HMIVideoAnalyzerEventPerson"8
@16@?0@"HMIPersonBlob"8
v32@?0@"HMIPersonBlob"8Q16^B24
v24@?0@"NSNumber"8@"NSNumber"16
<%@: %p> timeStamp: %@, detections: [%@], regionOfInterest: %@
<%@: %p> %@
self.dynamicConfiguration
@max.floatValue
@16@?0@"HMIVideoAnalyzerReportRecord"8
@sum.count
Precision
Recall
True Positive
False Negative
False Positive
Fragments
v16@?0@"HMIVideoAnalyzerFragmentResult"8
v24@?0@"NSString"8@"HMIVideoAnalyzerMutableReportSession"16
v16@?0#8
@"HMIVideoAnalyzerMutableReportComparison"20@?0#8f16
precision
recall
threshold
annotation
opacity
/System/Library/CoreServices/SystemVersion.plist
com.apple.HomeAI
HomeAIBundleVersion
Debug
Truth
image_id
B16@?0@"NSDictionary"8
classification_classes
@16@?0@"NSString"8
@24@?0@"NSString"8@"HMIVideoAnalyzerMutableReportSession"16
f56@?0@"HMIVideoAnalyzerEvent"8@"HMIVideoAnalyzerEvent"16{CGRect={CGPoint=dd}{CGSize=dd}}24
@"NSArray"16@?0@"HMIVideoAnalyzerEvent"8
@24@?0@"NSNumber"8@"NSNumber"16
@16@?0@"NSNumber"8
v32@?0@"HMIVideoAnalyzerFrameResult"8Q16^B24
v32@?0@"NSNumber"8@"NSNumber"16^B24
v16@?0@"NSNumber"8
score
%@%@
Object detection (%@)
Visualize%@.html
%lu %@s (Precision: %.3f, Recall: %.3f)
v16@?0@"HMIVideoAnalyzerReportRecord"8
@"NSValue"16@?0@"HMIVideoFrame"8
@16@?0@"HMIVideoFrame"8
q24@?0@"NSString"8@"NSString"16
Fragment%@%@.txt
v24@?0@"NSString"8@"NSArray"16
domain
range
@24@?0@"NSString"8@"NSString"16
'%@': '%@'
{%@}[datum.label]
labelExpr
$schema
https://vega.github.io/schema/vega-lite/v4.json
description
PR Curves
width
container
height
config
style
align
center
baseline
layer
mark
type
line
clip
true
point
encoding
field
quantitative
nominal
legend
text
left
%@_%.0f_%@_%lu.png
v32@?0@"HMIVideoAnalyzerEvent"8Q16^B24
v16@?0@"HMIVideoAnalyzerReportMatch"8
v32@?0@"NSUUID"8@"HMIVideoAnalyzerEventPerson"16^B24
HMIFC.ck.u
HMIFC.ck.dr
HMIFC.ck.dc
HMIFC.ck.fbb
PVFC:PVFC
PVFC_FB
PVFC_CB
Data Representation
Date Created
Face Bounding Box
Could not create image source
No meta data exists on image
mediaType == kCMMediaType_Video
Descriptor count = 
Descriptor length = 
 bytes
 = [
identifier
Identifier
Name
Manufacturer
Model
Firmware Version
Descriptor vectors nil
Success
Canceled
Error
UpdatePersonsModelTask
RemovePersonsModelTask
HomePersonClusteringTask
TuriTrialUpdateTask
FaceMisclassificationTask
PersonsModelsSummaryTask
UpdateTorsoModelTask
FeedbackTask
EmptyTask
resultCode
taskType
faceCrop
sourceUUID
homeUUID
isExternal
doImpurePersonCleanup
cameraProfileUUID
clipUUID
torsoAnnotations
Unknown task type: %@
v32@?0@"HMITask"8Q16^B24
HMITaskHomeUUIDKey is nil
HMIUpdatePersonsModelTaskIsExternal is nil
HMIPersonsModelTaskSourceUUID is nil
Failed to get HMPhotosPersonManager
Failed to get HMHomePersonManager
Failed to initialize data source
Disk-based Home Person Data Source not supported
HomeUUID is nil
cleanup key is missing
HMIUpdateTorsoModelTaskAnnotationsKey is nil
HMITaskService not available on this platform.
HMITR.c
HMITR.tp
HMITR.sea
HMITR.seu
v16@?0@"HMIFaceprint"8
warm_start_faceprint_model
CreateFaceprint
B16@?0@"HMIFaceprint"8
HMIFP.ck.u
HMIFP.ck.d
HMIFP.ck.mu
HMIFP.ck.fcu
modelUUID
faceCropUUID
Data
Model UUID
Face Crop UUID
<%0.3f %0.3f>
HMPMS.fce
Face Classification Enabled
com.apple.homeai.model.loader
/var/mobile/Library/Caches/com.apple.HomeAI/model_dir
[Model loading] failed to remove dir %@ err %@
[Model loading] failed to untar model asset into %@ err %@
faceCrops
authTag
internal
wrappedKey
1.2.840.113635.100.6.27.38
HMIFeedback Queue
HFFeedbackService
Cannot find camera profile.
Cannot find home for camera profile.
Cannot download clip asset.
Check network connectivity.
v24@?0@"HMCameraClipVideoAssetContext"8@"NSError"16
v16@?0Q8
v16@?0@"HMCameraClip"8
Cannot download clip for camera profile.
Ensure the clip belongs to the camera profile.
@16@?0@"HMCameraClipSignificantEvent"8
v24@?0@"HMCameraClip"8@"NSError"16
Unable to blur faces.
Unable to read the asset from disk.
Clip doesn't have a video track.
hkcvml-dev.apple.com
hkcvml.apple.com
https://%@/v2/clip-uuid/
feedback
%@.%@
json
application/json
Content-Type
v32@?0@"NSData"8@"NSURLResponse"16@"NSError"24
Invalid key size.
Cannot generate initialization vector.
Failed to encrypt data.
Failed to encrypt face crop data.
v24@?0@"NSURL"8@"NSError"16
v16@?0@"NSURL"8
HMIFC.pu
HMIFC.su
HMIFC.seu
HMIFC.fc
HMIFC.fp
HMIFC.c
HMIFC.ftc
HMIFC.f
Known
Uncertain
Source UUID
Session Entity UUID
Confidence
Familiarity
FaceCrop UUID
Faceprint UUID
FromTorsoClassification
@"NSUUID"16@?0@"HMIPerson"8
@"NSUUID"16@?0@"HMIPersonFaceCrop"8
v16@?0@"NSSet"8
@"NSUUID"16@?0@"HMIFaceCrop"8
Invalid person UUIDs
v32@?0@"HMIPerson"8@"NSSet"16^B24
B16@?0@"HMIPerson"8
Not implemented
Invalid persons, person already exists
v16@?0@"HMIPerson"8
Invalid faceCropUUIDs
B16@?0@"HMIPersonFaceCrop"8
Invalid persons, person UUID doesn't exists
Invalid personUUID
@"HMIPersonFaceCrop"16@?0@"HMIFaceCrop"8
B16@?0@"HMIFaceCrop"8
@"HMIPersonFaceCrop"16@?0@"HMIPersonFaceCrop"8
NOT IMPLEMENTED
@16@?0@"HMIPersonFaceCrop"8
SourceUUID:%@ HomeUUID:%@
v24@?0@"HMIVideoDecoder"8^{opaqueCMSampleBuffer=}16
Medium
High
%.4f
%.2f[%c]
HMIErrorDomain
Unexpected error
Failed to analyze
Failed to read fragment
Failed to verify fragment
Failed to transcode fragment
Video analyzer in error state
Model failed to load
Fragment is invalid
No pixel buffer
Unknown error code %ld
HMIErrorCodeUnexpectedError
HMIErrorCodeFailedToAnalyze
HMIErrorCodeFailedToReadFragment
HMIErrorCodeFailedToVerifyFragment
HMIErrorcodeFailedToTranscodeFragment
HMIErrorCodeAnalyzerInErrorState
HMIErrorCodeModelFailedToLoad
HMIErrorCodeFragmentIsInvalid
HMIPrivateErrorCodeNilPixelBuffer
HMIPrivateErrorCodeEmptyURL
HMIPrivateErrorCodeAVAssetReaderInitializationFailed
HMIPrivateErrorCodeFailedToLoadProperty
HMIPrivateErrorCodeAssetLoadCancelled
HMIPrivateErrorCodeReadingStartFailed
HMIPrivateErrorCodeNoTrackOutput
HMIPrivateErrorCodeSampleBufferUnavailable
HMIPrivateErrorCodeNoVideoTrackFound
HMIPrivateErrorCodeMultipleVideoTracksFound
HMIPrivateErrorInvalidPresentationTime
HMIPrivateErrorAVAssetReaderNotStarted
HMIPrivateErrorCodeSequentialIntegrityViolated
HMIPrivateErrorCodeAnalysisServiceNoConfiguration
HMIPrivateErrorCodeLoadingCoreMLModelFailed
HMIPrivateErrorCodeCoreMLPredictionFailed
HMIPrivateErrorCodeCoreMLOutputIncorrect
HMIPrivateErrorCodeCropAndResizeFailed
HMIPrivateErrorCodeCodecNotAvailable
HMIPrivateErrorCodeEncodingFailed
HMIPrivateErrorCodeInvalidVideoFrameFormatToSave
HMIPrivateErrorCodeScalerError
HMIPrivateErrorCodeFaceprintingFailed
HMIPrivateErrorCodeUpdatePersonsModelTaskFailed
HMIPrivateErrorCodeExternalPersonSourceDiskError
HMIPrivateErrorCodeLoadPersonsModelsFailed
HMIPrivateErrorCodeUpdatePersonsModelFailed
HMIPrivateErrorCodeRemovePersonsModelFailed
HMIPrivateErrorCodePersonsModelPredictionFailed
HMIPrivateErrorCodeNilDataSource
HMIPrivateErrorCodeUnknownTask
HMIPrivateErrorCodeHomePersonClusteringTaskFailed
HMIPrivateErrorCodeRemovePersonsModelTaskFailed
HMIPrivateErrorCodeFaceMisclassificationTaskFailed
HMIPrivateErrorCodePersonsModelsSummaryTaskFailed
HMIPrivateErrorCodeCleanupImpureHomePersonsOperationFailed
ERROR_%ld
fragments.count >= 1
[first isCombinableWithFragment:fragment]
v16@?0@"HMIVideoFragment"8
B28@?0I8@"NSData"12@"NSData"20
v12@?0I8
video/mp4
first
second
[%@]
Initialization Segment Data
Separable Segment Data
Sequence Numbers
Time Range
First Video Sample Byte Range
v32@?0@"HMIVideoAnalyzerEventPerson"8Q16^B24
v24@?0@"NSUUID"8@"HMIMutableCluster"16
(faceRecognition != nil) || (torsoRecognition != nil)
v16@?0@"NSUUID"8
v16@?0@"HMITorsoprint"8
@16@?0@"HMIFaceClassification"8
HMIASU.ck.ta
Torso Annotations
B16@?0@"HMITorsoAnnotation"8
@"HMITorsoAnnotation"16@?0@"HMIVideoAnalyzerEventFace"8
v16@?0@"HMIPoint"8
HMIPSUP.ck.p
HMIPSUP.ck.s
HMIP.ck.u
HMIP.ck.n
HMIP.ck.pl
personLinks
@"HMIPersonSourceUUIDPair"16@?0@"HMPersonLink"8
v16@?0@"<NSObject><NSCopying><NSSecureCoding>"8
data:;base64,%@
@16@?0@8
@24@?0@8@16
%.6f
data:;base64,
Invalid JSONObject class name: %@
Invalid JSONObject: %@
Cannot parse JSON data: %@
yyyy-MM-dd'T'HH:mm:ss.SSS'Z'
Error creating directory:%@ to save face classifications
faceprintUUID
%@-%03lu-%03u-%@-%@-%@
known
unknown
Error saving metadata to disk for face classification:%@
Error saving face crop image to disk for face classification:%@
HMITP.ck.u
HMITP.ck.d
HMITA.ck.fr
HMITA.ck.tps
HMITA.ck.tmv
faceRecognition
torsoprints
Torsoprints
TorsoModelVersion
HMIVideoEncoderWorkQueue
self.session == NULL
VTCompressionSessionCreate failed, err: %d
VTCompressionSessionPrepareToEncodeFrames failed, err: %d
Video encoding failed, err: %d
self.session
v24@?0i8I12^{opaqueCMSampleBuffer=}16
VTCompressionSessionEncodeFrameWithOutputHandler failed, err: %d
%@ is unavailable
@16@?0@"HMIFaceCrop"8
@16@?0@"HMIPerson"8
@16@?0@"HMFaceCrop"8
source != HMIPersonFaceCropSourceUnknown
HMPMS.ifple
HMPMS.sfce
Importing From Photo Library Enabled
Sharing Face Classifications Enabled
cameraManufacturer
cameraModel
recognitionType
face
torso
com.apple.HomeAI.PersonRecognitionEvent
@"NSMutableDictionary"8@?0
detectionScore
faceFilteredState
homeFamiliarity
v16@?0@"HMIFaceClassification"8
q24@?0@"NSNumber"8@"NSNumber"16
externalFamiliarity
sessionEntityAssignment
com.apple.HomeAI.FaceEvent
faceprintsClustered
clusters
personsCreated
faceprintsAssociated
faceprintingDuration
clusteringDuration
totalDuration
errorCode
errorDescription
com.apple.HomeAI.FaceClustering
v24@?0@"NSString"8@"NSNumber"16
v16@?0@"HMIPersonsModelSummary"8
@avg.self
externalLibraries
homeIdentities
averageExternalIdentities
averageHomeFaceCrops
averageExternalFaceCrops
homeToExternal
externalToExternal
com.apple.HomeAI.PersonsModels
@"NSDictionary"8@?0
B16@?0@"NSNumber"8
inclusion
exclusion
com.apple.HomeAI.MotionScore
zoneType
motionScore
com.apple.HomeAI.VideoAnalyzer.DidTerminate_v0
Fail
error
underlyingError
timeSinceAnalyzerStarted
com.apple.HomeAI.VideoAnalyzer.DidCreateTimelapseFragment_v0
bitrate
com.apple.HomeAI.VideoAnalyzer.DidAnalyzeFragment_v2
recognizeFaces
%@Trigger
%@Found
v32@?0@"NSString"8q16#24
motion
person
vehicle
package
transcode
analysisQuality
com.apple.HomeAI.VideoPackageAnalyzer.DidClassify_v0
com.apple.HomeAI.VideoPackageAnalyzer.DidReset_v0
interval
personsmodels
home
external
torso.bin
torso_to_facecrop.bin
torsoprinter_version.bin
user_defined_person_links.bin
v32@?0@"NSUUID"8@"HMIPersonsModel"16^B24
Error adding faceprints to model for personUUID: %@
v32@?0@"<NSObject><NSCopying><NSSecureCoding>"8@"NSArray"16^B24
Failed to remove stale Home VNPersonsModel with homeUUID: %@ sourceUUID: %@, error getting model storage path
Failed to remove stale Home VNPersonsModel with homeUUID: %@ sourceUUID: %@
Failed to persist VNPersonsModel for homeUUID: %@ sourceUUID: %@, error getting model storage path
Failed to persist VNPersonsModel for homeUUID: %@ sourceUUID: %@, path: %@
Failed to remove VNPersonsModel for homeUUID: %@ sourceUUID: %@, error getting model storage path
Failed to remove VNPersonsModel for homeUUID: %@ sourceUUID: %@, path: %@
Home persons model not found for homeUUID: %@
Failed to predict using VNPersonsModel for home persons model
Unable to get person model for homeUUID: %@ (self.personsModelsByHome is %@ nil)
Failed to predict using VNPersonsModel for homeUUID: %@ sourceUUID: %@
 not
v24@?0@"HMIPersonSourceUUIDPair"8^B16
v32@?0@"NSSet"8@"HMIPersonsModelPrediction"16^B24
q24@?0@"VNFaceObservation"8@"VNFaceObservation"16
v24@?0@"HMIFaceClassification"8^B16
@"VNFaceObservation"16@?0@"HMITorsoprint"8
v16@?0@"HMITorsoAnnotation"8
v16@?0@"HMIFaceCrop"8
Failed to predict using torso model for homeUUID: %@
@16@?0@"HMIPersonSourceUUIDPair"8
%@.bin
Directory to load/store persons models (%@) does not exist
Failed to enumerate persons models at path (%@)
\A[A-F0-9]{8}-[A-F0-9]{4}-[A-F0-9]{4}-[A-F0-9]{4}-[A-F0-9]{12}\.bin\Z
B16@?0@"NSURL"8
Failed to enumerate homes at path: %@
Directory to load/store home persons model (%@) contains multiples files: %@ there can only be one
Invalid file path in load model attempt: %@
Refusing to load %@ VNPersonsModel at path: %@
Failed to load VNPersonsModel at path: %@
@24@?0@"NSUUID"8@"HMIPersonsModel"16
v32@?0@"HMIPersonSourceUUIDPair"8@"NSSet"16^B24
v16@?0@"HMIDESLayerParameters"8
is_training
checkpoint
Loss/total_loss
B32@?0Q8@"<ETDataProvider>"16@"<ETTaskContext>"24
/tmp/train.espresso.net
Unknown reason.
Skipped
{code: %@, analysisFPS: %f, message: "%@"}
{code: %@, analysisFPS: %f}
v24@?0@"HMIExternalPersonManagerSettings"8@"NSError"16
@16@?0@"HMIVideoAnalyzer"8
@16@?0@"HMIVideoAnalyzerState"8
v16@?0@"HMIVideoAnalyzerConfiguration"8
v32@?0@"HMIVideoAnalyzer"8Q16^B24
Undefined
mediaserverd
homed
v16@?0@"HMIVideoAnalyzer"8
Scheduler state: 
usage: %@
, idle: %@
, %@: (%llu MB | %llu MB)
, mediaserverd: (%llu MB | %llu MB)
, homed: (%llu MB | %llu MB)
, thermalLevel: %lu
, build: %@
Release
, maxConcurrentAnalyzers: %lu
v32@?0@"NSString"8Q16^B24
usage
idle
footprint
maxFootprint
thermalLevel
build
analyzers
SignificantActivity.mlmodelc
HOMEAI_SIGNIFICANT_ACTIVITY_DETECTOR
v16@?0@"<TRINamespaceUpdateProtocol>"8
compiledModel
personThresholdHigh
personThresholdMedium
petThresholdHigh
petThresholdMedium
vehicleThresholdHigh
vehicleThresholdMedium
faceThreshold
descriptorA.length == descriptorB.length
cluster.objects.count > 0
faceObservations.count > 0
faceObservations.firstObject.faceprint != nil
faceprintLength == faceObservation.faceprint.lengthInBytes / sizeof(float)
Sample buffer has an invalid PTS.
randomUniform
value
{CGAffineTransform=dddddd}
probability
attributes
B16@?0@"HMICIFilterAttribute"8
HMIRotate
CIAffineTransform
inputAngle
inputTransform
v16@?0@"HMICIFilterAttribute"8
camera_recording
camera_recording_analyzer
camera_recording_analyzer_media
camera_recording_analyzer_scheduler
camera_recording_analyzer_scheduler_json
camera_recording_maintenance
Failed to load model at path: %@
HMIMLModel
/tmp/PackageAnalyzerReport-%@-%@.html
Package Analyzer
@"HMIVideoAnalyzerFrameResult"16@?0@"HMIVideoAnalyzerFrameResult"8
@"HMIVideoFrame"16@?0@"HMIVideoFrame"8
@"NSArray"16@?0@"HMIVideoAnalyzerFrameResult"8
Max Confidence Events
Frame Results
Thumbnails
Fragment
Configuration
HMICVFR.f
HMICVFR.roi
HMICVFR.ae
HMIOD.l
HMIOD.c
HMIOD.b.x
HMIOD.b.y
HMIOD.b.w
HMIOD.b.h
HMIOD.y.a
HMIOD.r.o
HMIP.x
HMIP.y
HMIAZ.p
HMIAZ.i
HMICameraVideoFrame does not support NSSecureCoding in the simulator
HKD://
analyzed-video-frames
 isInclusion:%d 
class-label
coordinates
overlap
activityZone
%@-%@-%@-%@.json
%@/%@
%@/activityzone-%@
B16@?0@"HMICameraActivityZone"8
filteringLevel
numDetectedObjects
com.apple.HomeAI.ActivityZone
noFiltering
resize_0
resize_10
resize_20
resize_26
resize_36
IFrameOnly
Full
Detector
Thumbnail Interval
Thumbnail Height
Timelapse Interval
Timelapse Preferred Fragment Duration
Max Fragment Duration
Max Fragment Analysis Duration
Decode Mode
Transcode
Transcode Codec
Passthrough Audio
Redact frames
Min Frame Quality
Min Frame Scale
Camera
Home UUID
Package Classifier Mode
Analysis FPS
minFrameQuality > 0 && minFrameQuality <= 1
minFrameScale > 0 && minFrameScale <= 1
analysisFPS > 0
Event Triggers
Recognize Faces
Activity Zone Count
q24@?0@"HMIVideoAnalyzerEvent"8@"HMIVideoAnalyzerEvent"16
B16@?0@"HMIStationaryObject"8
HMIVAEP.f
HMIVAEP.t
HMIVAEP.ibbe
Is Bounding Box & Confidence Estimated
Face
Torso
%@ %@ %@
com.apple.homed
syntheticEvents
personDetected
petDetected
vehicleDetected
packageDetected
analysisQOS
systemResourceUsageLevel
motionDetector
frameSelectorSampleRateMultiplier
frameSelectorTargetInterval
processingDevice
confidenceThresholdPersonHigh
confidenceThresholdPetHigh
confidenceThresholdVehicleHigh
confidenceThresholdFaceHigh
confidenceThresholdTorsoHigh
confidenceThresholdPackageHigh
confidenceThresholdPersonMedium
confidenceThresholdPetMedium
confidenceThresholdVehicleMedium
confidenceThresholdFaceMedium
confidenceThresholdTorsoMedium
confidenceThresholdPackageMedium
confidenceThresholdPackageClassifierMedium
confidenceThresholdPackageClassifierHigh
modelTimeout
uploadVideoAnalysisEvent
saveVideoFragmentResultHTML
assetReaderMaximizePowerEfficiency
assetReaderForceBGRA
analysisEnableTranscodeFragment
realTimeEncoding
espressoLowPriority
maxConcurrentAnalyzers
maxAnalysisFPS
opticalFlowLowPriority
opticalFlowBackgroundProcessing
saveDESRecords
skipMinDESRecordCount
DESSkipTraining
saveTrainedModel
DESSkipTrainingScalar
DESSkipPrivatize
enableDASTestConfiguration
personsModelStoragePath
personDataSourceDiskStoragePath
personsModelClassificationThresholdKnown
torsoPersonsModelClassificationThresholdKnown
faceVIPThresholdForTorsoAnnotation
personsModelClassificationThresholdUnknown
saveFaceCropsToDisk
showROI
useDevelopmentFeedbackService
eventTriggers
logHumanFriendlySchedulerState
schedulerStateLogFrequency
enableTorsoRecognition
enableSignposts
fragmentDiskBufferSize
logOtherProcessMemorySchedulerState
videoFrameSelectorMaxCandidates
useHEVC
taskServiceRunLocally
restartDecoderIfFormatChanges
user-interactive
user-initiated
unspecified
default
utility
background
Only NSNumber and NSString properties are supported.
v24@?0@"HMIVideoAssetWriter"8@"NSData"16
v32@?0@"HMIVideoAssetWriter"8@"NSData"16@"AVAssetSegmentReport"24
Encoder Queue
v24@?0@"HMIVideoEncoder"8^{opaqueCMSampleBuffer=}16
v24@?0@"HMIVideoFrameSampler"8^{opaqueCMSampleBuffer=}16
v32@?0@"HMIVideoFrameSampler"8@"HMIVideoEncoder"16^{opaqueCMSampleBuffer=}24
@"NSNumber"24@?0@"HMIVideoAnalyzerEvent"8@"NSNumber"16
FFArchive
yyyy-MM-dd
2021-05-15
q24@?0@"VNCluster"8@"VNCluster"16
v16@?0@"VNCluster"8
@"VNFaceObservation"16@?0@"NSNumber"8
B16@?0@"VNCluster"8
q24@?0@"HMIPersonFaceCrop"8@"HMIPersonFaceCrop"16
homePersonsAndFaceCrops
yyyy-MM-dd'T'HH-mm-ss
%@_%@.plist
@"NSUUID"16@?0@"HMIFaceprint"8
Fetch persons failed
CleanImpureHomePersonsOperation encountered failures
com.apple.HomeAI.%@%@%@.%tu
homeai: %@
Vision
Faceprints
Clusters
Persons
Associated
Faceprinting Duration
Clustering Duration
Total Duration
B16@?0@"HMIPersonSourceUUIDPair"8
v32@?0@"VNCluster"8@"NSUUID"16^B24
Error on dispatch_group_wait (associateFaceCrops)
Error associating face crops for %lu person%@: (
 ...
v16@?0@"HMIPersonFaceCrop"8
@"NSUUID"16@?0@"VNFaceObservation"8
@"NSUUID"16@?0@"NSUUID"8
v32@?0@"NSNumber"8@16^B24
HMICameraVideoFrameAnalyzerSignificantActivity
@"HMIFaceClassification"16@?0@"HMIPersonsModelPrediction"8
@"HMIVideoAnalyzerEvent"16@?0@"HMIVideoAnalyzerEvent"8
none
@"NSString"16@?0@"NSObject"8
high
medium
Unexpected event %@.
ClassifyFaceEvent
ClassifyTorsoEvent
HMIDESBT.u
HMIDESBackgroundTask
scale > 0 && scale <= 1
quality > 0 && quality <= 1
HMIVideoFrame failed to create compressed representation.
]1337;File=inline=1;preserveAspectRatio=1:%@
Data (JPEG)
CVPixelBuffer
Presentation Time Stamp
Size
Store
HMIVideoFrame does not support NSSecureCoding in the simulator.
/scratch.data
/Library/Spotlight/Backup/temp_%lu.dat
#EXTM3U
#EXT-X-VERSION:7
#EXT-X-TARGETDURATION:%.6f
#EXT-X-PLAYLIST-TYPE:VOD
#EXT-X-INDEPENDENT-SEGMENTS
#EXT-X-I-FRAMES-ONLY
#EXT-X-KEY:METHOD=AES-256-GCM,URI="%@"
#EXT-X-MAP:URI="%@"
#EXTINF:%.5f
#EXT-X-BYTERANGE:%lu@%lu
#EXT-X-ENDLIST
Bounding Box
P(%@|[%@])=%@
Package
Person
Vehicle
Motion
Event
event
#D62728
#2CA02C
#1F77B4
#9467BD
#FF7F0E
#8C564B
#7F7F7F
@24@?0#8@"NSString"16
HMITC.su
HMITC.pu
HMITC.conf
%.4lf
HMIVideoDecoderWorkQueue
Format description is missing.
Cannot accept format description.
Cannot create reorder buffer, err: %d.
Cannot create decoder.
Cannot decode frame, err: %d.
Cannot reorder frames.
Decoded sample has an invalid PTS.
HMIVideoAssetWriter
HMIVideoAssetWriterOutput
UTType class is not available.
HMICMSampleBufferIsAudio(sbuf)
Underlying asset writer has failed.
Couldn't append sample buffer because, exception %@
Failed to flush segment.
v32@?0@"<NSObject><NSCopying><NSSecureCoding>"8Q16^B24
@"NSDictionary"24@?0@"NSUUID"8@"HMIPersonsModel"16
v16@?0@"HMIPersonSourceUUIDPair"8
v32@?0@"NSUUID"8@"NSSet"16^B24
v32@?0@"NSUUID"8@"NSDictionary"16^B24
v24@?0@"VNFaceObservation"8^B16
B16@?0@"HMIMotionDetection"8
Sparse Optical Flow
%@ - %f
sparse
q24@?0@"HMIVideoFrameSelectorFrameCandidate"8@"HMIVideoFrameSelectorFrameCandidate"16
v16@?0@"HMIVideoFrameSelectorFrameCandidate"8
monitored
analysisFPS
timeSinceLastFragmentWasReceived
bufferFillRatio
bufferSize
delay
numDecodedSamples
numDidAnalyzeFrames
numDidAnalyzePackages
numDidCreateTimelapseFragments
averageAnalysisTime
decodeMode
transcodeCodecType
encode
encoder
activityZones
Name           
AFPS
Time
Last
Buffer               
Delay
     Decode:PTS
FR, FRAG
FR Time
Enc, Lapse:FRAG
Faces
Zones
Triggers
] %5ld KB
%.1f
%4ld:%.1f
%ld, %ld
%@%@%@ %@, %@:%ld
manufacturer
camera
Session has not received any new data for over 60 seconds.
Session Check
HMIFR.c
HMIFR.fc
HMIFR.fp
HMIFR.fqs
HMIFR.sea
HMIFR.seu
HMIFR.leus
TransitionMatrix
Joint
Face Classifications
Face Quality Score
selector
arguments
configuration
HMIVideoAnalyzer does not support remote analysis.
@"HMIVideoAnalyzer"16@?0@"HMIVideoAnalyzerConfiguration"8
VideoAnalyzerReport %@ %@
/tmp/%@.plist
fragmentResult
v16@?0@"HMIVideoAnalyzerEventFace"8
HMIVideoAnalyzerServer
HMIVideoAnalyzerServer - Input
HMIVideoAnalyzerServer - Encoder
dynamicConfiguration
Video fragment duration: %fs is greater than the max fragment duration value: %fs
%@_%@_%@
UnknownManufacturer
UnknownModel
UnknownFirmware
v20@?0@"NSString"8B16
YYYY-MM-dd-HH-mm-ss
sanitized.mp4
self.assetWriter == nil
self.timelapseOutputVideoFormat
 Timelapse
_encode
!self.encoder
self.inputVideoFormat
!self.timelapseEncoder
self.assetWriter
self.timelapseAssetWriter
v16@?0@"HMIVideoAnalyzerResultFilter"8
B16@?0@"AVAssetSegmentTrackReport"8
Analyzer is in full bypass mode.
Analyzer is in partial bypass mode, only IFrames are decoded.
/tmp/com.apple.HomeAI/familiar-face-data
Error initializing with home UUID: %@ source UUID:%@
Error enumerating persons
Error loading person from JSON
Error enumerating face crops for person UUID:%@
Error loading face crop from JSON
Error loading face crop image
HMIVAET.ro
HMIVAET.tr
Torso Roll
Torso Recognition
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-231.5.1/OpenCV/src/core/datastructs.cpp
cvReleaseMemStorage
cvSaveMemStoragePos
cvRestoreMemStoragePos
NULL storage pointer
cvMemStorageAlloc
Too large memory block is requested
datastructs.cpp
storage->free_space % CV_STRUCT_ALIGN == 0
requested size is negative or too big
(size_t)ptr % CV_STRUCT_ALIGN == 0
cvCreateSeq
Specified element size doesn't match to the size of the specified element type (try to use 0 for element type)
cvSetSeqBlockSize
Storage block size is too small to fit the sequence elements
cvCvtSeqToArray
cvStartReadSeq
cvChangeSeqBlock
cvGetSeqReaderPos
cvSetSeqReaderPos
cvSeqPush
ptr + elem_size <= seq->block_max
block->start_index > 0
NULL sequence pointer
cvSeqPushMulti
number of removed elements is negative
cvSeqPopMulti
delta > 0
cvClearSeq
Invalid sequence header
cvSeqSlice
Bad sequence slice
Bad input sequence
cvSeqSort
Null compare function
cvCreateSet
cvSetAdd
count <= CV_SET_ELEM_IDX_MASK+1
cvCreateGraph
cvGraphAddVtx
cvFindGraphEdgeByPtr
ofs == 1 || start_vtx == edge->vtx[0]
graph pointer is NULL
cvGraphAddEdgeByPtr
vertex pointers coinside (or set to NULL)
edge->flags >= 0
Invalid graph pointer
cvCloneGraph
cvInitTreeNodeIterator
cvNextTreeNode
icvInitMemStorage
icvDestroyMemStorage
icvGoNextMemBlock
parent->bottom == block
icvGrowSeq
The sequence has NULL storage pointer
storage->free_space >= delta
block->count % seq->elem_size == 0 && block->count > 0
seq->first->start_index == 0
icvFreeSeqBlock
(in_front_of ? block : block->prev)->count == 0
seq->ptr == block->data
block->count > 0 && block->count % seq->elem_size == 0
Unknown/unsupported border type
borderInterpolate
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-231.5.1/OpenCV/src/imgproc/filter.cpp
columnBorderType != BORDER_WRAP
!rowFilter.empty() && !columnFilter.empty()
bufType == srcType
0 <= anchor.x && anchor.x < ksize.width && 0 <= anchor.y && anchor.y < ksize.height
roi.x >= 0 && roi.y >= 0 && roi.width >= 0 && roi.height >= 0 && roi.x + roi.width <= wholeSize.width && roi.y + roi.height <= wholeSize.height
start
srcRoi.x >= 0 && srcRoi.y >= 0 && srcRoi.width >= 0 && srcRoi.height >= 0 && srcRoi.x + srcRoi.width <= src.cols && srcRoi.y + srcRoi.height <= src.rows
wholeSize.width > 0 && wholeSize.height > 0
proceed
src && dst && count > 0
srcY >= startY
dstY <= roi.height
src.type() == srcType && dst.type() == dstType
apply
dstOfs.x >= 0 && dstOfs.y >= 0 && dstOfs.x + srcRoi.width <= dst.cols && dstOfs.y + srcRoi.height <= dst.rows
_kernel.channels() == 1
getKernelType
cn == CV_MAT_CN(bufType) && ddepth >= std::max(sdepth, CV_32S) && kernel.type() == ddepth
getLinearRowFilter
Unsupported combination of source format (=%d), and buffer format (=%d)
cn == CV_MAT_CN(bufType) && sdepth >= std::max(ddepth, CV_32S) && kernel.type() == sdepth
getLinearColumnFilter
Unsupported combination of buffer format (=%d), and destination format (=%d)
cn == CV_MAT_CN(_dstType)
createSeparableLinearFilter
ktype == CV_8U || ktype == CV_32S || ktype == CV_32F || ktype == CV_64F
preprocess2DKernel
cn == CV_MAT_CN(dstType) && ddepth >= sdepth
getLinearFilter
Unsupported combination of source format (=%d), and destination format (=%d)
createLinearFilter
(symmetryType & (KERNEL_SYMMETRICAL | KERNEL_ASYMMETRICAL)) != 0
SymmColumnVec_32s8u
SymmColumnSmallVec_32s16s
SymmColumnSmallVec_32f
SymmColumnVec_32f16s
SymmColumnVec_32f
anchor.inside(Rect(0, 0, ksize.width, ksize.height))
normalizeAnchor
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-231.5.1/OpenCV/src/imgproc/precomp.hpp
(symmetryType & (KERNEL_SYMMETRICAL | KERNEL_ASYMMETRICAL)) != 0 && this->ksize <= 5
SymmRowSmallFilter
kernel.type() == DataType<DT>::type && (kernel.rows == 1 || kernel.cols == 1)
RowFilter
kernel.type() == DataType<ST>::type && (kernel.rows == 1 || kernel.cols == 1)
ColumnFilter
this->ksize == 3
SymmColumnSmallFilter
SymmColumnFilter
_kernel.type() == DataType<KT>::type
Filter2D
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-231.5.1/OpenCV/src/core/alloc.cpp
Failed to allocate %lu bytes
OutOfMemoryError
borderType != BORDER_CONSTANT
pyrDown
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-231.5.1/OpenCV/src/imgproc/pyramids.cpp
!_src.empty()
pyrDown_
std::abs(dsize.width*2 - ssize.width) <= 2 && std::abs(dsize.height*2 - ssize.height) <= 2
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-231.5.1/OpenCV/src/core/opengl_interop_deprecated.cpp
GlBuffer
GlTexture
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-231.5.1/OpenCV/src/core/persistence.cpp
Invalid pointer to file storage
The node is neither a map nor an empty collection
cvGetFileNodeByName
Null element name
cvStartWriteStruct
The file storage is opened for reading
cvEndWriteStruct
cvWriteInt
cvWriteString
cvWriteRawData
Negative number of elements
Null data pointer
persistence.cpp
cvStartReadRawData
Null pointer to source file node or reader
The file node should be a numerical scalar or a sequence
cvReadRawDataSlice
Null pointer to reader or destination array
The readed sequence is a scalar, thus len must be 1
The sequence element is not a numerical scalar
The sequence slice does not fit an integer number of records
Null pointers to source file node or destination array
cvReadRawData
opencv-sequence
opencv-sequence-tree
opencv-graph
opencv-sparse-matrix
opencv-image
opencv-matrix
opencv-nd-matrix
Invalid type info
cvRegisterType
Some of required function pointers (is_instance, release, read or write) are NULL
Type name should start with a letter or _
Type name should contain only letters, digits, - and _
NULL double pointer
cvRead
The node does not represent a user object (unknown type?)
Unknown array type
The storage is not opened
icvPuts
An attempt to add element without a key to a map, or add element with key to sequence
icvXMLWriteTag
A single _ is a reserved tag name
Closing tag should not include any attributes
Key should start with a letter or _
Key name may only contain alphanumeric characters [a-zA-Z0-9], '-' and '_'
icvDecodeFormat
fmt_pairs != 0 && max_len > 0
Invalid data type specification
Too long data type specification
(align & (align-1)) == 0 && size < INT_MAX
%.8e
-.Inf
.Inf
%.16e
elements with keys can not be written to sequence
icvXMLWriteScalar
icvYMLWrite
The key is an empty
The key is too long
Key must start with a letter or _
Key names may only contain alphanumeric characters [a-zA-Z0-9], '-', '_' and ' '
icvReleaseSeq
flags
count
Some of essential sequence attributes are absent
icvReadSeq
The sequence flags are invalid
curve
closed
hole
untyped
header_dt
header_user_data
One of "header_dt" and "header_user_data" is there, while the other is not
rect
origin
Only one of "header_user_data", "rect" and "origin" tags may occur
color
data
The image data is not found in file storage
The number of stored elements does not match to "count"
Too complex format for the matrix
icvDecodeSimpleFormat
recursive
false
False
FALSE
icvWriteSeqTree
CV_IS_SEQ( seq )
sequences
icvWriteSeq
level
The size of element calculated from "dt" and the elem_size do not match
icvGetFormat
Size of sequence element (elem_size) is inconsistent with seq->flags
%d%c
The size of header calculated from "header_dt" is greater than header_size
icvWriteHeaderData
opencv-sequence-tree instance should contain a field "sequences" that should be a sequence
icvReadSeqTree
All the sequence tree nodes should contain "level" field
level == prev_level + 1
icvReleaseGraph
vertex_dt
edge_dt
vertex_count
edge_count
Some of essential graph attributes are absent
icvReadGraph
oriented
Graph edges should start with 2 integers and a float
%df%s
vertices
edges
No edges data
No vertices data
Some of stored vertex indices are out of range
Duplicated edge has occured
cvAlignPtr
(align & (align-1)) == 0
icvWriteGraph
CV_IS_GRAPH(graph)
2if%s
sizes
Some of essential matrix attributes are absent
icvReadSparseMat
Could not determine sparse matrix dimensionality
The matrix data is not found in file storage
Sparse matrix data is corrupted
icvWriteSparseMat
CV_IS_SPARSE_MAT(mat)
(reader).seq->elem_size == sizeof(idx)
k < dims
Some of essential image attributes are absent
icvReadImage
layout
interleaved
Only interleaved images can be read
The matrix size does not match to the number of stored elements
icvWriteImage
CV_IS_IMAGE(image)
Images with planar data layout are not supported
top-left
bottom-left
planar
rows
cols
icvReadMat
icvWriteMat
CV_IS_MAT_HDR_Z(mat)
icvReadMatND
Could not determine the matrix dimensionality
icvWriteMatND
CV_IS_MATND_HDR(mat)
type == CV_32FC1 || type == CV_32FC2 || type == CV_64FC1 || type == CV_64FC2
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-231.5.1/OpenCV/src/core/dxt.cpp
This mode (using nonzero_rows with a single-column matrix) breaks the function's logic, so it is prohibited.
For fast convolution/correlation use 2-column matrix or single-row matrix instead
dxt.cpp
!inv
type == srcB.type() && srcA.size() == srcB.size()
mulSpectrums
(flags & DFT_NO_PERMUTE) == 0
(unsigned)k0 < (unsigned)n && (unsigned)k1 < (unsigned)n
factors[0] == factors[nf-1]
(unsigned)j < (unsigned)n2
(unsigned)j < (unsigned)n
RealDFT
tab_size == n
CCSIDFT
src != dst
DFTInit
nf < 34
elem_size == sizeof(Complex<float>)
channels() == CV_MAT_CN(dtype)
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-231.5.1/OpenCV/src/core/copy.cpp
mask.depth() == CV_8U && (mcn == 1 || mcn == cn)
size() == mask.size()
checkScalar(value, type(), _value.kind(), _InputArray::MAT )
setTo
mask.empty() || mask.type() == CV_8U
repeat
ny > 0 && nx > 0
maskarr == 0
cvCopy
src.depth() == dst.depth() && src.size == dst.size
(coi1 != 0 || src.channels() == 1) && (coi2 != 0 || dst.channels() == 1)
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-231.5.1/OpenCV/src/core/opengl_interop.cpp
The library is compiled without OpenGL support
throw_nogl
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-231.5.1/OpenCV/src/imgproc/thresh.cpp
Unknown threshold type
thresh_8u
thresh_16s
thresh_32f
img.depth() == CV_8U && winSize.width > 2 && winSize.height > 2
buildOpticalFlowPyramid
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-231.5.1/OpenCV/src/video/lkpyramid.cpp
maxLevel >= 0 && winSize.width > 2 && winSize.height > 2
calcOpticalFlowPyrLK
(npoints = prevPtsMat.checkVector(2, CV_32F, true)) >= 0
nextPtsMat.checkVector(2, CV_32F, true) == npoints
statusMat.isContinuous()
errMat.isContinuous()
levels1 >= 0
ofs.x >= winSize.width && ofs.y >= winSize.height && ofs.x + prevPyr[lvlStep1].cols + winSize.width <= fullSize.width && ofs.y + prevPyr[lvlStep1].rows + winSize.height <= fullSize.height
levels2 >= 0
ofs.x >= winSize.width && ofs.y >= winSize.height && ofs.x + nextPyr[lvlStep2].cols + winSize.width <= fullSize.width && ofs.y + nextPyr[lvlStep2].rows + winSize.height <= fullSize.height
prevPyr[level * lvlStep1].size() == nextPyr[level * lvlStep2].size()
prevPyr[level * lvlStep1].type() == nextPyr[level * lvlStep2].type()
depth == CV_8U
calcSharrDeriv
cvAlign
internal.hpp
scn == 1
convertAndUnrollScalar
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-231.5.1/OpenCV/src/core/arithm.cpp
op == CMP_LT || op == CMP_LE || op == CMP_EQ || op == CMP_NE || op == CMP_GE || op == CMP_GT
compare
The operation is neither 'array op array' (where arrays have the same size and the same type), nor 'array op scalar', nor 'scalar op array'
The operation is neither 'array op array' (where arrays have the same size and type), nor 'array op scalar', nor 'scalar op array'
binary_op
(mask.type() == CV_8UC1 || mask.type() == CV_8SC1)
mask.size == src1.size
The operation is neither 'array op array' (where arrays have the same size and the same number of channels), nor 'array op scalar', nor 'scalar op array'
arithm_op
src2.type() == CV_64F && (src2.rows == 4 || src2.rows == 1)
When the input arrays in add/subtract/multiply/divide functions have different types, the output array type must be explicitly specified
img.dims <= 2 && templ.dims <= 2 && corr.dims <= 2
crossCorr
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-231.5.1/OpenCV/src/imgproc/templmatch.cpp
depth == tdepth || tdepth == CV_32F
corrsize.height <= img.rows + templ.rows - 1 && corrsize.width <= img.cols + templ.cols - 1
ccn == 1 || delta == 0
the input arrays are too big
CV_MAT_CN(sumType) == CV_MAT_CN(srcType)
getRowSumFilter
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-231.5.1/OpenCV/src/imgproc/smooth.cpp
CV_MAT_CN(sumType) == CV_MAT_CN(dstType)
getColumnSumFilter
Unsupported combination of sum format (=%d), and destination format (=%d)
sumCount == ksize-1
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-231.5.1/OpenCV/src/imgproc/utils.cpp
top >= 0 && bottom >= 0 && left >= 0 && right >= 0
copyMakeBorder
value[0] == value[1] && value[0] == value[2] && value[0] == value[3]
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-231.5.1/OpenCV/src/imgproc/deriv.cpp
ktype == CV_32F || ktype == CV_64F
getScharrKernels
dx >= 0 && dy >= 0 && dx+dy == 1
getSobelKernels
The kernel size must be odd and not larger than 31
dx >= 0 && dy >= 0 && dx+dy > 0
ksize > order
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-231.5.1/OpenCV/src/core/convert.cpp
src && nsrcs > 0 && dst && ndsts > 0 && fromTo && npairs > 0
mixChannels
j < nsrcs && src[j].depth() == depth
i1 >= 0 && j < ndsts && dst[j].depth() == depth
type == B.type() && (type == CV_32FC1 || type == CV_64FC1 || type == CV_32FC2 || type == CV_64FC2)
gemm
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-231.5.1/OpenCV/src/core/matmul.cpp
a_size.width == len
a_size.height == len
C.type() == type && (((flags&GEMM_3_T) == 0 && C.rows == d_size.height && C.cols == d_size.width) || ((flags&GEMM_3_T) != 0 && C.rows == d_size.width && C.cols == d_size.height))
type == CV_64FC2
src1.type() == src2.type()
scaleAdd
src.channels() == 1
mulTransposed
delta.channels() == 1 && (delta.rows == src.rows || delta.rows == 1) && (delta.cols == src.cols || delta.cols == 1)
operator()
GEMM_TransposeBlock
matmul.cpp
MulTransposedR
delta_cols == 1
cn <= 4 && func != 0
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-231.5.1/OpenCV/src/core/stat.cpp
src.channels() == 1 && func != 0
countNonZero
mean
(cn == 1 && (mask.empty() || mask.type() == CV_8U)) || (cn >= 1 && mask.empty() && !minIdx && !maxIdx)
minMaxIdx
img.dims <= 2
src.type() == CV_8UC1
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-231.5.1/OpenCV/src/core/lapack.cpp
type == CV_32F || type == CV_64F
invert
m == n
method == DECOMP_LU || method == DECOMP_CHOLESKY
lapack.cpp
n == 1
type == _src2.type() && (type == CV_32F || type == CV_64F)
solve
(method != DECOMP_LU && method != DECOMP_CHOLESKY) || is_normal || src.rows == src.cols
src.rows == 1
The function can not solve under-determined linear systems
src.rows == src.cols
eigen
w.type() == u.type() && u.type() == vt.type() && u.data && vt.data && w.data
backSubst
u.cols >= nm && vt.rows >= nm && (w.size() == Size(nm, 1) || w.size() == Size(1, nm) || w.size() == Size(vt.rows, u.cols))
rhs.data == 0 || (rhs.type() == type && rhs.rows == m)
_SVDcompute
0 <= d && d <= CV_MAX_DIM && _sizes
create
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-231.5.1/OpenCV/src/core/matrix.cpp
step[dims-1] == (size_t)CV_ELEM_SIZE(flags)
m.dims >= 2
0 <= _rowRange.start && _rowRange.start <= _rowRange.end && _rowRange.end <= m.rows
0 <= _colRange.start && _colRange.start <= _colRange.end && _colRange.end <= m.cols
m.dims <= 2
0 <= roi.x && 0 <= roi.width && roi.x + roi.width <= m.cols && 0 <= roi.y && 0 <= roi.height && roi.y + roi.height <= m.rows
ranges
r == Range::all() || (0 <= r.start && r.start < r.end && r.end <= m.size[i])
dims <= 2
diag
img->dataOrder == IPL_DATA_ORDER_PIXEL
img->dataOrder == IPL_DATA_ORDER_PIXEL || img->roi->coi != 0
(int)nelems >= 0
reserve
resize
COI is not supported by the function
cvarrToMat
seq->total > 0 && CV_ELEM_SIZE(seq->flags) == seq->elem_size
dims <= 2 && step[0] > 0
locateROI
adjustROI
reshape
The matrix is not continuous, thus its number of rows can not be changed
Bad new number of rows
The total number of matrix elements is not divisible by the new number of rows
The total width is not divisible by the new number of channels
cn <= 4
scalarToRawData
i < 0
getMat
0 <= i && i < (int)vv.size()
This method is not implemented for oclMat yet
k == STD_VECTOR_MAT
0 <= i && i < (int)v.size()
getMatVector
This function in deprecated, do not use it
getGlBuffer
getGlTexture
k == GPU_MAT
getGpuMat
size
i < (int)vv.size()
total
empty
!fixedSize() || ((Mat*)obj)->size.operator()() == _sz
!fixedType() || ((Mat*)obj)->type() == mtype
!fixedSize() || ((gpu::GpuMat*)obj)->size() == _sz
!fixedType() || ((gpu::GpuMat*)obj)->type() == mtype
!fixedSize() || ((ogl::Buffer*)obj)->size() == _sz
!fixedType() || ((ogl::Buffer*)obj)->type() == mtype
!fixedSize() || ((Mat*)obj)->size.operator()() == Size(cols, rows)
!fixedSize() || ((gpu::GpuMat*)obj)->size() == Size(cols, rows)
!fixedSize() || ((ogl::Buffer*)obj)->size() == Size(cols, rows)
!fixedType() && !fixedSize()
CV_MAT_TYPE(mtype) == m.type()
m.dims == dims
m.size[j] == sizes[j]
mtype == type0 || (CV_MAT_CN(mtype) == 1 && ((1 << type0) & fixedDepthMask) != 0)
dims == 2 && ((sizes[0] == sz.height && sizes[1] == sz.width) || (allowTransposed && sizes[0] == sz.width && sizes[1] == sz.height))
dims == 2 && (sizes[0] == 1 || sizes[1] == 1 || sizes[0]*sizes[1] == 0)
!fixedSize() || len == vv.size()
mtype == type0 || (CV_MAT_CN(mtype) == CV_MAT_CN(type0) && ((1 << type0) & fixedDepthMask) != 0)
!fixedSize() || len == ((vector<uchar>*)v)->size() / esz
Vectors with element size %d are not supported. Please, modify OutputArray::create()
create() called for the missing output array
!fixedSize() || len == len0
v[j].empty()
i < (int)v.size()
!fixedType() || (CV_MAT_CN(mtype) == m.channels() && ((1 << CV_MAT_TYPE(flags)) & fixedDepthMask) != 0)
!fixedSize()
release
clear
k == MAT
getMatRef
setIdentity
src.dims <= 2 && esz <= (size_t)32
transpose
src.size() == dst.size() && (src.cols == 1 || src.rows == 1)
func != 0
m.dims <= 2 && m.rows == m.cols
completeSymm
src.dims <= 2
src.channels() == dst.channels()
_arrays && (_ptrs || _planes)
init
narrays <= 1000
arrays[i] != 0
A.size == arrays[i0]->size
A.step[d-1] == A.elemSize()
copyTo
convertTo
minMaxLoc
0 <= _dims && _dims <= CV_MAX_DIM
setSize
s >= 0
%s:%d: error: (%d) %s in function %s
%s:%d: error: (%d) %s
OpenCV Error: %s (%s) in %s, file %s, line %d
No Error
Backtrace
Unspecified error
Internal error
Insufficient memory
Bad argument
Iterations do not converge
Autotrace call
Incorrect size of input array
Null pointer
Division by zero occured
Image step is wrong
Inplace operation is not supported
Requested object was not found
Input image depth is not supported by function
Formats of input arguments do not match
Sizes of input arguments do not match
One of arguments' values is out of range
Unsupported format or combination of formats
Input COI is not supported
Bad number of channels
Bad flag (parameter or structure field)
Bad parameter of type CvPoint
Bad type of mask argument
Parsing error
The function/feature is not implemented
Memory block has been corrupted
Assertion failed
No GPU support
Gpu API call
No OpenGL support
OpenGL API call
Unknown %s code %d
status
module != 0 && module->name != 0 && module->version != 0
cvRegisterModule
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-231.5.1/OpenCV/src/core/system.cpp
cxcore
2.4.13.6
unknown function
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-231.5.1/OpenCV/include/opencv2/dynamicuda/dynamicuda.hpp
copy
copyWithMask
convert
mallocPitch
The library is compiled without CUDA support
qualityLevel > 0 && minDistance >= 0 && maxCorners >= 0
goodFeaturesToTrack
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-231.5.1/OpenCV/src/imgproc/featureselect.cpp
mask.empty() || (mask.type() == CV_8UC1 && mask.size() == image.size())
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-231.5.1/OpenCV/src/core/array.cpp
Non-positive width or height
cvCreateMatHeader
Invalid matrix type
cvInitMatHeader
Non-positive cols or rows
cvReleaseMat
Bad CvMat header
cvCloneMat
NULL matrix header pointer
cvInitMatNDHeader
invalid array data type
NULL <sizes> pointer
non-positive or too large number of dimensions
one of dimesion sizes is non-positive
The array is too big
cvCreateMatNDHeader
Bad CvMatND header
cvCloneMatND
src->dims <= CV_MAX_DIM
_dst.data == data0
Incorrect number of arrays
cvInitNArrayIterator
Some of required array pointers is NULL
Iterator pointer is NULL
COI set is not allowed here
Number of dimensions is the same for all arrays
Data type is not the same for all arrays
Number of channels is not the same for all arrays
Depth is not the same for all arrays
Mask should have 8uC1 or 8sC1 data type
Dimension sizes are the same for all arrays
cvNextNArraySlice
array.cpp
iterator != 0
cvCreateSparseMat
bad number of dimensions
cvReleaseSparseMat
Invalid sparse array header
cvCloneSparseMat
Invalid sparse matrix header
cvInitSparseMatIterator
NULL iterator pointer
Data is already allocated
cvCreateData
unrecognized or unsupported array type
cvReleaseData
Only continuous nD arrays are supported here
cvGetElemType
cvGetDims
Array should be CvMat or IplImage
cvGetSize
index is out of range
cvPtr2D
COI must be non-null in case of planar images
NULL pointer to indices
cvPtrND
NULL array pointer is passed
cvGetMat
The matrix has NULL data pointer
The image has NULL data pointer
Images with planar data layout should be used with COI selected
The image is interleaved and has over CV_CN_MAX channels
Pixel order should be used with coi == 0
Input array has NULL data pointer
Unrecognized or unsupported array type
cvCreateImage
null pointer to header
cvInitImageHeader
Bad input roi
Unsupported format
Bad input origin
Bad input align
cvReleaseImageHeader
cvReleaseImage
cvSetImageROI
rect.width >= 0 && rect.height >= 0 && rect.x < image->width && rect.y < image->height && rect.x + rect.width >= (int)(rect.width > 0) && rect.y + rect.height >= (int)(rect.height > 0)
cvSetImageCOI
cvGetImageCOI
Bad image header
cvCloneImage
cvGetMatND
icvGetNodePtr
CV_IS_SPARSE_MAT( mat )
One of indices is out of range
(newsize & (newsize - 1)) == 0
GRAY
BGRA
op == MORPH_ERODE || op == MORPH_DILATE
getMorphologyRowFilter
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-231.5.1/OpenCV/src/imgproc/morph.cpp
Unsupported data type (=%d)
getMorphologyColumnFilter
getMorphologyFilter
depth == CV_8U || depth == CV_16U || depth == CV_16S || depth == CV_32F || depth == CV_64F
createMorphologyFilter
shape == MORPH_RECT || shape == MORPH_CROSS || shape == MORPH_ELLIPSE
getStructuringElement
morphOp
((size_t)src[i] & 15) == 0
((size_t)_src[i] & 15) == 0
_kernel.type() == CV_8U
MorphFilter
src.type() == CV_8UC1 || src.type() == CV_32FC1
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-231.5.1/OpenCV/src/imgproc/corner.cpp
cornerEigenValsVecs
CV_MAT_CN(_type) == e.a.channels()
assign
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-231.5.1/OpenCV/src/core/matop.cpp
Unknown operation
Invalid matrix initializer type
%{public}@Storing faceprints:%@ failed with error:%@
%{public}@Storing faceprints:%@ completed successfully
%{public}@Sample has a very large duration, the source video is corrupt.
%{public}@Original Sample Buffer: %@
%{public}@Bogus atomSize %llu, recovering by adjusting size.
%{public}@Simulated crash reporting is not available on this system.
%{public}@Not generating a memory exception report for %@ since another report was generated within last %f seconds.
%{public}@Memory exception reporting is not available on this system.
%{public}@Torsoprint Version: %ld.%ld
%{public}@Removing faceCrops:%@ failed with error:%@
%{public}@Removing faceCrops:%@ completed successfully
%{public}@Received nil data source
%{public}@Fetching settings using data source: %@
%{public}@Error fetching settings: %@
%{public}@handleUpdatedPerson: %@
%{public}@handleUpdatedUnassociatedFaceCrop: %@
%{public}@handleUpdatedPersonFaceCrop: %@
%{public}@handleUpdatedFaceprint: %@
%{public}@handleUpdatedSettings: %@
%{public}@handleRemovedPersonWithUUID: %@
%{public}@handleRemovedFaceCropWithUUID: %@
%{public}@handleRemovedFaceprintWithUUID: %@
%{public}@Successfully handled face misclassification
%{public}@Error in handling face misclassification, error:%@
%{public}@Submitted face misclassification task, taskID:%u
%{public}@Not storing face crop and faceprint for HMIHomePersonManager.UUID:%@ from face event:%@
%{public}@Storing unknown to Home face crop:%@ and faceprint:%@
%{public}@Error storing unassociated face crop:%@, error:%@
%{public}@Stored unassociated face crop:%@
%{public}@Error storing faceprint:%@, error:%@
%{public}@Stored faceprint:%@
%{public}@Reached face crop limit for sessionEntityUUID:%@ for HMIHomePersonManager.UUID:%@; not storing
%{public}@Timer fired, but person data is not yet available, waiting...
%{public}@Timer fired, updating home persons model
%{public}@Not triggering daily VIP Model Core Analytics event, last event was sent less than 1 day ago
%{public}@Triggering daily VIP Model Core Analytics event
%{public}@Successfully ran persons model summary task
%{public}@Failed to run persons model summary task, error:%@
%{public}@Submitted persons model summary task, taskID:%u
%{public}@Unrecognized timer: %@
%{public}@Updating with settings: %@
%{public}@Settings have disabled face classification, removing home persons model
%{public}@Settings have enabled face classification, updating home persons model
%{public}@Storing face crop:%@ failed with error:%@
%{public}@Storing face crop:%@ completed successfully
%{public}@Storing faceprint:%@ failed with error:%@
%{public}@Storing faceprint:%@ completed successfully
%{public}@Registering for Thermal Level Notifications
%{public}@Cannot get available space, error: %@
%{public}@Footprint: %@, Average: %@, Peak: %@
%{public}@Initializing shared model
%{public}@Model is not bundled into framework. Default model is stored in Git LFS. Make sure Git LFS is installed in your local system.
%{public}@Failed to load model!
%{public}@Failed to read sample buffer, error: %@
%{public}@Asset reader failed, ignoring
%{public}@Track %@, %@
%{public}@Warnings: %@
%{public}@personManager is nil for homeUUID: %@
%{public}@Error refreshing home data: %@
%{public}@No homes were located
%{public}@Found home: name: %@, primary: %s, UUID: %@
Identifier = %@, Name = %@
HMISignpost
%{public}@Ignoring %@
%{public}@fetchAllPersonsWithCompletion
%{public}@fetchPersonsWithUUIDs:%@
%{public}@fetchAllPersonFaceCropsWithCompletion
%{public}@Received invalid HMPersonFaceCropSource: %ld
%{public}@fetchFaceCropsForPersonsWithUUIDs:%@
%{public}@fetchAllFaceprintsWithCompletion
%{public}@fetchFaceprintsForFaceCropsWithUUIDs:%@
%{public}@fetchSettingsWithCompletion
%{public}@performCloudPullWithCompletion
%{public}@addFaceprints:%@
%{public}@removeFaceprintsWithUUIDs:%@
%{public}@Could not initialize from decoded personUUID: %@
%{public}@BackgroundEstimator(PTS:%.2f) Unable to update background model (%lu/%lu)
%{public}@Background model assignment is NULL %.2f
%{public}@Foreground TimeStamp mismtach %.2f vs %.2f
%{public}@Unable to resize frame for background estimator %@
%{public}@BackgroundEstimator(PTS:%.2f) Reset background model due to past timestamp %.2f
%{public}@BackgroundEstimator(PTS:%.2f) Reset outdated background model %.2f
%{public}@BackgroundEstimator(PTS:%.2f) Reset background model due to image size change
%{public}@BackgroundEstimator(PTS:%.2f) Reset background model due to very large foreground object
%{public}@BackgroundEstimator(PTS:%.2f) Unable to alloc buffer
%{public}@BackgroundEstimator(PTS:%.2f) No background model
%{public}@Fullfilled content request: %@
%{public}@Fullfilled data request: %@
%{public}@Failed to loadValuesAsynchronouslyForKeys, due to timeout.
%{public}@Face below face quality thresholds: SVM recognizability = %lf, Yaw = %lf, discarding
%{public}@Face below ANFR face quality threshold: ANFR confidence = %lf, discarding
%{public}@personsModelPredictions is empty
%{public}@Positively classified face with facemask: (sourceUUID: %@, personUUID: %@)
%{public}@Face removed from unknown & uncertain bucket: has facemask
%{public}@Face removed from unknown and uncertian bucket, SSD confidence = %f, entropy of laplacian = %f, Face yaw = %f, box size: %f
%{public}@Added to unknown bucket yaw: %@
%{public}@Added to uncertain bucket yaw: %@
%{public}@Face recognition set is empty
%{public}@DES record saving is not permitted.
%{public}@There isn't enough available disk space (%ld MB) to save DES records.
%{public}@Couldn't determine the amount of available space disk space, continuing.
%{public}@Saving des record for video frame (PTS:%0.2fs) to disk
%{public}@Saving DES Record, recordInfo: %@, data: %@
%{public}@Saved DES Record: %@, error: %@
%{public}@Unable to create input image tensor with error %@
%{public}@Track(PTS:%.2f-%.2f), dF:%.2f(%.2f), dT:%.2f(%.2f,%.2f), GIOU:%.2f(%.2f), %@ vs %@
%{public}@HMIPersonTracker: unable to set %@ at index %lu / %lu
%{public}@HMIPersonTracker: unable to get %@ at index %lu / %lu
%{public}@Error creating frame analyzer: %@
%{public}@Cannot find ground truth for %@
%{public}@Ignoring error detecting face in Photos face crop, error: %@
%{public}@Could not initialize from decoded UUID: %@ dataRepresentation: %@ dateCreated: %@
%{public}@Dropping frame, lastSamplePTS > nextSamplePTS.
%{public}@TaskID: %u running...
%{public}@options is empty/nil, defaulting to Home persons clustering task with impure person cleanup
%{public}@Creating HMPhotosPersonManager for homeUUID:%@ sourceUUID:%@
%{public}@Creating HMHomePersonManager for homeUUID:%@
%{public}@Current device is not primary resident, skipping clustering
%{public}@Initializing HMITaskServiceServer
%{public}@Error fetching persons:%@
%{public}@Skipping sentinel faceprint in existingAtCurrentVersion
%{public}@Skipping sentinel faceprint in createdAtCurrentVersion
%{public}@Faceprint Version: %ld.%ld
%{public}@Warm starting faceprint model...
%{public}@Failed to create pixel buffer when warm starting faceprint model
%{public}@Failed to warm start faceprint model: %@
%{public}@Warm start of faceprint model took: %f
%{public}@Cropping face %@ from pixel buffer with dimensions: %.1f x %.1f roll: %.02f degrees
%{public}@Error pixel buffer type conversion %@.
%{public}@Error in rotating the face %@.
%{public}@Face was rotated by:%.02f degrees
%{public}@HMIPrivateErrorCodeCropAndResizeFailed %@
%{public}@Cropping face %@ from face crop with dimensions %.1f x %.1f
%{public}@%lu faceprint(s) exist for face crop:%@ but are not the current version
%{public}@Using existing faceprint for face crop:%@
%{public}@Faceprinting face crop:%@
%{public}@Skipping crop, encountered error faceprinting: %@
%{public}@Face crop has a facemask, creating sentinel faceprint
%{public}@Vision run-time version: %d.%02d.%02d (%d)
%{public}@releaseCachedResources is deprecated and is now a no-op.
%{public}@Could not initialize from decoded UUID: %@ data: %@ modelUUID: %@ faceCropUUID: %@
%{public}@[Model loading] failed to create asset dir: %@ error: %@
%{public}@[Model loading] unpackaging
%{public}@[Model loading] failed to remove dir
%{public}@[Model loading] failed to untar model
%{public}@Error during update torso model task: %@
%{public}@Trusting host: %@
%{public}@Force Certificate Pinning
%{public}@Error setting trust policies: %lu
%{public}@Invalid certificate: %@
%{public}@Downloading Clip
%{public}@Fetched Clip videoAssetContext: %@, error: %@
%{public}@Fetching Clip, progress %lu%%
%{public}@Face crops are not available.
%{public}@Fetching Face Crops
%{public}@Fetched Person UUIDs: %@
%{public}@Fetched Face Crops: %@, error: %@
%{public}@Fetching Clip
%{public}@Use the face-blurred video for upload
%{public}@Use the original video without audio track for upload
%{public}@Uploading payload data: %@, to URL %@
%{public}@Submitting clipUUID: %@, cameraProfileUUID: %@
%{public}@Stripped Audio %@, error: %@
%{public}@Downloaded %@, error: %@
%{public}@Failed to fetch pre-signed URL, error: %@
%{public}@Failed to request service result, error: %@
%{public}@Failed to decode service result, error: %@
%{public}@Service result: %@
%{public}@Deleting Temporary File %@
%{public}@Deleted Temporary File %@, error: %@
%{public}@Failed to generate persons model summmary, error:%@
%{public}@Failed to fetch face crops with error: %@
%{public}@Failed to fetch faceprints with error: %@
%{public}@Error faceprinting face crops:%@
%{public}@Person (%@) has no faceprints -- nothing to remove
%{public}@Nearest face crop to be removed: %@
%{public}@Failed to remove face crop with error: %@
%{public}@Successfully removed face crop (%@) via user indicated misclassification
%{public}@Failed to remove persons model, error:%@
%{public}@Successfully removed persons model
%{public}@%@: %@
%{public}@Provided sequenceNumbers: %@, don't match fragment's sequenceNumbers: %@
%{public}@Failed to read fragment data, err: %d
%{public}@No torso annotations -- skipping torso model update
%{public}@Adding torso to existing sessionEntityUUID: %@ (face)
%{public}@Adding face to existing sessionEntityUUID: %@ (torso)
%{public}@Session entity %@ already has a face recognition, skipping subsequent match
%{public}@Existing face classification: %@
%{public}@New face classification: %@
%{public}@Assigning session entity %@ the face classification: %@
%{public}@updateTorsoModelAndGetTorsoAnnotationsForHome: %@
%{public}@Creating torso annotation with %lu torsoprints
%{public}@Adding face to existing sessionEntityUUID: %@ (VIP)
%{public}@Adding torso to existing sessionEntityUUID: %@ (VIP)
%{public}@Adding face to existing sessionEntityUUID: %@ (NN)
%{public}@Adding torso to existing sessionEntityUUID: %@ (NN)
%{public}@Adding face to existing sessionEntityUUID: %@ (track)
%{public}@Adding torso to existing sessionEntityUUID: %@ (track)
%{public}@Adding new face sessionEntityUUID: %@
%{public}@Adding new torso sessionEntityUUID: %@
%{public}@Error removing persons with UUIDs:%@, error:%@
%{public}@Succesfully removed persons %@
%{public}@Removing faceprints:%@ failed with error:%@
%{public}@Removing faceprints:%@ completed successfully
%{public}@Could not decode torsoAnnotations
%{public}@Publishing local state
%{public}@Dropping remote analysis torso update since torso rec is not enabled on this device
%{public}@Dropped %lu incompatible torsoprint annotations out of %lu total
%{public}@Successfully update torso model
%{public}@Error in update torso model, error:%@
%{public}@Submitted torso model update task, taskID:%u
%{public}@Could not initialize from decoded sourceUUID: %@
%{public}@Could not initialize from decoded UUID: %@
%{public}@Saved face classification:%@ to disk
%{public}@Could not initialize from decoded UUID: %@ data: %@
%{public}@Could not initialize from decoded faceRecognition: %@ torsoprints: %@ torsoModelVersion: %@
%{public}@Invalidated with err: %d
%{public}@Encoder is in a failed state, ignoring sample.
%{public}@VTCompressionSessionEncodeFrameWithOutputHandler (Handler) failed, err: %d
%{public}@VTCompressionSessionEncodeFrameWithOutputHandler, frame dropped.
%{public}@VTCompressionSessionCompleteFrames failed, err: %d
%{public}@VTCompressionSessionPrepareToEncodeFrames failed, err: %d
%{public}@Cannot set property: %@, error: %d
%{public}@Cannot copy property: %@, error: %d
%{public}@addFaceCrops:%@
%{public}@addPersonFaceCrops:%@
%{public}@Received invalid HMIPersonFaceCropSource: %ld
%{public}@addPersons:%@
%{public}@fetchAllUnassociatedFaceCropsWithCompletion
%{public}@removeFaceCropsWithUUIDs:%@
%{public}@removePersonsWithUUIDs:%@
%{public}@associateFaceCropsWithUUIDs:%@ toPersonWithUUID:%@
%{public}@Writing updated userDefinedPersonLinksByHome[%@] to disk
%{public}@Stale Home VNPersonsModel with homeUUID: %@ sourceUUID: %@ detected, attempting to remove...
%{public}@Stale model path no longer on disk, proceeding with building persons model...
%{public}@Persisted VNPersonsModel for homeUUID: %@ sourceUUID: %@
%{public}@Did not remove VNPersonsModel for homeUUID: %@ sourceUUID: %@, no model found
%{public}@Error removing user defined person links file: %@
%{public}@Removed userDefinedPersonLinksByHome for homeUUID: %@
%{public}@Removed VNPersonsModel for homeUUID: %@ sourceUUID:%@
%{public}@Unable to build equivalency map for homeUUID: %@, error: %@
%{public}@Failure to lookup equivalency cell for %@ (equivalencyCellForHome is%@ nil)
%{public}@Found nil torsoToFaceCrop for home %@ with non-nil model!
%{public}@Unable to retrieve torsoprints for person: %@, %@
%{public}@Person %@ has %lu torsoprints
%{public}@Received torso annotation with no identifier: %@
%{public}@Received torso annotation with no classification corresponding to the linkedEntityUUID: %@
%{public}@Created new torso model with %lu persons and %d total torsoprints for home: %@
%{public}@Successfully updated torso model and face crop map for home: %@
%{public}@Resetting torso model and wiping data
%{public}@Successfully deleted torso data at path: %@
%{public}@Failed to delete torso directory at path: %@, error: %@
%{public}@Torso model version on disk doesn't match current version
%{public}@Found stale torso_to_facecrop file
%{public}@There is no current torso model for home: %@
%{public}@Torso model predicted person %@ with confidence %f
%{public}@Persons Model Storage Path:%@
%{public}@Failed to parse Home UUID from path: %@
%{public}@Failed to load External HMIPersonsModel at path: %@, error: %@
%{public}@Loaded External HMIPersonsModel for homeUUID: %@ sourceUUID: %@
%{public}@No home model found for homeUUID: %@
%{public}@Failed to load Home HMIPersonsModel at path: %@, error: %@
%{public}@Loaded Home HMIPersonsModel for homeUUID: %@ sourceUUID: %@
%{public}@Loaded %lu user defined equivalencies found for home: %@
%{public}@No user defined equivalencies found for home: %@ (reason: %@)
%{public}@Got nil for torso model file path, error: %@
%{public}@No torso model found for home %@ at path: %@
%{public}@Failed to load torsoToFaceCrop map, error: %@
%{public}@Failed to load torso model at path: %@, error: %@
%{public}@Successfully loaded torso model and face crop map for home: %@
%{public}@Resetting HMIPersonsModelManager
%{public}@Recipe doesn't have a custom learningRate value, learning rate in model definition is used for training: %e
%{public}@Recipe has a custom learningRate value, learning rate in recipe is used for training: %e
%{public}@Calculating pre-training loss value
%{public}@trainingCallback %lu, loss: %f
%{public}@Training was skipped because %@ is YES.
%{public}@Training Started
%{public}@Training Finished
%{public}@Saving trained model to %@
%{public}@Successfully saved trained model at %@
%{public}@Timer fired, updating external persons model
%{public}@Settings have disabled face classification, removing external persons model
%{public}@Settings have enabled face classification, updating external persons model
%{public}@Failed to remove persons model, error:%@, retrying...
%{public}@Submitted persons model remove task, taskID:%u, retryOnError:%@
%{public}@Cannot decode additional streams using H.264, %lu H.264 decoders are already being used.
%{public}@Cannot transcode additional streams using H.265, %lu H.265 encoders are already being used, trying with H.264.
%{public}@Cannot transcode additional streams, %lu H.264 encoders are already being used.
%{public}@Cannot generate timelapse, %lu H.264 encoders are already being used.
%{public}@%@
%{public}@Submitted task returned error: %@
%{public}@Clustering successful
%{public}@Clustering error
%{public}@Initializing HMIVisionSession
%{public}@Releasing vision session after period of inactivity
%{public}@Unloading model at path %@ after period of inactivity
%{public}@Failed to initialize model detector %@
%{public}@Detecting Package
%{public}@Skipping package detection analysis because package already was detected.
%{public}@Unable to resize frame for background estimator
%{public}@PackageAnalyzer(PTS:%.2f-%.2f) FG:%lu BG:%lu C:%lu AP:%lu P:%lu Sess:%d
%{public}@Detected package!
%{public}@Pixel buffer is unavailable (PTS:%.2f)
%{public}@Error creating activity zone result directory: %@
%{public}@Activity zone file path:%@
%{public}@Error converting activity zone results to JSON: %@
%{public}@Error writing activity zone results JSON to file: %@
%{public}@motionScore %f
%{public}@Inclusion zone:%@ intersecting with:(%@) Object coordinate %@ insetThreshold %f
%{public}@Exclusion zone:%@ intersecting with:(%@) Object coordinate %@ insetThreshold %f
%{public}@Events after activity zone filtering:(%@) Object coordinate %@ insetPercentage %f
%{public}@Add motion-vector stationary event %@
%{public}@Replace matched stationary event %@ for %@
%{public}@Add edge-distance stationary event %@
%{public}@Unsupported aspect ratio: (%d, %d)
%{public}@Encrypted Training Result
%{public}@Preference %@ is now %@, previously was %@
%{public}@Error fetching face crops for person:%@, error:%@
%{public}@Already started listening for the notification
%{public}@Unable to read the asset %@
%{public}@Finish re-encoding %.3f > %.3f
%{public}@Unable to init face detector %@
%{public}@Skip the frame @ %.3fs due to analyzer failure
%{public}@Skip the frame @ %.3fs due to blurring failure
%{public}@Failed to convert YCbCr to RGBA (%@)
%{public}@Failed to clone RGBA source image
%{public}@Failed to blur entire image (vImage_Error = %zd)
%{public}@Failed to copy blurred patch (vImage_Error = %zd)
%{public}@Failed to convert RGBA to YCbCr (%@)
%{public}@Fetching persons for HMICleanupImpureHomePersonsOperation
%{public}@Error fetching persons, error:%@
%{public}@Fetched %lu persons
%{public}@Fetching face crops for person: %@
%{public}@Error fetching facecrops for person:%@, error:%@
%{public}@Fetched %lu face crops for person: %@
%{public}@Ignoring error fetching faceprints for person:%@, error:%@
%{public}@Error faceprinting face crops for person:%@, error:%@
%{public}@Number of faceprints to cluster: %lu
%{public}@Clustering error:%@ treating identity: %@ as impure
%{public}@0 or 1 cluster exists, treating identity: %@ as pure
%{public}@Unnamed person %@ has %lu clusters, treating as impure
%{public}@Named person %@ with atleast 1 personLink has %lu clusters, treating as impure
%{public}@%lu clusters exists, for person %@ trying to club clusters using vip model
%{public}@Cluster size: %lu
%{public}@Error while creating vnpersonsmodel: %@, treating identity as impure
%{public}@Failed to predict using VNPersonsModel, error: %@, treating identity as impure
%{public}@Error while removing facecrops %@
%{public}@Reassociating %lu face crops to person UUID: %@
%{public}@Error while reassociating facecrops %@
%{public}@Error creating directory %@
%{public}@Error fetching attributes of the file: %@ at: %@. Attempting to delete it
%{public}@Deleted %@ to free up some space, error: %@
%{public}@Error while deleting %@ to free up some space, error: %@
%{public}@Disk buffer size of %@: %ld KB
%{public}@Archive familiar face data for home: %@ person: %@
%{public}@Cannot archive familiar face data for person %@, error: %@
%{public}@Couldn't get URL for home archives, error: %@
%{public}@Saving archived familiar face data for home: %@ person: %@ to: %@
%{public}@Couldn't save FF archive
%{public}@Saved FF archive
%{public}@Skipping person %@ due to nil or 0 face crops
%{public}@Person %@ has crops with unknown source, reassociating them
%{public}@Skipping person %@ as all crops are either old or have a non-unknown source
%{public}@Removing %lu sentinel facecrops for person %@
%{public}@0 faceprints for person: %@, skipping
%{public}@Found pure identity, skipping person %@
%{public}@Removing person %@ and associated crops
%{public}@HMICleanupImpureHomePersonsOperation exiting early because operation was canceled.
%{public}@Completed CleanImpureHomePersonsOperation
%{public}@CleanImpureHomePersonsOperation encountered %d failures
%{public}@Error while removing persons %@
%{public}@Spawning CleanupImpureHomePersonsOperation for %@ before home person clustering
%{public}@CleanupImpureHomePersonOperation finished with error:%@
%{public}@CleanupImpureHomePersonOperation finished successfully
%{public}@Error performing cloud pull:%@
%{public}@Fetching persons
%{public}@Fetched %lu persons (%lu unnamed)
%{public}@Exiting early because task was canceled.
%{public}@Skipping named person
%{public}@Deleting unnamed person %@ (0 face crops)
%{public}@Deleting unnamed person %@ (age = %f seconds)
%{public}@Error fetching face crops:%@
%{public}@Error fetching faceprints:%@
%{public}@Storing %lu newly created faceprints
%{public}@Error saving new faceprints:%@
%{public}@Removing %lu faceprints from old versions
%{public}@Error removing faceprints from old versions:%@
%{public}@Clustering error:%@
%{public}@Number of clusters: %lu
%{public}@Cluster of size %lu beneath threshold of %d
%{public}@Face prediction error:%@
%{public}@Assigning cluster to existing person with UUID: %@
%{public}@Error adding new persons:%@
%{public}@Error associating face crops with person (%@): %@
%{public}@Finished calls to associateFaceCropsWithUUIDs
%{public}@Error removing person with UUID:%@, error:%@
%{public}@Succesfully removed person %@
%{public}@Error fetching faceprints for face crop UUIDs:%@, error:%@
%{public}@Storing newly created faceprints: %@
%{public}@Removing existing faceprints at other versions: %@
%{public}@Failed to generate persons model, error:%@
%{public}@Invalid configuration: isExternalLibrary is NO but self.dataSource does not conform to HMIHomePersonManagerDataSource protocol!
%{public}@Successfully updated persons model
%{public}@WARNING: Model has %lu named persons -- limit supported is %d
%{public}@Error fetching faces to subsample for %@: %@
%{public}@Fetched 0 training faces for %@, this would remove all face crops! Skipping face crop removal.
%{public}@Expected subsampling to leave no more than %d, but got %lu faces selected. Enforcing limit.
%{public}@Subsampling will retain %lu from a total of %lu faces for %@
%{public}@Deleting a total of %lu face crops after subsampling
%{public}@Selected %lu persons for subsampling faces but did not choose any face crops to delete!
%{public}@Face Classification is enabled, but homeUUID is nil, skipping face recognition
%{public}@AnalyzerEvents(PTS:%.2fs): %@
%{public}@Creating analysis state update with %lu torso annotations
%{public}@Error while retrieving facecrop from torsomodel for personUUID: %@ homeUUID: %@
%{public}@Couldn't retrieve linked predictions from torsomodel for personUUID: %@ homeUUID: %@ error: %@
%{public}@Dropping Face event: %@ due to torso recognition
%{public}@Faceprinting failed for face: %@, error: %@
%{public}@Face: %@ didn't produce any classifications
%{public}@Torsoprinting failed for torso: %@, error: %@
%{public}@Video Frame:(PTS:%0.2fs) has %lu detection(s) before filtering
%{public}@Video Frame:(PTS:%0.2fs) has %lu detection(s) after filtering
%{public}@Unable to archive task %@: %@
%{public}@There is no task to schedule
%{public}@[Model loading] Error opening encrypted file
%{public}@[Model loading] Error reading mmaped encrypted file
%{public}@[Model loading] Error reading data from compressed file
%{public}@[Model loading] returning unsuccessfully post decompression due to invalid destination size
%{public}@[Model loading] Error in out file.
%{public}@failed stat %s
%{public}@[Model loading] failed to unpackage resources
%{public}@[Model loading] unpackaged resources version
%{public}@Error in opening temporary file.
%{public}@preallocated temporary file failed. %d
%{public}@Serious error in writing temporary file. %d
%{public}@Error fetching unassociated face crops:%@
%{public}@Error associating face crops (num UUIDs:%lu), to personUUID: %@ with source: %@ error:%@
%{public}@Succesfully associated face crops (num UUIDs %lu) to person UUID: %@ for source: %@
%{public}@Events file "%@" does not exist.
%{public}@Cannot read events from file "%@", error: %@
%{public}@Cannot load events file, exception: %@
%{public}@Video decoder is not running, ignoring %@
%{public}@Sample buffer has no samples, skipping.
%{public}@Invalid DTS, expected > %@, got %@, skipping.
%{public}@Restarting decoder because format description changed.
%{public}@Decoded sample is out of PTS order, sample: %@
%{public}@Decoded sample has an invalid PTS, sample: %@
%{public}@Decoder is already in a failed state.
%{public}@Decompression session decoded frames after decoder was deallocated, ignoring frames.
%{public}@Frame decode error %d
%{public}@Cannot add video input.
%{public}@Cannot add audio input.
%{public}@Failed to start writing, assetWriter.status: %ld, assetWriter:.error: %@
%{public}@Started writing at %@
%{public}@didOutputSegmentData segmentType: %ld
%{public}@Trying to recover by adjusting trim duration from %@ to the minimum trim duration: %@
%{public}@Asset writer has failed fatally, ignoring %@
%{public}@Failed to write to asset writer, error %@
%{public}@Dropped   %@ because an input for the media type was not found.
%{public}@Dropped    %@ because asset writer is waiting for a sync sample.
%{public}@Dropped    %@ because of input error %@
%{public}@Video / Audio Drift (video is ahead by) %+4.3f
%{public}@Dropped    %@ because an input %@ is not ready for more media data.
%{public}@Asset writer has failed fatally, ignoring flush.
%{public}@We don't have anything to flush, ignoring flush.
%{public}@Finished waiting for flushSegment, assetWriter.status: %ld, assetWriter.error: %@
%{public}@Number of all face observations: %ld
%{public}@Invalid entry in userDefinedPersonLinks: %@
%{public}@All links for %@ in userDefinedPersonLinks are invalid
%{public}@Skipping person who belongs to user defined equivalency cell: %@
%{public}@Comparing persons (%@, %@)
%{public}@Equivalency determined between pair: (%@, %@)!
%{public}@Cannot add to matching equivalency cell because it already has entry from this source: %@
%{public}@Failed to update persons model, error:%@, retrying...
%{public}@Failed to update persons model, error:%@
%{public}@Submitted persons model update task, taskID:%u, retryOnError:%@
%{public}@Skip torsoEvent with extreme roll (%.0f deg)
%{public}@Skip torsoEvent with extreme aspect ratio (w/h) (%.2f) pixelDim:(%f, %f) bbox:(%f, %f)
%{public}@Skip torsoEvent with torsoBox close to roi boundary. Dist: (%.4f)
%{public}@Failed to predict using torso vip model
%{public}@Synthesizing Motion Detections, Target: %@
%{public}@Adding Candidate: %@
%{public}@Selected: %@
%{public}@Creating analyzer with identifier: %@, configuration: %@
%{public}@VideoAnalyzerReport saved (%@)
%{public}@Recieved Message: %@
%{public}@Unknown %@
%{public}@Sent Message Reply: %@
%{public}@Camera, Manufacturer: %@, Model: %@, Fragment: %@, Sanitized Fragment Data: %@
%{public}@Finish Analyzer
%{public}@Analyzer has failed, ignoring finish.
%{public}@Video file %@ size is too large, maximum allowed is (%ld MB), no longer appending fragments.
%{public}@Disk buffer size remaining in %@, %ld MB
%{public}@Appending fragment to %@
%{public}@Saving fragment to %@
%{public}@Video format should not change.
%{public}@Audio format should not change.
%{public}@-[HMIVideoAnalyzerServer dealloc]
%{public}@Analyzer has failed or was cancelled, ignoring sample buffer.
%{public}@Analyzer has failed or was cancelled, ignoring flush.
%{public}@Bundling Fragment Result, timeRange: %@, frames: [%@], packages: [%@], thumbnails [%@]
%{public}@Analyzer frame result buffer should be empty. %@
%{public}@Package analyzer result buffer should be empty. %@
%{public}@Thumbnail buffer should be empty. %@
%{public}@Timelapse encoder failed, ignoring: error: %@
%{public}@Generated Fragment: %@ Outcome: %@ Max Confidence Events: %@
%{public}@Analyzer Failed: %@
%{public}@Analyzer is already in a failed state.
%{public}@Sending Result: %@
%{public}@analysisFPS changing from: %f to: %f
softlink:o:path:/System/Library/PrivateFrameworks/PrivateFederatedLearning.framework/PrivateFederatedLearning
HMIExternalPersonDataSourceDisk
HMIExternalPersonManagerDataSource
HMIPersonManagerDataSource
NSObject
HMIHTMLReportBox
HMIHTMLReport
HMIMutableCluster
HMFLogging
HMISystemResourceUsage
HMISystemResourceUsageMonitor
HMISystemResourceUsageMonitorProtocol
HMIVideoAnalyzerEventFace
HMIStoreFaceprintsOperation
HMIDataReader
HMIVideoEventEntry
HMIVideoEvent
HMIVideoEventBuffer
HMIVideoTimelineEntry
HMIVideoTimeline
HMIVideoTimelineProfiler
HMITimeIntervalAverage
HMIVideoAnalyzerEventMotion
HMITorsoprinter
HMIRemoveFaceCropsOperation
HMIHomePersonManager
HMFTimerDelegate
HMIStoreFaceCropOperation
HMIStoreFaceprintOperation
HMICameraVideoFrame
HMIVisionUtilities
HMIPairwiseMatch
HMIThermalMonitorService
HMIThermalMonitor
HMIMemorySampler
HMISignificantActivityFcosDetector
HMIVideoAssetReader
HMIHomeKitClient
HMHomeManagerDelegateAdapter
HMHomeManagerDelegate
HMISignpost
HMIFaceQualityFilterModelInput
MLFeatureProvider
HMIFaceQualityFilterSVM
HMIVideoNode
HMIVideoProcessingNode
HMIVideoAnalyzerProcessingNode
PFLBackgroundRunner
_DASExtensionRunner
HMIExternalPersonDataSourceHomeKit
HMI_CVML_Error
HMIPersonFaceCrop
NSCopying
NSSecureCoding
NSCoding
HMIVideoAnalyzerFrameResult
HMIForegroundBlob
HMIForegroundTrack
HMIBackgroundEstimator
HMIMemoryAVAsset
AVAssetResourceLoaderDelegate
MovingAverageEntry
MovingAverage
HMICameraVideoFrameResult
HMIVideoAnnotationParserRecord
HMIVideoAnnotationParserTrack
HMIVideoAnnotationParser
HMIDESMutableFloatArray
HMIFaceClassifierVIP
HMIFaceClassifier
HMIDESDetection
HMIDESDatasetSample
HMIDESDataset
HMIInputFeatureProvider
HMINMSConfiguration
HMIObjectDetection
HMIObjectDetectionUtils
HMIPersonBlob
HMIPersonTracker
HMIVideoFrameAnalyzerResult
HMIVideoFrameAnalyzer
HMIVideoFrameSamplerDelegate
HMIVideoFrameAnalyzerDelegateAdapter
HMIVideoFrameAnalyzerDelegate
HMIVideoAnalyzerReportMatch
HMIVideoAnalyzerReportRecord
HMIVideoAnalyzerMutableReportComparison
HMIVideoAnalyzerMutableReportSession
HMIVideoAnalyzerMutableReport
HMIFaceCrop
HMIVideoRetimer
HMIVideoRetimerDelegateAdapter
HMIVideoRetimerDelegate
HMICamera
HMITask
HMIHomeTask
HMIEmptyTask
HMITaskServiceServer
HMITaskService
HMIFetchPersonsOperation
HMFObject
HMITorsoRecognition
HMIUpdatedFaceprintsResult
HMIFaceprinter
HMIFaceDetectorVision
HMIFaceDetector
HMIFaceprint
HMIPoint
HMIHomePersonManagerSettings
NSMutableCopying
HMIMutableHomePersonManagerSettings
HMIModelLoader
HMIUpdateTorsoModelTask
HMIFeedbackSession
NSURLSessionDelegate
HMIFeedbackSubmitClipOperation
HMIFeedbackTask
HMIFeedback
HMIFaceClassification
HMIHomePersonDataSourceInMemory
HMIHomePersonManagerDataSource
HMIPersonsModelsSummaryTask
HMIFaceMisclassificationTask
HMIRemovePersonsModelTask
HMIVideoFrameGenerator
HMIPersonManager
HMIConfidence
HMIError
HMIVideoFragment
HMISessionEntityManager
HMIRemovePersonsOperation
HMIRemoveFaceprintsOperation
HMITuriTrialUpdateTask
HMIAnalysisStateUpdate
HMIAnalysisStateManager
HMIVideoAnnotator
HMISystemResourceUsageMonitorImpl
HMIVideoAnalyzerResultFilter
HMIVideoAnalyzerResultActivityZoneFilter
HMIPersonSourceUUIDPair
HMIPerson
HMIPersonsModelSummary
HMIPersonsModel
HMIJSONArchiver
HMIJSONUnarchiver
HMIFaceUtilities
HMITorsoprint
HMITorsoAnnotation
HMIVideoEncoder
HMIVideoEncoderDelegateAdapter
HMIVideoEncoderDelegate
HMIHomePersonDataSourceHomeKit
HMIExternalPersonManagerSettings
HMIMutableExternalPersonManagerSettings
HMIAnalytics
HMIPersonsModelsSummary
HMIPersonsModelPrediction
HMIPersonsModelManager
HMIDESLayerParameters
HMIDESTrainingResult
HMIDESTrainer
HMIVideoAnalyzerResultOutcome
HMIExternalPersonManager
HMIRemovePersonsModelOperation
HMIVideoAnalyzerSchedulerJSONLogger
HMIVideoAnalyzerScheduler
HMISystemResourceUsageMonitorDelegate
HMITuriTrialManager
HMIGreedyClustering
HMIVideoFrameSampler
HMIVideoFrameIntervalSampler
HMIVideoFrameSamplerDelegateAdapter
HMICIFilterAttributeValue
HMICIFilterAttribute
HMICIFilter
HMIVisionSession
HMIMLModel
HMIVideoPackageAnalyzerResult
HMIVideoPackageAnalyzer
HMIVideoPackageAnalyzerDelegate
HMIVideoPackageAnalyzerDelegateAdapter
HMIFaceQualityEntropyOfLaplacian
HMIVideoAnalyzerFragmentResult
HMICameraActivityZone
HMIVideoAnalyzerConfiguration
HMIVideoAnalyzerDynamicConfiguration
HMIStationaryObject
HMIVideoTemporalEventFilter
HMIMotionDetection
HMIAnalysisService
HMIVideoAnalyzerEventPerson
HMIDESPackageResult
HMIDESResultPackager
HMIPreference
HMIHomePersonDataSourceDisk
HMIFetchPersonFaceCropsOperation
HMINotifydObserver
HMIVideoAnalyzerEventVehicle
HMIFeedbackVisionProcessor
HMICleanupImpureHomePersonsOperation
HMIClusteringTaskSummary
HMIHomePersonClusteringTask
HMIFetchFaceprintsForFaceCropsOperation
HMIUpdatePersonsModelTask
HMIRegionOfInterestOperation
HMICameraVideoFrameAnalyzerSignificantActivity
HMICameraVideoFrameAnalyzer
HMIDESBackgroundTask
HMIVideoFrame
HMICompressionHelper
HMIVideoCommandBuffer
HMIVideoCommandBufferDelegateAdapter
HMIVideoCommandBufferDelegate
HMIVideoAnalyzerEventPet
HMIHLSPlaylist
HMIFetchUnassociatedFaceCropsOperation
HMIAssociateFaceCropsOperation
HMIVideoAnalyzerEvent
HMITorsoClassification
HMIVideoAnalyzerEventPackage
HMIVideoDecoder
HMIVideoDecoderDelegateAdapter
HMIVideoDecoderDelegate
HMIVideoAssetWriter
AVAssetWriterDelegate
HMIVideoAssetWriterDelegateAdapter
HMIVideoAssetWriterDelegate
HMIPersonsModelEquivalencyTable
HMIUpdatePersonsModelOperation
HMISparseOpticalFlowFeatureVector
HMISparseOpticalFlowMotionDetection
HMISparseOpticalFlowMotionDetector
HMIMotionDetector
HMITorsoClassifier
HMIVideoFrameSelectorFrameCandidate
HMIVideoFrameSelector
HMIVideoFrameSelectorDelegateAdapter
HMIVideoFrameSelectorDelegate
HMIVideoAnalyzerState
HMIFaceRecognition
HMIVideoAnalyzer
HMIVideoAnalyzerDelegatePrivate
HMIVideoAnalyzerDelegate
HMIVideoAnalyzerServer
HMIVideoAnalyzerDelegateAdapter
HMIPersonDataSourceDisk
HMIVideoAnalyzerEventTorso
workQueue
sourceURL
UUID
UUIDString
URLByAppendingPathComponent:
defaultManager
path
createDirectoryAtPath:withIntermediateDirectories:attributes:error:
stringWithFormat:
hmiPrivateErrorWithCode:description:underlyingError:
name
dictionaryWithObjects:forKeys:count:
serializeJSONObject:url:error:
countByEnumeratingWithState:objects:count:
personUUID
faceBoundingBox
dateCreated
dataRepresentation
writeToURL:atomically:
hmiPrivateErrorWithCode:description:
hmfErrorWithCode:
logCategory
isEqual:
class
self
performSelector:
performSelector:withObject:
performSelector:withObject:withObject:
isProxy
isKindOfClass:
isMemberOfClass:
conformsToProtocol:
respondsToSelector:
retain
release
autorelease
retainCount
zone
hash
superclass
description
debugDescription
TQ,R
T#,R
T@"NSString",R,C
fetchAllPersonsWithCompletion:
fetchPersonsWithUUIDs:completion:
fetchAllPersonFaceCropsWithCompletion:
fetchFaceCropsForPersonsWithUUIDs:completion:
fetchAllFaceprintsWithCompletion:
fetchFaceprintsForFaceCropsWithUUIDs:completion:
performCloudPullWithCompletion:
addFaceprints:completion:
removeFaceprintsWithUUIDs:completion:
fetchSettingsWithCompletion:
addPerson:completion:
addPersonFaceCrops:completion:
init
initWithBoundingBox:text:color:opacity:value:
boundingBox
text
color
opacity
value
.cxx_destruct
_opacity
_value
_text
_color
_boundingBox
T{CGRect={CGPoint=dd}{CGSize=dd}},R,V_boundingBox
T@"NSString",R,V_text
T@"NSString",R,V_color
Tf,R,V_opacity
Tf,R,V_value
initToFileAtPath:append:
stream
open
appendHeaderWithTitle:textColor:backgroundColor:
appendString:
close
outputPath
dealloc
bundleForClass:
URLForResource:withExtension:
dataWithContentsOfURL:
initWithData:encoding:
_loadResource:withExtension:
dataUsingEncoding:
bytes
length
write:maxLength:
appendFrame:text:boxes:imageBorder:imageColor:outlineBorder:outlineColor:
size
compressedFrameWithScale:quality:error:
array
addObject:
base64Encoded
componentsJoinedByString:
enumerateObjectsUsingBlock:
createPixelBufferFromJPEGData:error:
base64EncodedStringWithOptions:
frameResults
frame
presentationTimeStamp
appendFrameResult:frameTruth:description:
na_each:
dataWithContentsOfFile:
initWithData:
initWithVideoFragment:
valueWithCMTime:
na_map:
objectAtIndexedSubscript:
events
regionOfInterest
initWithFrame:events:regionOfInterest:
count
lastPathComponent
generateVideoFramesForTimes:completionHandler:
boxesForEvent:isTruth:
addObjectsFromArray:
boxForRegionOfInterest:
initWithKey:ascending:
allObjects
arrayWithObjects:count:
sortedArrayUsingDescriptors:
sessionEntityUUID
substringToIndex:
confidence
rgbColorCodeForEventClass:
face
faceRecognition
classifications
hmf_isEmpty
anyObject
fromTorsoClassification
torso
copy
initWithTitle:outputPath:
appendText:
appendFrame:text:
appendFaceCrop:imageBorder:imageColor:outlineBorder:outlineColor:
appendFragmentResult:
appendFragmentResult:assetPath:
flush
_stream
_outputPath
T@"NSOutputStream",R,V_stream
T@"NSString",R,V_outputPath
data
faceCentroid
initWithValue:count:
arrayWithObject:
torsoCentroid
faceCount
scale:
add:
floatArrayByScaling:
torsoCount
removeObjectsInRange:
logIdentifier
initWithFaceprint:
initWithTorsoprint:
faceprintUUIDs
torsoprintUUIDs
addLinkedEntityUUIDs:
linkedEntityUUIDs
addFaceprints:
addTorsoprints:
flushTorsoprints
setFaceRecognition:
torsoprints
setTorsoprints:
setFaceprintUUIDs:
setTorsoprintUUIDs:
setLinkedEntityUUIDs:
_faceCentroid
_torsoCentroid
_faceRecognition
_torsoprints
_faceprintUUIDs
_torsoprintUUIDs
_linkedEntityUUIDs
T@"NSMutableArray",&,N,V_faceprintUUIDs
T@"NSMutableArray",&,N,V_torsoprintUUIDs
T@"NSMutableSet",&,N,V_linkedEntityUUIDs
T@"NSMutableArray",&,V_torsoprints
T@"HMIDESMutableFloatArray",R,V_faceCentroid
T@"HMIDESMutableFloatArray",R,V_torsoCentroid
T@"HMIFaceRecognition",&,V_faceRecognition
systemResourceUsageLevel
setSystemResourceUsageLevel:
_systemResourceUsageLevel
Tq,N,V_systemResourceUsageLevel
productInfo
productClass
UTF8String
initWithProductClass:workQueue:
systemResourceUsageMonitorImpl
setDelegate:
delegate
getCurrentSystemResourceUsage
start
T@"<HMISystemResourceUsageMonitorDelegate>",W
_systemResourceUsageMonitorImpl
_workQueue
T@"HMISystemResourceUsageMonitorImpl",R,V_systemResourceUsageMonitorImpl
T@"NSObject<OS_dispatch_queue>",R,V_workQueue
initWithConfidence:boundingBox:yaw:roll:faceRecognition:userInfo:
initWithConfidence:boundingBox:yaw:roll:faceRecognition:torsoAnnotation:userInfo:
initWithConfidence:boundingBox:userInfo:
attributeDescriptions
initWithName:value:
torsoAnnotation
roll
arrayByAddingObjectsFromArray:
shortDescription
encodeWithCoder:
encodeObject:forKey:
initWithCoder:
decodeObjectOfClass:forKey:
userInfo
supportsSecureCoding
initWithConfidence:boundingBox:
initWithConfidence:boundingBox:faceRecognition:
_yaw
_roll
_torsoAnnotation
T@"NSNumber",R,V_yaw
T@"NSNumber",R,V_roll
T@"HMITorsoAnnotation",R,V_torsoAnnotation
T@"NSUUID",R
T@"HMIFaceRecognition",R,V_faceRecognition
initWithTimeout:
mainInsideAutoreleasePool
dataSource
faceprints
cancelWithError:
finish
initWithDataSource:faceprints:
main
_dataSource
_faceprints
T@"<HMIPersonManagerDataSource>",R,V_dataSource
T@"NSSet",R,V_faceprints
whitespaceCharacterSet
stringByTrimmingCharactersInSet:
subdataWithRange:
position
readUInt32
readUInt64
seek:
readData:
_data
_position
localeWithLocaleIdentifier:
setLocale:
setDateFormat:
dateFromString:
stringWithCapacity:
appendFormat:
dataWithLength:
mutableBytes
time
T{?=qiIq},R
initWithValue:time:
_time
T{?=qiIq},R,V_time
T@,R,V_value
removeAllObjects
hmf_removeFirstObject
indexOfObject:inSortedRange:options:usingComparator:
insertObject:atIndex:
hmf_objectsPassingTest:
indexesOfObjectsPassingTest:
objectsAtIndexes:
removeObjectsAtIndexes:
objectAtIndex:
initWithMaxCapacity:
objectsInTimeRange:includeEnd:
extractObjectsInTimeRange:
neighborsOfObject:
_lock
_maxCapacity
initWithTime:date:
date
_date
T@"NSDate",R,V_date
lastObject
dateAtTime:
timeIntervalSinceDate:
addDate:atTime:
timeIntervalSinceDateAtTime:
_buffer
addValue:
startedAtTime:
endedAtTime:
averageTime
_timeline
_average
initWithWindowSize:
numberWithDouble:
addNumber:
movingAverage
movingAverageForInterval:defaultValue:
valueForInterval:defaultValue:
isInternalInstall
distantPast
timeIntervalSinceNow
unlock
motionScore
numberWithFloat:
encodeDouble:forKey:
decodeDoubleForKey:
initWithConfidence:boundingBox:motionScore:
_motionScore
Tf,R,V_motionScore
currentTorsoRequestRevision
transferPixelBuffer:crop:size:pixelFormat:options:error:
setRevision:error:
observationWithRequestRevision:boundingBox:
setInputDetectedObjectObservations:
sharedInstance
vnSession
initWithCVPixelBuffer:options:session:
performRequests:error:
results
firstObject
torsoprint
descriptorData
initWithUUID:data:
currentModelUUID
createTorsoPixelBufferForTorsoEvent:pixelBuffer:error:
torsoprintForTorsoPixelBuffer:error:
faceCropUUIDs
removeFaceCropsWithUUIDs:completion:
initWithDataSource:faceCropUUIDs:
_faceCropUUIDs
T@"<HMIHomePersonManagerDataSource>",R,V_dataSource
T@"NSSet",R,V_faceCropUUIDs
initWithUUID:homeUUID:
setMaxConcurrentOperationCount:
initWithTimeInterval:options:
resume
dictionary
_updateSettings:
addExecutionBlock:
operationQueue
addOperation:
watchdogTimer
taskServiceClient
homeUUID
objectForKeyedSubscript:
isEqualToString:
submitTaskWithOptions:completionHandler:
initWithDataSource:faceCrop:
error
setCompletionBlock:
hmiPrivateErrorWithCode:
initWithDataSource:faceprint:
familiarity
sourceUUID
na_firstObjectPassingTest:
unknownFacesSavedCounts
intValue
numberWithInt:
setObject:forKeyedSubscript:
faceCrop
faceprint
storeUnassociatedFaceCrop:completion:
storeFaceprint:completion:
isPersonDataAvailableViaHomeKit
initWithSourceUUID:homeUUID:external:
suspend
analyticsTimer
settings
isFaceClassificationEnabled
standardUserDefaults
doubleForKey:
timeIntervalSinceReferenceDate
setDouble:forKey:
timerDidFire:
setDataSource:
handleUpdatedPerson:
handleUpdatedUnassociatedFaceCrop:
handleUpdatedPersonFaceCrop:
handleUpdatedFaceprint:
handleUpdatedSettings:
handleRemovedPersonWithUUID:
handleRemovedFaceCropWithUUID:
handleRemovedFaceprintWithUUID:
handleMisclassifiedPersonForFaceCrop:
handleNewFaceEvents:
_settings
_operationQueue
_watchdogTimer
_analyticsTimer
_unknownFacesSavedCounts
T@"NSOperationQueue",R,V_operationQueue
T@"HMFTimer",R,V_watchdogTimer
T@"HMFTimer",R,V_analyticsTimer
T@"NSMutableDictionary",R,V_unknownFacesSavedCounts
T@"HMIHomePersonManagerSettings",R,V_settings
T@"<HMIHomePersonManagerDataSource>",W,N,V_dataSource
setWithObject:
addFaceCrops:completion:
_faceCrop
T@"HMIFaceCrop",R,V_faceCrop
_faceprint
T@"HMIFaceprint",R,V_faceprint
initWithPixelBuffer:presentationTime:frameId:fragmentSequenceNumber:
presentationTime
frameId
initWithPixelBuffer:
initWithJPEGData:presentationTime:frameId:fragmentSequenceNumber:size:
fragmentSequenceNumber
pixelBuffer
jpegData
motionDetections
setMotionDetections:
_frameId
_fragmentSequenceNumber
_pixelBuffer
_jpegData
_motionDetections
_size
_presentationTime
T^{__CVBuffer=},R,V_pixelBuffer
T@"NSData",R,V_jpegData
T@"NSArray",&,V_motionDetections
T{?=qiIq},R,V_presentationTime
T{CGSize=dd},R,V_size
TQ,R,V_frameId
TQ,R,V_fragmentSequenceNumber
point
initWithName:
createPixelBufferWithSize:pixelFormat:useIOSurface:
numberWithUnsignedLong:
dictionaryWithDictionary:
setObject:forKey:
appendBytes:length:
createPixelBufferFromJPEGDataProvider:error:
imageWithData:
extent
contextWithOptions:
render:toCVPixelBuffer:
applyPadding:withOriginalSize:padding:
globalSession
releaseCachedResources
cropPixelBuffer:crop:error:
resizePixelBuffer:size:error:
resizePixelBuffer:size:pixelFormat:options:error:
transferPixelBuffer:pixelFormat:options:error:
createJPEGDataFromPixelBuffer:scale:encodeQuality:error:
createPixelBufferFromJPEGPath:error:
createPixelBufferFromImageData:error:
imposeMinSizeFor:withOriginalSize:minCrop:
maintainAspectRatio:originalSize:ratioThreshold:
transferPixelBuffer:rotationAngle:crop:size:precision:error:
releaseCachedVisionResources
initWithFirstIndex:secondIndex:score:
firstIndex
secondIndex
score
_score
_firstIndex
_secondIndex
TQ,R,V_firstIndex
TQ,R,V_secondIndex
Tf,R,V_score
compare:
initWithService:
readValue
_service
_updateThermalLevel
services
objectForKey:
readValueFromSensor:value:
readMaxValue:
thermalLevel
_client
_thermalLevelNotificationToken
_notificationQueue
_thermalLevel
_services
T@"NSMutableDictionary",R,V_services
TQ,R,V_thermalLevel
stringWithUTF8String:
integerValue
unsignedLongLongValue
tick
numberWithUnsignedLongLong:
setZeroPadsFractionDigits:
setAllowedUnits:
setCountStyle:
setAllowsNonnumericFormatting:
stringFromByteCount:
highWaterMark
initWithName:reason:userInfo:
stop
setHighWaterMark:
average
_highWaterMark
_tick
T@"HMFTimer",R,V_tick
T@"MovingAverage",R,V_average
Tq,V_highWaterMark
floatValue
inputDimensions
defaultAssetPath
fileURLWithPath:
initWithModelURL:
arrayWithCapacity:
_runNeuralNetworkOnPixelBuffer:offsetsZero:offsetsOne:scores:yaws:rolls:error:
_postProcessOffsetsZero:offsetsOne:scores:yaws:rolls:outputPredictions:
boolPreferenceForKey:defaultValue:
initWithPixelBuffer:presentationTimeStamp:
printWithScale:
inputFeatureValueName
initWithPixelBuffer:inputName:
sharedModel
predictionFromFeatures:error:
hmiPrivateErrorWithCode:underlyingError:
offsetsZeroFeatureValueNames
featureValueForName:
type
multiArrayValue
offsetsOneFeatureValueNames
scoresFeatureValueNames
yawsFeatureValueNames
rollsFeatureValueNames
shape
unsignedLongValue
strides
dataPointer
initWithLabelIndex:confidence:unclampedBoundingBox:yaw:roll:
nmsConfiguration
nmsMultiClass:output:nmsConfiguration:
labelIndex
initWithLabelIndex:confidence:boundingBox:yaw:roll:
initWithThresholdWithLabels:metricWithLabels:thresholdDefault:metricDefault:
pathForResource:ofType:
defaultNMSConfiguration
T@"HMIMLModel",R
initWithConfidenceThresholds:nmsConfiguration:assetPath:error:
predict:detectedObjects:error:
_confidenceThresholds
_anchorStrides
_inputFeatureValueName
_offsetsZeroFeatureValueNames
_offsetsOneFeatureValueNames
_scoresFeatureValueNames
_yawsFeatureValueNames
_rollsFeatureValueNames
_nmsConfiguration
_inputDimensions
T@"NSString",R,V_inputFeatureValueName
T@"NSArray",R,V_offsetsZeroFeatureValueNames
T@"NSArray",R,V_offsetsOneFeatureValueNames
T@"NSArray",R,V_scoresFeatureValueNames
T@"NSArray",R,V_yawsFeatureValueNames
T@"NSArray",R,V_rollsFeatureValueNames
T@"HMINMSConfiguration",R,V_nmsConfiguration
T{CGSize=dd},R,V_inputDimensions
initWithAsset:readVideoTrack:readAudioTrack:
assetReaderWithAsset:error:
_createOutputsForAsset:readVideo:readAudio:
tracksWithMediaType:
assetReaderTrackOutputWithTrack:outputSettings:
setAlwaysCopiesSampleData:
canAddOutput:
addOutput:
cancelReading
copyNextSampleBuffer
status
copyNextSampleBufferWithTrackIndexOutput:
startReading
_copyNextSampleBufferFromTrackOutput:
tracks
timeRange
string
base64EncodedDataWithOptions:
initWithAsset:
checkAndSaveCrashReportWithData:
_asset
_assetReader
_trackSamples
_trackOutputs
initWithCachePolicy:
setName:
setup
homes
hmf_firstObjectWithUUID:
personManager
residentDevices
isCurrentDevice
na_any:
uuid
photosPersonManagerWithUUID:
accessories
cameraProfiles
isSetup
defaultPrivateConfiguration
setOptions:
cachePolicy
setCachePolicy:
setDiscretionary:
homeKitOperationQueue
setDelegateQueue:
setDidUpdateHomes:
initWithHomeMangerConfiguration:
setHomeManager:
homeManager
dateWithTimeIntervalSinceNow:
_refreshBeforeDate:completionHandler:
isPrimary
initWithNoCaching
homePersonManagerForHomeUUID:
homeForHMPersonManagerUUID:
homePersonManagersForCurrentDevice
photosPersonManagerForHomeUUID:sourceUUID:
isCurrentDevicePrimaryResident
cameraProfileWithUUID:
homeWithCameraProfileUUID:
_setup
_homes
_homeKitOperationQueue
_cachePolicy
_homeManager
T@"NSOperationQueue",R,V_homeKitOperationQueue
TB,R,GisSetup,V_setup
TQ,R,V_cachePolicy
T@"HMHomeManager",&,V_homeManager
T@"NSArray",R,V_homes
didUpdateHomes
homeManager:didUpdateAuthorizationStatus:
homeManagerDidUpdateHomes:
homeManagerDidUpdatePrimaryHome:
homeManager:didAddHome:
homeManager:didRemoveHome:
homeManager:didReceiveAddAccessoryRequest:
_didUpdateHomes
T@?,C,V_didUpdateHomes
initWithName:deferred:
begin
signpostLog
beginDate
hasBegun
shouldSignpost
signpostIdentifier
identifier
_name
_beginDate
_signpostIdentifier
_identifier
T@"NSDate",R,V_beginDate
TQ,R,V_signpostIdentifier
T@"NSUUID",R,C,V_identifier
T@"NSString",R,C,V_name
getUUIDBytes:
dataWithBytesNoCopy:length:freeWhenDone:
getBytes:range:
inputName
setWithArray:
input
featureValueWithMultiArray:
featureNames
T@"NSSet",R,N
initWithInput:inputName:
setInput:
_input
_inputName
T@"MLMultiArray",&,N,V_input
T@"NSString",R,V_inputName
modelWithContentsOfURL:error:
doubleValue
setObject:atIndexedSubscript:
dataScalerInputName
scalerModel
dataScalerOutputName
svmInputName
mlModel
svmOutputName
dictionaryValue
modelPathForResource:
defaultRecognizabilityModelPath
defaultRecognizabilityDataScalerPath
defaultAestheticQualityModelPath
defaultAestheticQualityDataScalerPath
initWithModelPath:dataScalerPath:error:
predict:output:error:
_mlModel
_scalerModel
T@"MLModel",R,V_mlModel
T@"MLModel",R,V_scalerModel
_status
_error
Tq,R
T@"NSError",R
handleVideoSampleBuffer:
handleAudioSampleBuffer:
exceptionWithName:reason:userInfo:
handleSampleBuffer:
finishWithCompletionHandler:
flushAsync
initWithConfiguration:
configuration
dynamicConfiguration
setDynamicConfiguration:
_configuration
_dynamicConfiguration
T@"HMIVideoAnalyzerConfiguration",R,V_configuration
T@"HMIVideoAnalyzerDynamicConfiguration",&,V_dynamicConfiguration
task
setTask:
_task
T@"HMIDESBackgroundTask",&,V_task
photosPersonManager
personFromHomePerson:
source
initWithUUID:dataRepresentation:dateCreated:faceBoundingBox:personUUID:source:
modelUUID
faceCropUUID
initWithUUID:data:modelUUID:faceCropUUID:
addOrUpdateFaceprints:completion:
initWithHMPhotosPersonManager:
setPhotosPersonManager:
_photosPersonManager
T@"HMPhotosPersonManager",&,V_photosPersonManager
mainBundle
localizedStringForKey:value:table:
errorWithDomain:code:userInfo:
createNSErrorWithStatus:andMessage:
createNSExceptionWithStatus:andMessage:
initWithUUID:dataRepresentation:dateCreated:faceBoundingBox:
encodeInteger:forKey:
containsValueForKey:
decodeIntegerForKey:
copyWithZone:
TB,R
initWithUUID:dataRepresentation:dateCreated:faceBoundingBox:personUUID:
_personUUID
_source
T@"NSUUID",R,C,V_personUUID
Tq,R,V_source
redactedCopy
na_filter:
eventClasses
maxConfidenceEventForEventClass:
sortedArrayUsingComparator:
mutableCopy
setByAddingObjectsFromSet:
replaceObjectAtIndex:withObject:
removeObjectAtIndex:
decodeObjectOfClasses:forKey:
decodeRectForKey:
encodeRect:forKey:
combineFrameResults:withResults:
initWithFrame:events:
maxConfidenceEvents
_frame
_events
_regionOfInterest
T{CGRect={CGPoint=dd}{CGSize=dd}},R,V_regionOfInterest
T@"HMIVideoFrame",R,V_frame
T@"NSSet",R,V_events
initWithBoundingBox:timeStamp:blobArea:blobID:
timeStamp
blobArea
blobID
_blobID
_blobArea
_timeStamp
T{?=qiIq},R,V_timeStamp
Tf,R,V_blobArea
TS,R,V_blobID
blobs
stationaryIndexToBoundingBox:
initWithValue:levelThresholds:
shortNameForEventClass:
initWithBlob:
appendBlob:
isExpiredAtTimeStamp:
isStationaryAtTimeStamp:
similarityToBoundingBox:
lastBlob
createEventForEventClass:
_blobs
_eventClasses
T@"NSMutableArray",R,V_blobs
T@"NSMutableSet",R,V_eventClasses
T@"HMIForegroundBlob",R
T@"NSString",R
reset
_invalidateBackgroundForPixelBuffer:timeStamp:
setBackgroundChangeTimeStamp:
_ensureInternalBuffersForPixelBuffer:
numImages
minSampleSize
_predictForegroundFromPixelBuffer:timeStamp:
_updateBackgroundFromPixelBuffer:timeStamp:
foregroundTimeStamp
backgroundEvents
assignment
backgroundTimeStamp
_setAssignment:greaterThanType:value:boundingBox:scale:
_intersectionOverUnionFromBlob:boundingBox:assignment:
_stationaryTracks:timeStamp:
foregroundEvents
unionSet:
containsObject:
firstMotionDetectionInArray:withMode:
motionTimeStamps
valueWithBytes:objCType:
_expireMotionDetectionsAtTimeStamp:
backgroundExpireInterval
imageSize
backgroundChangeTimeStamp
backgroundChangeResetInterval
camera
videoPackageAnalyzerDidResetReferenceImageWithInterval:camera:
setBackgroundTimeStamp:
modelSize
_copyFromPixelBuffer:toInputBuffer:translateCol:translateRow:
runningMean
runningStd
_updateRunningMean:runningSquaredMean:fromInputBuffer:decay:
_updateRunningStd:withAuxBuffer:runningMean:runningSquaredMean:
setForegroundTimeStamp:
_foregroundPixelsFromPixelBuffer:attribute:assignment:
_blobsFromAssignment:timeStamp:
_foregroundBlobsFromBlobs:backgroundChanged:
adjustBrightness
backgroundChangeInterval
setAdjustBrightness:
rectValue
_correctRunningMeanBrightnessAtAttribute:
_updateCurrentTracks:blobs:timeStamp:
CMTimeValue
motionValidInterval
indexSetWithIndexesInRange:
containsIndex:
removeIndex:
removeObject:
enumerateIndexesUsingBlock:
setByAddingObjectsFromArray:
_exportBackgroundMean
_exportBackgroundStd
_exportForegroundDiffForPixelBuffer:
_exportForegroundAssignment
na_flatMap:
_exportInternalStateForPixelBuffer:exportMode:
_foregroundDifferencesFromPixelBuffer:differences:
_copyFromOutputBuffer:toPixelBuffer:
resizePixelBuffer:
analyzePixelBuffer:timeStamp:
hasNewBackground
assignBackgroundFromEvents:
assignForegroundFromEvents:regionOfInterest:pixelBuffer:timeStamp:
packageEvents
handleMotionDetection:inFrame:
exportInternalStateToReport:pixelBuffer:events:regionOfInterest:timeStamp:
setAssignment:
setRunningMean:
setRunningStd:
setNumImages:
setImageSize:
setModelSize:
_adjustBrightness
_tracks
_foregroundEvents
_backgroundEvents
_minSampleSize
_assignment
_runningMean
_runningStd
_numImages
_motionTimeStamps
_imageSize
_modelSize
_backgroundExpireInterval
_backgroundChangeInterval
_backgroundChangeResetInterval
_foregroundTimeStamp
_backgroundTimeStamp
_backgroundChangeTimeStamp
_motionValidInterval
T@"NSMutableSet",R,V_tracks
T@"NSMutableSet",R,V_foregroundEvents
T@"NSMutableArray",R,V_backgroundEvents
TQ,R,V_minSampleSize
T{?=qiIq},R,V_backgroundExpireInterval
T{?=qiIq},R,V_backgroundChangeInterval
T{?=qiIq},R,V_backgroundChangeResetInterval
T^S,V_assignment
T^f,V_runningMean
T^f,V_runningStd
TQ,V_numImages
T{CGSize=dd},V_imageSize
T{CGSize=dd},V_modelSize
T{?=qiIq},V_foregroundTimeStamp
T{?=qiIq},V_backgroundTimeStamp
T{?=qiIq},V_backgroundChangeTimeStamp
TB,V_adjustBrightness
T@"NSMutableArray",R,V_motionDetections
T@"NSMutableArray",R,V_motionTimeStamps
T{?=qiIq},R,V_motionValidInterval
T@"NSSet",R
URLWithString:
initWithURL:options:
resourceLoader
setDelegate:queue:
contentInformationRequest
setContentType:
setContentLength:
setByteRangeAccessSupported:
dataRequest
requestedOffset
requestedLength
respondWithData:
finishLoading
loadValuesAsynchronouslyForKeys:completionHandler:
resourceLoader:shouldWaitForLoadingOfRequestedResource:
resourceLoader:shouldWaitForRenewalOfRequestedResource:
resourceLoader:didCancelLoadingRequest:
resourceLoader:shouldWaitForResponseToAuthenticationChallenge:
resourceLoader:didCancelAuthenticationChallenge:
loadValuesSynchronously
initWithValue:
T@"NSNumber",R,V_value
windowSize
queue
setMovingAverage:
setQueue:
_movingAverage
_queue
_windowSize
T@"NSMutableArray",&,N,V_queue
TQ,R,N,V_windowSize
Td,V_movingAverage
_eventsFromAnalyzerEvents:
_annotationScoresFromAnalyzerEvents:
_detectionsFromAnalyzerEvents:
_faceClassificationsFromAnalyzerEvents:
numberWithInteger:
analyzerEvents
initWithFrame:events:annotationScores:detections:regionOfInterest:faceClassifications:
initWithFrame:regionOfInterest:analyzerEvents:
annotationScores
detections
faceClassifications
_annotationScores
_detections
_faceClassifications
_analyzerEvents
T@"HMICameraVideoFrame",R,V_frame
T@"NSDictionary",R,V_annotationScores
Tq,R,V_events
T@"NSArray",R,V_detections
T@"NSSet",R,V_faceClassifications
T@"NSSet",R,V_analyzerEvents
initWithBoundingBox:timeStamp:
Td,R,V_timeStamp
initWithEventClass:records:UUID:
eventClass
records
_eventClass
_records
_UUID
T#,R,V_eventClass
T@"NSArray",R,V_records
T@"NSUUID",R,V_UUID
eventClassForShortName:
eventForClass:boundingBox:UUID:
initWithPersonUUID:sourceUUID:sessionEntityUUID:confidence:familiarity:
initWithFaceCrop:faceprint:classifications:predictedLinkedEntityUUIDs:faceQualityScore:sessionEntityAssignment:sessionEntityUUID:
initWithConfidence:boundingBox:face:
initWithArray:
lastKnownTimeStamp
eventsForTimeStamp:
eventsForFragment
T@"NSArray",R,V_tracks
initWithLength:
fillWithFloat:
appendFloats:count:
unsignedIntegerValue
initWithFloats:count:
mutableFloats
appendData:
floats
subtract:
initWithDataTensor:
appendArray:
l2Norm
floatArrayByAdding:
floatArrayBySubtracting:
mutableCopyWithZone:
T@"NSData",R,&,N
TQ,R,N
Tr^f,R,N
T^f,R,N
numberPreferenceForKey:defaultValue:
initWithShape:dataType:error:
createFacePixelBufferForFaceEvent:pixelBuffer:roll:error:
computeJunkScoreForPixelBuffer:
qualityPredictionFromSVMUsingFaceQualityFilterSVM:detectorConfidence:laplacian:yaw:boxSize:error:
faceprinter
createFaceprintForFacePixelBuffer:fastMode:error:
predictPersonFromFaceObservation:homeUUID:error:
faceAttributes
facemaskCategory
label
classificationThresholdKnown
linkedEntityUUID
classificationThresholdUnknown
initWithUUID:sourceUUID:sessionEntityUUID:faceCrop:faceprint:confidence:familiarity:
classifyFaceEvent:pixelBuffer:fastMode:homeUUID:error:
initWithError:
faceRecognizabilityFilter
faceAestheticQualityFilter
_faceprinter
_faceRecognizabilityFilter
_faceAestheticQualityFilter
_classificationThresholdKnown
_classificationThresholdUnknown
T@"HMIFaceprinter",R,V_faceprinter
T@"HMIFaceQualityFilterSVM",R,V_faceRecognizabilityFilter
T@"HMIFaceQualityFilterSVM",R,V_faceAestheticQualityFilter
Td,R,V_classificationThresholdKnown
Td,R,V_classificationThresholdUnknown
initWithDictionary:regionOfInterest:
_confidence
_label
T@"NSNumber",R,V_confidence
T@"NSNumber",R,V_label
imageData
initWithDictionary:
probability
applyToImage:withProbability:
initWithImageData:regionOfInterest:detections:
createRegionOfInterestPixelBufferWithError:
arrayWithArray:
initWithData:type:shape:strides:
initWithBundleIdentifier:
isPermitted
saveRecordWithData:recordInfo:completion:
saveDESRecordWithVideoFrame:recordInfo:
augmentWithOptions:
createImageTensorWithError:
createBoxesTensorWithError:
createClassesTensorWithError:
createWeightsTensorWithError:
_boxesTensorData
_weightsTensorData
_classesTensorData
_imageData
T@"NSData",R,V_imageData
samples
imageName
boxesName
weightsName
classesName
initWithSamples:imageName:boxesName:weightsName:classesName:
dataPointAtIndex:error:
numberOfDataPoints
_samples
_imageName
_boxesName
_weightsName
_classesName
T@"NSArray",R,V_samples
T@"NSString",R,V_imageName
T@"NSString",R,V_boxesName
T@"NSString",R,V_weightsName
T@"NSString",R,V_classesName
initWithCVPixelBuffer:imageParameters:error:
setMaxNumberOfElements:
featureValueWithPixelBuffer:
thresholdWithLabels
thresholdDefault
metricWithLabels
metricDefault
thresholdForLabel:
metricForLabel:
_thresholdWithLabels
_metricWithLabels
_thresholdDefault
_metricDefault
T@"NSDictionary",R,V_thresholdWithLabels
T@"NSDictionary",R,V_metricWithLabels
T@"NSNumber",R,V_thresholdDefault
T@"NSNumber",R,V_metricDefault
_labelIndex
Ti,R,V_labelIndex
Td,R,V_confidence
T{CGRect={CGPoint=dd}{CGSize=dd}},R,N,V_boundingBox
nonMaximumSuppression:output:withThreshold:withMetric:
intersectionOverUnion:b:
intersectionOverMinArea:b:
convertObjectDetections:cropRect:originalImageSize:
indexSet
torsoRecognition
setBlobID:
faceDistanceFromDescriptor:toDescriptor:
initWithNewPersonEvent:timeStamp:
trackPersonBlob:
similarityToPersonBlob:
personIndices
setPersonIndices:
_torsoprint
_personIndices
T@"HMITorsoprint",R,V_torsoprint
T@"NSMutableIndexSet",&,V_personIndices
T@"NSUUID",&,V_blobID
previousPersons
removeAllIndexes
addIndex:
numberWithUnsignedInteger:
removeIndexes:
trackNewPersons:regionOfInterest:timeStamp:
setBlobID:atIndex:
getBlobIDAtIndex:
_previousPersons
T@"NSMutableArray",R,V_previousPersons
initWithFrame:events:regionOfInterest:motionDetections:
T@"NSArray",R,V_motionDetections
eventConfidenceThresholdsHigh
initWithMediumConfidenceThresholds:highConfidenceThresholds:analyzerConfiguration:error:
cameraVideoFrameAnalyzer
initWithInterval:
frameSampler
eventTriggers
recognizeFaces
analyze:targetEventTypes:enableFaceClassification:homeUUID:error:
initWithSampleBuffer:
redactFrames
frameAnalyzer:didAnalyzeFrame:error:
handleSampleBuffer:motionDetections:motionScore:
flushAndGetAnalysisStateUpdateForHome:enableFaceClassification:
frameAnalyzer:didProduceAnalysisStateUpdate:
frameSampler:didSampleFrame:
frameSampler:didDropFrame:
averageAnalysisTime
setFrameSampler:
_analysisTime
_delegate
_cameraVideoFrameAnalyzer
_frameSampler
T@"<HMICameraVideoFrameAnalyzer>",R,V_cameraVideoFrameAnalyzer
T@"HMIVideoFrameSampler",&,V_frameSampler
T@"<HMIVideoFrameAnalyzerDelegate>",W,V_delegate
Td,R
frameAnalyzerDidAnalyzeFrame
frameAnalyzerDidProduceAnalysisStateUpdate
setFrameAnalyzerDidAnalyzeFrame:
setFrameAnalyzerDidProduceAnalysisStateUpdate:
_frameAnalyzerDidAnalyzeFrame
_frameAnalyzerDidProduceAnalysisStateUpdate
T@?,C,V_frameAnalyzerDidAnalyzeFrame
T@?,C,V_frameAnalyzerDidProduceAnalysisStateUpdate
initWithTruth:prediction:score:
truth
prediction
_truth
_prediction
T@"NSNumber",R,V_truth
T@"NSNumber",R,V_prediction
valueForKeyPath:
initWithKey:classificationScore:
initWithKey:detectionScores:frameResultIndex:
initWithKey:trackingScores:frameResultIndices:
frameResultIndices
_key
_count
_frameResultIndices
T@"NSString",R,V_key
TQ,R,V_count
T@"NSArray",R,V_frameResultIndices
precision
recall
truePositive
falseNegative
falsePositive
initWithTruePositiveKeys:falseNegativeKeys:falsePositiveKeys:groupByKey:
truePositiveKeys
falseNegativeKeys
falsePositiveKeys
_precision
_recall
_truePositiveKeys
_falseNegativeKeys
_falsePositiveKeys
_truePositive
_falseNegative
_falsePositive
Tf,R,V_precision
Tf,R,V_recall
T@"NSArray",R,V_truePositiveKeys
T@"NSArray",R,V_falseNegativeKeys
T@"NSArray",R,V_falsePositiveKeys
Tq,R,V_truePositive
Tq,R,V_falseNegative
Tq,R,V_falsePositive
fragments
redactedCopyWithFrameResults:fragment:
initWithSource:
appendFragmentResult:redactFrames:
setSource:
_fragments
T@"NSString",C,V_source
T@"NSMutableArray",R,V_fragments
systemDeviceInformation
unarchivedObjectOfClass:fromData:error:
sessions
addEntriesFromDictionary:
allEvents
eventClassesArray
compareWithClassificationTruth:eventClass:confidenceThreshold:
averagePrecisionForMinPrecision:comparator:
compareWithDetectionTruth:eventClass:confidenceThreshold:iouThreshold:videoMetric:
chartDataWithBaseline:comparator:
archivedDataWithRootObject:requiringSecureCoding:error:
initWithContentsOfFile:
bundleWithIdentifier:
infoDictionary
numberWithBool:
fragment
outcome
initWithFragment:events:frameResults:thumbnails:configuration:outcome:
na_dictionaryByMappingValues:
initWithJPEGData:size:presentationTimeStamp:
greedyMatchBetweenPredictionEvents:truthEvents:falsePositiveIndices:falseNegativeIndices:eventClass:regionOfInterest:confidenceThreshold:scoreThreshold:scoreFunction:
enumerateKeysAndObjectsUsingBlock:
sihouetteScoreForMatches:previousMatches:truePositiveScores:falsePositiveScores:falseNegativeScores:
setArray:
exchangeObjectAtIndex:withObjectAtIndex:
subarrayWithRange:
stringByAppendingPathComponent:
selectFramesWithRecord:truth:frameResults:
stringByDeletingPathExtension
writeImageCropFromFrame:events:outputPath:source:
pixelBufferFrameWithError:
writeToFile:atomically:encoding:error:
allKeys
localizedStandardCompare:
sortedArrayUsingSelector:
null
dataWithJSONObject:options:error:
initWithCVPixelBuffer:
fileURLWithPath:relativeToURL:
colorSpace
writePNGRepresentationOfImage:toURL:format:colorSpace:options:error:
version
deviceInformation
T@"NSDictionary",R
initWithData:error:
appendFragmentResult:forKey:source:redactFrames:
appendFragmentResultsFromReport:
averagePrecisionWithClassificationTruth:minPrecision:
averagePrecisionWithDetectionTruth:minPrecision:iouThreshold:videoMetric:
chartDataWithClassificationTruth:isBaseline:
chartDataWithDetectionTruth:isBaseline:iouThreshold:videoMetric:
truthReportFromLegacyClassificationFormat:
truthReportFromLegacyDetectionFormat:
compareWithTrackingTruth:eventClass:confidenceThreshold:ioaThreshold:
writeHTMLReportComparison:truth:eventClass:comparisonType:assetPath:outputPath:limit:shuffle:
writeImageCropForEventClass:outputPath:assetPath:
writeFragmentFileComparison:eventClass:outputPath:
chartSpecWithRange:colors:labels:
_version
_deviceInformation
_sessions
T@"NSString",R,V_name
Tq,R,V_version
T@"NSDictionary",R,V_deviceInformation
T@"NSMutableDictionary",R,V_sessions
T@"NSData",R
detectFacesInImageData:error:
unalignedBoundingBox
faceBoxFromPhotosFaceCropImageData:
newDictionaryPopulatedWithFaceCropDataFromImageData:
JSONObjectWithData:options:error:
isEqualToData:
isEqualToDate:
faceCropFromPhotosFaceCropImageData:
_dataRepresentation
_dateCreated
_faceBoundingBox
T@"NSUUID",R,C,V_UUID
T@"NSData",R,C,V_dataRepresentation
T@"NSDate",R,C,V_dateCreated
T{CGRect={CGPoint=dd}{CGSize=dd}},R,V_faceBoundingBox
retimer:didRetimeSampleBuffer:
flushWithNextSamplePTS:
_lastSample
T@"<HMIVideoRetimerDelegate>",W,V_delegate
retimerDidRetimeSampleBuffer
setRetimerDidRetimeSampleBuffer:
_retimerDidRetimeSampleBuffer
T@?,C,V_retimerDidRetimeSampleBuffer
initWithIdentifier:name:manufacturer:model:firmwareVersion:
manufacturer
model
firmwareVersion
initWithIdentifier:name:manufacturer:model:
_manufacturer
_model
_firmwareVersion
T@"NSUUID",R,V_identifier
T@"NSString",R,V_manufacturer
T@"NSString",R,V_model
T@"NSString",R,V_firmwareVersion
initWithTaskID:timeout:
code
initWithTaskID:
taskID
_taskID
Ti,R,V_taskID
initWithTaskID:homeUUID:timeout:
_homeUUID
T@"NSUUID",R,V_homeUUID
initPrivate
nextTaskID
setNextTaskID:
buildUpdatePersonsModelTaskFromOptions:error:
buildRemovePersonsModelTaskFromOptions:error:
buildHomePersonClusteringTaskFromOptions:error:
buildTuriTrialUpdateTaskFromOptions:error:
getNextTaskID
buildFaceMisclassificationTaskFromOptions:error:
buildPersonsModelsSummaryTaskFromOptions:error:
buildSubmitFeedbackTaskFromOptions:error:
buildUpdateTorsoModelTaskFromOptions:error:
submitTask:completionHander:
operations
cancel
boolValue
stringPreferenceForKey:defaultValue:
initWithHMHomePersonManager:
initWithHomeUUID:sourceUUID:error:
initWithTaskID:homeUUID:sourceUUID:dataSource:externalLibrary:removeExcessFaceCrops:
initWithTaskID:homeUUID:sourceUUID:
initWithTaskID:dataSource:faceCrop:
initWithTaskID:homeUUID:dataSource:sourceUUID:personsModelManager:doImpurePersonCleanup:error:
initWithTaskID:homeUUID:
initWithTaskID:cameraProfileUUID:clipUUID:
initWithTaskID:homeUUID:torsoAnnotations:
cancelTask:
_nextTaskID
Ti,V_nextTaskID
taskService
allowedClasses
privateDescription
propertyDescription
T@"NSArray",R,C,N
initWithDataSource:
persons
_persons
T@"NSSet",R,V_persons
classification
sessionEntityAssignment
initWithTorsoprint:classification:predictedLinkedEntityUUIDs:sessionEntityAssignment:sessionEntityUUID:
predictedLinkedEntityUUIDs
_classification
_predictedLinkedEntityUUIDs
_sessionEntityAssignment
_sessionEntityUUID
T@"HMITorsoClassification",R,V_classification
T@"NSSet",R,V_predictedLinkedEntityUUIDs
Tq,R,V_sessionEntityAssignment
T@"NSUUID",R,V_sessionEntityUUID
existingAtCurrentVersion
isSentinelFaceprint
createdAtCurrentVersion
initWithExistingAtCurrentVersion:createdAtCurrentVersion:existingAtOtherVersions:
allAtCurrentVersion
existingAtOtherVersions
_existingAtOtherVersions
_createdAtCurrentVersion
_existingAtCurrentVersion
T@"NSSet",R,V_existingAtOtherVersions
T@"NSSet",R,V_createdAtCurrentVersion
T@"NSSet",R,V_existingAtCurrentVersion
faceprintDefaultRevision
_minorVersionFromVisionVersion:
faceObservationWithRequestRevision:unalignedBoundingBox:alignedBoundingBox:
setInputFaceObservations:
setDetectionLevel:
setRevision:
setFaceprint:
createFacePixelBufferForFaceDetection:pixelBuffer:roll:error:
createFacePixelBufferFromFaceCrop:error:
generateFaceprintForFaceCrop:error:
sentinelFaceprintWithUUID:modelUUID:faceCropUUID:
warmStart
updatedFaceprintsForFaceCrops:withExistingFaceprints:error:
shouldUseCPUOnlyForVisionFaceDetection
setUsesCPUOnly:
defaultRevision
initWithCVPixelBuffer:options:
initWithData:options:session:
detectFacesInPixelBuffer:error:
_modelUUID
_faceCropUUID
sentinelFaceprint
TB,R,GisSentinelFaceprint
T@"NSData",R,C,V_data
T@"NSUUID",R,C,V_modelUUID
T@"NSUUID",R,C,V_faceCropUUID
initWithPoint:
_point
T{CGPoint=dd},R,V_point
allocWithZone:
setFaceClassificationEnabled:
encodeBool:forKey:
decodeBoolForKey:
_faceClassificationEnabled
faceClassificationEnabled
TB,GisFaceClassificationEnabled,V_faceClassificationEnabled
TB,D,GisFaceClassificationEnabled
fileExistsAtPath:
fileHandleForReadingAtPath:
assetDirectoryPath
unpackageModelAssets:intoDirectory:error:
removeItemAtPath:error:
unTarFileWithFd:toPath:
unpackageModelAssetsAtPath:error:
pendingUpdates
setPendingUpdates:
_pendingUpdates
T@"NSObject<OS_dispatch_queue>",&,N,V_queue
T@"NSMutableSet",&,N,V_pendingUpdates
torsoAnnotations
updateTorsoModelForHome:torsoAnnotations:error:
_torsoAnnotations
T@"NSSet",R,V_torsoAnnotations
defaultSessionConfiguration
sessionWithConfiguration:delegate:delegateQueue:
URLSession:didBecomeInvalidWithError:
URLSession:didReceiveChallenge:completionHandler:
URLSessionDidFinishEventsForBackgroundURLSession:
homeKitClient
session
feedbackServiceHost
_homeKitClient
_session
_feedbackServiceHost
T@"NSURLSession",R,V_session
T@"NSString",R,V_feedbackServiceHost
T@"HMIHomeKitClient",R,V_homeKitClient
protectionSpace
host
serverTrust
credentialForTrust:
authenticationMethod
feedbackSession
_temporaryFileURLWithUUID:extension:error:
clipManager
initWithClipManager:clip:
setClipDestinationFileURL:
hmiPrivateErrorWithCode:description:suggestion:underlyingError:
setFetchVideoAssetContextCompletionBlock:
setDownloadProgressHandler:
significantEvents
faceClassification
person
setFaceCrops:
fetchClipWithUUID:completion:
blurFacesFromAssetURL:outputURL:duration:analysisFPS:windowSize:faceDetected:
assetWithURL:
isReadable
statusOfValueForKey:error:
composition
addMutableTrackWithMediaType:preferredTrackID:
duration
insertTimeRange:ofTrack:atTime:error:
initWithAsset:presetName:
setOutputFileType:
setOutputURL:
setShouldOptimizeForNetworkUse:
setTimeRange:
exportAsynchronouslyWithCompletionHandler:
feedbackServiceURL
requestWithURL:
setHTTPMethod:
setValue:forHTTPHeaderField:
uploadTaskWithRequest:fromFile:completionHandler:
_base64StringFromData:
faceCrops
_attachEncryptedDataUsingKey:toPayload:error:
setAssetData:
serviceResult
_createPayloadWithServiceResult:error:
_uploadPayloadData:uploadURL:completionHandler:
_stripAudioTrackAndFacesFromAsset:completionHandler:
setServiceResult:
_downloadClipWithCameraProfileUUID:clipUUID:completionHandler:
_requestPreSignedURLWithClipUUID:completionHandler:
feedbackRequestURLForClipWithUUID:
dataTaskWithURL:completionHandler:
removeItemAtURL:error:
_removeTemporaryFiles
_submitClipWithCameraProfileUUID:clipUUID:completionHandler:
initWithFeedbackSession:cameraProfileUUID:clipUUID:
_attachFaceCrops:toPayload:error:
cameraProfileUUID
clipUUID
temporaryFileURLs
assetData
_feedbackSession
_cameraProfileUUID
_clipUUID
_temporaryFileURLs
_faceCrops
_assetData
_serviceResult
T@"HMIFeedbackSession",R,V_feedbackSession
T@"NSUUID",R,V_cameraProfileUUID
T@"NSUUID",R,V_clipUUID
T@"NSMutableArray",R,V_temporaryFileURLs
T@"NSSet",&,V_faceCrops
T@"NSData",&,V_assetData
T@"NSDictionary",&,V_serviceResult
setQualityOfService:
_operation
submitFeedbackWithCameraProfileUUID:clipUUID:runRemotely:completionHandler:
submitFeedbackWithCameraProfileUUID:clipUUID:completionHandler:
initWithUUID:sourceUUID:faceBoundingBox:
initWithUUID:sourceUUID:sessionEntityUUID:faceBoundingBox:facecrop:faceprint:confidence:
initWithUUID:sourceUUID:sessionEntityUUID:faceCrop:faceprint:confidence:fromTorsoClassification:familiarity:
initWithUUID:name:personsModelIdentifier:faceBoundingBox:
personsModelIdentifier
_fromTorsoClassification
_personsModelIdentifier
_sourceUUID
_familiarity
TB,R,V_fromTorsoClassification
T@"NSString",R,V_identifier
T@"NSString",R,V_personsModelIdentifier
T@"NSUUID",R,V_personUUID
T@"NSUUID",R,V_sourceUUID
Tq,R,V_familiarity
personToFaceCrops
allValues
unassociatedFaceCrops
existingPersonUUIDs
isSubsetOfSet:
intersectsSet:
existingPersonFaceCropUUIDs
removeObjectForKey:
existingFaceCropUUIDs
setWithSet:
hmf_isEqualToUUID:
fetchAllUnassociatedFaceCropsWithCompletion:
addPersons:completion:
removePersonsWithUUIDs:completion:
associateFaceCropsWithUUIDs:toPersonWithUUID:forSource:completion:
removedPersonFaceCrops
_personToFaceCrops
_unassociatedFaceCrops
_removedPersonFaceCrops
T@"NSMutableDictionary",R,V_personToFaceCrops
T@"NSMutableSet",R,V_unassociatedFaceCrops
T@"NSSet",R,V_removedPersonFaceCrops
summaryForHomeUUID:error:
sendEventForPersonsModels:
removeNearestFaceprint:withFaceCrops:
T@"HMIPersonFaceCrop",R,V_faceCrop
removePersonsModelForHomeUUID:sourceUUID:error:
setDecoderDidDecodeSampleBuffer:
reader
handleSampleBuffer:outputFrame:
_reader
T@"HMIVideoAssetReader",R,V_reader
hmf_zeroUUID
initWithUUID:
supportsFaceClassification
setSupportsFaceClassification:
setPersonDataAvailableViaHomeKit:
_supportsFaceClassification
_personDataAvailableViaHomeKit
T@"NSUUID",R,C,V_homeUUID
T@"<HMIPersonManagerDataSource>",W,N,V_dataSource
TB,V_supportsFaceClassification
personDataAvailableViaHomeKit
TB,GisPersonDataAvailableViaHomeKit,V_personDataAvailableViaHomeKit
levelThresholds
level
characterAtIndex:
_levelThresholds
T@"NSArray",R,V_levelThresholds
Td,R,V_value
_hmiErrorWithCode:description:reason:suggestion:underlyingError:
hmiErrorWithCode:underlyingError:
hmiErrorWithCode:
hmiErrorWithCode:description:
hmiErrorWithCode:description:underlyingError:
initWithData:timeRange:
initWithInitializationSegment:separableSegment:timeRange:
isCombinableWithFragment:
separableSegment
sequenceNumbers
initializationSegment
initWithInitializationSegment:separableSegment:
firstVideoSampleByteRange
initWithInitializationSegment:separableSegment:timeRange:sequenceNumbers:firstVideoSampleByteRange:
redactedCopyWithMetadata
initWithInitializationSegment:separableSegment:timeRange:firstVideoSampleByteRange:
numberWithUnsignedInt:
_ensureAttributes
videoFormatDescription
audioFormatDescription
canFragmentData:
dataWithData:
videoTrackTimeRange
decodeCMTimeRangeForKey:
encodeCMTimeRange:forKey:
isInitializationSegment:combinableWithInitializationSegment:
fragmentData:handler:
initWithFragments:
placeholderCopy
initWithInitializationSegment:separableSegment:sequenceNumbers:
initWithInitializationSegment:separableSegment:timeRange:sequenceNumbers:
audioTrackTimeRange
baseDecodeTimeStamp
frameReorderingRequired
sanitizedData
sanitizedSeperableSegment
sequenceNumber
_attributesLoaded
_frameReorderingRequired
_videoFormatDescription
_audioFormatDescription
_initializationSegment
_separableSegment
_sequenceNumbers
_firstVideoSampleByteRange
_baseDecodeTimeStamp
_videoTrackTimeRange
_audioTrackTimeRange
_timeRange
T@"NSData",R,C
Tr^{opaqueCMFormatDescription=},R,V_videoFormatDescription
Tr^{opaqueCMFormatDescription=},R,V_audioFormatDescription
T{?={?=qiIq}{?=qiIq}},R,V_videoTrackTimeRange
T{?={?=qiIq}{?=qiIq}},R,V_audioTrackTimeRange
TB,R,V_frameReorderingRequired
T{?=qiIq},R,V_baseDecodeTimeStamp
T{_NSRange=QQ},R,V_firstVideoSampleByteRange
T@"NSData",R,V_initializationSegment
T@"NSData",R,V_separableSegment
T{?={?=qiIq}{?=qiIq}},R,V_timeRange
T@"NSArray",R,V_sequenceNumbers
sessionEntities
assignSessionEntityToFaceRecognition:torsoRecognition:predictedLinkedEntityUUIDs:availableSessionEntityUUIDs:sessionEntityAssignment:
clusterSessionEntityToFaceRecognition:torsoRecognition:predictedLinkedEntityUUIDs:availableSessionEntityUUIDs:sessionEntityAssignment:
createSessionEntityWithUUID:faceRecognition:torsoRecognition:predictedLinkedEntityUUIDs:sessionEntityAssignment:
updatePreviousPrintsForSessionEntityUUID:faceRecognition:torsoRecognition:
faceVIPThresholdForTorsoAnnotation
updatePersonEventWithPersonEvent:torsoAnnotation:sessionEntityUUID:predictedLinkedEntityUUIDs:sessionEntityAssignment:
initWithFaceRecognition:torsoprints:
submitTorsoprintsToModelManagerForHome:withTorsoAnnotations:
faceQualityScore
initWithPersonUUID:sourceUUID:confidence:
initWithConfidence:boundingBox:roll:torsoRecognition:
initWithConfidence:boundingBox:face:torso:
assignSessionEntitiesToPersonEvents:regionOfInterest:timeStamp:homeUUID:
updateTorsoModelAndGetTorsoAnnotationsForHome:
_sessionUUIDToPreviousFaceprints
_sessionUUIDToPreviousTorsoprints
_personTracker
_sessionEntities
_faceVIPThresholdForTorsoAnnotation
T@"NSMutableDictionary",R,V_sessionEntities
Td,R,V_faceVIPThresholdForTorsoAnnotation
personUUIDs
initWithDataSource:personUUIDs:
_personUUIDs
T@"NSSet",R,V_personUUIDs
initWithDataSource:faceprintUUIDs:
T@"NSSet",R,V_faceprintUUIDs
configure
loadModelFromTrialWithError:
initWithTorsoAnnotations:
initWithTorsoAnnotationsArray:
stateUpdateByMergingStateUpdate:
stateManager:didReceiveLocalUpdate:
handleRemoteStateUpdate:completionHandler:
shouldEnableTorsoRecognition
torsoModelVersion
initWithHomeUUID:
publishLocalState:
handleRemoteStateUpdate:
stateUpdateFromFaceEvents:
T@"<HMIAnalysisStateManagerDelegate>",W,V_delegate
_createFontWithSize:
initWithString:attributes:
drawText:at:color:
drawRect:width:color:
initWithPixelBuffer:fontSize:
draw:
drawTextHeaderBar:
drawBoundingBox:lineWidth:text:color:
drawPolygonWithNormalizedPoints:
_context
_colorSpace
_font
resourceUsageMonitor
_resourceUsageMonitor
T@"<HMISystemResourceUsageMonitorProtocol>",R,V_resourceUsageMonitor
T@"<HMISystemResourceUsageMonitorDelegate>",W,Vdelegate
initWith
applyWithFrameResult:
activityZones
filterEvents:withActivityZones:motionDetection:insetPercentageInclusion:insetPercentageExclusion:
initWithActivityZones:motionDetections:
_activityZones
T@"NSArray",R,V_activityZones
componentsSeparatedByString:
initWithUUIDString:
initWithPersonUUID:sourceUUID:
personManagerUUID
personSourceUUIDPairFromPersonLink:
initWithUUIDPairString:
UUIDPairString
T@"NSUUID",R,C,V_sourceUUID
initWithUUID:name:personLinks:
defaultFormatter
initWithName:value:options:formatter:
personLinks
initWithUUID:name:
_personLinks
T@"NSSet",R,C,V_personLinks
initWithSourceUUID:externalLibrary:faceCountsByPerson:
isExternalLibrary
faceCountsByPerson
_externalLibrary
_faceCountsByPerson
externalLibrary
TB,R,GisExternalLibrary,V_externalLibrary
T@"NSDictionary",R,V_faceCountsByPerson
visionPersonsModel
personUniqueIdentifiers
faceCountForPersonWithUniqueIdentifier:
initWithPersonsModel:homeUUID:sourceUUID:externalLibrary:
summary
_visionPersonsModel
T@"VNPersonsModel",R,V_visionPersonsModel
T@"HMIPersonsModelSummary",R
_addValueToContainer:forKey:
_containerIsArray
setValue:forKey:
_valueForNumber:
stringFromDate:
_JSONObjectWithObject:options:
initWithDictionary
_addClassToContainer:
object
numberWithLongLong:
decimalNumberWithString:
JSONObjectStringWithObject:pretty:options:
JSONObjectWithObject:options:
JSONObjectStringWithObject:
allowsKeyedCoding
initWithArray
encodeInt32:forKey:
encodeInt64:forKey:
objectJSON
objectPrettyJSON
options
_container
_options
Tq,V_options
hasPrefix:
hasSuffix:
_objectWithJSONObject:allowedClasses:
classMap
initWithJSONObject:
setClassMap:
objectWithJSONData:classMap:
objectWithJSONObject:classMap:
objectWithJSONObjectString:classMap:
decodeInt32ForKey:
decodeInt64ForKey:
container
_classMap
T@,R,V_container
T@"NSDictionary",&,V_classMap
timeZoneForSecondsFromGMT:
setTimeZone:
initWithLocaleIdentifier:
faceObservationWithRequestRevision:boundingBox:andAlignedBoundingBox:
initWithData:elementCount:elementType:lengthInBytes:confidence:requestRevision:
setFaceId:
initWithFaceEvent:torso:
initWithFaceEvent:
initWithTorsoEvent:
URLByAppendingPathExtension:
writeToURL:atomically:encoding:error:
faceObservationsFromFaceprintsForClustering:
faceObservationFromFaceprint:
mergedPersonEventsFromEvents:
saveFaceClassifications:videoId:fragmentId:frameId:baseURL:error:
initWithFaceRecognition:torsoprints:torsoModelVersion:
isEqualToArray:
_torsoModelVersion
T@"NSArray",R,V_torsoprints
T@"NSUUID",R,V_torsoModelVersion
_initSessionWithDimensions:codecType:useHardwareAcceleration:error:
setSession:
_setProperty:propertyValue:
setForceKeyFrameOnNextEncodedFrame:
delegateQueue
encoder:didFailWithError:
_invalidate
forceKeyFrameOnNextEncodedFrame
_invalidateWithErr:
encoder:didEncodeSampleBuffer:
_getProperty:propertyValue:
_setSInt32Property:propertyValue:
_getSInt32Property:propertyValueOut:
_setFloat64Property:propertyValue:
_getFloat64Property:propertyValueOut:
initWithDimensions:codecType:useHardwareAcceleration:error:
prepare
setMaxFrameDelayCount:
maxFrameDelayCount
setAverageBitRate:
averageBitRate
setQuality:
quality
setMaxKeyFrameIntervalDuration:
maxKeyFrameIntervalDuration
setExpectedFrameRate:
expectedFrameRate
setExpectedDuration:
expectedDuration
setDataRateLimit:
dataRateLimit
setLogIdentifier:
numberOfDroppedFrames
_forceKeyFrameOnNextEncodedFrame
_logIdentifier
_numberOfDroppedFrames
_delegateQueue
T^{OpaqueVTCompressionSession=},V_session
TB,V_forceKeyFrameOnNextEncodedFrame
T@"<HMIVideoEncoderDelegate>",W,V_delegate
T@"NSObject<OS_dispatch_queue>",R,V_delegateQueue
T@"NSString",&,V_logIdentifier
Tq,N
Td,N
T{HMIVideoEncoderDataRate=qq},N
TQ,R,V_numberOfDroppedFrames
encoderDidEncodeSampleBuffer
encoderDidFailWithError
setEncoderDidEncodeSampleBuffer:
setEncoderDidFailWithError:
_encoderDidEncodeSampleBuffer
_encoderDidFailWithError
T@?,C,V_encoderDidEncodeSampleBuffer
T@?,C,V_encoderDidFailWithError
homePersonManager
addOrUpdateFaceCrops:completion:
addOrUpdatePersons:completion:
setHomePersonManager:
_homePersonManager
T@"HMHomePersonManager",&,V_homePersonManager
isImportingFromPhotoLibraryEnabled
isSharingFaceClassificationsEnabled
setImportingFromPhotoLibraryEnabled:
setSharingFaceClassificationsEnabled:
_importingFromPhotoLibraryEnabled
_sharingFaceClassificationsEnabled
importingFromPhotoLibraryEnabled
TB,GisImportingFromPhotoLibraryEnabled,V_importingFromPhotoLibraryEnabled
sharingFaceClassificationsEnabled
TB,GisSharingFaceClassificationsEnabled,V_sharingFaceClassificationsEnabled
TB,D,GisImportingFromPhotoLibraryEnabled
TB,D,GisSharingFaceClassificationsEnabled
upload
lazyPayloads
payloadWithCamera:
sendEventWithName:payloadBuilder:
sendEventForPersonRecognitionType:camera:
sendEventForFaceEvent:homePersonManagerUUID:camera:
numberOfFaceprintsClustered
numberOfClusters
numberOfPersonsCreated
numberOfUnknownFaceprintsAssociated
faceprintingDuration
clusteringDuration
totalDuration
modelSummaries
bucketForValue:usingBuckets:
homeToExternalEquivalencies
externalToExternalEquivalencies
isInclusion
timeSinceAnalyzerStarted
transcode
analysisFPS
videoAnalyzerDidFindFaceEvent:homePersonManagerUUID:camera:
sendEventForClusteringTask:
sendEventsForFragmentResult:
videoAnalyzerDidTerminateWithError:state:
videoAnalyzerDidCreateTimelapseFragment:state:
videoAnalyzerDidAnalyzeFragmentWithResult:state:
videoPackageAnalyzerDidClassifyCandidateAsPackage:camera:
initWithModelSummaries:homeToExternalEquivalencies:externalToExternalEquivalencies:
_modelSummaries
_homeToExternalEquivalencies
_externalToExternalEquivalencies
T@"NSSet",R,V_modelSummaries
TQ,R,V_homeToExternalEquivalencies
TQ,R,V_externalToExternalEquivalencies
initWithSourceUUID:personUUID:confidence:linkedEntityUUID:
_linkedEntityUUID
T@"NSUUID",R,V_linkedEntityUUID
initWithPersonsModels:userDefinedPersonLinks:error:
personsModelsByHome
personsModelWithFaceObservationsByID:error:
setMaximumIdentities:
setMaximumTrainingFaceprintsPerIdentity:
addFaceObservations:toPersonWithUniqueIdentifier:error:
userDefinedPersonLinksByHome
isEqualToDictionary:
persistUserDefinedPersonLinks:forHomeUUID:error:
personsModelWithFaceObservations:error:
loadModelsWithError:
homePersonsModelForHomeWithUUID:
getModelStoragePathForModel:error:
persistModel:toPath:error:
buildEquivalencyMapForPersonsModels:userDefinedPersonLinks:error:
equivalencyTablesByHome
getUserDefinedPersonLinksStoragePathForHomeUUID:error:
predictPersonFromFaceObservation:limit:canceller:error:
predictedPersonUniqueIdentifier
equivalencyCellForPerson:
minimumUUIDInEquivalencyCell:
torsoModelsByHome
torsoToFaceCropByHome
trainingFaceObservationsForPersonWithUniqueIdentifier:canceller:error:
faceId
faceObservationFromTorsoprint:
getTorsoModelStoragePathForHomeUUID:error:
persistTorsoToFaceCrop:forHomeUUID:error:
persistTorsoprinterVersionForHomeUUID:error:
_isTorsoFaceCropMapStale:
_hasTorsoprinterVersionChangedForHome:
loadTorsoprinterVersion:error:
earlierDate:
currentCalendar
isDate:inSameDayAsDate:
equivalencyCellForPerson:homeUUID:error:
_reset
modelFromURL:options:error:
URLByDeletingLastPathComponent
setReadOnly:
writeToURL:options:error:
getRootModelStoragePathWithError:
pathWithComponents:
getModelStoragePathForHome:error:
URLByAppendingPathComponent:isDirectory:
absoluteURL
fileHandleForReadingFromURL:error:
readDataToEndOfFile
unarchivedObjectOfClasses:fromData:error:
getTorsoSubdirectoryPathForHomeUUID:error:
fileURLWithPathComponents:
getTorsoToFaceCropStoragePathForHomeUUID:error:
getTorsoprinterVersionStoragePathForHomeUUID:error:
contentsOfDirectoryAtURL:includingPropertiesForKeys:options:error:
regularExpressionWithPattern:options:error:
numberOfMatchesInString:options:range:
kick
modelURLsFromPath:error:
loadPersonsModelFromURL:externalLibrary:homeUUID:error:
loadUserDefinedPersonLinksForHomeUUID:error:
_loadTorsoDataForHomeUUID:intoTorsoModelsByHome:torsoToFaceCropByHome:
loadTorsoToFaceCrop:error:
_resetStaleTorsoStateForHome:torsoToFaceCropMap:
loadModelAtPath:error:
URLByDeletingPathExtension
pathExtension
attributesOfItemAtPath:error:
fileSize
personToEquivalencyCell
T@"HMIPersonsModelManager",R
buildPersonsModelForHomeUUID:sourceUUID:externalLibrary:faceObservationsByPerson:error:
predictHomePersonFromFaceObservation:homeUUID:error:
faceCropFromTorsoModelForHomeUUID:personUUID:sourceUUID:
predictPersonFromTorsoObservation:homeUUID:error:
linkedPredictionsForPrediction:homeUUID:error:
_userDefinedPersonLinksByHome
_personsModelsByHome
_torsoModelsByHome
_torsoToFaceCropByHome
_equivalencyTablesByHome
T@"NSDictionary",R,V_userDefinedPersonLinksByHome
T@"NSDictionary",R,V_personsModelsByHome
T@"NSDictionary",R,V_torsoModelsByHome
T@"NSDictionary",R,V_torsoToFaceCropByHome
T@"NSDictionary",R,V_equivalencyTablesByHome
initWithName:weights:biases:
weights
biases
_weights
_biases
T@"HMIDESMutableFloatArray",R,V_weights
T@"HMIDESMutableFloatArray",R,V_biases
flattedTrainingResult
initWithLayerParameters:losses:preTrainingLoss:postTrainingLoss:
layerParameters
losses
preTrainingLoss
postTrainingLoss
_preTrainingLoss
_postTrainingLoss
_layerParameters
_losses
T@"NSArray",R,V_layerParameters
T@"NSArray",R,V_losses
Tf,R,V_preTrainingLoss
Tf,R,V_postTrainingLoss
T@"HMIDESMutableFloatArray",R
Tf,R
getParameterOfType:forLayerNamed:error:
networkPath
initWithTrainingNetworkPath:inferenceInputs:inferenceOutputs:trainingInputs:trainingOutputs:trainingControlVariableName:withInitializer:error:
initWithTrainingModelDefinition:forPlatform:error:
getTensorNamed:
setTensorNamed:withValue:error:
doInferenceOnData:error:
preTrainingInferenceOutputDictionary:preTrainingtrainingLossKeyName:error:
getParametersFromLayers:fromTask:error:
doTrainingOnData:forNumberOfEpochs:withCallback:error:
saveTrainingNetwork:checkpoint:error:
initWithTrainingNetworkPath:data:error:
inferenceInputs:inferenceOutputs:trainingInputs:trainingOutputs:learningRate:error:
trainLayers:epochs:fromTask:shouldCalculatePreTrainingLoss:error:
_networkPath
T@"HMIDESDataset",R,V_data
T@"NSURL",R,V_networkPath
initWithCode:analysisFPS:message:
message
success
skipped
T@"HMIVideoAnalyzerResultOutcome",R
isSkipped
isSuccess
_message
_analysisFPS
_code
TQ,R,V_code
T@"NSString",R,V_message
Td,R,V_analysisFPS
T@"HMIExternalPersonManagerSettings",R,V_settings
T@"<HMIExternalPersonManagerDataSource>",W,N,V_dataSource
removePersonsModelWithRetryOnError:
external
_external
TB,R,V_external
weakObjectsPointerArray
isAudioAccessory
isProductTypeJ305
isProductTypeJ105
hmf_addObject:
reducedConfiguration:
registerAnalyzer:
analyzers
state
analyzerConfigurations
reducedConfiguration:configurations:
allowReducedConfiguration
decodeMode
transcodeCodecType
timelapseInterval
ignoreThermalAndSystemResourceUsageLevel
maxConcurrentAnalyzersForSystemResourceUsageLevel:
setMaxH264VideoDecoders:
maxH264VideoDecoders
setDecodeMode:
maxH265VideoEncoders
setTranscodeCodecType:
maxH264VideoEncoders
setTranscode:
setTimelapseInterval:
maxAnalysisFPSForSystemResourceUsageLevel:
setAnalysisFPS:
addPointer:
compact
_compactInternalAnalyzers
_updateAnalyzer:withIndex:
_logState
monitored
delay
logStateCount
setLogStateCount:
_shouldSkipLogState
check
stringWithString:
isIdle
processInfo
processName
tableColumns
tableValues
stringByPaddingToLength:withString:startingAtIndex:
stringByAppendingString:
JSONObject
systemResourceUsageDidChangeTo:
analyzerWithConfiguration:block:
analyzerStates
reducedConfiguration:states:
setIgnoreThermalAndSystemResourceUsageLevel:
setMaxH264VideoEncoders:
setMaxH265VideoEncoders:
internalAnalyzers
_registerLock
_usageMonitor
_usageLevel
_ignoreThermalAndSystemResourceUsageLevel
_maxH264VideoDecoders
_maxH264VideoEncoders
_maxH265VideoEncoders
_internalAnalyzers
_logStateCount
T@"NSPointerArray",R,V_internalAnalyzers
T@"NSArray",R
Tq,V_logStateCount
TB,V_ignoreThermalAndSystemResourceUsageLevel
TQ,V_maxH264VideoDecoders
TQ,V_maxH264VideoEncoders
TQ,V_maxH265VideoEncoders
clientWithIdentifier:
updateLevels
submitUpdateModelTask
addUpdateHandlerForNamespaceName:usingBlock:
levelForFactor:withNamespaceName:
fileValue
registerForTrialUpdates
personThresholdHigh
personThresholdMedium
petThresholdHigh
petThresholdMedium
vehicleThresholdHigh
vehicleThresholdMedium
faceThreshold
modelPath
_trialClient
_compiledModelArchivePath
_personThresholdHigh
_personThresholdMedium
_petThresholdHigh
_petThresholdMedium
_vehicleThresholdHigh
_vehicleThresholdMedium
_faceThreshold
_modelPath
Td,R,V_personThresholdHigh
Td,R,V_personThresholdMedium
Td,R,V_petThresholdHigh
Td,R,V_petThresholdMedium
Td,R,V_vehicleThresholdHigh
Td,R,V_vehicleThresholdMedium
Td,R,V_faceThreshold
T@"NSString",R,V_modelPath
lengthInBytes
initWithFaceThreshold:singleLinkThreshold:percentConnectionsThreshold:error:
addFaceObservations:toFaceDescriptorBuffer:error:
convertToClusters:
setObjects:
setClusterId:
objects
setTotalObjectCount:
setShouldUpdateRepresentative:
dataWithBytes:length:
centermostFaceprintInCluster:faceObservations:
getClustersWithFaces:error:
.cxx_construct
_greedyClusterer
T@"<HMIVideoFrameSamplerDelegate>",W,V_delegate
initWithFrameRate:
_interval
_firstPTS
_lastIndex
frameSamplerDidSampleFrame
frameSamplerDidDropFrame
setFrameSamplerDidSampleFrame:
setFrameSamplerDidDropFrame:
_frameSamplerDidSampleFrame
_frameSamplerDidDropFrame
T@?,C,V_frameSamplerDidSampleFrame
T@?,C,V_frameSamplerDidDropFrame
vectorWithString:
CGAffineTransformValue
valueAtIndex:
initWithDictionary:forType:
_type
T@"NSString",R,V_type
T@"HMICIFilterAttributeValue",R,V_value
attributes
attributeForKey:
filterWithName:withInputParameters:
expectedAttributeForKey:
outputImage
_probability
_attributes
Td,R,V_probability
T@"NSArray",R,V_attributes
T@"HMIVisionSession",R
T@"VNSession",R
stringByReplacingOccurrencesOfString:withString:
_ensureModelWithError:
usesCPUOnly
setAllowBackgroundGPUCompute:
setComputeUnits:
modelURL
modelWithContentsOfURL:configuration:error:
setModel:
setTransaction:
underlyingModel
transaction
_modelURL
_transaction
T@"NSURL",R,V_modelURL
T@"MLModel",&,V_model
T@"HMFOSTransaction",&,V_transaction
T@"MLModel",R
packageClassifierMode
eventConfidenceThresholdMedium
eventConfidenceThresholdHigh
eventConfidenceThresholdsMedium
saveAnalyzerResultsToDisk
numberOfDetectedPackagesInSession
backgroundEstimator
packageDetector
setReferenceFrame:
lastDetectionAnalysisTimeStamp
detectionAnalysisInterval
regionOfInterestFromEvents:frameSize:
eventsFromRegionOfInterest:frame:
referenceFrame
setLastDetectionAnalysisTimeStamp:
report
setNumberOfDetectedPackagesInSession:
packageAnalyzer:didDetectPackagesWithResult:error:
analyzePixelBuffer:regionOfInterest:error:
sampler
numberPreferenceForKey:
highConfidence
mediumConfidence
_numberOfDetectedPackagesInSession
_sampler
_packageClassifierMode
_packageDetector
_backgroundEstimator
_report
_highConfidence
_mediumConfidence
_referenceFrame
_lastDetectionAnalysisTimeStamp
_detectionAnalysisInterval
T@"HMIVideoFrameIntervalSampler",R,V_sampler
Tq,R,V_packageClassifierMode
T@"HMICameraVideoFrameAnalyzerSignificantActivity",R,V_packageDetector
T@"HMIBackgroundEstimator",R,V_backgroundEstimator
T@"HMIHTMLReport",R,V_report
T@"NSNumber",R,V_highConfidence
T@"NSNumber",R,V_mediumConfidence
Ti,V_numberOfDetectedPackagesInSession
T@"HMIVideoFrame",&,V_referenceFrame
T{?=qiIq},V_lastDetectionAnalysisTimeStamp
T{?=qiIq},R,V_detectionAnalysisInterval
T@"<HMIVideoPackageAnalyzerDelegate>",W,V_delegate
packageAnalyzerDidDetectPackages
setPackageAnalyzerDidDetectPackages:
_packageAnalyzerDidDetectPackages
T@?,C,V_packageAnalyzerDidDetectPackages
_numBins
_maxLaplacianScore
_minLaplacianScore
_binWidth
_maxScore
_histogram
thumbnails
_fragment
_thumbnails
_outcome
_frameResults
T@"HMIVideoFragment",R,V_fragment
T@"NSArray",R,V_thumbnails
T@"HMIVideoAnalyzerResultOutcome",R,V_outcome
T@"NSArray",R,V_frameResults
T@"HMIVideoAnalyzerDynamicConfiguration",R,V_configuration
isEqualToSet:
decodeIntForKey:
encodeInt:forKey:
initWithPoints:isInclusion:
points
overlapsWithElipseInsideRect:
activityZoneType
overlapsWithElipseInsideRect:withInsetPercentage:
checkIfObjectIsStaticWithBoundingBox:motionDetection:eventClass:
containsEvent:withInsetPercentage:
absoluteString
jsonReperesentaionOfDetectedObject:motionDetection:eventClass:
motionVectors
target
motion
na_all:
characterSetWithCharactersInString:
componentsSeparatedByCharactersInSet:
submitCoreAnalyticsEvent:filteringLevel:numberOfDetectedObjects:
activityZonesFromString:isInclusion:
submitCoreAnalyticsEventForActivityZones:motionScore:
generateAndSubmitStats:filteredEvents:motionDetection:activityZones:
initWithPoints:
saveToJsonActivityZones:motionDetection:videoFragmentUrl:frameId:UUID:detectionID:zoneID:
containsVectorWithSource:destination:
_inclusion
_points
T@"NSArray",R,C,V_points
inclusion
TB,R,GisInclusion,V_inclusion
thumbnailInterval
thumbnailHeight
timelapsePreferredFragmentDuration
maxFragmentDuration
maxFragmentAnalysisDuration
passthroughAudio
minFrameQuality
minFrameScale
enableTemporalEventFiltering
setTimelapsePreferredFragmentDuration:
setThumbnailInterval:
setThumbnailHeight:
setMaxFragmentAnalysisDuration:
setMaxFragmentDuration:
setPassthroughAudio:
setCamera:
setHomeUUID:
setMinFrameQuality:
setMinFrameScale:
setPackageClassifierMode:
setRedactFrames:
setAllowReducedConfiguration:
setEnableTemporalEventFiltering:
setSaveAnalyzerResultsToDisk:
decodeCMTimeForKey:
encodeCMTime:forKey:
timelapseVideo
setTimelapseVideo:
timelapseVideoPreferredFragmentDuration
setTimelapseVideoPreferredFragmentDuration:
_transcode
_passthroughAudio
_redactFrames
_allowReducedConfiguration
_enableTemporalEventFiltering
_saveAnalyzerResultsToDisk
_transcodeCodecType
_minFrameQuality
_minFrameScale
_thumbnailHeight
_maxFragmentAnalysisDuration
_camera
_decodeMode
_thumbnailInterval
_timelapseInterval
_timelapsePreferredFragmentDuration
_maxFragmentDuration
Tq,V_decodeMode
TB,V_redactFrames
Tq,V_packageClassifierMode
TB,V_allowReducedConfiguration
TB,V_enableTemporalEventFiltering
TB,V_saveAnalyzerResultsToDisk
Td,V_analysisFPS
T{?=qiIq},V_thumbnailInterval
TQ,V_thumbnailHeight
T{?=qiIq},V_timelapseInterval
T{?=qiIq},V_timelapsePreferredFragmentDuration
Td,V_maxFragmentAnalysisDuration
T{?=qiIq},V_maxFragmentDuration
TB,V_transcode
TI,V_transcodeCodecType
T@"HMICamera",&,V_camera
Td,V_minFrameQuality
Td,V_minFrameScale
T@"NSUUID",&,V_homeUUID
TB,V_passthroughAudio
setActivityZones:
setEventTriggers:
setRecognizeFaces:
_recognizeFaces
_eventTriggers
TB,V_recognizeFaces
Tq,V_eventTriggers
T@"NSArray",&,V_activityZones
initWithEvent:time:
event
_event
T@"HMIVideoAnalyzerEvent",R,V_event
targetEventClassRanks
timeInterval
_filterEvents:stationaryEvents:motionDetection:
stationaryObjects
_filterEvents:stationaryEvents:stationaryObjects:expirationPTS:imageSize:
prevFrameResult
_resetPreviousFrameResult:expirationPTS:regionOfInterest:
setPrevFrameResult:
_filterEvents:stationaryEvents:motionDetection:prevFrameResult:regionOfInterest:
applyEventTypeAndCheckIfSubBoundingIsStatic:forMetric:eventClass:confidence:
scoreForSubBoundingBox:forMetric:eventClass:confidence:
applyFilterWithFrameResult:motionDetection:
_prevFrameResult
_stationaryObjects
_targetEventClassRanks
_timeInterval
T@"HMIVideoAnalyzerFrameResult",&,V_prevFrameResult
T@"NSMutableArray",R,V_stationaryObjects
T{?=qiIq},R,V_timeInterval
T@"NSDictionary",R,V_targetEventClassRanks
raise:format:
initWithBoundingBox:
expectedClasses
requestAnalysisForPixelBuffer:withProperties:andCompletionHandler:
requestAnalysisForAssetData:withProperties:andCompletionHandler:
cancelRequest:
hasEstimatedBoundingBox
_isBoundingBoxEstimated
_face
_torso
isBoundingBoxEstimated
TB,R,GhasEstimatedBoundingBox,V_isBoundingBoxEstimated
T@"HMIVideoAnalyzerEventTorso",R,V_torso
T@"HMIVideoAnalyzerEventFace",R,V_face
initWithPackageResult:didPrivatizePackageResult:didEncryptPackageResult:maxNorm:l2Norm:
result
maxNorm
didPrivatizePackageResult
didEncryptPackageResult
_didPrivatizePackageResult
_didEncryptPackageResult
_l2Norm
_result
_maxNorm
T@"NSData",R,V_result
Tf,R,V_l2Norm
Td,R,V_maxNorm
TB,R,V_didPrivatizePackageResult
TB,R,V_didEncryptPackageResult
encryptedDataWithPublicKey:inPlaceDataFloatNumbers:count:error:
packageTrainingResult:privatize:maxNorm:normBinCount:encryptionKey:error:
isProductTypeB238
isProductTypeB520
numberPreferenceForKey:defaultValue:withMap:
maxConcurrentAnalyzersForCurrentThermalLevel
isProductTypeJ42
maxAnalysisFPSForCurrentThermalLevel
qosMap
preferenceCache
preferenceOverridesInternal
preferenceLoggedValues
initWithKey:options:domain:defaultValue:
systemPreferenceValueForKey:
logPreferenceForKey:value:
caseInsensitiveCompare:
pretendProductTypeIsUnknown
setPretendProductTypeIsUnknown:
T@"HMIPreference",R
maxVideoEncoderFrameRate
maxVideoEncoders
shouldGenerateThumbnailForAnalysisFPS:
analysisQOS
preferenceOverrides
addPreferenceOverrideFromDictionary:
setPreferenceOverrideFromDictionary:
removeAllPreferenceOverrides
numberPreferenceForKey:defaultValue:withParser:
valuePreferenceForKey:defaultValue:withParser:
valuePreferenceForKey:defaultValue:withMap:
hasPreferenceForKey:
preferenceCacheFlushTimer
_preferenceCacheFlushTimer
_preferenceCache
_preferenceLoggedValues
_preferenceOverridesInternal
T@"HMFTimer",R,V_preferenceCacheFlushTimer
T@"NSMutableDictionary",R,N,V_preferenceCache
T@"NSMutableDictionary",R,N,V_preferenceLoggedValues
T@"NSMutableDictionary",R,N,V_preferenceOverridesInternal
TI,R
setNumberStyle:
numberFromString:
initWithDataSource:person:
personFaceCrops
_person
_personFaceCrops
T@"HMIPerson",R,V_person
T@"NSSet",R,V_personFaceCrops
token
notificationName
publishValueForToken:
publishInitialValue
setToken:
callback
initWithNotificationName:andQueue:andCallback:
_token
_notificationName
_callback
T@"NSObject<OS_dispatch_queue>",R,N,V_queue
T@?,R,N,V_callback
Ti,N,V_token
Tr*,R,N,V_notificationName
dataWithContentsOfURL:options:error:
setAssetWriterDidOutputInitializationSegment:
setAssetWriterDidOutputSeparableSegment:
initWithVideoFormat:audioFormat:initialFragmentSequenceNumber:preferredOutputSegmentInterval:
defaultConfidenceThresholdsFeedback
_addEventsToEventQueue:events:
na_arrayByFlattening
_blurSampleBufferWithEncoder:sampleBuffer:events:
reencodeAssetURL:outputURL:bitRate:duration:analysisFPS:sampleFrameHandler:dropFrameHandler:
removeLastObject
_createBlurredPixelBuffer:events:
_blurRadiusForEvents:imageSize:
na_reduceWithInitialValue:reducer:
waitUntilFinished
numFailures
setNumFailures:
initWithDataSource:faceCropUUIDs:personUUID:source:
enumeratorAtPath:
nextObject
sortUsingComparator:
ffArchiveRootURLWithError:
purgeURLIfNeeded:
writeToFile:atomically:
fetchFaceCropsForPerson:
isAffectedDate:
dumpFFDataToCacheForPerson:personFaceCrops:
reassociateFaceCropsWithUnknownSource:toPersonUUID:
fetchOrCreateFaceprintsForCrops:person:
na_setByRemovingObjectsFromSet:
removeFaceCropsWithUUIDs:
isIdentityPureWithFaceprints:person:
removePerson:
fetchPersons
isCancelled
handleCleanupForPerson:
targetDate
initWithHomeUUID:dataSource:
clusterer
_numFailures
_clusterer
_targetDate
T@"HMIGreedyClustering",R,V_clusterer
Ti,V_numFailures
T@"NSDate",R,V_targetDate
setMaximumFractionDigits:
stringFromNumber:
decimalNumberHandlerWithRoundingMode:scale:raiseOnExactness:raiseOnOverflow:raiseOnUnderflow:raiseOnDivideByZero:
decimalValue
decimalNumberWithDecimal:
decimalNumberByRoundingAccordingToBehavior:
URLForDirectory:inDomain:appropriateForURL:create:error:
hmf_UUIDWithNamespace:data:
setNumberOfFaceprintsClustered:
setNumberOfClusters:
setNumberOfPersonsCreated:
setNumberOfUnknownFaceprintsAssociated:
setFaceprintingDuration:
setClusteringDuration:
setTotalDuration:
setError:
_numberOfFaceprintsClustered
_numberOfClusters
_numberOfPersonsCreated
_numberOfUnknownFaceprintsAssociated
_faceprintingDuration
_clusteringDuration
_totalDuration
Tq,V_numberOfFaceprintsClustered
Tq,V_numberOfClusters
Tq,V_numberOfPersonsCreated
Tq,V_numberOfUnknownFaceprintsAssociated
Td,V_faceprintingDuration
Td,V_clusteringDuration
Td,V_totalDuration
T@"NSError",&,V_error
doImpurePersonCleanup
_stageZero_expireUnnamedPersons
personsModelManager
personCreatedDateFromFaceCrops:
_stageOne_fetchFaceCrops
_stageTwo_fetchFaceprints:
_stageThree_generateFaceprintsForFaceCrops:existingFaceprints:
_stageFour_clusterFaceprints:
_stageFive_addPersons:clusterMapping:faceprints:
_stageSix_associateFaceCropsWithClusterMapping:faceprints:
startTime
_faceClassifier
_doImpurePersonCleanup
_personsModelManager
_summary
_startTime
T@"HMIClusteringTaskSummary",R,V_summary
T@"NSDate",R,V_startTime
TB,R,V_doImpurePersonCleanup
T@"HMIPersonsModelManager",R,V_personsModelManager
limitEnforcedSubsetFromPersons:
shouldRemoveExcessFaceCrops
subsampleFacesForPersons:withFaceObservationsMap:dataSource:vnUUIDToFaceCropUUIDMap:
_removeExcessFaceCrops
removeExcessFaceCrops
TB,R,GshouldRemoveExcessFaceCrops,V_removeExcessFaceCrops
initWithFrame:size:
cropRect
_cropRect
T{CGRect={CGPoint=dd}{CGSize=dd}},R,V_cropRect
T@"HMICameraVideoFrame",R,W,V_frame
T@"NSError",R,V_error
classHierarchyMap
mapTableWithKeyOptions:valueOptions:
significantActivityFcosDetector
regionOfInterestOperations
regionOfInterestOperationQueue
_predictEventsFromCropPixelBuffer:cropRect:imageSize:error:
_simulatedEventForEventClass:
saveDESRecordVideoFrame:analyzerEvents:regionOfInterest:
_eventsWithClassificationsFromEvents:videoFrame:regionOfInterest:homeUUID:enableTorsoRecognition:
_eventsWithSessionEntitiesFromEvents:regionOfInterest:timeStamp:homeUUID:
eventsWithFaceEventsFromTorsoEventsFromEvents:homeUUID:
_targetEventsSetFromTargetEventTypes:enableFaceClassification:enableTorsoRecognition:
_filterEvents:targetEventClasses:
eventsWithContentsOfFile:
_analyzerEventsFromObjectDetections:
sessionEntityManager
na_allObjectsPassTest:
analyzerConfiguration
lowercaseString
highConfidenceThresholds
mediumConfidenceThresholds
confidenceLevel
classifyTorsoEvent:regionOfInterest:pixelBuffer:homeUUID:error:
desLabelIndexForEventClass:
labelIndexForEventClass:
preAnalyze:
_rankForEventClass:
_createStationaryEventFromEvent:
faceClassifier
torsoClassifier
_mediumConfidenceThresholds
_highConfidenceThresholds
_regionOfInterestOperationQueue
_regionOfInterestOperations
_significantActivityFcosDetector
_torsoClassifier
_analyzerConfiguration
_sessionEntityManager
T@"NSDictionary",R,V_mediumConfidenceThresholds
T@"NSDictionary",R,V_highConfidenceThresholds
T@"NSOperationQueue",R,V_regionOfInterestOperationQueue
T@"NSMapTable",R,V_regionOfInterestOperations
T@"HMISignificantActivityFcosDetector",R,V_significantActivityFcosDetector
T@"HMIFaceClassifierVIP",R,V_faceClassifier
T@"HMITorsoClassifier",R,V_torsoClassifier
T@"HMFOSTransaction",&,N,V_transaction
T@"HMIVideoAnalyzerConfiguration",R,V_analyzerConfiguration
T@"HMISessionEntityManager",R,V_sessionEntityManager
T{CGSize=dd},R
taskIdentifier
taskRunnerClass
scheduleTask:
initWithURL:
activityForScheduling
_url
T@"NSURL",R,V_url
printWithHeight:
_store
_presentationTimeStamp
T@"NSData",R,V_data
T{?=qiIq},R,V_presentationTimeStamp
initWithContentsOfFile:options:error:
fileDescriptor
availableData
unpackageTarData:size:parentDir:
getDataOutWithSize:withFlag:fd:
uncompressedContentsForCompressedFile:outPath:
capacity
condition
lock
isFull
wait
setSize:
sampleBufferDelay
buffer:willHandleSampleBuffer:
signal
bufferWillFlush:
fillRatio
isEmpty
handleBlock:
videoDuration
_duration
_capacity
_condition
_sampleBufferDelay
_videoDuration
TQ,V_size
T@"NSCondition",R,V_condition
T@"HMITimeIntervalAverage",R,V_sampleBufferDelay
T@"<HMIVideoCommandBufferDelegate>",W,V_delegate
T{?=qiIq},R,V_videoDuration
TQ,R,V_capacity
bufferWillHandleSampleBuffer
bufferWillFlush
setBufferWillHandleSampleBuffer:
setBufferWillFlush:
_bufferWillHandleSampleBuffer
_bufferWillFlush
T@?,C,V_bufferWillHandleSampleBuffer
T@?,C,V_bufferWillFlush
lines
appendSeparableSegmentWithPath:duration:byteRange:
initWithPlaylistString:
initWithTargetDuration:
playlistString
appendIFrameOnly
appendEncryptionModeWithPath:
appendInitializationSegmentWithPath:
appendSeparableSegmentWithPath:duration:
_lines
T@"NSMutableArray",R,V_lines
T@"NSSet",R,V_unassociatedFaceCrops
T@"NSSet",R,C,V_faceCropUUIDs
dataWithContentsOfFile:options:error:
defaultConfidenceThresholdsHigh
defaultConfidenceThresholdsMedium
defaultConfidenceThreshold:confidenceLevel:
_userInfo
T@"HMIConfidence",R,V_confidence
T@"NSDictionary",R,V_userInfo
initWithFrameReordering:
initWithWeakObject:
lastSampleBufferDTS
setLastSampleBufferDTS:
_failWithDescription:
_createSessionWithFormatDescription:
buffer
_evictSampleBuffer:
setBuffer:
weakDecoder
lastSampleBufferPTS
decoder:didDecodeSampleBuffer:
setLastSampleBufferPTS:
reorderBufferSize
decoder:didFailWithError:
_didDecodeSampleBuffer:
setWeakDecoder:
_reorderBufferSize
_weakDecoder
_lastSampleBufferPTS
_lastSampleBufferDTS
T{?=qiIq},V_lastSampleBufferPTS
T{?=qiIq},V_lastSampleBufferDTS
TQ,R,V_reorderBufferSize
T^{opaqueCMBufferQueue=},V_buffer
T^{OpaqueVTDecompressionSession=},V_session
T@"HMFWeakObject",&,V_weakDecoder
T@"<HMIVideoDecoderDelegate>",W,V_delegate
decoderDidDecodeSampleBuffer
decoderDidFailWithError
setDecoderDidFailWithError:
_decoderDidDecodeSampleBuffer
_decoderDidFailWithError
T@?,C,V_decoderDidDecodeSampleBuffer
T@?,C,V_decoderDidFailWithError
typeWithIdentifier:
initWithContentType:
setOutputFileTypeProfile:
setPreferredOutputSegmentInterval:
setInitialMovieFragmentSequenceNumber:
setProducesCombinableFragments:
initWithMediaType:outputSettings:sourceFormatHint:
setExpectsMediaDataInRealTime:
setMediaTimeScale:
canAddInput:
addInput:
assetWriterInputWithMediaType:outputSettings:sourceFormatHint:
setCurrentFragmentStartTime:
assetWriter
setInitialSegmentStartTime:
startWriting
assetWriter:didOutputInitializationSegment:
assetWriter:didOutputSeparableSegment:segmentReport:
assetWriterDidOutputSeparableSegment
currentFragmentStartTime
preferredOutputSegmentInterval
flushWithCompletionHandler:
_flushAutomatically:
_appendSampleBuffer:
audioFormat
_startWritingAtStartTime:
startSessionAtSourceTime:
videoInput
audioInput
isReadyForMoreMediaData
mediaType
_removeTrimDurationAttachmentsFromAudioSampleBuffer:
allowRecoveryFromInsufficientAudioTrim
_ensureFirstAudioSampleBufferHasSufficientPrimingTrim:
appendSampleBuffer:
setLastVideoPresentationTimeStamp:
setLastAudioPresentationTimeStamp:
lastVideoPresentationTimeStamp
lastAudioPresentationTimeStamp
dropSamplesUntilSync
flushSegment
setDropSamplesUntilSync:
assetWriter:didFailWithError:
assetWriter:didOutputSegmentData:segmentType:segmentReport:
assetWriter:didOutputSegmentData:segmentType:
initWithVideoFormat:audioFormat:
setAssetWriter:
videoFormat
dropTrimDurationAttachments
setDropTrimDurationAttachments:
_dropSamplesUntilSync
_dropTrimDurationAttachments
_allowRecoveryFromInsufficientAudioTrim
_assetWriter
_videoFormat
_audioFormat
_assetWriterDidOutputSeparableSegment
_videoInput
_audioInput
_preferredOutputSegmentInterval
_currentFragmentStartTime
_lastVideoPresentationTimeStamp
_lastAudioPresentationTimeStamp
T@"AVAssetWriter",&,V_assetWriter
Tr^{opaqueCMFormatDescription=},R,V_videoFormat
Tr^{opaqueCMFormatDescription=},R,V_audioFormat
TB,V_dropSamplesUntilSync
TB,V_dropTrimDurationAttachments
TB,R,V_allowRecoveryFromInsufficientAudioTrim
T{?=qiIq},V_preferredOutputSegmentInterval
T{?=qiIq},V_currentFragmentStartTime
T@?,C,V_assetWriterDidOutputSeparableSegment
T@"AVAssetWriterInput",R,V_videoInput
T@"AVAssetWriterInput",R,V_audioInput
T{?=qiIq},V_lastVideoPresentationTimeStamp
T{?=qiIq},V_lastAudioPresentationTimeStamp
T@"<HMIVideoAssetWriterDelegate>",W,V_delegate
assetWriterDidOutputInitializationSegment
assetWriterDidFailWithError
setAssetWriterDidFailWithError:
_assetWriterDidOutputInitializationSegment
_assetWriterDidFailWithError
T@?,C,V_assetWriterDidOutputInitializationSegment
T@?,C,V_assetWriterDidFailWithError
faceObservationsForPersonWithUniqueIdentifier:error:
setByAddingObject:
facesAreSamePersonFromSet:andSet:
facesAreSamePersonFromSet:andSet:distanceThreshold:percentMatchingThreshold:
_personToEquivalencyCell
T@"NSDictionary",R,V_personToEquivalencyCell
updatePersonsModelWithRetryOnError:
initWithOrigin:motion:
midpoint
distance
origin
setEventClass:
_origin
_motion
T{CGPoint=dd},R,V_origin
T{CGPoint=dd},R
T{CGVector=dd},R,V_motion
T{CGRect={CGPoint=dd}{CGSize=dd}},R
T#,&,V_eventClass
motionMode
classMotionScoreMap
classPaddingMap
initWithBoundingBox:size:motionVectors:motionScore:motionMode:
_motionVectors
_motionMode
T@"NSArray",R,V_motionVectors
TQ,R,V_motionMode
calculateMotionDetection:score:srcFeatureCVPoints:dstFeatreCVPoints:activityZones:operationMode:srcPyramid:frameSize:brightnessChange:
applyActivityZoneFilteringOnSourcePoint:destinationPoint:frameSize:activityZones:
_computeOpticalFlow:with:globalMotionScore:activityZones:operationMode:
resizedSampleBuffers
detectWithGlobalMotionScore:referencePixelBuffer:targetPixelBuffer:activityZones:detectorMode:
resizeSampleBuffer:
drainResizedSampleBuffersExpiredBefore:
_resizedSampleBuffers
T^{__CFArray=},R,V_resizedSampleBuffers
torsoprinter
_torsoprinter
T@"HMITorsoprinter",R,V_torsoprinter
sbuf
initWithSampleBuffer:score:motionDetections:
_sbuf
T^{opaqueCMSampleBuffer=},R,V_sbuf
setSampleRate:
alloc
_handleSampleBuffer:reference:
frameSelector:prepareFrame:
_drainCandidatesThatExpiredBefore:
frameSelector:didSelectFrame:motionDetections:motionScore:
frameSelector:didDetectMotion:inFrame:
_synthesizeMotionDetectionWithTarget:
maxCandidates
prepareFrame:
references
detector
maxReferences
setMaxReferences:
resetReferences
setResetReferences:
_candidates
_enabled
_referenceInterval
_targetInterval
_expirationInterval
_resetReferences
_maxCandidates
_references
_detector
_maxReferences
T{os_unfair_lock_s=I},R,N,V_lock
TQ,R,V_maxCandidates
T^{__CFArray=},R,V_references
T@"<HMIMotionDetector>",R,V_detector
TQ,V_maxReferences
TB,V_resetReferences
T@"<HMIVideoFrameSelectorDelegate>",W,V_delegate
frameSelectorDidSelectFrame
frameSelectorDidDetectMotion
frameSelectorPrepareFrame
setFrameSelectorDidSelectFrame:
setFrameSelectorDidDetectMotion:
setFrameSelectorPrepareFrame:
_frameSelectorDidSelectFrame
_frameSelectorDidDetectMotion
_frameSelectorPrepareFrame
T@?,C,V_frameSelectorDidSelectFrame
T@?,C,V_frameSelectorDidDetectMotion
T@?,C,V_frameSelectorPrepareFrame
timeSinceLastFragmentWasReceived
bufferFillRatio
bufferSize
numDecodedSamples
currentPTS
numDidAnalyzeFrames
numDidAnalyzeFragments
encode
encoder
numDidCreateTimelapseFragments
numDidAnalyzePackages
initWithConfiguration:dynamicConfiguration:identifier:monitored:analysisFPS:timeSinceAnalyzerStarted:timeSinceLastFragmentWasReceived:bufferFillRatio:bufferSize:delay:currentPTS:numDecodedSamples:numDidAnalyzeFrames:numDidAnalyzeFragments:numDidAnalyzePackages:numDidCreateTimelapseFragments:averageAnalysisTime:encode:encoder:
_monitored
_encode
_encoder
_timeSinceAnalyzerStarted
_timeSinceLastFragmentWasReceived
_bufferFillRatio
_bufferSize
_delay
_numDecodedSamples
_numDidAnalyzeFrames
_numDidAnalyzeFragments
_numDidAnalyzePackages
_numDidCreateTimelapseFragments
_averageAnalysisTime
_currentPTS
T@"HMIVideoAnalyzerDynamicConfiguration",R,V_dynamicConfiguration
TB,R,V_monitored
Td,R,V_timeSinceAnalyzerStarted
Td,R,V_timeSinceLastFragmentWasReceived
Td,R,V_bufferFillRatio
TQ,R,V_bufferSize
Td,R,V_delay
T{?=qiIq},R,V_currentPTS
TQ,R,V_numDecodedSamples
TQ,R,V_numDidAnalyzeFrames
TQ,R,V_numDidAnalyzeFragments
TQ,R,V_numDidAnalyzePackages
TQ,R,V_numDidCreateTimelapseFragments
Td,R,V_averageAnalysisTime
TB,R,V_encode
TB,R,V_encoder
initWithFaceCrop:faceprint:classifications:
initWithFaceCrop:faceprint:classifications:predictedLinkedEntityUUIDs:
_classifications
_faceQualityScore
T@"NSSet",R,V_classifications
Td,R,V_faceQualityScore
analyzerWithConfiguration:identifier:remote:error:
initWithConfiguration:identifier:
handleAssetData:withOptions:completionHandler:
analyzerWithOptions:error:
analyzerWithConfiguration:identifier:error:
handleMessageWithOptions:completionHandler:
analyzeFragment:configuration:
setMonitored:
setEncode:
finalizeFragmentResult:homePersonManager:analysisStateManager:
analysisStateManager
setAnalysisStateManager:
externalPersonManagers
setExternalPersonManagers:
_analysisStateManager
_externalPersonManagers
_state
T@"NSDictionary",R,C,V_options
T@"HMIVideoAnalyzerConfiguration",R,C,V_configuration
T@"HMIVideoAnalyzerState",R,V_state
TB,N
T@"HMIVideoAnalyzerMutableReport",R,V_report
T@"<HMIVideoAnalyzerDelegate>",W,V_delegate
T@"HMIHomePersonManager",&,V_homePersonManager
T@"HMIAnalysisStateManager",&,V_analysisStateManager
T@"NSSet",&,V_externalPersonManagers
TQ,R,V_status
inputQueue
setLastFragmentRecievedDate:
_saveFragmentDataToDisk:diskBufferSize:
commandBuffer
_notifyDelegateDidFailWithError:
dynamicConfigurationBuffer
_prepareForInputVideoFormat:audioFormat:
_ensureDecoderForFragment:
_ensureEncoder
_ensureTimelapseEncoder
handleSampleBuffer:errorHandler:
_handleDecodedSampleBuffer:
timeline
hasFailed
timelapseAssetWriter
timelapseEncoder
fileHandleForWritingAtPath:
seekToEndOfFile
writeData:
closeFile
startDate
stringByAppendingPathExtension:
inputVideoFormat
inputAudioFormat
_configureAssetWriter
timelapseOutputVideoFormat
_configureTimelapseAssetWriter
setTimelapseAssetWriter:
setEncoder:
encoderQueue
setTimelapseEncoder:
decoder
frameSelector
frameAnalyzer
_configureEncoder
_configureTimelapseEncoder
_prepareForTimelapseOutputVideoFormat:
dynamicConfigurationForTime:
frameThumbnailSampler
frameTimelapseSampler
packageAnalyzer
temporalEventFilter
_filterFrameResult:dynamicConfiguration:motionDetections:
frameAnalyzerFrameResultBuffer
_notifyDelegateDidAnalyzeFrameWithResult:
_notifyDelegateDidProduceAnalysisStateUpdate:
thumbnailBuffer
packageAnalyzerFrameResultBuffer
setInitializationSegment:
setTimelapseInitializationSegment:
trackReports
firstVideoSampleInformation
offset
timelapseInitializationSegment
_notifyDelegateDidCreateTimelapseFragment:
_notifyDelegateDidAnalyzeFragmentWithResult:
analyzer:didAnalyzeFragmentWithResult:
_produceResult:withArguments:
analyzer:didAnalyzeFrameWithResult:
analyzer:didCreateTimelapseFragment:
setHasFailed:
analyzer:didFailWithError:
analyzer:didProduceAnalysisStateUpdate:
analyzer:didProduceResult:
lastFragmentRecievedDate
setInputVideoFormat:
setInputAudioFormat:
setTimelapseOutputVideoFormat:
setCurrentPTS:
currentDTS
setCurrentDTS:
setCancelled:
_hasFailed
_cancelled
_inputQueue
_encoderQueue
_inputVideoFormat
_inputAudioFormat
_timelapseOutputVideoFormat
_commandBuffer
_decoder
_frameThumbnailSampler
_frameTimelapseSampler
_timelapseEncoder
_frameSelector
_frameAnalyzer
_timelapseAssetWriter
_frameAnalyzerFrameResultBuffer
_packageAnalyzerFrameResultBuffer
_thumbnailBuffer
_timelapseInitializationSegment
_dynamicConfigurationBuffer
_packageAnalyzer
_temporalEventFilter
_startDate
_lastFragmentRecievedDate
_currentDTS
T@"NSObject<OS_dispatch_queue>",R,V_inputQueue
T@"NSObject<OS_dispatch_queue>",R,V_encoderQueue
Tr^{opaqueCMFormatDescription=},V_inputVideoFormat
Tr^{opaqueCMFormatDescription=},V_inputAudioFormat
Tr^{opaqueCMFormatDescription=},V_timelapseOutputVideoFormat
T@"HMIVideoCommandBuffer",R,V_commandBuffer
T@"HMIVideoDecoder",R,V_decoder
T@"HMIVideoFrameSampler",R,V_frameThumbnailSampler
T@"HMIVideoFrameSampler",R,V_frameTimelapseSampler
T@"HMIVideoEncoder",&,V_encoder
T@"HMIVideoEncoder",&,V_timelapseEncoder
T@"HMIVideoFrameSelector",R,V_frameSelector
T@"HMIVideoFrameAnalyzer",R,V_frameAnalyzer
T@"HMIVideoAssetWriter",&,V_assetWriter
T@"HMIVideoAssetWriter",&,V_timelapseAssetWriter
T{?=qiIq},V_currentPTS
T{?=qiIq},V_currentDTS
T@"HMIVideoEventBuffer",R,V_frameAnalyzerFrameResultBuffer
T@"HMIVideoEventBuffer",R,V_packageAnalyzerFrameResultBuffer
T@"HMIVideoEventBuffer",R,V_thumbnailBuffer
T@"NSData",&,V_initializationSegment
T@"NSData",&,V_timelapseInitializationSegment
T@"HMIVideoEventBuffer",R,V_dynamicConfigurationBuffer
T@"HMIVideoPackageAnalyzer",R,V_packageAnalyzer
T@"HMIVideoTemporalEventFilter",R,V_temporalEventFilter
T@"HMIVideoTimeline",R,V_timeline
T@"NSDate",R,V_startDate
T@"NSDate",&,V_lastFragmentRecievedDate
TB,V_hasFailed
cancelled
TB,GisCancelled,V_cancelled
analyzerDidAnalyzeFrameWithResult
analyzerDidAnalyzeFragmentWithResult
analyzerDidFailWithError
analyzerDidCreateTimelapseFragment
analyzerDidProduceAnalysisStateUpdate
setAnalyzerDidAnalyzeFrameWithResult:
setAnalyzerDidAnalyzeFragmentWithResult:
setAnalyzerDidFailWithError:
setAnalyzerDidCreateTimelapseFragment:
setAnalyzerDidProduceAnalysisStateUpdate:
_analyzerDidAnalyzeFrameWithResult
_analyzerDidAnalyzeFragmentWithResult
_analyzerDidFailWithError
_analyzerDidCreateTimelapseFragment
_analyzerDidProduceAnalysisStateUpdate
T@?,C,V_analyzerDidAnalyzeFrameWithResult
T@?,C,V_analyzerDidAnalyzeFragmentWithResult
T@?,C,V_analyzerDidFailWithError
T@?,C,V_analyzerDidCreateTimelapseFragment
T@?,C,V_analyzerDidProduceAnalysisStateUpdate
getStoragePath
getResourceValue:forKey:error:
_sourceURL
T@"NSURL",R,V_sourceURL
_torsoRecognition
T@"HMITorsoRecognition",R,V_torsoRecognition
@16@0:8
B24@0:8@16
#16@0:8
@24@0:8:16
@32@0:8:16@24
@40@0:8:16@24@32
B16@0:8
B24@0:8#16
B24@0:8:16
Vv16@0:8
Q16@0:8
^{_NSZone=}16@0:8
B24@0:8@"Protocol"16
@"NSString"16@0:8
v24@0:8@?16
v32@0:8@16@?24
v24@0:8@?<v@?@"NSSet"@"NSError">16
v32@0:8@"NSSet"16@?<v@?@"NSSet"@"NSError">24
v24@0:8@?<v@?@"NSError">16
v32@0:8@"NSSet"16@?<v@?@"NSError">24
v24@0:8@?<v@?@"HMIExternalPersonManagerSettings"@"NSError">16
@72@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16@48@56f64f68
{CGRect={CGPoint=dd}{CGSize=dd}}16@0:8
f16@0:8
v16@0:8
@"NSString"
{CGRect="origin"{CGPoint="x"d"y"d}"size"{CGSize="width"d"height"d}}
@28@0:8@16B24
@48@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16
@32@0:8@16@24
v40@0:8@16@24@32
v24@0:8@16
v32@0:8@16@24
v64@0:8@16@24@32i40@44i52@56
v48@0:8@16i24@28i36@40
@"NSOutputStream"
@"NSObject"16@0:8
@24@0:8@16
@"HMIDESMutableFloatArray"
@"HMIFaceRecognition"
@"NSMutableArray"
@"NSMutableSet"
q16@0:8
v24@0:8q16
@"HMISystemResourceUsage"16@0:8
@"<HMISystemResourceUsageMonitorDelegate>"16@0:8
v24@0:8@"<HMISystemResourceUsageMonitorDelegate>"16
@"HMISystemResourceUsageMonitorImpl"
@"NSObject<OS_dispatch_queue>"
@56@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24
@64@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24@56
@88@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24@56@64@72@80
@96@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24@56@64@72@80@88
@"NSNumber"
@"HMITorsoAnnotation"
@"<HMIPersonManagerDataSource>"
@"NSSet"
I16@0:8
v24@0:8Q16
@24@0:8Q16
@"NSData"
{?=qiIq}16@0:8
@48@0:8@16{?=qiIq}24
{?="value"q"timescale"i"flags"I"epoch"q}
@24@0:8q16
@68@0:8{?={?=qiIq}{?=qiIq}}16B64
@64@0:8{?={?=qiIq}{?=qiIq}}16
{os_unfair_lock_s="_os_unfair_lock_opaque"I}
@48@0:8{?=qiIq}16@40
@"NSDate"
v48@0:8@16{?=qiIq}24
@40@0:8{?=qiIq}16
d40@0:8{?=qiIq}16
@"HMIVideoEventBuffer"
v40@0:8{?=qiIq}16
d16@0:8
@"HMIVideoTimeline"
@"HMITimeIntervalAverage"
v24@0:8d16
d32@0:8d16d24
@"MovingAverage"
@60@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24f56
^{__CVBuffer=}40@0:8@16^{__CVBuffer=}24^@32
@32@0:8^{__CVBuffer=}16^@24
@"<HMIHomePersonManagerDataSource>"
v24@0:8@"HMFTimer"16
@"HMIHomePersonManagerSettings"
@"NSOperationQueue"
@"HMFTimer"
@"NSMutableDictionary"
@"HMIFaceCrop"
@"HMIFaceprint"
@24@0:8^{__CVBuffer=}16
@64@0:8^{__CVBuffer=}16{?=qiIq}24Q48Q56
@80@0:8@16{?=qiIq}24Q48Q56{CGSize=dd}64
{CGSize=dd}16@0:8
^{__CVBuffer=}16@0:8
^{__CVBuffer=}
@"NSArray"
{CGSize="width"d"height"d}
^{__CVBuffer=}64@0:8^{__CVBuffer=}16{CGRect={CGPoint=dd}{CGSize=dd}}24^@56
^{__CVBuffer=}48@0:8^{__CVBuffer=}16{CGSize=dd}24^@40
^{__CVBuffer=}60@0:8^{__CVBuffer=}16{CGSize=dd}24I40q44^@52
^{__CVBuffer=}44@0:8^{__CVBuffer=}16I24q28^@36
^{__CVBuffer=}92@0:8^{__CVBuffer=}16{CGRect={CGPoint=dd}{CGSize=dd}}24{CGSize=dd}56I72q76^@84
@40@0:8^{__CVBuffer=}16f24f28^@32
^{__CVBuffer=}32@0:8^{CGDataProvider=}16^@24
^{__CVBuffer=}32@0:8@16^@24
^{__CVBuffer=}40@0:8{CGSize=dd}16I32B36
{CGRect={CGPoint=dd}{CGSize=dd}}80@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{CGSize=dd}48{CGSize=dd}64
{CGRect={CGPoint=dd}{CGSize=dd}}68@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{CGSize=dd}48f64
^{__CVBuffer=}92@0:8^{__CVBuffer=}16f24{CGRect={CGPoint=dd}{CGSize=dd}}28{CGSize=dd}60Q76^@84
@36@0:8Q16Q24f32
@24@0:8^{__IOHIDServiceClient=}16
^{__IOHIDServiceClient=}
B28@0:8i16^d20
B24@0:8^d16
^{__IOHIDEventSystemClient=}
@48@0:8@16@24@32^@40
B40@0:8^{__CVBuffer=}16@24^@32
B72@0:8^{__CVBuffer=}16@24@32@40@48@56^@64
v64@0:8@16@24@32@40@48@56
[7f]
[5{CGSize="width"d"height"d}]
@"HMINMSConfiguration"
@32@0:8@16B24B28
B32@0:8@16B24B28
^{opaqueCMSampleBuffer=}24@0:8@16
^{opaqueCMSampleBuffer=}16@0:8
^{opaqueCMSampleBuffer=}24@0:8^Q16
@"AVAsset"
@"AVAssetReader"
^{__CFArray=}
@"HMHomeManager"
v32@0:8@16Q24
v32@0:8@"HMHomeManager"16Q24
v24@0:8@"HMHomeManager"16
v32@0:8@"HMHomeManager"16@"HMHome"24
v32@0:8@"HMHomeManager"16@"HMAddAccessoryRequest"24
@?16@0:8
@"NSUUID"
@"MLFeatureValue"24@0:8@"NSString"16
@"NSSet"16@0:8
@"MLMultiArray"
@40@0:8@16@24^@32
B40@0:8@16^d24^@32
@"MLModel"
@"NSError"
v24@0:8^{opaqueCMSampleBuffer=}16
@"HMIVideoAnalyzerConfiguration"
@"HMIVideoAnalyzerDynamicConfiguration"
@"HMIDESBackgroundTask"
@"HMPhotosPersonManager"
@32@0:8q16@24
@24@0:8^{_NSZone=}16
v24@0:8@"NSCoder"16
@24@0:8@"NSCoder"16
@80@0:8@16@24@32{CGRect={CGPoint=dd}{CGSize=dd}}40@72
@88@0:8@16@24@32{CGRect={CGPoint=dd}{CGSize=dd}}40@72q80
@64@0:8@16@24{CGRect={CGPoint=dd}{CGSize=dd}}32
@24@0:8#16
@"HMIVideoFrame"
@80@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{?=qiIq}48f72S76
S16@0:8
B40@0:8{?=qiIq}16
f48@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16
Q48@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16
^{__CVBuffer=}24@0:8^{__CVBuffer=}16
@48@0:8^{__CVBuffer=}16{?=qiIq}24
v88@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24^{__CVBuffer=}56{?=qiIq}64
v32@0:8@16^{opaqueCMSampleBuffer=}24
B48@0:8^{__CVBuffer=}16{?=qiIq}24
@32@0:8@16^B24
v56@0:8@16@24{?=qiIq}32
v24@0:8^{__CVBuffer=}16
v40@0:8^{__CVBuffer=}16^f24i32i36
v32@0:8r^f16^{__CVBuffer=}24
f64@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24^S56
v68@0:8^S16S24S28{CGRect={CGPoint=dd}{CGSize=dd}}32f64
v44@0:8^f16^f24r^f32f40
v48@0:8^f16^f24r^f32r^f40
v24@0:8r*16
v40@0:8^{__CVBuffer=}16*24^S32
v32@0:8^{__CVBuffer=}16^f24
@48@0:8^S16{?=qiIq}24
v96@0:8@16^{__CVBuffer=}24@32{CGRect={CGPoint=dd}{CGSize=dd}}40{?=qiIq}72
^{__CVBuffer=}32@0:8^{__CVBuffer=}16Q24
^S16@0:8
v24@0:8^S16
^f16@0:8
v24@0:8^f16
v32@0:8{CGSize=dd}16
v20@0:8B16
B32@0:8@16@24
B32@0:8@"AVAssetResourceLoader"16@"AVAssetResourceLoadingRequest"24
B32@0:8@"AVAssetResourceLoader"16@"AVAssetResourceRenewalRequest"24
v32@0:8@"AVAssetResourceLoader"16@"AVAssetResourceLoadingRequest"24
B32@0:8@"AVAssetResourceLoader"16@"NSURLAuthenticationChallenge"24
v32@0:8@"AVAssetResourceLoader"16@"NSURLAuthenticationChallenge"24
q24@0:8@16
@88@0:8@16q24@32@40{CGRect={CGPoint=dd}{CGSize=dd}}48@80
@"HMICameraVideoFrame"
@"NSDictionary"
@56@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16d48
@40@0:8#16@24@32
@64@0:8#16{CGRect={CGPoint=dd}{CGSize=dd}}24@56
@28@0:8f16Q20
@32@0:8r^f16Q24
v20@0:8f16
v32@0:8r^f16Q24
r^f16@0:8
@20@0:8f16
@"NSMutableData"
@52@0:8@16^{__CVBuffer=}24B32@36^@44
@"HMIVideoAnalyzerEventFace"52@0:8@"HMIVideoAnalyzerEventFace"16^{__CVBuffer=}24B32@"NSUUID"36^@44
@24@0:8^@16
@64@0:8@16d24d32d40d48^@56
@"HMIFaceprinter"
@"HMIFaceQualityFilterSVM"
^{__CVBuffer=}24@0:8^@16
@56@0:8@16@24@32@40@48
@32@0:8Q16^@24
@32@0:8^{__CVBuffer=}16@24
@48@0:8@16@24@32@40
@76@0:8i16d20{CGRect={CGPoint=dd}{CGSize=dd}}28@60@68
i16@0:8
f80@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{CGRect={CGPoint=dd}{CGSize=dd}}48
v48@0:8@16@24d32q40
@72@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24{CGSize=dd}56
f24@0:8@16
@"HMITorsoprint"
@"NSMutableIndexSet"
v80@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24{?=qiIq}56
@72@0:8@16@24{CGRect={CGPoint=dd}{CGSize=dd}}32@64
v32@0:8@"HMIVideoFrameSampler"16^{opaqueCMSampleBuffer=}24
B40@0:8^{opaqueCMSampleBuffer=}16@24d32
@"<HMIVideoFrameAnalyzerDelegate>"
@"<HMICameraVideoFrameAnalyzer>"
@"HMIVideoFrameSampler"
v40@0:8@"HMIVideoFrameAnalyzer"16@"HMIVideoFrameAnalyzerResult"24@"NSError"32
v32@0:8@"HMIVideoFrameAnalyzer"16@"HMIAnalysisStateUpdate"24
@36@0:8@16@24f32
@40@0:8@16@24Q32
@40@0:8@16@24@32
@44@0:8@16@24@32B40
v28@0:8@16B24
v48@0:8@16@24@32@40
@104@0:8@16@24@32@40#48{CGRect={CGPoint=dd}{CGSize=dd}}56f88f92@?96
v56@0:8@16@24@32@40@48
@32@0:8@16^@24
v44@0:8@16@24@32B40
@28@0:8f16@?20
@28@0:8@16f24
@36@0:8@16f24f28B32
@28@0:8B16@?20
@36@0:8@16B24f28B32
@36@0:8@16#24f32
@44@0:8@16#24f32f36B40
@40@0:8@16#24f32f36
v76@0:8@16@24#32@40@48@56Q64B72
v40@0:8#16@24@32
v40@0:8@16#24@32
@64@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16@48@56
{CGRect={CGPoint=dd}{CGSize=dd}}24@0:8@16
@72@0:8@16@24@32{CGRect={CGPoint=dd}{CGSize=dd}}40
^{opaqueCMSampleBuffer=}
@"<HMIVideoRetimerDelegate>"
v32@0:8@"HMIVideoRetimer"16^{opaqueCMSampleBuffer=}24
@20@0:8i16
@28@0:8i16d20
@36@0:8i16@20d28
i32@0:8@16@?24
B20@0:8i16
v20@0:8i16
@"NSArray"16@0:8
@56@0:8@16@24@32q40@48
@"HMITorsoClassification"
^{__CVBuffer=}48@0:8@16^{__CVBuffer=}24@32^@40
q20@0:8i16
@36@0:8^{__CVBuffer=}16B24^@28
@"NSArray"32@0:8^{__CVBuffer=}16^@24
@"NSArray"32@0:8@"NSData"16^@24
@32@0:8{CGPoint=dd}16
{CGPoint=dd}16@0:8
{CGPoint="x"d"y"d}
B40@0:8@16@24^@32
@36@0:8i16@20@28
v40@0:8@16@24@?32
v32@0:8@"NSURLSession"16@"NSError"24
v40@0:8@"NSURLSession"16@"NSURLAuthenticationChallenge"24@?<v@?q@"NSURLCredential">32
v24@0:8@"NSURLSession"16
@"HMIHomeKitClient"
@"NSURLSession"
@"HMIFeedbackSession"
@"HMFOperation"
v44@0:8@16@24B32@?36
@56@0:8@16@24@32d40q48
@72@0:8@16@24@32@40@48d56q64
@76@0:8@16@24@32@40@48d56B64q68
@96@0:8@16@24@32{CGRect={CGPoint=dd}{CGSize=dd}}40@72@80d88
v48@0:8@16@24q32@?40
v24@0:8@?<v@?@"HMIHomePersonManagerSettings"@"NSError">16
v48@0:8@"NSSet"16@"NSUUID"24q32@?<v@?@"NSError">40
@28@0:8i16@20
@"HMIPersonFaceCrop"
@"HMIVideoAssetReader"
@32@0:8d16@24
@56@0:8Q16@24@32@40@48
@40@0:8q16@24@32
@48@0:8q16@24@32@40
@72@0:8@16{?={?=qiIq}{?=qiIq}}24
@80@0:8@16@24{?={?=qiIq}{?=qiIq}}32
@96@0:8@16@24{?={?=qiIq}{?=qiIq}}32{_NSRange=QQ}80
@88@0:8@16@24{?={?=qiIq}{?=qiIq}}32@80
@104@0:8@16@24{?={?=qiIq}{?=qiIq}}32@80{_NSRange=QQ}88
{?={?=qiIq}{?=qiIq}}16@0:8
r^{opaqueCMFormatDescription=}16@0:8
{_NSRange=QQ}16@0:8
r^{opaqueCMFormatDescription=}
{_NSRange="location"Q"length"Q}
{?="start"{?="value"q"timescale"i"flags"I"epoch"q}"duration"{?="value"q"timescale"i"flags"I"epoch"q}}
@56@0:8@16@24@32@40q48
@88@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24{?=qiIq}56@80
@56@0:8@16@24@32@40^q48
v56@0:8@16@24@32@40^q48
@"HMIPersonTracker"
@"<HMIAnalysisStateManagerDelegate>"
@32@0:8^{__CVBuffer=}16d24
r^{__CTFont=}24@0:8d16
v48@0:8@16{CGPoint=dd}24r^d40
v64@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16d48r^d56
v72@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16d48@56r^d64
^{CGContext=}
^{CGColorSpace=}
^{__CTFont=}
@"<HMISystemResourceUsageMonitorDelegate>"
@"<HMISystemResourceUsageMonitorProtocol>"
@36@0:8@16B24@28
@"VNPersonsModel"
@32@0:8@16q24
@36@0:8@16B24q28
v24@0:8#16
v28@0:8i16@20
v32@0:8q16@24
v32@0:8d16@24
v28@0:8B16@20
i24@0:8@16
d24@0:8@16
@32@0:8#16@24
B64@0:8@16@24Q32@40@48^@56
@40@0:8{?=ii}16I24B28^@32
B40@0:8{?=ii}16I24B28^@32
i32@0:8r^{__CFString=}16r^v24
i32@0:8r^{__CFString=}16r^^v24
i28@0:8r^{__CFString=}16i24
i32@0:8r^{__CFString=}16^i24
i32@0:8r^{__CFString=}16d24
i32@0:8r^{__CFString=}16^d24
v32@0:8{HMIVideoEncoderDataRate=qq}16
{HMIVideoEncoderDataRate=qq}16@0:8
^{OpaqueVTCompressionSession=}16@0:8
v24@0:8^{OpaqueVTCompressionSession=}16
^{OpaqueVTCompressionSession=}
@"<HMIVideoEncoderDelegate>"
v32@0:8@"HMIVideoEncoder"16^{opaqueCMSampleBuffer=}24
v32@0:8@"HMIVideoEncoder"16@"NSError"24
@"HMHomePersonManager"
B32@0:8@16@?24
q32@0:8q16@24
@40@0:8@16Q24Q32
B32@0:8@16^@24
B52@0:8@16@24B32@36^@44
B24@0:8^@16
@44@0:8@16B24@28^@36
@40@0:8@16@24f32f36
@64@0:8@16@24@32@40@48^@56
f40@0:8@16@24^@32
@52@0:8@16Q24@32B40^@44
@"HMIDESDataset"
@"NSURL"
@40@0:8Q16d24@32
@"<HMIExternalPersonManagerDataSource>"
@"HMIExternalPersonManagerSettings"
@36@0:8@16@24B32
@32@0:8@16@?24
@"HMISystemResourceUsageMonitor"
@"NSPointerArray"
@"TRIClient"
B40@0:8@16^v24^@32
f32@0:8@16@24
@24@0:8^v16
{shared_ptr<homeai::clustering::GreedyClusterer>="__ptr_"^{GreedyClusterer}"__cntrl_"^{__shared_weak_count}}
@"<HMIVideoFrameSamplerDelegate>"
@"HMICIFilterAttributeValue"
@32@0:8@16d24
@"VNSession"
@"HMFOSTransaction"
v40@0:8@"HMIVideoPackageAnalyzer"16@"HMIVideoPackageAnalyzerResult"24@"NSError"32
{CGRect={CGPoint=dd}{CGSize=dd}}40@0:8@16{CGSize=dd}24
@56@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16@48
@"<HMIVideoPackageAnalyzerDelegate>"
@"HMIVideoFrameIntervalSampler"
@"HMICameraVideoFrameAnalyzerSignificantActivity"
@"HMIBackgroundEstimator"
@"HMIHTMLReport"
f24@0:8^{__CVBuffer=}16
{vector<float, std::allocator<float>>="__begin_"^f"__end_"^f"__end_cap_"{__compressed_pair<float *, std::allocator<float>>="__value_"^f}}
@64@0:8@16@24@32@40@48@56
@24@0:8B16B20
@"HMIVideoFragment"
@"HMIVideoAnalyzerResultOutcome"
@48@0:8@16@24@32f40f44
v28@0:8@16f24
B48@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16
B52@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16f48
B28@0:8@16f24
B72@0:8@16@24@32@40@48@56@64
B64@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16@48#56
B48@0:8{CGPoint=dd}16{CGPoint=dd}32
v20@0:8I16
@"HMICamera"
@"HMIVideoAnalyzerEvent"
@80@0:8@16@24@32{?=qiIq}40{CGSize=dd}64
@80@0:8@16@24@32@40{CGRect={CGPoint=dd}{CGSize=dd}}48
B80@0:8@16{?=qiIq}24{CGRect={CGPoint=dd}{CGSize=dd}}48
@"HMIVideoAnalyzerFrameResult"
B68@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16q48#56f64
i40@0:8^{__CVBuffer=}16@24@?32
i40@0:8@16@24@?32
@72@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24@56@64
@"HMIVideoAnalyzerEventFace"
@"HMIVideoAnalyzerEventTorso"
@44@0:8@16B24B28d32f40
@60@0:8@16B24d28Q36@44^@52
Q24@0:8q16
d24@0:8q16
B24@0:8d16
@40@0:8@16@24@?32
B28@0:8@16B24
@"HMIPerson"
@40@0:8r*16@24@?32
r*16@0:8
B84@0:8@16@24q32{?=qiIq}40f64@?68@?76
B76@0:8@16@24{?=qiIq}32f56Q60^B68
v40@0:8@16^{opaqueCMSampleBuffer=}24@32
^{__CVBuffer=}32@0:8^{__CVBuffer=}16@24
I40@0:8@16{CGSize=dd}24
@"HMIGreedyClustering"
@64@0:8i16@20@28@36@44B52^@56
@"<HMIFaceClassifier>"
@"HMIPersonsModelManager"
@"HMIClusteringTaskSummary"
@52@0:8i16@20@28@36B44B48
@40@0:8@16{CGSize=dd}24
@52@0:8@16q24B32@36^@44
@64@0:8^{__CVBuffer=}16{CGRect={CGPoint=dd}{CGSize=dd}}24^@56
@48@0:8@"NSDictionary"16@"NSDictionary"24@"HMIVideoAnalyzerConfiguration"32^@40
v24@0:8@"HMICameraVideoFrame"16
@"HMICameraVideoFrameResult"52@0:8@"HMICameraVideoFrame"16q24B32@"NSUUID"36^@44
@"NSSet"64@0:8^{__CVBuffer=}16{CGRect={CGPoint=dd}{CGSize=dd}}24^@56
@"HMIAnalysisStateUpdate"28@0:8@"NSUUID"16B24
@"NSDictionary"16@0:8
@80@0:8^{__CVBuffer=}16{CGRect={CGPoint=dd}{CGSize=dd}}24{CGSize=dd}56^@72
q24@0:8#16
@32@0:8q16B24B28
@76@0:8@16@24{CGRect={CGPoint=dd}{CGSize=dd}}32@64B72
v64@0:8@16@24{CGRect={CGPoint=dd}{CGSize=dd}}32
@"NSMapTable"
@"HMISignificantActivityFcosDetector"
@"HMIFaceClassifierVIP"
@"HMITorsoClassifier"
@"HMISessionEntityManager"
@64@0:8@16{CGSize=dd}24{?=qiIq}40
@24@0:8^{opaqueCMSampleBuffer=}16
@40@0:8d16d24^@32
*40@0:8Q16Q24^i32
i40@0:8^v16Q24r*32
i32@0:8@16@24
@"NSCondition"
@"<HMIVideoCommandBufferDelegate>"
v32@0:8@"HMIVideoCommandBuffer"16^{opaqueCMSampleBuffer=}24
v24@0:8@"HMIVideoCommandBuffer"16
@24@0:8d16
v32@0:8@16d24
v48@0:8@16d24{_NSRange=QQ}32
@48@0:8@16@24@32q40
#24@0:8@16
@32@0:8#16q24
@"HMIConfidence"
@40@0:8@16@24d32
@20@0:8B16
B28@0:8^{opaqueCMSampleBuffer=}16B24
B24@0:8r^{opaqueCMFormatDescription=}16
^{opaqueCMBufferQueue=}16@0:8
v24@0:8^{opaqueCMBufferQueue=}16
^{OpaqueVTDecompressionSession=}16@0:8
v24@0:8^{OpaqueVTDecompressionSession=}16
@"<HMIVideoDecoderDelegate>"
^{opaqueCMBufferQueue=}
^{OpaqueVTDecompressionSession=}
@"HMFWeakObject"
v32@0:8@"HMIVideoDecoder"16^{opaqueCMSampleBuffer=}24
v32@0:8@"HMIVideoDecoder"16@"NSError"24
v48@0:8@16@24q32@40
v40@0:8@16@24q32
v48@0:8@"AVAssetWriter"16@"NSData"24q32@"AVAssetSegmentReport"40
v40@0:8@"AVAssetWriter"16@"NSData"24q32
@32@0:8r^{opaqueCMFormatDescription=}16r^{opaqueCMFormatDescription=}24
@64@0:8r^{opaqueCMFormatDescription=}16r^{opaqueCMFormatDescription=}24Q32{?=qiIq}40
@"<HMIVideoAssetWriterDelegate>"
@"AVAssetWriter"
@"AVAssetWriterInput"
v32@0:8@"HMIVideoAssetWriter"16@"NSData"24
v40@0:8@"HMIVideoAssetWriter"16@"NSData"24@"AVAssetSegmentReport"32
v32@0:8@"HMIVideoAssetWriter"16@"NSError"24
B48@0:8@16@24d32d40
@48@0:8{CGPoint=dd}16{CGVector=dd}32
{CGVector=dd}16@0:8
{CGVector="dx"d"dy"d}
@32@0:8@16Q24
@84@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{CGSize=dd}48@64f72Q76
f68@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16q48#56f64
@56@0:8^f16^{__CVBuffer=}24^{__CVBuffer=}32@40Q48
^{opaqueCMSampleBuffer=}24@0:8^{opaqueCMSampleBuffer=}16
@"NSArray"56@0:8^f16^{__CVBuffer=}24^{__CVBuffer=}32@"NSArray"40Q48
@56@0:8^{__CVBuffer=}16^{__CVBuffer=}24^f32@40Q48
@172@0:8{vector<unsigned char, std::allocator<unsigned char>>=**{__compressed_pair<unsigned char *, std::allocator<unsigned char>>=*}}16{vector<float, std::allocator<float>>=^f^f{__compressed_pair<float *, std::allocator<float>>=^f}}40{vector<cv::Point_<float>, std::allocator<cv::Point_<float>>>=^v^v{__compressed_pair<cv::Point_<float> *, std::allocator<cv::Point_<float>>>=^v}}64{vector<cv::Point_<float>, std::allocator<cv::Point_<float>>>=^v^v{__compressed_pair<cv::Point_<float> *, std::allocator<cv::Point_<float>>>=^v}}88@112Q120{vector<cv::Mat, std::allocator<cv::Mat>>=^{Mat}^{Mat}{__compressed_pair<cv::Mat *, std::allocator<cv::Mat>>=^{Mat}}}128{CGSize=dd}152f168
B72@0:8{CGPoint=dd}16{CGPoint=dd}32{CGSize=dd}48@64
^{__CFArray=}16@0:8
@80@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24^{__CVBuffer=}56@64^@72
@"HMITorsoprinter"
@36@0:8^{opaqueCMSampleBuffer=}16f24@28
v32@0:8^{opaqueCMSampleBuffer=}16^{opaqueCMSampleBuffer=}24
{os_unfair_lock_s=I}16@0:8
@"<HMIVideoFrameSelectorDelegate>"
@"<HMIMotionDetector>"
v48@0:8@16^{opaqueCMSampleBuffer=}24@32d40
v40@0:8@16@24^{opaqueCMSampleBuffer=}32
^{opaqueCMSampleBuffer=}32@0:8@16^{opaqueCMSampleBuffer=}24
v48@0:8@"HMIVideoFrameSelector"16^{opaqueCMSampleBuffer=}24@"NSArray"32d40
v40@0:8@"HMIVideoFrameSelector"16@"NSArray"24^{opaqueCMSampleBuffer=}32
^{opaqueCMSampleBuffer=}32@0:8@"HMIVideoFrameSelector"16^{opaqueCMSampleBuffer=}24
@172@0:8@16@24@32B40d44d52d60d68Q76d84{?=qiIq}92Q116Q124Q132Q140Q148d156B164B168
@72@0:8@16@24@32@40d48q56@64
@44@0:8@16@24B32^@36
@"<HMIVideoAnalyzerDelegate>"
@"HMIHomePersonManager"
@"HMIAnalysisStateManager"
@"HMIVideoAnalyzerState"
@"HMIVideoAnalyzerMutableReport"
v32@0:8@"HMIVideoAnalyzer"16@"HMIVideoAnalyzerFragmentResult"24
v32@0:8@"HMIVideoAnalyzer"16@"HMIVideoAnalyzerFrameResult"24
v32@0:8@"HMIVideoAnalyzer"16@"NSError"24
v32@0:8@"HMIVideoAnalyzer"16@"HMIVideoFragment"24
v32@0:8@"HMIVideoAnalyzer"16@"HMIAnalysisStateUpdate"24
v32@0:8@"HMIVideoAnalyzer"16@"NSDictionary"24
v32@0:8^{opaqueCMSampleBuffer=}16@?24
v32@0:8r^{opaqueCMFormatDescription=}16r^{opaqueCMFormatDescription=}24
v24@0:8r^{opaqueCMFormatDescription=}16
v32@0:8:16@24
@"HMIVideoCommandBuffer"
@"HMIVideoDecoder"
@"HMIVideoEncoder"
@"HMIVideoFrameSelector"
@"HMIVideoFrameAnalyzer"
@"HMIVideoAssetWriter"
@"HMIVideoPackageAnalyzer"
@"HMIVideoTemporalEventFilter"
@"HMITorsoRecognition"
gepj
v024
ffffff
333333
333333
333333
333333
333333
333333
333333
@(#)PROGRAM:HomeAI  PROJECT:HomeAI-231.5.1
Mb@?
UsesIOSurface
HardwareAcceleratedTransfer
HighSpeedTransfer
HMIPixelBufferTransferMultiStageResize
a0cTa1cTa2cTa3cTa0hTa1hTa2hTa3hT
!$'*-0369<?BEHKNQTWZ]`cfilorux{~
 #&),/258;>ADGJMPSVY\_behknqtwz}
L?_p
r@ffffff
.>333?{
?N6homeai3mod28ImageDescriptorBufferFloat32E
gn\\
@LE\\
@LEN6homeai10clustering15GreedyClustererE
NSt3__120__shared_ptr_emplaceINS_6vectorIxNS_9allocatorIxEEEENS2_IS4_EEEE
NSt3__120__shared_ptr_emplaceIN6homeai10clustering15GreedyClusterer9cluster_tENS_9allocatorIS4_EEEE
NSt3__120__shared_ptr_emplaceINS_6vectorINS_5tupleIJxxfEEENS_9allocatorIS3_EEEENS4_IS6_EEEE
,92N6homeai3mod29ImageDescriptorBufferAbstractE
?ffffff
 #&),/2z5z8;>ADGzJMPSVY\_behknqzzztzw
333333
>NSt3__120__shared_ptr_emplaceIN6homeai10clustering15GreedyClustererENS_9allocatorIS3_EEEE
MbP?
Motion
Person
Vehicle
Package
?fff?
N2cv13BaseRowFilterE
N2cv16BaseColumnFilterE
N2cv10BaseFilterE
N2cv12FilterEngineE
N2cv18SymmRowSmallFilterIhiNS_17SymmRowSmallNoVecEEE
N2cv9RowFilterIhiNS_17SymmRowSmallNoVecEEE
N2cv18SymmRowSmallFilterIffNS_17SymmRowSmallNoVecEEE
N2cv9RowFilterIffNS_17SymmRowSmallNoVecEEE
N2cv9RowFilterIhiNS_8RowNoVecEEE
N2cv9RowFilterIhfNS_8RowNoVecEEE
N2cv9RowFilterIhdNS_8RowNoVecEEE
N2cv9RowFilterItfNS_8RowNoVecEEE
N2cv9RowFilterItdNS_8RowNoVecEEE
N2cv9RowFilterIsfNS_8RowNoVecEEE
N2cv9RowFilterIsdNS_8RowNoVecEEE
N2cv9RowFilterIffNS_8RowNoVecEEE
N2cv9RowFilterIfdNS_8RowNoVecEEE
N2cv9RowFilterIddNS_8RowNoVecEEE
N2cv12ColumnFilterINS_13FixedPtCastExIihEENS_11ColumnNoVecEEE
N2cv12ColumnFilterINS_4CastIfhEENS_11ColumnNoVecEEE
N2cv12ColumnFilterINS_4CastIdhEENS_11ColumnNoVecEEE
N2cv12ColumnFilterINS_4CastIftEENS_11ColumnNoVecEEE
N2cv12ColumnFilterINS_4CastIdtEENS_11ColumnNoVecEEE
N2cv12ColumnFilterINS_4CastIfsEENS_11ColumnNoVecEEE
N2cv12ColumnFilterINS_4CastIdsEENS_11ColumnNoVecEEE
N2cv12ColumnFilterINS_4CastIffEENS_11ColumnNoVecEEE
N2cv12ColumnFilterINS_4CastIddEENS_11ColumnNoVecEEE
N2cv21SymmColumnSmallFilterINS_13FixedPtCastExIihEENS_11ColumnNoVecEEE
N2cv16SymmColumnFilterINS_13FixedPtCastExIihEENS_11ColumnNoVecEEE
N2cv21SymmColumnSmallFilterINS_4CastIisEENS_20SymmColumnSmallNoVecEEE
N2cv16SymmColumnFilterINS_4CastIisEENS_20SymmColumnSmallNoVecEEE
N2cv12ColumnFilterINS_4CastIisEENS_20SymmColumnSmallNoVecEEE
N2cv21SymmColumnSmallFilterINS_4CastIffEENS_20SymmColumnSmallNoVecEEE
N2cv16SymmColumnFilterINS_4CastIffEENS_20SymmColumnSmallNoVecEEE
N2cv12ColumnFilterINS_4CastIffEENS_20SymmColumnSmallNoVecEEE
N2cv16SymmColumnFilterINS_4CastIfhEENS_11ColumnNoVecEEE
N2cv16SymmColumnFilterINS_4CastIdhEENS_11ColumnNoVecEEE
N2cv16SymmColumnFilterINS_4CastIftEENS_11ColumnNoVecEEE
N2cv16SymmColumnFilterINS_4CastIdtEENS_11ColumnNoVecEEE
N2cv16SymmColumnFilterINS_4CastIisEENS_11ColumnNoVecEEE
N2cv12ColumnFilterINS_4CastIisEENS_11ColumnNoVecEEE
N2cv16SymmColumnFilterINS_4CastIfsEENS_11ColumnNoVecEEE
N2cv16SymmColumnFilterINS_4CastIdsEENS_11ColumnNoVecEEE
N2cv16SymmColumnFilterINS_4CastIffEENS_11ColumnNoVecEEE
N2cv16SymmColumnFilterINS_4CastIddEENS_11ColumnNoVecEEE
N2cv8Filter2DIhNS_4CastIfhEENS_11FilterNoVecEEE
N2cv8Filter2DIhNS_4CastIftEENS_11FilterNoVecEEE
N2cv8Filter2DIhNS_4CastIfsEENS_11FilterNoVecEEE
N2cv8Filter2DIhNS_4CastIffEENS_11FilterNoVecEEE
N2cv8Filter2DIhNS_4CastIddEENS_11FilterNoVecEEE
N2cv8Filter2DItNS_4CastIftEENS_11FilterNoVecEEE
N2cv8Filter2DItNS_4CastIffEENS_11FilterNoVecEEE
N2cv8Filter2DItNS_4CastIddEENS_11FilterNoVecEEE
N2cv8Filter2DIsNS_4CastIfsEENS_11FilterNoVecEEE
N2cv8Filter2DIsNS_4CastIffEENS_11FilterNoVecEEE
N2cv8Filter2DIsNS_4CastIddEENS_11FilterNoVecEEE
N2cv8Filter2DIfNS_4CastIffEENS_11FilterNoVecEEE
N2cv8Filter2DIdNS_4CastIddEENS_11FilterNoVecEEE
 !"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\]^_`abcdefghijklmnopqrstuvwxyz{|}~
(Zm9
+5:DH)K
9CHR
4Yucwsifdr
?qxs?qxs
9D)5:
@`|-A
PF SdF
 [@W:[
D]eV
"x@
z4I5z
?~my
!Y?4
!I?u
!)?gX
?g^FT
?$"LT
?$"LT
N2cv15ThresholdRunnerE
?N2cv6detail16LKTrackerInvokerE
:ss:
9qq9
6kk6
7mm7
9qq9
N2cv9ColumnSumIihEE
N2cv9ColumnSumIitEE
N2cv9ColumnSumIisEE
N2cv6RowSumIhiEE
N2cv6RowSumIhdEE
N2cv6RowSumItiEE
N2cv6RowSumItdEE
N2cv6RowSumIsiEE
N2cv6RowSumIiiEE
N2cv6RowSumIsdEE
N2cv6RowSumIfdEE
N2cv6RowSumIddEE
N2cv9ColumnSumIdhEE
N2cv9ColumnSumIdtEE
N2cv9ColumnSumIdsEE
N2cv9ColumnSumIiiEE
N2cv9ColumnSumIifEE
N2cv9ColumnSumIdfEE
N2cv9ColumnSumIidEE
N2cv9ColumnSumIddEE
 !"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\]^_`abcdefghijklmnopqrstuvwxyz{|}~
49<_
6:>Z.ZBZZZ2ZZZF{
$yV'*-0N2cv11_InputArrayE
N2cv12_OutputArrayE
 #&),/258;>AD
ZZ9ZZZZZZ<Z?ZBZZZZEHZKNQTWgN2cv9ExceptionE
14EmptyFuncTable
12GpuFuncTable
N2cv16ParallelLoopBodyE
 FR ^j
bnz
N2cv16MorphologyRunnerE
N2cv14MorphRowFilterINS_5MinOpIhEENS_13MorphRowNoVecEEE
N2cv14MorphRowFilterINS_5MinOpItEENS_13MorphRowNoVecEEE
N2cv14MorphRowFilterINS_5MinOpIsEENS_13MorphRowNoVecEEE
N2cv14MorphRowFilterINS_5MinOpIfEENS_13MorphRowNoVecEEE
N2cv14MorphRowFilterINS_5MinOpIdEENS_13MorphRowNoVecEEE
N2cv14MorphRowFilterINS_5MaxOpIhEENS_13MorphRowNoVecEEE
N2cv14MorphRowFilterINS_5MaxOpItEENS_13MorphRowNoVecEEE
N2cv14MorphRowFilterINS_5MaxOpIsEENS_13MorphRowNoVecEEE
N2cv14MorphRowFilterINS_5MaxOpIfEENS_13MorphRowNoVecEEE
N2cv14MorphRowFilterINS_5MaxOpIdEENS_13MorphRowNoVecEEE
N2cv17MorphColumnFilterINS_5MinOpIhEENS_16MorphColumnNoVecEEE
N2cv17MorphColumnFilterINS_5MinOpItEENS_16MorphColumnNoVecEEE
N2cv17MorphColumnFilterINS_5MinOpIsEENS_16MorphColumnNoVecEEE
N2cv17MorphColumnFilterINS_5MinOpIfEENS_16MorphColumnNoVecEEE
N2cv17MorphColumnFilterINS_5MinOpIdEENS_16MorphColumnNoVecEEE
N2cv17MorphColumnFilterINS_5MaxOpIhEENS_16MorphColumnNoVecEEE
N2cv17MorphColumnFilterINS_5MaxOpItEENS_16MorphColumnNoVecEEE
N2cv17MorphColumnFilterINS_5MaxOpIsEENS_16MorphColumnNoVecEEE
N2cv17MorphColumnFilterINS_5MaxOpIfEENS_16MorphColumnNoVecEEE
N2cv17MorphColumnFilterINS_5MaxOpIdEENS_16MorphColumnNoVecEEE
N2cv11MorphFilterINS_5MinOpIhEENS_10MorphNoVecEEE
N2cv11MorphFilterINS_5MinOpItEENS_10MorphNoVecEEE
N2cv11MorphFilterINS_5MinOpIsEENS_10MorphNoVecEEE
N2cv11MorphFilterINS_5MinOpIfEENS_10MorphNoVecEEE
N2cv11MorphFilterINS_5MinOpIdEENS_10MorphNoVecEEE
N2cv11MorphFilterINS_5MaxOpIhEENS_10MorphNoVecEEE
N2cv11MorphFilterINS_5MaxOpItEENS_10MorphNoVecEEE
N2cv11MorphFilterINS_5MaxOpIsEENS_10MorphNoVecEEE
N2cv11MorphFilterINS_5MaxOpIfEENS_10MorphNoVecEEE
N2cv11MorphFilterINS_5MaxOpIdEENS_10MorphNoVecEEE
N2cv5MatOpE
N2cv14MatOp_IdentityE
N2cv11MatOp_AddExE
N2cv9MatOp_BinE
N2cv9MatOp_CmpE
N2cv10MatOp_GEMME
N2cv12MatOp_InvertE
N2cv7MatOp_TE
N2cv11MatOp_SolveE
N2cv17MatOp_InitializerE
Error creating directory for person:%@
version
UUID
displayName
person.json
Error saving metadata to disk for person:%@
v8@?0
Error creating directory for person face crop:%@
faceBoundingBox
dateCreated
facecrop.json
Error saving metadata to disk for person face crop:%@
facecrop.jpeg
Error saving face crop image to disk for person face crop:%@
#ffffff
#000000
</body>
</html>
Visualizer saved (%@)
HMIHTMLReport
<html>
<head><title>%@</title></head>
<style>
</style>
<body text='%@' bgcolor='%@'>
<script>
</script>
%@<br>
border:%dpx;
border:%dpx solid %@;
outline:%dpx;
outline:%dpx solid %@;
<div class='image'>
<img width='%d' height='%d' src='data:image/jpeg;base64,%@' style='%@'/>
<div class='rect' style='top:%dpx; left:%dpx; width:%dpx; height:%dpx; border-color:%@; opacity:%.1f' threshold='%.3f'>%@</div>
v32@?0@"HMIHTMLReportBox"8Q16^B24
<div class="text">%@</div>
</div>
%.3fs
v16@?0@"HMIVideoAnalyzerFrameResult"8
@"NSValue"16@?0@"HMIVideoAnalyzerFrameResult"8
[%lu/%lu] %@ (%.2fs)
v32@?0@"HMIVideoFrame"8Q16^B24
v16@?0@"NSArray"8
v24@?0@"HMIVideoAnalyzerEvent"8^B16
confidence.value
v16@?0@"HMIVideoAnalyzerEvent"8
%.3f %@
%.3f
%.3f %@ %.2f %@
#ffff00
HMISystemResourceUsageLevelLow
HMISystemResourceUsageLevelMed
HMISystemResourceUsageLevelHigh
HMISystemResourceUsageLevelUnknown
HMISystemResourceUsageLevelUndefined
FaceFilteredState
HMIVAEF.fr
HMIVAEF.ta
HMIVAEF.ya
HMIVAEF.ro
None
QualitySVMKnown
QualitySVMUnknown
QualityANFRScore
NoPredictions
HasFaceMask
Face Recognition
Torso Annotation
Face Yaw
Face Roll
%@@(%@,%@)
v16@?0@"NSError"8
%c%c%c%c
Type: %@, DTS: %@, PTS: %@, DUR: %@, NUM: %ld [%@]
Sync
%c%02d:%02d:%02d.%03d
%*lld/%-6d %@
%*lld %@
POSITIVE INFINITY
NEGATIVE INFINITY
   INDEFINITE    
  INVALID TIME   
%@ DTS %@ PTS %@ dur %@%@
PTS: %.2f
en_US_POSIX
yyyy-MM-dd'T'HH:mm:ss
%.3f x %.3f
%.3f, %.3f %@
%.2f
%02x
q24@?0@"<HMIVideoEvent>"8@"<HMIVideoEvent>"16
B32@?0@"<HMIVideoEvent>"8Q16^B24
@16@?0@"<HMIVideoEvent>"8
Time: %@, Date: %@
HMIVAEM.ms
Motion score
%@@(%@)
Failed to set request revision
failed to perform image request
Expected 1 torsoprint, but got %lu torsoprints
torsoprint is nil
HMICoreAnalyticsVIPModelReportTime
v24@?0@"HMIHomePersonManagerSettings"8@"NSError"16
v24@?0@"NSDictionary"8@"NSError"16
B16@?0@"HMIFaceClassification"8
UUID:%@ HomeUUID:%@
Frame %lu @ %@
Logi Circle 2
%#{flags}
transferPixelBuffer
VTPixelTransferSessionCreate failed. Error %d
VTSessionSetProperty failed. Error %d
VTPixelTransferSessionTransferImage failed. Error %d
JPEGDataFromPixelBuffer
Invalid crop for affine transform
Error in pixelbuffer format for affine transform
Error generating pixelbuffer for affine transform
Error applying affine transform
q24@?0@"HMIPairwiseMatch"8@"HMIPairwiseMatch"16
v12@?0i8
/private/var
OutOfMemory
Reached high water mark.
PrimaryUsagePage
PrimaryUsage
LocationID
image/Placeholder
HomeSSD/box0_offset0
HomeSSD/box0_offset1
HomeSSD/box0_offset2
HomeSSD/box0_offset3
HomeSSD/box0_offset4
HomeSSD/box1_offset0
HomeSSD/box1_offset1
HomeSSD/box1_offset2
HomeSSD/box1_offset3
HomeSSD/box1_offset4
HomeSSD/class_prob0
HomeSSD/class_prob1
HomeSSD/class_prob2
HomeSSD/class_prob3
HomeSSD/class_prob4
HomeSSD/object_yaw0
HomeSSD/object_yaw1
HomeSSD/object_yaw2
HomeSSD/object_yaw3
HomeSSD/object_yaw4
HomeSSD/object_roll0
HomeSSD/object_roll1
HomeSSD/object_roll2
HomeSSD/object_roll3
HomeSSD/object_roll4
SignificantActivityFcosDetector
CoreML output nil or not of type MLFeatureTypeMultiArray
SignificantActivityFcos
mlmodelc
name
VeryLongTrackDuration
discussion
Track %@ has an unexpectedly long track duration %@.
v16@?0@"AVAssetTrack"8
WidelyDifferingTrackDurations
Track durations vary widely, this is usually caused by a corrupt video / audio sample duration.
v16@?0@"NSDictionary"8
Sanitized Data
Asset Check
HMIHomeKitClient HomeKit Delegate Queue
B16@?0@"HMHome"8
B16@?0@"HMResidentDevice"8
B16@?0@"HMCameraProfile"8
v16@?0@"HMHomeManager"8
Not supported error
General error
Espresso error
incorrect binserializer key
small sparsity error
feature extraction error
initialization error
no saved state to revert
nominal distance not changed
batch size violation
computation kill request was issued
too few IDs to build VIP model
video error
error with projection computation
missing positional parameter
inconsistent state error
warping error
OpenGL error
invalid format
out of bounds
singular point configuration error
division by zero
LAPACK error
platform endianess not supported
hash already in use
invalid ID
invalid data type
data inconsistency error
I/O error
unknown option
invalid option
missing option
delegate error
vImage related error
memory allocation error
invalid parameter
unexpected null pointer
internal error
not implemented error
CVML_status module %d error %lld
BinSerializer
Face3D
FaceDescriptor
FaceFrontalizer
FaceWarper
Geometry2D
Geometry3D
ImageGrouping
ImageQuality
LandmarkDetector
MomentProcessor
FaceboxAligner
ImageDescriptor
ImageClassifier
ImageProcessing
VIPIdentification
ImageRegistration
SimilarityMatrix
Clustering
HumanDetector
FaceRegionMap
ObjectDetector
ObjectTracker
SRCClassifier
Kmeans
SparseCoding
FaceID
BoostedClassifier
FaceSegmenter
ImageAnalyzer
FaceAttributes
FaceprintAndAttributes
FaceQuality
Generic
ImageTools
VideoTools
ImageWarper
ThirdParty
BinSerializerProcessor
AppleNetParser
FaceProcessorCLI
ImageClassifierCLI
MPCmdlineClientCLI
ClusteringCLI
ImageProcessorCLI
PhotosProcessorCLI
CVMLEngine
CVML Module %lld
%@ (%@)
input
transformed_features
classProbability
FaceRecognizabilityFilterSVM
FaceRecognizabilityFilterSVMDataScaler
FaceAestheticQualityFilterSVM
FaceAestheticQualityFilterSVMDataScaler
You must override %@ in a subclass
@16@?0@"HMPerson"8
v24@?0@"NSSet"8@"NSError"16
@16@?0@"HMPersonFaceCrop"8
@16@?0@"HMFaceprint"8
@16@?0@"HMIFaceprint"8
com.apple.cvml.%@
CVML module = %@
HMIFC.ck.pu
HMIFC.ck.so
Unknown
PhotoLibrary
User
ImpureClusteringCleanup
Unknown source: %ld
dataRepresentation
personUUID
Person UUID
Source
frame
Frame
Events
Region of Interest
B16@?0@"HMIVideoAnalyzerEvent"8
@16@?0#8
q24@?0@"HMIVideoAnalyzerFrameResult"8@"HMIVideoAnalyzerFrameResult"16
Track:%.2f-%.2f @ %@ (%@)
CVPixelBufferGetPixelFormatType(pixelBuffer) == kCVPixelFormatType_420YpCbCr8BiPlanarVideoRange || CVPixelBufferGetPixelFormatType(pixelBuffer) == kCVPixelFormatType_420YpCbCr8BiPlanarFullRange
v16@?0@"HMIForegroundTrack"8
B16@?0@"HMIForegroundTrack"8
{CGRect={CGPoint=dd}{CGSize=dd}}
B16@?0@"NSValue"8
@16@?0@"HMIForegroundTrack"8
B16@?0@"HMIForegroundBlob"8
v32@?0@"HMIForegroundBlob"8Q16^B24
v32@?0@"HMIForegroundTrack"8Q16^B24
v16@?0@"HMIPairwiseMatch"8
v24@?0Q8^B16
v16@?0@"NSValue"8
CVPixelBufferGetPixelFormatType(pixelBuffer) == kCVPixelFormatType_420YpCbCr8BiPlanarFullRange
@16@?0@"NSValue"8
%.2f Result
%.2f Mean
%.2f Std
%.2f Diff
@"NSSet"16@?0@"HMIForegroundTrack"8
%.2f Assign
allocator<T>::allocate(size_t n) 'n' exceeds maximum supported size
hmi://in-memory
HMIMemoryAVAsset
tracks
Invalid parameter for windowSize: %lu, windowSize must be > 0
v16@?0@"MovingAverageEntry"8
@16@?0@"HMIVideoAnalyzerEvent"8
class
timestamp
uuid
v32@?0@"NSArray"8Q16^B24
@16@?0@"NSDictionary"8
v16@?0@"HMIVideoAnnotationParserTrack"8
{CGRect={CGPoint=dd}{CGSize=dd}}80@?0{CGRect={CGPoint=dd}{CGSize=dd}}8{CGRect={CGPoint=dd}{CGSize=dd}}40f72f76
@16@?0@"HMIVideoAnnotationParserTrack"8
@16@?0@"HMIPersonsModelPrediction"8
detections
bounds
label
confidence
filters
v16@?0@8
com.apple.HomeAIDESPlugin
v24@?0@"NSUUID"8@"NSError"16
bias_b
bias_g
bias_r
is_network_bgr
scale
Label:%d Confidence:%f X:%f Y:%f Width:%f Height:%f Yaw:%@}
q24@?0@"HMIObjectDetection"8@"HMIObjectDetection"16
@16@?0@"HMIObjectDetection"8
@"HMIPersonBlob"16@?0@"HMIVideoAnalyzerEventPerson"8
@16@?0@"HMIPersonBlob"8
v32@?0@"HMIPersonBlob"8Q16^B24
v24@?0@"NSNumber"8@"NSNumber"16
<%@: %p> timeStamp: %@, detections: [%@], regionOfInterest: %@
<%@: %p> %@
self.dynamicConfiguration
@max.floatValue
@16@?0@"HMIVideoAnalyzerReportRecord"8
@sum.count
Precision
Recall
True Positive
False Negative
False Positive
Fragments
v16@?0@"HMIVideoAnalyzerFragmentResult"8
v24@?0@"NSString"8@"HMIVideoAnalyzerMutableReportSession"16
v16@?0#8
@"HMIVideoAnalyzerMutableReportComparison"20@?0#8f16
precision
recall
threshold
annotation
opacity
/System/Library/CoreServices/SystemVersion.plist
com.apple.HomeAI
HomeAIBundleVersion
Debug
Truth
image_id
B16@?0@"NSDictionary"8
classification_classes
@16@?0@"NSString"8
@24@?0@"NSString"8@"HMIVideoAnalyzerMutableReportSession"16
f56@?0@"HMIVideoAnalyzerEvent"8@"HMIVideoAnalyzerEvent"16{CGRect={CGPoint=dd}{CGSize=dd}}24
@"NSArray"16@?0@"HMIVideoAnalyzerEvent"8
@24@?0@"NSNumber"8@"NSNumber"16
@16@?0@"NSNumber"8
v32@?0@"HMIVideoAnalyzerFrameResult"8Q16^B24
v32@?0@"NSNumber"8@"NSNumber"16^B24
v16@?0@"NSNumber"8
score
%@%@
Object detection (%@)
Visualize%@.html
%lu %@s (Precision: %.3f, Recall: %.3f)
v16@?0@"HMIVideoAnalyzerReportRecord"8
@"NSValue"16@?0@"HMIVideoFrame"8
@16@?0@"HMIVideoFrame"8
q24@?0@"NSString"8@"NSString"16
Fragment%@%@.txt
v24@?0@"NSString"8@"NSArray"16
domain
range
@24@?0@"NSString"8@"NSString"16
'%@': '%@'
{%@}[datum.label]
labelExpr
$schema
https://vega.github.io/schema/vega-lite/v4.json
description
PR Curves
width
container
height
data
config
style
align
center
baseline
layer
mark
type
line
clip
true
point
encoding
field
quantitative
color
nominal
legend
text
left
%@_%.0f_%@_%lu.png
v32@?0@"HMIVideoAnalyzerEvent"8Q16^B24
v16@?0@"HMIVideoAnalyzerReportMatch"8
v32@?0@"NSUUID"8@"HMIVideoAnalyzerEventPerson"16^B24
HMIFC.ck.u
HMIFC.ck.dr
HMIFC.ck.dc
HMIFC.ck.fbb
PVFC:PVFC
PVFC_FB
PVFC_CB
Data Representation
Date Created
Face Bounding Box
Could not create image source
No meta data exists on image
mediaType == kCMMediaType_Video
Descriptor count = 
Descriptor length = 
 bytes
 = [
identifier
Identifier
Name
Manufacturer
Model
Firmware Version
Descriptor vectors nil
Success
Canceled
Error
UpdatePersonsModelTask
RemovePersonsModelTask
HomePersonClusteringTask
TuriTrialUpdateTask
FaceMisclassificationTask
PersonsModelsSummaryTask
UpdateTorsoModelTask
FeedbackTask
EmptyTask
resultCode
taskType
faceCrop
sourceUUID
homeUUID
isExternal
doImpurePersonCleanup
cameraProfileUUID
clipUUID
torsoAnnotations
Unknown task type: %@
v32@?0@"HMITask"8Q16^B24
HMITaskHomeUUIDKey is nil
HMIUpdatePersonsModelTaskIsExternal is nil
HMIPersonsModelTaskSourceUUID is nil
Failed to get HMPhotosPersonManager
Failed to get HMHomePersonManager
Failed to initialize data source
Disk-based Home Person Data Source not supported
HomeUUID is nil
cleanup key is missing
HMIUpdateTorsoModelTaskAnnotationsKey is nil
HMITaskService not available on this platform.
HMITR.c
HMITR.tp
HMITR.sea
HMITR.seu
v16@?0@"HMIFaceprint"8
warm_start_faceprint_model
CreateFaceprint
B16@?0@"HMIFaceprint"8
HMIFP.ck.u
HMIFP.ck.d
HMIFP.ck.mu
HMIFP.ck.fcu
modelUUID
faceCropUUID
Data
Model UUID
Face Crop UUID
<%0.3f %0.3f>
HMPMS.fce
Face Classification Enabled
com.apple.homeai.model.loader
/var/mobile/Library/Caches/com.apple.HomeAI/model_dir
[Model loading] failed to remove dir %@ err %@
[Model loading] failed to untar model asset into %@ err %@
faceCrops
authTag
internal
wrappedKey
1.2.840.113635.100.6.27.38
HMIFeedback Queue
HFFeedbackService
Cannot find camera profile.
Cannot find home for camera profile.
Cannot download clip asset.
Check network connectivity.
v24@?0@"HMCameraClipVideoAssetContext"8@"NSError"16
v16@?0Q8
v16@?0@"HMCameraClip"8
Cannot download clip for camera profile.
Ensure the clip belongs to the camera profile.
@16@?0@"HMCameraClipSignificantEvent"8
v24@?0@"HMCameraClip"8@"NSError"16
Unable to blur faces.
Unable to read the asset from disk.
Clip doesn't have a video track.
hkcvml-dev.apple.com
hkcvml.apple.com
https://%@/v2/clip-uuid/
feedback
%@.%@
json
application/json
Content-Type
v32@?0@"NSData"8@"NSURLResponse"16@"NSError"24
Invalid key size.
Cannot generate initialization vector.
Failed to encrypt data.
Failed to encrypt face crop data.
v24@?0@"NSURL"8@"NSError"16
v16@?0@"NSURL"8
HMIFC.pu
HMIFC.su
HMIFC.seu
HMIFC.fc
HMIFC.fp
HMIFC.c
HMIFC.ftc
HMIFC.f
Known
Uncertain
Source UUID
Session Entity UUID
Confidence
Familiarity
FaceCrop UUID
Faceprint UUID
FromTorsoClassification
@"NSUUID"16@?0@"HMIPerson"8
@"NSUUID"16@?0@"HMIPersonFaceCrop"8
v16@?0@"NSSet"8
@"NSUUID"16@?0@"HMIFaceCrop"8
Invalid person UUIDs
v32@?0@"HMIPerson"8@"NSSet"16^B24
B16@?0@"HMIPerson"8
Not implemented
Invalid persons, person already exists
v16@?0@"HMIPerson"8
Invalid faceCropUUIDs
B16@?0@"HMIPersonFaceCrop"8
Invalid persons, person UUID doesn't exists
Invalid personUUID
@"HMIPersonFaceCrop"16@?0@"HMIFaceCrop"8
B16@?0@"HMIFaceCrop"8
@"HMIPersonFaceCrop"16@?0@"HMIPersonFaceCrop"8
NOT IMPLEMENTED
@16@?0@"HMIPersonFaceCrop"8
SourceUUID:%@ HomeUUID:%@
v24@?0@"HMIVideoDecoder"8^{opaqueCMSampleBuffer=}16
Medium
High
%.4f
%.2f[%c]
HMIErrorDomain
Unexpected error
Failed to analyze
Failed to read fragment
Failed to verify fragment
Failed to transcode fragment
Video analyzer in error state
Model failed to load
Fragment is invalid
No pixel buffer
Unknown error code %ld
HMIErrorCodeUnexpectedError
HMIErrorCodeFailedToAnalyze
HMIErrorCodeFailedToReadFragment
HMIErrorCodeFailedToVerifyFragment
HMIErrorcodeFailedToTranscodeFragment
HMIErrorCodeAnalyzerInErrorState
HMIErrorCodeModelFailedToLoad
HMIErrorCodeFragmentIsInvalid
HMIPrivateErrorCodeNilPixelBuffer
HMIPrivateErrorCodeEmptyURL
HMIPrivateErrorCodeAVAssetReaderInitializationFailed
HMIPrivateErrorCodeFailedToLoadProperty
HMIPrivateErrorCodeAssetLoadCancelled
HMIPrivateErrorCodeReadingStartFailed
HMIPrivateErrorCodeNoTrackOutput
HMIPrivateErrorCodeSampleBufferUnavailable
HMIPrivateErrorCodeNoVideoTrackFound
HMIPrivateErrorCodeMultipleVideoTracksFound
HMIPrivateErrorInvalidPresentationTime
HMIPrivateErrorAVAssetReaderNotStarted
HMIPrivateErrorCodeSequentialIntegrityViolated
HMIPrivateErrorCodeAnalysisServiceNoConfiguration
HMIPrivateErrorCodeLoadingCoreMLModelFailed
HMIPrivateErrorCodeCoreMLPredictionFailed
HMIPrivateErrorCodeCoreMLOutputIncorrect
HMIPrivateErrorCodeCropAndResizeFailed
HMIPrivateErrorCodeCodecNotAvailable
HMIPrivateErrorCodeEncodingFailed
HMIPrivateErrorCodeInvalidVideoFrameFormatToSave
HMIPrivateErrorCodeScalerError
HMIPrivateErrorCodeFaceprintingFailed
HMIPrivateErrorCodeUpdatePersonsModelTaskFailed
HMIPrivateErrorCodeExternalPersonSourceDiskError
HMIPrivateErrorCodeLoadPersonsModelsFailed
HMIPrivateErrorCodeUpdatePersonsModelFailed
HMIPrivateErrorCodeRemovePersonsModelFailed
HMIPrivateErrorCodePersonsModelPredictionFailed
HMIPrivateErrorCodeNilDataSource
HMIPrivateErrorCodeUnknownTask
HMIPrivateErrorCodeHomePersonClusteringTaskFailed
HMIPrivateErrorCodeRemovePersonsModelTaskFailed
HMIPrivateErrorCodeFaceMisclassificationTaskFailed
HMIPrivateErrorCodePersonsModelsSummaryTaskFailed
HMIPrivateErrorCodeCleanupImpureHomePersonsOperationFailed
ERROR_%ld
fragments.count >= 1
[first isCombinableWithFragment:fragment]
v16@?0@"HMIVideoFragment"8
B28@?0I8@"NSData"12@"NSData"20
v12@?0I8
video/mp4
first
second
[%@]
Initialization Segment Data
Separable Segment Data
Sequence Numbers
Time Range
First Video Sample Byte Range
v32@?0@"HMIVideoAnalyzerEventPerson"8Q16^B24
v24@?0@"NSUUID"8@"HMIMutableCluster"16
(faceRecognition != nil) || (torsoRecognition != nil)
v16@?0@"NSUUID"8
v16@?0@"HMITorsoprint"8
@16@?0@"HMIFaceClassification"8
HMIASU.ck.ta
Torso Annotations
B16@?0@"HMITorsoAnnotation"8
@"HMITorsoAnnotation"16@?0@"HMIVideoAnalyzerEventFace"8
v16@?0@"HMIPoint"8
HMIPSUP.ck.p
HMIPSUP.ck.s
HMIP.ck.u
HMIP.ck.n
HMIP.ck.pl
personLinks
@"HMIPersonSourceUUIDPair"16@?0@"HMPersonLink"8
v16@?0@"<NSObject><NSCopying><NSSecureCoding>"8
data:;base64,%@
@16@?0@8
@24@?0@8@16
%.6f
data:;base64,
Invalid JSONObject class name: %@
Invalid JSONObject: %@
Cannot parse JSON data: %@
yyyy-MM-dd'T'HH:mm:ss.SSS'Z'
Error creating directory:%@ to save face classifications
faceprintUUID
%@-%03lu-%03u-%@-%@-%@
known
unknown
Error saving metadata to disk for face classification:%@
Error saving face crop image to disk for face classification:%@
HMITP.ck.u
HMITP.ck.d
HMITA.ck.fr
HMITA.ck.tps
HMITA.ck.tmv
faceRecognition
torsoprints
Torsoprints
TorsoModelVersion
HMIVideoEncoderWorkQueue
self.session == NULL
VTCompressionSessionCreate failed, err: %d
VTCompressionSessionPrepareToEncodeFrames failed, err: %d
Video encoding failed, err: %d
self.session
v24@?0i8I12^{opaqueCMSampleBuffer=}16
VTCompressionSessionEncodeFrameWithOutputHandler failed, err: %d
%@ is unavailable
@16@?0@"HMIFaceCrop"8
@16@?0@"HMIPerson"8
@16@?0@"HMFaceCrop"8
source != HMIPersonFaceCropSourceUnknown
HMPMS.ifple
HMPMS.sfce
Importing From Photo Library Enabled
Sharing Face Classifications Enabled
cameraManufacturer
cameraModel
recognitionType
face
torso
com.apple.HomeAI.PersonRecognitionEvent
@"NSMutableDictionary"8@?0
detectionScore
faceFilteredState
homeFamiliarity
v16@?0@"HMIFaceClassification"8
q24@?0@"NSNumber"8@"NSNumber"16
externalFamiliarity
sessionEntityAssignment
com.apple.HomeAI.FaceEvent
faceprintsClustered
clusters
personsCreated
faceprintsAssociated
faceprintingDuration
clusteringDuration
totalDuration
errorCode
errorDescription
com.apple.HomeAI.FaceClustering
v24@?0@"NSString"8@"NSNumber"16
v16@?0@"HMIPersonsModelSummary"8
@avg.self
externalLibraries
homeIdentities
averageExternalIdentities
averageHomeFaceCrops
averageExternalFaceCrops
homeToExternal
externalToExternal
com.apple.HomeAI.PersonsModels
@"NSDictionary"8@?0
B16@?0@"NSNumber"8
inclusion
exclusion
com.apple.HomeAI.MotionScore
zoneType
motionScore
com.apple.HomeAI.VideoAnalyzer.DidTerminate_v0
Fail
status
error
underlyingError
timeSinceAnalyzerStarted
com.apple.HomeAI.VideoAnalyzer.DidCreateTimelapseFragment_v0
bitrate
com.apple.HomeAI.VideoAnalyzer.DidAnalyzeFragment_v2
recognizeFaces
%@Trigger
%@Found
v32@?0@"NSString"8q16#24
motion
person
vehicle
package
transcode
analysisQuality
com.apple.HomeAI.VideoPackageAnalyzer.DidClassify_v0
com.apple.HomeAI.VideoPackageAnalyzer.DidReset_v0
interval
personsmodels
home
external
torso.bin
torso_to_facecrop.bin
torsoprinter_version.bin
user_defined_person_links.bin
v32@?0@"NSUUID"8@"HMIPersonsModel"16^B24
Error adding faceprints to model for personUUID: %@
v32@?0@"<NSObject><NSCopying><NSSecureCoding>"8@"NSArray"16^B24
Failed to remove stale Home VNPersonsModel with homeUUID: %@ sourceUUID: %@, error getting model storage path
Failed to remove stale Home VNPersonsModel with homeUUID: %@ sourceUUID: %@
Failed to persist VNPersonsModel for homeUUID: %@ sourceUUID: %@, error getting model storage path
Failed to persist VNPersonsModel for homeUUID: %@ sourceUUID: %@, path: %@
Failed to remove VNPersonsModel for homeUUID: %@ sourceUUID: %@, error getting model storage path
Failed to remove VNPersonsModel for homeUUID: %@ sourceUUID: %@, path: %@
Home persons model not found for homeUUID: %@
Failed to predict using VNPersonsModel for home persons model
Unable to get person model for homeUUID: %@ (self.personsModelsByHome is %@ nil)
Failed to predict using VNPersonsModel for homeUUID: %@ sourceUUID: %@
 not
v24@?0@"HMIPersonSourceUUIDPair"8^B16
v32@?0@"NSSet"8@"HMIPersonsModelPrediction"16^B24
q24@?0@"VNFaceObservation"8@"VNFaceObservation"16
v24@?0@"HMIFaceClassification"8^B16
@"VNFaceObservation"16@?0@"HMITorsoprint"8
v16@?0@"HMITorsoAnnotation"8
v16@?0@"HMIFaceCrop"8
Failed to predict using torso model for homeUUID: %@
@16@?0@"HMIPersonSourceUUIDPair"8
%@.bin
Directory to load/store persons models (%@) does not exist
Failed to enumerate persons models at path (%@)
\A[A-F0-9]{8}-[A-F0-9]{4}-[A-F0-9]{4}-[A-F0-9]{4}-[A-F0-9]{12}\.bin\Z
B16@?0@"NSURL"8
Failed to enumerate homes at path: %@
Directory to load/store home persons model (%@) contains multiples files: %@ there can only be one
Invalid file path in load model attempt: %@
Refusing to load %@ VNPersonsModel at path: %@
Failed to load VNPersonsModel at path: %@
@24@?0@"NSUUID"8@"HMIPersonsModel"16
v32@?0@"HMIPersonSourceUUIDPair"8@"NSSet"16^B24
v16@?0@"HMIDESLayerParameters"8
is_training
checkpoint
Loss/total_loss
B32@?0Q8@"<ETDataProvider>"16@"<ETTaskContext>"24
/tmp/train.espresso.net
Unknown reason.
Skipped
{code: %@, analysisFPS: %f, message: "%@"}
{code: %@, analysisFPS: %f}
v24@?0@"HMIExternalPersonManagerSettings"8@"NSError"16
@16@?0@"HMIVideoAnalyzer"8
@16@?0@"HMIVideoAnalyzerState"8
v16@?0@"HMIVideoAnalyzerConfiguration"8
v32@?0@"HMIVideoAnalyzer"8Q16^B24
Undefined
mediaserverd
homed
v16@?0@"HMIVideoAnalyzer"8
Scheduler state: 
usage: %@
, idle: %@
, %@: (%llu MB | %llu MB)
, mediaserverd: (%llu MB | %llu MB)
, homed: (%llu MB | %llu MB)
, thermalLevel: %lu
, build: %@
Release
, maxConcurrentAnalyzers: %lu
v32@?0@"NSString"8Q16^B24
usage
idle
footprint
maxFootprint
thermalLevel
build
analyzers
SignificantActivity.mlmodelc
HOMEAI_SIGNIFICANT_ACTIVITY_DETECTOR
v16@?0@"<TRINamespaceUpdateProtocol>"8
compiledModel
personThresholdHigh
personThresholdMedium
petThresholdHigh
petThresholdMedium
vehicleThresholdHigh
vehicleThresholdMedium
faceThreshold
descriptorA.length == descriptorB.length
cluster.objects.count > 0
faceObservations.count > 0
faceObservations.firstObject.faceprint != nil
faceprintLength == faceObservation.faceprint.lengthInBytes / sizeof(float)
Sample buffer has an invalid PTS.
randomUniform
value
{CGAffineTransform=dddddd}
probability
attributes
B16@?0@"HMICIFilterAttribute"8
HMIRotate
CIAffineTransform
inputAngle
inputTransform
v16@?0@"HMICIFilterAttribute"8
camera_recording
camera_recording_analyzer
camera_recording_analyzer_media
camera_recording_analyzer_scheduler
camera_recording_analyzer_scheduler_json
camera_recording_maintenance
Failed to load model at path: %@
HMIMLModel
/tmp/PackageAnalyzerReport-%@-%@.html
Package Analyzer
@"HMIVideoAnalyzerFrameResult"16@?0@"HMIVideoAnalyzerFrameResult"8
@"HMIVideoFrame"16@?0@"HMIVideoFrame"8
@"NSArray"16@?0@"HMIVideoAnalyzerFrameResult"8
Max Confidence Events
Frame Results
Thumbnails
Fragment
Configuration
HMICVFR.f
HMICVFR.roi
HMICVFR.ae
HMIOD.l
HMIOD.c
HMIOD.b.x
HMIOD.b.y
HMIOD.b.w
HMIOD.b.h
HMIOD.y.a
HMIOD.r.o
HMIP.x
HMIP.y
HMIAZ.p
HMIAZ.i
HMICameraVideoFrame does not support NSSecureCoding in the simulator
HKD://
analyzed-video-frames
 isInclusion:%d 
class-label
coordinates
overlap
activityZone
%@-%@-%@-%@.json
%@/%@
%@/activityzone-%@
B16@?0@"HMICameraActivityZone"8
filteringLevel
numDetectedObjects
com.apple.HomeAI.ActivityZone
noFiltering
resize_0
resize_10
resize_20
resize_26
resize_36
IFrameOnly
Full
Detector
Thumbnail Interval
Thumbnail Height
Timelapse Interval
Timelapse Preferred Fragment Duration
Max Fragment Duration
Max Fragment Analysis Duration
Decode Mode
Transcode
Transcode Codec
Passthrough Audio
Redact frames
Min Frame Quality
Min Frame Scale
Camera
Home UUID
Package Classifier Mode
Analysis FPS
minFrameQuality > 0 && minFrameQuality <= 1
minFrameScale > 0 && minFrameScale <= 1
analysisFPS > 0
Event Triggers
Recognize Faces
Activity Zone Count
q24@?0@"HMIVideoAnalyzerEvent"8@"HMIVideoAnalyzerEvent"16
B16@?0@"HMIStationaryObject"8
HMIVAEP.f
HMIVAEP.t
HMIVAEP.ibbe
Is Bounding Box & Confidence Estimated
Face
Torso
%@ %@ %@
com.apple.homed
syntheticEvents
personDetected
petDetected
vehicleDetected
packageDetected
analysisQOS
systemResourceUsageLevel
motionDetector
frameSelectorSampleRateMultiplier
frameSelectorTargetInterval
processingDevice
confidenceThresholdPersonHigh
confidenceThresholdPetHigh
confidenceThresholdVehicleHigh
confidenceThresholdFaceHigh
confidenceThresholdTorsoHigh
confidenceThresholdPackageHigh
confidenceThresholdPersonMedium
confidenceThresholdPetMedium
confidenceThresholdVehicleMedium
confidenceThresholdFaceMedium
confidenceThresholdTorsoMedium
confidenceThresholdPackageMedium
confidenceThresholdPackageClassifierMedium
confidenceThresholdPackageClassifierHigh
modelTimeout
uploadVideoAnalysisEvent
saveVideoFragmentResultHTML
assetReaderMaximizePowerEfficiency
assetReaderForceBGRA
analysisEnableTranscodeFragment
realTimeEncoding
espressoLowPriority
maxConcurrentAnalyzers
maxAnalysisFPS
opticalFlowLowPriority
opticalFlowBackgroundProcessing
saveDESRecords
skipMinDESRecordCount
DESSkipTraining
saveTrainedModel
DESSkipTrainingScalar
DESSkipPrivatize
enableDASTestConfiguration
personsModelStoragePath
personDataSourceDiskStoragePath
personsModelClassificationThresholdKnown
torsoPersonsModelClassificationThresholdKnown
faceVIPThresholdForTorsoAnnotation
personsModelClassificationThresholdUnknown
saveFaceCropsToDisk
showROI
useDevelopmentFeedbackService
eventTriggers
logHumanFriendlySchedulerState
schedulerStateLogFrequency
enableTorsoRecognition
enableSignposts
fragmentDiskBufferSize
logOtherProcessMemorySchedulerState
videoFrameSelectorMaxCandidates
useHEVC
taskServiceRunLocally
restartDecoderIfFormatChanges
user-interactive
user-initiated
unspecified
default
utility
background
Only NSNumber and NSString properties are supported.
v24@?0@"HMIVideoAssetWriter"8@"NSData"16
v32@?0@"HMIVideoAssetWriter"8@"NSData"16@"AVAssetSegmentReport"24
Encoder Queue
v24@?0@"HMIVideoEncoder"8^{opaqueCMSampleBuffer=}16
v24@?0@"HMIVideoFrameSampler"8^{opaqueCMSampleBuffer=}16
v32@?0@"HMIVideoFrameSampler"8@"HMIVideoEncoder"16^{opaqueCMSampleBuffer=}24
@"NSNumber"24@?0@"HMIVideoAnalyzerEvent"8@"NSNumber"16
FFArchive
yyyy-MM-dd
2021-05-15
q24@?0@"VNCluster"8@"VNCluster"16
v16@?0@"VNCluster"8
@"VNFaceObservation"16@?0@"NSNumber"8
B16@?0@"VNCluster"8
q24@?0@"HMIPersonFaceCrop"8@"HMIPersonFaceCrop"16
homePersonsAndFaceCrops
yyyy-MM-dd'T'HH-mm-ss
%@_%@.plist
@"NSUUID"16@?0@"HMIFaceprint"8
Fetch persons failed
CleanImpureHomePersonsOperation encountered failures
com.apple.HomeAI.%@%@%@.%tu
homeai: %@
Vision
Faceprints
Clusters
Persons
Associated
Faceprinting Duration
Clustering Duration
Total Duration
B16@?0@"HMIPersonSourceUUIDPair"8
v32@?0@"VNCluster"8@"NSUUID"16^B24
Error on dispatch_group_wait (associateFaceCrops)
Error associating face crops for %lu person%@: (
 ...
v16@?0@"HMIPersonFaceCrop"8
@"NSUUID"16@?0@"VNFaceObservation"8
@"NSUUID"16@?0@"NSUUID"8
v32@?0@"NSNumber"8@16^B24
HMICameraVideoFrameAnalyzerSignificantActivity
@"HMIFaceClassification"16@?0@"HMIPersonsModelPrediction"8
@"HMIVideoAnalyzerEvent"16@?0@"HMIVideoAnalyzerEvent"8
none
@"NSString"16@?0@"NSObject"8
high
medium
Unexpected event %@.
ClassifyFaceEvent
ClassifyTorsoEvent
HMIDESBT.u
HMIDESBackgroundTask
scale > 0 && scale <= 1
quality > 0 && quality <= 1
HMIVideoFrame failed to create compressed representation.
]1337;File=inline=1;preserveAspectRatio=1:%@
Data (JPEG)
CVPixelBuffer
Presentation Time Stamp
Size
Store
HMIVideoFrame does not support NSSecureCoding in the simulator.
/scratch.data
/Library/Spotlight/Backup/temp_%lu.dat
#EXTM3U
#EXT-X-VERSION:7
#EXT-X-TARGETDURATION:%.6f
#EXT-X-PLAYLIST-TYPE:VOD
#EXT-X-INDEPENDENT-SEGMENTS
#EXT-X-I-FRAMES-ONLY
#EXT-X-KEY:METHOD=AES-256-GCM,URI="%@"
#EXT-X-MAP:URI="%@"
#EXTINF:%.5f
#EXT-X-BYTERANGE:%lu@%lu
#EXT-X-ENDLIST
Bounding Box
P(%@|[%@])=%@
Package
Person
Vehicle
Motion
Event
event
#D62728
#2CA02C
#1F77B4
#9467BD
#FF7F0E
#8C564B
#7F7F7F
@24@?0#8@"NSString"16
HMITC.su
HMITC.pu
HMITC.conf
%.4lf
HMIVideoDecoderWorkQueue
Format description is missing.
Cannot accept format description.
Cannot create reorder buffer, err: %d.
Cannot create decoder.
Cannot decode frame, err: %d.
Cannot reorder frames.
Decoded sample has an invalid PTS.
HMIVideoAssetWriter
HMIVideoAssetWriterOutput
UTType class is not available.
HMICMSampleBufferIsAudio(sbuf)
Underlying asset writer has failed.
Couldn't append sample buffer because, exception %@
Failed to flush segment.
v32@?0@"<NSObject><NSCopying><NSSecureCoding>"8Q16^B24
@"NSDictionary"24@?0@"NSUUID"8@"HMIPersonsModel"16
v16@?0@"HMIPersonSourceUUIDPair"8
v32@?0@"NSUUID"8@"NSSet"16^B24
v32@?0@"NSUUID"8@"NSDictionary"16^B24
v24@?0@"VNFaceObservation"8^B16
B16@?0@"HMIMotionDetection"8
Sparse Optical Flow
%@ - %f
sparse
q24@?0@"HMIVideoFrameSelectorFrameCandidate"8@"HMIVideoFrameSelectorFrameCandidate"16
v16@?0@"HMIVideoFrameSelectorFrameCandidate"8
monitored
analysisFPS
timeSinceLastFragmentWasReceived
bufferFillRatio
bufferSize
delay
numDecodedSamples
numDidAnalyzeFrames
numDidAnalyzePackages
numDidCreateTimelapseFragments
averageAnalysisTime
decodeMode
transcodeCodecType
encode
encoder
activityZones
Name           
AFPS
Time
Last
Buffer               
Delay
     Decode:PTS
FR, FRAG
FR Time
Enc, Lapse:FRAG
Faces
Zones
Triggers
] %5ld KB
%.1f
%4ld:%.1f
%ld, %ld
%@%@%@ %@, %@:%ld
manufacturer
camera
Session has not received any new data for over 60 seconds.
Session Check
HMIFR.c
HMIFR.fc
HMIFR.fp
HMIFR.fqs
HMIFR.sea
HMIFR.seu
HMIFR.leus
TransitionMatrix
Joint
Face Classifications
Face Quality Score
selector
arguments
configuration
HMIVideoAnalyzer does not support remote analysis.
@"HMIVideoAnalyzer"16@?0@"HMIVideoAnalyzerConfiguration"8
VideoAnalyzerReport %@ %@
/tmp/%@.plist
fragmentResult
v16@?0@"HMIVideoAnalyzerEventFace"8
HMIVideoAnalyzerServer
HMIVideoAnalyzerServer - Input
HMIVideoAnalyzerServer - Encoder
dynamicConfiguration
Video fragment duration: %fs is greater than the max fragment duration value: %fs
%@_%@_%@
UnknownManufacturer
UnknownModel
UnknownFirmware
v20@?0@"NSString"8B16
YYYY-MM-dd-HH-mm-ss
sanitized.mp4
self.assetWriter == nil
self.timelapseOutputVideoFormat
 Timelapse
_encode
!self.encoder
self.inputVideoFormat
!self.timelapseEncoder
self.assetWriter
self.timelapseAssetWriter
v16@?0@"HMIVideoAnalyzerResultFilter"8
B16@?0@"AVAssetSegmentTrackReport"8
Analyzer is in full bypass mode.
Analyzer is in partial bypass mode, only IFrames are decoded.
/tmp/com.apple.HomeAI/familiar-face-data
Error initializing with home UUID: %@ source UUID:%@
Error enumerating persons
Error loading person from JSON
Error enumerating face crops for person UUID:%@
Error loading face crop from JSON
Error loading face crop image
HMIVAET.ro
HMIVAET.tr
Torso Roll
Torso Recognition
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-231.5.1/OpenCV/src/core/datastructs.cpp
cvReleaseMemStorage
cvSaveMemStoragePos
cvRestoreMemStoragePos
NULL storage pointer
cvMemStorageAlloc
Too large memory block is requested
datastructs.cpp
storage->free_space % CV_STRUCT_ALIGN == 0
requested size is negative or too big
(size_t)ptr % CV_STRUCT_ALIGN == 0
cvCreateSeq
Specified element size doesn't match to the size of the specified element type (try to use 0 for element type)
cvSetSeqBlockSize
Storage block size is too small to fit the sequence elements
cvCvtSeqToArray
cvStartReadSeq
cvChangeSeqBlock
cvGetSeqReaderPos
cvSetSeqReaderPos
cvSeqPush
ptr + elem_size <= seq->block_max
block->start_index > 0
NULL sequence pointer
cvSeqPushMulti
number of removed elements is negative
cvSeqPopMulti
delta > 0
cvClearSeq
Invalid sequence header
cvSeqSlice
Bad sequence slice
Bad input sequence
cvSeqSort
Null compare function
cvCreateSet
cvSetAdd
count <= CV_SET_ELEM_IDX_MASK+1
cvCreateGraph
cvGraphAddVtx
cvFindGraphEdgeByPtr
ofs == 1 || start_vtx == edge->vtx[0]
graph pointer is NULL
cvGraphAddEdgeByPtr
vertex pointers coinside (or set to NULL)
edge->flags >= 0
Invalid graph pointer
cvCloneGraph
cvInitTreeNodeIterator
cvNextTreeNode
icvInitMemStorage
icvDestroyMemStorage
icvGoNextMemBlock
parent->bottom == block
icvGrowSeq
The sequence has NULL storage pointer
storage->free_space >= delta
block->count % seq->elem_size == 0 && block->count > 0
seq->first->start_index == 0
icvFreeSeqBlock
(in_front_of ? block : block->prev)->count == 0
seq->ptr == block->data
block->count > 0 && block->count % seq->elem_size == 0
Unknown/unsupported border type
borderInterpolate
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-231.5.1/OpenCV/src/imgproc/filter.cpp
columnBorderType != BORDER_WRAP
!rowFilter.empty() && !columnFilter.empty()
bufType == srcType
0 <= anchor.x && anchor.x < ksize.width && 0 <= anchor.y && anchor.y < ksize.height
roi.x >= 0 && roi.y >= 0 && roi.width >= 0 && roi.height >= 0 && roi.x + roi.width <= wholeSize.width && roi.y + roi.height <= wholeSize.height
start
srcRoi.x >= 0 && srcRoi.y >= 0 && srcRoi.width >= 0 && srcRoi.height >= 0 && srcRoi.x + srcRoi.width <= src.cols && srcRoi.y + srcRoi.height <= src.rows
wholeSize.width > 0 && wholeSize.height > 0
proceed
src && dst && count > 0
srcY >= startY
dstY <= roi.height
src.type() == srcType && dst.type() == dstType
apply
dstOfs.x >= 0 && dstOfs.y >= 0 && dstOfs.x + srcRoi.width <= dst.cols && dstOfs.y + srcRoi.height <= dst.rows
_kernel.channels() == 1
getKernelType
cn == CV_MAT_CN(bufType) && ddepth >= std::max(sdepth, CV_32S) && kernel.type() == ddepth
getLinearRowFilter
Unsupported combination of source format (=%d), and buffer format (=%d)
cn == CV_MAT_CN(bufType) && sdepth >= std::max(ddepth, CV_32S) && kernel.type() == sdepth
getLinearColumnFilter
Unsupported combination of buffer format (=%d), and destination format (=%d)
cn == CV_MAT_CN(_dstType)
createSeparableLinearFilter
ktype == CV_8U || ktype == CV_32S || ktype == CV_32F || ktype == CV_64F
preprocess2DKernel
cn == CV_MAT_CN(dstType) && ddepth >= sdepth
getLinearFilter
Unsupported combination of source format (=%d), and destination format (=%d)
createLinearFilter
anchor.inside(Rect(0, 0, ksize.width, ksize.height))
normalizeAnchor
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-231.5.1/OpenCV/src/imgproc/precomp.hpp
(symmetryType & (KERNEL_SYMMETRICAL | KERNEL_ASYMMETRICAL)) != 0 && this->ksize <= 5
SymmRowSmallFilter
kernel.type() == DataType<DT>::type && (kernel.rows == 1 || kernel.cols == 1)
RowFilter
kernel.type() == DataType<ST>::type && (kernel.rows == 1 || kernel.cols == 1)
ColumnFilter
this->ksize == 3
SymmColumnSmallFilter
(symmetryType & (KERNEL_SYMMETRICAL | KERNEL_ASYMMETRICAL)) != 0
SymmColumnFilter
_kernel.type() == DataType<KT>::type
Filter2D
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-231.5.1/OpenCV/src/core/alloc.cpp
Failed to allocate %lu bytes
OutOfMemoryError
borderType != BORDER_CONSTANT
pyrDown
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-231.5.1/OpenCV/src/imgproc/pyramids.cpp
!_src.empty()
pyrDown_
std::abs(dsize.width*2 - ssize.width) <= 2 && std::abs(dsize.height*2 - ssize.height) <= 2
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-231.5.1/OpenCV/src/core/opengl_interop_deprecated.cpp
GlBuffer
GlTexture
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-231.5.1/OpenCV/src/core/persistence.cpp
Invalid pointer to file storage
The node is neither a map nor an empty collection
cvGetFileNodeByName
Null element name
cvStartWriteStruct
The file storage is opened for reading
cvEndWriteStruct
cvWriteInt
cvWriteString
cvWriteRawData
Negative number of elements
Null data pointer
persistence.cpp
cvStartReadRawData
Null pointer to source file node or reader
The file node should be a numerical scalar or a sequence
cvReadRawDataSlice
Null pointer to reader or destination array
The readed sequence is a scalar, thus len must be 1
The sequence element is not a numerical scalar
The sequence slice does not fit an integer number of records
Null pointers to source file node or destination array
cvReadRawData
opencv-sequence
opencv-sequence-tree
opencv-graph
opencv-sparse-matrix
opencv-image
opencv-matrix
opencv-nd-matrix
Invalid type info
cvRegisterType
Some of required function pointers (is_instance, release, read or write) are NULL
Type name should start with a letter or _
Type name should contain only letters, digits, - and _
NULL double pointer
cvRead
The node does not represent a user object (unknown type?)
The storage is not opened
icvPuts
An attempt to add element without a key to a map, or add element with key to sequence
icvXMLWriteTag
A single _ is a reserved tag name
Closing tag should not include any attributes
Key should start with a letter or _
Key name may only contain alphanumeric characters [a-zA-Z0-9], '-' and '_'
icvDecodeFormat
fmt_pairs != 0 && max_len > 0
Invalid data type specification
Too long data type specification
%.8e
-.Inf
.Inf
%.16e
elements with keys can not be written to sequence
icvXMLWriteScalar
icvYMLWrite
The key is an empty
The key is too long
Key must start with a letter or _
Key names may only contain alphanumeric characters [a-zA-Z0-9], '-', '_' and ' '
icvReleaseSeq
flags
count
Some of essential sequence attributes are absent
icvReadSeq
The sequence flags are invalid
curve
closed
hole
untyped
header_dt
header_user_data
One of "header_dt" and "header_user_data" is there, while the other is not
rect
origin
Only one of "header_user_data", "rect" and "origin" tags may occur
The image data is not found in file storage
The number of stored elements does not match to "count"
Too complex format for the matrix
icvDecodeSimpleFormat
recursive
false
False
FALSE
icvWriteSeqTree
CV_IS_SEQ( seq )
sequences
icvWriteSeq
level
 untyped
The size of element calculated from "dt" and the elem_size do not match
icvGetFormat
Size of sequence element (elem_size) is inconsistent with seq->flags
%d%c
The size of header calculated from "header_dt" is greater than header_size
icvWriteHeaderData
opencv-sequence-tree instance should contain a field "sequences" that should be a sequence
icvReadSeqTree
All the sequence tree nodes should contain "level" field
level == prev_level + 1
icvReleaseGraph
vertex_dt
edge_dt
vertex_count
edge_count
Some of essential graph attributes are absent
icvReadGraph
oriented
Graph edges should start with 2 integers and a float
%df%s
vertices
edges
No edges data
No vertices data
Some of stored vertex indices are out of range
Duplicated edge has occured
cvAlignPtr
(align & (align-1)) == 0
icvWriteGraph
CV_IS_GRAPH(graph)
2if%s
sizes
Some of essential matrix attributes are absent
icvReadSparseMat
Could not determine sparse matrix dimensionality
The matrix data is not found in file storage
Sparse matrix data is corrupted
icvWriteSparseMat
CV_IS_SPARSE_MAT(mat)
(reader).seq->elem_size == sizeof(idx)
k < dims
Some of essential image attributes are absent
icvReadImage
layout
interleaved
Only interleaved images can be read
The matrix size does not match to the number of stored elements
icvWriteImage
CV_IS_IMAGE(image)
Images with planar data layout are not supported
top-left
bottom-left
planar
rows
cols
icvReadMat
icvWriteMat
CV_IS_MAT_HDR_Z(mat)
icvReadMatND
Could not determine the matrix dimensionality
icvWriteMatND
CV_IS_MATND_HDR(mat)
type == CV_32FC1 || type == CV_32FC2 || type == CV_64FC1 || type == CV_64FC2
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-231.5.1/OpenCV/src/core/dxt.cpp
This mode (using nonzero_rows with a single-column matrix) breaks the function's logic, so it is prohibited.
For fast convolution/correlation use 2-column matrix or single-row matrix instead
dxt.cpp
!inv
type == srcB.type() && srcA.size() == srcB.size()
mulSpectrums
(flags & DFT_NO_PERMUTE) == 0
(unsigned)k0 < (unsigned)n && (unsigned)k1 < (unsigned)n
factors[0] == factors[nf-1]
(unsigned)j < (unsigned)n2
(unsigned)j < (unsigned)n
RealDFT
tab_size == n
CCSIDFT
src != dst
DFTInit
nf < 34
elem_size == sizeof(Complex<float>)
channels() == CV_MAT_CN(dtype)
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-231.5.1/OpenCV/src/core/copy.cpp
mask.depth() == CV_8U && (mcn == 1 || mcn == cn)
size() == mask.size()
checkScalar(value, type(), _value.kind(), _InputArray::MAT )
setTo
mask.empty() || mask.type() == CV_8U
repeat
ny > 0 && nx > 0
maskarr == 0
cvCopy
src.depth() == dst.depth() && src.size == dst.size
(coi1 != 0 || src.channels() == 1) && (coi2 != 0 || dst.channels() == 1)
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-231.5.1/OpenCV/src/core/opengl_interop.cpp
The library is compiled without OpenGL support
throw_nogl
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-231.5.1/OpenCV/src/imgproc/thresh.cpp
Unknown threshold type
thresh_8u
thresh_16s
thresh_32f
img.depth() == CV_8U && winSize.width > 2 && winSize.height > 2
buildOpticalFlowPyramid
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-231.5.1/OpenCV/src/video/lkpyramid.cpp
maxLevel >= 0 && winSize.width > 2 && winSize.height > 2
calcOpticalFlowPyrLK
(npoints = prevPtsMat.checkVector(2, CV_32F, true)) >= 0
nextPtsMat.checkVector(2, CV_32F, true) == npoints
statusMat.isContinuous()
errMat.isContinuous()
levels1 >= 0
ofs.x >= winSize.width && ofs.y >= winSize.height && ofs.x + prevPyr[lvlStep1].cols + winSize.width <= fullSize.width && ofs.y + prevPyr[lvlStep1].rows + winSize.height <= fullSize.height
levels2 >= 0
ofs.x >= winSize.width && ofs.y >= winSize.height && ofs.x + nextPyr[lvlStep2].cols + winSize.width <= fullSize.width && ofs.y + nextPyr[lvlStep2].rows + winSize.height <= fullSize.height
prevPyr[level * lvlStep1].size() == nextPyr[level * lvlStep2].size()
prevPyr[level * lvlStep1].type() == nextPyr[level * lvlStep2].type()
depth == CV_8U
calcSharrDeriv
cvAlign
internal.hpp
(align & (align-1)) == 0 && size < INT_MAX
scn == 1
convertAndUnrollScalar
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-231.5.1/OpenCV/src/core/arithm.cpp
op == CMP_LT || op == CMP_LE || op == CMP_EQ || op == CMP_NE || op == CMP_GE || op == CMP_GT
compare
The operation is neither 'array op array' (where arrays have the same size and the same type), nor 'array op scalar', nor 'scalar op array'
The operation is neither 'array op array' (where arrays have the same size and type), nor 'array op scalar', nor 'scalar op array'
binary_op
(mask.type() == CV_8UC1 || mask.type() == CV_8SC1)
mask.size == src1.size
The operation is neither 'array op array' (where arrays have the same size and the same number of channels), nor 'array op scalar', nor 'scalar op array'
arithm_op
src2.type() == CV_64F && (src2.rows == 4 || src2.rows == 1)
When the input arrays in add/subtract/multiply/divide functions have different types, the output array type must be explicitly specified
img.dims <= 2 && templ.dims <= 2 && corr.dims <= 2
crossCorr
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-231.5.1/OpenCV/src/imgproc/templmatch.cpp
depth == tdepth || tdepth == CV_32F
corrsize.height <= img.rows + templ.rows - 1 && corrsize.width <= img.cols + templ.cols - 1
ccn == 1 || delta == 0
the input arrays are too big
CV_MAT_CN(sumType) == CV_MAT_CN(srcType)
getRowSumFilter
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-231.5.1/OpenCV/src/imgproc/smooth.cpp
CV_MAT_CN(sumType) == CV_MAT_CN(dstType)
getColumnSumFilter
Unsupported combination of sum format (=%d), and destination format (=%d)
sumCount == ksize-1
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-231.5.1/OpenCV/src/imgproc/utils.cpp
top >= 0 && bottom >= 0 && left >= 0 && right >= 0
copyMakeBorder
value[0] == value[1] && value[0] == value[2] && value[0] == value[3]
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-231.5.1/OpenCV/src/imgproc/deriv.cpp
ktype == CV_32F || ktype == CV_64F
getScharrKernels
dx >= 0 && dy >= 0 && dx+dy == 1
getSobelKernels
The kernel size must be odd and not larger than 31
dx >= 0 && dy >= 0 && dx+dy > 0
ksize > order
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-231.5.1/OpenCV/src/core/convert.cpp
src && nsrcs > 0 && dst && ndsts > 0 && fromTo && npairs > 0
mixChannels
j < nsrcs && src[j].depth() == depth
i1 >= 0 && j < ndsts && dst[j].depth() == depth
type == B.type() && (type == CV_32FC1 || type == CV_64FC1 || type == CV_32FC2 || type == CV_64FC2)
gemm
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-231.5.1/OpenCV/src/core/matmul.cpp
a_size.width == len
a_size.height == len
C.type() == type && (((flags&GEMM_3_T) == 0 && C.rows == d_size.height && C.cols == d_size.width) || ((flags&GEMM_3_T) != 0 && C.rows == d_size.width && C.cols == d_size.height))
type == CV_64FC2
src1.type() == src2.type()
scaleAdd
src.channels() == 1
mulTransposed
delta.channels() == 1 && (delta.rows == src.rows || delta.rows == 1) && (delta.cols == src.cols || delta.cols == 1)
operator()
GEMM_TransposeBlock
matmul.cpp
MulTransposedR
delta_cols == 1
cn <= 4 && func != 0
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-231.5.1/OpenCV/src/core/stat.cpp
src.channels() == 1 && func != 0
countNonZero
mean
(cn == 1 && (mask.empty() || mask.type() == CV_8U)) || (cn >= 1 && mask.empty() && !minIdx && !maxIdx)
minMaxIdx
img.dims <= 2
src.type() == CV_8UC1
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-231.5.1/OpenCV/src/core/lapack.cpp
type == CV_32F || type == CV_64F
invert
m == n
method == DECOMP_LU || method == DECOMP_CHOLESKY
lapack.cpp
n == 1
type == _src2.type() && (type == CV_32F || type == CV_64F)
solve
(method != DECOMP_LU && method != DECOMP_CHOLESKY) || is_normal || src.rows == src.cols
src.rows == 1
The function can not solve under-determined linear systems
src.rows == src.cols
eigen
w.type() == u.type() && u.type() == vt.type() && u.data && vt.data && w.data
backSubst
u.cols >= nm && vt.rows >= nm && (w.size() == Size(nm, 1) || w.size() == Size(1, nm) || w.size() == Size(vt.rows, u.cols))
rhs.data == 0 || (rhs.type() == type && rhs.rows == m)
_SVDcompute
0 <= d && d <= CV_MAX_DIM && _sizes
create
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-231.5.1/OpenCV/src/core/matrix.cpp
step[dims-1] == (size_t)CV_ELEM_SIZE(flags)
m.dims >= 2
0 <= _rowRange.start && _rowRange.start <= _rowRange.end && _rowRange.end <= m.rows
0 <= _colRange.start && _colRange.start <= _colRange.end && _colRange.end <= m.cols
m.dims <= 2
0 <= roi.x && 0 <= roi.width && roi.x + roi.width <= m.cols && 0 <= roi.y && 0 <= roi.height && roi.y + roi.height <= m.rows
ranges
r == Range::all() || (0 <= r.start && r.start < r.end && r.end <= m.size[i])
dims <= 2
diag
img->dataOrder == IPL_DATA_ORDER_PIXEL
img->dataOrder == IPL_DATA_ORDER_PIXEL || img->roi->coi != 0
(int)nelems >= 0
reserve
resize
COI is not supported by the function
cvarrToMat
seq->total > 0 && CV_ELEM_SIZE(seq->flags) == seq->elem_size
Unknown array type
dims <= 2 && step[0] > 0
locateROI
adjustROI
reshape
The matrix is not continuous, thus its number of rows can not be changed
Bad new number of rows
The total number of matrix elements is not divisible by the new number of rows
The total width is not divisible by the new number of channels
cn <= 4
scalarToRawData
i < 0
getMat
0 <= i && i < (int)vv.size()
This method is not implemented for oclMat yet
k == STD_VECTOR_MAT
0 <= i && i < (int)v.size()
getMatVector
This function in deprecated, do not use it
getGlBuffer
getGlTexture
k == GPU_MAT
getGpuMat
size
i < (int)vv.size()
total
empty
!fixedSize() || ((Mat*)obj)->size.operator()() == _sz
!fixedType() || ((Mat*)obj)->type() == mtype
!fixedSize() || ((gpu::GpuMat*)obj)->size() == _sz
!fixedType() || ((gpu::GpuMat*)obj)->type() == mtype
!fixedSize() || ((ogl::Buffer*)obj)->size() == _sz
!fixedType() || ((ogl::Buffer*)obj)->type() == mtype
!fixedSize() || ((Mat*)obj)->size.operator()() == Size(cols, rows)
!fixedSize() || ((gpu::GpuMat*)obj)->size() == Size(cols, rows)
!fixedSize() || ((ogl::Buffer*)obj)->size() == Size(cols, rows)
!fixedType() && !fixedSize()
CV_MAT_TYPE(mtype) == m.type()
m.dims == dims
m.size[j] == sizes[j]
mtype == type0 || (CV_MAT_CN(mtype) == 1 && ((1 << type0) & fixedDepthMask) != 0)
dims == 2 && ((sizes[0] == sz.height && sizes[1] == sz.width) || (allowTransposed && sizes[0] == sz.width && sizes[1] == sz.height))
dims == 2 && (sizes[0] == 1 || sizes[1] == 1 || sizes[0]*sizes[1] == 0)
!fixedSize() || len == vv.size()
mtype == type0 || (CV_MAT_CN(mtype) == CV_MAT_CN(type0) && ((1 << type0) & fixedDepthMask) != 0)
!fixedSize() || len == ((vector<uchar>*)v)->size() / esz
Vectors with element size %d are not supported. Please, modify OutputArray::create()
create() called for the missing output array
!fixedSize() || len == len0
v[j].empty()
i < (int)v.size()
!fixedType() || (CV_MAT_CN(mtype) == m.channels() && ((1 << CV_MAT_TYPE(flags)) & fixedDepthMask) != 0)
!fixedSize()
release
clear
k == MAT
getMatRef
setIdentity
src.dims <= 2 && esz <= (size_t)32
transpose
src.size() == dst.size() && (src.cols == 1 || src.rows == 1)
func != 0
m.dims <= 2 && m.rows == m.cols
completeSymm
src.dims <= 2
src.channels() == dst.channels()
_arrays && (_ptrs || _planes)
init
narrays <= 1000
arrays[i] != 0
A.size == arrays[i0]->size
A.step[d-1] == A.elemSize()
copyTo
convertTo
minMaxLoc
0 <= _dims && _dims <= CV_MAX_DIM
setSize
s >= 0
%s:%d: error: (%d) %s in function %s
%s:%d: error: (%d) %s
OpenCV Error: %s (%s) in %s, file %s, line %d
No Error
Backtrace
Unspecified error
Internal error
Insufficient memory
Bad argument
Iterations do not converge
Autotrace call
Incorrect size of input array
Null pointer
Division by zero occured
Image step is wrong
Inplace operation is not supported
Requested object was not found
Input image depth is not supported by function
Formats of input arguments do not match
Sizes of input arguments do not match
One of arguments' values is out of range
Unsupported format or combination of formats
Input COI is not supported
Bad number of channels
Bad flag (parameter or structure field)
Bad parameter of type CvPoint
Bad type of mask argument
Parsing error
The function/feature is not implemented
Memory block has been corrupted
Assertion failed
No GPU support
Gpu API call
No OpenGL support
OpenGL API call
Unknown %s code %d
module != 0 && module->name != 0 && module->version != 0
cvRegisterModule
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-231.5.1/OpenCV/src/core/system.cpp
cxcore
2.4.13.6
unknown function
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-231.5.1/OpenCV/include/opencv2/dynamicuda/dynamicuda.hpp
copy
copyWithMask
convert
mallocPitch
The library is compiled without CUDA support
qualityLevel > 0 && minDistance >= 0 && maxCorners >= 0
goodFeaturesToTrack
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-231.5.1/OpenCV/src/imgproc/featureselect.cpp
mask.empty() || (mask.type() == CV_8UC1 && mask.size() == image.size())
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-231.5.1/OpenCV/src/core/array.cpp
Non-positive width or height
cvCreateMatHeader
Invalid matrix type
cvInitMatHeader
Non-positive cols or rows
cvReleaseMat
Bad CvMat header
cvCloneMat
NULL matrix header pointer
cvInitMatNDHeader
invalid array data type
NULL <sizes> pointer
non-positive or too large number of dimensions
one of dimesion sizes is non-positive
The array is too big
cvCreateMatNDHeader
Bad CvMatND header
cvCloneMatND
src->dims <= CV_MAX_DIM
_dst.data == data0
Incorrect number of arrays
cvInitNArrayIterator
Some of required array pointers is NULL
Iterator pointer is NULL
COI set is not allowed here
Number of dimensions is the same for all arrays
Data type is not the same for all arrays
Number of channels is not the same for all arrays
Depth is not the same for all arrays
Mask should have 8uC1 or 8sC1 data type
Dimension sizes are the same for all arrays
cvNextNArraySlice
array.cpp
iterator != 0
cvCreateSparseMat
bad number of dimensions
cvReleaseSparseMat
Invalid sparse array header
cvCloneSparseMat
Invalid sparse matrix header
cvInitSparseMatIterator
NULL iterator pointer
Data is already allocated
cvCreateData
unrecognized or unsupported array type
cvReleaseData
Only continuous nD arrays are supported here
cvGetElemType
cvGetDims
Array should be CvMat or IplImage
cvGetSize
index is out of range
cvPtr2D
COI must be non-null in case of planar images
NULL pointer to indices
cvPtrND
NULL array pointer is passed
cvGetMat
The matrix has NULL data pointer
The image has NULL data pointer
Images with planar data layout should be used with COI selected
The image is interleaved and has over CV_CN_MAX channels
Pixel order should be used with coi == 0
Input array has NULL data pointer
Unrecognized or unsupported array type
cvCreateImage
null pointer to header
cvInitImageHeader
Bad input roi
Unsupported format
Bad input origin
Bad input align
cvReleaseImageHeader
cvReleaseImage
cvSetImageROI
rect.width >= 0 && rect.height >= 0 && rect.x < image->width && rect.y < image->height && rect.x + rect.width >= (int)(rect.width > 0) && rect.y + rect.height >= (int)(rect.height > 0)
cvSetImageCOI
cvGetImageCOI
Bad image header
cvCloneImage
cvGetMatND
icvGetNodePtr
CV_IS_SPARSE_MAT( mat )
One of indices is out of range
(newsize & (newsize - 1)) == 0
GRAY
BGRA
op == MORPH_ERODE || op == MORPH_DILATE
getMorphologyRowFilter
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-231.5.1/OpenCV/src/imgproc/morph.cpp
Unsupported data type (=%d)
getMorphologyColumnFilter
getMorphologyFilter
depth == CV_8U || depth == CV_16U || depth == CV_16S || depth == CV_32F || depth == CV_64F
createMorphologyFilter
shape == MORPH_RECT || shape == MORPH_CROSS || shape == MORPH_ELLIPSE
getStructuringElement
morphOp
_kernel.type() == CV_8U
MorphFilter
src.type() == CV_8UC1 || src.type() == CV_32FC1
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-231.5.1/OpenCV/src/imgproc/corner.cpp
cornerEigenValsVecs
CV_MAT_CN(_type) == e.a.channels()
assign
/Library/Caches/com.apple.xbs/Sources/HomeAI_Sim/HomeAI-231.5.1/OpenCV/src/core/matop.cpp
Unknown operation
Invalid matrix initializer type
%{public}@Storing faceprints:%@ failed with error:%@
%{public}@Storing faceprints:%@ completed successfully
%{public}@Sample has a very large duration, the source video is corrupt.
%{public}@Original Sample Buffer: %@
%{public}@Bogus atomSize %llu, recovering by adjusting size.
%{public}@Simulated crash reporting is not available on this system.
%{public}@Not generating a memory exception report for %@ since another report was generated within last %f seconds.
%{public}@Memory exception reporting is not available on this system.
%{public}@Torsoprint Version: %ld.%ld
%{public}@Removing faceCrops:%@ failed with error:%@
%{public}@Removing faceCrops:%@ completed successfully
%{public}@Received nil data source
%{public}@Fetching settings using data source: %@
%{public}@Error fetching settings: %@
%{public}@handleUpdatedPerson: %@
%{public}@handleUpdatedUnassociatedFaceCrop: %@
%{public}@handleUpdatedPersonFaceCrop: %@
%{public}@handleUpdatedFaceprint: %@
%{public}@handleUpdatedSettings: %@
%{public}@handleRemovedPersonWithUUID: %@
%{public}@handleRemovedFaceCropWithUUID: %@
%{public}@handleRemovedFaceprintWithUUID: %@
%{public}@Successfully handled face misclassification
%{public}@Error in handling face misclassification, error:%@
%{public}@Submitted face misclassification task, taskID:%u
%{public}@Not storing face crop and faceprint for HMIHomePersonManager.UUID:%@ from face event:%@
%{public}@Storing unknown to Home face crop:%@ and faceprint:%@
%{public}@Error storing unassociated face crop:%@, error:%@
%{public}@Stored unassociated face crop:%@
%{public}@Error storing faceprint:%@, error:%@
%{public}@Stored faceprint:%@
%{public}@Reached face crop limit for sessionEntityUUID:%@ for HMIHomePersonManager.UUID:%@; not storing
%{public}@Timer fired, but person data is not yet available, waiting...
%{public}@Timer fired, updating home persons model
%{public}@Not triggering daily VIP Model Core Analytics event, last event was sent less than 1 day ago
%{public}@Triggering daily VIP Model Core Analytics event
%{public}@Successfully ran persons model summary task
%{public}@Failed to run persons model summary task, error:%@
%{public}@Submitted persons model summary task, taskID:%u
%{public}@Unrecognized timer: %@
%{public}@Updating with settings: %@
%{public}@Settings have disabled face classification, removing home persons model
%{public}@Settings have enabled face classification, updating home persons model
%{public}@Storing face crop:%@ failed with error:%@
%{public}@Storing face crop:%@ completed successfully
%{public}@Storing faceprint:%@ failed with error:%@
%{public}@Storing faceprint:%@ completed successfully
%{public}@Registering for Thermal Level Notifications
%{public}@Cannot get available space, error: %@
%{public}@Footprint: %@, Average: %@, Peak: %@
%{public}@Initializing shared model
%{public}@Model is not bundled into framework. Default model is stored in Git LFS. Make sure Git LFS is installed in your local system.
%{public}@Failed to load model!
%{public}@Failed to read sample buffer, error: %@
%{public}@Asset reader failed, ignoring
%{public}@Track %@, %@
%{public}@Warnings: %@
%{public}@personManager is nil for homeUUID: %@
%{public}@Error refreshing home data: %@
%{public}@No homes were located
%{public}@Found home: name: %@, primary: %s, UUID: %@
Identifier = %@, Name = %@
HMISignpost
%{public}@Ignoring %@
%{public}@fetchAllPersonsWithCompletion
%{public}@fetchPersonsWithUUIDs:%@
%{public}@fetchAllPersonFaceCropsWithCompletion
%{public}@Received invalid HMPersonFaceCropSource: %ld
%{public}@fetchFaceCropsForPersonsWithUUIDs:%@
%{public}@fetchAllFaceprintsWithCompletion
%{public}@fetchFaceprintsForFaceCropsWithUUIDs:%@
%{public}@fetchSettingsWithCompletion
%{public}@performCloudPullWithCompletion
%{public}@addFaceprints:%@
%{public}@removeFaceprintsWithUUIDs:%@
%{public}@Could not initialize from decoded personUUID: %@
%{public}@BackgroundEstimator(PTS:%.2f) Unable to update background model (%lu/%lu)
%{public}@Background model assignment is NULL %.2f
%{public}@Foreground TimeStamp mismtach %.2f vs %.2f
%{public}@Unable to resize frame for background estimator %@
%{public}@BackgroundEstimator(PTS:%.2f) Reset background model due to past timestamp %.2f
%{public}@BackgroundEstimator(PTS:%.2f) Reset outdated background model %.2f
%{public}@BackgroundEstimator(PTS:%.2f) Reset background model due to image size change
%{public}@BackgroundEstimator(PTS:%.2f) Reset background model due to very large foreground object
%{public}@BackgroundEstimator(PTS:%.2f) Unable to alloc buffer
%{public}@BackgroundEstimator(PTS:%.2f) No background model
%{public}@Fullfilled content request: %@
%{public}@Fullfilled data request: %@
%{public}@Failed to loadValuesAsynchronouslyForKeys, due to timeout.
%{public}@Face below face quality thresholds: SVM recognizability = %lf, Yaw = %lf, discarding
%{public}@Face below ANFR face quality threshold: ANFR confidence = %lf, discarding
%{public}@personsModelPredictions is empty
%{public}@Positively classified face with facemask: (sourceUUID: %@, personUUID: %@)
%{public}@Face removed from unknown & uncertain bucket: has facemask
%{public}@Face removed from unknown and uncertian bucket, SSD confidence = %f, entropy of laplacian = %f, Face yaw = %f, box size: %f
%{public}@Added to unknown bucket yaw: %@
%{public}@Added to uncertain bucket yaw: %@
%{public}@Face recognition set is empty
%{public}@DES record saving is not permitted.
%{public}@There isn't enough available disk space (%ld MB) to save DES records.
%{public}@Couldn't determine the amount of available space disk space, continuing.
%{public}@Saving des record for video frame (PTS:%0.2fs) to disk
%{public}@Saving DES Record, recordInfo: %@, data: %@
%{public}@Saved DES Record: %@, error: %@
%{public}@Unable to create input image tensor with error %@
%{public}@Track(PTS:%.2f-%.2f), dF:%.2f(%.2f), dT:%.2f(%.2f,%.2f), GIOU:%.2f(%.2f), %@ vs %@
%{public}@HMIPersonTracker: unable to set %@ at index %lu / %lu
%{public}@HMIPersonTracker: unable to get %@ at index %lu / %lu
%{public}@Error creating frame analyzer: %@
%{public}@Cannot find ground truth for %@
%{public}@Ignoring error detecting face in Photos face crop, error: %@
%{public}@Could not initialize from decoded UUID: %@ dataRepresentation: %@ dateCreated: %@
%{public}@Dropping frame, lastSamplePTS > nextSamplePTS.
%{public}@TaskID: %u running...
%{public}@options is empty/nil, defaulting to Home persons clustering task with impure person cleanup
%{public}@Creating HMPhotosPersonManager for homeUUID:%@ sourceUUID:%@
%{public}@Creating HMHomePersonManager for homeUUID:%@
%{public}@Current device is not primary resident, skipping clustering
%{public}@Initializing HMITaskServiceServer
%{public}@Error fetching persons:%@
%{public}@Skipping sentinel faceprint in existingAtCurrentVersion
%{public}@Skipping sentinel faceprint in createdAtCurrentVersion
%{public}@Faceprint Version: %ld.%ld
%{public}@Warm starting faceprint model...
%{public}@Failed to create pixel buffer when warm starting faceprint model
%{public}@Failed to warm start faceprint model: %@
%{public}@Warm start of faceprint model took: %f
%{public}@Cropping face %@ from pixel buffer with dimensions: %.1f x %.1f roll: %.02f degrees
%{public}@Error pixel buffer type conversion %@.
%{public}@Error in rotating the face %@.
%{public}@Face was rotated by:%.02f degrees
%{public}@HMIPrivateErrorCodeCropAndResizeFailed %@
%{public}@Cropping face %@ from face crop with dimensions %.1f x %.1f
%{public}@%lu faceprint(s) exist for face crop:%@ but are not the current version
%{public}@Using existing faceprint for face crop:%@
%{public}@Faceprinting face crop:%@
%{public}@Skipping crop, encountered error faceprinting: %@
%{public}@Face crop has a facemask, creating sentinel faceprint
%{public}@Vision run-time version: %d.%02d.%02d (%d)
%{public}@releaseCachedResources is deprecated and is now a no-op.
%{public}@Could not initialize from decoded UUID: %@ data: %@ modelUUID: %@ faceCropUUID: %@
%{public}@[Model loading] failed to create asset dir: %@ error: %@
%{public}@[Model loading] unpackaging
%{public}@[Model loading] failed to remove dir
%{public}@[Model loading] failed to untar model
%{public}@Error during update torso model task: %@
%{public}@Trusting host: %@
%{public}@Force Certificate Pinning
%{public}@Error setting trust policies: %lu
%{public}@Invalid certificate: %@
%{public}@Downloading Clip
%{public}@Fetched Clip videoAssetContext: %@, error: %@
%{public}@Fetching Clip, progress %lu%%
%{public}@Face crops are not available.
%{public}@Fetching Face Crops
%{public}@Fetched Person UUIDs: %@
%{public}@Fetched Face Crops: %@, error: %@
%{public}@Fetching Clip
%{public}@Use the face-blurred video for upload
%{public}@Use the original video without audio track for upload
%{public}@Uploading payload data: %@, to URL %@
%{public}@Submitting clipUUID: %@, cameraProfileUUID: %@
%{public}@Stripped Audio %@, error: %@
%{public}@Downloaded %@, error: %@
%{public}@Failed to fetch pre-signed URL, error: %@
%{public}@Failed to request service result, error: %@
%{public}@Failed to decode service result, error: %@
%{public}@Service result: %@
%{public}@Deleting Temporary File %@
%{public}@Deleted Temporary File %@, error: %@
%{public}@Failed to generate persons model summmary, error:%@
%{public}@Failed to fetch face crops with error: %@
%{public}@Failed to fetch faceprints with error: %@
%{public}@Error faceprinting face crops:%@
%{public}@Person (%@) has no faceprints -- nothing to remove
%{public}@Nearest face crop to be removed: %@
%{public}@Failed to remove face crop with error: %@
%{public}@Successfully removed face crop (%@) via user indicated misclassification
%{public}@Failed to remove persons model, error:%@
%{public}@Successfully removed persons model
%{public}@%@: %@
%{public}@Provided sequenceNumbers: %@, don't match fragment's sequenceNumbers: %@
%{public}@Failed to read fragment data, err: %d
%{public}@No torso annotations -- skipping torso model update
%{public}@Adding torso to existing sessionEntityUUID: %@ (face)
%{public}@Adding face to existing sessionEntityUUID: %@ (torso)
%{public}@Session entity %@ already has a face recognition, skipping subsequent match
%{public}@Existing face classification: %@
%{public}@New face classification: %@
%{public}@Assigning session entity %@ the face classification: %@
%{public}@updateTorsoModelAndGetTorsoAnnotationsForHome: %@
%{public}@Creating torso annotation with %lu torsoprints
%{public}@Adding face to existing sessionEntityUUID: %@ (VIP)
%{public}@Adding torso to existing sessionEntityUUID: %@ (VIP)
%{public}@Adding face to existing sessionEntityUUID: %@ (NN)
%{public}@Adding torso to existing sessionEntityUUID: %@ (NN)
%{public}@Adding face to existing sessionEntityUUID: %@ (track)
%{public}@Adding torso to existing sessionEntityUUID: %@ (track)
%{public}@Adding new face sessionEntityUUID: %@
%{public}@Adding new torso sessionEntityUUID: %@
%{public}@%@
%{public}@Error removing persons with UUIDs:%@, error:%@
%{public}@Succesfully removed persons %@
%{public}@Removing faceprints:%@ failed with error:%@
%{public}@Removing faceprints:%@ completed successfully
%{public}@Could not decode torsoAnnotations
%{public}@Publishing local state
%{public}@Dropping remote analysis torso update since torso rec is not enabled on this device
%{public}@Dropped %lu incompatible torsoprint annotations out of %lu total
%{public}@Successfully update torso model
%{public}@Error in update torso model, error:%@
%{public}@Submitted torso model update task, taskID:%u
%{public}@Could not initialize from decoded sourceUUID: %@
%{public}@Could not initialize from decoded UUID: %@
%{public}@Saved face classification:%@ to disk
%{public}@Could not initialize from decoded UUID: %@ data: %@
%{public}@Could not initialize from decoded faceRecognition: %@ torsoprints: %@ torsoModelVersion: %@
%{public}@Invalidated with err: %d
%{public}@Encoder is in a failed state, ignoring sample.
%{public}@VTCompressionSessionEncodeFrameWithOutputHandler (Handler) failed, err: %d
%{public}@VTCompressionSessionEncodeFrameWithOutputHandler, frame dropped.
%{public}@VTCompressionSessionCompleteFrames failed, err: %d
%{public}@VTCompressionSessionPrepareToEncodeFrames failed, err: %d
%{public}@Cannot set property: %@, error: %d
%{public}@Cannot copy property: %@, error: %d
%{public}@addFaceCrops:%@
%{public}@addPersonFaceCrops:%@
%{public}@Received invalid HMIPersonFaceCropSource: %ld
%{public}@addPersons:%@
%{public}@fetchAllUnassociatedFaceCropsWithCompletion
%{public}@removeFaceCropsWithUUIDs:%@
%{public}@removePersonsWithUUIDs:%@
%{public}@associateFaceCropsWithUUIDs:%@ toPersonWithUUID:%@
%{public}@Writing updated userDefinedPersonLinksByHome[%@] to disk
%{public}@Stale Home VNPersonsModel with homeUUID: %@ sourceUUID: %@ detected, attempting to remove...
%{public}@Stale model path no longer on disk, proceeding with building persons model...
%{public}@Persisted VNPersonsModel for homeUUID: %@ sourceUUID: %@
%{public}@Did not remove VNPersonsModel for homeUUID: %@ sourceUUID: %@, no model found
%{public}@Error removing user defined person links file: %@
%{public}@Removed userDefinedPersonLinksByHome for homeUUID: %@
%{public}@Removed VNPersonsModel for homeUUID: %@ sourceUUID:%@
%{public}@Unable to build equivalency map for homeUUID: %@, error: %@
%{public}@Failure to lookup equivalency cell for %@ (equivalencyCellForHome is%@ nil)
%{public}@Found nil torsoToFaceCrop for home %@ with non-nil model!
%{public}@Unable to retrieve torsoprints for person: %@, %@
%{public}@Person %@ has %lu torsoprints
%{public}@Received torso annotation with no identifier: %@
%{public}@Received torso annotation with no classification corresponding to the linkedEntityUUID: %@
%{public}@Created new torso model with %lu persons and %d total torsoprints for home: %@
%{public}@Successfully updated torso model and face crop map for home: %@
%{public}@Resetting torso model and wiping data
%{public}@Successfully deleted torso data at path: %@
%{public}@Failed to delete torso directory at path: %@, error: %@
%{public}@Torso model version on disk doesn't match current version
%{public}@Found stale torso_to_facecrop file
%{public}@There is no current torso model for home: %@
%{public}@Torso model predicted person %@ with confidence %f
%{public}@Persons Model Storage Path:%@
%{public}@Failed to parse Home UUID from path: %@
%{public}@Failed to load External HMIPersonsModel at path: %@, error: %@
%{public}@Loaded External HMIPersonsModel for homeUUID: %@ sourceUUID: %@
%{public}@No home model found for homeUUID: %@
%{public}@Failed to load Home HMIPersonsModel at path: %@, error: %@
%{public}@Loaded Home HMIPersonsModel for homeUUID: %@ sourceUUID: %@
%{public}@Loaded %lu user defined equivalencies found for home: %@
%{public}@No user defined equivalencies found for home: %@ (reason: %@)
%{public}@Got nil for torso model file path, error: %@
%{public}@No torso model found for home %@ at path: %@
%{public}@Failed to load torsoToFaceCrop map, error: %@
%{public}@Failed to load torso model at path: %@, error: %@
%{public}@Successfully loaded torso model and face crop map for home: %@
%{public}@Resetting HMIPersonsModelManager
%{public}@Recipe doesn't have a custom learningRate value, learning rate in model definition is used for training: %e
%{public}@Recipe has a custom learningRate value, learning rate in recipe is used for training: %e
%{public}@Calculating pre-training loss value
%{public}@trainingCallback %lu, loss: %f
%{public}@Training was skipped because %@ is YES.
%{public}@Training Started
%{public}@Training Finished
%{public}@Saving trained model to %@
%{public}@Successfully saved trained model at %@
%{public}@Timer fired, updating external persons model
%{public}@Settings have disabled face classification, removing external persons model
%{public}@Settings have enabled face classification, updating external persons model
%{public}@Failed to remove persons model, error:%@, retrying...
%{public}@Submitted persons model remove task, taskID:%u, retryOnError:%@
%{public}@Cannot decode additional streams using H.264, %lu H.264 decoders are already being used.
%{public}@Cannot transcode additional streams using H.265, %lu H.265 encoders are already being used, trying with H.264.
%{public}@Cannot transcode additional streams, %lu H.264 encoders are already being used.
%{public}@Cannot generate timelapse, %lu H.264 encoders are already being used.
%{public}@Submitted task returned error: %@
%{public}@Clustering successful
%{public}@Clustering error
%{public}@Initializing HMIVisionSession
%{public}@Releasing vision session after period of inactivity
%{public}@Unloading model at path %@ after period of inactivity
%{public}@Failed to initialize model detector %@
%{public}@Detecting Package
%{public}@Skipping package detection analysis because package already was detected.
%{public}@Unable to resize frame for background estimator
%{public}@PackageAnalyzer(PTS:%.2f-%.2f) FG:%lu BG:%lu C:%lu AP:%lu P:%lu Sess:%d
%{public}@Detected package!
%{public}@Pixel buffer is unavailable (PTS:%.2f)
%{public}@Error creating activity zone result directory: %@
%{public}@Activity zone file path:%@
%{public}@Error converting activity zone results to JSON: %@
%{public}@Error writing activity zone results JSON to file: %@
%{public}@motionScore %f
%{public}@Inclusion zone:%@ intersecting with:(%@) Object coordinate %@ insetThreshold %f
%{public}@Exclusion zone:%@ intersecting with:(%@) Object coordinate %@ insetThreshold %f
%{public}@Events after activity zone filtering:(%@) Object coordinate %@ insetPercentage %f
%{public}@Add motion-vector stationary event %@
%{public}@Replace matched stationary event %@ for %@
%{public}@Add edge-distance stationary event %@
%{public}@Unsupported aspect ratio: (%d, %d)
%{public}@Encrypted Training Result
%{public}@Preference %@ is now %@, previously was %@
%{public}@Error fetching face crops for person:%@, error:%@
%{public}@Already started listening for the notification
%{public}@Unable to read the asset %@
%{public}@Finish re-encoding %.3f > %.3f
%{public}@Unable to init face detector %@
%{public}@Skip the frame @ %.3fs due to analyzer failure
%{public}@Skip the frame @ %.3fs due to blurring failure
%{public}@Failed to convert YCbCr to RGBA (%@)
%{public}@Failed to clone RGBA source image
%{public}@Failed to blur entire image (vImage_Error = %zd)
%{public}@Failed to copy blurred patch (vImage_Error = %zd)
%{public}@Failed to convert RGBA to YCbCr (%@)
%{public}@Fetching persons for HMICleanupImpureHomePersonsOperation
%{public}@Error fetching persons, error:%@
%{public}@Fetched %lu persons
%{public}@Fetching face crops for person: %@
%{public}@Error fetching facecrops for person:%@, error:%@
%{public}@Fetched %lu face crops for person: %@
%{public}@Ignoring error fetching faceprints for person:%@, error:%@
%{public}@Error faceprinting face crops for person:%@, error:%@
%{public}@Number of faceprints to cluster: %lu
%{public}@Clustering error:%@ treating identity: %@ as impure
%{public}@0 or 1 cluster exists, treating identity: %@ as pure
%{public}@Unnamed person %@ has %lu clusters, treating as impure
%{public}@Named person %@ with atleast 1 personLink has %lu clusters, treating as impure
%{public}@%lu clusters exists, for person %@ trying to club clusters using vip model
%{public}@Cluster size: %lu
%{public}@Error while creating vnpersonsmodel: %@, treating identity as impure
%{public}@Failed to predict using VNPersonsModel, error: %@, treating identity as impure
%{public}@Error while removing facecrops %@
%{public}@Reassociating %lu face crops to person UUID: %@
%{public}@Error while reassociating facecrops %@
%{public}@Error creating directory %@
%{public}@Error fetching attributes of the file: %@ at: %@. Attempting to delete it
%{public}@Deleted %@ to free up some space, error: %@
%{public}@Error while deleting %@ to free up some space, error: %@
%{public}@Disk buffer size of %@: %ld KB
%{public}@Archive familiar face data for home: %@ person: %@
%{public}@Cannot archive familiar face data for person %@, error: %@
%{public}@Couldn't get URL for home archives, error: %@
%{public}@Saving archived familiar face data for home: %@ person: %@ to: %@
%{public}@Couldn't save FF archive
%{public}@Saved FF archive
%{public}@Skipping person %@ due to nil or 0 face crops
%{public}@Person %@ has crops with unknown source, reassociating them
%{public}@Skipping person %@ as all crops are either old or have a non-unknown source
%{public}@Removing %lu sentinel facecrops for person %@
%{public}@0 faceprints for person: %@, skipping
%{public}@Found pure identity, skipping person %@
%{public}@Removing person %@ and associated crops
%{public}@HMICleanupImpureHomePersonsOperation exiting early because operation was canceled.
%{public}@Completed CleanImpureHomePersonsOperation
%{public}@CleanImpureHomePersonsOperation encountered %d failures
%{public}@Error while removing persons %@
%{public}@Spawning CleanupImpureHomePersonsOperation for %@ before home person clustering
%{public}@CleanupImpureHomePersonOperation finished with error:%@
%{public}@CleanupImpureHomePersonOperation finished successfully
%{public}@Error performing cloud pull:%@
%{public}@Fetching persons
%{public}@Fetched %lu persons (%lu unnamed)
%{public}@Exiting early because task was canceled.
%{public}@Skipping named person
%{public}@Deleting unnamed person %@ (0 face crops)
%{public}@Deleting unnamed person %@ (age = %f seconds)
%{public}@Error fetching face crops:%@
%{public}@Error fetching faceprints:%@
%{public}@Storing %lu newly created faceprints
%{public}@Error saving new faceprints:%@
%{public}@Removing %lu faceprints from old versions
%{public}@Error removing faceprints from old versions:%@
%{public}@Clustering error:%@
%{public}@Number of clusters: %lu
%{public}@Cluster of size %lu beneath threshold of %d
%{public}@Face prediction error:%@
%{public}@Assigning cluster to existing person with UUID: %@
%{public}@Error adding new persons:%@
%{public}@Error associating face crops with person (%@): %@
%{public}@Finished calls to associateFaceCropsWithUUIDs
%{public}@Error removing person with UUID:%@, error:%@
%{public}@Succesfully removed person %@
%{public}@Error fetching faceprints for face crop UUIDs:%@, error:%@
%{public}@Storing newly created faceprints: %@
%{public}@Removing existing faceprints at other versions: %@
%{public}@Failed to generate persons model, error:%@
%{public}@Invalid configuration: isExternalLibrary is NO but self.dataSource does not conform to HMIHomePersonManagerDataSource protocol!
%{public}@Successfully updated persons model
%{public}@WARNING: Model has %lu named persons -- limit supported is %d
%{public}@Error fetching faces to subsample for %@: %@
%{public}@Fetched 0 training faces for %@, this would remove all face crops! Skipping face crop removal.
%{public}@Expected subsampling to leave no more than %d, but got %lu faces selected. Enforcing limit.
%{public}@Subsampling will retain %lu from a total of %lu faces for %@
%{public}@Deleting a total of %lu face crops after subsampling
%{public}@Selected %lu persons for subsampling faces but did not choose any face crops to delete!
%{public}@Face Classification is enabled, but homeUUID is nil, skipping face recognition
%{public}@AnalyzerEvents(PTS:%.2fs): %@
%{public}@Creating analysis state update with %lu torso annotations
%{public}@Error while retrieving facecrop from torsomodel for personUUID: %@ homeUUID: %@
%{public}@Couldn't retrieve linked predictions from torsomodel for personUUID: %@ homeUUID: %@ error: %@
%{public}@Dropping Face event: %@ due to torso recognition
%{public}@Faceprinting failed for face: %@, error: %@
%{public}@Face: %@ didn't produce any classifications
%{public}@Torsoprinting failed for torso: %@, error: %@
%{public}@Video Frame:(PTS:%0.2fs) has %lu detection(s) before filtering
%{public}@Video Frame:(PTS:%0.2fs) has %lu detection(s) after filtering
%{public}@Unable to archive task %@: %@
%{public}@There is no task to schedule
%{public}@[Model loading] Error opening encrypted file
%{public}@[Model loading] Error reading mmaped encrypted file
%{public}@[Model loading] Error reading data from compressed file
%{public}@[Model loading] returning unsuccessfully post decompression due to invalid destination size
%{public}@[Model loading] Error in out file.
%{public}@failed stat %s
%{public}@[Model loading] failed to unpackage resources
%{public}@[Model loading] unpackaged resources version
%{public}@Error in opening temporary file.
%{public}@preallocated temporary file failed. %d
%{public}@Serious error in writing temporary file. %d
%{public}@Error fetching unassociated face crops:%@
%{public}@Error associating face crops (num UUIDs:%lu), to personUUID: %@ with source: %@ error:%@
%{public}@Succesfully associated face crops (num UUIDs %lu) to person UUID: %@ for source: %@
%{public}@Events file "%@" does not exist.
%{public}@Cannot read events from file "%@", error: %@
%{public}@Cannot load events file, exception: %@
%{public}@Video decoder is not running, ignoring %@
%{public}@Sample buffer has no samples, skipping.
%{public}@Invalid DTS, expected > %@, got %@, skipping.
%{public}@Restarting decoder because format description changed.
%{public}@Decoded sample is out of PTS order, sample: %@
%{public}@Decoded sample has an invalid PTS, sample: %@
%{public}@Decoder is already in a failed state.
%{public}@Decompression session decoded frames after decoder was deallocated, ignoring frames.
%{public}@Frame decode error %d
%{public}@Cannot add video input.
%{public}@Cannot add audio input.
%{public}@Failed to start writing, assetWriter.status: %ld, assetWriter:.error: %@
%{public}@Started writing at %@
%{public}@didOutputSegmentData segmentType: %ld
%{public}@Trying to recover by adjusting trim duration from %@ to the minimum trim duration: %@
%{public}@Asset writer has failed fatally, ignoring %@
%{public}@Failed to write to asset writer, error %@
%{public}@Dropped   %@ because an input for the media type was not found.
%{public}@Dropped    %@ because asset writer is waiting for a sync sample.
%{public}@Dropped    %@ because of input error %@
%{public}@Video / Audio Drift (video is ahead by) %+4.3f
%{public}@Dropped    %@ because an input %@ is not ready for more media data.
%{public}@Asset writer has failed fatally, ignoring flush.
%{public}@We don't have anything to flush, ignoring flush.
%{public}@Finished waiting for flushSegment, assetWriter.status: %ld, assetWriter.error: %@
%{public}@Number of all face observations: %ld
%{public}@Invalid entry in userDefinedPersonLinks: %@
%{public}@All links for %@ in userDefinedPersonLinks are invalid
%{public}@Skipping person who belongs to user defined equivalency cell: %@
%{public}@Comparing persons (%@, %@)
%{public}@Equivalency determined between pair: (%@, %@)!
%{public}@Cannot add to matching equivalency cell because it already has entry from this source: %@
%{public}@Failed to update persons model, error:%@, retrying...
%{public}@Failed to update persons model, error:%@
%{public}@Submitted persons model update task, taskID:%u, retryOnError:%@
%{public}@Skip torsoEvent with extreme roll (%.0f deg)
%{public}@Skip torsoEvent with extreme aspect ratio (w/h) (%.2f) pixelDim:(%f, %f) bbox:(%f, %f)
%{public}@Skip torsoEvent with torsoBox close to roi boundary. Dist: (%.4f)
%{public}@Failed to predict using torso vip model
%{public}@Synthesizing Motion Detections, Target: %@
%{public}@Adding Candidate: %@
%{public}@Selected: %@
%{public}@Creating analyzer with identifier: %@, configuration: %@
%{public}@VideoAnalyzerReport saved (%@)
%{public}@Recieved Message: %@
%{public}@Unknown %@
%{public}@Sent Message Reply: %@
%{public}@Camera, Manufacturer: %@, Model: %@, Fragment: %@, Sanitized Fragment Data: %@
%{public}@Finish Analyzer
%{public}@Analyzer has failed, ignoring finish.
%{public}@Video file %@ size is too large, maximum allowed is (%ld MB), no longer appending fragments.
%{public}@Disk buffer size remaining in %@, %ld MB
%{public}@Appending fragment to %@
%{public}@Saving fragment to %@
%{public}@Video format should not change.
%{public}@Audio format should not change.
%{public}@-[HMIVideoAnalyzerServer dealloc]
%{public}@Analyzer has failed or was cancelled, ignoring sample buffer.
%{public}@Analyzer has failed or was cancelled, ignoring flush.
%{public}@Bundling Fragment Result, timeRange: %@, frames: [%@], packages: [%@], thumbnails [%@]
%{public}@Analyzer frame result buffer should be empty. %@
%{public}@Package analyzer result buffer should be empty. %@
%{public}@Thumbnail buffer should be empty. %@
%{public}@Timelapse encoder failed, ignoring: error: %@
%{public}@Generated Fragment: %@ Outcome: %@ Max Confidence Events: %@
%{public}@Analyzer Failed: %@
%{public}@Analyzer is already in a failed state.
%{public}@Sending Result: %@
%{public}@analysisFPS changing from: %f to: %f
softlink:o:path:/System/Library/PrivateFrameworks/PrivateFederatedLearning.framework/PrivateFederatedLearning
HMIExternalPersonDataSourceDisk
HMIExternalPersonManagerDataSource
HMIPersonManagerDataSource
NSObject
HMIHTMLReportBox
HMIHTMLReport
HMIMutableCluster
HMFLogging
HMISystemResourceUsage
HMISystemResourceUsageMonitor
HMISystemResourceUsageMonitorProtocol
HMIVideoAnalyzerEventFace
HMIStoreFaceprintsOperation
HMIDataReader
HMIVideoEventEntry
HMIVideoEvent
HMIVideoEventBuffer
HMIVideoTimelineEntry
HMIVideoTimeline
HMIVideoTimelineProfiler
HMITimeIntervalAverage
HMIVideoAnalyzerEventMotion
HMITorsoprinter
HMIRemoveFaceCropsOperation
HMIHomePersonManager
HMFTimerDelegate
HMIStoreFaceCropOperation
HMIStoreFaceprintOperation
HMICameraVideoFrame
HMIVisionUtilities
HMIPairwiseMatch
HMIThermalMonitorService
HMIThermalMonitor
HMIMemorySampler
HMISignificantActivityFcosDetector
HMIVideoAssetReader
HMIHomeKitClient
HMHomeManagerDelegateAdapter
HMHomeManagerDelegate
HMISignpost
HMIFaceQualityFilterModelInput
MLFeatureProvider
HMIFaceQualityFilterSVM
HMIVideoNode
HMIVideoProcessingNode
HMIVideoAnalyzerProcessingNode
PFLBackgroundRunner
_DASExtensionRunner
HMIExternalPersonDataSourceHomeKit
HMI_CVML_Error
HMIPersonFaceCrop
NSCopying
NSSecureCoding
NSCoding
HMIVideoAnalyzerFrameResult
HMIForegroundBlob
HMIForegroundTrack
HMIBackgroundEstimator
HMIMemoryAVAsset
AVAssetResourceLoaderDelegate
MovingAverageEntry
MovingAverage
HMICameraVideoFrameResult
HMIVideoAnnotationParserRecord
HMIVideoAnnotationParserTrack
HMIVideoAnnotationParser
HMIDESMutableFloatArray
HMIFaceClassifierVIP
HMIFaceClassifier
HMIDESDetection
HMIDESDatasetSample
HMIDESDataset
HMIInputFeatureProvider
HMINMSConfiguration
HMIObjectDetection
HMIObjectDetectionUtils
HMIPersonBlob
HMIPersonTracker
HMIVideoFrameAnalyzerResult
HMIVideoFrameAnalyzer
HMIVideoFrameSamplerDelegate
HMIVideoFrameAnalyzerDelegateAdapter
HMIVideoFrameAnalyzerDelegate
HMIVideoAnalyzerReportMatch
HMIVideoAnalyzerReportRecord
HMIVideoAnalyzerMutableReportComparison
HMIVideoAnalyzerMutableReportSession
HMIVideoAnalyzerMutableReport
HMIFaceCrop
HMIVideoRetimer
HMIVideoRetimerDelegateAdapter
HMIVideoRetimerDelegate
HMICamera
HMITask
HMIHomeTask
HMIEmptyTask
HMITaskServiceServer
HMITaskService
HMIFetchPersonsOperation
HMFObject
HMITorsoRecognition
HMIUpdatedFaceprintsResult
HMIFaceprinter
HMIFaceDetectorVision
HMIFaceDetector
HMIFaceprint
HMIPoint
HMIHomePersonManagerSettings
NSMutableCopying
HMIMutableHomePersonManagerSettings
HMIModelLoader
HMIUpdateTorsoModelTask
HMIFeedbackSession
NSURLSessionDelegate
HMIFeedbackSubmitClipOperation
HMIFeedbackTask
HMIFeedback
HMIFaceClassification
HMIHomePersonDataSourceInMemory
HMIHomePersonManagerDataSource
HMIPersonsModelsSummaryTask
HMIFaceMisclassificationTask
HMIRemovePersonsModelTask
HMIVideoFrameGenerator
HMIPersonManager
HMIConfidence
HMIError
HMIVideoFragment
HMISessionEntityManager
HMIRemovePersonsOperation
HMIRemoveFaceprintsOperation
HMITuriTrialUpdateTask
HMIAnalysisStateUpdate
HMIAnalysisStateManager
HMIVideoAnnotator
HMISystemResourceUsageMonitorImpl
HMIVideoAnalyzerResultFilter
HMIVideoAnalyzerResultActivityZoneFilter
HMIPersonSourceUUIDPair
HMIPerson
HMIPersonsModelSummary
HMIPersonsModel
HMIJSONArchiver
HMIJSONUnarchiver
HMIFaceUtilities
HMITorsoprint
HMITorsoAnnotation
HMIVideoEncoder
HMIVideoEncoderDelegateAdapter
HMIVideoEncoderDelegate
HMIHomePersonDataSourceHomeKit
HMIExternalPersonManagerSettings
HMIMutableExternalPersonManagerSettings
HMIAnalytics
HMIPersonsModelsSummary
HMIPersonsModelPrediction
HMIPersonsModelManager
HMIDESLayerParameters
HMIDESTrainingResult
HMIDESTrainer
HMIVideoAnalyzerResultOutcome
HMIExternalPersonManager
HMIRemovePersonsModelOperation
HMIVideoAnalyzerSchedulerJSONLogger
HMIVideoAnalyzerScheduler
HMISystemResourceUsageMonitorDelegate
HMITuriTrialManager
HMIGreedyClustering
HMIVideoFrameSampler
HMIVideoFrameIntervalSampler
HMIVideoFrameSamplerDelegateAdapter
HMICIFilterAttributeValue
HMICIFilterAttribute
HMICIFilter
HMIVisionSession
HMIMLModel
HMIVideoPackageAnalyzerResult
HMIVideoPackageAnalyzer
HMIVideoPackageAnalyzerDelegate
HMIVideoPackageAnalyzerDelegateAdapter
HMIFaceQualityEntropyOfLaplacian
HMIVideoAnalyzerFragmentResult
HMICameraActivityZone
HMIVideoAnalyzerConfiguration
HMIVideoAnalyzerDynamicConfiguration
HMIStationaryObject
HMIVideoTemporalEventFilter
HMIMotionDetection
HMIAnalysisService
HMIVideoAnalyzerEventPerson
HMIDESPackageResult
HMIDESResultPackager
HMIPreference
HMIHomePersonDataSourceDisk
HMIFetchPersonFaceCropsOperation
HMINotifydObserver
HMIVideoAnalyzerEventVehicle
HMIFeedbackVisionProcessor
HMICleanupImpureHomePersonsOperation
HMIClusteringTaskSummary
HMIHomePersonClusteringTask
HMIFetchFaceprintsForFaceCropsOperation
HMIUpdatePersonsModelTask
HMIRegionOfInterestOperation
HMICameraVideoFrameAnalyzerSignificantActivity
HMICameraVideoFrameAnalyzer
HMIDESBackgroundTask
HMIVideoFrame
HMICompressionHelper
HMIVideoCommandBuffer
HMIVideoCommandBufferDelegateAdapter
HMIVideoCommandBufferDelegate
HMIVideoAnalyzerEventPet
HMIHLSPlaylist
HMIFetchUnassociatedFaceCropsOperation
HMIAssociateFaceCropsOperation
HMIVideoAnalyzerEvent
HMITorsoClassification
HMIVideoAnalyzerEventPackage
HMIVideoDecoder
HMIVideoDecoderDelegateAdapter
HMIVideoDecoderDelegate
HMIVideoAssetWriter
AVAssetWriterDelegate
HMIVideoAssetWriterDelegateAdapter
HMIVideoAssetWriterDelegate
HMIPersonsModelEquivalencyTable
HMIUpdatePersonsModelOperation
HMISparseOpticalFlowFeatureVector
HMISparseOpticalFlowMotionDetection
HMISparseOpticalFlowMotionDetector
HMIMotionDetector
HMITorsoClassifier
HMIVideoFrameSelectorFrameCandidate
HMIVideoFrameSelector
HMIVideoFrameSelectorDelegateAdapter
HMIVideoFrameSelectorDelegate
HMIVideoAnalyzerState
HMIFaceRecognition
HMIVideoAnalyzer
HMIVideoAnalyzerDelegatePrivate
HMIVideoAnalyzerDelegate
HMIVideoAnalyzerServer
HMIVideoAnalyzerDelegateAdapter
HMIPersonDataSourceDisk
HMIVideoAnalyzerEventTorso
workQueue
sourceURL
UUID
UUIDString
URLByAppendingPathComponent:
defaultManager
path
createDirectoryAtPath:withIntermediateDirectories:attributes:error:
stringWithFormat:
hmiPrivateErrorWithCode:description:underlyingError:
name
dictionaryWithObjects:forKeys:count:
serializeJSONObject:url:error:
countByEnumeratingWithState:objects:count:
personUUID
faceBoundingBox
dateCreated
dataRepresentation
writeToURL:atomically:
hmiPrivateErrorWithCode:description:
hmfErrorWithCode:
logCategory
isEqual:
class
self
performSelector:
performSelector:withObject:
performSelector:withObject:withObject:
isProxy
isKindOfClass:
isMemberOfClass:
conformsToProtocol:
respondsToSelector:
retain
release
autorelease
retainCount
zone
hash
superclass
description
debugDescription
TQ,R
T#,R
T@"NSString",R,C
fetchAllPersonsWithCompletion:
fetchPersonsWithUUIDs:completion:
fetchAllPersonFaceCropsWithCompletion:
fetchFaceCropsForPersonsWithUUIDs:completion:
fetchAllFaceprintsWithCompletion:
fetchFaceprintsForFaceCropsWithUUIDs:completion:
performCloudPullWithCompletion:
addFaceprints:completion:
removeFaceprintsWithUUIDs:completion:
fetchSettingsWithCompletion:
addPerson:completion:
addPersonFaceCrops:completion:
init
initWithBoundingBox:text:color:opacity:value:
boundingBox
text
color
opacity
value
.cxx_destruct
_opacity
_value
_text
_color
_boundingBox
T{CGRect={CGPoint=dd}{CGSize=dd}},R,V_boundingBox
T@"NSString",R,V_text
T@"NSString",R,V_color
Tf,R,V_opacity
Tf,R,V_value
initToFileAtPath:append:
stream
open
appendHeaderWithTitle:textColor:backgroundColor:
appendString:
close
outputPath
dealloc
bundleForClass:
URLForResource:withExtension:
dataWithContentsOfURL:
initWithData:encoding:
_loadResource:withExtension:
dataUsingEncoding:
bytes
length
write:maxLength:
appendFrame:text:boxes:imageBorder:imageColor:outlineBorder:outlineColor:
size
compressedFrameWithScale:quality:error:
array
addObject:
base64Encoded
componentsJoinedByString:
enumerateObjectsUsingBlock:
createPixelBufferFromJPEGData:error:
base64EncodedStringWithOptions:
frameResults
frame
presentationTimeStamp
appendFrameResult:frameTruth:description:
na_each:
dataWithContentsOfFile:
initWithData:
initWithVideoFragment:
valueWithCMTime:
na_map:
objectAtIndexedSubscript:
events
regionOfInterest
initWithFrame:events:regionOfInterest:
count
lastPathComponent
generateVideoFramesForTimes:completionHandler:
boxesForEvent:isTruth:
addObjectsFromArray:
boxForRegionOfInterest:
initWithKey:ascending:
allObjects
arrayWithObjects:count:
sortedArrayUsingDescriptors:
sessionEntityUUID
substringToIndex:
confidence
rgbColorCodeForEventClass:
face
faceRecognition
classifications
hmf_isEmpty
anyObject
fromTorsoClassification
torso
copy
initWithTitle:outputPath:
appendText:
appendFrame:text:
appendFaceCrop:imageBorder:imageColor:outlineBorder:outlineColor:
appendFragmentResult:
appendFragmentResult:assetPath:
flush
_stream
_outputPath
T@"NSOutputStream",R,V_stream
T@"NSString",R,V_outputPath
data
faceCentroid
initWithValue:count:
arrayWithObject:
torsoCentroid
faceCount
scale:
add:
floatArrayByScaling:
torsoCount
removeObjectsInRange:
logIdentifier
initWithFaceprint:
initWithTorsoprint:
faceprintUUIDs
torsoprintUUIDs
addLinkedEntityUUIDs:
linkedEntityUUIDs
addFaceprints:
addTorsoprints:
flushTorsoprints
setFaceRecognition:
torsoprints
setTorsoprints:
setFaceprintUUIDs:
setTorsoprintUUIDs:
setLinkedEntityUUIDs:
_faceCentroid
_torsoCentroid
_faceRecognition
_torsoprints
_faceprintUUIDs
_torsoprintUUIDs
_linkedEntityUUIDs
T@"NSMutableArray",&,N,V_faceprintUUIDs
T@"NSMutableArray",&,N,V_torsoprintUUIDs
T@"NSMutableSet",&,N,V_linkedEntityUUIDs
T@"NSMutableArray",&,V_torsoprints
T@"HMIDESMutableFloatArray",R,V_faceCentroid
T@"HMIDESMutableFloatArray",R,V_torsoCentroid
T@"HMIFaceRecognition",&,V_faceRecognition
systemResourceUsageLevel
setSystemResourceUsageLevel:
_systemResourceUsageLevel
Tq,N,V_systemResourceUsageLevel
productInfo
productClass
UTF8String
initWithProductClass:workQueue:
systemResourceUsageMonitorImpl
setDelegate:
delegate
getCurrentSystemResourceUsage
start
T@"<HMISystemResourceUsageMonitorDelegate>",W
_systemResourceUsageMonitorImpl
_workQueue
T@"HMISystemResourceUsageMonitorImpl",R,V_systemResourceUsageMonitorImpl
T@"NSObject<OS_dispatch_queue>",R,V_workQueue
initWithConfidence:boundingBox:yaw:roll:faceRecognition:userInfo:
initWithConfidence:boundingBox:yaw:roll:faceRecognition:torsoAnnotation:userInfo:
initWithConfidence:boundingBox:userInfo:
attributeDescriptions
initWithName:value:
torsoAnnotation
roll
arrayByAddingObjectsFromArray:
shortDescription
encodeWithCoder:
encodeObject:forKey:
initWithCoder:
decodeObjectOfClass:forKey:
userInfo
supportsSecureCoding
initWithConfidence:boundingBox:
initWithConfidence:boundingBox:faceRecognition:
_yaw
_roll
_torsoAnnotation
T@"NSNumber",R,V_yaw
T@"NSNumber",R,V_roll
T@"HMITorsoAnnotation",R,V_torsoAnnotation
T@"NSUUID",R
T@"HMIFaceRecognition",R,V_faceRecognition
initWithTimeout:
mainInsideAutoreleasePool
dataSource
faceprints
cancelWithError:
finish
initWithDataSource:faceprints:
main
_dataSource
_faceprints
T@"<HMIPersonManagerDataSource>",R,V_dataSource
T@"NSSet",R,V_faceprints
whitespaceCharacterSet
stringByTrimmingCharactersInSet:
subdataWithRange:
position
readUInt32
readUInt64
seek:
readData:
_data
_position
localeWithLocaleIdentifier:
setLocale:
setDateFormat:
dateFromString:
stringWithCapacity:
appendFormat:
dataWithLength:
mutableBytes
time
T{?=qiIq},R
initWithValue:time:
_time
T{?=qiIq},R,V_time
T@,R,V_value
removeAllObjects
hmf_removeFirstObject
indexOfObject:inSortedRange:options:usingComparator:
insertObject:atIndex:
hmf_objectsPassingTest:
indexesOfObjectsPassingTest:
objectsAtIndexes:
removeObjectsAtIndexes:
objectAtIndex:
initWithMaxCapacity:
objectsInTimeRange:includeEnd:
extractObjectsInTimeRange:
neighborsOfObject:
_lock
_maxCapacity
initWithTime:date:
date
_date
T@"NSDate",R,V_date
lastObject
dateAtTime:
timeIntervalSinceDate:
addDate:atTime:
timeIntervalSinceDateAtTime:
_buffer
addValue:
startedAtTime:
endedAtTime:
averageTime
_timeline
_average
initWithWindowSize:
numberWithDouble:
addNumber:
movingAverage
movingAverageForInterval:defaultValue:
valueForInterval:defaultValue:
isInternalInstall
distantPast
timeIntervalSinceNow
unlock
motionScore
numberWithFloat:
encodeDouble:forKey:
decodeDoubleForKey:
initWithConfidence:boundingBox:motionScore:
_motionScore
Tf,R,V_motionScore
currentTorsoRequestRevision
transferPixelBuffer:crop:size:pixelFormat:options:error:
setRevision:error:
observationWithRequestRevision:boundingBox:
setInputDetectedObjectObservations:
sharedInstance
vnSession
initWithCVPixelBuffer:options:session:
performRequests:error:
results
firstObject
torsoprint
descriptorData
initWithUUID:data:
currentModelUUID
createTorsoPixelBufferForTorsoEvent:pixelBuffer:error:
torsoprintForTorsoPixelBuffer:error:
faceCropUUIDs
removeFaceCropsWithUUIDs:completion:
initWithDataSource:faceCropUUIDs:
_faceCropUUIDs
T@"<HMIHomePersonManagerDataSource>",R,V_dataSource
T@"NSSet",R,V_faceCropUUIDs
initWithUUID:homeUUID:
setMaxConcurrentOperationCount:
initWithTimeInterval:options:
resume
dictionary
_updateSettings:
addExecutionBlock:
operationQueue
addOperation:
watchdogTimer
taskServiceClient
homeUUID
objectForKeyedSubscript:
isEqualToString:
submitTaskWithOptions:completionHandler:
initWithDataSource:faceCrop:
error
setCompletionBlock:
hmiPrivateErrorWithCode:
initWithDataSource:faceprint:
familiarity
sourceUUID
na_firstObjectPassingTest:
unknownFacesSavedCounts
intValue
numberWithInt:
setObject:forKeyedSubscript:
faceCrop
faceprint
storeUnassociatedFaceCrop:completion:
storeFaceprint:completion:
isPersonDataAvailableViaHomeKit
initWithSourceUUID:homeUUID:external:
suspend
analyticsTimer
settings
isFaceClassificationEnabled
standardUserDefaults
doubleForKey:
timeIntervalSinceReferenceDate
setDouble:forKey:
timerDidFire:
setDataSource:
handleUpdatedPerson:
handleUpdatedUnassociatedFaceCrop:
handleUpdatedPersonFaceCrop:
handleUpdatedFaceprint:
handleUpdatedSettings:
handleRemovedPersonWithUUID:
handleRemovedFaceCropWithUUID:
handleRemovedFaceprintWithUUID:
handleMisclassifiedPersonForFaceCrop:
handleNewFaceEvents:
_settings
_operationQueue
_watchdogTimer
_analyticsTimer
_unknownFacesSavedCounts
T@"NSOperationQueue",R,V_operationQueue
T@"HMFTimer",R,V_watchdogTimer
T@"HMFTimer",R,V_analyticsTimer
T@"NSMutableDictionary",R,V_unknownFacesSavedCounts
T@"HMIHomePersonManagerSettings",R,V_settings
T@"<HMIHomePersonManagerDataSource>",W,N,V_dataSource
setWithObject:
addFaceCrops:completion:
_faceCrop
T@"HMIFaceCrop",R,V_faceCrop
_faceprint
T@"HMIFaceprint",R,V_faceprint
initWithPixelBuffer:presentationTime:frameId:fragmentSequenceNumber:
presentationTime
frameId
initWithPixelBuffer:
initWithJPEGData:presentationTime:frameId:fragmentSequenceNumber:size:
fragmentSequenceNumber
pixelBuffer
jpegData
motionDetections
setMotionDetections:
_frameId
_fragmentSequenceNumber
_pixelBuffer
_jpegData
_motionDetections
_size
_presentationTime
T^{__CVBuffer=},R,V_pixelBuffer
T@"NSData",R,V_jpegData
T@"NSArray",&,V_motionDetections
T{?=qiIq},R,V_presentationTime
T{CGSize=dd},R,V_size
TQ,R,V_frameId
TQ,R,V_fragmentSequenceNumber
point
initWithName:
createPixelBufferWithSize:pixelFormat:useIOSurface:
numberWithUnsignedLong:
dictionaryWithDictionary:
setObject:forKey:
appendBytes:length:
createPixelBufferFromJPEGDataProvider:error:
imageWithData:
extent
contextWithOptions:
render:toCVPixelBuffer:
applyPadding:withOriginalSize:padding:
globalSession
releaseCachedResources
cropPixelBuffer:crop:error:
resizePixelBuffer:size:error:
resizePixelBuffer:size:pixelFormat:options:error:
transferPixelBuffer:pixelFormat:options:error:
createJPEGDataFromPixelBuffer:scale:encodeQuality:error:
createPixelBufferFromJPEGPath:error:
createPixelBufferFromImageData:error:
imposeMinSizeFor:withOriginalSize:minCrop:
maintainAspectRatio:originalSize:ratioThreshold:
transferPixelBuffer:rotationAngle:crop:size:precision:error:
releaseCachedVisionResources
initWithFirstIndex:secondIndex:score:
firstIndex
secondIndex
score
_score
_firstIndex
_secondIndex
TQ,R,V_firstIndex
TQ,R,V_secondIndex
Tf,R,V_score
compare:
initWithService:
readValue
_service
_updateThermalLevel
services
objectForKey:
readValueFromSensor:value:
readMaxValue:
thermalLevel
_client
_thermalLevelNotificationToken
_notificationQueue
_thermalLevel
_services
T@"NSMutableDictionary",R,V_services
TQ,R,V_thermalLevel
stringWithUTF8String:
integerValue
unsignedLongLongValue
tick
numberWithUnsignedLongLong:
setZeroPadsFractionDigits:
setAllowedUnits:
setCountStyle:
setAllowsNonnumericFormatting:
stringFromByteCount:
highWaterMark
initWithName:reason:userInfo:
stop
setHighWaterMark:
average
_highWaterMark
_tick
T@"HMFTimer",R,V_tick
T@"MovingAverage",R,V_average
Tq,V_highWaterMark
floatValue
inputDimensions
defaultAssetPath
fileURLWithPath:
initWithModelURL:
arrayWithCapacity:
_runNeuralNetworkOnPixelBuffer:offsetsZero:offsetsOne:scores:yaws:rolls:error:
_postProcessOffsetsZero:offsetsOne:scores:yaws:rolls:outputPredictions:
boolPreferenceForKey:defaultValue:
initWithPixelBuffer:presentationTimeStamp:
printWithScale:
inputFeatureValueName
initWithPixelBuffer:inputName:
sharedModel
predictionFromFeatures:error:
hmiPrivateErrorWithCode:underlyingError:
offsetsZeroFeatureValueNames
featureValueForName:
type
multiArrayValue
offsetsOneFeatureValueNames
scoresFeatureValueNames
yawsFeatureValueNames
rollsFeatureValueNames
shape
unsignedLongValue
strides
dataPointer
initWithLabelIndex:confidence:unclampedBoundingBox:yaw:roll:
nmsConfiguration
nmsMultiClass:output:nmsConfiguration:
labelIndex
initWithLabelIndex:confidence:boundingBox:yaw:roll:
initWithThresholdWithLabels:metricWithLabels:thresholdDefault:metricDefault:
pathForResource:ofType:
defaultNMSConfiguration
T@"HMIMLModel",R
initWithConfidenceThresholds:nmsConfiguration:assetPath:error:
predict:detectedObjects:error:
_confidenceThresholds
_anchorStrides
_inputFeatureValueName
_offsetsZeroFeatureValueNames
_offsetsOneFeatureValueNames
_scoresFeatureValueNames
_yawsFeatureValueNames
_rollsFeatureValueNames
_nmsConfiguration
_inputDimensions
T@"NSString",R,V_inputFeatureValueName
T@"NSArray",R,V_offsetsZeroFeatureValueNames
T@"NSArray",R,V_offsetsOneFeatureValueNames
T@"NSArray",R,V_scoresFeatureValueNames
T@"NSArray",R,V_yawsFeatureValueNames
T@"NSArray",R,V_rollsFeatureValueNames
T@"HMINMSConfiguration",R,V_nmsConfiguration
T{CGSize=dd},R,V_inputDimensions
initWithAsset:readVideoTrack:readAudioTrack:
assetReaderWithAsset:error:
_createOutputsForAsset:readVideo:readAudio:
tracksWithMediaType:
assetReaderTrackOutputWithTrack:outputSettings:
setAlwaysCopiesSampleData:
canAddOutput:
addOutput:
cancelReading
copyNextSampleBuffer
status
copyNextSampleBufferWithTrackIndexOutput:
startReading
_copyNextSampleBufferFromTrackOutput:
tracks
timeRange
string
base64EncodedDataWithOptions:
initWithAsset:
checkAndSaveCrashReportWithData:
_asset
_assetReader
_trackSamples
_trackOutputs
initWithCachePolicy:
setName:
setup
homes
hmf_firstObjectWithUUID:
personManager
residentDevices
isCurrentDevice
na_any:
uuid
photosPersonManagerWithUUID:
accessories
cameraProfiles
isSetup
defaultPrivateConfiguration
setOptions:
cachePolicy
setCachePolicy:
setDiscretionary:
homeKitOperationQueue
setDelegateQueue:
setDidUpdateHomes:
initWithHomeMangerConfiguration:
setHomeManager:
homeManager
dateWithTimeIntervalSinceNow:
_refreshBeforeDate:completionHandler:
isPrimary
initWithNoCaching
homePersonManagerForHomeUUID:
homeForHMPersonManagerUUID:
homePersonManagersForCurrentDevice
photosPersonManagerForHomeUUID:sourceUUID:
isCurrentDevicePrimaryResident
cameraProfileWithUUID:
homeWithCameraProfileUUID:
_setup
_homes
_homeKitOperationQueue
_cachePolicy
_homeManager
T@"NSOperationQueue",R,V_homeKitOperationQueue
TB,R,GisSetup,V_setup
TQ,R,V_cachePolicy
T@"HMHomeManager",&,V_homeManager
T@"NSArray",R,V_homes
didUpdateHomes
homeManager:didUpdateAuthorizationStatus:
homeManagerDidUpdateHomes:
homeManagerDidUpdatePrimaryHome:
homeManager:didAddHome:
homeManager:didRemoveHome:
homeManager:didReceiveAddAccessoryRequest:
_didUpdateHomes
T@?,C,V_didUpdateHomes
initWithName:deferred:
begin
signpostLog
beginDate
hasBegun
shouldSignpost
signpostIdentifier
identifier
_name
_beginDate
_signpostIdentifier
_identifier
T@"NSDate",R,V_beginDate
TQ,R,V_signpostIdentifier
T@"NSUUID",R,C,V_identifier
T@"NSString",R,C,V_name
getUUIDBytes:
dataWithBytesNoCopy:length:freeWhenDone:
getBytes:range:
inputName
setWithArray:
input
featureValueWithMultiArray:
featureNames
T@"NSSet",R,N
initWithInput:inputName:
setInput:
_input
_inputName
T@"MLMultiArray",&,N,V_input
T@"NSString",R,V_inputName
modelWithContentsOfURL:error:
doubleValue
setObject:atIndexedSubscript:
dataScalerInputName
scalerModel
dataScalerOutputName
svmInputName
mlModel
svmOutputName
dictionaryValue
modelPathForResource:
defaultRecognizabilityModelPath
defaultRecognizabilityDataScalerPath
defaultAestheticQualityModelPath
defaultAestheticQualityDataScalerPath
initWithModelPath:dataScalerPath:error:
predict:output:error:
_mlModel
_scalerModel
T@"MLModel",R,V_mlModel
T@"MLModel",R,V_scalerModel
_status
_error
Tq,R
T@"NSError",R
handleVideoSampleBuffer:
handleAudioSampleBuffer:
exceptionWithName:reason:userInfo:
handleSampleBuffer:
finishWithCompletionHandler:
flushAsync
initWithConfiguration:
configuration
dynamicConfiguration
setDynamicConfiguration:
_configuration
_dynamicConfiguration
T@"HMIVideoAnalyzerConfiguration",R,V_configuration
T@"HMIVideoAnalyzerDynamicConfiguration",&,V_dynamicConfiguration
task
setTask:
_task
T@"HMIDESBackgroundTask",&,V_task
photosPersonManager
personFromHomePerson:
source
initWithUUID:dataRepresentation:dateCreated:faceBoundingBox:personUUID:source:
modelUUID
faceCropUUID
initWithUUID:data:modelUUID:faceCropUUID:
addOrUpdateFaceprints:completion:
initWithHMPhotosPersonManager:
setPhotosPersonManager:
_photosPersonManager
T@"HMPhotosPersonManager",&,V_photosPersonManager
mainBundle
localizedStringForKey:value:table:
errorWithDomain:code:userInfo:
createNSErrorWithStatus:andMessage:
createNSExceptionWithStatus:andMessage:
initWithUUID:dataRepresentation:dateCreated:faceBoundingBox:
encodeInteger:forKey:
containsValueForKey:
decodeIntegerForKey:
copyWithZone:
TB,R
initWithUUID:dataRepresentation:dateCreated:faceBoundingBox:personUUID:
_personUUID
_source
T@"NSUUID",R,C,V_personUUID
Tq,R,V_source
redactedCopy
na_filter:
eventClasses
maxConfidenceEventForEventClass:
sortedArrayUsingComparator:
mutableCopy
setByAddingObjectsFromSet:
replaceObjectAtIndex:withObject:
removeObjectAtIndex:
decodeObjectOfClasses:forKey:
decodeRectForKey:
encodeRect:forKey:
combineFrameResults:withResults:
initWithFrame:events:
maxConfidenceEvents
_frame
_events
_regionOfInterest
T{CGRect={CGPoint=dd}{CGSize=dd}},R,V_regionOfInterest
T@"HMIVideoFrame",R,V_frame
T@"NSSet",R,V_events
initWithBoundingBox:timeStamp:blobArea:blobID:
timeStamp
blobArea
blobID
_blobID
_blobArea
_timeStamp
T{?=qiIq},R,V_timeStamp
Tf,R,V_blobArea
TS,R,V_blobID
blobs
stationaryIndexToBoundingBox:
initWithValue:levelThresholds:
shortNameForEventClass:
initWithBlob:
appendBlob:
isExpiredAtTimeStamp:
isStationaryAtTimeStamp:
similarityToBoundingBox:
lastBlob
createEventForEventClass:
_blobs
_eventClasses
T@"NSMutableArray",R,V_blobs
T@"NSMutableSet",R,V_eventClasses
T@"HMIForegroundBlob",R
T@"NSString",R
reset
_invalidateBackgroundForPixelBuffer:timeStamp:
setBackgroundChangeTimeStamp:
_ensureInternalBuffersForPixelBuffer:
numImages
minSampleSize
_predictForegroundFromPixelBuffer:timeStamp:
_updateBackgroundFromPixelBuffer:timeStamp:
foregroundTimeStamp
backgroundEvents
assignment
backgroundTimeStamp
_setAssignment:greaterThanType:value:boundingBox:scale:
_intersectionOverUnionFromBlob:boundingBox:assignment:
_stationaryTracks:timeStamp:
foregroundEvents
unionSet:
containsObject:
firstMotionDetectionInArray:withMode:
motionTimeStamps
valueWithBytes:objCType:
_expireMotionDetectionsAtTimeStamp:
backgroundExpireInterval
imageSize
backgroundChangeTimeStamp
backgroundChangeResetInterval
camera
videoPackageAnalyzerDidResetReferenceImageWithInterval:camera:
setBackgroundTimeStamp:
modelSize
_copyFromPixelBuffer:toInputBuffer:translateCol:translateRow:
runningMean
runningStd
_updateRunningMean:runningSquaredMean:fromInputBuffer:decay:
_updateRunningStd:withAuxBuffer:runningMean:runningSquaredMean:
setForegroundTimeStamp:
_foregroundPixelsFromPixelBuffer:attribute:assignment:
_blobsFromAssignment:timeStamp:
_foregroundBlobsFromBlobs:backgroundChanged:
adjustBrightness
backgroundChangeInterval
setAdjustBrightness:
rectValue
_correctRunningMeanBrightnessAtAttribute:
_updateCurrentTracks:blobs:timeStamp:
CMTimeValue
motionValidInterval
indexSetWithIndexesInRange:
containsIndex:
removeIndex:
removeObject:
enumerateIndexesUsingBlock:
setByAddingObjectsFromArray:
_exportBackgroundMean
_exportBackgroundStd
_exportForegroundDiffForPixelBuffer:
_exportForegroundAssignment
na_flatMap:
_exportInternalStateForPixelBuffer:exportMode:
_foregroundDifferencesFromPixelBuffer:differences:
_copyFromOutputBuffer:toPixelBuffer:
resizePixelBuffer:
analyzePixelBuffer:timeStamp:
hasNewBackground
assignBackgroundFromEvents:
assignForegroundFromEvents:regionOfInterest:pixelBuffer:timeStamp:
packageEvents
handleMotionDetection:inFrame:
exportInternalStateToReport:pixelBuffer:events:regionOfInterest:timeStamp:
setAssignment:
setRunningMean:
setRunningStd:
setNumImages:
setImageSize:
setModelSize:
_adjustBrightness
_tracks
_foregroundEvents
_backgroundEvents
_minSampleSize
_assignment
_runningMean
_runningStd
_numImages
_motionTimeStamps
_imageSize
_modelSize
_backgroundExpireInterval
_backgroundChangeInterval
_backgroundChangeResetInterval
_foregroundTimeStamp
_backgroundTimeStamp
_backgroundChangeTimeStamp
_motionValidInterval
T@"NSMutableSet",R,V_tracks
T@"NSMutableSet",R,V_foregroundEvents
T@"NSMutableArray",R,V_backgroundEvents
TQ,R,V_minSampleSize
T{?=qiIq},R,V_backgroundExpireInterval
T{?=qiIq},R,V_backgroundChangeInterval
T{?=qiIq},R,V_backgroundChangeResetInterval
T^S,V_assignment
T^f,V_runningMean
T^f,V_runningStd
TQ,V_numImages
T{CGSize=dd},V_imageSize
T{CGSize=dd},V_modelSize
T{?=qiIq},V_foregroundTimeStamp
T{?=qiIq},V_backgroundTimeStamp
T{?=qiIq},V_backgroundChangeTimeStamp
TB,V_adjustBrightness
T@"NSMutableArray",R,V_motionDetections
T@"NSMutableArray",R,V_motionTimeStamps
T{?=qiIq},R,V_motionValidInterval
T@"NSSet",R
URLWithString:
initWithURL:options:
resourceLoader
setDelegate:queue:
contentInformationRequest
setContentType:
setContentLength:
setByteRangeAccessSupported:
dataRequest
requestedOffset
requestedLength
respondWithData:
finishLoading
loadValuesAsynchronouslyForKeys:completionHandler:
resourceLoader:shouldWaitForLoadingOfRequestedResource:
resourceLoader:shouldWaitForRenewalOfRequestedResource:
resourceLoader:didCancelLoadingRequest:
resourceLoader:shouldWaitForResponseToAuthenticationChallenge:
resourceLoader:didCancelAuthenticationChallenge:
loadValuesSynchronously
initWithValue:
T@"NSNumber",R,V_value
windowSize
queue
setMovingAverage:
setQueue:
_movingAverage
_queue
_windowSize
T@"NSMutableArray",&,N,V_queue
TQ,R,N,V_windowSize
Td,V_movingAverage
_eventsFromAnalyzerEvents:
_annotationScoresFromAnalyzerEvents:
_detectionsFromAnalyzerEvents:
_faceClassificationsFromAnalyzerEvents:
numberWithInteger:
analyzerEvents
initWithFrame:events:annotationScores:detections:regionOfInterest:faceClassifications:
initWithFrame:regionOfInterest:analyzerEvents:
annotationScores
detections
faceClassifications
_annotationScores
_detections
_faceClassifications
_analyzerEvents
T@"HMICameraVideoFrame",R,V_frame
T@"NSDictionary",R,V_annotationScores
Tq,R,V_events
T@"NSArray",R,V_detections
T@"NSSet",R,V_faceClassifications
T@"NSSet",R,V_analyzerEvents
initWithBoundingBox:timeStamp:
Td,R,V_timeStamp
initWithEventClass:records:UUID:
eventClass
records
_eventClass
_records
_UUID
T#,R,V_eventClass
T@"NSArray",R,V_records
T@"NSUUID",R,V_UUID
eventClassForShortName:
eventForClass:boundingBox:UUID:
initWithPersonUUID:sourceUUID:sessionEntityUUID:confidence:familiarity:
initWithFaceCrop:faceprint:classifications:predictedLinkedEntityUUIDs:faceQualityScore:sessionEntityAssignment:sessionEntityUUID:
initWithConfidence:boundingBox:face:
initWithArray:
lastKnownTimeStamp
eventsForTimeStamp:
eventsForFragment
T@"NSArray",R,V_tracks
initWithLength:
fillWithFloat:
appendFloats:count:
unsignedIntegerValue
initWithFloats:count:
mutableFloats
appendData:
floats
subtract:
initWithDataTensor:
appendArray:
l2Norm
floatArrayByAdding:
floatArrayBySubtracting:
mutableCopyWithZone:
T@"NSData",R,&,N
TQ,R,N
Tr^f,R,N
T^f,R,N
numberPreferenceForKey:defaultValue:
initWithShape:dataType:error:
createFacePixelBufferForFaceEvent:pixelBuffer:roll:error:
computeJunkScoreForPixelBuffer:
qualityPredictionFromSVMUsingFaceQualityFilterSVM:detectorConfidence:laplacian:yaw:boxSize:error:
faceprinter
createFaceprintForFacePixelBuffer:fastMode:error:
predictPersonFromFaceObservation:homeUUID:error:
faceAttributes
facemaskCategory
label
classificationThresholdKnown
linkedEntityUUID
classificationThresholdUnknown
initWithUUID:sourceUUID:sessionEntityUUID:faceCrop:faceprint:confidence:familiarity:
classifyFaceEvent:pixelBuffer:fastMode:homeUUID:error:
initWithError:
faceRecognizabilityFilter
faceAestheticQualityFilter
_faceprinter
_faceRecognizabilityFilter
_faceAestheticQualityFilter
_classificationThresholdKnown
_classificationThresholdUnknown
T@"HMIFaceprinter",R,V_faceprinter
T@"HMIFaceQualityFilterSVM",R,V_faceRecognizabilityFilter
T@"HMIFaceQualityFilterSVM",R,V_faceAestheticQualityFilter
Td,R,V_classificationThresholdKnown
Td,R,V_classificationThresholdUnknown
initWithDictionary:regionOfInterest:
_confidence
_label
T@"NSNumber",R,V_confidence
T@"NSNumber",R,V_label
imageData
initWithDictionary:
probability
applyToImage:withProbability:
initWithImageData:regionOfInterest:detections:
createRegionOfInterestPixelBufferWithError:
arrayWithArray:
initWithData:type:shape:strides:
initWithBundleIdentifier:
isPermitted
saveRecordWithData:recordInfo:completion:
saveDESRecordWithVideoFrame:recordInfo:
augmentWithOptions:
createImageTensorWithError:
createBoxesTensorWithError:
createClassesTensorWithError:
createWeightsTensorWithError:
_boxesTensorData
_weightsTensorData
_classesTensorData
_imageData
T@"NSData",R,V_imageData
samples
imageName
boxesName
weightsName
classesName
initWithSamples:imageName:boxesName:weightsName:classesName:
dataPointAtIndex:error:
numberOfDataPoints
_samples
_imageName
_boxesName
_weightsName
_classesName
T@"NSArray",R,V_samples
T@"NSString",R,V_imageName
T@"NSString",R,V_boxesName
T@"NSString",R,V_weightsName
T@"NSString",R,V_classesName
initWithCVPixelBuffer:imageParameters:error:
setMaxNumberOfElements:
featureValueWithPixelBuffer:
thresholdWithLabels
thresholdDefault
metricWithLabels
metricDefault
thresholdForLabel:
metricForLabel:
_thresholdWithLabels
_metricWithLabels
_thresholdDefault
_metricDefault
T@"NSDictionary",R,V_thresholdWithLabels
T@"NSDictionary",R,V_metricWithLabels
T@"NSNumber",R,V_thresholdDefault
T@"NSNumber",R,V_metricDefault
_labelIndex
Ti,R,V_labelIndex
Td,R,V_confidence
T{CGRect={CGPoint=dd}{CGSize=dd}},R,N,V_boundingBox
nonMaximumSuppression:output:withThreshold:withMetric:
intersectionOverUnion:b:
intersectionOverMinArea:b:
convertObjectDetections:cropRect:originalImageSize:
indexSet
torsoRecognition
setBlobID:
faceDistanceFromDescriptor:toDescriptor:
initWithNewPersonEvent:timeStamp:
trackPersonBlob:
similarityToPersonBlob:
personIndices
setPersonIndices:
_torsoprint
_personIndices
T@"HMITorsoprint",R,V_torsoprint
T@"NSMutableIndexSet",&,V_personIndices
T@"NSUUID",&,V_blobID
previousPersons
removeAllIndexes
addIndex:
numberWithUnsignedInteger:
removeIndexes:
trackNewPersons:regionOfInterest:timeStamp:
setBlobID:atIndex:
getBlobIDAtIndex:
_previousPersons
T@"NSMutableArray",R,V_previousPersons
initWithFrame:events:regionOfInterest:motionDetections:
T@"NSArray",R,V_motionDetections
eventConfidenceThresholdsHigh
initWithMediumConfidenceThresholds:highConfidenceThresholds:analyzerConfiguration:error:
cameraVideoFrameAnalyzer
initWithInterval:
frameSampler
eventTriggers
recognizeFaces
analyze:targetEventTypes:enableFaceClassification:homeUUID:error:
initWithSampleBuffer:
redactFrames
frameAnalyzer:didAnalyzeFrame:error:
handleSampleBuffer:motionDetections:motionScore:
flushAndGetAnalysisStateUpdateForHome:enableFaceClassification:
frameAnalyzer:didProduceAnalysisStateUpdate:
frameSampler:didSampleFrame:
frameSampler:didDropFrame:
averageAnalysisTime
setFrameSampler:
_analysisTime
_delegate
_cameraVideoFrameAnalyzer
_frameSampler
T@"<HMICameraVideoFrameAnalyzer>",R,V_cameraVideoFrameAnalyzer
T@"HMIVideoFrameSampler",&,V_frameSampler
T@"<HMIVideoFrameAnalyzerDelegate>",W,V_delegate
Td,R
frameAnalyzerDidAnalyzeFrame
frameAnalyzerDidProduceAnalysisStateUpdate
setFrameAnalyzerDidAnalyzeFrame:
setFrameAnalyzerDidProduceAnalysisStateUpdate:
_frameAnalyzerDidAnalyzeFrame
_frameAnalyzerDidProduceAnalysisStateUpdate
T@?,C,V_frameAnalyzerDidAnalyzeFrame
T@?,C,V_frameAnalyzerDidProduceAnalysisStateUpdate
initWithTruth:prediction:score:
truth
prediction
_truth
_prediction
T@"NSNumber",R,V_truth
T@"NSNumber",R,V_prediction
valueForKeyPath:
initWithKey:classificationScore:
initWithKey:detectionScores:frameResultIndex:
initWithKey:trackingScores:frameResultIndices:
frameResultIndices
_key
_count
_frameResultIndices
T@"NSString",R,V_key
TQ,R,V_count
T@"NSArray",R,V_frameResultIndices
precision
recall
truePositive
falseNegative
falsePositive
initWithTruePositiveKeys:falseNegativeKeys:falsePositiveKeys:groupByKey:
truePositiveKeys
falseNegativeKeys
falsePositiveKeys
_precision
_recall
_truePositiveKeys
_falseNegativeKeys
_falsePositiveKeys
_truePositive
_falseNegative
_falsePositive
Tf,R,V_precision
Tf,R,V_recall
T@"NSArray",R,V_truePositiveKeys
T@"NSArray",R,V_falseNegativeKeys
T@"NSArray",R,V_falsePositiveKeys
Tq,R,V_truePositive
Tq,R,V_falseNegative
Tq,R,V_falsePositive
fragments
redactedCopyWithFrameResults:fragment:
initWithSource:
appendFragmentResult:redactFrames:
setSource:
_fragments
T@"NSString",C,V_source
T@"NSMutableArray",R,V_fragments
systemDeviceInformation
unarchivedObjectOfClass:fromData:error:
sessions
addEntriesFromDictionary:
allEvents
eventClassesArray
compareWithClassificationTruth:eventClass:confidenceThreshold:
averagePrecisionForMinPrecision:comparator:
compareWithDetectionTruth:eventClass:confidenceThreshold:iouThreshold:videoMetric:
chartDataWithBaseline:comparator:
archivedDataWithRootObject:requiringSecureCoding:error:
initWithContentsOfFile:
bundleWithIdentifier:
infoDictionary
numberWithBool:
fragment
outcome
initWithFragment:events:frameResults:thumbnails:configuration:outcome:
na_dictionaryByMappingValues:
initWithJPEGData:size:presentationTimeStamp:
greedyMatchBetweenPredictionEvents:truthEvents:falsePositiveIndices:falseNegativeIndices:eventClass:regionOfInterest:confidenceThreshold:scoreThreshold:scoreFunction:
enumerateKeysAndObjectsUsingBlock:
sihouetteScoreForMatches:previousMatches:truePositiveScores:falsePositiveScores:falseNegativeScores:
setArray:
exchangeObjectAtIndex:withObjectAtIndex:
subarrayWithRange:
stringByAppendingPathComponent:
selectFramesWithRecord:truth:frameResults:
stringByDeletingPathExtension
writeImageCropFromFrame:events:outputPath:source:
pixelBufferFrameWithError:
writeToFile:atomically:encoding:error:
allKeys
localizedStandardCompare:
sortedArrayUsingSelector:
null
dataWithJSONObject:options:error:
initWithCVPixelBuffer:
fileURLWithPath:relativeToURL:
colorSpace
writePNGRepresentationOfImage:toURL:format:colorSpace:options:error:
version
deviceInformation
T@"NSDictionary",R
initWithData:error:
appendFragmentResult:forKey:source:redactFrames:
appendFragmentResultsFromReport:
averagePrecisionWithClassificationTruth:minPrecision:
averagePrecisionWithDetectionTruth:minPrecision:iouThreshold:videoMetric:
chartDataWithClassificationTruth:isBaseline:
chartDataWithDetectionTruth:isBaseline:iouThreshold:videoMetric:
truthReportFromLegacyClassificationFormat:
truthReportFromLegacyDetectionFormat:
compareWithTrackingTruth:eventClass:confidenceThreshold:ioaThreshold:
writeHTMLReportComparison:truth:eventClass:comparisonType:assetPath:outputPath:limit:shuffle:
writeImageCropForEventClass:outputPath:assetPath:
writeFragmentFileComparison:eventClass:outputPath:
chartSpecWithRange:colors:labels:
_version
_deviceInformation
_sessions
T@"NSString",R,V_name
Tq,R,V_version
T@"NSDictionary",R,V_deviceInformation
T@"NSMutableDictionary",R,V_sessions
T@"NSData",R
detectFacesInImageData:error:
unalignedBoundingBox
faceBoxFromPhotosFaceCropImageData:
newDictionaryPopulatedWithFaceCropDataFromImageData:
JSONObjectWithData:options:error:
isEqualToData:
isEqualToDate:
faceCropFromPhotosFaceCropImageData:
_dataRepresentation
_dateCreated
_faceBoundingBox
T@"NSUUID",R,C,V_UUID
T@"NSData",R,C,V_dataRepresentation
T@"NSDate",R,C,V_dateCreated
T{CGRect={CGPoint=dd}{CGSize=dd}},R,V_faceBoundingBox
retimer:didRetimeSampleBuffer:
flushWithNextSamplePTS:
_lastSample
T@"<HMIVideoRetimerDelegate>",W,V_delegate
retimerDidRetimeSampleBuffer
setRetimerDidRetimeSampleBuffer:
_retimerDidRetimeSampleBuffer
T@?,C,V_retimerDidRetimeSampleBuffer
initWithIdentifier:name:manufacturer:model:firmwareVersion:
manufacturer
model
firmwareVersion
initWithIdentifier:name:manufacturer:model:
_manufacturer
_model
_firmwareVersion
T@"NSUUID",R,V_identifier
T@"NSString",R,V_manufacturer
T@"NSString",R,V_model
T@"NSString",R,V_firmwareVersion
initWithTaskID:timeout:
code
initWithTaskID:
taskID
_taskID
Ti,R,V_taskID
initWithTaskID:homeUUID:timeout:
_homeUUID
T@"NSUUID",R,V_homeUUID
initPrivate
nextTaskID
setNextTaskID:
buildUpdatePersonsModelTaskFromOptions:error:
buildRemovePersonsModelTaskFromOptions:error:
buildHomePersonClusteringTaskFromOptions:error:
buildTuriTrialUpdateTaskFromOptions:error:
getNextTaskID
buildFaceMisclassificationTaskFromOptions:error:
buildPersonsModelsSummaryTaskFromOptions:error:
buildSubmitFeedbackTaskFromOptions:error:
buildUpdateTorsoModelTaskFromOptions:error:
submitTask:completionHander:
operations
cancel
boolValue
stringPreferenceForKey:defaultValue:
initWithHMHomePersonManager:
initWithHomeUUID:sourceUUID:error:
initWithTaskID:homeUUID:sourceUUID:dataSource:externalLibrary:removeExcessFaceCrops:
initWithTaskID:homeUUID:sourceUUID:
initWithTaskID:dataSource:faceCrop:
initWithTaskID:homeUUID:dataSource:sourceUUID:personsModelManager:doImpurePersonCleanup:error:
initWithTaskID:homeUUID:
initWithTaskID:cameraProfileUUID:clipUUID:
initWithTaskID:homeUUID:torsoAnnotations:
cancelTask:
_nextTaskID
Ti,V_nextTaskID
taskService
allowedClasses
privateDescription
propertyDescription
T@"NSArray",R,C,N
initWithDataSource:
persons
_persons
T@"NSSet",R,V_persons
classification
sessionEntityAssignment
initWithTorsoprint:classification:predictedLinkedEntityUUIDs:sessionEntityAssignment:sessionEntityUUID:
predictedLinkedEntityUUIDs
_classification
_predictedLinkedEntityUUIDs
_sessionEntityAssignment
_sessionEntityUUID
T@"HMITorsoClassification",R,V_classification
T@"NSSet",R,V_predictedLinkedEntityUUIDs
Tq,R,V_sessionEntityAssignment
T@"NSUUID",R,V_sessionEntityUUID
existingAtCurrentVersion
isSentinelFaceprint
createdAtCurrentVersion
initWithExistingAtCurrentVersion:createdAtCurrentVersion:existingAtOtherVersions:
allAtCurrentVersion
existingAtOtherVersions
_existingAtOtherVersions
_createdAtCurrentVersion
_existingAtCurrentVersion
T@"NSSet",R,V_existingAtOtherVersions
T@"NSSet",R,V_createdAtCurrentVersion
T@"NSSet",R,V_existingAtCurrentVersion
faceprintDefaultRevision
_minorVersionFromVisionVersion:
faceObservationWithRequestRevision:unalignedBoundingBox:alignedBoundingBox:
setInputFaceObservations:
setDetectionLevel:
setRevision:
setFaceprint:
createFacePixelBufferForFaceDetection:pixelBuffer:roll:error:
createFacePixelBufferFromFaceCrop:error:
generateFaceprintForFaceCrop:error:
sentinelFaceprintWithUUID:modelUUID:faceCropUUID:
warmStart
updatedFaceprintsForFaceCrops:withExistingFaceprints:error:
shouldUseCPUOnlyForVisionFaceDetection
setUsesCPUOnly:
defaultRevision
initWithCVPixelBuffer:options:
initWithData:options:session:
detectFacesInPixelBuffer:error:
_modelUUID
_faceCropUUID
sentinelFaceprint
TB,R,GisSentinelFaceprint
T@"NSData",R,C,V_data
T@"NSUUID",R,C,V_modelUUID
T@"NSUUID",R,C,V_faceCropUUID
initWithPoint:
_point
T{CGPoint=dd},R,V_point
allocWithZone:
setFaceClassificationEnabled:
encodeBool:forKey:
decodeBoolForKey:
_faceClassificationEnabled
faceClassificationEnabled
TB,GisFaceClassificationEnabled,V_faceClassificationEnabled
TB,D,GisFaceClassificationEnabled
fileExistsAtPath:
fileHandleForReadingAtPath:
assetDirectoryPath
unpackageModelAssets:intoDirectory:error:
removeItemAtPath:error:
unTarFileWithFd:toPath:
unpackageModelAssetsAtPath:error:
pendingUpdates
setPendingUpdates:
_pendingUpdates
T@"NSObject<OS_dispatch_queue>",&,N,V_queue
T@"NSMutableSet",&,N,V_pendingUpdates
torsoAnnotations
updateTorsoModelForHome:torsoAnnotations:error:
_torsoAnnotations
T@"NSSet",R,V_torsoAnnotations
defaultSessionConfiguration
sessionWithConfiguration:delegate:delegateQueue:
URLSession:didBecomeInvalidWithError:
URLSession:didReceiveChallenge:completionHandler:
URLSessionDidFinishEventsForBackgroundURLSession:
homeKitClient
session
feedbackServiceHost
_homeKitClient
_session
_feedbackServiceHost
T@"NSURLSession",R,V_session
T@"NSString",R,V_feedbackServiceHost
T@"HMIHomeKitClient",R,V_homeKitClient
protectionSpace
host
serverTrust
credentialForTrust:
authenticationMethod
feedbackSession
_temporaryFileURLWithUUID:extension:error:
clipManager
initWithClipManager:clip:
setClipDestinationFileURL:
hmiPrivateErrorWithCode:description:suggestion:underlyingError:
setFetchVideoAssetContextCompletionBlock:
setDownloadProgressHandler:
significantEvents
faceClassification
person
setFaceCrops:
fetchClipWithUUID:completion:
blurFacesFromAssetURL:outputURL:duration:analysisFPS:windowSize:faceDetected:
assetWithURL:
isReadable
statusOfValueForKey:error:
composition
addMutableTrackWithMediaType:preferredTrackID:
duration
insertTimeRange:ofTrack:atTime:error:
initWithAsset:presetName:
setOutputFileType:
setOutputURL:
setShouldOptimizeForNetworkUse:
setTimeRange:
exportAsynchronouslyWithCompletionHandler:
feedbackServiceURL
requestWithURL:
setHTTPMethod:
setValue:forHTTPHeaderField:
uploadTaskWithRequest:fromFile:completionHandler:
_base64StringFromData:
faceCrops
_attachEncryptedDataUsingKey:toPayload:error:
setAssetData:
serviceResult
_createPayloadWithServiceResult:error:
_uploadPayloadData:uploadURL:completionHandler:
_stripAudioTrackAndFacesFromAsset:completionHandler:
setServiceResult:
_downloadClipWithCameraProfileUUID:clipUUID:completionHandler:
_requestPreSignedURLWithClipUUID:completionHandler:
feedbackRequestURLForClipWithUUID:
dataTaskWithURL:completionHandler:
removeItemAtURL:error:
_removeTemporaryFiles
_submitClipWithCameraProfileUUID:clipUUID:completionHandler:
initWithFeedbackSession:cameraProfileUUID:clipUUID:
_attachFaceCrops:toPayload:error:
cameraProfileUUID
clipUUID
temporaryFileURLs
assetData
_feedbackSession
_cameraProfileUUID
_clipUUID
_temporaryFileURLs
_faceCrops
_assetData
_serviceResult
T@"HMIFeedbackSession",R,V_feedbackSession
T@"NSUUID",R,V_cameraProfileUUID
T@"NSUUID",R,V_clipUUID
T@"NSMutableArray",R,V_temporaryFileURLs
T@"NSSet",&,V_faceCrops
T@"NSData",&,V_assetData
T@"NSDictionary",&,V_serviceResult
setQualityOfService:
_operation
submitFeedbackWithCameraProfileUUID:clipUUID:runRemotely:completionHandler:
submitFeedbackWithCameraProfileUUID:clipUUID:completionHandler:
initWithUUID:sourceUUID:faceBoundingBox:
initWithUUID:sourceUUID:sessionEntityUUID:faceBoundingBox:facecrop:faceprint:confidence:
initWithUUID:sourceUUID:sessionEntityUUID:faceCrop:faceprint:confidence:fromTorsoClassification:familiarity:
initWithUUID:name:personsModelIdentifier:faceBoundingBox:
personsModelIdentifier
_fromTorsoClassification
_personsModelIdentifier
_sourceUUID
_familiarity
TB,R,V_fromTorsoClassification
T@"NSString",R,V_identifier
T@"NSString",R,V_personsModelIdentifier
T@"NSUUID",R,V_personUUID
T@"NSUUID",R,V_sourceUUID
Tq,R,V_familiarity
personToFaceCrops
allValues
unassociatedFaceCrops
existingPersonUUIDs
isSubsetOfSet:
intersectsSet:
existingPersonFaceCropUUIDs
removeObjectForKey:
existingFaceCropUUIDs
setWithSet:
hmf_isEqualToUUID:
fetchAllUnassociatedFaceCropsWithCompletion:
addPersons:completion:
removePersonsWithUUIDs:completion:
associateFaceCropsWithUUIDs:toPersonWithUUID:forSource:completion:
removedPersonFaceCrops
_personToFaceCrops
_unassociatedFaceCrops
_removedPersonFaceCrops
T@"NSMutableDictionary",R,V_personToFaceCrops
T@"NSMutableSet",R,V_unassociatedFaceCrops
T@"NSSet",R,V_removedPersonFaceCrops
summaryForHomeUUID:error:
sendEventForPersonsModels:
removeNearestFaceprint:withFaceCrops:
T@"HMIPersonFaceCrop",R,V_faceCrop
removePersonsModelForHomeUUID:sourceUUID:error:
setDecoderDidDecodeSampleBuffer:
reader
handleSampleBuffer:outputFrame:
_reader
T@"HMIVideoAssetReader",R,V_reader
hmf_zeroUUID
initWithUUID:
supportsFaceClassification
setSupportsFaceClassification:
setPersonDataAvailableViaHomeKit:
_supportsFaceClassification
_personDataAvailableViaHomeKit
T@"NSUUID",R,C,V_homeUUID
T@"<HMIPersonManagerDataSource>",W,N,V_dataSource
TB,V_supportsFaceClassification
personDataAvailableViaHomeKit
TB,GisPersonDataAvailableViaHomeKit,V_personDataAvailableViaHomeKit
levelThresholds
level
characterAtIndex:
_levelThresholds
T@"NSArray",R,V_levelThresholds
Td,R,V_value
_hmiErrorWithCode:description:reason:suggestion:underlyingError:
hmiErrorWithCode:underlyingError:
hmiErrorWithCode:
hmiErrorWithCode:description:
hmiErrorWithCode:description:underlyingError:
initWithData:timeRange:
initWithInitializationSegment:separableSegment:timeRange:
isCombinableWithFragment:
separableSegment
sequenceNumbers
initializationSegment
initWithInitializationSegment:separableSegment:
firstVideoSampleByteRange
initWithInitializationSegment:separableSegment:timeRange:sequenceNumbers:firstVideoSampleByteRange:
redactedCopyWithMetadata
initWithInitializationSegment:separableSegment:timeRange:firstVideoSampleByteRange:
numberWithUnsignedInt:
_ensureAttributes
videoFormatDescription
audioFormatDescription
canFragmentData:
dataWithData:
videoTrackTimeRange
decodeCMTimeRangeForKey:
encodeCMTimeRange:forKey:
isInitializationSegment:combinableWithInitializationSegment:
fragmentData:handler:
initWithFragments:
placeholderCopy
initWithInitializationSegment:separableSegment:sequenceNumbers:
initWithInitializationSegment:separableSegment:timeRange:sequenceNumbers:
audioTrackTimeRange
baseDecodeTimeStamp
frameReorderingRequired
sanitizedData
sanitizedSeperableSegment
sequenceNumber
_attributesLoaded
_frameReorderingRequired
_videoFormatDescription
_audioFormatDescription
_initializationSegment
_separableSegment
_sequenceNumbers
_firstVideoSampleByteRange
_baseDecodeTimeStamp
_videoTrackTimeRange
_audioTrackTimeRange
_timeRange
T@"NSData",R,C
Tr^{opaqueCMFormatDescription=},R,V_videoFormatDescription
Tr^{opaqueCMFormatDescription=},R,V_audioFormatDescription
T{?={?=qiIq}{?=qiIq}},R,V_videoTrackTimeRange
T{?={?=qiIq}{?=qiIq}},R,V_audioTrackTimeRange
TB,R,V_frameReorderingRequired
T{?=qiIq},R,V_baseDecodeTimeStamp
T{_NSRange=QQ},R,V_firstVideoSampleByteRange
T@"NSData",R,V_initializationSegment
T@"NSData",R,V_separableSegment
T{?={?=qiIq}{?=qiIq}},R,V_timeRange
T@"NSArray",R,V_sequenceNumbers
sessionEntities
assignSessionEntityToFaceRecognition:torsoRecognition:predictedLinkedEntityUUIDs:availableSessionEntityUUIDs:sessionEntityAssignment:
clusterSessionEntityToFaceRecognition:torsoRecognition:predictedLinkedEntityUUIDs:availableSessionEntityUUIDs:sessionEntityAssignment:
createSessionEntityWithUUID:faceRecognition:torsoRecognition:predictedLinkedEntityUUIDs:sessionEntityAssignment:
updatePreviousPrintsForSessionEntityUUID:faceRecognition:torsoRecognition:
faceVIPThresholdForTorsoAnnotation
updatePersonEventWithPersonEvent:torsoAnnotation:sessionEntityUUID:predictedLinkedEntityUUIDs:sessionEntityAssignment:
initWithFaceRecognition:torsoprints:
submitTorsoprintsToModelManagerForHome:withTorsoAnnotations:
faceQualityScore
initWithPersonUUID:sourceUUID:confidence:
initWithConfidence:boundingBox:roll:torsoRecognition:
initWithConfidence:boundingBox:face:torso:
assignSessionEntitiesToPersonEvents:regionOfInterest:timeStamp:homeUUID:
updateTorsoModelAndGetTorsoAnnotationsForHome:
_sessionUUIDToPreviousFaceprints
_sessionUUIDToPreviousTorsoprints
_personTracker
_sessionEntities
_faceVIPThresholdForTorsoAnnotation
T@"NSMutableDictionary",R,V_sessionEntities
Td,R,V_faceVIPThresholdForTorsoAnnotation
personUUIDs
initWithDataSource:personUUIDs:
_personUUIDs
T@"NSSet",R,V_personUUIDs
initWithDataSource:faceprintUUIDs:
T@"NSSet",R,V_faceprintUUIDs
configure
loadModelFromTrialWithError:
initWithTorsoAnnotations:
initWithTorsoAnnotationsArray:
stateUpdateByMergingStateUpdate:
stateManager:didReceiveLocalUpdate:
handleRemoteStateUpdate:completionHandler:
shouldEnableTorsoRecognition
torsoModelVersion
initWithHomeUUID:
publishLocalState:
handleRemoteStateUpdate:
stateUpdateFromFaceEvents:
T@"<HMIAnalysisStateManagerDelegate>",W,V_delegate
_createFontWithSize:
initWithString:attributes:
drawText:at:color:
drawRect:width:color:
initWithPixelBuffer:fontSize:
draw:
drawTextHeaderBar:
drawBoundingBox:lineWidth:text:color:
drawPolygonWithNormalizedPoints:
_context
_colorSpace
_font
resourceUsageMonitor
_resourceUsageMonitor
T@"<HMISystemResourceUsageMonitorProtocol>",R,V_resourceUsageMonitor
T@"<HMISystemResourceUsageMonitorDelegate>",W,Vdelegate
initWith
applyWithFrameResult:
activityZones
filterEvents:withActivityZones:motionDetection:insetPercentageInclusion:insetPercentageExclusion:
initWithActivityZones:motionDetections:
_activityZones
T@"NSArray",R,V_activityZones
componentsSeparatedByString:
initWithUUIDString:
initWithPersonUUID:sourceUUID:
personManagerUUID
personSourceUUIDPairFromPersonLink:
initWithUUIDPairString:
UUIDPairString
T@"NSUUID",R,C,V_sourceUUID
initWithUUID:name:personLinks:
defaultFormatter
initWithName:value:options:formatter:
personLinks
initWithUUID:name:
_personLinks
T@"NSSet",R,C,V_personLinks
initWithSourceUUID:externalLibrary:faceCountsByPerson:
isExternalLibrary
faceCountsByPerson
_externalLibrary
_faceCountsByPerson
externalLibrary
TB,R,GisExternalLibrary,V_externalLibrary
T@"NSDictionary",R,V_faceCountsByPerson
visionPersonsModel
personUniqueIdentifiers
faceCountForPersonWithUniqueIdentifier:
initWithPersonsModel:homeUUID:sourceUUID:externalLibrary:
summary
_visionPersonsModel
T@"VNPersonsModel",R,V_visionPersonsModel
T@"HMIPersonsModelSummary",R
_addValueToContainer:forKey:
_containerIsArray
setValue:forKey:
_valueForNumber:
stringFromDate:
_JSONObjectWithObject:options:
initWithDictionary
_addClassToContainer:
object
numberWithLongLong:
decimalNumberWithString:
JSONObjectStringWithObject:pretty:options:
JSONObjectWithObject:options:
JSONObjectStringWithObject:
allowsKeyedCoding
initWithArray
encodeInt32:forKey:
encodeInt64:forKey:
objectJSON
objectPrettyJSON
options
_container
_options
Tq,V_options
hasPrefix:
hasSuffix:
_objectWithJSONObject:allowedClasses:
classMap
initWithJSONObject:
setClassMap:
objectWithJSONData:classMap:
objectWithJSONObject:classMap:
objectWithJSONObjectString:classMap:
decodeInt32ForKey:
decodeInt64ForKey:
container
_classMap
T@,R,V_container
T@"NSDictionary",&,V_classMap
timeZoneForSecondsFromGMT:
setTimeZone:
initWithLocaleIdentifier:
faceObservationWithRequestRevision:boundingBox:andAlignedBoundingBox:
initWithData:elementCount:elementType:lengthInBytes:confidence:requestRevision:
setFaceId:
initWithFaceEvent:torso:
initWithFaceEvent:
initWithTorsoEvent:
URLByAppendingPathExtension:
writeToURL:atomically:encoding:error:
faceObservationsFromFaceprintsForClustering:
faceObservationFromFaceprint:
mergedPersonEventsFromEvents:
saveFaceClassifications:videoId:fragmentId:frameId:baseURL:error:
initWithFaceRecognition:torsoprints:torsoModelVersion:
isEqualToArray:
_torsoModelVersion
T@"NSArray",R,V_torsoprints
T@"NSUUID",R,V_torsoModelVersion
_initSessionWithDimensions:codecType:useHardwareAcceleration:error:
setSession:
_setProperty:propertyValue:
setForceKeyFrameOnNextEncodedFrame:
delegateQueue
encoder:didFailWithError:
_invalidate
forceKeyFrameOnNextEncodedFrame
_invalidateWithErr:
encoder:didEncodeSampleBuffer:
_getProperty:propertyValue:
_setSInt32Property:propertyValue:
_getSInt32Property:propertyValueOut:
_setFloat64Property:propertyValue:
_getFloat64Property:propertyValueOut:
initWithDimensions:codecType:useHardwareAcceleration:error:
prepare
setMaxFrameDelayCount:
maxFrameDelayCount
setAverageBitRate:
averageBitRate
setQuality:
quality
setMaxKeyFrameIntervalDuration:
maxKeyFrameIntervalDuration
setExpectedFrameRate:
expectedFrameRate
setExpectedDuration:
expectedDuration
setDataRateLimit:
dataRateLimit
setLogIdentifier:
numberOfDroppedFrames
_forceKeyFrameOnNextEncodedFrame
_logIdentifier
_numberOfDroppedFrames
_delegateQueue
T^{OpaqueVTCompressionSession=},V_session
TB,V_forceKeyFrameOnNextEncodedFrame
T@"<HMIVideoEncoderDelegate>",W,V_delegate
T@"NSObject<OS_dispatch_queue>",R,V_delegateQueue
T@"NSString",&,V_logIdentifier
Tq,N
Td,N
T{HMIVideoEncoderDataRate=qq},N
TQ,R,V_numberOfDroppedFrames
encoderDidEncodeSampleBuffer
encoderDidFailWithError
setEncoderDidEncodeSampleBuffer:
setEncoderDidFailWithError:
_encoderDidEncodeSampleBuffer
_encoderDidFailWithError
T@?,C,V_encoderDidEncodeSampleBuffer
T@?,C,V_encoderDidFailWithError
homePersonManager
addOrUpdateFaceCrops:completion:
addOrUpdatePersons:completion:
setHomePersonManager:
_homePersonManager
T@"HMHomePersonManager",&,V_homePersonManager
isImportingFromPhotoLibraryEnabled
isSharingFaceClassificationsEnabled
setImportingFromPhotoLibraryEnabled:
setSharingFaceClassificationsEnabled:
_importingFromPhotoLibraryEnabled
_sharingFaceClassificationsEnabled
importingFromPhotoLibraryEnabled
TB,GisImportingFromPhotoLibraryEnabled,V_importingFromPhotoLibraryEnabled
sharingFaceClassificationsEnabled
TB,GisSharingFaceClassificationsEnabled,V_sharingFaceClassificationsEnabled
TB,D,GisImportingFromPhotoLibraryEnabled
TB,D,GisSharingFaceClassificationsEnabled
upload
lazyPayloads
payloadWithCamera:
sendEventWithName:payloadBuilder:
sendEventForPersonRecognitionType:camera:
sendEventForFaceEvent:homePersonManagerUUID:camera:
numberOfFaceprintsClustered
numberOfClusters
numberOfPersonsCreated
numberOfUnknownFaceprintsAssociated
faceprintingDuration
clusteringDuration
totalDuration
modelSummaries
bucketForValue:usingBuckets:
homeToExternalEquivalencies
externalToExternalEquivalencies
isInclusion
timeSinceAnalyzerStarted
transcode
analysisFPS
videoAnalyzerDidFindFaceEvent:homePersonManagerUUID:camera:
sendEventForClusteringTask:
sendEventsForFragmentResult:
videoAnalyzerDidTerminateWithError:state:
videoAnalyzerDidCreateTimelapseFragment:state:
videoAnalyzerDidAnalyzeFragmentWithResult:state:
videoPackageAnalyzerDidClassifyCandidateAsPackage:camera:
initWithModelSummaries:homeToExternalEquivalencies:externalToExternalEquivalencies:
_modelSummaries
_homeToExternalEquivalencies
_externalToExternalEquivalencies
T@"NSSet",R,V_modelSummaries
TQ,R,V_homeToExternalEquivalencies
TQ,R,V_externalToExternalEquivalencies
initWithSourceUUID:personUUID:confidence:linkedEntityUUID:
_linkedEntityUUID
T@"NSUUID",R,V_linkedEntityUUID
initWithPersonsModels:userDefinedPersonLinks:error:
personsModelsByHome
personsModelWithFaceObservationsByID:error:
setMaximumIdentities:
setMaximumTrainingFaceprintsPerIdentity:
addFaceObservations:toPersonWithUniqueIdentifier:error:
userDefinedPersonLinksByHome
isEqualToDictionary:
persistUserDefinedPersonLinks:forHomeUUID:error:
personsModelWithFaceObservations:error:
loadModelsWithError:
homePersonsModelForHomeWithUUID:
getModelStoragePathForModel:error:
persistModel:toPath:error:
buildEquivalencyMapForPersonsModels:userDefinedPersonLinks:error:
equivalencyTablesByHome
getUserDefinedPersonLinksStoragePathForHomeUUID:error:
predictPersonFromFaceObservation:limit:canceller:error:
predictedPersonUniqueIdentifier
equivalencyCellForPerson:
minimumUUIDInEquivalencyCell:
torsoModelsByHome
torsoToFaceCropByHome
trainingFaceObservationsForPersonWithUniqueIdentifier:canceller:error:
faceId
faceObservationFromTorsoprint:
getTorsoModelStoragePathForHomeUUID:error:
persistTorsoToFaceCrop:forHomeUUID:error:
persistTorsoprinterVersionForHomeUUID:error:
_isTorsoFaceCropMapStale:
_hasTorsoprinterVersionChangedForHome:
loadTorsoprinterVersion:error:
earlierDate:
currentCalendar
isDate:inSameDayAsDate:
equivalencyCellForPerson:homeUUID:error:
_reset
modelFromURL:options:error:
URLByDeletingLastPathComponent
setReadOnly:
writeToURL:options:error:
getRootModelStoragePathWithError:
pathWithComponents:
getModelStoragePathForHome:error:
URLByAppendingPathComponent:isDirectory:
absoluteURL
fileHandleForReadingFromURL:error:
readDataToEndOfFile
unarchivedObjectOfClasses:fromData:error:
getTorsoSubdirectoryPathForHomeUUID:error:
fileURLWithPathComponents:
getTorsoToFaceCropStoragePathForHomeUUID:error:
getTorsoprinterVersionStoragePathForHomeUUID:error:
contentsOfDirectoryAtURL:includingPropertiesForKeys:options:error:
regularExpressionWithPattern:options:error:
numberOfMatchesInString:options:range:
kick
modelURLsFromPath:error:
loadPersonsModelFromURL:externalLibrary:homeUUID:error:
loadUserDefinedPersonLinksForHomeUUID:error:
_loadTorsoDataForHomeUUID:intoTorsoModelsByHome:torsoToFaceCropByHome:
loadTorsoToFaceCrop:error:
_resetStaleTorsoStateForHome:torsoToFaceCropMap:
loadModelAtPath:error:
URLByDeletingPathExtension
pathExtension
attributesOfItemAtPath:error:
fileSize
personToEquivalencyCell
T@"HMIPersonsModelManager",R
buildPersonsModelForHomeUUID:sourceUUID:externalLibrary:faceObservationsByPerson:error:
predictHomePersonFromFaceObservation:homeUUID:error:
faceCropFromTorsoModelForHomeUUID:personUUID:sourceUUID:
predictPersonFromTorsoObservation:homeUUID:error:
linkedPredictionsForPrediction:homeUUID:error:
_userDefinedPersonLinksByHome
_personsModelsByHome
_torsoModelsByHome
_torsoToFaceCropByHome
_equivalencyTablesByHome
T@"NSDictionary",R,V_userDefinedPersonLinksByHome
T@"NSDictionary",R,V_personsModelsByHome
T@"NSDictionary",R,V_torsoModelsByHome
T@"NSDictionary",R,V_torsoToFaceCropByHome
T@"NSDictionary",R,V_equivalencyTablesByHome
initWithName:weights:biases:
weights
biases
_weights
_biases
T@"HMIDESMutableFloatArray",R,V_weights
T@"HMIDESMutableFloatArray",R,V_biases
flattedTrainingResult
initWithLayerParameters:losses:preTrainingLoss:postTrainingLoss:
layerParameters
losses
preTrainingLoss
postTrainingLoss
_preTrainingLoss
_postTrainingLoss
_layerParameters
_losses
T@"NSArray",R,V_layerParameters
T@"NSArray",R,V_losses
Tf,R,V_preTrainingLoss
Tf,R,V_postTrainingLoss
T@"HMIDESMutableFloatArray",R
Tf,R
getParameterOfType:forLayerNamed:error:
networkPath
initWithTrainingNetworkPath:inferenceInputs:inferenceOutputs:trainingInputs:trainingOutputs:trainingControlVariableName:withInitializer:error:
initWithTrainingModelDefinition:forPlatform:error:
getTensorNamed:
setTensorNamed:withValue:error:
doInferenceOnData:error:
preTrainingInferenceOutputDictionary:preTrainingtrainingLossKeyName:error:
getParametersFromLayers:fromTask:error:
doTrainingOnData:forNumberOfEpochs:withCallback:error:
saveTrainingNetwork:checkpoint:error:
initWithTrainingNetworkPath:data:error:
inferenceInputs:inferenceOutputs:trainingInputs:trainingOutputs:learningRate:error:
trainLayers:epochs:fromTask:shouldCalculatePreTrainingLoss:error:
_networkPath
T@"HMIDESDataset",R,V_data
T@"NSURL",R,V_networkPath
initWithCode:analysisFPS:message:
message
success
skipped
T@"HMIVideoAnalyzerResultOutcome",R
isSkipped
isSuccess
_message
_analysisFPS
_code
TQ,R,V_code
T@"NSString",R,V_message
Td,R,V_analysisFPS
T@"HMIExternalPersonManagerSettings",R,V_settings
T@"<HMIExternalPersonManagerDataSource>",W,N,V_dataSource
removePersonsModelWithRetryOnError:
external
_external
TB,R,V_external
weakObjectsPointerArray
isAudioAccessory
isProductTypeJ305
isProductTypeJ105
hmf_addObject:
reducedConfiguration:
registerAnalyzer:
analyzers
state
analyzerConfigurations
reducedConfiguration:configurations:
allowReducedConfiguration
decodeMode
transcodeCodecType
timelapseInterval
ignoreThermalAndSystemResourceUsageLevel
maxConcurrentAnalyzersForSystemResourceUsageLevel:
setMaxH264VideoDecoders:
maxH264VideoDecoders
setDecodeMode:
maxH265VideoEncoders
setTranscodeCodecType:
maxH264VideoEncoders
setTranscode:
setTimelapseInterval:
maxAnalysisFPSForSystemResourceUsageLevel:
setAnalysisFPS:
addPointer:
compact
_compactInternalAnalyzers
_updateAnalyzer:withIndex:
_logState
monitored
delay
logStateCount
setLogStateCount:
_shouldSkipLogState
check
stringWithString:
isIdle
processInfo
processName
tableColumns
tableValues
stringByPaddingToLength:withString:startingAtIndex:
stringByAppendingString:
JSONObject
systemResourceUsageDidChangeTo:
analyzerWithConfiguration:block:
analyzerStates
reducedConfiguration:states:
setIgnoreThermalAndSystemResourceUsageLevel:
setMaxH264VideoEncoders:
setMaxH265VideoEncoders:
internalAnalyzers
_registerLock
_usageMonitor
_usageLevel
_ignoreThermalAndSystemResourceUsageLevel
_maxH264VideoDecoders
_maxH264VideoEncoders
_maxH265VideoEncoders
_internalAnalyzers
_logStateCount
T@"NSPointerArray",R,V_internalAnalyzers
T@"NSArray",R
Tq,V_logStateCount
TB,V_ignoreThermalAndSystemResourceUsageLevel
TQ,V_maxH264VideoDecoders
TQ,V_maxH264VideoEncoders
TQ,V_maxH265VideoEncoders
clientWithIdentifier:
updateLevels
submitUpdateModelTask
addUpdateHandlerForNamespaceName:usingBlock:
levelForFactor:withNamespaceName:
fileValue
registerForTrialUpdates
personThresholdHigh
personThresholdMedium
petThresholdHigh
petThresholdMedium
vehicleThresholdHigh
vehicleThresholdMedium
faceThreshold
modelPath
_trialClient
_compiledModelArchivePath
_personThresholdHigh
_personThresholdMedium
_petThresholdHigh
_petThresholdMedium
_vehicleThresholdHigh
_vehicleThresholdMedium
_faceThreshold
_modelPath
Td,R,V_personThresholdHigh
Td,R,V_personThresholdMedium
Td,R,V_petThresholdHigh
Td,R,V_petThresholdMedium
Td,R,V_vehicleThresholdHigh
Td,R,V_vehicleThresholdMedium
Td,R,V_faceThreshold
T@"NSString",R,V_modelPath
lengthInBytes
initWithFaceThreshold:singleLinkThreshold:percentConnectionsThreshold:error:
addFaceObservations:toFaceDescriptorBuffer:error:
convertToClusters:
setObjects:
setClusterId:
objects
setTotalObjectCount:
setShouldUpdateRepresentative:
dataWithBytes:length:
centermostFaceprintInCluster:faceObservations:
getClustersWithFaces:error:
.cxx_construct
_greedyClusterer
T@"<HMIVideoFrameSamplerDelegate>",W,V_delegate
initWithFrameRate:
_interval
_firstPTS
_lastIndex
frameSamplerDidSampleFrame
frameSamplerDidDropFrame
setFrameSamplerDidSampleFrame:
setFrameSamplerDidDropFrame:
_frameSamplerDidSampleFrame
_frameSamplerDidDropFrame
T@?,C,V_frameSamplerDidSampleFrame
T@?,C,V_frameSamplerDidDropFrame
vectorWithString:
CGAffineTransformValue
valueAtIndex:
initWithDictionary:forType:
_type
T@"NSString",R,V_type
T@"HMICIFilterAttributeValue",R,V_value
attributes
attributeForKey:
filterWithName:withInputParameters:
expectedAttributeForKey:
outputImage
_probability
_attributes
Td,R,V_probability
T@"NSArray",R,V_attributes
T@"HMIVisionSession",R
T@"VNSession",R
stringByReplacingOccurrencesOfString:withString:
_ensureModelWithError:
usesCPUOnly
setAllowBackgroundGPUCompute:
setComputeUnits:
modelURL
modelWithContentsOfURL:configuration:error:
setModel:
setTransaction:
underlyingModel
transaction
_modelURL
_transaction
T@"NSURL",R,V_modelURL
T@"MLModel",&,V_model
T@"HMFOSTransaction",&,V_transaction
T@"MLModel",R
packageClassifierMode
eventConfidenceThresholdMedium
eventConfidenceThresholdHigh
eventConfidenceThresholdsMedium
saveAnalyzerResultsToDisk
numberOfDetectedPackagesInSession
backgroundEstimator
packageDetector
setReferenceFrame:
lastDetectionAnalysisTimeStamp
detectionAnalysisInterval
regionOfInterestFromEvents:frameSize:
eventsFromRegionOfInterest:frame:
referenceFrame
setLastDetectionAnalysisTimeStamp:
report
setNumberOfDetectedPackagesInSession:
packageAnalyzer:didDetectPackagesWithResult:error:
analyzePixelBuffer:regionOfInterest:error:
sampler
numberPreferenceForKey:
highConfidence
mediumConfidence
_numberOfDetectedPackagesInSession
_sampler
_packageClassifierMode
_packageDetector
_backgroundEstimator
_report
_highConfidence
_mediumConfidence
_referenceFrame
_lastDetectionAnalysisTimeStamp
_detectionAnalysisInterval
T@"HMIVideoFrameIntervalSampler",R,V_sampler
Tq,R,V_packageClassifierMode
T@"HMICameraVideoFrameAnalyzerSignificantActivity",R,V_packageDetector
T@"HMIBackgroundEstimator",R,V_backgroundEstimator
T@"HMIHTMLReport",R,V_report
T@"NSNumber",R,V_highConfidence
T@"NSNumber",R,V_mediumConfidence
Ti,V_numberOfDetectedPackagesInSession
T@"HMIVideoFrame",&,V_referenceFrame
T{?=qiIq},V_lastDetectionAnalysisTimeStamp
T{?=qiIq},R,V_detectionAnalysisInterval
T@"<HMIVideoPackageAnalyzerDelegate>",W,V_delegate
packageAnalyzerDidDetectPackages
setPackageAnalyzerDidDetectPackages:
_packageAnalyzerDidDetectPackages
T@?,C,V_packageAnalyzerDidDetectPackages
_numBins
_maxLaplacianScore
_minLaplacianScore
_binWidth
_maxScore
_histogram
thumbnails
_fragment
_thumbnails
_outcome
_frameResults
T@"HMIVideoFragment",R,V_fragment
T@"NSArray",R,V_thumbnails
T@"HMIVideoAnalyzerResultOutcome",R,V_outcome
T@"NSArray",R,V_frameResults
T@"HMIVideoAnalyzerDynamicConfiguration",R,V_configuration
isEqualToSet:
decodeIntForKey:
encodeInt:forKey:
initWithPoints:isInclusion:
points
overlapsWithElipseInsideRect:
activityZoneType
overlapsWithElipseInsideRect:withInsetPercentage:
checkIfObjectIsStaticWithBoundingBox:motionDetection:eventClass:
containsEvent:withInsetPercentage:
absoluteString
jsonReperesentaionOfDetectedObject:motionDetection:eventClass:
motionVectors
target
motion
na_all:
characterSetWithCharactersInString:
componentsSeparatedByCharactersInSet:
submitCoreAnalyticsEvent:filteringLevel:numberOfDetectedObjects:
activityZonesFromString:isInclusion:
submitCoreAnalyticsEventForActivityZones:motionScore:
generateAndSubmitStats:filteredEvents:motionDetection:activityZones:
initWithPoints:
saveToJsonActivityZones:motionDetection:videoFragmentUrl:frameId:UUID:detectionID:zoneID:
containsVectorWithSource:destination:
_inclusion
_points
T@"NSArray",R,C,V_points
inclusion
TB,R,GisInclusion,V_inclusion
thumbnailInterval
thumbnailHeight
timelapsePreferredFragmentDuration
maxFragmentDuration
maxFragmentAnalysisDuration
passthroughAudio
minFrameQuality
minFrameScale
enableTemporalEventFiltering
setTimelapsePreferredFragmentDuration:
setThumbnailInterval:
setThumbnailHeight:
setMaxFragmentAnalysisDuration:
setMaxFragmentDuration:
setPassthroughAudio:
setCamera:
setHomeUUID:
setMinFrameQuality:
setMinFrameScale:
setPackageClassifierMode:
setRedactFrames:
setAllowReducedConfiguration:
setEnableTemporalEventFiltering:
setSaveAnalyzerResultsToDisk:
decodeCMTimeForKey:
encodeCMTime:forKey:
timelapseVideo
setTimelapseVideo:
timelapseVideoPreferredFragmentDuration
setTimelapseVideoPreferredFragmentDuration:
_transcode
_passthroughAudio
_redactFrames
_allowReducedConfiguration
_enableTemporalEventFiltering
_saveAnalyzerResultsToDisk
_transcodeCodecType
_minFrameQuality
_minFrameScale
_thumbnailHeight
_maxFragmentAnalysisDuration
_camera
_decodeMode
_thumbnailInterval
_timelapseInterval
_timelapsePreferredFragmentDuration
_maxFragmentDuration
Tq,V_decodeMode
TB,V_redactFrames
Tq,V_packageClassifierMode
TB,V_allowReducedConfiguration
TB,V_enableTemporalEventFiltering
TB,V_saveAnalyzerResultsToDisk
Td,V_analysisFPS
T{?=qiIq},V_thumbnailInterval
TQ,V_thumbnailHeight
T{?=qiIq},V_timelapseInterval
T{?=qiIq},V_timelapsePreferredFragmentDuration
Td,V_maxFragmentAnalysisDuration
T{?=qiIq},V_maxFragmentDuration
TB,V_transcode
TI,V_transcodeCodecType
T@"HMICamera",&,V_camera
Td,V_minFrameQuality
Td,V_minFrameScale
T@"NSUUID",&,V_homeUUID
TB,V_passthroughAudio
setActivityZones:
setEventTriggers:
setRecognizeFaces:
_recognizeFaces
_eventTriggers
TB,V_recognizeFaces
Tq,V_eventTriggers
T@"NSArray",&,V_activityZones
initWithEvent:time:
event
_event
T@"HMIVideoAnalyzerEvent",R,V_event
targetEventClassRanks
timeInterval
_filterEvents:stationaryEvents:motionDetection:
stationaryObjects
_filterEvents:stationaryEvents:stationaryObjects:expirationPTS:imageSize:
prevFrameResult
_resetPreviousFrameResult:expirationPTS:regionOfInterest:
setPrevFrameResult:
_filterEvents:stationaryEvents:motionDetection:prevFrameResult:regionOfInterest:
applyEventTypeAndCheckIfSubBoundingIsStatic:forMetric:eventClass:confidence:
scoreForSubBoundingBox:forMetric:eventClass:confidence:
applyFilterWithFrameResult:motionDetection:
_prevFrameResult
_stationaryObjects
_targetEventClassRanks
_timeInterval
T@"HMIVideoAnalyzerFrameResult",&,V_prevFrameResult
T@"NSMutableArray",R,V_stationaryObjects
T{?=qiIq},R,V_timeInterval
T@"NSDictionary",R,V_targetEventClassRanks
raise:format:
initWithBoundingBox:
expectedClasses
requestAnalysisForPixelBuffer:withProperties:andCompletionHandler:
requestAnalysisForAssetData:withProperties:andCompletionHandler:
cancelRequest:
hasEstimatedBoundingBox
_isBoundingBoxEstimated
_face
_torso
isBoundingBoxEstimated
TB,R,GhasEstimatedBoundingBox,V_isBoundingBoxEstimated
T@"HMIVideoAnalyzerEventTorso",R,V_torso
T@"HMIVideoAnalyzerEventFace",R,V_face
initWithPackageResult:didPrivatizePackageResult:didEncryptPackageResult:maxNorm:l2Norm:
result
maxNorm
didPrivatizePackageResult
didEncryptPackageResult
_didPrivatizePackageResult
_didEncryptPackageResult
_l2Norm
_result
_maxNorm
T@"NSData",R,V_result
Tf,R,V_l2Norm
Td,R,V_maxNorm
TB,R,V_didPrivatizePackageResult
TB,R,V_didEncryptPackageResult
encryptedDataWithPublicKey:inPlaceDataFloatNumbers:count:error:
packageTrainingResult:privatize:maxNorm:normBinCount:encryptionKey:error:
isProductTypeB238
isProductTypeB520
numberPreferenceForKey:defaultValue:withMap:
maxConcurrentAnalyzersForCurrentThermalLevel
isProductTypeJ42
maxAnalysisFPSForCurrentThermalLevel
qosMap
preferenceCache
preferenceOverridesInternal
preferenceLoggedValues
initWithKey:options:domain:defaultValue:
systemPreferenceValueForKey:
logPreferenceForKey:value:
caseInsensitiveCompare:
pretendProductTypeIsUnknown
setPretendProductTypeIsUnknown:
T@"HMIPreference",R
maxVideoEncoderFrameRate
maxVideoEncoders
shouldGenerateThumbnailForAnalysisFPS:
analysisQOS
preferenceOverrides
addPreferenceOverrideFromDictionary:
setPreferenceOverrideFromDictionary:
removeAllPreferenceOverrides
numberPreferenceForKey:defaultValue:withParser:
valuePreferenceForKey:defaultValue:withParser:
valuePreferenceForKey:defaultValue:withMap:
hasPreferenceForKey:
preferenceCacheFlushTimer
_preferenceCacheFlushTimer
_preferenceCache
_preferenceLoggedValues
_preferenceOverridesInternal
T@"HMFTimer",R,V_preferenceCacheFlushTimer
T@"NSMutableDictionary",R,N,V_preferenceCache
T@"NSMutableDictionary",R,N,V_preferenceLoggedValues
T@"NSMutableDictionary",R,N,V_preferenceOverridesInternal
TI,R
setNumberStyle:
numberFromString:
initWithDataSource:person:
personFaceCrops
_person
_personFaceCrops
T@"HMIPerson",R,V_person
T@"NSSet",R,V_personFaceCrops
token
notificationName
publishValueForToken:
publishInitialValue
setToken:
callback
initWithNotificationName:andQueue:andCallback:
_token
_notificationName
_callback
T@"NSObject<OS_dispatch_queue>",R,N,V_queue
T@?,R,N,V_callback
Ti,N,V_token
Tr*,R,N,V_notificationName
dataWithContentsOfURL:options:error:
setAssetWriterDidOutputInitializationSegment:
setAssetWriterDidOutputSeparableSegment:
initWithVideoFormat:audioFormat:initialFragmentSequenceNumber:preferredOutputSegmentInterval:
defaultConfidenceThresholdsFeedback
_addEventsToEventQueue:events:
na_arrayByFlattening
_blurSampleBufferWithEncoder:sampleBuffer:events:
reencodeAssetURL:outputURL:bitRate:duration:analysisFPS:sampleFrameHandler:dropFrameHandler:
removeLastObject
_createBlurredPixelBuffer:events:
_blurRadiusForEvents:imageSize:
na_reduceWithInitialValue:reducer:
waitUntilFinished
numFailures
setNumFailures:
initWithDataSource:faceCropUUIDs:personUUID:source:
enumeratorAtPath:
nextObject
sortUsingComparator:
ffArchiveRootURLWithError:
purgeURLIfNeeded:
writeToFile:atomically:
fetchFaceCropsForPerson:
isAffectedDate:
dumpFFDataToCacheForPerson:personFaceCrops:
reassociateFaceCropsWithUnknownSource:toPersonUUID:
fetchOrCreateFaceprintsForCrops:person:
na_setByRemovingObjectsFromSet:
removeFaceCropsWithUUIDs:
isIdentityPureWithFaceprints:person:
removePerson:
fetchPersons
isCancelled
handleCleanupForPerson:
targetDate
initWithHomeUUID:dataSource:
clusterer
_numFailures
_clusterer
_targetDate
T@"HMIGreedyClustering",R,V_clusterer
Ti,V_numFailures
T@"NSDate",R,V_targetDate
setMaximumFractionDigits:
stringFromNumber:
decimalNumberHandlerWithRoundingMode:scale:raiseOnExactness:raiseOnOverflow:raiseOnUnderflow:raiseOnDivideByZero:
decimalValue
decimalNumberWithDecimal:
decimalNumberByRoundingAccordingToBehavior:
URLForDirectory:inDomain:appropriateForURL:create:error:
hmf_UUIDWithNamespace:data:
setNumberOfFaceprintsClustered:
setNumberOfClusters:
setNumberOfPersonsCreated:
setNumberOfUnknownFaceprintsAssociated:
setFaceprintingDuration:
setClusteringDuration:
setTotalDuration:
setError:
_numberOfFaceprintsClustered
_numberOfClusters
_numberOfPersonsCreated
_numberOfUnknownFaceprintsAssociated
_faceprintingDuration
_clusteringDuration
_totalDuration
Tq,V_numberOfFaceprintsClustered
Tq,V_numberOfClusters
Tq,V_numberOfPersonsCreated
Tq,V_numberOfUnknownFaceprintsAssociated
Td,V_faceprintingDuration
Td,V_clusteringDuration
Td,V_totalDuration
T@"NSError",&,V_error
doImpurePersonCleanup
_stageZero_expireUnnamedPersons
personsModelManager
personCreatedDateFromFaceCrops:
_stageOne_fetchFaceCrops
_stageTwo_fetchFaceprints:
_stageThree_generateFaceprintsForFaceCrops:existingFaceprints:
_stageFour_clusterFaceprints:
_stageFive_addPersons:clusterMapping:faceprints:
_stageSix_associateFaceCropsWithClusterMapping:faceprints:
startTime
_faceClassifier
_doImpurePersonCleanup
_personsModelManager
_summary
_startTime
T@"HMIClusteringTaskSummary",R,V_summary
T@"NSDate",R,V_startTime
TB,R,V_doImpurePersonCleanup
T@"HMIPersonsModelManager",R,V_personsModelManager
limitEnforcedSubsetFromPersons:
shouldRemoveExcessFaceCrops
subsampleFacesForPersons:withFaceObservationsMap:dataSource:vnUUIDToFaceCropUUIDMap:
_removeExcessFaceCrops
removeExcessFaceCrops
TB,R,GshouldRemoveExcessFaceCrops,V_removeExcessFaceCrops
initWithFrame:size:
cropRect
_cropRect
T{CGRect={CGPoint=dd}{CGSize=dd}},R,V_cropRect
T@"HMICameraVideoFrame",R,W,V_frame
T@"NSError",R,V_error
classHierarchyMap
mapTableWithKeyOptions:valueOptions:
significantActivityFcosDetector
regionOfInterestOperations
regionOfInterestOperationQueue
_predictEventsFromCropPixelBuffer:cropRect:imageSize:error:
_simulatedEventForEventClass:
saveDESRecordVideoFrame:analyzerEvents:regionOfInterest:
_eventsWithClassificationsFromEvents:videoFrame:regionOfInterest:homeUUID:enableTorsoRecognition:
_eventsWithSessionEntitiesFromEvents:regionOfInterest:timeStamp:homeUUID:
eventsWithFaceEventsFromTorsoEventsFromEvents:homeUUID:
_targetEventsSetFromTargetEventTypes:enableFaceClassification:enableTorsoRecognition:
_filterEvents:targetEventClasses:
eventsWithContentsOfFile:
_analyzerEventsFromObjectDetections:
sessionEntityManager
na_allObjectsPassTest:
analyzerConfiguration
lowercaseString
highConfidenceThresholds
mediumConfidenceThresholds
confidenceLevel
classifyTorsoEvent:regionOfInterest:pixelBuffer:homeUUID:error:
desLabelIndexForEventClass:
labelIndexForEventClass:
preAnalyze:
_rankForEventClass:
_createStationaryEventFromEvent:
faceClassifier
torsoClassifier
_mediumConfidenceThresholds
_highConfidenceThresholds
_regionOfInterestOperationQueue
_regionOfInterestOperations
_significantActivityFcosDetector
_torsoClassifier
_analyzerConfiguration
_sessionEntityManager
T@"NSDictionary",R,V_mediumConfidenceThresholds
T@"NSDictionary",R,V_highConfidenceThresholds
T@"NSOperationQueue",R,V_regionOfInterestOperationQueue
T@"NSMapTable",R,V_regionOfInterestOperations
T@"HMISignificantActivityFcosDetector",R,V_significantActivityFcosDetector
T@"HMIFaceClassifierVIP",R,V_faceClassifier
T@"HMITorsoClassifier",R,V_torsoClassifier
T@"HMFOSTransaction",&,N,V_transaction
T@"HMIVideoAnalyzerConfiguration",R,V_analyzerConfiguration
T@"HMISessionEntityManager",R,V_sessionEntityManager
T{CGSize=dd},R
taskIdentifier
taskRunnerClass
scheduleTask:
initWithURL:
activityForScheduling
_url
T@"NSURL",R,V_url
printWithHeight:
_store
_presentationTimeStamp
T@"NSData",R,V_data
T{?=qiIq},R,V_presentationTimeStamp
initWithContentsOfFile:options:error:
fileDescriptor
availableData
unpackageTarData:size:parentDir:
getDataOutWithSize:withFlag:fd:
uncompressedContentsForCompressedFile:outPath:
capacity
condition
lock
isFull
wait
setSize:
sampleBufferDelay
buffer:willHandleSampleBuffer:
signal
bufferWillFlush:
fillRatio
isEmpty
handleBlock:
videoDuration
_duration
_capacity
_condition
_sampleBufferDelay
_videoDuration
TQ,V_size
T@"NSCondition",R,V_condition
T@"HMITimeIntervalAverage",R,V_sampleBufferDelay
T@"<HMIVideoCommandBufferDelegate>",W,V_delegate
T{?=qiIq},R,V_videoDuration
TQ,R,V_capacity
bufferWillHandleSampleBuffer
bufferWillFlush
setBufferWillHandleSampleBuffer:
setBufferWillFlush:
_bufferWillHandleSampleBuffer
_bufferWillFlush
T@?,C,V_bufferWillHandleSampleBuffer
T@?,C,V_bufferWillFlush
lines
appendSeparableSegmentWithPath:duration:byteRange:
initWithPlaylistString:
initWithTargetDuration:
playlistString
appendIFrameOnly
appendEncryptionModeWithPath:
appendInitializationSegmentWithPath:
appendSeparableSegmentWithPath:duration:
_lines
T@"NSMutableArray",R,V_lines
T@"NSSet",R,V_unassociatedFaceCrops
T@"NSSet",R,C,V_faceCropUUIDs
dataWithContentsOfFile:options:error:
defaultConfidenceThresholdsHigh
defaultConfidenceThresholdsMedium
defaultConfidenceThreshold:confidenceLevel:
_userInfo
T@"HMIConfidence",R,V_confidence
T@"NSDictionary",R,V_userInfo
initWithFrameReordering:
initWithWeakObject:
lastSampleBufferDTS
setLastSampleBufferDTS:
_failWithDescription:
_createSessionWithFormatDescription:
buffer
_evictSampleBuffer:
setBuffer:
weakDecoder
lastSampleBufferPTS
decoder:didDecodeSampleBuffer:
setLastSampleBufferPTS:
reorderBufferSize
decoder:didFailWithError:
_didDecodeSampleBuffer:
setWeakDecoder:
_reorderBufferSize
_weakDecoder
_lastSampleBufferPTS
_lastSampleBufferDTS
T{?=qiIq},V_lastSampleBufferPTS
T{?=qiIq},V_lastSampleBufferDTS
TQ,R,V_reorderBufferSize
T^{opaqueCMBufferQueue=},V_buffer
T^{OpaqueVTDecompressionSession=},V_session
T@"HMFWeakObject",&,V_weakDecoder
T@"<HMIVideoDecoderDelegate>",W,V_delegate
decoderDidDecodeSampleBuffer
decoderDidFailWithError
setDecoderDidFailWithError:
_decoderDidDecodeSampleBuffer
_decoderDidFailWithError
T@?,C,V_decoderDidDecodeSampleBuffer
T@?,C,V_decoderDidFailWithError
typeWithIdentifier:
initWithContentType:
setOutputFileTypeProfile:
setPreferredOutputSegmentInterval:
setInitialMovieFragmentSequenceNumber:
setProducesCombinableFragments:
initWithMediaType:outputSettings:sourceFormatHint:
setExpectsMediaDataInRealTime:
setMediaTimeScale:
canAddInput:
addInput:
assetWriterInputWithMediaType:outputSettings:sourceFormatHint:
setCurrentFragmentStartTime:
assetWriter
setInitialSegmentStartTime:
startWriting
assetWriter:didOutputInitializationSegment:
assetWriter:didOutputSeparableSegment:segmentReport:
assetWriterDidOutputSeparableSegment
currentFragmentStartTime
preferredOutputSegmentInterval
flushWithCompletionHandler:
_flushAutomatically:
_appendSampleBuffer:
audioFormat
_startWritingAtStartTime:
startSessionAtSourceTime:
videoInput
audioInput
isReadyForMoreMediaData
mediaType
_removeTrimDurationAttachmentsFromAudioSampleBuffer:
allowRecoveryFromInsufficientAudioTrim
_ensureFirstAudioSampleBufferHasSufficientPrimingTrim:
appendSampleBuffer:
setLastVideoPresentationTimeStamp:
setLastAudioPresentationTimeStamp:
lastVideoPresentationTimeStamp
lastAudioPresentationTimeStamp
dropSamplesUntilSync
flushSegment
setDropSamplesUntilSync:
assetWriter:didFailWithError:
assetWriter:didOutputSegmentData:segmentType:segmentReport:
assetWriter:didOutputSegmentData:segmentType:
initWithVideoFormat:audioFormat:
setAssetWriter:
videoFormat
dropTrimDurationAttachments
setDropTrimDurationAttachments:
_dropSamplesUntilSync
_dropTrimDurationAttachments
_allowRecoveryFromInsufficientAudioTrim
_assetWriter
_videoFormat
_audioFormat
_assetWriterDidOutputSeparableSegment
_videoInput
_audioInput
_preferredOutputSegmentInterval
_currentFragmentStartTime
_lastVideoPresentationTimeStamp
_lastAudioPresentationTimeStamp
T@"AVAssetWriter",&,V_assetWriter
Tr^{opaqueCMFormatDescription=},R,V_videoFormat
Tr^{opaqueCMFormatDescription=},R,V_audioFormat
TB,V_dropSamplesUntilSync
TB,V_dropTrimDurationAttachments
TB,R,V_allowRecoveryFromInsufficientAudioTrim
T{?=qiIq},V_preferredOutputSegmentInterval
T{?=qiIq},V_currentFragmentStartTime
T@?,C,V_assetWriterDidOutputSeparableSegment
T@"AVAssetWriterInput",R,V_videoInput
T@"AVAssetWriterInput",R,V_audioInput
T{?=qiIq},V_lastVideoPresentationTimeStamp
T{?=qiIq},V_lastAudioPresentationTimeStamp
T@"<HMIVideoAssetWriterDelegate>",W,V_delegate
assetWriterDidOutputInitializationSegment
assetWriterDidFailWithError
setAssetWriterDidFailWithError:
_assetWriterDidOutputInitializationSegment
_assetWriterDidFailWithError
T@?,C,V_assetWriterDidOutputInitializationSegment
T@?,C,V_assetWriterDidFailWithError
faceObservationsForPersonWithUniqueIdentifier:error:
setByAddingObject:
facesAreSamePersonFromSet:andSet:
facesAreSamePersonFromSet:andSet:distanceThreshold:percentMatchingThreshold:
_personToEquivalencyCell
T@"NSDictionary",R,V_personToEquivalencyCell
updatePersonsModelWithRetryOnError:
initWithOrigin:motion:
midpoint
distance
origin
setEventClass:
_origin
_motion
T{CGPoint=dd},R,V_origin
T{CGPoint=dd},R
T{CGVector=dd},R,V_motion
T{CGRect={CGPoint=dd}{CGSize=dd}},R
T#,&,V_eventClass
motionMode
classMotionScoreMap
classPaddingMap
initWithBoundingBox:size:motionVectors:motionScore:motionMode:
_motionVectors
_motionMode
T@"NSArray",R,V_motionVectors
TQ,R,V_motionMode
calculateMotionDetection:score:srcFeatureCVPoints:dstFeatreCVPoints:activityZones:operationMode:srcPyramid:frameSize:brightnessChange:
applyActivityZoneFilteringOnSourcePoint:destinationPoint:frameSize:activityZones:
_computeOpticalFlow:with:globalMotionScore:activityZones:operationMode:
resizedSampleBuffers
detectWithGlobalMotionScore:referencePixelBuffer:targetPixelBuffer:activityZones:detectorMode:
resizeSampleBuffer:
drainResizedSampleBuffersExpiredBefore:
_resizedSampleBuffers
T^{__CFArray=},R,V_resizedSampleBuffers
torsoprinter
_torsoprinter
T@"HMITorsoprinter",R,V_torsoprinter
sbuf
initWithSampleBuffer:score:motionDetections:
_sbuf
T^{opaqueCMSampleBuffer=},R,V_sbuf
setSampleRate:
alloc
_handleSampleBuffer:reference:
frameSelector:prepareFrame:
_drainCandidatesThatExpiredBefore:
frameSelector:didSelectFrame:motionDetections:motionScore:
frameSelector:didDetectMotion:inFrame:
_synthesizeMotionDetectionWithTarget:
maxCandidates
prepareFrame:
references
detector
maxReferences
setMaxReferences:
resetReferences
setResetReferences:
_candidates
_enabled
_referenceInterval
_targetInterval
_expirationInterval
_resetReferences
_maxCandidates
_references
_detector
_maxReferences
T{os_unfair_lock_s=I},R,N,V_lock
TQ,R,V_maxCandidates
T^{__CFArray=},R,V_references
T@"<HMIMotionDetector>",R,V_detector
TQ,V_maxReferences
TB,V_resetReferences
T@"<HMIVideoFrameSelectorDelegate>",W,V_delegate
frameSelectorDidSelectFrame
frameSelectorDidDetectMotion
frameSelectorPrepareFrame
setFrameSelectorDidSelectFrame:
setFrameSelectorDidDetectMotion:
setFrameSelectorPrepareFrame:
_frameSelectorDidSelectFrame
_frameSelectorDidDetectMotion
_frameSelectorPrepareFrame
T@?,C,V_frameSelectorDidSelectFrame
T@?,C,V_frameSelectorDidDetectMotion
T@?,C,V_frameSelectorPrepareFrame
timeSinceLastFragmentWasReceived
bufferFillRatio
bufferSize
numDecodedSamples
currentPTS
numDidAnalyzeFrames
numDidAnalyzeFragments
encode
encoder
numDidCreateTimelapseFragments
numDidAnalyzePackages
initWithConfiguration:dynamicConfiguration:identifier:monitored:analysisFPS:timeSinceAnalyzerStarted:timeSinceLastFragmentWasReceived:bufferFillRatio:bufferSize:delay:currentPTS:numDecodedSamples:numDidAnalyzeFrames:numDidAnalyzeFragments:numDidAnalyzePackages:numDidCreateTimelapseFragments:averageAnalysisTime:encode:encoder:
_monitored
_encode
_encoder
_timeSinceAnalyzerStarted
_timeSinceLastFragmentWasReceived
_bufferFillRatio
_bufferSize
_delay
_numDecodedSamples
_numDidAnalyzeFrames
_numDidAnalyzeFragments
_numDidAnalyzePackages
_numDidCreateTimelapseFragments
_averageAnalysisTime
_currentPTS
T@"HMIVideoAnalyzerDynamicConfiguration",R,V_dynamicConfiguration
TB,R,V_monitored
Td,R,V_timeSinceAnalyzerStarted
Td,R,V_timeSinceLastFragmentWasReceived
Td,R,V_bufferFillRatio
TQ,R,V_bufferSize
Td,R,V_delay
T{?=qiIq},R,V_currentPTS
TQ,R,V_numDecodedSamples
TQ,R,V_numDidAnalyzeFrames
TQ,R,V_numDidAnalyzeFragments
TQ,R,V_numDidAnalyzePackages
TQ,R,V_numDidCreateTimelapseFragments
Td,R,V_averageAnalysisTime
TB,R,V_encode
TB,R,V_encoder
initWithFaceCrop:faceprint:classifications:
initWithFaceCrop:faceprint:classifications:predictedLinkedEntityUUIDs:
_classifications
_faceQualityScore
T@"NSSet",R,V_classifications
Td,R,V_faceQualityScore
analyzerWithConfiguration:identifier:remote:error:
initWithConfiguration:identifier:
handleAssetData:withOptions:completionHandler:
analyzerWithOptions:error:
analyzerWithConfiguration:identifier:error:
handleMessageWithOptions:completionHandler:
analyzeFragment:configuration:
setMonitored:
setEncode:
finalizeFragmentResult:homePersonManager:analysisStateManager:
analysisStateManager
setAnalysisStateManager:
externalPersonManagers
setExternalPersonManagers:
_analysisStateManager
_externalPersonManagers
_state
T@"NSDictionary",R,C,V_options
T@"HMIVideoAnalyzerConfiguration",R,C,V_configuration
T@"HMIVideoAnalyzerState",R,V_state
TB,N
T@"HMIVideoAnalyzerMutableReport",R,V_report
T@"<HMIVideoAnalyzerDelegate>",W,V_delegate
T@"HMIHomePersonManager",&,V_homePersonManager
T@"HMIAnalysisStateManager",&,V_analysisStateManager
T@"NSSet",&,V_externalPersonManagers
TQ,R,V_status
inputQueue
setLastFragmentRecievedDate:
_saveFragmentDataToDisk:diskBufferSize:
commandBuffer
_notifyDelegateDidFailWithError:
dynamicConfigurationBuffer
_prepareForInputVideoFormat:audioFormat:
_ensureDecoderForFragment:
_ensureEncoder
_ensureTimelapseEncoder
handleSampleBuffer:errorHandler:
_handleDecodedSampleBuffer:
timeline
hasFailed
timelapseAssetWriter
timelapseEncoder
fileHandleForWritingAtPath:
seekToEndOfFile
writeData:
closeFile
startDate
stringByAppendingPathExtension:
inputVideoFormat
inputAudioFormat
_configureAssetWriter
timelapseOutputVideoFormat
_configureTimelapseAssetWriter
setTimelapseAssetWriter:
setEncoder:
encoderQueue
setTimelapseEncoder:
decoder
frameSelector
frameAnalyzer
_configureEncoder
_configureTimelapseEncoder
_prepareForTimelapseOutputVideoFormat:
dynamicConfigurationForTime:
frameThumbnailSampler
frameTimelapseSampler
packageAnalyzer
temporalEventFilter
_filterFrameResult:dynamicConfiguration:motionDetections:
frameAnalyzerFrameResultBuffer
_notifyDelegateDidAnalyzeFrameWithResult:
_notifyDelegateDidProduceAnalysisStateUpdate:
thumbnailBuffer
packageAnalyzerFrameResultBuffer
setInitializationSegment:
setTimelapseInitializationSegment:
trackReports
firstVideoSampleInformation
offset
timelapseInitializationSegment
_notifyDelegateDidCreateTimelapseFragment:
_notifyDelegateDidAnalyzeFragmentWithResult:
analyzer:didAnalyzeFragmentWithResult:
_produceResult:withArguments:
analyzer:didAnalyzeFrameWithResult:
analyzer:didCreateTimelapseFragment:
setHasFailed:
analyzer:didFailWithError:
analyzer:didProduceAnalysisStateUpdate:
analyzer:didProduceResult:
lastFragmentRecievedDate
setInputVideoFormat:
setInputAudioFormat:
setTimelapseOutputVideoFormat:
setCurrentPTS:
currentDTS
setCurrentDTS:
setCancelled:
_hasFailed
_cancelled
_inputQueue
_encoderQueue
_inputVideoFormat
_inputAudioFormat
_timelapseOutputVideoFormat
_commandBuffer
_decoder
_frameThumbnailSampler
_frameTimelapseSampler
_timelapseEncoder
_frameSelector
_frameAnalyzer
_timelapseAssetWriter
_frameAnalyzerFrameResultBuffer
_packageAnalyzerFrameResultBuffer
_thumbnailBuffer
_timelapseInitializationSegment
_dynamicConfigurationBuffer
_packageAnalyzer
_temporalEventFilter
_startDate
_lastFragmentRecievedDate
_currentDTS
T@"NSObject<OS_dispatch_queue>",R,V_inputQueue
T@"NSObject<OS_dispatch_queue>",R,V_encoderQueue
Tr^{opaqueCMFormatDescription=},V_inputVideoFormat
Tr^{opaqueCMFormatDescription=},V_inputAudioFormat
Tr^{opaqueCMFormatDescription=},V_timelapseOutputVideoFormat
T@"HMIVideoCommandBuffer",R,V_commandBuffer
T@"HMIVideoDecoder",R,V_decoder
T@"HMIVideoFrameSampler",R,V_frameThumbnailSampler
T@"HMIVideoFrameSampler",R,V_frameTimelapseSampler
T@"HMIVideoEncoder",&,V_encoder
T@"HMIVideoEncoder",&,V_timelapseEncoder
T@"HMIVideoFrameSelector",R,V_frameSelector
T@"HMIVideoFrameAnalyzer",R,V_frameAnalyzer
T@"HMIVideoAssetWriter",&,V_assetWriter
T@"HMIVideoAssetWriter",&,V_timelapseAssetWriter
T{?=qiIq},V_currentPTS
T{?=qiIq},V_currentDTS
T@"HMIVideoEventBuffer",R,V_frameAnalyzerFrameResultBuffer
T@"HMIVideoEventBuffer",R,V_packageAnalyzerFrameResultBuffer
T@"HMIVideoEventBuffer",R,V_thumbnailBuffer
T@"NSData",&,V_initializationSegment
T@"NSData",&,V_timelapseInitializationSegment
T@"HMIVideoEventBuffer",R,V_dynamicConfigurationBuffer
T@"HMIVideoPackageAnalyzer",R,V_packageAnalyzer
T@"HMIVideoTemporalEventFilter",R,V_temporalEventFilter
T@"HMIVideoTimeline",R,V_timeline
T@"NSDate",R,V_startDate
T@"NSDate",&,V_lastFragmentRecievedDate
TB,V_hasFailed
cancelled
TB,GisCancelled,V_cancelled
analyzerDidAnalyzeFrameWithResult
analyzerDidAnalyzeFragmentWithResult
analyzerDidFailWithError
analyzerDidCreateTimelapseFragment
analyzerDidProduceAnalysisStateUpdate
setAnalyzerDidAnalyzeFrameWithResult:
setAnalyzerDidAnalyzeFragmentWithResult:
setAnalyzerDidFailWithError:
setAnalyzerDidCreateTimelapseFragment:
setAnalyzerDidProduceAnalysisStateUpdate:
_analyzerDidAnalyzeFrameWithResult
_analyzerDidAnalyzeFragmentWithResult
_analyzerDidFailWithError
_analyzerDidCreateTimelapseFragment
_analyzerDidProduceAnalysisStateUpdate
T@?,C,V_analyzerDidAnalyzeFrameWithResult
T@?,C,V_analyzerDidAnalyzeFragmentWithResult
T@?,C,V_analyzerDidFailWithError
T@?,C,V_analyzerDidCreateTimelapseFragment
T@?,C,V_analyzerDidProduceAnalysisStateUpdate
getStoragePath
getResourceValue:forKey:error:
_sourceURL
T@"NSURL",R,V_sourceURL
_torsoRecognition
T@"HMITorsoRecognition",R,V_torsoRecognition
@16@0:8
B24@0:8@16
#16@0:8
@24@0:8:16
@32@0:8:16@24
@40@0:8:16@24@32
B16@0:8
B24@0:8#16
B24@0:8:16
Vv16@0:8
Q16@0:8
^{_NSZone=}16@0:8
B24@0:8@"Protocol"16
@"NSString"16@0:8
v24@0:8@?16
v32@0:8@16@?24
v24@0:8@?<v@?@"NSSet"@"NSError">16
v32@0:8@"NSSet"16@?<v@?@"NSSet"@"NSError">24
v24@0:8@?<v@?@"NSError">16
v32@0:8@"NSSet"16@?<v@?@"NSError">24
v24@0:8@?<v@?@"HMIExternalPersonManagerSettings"@"NSError">16
@72@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16@48@56f64f68
{CGRect={CGPoint=dd}{CGSize=dd}}16@0:8
f16@0:8
v16@0:8
@"NSString"
{CGRect="origin"{CGPoint="x"d"y"d}"size"{CGSize="width"d"height"d}}
@28@0:8@16B24
@48@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16
@32@0:8@16@24
v40@0:8@16@24@32
v24@0:8@16
v32@0:8@16@24
v64@0:8@16@24@32i40@44i52@56
v48@0:8@16i24@28i36@40
@"NSOutputStream"
@"NSObject"16@0:8
@24@0:8@16
@"HMIDESMutableFloatArray"
@"HMIFaceRecognition"
@"NSMutableArray"
@"NSMutableSet"
q16@0:8
v24@0:8q16
@"HMISystemResourceUsage"16@0:8
@"<HMISystemResourceUsageMonitorDelegate>"16@0:8
v24@0:8@"<HMISystemResourceUsageMonitorDelegate>"16
@"HMISystemResourceUsageMonitorImpl"
@"NSObject<OS_dispatch_queue>"
@56@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24
@64@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24@56
@88@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24@56@64@72@80
@96@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24@56@64@72@80@88
@"NSNumber"
@"HMITorsoAnnotation"
@"<HMIPersonManagerDataSource>"
@"NSSet"
I16@0:8
v24@0:8Q16
@24@0:8Q16
@"NSData"
{?=qiIq}16@0:8
@48@0:8@16{?=qiIq}24
{?="value"q"timescale"i"flags"I"epoch"q}
@24@0:8q16
@68@0:8{?={?=qiIq}{?=qiIq}}16B64
@64@0:8{?={?=qiIq}{?=qiIq}}16
{os_unfair_lock_s="_os_unfair_lock_opaque"I}
@48@0:8{?=qiIq}16@40
@"NSDate"
v48@0:8@16{?=qiIq}24
@40@0:8{?=qiIq}16
d40@0:8{?=qiIq}16
@"HMIVideoEventBuffer"
v40@0:8{?=qiIq}16
d16@0:8
@"HMIVideoTimeline"
@"HMITimeIntervalAverage"
v24@0:8d16
d32@0:8d16d24
@"MovingAverage"
@60@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24f56
^{__CVBuffer=}40@0:8@16^{__CVBuffer=}24^@32
@32@0:8^{__CVBuffer=}16^@24
@"<HMIHomePersonManagerDataSource>"
v24@0:8@"HMFTimer"16
@"HMIHomePersonManagerSettings"
@"NSOperationQueue"
@"HMFTimer"
@"NSMutableDictionary"
@"HMIFaceCrop"
@"HMIFaceprint"
@24@0:8^{__CVBuffer=}16
@64@0:8^{__CVBuffer=}16{?=qiIq}24Q48Q56
@80@0:8@16{?=qiIq}24Q48Q56{CGSize=dd}64
{CGSize=dd}16@0:8
^{__CVBuffer=}16@0:8
^{__CVBuffer=}
@"NSArray"
{CGSize="width"d"height"d}
^{__CVBuffer=}64@0:8^{__CVBuffer=}16{CGRect={CGPoint=dd}{CGSize=dd}}24^@56
^{__CVBuffer=}48@0:8^{__CVBuffer=}16{CGSize=dd}24^@40
^{__CVBuffer=}60@0:8^{__CVBuffer=}16{CGSize=dd}24I40q44^@52
^{__CVBuffer=}44@0:8^{__CVBuffer=}16I24q28^@36
^{__CVBuffer=}92@0:8^{__CVBuffer=}16{CGRect={CGPoint=dd}{CGSize=dd}}24{CGSize=dd}56I72q76^@84
@40@0:8^{__CVBuffer=}16f24f28^@32
^{__CVBuffer=}32@0:8^{CGDataProvider=}16^@24
^{__CVBuffer=}32@0:8@16^@24
^{__CVBuffer=}40@0:8{CGSize=dd}16I32B36
{CGRect={CGPoint=dd}{CGSize=dd}}80@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{CGSize=dd}48{CGSize=dd}64
{CGRect={CGPoint=dd}{CGSize=dd}}68@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{CGSize=dd}48f64
^{__CVBuffer=}92@0:8^{__CVBuffer=}16f24{CGRect={CGPoint=dd}{CGSize=dd}}28{CGSize=dd}60Q76^@84
@36@0:8Q16Q24f32
@24@0:8^{__IOHIDServiceClient=}16
^{__IOHIDServiceClient=}
B28@0:8i16^d20
B24@0:8^d16
^{__IOHIDEventSystemClient=}
@48@0:8@16@24@32^@40
B40@0:8^{__CVBuffer=}16@24^@32
B72@0:8^{__CVBuffer=}16@24@32@40@48@56^@64
v64@0:8@16@24@32@40@48@56
[7f]
[5{CGSize="width"d"height"d}]
@"HMINMSConfiguration"
@32@0:8@16B24B28
B32@0:8@16B24B28
^{opaqueCMSampleBuffer=}24@0:8@16
^{opaqueCMSampleBuffer=}16@0:8
^{opaqueCMSampleBuffer=}24@0:8^Q16
@"AVAsset"
@"AVAssetReader"
^{__CFArray=}
@"HMHomeManager"
v32@0:8@16Q24
v32@0:8@"HMHomeManager"16Q24
v24@0:8@"HMHomeManager"16
v32@0:8@"HMHomeManager"16@"HMHome"24
v32@0:8@"HMHomeManager"16@"HMAddAccessoryRequest"24
@?16@0:8
@"NSUUID"
@"MLFeatureValue"24@0:8@"NSString"16
@"NSSet"16@0:8
@"MLMultiArray"
@40@0:8@16@24^@32
B40@0:8@16^d24^@32
@"MLModel"
@"NSError"
v24@0:8^{opaqueCMSampleBuffer=}16
@"HMIVideoAnalyzerConfiguration"
@"HMIVideoAnalyzerDynamicConfiguration"
@"HMIDESBackgroundTask"
@"HMPhotosPersonManager"
@32@0:8q16@24
@24@0:8^{_NSZone=}16
v24@0:8@"NSCoder"16
@24@0:8@"NSCoder"16
@80@0:8@16@24@32{CGRect={CGPoint=dd}{CGSize=dd}}40@72
@88@0:8@16@24@32{CGRect={CGPoint=dd}{CGSize=dd}}40@72q80
@64@0:8@16@24{CGRect={CGPoint=dd}{CGSize=dd}}32
@24@0:8#16
@"HMIVideoFrame"
@80@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{?=qiIq}48f72S76
S16@0:8
B40@0:8{?=qiIq}16
f48@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16
Q48@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16
^{__CVBuffer=}24@0:8^{__CVBuffer=}16
@48@0:8^{__CVBuffer=}16{?=qiIq}24
v88@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24^{__CVBuffer=}56{?=qiIq}64
v32@0:8@16^{opaqueCMSampleBuffer=}24
B48@0:8^{__CVBuffer=}16{?=qiIq}24
@32@0:8@16^B24
v56@0:8@16@24{?=qiIq}32
v24@0:8^{__CVBuffer=}16
v40@0:8^{__CVBuffer=}16^f24i32i36
v32@0:8r^f16^{__CVBuffer=}24
f64@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24^S56
v68@0:8^S16S24S28{CGRect={CGPoint=dd}{CGSize=dd}}32f64
v44@0:8^f16^f24r^f32f40
v48@0:8^f16^f24r^f32r^f40
v24@0:8r*16
v40@0:8^{__CVBuffer=}16*24^S32
v32@0:8^{__CVBuffer=}16^f24
@48@0:8^S16{?=qiIq}24
v96@0:8@16^{__CVBuffer=}24@32{CGRect={CGPoint=dd}{CGSize=dd}}40{?=qiIq}72
^{__CVBuffer=}32@0:8^{__CVBuffer=}16Q24
^S16@0:8
v24@0:8^S16
^f16@0:8
v24@0:8^f16
v32@0:8{CGSize=dd}16
v20@0:8B16
B32@0:8@16@24
B32@0:8@"AVAssetResourceLoader"16@"AVAssetResourceLoadingRequest"24
B32@0:8@"AVAssetResourceLoader"16@"AVAssetResourceRenewalRequest"24
v32@0:8@"AVAssetResourceLoader"16@"AVAssetResourceLoadingRequest"24
B32@0:8@"AVAssetResourceLoader"16@"NSURLAuthenticationChallenge"24
v32@0:8@"AVAssetResourceLoader"16@"NSURLAuthenticationChallenge"24
q24@0:8@16
@88@0:8@16q24@32@40{CGRect={CGPoint=dd}{CGSize=dd}}48@80
@"HMICameraVideoFrame"
@"NSDictionary"
@56@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16d48
@40@0:8#16@24@32
@64@0:8#16{CGRect={CGPoint=dd}{CGSize=dd}}24@56
@28@0:8f16Q20
@32@0:8r^f16Q24
v20@0:8f16
v32@0:8r^f16Q24
r^f16@0:8
@20@0:8f16
@"NSMutableData"
@52@0:8@16^{__CVBuffer=}24B32@36^@44
@"HMIVideoAnalyzerEventFace"52@0:8@"HMIVideoAnalyzerEventFace"16^{__CVBuffer=}24B32@"NSUUID"36^@44
@24@0:8^@16
@64@0:8@16d24d32d40d48^@56
@"HMIFaceprinter"
@"HMIFaceQualityFilterSVM"
^{__CVBuffer=}24@0:8^@16
@56@0:8@16@24@32@40@48
@32@0:8Q16^@24
@32@0:8^{__CVBuffer=}16@24
@48@0:8@16@24@32@40
@76@0:8i16d20{CGRect={CGPoint=dd}{CGSize=dd}}28@60@68
i16@0:8
f80@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{CGRect={CGPoint=dd}{CGSize=dd}}48
v48@0:8@16@24d32q40
@72@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24{CGSize=dd}56
f24@0:8@16
@"HMITorsoprint"
@"NSMutableIndexSet"
v80@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24{?=qiIq}56
@72@0:8@16@24{CGRect={CGPoint=dd}{CGSize=dd}}32@64
v32@0:8@"HMIVideoFrameSampler"16^{opaqueCMSampleBuffer=}24
B40@0:8^{opaqueCMSampleBuffer=}16@24d32
@"<HMIVideoFrameAnalyzerDelegate>"
@"<HMICameraVideoFrameAnalyzer>"
@"HMIVideoFrameSampler"
v40@0:8@"HMIVideoFrameAnalyzer"16@"HMIVideoFrameAnalyzerResult"24@"NSError"32
v32@0:8@"HMIVideoFrameAnalyzer"16@"HMIAnalysisStateUpdate"24
@36@0:8@16@24f32
@40@0:8@16@24Q32
@40@0:8@16@24@32
@44@0:8@16@24@32B40
v28@0:8@16B24
v48@0:8@16@24@32@40
@104@0:8@16@24@32@40#48{CGRect={CGPoint=dd}{CGSize=dd}}56f88f92@?96
v56@0:8@16@24@32@40@48
@32@0:8@16^@24
v44@0:8@16@24@32B40
@28@0:8f16@?20
@28@0:8@16f24
@36@0:8@16f24f28B32
@28@0:8B16@?20
@36@0:8@16B24f28B32
@36@0:8@16#24f32
@44@0:8@16#24f32f36B40
@40@0:8@16#24f32f36
v76@0:8@16@24#32@40@48@56Q64B72
v40@0:8#16@24@32
v40@0:8@16#24@32
@64@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16@48@56
{CGRect={CGPoint=dd}{CGSize=dd}}24@0:8@16
@72@0:8@16@24@32{CGRect={CGPoint=dd}{CGSize=dd}}40
^{opaqueCMSampleBuffer=}
@"<HMIVideoRetimerDelegate>"
v32@0:8@"HMIVideoRetimer"16^{opaqueCMSampleBuffer=}24
@20@0:8i16
@28@0:8i16d20
@36@0:8i16@20d28
i32@0:8@16@?24
B20@0:8i16
v20@0:8i16
@"NSArray"16@0:8
@56@0:8@16@24@32q40@48
@"HMITorsoClassification"
^{__CVBuffer=}48@0:8@16^{__CVBuffer=}24@32^@40
q20@0:8i16
@36@0:8^{__CVBuffer=}16B24^@28
@"NSArray"32@0:8^{__CVBuffer=}16^@24
@"NSArray"32@0:8@"NSData"16^@24
@32@0:8{CGPoint=dd}16
{CGPoint=dd}16@0:8
{CGPoint="x"d"y"d}
B40@0:8@16@24^@32
@36@0:8i16@20@28
v40@0:8@16@24@?32
v32@0:8@"NSURLSession"16@"NSError"24
v40@0:8@"NSURLSession"16@"NSURLAuthenticationChallenge"24@?<v@?q@"NSURLCredential">32
v24@0:8@"NSURLSession"16
@"HMIHomeKitClient"
@"NSURLSession"
@"HMIFeedbackSession"
@"HMFOperation"
v44@0:8@16@24B32@?36
@56@0:8@16@24@32d40q48
@72@0:8@16@24@32@40@48d56q64
@76@0:8@16@24@32@40@48d56B64q68
@96@0:8@16@24@32{CGRect={CGPoint=dd}{CGSize=dd}}40@72@80d88
v48@0:8@16@24q32@?40
v24@0:8@?<v@?@"HMIHomePersonManagerSettings"@"NSError">16
v48@0:8@"NSSet"16@"NSUUID"24q32@?<v@?@"NSError">40
@28@0:8i16@20
@"HMIPersonFaceCrop"
@"HMIVideoAssetReader"
@32@0:8d16@24
@56@0:8Q16@24@32@40@48
@40@0:8q16@24@32
@48@0:8q16@24@32@40
@72@0:8@16{?={?=qiIq}{?=qiIq}}24
@80@0:8@16@24{?={?=qiIq}{?=qiIq}}32
@96@0:8@16@24{?={?=qiIq}{?=qiIq}}32{_NSRange=QQ}80
@88@0:8@16@24{?={?=qiIq}{?=qiIq}}32@80
@104@0:8@16@24{?={?=qiIq}{?=qiIq}}32@80{_NSRange=QQ}88
{?={?=qiIq}{?=qiIq}}16@0:8
r^{opaqueCMFormatDescription=}16@0:8
{_NSRange=QQ}16@0:8
r^{opaqueCMFormatDescription=}
{_NSRange="location"Q"length"Q}
{?="start"{?="value"q"timescale"i"flags"I"epoch"q}"duration"{?="value"q"timescale"i"flags"I"epoch"q}}
@56@0:8@16@24@32@40q48
@88@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24{?=qiIq}56@80
@56@0:8@16@24@32@40^q48
v56@0:8@16@24@32@40^q48
@"HMIPersonTracker"
@"<HMIAnalysisStateManagerDelegate>"
@32@0:8^{__CVBuffer=}16d24
r^{__CTFont=}24@0:8d16
v48@0:8@16{CGPoint=dd}24r^d40
v64@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16d48r^d56
v72@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16d48@56r^d64
^{CGContext=}
^{CGColorSpace=}
^{__CTFont=}
@"<HMISystemResourceUsageMonitorDelegate>"
@"<HMISystemResourceUsageMonitorProtocol>"
@36@0:8@16B24@28
@"VNPersonsModel"
@32@0:8@16q24
@36@0:8@16B24q28
v24@0:8#16
v28@0:8i16@20
v32@0:8q16@24
v32@0:8d16@24
v28@0:8B16@20
i24@0:8@16
d24@0:8@16
@32@0:8#16@24
B64@0:8@16@24Q32@40@48^@56
@40@0:8{?=ii}16I24B28^@32
B40@0:8{?=ii}16I24B28^@32
i32@0:8r^{__CFString=}16r^v24
i32@0:8r^{__CFString=}16r^^v24
i28@0:8r^{__CFString=}16i24
i32@0:8r^{__CFString=}16^i24
i32@0:8r^{__CFString=}16d24
i32@0:8r^{__CFString=}16^d24
v32@0:8{HMIVideoEncoderDataRate=qq}16
{HMIVideoEncoderDataRate=qq}16@0:8
^{OpaqueVTCompressionSession=}16@0:8
v24@0:8^{OpaqueVTCompressionSession=}16
^{OpaqueVTCompressionSession=}
@"<HMIVideoEncoderDelegate>"
v32@0:8@"HMIVideoEncoder"16^{opaqueCMSampleBuffer=}24
v32@0:8@"HMIVideoEncoder"16@"NSError"24
@"HMHomePersonManager"
B32@0:8@16@?24
q32@0:8q16@24
@40@0:8@16Q24Q32
B32@0:8@16^@24
B52@0:8@16@24B32@36^@44
B24@0:8^@16
@44@0:8@16B24@28^@36
@40@0:8@16@24f32f36
@64@0:8@16@24@32@40@48^@56
f40@0:8@16@24^@32
@52@0:8@16Q24@32B40^@44
@"HMIDESDataset"
@"NSURL"
@40@0:8Q16d24@32
@"<HMIExternalPersonManagerDataSource>"
@"HMIExternalPersonManagerSettings"
@36@0:8@16@24B32
@32@0:8@16@?24
@"HMISystemResourceUsageMonitor"
@"NSPointerArray"
@"TRIClient"
B40@0:8@16^v24^@32
f32@0:8@16@24
@24@0:8^v16
{shared_ptr<homeai::clustering::GreedyClusterer>="__ptr_"^{GreedyClusterer}"__cntrl_"^{__shared_weak_count}}
@"<HMIVideoFrameSamplerDelegate>"
@"HMICIFilterAttributeValue"
@32@0:8@16d24
@"VNSession"
@"HMFOSTransaction"
v40@0:8@"HMIVideoPackageAnalyzer"16@"HMIVideoPackageAnalyzerResult"24@"NSError"32
{CGRect={CGPoint=dd}{CGSize=dd}}40@0:8@16{CGSize=dd}24
@56@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16@48
@"<HMIVideoPackageAnalyzerDelegate>"
@"HMIVideoFrameIntervalSampler"
@"HMICameraVideoFrameAnalyzerSignificantActivity"
@"HMIBackgroundEstimator"
@"HMIHTMLReport"
f24@0:8^{__CVBuffer=}16
{vector<float, std::allocator<float>>="__begin_"^f"__end_"^f"__end_cap_"{__compressed_pair<float *, std::allocator<float>>="__value_"^f}}
@64@0:8@16@24@32@40@48@56
@24@0:8B16B20
@"HMIVideoFragment"
@"HMIVideoAnalyzerResultOutcome"
@48@0:8@16@24@32f40f44
v28@0:8@16f24
B48@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16
B52@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16f48
B28@0:8@16f24
B72@0:8@16@24@32@40@48@56@64
B64@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16@48#56
B48@0:8{CGPoint=dd}16{CGPoint=dd}32
v20@0:8I16
@"HMICamera"
@"HMIVideoAnalyzerEvent"
@80@0:8@16@24@32{?=qiIq}40{CGSize=dd}64
@80@0:8@16@24@32@40{CGRect={CGPoint=dd}{CGSize=dd}}48
B80@0:8@16{?=qiIq}24{CGRect={CGPoint=dd}{CGSize=dd}}48
@"HMIVideoAnalyzerFrameResult"
B68@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16q48#56f64
i40@0:8^{__CVBuffer=}16@24@?32
i40@0:8@16@24@?32
@72@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24@56@64
@"HMIVideoAnalyzerEventFace"
@"HMIVideoAnalyzerEventTorso"
@44@0:8@16B24B28d32f40
@60@0:8@16B24d28Q36@44^@52
Q24@0:8q16
d24@0:8q16
B24@0:8d16
@40@0:8@16@24@?32
B28@0:8@16B24
@"HMIPerson"
@40@0:8r*16@24@?32
r*16@0:8
B84@0:8@16@24q32{?=qiIq}40f64@?68@?76
B76@0:8@16@24{?=qiIq}32f56Q60^B68
v40@0:8@16^{opaqueCMSampleBuffer=}24@32
^{__CVBuffer=}32@0:8^{__CVBuffer=}16@24
I40@0:8@16{CGSize=dd}24
@"HMIGreedyClustering"
@64@0:8i16@20@28@36@44B52^@56
@"<HMIFaceClassifier>"
@"HMIPersonsModelManager"
@"HMIClusteringTaskSummary"
@52@0:8i16@20@28@36B44B48
@40@0:8@16{CGSize=dd}24
@52@0:8@16q24B32@36^@44
@64@0:8^{__CVBuffer=}16{CGRect={CGPoint=dd}{CGSize=dd}}24^@56
@48@0:8@"NSDictionary"16@"NSDictionary"24@"HMIVideoAnalyzerConfiguration"32^@40
v24@0:8@"HMICameraVideoFrame"16
@"HMICameraVideoFrameResult"52@0:8@"HMICameraVideoFrame"16q24B32@"NSUUID"36^@44
@"NSSet"64@0:8^{__CVBuffer=}16{CGRect={CGPoint=dd}{CGSize=dd}}24^@56
@"HMIAnalysisStateUpdate"28@0:8@"NSUUID"16B24
@"NSDictionary"16@0:8
@80@0:8^{__CVBuffer=}16{CGRect={CGPoint=dd}{CGSize=dd}}24{CGSize=dd}56^@72
q24@0:8#16
@32@0:8q16B24B28
@76@0:8@16@24{CGRect={CGPoint=dd}{CGSize=dd}}32@64B72
v64@0:8@16@24{CGRect={CGPoint=dd}{CGSize=dd}}32
@"NSMapTable"
@"HMISignificantActivityFcosDetector"
@"HMIFaceClassifierVIP"
@"HMITorsoClassifier"
@"HMISessionEntityManager"
@64@0:8@16{CGSize=dd}24{?=qiIq}40
@24@0:8^{opaqueCMSampleBuffer=}16
@40@0:8d16d24^@32
*40@0:8Q16Q24^i32
i40@0:8^v16Q24r*32
i32@0:8@16@24
@"NSCondition"
@"<HMIVideoCommandBufferDelegate>"
v32@0:8@"HMIVideoCommandBuffer"16^{opaqueCMSampleBuffer=}24
v24@0:8@"HMIVideoCommandBuffer"16
@24@0:8d16
v32@0:8@16d24
v48@0:8@16d24{_NSRange=QQ}32
@48@0:8@16@24@32q40
#24@0:8@16
@32@0:8#16q24
@"HMIConfidence"
@40@0:8@16@24d32
@20@0:8B16
B28@0:8^{opaqueCMSampleBuffer=}16B24
B24@0:8r^{opaqueCMFormatDescription=}16
^{opaqueCMBufferQueue=}16@0:8
v24@0:8^{opaqueCMBufferQueue=}16
^{OpaqueVTDecompressionSession=}16@0:8
v24@0:8^{OpaqueVTDecompressionSession=}16
@"<HMIVideoDecoderDelegate>"
^{opaqueCMBufferQueue=}
^{OpaqueVTDecompressionSession=}
@"HMFWeakObject"
v32@0:8@"HMIVideoDecoder"16^{opaqueCMSampleBuffer=}24
v32@0:8@"HMIVideoDecoder"16@"NSError"24
v48@0:8@16@24q32@40
v40@0:8@16@24q32
v48@0:8@"AVAssetWriter"16@"NSData"24q32@"AVAssetSegmentReport"40
v40@0:8@"AVAssetWriter"16@"NSData"24q32
@32@0:8r^{opaqueCMFormatDescription=}16r^{opaqueCMFormatDescription=}24
@64@0:8r^{opaqueCMFormatDescription=}16r^{opaqueCMFormatDescription=}24Q32{?=qiIq}40
@"<HMIVideoAssetWriterDelegate>"
@"AVAssetWriter"
@"AVAssetWriterInput"
v32@0:8@"HMIVideoAssetWriter"16@"NSData"24
v40@0:8@"HMIVideoAssetWriter"16@"NSData"24@"AVAssetSegmentReport"32
v32@0:8@"HMIVideoAssetWriter"16@"NSError"24
B48@0:8@16@24d32d40
@48@0:8{CGPoint=dd}16{CGVector=dd}32
{CGVector=dd}16@0:8
{CGVector="dx"d"dy"d}
@32@0:8@16Q24
@84@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{CGSize=dd}48@64f72Q76
f68@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16q48#56f64
@56@0:8^f16^{__CVBuffer=}24^{__CVBuffer=}32@40Q48
^{opaqueCMSampleBuffer=}24@0:8^{opaqueCMSampleBuffer=}16
@"NSArray"56@0:8^f16^{__CVBuffer=}24^{__CVBuffer=}32@"NSArray"40Q48
@56@0:8^{__CVBuffer=}16^{__CVBuffer=}24^f32@40Q48
@172@0:8{vector<unsigned char, std::allocator<unsigned char>>=**{__compressed_pair<unsigned char *, std::allocator<unsigned char>>=*}}16{vector<float, std::allocator<float>>=^f^f{__compressed_pair<float *, std::allocator<float>>=^f}}40{vector<cv::Point_<float>, std::allocator<cv::Point_<float>>>=^v^v{__compressed_pair<cv::Point_<float> *, std::allocator<cv::Point_<float>>>=^v}}64{vector<cv::Point_<float>, std::allocator<cv::Point_<float>>>=^v^v{__compressed_pair<cv::Point_<float> *, std::allocator<cv::Point_<float>>>=^v}}88@112Q120{vector<cv::Mat, std::allocator<cv::Mat>>=^{Mat}^{Mat}{__compressed_pair<cv::Mat *, std::allocator<cv::Mat>>=^{Mat}}}128{CGSize=dd}152f168
B72@0:8{CGPoint=dd}16{CGPoint=dd}32{CGSize=dd}48@64
^{__CFArray=}16@0:8
@80@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24^{__CVBuffer=}56@64^@72
@"HMITorsoprinter"
@36@0:8^{opaqueCMSampleBuffer=}16f24@28
v32@0:8^{opaqueCMSampleBuffer=}16^{opaqueCMSampleBuffer=}24
{os_unfair_lock_s=I}16@0:8
@"<HMIVideoFrameSelectorDelegate>"
@"<HMIMotionDetector>"
v48@0:8@16^{opaqueCMSampleBuffer=}24@32d40
v40@0:8@16@24^{opaqueCMSampleBuffer=}32
^{opaqueCMSampleBuffer=}32@0:8@16^{opaqueCMSampleBuffer=}24
v48@0:8@"HMIVideoFrameSelector"16^{opaqueCMSampleBuffer=}24@"NSArray"32d40
v40@0:8@"HMIVideoFrameSelector"16@"NSArray"24^{opaqueCMSampleBuffer=}32
^{opaqueCMSampleBuffer=}32@0:8@"HMIVideoFrameSelector"16^{opaqueCMSampleBuffer=}24
@172@0:8@16@24@32B40d44d52d60d68Q76d84{?=qiIq}92Q116Q124Q132Q140Q148d156B164B168
@72@0:8@16@24@32@40d48q56@64
@44@0:8@16@24B32^@36
@"<HMIVideoAnalyzerDelegate>"
@"HMIHomePersonManager"
@"HMIAnalysisStateManager"
@"HMIVideoAnalyzerState"
@"HMIVideoAnalyzerMutableReport"
v32@0:8@"HMIVideoAnalyzer"16@"HMIVideoAnalyzerFragmentResult"24
v32@0:8@"HMIVideoAnalyzer"16@"HMIVideoAnalyzerFrameResult"24
v32@0:8@"HMIVideoAnalyzer"16@"NSError"24
v32@0:8@"HMIVideoAnalyzer"16@"HMIVideoFragment"24
v32@0:8@"HMIVideoAnalyzer"16@"HMIAnalysisStateUpdate"24
v32@0:8@"HMIVideoAnalyzer"16@"NSDictionary"24
v32@0:8^{opaqueCMSampleBuffer=}16@?24
v32@0:8r^{opaqueCMFormatDescription=}16r^{opaqueCMFormatDescription=}24
v24@0:8r^{opaqueCMFormatDescription=}16
v32@0:8:16@24
@"HMIVideoCommandBuffer"
@"HMIVideoDecoder"
@"HMIVideoEncoder"
@"HMIVideoFrameSelector"
@"HMIVideoFrameAnalyzer"
@"HMIVideoAssetWriter"
@"HMIVideoPackageAnalyzer"
@"HMIVideoTemporalEventFilter"
@"HMITorsoRecognition"
gepj
v024
ffffff
333333
333333
333333
333333
333333
333333
333333
