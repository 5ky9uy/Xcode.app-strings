%s invalid packets
%s ::: Error creating output file %{public}@, err: %{public}d
%s ::: Error writing to output wave file. : %{public}ld
%s metaInfo passed is nil - Bailing out
%s Unable to read data from file: %@
%s Could not read existing %@ file: err: %@
%s ERR: Failed to create json %{public}@ with %{public}@
%s value = %{public}d
%s Couldn't create SSV log directory at path %{public}@ %{public}@
%s Couldn't create speech log directory at path %{public}@ %{public}@
%s Force enabling VoiceTrigger AP mode ? %{public}@
%s Force enabling VoiceTrigger AOP mode ? %{public}@
%s Couldn't create SoS log directory at path %{public}@ %{public}@
%s enableAudioInection: is only available on internal builds
%s setAudioInjectionFilePath: is only available on internal builds
%s kCSAudioInjectionFilePathKey is not array type
%s kCSAudioInjectionFilePathKey array size = %d
%s kCSAudioInjectionFilePathKey doesn't have NSString as an array entry
%s Override iOS barge-in support key to: %{public}@
%s Shouldn't be called on non-iOS platform
%s SpkrId:: ERR: Hybrid endpointer not ready for processing request
%s SpkrId:: VAD processed %f secs of audio
%s SpkrId:: Endpoint already reported. Not scheduling
%s SpkrId:: Found Endpoint at: [%f %f %f %f]
%s SpkrId:: Found startpoint at: [%f %f %f %f]
%s ::: Error reading file %@, err: %d
%s CSAudioFileReader requires prepare recording settings to feed audio
%s CSAudioFileReader only support LinearPCM to feed
%s Setting ExtAudioFileSetProperty failed : %d
%s Starting audio file feed timer, bufferDuration = %f sampleRate = %f, bytesPerFrame = %d, channelsPerFrame = %d
%s ::: Error reading data from audio file : %d
%s Reach to EOF, chunkSize = %d
%s Stopping audio file feed timer
%s Plan removing the temp file %{public}@
%s Failed to remove temp file %{public}@ reason: %{public}@
%s Start copying %{public}u bytes of data to crashreporter with wav file header offset %{public}llu
%s Failed to read data from %{public}@
%s Finished copying data to crashreporter.
%s Logging audio file into : %{public}@
%s ERR: reading contents of gradingDir: %{public}@ with error %{public}@
%s Deleting orphaned grading file %{public}@
%s ERR: Failed to delete %{public}@ with error %{public}@
%s Removing non-dir at AttentiveSiri AudioLog dir: %@
%s Error removing %@: err: %@
%s Failed to create AudioLogging directory for AttentiveSiri: %@
%s Created AudioLogging dir for AttentiveSiri at: %@
%s Trying to set record buffer duration to %lf
%s Failed setting record buffer duration. Duration is %lf
%s Error initializing voice controller with context %@ %@
%s AVVC startRecordingWithSettings failed.
%s AVVC Stop Recording
%s audioInput:[%@]
%s No Reocrd Route detected
%s audioOutput:[%@]
%s SpkrId:: Failed to create SSRSpeakerRecognizerPSR
%s SpkrId:: %@::uniqueUttTag: %@, extraSamplesAtStart: %lu, _tdEndInSampleCount: %lu(%f ms)
%s SpkrId:: CSSpIdVTSpeakerRecognizer dealloc
%s SpkrId:: Discarded ScoreCard for mismatch session - {%{public}@, %{public}@}
%s Cannot generate subChunk since channel(%{public}tu) is larger than number of channels(%{public}tu)
%s Cannot generate subChunk if it reuqest more than it has : %{public}tu %{public}tu %{public}tu
%s SpkrId:: Processing ended at: numSamplesProcessed=%lu, totalSampleCountToReach=%lu
%s SSROrch[%{public}@]:: Failed to create PSR Recognizer
%s SSROrch[%{public}@]:: Failed to create SAT Recognizer
%s SSROrch[%{public}@]:: Successfully initialized with {%{public}@, %{public}@}
%s SSROrch[%{public}@]:: Ignoring addAudio, endAudio: %d procSamples: %lu maxProcSamples: %lu
%s SSROrch[%{public}@]:: Scorecard %{public}@ with delay:%{public}ldms, processed:%{public}ldms, await:%{public}ldms
%s ERR: Posting diagnostic report for abnormal score delay - %ldms
%s SSROrch[%{public}@]:: ERR: Scores for profileId %{public}@ not present in %{public}@ - Skipping
%s SSROrch[%{public}@]:: Sync score report with %{public}f delay - with known user scores %{public}@
%s SSROrch[%{public}@]:: ERR: VoiceInfo is nil from recognizer %{public}@
%s SSROrch[%{public}@]:: Discarded speaker scores for session - %{public}@
%s SSROrch[%{public}@]:: EndAudioCalled is false, returning for recognizer %{public}@
%s SSROrch[%{public}@]:: PSR Analyzer finished the session with %{public}@
%s SSROrch[%{public}@]:: SAT Analyzer finished the session with %{public}@
%s SSROrch[%{public}@]:: Wait for %{public}@ analyzer to complete the session - %{public}@
%s SSROrch[%{public}@]:: Finished the session with known user scores %{public}@
%s SSROrch[%{public}@]:: Discarded speaker scores for session
%s SSROrch[%{public}@]:: Speech started at - %ldms
%s SSROrch[%{public}@]:: Starting a new segment of speech - %ldms
%s SAT Supervector created
%s Skipping PHS DES record creation
%s ERR: failed to create PHS DES recored with error: %@
%s Failed to create PHS DES record: %{public}@
%s Created PHS DES record with identifier: %{public}@
%s %@
%s CSVoiceTriggerAsset found: %{public}@
%s Locale: [%{public}@]
%s No locale set when creating phrase spotter.
%s Creation of Keyword Detector failed.
%s %s async called
%s %{public}s Called
%s %{public}s async called
%s Called before completion called
%s BEGIN num:%{public}ld use:%{public}d
%s AudioSession setup failed
%s Has wrong audio routing, ask user to unplug headset
%s Start Audio Session failed
%s _sessionNumber [%{public}ld]
%s %{public}s Canceling Training
%s Called with status : %{public}d
%s %{public}s called
%s AudioSession StartRecording Failed
%s Setting suspendAudio:[%{public}d]
%s Resume training
%s Suspend training
%s Stop Listening
%s ERR: Failed to save explicit utterance
%s ERR: SpeakerRecognition is not available on this platform
%s Creating OS Transaction for %{public}@
%s Release OS Transaction for %{public}@
%s ERR: voiceProfile is nil - Bailing out
%s ERR: Failed in trigger processing %{public}@ with %{public}@
%s Trigger Score %{public}f not satisfied implicit VT threshold %f
%s Using recognizer scale factor: %f for phrase detector
%s ERR: Failed to create trigger phrase detectors
%s Processing %{public}@ for trigger word detection
%s Failed to read file: %@
%s ERR: Failed processing %{public}@ with error %{public}@
%s ERR: %{public}@
%s Best trigger score for %{public}@ is %{public}f (%{public}f, %{public}f)
%s Dealloc of CSRemoteControlClient, it should close connection
%s Fallback asset resource path : %{public}@
%s Cannot find corespeech asset from resourcePath : %{public}@
%s Configuration file is not exists : %{public}@
%s Cannot read configuration file : %{public}@
%s Cannot decode configuration json file : %{public}@
%s Configuration json file is not expected format
%s Cannot access to %{public}@ %{public}@ using default value=%{public}@
%s SpkrId:: Unknown CSSpIdType string: %@
%s ERR: Unknown CSSpIdType: %lu
%s Unknown CSSpIdType: %lu
%s SpkrId:: path is nil - Bailing out
%s SpkrId:: Direntry with same name exists, this will be removed: %@
%s SpkrId:: Creating Directory : %@
%s SpkrId:: Creating Directory failed : %@
%s SpkrId:: Exceeded privacy limit for grading utterances : %ld (%d)
%s SpkrId:: VoiceId not supported in language %{public}@
%s SpkrId:: ERR: filePath passed as nil - Bailing out
%s SpkrId:: ERR: Could not read existing %@ file: err: %@
%s SpkrId:: ERR: %@ is a directory
%s SpkrId:: ERR: file do not exist - %@
%s Unknown Device category for deviceProduceType: %@
%s ERR: Unknown device. returning false: %{public}@
%s ERR: satLanguagePath is nil. returning false
%s Voice Profile Mismatch - CurrentDeviceCategory %@ VoiceProfileCategory %@
%s Malformed audio-dir URL for string <%{public}@>:url
%s ERR: reading contents of audioDir: %{public}@
%s No jsonFiles found in %{public}@: jsonFiles.count=%{public}lu
%s Unexpected: empty JSON data for file: %{public}@
%s Error reading metaDict at path: %{public}@
%s metaProductType: %{public}@
%s vtprofile: currDevice=[%{public}@:%{public}@] ; vpDirDevice=[%{public}@:%{public}@]
%s VoiceProfile MATCH
%s VoiceProfile MIS-MATCH
%s Could not find productType in VT-Meta file, trying next one
%s No compatible VT profile found for CurrDevice: %{public}@
%s ERR: fetching contents of %{public}@ failed with error %{public}@
%s ERR: Directory is nil - Bailing out
%s ERR: %{public}@ doesnt exist - Bailing out
%s Dump content of %{public}@ - %{public}@
%s Error reading directory at %@: err: %@
%s %@ is empty
%s Fetching homeUserId for siriProfileId %{public}@
%s siriProfileId %{public}@ maps to homeUserId %{public}@
%s ERR: Home User Id erred %{public}@ for Siri Profile Id %{private}@
%s ERR: Seeing more than one voice profiles for Siri App Domain
%s SpkrId:: Error creating uttMetaJsonData: %@
%s SpkrId:: Failed to create UttMeta...
%s Setting payloadstartSample %lu for trigger duration of %fsecs
%s ERR: Setting max payloadstartSample %lu for trigger duration of %fsecs
%s Deleted file %{public}@
%s EOF: utteranceLength: %lums, tdlength: %lums tdtiLength: %lums tdtiDiscardedLength: %lums
%s satAudioDirectory is nil - Bailing out
%s metaVersionFile %{public}@ doesnt exist
%s metaVersionFile %{public}@ doesnt exist - Skipping
%s ERR: Fetching contents of %{public}@ failed with error - %{public}@
%s Siri language is nil, falling back to %@
%s Cannot create CSVTUIKeywordDetector since there is no asset available
%s Cannot create CSVTUIKeywordDetector since we cannot initialize NDAPI
%s Sending Analytics Event - %{public}@
%s Delta is larger than anchorHostTime: anchorSampleCount = %{public}lld, sampleTime = %{public}lld, anchorHostTime = %{public}lld
%s Delta is larger than anchorSampleCount
%s Not supported on this platform
%s cannot handle nil event 
%s ignore unknown types of message: %{public}@
%s cannot handle nil error
%s Listener connection disconnected
%s connection error: %{public}s
%s Processing onboarded Siri user: %{public}@
%s Detected matching %{public}d users: %{public}@
%s Valid profile not found %{public}@ and %{public}@ - defaulting to %{public}@
%s Adding invalid user for deletion - %{public}@
%s Skipping retaining user %{public}@
%s Detected invalid user: %{public}@
%s File path doesnt exist - %{public}@
%s ERR: Failed reading contents of SAT root %{public}@ with %{public}@
%s App domains in use - %{public}@
%s ERR: Failed determining if file is dir-entry url=%{public}@ with %{public}@
%s Deleting invalid file %{public}@
%s Deleting invalid domain %{public}@ not part of domains %{public}@
%s Processing domain - %{public}@
%s ERR: Failed reading AppDomain %{public}@ at %{public}@ with %{public}@
%s Processing locale - %{public}@
%s Deleting invalid locale %{public}@ not supported in set %{public}@ and current language %{public}@
%s Processing profile - %{public}@
%s Deleting invalid profile %{public}@
%s Removing Implicit utterance cache directory at %{public}@
%s Processing profile %{public}@ with version %{public}d and identity %{public}@
%s Found legacy voice profile - Skipping
%s Deleting invalid SAT entry: %{public}@
%s ERR: Failed to get atrributes of file %{public}@, err %{public}@, size %{public}llu
%s Deleting invalid SAT entry: %{public}@ %{public}@
%s Found non-meta file: %{public}@
%s Deleting invalid SAT entry: %{public}@ : <%{public}@>
%s Processed %{public}@ with %{public}d explicit and %{public}d implicit utterances
%s ObsoleteCutOffDate is nil - Bailing out
%s Checking payload utterances prior to %{public}@ for profile %{public}@ and modelType %d
%s Deleting lifetimeexpired SAT entry %{public}@
%s Deleted lifetimeexpired metafile %{public}@
%s Deleting model file %{public}@ with err %{public}@
%s SpkrId:: ERR: missing arguments to create voice profile - Bailing out
%s Deleted %{public}@ for privacy
%s Successfully imported TD %{public}lu utterances to profile %{public}@
%s Importing to %{public}@ of %{public}@ from import Dir %{public}@
%s Successfully imported TDTI %{public}lu utterances to profile %{public}@
%s Successfully copied %{public}lu(%{public}lu) TD utterances to TDTI which now has %{public}lu(%{public}lu) utterances
%s Copied %{public}@ to %{public}@ with error %{public}@
%s Error to copy utterance from %{public}@ to %{public}@, error: %{public}@
%s Copied Utterance from %{public}@ to %{public}@
%s Error to copy jsonFile from %{public}@ to %{public}@, error: %{public}@
%s Successfully copied %{public}lu(%{public}lu) utterances to profile %{public}@
%s ERR: date is nil - Bailing out
%s Filtered %@ with enrolled date %@
%s Couldn't delete invalidated speaker model at path %{public}@ %{public}@
%s No Audio file exists when enrollment marker is set, remove marker
%s Contents of audio dir - %{public}@
%s SAT path doesnt exist - %@
%s Coudn't mark SAT enrollment %{public}@ at path %{public}@
%s Marked SAT enrollment %{public}@ at path %{public}@
%s We can't mark SAT %{public}@ when there is no audio directory
%s ERR: Unknown device-category for device: %{public}@
%s ERR: error creating updatedVoiceProfileJsonData from: %@, err: %@
%s ERR: error removing voice profile version file at: %@, err: %@
%s ERR: Error writing voice profile version file at: %@, err:%@
%s ERR: Profile dict is nil - Bailing out
%s ERR: error updating updatedVoiceProfileJsonData from: %@, err: %@
%s ERR: Failed initialization in _EARSyncSpeechRecognizer initWithConfiguration
%s ERR: processAudioChunk failed
%s ERR: endAudio failed
%s Start Recording Host Time = %{public}llu
%s Called with explicit spId type %{public}d - Bailing out
%s Voice Profile pruning cookie from Asset %{public}@ lastCookie %{public}@
%s Pruning cookie unavailable from asset - Bailing out
%s Already pruned voice profile - Bailing out
%s ERR: Failed updating pruning cookie
%s Explicit utterances: %{public}@, Implicit utterances: %{public}@
%s ERR: No explicit utterances!!! - Bailing out
%s ERR: Low explicit utterances - Bailing out
%s Zero implicit utterances - Bailing out
%s Pruning(1)::----------------------------- Retrain profile to create explicit model ---------------------------------------
%s ERR: Failed to create SSR context with error %{public}@ - Bailing out
%s Pruning(2)::----------------------------- Check Explicit Utterance scores ---------------------------------------
%s Low Score Explicit utterance[%{public}d]: %{public}@ --> T:(%{public}.3f, %{public}.3f) S:(%{public}.3f, %{public}.3f) P:(%{public}.3f, %{public}.3f) C:%{public}.3f
%s Explicit utterance[%{public}d]: %{public}@ --> T:(%{public}.3f, %{public}.3f) S:(%{public}.3f, %{public}.3f) P:(%{public}.3f(%{public}.3f) C:%{public}.3f
%s ERR: Detected explicit utterances with lower scores, Bailing out
%s Pruning(3)::----------------------------- Implicit selection ---------------------------------------
%s ERR: ScoreCard is nil in voice profile pruning - Bailing out
%s Deleting low Score Implicit utterance[%{public}d]: %{public}@ --> T:(%{public}.3f, %{public}.3f) S:(%{public}.3f, %{public}.3f) P:(%{public}.3f, %{public}.3f) C:%{public}.3f
%s Implicit utterance[%{public}d]: %{public}@ --> T:(%{public}.3f, %{public}.3f) S:(%{public}.3f, %{public}.3f) P:(%{public}.3f, %{public}.3f) C:%{public}.3f
%s Pruning(4)::----------------------------- Implicit sampling ---------------------------------------
%s Utterance selection totalImplicit: %{public}lu selectionIndex: %{public}lu retentionCount: %{public}lu deleteCount: %{public}lu 
%s Deleting implicit utterances(%lu) - %{public}@
%s Pruning(5)::----------------------------- Retrain the voice profile ---------------------------------------
%s ERR: creating pruning voice profile failed with %{public}@
%s ERR: Failed in processing %{public}@ with %{public}@
%s ERR: %@
%s Deleting %{public}@
%s %{public}@
%s Skipping SAT Model {%{public}@, %{public}@} for %{public}@
%s Skipping model {%{public}@, %{public}@} for %{public}@
%s Added model context {%{public}@, %{public}@} for %{public}@
%s ERR: uttPath is nil -  Bailing out
%s ERR: uttPath is nil - Bailing out
%s ERR: uttMeta is nil - Bailing out
%s ::: Error creating json Metadata: %{public}@
%s ERR: uttMetaURL is nil, defaulting to NO
%s ERR: metaData is nil, defaulting to NO for %{public}@
%s ERR: read metafile %{public}@ failed with %{public}@ - defaulting to NO
%s ERR: missing %{public}@ key in %{public}@ - defaulting to NO
%s ERR: uttMetaURL is nil - Bailing out
%s ERR: metaData is nil for %{public}@ - Bailing out
%s ERR: read metafile %{public}@ failed with %{public}@ - Bailing out
%s ERR: metaDict from file %{public}@ isnt a dictionary - %{public}@
%s ERR: missing %{public}@ key in %{public}@ - Bailing out
%s ERR: %{public}@ is not present
%s ERR: Unexpected. metaVersionFileData is empty while the file exists at: %{public}@
%s Json-Err reading metaVersionFile: %{public}@: err: %{public}@
%s Failed to create regular expression : %{public}@
%s Testing [%@] against regex.
%s Fired VoiceTrigger Timeout
%s Stop right now since ASR has issue
%s EOS Timeout Fired
%s Force endpoint fired
%s Discarding surplus audio of %{public}lu samples, audio sample available %{public}lu (%{public}lu, %{public}lu, %{public}lu)
%s AudioSession Started
%s AudioSession Stopped
%s Begin of speech detected
%s End of speech detected with endpoint type: %{public}ld
%s unknown endpoint type
%s Called with status : %{public}d and success : %{public}d utteranceStored : %{public}d
%s ERR: Failed to store utterance, overiding status
%s Non Final: [%{public}@]
%s NON Final Matching
%s Final: [%{public}@]
%s Final Matching
%s Final Not Matching
%s SPEECH RECOGNITION TASK FINISH UNSUCCESSFULLY
%s %{public}@ %{public}@ %{public}ld %{public}@
%s Recog Result: [%{public}ld]
%s returning status to UI : %{public}d
%s Already reported status or no callback
%s Will suspend training
%s Will resume training
%s Decide to delay ending ASR: [%{public}ld] samples
%s Triggered! Event info: %{public}@
%{public}9lld %{public}9lld %{public}9lld
%s analyzing.... score so far: %{public}5.3f
%s feeding tailing: [%{public}ld] samples
%s correctSampleSize:    [%{public}ld]
%s accumSampleSize:      [%{public}ld]
%s startBufferIndex:     [%{public}ld]
%s startBufferSampleSize:[%{public}ld]
%s samplesToBeDeleted:   [%{public}ld]
%s Total Number of buffs:[%{public}ld]
%s Adjusting the start buffer
%s Adjusting the array elements
%s Unsupported
%s %{public}s CALLED
%s Master Timeout Fired
%s recognized text = %{public}@
%s ERR: Profile not available for %{public}@ & %{public}@ - Bailing out
%s ERR: No configured Siri Profiles
%s ERR: More than one Siri Voice Profiles - %{public}@
%s ERR: Failed to add profile into the store with error %{public}@
%s ERR: FilePath is nil - Bailing out
%s ERR: Training utterance doesnt exist at %@ - Bailing out
%s ERR: Training utterance is marked as directory at %@ - Bailing out
%s VoiceTriggerEventInfo is nil - Bailing out
%s kVTEILanguageCode is nil - Bailing out
%s Received implicit utterance for %{public}@ from %{public}@ with context %d
%s ERR: trigger score not found in VTEI - Bailing out
%s ERR: SAT did not trigger!!! - Bailing out
%s Implicit training not enabled for %{public}@
%s sharedSiriId is nil - Bailing out
%s Rejecting barge-in utterance from adding to voice profile
%s Privacy disallowed implicit utterance %{public}@ - skipping
%s ERR: Failed to segment %{public}@ with %{public}@ - Bailing out
%s Processed implicit utterance %{public}@ successfully
%s ERR: Failed to process implicit utterance %{public}@ with error %{public}@
%s Enrolling voice profile of %{public}@ 
%s Failed enrolling %{public}@ with error %{public}@
%s Failed to get device hash list %{public}@
%s Processing sync data from device hash: %{public}@
%s Skipping language [%{public}@] since we already have enrollment data for this language
%s Skipping language [%{public}@] as file path doesnt exist - %{public}@
%s Skipping language [%{public}@] as voice profile not compatible
%s Error to copy profile from %{public}@ to %{public}@, error: %{public}@
%s language: %{public}@, enableVTAfterSyncLanguage: %{public}@, currSiriLanguage: %{public}@
%s Enabling VoiceTrigger Upon VoiceProfile sync for language: %{public}@ and enrolled language: %{public}@
%s VoiceTrigger does not exist for this platform, not setting VoiceTriggerEnabled
%s Not enabling VoiceTrigger Upon VoiceProfile sync for language: %{public}@
%s Sucessfully migrated language %{public}@
%s Migrated language %{public}@ but failed to mark SAT enrollment
%s Sucessfully marked as migrated for language : %{public}@
%s Failed to mark migrated for language : %{public}@
%s Failed to remove update path [%{public}@] upon migration completion, error: %{public}@
%s Failed to download voice profile with error %{public}@ and category %{public}@ 
%s Successfully enrolled voice profile %{public}@ with %{public}@ profile
%s Skipped enrolling voice profile %{public}@ with %{public}@ profile
%s ERR: Failed in enrolling Voice profile %{public}@ with category %{public}@ profile
%s Failed to enroll siriProfileId %{public}@ with %{public}@
%s Language %{public}@ not supported in %{public}@ - Skipping
%s Skipping profile download for %{public}@ - %{public}@ not matching current %{public}@
%s Skipping profile download for %{public}@ - voiceId not supported in %{public}@
%s Adding voiceprofile for %{public}@ in language %{public}@ completed with error %{public}@
%s ERR: Failed migrating Voice profile with ID %{public}@ for language %{public}@ with error %{public}@
%s Successfully added %{public}@ in %.2fms
%s Sucessfully enrolled %{public}@ for language %{public}@
%s Deleting audio directories of profile %{public}@ for privacy reasons
%s PHS update directory already exists, remove before we move forward
%s Failed to delete PHS update directory
%s Failed to create PHS update directory
%s Upload of Voice Profile for %{public}@ completed with error %{public}@
%s Upload of Voice Profile for %{public}@ completed successfully
%s ERR: Failed to delete existing SATUpload diretory : %{public}@
%s Upload trigger of voice profile of %{public}@ 
%s Upload not supported on %{public}@
%s Legacy upload API called on Horseman - Bailing out
%s Cannot delete existing SATUpload Diretory : %{public}@
%s Cannot create SAT Upload Directory : %{public}@
%s Cannot create %{public}@ with error %{public}@ - Skipping language
%s Cannot create directory(%{public}@)
%s Cannot copy file: %{public}@ to %{public}@
%s ERR: siriProfileId is nil - Bailing out
%s Unsupported languagecode %{public}@ in %{public}@ - Skipping
%s Skipping uploading %{public}@ legacy version (%lu) of voice profile, current version %lu
%s Skipping uploading %{public}@ voice profile for profileId %{public}@
%s Skipping audiocache file %@
%s Cannot copy file %{public}@ to %{public}@ with error %{public}@
%s ERR: Failed to copy from %{public}@ to %{public}@ with error %{public}@
%s ERR: Cannot copy file %{public}@ to %{public}@ with error %{public}@
%s Successfully copied %{public}d utterances from %{public}@ to %{public}@
%s Cannot copy voice profile from %{public}@ to %{public}@ with error %{public}@
%s Triggering upload of voice profile %{public}@
%s Upload of voice profile at %{public}@ with category %{public}@ completed successfully
%s Triggering upload of explicit voice profile %{public}@
%s Upload of explicit voice profile at %{public}@ completed successfully
%s Setting VoiceProfile Training Sync for language: %{public}@
%s ERR: %{public}s: Bailing out as language is nil!
%s VoiceProfile training sync language: %{public}@, VoiceProfile language: %{public}@
%s ERR: Unknown device - returning nil
%s NonAOP device-category - returning nil
%s ERR: Fetching cached devices resulted in error %{public}@
%s Devices with voice profile is nil!
%s Skipping %{public}@ not locally present
%s ERR: error creating profilesJsonData: %@, err: %@
%s Cached devices with VoiceProfile in iCloud: %{public}@
%s CachedVoiceProfileFetch: Done Waiting with timedOut=%ld, waitTimeMs: %fms
%s ERR: metaBlob is nil. Returning false, language: %{public}@
%s ERR: Unknown device. Returning false, language: %{public}@
%s ERR: Unknown device-category for device: %{public}@, languageCode: %{public}@
%s ERR: Failed to deserialize metaBlob with error %{public}@
%s Looking VoiceProfile for CurrDevice: %{public}@{%{public}@} in metablob %{public}@
%s currDevice=[%{public}@ : {%{public}@}] ; syncedDevice=[%{public}@ : {%{public}@}]
%s CurrDevice: [%{public}@ : {%{public}@}] DOES NOT have VoiceProfile synced in iCloud
%s Querying VoiceProfile upload state on %{public}@
%s Looking VoiceProfile for CurrDevice: %{public}@{%{public}@} in devices %{public}@
%s VoiceProfile available locally for %{public}@, not uploaded yet
%s Searching for synced-VoiceProfile for CurrDevice: %{public}@{%{public}@}
%s Will Enable VoiceTrigger after VoiceProfile sync for language: %{public}@
languageCode: %{public}@
%s Devices with VoiceProfile in iCloud for language: %{public}@:%{public}@
%s Timedout waiting for AFSettingsConnection:getDevicesWithAvailablePHSAssetsForLanguage: %{public}ld, waitedFor: %{public}d, Returning nil
%s timeToRet(AFSettingsConnection:getDevicesWithAvailablePHSAssetsForLanguage:): %{public}fms
%s voiceProfileArray is nil!
%s Profiles already migrated, check for enrollment on %{public}@ on profile %{public}@
%s ERR: Failed to delete Voice Profile %{public}@ with error %{public}@
%s Couldn't delete SAT directory at path %@ %@
%s Couldn't delete SAT cache directory at path %@ %@
%s Language Code is nil!
%s numChannels: %{public}lu, recordingDuration: %{public}f, sampleRate: %{public}f
%s Cannot copy samples since this is empty
%s Could NOT copyFrom: %{public}lu to: %{public}lu, retSampleCount: %{public}lu
%s copyBuffer: oldestSample: %{public}lu latestSample: %{public}lu, numSamplesCopied: %{public}lu
%s CSAudioCircularBuffer.reset
%s saveRecordingBufferFrom: %{public}lu to: %{public}lu toURL: %{public}@
%s csrb: %{public}@
%s Invalid request: (%{public}lu, %{public}lu): noting to write to file
%s Invalid request: reqStartSample=%{public}lu, reqEndSample=%{public}lu, oldestSampleInBuffer: %{public}lu, latestSampleInBuffer=%{public}lu
%s default to recordContext : %{public}@
%s ERR: Failed to create asset providers - Bailing out
%s parsing provider: %@ name: %@
%s ERR: got nil assets from provider: %@
%s Got asset with version: %@ from provider: %@
%s ERR: Asset not available from provider: %@
%s SpkrId:: Scorecard for {%{public}d, %{public}.2fsec %dms} - %{public}@
%s SpkrId:: %@
%s SpkrId:: Discarded speaker scores for session - %{public}@
%s SpkrId:: Final - %@
%s ::: found %{public}lu installed assets for matching query: %{public}@
%s ERR: Failed to asset for %{public}@
%s Error running query: %{public}@, error: %{public}lu
%s Failed to get assetString for assetType %{public}d
%s ::: found %{public}lu assets matching query: %{public}@
%s Error running asset-query: %{public}@, error: %{public}lu
%s Asset state : %{public}ld
%s Chosen Asset %{public}@
%s SpkrId:: ERR: appDomain passed as nil
%s SpkrId:: ERR: locale passed as nil
%s Profiles already migrated - Bailing out
%s Migration of voice profile is triggered...
%s Sat directory doesnt exist %@
%s Language %{public}@ not supported in %{public}@ - Deleting
%s Migrating voice profiles in languages - %{public}@
%s Voice profile migration for language - %{public}@
%s Skipped migrating non-siri landed profile - %{public}@, %{public}@
%s voice profile created is nil!!! - Skipping %{public}@
%s Moving contents from %{public}@ to %{public}@ failed with error %{public}@
%s Completed migrating voiceprofile for %{public}@ in language %{public}@
%s ERR: Deleted duplicated voiceprofile(%lu): %{public}@
%s Triggering voice profiles download
%s Cleanup model files with assets %{public}@
%s Synchronize voiceprofiles with Assistant...
%s ERR: Deleted stale voiceprofile(%lu): %@
%s Missing user models - Triggering voice profiles download
%s Needs retraining - Triggering voice profiles download
%s Implicit training not required for %{public}@
%s ERR: Failed in adding %{public}@ to %{public}@ with error %{public}@
%s Added Implicit SAT vector from %{public}@ to profile %{public}@
%s ERR: Failed purging profile %{public}@ with error - %{public}@
%s ERR: Failed to get top scorer in %{public}@ - Bailing out
%s ERR: Utterance scored %{public}f (%{public}@) with next top score %{public}f (%{public}@) for profileId %{public}@
%s Utterance scored %{public}f (%{public}@) with next top score %{public}f (%{public}@) and thresholds (%{public}f, %{public}f)
%s Utterance scored %{public}f for %{public}@) and thresholds (%{public}f, %{public}f)
%s Skipping retraining for language %{public}@, current %{public}@
%s Deleting VoiceProfile %{public}@
%s Err: %@
%s Needs Retraining Profile %{public}@ - existing {%{public}@} downloaded {%{public}@, %{public}d}
%s Needs Retraining Model for Profile %{public}@ - existing {%{public}@} downloaded {%{public}@, %{public}d}
%s Skipping Profile %{public}@ - existing {%{public}@} downloaded {%{public}@, %{public}d}
%s Needs Retraining %{public}@ model update for profile %{public}@ 
%s Trigger %{public}@ Voice Profile training with import Dir %{public}@
%s Error to copy TD utterance from %{public}@ to %{public}@, error: %{public}@
%s Copied TD Utterance from %{public}@ to %{public}@
%s Error to copy TD jsonFile from %{public}@ to %{public}@, error: %{public}@
%s Copied TD jsonFile from %{public}@ to %{public}@
%s Retraining %{public}@ for locale %{public}@
%s Needs retraining %{public}@ - Triggering voice profiles download
%s ERR: Failed retraining LiveOn onboarded users with error %{public}@
%s Successfully retrained LiveOn onboarded users
%s Skipping retraining for %{public}@
%s Retraining successfully finished for %{public}@ in %{public}fms
%s Adding User Voice Profile %@
%s Updating User Voice Profile to %@ from %@
%s Deleting User Voice Profile %@
%s User Voice Profile not found %@ - Bailing out
%s ERR: UserVoiceProfile Action undefined %ld - Bailing out
%s Updating profile %{public}@ with userName %{public}@
%s Retraining for locale %{public}@ with force %d
%s ERR: Retraining failed for %{public}@ with error %{public}@ in %{public}fms
%s Retraining finished for %@ with error %@ in %fms
%s ERR: ignoring filtering option as %{public}@ or %{public}@ is nil
%s ERR: ignoring filtering option as VTAssets not found on %{public}@
%s ERR: Failed training %{public}@ of %{public}@ with error %{public}@
%s Failed to create %{public}@ model with error %{public}@ for profile %{public}@
%s Utterance addition policy skipped for explicit utt: %@
%s Skipping retraining for %{public}@ - for {%{public}@, %{public}@}
%s ERR: Failed resetting for %{public}@ - for {%{public}@, %{public}@}
%s ERR: Failed in retraining %{public}@ with error %{public}@
%s ERR: Failed to delete the model at %{public}@
%s Added utterance %{public}@ to {%{public}@, %{public}@, %{public}@, %{public}@} with score %{public}@
%s Rejected utterance %{public}@ with error %{public}@
%s Failed to create biometricdevice with error %@
%s triggerTimeStamp is nil - Bailing out
%s Biometric match happened in last %f secs
%s Biometric match result: %@ happened in last %f secs
%s No biometric information available
%s BiometricMatchEvent: result = %u, timeStamp = %llu
%s BiometricMatchEvents unavailable with error %@
%s ERR: Biometric device is nil - Bailing out
%s Saving utterance and meta as %{public}@ training.
%s Failed to write utterance into %{public}@
%s Saving %{public}@ at %{public}@ as %{public}@ training.
%s numSamplesToWrite %{public}lu
%s Failed to get CSAudioFileWriter:
%s Failed to addSamples to CSAudioFileWriter: %{public}@
%s Failed to endAudio on CSAudioFileWriter: %{public}@
%s ERR: called with nil metaPath
%s ERR: called with nil uttPath
%s ERR: called with nil uttMeta
%s Cannot create json file : %{public}@
%s Skipping Model {%{public}@, %{public}@} for %{public}@
%s Skipping Model {%{public}@, %{public}@} as file doesnt exist at %{public}@
%s Device supporting barge-in ? %{public}@
%s Failed to find matching service to IOPlatformExpertDevice
%s Fetched hardware revision : %{public}@
%s Failed to find property "config-number"
%s ::: SSR logging initialized (%s)
%s Couldn't create CoreSpeech log directory at path %{public}@ %{public}@
%s unbalanced dispatch_group_enter and leave : ignore we are ignore dispatch_group_leave
-[CSAudioChunkForTV initWithXPCObject:]
T@"NSArray",&,N,V_packets
Tf,N,V_avgPower
Tf,N,V_peakPower
TQ,N,V_timeStamp
TI,N,V_numChannels
TI,N,V_audioFormat
TQ,N,V_streamHandleID
avgPower
peakPower
timeStamp
numChannels
audioFormat
streamHandleID
packets
-[CSPlainAudioFileWriter initWithURL:inputFormat:outputFormat:]
-[CSPlainAudioFileWriter addSamples:numSamples:]
json
en_US_POSIX
yyyy_MM_dd-HHmmss.SSS
productType
productVersion
buildVersion
-[CSPlainAudioFileWriter addContextKey:withContext:]
-[CSPlainAudioFileWriter addContextKey:fromMetaFile:]
hash
TQ,R
superclass
T#,R
description
T@"NSString",R,C
debugDescription
fileURL
T@"NSURL",R,N,V_fileURL
com.apple.voicetrigger
com.apple.voicetrigger.notbackedup
kCSPreferencesJarvisTriggerModeDidChangeDarwinNotification
v8@?0
VoiceTrigger Enabled
Phrase Detector Enabled
AttentiveSiri Enabled
AttentiveSiri AudioLogging Enabled
VoiceTrigger CoreSpeech Enabled
-[CSPreferences voiceTriggerInCoreSpeech]_block_invoke
Enable Two Shot Notification
com.apple.demo-settings
StoreDemoMode
File Logging Level
Library
Logs/CrashReporter/VoiceTrigger/audio/
/Logs/CrashReporter/Assistant/smartSiriVolumeContextAwareLogs/
Logs/CrashReporter/Assistant/smartSiriVolumeContextAwareLogs/
-[CSPreferences getSSVLogFilePathWithSessionIdentifier:]
/tmp
%@/SSV_%@.json
VoiceTrigger/TrialAssetData
/Logs/CrashReporter/Assistant/
SpeechLogs
-[CSPreferences assistantAudioFileLogDirectory]
VoiceTrigger
siriBC
Second Pass Audio Logging Enabled
Jarvis Audio Logging Enabled
Jarvis Trigger Mode
Enable SoS Audio Logging
Force VoiceTrigger AP Mode
-[CSPreferences forceVoiceTriggerAPMode]_block_invoke
Force VoiceTrigger AOP Mode
-[CSPreferences forceVoiceTriggerAOPMode]_block_invoke
mobile
Logs/CrashReporter/CoreSpeech/sos/
-[CSPreferences getStartOfSpeechAudioLogFilePath]
yyyyMMdd_HHmmss.SSS
%@/%@
Remote VoiceTrigger Delay
Remote VoiceTrigger Endpoint Timeout
VoiceTrigger/interstitial
Myriad File Logging Enabled
Audio Injection Enabled
Programmable Audio Injection Enabled
-[CSPreferences enableAudioInjection:withKey:]
-[CSPreferences setAudioInjectionFilePath:]
Audio Injection File Path
-[CSPreferences audioInjectionFilePath]
-[CSPreferences audioInjectionFilePath]_block_invoke
v32@?0@8Q16^B24
SpeakerId Enabled
SpeakerId Score Type
SmartSiriVolume SoftVolume Enabled
Audio Session Activation Delay
Max Number Logging Files
Max Number Grading Files
Enable SiriActivation HomePod
Enable SiriActivation watchOS
IOS Support Barge-in
-[CSPreferences iOSBargeInSupportEnabled]_block_invoke
enabled
disabled
-[CSPreferences iOSBargeInSupportEnabled]
Overwrite Remote VAD Score
Hearst First Pass Model Version
Hearst Second Pass Model Version
Hearst Fake Model Path
VoiceTrigger Companion Sync Enabled
Enable OpportuneSpeakListener Bypass
Bypass Personalized HeySiri
MultiPhraseVTEnabled
MultiChannelAudioLoggingEnabled
Enable AdBlocker Audio Logging
Enable Self Trigger Audio Logging
Running VoiceTrigger Mac
com.apple.ssr.vad.spg
-[SSRVoiceActivityDetector initWithContext:delegate:]
SearchOrMessaging
-[SSRVoiceActivityDetector processAudioData:numSamples:]
-[SSRVoiceActivityDetector clientSilenceFeaturesAvailable:]
context
T@"SSRSpeakerRecognitionContext",&,N,V_context
delegate
T@"<SSRVoiceActivityDetectorDelegate>",W,N,V_delegate
earSpg
T@"EARCaesuraSilencePosteriorGenerator",&,N,V_earSpg
hybridClassifier
T@"_EAREndpointer",&,N,V_hybridClassifier
defaultServerEpFeatures
T@"CSServerEndpointFeatures",&,N,V_defaultServerEpFeatures
segmentStartPointSampleCount
Tq,N,V_segmentStartPointSampleCount
numSamplesProcessed
TQ,N,V_numSamplesProcessed
endpointReported
TB,N,V_endpointReported
startpointReported
TB,N,V_startpointReported
spgQueue
T@"NSObject<OS_dispatch_queue>",&,N,V_spgQueue
CSAudioFileReader Queue
-[CSAudioFileReader initWithURL:]
-[CSAudioFileReader prepareRecording:]
-[CSAudioFileReader startRecording]
-[CSAudioFileReader _readAudioBufferAndFeed]
-[CSAudioFileReader stopRecording]
T@"<CSAudioFileReaderDelegate>",W,N,V_delegate
OPP-
PCM-
OPUS_
Ads-
-synced
com.apple.CoreSpeech.AudioLogging
+[CSAudioFileManager generateDeviceAudioLogging:speechId:]_block_invoke
FLLR
+[CSAudioFileManager _readDataFromFileHandle:toFileHandle:]
%@%@.wav
%@/%@%@.wav
+[CSAudioFileManager _createAudioFileWriterForAdBlockerWithLoggingDir:inputFormat:outputFormat:]
+[CSAudioFileManager _createAudioFileWriterForOpportuneSpeakListenerWithLoggingDir:inputFormat:outputFormat:]
+[CSAudioFileManager _createAudioFileWriterWithLoggingDir:withLoggingUUID:inputFormat:outputFormat:]
^%@*
%@(%@)?.wav$
+[CSAudioFileManager cleanupOrphanedGradingFiles]
+[CSAudioFileManager cleanupOrphanedGradingFiles]_block_invoke
v32@?0@"NSString"8@"NSURL"16^B24
attsiri
+[CSAudioFileManager audioFileWriterForAttentiveSiri]
%@.wav
Borealis Input
com.apple.VoiceTriggerUI.AVVCSessionQueue
-[CSVTUIAudioSessionAVVC voiceController]
-[CSVTUIAudioSessionAVVC prepareRecord]
-[CSVTUIAudioSessionAVVC startRecording]
-[CSVTUIAudioSessionAVVC stopRecording]
-[CSVTUIAudioSessionAVVC _hasInputAudioRoute]
-[CSVTUIAudioSessionAVVC _hasCorrectInputAudioRoute]
-[CSVTUIAudioSessionAVVC _hasCorrectOutputAudioRoute]
T@"<CSVTUIAudioSessionDelegate>",W,N,V_delegate
SSRSpeakerRecognizerPSR.m
Incorrect ctx for VTSpeakerRecognizer: %@
com.apple.ssr.psrq
-[SSRSpeakerRecognizerPSR initWithContext:delegate:]
extraSamplesAtStart
triggerEndSeconds
-[SSRSpeakerRecognizerPSR dealloc]
-[SSRSpeakerRecognizerPSR voiceRecognitionPSRAnalyzer:hasVoiceRecognitionInfo:]
extraAudioAtStartInMs
tdEndInMs
-[SSRSpeakerRecognizerPSR voiceRecognitionPSRAnalyzerFinishedProcessing:withVoiceRecognitionInfo:]
lastScoreCard
T@"NSDictionary",R,N
spIdCtx
T@"SSRSpeakerRecognitionContext",&,N,V_spIdCtx
sessionId
T@"NSString",&,N,V_sessionId
lastSpeakerInfo
T@"NSDictionary",&,N,V_lastSpeakerInfo
queue
T@"NSObject<OS_dispatch_queue>",&,N,V_queue
T@"<SSRSpeakerRecognizerDelegate>",W,N,V_delegate
invocationStyleStr
T@"NSString",&,N,V_invocationStyleStr
TQ,N,V_extraSamplesAtStart
vtEndInSampleCount
TQ,N,V_vtEndInSampleCount
endInSampleCount
TQ,N,V_endInSampleCount
processingEnded
TB,N,V_processingEnded
totalNumSamplesReceived
TQ,N,V_totalNumSamplesReceived
psrAnalyzer
T@"SSRSpeakerAnalyzerPSR",&,N,V_psrAnalyzer
-[CSAudioChunk subChunkFrom:numSamples:forChannel:]
-[CSAudioChunk subChunkFrom:numSamples:]
-[CSAudioChunk splitAudioChunkSuchThatNumSamplesReceivedSoFar:reachesACountOf:completionHandler:]
T@"NSData",R,N,V_data
TQ,R,N,V_numChannels
TQ,R,N,V_numSamples
TQ,R,N,V_sampleByteDepth
TQ,R,N,V_startSampleCount
TQ,R,N,V_hostTime
remoteVADAvailable
TB,R,N
T@"NSData",&,N,V_remoteVAD
xpcObject
T@"NSObject<OS_xpc_object>",R,N
numSamples
sampleByteDepth
startSampleCount
hostTime
data
remoteVAD
[requestHistoricalAudioDataWithHostTime = %@]
[requestHistoricalAudioDataSampleCount = %@]
[useOpportunisticZLL = %@]
[startRecordingHostTime = %llu]
[startRecordingSampleCount = %llu]
[alertBehavior = %llu %llu %llu]
[skipAlertBehavior = %@]
TB,N,V_requestHistoricalAudioDataWithHostTime
TB,N,V_requestHistoricalAudioDataSampleCount
TQ,N,V_startRecordingHostTime
TQ,N,V_startRecordingSampleCount
TB,N,V_useOpportunisticZLL
Tq,N,V_startAlertBehavior
Tq,N,V_stopAlertBehavior
Tq,N,V_errorAlertBehavior
TB,N,V_skipAlertBehavior
localizedDescription
T@"NSString",R,N
requestHistoricalAudioDataWithHostTime
requestHistoricalAudioDataSampleCount
startRecordingHostTime
startRecordingSampleCount
useOpportunisticZLL
startAlertBehavior
stopAlertBehavior
errorAlertBehavior
skipAlertBehavior
com.apple.speakerrecognition
recognition
-[SSRSpeakerRecognitionOrchestrator initWithContext:withDelegate:error:]
ERR: Failed to init PSR and SAT recognizers - Bailing out
reason
ERR: Failed to init VAD - Bailing out
com.apple.ssr.orchestratorq
SAT+PSR
-[SSRSpeakerRecognitionOrchestrator processAudio:numSamples:]_block_invoke
-[SSRSpeakerRecognitionOrchestrator _logSpeakerIdProcessorScoreDelayWithScoreInfo:hasFinished:]
finished
reported
-[SSRSpeakerRecognitionOrchestrator orchestratorScoresWithPSRScores:withSATScores:withSegmentStartTime:]
%@_%d
-[SSRSpeakerRecognitionOrchestrator getLatestVoiceRecognitionInfo]
-[SSRSpeakerRecognitionOrchestrator speakerRecognizer:hasSpeakerIdInfo:]_block_invoke
-[SSRSpeakerRecognitionOrchestrator speakerRecognizerFinishedProcessing:withFinalSpeakerIdInfo:]_block_invoke
-[SSRSpeakerRecognitionOrchestrator SSRVoiceActivityDetector:didDetectStartPointAt:]_block_invoke
-[SSRSpeakerRecognitionOrchestrator SSRVoiceActivityDetector:didDetectEndPointAt:]_block_invoke
T@"<SSRSpeakerRecognitionOrchestratorDelegate>",W,N,V_delegate
ssrUttLogger
T@"<CSAudioFileWriter>",&,N,V_ssrUttLogger
myriadResult
TQ,N,V_myriadResult
psrRecognizer
T@"<SSRSpeakerRecognizer>",&,N,V_psrRecognizer
satRecognizer
T@"<SSRSpeakerRecognizer>",&,N,V_satRecognizer
T@"SSRVoiceActivityDetector",&,N,V_vad
psrLastSpeakerInfo
T@"NSDictionary",&,N,V_psrLastSpeakerInfo
satLastSpeakerInfo
T@"NSDictionary",&,N,V_satLastSpeakerInfo
combinedScores
T@"NSDictionary",&,N,V_combinedScores
psrFinalSpeakerInfo
T@"NSDictionary",&,N,V_psrFinalSpeakerInfo
satFinalSpeakerInfo
T@"NSDictionary",&,N,V_satFinalSpeakerInfo
debugUtteranceAudioFilePath
T@"NSString",&,N,V_debugUtteranceAudioFilePath
debugUtteranceJsonFilePath
T@"NSString",&,N,V_debugUtteranceJsonFilePath
modelFilePath
T@"NSURL",R,N
implicitTrainingRequired
retrainerType
TQ,R,N
T@"NSURL",R,N,VmodelFilePath
TB,R,N,VimplicitTrainingRequired
TQ,R,N,VretrainerType
+[CSAudioStreamRequest defaultRequestWithContext:]
+[CSAudioStreamRequest requestForLpcmRecordSettingsWithContext:]
+[CSAudioStreamRequest requestForOpusRecordSettingsWithContext:]
+[CSAudioStreamRequest requestForSpeexRecordSettingsWithContext:]
T@"CSAudioRecordContext",&,N,V_recordContext
TB,N,V_requiresHistoricalBuffer
TB,N,V_useCustomizedRecordSettings
Tq,N,V_audioFormat
Td,N,V_sampleRate
TI,N,V_lpcmBitDepth
TB,N,V_lpcmIsFloat
numberOfChannels
TI,N,V_numberOfChannels
TI,N,V_encoderBitRate
TB,N,V_isSiri
requiresHistoricalBuffer
useCustomizedRecordSettings
sampleRate
lpcmBitDepth
lpcmIsFloat
NumberOfChannels
encoderBitRate
isSiri
recordContext
+[SSRDESRecordWriter createDESRecordWithSuperVector:withMetaInfo:]
com.apple.fides.phs
+[SSRDESRecordWriter createDESRecordWithSuperVector:withMetaInfo:]_block_invoke
Name
MetaInfo
v24@?0@"NSUUID"8@"NSError"16
v32@?0@"NSNumber"8@"NSString"16@"NSError"24
ERR: received nil dob: %@ / name: %@
+[SSRDESRecordWriter fetchMedicalDataWithCompletion:]_block_invoke
v24@?0@"_HKMedicalIDData"8@"NSError"16
profileID
T@"NSString",R,N,V_profileID
sysConfigRoot
T@"NSString",R,N,V_sysConfigRoot
psrConfigFilePath
T@"NSString",R,N,V_psrConfigFilePath
psrConfigRoot
T@"NSString",R,N,V_psrConfigRoot
satModelAvailable
TB,R,N,V_satModelAvailable
com.apple.VoiceTriggerUI.TrainingSessionQueue
com.apple.VoiceTriggerUI.TrainingManager
-[SSRVTUITrainingManager setLocaleIdentifier:]
-[SSRVTUITrainingManager createKeywordDetector]
-[SSRVTUITrainingManager prepareWithCompletion:]_block_invoke
-[SSRVTUITrainingManager cleanupWithCompletion:]
-[SSRVTUITrainingManager cleanupWithCompletion:]_block_invoke
-[SSRVTUITrainingManager trainUtterance:shouldUseASR:completion:]
-[SSRVTUITrainingManager trainUtterance:shouldUseASR:completion:]_block_invoke_2
-[SSRVTUITrainingManager trainUtterance:shouldUseASR:completion:]_block_invoke
-[SSRVTUITrainingManager cancelTrainingForID:]
-[SSRVTUITrainingManager closeSessionBeforeStartWithStatus:successfully:withCompletion:]
-[SSRVTUITrainingManager _shouldShowHeadsetDisconnectionMessage]
-[SSRVTUITrainingManager _startAudioSession]
-[SSRVTUITrainingManager setSuspendAudio:]
-[SSRVTUITrainingManager setSuspendAudio:]_block_invoke
-[SSRVTUITrainingManager CSVTUITrainingSessionStopListen]
-[SSRVTUITrainingManager CSVTUITrainingSession:hasTrainUtterance:languageCode:payload:]
voiceProfile
T@"SSRVoiceProfile",R
Tf,V_rms
T@"<SSRVTUITrainingManagerDelegate>",W,N,V_delegate
speechRecognizerAvailable
TB,R,V_speechRecognizerAvailable
audioSource
suspendAudio
-[SSRVoiceProfileRetrainerFactory init]
%@-%@
-[CSOSTransaction initWithDescription:]
-[CSOSTransaction dealloc]
route
isRemoteDevice
remoteDeviceUID
remoteDeviceProductIdentifier
%@ {route = %@, isRemoteDevice = %d, remoteDeviceUID = %@, remoteDeviceProductIdentifier = %@}
supportsSecureCoding
TB,R
T@"NSString",R,C,N,V_route
TB,R,N,V_isRemoteDevice
T@"NSUUID",R,C,N,V_remoteDeviceUID
T@"NSString",R,C,N,V_remoteDeviceProductIdentifier
-[SSRVoiceProfileMetaContext initWithVoiceProfile:]
[profileId: %@, language: %@, product: %@, version: %@, homeId: %@, name: %@]
appDomain
T@"NSString",&,N,V_appDomain
profileId
T@"NSString",&,N,V_profileId
languageCode
T@"NSString",&,N,V_languageCode
productCategory
T@"NSString",&,N,V_productCategory
version
T@"NSNumber",&,N,V_version
dateAdded
T@"NSDate",&,N,V_dateAdded
sharedSiriId
T@"NSString",&,N,V_sharedSiriId
homeId
T@"NSString",&,N,V_homeId
userName
T@"NSString",&,N,V_userName
voiceTriggerSecondPass
v20@?0@"NSError"8f16
+[SSRTriggerPhraseDetector filterVTAudioFiles:withLocale:withAsset:]
-[SSRTriggerPhraseDetector initWithLocale:asset:]
-[SSRTriggerPhraseDetector computeTriggerConfidenceForAudio:withCompletion:]
-[SSRTriggerPhraseDetector computeTriggerConfidenceForAudio:withCompletion:]_block_invoke
v44@?0{AudioBufferList=I[1{AudioBuffer=II^v}]}8B32@"NSError"36
NDAPI: missing best_score for %@
best_score
Quasar: missing best_score for %@
detectorNDAPI
T@"SSRTriggerPhraseDetectorNDAPI",&,N,V_detectorNDAPI
detectorQuasar
T@"SSRTriggerPhraseDetectorQuasar",&,N,V_detectorQuasar
recognizerScoreScaleFactor
Tf,N,V_recognizerScoreScaleFactor
com.apple.corespeech
CSRemoteControlClient
-[CSRemoteControlClient dealloc]
T@"<CSRemoteControlClientDelegate>",W,N,V_delegate
Languages
Footprint
Premium
corespeech.json
assets.json
speakerRecognition.json
adBlockerPayload.bin
hybridendpointer.json
hybridendpointer_marsh.json
CSAsset.m
ERR: Unknown assetType: %lu
/System/Library/PrivateFrameworks/CoreSpeech.framework
+[CSAsset fallBackAssetResourcePath]
defaultFallbackHearst
defaultFallbackAdBlocker
-[CSAsset initWithResourcePath:configFile:configVersion:assetProvderType:]
-[CSAsset _decodeJson:]
-[CSAsset getNumberForKey:category:default:]
-[CSAsset getStringForKey:category:default:]
configVersion:%@ resourcePath:%@ path:%@
MobileAssets
Trial
Unknown
path
T@"NSString",R,N,V_path
resourcePath
T@"NSString",R,N,V_resourcePath
dictionary
hashFromResourcePath
configVersion
T@"NSString",R,N,V_configVersion
assetProvider
TQ,R,N,V_assetProvider
totalSamplesAtEndOfCapture
spIdAudioProcessedDuration
spIdUnknownUserScore
spIdKnownUserScores
spIdUserScoresVersion
spIdScoreThresholdingType
spIdVTInvocationScoreThresholdingType
spIdNonVTInvocationScoreThresholdingType
SpIdScoreThreshold
spIdAssetVersion
spIdDirectionSigsArr
shouldRecordUserAudio
shouldRecordPayload
bestVoiceTriggerScore
segmentStartTime
segmentCounter
myriad
ssrMeta
voiceTriggerRequestUID
numEnrollmentUtt
combinationWeight
numSpeakerVectors
psrContext
spIdKnownUserPSRScores
spIdKnownUserPSRExpScores
satContext
spIdKnownUserSATScores
spIdKnownUserSATExpScores
Unknown InvocationStyle: %lu
tdti
tdtiexplicit
tdexplicit
Unknown CSSpIdType: %lu
+[CSUtils(SSR) spIdTypeForString:]
Unknown SpeakerRecognizerType: %lu
Unknown VoiceProfileRetrainerType: %lu
config.txt
config_td_spid.txt
config_sr_sat.txt
+[CSUtils(SSR) satConfigFileNameForCSSpIdType:]
config_tdti_spid.txt
+[CSUtils(SSR) psrConfigFileNameForCSSpIdType:]
config_ti_spid.txt
+[CSUtils(SSR) satConfigFileNameForCSSpIdType:forModelType:forAssetType:]
spid-imported
+[CSUtils(SSR) createDirectoryIfDoesNotExist:]
Library/Logs/CrashReporter/ssr
self ENDSWITH '.wav'
+[CSUtils(SSR) ssrAudioLogsCountWithinPrivacyLimit]
+[CSUtils(SSR) cleanupOrphanedVoiceIdGradingFiles]
+[CSUtils(SSR) cleanupOrphanedVoiceIdGradingFiles]_block_invoke
VoiceProfileCache
AudioFileOpenURL Failed : %@
EARTests
AudioFileOpenURL failed: %@
ExtAudioFileWrapAudioFileID failed: %@
Error reading audio-file: %d
EOF. Num bytes read: %lu
+[CSUtils(SSR) isSpeakerRecognitionSupportedInLocale:]
+[CSUtils(SSR) readJsonFileAtPath:]
kCSDeviceCategory_Unknown
kCSDeviceCategory_iOS_NonAop
kCSDeviceCategory_iOS_Aop
kCSDeviceCategory_macOS
kCSDeviceCategory_audioAccessory
kCSDeviceCategory_iOS_Aop_Explicit
iPad3,4
iPad3,5
iPad3,6
iPad4,1
iPad4,2
iPad4,3
iPad4,4
iPad4,5
iPad4,6
iPad4,7
iPad4,8
iPad4,9
iPad5,1
iPad5,2
iPad5,3
iPad5,4
iPad6,7
iPad6,8
iPad6,11
iPad6,12
iPhone5,1
iPhone5,2
iPhone5,3
iPhone5,4
iPhone6,1
iPhone6,2
iPhone7,1
iPhone7,2
iPod
iPad
iPhone
Accessory
+[CSUtils(SSR) deviceCategoryForDeviceProductType:]
+[CSUtils(SSR) isCurrentDeviceCompatibleWithNewerVoiceProfileAt:]
+[CSUtils(SSR) isCurrentDeviceCompatibleWithVoiceProfileAt:]
audio
pathExtension='json'
Caches/VoiceTrigger/ImplicitUtterences
+[CSUtils(SSR) getNumberOfAudioFilesInDirectory:]
v32@?0@"NSString"8Q16^B24
+[CSUtils(SSR) dumpFilesInDirectory:]
+[CSUtils(SSR) getContentsOfDirectory:]
+[CSUtils(SSR) getHomeUserIdForVoiceProfile:withCompletion:]
+[CSUtils(SSR) getHomeUserIdForVoiceProfile:withCompletion:]_block_invoke
v24@?0@"NSString"8@"NSError"16
homeUserId query for siriProfileId %@ timedout !
+[CSUtils(SSR) getVoiceProfileForSiriProfileId:forLanguageCode:]
+[CSUtils(SSR) logSpeakerRecognitionGradingMetadataAtFilepath:withScoreInfo:]
.wav
ERR: Audio path is nil - Bailing out
+[CSUtils(SSR) segmentVoiceTriggerFromAudioFile:withVTEventInfo:withStorePayloadPortion:withCompletion:]
ERR: Failed initializing loggers at %@ and %@
+[CSUtils(SSR) segmentVoiceTriggerFromAudioFile:withVTEventInfo:withStorePayloadPortion:withCompletion:]_block_invoke
ERR: Failed to read file: %@
+[CSUtils(SSR) getEnrollmentUtterancesFromDirectory:]
B24@?0@"NSURL"8@"NSDictionary"16
+[CSUtils(SSR) getExplicitEnrollmentUtterancesFromDirectory:]_block_invoke
v32@?0@"NSURL"8Q16^B24
+[CSUtils(SSR) getExplicitOnlyEnrollmentUtterancesFromDirectory:]_block_invoke
+[CSUtils(SSR) getImplicitEnrollmentUtterancesFromDirectory:]_block_invoke
q24@?0@"NSURL"8@"NSURL"16
+[CSUtils(SSR) _getUtterancesFromDirectory:]
self.absoluteString ENDSWITH '.wav'
+[CSUtils(SSR) removeItemAtPath:]
Failed to get contents of %@ with error %@
+[CSUtils(SSR) moveContentsOfSrcDirectory:toDestDirectory:]
Failed to move %@ to %@ with error %@
+[CSUtils(LanguageCode) getSiriLanguageWithFallback:]
triggerStartSampleCount
triggerEndSampleCount
isTriggerEvent
totalSampleCount
triggerScore
isMaximized
-[CSVTUIKeywordDetector initWithAsset:]
Serial CSPolicy queue
CSRemoteRecordClient Queue
T@"<CSRemoteRecordClientDelegate>",W,N,V_delegate
VoiceTriggerEventInfo
com.apple.ssr
locale
asset
profileUpdateFailedExplicitUttScore
profileUpdateDiscardImplicitUttScore
profileUpdateExplicitUttScore
profileUpdateImplicitUttScore
profileUpdateNumPrunedUttsPHS
profileUpdateNumDiscardedUttsPHS
profileUpdateNumRetainedUttsPHS
profileUpdateScoreMSE
profileUpdateFailCode
speakerRecognitionWaitTimeMs
speakerRecognitionProcessingStatus
retrainingWaitTimeMs
retrainingStatusCode
TdPsrSATRetrainingTimedOut
TdPsrExtraAudioSamplesProcessed
TdPsrFailedDuringSATDetection
xx_XX
unknown
@"NSDictionary"8@?0
%@.%d
-[SSRLoggingAggregator pushAnalytics]
voiceProfilePruningFailureReasonCode
TQ,N,V_voiceProfilePruningFailureReasonCode
voiceProfileUpdateScoreMSE
Tf,N,V_voiceProfileUpdateScoreMSE
voiceProfileDiscardedUtteranceCount
TQ,N,V_voiceProfileDiscardedUtteranceCount
voiceProfilePrunedUtteranceCount
TQ,N,V_voiceProfilePrunedUtteranceCount
voiceProfileRetainedUtteranceCount
TQ,N,V_voiceProfileRetainedUtteranceCount
voiceProfileRetrainingFailureReasonCode
TQ,N,V_voiceProfileRetrainingFailureReasonCode
retrainingWaitTime
Td,N,V_retrainingWaitTime
TQ,N,V_speakerRecognitionProcessingStatus
speakerRecognitionWaitTime
Td,N,V_speakerRecognitionWaitTime
speakerRecognitionPSRProcessingStatus
TQ,N,V_speakerRecognitionPSRProcessingStatus
speakerRecognitionSATProcessingStatus
TQ,N,V_speakerRecognitionSATProcessingStatus
voic
carplay
hearst
raisetospeak
auto
+[CSUtils(Time) hostTimeFromSampleCount:anchorHostTime:anchorSampleCount:]
+[CSUtils(Time) sampleCountFromHostTime:anchorHostTime:anchorSampleCount:]
+[CSUtils(Time) macHostTimeFromBridgeHostTime:]
com.apple.corespeech.corespeechd.voiceid.xpc
v16@?0@"NSObject<OS_xpc_object>"8
-[CSVoiceIdXPCClient _handleListenerEvent:]
-[CSVoiceIdXPCClient _handleListenerError:]
v20@?0B8@"NSError"12
utterencePath
audioDevice
audioRecord
voiceTriggerEventInfo
otherCtxt
type
body
result
resultErrorDomain
resultErrorCode
xpcConnection
T@"NSObject<OS_xpc_object>",&,N,V_xpcConnection
B24@?0@"SSRVoiceProfile"8@"NSDictionary"16
-[SSRVoiceProfileStoreCleaner filterDuplicatedSiriProfilesFrom:]
Primary
-[SSRVoiceProfileStoreCleaner filterInvalidSiriProfilesFrom:]
kAFAssistantErrorDomain
-[SSRVoiceProfileStoreCleaner filterInvalidSiriProfilesFrom:]_block_invoke
-[SSRVoiceProfileStoreCleaner cleanupProfileStore]
ERR: Failed to get appdomain for profile %@
-[SSRVoiceProfileStoreCleaner cleanupProfileStore]_block_invoke
v32@?0@"SSRVoiceProfile"8Q16^B24
-[SSRVoiceProfileStoreCleaner _cleanupAppDomain:]
-[SSRVoiceProfileStoreCleaner _cleanuplanguageCodePath:forAppDomain:]
-[SSRVoiceProfileStoreCleaner _cleanupImplicitUtteranceCacheForProfile:]
-[SSRVoiceProfileStoreCleaner _cleanupOrphanedMetafilesForProfile:payloadUtteranceLifeTimeInDays:]
-[SSRVoiceProfileStoreCleaner _cleanupContentsOfSatFolder:]
-[SSRVoiceProfileStoreCleaner _cleanupInvalidAudioFiles:]
Failed reading contents of audioDir: %@, err: %@
-[SSRVoiceProfileStoreCleaner _cleanupOrphanedMetafilesAtURL:]
enrollment_completed
-[SSRVoiceProfileStoreCleaner _cleanupPayloadUtterancesFromProfile:forModelType:exceedingLifeTimeInDays:]
obsoleteCutOffDate is nil - Bailing out
-[SSRVoiceProfileStoreCleaner _cleanupPayloadUtterancesFromProfile:forModelType:exceedingLifeTimeInDays:]_block_invoke
Error reading contents of modelDir: %@, err: %@
-[SSRVoiceProfileStoreCleaner _cleanupModelFilesAtDir:forAssetArray:]
UserVoiceProfileDateTrained
UserVoiceProfileLocale
UserVoiceProfileAppDomain
UserVoiceProfilePath
UserVoiceProfileID
UserSharedSiriID
UserVoiceProfileUserName
VoiceProfileIdentifier
VoiceProfileCompatabiltyVersion
VoiceProfileProductType
VoiceProfileSWVersion
VoiceProfileCategoryType
UserVoiceProfileOnboardType
UserVoiceProfileExpSatVecCount
VoiceProfilePruningCookie
-[SSRVoiceProfile initNewVoiceProfileWithLocale:withAppDomain:]
Creating SSRVoiceProfile with no profileId vpDict: %@
-[SSRVoiceProfile deleteAudioFilesForPrivacy]
-[SSRVoiceProfile importVoiceProfileAtPath:]
utterances passed is nil!
-[SSRVoiceProfile addUtterances:spIdType:]
Failed to copy utterances with error %@
-[SSRVoiceProfile getImplicitEnrollmentUtterancesPriorTo:forType:]
-[SSRVoiceProfile getImplicitEnrollmentUtterancesPriorTo:forType:]_block_invoke
audiocache
td-sr-model
model
-[SSRVoiceProfile deleteModelForSpidType:recognizerType:]
enrollment_migrated
-[SSRVoiceProfile _isSATMarkedWithMarker:]
-[SSRVoiceProfile _markSATEnrollmentWithMarker:]
-[SSRVoiceProfile _updateVoiceProfileVersionFile]
-[SSRVoiceProfile updatePruningCookie:]
profileBasePath
T@"NSString",&,N,V_profileBasePath
voiceProfileBasePath
voiceProfileImplicitCacheDirPath
voiceProfileIdentity
voiceProfileVersion
pruningCookie
numberOfExplicitSatVectors
Tq,N,V_numberOfExplicitSatVectors
T@"NSString",R,N,V_locale
T@"NSString",R,N,V_appDomain
T@"NSDate",R,N,V_dateAdded
siriProfileId
T@"NSString",R,N,V_siriProfileId
Dictation
-[SSRTriggerPhraseDetectorQuasar initWithLocale:configPath:resourcePath:]
-[SSRTriggerPhraseDetectorQuasar reset]
Second Pass
-[SSRTriggerPhraseDetectorQuasar analyzeWavData:numSamples:]
-[SSRTriggerPhraseDetectorQuasar endAudio]
-[CSAudioStartStreamOption(AVVC) avvcStartRecordSettingsWithAudioStreamHandleId:]
VoiceId
satinitfailed
satmodelfilefailed
satvectorfailed
tdsrfailed
tdsrtimeout
retrainsatfailed
explicituttrejected
toolessaudiofiles
unrecognizedmetadata
delayedscores
missinghomeidforclouduser
voiceidstaleprofiledetected
Audio
didStartWatchDogFire
didStopWatchDogFire
streamDeallocDuringStreaming
resourceNotAvailable
recordStoppedBySessionInterruption
InsufficientPriority
secondPassCompleteWatchDogFire
Endpointer
endpointerModelVersionIsNil
{wordCount: %ld, trailingSilDuration: %ld, eosLikelihood: %f, pauseCounts: (%@), silencePosterior: %f, taskName: %@, processedAudioDurationInMilliseconds: %ld}
WordCount
TrailingSilDuration
EOSLikelihood
PauseCounts
SilencePosterior
ProcessedAudioDurationInMilliseconds
wordCount
Tq,N,V_wordCount
trailingSilenceDuration
Tq,N,V_trailingSilenceDuration
eosLikelihood
Td,N,V_eosLikelihood
pauseCounts
T@"NSArray",C,N,V_pauseCounts
silencePosterior
Td,N,V_silencePosterior
processedAudioDurationInMilliseconds
Tq,N,V_processedAudioDurationInMilliseconds
taskName
T@"NSString",C,N,V_taskName
CSSampleCountHostTimeConverter
anchorSampleCount
TQ,N,V_anchorSampleCount
anchorHostTime
TQ,N,V_anchorHostTime
pruning
-[SSRVoiceProfilePruner pruneVoiceProfile:forSpIdType:withAsset:]
v32@?0f8f12f16f20@"NSError"24
-[SSRVoiceProfilePruner _getScoresForAudio:withController:withDetector:forProfile:withCompletion:]
Failed to get scoreCard - Bailing out
v16@?0@"NSError"8
Pruner: Timeout (%fms) waiting for retraining - Bailing out
-[SSRVoiceProfilePruner _retrainVoiceProfile:withAsset:]
-[SSRVoiceProfilePruner _deleteUtterances:]
SSRVoiceRetrainingVoiceProfile
SSRVoiceRetrainingCompareVoiceProfiles
SSRVoiceRetrainingAsset
SSRVoiceRetrainingSpIdType
SSRVoiceRetrainingFilterToVoiceTriggerUtterances
SSRVoiceRetrainingForce
SSRVoiceRetrainingPayloadProfile
retraining
ERR: VoiceProfile is invalid - Bailing out
-[SSRVoiceProfileRetrainingContext initWithVoiceRetrainingContext:error:]
ERR: Last known assets are nil - Bailing out
ERR: _modelsContext is nil - Bailing out
[SessionId: %@, Asset: %@, ProfileID: %@]
compareVoiceProfileArray
T@"NSArray",&,N,V_compareVoiceProfileArray
T@"SSRVoiceProfile",&,N,V_voiceProfile
spIdType
TQ,R,N,V_spIdType
resourceFilePath
T@"NSURL",R,N,V_resourceFilePath
filterToVoiceTriggerUtterances
TB,R,N,V_filterToVoiceTriggerUtterances
forceRetrain
TB,R,N,V_forceRetrain
maxAllowedSpeakerVectors
TQ,R,N,V_maxAllowedSpeakerVectors
modelsContext
T@"NSDictionary",R,N,V_modelsContext
T@"CSAsset",&,N,V_asset
logAggregator
T@"SSRLoggingAggregator",&,N,V_logAggregator
T@"NSString",R,N,V_sessionId
configFilePath
T@"NSURL",R,N,V_configFilePath
voiceProfileModelFilePath
T@"NSURL",R,N,V_voiceProfileModelFilePath
compareModelFilePaths
T@"NSDictionary",R,N,V_compareModelFilePaths
voiceTriggerInfo
T@"NSDictionary",C,N,V_voiceTriggerInfo
rtsTriggerInfo
T@"NSDictionary",C,N,V_rtsTriggerInfo
triggerNotifiedMachTime
TQ,N,V_triggerNotifiedMachTime
meta_version.json
enrollment_version.json
meta_version
trainingType
explicit
implicit
handheld
near-field
far-field
utteranceWav
triggerSource
audioInputSource
otherSourceProfileMatch
containsPayload
grainedDate
+[SSRVoiceProfileMetadataManager saveUtteranceMetadataForUtterance:enrollmentType:isHandheldEnrollment:triggerSource:audioInput:otherBiometricResult:containsPayload:]
+[SSRVoiceProfileMetadataManager _getBaseMetaDictionaryForUtterancePath:]
+[SSRVoiceProfileMetadataManager _writeMetaDict:forUtterancePath:]
.json
+[SSRVoiceProfileMetadataManager isUtteranceImplicitlyTrained:]
+[SSRVoiceProfileMetadataManager getUtteranceEnrollmentType:]
+[SSRVoiceProfileMetadataManager recordedTimeStampOfFile:]
yyyyMMdd
nohash
((?:[a-z]|[0-9])*)\.asset
+[CSUtils(ResourcePathHash) assetHashInResourcePath:]
+[CSVTUIRegularExpressionMatcher matchWithString:TrailingStr:LeadingStr:Pattern:]
-[CSVTUITrainingSessionWithPayload _firedVoiceTriggerTimeout]
-[CSVTUITrainingSessionWithPayload _firedEndPointTimeout]
v16@?0@"NSDictionary"8
-[CSVTUITrainingSessionWithPayload handleAudioInput:]_block_invoke
-[CSVTUITrainingSessionWithPayload audioSessionDidStartRecording:error:]
-[CSVTUITrainingSessionWithPayload audioSessionDidStopRecording:]
-[CSVTUITrainingSessionWithPayload didDetectBeginOfSpeech]
-[CSVTUITrainingSessionWithPayload didDetectEndOfSpeech:]
-[CSVTUITrainingSessionWithPayload closeSessionWithStatus:successfully:]
-[CSVTUITrainingSessionWithPayload speechRecognitionTask:didHypothesizeTranscription:]_block_invoke
-[CSVTUITrainingSessionWithPayload speechRecognitionTask:didFinishRecognition:]_block_invoke
-[CSVTUITrainingSessionWithPayload speechRecognitionTask:didFinishSuccessfully:]_block_invoke
-[CSVTUITrainingSessionWithPayload matchRecognitionResult:withMatchedBlock:withNonMatchedBlock:]
T@"NSDictionary",&,N,V_voiceTriggerEventInfo
-[CSVTUITrainingSession closeSessionWithStatus:successfully:]
-[CSVTUITrainingSession closeSessionWithStatus:successfully:complete:]_block_invoke
-[CSVTUITrainingSession suspendTraining]
-[CSVTUITrainingSession suspendTraining]_block_invoke
-[CSVTUITrainingSession resumeTraining]
-[CSVTUITrainingSession resumeTraining]_block_invoke
-[CSVTUITrainingSession setupPhraseSpotter]
-[CSVTUITrainingSession handleAudioInput:]_block_invoke_2
-[CSVTUITrainingSession handleAudioBufferForVTWithAudioInput:withDetectedBlock:]
-[CSVTUITrainingSession feedSpeechRecognitionTrailingSamplesWithCompletedBlock:]
-[CSVTUITrainingSession trimBeginingOfPCMBufferWithVoiceTriggerEventInfo:]
-[CSVTUITrainingSession audioSessionUnsupportedAudioRoute]
-[CSVTUITrainingSession didDetectBeginOfSpeech]
-[CSVTUITrainingSession didDetectEndOfSpeech:]
PHS explicit training utterance
[%ld] VTUISession Number:[%ld]
-[CSVTUITrainingSession startMasterTimerWithTimeout:]
-[CSVTUITrainingSession handleMasterTimeout:]
-[CSVTUITrainingSession stopMasterTimer]
-[CSVTUITrainingSession speechRecognitionTask:didHypothesizeTranscription:]
triggerFireMachTime
satTriggered
firstPassTriggerSource
deviceHandHeld
ApplicationProcessor
com.apple.voicetrigger.PHSProfileDownloadTrigger
VoiceProfileAvailabilityMetaBlobVersion
com.apple.cs.profileManager
VoiceTrigger/SAT
-[SSRVoiceProfileManager discardSiriEnrollmentForProfileId:forLanguageCode:]
-[SSRVoiceProfileManager _getVoiceProfilesForSiriProfileId:withLanguageCode:]
ERR: profile is nil - Bailing out
-[SSRVoiceProfileManager addUtterances:toProfile:withContext:withCompletion:]
ERR: Context is nil - Bailing out
ERR: Failed to copy %@ to %@, error: %@
-[SSRVoiceProfileManager addUtterances:toProfile:withContext:withCompletion:]_block_invoke
ERR: Failed in marking Enrollment as Successful for profile %@
-[SSRVoiceProfileManager notifyImplicitTrainingUtteranceAvailable:forVoiceProfileId:withRecordDeviceInfo:withRecordCtxt:withVoiceTriggerCtxt:withOtherCtxt:withCompletion:]
Failed to get asset for locale %@
%lld
ERR: Voice Profile not found for %@ - Bailing out
ERR: Voice Profile locale %@ not matching with %@ - Bailing out
v32@?0@"NSError"8@"NSURL"16@"NSURL"24
-[SSRVoiceProfileManager notifyImplicitTrainingUtteranceAvailable:forVoiceProfileId:withRecordDeviceInfo:withRecordCtxt:withVoiceTriggerCtxt:withOtherCtxt:withCompletion:]_block_invoke
-[SSRVoiceProfileManager notifyUserVoiceProfileDownloadReadyForUser:getData:completion:]_block_invoke
Primary User
Missing downloadTriggerBlock - Bailing out
Unknown device category for device type %@ - Bailing out
-[SSRVoiceProfileManager notifyUserVoiceProfileUpdateReady]_block_invoke
Enable VoiceTrigger Upon VoiceProfile Sync For Language
-[SSRVoiceProfileManager _downloadAndEnrollVoiceProfileForProfileId:withDownloadTriggerBlock:]_block_invoke_2
-[SSRVoiceProfileManager _downloadAndEnrollVoiceProfileForProfileId:withDownloadTriggerBlock:]_block_invoke
v24@?0@"NSError"8@"NSString"16
@"NSError"16@?0Q8
-[SSRVoiceProfileManager _downloadAndEnrollVoiceProfileForProfileId:withDownloadTriggerBlock:]
SAT download path is nil - Bailing out
-[SSRVoiceProfileManager _downloadVoiceProfileForProfileId:forDeviceCategory:withDownloadTriggerBlock:withCompletion:]
Download for %@ failed with %@
-[SSRVoiceProfileManager _enrollVoiceProfileForSiriProfileId:fromCacheDirectoryPath:withCategoryType:]
Skipping profile Update for %@ in %@
ERR: Migrated language %@ for %@ but failed to mark SAT enrollment
ERR: Failed to mark migrated for %@ in language %@
Failed to init retrainCtxt for profileID %@ with error %@
-[SSRVoiceProfileManager _enrollVoiceProfileForSiriProfileId:fromCacheDirectoryPath:withCategoryType:]_block_invoke
userAddition timedout for siriProfileId %@ after %fms
Failed to enroll user - %@
SourcePath (%@) or DestinationPath (%@) is nil - Bailing out
-[SSRVoiceProfileManager _enableVoiceTriggerIfLanguageMatches:]
Caches/VoiceTrigger/SATUpdate
Caches/VoiceTrigger/SATUpdateNewerZone
_%d_%d
-[SSRVoiceProfileManager _getUserVoiceProfileDownloadCacheDirectoryWithUpdatePath:]
-[SSRVoiceProfileManager notifyUserVoiceProfileUploadCompleteForSiriProfileId:withError:]_block_invoke
-[SSRVoiceProfileManager uploadUserVoiceProfileForSiriProfileId:withUploadTrigger:completion:]_block_invoke
@"NSError"24@?0@"SSRVoiceProfileMetaContext"8@"NSString"16
-[SSRVoiceProfileManager getUserVoiceProfileUploadPathWithEnrolledLanguageList:]
-[SSRVoiceProfileManager getUserVoiceProfileUploadPathWithEnrolledLanguageList:]_block_invoke
-[SSRVoiceProfileManager notifyUserVoiceProfileUploadComplete]_block_invoke
-[SSRVoiceProfileManager _getVoiceProfilePathsToBeUploadedForSiriProfileId:]
-[SSRVoiceProfileManager _copyExplicitEnrollmentFilesFromPath:toPath:withCompletion:]
Failed to copy to SATUpload Diretory : %@
v24@?0@"NSError"8Q16
-[SSRVoiceProfileManager _copyVoiceProfileAtPath:toPath:]
ERR: Number of training utterances copied from %@ to %@ is too less %ld
Cannot delete existing SATUpload Diretory : %@
-[SSRVoiceProfileManager _prepareVoiceProfileWithSiriProfileId:withUploadBlock:]
Cannot create SAT Upload Directory : %@
Failed to upload %@ with error %@ - Bailing out
-[SSRVoiceProfileManager _markVoiceProfileTrainingSyncForLanguage:]
Enable VoiceProfile Training Sync For Language
-[SSRVoiceProfileManager _isMarkedForVoiceProfileTrainingSyncForLanguage:]
-[SSRVoiceProfileManager getCachedVoiceProfileAvailabilityMetaBlob]
-[SSRVoiceProfileManager getCachedVoiceProfileAvailabilityMetaBlob]_block_invoke
v24@?0@"NSDictionary"8@"NSError"16
-[SSRVoiceProfileManager hasVoiceProfileIniCloudForLanguageCode:withBackupMetaBlob:]
ERR: Unknown product type. Returning false, language: %@
-[SSRVoiceProfileManager isVoiceProfileUploadedToiCloudForLanguageCode:withCompletionBlock:]
ERR: Unknown device-category for device: %@, languageCode: %@
-[SSRVoiceProfileManager isVoiceProfileUploadedToiCloudForLanguageCode:withCompletionBlock:]_block_invoke
-[SSRVoiceProfileManager hasVoiceProfileIniCloudForLanguageCode:]
-[SSRVoiceProfileManager enableVoiceTriggerUponVoiceProfileSyncForLanguage:]
-[SSRVoiceProfileManager devicesWithVoiceProfileIniCloudForLanguage:]
-[SSRVoiceProfileManager devicesWithVoiceProfileIniCloudForLanguage:]_block_invoke
v16@?0@"NSArray"8
Caches/VoiceTrigger/SATLegacyUpload
-[SSRVoiceProfileManager provisionedVoiceProfilesForAppDomain:withLocale:]
q24@?0@"SSRVoiceProfile"8@"SSRVoiceProfile"16
-[SSRVoiceProfileManager provisionedVoiceProfilesForLocale:]
ERR: Voice Profile sent as nil - Bailing out
-[SSRVoiceProfileManager triggerRetrainingVoiceProfile:withContext:withCompletion:]
ERR: Voice Profile not found for Id %@ - Bailing out
-[SSRVoiceProfileManager markSATEnrollmentSuccessForVoiceProfile:]
-[SSRVoiceProfileManager markSATEnrollmentSuccessForVoiceProfile:]_block_invoke
-[SSRVoiceProfileManager isSATEnrolledForSiriProfileId:forLanguageCode:]
ERR: Voice Profile passed is nil - Bailing out
-[SSRVoiceProfileManager deleteUserVoiceProfile:]
-[SSRVoiceProfileManager deleteAllVoiceProfilesForAppDomain:]
Library/Caches/VoiceTrigger
Caches/VoiceTrigger/SATUpload
td/audio
-[SSRVoiceProfileManager _isLegacyEnrollmentMarkedWith:forLanguageCode:]
currentDeviceCategory
TQ,N,V_currentDeviceCategory
xpcClient
T@"CSVoiceIdXPCClient",&,N,V_xpcClient
SSRVoiceProfileStore
Class getSSRVoiceProfileStoreClass(void)_block_invoke
SSRVoiceProfileManager.m
Unable to find class %s
void *CoreSpeechLibrary(void)
/System/Library/PrivateFrameworks/CoreSpeech.framework/CoreSpeech
-[CSAudioCircularBuffer initWithNumChannels:recordingDuration:samplingRate:]
-[CSAudioCircularBuffer copySamplesFromHostTime:]
-[CSAudioCircularBuffer copySamplesFrom:to:]
-[CSAudioCircularBuffer copybufferFrom:to:]
-[CSAudioCircularBuffer copyBufferWithNumSamplesCopiedIn:]
-[CSAudioCircularBuffer reset]
-[CSAudioCircularBuffer saveRecordingBufferFrom:to:toURL:]
bufferLength
TQ,N,V_bufferLength
copySamples
  mNumChannels: 
  mRecordingDurationInSecs: 
  mSampleRate: 
  mBytesPerSample: 
  mBufferLengthInSamples: 
  mNextWritePos: 
  mSamplesCount: 
  mMemoryPool(
): [
    chan-
: sz=
: mem-sz: 
CSInitialContinousZeros
CSMaxContinousZeros
CSMidSegmentContinousZeros
start
allocator<T>::allocate(size_t n) 'n' exceeds maximum supported size
-[CSNNVADEndpointAnalyzer init]
-[CSNNVADEndpointAnalyzer preheat]
-[CSNNVADEndpointAnalyzer reset]
-[CSNNVADEndpointAnalyzer processAudioSamplesAsynchronously:]
-[CSNNVADEndpointAnalyzer recordingStoppedForReason:]
-[CSNNVADEndpointAnalyzer stopEndpointer]
-[CSNNVADEndpointAnalyzer resetForNewRequestWithSampleRate:recordContext:recordSettings:]
endpointStyle
Tq,N
delay
Td,N
startWaitTime
automaticEndpointingSuspensionEndTime
minimumDurationForEndpointer
lastEndOfVoiceActivityTime
Td,R,N
lastStartOfVoiceActivityTime
bypassSamples
endpointMode
interspeechWaitTime
endWaitTime
saveSamplesSeenInReset
TB,N
T@"<CSEndpointAnalyzerDelegate>",W,N
implDelegate
T@"<CSEndpointAnalyzerImplDelegate>",W,N
canProcessCurrentRequest
activeChannel
TQ,N
endpointerModelVersion
elapsedTimeWithNoSpeech
T@"<CSEndpointAnalyzerDelegate>",W,N,Vdelegate
T@"<CSEndpointAnalyzerImplDelegate>",W,N,VimplDelegate
TB,R,N,VcanProcessCurrentRequest
TQ,N,VactiveChannel
Tq,N,VendpointStyle
Td,N,Vdelay
Td,N,VstartWaitTime
Td,N,VautomaticEndpointingSuspensionEndTime
Td,N,VminimumDurationForEndpointer
Td,R,N,VlastEndOfVoiceActivityTime
Td,R,N,VlastStartOfVoiceActivityTime
+[CSAudioRecordContext defaultContext]
CSAudioRecordTypeUnspecified
CSAudioRecordTypeHomePress
CSAudioRecordTypeWiredHeadsetButtonPress
CSAudioRecordTypeBluetoothHeadSetButtonPress
CSAudioRecordTypeUIButtonPress
CSAudioRecordTypeServerInvoke
CSAudioRecordTypeVoiceTrigger
CSAudioRecordTypeStark
CSAudioRecordTypeTVRemote
CSAudioRecordTypeRaiseToSpeak
CSAudioRecordTypeHearstDoubleTap
CSAudioRecordTypeHearstVoice
CSAudioRecordTypeJarvis
CSAudioRecordTypePost
CSAudioRecordTypeDictation
CSAudioRecordTypeVoiceTriggerTraining
CSAudioRecordTypeOpportuneSpeaker
CSAudioRecordTypeRemoraVoice
CSAudioRecordTypeUnknown
recordType[%@] deviceId[%@] alwaysUseBuiltInMic[%d]
Tq,N,V_type
T@"NSString",&,N,V_deviceId
TB,N,V_alwaysUseRemoteBuiltInMic
alwaysUseRemoteBuiltInMic
deviceId
Serial SSRAssetManager queue
en-US
-[SSRAssetManager init]
-[SSRAssetManager allInstalledAssetsOfType:forLanguage:]_block_invoke_2
-[SSRAssetManager allInstalledAssetsOfType:forLanguage:]_block_invoke
v32@?0@"<SSRAssetProviding>"8Q16^B24
-[SSRAssetManager _latestVersionedAssetOfType:fromProviders:forLocale:]_block_invoke
assetProviders
T@"NSArray",&,N,V_assetProviders
currentLanguageCode
T@"NSString",&,N,V_currentLanguageCode
T@"<SSRAssetManagerDelegate>",W,N,V_delegate
-[SSRSpeakerRecognitionController voiceRecognitionOrchestrator:hasVoiceRecognitionInfo:]
ERR: Scorecard not available in score dictionary - %@
-[SSRSpeakerRecognitionController voiceRecognitionOrchestratorFinishedProcessing:withFinalVoiceRecognitionInfo:]
T@"<SSRSpeakerRecognitionControllerDelegate>",W,N,V_delegate
orchestrator
T@"SSRSpeakerRecognitionOrchestrator",&,N,V_orchestrator
T@"NSDictionary",&,N,V_lastScoreCard
-[SSRMobileAssetProvider allInstalledAssetsOfType:forLanguage:]
q24@?0@"MAAsset"8@"MAAsset"16
-[SSRMobileAssetProvider allInstalledAssetsOfType:forLanguage:]_block_invoke_2
v32@?0@"MAAsset"8Q16^B24
com.apple.MobileAsset.VoiceTriggerAssets
com.apple.MobileAsset.VoiceTriggerAssetsIPad
com.apple.MobileAsset.VoiceTriggerAssetsWatch
com.apple.MobileAsset.VoiceTriggerAssetsMarsh
com.apple.MobileAsset.VoiceTriggerAssetsMac
com.apple.MobileAsset.SpeakerRecognitionAssets
com.apple.MobileAsset.SpeechEndpointAssets
-[SSRMobileAssetProvider _buildAssetQueryForAssetType:]
-[SSRMobileAssetProvider _installedMobileAssetOfType:forLanguage:]
-[SSRMobileAssetProvider _findLatestInstalledAsset:]
com.apple.corespeech.voiceprofilestore
-[SSRVoiceProfileStore userVoiceProfilesForAppDomain:]
-[SSRVoiceProfileStore userVoiceProfilesForLocale:]
-[SSRVoiceProfileStore migrateVoiceProfilesIfNeededWithCompletionBlock:]_block_invoke
Filtered languages is nil - %@
spid
trained_users.json
Could not read existing %@ file: err: %@
-[SSRVoiceProfileStore cleanupVoiceProfileStore:]_block_invoke_2
-[SSRVoiceProfileStore cleanupVoiceProfileStore:]_block_invoke
-[SSRVoiceProfileStore cleanupVoiceProfileModelFilesForLocale:]_block_invoke
-[SSRVoiceProfileStore _synchronizeSiriVoiceProfilesWithAssistant]_block_invoke
com.apple.siri.corespeech.voiceprofilelist.change
Utterance %@ in profile %@ not satisfied the implicit VT policy
-[SSRVoiceProfileStore addImplicitUtterance:toVoiceProfile:withAsset:withTriggerSource:withAudioInput:withCompletion:]_block_invoke
Rejecting Implicit utterance %@ for profile %@
Utterance %@ in profile %@ not satisfied the implicit policy
@"NSError"24@?0@"NSURL"8@"NSDictionary"16
Utterance %@ rejected for profile %@
v32@?0@"NSError"8@"NSDictionary"16@"NSDictionary"24
B16@?0@"NSDictionary"8
-[SSRVoiceProfileStore _logVoiceProfileConfusionForAsset:withCleanup:]_block_invoke
v32@?0@"<SSRVoiceProfileRetrainer>"8Q16^B24
-[SSRVoiceProfileStore evaluateImplicitAdditionPolicyWithScores:forProfile:withImplicitThreshold:withDeltaThreshold:]
v32@?0@"NSString"8@"NSNumber"16^B24
Profile is nil!
-[SSRVoiceProfileStore addUserVoiceProfile:withContext:withCompletion:]_block_invoke
-[SSRVoiceProfileStore _deleteUserVoiceProfile:]
Deleting profile data at %@ failed with error %@
Profile path is nil!
-[SSRVoiceProfileStore checkIfVoiceProfile:needsUpdatedWith:withCategory:]
-[SSRVoiceProfileStore _checkIfRetrainingRequiredForProfile:]_block_invoke
Failed to init retrainers for profileID %@ with ctxt %@
B16@?0Q8
-[SSRVoiceProfileStore _prepareVoiceProfileAtPath:forImportToProfile:]
-[SSRVoiceProfileStore _prepareVoiceProfileAtPath:forImportToProfile:]_block_invoke
-[SSRVoiceProfileStore retrainVoiceProfile:withContext:withCompletion:]_block_invoke
-[SSRVoiceProfileStore _updateTrainedUsersWithAction:UserVoiceProfile:]
Voice Profile not found for profileId: %@ - Bailing out
-[SSRVoiceProfileStore updateVoiceProfile:withUserName:]
-[SSRVoiceProfileStore _retrainLiveOnOnboardedProfilesForLanguage:withForceRetrain:withCompletion:]
VoiceProfile is nil - Bailing out
-[SSRVoiceProfileStore _retrainVoiceProfile:withContext:]
context is nil - Bailing out
Invalid spIdType %d - Bailing out
Too less (%d) audio files in %@ 
-[SSRVoiceProfileStore _retrainVoiceProfile:withContext:withUtterances:]
Retraining failed for profile %@
-[SSRVoiceProfileStore _retrainVoiceProfile:withContext:withUtterances:]_block_invoke
-[SSRVoiceProfileStore _isImplicitUtterance:]
Failed to copy %@ to %@ with error %@
-[SSRVoiceProfileStore copyAudioFiles:toProfile:forModelType:]
voiceProfileArray
T@"NSMutableArray",&,V_voiceProfileArray
storePrefs
T@"SSRVoiceProfileStorePrefs",&,N,V_storePrefs
-[CSSelectiveChannelAudioFileWriter initWithURL:inputFormat:outputFormat:channelBitset:]
v16@?0Q8
-[CSSelectiveChannelAudioFileWriter addSamples:numSamples:]
TI,R,N,V_numberOfChannels
i32@?0^I8^{AudioBufferList=I[1{AudioBuffer=II^v}]}16^^{AudioStreamPacketDescription}24
T@"<CSAudioDecoderDelegate>",W,V_delegate
-[SSRBiometricMatch init]
-[SSRBiometricMatch getLastBiometricMatchForVoiceTriggerTimeStamp:]
MATCH
MIS-MATCH
-[SSRBiometricMatch _getLastBiometricMatchEvent:atTime:]
biometricDevice
T@"BKDevice",&,N,V_biometricDevice
BKDevice
Class getBKDeviceClass(void)_block_invoke
SSRBiometricMatch.m
void *BiometricKitLibrary(void)
/System/Library/PrivateFrameworks/BiometricKit.framework/BiometricKit
BKDeviceManager
Class getBKDeviceManagerClass(void)_block_invoke
+[SSREnrollmentDataManager saveRawUtteranceAndMetadata:to:isExplicitEnrollment:]
+[SSREnrollmentDataManager saveUtteranceAndMetadata:atDirectory:isExplicitEnrollment:]
+[SSREnrollmentDataManager saveUtterance:utteranceAudioPath:numSamplesToWrite:isExplicitEnrollment:]
+[SSREnrollmentDataManager saveMetadata:isExplicitEnrollment:]
+[SSREnrollmentDataManager _getBaseMetaDictionaryForUtterancePath:]
+[SSREnrollmentDataManager writeMetaDict:atMetaPath:]
Known User Voice Profiles
Voice Profile Store Version
SSRSpeakerRecognitionStyle
SSRSpeakerRecognitionAsset
SSRSpeakerRecognitionVADAssetPath
SSRSpeakerRecognitionLocale
SSRSpeakerRecognitionVTEventInfo
SSRSpeakerRecognitionProfileArray
SSRSpeakerRecognitionUsePayloadProfile
SSRSpeakerRecognitionMaxAudioSecs
com.apple.siri
com.apple.siridebug
ERR: SpeakerRecognition not enabled - Bailing out
-[SSRSpeakerRecognitionContext initWithVoiceRecognitionContext:error:]
ERR: Invalid Speaker Recognition style - Bailing out
ERR: Asset not picked - Bailing out
ERR: Endpointer Asset not picked - Bailing out
v24@?0@"NSDictionary"8@"NSDictionary"16
ERR: ModelsContext is nil for locale %@ - Bailing out
%@_%@_%@
[SessionId: %@, RecognitionStyle:(%lu)%@, Asset: %@, vtEventInfo: %@]
-[SSRSpeakerRecognitionContext composeModelContextsForProfiles:forSpIdType:forAsset:completion:]
-[SSRSpeakerRecognitionContext pickAssetForProfiles:forSpIdType:]
T@"NSArray",&,N,V_voiceProfileArray
TQ,N,V_spIdType
T@"NSString",&,N,V_locale
TQ,R,N,V_activeChannel
scoreType
TQ,R,N,V_scoreType
recognitionStyle
TQ,R,N,V_recognitionStyle
Tf,R,N,V_combinationWeight
vtEventInfo
T@"NSDictionary",R,N,V_vtEventInfo
vadResourcePath
T@"NSURL",R,N,V_vadResourcePath
expModelsContext
T@"NSDictionary",R,N,V_expModelsContext
numEnrollmentUtterances
maxAllowedAudioSamples
TQ,R,N,V_maxAllowedAudioSamples
debugUtteranceAudioFile
T@"NSString",R,N,V_debugUtteranceAudioFile
debugUtteranceMetaFile
T@"NSString",R,N,V_debugUtteranceMetaFile
voiceProfilesModelFilePaths
T@"NSDictionary",R,N,V_voiceProfilesModelFilePaths
regex.json
Cannot parse to JSON
Cannot find the file
trailing_garbage
leading_garbage
regex
com.apple.da
liveOnHomePod
DeviceClassNumber
U+73bmG4kBGj6kpreQXUTQ
InternalBuild
CSSafeSetOutErrorWithNSError
PTQ+ABwag03BwO/CKvIK/A
+[CSUtils isIOSDeviceSupportingBargeIn]_block_invoke
BuildVersion
IOPlatformExpertDevice
+[CSUtils deviceHwRevision]
config-number
speakerRecognition
satThreshold
implicitProfileThreshold
implicitProfileDeltaThreshold
implicitVTThreshold
pruningExplicitSATThreshold
pruningExplicitPSRThreshold
pruningSATThreshold
pruningPSRThreshold
numPruningRetentionUtt
maxEnrollmentUtterances
configFileRecognizer
configFileNDAPI
voiceTriggerSecondPassAOP
implicit_training_enabled
multiUserHighScoreThreshold
multiUserLowScoreThreshold
multiUserConfidentScoreThreshold
multiUserDeltaScoreThreshold
recognizer.json
containsSpeakerRecognitionCategory
satScoreThreshold
Tf,R,N
Tq,R,N
psrCombinationWeight
satImplicitProfileThreshold
satImplicitProfileDeltaThreshold
satVTImplicitThreshold
satImplicitTrainingEnabled
pruningExplicitUttThresholdSAT
pruningExplicitUttThresholdPSR
pruningThresholdSAT
pruningThresholdPSR
pruningNumRetentionUtterance
maxAllowedEnrollmentUtterances
voiceProfilePruningCookie
keywordDetectorNDAPIConfigFilePath
keywordDetectorQuasarConfigFilePath
Framework
Logs/CrashReporter/CoreSpeech/
Logs/CrashReporter/CoreSpeech/audio/
%@/%@%@%@
::: Initializing VoiceRecognition logging...
yyyyMMdd-HHmmss
CSLogInitIfNeeded_block_invoke
gitrelno_unavailable
_CSGetOrCreateAudioLogDirectory
-[CSDispatchGroup leave]
@NSt3__118basic_stringstreamIcNS_11char_traitsIcEENS_9allocatorIcEEEE
NSt3__115basic_stringbufIcNS_11char_traitsIcEENS_9allocatorIcEEEE
NSt3__119basic_ostringstreamIcNS_11char_traitsIcEENS_9allocatorIcEEEE
pbhw
pbtb
pbiu
otua
ciov
bhev
eltb
siar
tdtb
cvdh
cvpc
tcid
tsop
rtsh
tvps
cvdh
softlink:r:path:/System/Library/PrivateFrameworks/CoreAnalytics.framework/CoreAnalytics
softlink:r:path:/System/Library/PrivateFrameworks/CoreSpeech.framework/CoreSpeech
softlink:r:path:/System/Library/PrivateFrameworks/BiometricKit.framework/BiometricKit
CSAudioChunkForTV
CSPlainAudioFileWriter
CSAudioFileWriter
NSObject
CSPreferences
AVVC
SSRVoiceActivityDetector
EARCaesuraSilencePosteriorGeneratorDelegate
CSAudioFileReader
CSAudioFileManager
CSVTUIAudioSessionAVVC
CSVTUIAudioSession
AVVoiceControllerRecordDelegate
AVVoiceControllerPlaybackDelegate
SSRSpeakerAnalyzerPSR
SSRSpeakerRecognizerPSR
SSRSpeakerAnalyzerPSRDelegate
SSRSpeakerRecognizer
CSAudioChunk
CSAudioStartStreamOption
CSAudioPowerMeter
SSRTriggerPhraseDetectorNDAPI
SSRSpeakerRecognitionOrchestrator
SSRSpeakerRecognizerDelegate
SSRVoiceActivityDetectorDelegate
SSRVoiceProfileRetrainerSAT
SSRVoiceProfileRetrainer
CSAudioStreamRequest
SSRDESRecordWriter
AudioStreamBasicDescription
SSRSpeakerRecognitionScorer
SSRVTUITrainingManager
CSVTUITrainingSessionDelegate
CSVTUIAudioSessionDelegate
CSEndpointAnalyzerDelegate
SSRVoiceProfileRetrainerFactory
CSOSTransaction
CSAudioRecordDeviceInfo
NSCopying
NSSecureCoding
NSCoding
SSRVoiceProfileMetaContext
SSRTriggerPhraseDetector
CSRemoteControlClient
CSAsset
LanguageCode
SSRTrialAssetProvider
SSRAssetProviding
CSVTUIKeywordDetector
CSPolicy
CSEventMonitorDelegate
CSRemoteRecordClient
SSRVoiceProfileComposer
SSRVoiceProfileRetrainerPSR
SSRLoggingAggregator
RecordContext
SSRSpeakerRecognizerSAT
Time
CSVoiceIdXPCClient
SSRVoiceProfileStoreCleaner
SSRVoiceProfile
SSRTriggerPhraseDetectorQuasar
CSPowerAssertionGibraltar
CSDiagnosticReporter
CSServerEndpointFeatures
CSAVVoiceTriggerClientManager
LPCMTypeConversion
CSVTUIEditDistance
CSAudioTimeConverter
SSRVoiceProfilePruner
SSRSpeakerRecognitionControllerDelegate
SSRVoiceProfileRetrainingContext
SSRVoiceProfileModelContext
CSVoiceTriggerEventInfoProvider
SSRVoiceProfileMetadataManager
ResourcePathHash
CSVTUIRegularExpressionMatcher
CSVTUITrainingSessionWithPayload
SFSpeechRecognitionTaskDelegate
CSVTUIEndPointDelegate
CSVTUITrainingSession
SSRVoiceProfileManager
CSAudioCircularBuffer
CSAudioZeroFilter
CSNNVADEndpointAnalyzer
CSEndpointAnalyzerImpl
CSEndpointAnalyzer
CSAudioRecordContext
SSRAssetManager
SSRSpeakerRecognitionController
SSRSpeakerRecognitionOrchestratorDelegate
SSRMobileAssetProvider
SSRVoiceProfileStore
CSSelectiveChannelAudioFileWriter
CSAudioDecoder
SSRBiometricMatch
BKDeviceDelegate
SSRSpeakerAnalyzerSAT
SSREnrollmentDataManager
SSRVoiceProfileStorePrefs
SSRSpeakerRecognitionContext
SSRSpeakerRecognitionModelContext
CSVTUIASRGrammars
NSURLSessionDelegate
CSUtils
SSRAESKeyManager
CSConfig
SpeakerRecognition
AudioHardware
CSDispatchGroup
init
_cs_initWithXPCObject:
_cs_xpcObject
initWithXPCObject:
xpcObject
packets
setPackets:
avgPower
setAvgPower:
peakPower
setPeakPower:
timeStamp
setTimeStamp:
numChannels
setNumChannels:
audioFormat
setAudioFormat:
streamHandleID
setStreamHandleID:
.cxx_destruct
_avgPower
_peakPower
_numChannels
_audioFormat
_packets
_timeStamp
_streamHandleID
utteranceFileASBD
lpcmInt16ASBD
initWithURL:inputFormat:outputFormat:
fileURLWithPath:
endAudio
dealloc
URLByDeletingPathExtension
URLByAppendingPathExtension:
defaultManager
path
fileExistsAtPath:
localeWithLocaleIdentifier:
setLocale:
setDateFormat:
stringFromDate:
deviceProductType
deviceProductVersion
deviceBuildVersion
numberWithBool:
dictionaryWithObjects:forKeys:count:
dictionaryWithDictionary:
setObject:forKey:
dataWithJSONObject:options:error:
writeToFile:atomically:
dataWithContentsOfFile:
JSONObjectWithData:options:error:
mutableCopy
addContextKey:withContext:
isEqual:
class
self
performSelector:
performSelector:withObject:
performSelector:withObject:withObject:
isProxy
isKindOfClass:
isMemberOfClass:
conformsToProtocol:
respondsToSelector:
retain
release
autorelease
retainCount
zone
hash
superclass
description
debugDescription
addSamples:numSamples:
initWithURL:
initWithFilepath:
addContextKey:fromMetaFile:
fileURL
isWriting
fFile
inASBD
outASBD
_fileURL
boolValue
_storeModeEnabled
sharedPreferences
runningVoiceTriggerOnMac
setFileLoggingLevel:
fileLoggingLevel
intValue
stringByAppendingPathComponent:
baseDir
fileExistsAtPath:isDirectory:
createDirectoryAtPath:withIntermediateDirectories:attributes:error:
localizedDescription
stringWithFormat:
assistantLogDirectory
myriadHashDirectory
numberWithInteger:
integerValue
date
stringByReplacingOccurrencesOfString:withString:
floatValue
interstitialRelativeDirForLevel:
enableAudioInjection:withKey:
audioInjectionEnabledWithKey:
count
enumerateObjectsUsingBlock:
unsignedIntegerValue
smartSiriVolumeContextAwareEnabled
fileLoggingIsEnabled
voiceTriggerEnabled
phraseSpotterEnabled
isAttentiveSiriEnabled
isAttentiveSiriAudioLoggingEnabled
voiceTriggerInCoreSpeech
twoShotNotificationEnabled
setFileLoggingIsEnabled:
voiceTriggerAudioLogDirectory
ssvLogDirectory
getSSVLogFilePathWithSessionIdentifier:
trialBaseAssetDirectory
assistantAudioFileLogDirectory
myriadHashFilePath
secondPassAudioLoggingEnabled
jarvisAudioLoggingEnabled
setJarvisTriggerMode:
getJarvisTriggerMode
startOfSpeechAudioLoggingEnabled
forceVoiceTriggerAPMode
forceVoiceTriggerAOPMode
getStartOfSpeechAudioLogFilePath
_isDirectory:
remoteVoiceTriggerDelayTime
remoteVoiceTriggerEndpointTimeoutWithDefault:
interstitialAbsoluteDirForLevel:
myriadFileLoggingEnabled
enableAudioInjection:
audioInjectionEnabled
enableProgrammableAudioInjection:
programmableAudioInjectionEnabled
setAudioInjectionFilePath:
audioInjectionFilePath
isPHSSupported
_isRemoteVoiceTriggerAvailable
isSpeakerRecognitionAvailable
speakerIdScoreReportingType
smartSiriVolumeSoftVolumeEnabled
smartSiriVolumeContextAwareLoggingEnabled
audioSessionActivationDelay
maxNumLoggingFiles
maxNumGradingFiles
useSiriActivationSPIForHomePod
useSiriActivationSPIForwatchOS
iOSBargeInSupportEnabled
shouldOverwriteRemoteVADScore
overwritingRemoteVADScore
setHearstFirstPassModelVersion:
setHearstSecondPassModelVersion:
fakeHearstModelPath
companionSyncVoiceTriggerUtterancesEnabled
opportuneSpeakListenerBypassEnabled
bypassPersonalizedHeySiri
isMultiPhraseVTEnabled
isMultiChannelAudioLoggingEnabled
isAdBlockerAudioLoggingEnabled
isSelfTriggerFileLoggingEnabled
avvcContext
objectForKeyedSubscript:
initWithMode:deviceUID:
avvcContextSettings
getFixedHighPrioritySerialQueueWithLabel:
_initializeSPGWithContext:
vadResourcePath
initWithConfiguration:modelVersion:
inputRecordingSampleRate
requestSupportedWithSamplingRate:
defaultServerEndpointFeatures
wordCount
trailingSilenceDuration
endOfSentenceLikelihood
silencePosterior
initWithWordCount:trailingSilenceDuration:eosLikelihood:pauseCounts:silencePosterior:taskName:processedAudioDurationInMilliseconds:
addAudio:numSamples:
initWithConfigFile:samplingRate:queue:
setDelegate:
eosLikelihood
pauseCounts
silenceFramesCountMs
silenceProbability
silenceDurationMs
processedAudioMs
initWithWordCount:trailingSilenceDuration:endOfSentenceLikelihood:pauseCounts:silencePosterior:clientSilenceFramesCountMs:clientSilenceProbability:silencePosteriorNF:serverFeaturesLatency:
didEndpointWithFeatures:audioTimestamp:featuresToLog:endpointPosterior:extraDelayMs:
SSRVoiceActivityDetector:didDetectEndPointAt:
SSRVoiceActivityDetector:didDetectStartPointAt:
clientSilenceFeaturesAvailable:
silenceDurationEstimateAvailable:numEstimates:clientProcessedAudioMs:
initWithContext:delegate:
processAudioData:numSamples:
resetWithContext:
context
setContext:
delegate
earSpg
setEarSpg:
hybridClassifier
setHybridClassifier:
defaultServerEpFeatures
setDefaultServerEpFeatures:
segmentStartPointSampleCount
setSegmentStartPointSampleCount:
numSamplesProcessed
setNumSamplesProcessed:
endpointReported
setEndpointReported:
startpointReported
setStartpointReported:
spgQueue
setSpgQueue:
_numConsecutiveNonSilenceFrames
_endpointReported
_startpointReported
_context
_delegate
_earSpg
_hybridClassifier
_defaultServerEpFeatures
_segmentStartPointSampleCount
_numSamplesProcessed
_spgQueue
unsignedIntValue
_readAudioBufferAndFeed
audioFileReaderDidStartRecording:successfully:error:
dataWithLength:
bytes
audioFileReaderBufferAvailable:buffer:atTime:
audioFileReaderDidStopRecording:forReason:
close
setRecordBufferDuration:
prepareRecording:
startRecording
stopRecording
readSamplesFromChannelIdx:
_fFile
_queue
_audioFeedTimer
_bufferDuration
_outASBD
_sharedAudioLoggingQueue
URLByDeletingLastPathComponent
containsString:
removeItemAtURL:error:
inputRecordingNumberOfChannels
inputRecordingSampleByteDepth
seekToEndOfFile
seekToFileOffset:
readDataOfLength:
getBytes:length:
initWithData:encoding:
isEqualToString:
offsetInFile
length
writeData:
_createAudioFileWriterForOpportuneSpeakListenerWithLoggingDir:inputFormat:outputFormat:
_createAudioFileWriterWithLoggingDir:withLoggingUUID:inputFormat:outputFormat:
_getDateLabel
getNumElementInBitset:
lpcmNonInterleavedASBDWithSampleRate:numberOfChannels:
lpcmInterleavedASBDWithSampleRate:numberOfChannels:
initWithURL:inputFormat:outputFormat:channelBitset:
_createAudioFileWriterForAdBlockerWithLoggingDir:inputFormat:outputFormat:
pruneLogFiles
URLWithString:
removeLogFilesInDirectory:matchingPattern:beforeDays:
pruneNumberOfGradingFilesTo:
pruneNumberOfLogFilesTo:
arrayWithObjects:
countByEnumeratingWithState:objects:count:
clearLogFilesInDirectory:matchingPattern:exceedNumber:
cleanupOrphanedGradingFiles
arrayWithObjects:count:
contentsOfDirectoryAtURL:includingPropertiesForKeys:options:error:
dictionary
absoluteString
lastPathComponent
stringByDeletingPathExtension
setObject:forKeyedSubscript:
removeObjectForKey:
removeItemAtPath:error:
enumerateKeysAndObjectsUsingBlock:
generateDeviceAudioLogging:speechId:
_readDataFromFileHandle:toFileHandle:
createAudioFileWriterForOpportuneSpeakListenerWithInputFormat:outputFormat:
createAudioFileWriterForRemoteVADWithInputFormat:outputFormat:withLoggingUUID:
createAudioFileWriterWithInputFormat:outputFormat:withLoggingUUID:
createSelectiveChannelAudioFileWriterWithChannelBitset:
createAudioFileWriterForAdBlockerWithInputFormat:outputFormat:
removeLogFilesOlderThanNDays:
audioFileWriterForAttentiveSiri
initWithContext:error:
voiceController
isRecording
setSynchronousCallbackEnabled:
setMeteringEnabled:
setStopOnEndpointEnabled:
setRecordEndpointMode:
setRecordDelegate:
setPlaybackDelegate:
getRecordBufferDuration
prepareRecordWithSettings:error:
numberWithUnsignedLongLong:
startRecordingWithSettings:error:
releaseAudioSession:
setEndpointerDelegate:
playRecordStartingAlertAndResetEndpointer
updateMeters
averagePowerForChannel:
recordRoute
playbackRoute
_hasInputAudioRoute
_hasCorrectInputAudioRoute
_hasCorrectOutputAudioRoute
data
bytesDataSize
initWithBytes:length:
audioSessionRecordBufferAvailable:
audioSessionDidStartRecording:error:
audioSessionDidStopRecording:
convertStopReason:
audioSessionErrorDidOccur:
audioSessionUnsupportedAudioRoute
prepareRecord
audioSource
resetEndPointer
releaseAudioSession
hasAudioRoute
hasCorrectAudioRoute
averagePower
voiceControllerDidStartRecording:successfully:
voiceControllerDidStartRecording:successfully:error:
voiceControllerDidStopRecording:forReason:
voiceControllerDidDetectStartpoint:
voiceControllerDidDetectEndpoint:ofType:
voiceControllerDidDetectEndpoint:ofType:atTime:
voiceControllerEncoderErrorDidOccur:error:
voiceControllerDidFinishAlertPlayback:ofType:error:
voiceControllerDidFinishAlertPlayback:withSettings:error:
voiceControllerRecordHardwareConfigurationDidChange:toConfiguration:
voiceControllerBeginRecordInterruption:
voiceControllerBeginRecordInterruption:withContext:
voiceControllerEndRecordInterruption:
voiceControllerMediaServicesWereLost:
voiceControllerMediaServicesWereReset:
voiceControllerWillSetAudioSessionActive:willActivate:
voiceControllerDidSetAudioSessionActive:isActivated:
voiceControllerRecordBufferAvailable:buffer:
voiceControllerLPCMRecordBufferAvailable:buffer:
voiceControllerWirelessSplitterRouteAvailable:devices:
voiceControllerDidStartRecording:forStream:successfully:error:
voiceControllerDidStopRecording:forStream:forReason:
voiceControllerAudioCallback:forStream:buffer:
voiceControllerStreamInvalidated:forStream:
voiceControllerPlaybackBufferAvailable:buffer:
voiceControllerDidStartPlaying:successfully:
voiceControllerDidStopPlaying:forReason:
voiceControllerDecoderErrorDidOccur:error:
voiceControllerPlaybackHardwareConfigurationDidChange:toConfiguration:
voiceControllerBeginPlaybackInterruption:
voiceControllerEndPlaybackInterruption:
_voiceController
initWithVoiceRecognitionContext:delegate:queue:
processAudioData:
resetForNewRequest
getVoiceRecognizerResults
recognitionStyle
currentHandler
handleFailureInMethod:object:file:lineNumber:description:
_initializeWithContext:
stringForInvocationStyle:
sessionId
vtEventInfo
numberWithFloat:
speakerRecognizer:hasSpeakerIdInfo:
speakerRecognizerFinishedProcessing:withFinalSpeakerIdInfo:
voiceRecognitionPSRAnalyzer:hasVoiceRecognitionInfo:
voiceRecognitionPSRAnalyzerFinishedProcessing:withVoiceRecognitionInfo:
lastScoreCard
spIdCtx
setSpIdCtx:
setSessionId:
lastSpeakerInfo
setLastSpeakerInfo:
queue
setQueue:
invocationStyleStr
setInvocationStyleStr:
extraSamplesAtStart
setExtraSamplesAtStart:
vtEndInSampleCount
setVtEndInSampleCount:
endInSampleCount
setEndInSampleCount:
processingEnded
setProcessingEnded:
totalNumSamplesReceived
setTotalNumSamplesReceived:
psrAnalyzer
setPsrAnalyzer:
_processingEnded
_spIdCtx
_sessionId
_lastSpeakerInfo
_invocationStyleStr
_extraSamplesAtStart
_vtEndInSampleCount
_endInSampleCount
_totalNumSamplesReceived
_psrAnalyzer
subdataWithRange:
dataWithCapacity:
appendData:
hostTimeFromSampleCount:anchorHostTime:anchorSampleCount:
initWithData:numChannels:numSamples:sampleByteDepth:startSampleCount:hostTime:remoteVAD:
copy
apply12dBGain:
numSamples
subChunkFrom:numSamples:
dataForChannel:
dataWithRemoteVADWithScaleFactor:numAudioSamplesPerRemoteVAD:
remoteVADAvailable
subChunkFrom:numSamples:forChannel:
gainCompensatedChunk
skipSamplesAtStartSuchThatNumSamplesReceivedSoFar:reachesACountOf:completionHandler:
splitAudioChunkSuchThatNumSamplesReceivedSoFar:reachesACountOf:completionHandler:
sampleByteDepth
startSampleCount
hostTime
remoteVAD
setRemoteVAD:
_data
_numSamples
_sampleByteDepth
_startSampleCount
_hostTime
_remoteVAD
setSkipAlertBehavior:
appendFormat:
noAlertOption
requestHistoricalAudioDataWithHostTime
setRequestHistoricalAudioDataWithHostTime:
requestHistoricalAudioDataSampleCount
setRequestHistoricalAudioDataSampleCount:
startRecordingHostTime
setStartRecordingHostTime:
startRecordingSampleCount
setStartRecordingSampleCount:
useOpportunisticZLL
setUseOpportunisticZLL:
startAlertBehavior
setStartAlertBehavior:
stopAlertBehavior
setStopAlertBehavior:
errorAlertBehavior
setErrorAlertBehavior:
skipAlertBehavior
_requestHistoricalAudioDataWithHostTime
_requestHistoricalAudioDataSampleCount
_useOpportunisticZLL
_skipAlertBehavior
_startRecordingHostTime
_startRecordingSampleCount
_startAlertBehavior
_stopAlertBehavior
_errorAlertBehavior
_reset
_scaleDecayConstants:
_savePeaks:averagePower:maxSample:
peakValueSinceLastCall
setPeakValueSinceLastCall:
setSawNotANumber:
setSawInfinity:
_zapgremlins:
_linearToDB:
_ampToDB:
initWithSampleRate:
reset
processShortBuffer:stride:inFrameToProcess:
processFloatBuffer:stride:inFrameToProcess:
getPeakPowerDB
getAveragePowerDB
_averagePowerI
_averagePowerF
_instantaneousMode
_peak
_maxPeak
_decay
_peakDecay
_averagePowerPeak
_peakHoldCount
_sampleRate
_previousBlockSize
_decay1
_peakDecay1
_clipping
initWithConfigPath:resourcePath:phraseId:
analyzeWavData:numSamples:
getSuperVectorWithEndPoint:
errorWithDomain:code:userInfo:
logAggregator
setSpeakerRecognitionProcessingStatus:
debugUtteranceAudioFile
debugUtteranceMetaFile
updateDebugFilePathsForSegment:
ssrAudioLogsCountWithinPrivacyLimit
timeIntervalSinceReferenceDate
maxAllowedAudioSamples
_resetWithContext:
sharedInstance
submitVoiceIdIssueReport:
addEntriesFromDictionary:
combinationWeight
numEnrollmentUtterances
scoreType
numberWithUnsignedInteger:
configVersion
numberWithDouble:
logSpeakerRecognitionGradingMetadataAtFilepath:withScoreInfo:
stringByDeletingLastPathComponent
stringByAppendingPathExtension:
orchestratorScoresWithPSRScores:withSATScores:withSegmentStartTime:
timeIntervalSinceDate:
setSpeakerRecognitionWaitTime:
voiceRecognitionOrchestrator:hasVoiceRecognitionInfo:
_logSpeakerIdProcessorScoreDelayWithScoreInfo:hasFinished:
voiceRecognitionOrchestratorFinishedProcessing:withFinalVoiceRecognitionInfo:
initWithContext:withDelegate:error:
processAudio:numSamples:
getLatestVoiceRecognitionInfo
ssrUttLogger
setSsrUttLogger:
myriadResult
setMyriadResult:
psrRecognizer
setPsrRecognizer:
satRecognizer
setSatRecognizer:
setVad:
psrLastSpeakerInfo
setPsrLastSpeakerInfo:
satLastSpeakerInfo
setSatLastSpeakerInfo:
combinedScores
setCombinedScores:
psrFinalSpeakerInfo
setPsrFinalSpeakerInfo:
satFinalSpeakerInfo
setSatFinalSpeakerInfo:
debugUtteranceAudioFilePath
setDebugUtteranceAudioFilePath:
debugUtteranceJsonFilePath
setDebugUtteranceJsonFilePath:
_lastScoreReportTimeStamp
_lastSegmentStartTime
_segmentCounter
_numSamplesAddedToSpeakerRecognizers
_endAudioCalled
_startPointReported
_ssrUttLogger
_myriadResult
_psrRecognizer
_satRecognizer
_vad
_psrLastSpeakerInfo
_satLastSpeakerInfo
_combinedScores
_psrFinalSpeakerInfo
_satFinalSpeakerInfo
_debugUtteranceAudioFilePath
_debugUtteranceJsonFilePath
initWithVoiceRetrainingContext:
resetModelForRetraining
addUtterances:withPolicy:withCompletion:
needsRetrainingWithAudioFiles:
purgeConfusionInformationWithPolicy:
modelFilePath
implicitTrainingRequired
retrainerType
audioConverterBitrate
setEncoderBitRate:
setSampleRate:
setNumberOfChannels:
inputRecordingSampleBitDepth
setLpcmBitDepth:
inputRecordingIsFloat
setLpcmIsFloat:
setRecordContext:
setUseCustomizedRecordSettings:
defaultRequestWithContext:
requestForLpcmRecordSettingsWithContext:
requestForOpusRecordSettingsWithContext:
requestForSpeexRecordSettingsWithContext:
recordContext
requiresHistoricalBuffer
setRequiresHistoricalBuffer:
useCustomizedRecordSettings
sampleRate
lpcmBitDepth
lpcmIsFloat
numberOfChannels
encoderBitRate
isSiri
setIsSiri:
_requiresHistoricalBuffer
_useCustomizedRecordSettings
_lpcmIsFloat
_isSiri
_lpcmBitDepth
_numberOfChannels
_encoderBitRate
_recordContext
initWithBundleIdentifier:
isPermitted
shouldMakeRecordWithFrequency:
UUIDString
saveRecordWithData:recordInfo:completion:
fetchMedicalDataWithCompletion:
initWithHealthStore:
gregorianBirthday
name
currentCalendar
components:fromDate:
components:fromDateComponents:toDateComponents:options:
year
componentsSeparatedByString:
firstObject
fetchMedicalIDDataWithCompletion:
createDESRecordWithSuperVector:withMetaInfo:
inputRecordingSampleRateNarrowBand
inputRecordingBytesPerPacket
inputRecordingFramesPerPacket
inputRecordingBytesPerFrame
lpcmInt16NarrowBandASBD
lpcmFloatASBD
opusASBD
opusNarrowBandASBD
speexASBD
lpcmInterleavedASBD
lpcmInterleavedWithRemoteVADASBD
lpcmNonInterleavedASBD
lpcmNonInterleavedWithRemoteVADASBD
lpcmASBD
lpcmNarrowBandASBD
aiffFileASBD
createVoiceScorersWithVoiceProfiles:withConfigFile:withResourceFile:withOffsetsType:
initWithProfileID:withModelFile:withConfigFile:withResourceFile:withOffsetsType:
resetScorerWithModelFilePath:
analyzeSpeakerVector:withDimensions:withThresholdType:
scoreSpeakerVector:withDimensions:withThresholdType:
analyzeSuperVector:withDimensions:withThresholdType:
updateSAT
getSATVectorCount
getSpeakerVectorAtIndex:
deleteVectorAtIndex:
profileID
sysConfigRoot
psrConfigFilePath
psrConfigRoot
satModelAvailable
_satModelAvailable
_profileID
_sysConfigRoot
_psrConfigFilePath
_psrConfigRoot
initWithLocaleIdentifier:withAudioSession:withAppDomain:
setLocaleIdentifier:
initNewVoiceProfileWithLocale:withAppDomain:
sharedManager
installedAssetOfType:forLanguage:
createKeywordDetector
initWithAsset:
initWithLocale:
_stopAudioSession
destroySpeakerTrainer
_destroyAudioSession
_setupAudioSession
closeSessionBeforeStartWithStatus:successfully:withCompletion:
_createAudioAnalyzer
_shouldShowHeadsetDisconnectionMessage
_startAudioSession
createSpeechRecognizer
sharedtrainingSessionQueue
initWithUtteranceId:sessionNumber:Locale:audioSession:keywordDetector:speechRecognizer:speechRecognitionRequest:sessionDelegate:sessionDispatchQueue:completion:
addObject:
startTraining
suspendTraining
closeSessionWithStatus:successfully:complete:
_audioSource
setEndpointStyle:
setStartWaitTime:
setEndWaitTime:
setInterspeechWaitTime:
preheat
resetForNewRequestWithSampleRate:recordContext:recordSettings:
resumeTraining
VTUITrainingManagerFeedLevel:
VTUITrainingManagerStopListening
sharedTrainer
addUtterance:toProfile:withAsset:
processAudioSamplesAsynchronously:
didDetectBeginOfSpeech
didDetectEndOfSpeech:
trainingManagerWithLocaleID:withAppDomain:
CSVTUITrainingSessionRMSAvailable:
CSVTUITrainingSessionStopListen
CSVTUITrainingSession:hasTrainUtterance:languageCode:payload:
endpointer:didDetectStartpointAtTime:
endpointer:didDetectHardEndpointAtTime:withMetrics:
voiceProfile
prepareWithCompletion:
_beginOfSpeechDetected
_endOfSpeechDetected
cleanupWithCompletion:
trainUtterance:shouldUseASR:completion:
cancelTrainingForID:
suspendAudio
setSuspendAudio:
startRMS
stopRMS
shouldPerformRMS
didDetectForceEndPoint
setRms:
speechRecognizerAvailable
_performRMS
_locale
_audioSession
_audioAnalyzer
_keywordDetector
_trainingSessions
_currentTrainingSession
_sessionNumber
_suspendAudio
_cleanupCompletion
_speechRecognizer
_currentAsset
_profile
_speechRecognizerAvailable
_rms
voiceRetrainersWithContext:
UUID
UTF8String
initWithDescription:
_transaction
_description
stringWithUTF8String:
initWithUUIDString:
initWithRoute:isRemoteDevice:remoteDeviceUID:remoteDeviceProductIdentifier:
initWithFormat:
encodeObject:forKey:
decodeObjectOfClass:forKey:
isRemoteDevice
remoteDeviceUID
remoteProductIdentifier
supportsSecureCoding
copyWithZone:
encodeWithCoder:
initWithCoder:
initWithAVVCRecordDeviceInfo:
route
remoteDeviceProductIdentifier
_isRemoteDevice
_route
_remoteDeviceUID
_remoteDeviceProductIdentifier
appDomain
siriProfileId
locale
userName
dateAdded
initWithVoiceProfile:
initWithSharedSiriId:languageCode:productCategory:version:
setAppDomain:
profileId
setProfileId:
languageCode
setLanguageCode:
productCategory
setProductCategory:
version
setVersion:
setDateAdded:
sharedSiriId
setSharedSiriId:
homeId
setHomeId:
setUserName:
_appDomain
_profileId
_languageCode
_productCategory
_version
_dateAdded
_sharedSiriId
_homeId
_userName
initWithLocale:asset:
computeTriggerConfidenceForAudio:withCompletion:
removeObject:
satVTImplicitThreshold
resourcePath
keywordDetectorNDAPIConfigFilePath
supportPremiumModel
VTSecondPassConfigPathRecognizerExistFrom:
keywordDetectorQuasarConfigFilePath
initWithLocale:configPath:resourcePath:
VTSecondPassRecognizerScoreScaleFactorFrom:
dataWithBytes:length:
streamAudioFromFileUrl:audioStreamBasicDescriptor:samplesPerStreamChunk:audioDataAvailableHandler:
bestScore
filterVTAudioFiles:withLocale:withAsset:
detectorNDAPI
setDetectorNDAPI:
detectorQuasar
setDetectorQuasar:
recognizerScoreScaleFactor
setRecognizerScoreScaleFactor:
_recognizerScoreScaleFactor
_detectorNDAPI
_detectorQuasar
waitingForConnection:error:
isConnected
getLocalUrl
string
_compatibilityVersion
stringValue
appendString:
_footprint
assetForAssetType:resourcePath:configVersion:
attributes
objectForKey:
state
isPremium
getCSAssetOfType:
isDownloading
isCSAssetInstalled
canBePurged
isLatestCompareTo:
assetForAssetType:resourcePath:configVersion:assetProvider:
hybridEndpointerAssetFilename
contentsOfDirectoryAtPath:error:
initWithResourcePath:configFile:configVersion:assetProvderType:
fallBackAssetResourcePath
_decodeJson:
getNumberForKey:category:default:
assetHashInResourcePath:
getConfigFileNameForAssetType:
defaultFallBackAssetForSmartSiriVolume
defaultFallBackAssetForHearst
defaultFallBackAssetForAdBlocker
getBoolForKey:category:default:
getStringForKey:category:default:
containsKey:category:
containsCategory:
hashFromResourcePath
isEqualAsset:
stringForCurrentAssetProviderType
assetProvider
_decodedInfo
_path
_resourcePath
_configVersion
_assetProvider
URLsForDirectory:inDomains:
lastObject
createDirectoryIfDoesNotExist:
ssrAudioLogsDir
predicateWithFormat:
filteredArrayUsingPredicate:
removeItemAtPath:
URLByAppendingPathComponent:
mutableBytes
satConfigFileNameForCSSpIdType:forModelType:forAssetType:
readJsonFileAtPath:
setWithObjects:
containsObject:
deviceCategoryForDeviceProductType:
getVoiceProfileProductCategoryFromVersionFilePath:
deviceCategoryStringRepresentationForCategoryType:
dataWithContentsOfURL:
pathExtension
enumeratorAtPath:
enter
leave
getHomeUserIdForSharedUserId:completion:
waitWithTimeout:
userVoiceProfilesForAppDomain:
userVoiceProfilesForAppDomain:forLocale:
objectAtIndexedSubscript:
stringByAppendingString:
getExplicitEnrollmentUtterancesFromDirectory:
getImplicitEnrollmentUtterancesFromDirectory:
arrayByAddingObjectsFromArray:
_getUtterancesFromDirectory:
isUtteranceImplicitlyTrained:
predicateWithBlock:
getUtteranceEnrollmentType:
insertObject:atIndex:
recordedTimeStampOfFile:
compare:
compare:options:
sortedArrayUsingComparator:
moveItemAtPath:toPath:error:
stringForCSSpIdType:
explicitSpIdTypeForSpId:
spIdTypeForString:
stringForSpeakerRecognizerType:
stringForVoiceProfileRetrainerType:
satConfigFileNameForCSSpIdType:
psrConfigFileNameForCSSpIdType:
spIdVoiceProfileImportRootDir
cleanupOrphanedVoiceIdGradingFiles
spidAudioTrainUtterancesDir
isSpeakerRecognitionSupportedInLocale:
getVoiceProfileIdentityFromVersionFilePath:
isCurrentDeviceCompatibleWithNewerVoiceProfileAt:
isCurrentDeviceCompatibleWithVoiceProfileAt:
getImplicitUtteranceCacheDirectory
getNumberOfAudioFilesInDirectory:
dumpFilesInDirectory:
getContentsOfDirectory:
getHomeUserIdForVoiceProfile:withCompletion:
getVoiceProfilesForSiriProfileId:
getVoiceProfileForSiriProfileId:forLanguageCode:
segmentVoiceTriggerFromAudioFile:withVTEventInfo:withStorePayloadPortion:withCompletion:
getEnrollmentUtterancesFromDirectory:
getExplicitOnlyEnrollmentUtterancesFromDirectory:
getEnrollmentUtterancesCountFromDirectory:withCountBlock:
moveContentsOfSrcDirectory:toDestDirectory:
getSiriLanguageWithFallback:
getAssetProviderType
installedAssetOfType:forLanguageCode:
allInstalledAssetsOfType:forLanguage:
reloadForLocale:
CVTThreshold
VTSecondPassCategoryForFirstPassSource:
VTSecondPassPreTriggerAudioTimeFrom:
inputRecordingDurationInSecs
initWithNumChannels:recordingDuration:samplingRate:
bestStart
bestEnd
samplesFed
sampleCount
_sampleLengthFrom:To:
copySamplesFrom:to:
channelForProcessedInput
analyze:
triggeredUtterance:
_keywordAnalyzer
_lastKeywordScore
_keywordThreshold
_audioBuffer
array
removeObserver:
addObserver:
_checkAllConditionsEnabled
type
CSEventMonitorDidReceiveEvent:
setCallback:
addConditions:
subscribeEventMonitor:
isEnabled
notifyCallback:option:
notifyCallbackWithOption:
_monitors
_conditions
_callback
startRecordingWithOptions:error:
stopRecording:
didPlayEndpointBeep
voiceTriggerEventInfo
hasPendingTwoShotBeep
addUtterance:toProfile:
stringByAppendingFormat:
numberWithInt:
initWithEvent:locale:configVersion:
pushAnalyticsWithLazyBlock:
setVoiceProfilePruningFailureReasonCode:
setVoiceProfileUpdateScoreMSE:
setVoiceProfileDiscardedUtteranceCount:
setvoiceProfilePrunedUtteranceCount:
setVoiceProfileRetainedUtteranceCount:
appendVoiceProfileExplicitUtteranceScoreWith:
appendVoiceProfileImplicitUtteranceScoreWith:
appendVoiceProfileDiscardedImplicitUtteranceScoreWith:
appendVoiceProfileFailedExplicitUtteranceScoreWith:
setVoiceProfileRetrainingFailureReasonCode:
setRetrainingWaitTime:
pushAnalytics
voiceProfilePruningFailureReasonCode
voiceProfileUpdateScoreMSE
voiceProfileDiscardedUtteranceCount
voiceProfilePrunedUtteranceCount
setVoiceProfilePrunedUtteranceCount:
voiceProfileRetainedUtteranceCount
voiceProfileRetrainingFailureReasonCode
retrainingWaitTime
speakerRecognitionProcessingStatus
speakerRecognitionWaitTime
speakerRecognitionPSRProcessingStatus
setSpeakerRecognitionPSRProcessingStatus:
speakerRecognitionSATProcessingStatus
setSpeakerRecognitionSATProcessingStatus:
_eventString
_eventContext
explicitUtteranceIndex
explicitFailedUtteranceIndex
implicitUtteranceIndex
implicitDiscardedUtteranceIndex
_voiceProfileUpdateScoreMSE
_voiceProfilePruningFailureReasonCode
_voiceProfileDiscardedUtteranceCount
_voiceProfilePrunedUtteranceCount
_voiceProfileRetainedUtteranceCount
_voiceProfileRetrainingFailureReasonCode
_retrainingWaitTime
_speakerRecognitionProcessingStatus
_speakerRecognitionWaitTime
_speakerRecognitionPSRProcessingStatus
_speakerRecognitionSATProcessingStatus
isRecordContextVoiceTrigger:
isRecordContextJarvisVoiceTrigger:
isRecordContextHearstDoubleTap:
isRecordContextRaiseToSpeak:
isRecordContextAutoPrompt:
isRecordContextHomeButtonPress:
isRecordContextSpeakerIdTrainingTrigger:
isRecordContextHearstVoiceTrigger:
isRecordContextJarvisButtonPress:
recordContextString:
getHostClockFrequency
secondsToHostTime:
hostTimeToSeconds:
hostTimeToTimeInterval:
sampleCountFromHostTime:anchorHostTime:anchorSampleCount:
macHostTimeFromBridgeHostTime:
_handleListenerEvent:
disconnect
_handleListenerError:
initWithDictionary:
_sendMessage:connection:completion:
_decodeError:
connect
notifyImplicitUtterance:audioDeviceType:audioRecordType:voiceTriggerEventInfo:otherCtxt:completion:
xpcConnection
setXpcConnection:
_xpcConnection
isMarkedSATEnrolled
reverseObjectEnumerator
domain
code
provisionedVoiceProfilesForAppDomain:withLocale:
SSRSpeakerProfilesBasePath
initWithCapacity:
getResourceValue:forKey:error:
_cleanupAppDomain:
SSRBasePathForAppDomain:
_cleanuplanguageCodePath:forAppDomain:
voiceProfileForId:
_cleanupOrphanedMetafilesForProfile:payloadUtteranceLifeTimeInDays:
_cleanupImplicitUtteranceCacheForProfile:
voiceProfileImplicitCacheDirPath
voiceProfileIdentity
voiceProfileVersion
voiceProfileBasePath
_cleanupContentsOfSatFolder:
voiceProfileAudioDirPathForSpidType:
_cleanupOrphanedMetafilesAtURL:
_cleanupPayloadUtterancesFromProfile:forModelType:exceedingLifeTimeInDays:
attributesOfItemAtPath:error:
fileSize
_cleanupInvalidAudioFiles:
dateWithTimeIntervalSinceNow:
getImplicitEnrollmentUtterancesPriorTo:forType:
voiceProfileModelDirForSpidType:recognizerType:
_cleanupModelFilesAtDir:forAssetArray:
arrayWithCapacity:
filterDuplicatedSiriProfilesFrom:
filterInvalidSiriProfilesFrom:
cleanupProfileStore
cleanupInvalidModelsForProfile:withAssetArray:
doubleValue
dateWithTimeIntervalSince1970:
timeIntervalSince1970
dictionaryWithObjectsAndKeys:
dictionaryRepresentation
addUtterances:spIdType:
copyItemAtURL:toURL:error:
_voiceProfilePathForSpidType:
_getProfileVersionFilePath
_updateVoiceProfileVersionFile
_markSATEnrollmentWithMarker:
_isSATMarkedWithMarker:
createFileAtPath:contents:attributes:
setValue:forKey:
writeToFile:options:error:
setSharedSiriProfileId:
voiceProfileModelFilePathForRecognizerType:spIdType:
deleteAudioFilesForPrivacy
importVoiceProfileAtPath:
getExplicitEnrollmentUtterancesForType:
getEnrollmentUtterancesForModelType:
getImplicitEnrollmentUtterancesForType:
deleteModelForSpidType:recognizerType:
markSATEnrollmentSuccess
markSATEnrollmentMigrated
isMarkedSATMigrated
pruningCookie
updatePruningCookie:
profileBasePath
setProfileBasePath:
numberOfExplicitSatVectors
setNumberOfExplicitSatVectors:
_siriProfileId
_profileBasePath
_numberOfExplicitSatVectors
initWithConfiguration:
resetWithSamplingRate:language:taskType:userId:sessionId:deviceId:farField:audioSource:maxAudioBufferSizeSeconds:
resultsWithAddedAudio:numberOfSamples:taskName:
tokens
confidence
resultsWithEndedAudio
_syncRecognizer
initWithName:timeout:
invalidate
initWithStreamID:atStartHostTime:
avvcAlertBehavior
setStartAlert:
setStopAlert:
setStopOnErrorAlert:
setSkipAlert:
_alertBehaviorTypeFromAVVCOberrideType:
_avvcAlertOverrideType:
avvcStartRecordSettingsWithAudioStreamHandleId:
avvcSettings
setAVVCAlertBehavior:
submitAudioIssueReport:
submitVoiceTriggerIssueReport:
submitEndpointerIssueReport:
componentsJoinedByString:
initWithWordCount:trailingSilenceFrames:endOfSilenceLikelihood:pauseCounts:silencePosterior:taskName:
setWordCount:
setTrailingSilenceDuration:
setEosLikelihood:
setPauseCounts:
setSilencePosterior:
processedAudioDurationInMilliseconds
setProcessedAudioDurationInMilliseconds:
taskName
setTaskName:
_wordCount
_trailingSilenceDuration
_eosLikelihood
_pauseCounts
_silencePosterior
_processedAudioDurationInMilliseconds
_taskName
sharedVoiceTriggerClient
initWithLength:
applyGain:toBuffer:
convertToFloatLPCMBufFromShortLPCMBuf:
convertToShortLPCMBufFromFloatLPCMBuf:
applyNegative12dBGain:
lowercaseString
hasPrefix:
substringFromIndex:
regularExpressionWithPattern:options:error:
replaceMatchesInString:options:range:withTemplate:
_stringByStrippingLeadingNoise:
_stringByStrippingTrailingNoise:
rangeOfString:options:
numberOfMatchesInString:options:range:
_firstMatchesForRegularExpression:
firstMatchInString:options:range:
numberOfRanges
rangeAtIndex:
substringWithRange:
_stringByFixingNamePattern:
_stringByStrippingNoiseLeadingNoise:TrailingNoise:
_hasSubstring:
_matchesRegularExpression:
_caseInsensitiveHasMatchInEnumeration:
_firstMatchesForRegularExpressions:
processSampleCount:hostTime:
hostTimeFromSampleCount:
sampleCountFromHostTime:
anchorSampleCount
setAnchorSampleCount:
anchorHostTime
setAnchorHostTime:
_anchorSampleCount
_anchorHostTime
pruningNumRetentionUtterance
voiceProfilePruningCookie
pruningThresholdPSR
pruningThresholdSAT
pruningExplicitUttThresholdPSR
pruningExplicitUttThresholdSAT
_retrainVoiceProfile:withAsset:
initWithVoiceRecognitionContext:error:
_getScoresForAudio:withController:withDetector:forProfile:withCompletion:
_deleteUtterances:
setWithArray:
minusSet:
allObjects
processAudio:withNumberOfSamples:
getLatestSpeakerInfo
initWithVoiceRetrainingContext:error:
triggerRetrainingVoiceProfile:withContext:withCompletion:
speakerRecognitionController:hasSpeakerInfo:
speakerRecognitionFinishedProcessing:withFinalSpeakerInfo:
pruneVoiceProfile:forSpIdType:withAsset:
initWithConfigFilePath:withModelPath:withCompareModelFilePaths:
maxAllowedEnrollmentUtterances
compareVoiceProfileArray
setCompareVoiceProfileArray:
setVoiceProfile:
spIdType
resourceFilePath
filterToVoiceTriggerUtterances
forceRetrain
maxAllowedSpeakerVectors
modelsContext
asset
setAsset:
setLogAggregator:
_filterToVoiceTriggerUtterances
_forceRetrain
_compareVoiceProfileArray
_voiceProfile
_spIdType
_resourceFilePath
_maxAllowedSpeakerVectors
_modelsContext
_asset
_logAggregator
configFilePath
voiceProfileModelFilePath
compareModelFilePaths
_configFilePath
_voiceProfileModelFilePath
_compareModelFilePaths
voiceTriggerInfo
setVoiceTriggerInfo:
rtsTriggerInfo
setRtsTriggerInfo:
triggerNotifiedMachTime
setTriggerNotifiedMachTime:
_voiceTriggerInfo
_rtsTriggerInfo
_triggerNotifiedMachTime
_getBaseMetaDictionaryForUtterancePath:
timeStampWithSaltGrain
_writeMetaDict:forUtterancePath:
dateFromString:
saveUtteranceMetadataForUtterance:enrollmentType:isHandheldEnrollment:triggerSource:audioInput:otherBiometricResult:containsPayload:
matchWithString:TrailingStr:LeadingStr:Pattern:
isAvailable
_firedVoiceTriggerTimeout
shouldHandleSession
shouldMatchPayload
finishSpeechRecognitionTask
closeSessionWithStatus:successfully:
_firedEndPointTimeout
updateMeterAndForward
pushAudioInputIntoPCMBuffer:
setupSpeechRecognitionTaskWithVoiceTriggerEventInfo:
feedSpeechRecognitionWithPCMBuffer
_registerVoiceTriggerTimeout
handleAudioBufferForVTWithAudioInput:withDetectedBlock:
handleAudioInput:
_reportStopListening
_registerEndPointTimeout
_registerForceEndPointTimeout
requestTriggeredUtterance:
formattedString
whitespaceAndNewlineCharacterSet
stringByTrimmingCharactersInSet:
matchRecognitionResult:withMatchedBlock:withNonMatchedBlock:
bestTranscription
sharedGrammars
getTrailingPatternsForUtt:Locale:
getLeadingPatternsForUtt:Locale:
getRegexPatternsForUtt:Locale:
speechRecognitionDidDetectSpeech:
speechRecognitionTask:didHypothesizeTranscription:
speechRecognitionTask:didFinishRecognition:
speechRecognitionTaskFinishedReadingAudio:
speechRecognitionTaskWasCancelled:
speechRecognitionTask:didFinishSuccessfully:
setVoiceTriggerEventInfo:
_detectBOS
_ASRResultReceived
_reportedStopListening
_utteranceStored
_numSamplesFed
_bestTriggerSampleStart
_voiceTriggerEventInfo
setupPhraseSpotter
startMasterTimerWithTimeout:
resultAlreadyReported
stopMasterTimer
closeSessionWithCompletion:
removeAllObjects
trimBeginingOfPCMBufferWithVoiceTriggerEventInfo:
computeRequiredTrailingSamples
feedSpeechRecognitionTrailingSamplesWithCompletedBlock:
numSamplesInPCMBuffer
objectAtIndex:
appendAudioPCMBuffer:
removeObjectAtIndex:
frameLength
initWithCommonFormat:sampleRate:channels:interleaved:
initWithPCMFormat:frameCapacity:
mutableAudioBufferList
setFrameLength:
replaceObjectAtIndex:withObject:
removeObjectsInRange:
createAVAudioPCMBufferWithNSData:
getLMEforLocale:
setContextualStrings:
setTaskHint:
_setVoiceTriggerEventInfo:
recognitionTaskWithRequest:delegate:
finish
handleMasterTimeout:
scheduledTimerWithTimeInterval:target:selector:userInfo:repeats:
_status
_utteranceId
_speechRecognitionRequest
_speechRecognitionTask
_masterTimer
_pcmBufArray
_resultReported
_sessionProcess
_sessionSuspended
_ASRErrorOccured
_sessionDelegate
_trainingCompletion
_numRequiredTrailingSamples
_numTrailingSamples
setCurrentDeviceCategory:
discardSiriEnrollmentForProfileId:forLanguageCode:
_getVoiceProfilesForSiriProfileId:withLanguageCode:
deleteUserVoiceProfile:
updateVoiceProfile:withUserName:
_markVoiceProfileTrainingSyncForLanguage:
addUserVoiceProfile:withContext:withCompletion:
satImplicitTrainingEnabled
isIOSDeviceSupportingBargeIn
unsignedLongLongValue
getLastBiometricMatchForVoiceTriggerTimeStamp:
addImplicitUtterance:toVoiceProfile:withAsset:withTriggerSource:withAudioInput:withCompletion:
_CSSATDownloadPath
_getUserVoiceProfileDownloadCacheDirectoryWithUpdatePath:
_downloadAndEnrollVoiceProfileForProfileId:withDownloadTriggerBlock:
_enrollVoiceProfileForSiriProfileId:fromCacheDirectoryPath:withCategoryType:
_downloadVoiceProfileForProfileId:forDeviceCategory:withDownloadTriggerBlock:withCompletion:
_getUserVoiceProfileDownloadCacheDirectoryForProfileId:forDeviceCategory:forVoiceProfileVersion:
checkIfVoiceProfile:needsUpdatedWith:withCategory:
_enableVoiceTriggerIfLanguageMatches:
_CSSATUploadPathForSiriProfileId:
_prepareVoiceProfileWithSiriProfileId:withUploadBlock:
_CSSATLegacyUploadPath
_getVoiceProfilePathsToBeUploadedForSiriProfileId:
copyItemAtPath:toPath:error:
_copyExplicitEnrollmentFilesFromPath:toPath:withCompletion:
_copyVoiceProfileAtPath:toPath:
_isMarkedForVoiceProfileTrainingSyncForLanguage:
getDevicesWithAvailablePHSAssetsOnDeviceCheck:
devicesWithVoiceProfileIniCloudForLanguage:
getDevicesWithAvailablePHSAssetsForLanguage:completion:
userVoiceProfilesForLocale:
userVoiceProfileForVoiceProfileID:
migrateVoiceProfilesIfNeededWithCompletionBlock:
cleanupVoiceProfileStore:
cleanupVoiceProfileModelFilesForLocale:
retrainVoiceProfile:withContext:withCompletion:
sharedStorePrefs
getVoiceProfileStoreVersion
_isLegacyEnrollmentMarkedWith:forLanguageCode:
_CSSATCachePath
getSATEnrollmentPath
modelDirectoryPathForProfile:
discardSiriEnrollmentForLanguageCode:
newVoiceProfileWithLocale:withAppDomain:
addUtterances:toProfile:withContext:withCompletion:
notifyImplicitTrainingUtteranceAvailable:forVoiceProfileId:withRecordDeviceInfo:withRecordCtxt:withVoiceTriggerCtxt:withOtherCtxt:withCompletion:
getUserVoiceProfileUpdateDirectory
notifyUserVoiceProfileDownloadReadyForUser:getData:completion:
notifyUserVoiceProfileUpdateReady
notifyUserVoiceProfileUploadCompleteForSiriProfileId:withError:
uploadUserVoiceProfileForSiriProfileId:withUploadTrigger:completion:
getUserVoiceProfileUploadPathWithEnrolledLanguageList:
notifyUserVoiceProfileUploadComplete
getCachedVoiceProfileAvailabilityMetaBlob
hasVoiceProfileIniCloudForLanguageCode:withBackupMetaBlob:
isVoiceProfileUploadedToiCloudForLanguageCode:withCompletionBlock:
hasVoiceProfileIniCloudForLanguageCode:
enableVoiceTriggerUponVoiceProfileSyncForLanguage:
provisionedVoiceProfilesForLocale:
triggerVoiceProfileMigrationWithCompletion:
triggerVoiceProfileCleanupWithCompletion:
pruneImplicitUtterancesOfProfile:withAsset:
markSATEnrollmentSuccessForVoiceProfile:
isSATEnrolledForSiriProfileId:forLanguageCode:
isSATEnrollmentMigratedForSiriProfileId:forLanguageCode:
deleteAllVoiceProfilesForAppDomain:
currentDeviceCategory
xpcClient
setXpcClient:
_currentDeviceCategory
_xpcClient
handleFailureInFunction:file:lineNumber:description:
addSamples:numSamples:atHostTime:
stringWithCString:encoding:
createAudioCircularBufferWithDefaultSettings
copySamplesFromHostTime:
copybufferFrom:to:
copyBufferWithNumSamplesCopiedIn:
saveRecordingBufferFrom:to:toURL:
bufferLength
setBufferLength:
.cxx_construct
_csAudioCircularBufferImpl
_bufferLength
numberWithUnsignedLong:
initWithZeroWindowSize:approxAbsSpeechThreshold:numHostTicksPerAudioSample:
filterZerosInAudioPacket:atBufferHostTime:filteredPacket:
endAudioAndFetchAnyTrailingZerosPacket:
metrics
_audioZeroFilterImpl
endpointStyle
delay
setDelay:
startWaitTime
automaticEndpointingSuspensionEndTime
setAutomaticEndpointingSuspensionEndTime:
minimumDurationForEndpointer
setMinimumDurationForEndpointer:
lastEndOfVoiceActivityTime
lastStartOfVoiceActivityTime
bypassSamples
setBypassSamples:
endpointMode
setEndpointMode:
interspeechWaitTime
endWaitTime
saveSamplesSeenInReset
setSaveSamplesSeenInReset:
stopEndpointer
recordingStoppedForReason:
trailingSilenceDurationAtEndpoint
implDelegate
setImplDelegate:
canProcessCurrentRequest
activeChannel
setActiveChannel:
processServerEndpointFeatures:
updateEndpointerThreshold:
updateEndpointerDelayedTrigger:
shouldAcceptEagerResultForDuration:resultsCompletionHandler:
handleVoiceTriggerWithActivationInfo:
endpointerModelVersion
elapsedTimeWithNoSpeech
initWithRecordType:deviceId:
setAlwaysUseRemoteBuiltInMic:
contextForServerInvoke
recordTypeFromAVVCActivationMode:
setType:
setDeviceId:
_createAVVCContextWithType:deviceId:
avvcActivationMode:
deviceId
isBuiltInVoiceTriggered
isHearstVoiceTriggered
isJarvisVoiceTriggered
isHearstDoubleTapTriggered
recordTypeString:
contextForHearstVoiceTriggerWithDeviceId:
contextForRemoraVoiceTriggerWithDeviceId:
contextForOpportuneSpeakerListener
contextForBuiltInVoiceTrigger
contextForJarvisWithDeviceId:
contextForBTLEWithDeviceId:
contextForVoiceTriggerTraining
contextForHomeButton
defaultContext
initWithAVVCContext:
isVoiceTriggered
isTriggeredFromHearst
isRTSTriggered
isServerInvoked
isStarkTriggered
isDictation
alwaysUseRemoteBuiltInMic
_alwaysUseRemoteBuiltInMic
_type
_deviceId
_latestVersionedAssetOfType:fromProviders:forLocale:
addObjectsFromArray:
scannerWithString:
scanFloat:
_convertVersionStringToFloat:
assetProviders
setAssetProviders:
currentLanguageCode
setCurrentLanguageCode:
_assetProviders
_currentLanguageCode
orchestrator
setOrchestrator:
setLastScoreCard:
_orchestrator
_lastScoreCard
supportsSpeakerRecognitionAssets
_installedMobileAssetOfType:forLanguage:
_buildAssetQueryForAssetType:
returnTypes:
queryMetaDataSync
results
_filteredAssets:forLanguage:
queryParams
_getSSRAssetTypeString
_getSSRAssetCurrentCompatibilityVersion
_getVoiceTriggerAssetTypeString
_getVoiceTriggerAssetCurrentCompatibilityVersion
_getEndpointAssetTypeString
_getEndpointAssetCurrentCompatibilityVersion
initWithType:
addKeyValuePair:with:
_findLatestInstalledAsset:
valueForKey:
initStore
_loadVoiceProfiles
_updateTrainedUsersWithAction:UserVoiceProfile:
_deleteUserVoiceProfile:
defaultCenter
postNotificationName:object:
_synchronizeSiriVoiceProfilesWithAssistant
_checkIfRetrainingRequiredForProfile:
satImplicitProfileThreshold
satImplicitProfileDeltaThreshold
evaluateImplicitAdditionPolicyWithScores:forProfile:withImplicitThreshold:withDeltaThreshold:
_logVoiceProfileConfusionForAsset:withCleanup:
_getTopScoringProfileIdFromScores:
_retrainVoiceProfile:withContext:
_retrainLiveOnOnboardedProfilesForLanguage:withForceRetrain:withCompletion:
_enrolledVoiceProfiles
loadKnownUserVoiceProfiles
_saveTrainedUsers:
setVoiceProfileStoreVersion:
saveKnownUserVoiceProfiles:
_retrainVoiceProfile:withContext:withUtterances:
_isImplicitUtterance:
logVoiceProfileConfusionWithCleanup:
_prepareVoiceProfileAtPath:forImportToProfile:
copyAudioFiles:toProfile:forModelType:
voiceProfileArray
setVoiceProfileArray:
storePrefs
setStorePrefs:
_voiceProfileArray
_storePrefs
iterateBitset:block:
selectedChannelList
initWithInASBD:outASBD:
audioDecoderDidDecodePackets:audioStreamHandleId:buffer:remoteVAD:timestamp:receivedNumChannels:
opusDecoder
speexDecoder
addPackets:audioStreamHandleId:remoteVAD:timestamp:receivedNumChannels:
_decoder
_inASBD
availableDevices
deviceWithDescriptor:error:
_getLastBiometricMatchEvent:atTime:
lastMatchEventWithError:
result
device:matchEventOccurred:
biometricDevice
setBiometricDevice:
_biometricDevice
saveMetadata:isExplicitEnrollment:
saveUtterance:utteranceAudioPath:numSamplesToWrite:isExplicitEnrollment:
writeMetaDict:atMetaPath:
saveRawUtteranceAndMetadata:to:isExplicitEnrollment:
saveUtteranceAndMetadata:atDirectory:isExplicitEnrollment:
pickAssetForProfiles:forSpIdType:
psrCombinationWeight
composeModelContextsForProfiles:forSpIdType:forAsset:completion:
initWithConfigFilePath:withModelFilePaths:
_checkIfModelsPresentForProfiles:forSpIdType:forAsset:
setSpIdType:
expModelsContext
_combinationWeight
_activeChannel
_scoreType
_recognitionStyle
_vtEventInfo
_vadResourcePath
_expModelsContext
_maxAllowedAudioSamples
_debugUtteranceAudioFile
_debugUtteranceMetaFile
voiceProfilesModelFilePaths
_voiceProfilesModelFilePaths
createGrammars
bundleForClass:
bundlePath
_getTrailingPatternsWithGrammars:withLocale:
_getLeadingPatternsWithGrammars:withLocale:
_getRegexPatternsWithGrammars:withUtt:withLocale:
_getLMEWithGrammar:withLocale:
URLSession:didBecomeInvalidWithError:
URLSession:didReceiveChallenge:completionHandler:
URLSessionDidFinishEventsForBackgroundURLSession:
_grammar
supportRaiseToSpeak
supportHearstVoiceTrigger
supportPremiumWatchAssets
shouldRunVTOnCS
channelForOutputReference
supportTTS
supportJarvisVoiceTrigger
supportBluetoothDeviceVoiceTrigger
rootQueueWithFixedPriority:
supportHybridEndpointer
csAudioProcessingQueuePriority
characterSetWithCharactersInString:
componentsSeparatedByCharactersInSet:
supportContinuousVoiceTrigger
supportKeywordDetector
supportPremiumAssets
supportOpportunisticZLL
supportSelfTriggerSuppression
supportCSTwoShotDecision
supportSmartVolume
supportSAT
supportCompactPlus
supportAdBlocker
supportContinuousAudioFingerprint
supportPhatic
shouldDelayPhaticForMyriadDecision
supportSessionActivateDelay
supportLanguageDetector
shouldDownloadVTAssetsOnDaemon
supportLazySessionActivation
hasRemoteCoreSpeech
supportRemoraVoiceTrigger
shouldDeinterleaveAudioOnCS
supportCircularBuffer
supportBeepCanceller
supportZeroFilter
getFixedPrioritySerialQueueWithLabel:fixedPriority:
systemUpTime
deviceUserAssignedName
deviceHwRevision
getVoiceTriggerProfilesAESKey
generateIfNecessaryVoiceTriggerProfilesAESKey
generateIfNecessaryAESKeyWithKeySizeInBits:applicationTag:keyLabel:shouldGenerateIfNecessary:
generateAESKeyWithKeySizeInBits:
storeAESKeyInKeychain:applicationTag:keyLabel:
getAESKeyFromKeychainWithApplicationTag:keyLabel:
deleteAESKeyWithApplicationTag:keyLabel:
getKeychainAttributesForAESKeyWithApplicationTag:keyLabel:
hearstNumberOfBytesPerChunk
hearstNumberOfSamplesPerChunk
EncryptionAudioSampleByteDepth
inputRecordingEncoderAudioQuality
inputRecordingSampleRateConverterAlgorithm
inputRecordingBufferDuration
zeroFilterWindowSizeInMs
zeroFilterWindowSizeInMsForReport
zeroFilterApproxAbsSpeechThreshold
daysBeforeRemovingLogFiles
remoteVADDuration
serverLoggingChannelBitset
continousFingerprintBufferDuration
containsSpeakerRecognitionCategory
satScoreThreshold
multiUserLowScoreThreshold
multiUserHighScoreThreshold
multiUserConfidentScoreThreshold
multiUserDeltaScoreThreshold
hasRemoteBuiltInMic
_dispatchGroup
_dispatchGroupCounter
@24@0:8@16
@16@0:8
v24@0:8@16
f16@0:8
v20@0:8f16
Q16@0:8
v24@0:8Q16
I16@0:8
v20@0:8I16
v16@0:8
@"NSArray"
B24@0:8@16
#16@0:8
@24@0:8:16
@32@0:8:16@24
@40@0:8:16@24@32
B16@0:8
B24@0:8#16
B24@0:8:16
Vv16@0:8
^{_NSZone=}16@0:8
B24@0:8@"Protocol"16
@"NSString"16@0:8
B32@0:8r^v16Q24
@104@0:8@16{AudioStreamBasicDescription=dIIIIIIII}24{AudioStreamBasicDescription=dIIIIIIII}64
B32@0:8r^v16q24
v32@0:8@16@24
^{OpaqueExtAudioFile=}
{AudioStreamBasicDescription="mSampleRate"d"mFormatID"I"mFormatFlags"I"mBytesPerPacket"I"mFramesPerPacket"I"mBytesPerFrame"I"mChannelsPerFrame"I"mBitsPerChannel"I"mReserved"I}
@"NSURL"
v20@0:8B16
v24@0:8q16
q16@0:8
d16@0:8
d24@0:8d16
@24@0:8q16
B20@0:8B16
B28@0:8B16^{__CFString=}20
B24@0:8^{__CFString=}16
v36@0:8^f16Q24f32
v24@0:8@"EARClientSilenceFeatures"16
@32@0:8@16@24
v32@0:8@16Q24
@"SSRSpeakerRecognitionContext"
@"<SSRVoiceActivityDetectorDelegate>"
@"EARCaesuraSilencePosteriorGenerator"
@"_EAREndpointer"
@"CSServerEndpointFeatures"
@"NSObject<OS_dispatch_queue>"
B24@0:8d16
@20@0:8I16
@"NSObject<OS_dispatch_source>"
@"<CSAudioFileReaderDelegate>"
@96@0:8{AudioStreamBasicDescription=dIIIIIIII}16{AudioStreamBasicDescription=dIIIIIIII}56
@104@0:8{AudioStreamBasicDescription=dIIIIIIII}16{AudioStreamBasicDescription=dIIIIIIII}56@96
@24@0:8Q16
@112@0:8@16@24{AudioStreamBasicDescription=dIIIIIIII}32{AudioStreamBasicDescription=dIIIIIIII}72
v24@0:8@"<CSVTUIAudioSessionDelegate>"16
v24@0:8@"<Endpointer>"16
v28@0:8@16B24
v36@0:8@16B24@28
v32@0:8@16q24
v28@0:8@16i24
v36@0:8@16i24d28
v36@0:8@16i24@28
v40@0:8@16@24@32
v28@0:8B16@20
v44@0:8@16Q24B32@36
v40@0:8@16Q24q32
v40@0:8@16Q24@32
v28@0:8@"AVVoiceController"16B24
v36@0:8@"AVVoiceController"16B24@"NSError"28
v32@0:8@"AVVoiceController"16q24
v24@0:8@"AVVoiceController"16
v28@0:8@"AVVoiceController"16i24
v36@0:8@"AVVoiceController"16i24d28
v32@0:8@"AVVoiceController"16@"NSError"24
v36@0:8@"AVVoiceController"16i24@"NSError"28
v40@0:8@"AVVoiceController"16@"AVVCAlertInformation"24@"NSError"32
v32@0:8@"AVVoiceController"16@"NSDictionary"24
v32@0:8@"AVVoiceController"16@"AVVCAudioBuffer"24
v28@0:8B16@"NSArray"20
v44@0:8@"AVVoiceController"16Q24B32@"NSError"36
v40@0:8@"AVVoiceController"16Q24q32
v40@0:8@"AVVoiceController"16Q24@"AVVCAudioBuffer"32
v32@0:8@"AVVoiceController"16Q24
q24@0:8q16
@"AVVoiceController"
@"<CSVTUIAudioSessionDelegate>"
@40@0:8@16@24@32
v32@0:8@"SSRSpeakerAnalyzerPSR"16@"NSDictionary"24
@32@0:8@"SSRSpeakerRecognitionContext"16@"<SSRSpeakerRecognizerDelegate>"24
v32@0:8@"NSData"16Q24
v24@0:8@"SSRSpeakerRecognitionContext"16
@"NSDictionary"16@0:8
@"NSString"
@"NSDictionary"
@"<SSRSpeakerRecognizerDelegate>"
@"SSRSpeakerAnalyzerPSR"
@72@0:8@16Q24Q32Q40Q48Q56@64
@28@0:8f16Q20
@40@0:8Q16Q24Q32
@32@0:8Q16Q24
v40@0:8Q16Q24@?32
@"NSData"
@20@0:8f16
v20@0:8i16
v32@0:8r^s16i24i28
v32@0:8r^f16i24i28
v28@0:8i16i20i24
v24@0:8^d16
@"CSAudioUnitMeterClipping"
@40@0:8@16@24Q32
@32@0:8@16Q24
v32@0:8@"<SSRSpeakerRecognizer>"16@"NSDictionary"24
v32@0:8@"SSRVoiceActivityDetector"16Q24
@40@0:8@16@24^@32
@40@0:8@16@24d32
@"<SSRSpeakerRecognitionOrchestratorDelegate>"
@"<CSAudioFileWriter>"
@"<SSRSpeakerRecognizer>"
@"SSRVoiceActivityDetector"
v40@0:8@16@?24@?32
@24@0:8@?16
@24@0:8@"SSRVoiceProfileRetrainingContext"16
v40@0:8@"NSArray"16@?<@"NSError"@?@"NSURL"@"NSDictionary">24@?<v@?@"NSError"@"NSDictionary"@"NSDictionary">32
B24@0:8@"NSArray"16
@"NSError"24@0:8@?<B@?@"NSDictionary">16
@"NSURL"16@0:8
v24@0:8d16
@"CSAudioRecordContext"
v24@0:8@?16
{AudioStreamBasicDescription=dIIIIIIII}16@0:8
{AudioStreamBasicDescription=dIIIIIIII}24@0:8f16I20
@48@0:8@16@24@32Q40
@56@0:8@16@24@32@40Q48
f40@0:8@16Q24Q32
B44@0:8@16@24@32B40
B44@0:8@"CSVTUITrainingSession"16@"NSData"24@"NSString"32B40
v28@0:8B16@"NSError"20
v24@0:8@"NSData"16
v24@0:8@"NSError"16
v32@0:8@16d24
v40@0:8@16d24@32
v32@0:8@"<CSEndpointAnalyzer>"16d24
v40@0:8@"<CSEndpointAnalyzer>"16d24@"CSEndpointerMetrics"32
q36@0:8q16B24@?28
B24@0:8q16
v32@0:8i16B20@?24
@"<CSVTUIAudioSession>"
@"CSNNVADEndpointAnalyzer"
@"CSVTUIKeywordDetector"
@"NSMutableArray"
@"CSVTUITrainingSession"
@"SFSpeechRecognizer"
@"CSAsset"
@"SSRVoiceProfile"
@"<SSRVTUITrainingManagerDelegate>"
@"NSObject<OS_os_transaction>"
@24@0:8^{_NSZone=}16
v24@0:8@"NSCoder"16
@24@0:8@"NSCoder"16
@44@0:8@16B24@28@36
@"NSUUID"
@48@0:8@16@24@32@40
@"NSNumber"
@"NSDate"
v32@0:8@16@?24
@"SSRTriggerPhraseDetectorNDAPI"
@"SSRTriggerPhraseDetectorQuasar"
B32@0:8d16^@24
@"<CSRemoteControlClientDelegate>"
@40@0:8Q16@24@32
@48@0:8Q16@24@32Q40
B36@0:8@16@24B32
B32@0:8@16@24
Q24@0:8Q16
Q24@0:8@16
v80@0:8@16{AudioStreamBasicDescription=dIIIIIIII}24Q64@?72
q24@0:8@16
v44@0:8@16@24B32@?36
@32@0:8Q16@24
@"CSAsset"32@0:8Q16@"NSString"24
@"NSArray"32@0:8Q16@"NSString"24
Q24@0:8I16I20
@"CSAudioCircularBuffer"
v28@0:8B16Q20
B32@0:8@16^@24
B24@0:8^@16
@"<CSRemoteRecordClientDelegate>"
B40@0:8@16@24@32
@"NSMutableDictionary"
Q20@0:8f16
f24@0:8Q16
d24@0:8Q16
Q40@0:8Q16Q24Q32
v64@0:8@16@24@32@40@48@?56
v40@0:8@16@24@?32
@"NSObject<OS_xpc_object>"
@32@0:8@16q24
@40@0:8@16Q24q32
B32@0:8Q16Q24
@"_EARSyncSpeechRecognizer"
@32@0:8@16d24
@72@0:8q16q24d32@40d48@56q64
@64@0:8q16q24d32@40d48@56
v28@0:8f16@20
v32@0:8Q16Q24
v32@0:8@"SSRSpeakerRecognitionController"16@"NSDictionary"24
v56@0:8@16@24@32@40@?48
@32@0:8@16^@24
@"SSRLoggingAggregator"
v64@0:8@16@24B32@36@44Q52B60
q48@0:8@16@24@32@40
v24@0:8@"SFSpeechRecognitionTask"16
v32@0:8@"SFSpeechRecognitionTask"16@"SFTranscription"24
v32@0:8@"SFSpeechRecognitionTask"16@"SFSpeechRecognitionResult"24
v28@0:8@"SFSpeechRecognitionTask"16B24
v24@0:8i16B20
@96@0:8q16q24@32@40@48@56@64@72@80@?88
@"SFSpeechAudioBufferRecognitionRequest"
@"SFSpeechRecognitionTask"
@"NSTimer"
@"<CSVTUITrainingSessionDelegate>"
v48@0:8@16@24@32@?40
v72@0:8@16@24@32@40@48@56@?64
@32@0:8@16@?24
v48@0:8@16Q24@?32@?40
@40@0:8@16Q24Q32
@24@0:8^@16
@"CSVoiceIdXPCClient"
@32@0:8Q16f24f28
v32@0:8r^v16Q24
v40@0:8r^v16Q24Q32
@24@0:8^Q16
v40@0:8Q16Q24@32
{unique_ptr<corespeech::CSAudioCircularBufferImpl<unsigned short>, std::__1::default_delete<corespeech::CSAudioCircularBufferImpl<unsigned short> > >="__ptr_"{__compressed_pair<corespeech::CSAudioCircularBufferImpl<unsigned short> *, std::__1::default_delete<corespeech::CSAudioCircularBufferImpl<unsigned short> > >="__value_"^{CSAudioCircularBufferImpl<unsigned short>}}}
@36@0:8Q16S24d28
Q40@0:8@16Q24^@32
Q24@0:8^@16
{unique_ptr<CSAudioZeroFilterImpl<unsigned short>, std::__1::default_delete<CSAudioZeroFilterImpl<unsigned short> > >="__ptr_"{__compressed_pair<CSAudioZeroFilterImpl<unsigned short> *, std::__1::default_delete<CSAudioZeroFilterImpl<unsigned short> > >="__value_"^{CSAudioZeroFilterImpl<unsigned short>}}}
v40@0:8Q16@24@32
v32@0:8d16@?24
v40@0:8Q16@"NSDictionary"24@"NSDictionary"32
v24@0:8@"CSAudioChunk"16
@"<CSEndpointAnalyzerDelegate>"16@0:8
v24@0:8@"<CSEndpointAnalyzerDelegate>"16
@"<CSEndpointAnalyzerImplDelegate>"16@0:8
v24@0:8@"<CSEndpointAnalyzerImplDelegate>"16
v24@0:8@"CSServerEndpointFeatures"16
v32@0:8d16@?<v@?B@"NSArray">24
v24@0:8@"NSDictionary"16
@"<CSEndpointAnalyzerDelegate>"
@"<CSEndpointAnalyzerImplDelegate>"
@32@0:8q16@24
f24@0:8@16
@"<SSRAssetManagerDelegate>"
v32@0:8@"SSRSpeakerRecognitionOrchestrator"16@"NSDictionary"24
@"<SSRSpeakerRecognitionControllerDelegate>"
@"SSRSpeakerRecognitionOrchestrator"
B40@0:8@16@24f32f36
B40@0:8@16@24Q32
v32@0:8Q16@24
v36@0:8@16B24@?28
@"SSRVoiceProfileStorePrefs"
@112@0:8@16{AudioStreamBasicDescription=dIIIIIIII}24{AudioStreamBasicDescription=dIIIIIIII}64Q104
v52@0:8@16Q24@32Q40I48
^{OpaqueAudioConverter=}
@"<CSAudioDecoderDelegate>"
v32@0:8@"BKDevice"16@"BKMatchEvent"24
B32@0:8^B16^Q24
@"BKDevice"
v36@0:8@16@24B32
B44@0:8@16@24Q32B40
B28@0:8@16B24
v48@0:8@16Q24@32@?40
B40@0:8@16Q24@32
v32@0:8@"NSURLSession"16@"NSError"24
v40@0:8@"NSURLSession"16@"NSURLAuthenticationChallenge"24@?<v@?q@"NSURLCredential">32
v24@0:8@"NSURLSession"16
@40@0:8@16q24@32
@20@0:8i16
@28@0:8@16i24
@44@0:8Q16@24@32B40
S16@0:8
i16@0:8
q24@0:8Q16
@"NSObject<OS_dispatch_group>"
mcpl
333333
333333
