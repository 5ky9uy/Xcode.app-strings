VoiceServicesErrorDomain
Synthesis is cancelled/interrupted.
writeWaveToFilePath failed.
v8@?0
@"NSError"16@?0@"VSSpeechSynthesisCallbackResult"8
v16@?0@"VSSpeechWordTimingInfo"8
VSAudioPlaybackServiceAVSBARQueue
Error in creating block buffer for Sample buffer
Error in CMAudioFormatDescriptionCreate
Error in CMAudioSampleBufferCreateWithPacketDescriptions
Error in creating block buffer for Silence buffer
Error in CMAudioFormatDescriptionCreate from Silence buffer creation
Error in CMAudioSampleBufferCreateWithPacketDescriptions from silence buffer
{?=qiIq}
Timeout waiting for AVSampleBufferRenderSynchronizer
q24@?0@"NSValue"8@"NSValue"16
v32@?0@"NSValue"8Q16^B24
cached_engine_%@_%@
Use `initWithRequest:`.
%@ %@ %@ %@ %.2f %.2f %.2f %@ %@
Speech is cancelled/interrupted.
v20@?0B8@"NSString"12
ServerTTSErrorDomain
Use `initWithRequest:shouldSpeak:`.
%@ %@
v40@?0@"NSData"8Q16@"NSData"24^B32
Unable to create playback service
EagerTTS:%@:%@:%@:%@:%@:%@:%@:%@:%@
Cancelled
Finished
speaking
synthesizing
voice
(null)
is_eager
is_one_shot
is_time_out
is_device_tts
source_of_tts
com.apple.springboard
com.apple.siri
com.apple.CarPlayApp
com.apple.Carousel
com.apple.AssistantServices
com.apple.SiriHeadlessService
\mrk=emo=whisper\
com.apple.voiced.can-dump-audio
com.apple.voiced.request.durationestimation
com.apple.voiced.request.speech
com.apple.voiced.request.presynthesis
unknown:unknown:PresynthesizedAudio
unknown:PresynthesizedAudio:unknown
com.apple.voiced.request.synthesis
com.apple.voiced.request.text_to_phonemes
VSAudioPowerUpdateQueue
-[VSSpeechXPCHandler beginAudioPowerUpdateWithReply:]
v16@?0@"AFXPCWrapper"8
-[VSSpeechXPCHandler endAudioPowerUpdate]
%@:%@:%@:%@
Missing languageCode
-[VSSpeechXPCHandler getVoiceInfoForLanguageCode:name:footprint:gender:type:reply:]
VSOspreyTTSCoreCallbackQueue
v24@?0@"VSAudioData"8@"NSArray"16
v16@?0@"NSError"8
Osprey round-trip TTS timed out
v28@?0@"VSVoiceAsset"8@"VSVoiceResourceAsset"16f24
Osprey streaming network stall
Osprey streaming TTS timed out
Osprey core is not able to provide audio in time
com.apple.voiced.postInstall
v20@?0f8^B12
com.apple.voiced.neural-compiling
v12@?0i8
v16@?0@"NSObject<OS_xpc_object>"8
com.apple.MobileAsset.VoiceServicesVocalizerVoice
com.apple.MobileAsset.VoiceServices.CustomVoice
com.apple.MobileAsset.VoiceServices.GryphonVoice
com.apple.MobileAsset.VoiceServices.VoiceResources
com.apple.voiced.voicePreviewQueue
com.apple.voiceservices
VoicePreviews
_Buddy
%@_%@%@.caf
v20@?0d8B16
InvalidCache
Audio duration too short
duration %.2f second
TTSResources/PreinstallCache/
VSSpeechCacheErrorDomain
-[VSSpeechCache initWithStorePath:]
VoiceServices
Cache type name too long
-[VSSpeechCache addCache:]
%@_%@
%@:%@:%@:%@:%@:%@
gryphon
unknown
premium
%@:%@:%@
preinstalledCache
FlatBuffers 1.12.0
v24@?0^v8Q16
resource
context_info
meta_info
context
experiment
feature_flags
decoder_description
playback_description
word_timing_info
v20@?0r*8I16
content
Verifier
flatbuffers.h
size_ < FLATBUFFERS_MAX_BUFFER_SIZE
NotNested
!nested
!num_field_loc
ensure_space
cur_ >= scratch_ && scratch_ >= buf_
size() < FLATBUFFERS_MAX_BUFFER_SIZE
reallocate_downward
new_size > old_size
EndTable
nested
table_object_size < 0x10000
!ReadScalar<voffset_t>(buf_.data() + field_location->id)
data
cur_
scratch_end
scratch_
scratch_data
buf_
finished
ReferTo
off && off <= GetSize()
EndVector
Finish
strlen(file_identifier) == kFileIdentifierLength
allocator<T>::allocate(size_t n) 'n' exceeds maximum supported size
'%c%c%c%c', %.0fhz, %d bits, %d FPP, 
%@:%@
voiced_tts_playback_queue
Error AudioQueueStart
Error AudioQueueFlush
Error AudioQueueStop
Error AudioQueuePause
Error AudioQueueAddPropertyListener
Error AudioQueueRemovePropertyListener
YYYY-MM-dd hh:mm:ss:SSS
Error AudioQueueGetProperty isRunning
Unable to enable kAudioQueueProperty_EnableLevelMetering
Unable to disable kAudioQueueProperty_EnableLevelMetering
-[VSAudioPlaybackServiceAT getAveragePower:andPeakPower:]
Unable to get kAudioQueueProperty_CurrentLevelMeterDB
hdft
Hdft
usbD
hx90
wx90
rhac
wdef
com.apple.voiceservices.notification.voice-update
Missing utterance in the request (preprocessing missing?).
No voice available
Compact voice is explicitly disabled.
Voice is deleted already.
Can't create VSSpeechEngine
CACHE_DELETE_AMOUNT
CACHE_DELETE_VOLUME
com.apple.voiced.CacheDelete
r^{__CFDictionary=}20@?0i8r^{__CFDictionary=}12
com.apple.voiced.downloadQueue
on accessory %@
q24@?0@"VSVoiceAsset"8@"VSVoiceAsset"16
v20@?0d8f16
https://dejavu.apple.com
https://seed-dejavu.siri.apple.com
https://carry-dejavu.siri.apple.com
/siri.speech.qss_fb.Blazar/TextToSpeechRouter
v16@?0@"OspreyMutableRequest"8
-[OspreyTTSService roundTripTTS:responseHandler:]_block_invoke
Empty data
Invalid data
Error %d in response: %@
v24@?0@"NSData"8@"NSError"16
/siri.speech.qss_fb.Blazar/TextToSpeechRouterStreaming
Corrupted Osprey response.
-[OspreyTTSService streamTTS:beginHandler:chunkHandler:endHandler:completion:]_block_invoke
v16@?0@"NSData"8
/private/var/mobile/Library/Logs/CrashReporter/VoiceServices/
mobile
-[VSDiagnosticService createDirectoryIfNeeded]
yyyy_MM_dd-HHmmss.SSS
TTS-%@
.tmp
.wav
TTSMetrics-%lld
.json
default
ServerTTSTimeoutV2
DeviceWaitTimeV2
TTSExperimentConfig
identifier
method
delayed
AllowedAppId
com.apple.MapsSupport
com.apple.Translate
com.apple.SessionTrackerApp
Use `initWithRequest:withStreamID:`.
Unknown inline streaming error %d, %@
Missing utterance in the request (preprocessing missing?). Can't fallback to device TTS.
voice_resource
task: inprogress %@, request: %@
Can't create VSAudioPlaybackService
Can't decode audio data
audio_duration
com.apple.voiced.VSInlineStreamService
fe_feature
fe_feature_only
language
gender
name
version
quality
type
channel_type
app_id
dialog_identifier
experiment_identifier
speech_id
session_id
text
audio_type
enable_word_timing_info
voice_name
preferred_voice_type
value
sample_rate
format_id
format_flags
bytes_per_packet
frames_per_packet
bytes_per_frame
channels_per_frame
bits_per_channel
reserved
word
sample_idx
offset
length
timestamp
error_code
error_str
audio
stream_id
streaming_playback_buffer_size_in_seconds
current_pkt_number
total_pkt_number
content_type
com.apple.voiceservices.notification.voice-purge
com.apple.voiced.prewarmQueue
Prewarm textify emoji
gryphon_frontend
VoiceServices/config
-[VSServerTTSClient ospreyStartSynthesisRequest:responseHandler:completion:]_block_invoke
Unable to process audio data.
v24@?0@"OPTTSTextToSpeechResponse"8@"NSError"16
v16@?0@"OPTTSBeginTextToSpeechStreamingResponse"8
v16@?0@"OPTTSPartialTextToSpeechStreamingResponse"8
v16@?0@"OPTTSFinalTextToSpeechStreamingResponse"8
rate
pitch
volume
isEager
neuralIssue
com.apple.voiced.cachingQueue
v32@?0@"NSData"8Q16@"NSData"24
Speaker
CarAudioOutput
com.apple.voiced.pthreadQueue
com.apple.voiced.speakingQueue
audioDataFromFile:error:
AudioFileOpenURL
AudioFileGetProperty kAudioFilePropertyDataFormat
AudioFileGetProperty kAudioFilePropertyAudioDataByteCount
AudioFileGetProperty kAudioFilePropertyAudioDataPacketCount
+[VSAudioData(SAUIAudioData) audioDataWithASBD:rawData:]
male
female
neutral
\vol=%d\%@
\rate=%d\%@
\pitch=%d\%@
SynthesisTask done synthesize %lu characters, audio duration %f, error %@
Task %llu reported word time info
Device SpeakTask %llu: Instrument metric: %@
Task %llu started speaking
Device EagerTask %llu: Instrument metric: %@
Task %llu reported finish, error: %@
Can't retrieve session with ID: %d
#AVSBAR initialized with session ID: %d, reusing previous synchronizer: %{BOOL}d
VSAudioPlaybackService %p init latency: %.3f
mediaserverd reset
#AVSBAR synchronizer.rate will be set to 1 with enqueued audio duration %f sec. Previous rate: %f
#AVSBAR already stopped or paused: will not resume rate
_synchronizer play rate high latency: %.3f sec
#AVSBAR synchronizer.rate was set to 1. Current rate: %f
Invalid sample buffer
Invalid silence buffer
Error in creating block buffer for Sample buffer
Error in CMAudioFormatDescriptionCreate
Error in CMAudioSampleBufferCreateWithPacketDescriptions
#AVSBAR already stopped or waiting for finish: will not enqueue more
#AVSBAR empty audio data: will not enqueue it
Adding to enqueuedMappedAudioInfo: %f sec
Error in creating block buffer for Silence buffer
Error in CMAudioFormatDescriptionCreate from Silence buffer creation
Error in CMAudioSampleBufferCreateWithPacketDescriptions from silence buffer
#AVSBAR EndOfDataAttachment ready for enqueuing
#AVSBAR Call to provide more audio data during state %ld.
#AVSBAR Enqueuing to %@: %f sec
_renderer enqueueSampleBuffer high latency: %.3f sec
#AVSBAR Renderer %@ not anymore ready for more media data. enqueuedMappedAudioInfo count left: %lu
#AVSBAR flushAndStop
#AVSBAR already stopped or waiting for finish
#AVSBAR Synchronizer reached endTime
#AVSBAR Waiting for synchronizer finishing playing between current %f sec and until %f sec
#AVSBAR Synchronizer is stalled with rate %f at time %f.
Stopping synchronizer and renderer
#AVSBAR synchronizer.rate will be set to 0 and time set to 0 (from current time: %f). Then renderer will be flushed.
_synchronizer stop rate high latency: %.3f sec
#AVSBAR synchronizer.rate was set to 0. Current rate: %f
#AVSBAR renderer was flushed
Pausing synchronizer
#AVSBAR synchronizer.rate will be set to 0 (at current time: %f).
_synchronizer pause rate high latency: %.3f sec
#AVSBAR Dropping %lu enqueued data
Error AudioUnitSetProperty _floatConverter %@
Error AudioUnitSetProperty _integerConverter %@
Error AudioComponentInstanceNew _voiceBoostUnit %@
Error AudioUnitSetProperty _voiceBoostUnit, kAudioUnitProperty_MaximumFramesPerSlice %@
Error AudioUnitSetProperty _voiceBoostUnit, kAudioUnitProperty_StreamFormat, kAudioUnitScope_Input %@
Error AudioUnitSetProperty _voiceBoostUnit, kAudioUnitProperty_StreamFormat, kAudioUnitScope_Output, %@
Error AudioUnitInitialize _voiceBoostUnit %@
Error AudioUnitSetParameter %@
Error AudioConverterConvertComplexBuffer _floatConverter %@
Error AudioUnitProcess _voiceBoostUnit %@
Error AudioConverterConvertComplexBuffer _integerConverter %@
Using timestamp inside voiced for Estimation task
Created Estimation task %llu
Unable to create engine for request %@
Estimated duration: %.2f, for utterance: %@
Using timestamp inside voiced for task
Created Task %llu (%p)
Starting speech task %llu
Short-term cached synthesis is found for text '%@'
Detected synthesis stall, starting tailspin
Finished tail spin, success:%d, file: %@
Holding audio playback before we get fast synthesis.
SpeakTask done synthesize %lu characters, audio duration %f, error %@
Device task %llu: Instrument metric: %@
Using requestCreatedTimestamp inside voiced for Server task
Created Server task %llu: shouldSpeak %{BOOL}d
Received server TTS response. Use Server TTS.
Received device synthesis previously, ignore server TTS.
Encountered Osprey streaming network stall. Retry with device TTS.
Server network error: %@
Start device synthesis fallback.
Start playing device synthesis instead.
Received server TTS previously, ignore device TTS
Received audio from device synthesis. Use device synthesis immediately.
Received audio from device synthesis, but it's deferred.
Inline server TTS is previously cached.
Eager server TTS is previously cached.
Device TTS is racing with Server TTS
Device TTS wait time for server audio: %.2f
Device TTS will not race
Server task %llu started speaking
Server task %llu: Instrument metric: %@
Error in server task %llu, error: %@
Server task %llu: %@ %@ utterance: '%@', %{public}@
Invalidate VSSpeechXPCHandler, cancelling all related tasks
%{public}@ is not TTS language, fallback to %{public}@
Use SSML input: %@
Utterance to synthesize for request %llu: '%@'
Overwriting volume with internal default: %.3f
Overwriting rate with internal default: %.3f
Overwriting pitch with internal default: %.3f
Process is not entitled for dumping audio. Ignore outputPath
Unexpected client '%{public}@' sets Siri request ID.
Overriding disableDeviceRacing with internal default
Update with connection identifier: %{public}@, keepActive:%{BOOL}d
Keep active session for '%@'
Remove active session for '%@'
Find on-going task: %@, ignoring prewarm request: %@
Server Inline Streaming TTS is disabled in internal settings
Server TTS is disabled in internal settings
Created stream speak task %llu
Created server speak task %llu
Created speak task %llu
Created presynthesized task %llu
Cache #PresynthesizedRequest %llu with text: %@
Cache #PresynthesizedRequest %llu skipped: no audio
Ignore stopPresynthesizedAudioRequest. Cannot find task with associated request %llu.
Created server synthesis task %llu
Created synthesis task %llu
Found matched inline streaming request, cancel synthesis task %llu
Ignore pauseSpeechRequest. Current request is different than requested request.
Ignore continueSpeechRequest. Current request is different than requested request.
Ignore stopSpeechRequest. Cannot find task with associated request %llu.
Created phonemes task %llu
ignored client '%{public}@' setting auto-download for a non-existing accessoryId '%@'
client '%{public}@' and accessory '%@' set auto download voice assets:%{public}@
Start cellular download for %@
Begin getVoiceInfoForLanguageCode: %{public}@, %@, %@
%s override voice info for server TTS platform, %@
End getVoiceInfoForLanguageCode: %@
Ignore stream object with nil stream ID: %@
Enqueue stream object %@, streamId: %@
Received invokeDaemon, I'm listening
Received killDaemon, shutting down
Simulate network stall is on, ignore audio object
Refresh timeout value as %.2f
Simulate network stall is on, ignore completion callback
Network stall in osprey streaming
Timeout in osprey streaming
Osprey core %p is cancelled
Registered xpc activity com.apple.voiced.postInstall
Running activity com.apple.voiced.postInstall
com.apple.voiced.postInstall is requested to be deferred.
Unable to set defer state for com.apple.voiced.postInstall
Assets migration progress: %f
Re-triggering neural compiling afer OS upgrade.
Triggered 'com.apple.voiced.neural-compiling' with error %d
Migration service finished.
xpc activity com.apple.voiced.postInstall, failed to set state to done.
Unexpected xpc activity state %d for 'com.apple.voiced.postInstall'
Defaults disables reset, skip resetting MobileAsset default URL
Resetting MobileAsset default URL
Unable to locate preview sample file at '%@'
StartVoicePreview for languageCode %@ voiceName %@ previewType %ld
Preview ignored for %@
Unable to play preview sample file at '%@'
StopVoicePreview for file '%@'
Reading cache %@ error: %@
Error %s, %@
Cache type name too long %@
{public}%@ is not TTS language, falling back to %{public}@
Error in reading audio data from file: %@ error:%@
Error AudioQueueNewOutputWithAudioSession %@
Unable to set kAudioQueueProperty_ClientUID, errno: %@
Error CMTimebaseCreateWithSourceClock: %@
AudioQueue initialized with session ID: %d
Error AudioQueueDispose %@
Signal AudioQueue running state change
Error AudioQueueStart %@
VSAudioPlaybackService %p success AudioQueueStart
Error AudioQueueAllocateBuffer %@
Audio queue start sample time: %.0f
Detected stalled audio generation, will enqueue %d silence frame to compensate.
Error AudioQueueEnqueueBuffer %@
VSAudioPlaybackService %p enqueued audio buffer at sample time: %.2f, size: %ld, total enqueued samples: %.0f, discontinuity: %{BOOL}d
AudioQueue will flushAndStop
Timeout in AudioQueue dequeue condition.
Error AudioQueueFlush %@
Error AudioQueueStop %@
AudioQueue will stop
Error AudioQueuePause %@
VSAudioPlaybackService %p success AudioQueuePause
Error AudioQueueAddPropertyListener %@
Error AudioQueueRemovePropertyListener %@
Detected stall of audio queue, based on NSDate. Now: %@, supposed end time: %@, Tolerance: %.2f
Error AudioQueueGetProperty isRunning %@
Unable to enable kAudioQueueProperty_EnableLevelMetering, err: %@
Unable to disable kAudioQueueProperty_EnableLevelMetering, err: %@
Error: %s, errno: %@
VSAudioPlaybackService %p played audio buffer at sample time: %f, size: %ld
Error AudioQueueFreeBuffer %@
Current audio output route: %@
AudioPlayback
Device core %p is cancelled
On-disk cached synthesis %@ is found.
In-memory cached synthesis %@ is found.
Reset MobileAsset query cache and retry selecting voice
No voice available
Compact voice is explicitly disabled.
Voice is deleted at path '%@'
#CacheDelete asset cleaning is disabled in internal setting. Skip purgeable assets for urgency %d
#CacheDelete purgeable active voice asset: %@
#CacheDelete purgeable inactive voice asset: %@
#CacheDelete query purgeable size, urgency: %d / %d, info: %@
#CacheDelete purge, urgency: %d / %d, info: %@
#CacheDelete periodic purge, urgency: %d / %d, info: %@
Asset update is disabled in internal settings.
Start updating voice and voice resources.
%@ %@ has a subscribed voice: %{public}@
Voice download is in progress, skip new download. %@
Updating target voice: %@
Voice update decision: shouldDownload:%d, canUseBattery:%d. Reason: triggerType:%d, compactVoiceSelected:%d, mismatchedVoiceName:%d, activeSiriUser:%d, serverExperimentDelay:%d
Downloaded voice is not ready to use. Start ANE compiling immediately for voice: %@
Voice asset downloading progress: %.2f, remainingTime: %.2f, voice: %@
Start ANE compiling immediately for voice: %@
Updating VoiceResource for '%{public}@'
Sent Osprey grpc request with speech_id '%@', session_id '%@', app_id '%@'
%s, Error: %@
Sent Osprey streaming request with speech_id '%@', session_id '%@', stream_id '%@', app_id '%@'
Corrupted Osprey response, stream ID: %@
Osprey streaming received Begin response with non 200 status: %d
Osprey streaming received Begin response %@
Osprey streaming received Chunk response with non 200 status: %d
Osprey streaming received Chunk response, pkt number: %d
Osprey streaming received End response with non 200 status: %d
Osprey streaming received End response, total pkt: %d
%s, Unknown response from Osprey for streaming TTS
Osprey streaming invokes completion with error %@
Osprey streaming invokes completion callback
Created audio dump directory %@
No compressed audio do dump
Unable to create intermediate audio dump at '%@'
Unable to create audio dump at '%@', error: %@
Audio save as %@
No audio do dump
No json data to dump
Unable to parse json for dictionary '%@', error: %@
Unable to create instrument metrics json dump at '%@'
Instrument metrics json dump saved as %@
Unable to parse json for key '%@', error: %@
JSON for key '%@' is not dictionary
Added short term cache:%@ for key:'%@'
Removed short term cache for key:'%@'
Removed short term cache for all keys
Using timestamp inside voiced for Stream task
Created Stream task %llu: streamID %@
Simulate network stall is on, ignore object %@
Unknown streaming object: %@
Handle stream begin with streamId: %@, text: %@, decoder: %@
Ignoring stream begin: error already occurred: %ld
Handle stream chunk with streamId: %@
Reached buffer threshold. Start playing audio.
Handle stream end with streamId: %@, count: %@
Stream TTS network stall.
Inline streaming TTS timeout.
Streaming error: %@, error_code: %d
Error in stream task %llu, error: %@
Stream task %llu: %@ speaking text: '%@', %{public}@
Stream task %llu: Instrument metric: %@
Initializing fallback playback service
Using timestamp inside voiced for Presynthesized task
Created Presynthesized Task %llu
Speaking pre-synthesized audio: %@
Error in audio task %llu, error: %@
Audio task %llu: %@ speaking utterance '%@', %{public}@
Received inline streaming TTS with id %@, text: %@
Notification for %@ is on-going. Posting object immediately %@
Notification for %@ has not started. Cache object %@
Start notifying for: %@
No cached object found for notification %@.
%d cached objects found for notification: %@
Notify %@ with cached object %@
Remove notification %@
Prewarming: Invoked with request: '%@'
Unable to initialize Device Authentication session: %@
Device Authentication session is initialized
Unable to prewarm, error: %@
Can't prewarm engine with path '%@'
Prewarm finished. Latency: %.3f
Prewarming: Completed with request: '%@'
Can't create engine with path '%@'
Voice specific resources found.
Specified resource file '%@' does not exist at: '%@'
disableServerTTS is enabled by user default, disable server TTS
forceServerTTS is enabled by user default, force server TTS
forceServerTTS is enabled by speech request, force server TTS
Preinstalled cache is found, disable server TTS
Neural voice is found on device without fallback condition, disable server TTS
Short term cache is found for the text, use server TTS
Server TTS is disabled since '%{public}@' is not in the list of allowed apps
%s, %@
Unable to get power policy from Siri, error: %@
playbackService is initialized already.
Can't create VSAudioPlaybackService
Starting AudioQueue
Task %llu fetched voice %@
Error in device task %llu, error: %@
Device task %llu: %@ %@ utterance: '%@', %{public}@
Cached streamAudio in task %llu with hash %@ in memory
Cached audio in task %llu with hash %@ in memory
Error converting audio during caching. %@
Error converting stream audio during caching. %@
Caching is disabled. Skipping caching.
Unrecognized audio object, skip caching
Audio duration is too short: %.2f second, skip caching
Audio duration is too long: %.2f second, skip caching
Compressing audio for caching.
Audio compressed for caching.
Can't add audio cache, error: %@
Preinstalled cached synthesis %@ is found.
AssistantSiriAnalytics should always derive an identifier for SISchemaComponentName_COMPONENTNAME_TTS
Error AudioFileCreateWithURL: '%@', code: %@
Unable to begin OPUS decoder, %@
Error during decoding, %@
Error AudioFileWriteBytes: '%@', code: %@
Error AudioFileClose: '%@', code: %@
Start spinNextTask
Dispatch speaking task %llu
Starting task %llu
PresynthesisTask %llu requested to wait another speaking task %llu
New speak task %llu interrupts speaking task %llu
New speak task %llu waits for speaking task %llu
%llu interrupt task %llu
Speak task %llu is attached to eager task %llu
Dispatch synthesis task %llu
Finish spinNextTask
Starting text to phonemes task %llu
Finished text to phonemes task %llu
decoderStreamDescription formatID: %@, sample rate: %@
Unknown server audio format ID: %d
%s, invalid opus data
%s, Unknown format: %d
Invalid chunk size: %d at offset %d, bytes count = %d
Unable to convert OPUS to PCM. %@
Decoding opus for dumping.
Opus decoded for dumping.
OPTTSTextToSpeechResponse word timing info, offset: %ld, length: %ld, word: %@, sampleIndex: %d, timestamp: %.2f
Unable to madvise file '%@' MADV_DONTNEED, error: %s
Unable to munmap file '%@', error: %s
Unable to open file '%s', error: %s
Unable to get size of file '%s', error: %s
Unable to mmap '%s', error: %s
fcntl called on file '%@', size: %lu
0Emcpl
C333333
N11flatbuffers16DefaultAllocatorE
N11flatbuffers9AllocatorE
VSSpeechSynthesisTask
VSSpeechEagerProtocol
VSSpeechSpeakableProtocol
VSSpeechTaskProtocol
NSObject
VSAudioMappedInfoAVSBAR
VSAudioMappedInfo
VSAudioPlaybackServiceAVSBAR
VSAudioPlaybackServiceProtocol
AFAudioPowerProviding
VSVoiceBooster
VSDurationEstimationTask
VSHMHomeManager
VSSpeechSpeakTask
VSSpeechServerTask
VSDeviceTTSCoreDelegate
VSOspreyTTSCoreDelegate
VSSpeechXPCHandler
VSSpeechXPCServiceProtocol
VSSpeechServiceDelegate
VSOspreyTTSCore
VSPostInstallService
VSVoicePreviewTask
AVAudioPlayerDelegate
VSSpeechCacheAudio
VSSpeechCacheItem
VSSpeechCache
OPTTSTTSRequestFeatureFlags
FLTBFBufferAccessor
NSCopying
OPTTSTextToSpeechVoice
OPTTSTextToSpeechResource
OPTTSTextToSpeechMeta
OPTTSTextToSpeechRequestMeta
OPTTSTextToSpeechRequestContext
OPTTSTextToSpeechRequestExperiment
OPTTSTextToSpeechRequest
OPTTSTextToSpeechRequest_ContextInfoEntry
OPTTSAudioDescription
OPTTSWordTimingInfo
OPTTSTextToSpeechResponse
OPTTSStartTextToSpeechStreamingRequest
OPTTSStartTextToSpeechStreamingRequest_ContextInfoEntry
OPTTSBeginTextToSpeechStreamingResponse
OPTTSPartialTextToSpeechStreamingResponse
OPTTSFinalTextToSpeechStreamingResponse
OPTTSTextToSpeechRouterStreamingStreamingRequest
OPTTSTextToSpeechRouterStreamingStreamingResponse
VSAceObjectUtility
VSHHManagementClient
VSAudioMappedInfoAT
VSOccasionalTimesObserver
VSAudioPlaybackServiceAT
VSAudioRouteInfo
VSAudioPlaybackService
VSDeviceTTSCore
VSCacheDeleteService
VSTimeoutCondition
VSDownloadService
OspreyTTSService
SpeechService
VSDiagnosticService
VSSiriServerConfiguration
VSShortTermCache
VSSiriInlineTTSStreamTask
VSSpeechPresynthesizedTask
VSSpeechAudioPowerService
VSInlineStreamService
OPTTSMutableTTSRequestFeatureFlags
OPTTSMutableTextToSpeechVoice
OPTTSMutableTextToSpeechResource
OPTTSMutableTextToSpeechMeta
OPTTSMutableTextToSpeechRequestMeta
OPTTSMutableTextToSpeechRequestContext
OPTTSMutableTextToSpeechRequestExperiment
OPTTSMutableTextToSpeechRequest
OPTTSMutableTextToSpeechRequest_ContextInfoEntry
OPTTSMutableAudioDescription
OPTTSMutableWordTimingInfo
OPTTSMutableTextToSpeechResponse
OPTTSMutableStartTextToSpeechStreamingRequest
OPTTSMutableStartTextToSpeechStreamingRequest_ContextInfoEntry
OPTTSMutableBeginTextToSpeechStreamingResponse
OPTTSMutablePartialTextToSpeechStreamingResponse
OPTTSMutableFinalTextToSpeechStreamingResponse
OPTTSMutableTextToSpeechRouterStreamingStreamingRequest
OPTTSMutableTextToSpeechRouterStreamingStreamingResponse
VSPrewarmService
VSServerTTSClient
Utilities
VSCachingService
VSSiriInstrumentation
VSStreamAudioMappedInfo
VSStreamAudioData
VSSpeechTaskQueue
VSTextToPhonemesTask
SAUIAudioData
VSHelpers
VSMemoryMap
initWithRequest:
speakTask
fetchVoiceResource
fetchVoiceAsset
request
text
languageCode
voiceName
estimatedTTSWordTimingForText:withLanguage:voiceName:
setTimingInfos:
timingInfos
contextInfo
adjustWordTimingInfo:forContext:
error
cachingService
fetchCacheForTask:
speechCache
synthesize
streamAudio
duration
instrumentMetrics
setAudioDuration:
reportTimingInfo
shouldStreamAudioData
delegate
audio
synthesisRequest:didGenerateAudioChunk:
speakCachedAudio
speechBeginTimestamp
setSpeechEndTimestamp:
playbackService
discontinuedDuringPlayback
setErrorCode:
setReadyForEagerTask:
isCancelled
dictionaryWithObjects:forKeys:count:
errorWithDomain:code:userInfo:
setError:
reportInstrumentMetrics
defaultService
dumpStreamAudio:forRequest:
outputPath
path
writeWaveToFilePath:
shouldCache
synthesisHasIssue
enqueueCache
reportFinish
logFinish
prepareForSynthesis
audioSessionID
startPlaybackServiceWithAudioSessionID:
setSynthesisBeginTimestamp:
engine
utterance
canLogRequestText
state
mutablePCMData
voiceBooster
processData:
appendAudioData:packetCount:packetDescriptions:
asbd
setAsbd:
setAudioData:
enqueue:packetCount:packetDescriptions:
taskAuxiliaryQueue
reportSpeechStart
wordTimingInfos
initWithArray:copyItems:
setObserverForWordTimings:
setSynthesisEndTimestamp:
neuralDidFallback
hasAudioClick
setSynthesisHasIssue:
numOfPromptsTriggered
setPromptCount:
hasAlignmentStall
setNeuralAlignmentStall:
setNeuralAudioClick:
setNeuralFallback:
synthesizeText:loggable:callback:
length
waitUntilAudioFinished
speechRequest:didStartWithMark:forRange:
textRange
setBoundaryTimeObserverForTimingInfos:usingBlock:
synthesisRequest:didReceiveTimingInfo:
requestCreatedTimestamp
speechRequest:didReceiveTimingInfo:
speechRequest:didReportInstrumentMetrics:
setUtterance:
voiceAssetKey
setVoiceAssetKey:
voiceResourceAssetKey
setVoiceResourceAssetKey:
synthesisBeginTimestamp
synthesisEndTimestamp
setSpeechBeginTimestamp:
speechEndTimestamp
audioStartTimestampDiffs
setAudioStartTimestampDiffs:
audioDuration
isWarmStart
setIsWarmStart:
setEagerRequestCreatedTimestampDiffs:
promptCount
errorCode
dictionaryMetrics
dumpInstrumentMetrics:withTimestamp:
speechRequestDidStart:
speechRequest:didStopWithSuccess:phonemesSpoken:error:
phonemes
componentsJoinedByString:
synthesisRequest:didFinishWithInstrumentMetrics:error:
isEqual:
class
self
performSelector:
performSelector:withObject:
performSelector:withObject:withObject:
isProxy
isKindOfClass:
isMemberOfClass:
conformsToProtocol:
respondsToSelector:
retain
release
autorelease
retainCount
zone
hash
superclass
description
debugDescription
TQ,R
T#,R
T@"NSString",R,C
start
suspend
resume
cancel
taskHash
isSpeaking
voiceKey
setSiriInstrumentation:
audioPowerProvider
readyForEagerTask
setSpeakTask:
main
.cxx_destruct
_readyForEagerTask
_speakTask
T@"VSSpeechSpeakTask",&,N,V_speakTask
TB,N,V_readyForEagerTask
audioBytesRange
setAudioBytesRange:
packetCount
setPacketCount:
packetDescriptionsRange
setPacketDescriptionsRange:
T{_NSRange=QQ},N
TQ,N
endOfSiriTTSUtterance
setEndOfSiriTTSUtterance:
sampleBuffer
_endOfSiriTTSUtterance
_packetCount
_audioBytesRange
_packetDescriptionsRange
T{_NSRange=QQ},N,V_audioBytesRange
TQ,N,V_packetCount
T{_NSRange=QQ},N,V_packetDescriptionsRange
TB,N,V_endOfSiriTTSUtterance
init
sharedInstance
opaqueSessionID
retrieveSessionWithID:
array
renderers
firstObject
audioSession
currentTime
setAudioSession:
setDelaysRateChangeUntilHasSufficientMediaData:
addRenderer:
defaultCenter
handleMediaServerReset
addObserver:selector:name:object:
removeObserver:
stopWaiting
freeAudioQueue
dealloc
stop
_play
rate
synchronizer
setRate:
createSampleBuffer:
createSilenceEndBuffer
bytesAtOffset:
appendData:
duration:
addObject:
_startProvidingData
provideMoreData
requestMediaDataWhenReadyOnQueue:usingBlock:
lastObject
count
isReadyForMoreMediaData
removeObjectAtIndex:
createSampleBufferIdNeeded:
renderer
enqueueSampleBuffer:
countByEnumeratingWithState:objects:count:
addEndOfDataAttachment
valueWithBytes:objCType:
arrayWithObjects:count:
addBoundaryTimeObserverForTimes:queue:usingBlock:
removeTimeObserver:
stopRequestingMediaData
setRate:time:
flush
removeAllObjects
CMTimeValue
sortedArrayUsingComparator:
enumerateObjectsWithOptions:usingBlock:
volume
initWithAudioSessionID:asbd:
flushAndStop
pause
addBoundaryTimeObserverForTimes:usingBlock:
sessionID
setSessionID:
T{AudioStreamBasicDescription=dIIIIIIII},R,N
TI,N
TB,R,N
T@"NSError",&,N
willBeginAccessPower
getAveragePower:andPeakPower:
didEndAccessPower
audioQueueBufferLock
setAudioQueueBufferLock:
setRenderer:
setSynchronizer:
dataQueue
setDataQueue:
stateLock
setStateLock:
setState:
mappedAudioQueuedTimeStamp
setMappedAudioQueuedTimeStamp:
rendererEnqueuedAudioDuration
setRendererEnqueuedAudioDuration:
outputRoute
setOutputRoute:
mappedData
setMappedData:
enqueuedMappedAudioInfo
setEnqueuedMappedAudioInfo:
startedProvidingData
setStartedProvidingData:
noRemainTasks
setNoRemainTasks:
_discontinuedDuringPlayback
_startedProvidingData
_sessionID
_error
_renderer
_synchronizer
_dataQueue
_state
_rendererEnqueuedAudioDuration
_outputRoute
_mappedData
_enqueuedMappedAudioInfo
_noRemainTasks
_mappedAudioQueuedTimeStamp
_asbd
_audioQueueBufferLock
_stateLock
T{_opaque_pthread_mutex_t=q[56c]},N,V_audioQueueBufferLock
T{AudioStreamBasicDescription=dIIIIIIII},N,V_asbd
T@"AVSampleBufferAudioRenderer",&,N,V_renderer
T@"AVSampleBufferRenderSynchronizer",&,N,V_synchronizer
T@"NSObject<OS_dispatch_queue>",&,N,V_dataQueue
T{_opaque_pthread_mutex_t=q[56c]},N,V_stateLock
Tq,N,V_state
T{?=qiIq},N,V_mappedAudioQueuedTimeStamp
Td,N,V_rendererEnqueuedAudioDuration
T@"NSString",&,N,V_outputRoute
T@"VSMappedData",&,N,V_mappedData
T@"NSMutableArray",&,N,V_enqueuedMappedAudioInfo
TB,N,V_startedProvidingData
T@"NSObject<OS_dispatch_semaphore>",&,N,V_noRemainTasks
TI,N,V_sessionID
TB,R,N,V_discontinuedDuringPlayback
T@"NSError",&,N,V_error
uninitialize
vs_stringFrom4CC:
initialize
voiceBoostGainDecibels
pcmBufferSize
mutableBytes
dataWithLength:
initWithStreamDescription:pcmBufferSize:
setVoiceBoostGainDecibels:
setPcmBufferSize:
floatConverter
setFloatConverter:
integerConverter
setIntegerConverter:
voiceBoostUnit
setVoiceBoostUnit:
audioTimeStamp
setAudioTimeStamp:
_voiceBoostGainDecibels
_pcmBufferSize
_floatConverter
_integerConverter
_voiceBoostUnit
_audioTimeStamp
TQ,N,V_pcmBufferSize
T^{OpaqueAudioConverter=},N,V_floatConverter
T^{OpaqueAudioConverter=},N,V_integerConverter
T^{OpaqueAudioComponentInstance=},N,V_voiceBoostUnit
T{AudioTimeStamp=dQdQ{SMPTETime=ssIIIssss}II},N,V_audioTimeStamp
Tf,N,V_voiceBoostGainDecibels
shortTermCachedEngines
stringWithFormat:
objectForKey:
exceptionWithName:reason:userInfo:
setRequestCreatedTimestamp:
deviceCore
selectedVoice
selectedVoiceResource
shortTermCachedEngineForVoice:voiceResource:
setEngine:
sharedManager
selectVoiceForLang:name:type:gender:footprint:
voicePath
initWithVoicePath:resourcePath:
setObject:forKey:timeToLive:
compressedAudio
setRequest:
estimatedDuration
setInstrumentMetrics:
setDeviceCore:
_request
_estimatedDuration
_instrumentMetrics
_deviceCore
T@"VSDeviceTTSCore",&,N,V_deviceCore
T@"VSSpeechRequest",&,N,V_request
Td,R,N,V_estimatedDuration
T@"VSInstrumentMetrics",&,N,V_instrumentMetrics
transferPreinstallErrorMessagesOfLanguage:voiceName:forAccessoryID:
canUseServerTTS
setCanUseServerTTS:
clientBundleIdentifier
setClientBundleIdentifier:
setSourceOfTTS:
standardService
sharedService
voiceSelection
voiceResource
pitch
customResourceURLs
sha256hex
popShortTermCacheForHash:
logText
setIsServerTTS:
setIsCacheHitFromMemory:
voiceResourceKey
setSpeechCache:
synthesizeAndSpeak
setIsSpeechRequest:
dumpCompressedAudio:forRequest:
collectTailspin:
isExecuting
isFinished
stopAtMarker:
neuralPlaybackSemaphore
pausePlayback
resumePlayback
voiceData
type
setNeuralPlaybackSemaphore:
loadResource:error:
setPhonemes:
UTF8String
unloadResource:
reportInstrumentMetrics:
code
siriInstrumentation
numberWithInteger:
instrumentSpeechFailedWithErrorCodes:
languages
isEqualToString:
name
instrumentVoiceFallbackOccurredWithVoice:resource:
instrumentSpeechCancelled
ttsSynthesisLatency
cappedRealTimeFactor
instrumentSpeechEndedWithAudioDuration:synthesisLatency:realTimeFactor:promptCount:errorCode:
sourceOfTTS
timeToSpeakLatency
outputRouteInfo
footprint
contentVersion
unsignedIntegerValue
shouldWhisper
instrumentSpeechStartedWithSource:customerPerceivedLatency:audioOutputRoute:voiceType:voiceFootprint:voiceVersion:resourceVersion:isWhisper:
setDelegate:
setVoiceBooster:
setPlaybackService:
setVoiceSelection:
setVoiceResource:
setCachingService:
prewarmService
setPrewarmService:
setCompressedAudio:
setStreamAudio:
setTaskAuxiliaryQueue:
_synthesisHasIssue
_timingInfos
_delegate
_engine
_voiceBooster
_playbackService
_voiceSelection
_voiceResource
_cachingService
_prewarmService
_siriInstrumentation
_speechCache
_phonemes
_compressedAudio
_streamAudio
_taskAuxiliaryQueue
_neuralPlaybackSemaphore
T@"NSObject<OS_dispatch_semaphore>",&,N,V_neuralPlaybackSemaphore
T@"NSArray",&,N,V_timingInfos
T@"<VSSpeechServiceDelegate>",W,N,V_delegate
T@"VSSpeechEngine",&,N,V_engine
T@"VSVoiceBooster",&,N,V_voiceBooster
T@"VSAudioPlaybackService",&,N,V_playbackService
T@"VSVoiceAssetSelection",&,N,V_voiceSelection
T@"VSVoiceResourceAsset",&,N,V_voiceResource
T@"VSCachingService",&,N,V_cachingService
T@"VSPrewarmService",&,N,V_prewarmService
TB,N,V_synthesisHasIssue
T@"VSSiriInstrumentation",&,N,V_siriInstrumentation
T@"<VSSpeechCacheItem>",&,N,V_speechCache
T@"NSArray",&,N,V_phonemes
T@"VSAudioData",&,N,V_compressedAudio
T@"VSStreamAudioData",&,N,V_streamAudio
T@"NSObject<OS_dispatch_queue>",&,N,V_taskAuxiliaryQueue
isNeuralFallbackCondition
standardInstance
defaultConfig
useDeviceSynthesis
synthesisCore
useServerResponse
setUseServerResponse:
ospreyCore
voice
wordTimingInfo
setWordTimingInfo:
enqueueAudioData:
serverAudio
setServerAudio:
concatenateWithAudio:
localizedInterstitialStringForKey:language:
copy
setText:
setUseDeviceSynthesis:
localizedDescription
setIsServerTimeout:
disableDeviceRacing
deferredTTSTimingInfo
setPacketDescriptions:
enumerateAudioWithBlock:
domain
internalSettings
deviceTTSWaitTime
serverTTSConfig
deviceWaitTimeForAppId:
shouldRelyOnServerTTS
shouldDeferDeviceTTS
setDeferredTTSTimingInfo:
shouldSpeak
accessoryID
initWithAudioSessionID:asbd:useAVSBAR:
audioRouteName
setAudioOutputRoute:
audioData
packetDescriptions
speechStartReported
setSpeechStartReported:
gender
numberWithLong:
voiceType
numberWithDouble:
experimentIdentifier
setExperimentIdentifier:
proceedWithSpeechCache:
eagerTaskHashForRequest:
setIsEagerCache:
proceedWithServerTTS
enqueueShortTermCacheWithHash:audio:timingInfo:voiceKey:voiceResourceKey:completion:
writeAudioIfNeeded:
selectVoiceResourceAssetForLanguage:
defaultVoiceGender
setGender:
defaultVoiceNameForGender:
setVoiceName:
setIsServerTTSRacing:
waitUntilFinished
waitUntilFinishedIfAudioReceivedWithin:
speakRetryPhrase
fallbackToDeviceSynthesis
isServerTimeout
isServerTTS
descriptiveKey
isEagerCache
logUtterance
numberWithBool:
isSynthesisCached
numberWithInt:
stringOfSourceOfTTS:
writeToFilePath:
handleDeviceSynthesis:timingInfo:
handleServerResponse:timingInfo:
synthesisCore:didReceiveAudio:
synthesisCore:didReceiveProcessingWordTimingInfo:
synthesisCore:didReceiveWordTimingInfo:
ospreyCore:didReceiveAudio:wordTimingInfo:
ospreyCore:didFinishWithError:
initWithRequest:shouldSpeak:
broadcastTimeoutCondition
setShouldSpeak:
setIsNeuralFallbackCondition:
timeoutCondition
setTimeoutCondition:
setDeviceTTSWaitTime:
setSynthesisCore:
racingMutex
setRacingMutex:
setInternalSettings:
setOspreyCore:
setServerTTSConfig:
_shouldSpeak
_isNeuralFallbackCondition
_useServerResponse
_useDeviceSynthesis
_speechStartReported
_isEagerCache
_wordTimingInfo
_deviceTTSWaitTime
_synthesisCore
_serverAudio
_deferredTTSTimingInfo
_internalSettings
_ospreyCore
_serverTTSConfig
_timeoutCondition
_racingMutex
TB,N,V_shouldSpeak
TB,N,V_isNeuralFallbackCondition
T@"NSArray",&,N,V_wordTimingInfo
T{_opaque_pthread_cond_t=q[40c]},N,V_timeoutCondition
Td,N,V_deviceTTSWaitTime
T@"VSSpeechServerTask",&,N,V_speakTask
T@"VSDeviceTTSCore",&,N,V_synthesisCore
TB,N,V_useServerResponse
TB,N,V_useDeviceSynthesis
TB,N,V_speechStartReported
TB,N,V_isEagerCache
T{_opaque_pthread_mutex_t=q[56c]},N,V_racingMutex
T@"VSAudioData",&,N,V_serverAudio
T@"NSArray",&,N,V_deferredTTSTimingInfo
T@"VSSpeechInternalSettings",&,N,V_internalSettings
T@"VSOspreyTTSCore",&,N,V_ospreyCore
T@"VSSiriServerConfiguration",&,N,V_serverTTSConfig
invalidate
activeSessionCount
setActiveSessionCount:
mainDeviceQueue
currentTask
cancelTasksWithDelegate:
setWithArray:
containsObject:
availableLanguages
fallbackLanguageForLanguage:
setLanguageCode:
performLanguageFallBackIfNeededWithRequest:
vs_insertContextInfo:
vs_substituteAudioWithLocalPath
vs_textifyEmojiWithLanguage:
precomposedStringWithCanonicalMapping
whisper
stringByAppendingString:
useSSMLInput
vs_convertToSSML
defaultVolume
setVolume:
defaultRate
defaultPitch
setPitch:
valueForEntitlement:
boolValue
setOutputPath:
siriRequestId
isSiriClientBundleIdentifier:
setSiriRequestId:
setDisableDeviceRacing:
prewarmWithRequest:
hasPhaticResponses:
preprocessRequestBeforeSynthesis:
setCompletionBlock:
UUIDString
parallelQueueWithIdentifier:
addTask:
initWithSiriRequestId:
instrumentRequestReceivedWithText:requestedVoiceType:requestedVoiceFootprint:isPrivate:
defaultInstance
date
setLastTTSRequestDate:
disableInlineStreamTTS
disableServerTTS
popInlineStreamRequestForSpeakRequest:
identifier
initWithRequest:withStreamID:
shouldUseServerTTSForRequest:
addInlineStreamRequest:
audioDataFromPresynthesisRequest:
taskWithCreatedTimeStamp:
cancelTask:
hasInlineStreamRequestForSpeakRequest:
createdTimestampWithTask:
suspendCurrentTask
resumeCurrentTask
previewType
previewRequestDidStartPlaying:
startVoicePreviewForLanguageCode:voiceName:previewType:startedPlaying:completion:
stopVoicePreview
setReply:
setPhonemeSystem:
sharedServices
audioPowerUpdateQueue
initWithProvider:queue:frequency:delegate:
audioPowerUpdater
createNewXPCWrapperWithCompletion:
beginUpdate
endUpdate
setAudioPowerUpdater:
installedAssetsForType:voicename:language:gender:footprint:
remoteObjectProxy
speechRequestDidPause:
speechRequestDidContinue:
audioRequestDidStart:
audioRequest:didReportInstrumentMetrics:error:
audioRequest:didStopAtEnd:error:
cleanUnknownAccessoriesPreferences
cleanUnusedAssets
installedVoiceResources
isExistingAccessoryId:
subscribedVoicesForClientID:accessoryID:
dictionaryRepresentation
isEqualToDictionary:
setSubscribedVoices:forClientID:accessoryID:
typeStringFromType:
footprintStringFromFootprint:
initWithType:
cancelDownloadForAssets:
updateVoicesAndVoiceResources
valueForKey:
arrayWithCapacity:
voiceAssetsForSubscription:
setName:
setType:
clientID
initWithClient:accessory:voice:
cancelDownload:completion:
downloadOptionsWithBattery:
setAllowsCellularAccess:
downloadVoiceAsset:options:progressUpdateHandler:
setFootprint:
setContentVersion:
setLanguages:
streamId
enqueueStreamId:withObject:
updateWithConnectionIdentifier:keepActive:
prewarmIfNeededWithRequest:reply:
queryPhaticCapabilityWithRequest:reply:
estimateDurationWithRequest:reply:
startSpeechRequest:reply:
startSynthesisRequest:
pauseSpeechRequest:atMark:
continueSpeechRequest:
stopSpeechRequest:atMark:
startPresynthesizedAudioRequest:
cachePresynthesizedAudioRequest:
stopPresynthesizedAudioRequest:
getVoiceNamesForLanguage:reply:
getFootprintsForVoiceName:languageCode:reply:
getSpeechIsActiveReply:
getSpeechIsActiveForConnectionReply:
startVoicePreviewRequest:reply:
startPhonemesRequest:phonemeSystem:reply:
beginAudioPowerUpdateWithReply:
endAudioPowerUpdate
cleanUnusedAssets:
getLocalVoicesForLanguage:reply:
getLocalVoiceResourcesReply:
setSubscribedVoiceAssets:withClientID:forAccessoryID:
getSubscribedVoiceAssetsWithClientID:forAccessoryID:reply:
getAllVoiceSubscriptionsWithReply:
triggerCellularDownloadedVoiceAssets:withClientID:
getVoiceResourceForLanguage:reply:
getVoiceInfoForLanguageCode:name:footprint:gender:type:reply:
forwardStreamObject:
invokeDaemon:
killDaemon
initWithConnection:
connectionIdentifier
setConnectionIdentifier:
hubManagementClient
setHubManagementClient:
homeManager
setHomeManager:
setAudioPowerUpdateQueue:
synthesizerTransaction
setSynthesizerTransaction:
_connection
_connectionIdentifier
_hubManagementClient
_homeManager
_audioPowerUpdateQueue
_audioPowerUpdater
_synthesizerTransaction
T@"NSString",&,N,V_connectionIdentifier
T@"VSHHManagementClient",&,N,V_hubManagementClient
T@"VSHMHomeManager",&,N,V_homeManager
T@"NSObject<OS_dispatch_queue>",&,N,V_audioPowerUpdateQueue
T@"AFAudioPowerUpdater",&,N,V_audioPowerUpdater
T@"NSObject<OS_os_transaction>",&,N,V_synthesizerTransaction
serverTTSTimeout
serverConfig
timeoutForAppId:
didReceiveAudioCondition
lock
broadcast
unlock
timeout
initWithTimeoutValue:
serverTTSClient
setDidReceiveAudio:
delegateCallbackQueue
ospreyStartSynthesisRequest:responseHandler:completion:
wait
bufferDurationLimit
simulateNetworkStall
serverFirstPacketTimestamp
setServerFirstPacketTimestamp:
setServerLastPacketTimestamp:
setServerStreamedAudioDuration:
didReceiveAudio
objectAtIndexedSubscript:
dateByAddingTimeInterval:
timeIntervalSinceNow
setTimeoutValue:
timeoutValue
refresh
setVoice:
streamBufferDuration
setBufferDurationLimit:
ospreyStartStreamingRequest:dataHandler:metaInfoHandler:completion:
disableOspreyStreaming
performStreamingOspreyTTS
performRoundTripOspreyTTS
dateWithTimeIntervalSinceNow:
waitUntilDate:
setServerTTSClient:
setServerConfig:
setDidReceiveAudioCondition:
setDelegateCallbackQueue:
_didReceiveAudio
_voice
_serverTTSClient
_serverConfig
_bufferDurationLimit
_didReceiveAudioCondition
_delegateCallbackQueue
T@"VSServerTTSClient",&,N,V_serverTTSClient
T@"VSSiriServerConfiguration",&,N,V_serverConfig
Td,N,V_bufferDurationLimit
T@"VSTimeoutCondition",&,N,V_timeoutCondition
TB,N,V_didReceiveAudio
T@"NSCondition",&,N,V_didReceiveAudioCondition
T@"NSObject<OS_dispatch_queue>",&,N,V_delegateCallbackQueue
T@"VSSpeechRequest",R,N,V_request
T@"<VSOspreyTTSCoreDelegate>",W,N,V_delegate
T@"VSInstrumentMetrics",W,N,V_instrumentMetrics
T@"VSVoiceAsset",&,N,V_voice
resetMobileAssetDefaults
migrateAssetsWithProgress:
clearSynthesisCache
isANECompilationPlatform
isWatch
disableMobileAssetURLReset
defaultCacheStore
deleteCache
registerPostInstallActivity
setSpeakingQueue:
bundleWithIdentifier:
resourcePath
pathWithComponents:
defaultManager
fileExistsAtPath:
fileURLWithPath:
previewAudioURLForLanguage:voiceName:previewType:
speakingQueue
startVoicePreviewWithURL:startedPlaying:completion:
lastPathComponent
setCurrentPreviewURL:
setPreviewPlayer:
setActive:withOptions:error:
initWithContentsOfURL:fileTypeHint:error:
setCategory:error:
setActive:error:
play
previewPlayer
completion
setMeteringEnabled:
updateMeters
averagePowerForChannel:
peakPowerForChannel:
audioPlayerDidFinishPlaying:successfully:
audioPlayerDecodeErrorDidOccur:error:
audioPlayerBeginInterruption:
audioPlayerEndInterruption:withOptions:
audioPlayerEndInterruption:withFlags:
audioPlayerEndInterruption:
setCompletion:
currentPreviewURL
_previewPlayer
_completion
_currentPreviewURL
_speakingQueue
T@"AVAudioPlayer",&,N,V_previewPlayer
T@?,C,N,V_completion
T@"NSURL",C,N,V_currentPreviewURL
T@"NSObject<OS_dispatch_queue>",&,N,V_speakingQueue
data
appendBytes:length:
archivedDataWithRootObject:requiringSecureCoding:error:
dataUsingEncoding:
getBytes:range:
subdataWithRange:
setWithObjects:
unarchivedObjectOfClasses:fromData:error:
initWithData:encoding:
serializedData
initWithKey:data:
initWithKey:audio:wordTimingInfo:voiceKey:voiceResourceKey:
magicVersion
setKey:
_magicVersion
_voiceKey
_voiceResourceKey
_audio
_key
_audioData
_packetDescriptions
T@"NSString",&,N,V_key
T@"NSData",&,N,V_audioData
Tq,N,V_packetCount
T@"NSData",&,N,V_packetDescriptions
Tq,R,N,V_magicVersion
T@"NSArray",R,N,V_timingInfos
T@"NSString",R,N,V_voiceKey
T@"NSString",R,N,V_voiceResourceKey
T@"VSAudioData",R,N,V_audio
bundlePath
stringByAppendingPathComponent:
createDirectoryAtPath:withIntermediateDirectories:attributes:error:
initWithStorePath:
dataWithCapacity:
setLength:
dirPath
writeToFile:options:error:
preinstalledAudioHashForLanguage:name:
preinstalledCacheDir
stringByAppendingPathExtension:
isReadableFileAtPath:
audioDataFromFile:error:
dataWithContentsOfFile:
bytes
stringWithUTF8String:
preinstalledCacheForText:language:name:
isHomePod
cleanDirectory:withLRULimit:
directorySize:
removeDirectory:
T@"VSSpeechCache",R
addCache:
cacheDataForKey:
isPreinstalledCacheAvailableForRequest:
cleanCache
totalCacheSize
setDirPath:
setPreinstalledCacheDir:
_dirPath
_preinstalledCacheDir
T@"NSString",&,N,V_dirPath
T@"NSString",&,N,V_preinstalledCacheDir
initWithFlatbuffData:root:verify:
dictionary
fe_feature
fe_feature_only
addObjectToBuffer:
initWithBytesNoCopy:length:deallocator:
flatbuffData
copyWithZone:
initWithFlatbuffData:
initAndVerifyWithFlatbuffData:
initWithFlatbuffData:root:
_storage
_data
_root
initWithBytes:length:encoding:
language
version
quality
T@"NSString",R,N
objectForKeyedSubscript:
setObject:forKeyedSubscript:
resource
T@"OPTTSTextToSpeechVoice",R,N
T@"OPTTSTextToSpeechResource",R,N
channel_type
app_id
Tq,R,N
context_info
dialog_identifier
T@"NSArray",R,N
experiment_identifier
speech_id
session_id
audio_type
enable_word_timing_info
voice_name
preferred_voice_type
meta_info
context
experiment
feature_flags
T@"OPTTSTextToSpeechRequestMeta",R,N
T@"OPTTSTextToSpeechRequestContext",R,N
T@"OPTTSTextToSpeechRequestExperiment",R,N
T@"OPTTSTTSRequestFeatureFlags",R,N
value
sample_rate
format_id
format_flags
bytes_per_packet
frames_per_packet
bytes_per_frame
channels_per_frame
bits_per_channel
reserved
Td,R,N
TI,R,N
word
sample_idx
offset
timestamp
Tf,R,N
dataWithBytes:length:
error_code
error_str
audio:
decoder_description
playback_description
word_timing_info
Ti,R,N
T@"NSData",R,N
T@"OPTTSAudioDescription",R,N
T@"OPTTSTextToSpeechMeta",R,N
stream_id
streaming_playback_buffer_size_in_seconds
current_pkt_number
total_pkt_number
content_type
contentAsOPTTSStartTextToSpeechStreamingRequest
T@"OPTTSStartTextToSpeechStreamingRequest",R,N
contentAsOPTTSBeginTextToSpeechStreamingResponse
contentAsOPTTSPartialTextToSpeechStreamingResponse
contentAsOPTTSFinalTextToSpeechStreamingResponse
T@"OPTTSBeginTextToSpeechStreamingResponse",R,N
T@"OPTTSPartialTextToSpeechStreamingResponse",R,N
T@"OPTTSFinalTextToSpeechStreamingResponse",R,N
formatID
intValue
sampleRate
floatValue
bitsPerChannel
framesPerPacket
vsDescription
stringByReplacingOccurrencesOfString:withString:
resourceVersion
lowercaseString
_resetNextFireTime
_reallyInvalidate
initWithTimebase:times:queue:block:
timebase
_timerQueue
_timerSource
_invalid
_times
_nextFireTime
_block
_timebase
T^{OpaqueCMTimebase=},R,V_timebase
dequeueAvailableMappedAudio
_enqueueAudioBytesLength:audioBytes:packetCount:packetDescriptions:
waitForAudioQueueStop
signal
signalQueueRunningStateChange
isAudioQueueRunning
isAudioQueueStalled
timeIntervalSinceDate:
setDateFormat:
stringFromDate:
audioQueue
setAudioQueue:
setTimebase:
waitForStateChangeMutex
setWaitForStateChangeMutex:
stateChangeCondition
setStateChangeCondition:
enqueuedSampleCount
setEnqueuedSampleCount:
audioStartTimeStamp
setAudioStartTimeStamp:
audioQueueStartDate
setAudioQueueStartDate:
audioQueueFutureEndDate
setAudioQueueFutureEndDate:
playingBufferCount
setPlayingBufferCount:
dequeueCondition
setDequeueCondition:
_audioQueue
_enqueuedSampleCount
_audioQueueStartDate
_audioQueueFutureEndDate
_playingBufferCount
_dequeueCondition
_stateChangeCondition
_waitForStateChangeMutex
_audioStartTimeStamp
T^{OpaqueAudioQueue=},N,V_audioQueue
T^{OpaqueCMTimebase=},N,V_timebase
T{_opaque_pthread_mutex_t=q[56c]},N,V_waitForStateChangeMutex
T{_opaque_pthread_cond_t=q[40c]},N,V_stateChangeCondition
Td,N,V_enqueuedSampleCount
T{AudioTimeStamp=dQdQ{SMPTETime=ssIIIssss}II},N,V_audioStartTimeStamp
T@"NSDate",&,N,V_audioQueueStartDate
T@"NSDate",&,N,V_audioQueueFutureEndDate
TQ,N,V_playingBufferCount
T@"NSCondition",&,N,V_dequeueCondition
initWithRouteAttributes:
isBluetoothRoute
isAppleProduct
routeInfo
_routeInfo
T@"NSDictionary",R,N,V_routeInfo
currentRoute
outputs
portType
sharedAVSystemController
attributeForKey:
setTimingObserver:
durationOfAudioDataLength:withAudioDescription:
bytesOfDuration:withAudioDescription:
implementation
setImplementation:
playbackIntervalId
setPlaybackIntervalId:
timingObserver
_outputRouteInfo
_implementation
_playbackIntervalId
_timingObserver
T@"<VSAudioPlaybackServiceProtocol><AFAudioPowerProviding>",&,N,V_implementation
TQ,N,V_playbackIntervalId
T@,&,N,V_timingObserver
T@"VSAudioRouteInfo",R,N,V_outputRouteInfo
T@"NSError",R,N
voiceSelectionWithRequest:error:
getCacheForHash:
reportWordTimingInfo:
reportAudio:
reportProcessingWordTimingInfo:
enqueueCacheWithHash:streamAudio:timingInfo:voiceKey:voiceResourceKey:completion:
inMemoryCacheForHash:
onDiskCacheForHash:
setIsCacheHitFromDisk:
voiceSelection_noRetry_WithRequest:error:
resetCache
disableCompactVoiceFallback
cachedEngineForVoice:resources:
loadEngineForVoice:resources:
initWithASBD:
gainDecibelWithVolume:
setSelectedVoice:
setSelectedVoiceResource:
_selectedVoice
_selectedVoiceResource
T@"VSVoiceAssetSelection",&,N,V_selectedVoice
T@"VSVoiceResourceAsset",&,N,V_selectedVoiceResource
T@"<VSDeviceTTSCoreDelegate>",W,N,V_delegate
T@"VSAudioData",R,N,V_compressedAudio
T@"VSStreamAudioData",R,N,V_streamAudio
disableAssetCleaning
longLongValue
activeVoiceAssets
inactiveVoiceAssets
addObjectsFromArray:
totalSizeOfAssets:
size
purgeableAssetsWithInfo:urgency:
totalDiagnosticFileSize
numberWithLongLong:
purgeImpl:urgency:
purgeAsset:
removeVoiceResource:completion:
removeDirectory
purgeable:urgency:
purge:urgency:
periodic:urgency:
registerCacheDelete
refreshTimeoutCondition
_waitForTimeInterval:
shouldStop
setRefreshTimeoutCondition:
setShouldStop:
_shouldStop
_timeoutValue
_refreshTimeoutCondition
T@"NSCondition",&,N,V_refreshTimeoutCondition
TB,N,V_shouldStop
Td,N,V_timeoutValue
initWithType:assetsManager:
inProgressDownloadVoiceKeys
removeObject:
disableAssetUpdate
preferenceInterface
orderedSet
updateVoiceIfNeeded:
updateTrialVoiceResourceWithLanguage:
assetsManager
lastTTSRequestDate
shouldDelayVoiceUpdate
isHomeHub
isVoiceReadyToUse
addInProgressDownloadVoiceKey:
removeInProgressDownloadVoiceKey:
downloadVoiceResource:options:completion:
preferredDownloadForVoice:
downloadQueue
setAssetsManager:
setPreferenceInterface:
updateLock
setUpdateLock:
_type
_assetsManager
_preferenceInterface
_updateLock
T@"VSMobileAssetsManager",&,N,V_assetsManager
T@"VSPreferencesInterface",&,N,V_preferenceInterface
T@"NSLock",&,N,V_updateLock
TQ,R,N,V_type
ospreyEndpointURL
isSeedBuild
isInternalBuild
ospreyServiceEndpointURL
defaultSessionConfiguration
setTimeoutIntervalForRequest:
setTimeoutIntervalForResource:
URLWithString:
initWithURL:configuration:
setUseCompression:
deviceUUID
deviceID
setSpeech_id:
UUID
setSession_id:
setExperiment_identifier:
setExperiment:
setClientTraceIdentifier:
unaryRequestWithMethodName:requestData:requestBuilder:responseHandler:
setLanguage:
setVoice_name:
setAudio_type:
setEnable_word_timing_info:
setStream_id:
setMeta_info:
setContent_type:
setContentAsOPTTSStartTextToSpeechStreamingRequest:
serverStreamingRequestWithMethodName:requestData:requestBuilder:streamingResponseHandler:completion:
roundTripTTS:responseHandler:
streamTTS:beginHandler:chunkHandler:endHandler:completion:
setDeviceID:
_deviceID
T@"NSString",&,N,V_deviceID
enableAudioDump
initWithDirectory:
audioDumpPath
cleanDirectory:withDateOlderThan:
fileExistsAtPath:isDirectory:
createDirectoryIfNeeded
audioDumpFileAttributes
createFileAtPath:contents:attributes:
moveItemAtPath:toPath:error:
dataWithJSONObject:options:error:
writeToFile:atomically:
removeOldFiles
setAudioDumpPath:
setAudioDumpFileAttributes:
_audioDumpPath
_audioDumpFileAttributes
T@"NSString",&,N,V_audioDumpPath
T@"NSDictionary",&,N,V_audioDumpFileAttributes
userDefaultsKnowledgeStore
setKnowledgeStore:
knowledgeStore
JSONObjectWithData:options:error:
dictForKey:
configForAppId:key:
componentsSeparatedByString:
allowedAppID
_knowledgeStore
T@"CKKnowledgeStore",&,N,V_knowledgeStore
cacheTimer
timeToLiveTimerFired:
timerWithTimeInterval:target:selector:userInfo:repeats:
cache
setObject:forKey:
mainRunLoop
addTimer:forMode:
userInfo
removeObjectForKey:
setCache:
setCacheTimer:
_cache
_cacheTimer
T@"NSCache",&,N,V_cache
T@"NSMutableDictionary",&,N,V_cacheTimer
setIsServerStreamTTS:
streamID
removeStreamId:
object
handleBegin:
handleChunk:
handleEnd:
speechSynthesisVoice
setStreamingVoice:
speechSynthesisResource
setStreamingResource:
decoderStreamDescription
streamingPlaybackBufferSize
asbdFromDescription:
setPlaybackServices:
playbackServices
signalNewDataWithError:
populateWithPCMData:
populateWithOpusData:
startPlayback
setPlaybackBeginDate:
playbackBeginDate
errorMessage
handleStreamNotification:
startStreamingWithId:
waitForNewData:
retryDeviceOnNetworkStall
deviceTTSCore
finalTimingInfo
streamingVoice
typeFromString:
footprintFromString:
integerValue
streamingResource
setStreamID:
setDeviceTTSCore:
setFinalTimingInfo:
_streamID
_streamingResource
_streamingVoice
_deviceTTSCore
_playbackServices
_finalTimingInfo
_playbackBeginDate
T@"NSString",&,N,V_streamID
T@"SATTSSpeechSynthesisResource",&,N,V_streamingResource
T@"SATTSSpeechSynthesisVoice",&,N,V_streamingVoice
T@"VSDeviceTTSCore",&,N,V_deviceTTSCore
T@"VSAudioPlaybackService",&,N,V_playbackServices
T@"NSMutableArray",&,N,V_finalTimingInfo
T@"NSDate",&,N,V_playbackBeginDate
T@"VSPresynthesizedAudioRequest",R,N,V_request
T@"NSMutableData",&,N,V_audioData
getCurrentAudioPowerProvider
previousProvider
setPreviousProvider:
_previousProvider
T@"<AFAudioPowerProviding>",W,N,V_previousProvider
streamRequestQueue
ongoingNotifications
notifyQueue
postNotificationName:object:
queuedNotification
setQueuedNotification:
setOngoingNotifications:
setStreamRequestQueue:
setLock:
recursiveLockAttr
setRecursiveLockAttr:
setNotifyQueue:
_queuedNotification
_ongoingNotifications
_streamRequestQueue
_notifyQueue
_recursiveLockAttr
_lock
T@"NSMutableDictionary",&,N,V_queuedNotification
T@"NSMutableSet",&,N,V_ongoingNotifications
T@"NSMutableArray",&,N,V_streamRequestQueue
T{_opaque_pthread_mutex_t=q[56c]},N,V_lock
T{_opaque_pthread_mutexattr_t=q[8c]},N,V_recursiveLockAttr
T@"NSObject<OS_dispatch_queue>",&,N,V_notifyQueue
allocWithZone:
initWithBool:
setFe_feature:
setFe_feature_only:
TB,N
setVersion:
setQuality:
T@"NSString",C,N
setResource:
T@"OPTTSTextToSpeechVoice",C,N
T@"OPTTSTextToSpeechResource",C,N
initWithInteger:
setChannel_type:
setApp_id:
Tq,N
setContext_info:
setDialog_identifier:
T@"NSArray",C,N
setPreferred_voice_type:
setContext:
setFeature_flags:
T@"OPTTSTextToSpeechRequestMeta",C,N
T@"OPTTSTextToSpeechRequestContext",C,N
T@"OPTTSTextToSpeechRequestExperiment",C,N
T@"OPTTSTTSRequestFeatureFlags",C,N
setValue:
doubleValue
initWithDouble:
initWithUnsignedInteger:
setSample_rate:
setFormat_id:
setFormat_flags:
setBytes_per_packet:
setFrames_per_packet:
setBytes_per_frame:
setChannels_per_frame:
setBits_per_channel:
setReserved:
Td,N
initWithFloat:
setWord:
setSample_idx:
setOffset:
setTimestamp:
Tf,N
initWithInt:
setError_code:
setError_str:
setAudio:
setDecoder_description:
setPlayback_description:
setWord_timing_info:
Ti,N
T@"NSData",C,N
T@"OPTTSAudioDescription",C,N
T@"OPTTSTextToSpeechMeta",C,N
setStreaming_playback_buffer_size_in_seconds:
setCurrent_pkt_number:
setTotal_pkt_number:
T@"OPTTSStartTextToSpeechStreamingRequest",C,N
setContentAsOPTTSBeginTextToSpeechStreamingResponse:
setContentAsOPTTSPartialTextToSpeechStreamingResponse:
setContentAsOPTTSFinalTextToSpeechStreamingResponse:
T@"OPTTSBeginTextToSpeechStreamingResponse",C,N
T@"OPTTSPartialTextToSpeechStreamingResponse",C,N
T@"OPTTSFinalTextToSpeechStreamingResponse",C,N
handleVoiceSelectionPurge:
cachedEngine
unloadEngine
prewarmQueue
initializeDeviceAuthenticationSessionWithCompletion:
_cachedEngineForVoice:resources:
_engineForVoice:resources:
preheat
lowInactiveMemory
resourceMimeTypes
searchPathURL
_loadVoiceResources:forEngine:
fileURLWithPathComponents:
setSearchPathURL:
resourceList
loadResourceAtPath:mimeType:error:
setCachedEngine:
setLoadedResources:
unloadCachedEngineWithVoice:
waitUntilPrewarmFinish
loadedResources
setPrewarmQueue:
_activeSessionCount
_cachedEngine
_loadedResources
_prewarmQueue
T@"VSSpeechEngine",&,N,V_cachedEngine
T@"VSVoiceResourceAsset",&,N,V_loadedResources
T@"NSObject<OS_dispatch_queue>",&,N,V_prewarmQueue
Tq,N,V_activeSessionCount
forceServerTTS
shortTermCacheForHash:
requestFromVSRequest:
audioStreamBasicDescription
audioDataWithASBD:rawData:
vs_wordTimingInfos:withText:
vs_voice
vs_voiceResource
powerProfile
currentPowerPolicyWithError:
ttsPolicy
setPowerProfile:
_fetchVoiceAsset_NoRetry
eagerRequestCreatedTimestampDiffs
neuralAlignmentStall
neuralAudioClick
neuralFallback
enqueueCacheWithHash:audio:timingInfo:voiceKey:voiceResourceKey:completion:
initWithCache:shortTermMemory:
initWithSourceASBD:
setErrorHandler:
setOpusDataHandler:
beginEncoding
encodeChunk:
endEncoding
_enqueueCacheWithHash:audioObject:timingInfo:voiceKey:voiceResourceKey:completion:
disableCache
compressStreamAudio:
compressAudio:
threadLock
inMemoryCaches
cachingQueue
cacheStore
_inMemoryCacheForHash:
_onDiskCacheForHash:
shortTermCache
setThreadLock:
setInMemoryCaches:
setCacheStore:
setShortTermCache:
setCachingQueue:
_threadLock
_inMemoryCaches
_cacheStore
_shortTermCache
_cachingQueue
T@"NSLock",&,N,V_threadLock
T@"NSMutableArray",&,N,V_inMemoryCaches
T@"VSSpeechCache",&,N,V_cacheStore
T@"VSShortTermCache",&,N,V_shortTermCache
T@"NSObject<OS_dispatch_queue>",&,N,V_cachingQueue
derivedIdentifierForComponentName:fromSourceIdentifier:
makeRequestLinkEvent
initWithNSUUID:
setUuid:
setComponent:
setSource:
setTarget:
sharedStream
emitMessage:
setTextToSynthesize:
setLinkId:
setRequestReceivedTier1:
setEventMetadata:
ttsId
eventMetadata
setTtsId:
linkId
setRequestedVoiceContext:
schemaVoiceTypeFromType:
requestedVoiceContext
setVoiceType:
schemaFootprintFromFootprint:
setVoiceFootprint:
setInputTextLength:
setRequestReceived:
instrumentPowerEvent:ttsId:
outputRouteFromRouteInfo:
setCustomerPerceivedLatencyInSecond:
synthesisSourceFromSource:
setSynthesisSource:
setVoiceContext:
voiceContext
setVoiceVersion:
setResourceVersion:
setSynthesisEffect:
setStartedOrChanged:
contextId
setContextId:
setSpeechContext:
setSynthesizedAudioDurationInSecond:
setSynthesisLatencyInSecond:
setSynthesisRealTimeFactor:
setErrorCodes:
setEnded:
setFailed:
setExists:
setCancelled:
setVoiceSettings:
schemaVoiceGenderFromGender:
voiceSettings
setVoiceGender:
convertLanguageCodeToSchemaLocale:
setVoiceAccent:
setVoiceFallbackOccurred:
initWithCurrentProcess
sharedPowerLogger
captureSnapshot
logWithEventContext:
logWithEventContext:ttsIdentifier:
instrumentVoicedProcessStartedPowerEvent
_siriRequestId
_ttsId
_contextId
T@"NSUUID",&,N,V_siriRequestId
T@"NSUUID",&,N,V_ttsId
T@"NSUUID",&,N,V_contextId
dataWithBytesNoCopy:length:freeWhenDone:
beginChunkDecoderForStreamDescription:
decodeChunk:outError:
endChunkDecoding
mappedAudioInfo
setMappedAudioInfo:
_mappedAudioInfo
T@"NSMutableArray",&,N,V_mappedAudioInfo
T{AudioStreamBasicDescription=dIIIIIIII},R,N,V_asbd
tasksWithDelegate:
allValues
spinNextTask
isSimilarTo:
setCurrentTask:
setLastSynthesisRequest:
enqueue
shouldWaitCurrentSpeaking
speakTasks
eagerTasks
setEagerTasks:
setSpeakTasks:
threadMutex
setThreadMutex:
threadMutexAttr
setThreadMutexAttr:
lastSynthesisRequest
lastSynthesisRequestCreatedTimeStamp
setLastSynthesisRequestCreatedTimeStamp:
_eagerTasks
_speakTasks
_currentTask
_lastSynthesisRequest
_lastSynthesisRequestCreatedTimeStamp
_threadMutexAttr
_threadMutex
T@"NSMutableArray",&,N,V_eagerTasks
T@"NSMutableArray",&,N,V_speakTasks
T@"NSOperation<VSSpeechTaskProtocol>",&,N,V_currentTask
T{_opaque_pthread_mutex_t=q[56c]},N,V_threadMutex
T{_opaque_pthread_mutexattr_t=q[8c]},N,V_threadMutexAttr
T@"VSSpeechRequest",&,N,V_lastSynthesisRequest
TQ,N,V_lastSynthesisRequestCreatedTimeStamp
phonemeSystem
generateTTSPhonemes:voicePath:phonemeSystem:error:
reply
_phonemeSystem
_reply
Tq,N,V_phonemeSystem
T@?,C,N,V_reply
formatFlags
unsignedIntValue
bytesPerPacket
bytesPerFrame
channelsPerFrame
audioBuffer
playerStreamDescription
populatePCMDataWithSiriOpusSData:withOpusASBD:
decodeChunks:streamDescription:outError:
pcmAudioDataFromOpusAudio:
audioDataFromSAUIAudioData:
genderStringFromGender:
setTextRange:
setStartTime:
utf16TimingInfoWithUTF8Range:withText:
genderFromString:
mmap
initWithFilePath:
madvise
filePath
fileSize
_filePath
_fileSize
T@"NSString",R,N,V_filePath
TQ,R,N,V_fileSize
T^v,R,N,V_mappedData
Ti,R,N,V_fd
B24@0:8@16
#16@0:8
@16@0:8
@24@0:8:16
@32@0:8:16@24
@40@0:8:16@24@32
B16@0:8
B24@0:8#16
B24@0:8:16
Vv16@0:8
Q16@0:8
^{_NSZone=}16@0:8
B24@0:8@"Protocol"16
@"NSString"16@0:8
v16@0:8
@"VSInstrumentMetrics"16@0:8
v24@0:8@16
v24@0:8@"NSArray"16
@"VSSpeechRequest"16@0:8
v24@0:8@"VSSiriInstrumentation"16
@"<AFAudioPowerProviding>"16@0:8
v24@0:8@"<VSSpeechSpeakableProtocol>"16
@24@0:8@16
v20@0:8B16
@"VSSpeechSpeakTask"
{_NSRange=QQ}16@0:8
v32@0:8{_NSRange=QQ}16
v24@0:8Q16
^{opaqueCMSampleBuffer=}
{_NSRange="location"Q"length"Q}
@60@0:8I16{AudioStreamBasicDescription=dIIIIIIII}20
v40@0:8@16q24@32
@32@0:8@16@?24
{AudioStreamBasicDescription=dIIIIIIII}16@0:8
I16@0:8
v20@0:8I16
v40@0:8@"NSData"16q24@"NSData"32
@"NSError"16@0:8
@32@0:8@"NSArray"16@?<v@?{?=qiIq}>24
v24@0:8@"NSError"16
B32@0:8^f16^f24
^{opaqueCMSampleBuffer=}24@0:8@16
d24@0:8@16
^{opaqueCMSampleBuffer=}16@0:8
v56@0:8{AudioStreamBasicDescription=dIIIIIIII}16
{_opaque_pthread_mutex_t=q[56c]}16@0:8
v80@0:8{_opaque_pthread_mutex_t=q[56c]}16
q16@0:8
v24@0:8q16
{?=qiIq}16@0:8
v40@0:8{?=qiIq}16
d16@0:8
v24@0:8d16
@"NSError"
@"AVSampleBufferAudioRenderer"
@"AVSampleBufferRenderSynchronizer"
@"NSObject<OS_dispatch_queue>"
@"NSString"
@"VSMappedData"
@"NSMutableArray"
@"NSObject<OS_dispatch_semaphore>"
{?="value"q"timescale"i"flags"I"epoch"q}
{AudioStreamBasicDescription="mSampleRate"d"mFormatID"I"mFormatFlags"I"mBytesPerPacket"I"mFramesPerPacket"I"mBytesPerFrame"I"mChannelsPerFrame"I"mBitsPerChannel"I"mReserved"I}
{_opaque_pthread_mutex_t="__sig"q"__opaque"[56c]}
@64@0:8{AudioStreamBasicDescription=dIIIIIIII}16Q56
v20@0:8f16
f16@0:8
^{OpaqueAudioConverter=}16@0:8
v24@0:8^{OpaqueAudioConverter=}16
^{OpaqueAudioComponentInstance=}16@0:8
v24@0:8^{OpaqueAudioComponentInstance=}16
{AudioTimeStamp=dQdQ{SMPTETime=ssIIIssss}II}16@0:8
v80@0:8{AudioTimeStamp=dQdQ{SMPTETime=ssIIIssss}II}16
^{OpaqueAudioConverter=}
^{OpaqueAudioComponentInstance=}
{AudioTimeStamp="mSampleTime"d"mHostTime"Q"mRateScalar"d"mWordClockTime"Q"mSMPTETime"{SMPTETime="mSubframes"s"mSubframeDivisor"s"mCounter"I"mType"I"mFlags"I"mHours"s"mMinutes"s"mSeconds"s"mFrames"s}"mFlags"I"mReserved"I}
@32@0:8@16@24
@"VSSpeechRequest"
@"VSInstrumentMetrics"
@"VSDeviceTTSCore"
v40@0:8@16@24@32
@"NSArray"
@"<VSSpeechServiceDelegate>"
@"VSSpeechEngine"
@"VSVoiceBooster"
@"VSAudioPlaybackService"
@"VSVoiceAssetSelection"
@"VSVoiceResourceAsset"
@"VSCachingService"
@"VSPrewarmService"
@"VSSiriInstrumentation"
@"<VSSpeechCacheItem>"
@"VSAudioData"
@"VSStreamAudioData"
v32@0:8@16@24
v32@0:8@"VSDeviceTTSCore"16@"VSAudioData"24
v32@0:8@"VSDeviceTTSCore"16@"NSArray"24
v40@0:8@"VSOspreyTTSCore"16@"VSAudioData"24@"NSArray"32
v32@0:8@"VSOspreyTTSCore"16@"NSError"24
@28@0:8@16B24
{_opaque_pthread_cond_t=q[40c]}16@0:8
v64@0:8{_opaque_pthread_cond_t=q[40c]}16
@"VSSpeechServerTask"
@"VSSpeechInternalSettings"
@"VSOspreyTTSCore"
@"VSSiriServerConfiguration"
{_opaque_pthread_cond_t="__sig"q"__opaque"[40c]}
Vv28@0:8@16B24
Vv32@0:8@16@?24
Vv24@0:8@16
Vv32@0:8@16q24
Vv40@0:8@16@24@?32
Vv24@0:8@?16
Vv40@0:8@16q24@?32
Vv40@0:8@16@24@32
Vv32@0:8@16@24
Vv64@0:8@16@24q32q40q48@?56
Vv28@0:8@"NSString"16B24
Vv32@0:8@"VSSpeechRequest"16@?<v@?@"NSError">24
Vv32@0:8@"VSSpeechRequest"16@?<v@?B>24
Vv32@0:8@"VSSpeechRequest"16@?<v@?d@"NSError">24
Vv32@0:8@"VSSpeechRequest"16@?<v@?>24
Vv24@0:8@"VSSpeechRequest"16
Vv32@0:8@"VSSpeechRequest"16q24
Vv24@0:8@"VSPresynthesizedAudioRequest"16
Vv32@0:8@"NSString"16@?<v@?@"NSArray">24
Vv40@0:8@"NSString"16@"NSString"24@?<v@?@"NSArray">32
Vv24@0:8@?<v@?B>16
Vv32@0:8@"VSPreviewRequest"16@?<v@?dB>24
Vv40@0:8@"VSSpeechRequest"16q24@?<v@?@"NSString"@"NSError">32
Vv24@0:8@?<v@?@"AFXPCWrapper">16
Vv24@0:8@?<v@?@"NSError">16
Vv32@0:8@"NSString"16@?<v@?@"NSArray"@"NSError">24
Vv24@0:8@?<v@?@"NSArray"@"NSError">16
Vv40@0:8@"NSArray"16@"NSString"24@"NSUUID"32
Vv40@0:8@"NSString"16@"NSUUID"24@?<v@?@"NSArray">32
Vv24@0:8@?<v@?@"NSArray">16
Vv32@0:8@"NSArray"16@"NSString"24
Vv32@0:8@"NSString"16@?<v@?@"VSVoiceResourceAsset">24
Vv64@0:8@"NSString"16@"NSString"24q32q40q48@?<v@?@"VSVoiceAsset"@"NSError">56
Vv24@0:8@"SATTSSpeechSynthesisStreaming"16
Vv24@0:8@?<v@?i>16
Vv48@0:8@16q24{_NSRange=QQ}32
Vv44@0:8@16B24@28@36
Vv36@0:8@16B24@28
Vv48@0:8@"VSSpeechRequest"16q24{_NSRange=QQ}32
Vv44@0:8@"VSSpeechRequest"16B24@"NSString"28@"NSError"36
Vv32@0:8@"VSSpeechRequest"16@"VSInstrumentMetrics"24
Vv32@0:8@"VSSpeechRequest"16@"NSArray"24
Vv32@0:8@"VSSpeechRequest"16@"VSAudioData"24
Vv40@0:8@"VSSpeechRequest"16@"VSInstrumentMetrics"24@"NSError"32
Vv36@0:8@"VSPresynthesizedAudioRequest"16B24@"NSError"28
Vv40@0:8@"VSPresynthesizedAudioRequest"16@"VSInstrumentMetrics"24@"NSError"32
Vv24@0:8@"VSPreviewRequest"16
@"NSXPCConnection"
@"VSHHManagementClient"
@"VSHMHomeManager"
@"AFAudioPowerUpdater"
@"NSObject<OS_os_transaction>"
@"<VSOspreyTTSCoreDelegate>"
@"VSVoiceAsset"
@"VSServerTTSClient"
@"VSTimeoutCondition"
@"NSCondition"
@40@0:8@16@24q32
v56@0:8@16@24q32@?40@?48
v28@0:8@16B24
v32@0:8@16Q24
v28@0:8@"AVAudioPlayer"16B24
v32@0:8@"AVAudioPlayer"16@"NSError"24
v24@0:8@"AVAudioPlayer"16
v32@0:8@"AVAudioPlayer"16Q24
v40@0:8@16@?24@?32
@?16@0:8
v24@0:8@?16
@"AVAudioPlayer"
@"NSURL"
@"NSData"16@0:8
@32@0:8@"NSString"16@"NSData"24
@56@0:8@16@24@32@40@48
@"NSData"
@40@0:8@16@24@32
@24@0:8^{_NSZone=}16
@32@0:8@16r^{TTSRequestFeatureFlags=[1C]}24
@36@0:8@16r^{TTSRequestFeatureFlags=[1C]}24B32
{Offset<siri::speech::schema_fb::TTSRequestFeatureFlags>=I}24@0:8^v16
@"NSMutableDictionary"
r^{TTSRequestFeatureFlags=[1C]}
@32@0:8@16r^{TextToSpeechVoice=[1C]}24
@36@0:8@16r^{TextToSpeechVoice=[1C]}24B32
{Offset<siri::speech::schema_fb::TextToSpeechVoice>=I}24@0:8^v16
r^{TextToSpeechVoice=[1C]}
@32@0:8@16r^{TextToSpeechResource=[1C]}24
@36@0:8@16r^{TextToSpeechResource=[1C]}24B32
{Offset<siri::speech::schema_fb::TextToSpeechResource>=I}24@0:8^v16
r^{TextToSpeechResource=[1C]}
@32@0:8@16r^{TextToSpeechMeta=[1C]}24
@36@0:8@16r^{TextToSpeechMeta=[1C]}24B32
{Offset<siri::speech::schema_fb::TextToSpeechMeta>=I}24@0:8^v16
r^{TextToSpeechMeta=[1C]}
@32@0:8@16r^{TextToSpeechRequestMeta=[1C]}24
@36@0:8@16r^{TextToSpeechRequestMeta=[1C]}24B32
{Offset<siri::speech::schema_fb::TextToSpeechRequestMeta>=I}24@0:8^v16
r^{TextToSpeechRequestMeta=[1C]}
@32@0:8@16r^{TextToSpeechRequestContext=[1C]}24
@36@0:8@16r^{TextToSpeechRequestContext=[1C]}24B32
{Offset<siri::speech::schema_fb::TextToSpeechRequestContext>=I}24@0:8^v16
r^{TextToSpeechRequestContext=[1C]}
@32@0:8@16r^{TextToSpeechRequestExperiment=[1C]}24
@36@0:8@16r^{TextToSpeechRequestExperiment=[1C]}24B32
{Offset<siri::speech::schema_fb::TextToSpeechRequestExperiment>=I}24@0:8^v16
r^{TextToSpeechRequestExperiment=[1C]}
@32@0:8@16r^{TextToSpeechRequest=[1C]}24
@36@0:8@16r^{TextToSpeechRequest=[1C]}24B32
{Offset<siri::speech::schema_fb::TextToSpeechRequest>=I}24@0:8^v16
r^{TextToSpeechRequest=[1C]}
@32@0:8@16r^{ContextInfoEntry=[1C]}24
@36@0:8@16r^{ContextInfoEntry=[1C]}24B32
{Offset<siri::speech::schema_fb::TextToSpeechRequest_::ContextInfoEntry>=I}24@0:8^v16
r^{ContextInfoEntry=[1C]}
@32@0:8@16r^{AudioDescription=[1C]}24
@36@0:8@16r^{AudioDescription=[1C]}24B32
{Offset<siri::speech::schema_fb::AudioDescription>=I}24@0:8^v16
r^{AudioDescription=[1C]}
@32@0:8@16r^{WordTimingInfo=[1C]}24
@36@0:8@16r^{WordTimingInfo=[1C]}24B32
{Offset<siri::speech::schema_fb::WordTimingInfo>=I}24@0:8^v16
r^{WordTimingInfo=[1C]}
@32@0:8@16r^{TextToSpeechResponse=[1C]}24
@36@0:8@16r^{TextToSpeechResponse=[1C]}24B32
i16@0:8
{Offset<siri::speech::schema_fb::TextToSpeechResponse>=I}24@0:8^v16
r^{TextToSpeechResponse=[1C]}
@32@0:8@16r^{StartTextToSpeechStreamingRequest=[1C]}24
@36@0:8@16r^{StartTextToSpeechStreamingRequest=[1C]}24B32
{Offset<siri::speech::schema_fb::StartTextToSpeechStreamingRequest>=I}24@0:8^v16
r^{StartTextToSpeechStreamingRequest=[1C]}
{Offset<siri::speech::schema_fb::StartTextToSpeechStreamingRequest_::ContextInfoEntry>=I}24@0:8^v16
@32@0:8@16r^{BeginTextToSpeechStreamingResponse=[1C]}24
@36@0:8@16r^{BeginTextToSpeechStreamingResponse=[1C]}24B32
{Offset<siri::speech::schema_fb::BeginTextToSpeechStreamingResponse>=I}24@0:8^v16
r^{BeginTextToSpeechStreamingResponse=[1C]}
@32@0:8@16r^{PartialTextToSpeechStreamingResponse=[1C]}24
@36@0:8@16r^{PartialTextToSpeechStreamingResponse=[1C]}24B32
{Offset<siri::speech::schema_fb::PartialTextToSpeechStreamingResponse>=I}24@0:8^v16
r^{PartialTextToSpeechStreamingResponse=[1C]}
@32@0:8@16r^{FinalTextToSpeechStreamingResponse=[1C]}24
@36@0:8@16r^{FinalTextToSpeechStreamingResponse=[1C]}24B32
{Offset<siri::speech::schema_fb::FinalTextToSpeechStreamingResponse>=I}24@0:8^v16
r^{FinalTextToSpeechStreamingResponse=[1C]}
@32@0:8@16r^{TextToSpeechRouterStreamingStreamingRequest=[1C]}24
@36@0:8@16r^{TextToSpeechRouterStreamingStreamingRequest=[1C]}24B32
{Offset<siri::speech::qss_fb::TextToSpeechRouterStreamingStreamingRequest>=I}24@0:8^v16
r^{TextToSpeechRouterStreamingStreamingRequest=[1C]}
@32@0:8@16r^{TextToSpeechRouterStreamingStreamingResponse=[1C]}24
@36@0:8@16r^{TextToSpeechRouterStreamingStreamingResponse=[1C]}24B32
{Offset<siri::speech::qss_fb::TextToSpeechRouterStreamingStreamingResponse>=I}24@0:8^v16
r^{TextToSpeechRouterStreamingStreamingResponse=[1C]}
@48@0:8^{OpaqueCMTimebase=}16@24@32@?40
^{OpaqueCMTimebase=}16@0:8
@"NSObject<OS_dispatch_source>"
^{OpaqueCMTimebase=}
@44@0:8I16r^v20q28r^v36
^{OpaqueAudioQueue=}16@0:8
v24@0:8^{OpaqueAudioQueue=}16
v24@0:8^{OpaqueCMTimebase=}16
^{OpaqueAudioQueue=}
@"NSDate"
@"NSDictionary"
d64@0:8Q16{AudioStreamBasicDescription=dIIIIIIII}24
Q64@0:8d16{AudioStreamBasicDescription=dIIIIIIII}24
@64@0:8I16{AudioStreamBasicDescription=dIIIIIIII}20B60
v32@0:8@16@?24
@"VSAudioRouteInfo"
@"<VSAudioPlaybackServiceProtocol><AFAudioPowerProviding>"
@32@0:8@16^@24
@"<VSDeviceTTSCoreDelegate>"
@28@0:8@16i24
q24@0:8@16
@24@0:8d16
B24@0:8d16
@24@0:8Q16
@32@0:8Q16@24
@"VSMobileAssetsManager"
@"VSPreferencesInterface"
@"NSLock"
v56@0:8@16@?24@?32@?40@?48
f24@0:8d16
v32@0:8@16q24
@"CKKnowledgeStore"
v40@0:8@16@24d32
@"NSCache"
@"SATTSSpeechSynthesisResource"
@"SATTSSpeechSynthesisVoice"
@"VSPresynthesizedAudioRequest"
@"NSMutableData"
@"<AFAudioPowerProviding>"
{_opaque_pthread_mutexattr_t=q[8c]}16@0:8
v32@0:8{_opaque_pthread_mutexattr_t=q[8c]}16
@"NSMutableSet"
{_opaque_pthread_mutexattr_t="__sig"q"__opaque"[8c]}
v20@0:8i16
v48@0:8@16@?24@?32@?40
v64@0:8@16@24@32@40@48@?56
@"VSSpeechCache"
@"VSShortTermCache"
i24@0:8q16
i24@0:8@16
v44@0:8@16q24q32B40
v76@0:8q16d24@32q40q48Q56Q64B72
v56@0:8d16d24d32Q40q48
@"NSUUID"
@56@0:8{AudioStreamBasicDescription=dIIIIIIII}16
v40@0:8@16Q24@32
Q24@0:8@16
@"NSOperation<VSSpeechTaskProtocol>"
B64@0:8@16{AudioStreamBasicDescription=dIIIIIIII}24
@64@0:8{AudioStreamBasicDescription=dIIIIIIII}16@56
{AudioStreamBasicDescription=dIIIIIIII}24@0:8@16
@24@0:8q16
^v16@0:8
mcpl
xfuakrfc
333333
N11flatbuffers16DefaultAllocatorE
N11flatbuffers9AllocatorE
VoiceServicesErrorDomain
Synthesis is cancelled/interrupted.
writeWaveToFilePath failed.
v8@?0
@"NSError"16@?0@"VSSpeechSynthesisCallbackResult"8
v16@?0@"VSSpeechWordTimingInfo"8
VSAudioPlaybackServiceAVSBARQueue
Error in creating block buffer for Sample buffer
Error in CMAudioFormatDescriptionCreate
Error in CMAudioSampleBufferCreateWithPacketDescriptions
Error in creating block buffer for Silence buffer
Error in CMAudioFormatDescriptionCreate from Silence buffer creation
Error in CMAudioSampleBufferCreateWithPacketDescriptions from silence buffer
{?=qiIq}
Timeout waiting for AVSampleBufferRenderSynchronizer
q24@?0@"NSValue"8@"NSValue"16
v32@?0@"NSValue"8Q16^B24
cached_engine_%@_%@
Use `initWithRequest:`.
%@ %@ %@ %@ %.2f %.2f %.2f %@ %@
Speech is cancelled/interrupted.
v20@?0B8@"NSString"12
ServerTTSErrorDomain
Use `initWithRequest:shouldSpeak:`.
%@ %@
v40@?0@"NSData"8Q16@"NSData"24^B32
Unable to create playback service
EagerTTS:%@:%@:%@:%@:%@:%@:%@:%@:%@
Cancelled
Finished
speaking
synthesizing
voice
(null)
is_eager
is_one_shot
is_time_out
is_device_tts
source_of_tts
com.apple.springboard
com.apple.siri
com.apple.CarPlayApp
com.apple.Carousel
com.apple.AssistantServices
com.apple.SiriHeadlessService
\mrk=emo=whisper\
com.apple.voiced.can-dump-audio
com.apple.voiced.request.durationestimation
com.apple.voiced.request.speech
com.apple.voiced.request.presynthesis
unknown:unknown:PresynthesizedAudio
unknown:PresynthesizedAudio:unknown
com.apple.voiced.request.synthesis
com.apple.voiced.request.text_to_phonemes
VSAudioPowerUpdateQueue
-[VSSpeechXPCHandler beginAudioPowerUpdateWithReply:]
v16@?0@"AFXPCWrapper"8
-[VSSpeechXPCHandler endAudioPowerUpdate]
%@:%@:%@:%@
Missing languageCode
-[VSSpeechXPCHandler getVoiceInfoForLanguageCode:name:footprint:gender:type:reply:]
VSOspreyTTSCoreCallbackQueue
v24@?0@"VSAudioData"8@"NSArray"16
v16@?0@"NSError"8
Osprey round-trip TTS timed out
v28@?0@"VSVoiceAsset"8@"VSVoiceResourceAsset"16f24
Osprey streaming network stall
Osprey streaming TTS timed out
Osprey core is not able to provide audio in time
com.apple.voiced.postInstall
v20@?0f8^B12
com.apple.voiced.neural-compiling
v12@?0i8
v16@?0@"NSObject<OS_xpc_object>"8
com.apple.MobileAsset.VoiceServicesVocalizerVoice
com.apple.MobileAsset.VoiceServices.CustomVoice
com.apple.MobileAsset.VoiceServices.GryphonVoice
com.apple.MobileAsset.VoiceServices.VoiceResources
com.apple.voiced.voicePreviewQueue
com.apple.voiceservices
VoicePreviews
_Buddy
%@_%@%@.caf
v20@?0d8B16
InvalidCache
Audio duration too short
duration %.2f second
TTSResources/PreinstallCache/
VSSpeechCacheErrorDomain
-[VSSpeechCache initWithStorePath:]
VoiceServices
Cache type name too long
-[VSSpeechCache addCache:]
%@_%@
%@:%@:%@:%@:%@:%@
gryphon
unknown
premium
%@:%@:%@
preinstalledCache
FlatBuffers 1.12.0
v24@?0^v8Q16
resource
context_info
meta_info
context
experiment
feature_flags
decoder_description
playback_description
word_timing_info
v20@?0r*8I16
content
Verifier
flatbuffers.h
size_ < FLATBUFFERS_MAX_BUFFER_SIZE
NotNested
!nested
!num_field_loc
ensure_space
cur_ >= scratch_ && scratch_ >= buf_
size() < FLATBUFFERS_MAX_BUFFER_SIZE
reallocate_downward
new_size > old_size
EndTable
nested
table_object_size < 0x10000
!ReadScalar<voffset_t>(buf_.data() + field_location->id)
data
cur_
scratch_end
scratch_
scratch_data
buf_
finished
ReferTo
off && off <= GetSize()
EndVector
Finish
strlen(file_identifier) == kFileIdentifierLength
allocator<T>::allocate(size_t n) 'n' exceeds maximum supported size
'%c%c%c%c', %.0fhz, %d bits, %d FPP, 
%@:%@
voiced_tts_playback_queue
Error AudioQueueStart
Error AudioQueueFlush
Error AudioQueueStop
Error AudioQueuePause
Error AudioQueueAddPropertyListener
Error AudioQueueRemovePropertyListener
YYYY-MM-dd hh:mm:ss:SSS
Error AudioQueueGetProperty isRunning
Unable to enable kAudioQueueProperty_EnableLevelMetering
Unable to disable kAudioQueueProperty_EnableLevelMetering
-[VSAudioPlaybackServiceAT getAveragePower:andPeakPower:]
Unable to get kAudioQueueProperty_CurrentLevelMeterDB
hdft
Hdft
usbD
hx90
wx90
rhac
wdef
com.apple.voiceservices.notification.voice-update
Missing utterance in the request (preprocessing missing?).
No voice available
Compact voice is explicitly disabled.
Voice is deleted already.
Can't create VSSpeechEngine
CACHE_DELETE_AMOUNT
CACHE_DELETE_VOLUME
com.apple.voiced.CacheDelete
r^{__CFDictionary=}20@?0i8r^{__CFDictionary=}12
com.apple.voiced.downloadQueue
on accessory %@
q24@?0@"VSVoiceAsset"8@"VSVoiceAsset"16
v20@?0d8f16
https://dejavu.apple.com
https://seed-dejavu.siri.apple.com
https://carry-dejavu.siri.apple.com
/siri.speech.qss_fb.Blazar/TextToSpeechRouter
v16@?0@"OspreyMutableRequest"8
-[OspreyTTSService roundTripTTS:responseHandler:]_block_invoke_2
Empty data
-[OspreyTTSService roundTripTTS:responseHandler:]_block_invoke
Invalid data
Error %d in response: %@
v24@?0@"NSData"8@"NSError"16
/siri.speech.qss_fb.Blazar/TextToSpeechRouterStreaming
Corrupted Osprey response.
-[OspreyTTSService streamTTS:beginHandler:chunkHandler:endHandler:completion:]_block_invoke
v16@?0@"NSData"8
/private/var/mobile/Library/Logs/CrashReporter/VoiceServices/
mobile
-[VSDiagnosticService createDirectoryIfNeeded]
yyyy_MM_dd-HHmmss.SSS
TTS-%@
.tmp
.wav
TTSMetrics-%lld
.json
default
ServerTTSTimeoutV2
DeviceWaitTimeV2
TTSExperimentConfig
identifier
method
delayed
AllowedAppId
com.apple.MapsSupport
com.apple.Translate
com.apple.SessionTrackerApp
Use `initWithRequest:withStreamID:`.
Unknown inline streaming error %d, %@
Missing utterance in the request (preprocessing missing?). Can't fallback to device TTS.
voice_resource
task: inprogress %@, request: %@
Can't create VSAudioPlaybackService
Can't decode audio data
audio_duration
com.apple.voiced.VSInlineStreamService
fe_feature
fe_feature_only
language
gender
name
version
quality
type
channel_type
app_id
dialog_identifier
experiment_identifier
speech_id
session_id
text
audio_type
enable_word_timing_info
voice_name
preferred_voice_type
value
sample_rate
format_id
format_flags
bytes_per_packet
frames_per_packet
bytes_per_frame
channels_per_frame
bits_per_channel
reserved
word
sample_idx
offset
length
timestamp
error_code
error_str
audio
stream_id
streaming_playback_buffer_size_in_seconds
current_pkt_number
total_pkt_number
content_type
com.apple.voiceservices.notification.voice-purge
com.apple.voiced.prewarmQueue
Prewarm textify emoji
gryphon_frontend
VoiceServices/config
-[VSServerTTSClient ospreyStartSynthesisRequest:responseHandler:completion:]_block_invoke
Unable to process audio data.
v24@?0@"OPTTSTextToSpeechResponse"8@"NSError"16
v16@?0@"OPTTSBeginTextToSpeechStreamingResponse"8
v16@?0@"OPTTSPartialTextToSpeechStreamingResponse"8
v16@?0@"OPTTSFinalTextToSpeechStreamingResponse"8
rate
pitch
volume
isEager
neuralIssue
com.apple.voiced.cachingQueue
v32@?0@"NSData"8Q16@"NSData"24
Speaker
CarAudioOutput
com.apple.voiced.pthreadQueue
com.apple.voiced.speakingQueue
audioDataFromFile:error:
AudioFileOpenURL
AudioFileGetProperty kAudioFilePropertyDataFormat
AudioFileGetProperty kAudioFilePropertyAudioDataByteCount
AudioFileGetProperty kAudioFilePropertyAudioDataPacketCount
+[VSAudioData(SAUIAudioData) audioDataWithASBD:rawData:]
male
female
neutral
\vol=%d\%@
\rate=%d\%@
\pitch=%d\%@
SynthesisTask done synthesize %lu characters, audio duration %f, error %@
Task %llu reported word time info
Device SpeakTask %llu: Instrument metric: %@
Task %llu started speaking
Device EagerTask %llu: Instrument metric: %@
Task %llu reported finish, error: %@
Can't retrieve session with ID: %d
#AVSBAR initialized with session ID: %d, reusing previous synchronizer: %{BOOL}d
VSAudioPlaybackService %p init latency: %.3f
mediaserverd reset
#AVSBAR synchronizer.rate will be set to 1 with enqueued audio duration %f sec. Previous rate: %f
#AVSBAR already stopped or paused: will not resume rate
_synchronizer play rate high latency: %.3f sec
#AVSBAR synchronizer.rate was set to 1. Current rate: %f
Invalid sample buffer
Invalid silence buffer
Error in creating block buffer for Sample buffer
Error in CMAudioFormatDescriptionCreate
Error in CMAudioSampleBufferCreateWithPacketDescriptions
#AVSBAR already stopped or waiting for finish: will not enqueue more
#AVSBAR empty audio data: will not enqueue it
Adding to enqueuedMappedAudioInfo: %f sec
Error in creating block buffer for Silence buffer
Error in CMAudioFormatDescriptionCreate from Silence buffer creation
Error in CMAudioSampleBufferCreateWithPacketDescriptions from silence buffer
#AVSBAR EndOfDataAttachment ready for enqueuing
#AVSBAR Call to provide more audio data during state %ld.
#AVSBAR Enqueuing to %@: %f sec
_renderer enqueueSampleBuffer high latency: %.3f sec
#AVSBAR Renderer %@ not anymore ready for more media data. enqueuedMappedAudioInfo count left: %lu
#AVSBAR flushAndStop
#AVSBAR already stopped or waiting for finish
#AVSBAR Synchronizer reached endTime
#AVSBAR Waiting for synchronizer finishing playing between current %f sec and until %f sec
#AVSBAR Synchronizer is stalled with rate %f at time %f.
Stopping synchronizer and renderer
#AVSBAR synchronizer.rate will be set to 0 and time set to 0 (from current time: %f). Then renderer will be flushed.
_synchronizer stop rate high latency: %.3f sec
#AVSBAR synchronizer.rate was set to 0. Current rate: %f
#AVSBAR renderer was flushed
Pausing synchronizer
#AVSBAR synchronizer.rate will be set to 0 (at current time: %f).
_synchronizer pause rate high latency: %.3f sec
#AVSBAR Dropping %lu enqueued data
Error AudioUnitSetProperty _floatConverter %@
Error AudioUnitSetProperty _integerConverter %@
Error AudioComponentInstanceNew _voiceBoostUnit %@
Error AudioUnitSetProperty _voiceBoostUnit, kAudioUnitProperty_MaximumFramesPerSlice %@
Error AudioUnitSetProperty _voiceBoostUnit, kAudioUnitProperty_StreamFormat, kAudioUnitScope_Input %@
Error AudioUnitSetProperty _voiceBoostUnit, kAudioUnitProperty_StreamFormat, kAudioUnitScope_Output, %@
Error AudioUnitInitialize _voiceBoostUnit %@
Error AudioUnitSetParameter %@
Error AudioConverterConvertComplexBuffer _floatConverter %@
Error AudioUnitProcess _voiceBoostUnit %@
Error AudioConverterConvertComplexBuffer _integerConverter %@
Using timestamp inside voiced for Estimation task
Created Estimation task %llu
Unable to create engine for request %@
Estimated duration: %.2f, for utterance: %@
Using timestamp inside voiced for task
Created Task %llu (%p)
Starting speech task %llu
Short-term cached synthesis is found for text '%@'
Detected synthesis stall, starting tailspin
Finished tail spin, success:%d, file: %@
Holding audio playback before we get fast synthesis.
SpeakTask done synthesize %lu characters, audio duration %f, error %@
Device task %llu: Instrument metric: %@
Using requestCreatedTimestamp inside voiced for Server task
Created Server task %llu: shouldSpeak %{BOOL}d
Received server TTS response. Use Server TTS.
Received device synthesis previously, ignore server TTS.
Encountered Osprey streaming network stall. Retry with device TTS.
Server network error: %@
Start device synthesis fallback.
Start playing device synthesis instead.
Received server TTS previously, ignore device TTS
Received audio from device synthesis. Use device synthesis immediately.
Received audio from device synthesis, but it's deferred.
Inline server TTS is previously cached.
Eager server TTS is previously cached.
Device TTS is racing with Server TTS
Device TTS wait time for server audio: %.2f
Device TTS will not race
Server task %llu started speaking
Server task %llu: Instrument metric: %@
Error in server task %llu, error: %@
Server task %llu: %@ %@ utterance: '%@', %{public}@
Invalidate VSSpeechXPCHandler, cancelling all related tasks
%{public}@ is not TTS language, fallback to %{public}@
Use SSML input: %@
Utterance to synthesize for request %llu: '%@'
Overwriting volume with internal default: %.3f
Overwriting rate with internal default: %.3f
Overwriting pitch with internal default: %.3f
Process is not entitled for dumping audio. Ignore outputPath
Unexpected client '%{public}@' sets Siri request ID.
Overriding disableDeviceRacing with internal default
Update with connection identifier: %{public}@, keepActive:%{BOOL}d
Keep active session for '%@'
Remove active session for '%@'
Find on-going task: %@, ignoring prewarm request: %@
Server Inline Streaming TTS is disabled in internal settings
Server TTS is disabled in internal settings
Created stream speak task %llu
Created server speak task %llu
Created speak task %llu
Created presynthesized task %llu
Cache #PresynthesizedRequest %llu with text: %@
Cache #PresynthesizedRequest %llu skipped: no audio
Ignore stopPresynthesizedAudioRequest. Cannot find task with associated request %llu.
Created server synthesis task %llu
Created synthesis task %llu
Found matched inline streaming request, cancel synthesis task %llu
Ignore pauseSpeechRequest. Current request is different than requested request.
Ignore continueSpeechRequest. Current request is different than requested request.
Ignore stopSpeechRequest. Cannot find task with associated request %llu.
Created phonemes task %llu
ignored client '%{public}@' setting auto-download for a non-existing accessoryId '%@'
client '%{public}@' and accessory '%@' set auto download voice assets:%{public}@
Start cellular download for %@
Begin getVoiceInfoForLanguageCode: %{public}@, %@, %@
%s override voice info for server TTS platform, %@
End getVoiceInfoForLanguageCode: %@
Ignore stream object with nil stream ID: %@
Enqueue stream object %@, streamId: %@
Received invokeDaemon, I'm listening
Received killDaemon, shutting down
Simulate network stall is on, ignore audio object
Refresh timeout value as %.2f
Simulate network stall is on, ignore completion callback
Network stall in osprey streaming
Timeout in osprey streaming
Osprey core %p is cancelled
Registered xpc activity com.apple.voiced.postInstall
Running activity com.apple.voiced.postInstall
com.apple.voiced.postInstall is requested to be deferred.
Unable to set defer state for com.apple.voiced.postInstall
Assets migration progress: %f
Re-triggering neural compiling afer OS upgrade.
Triggered 'com.apple.voiced.neural-compiling' with error %d
Migration service finished.
xpc activity com.apple.voiced.postInstall, failed to set state to done.
Unexpected xpc activity state %d for 'com.apple.voiced.postInstall'
Defaults disables reset, skip resetting MobileAsset default URL
Resetting MobileAsset default URL
Unable to locate preview sample file at '%@'
StartVoicePreview for languageCode %@ voiceName %@ previewType %ld
Preview ignored for %@
Unable to play preview sample file at '%@'
StopVoicePreview for file '%@'
Reading cache %@ error: %@
Error %s, %@
Cache type name too long %@
{public}%@ is not TTS language, falling back to %{public}@
Error in reading audio data from file: %@ error:%@
Error AudioQueueNewOutputWithAudioSession %@
Unable to set kAudioQueueProperty_ClientUID, errno: %@
Error CMTimebaseCreateWithSourceClock: %@
AudioQueue initialized with session ID: %d
Error AudioQueueDispose %@
Signal AudioQueue running state change
Error AudioQueueStart %@
VSAudioPlaybackService %p success AudioQueueStart
Error AudioQueueAllocateBuffer %@
Audio queue start sample time: %.0f
Detected stalled audio generation, will enqueue %d silence frame to compensate.
Error AudioQueueEnqueueBuffer %@
VSAudioPlaybackService %p enqueued audio buffer at sample time: %.2f, size: %ld, total enqueued samples: %.0f, discontinuity: %{BOOL}d
AudioQueue will flushAndStop
Timeout in AudioQueue dequeue condition.
Error AudioQueueFlush %@
Error AudioQueueStop %@
AudioQueue will stop
Error AudioQueuePause %@
VSAudioPlaybackService %p success AudioQueuePause
Error AudioQueueAddPropertyListener %@
Error AudioQueueRemovePropertyListener %@
Detected stall of audio queue, based on NSDate. Now: %@, supposed end time: %@, Tolerance: %.2f
Error AudioQueueGetProperty isRunning %@
Unable to enable kAudioQueueProperty_EnableLevelMetering, err: %@
Unable to disable kAudioQueueProperty_EnableLevelMetering, err: %@
Error: %s, errno: %@
VSAudioPlaybackService %p played audio buffer at sample time: %f, size: %ld
Error AudioQueueFreeBuffer %@
Current audio output route: %@
AudioPlayback
Device core %p is cancelled
On-disk cached synthesis %@ is found.
In-memory cached synthesis %@ is found.
Reset MobileAsset query cache and retry selecting voice
No voice available
Compact voice is explicitly disabled.
Voice is deleted at path '%@'
#CacheDelete asset cleaning is disabled in internal setting. Skip purgeable assets for urgency %d
#CacheDelete purgeable active voice asset: %@
#CacheDelete purgeable inactive voice asset: %@
#CacheDelete query purgeable size, urgency: %d / %d, info: %@
#CacheDelete purge, urgency: %d / %d, info: %@
#CacheDelete periodic purge, urgency: %d / %d, info: %@
Asset update is disabled in internal settings.
Start updating voice and voice resources.
%@ %@ has a subscribed voice: %{public}@
Voice download is in progress, skip new download. %@
Updating target voice: %@
Voice update decision: shouldDownload:%d, canUseBattery:%d. Reason: triggerType:%d, compactVoiceSelected:%d, mismatchedVoiceName:%d, activeSiriUser:%d, serverExperimentDelay:%d
Downloaded voice is not ready to use. Start ANE compiling immediately for voice: %@
Voice asset downloading progress: %.2f, remainingTime: %.2f, voice: %@
Start ANE compiling immediately for voice: %@
Updating VoiceResource for '%{public}@'
Sent Osprey grpc request with speech_id '%@', session_id '%@', app_id '%@'
%s, Error: %@
Sent Osprey streaming request with speech_id '%@', session_id '%@', stream_id '%@', app_id '%@'
Corrupted Osprey response, stream ID: %@
Osprey streaming received Begin response with non 200 status: %d
Osprey streaming received Begin response %@
Osprey streaming received Chunk response with non 200 status: %d
Osprey streaming received Chunk response, pkt number: %d
Osprey streaming received End response with non 200 status: %d
Osprey streaming received End response, total pkt: %d
%s, Unknown response from Osprey for streaming TTS
Osprey streaming invokes completion with error %@
Osprey streaming invokes completion callback
Created audio dump directory %@
No compressed audio do dump
Unable to create intermediate audio dump at '%@'
Unable to create audio dump at '%@', error: %@
Audio save as %@
No audio do dump
No json data to dump
Unable to parse json for dictionary '%@', error: %@
Unable to create instrument metrics json dump at '%@'
Instrument metrics json dump saved as %@
Unable to parse json for key '%@', error: %@
JSON for key '%@' is not dictionary
Added short term cache:%@ for key:'%@'
Removed short term cache for key:'%@'
Removed short term cache for all keys
Using timestamp inside voiced for Stream task
Created Stream task %llu: streamID %@
Simulate network stall is on, ignore object %@
Unknown streaming object: %@
Handle stream begin with streamId: %@, text: %@, decoder: %@
Ignoring stream begin: error already occurred: %ld
Handle stream chunk with streamId: %@
Reached buffer threshold. Start playing audio.
Handle stream end with streamId: %@, count: %@
Stream TTS network stall.
Inline streaming TTS timeout.
Streaming error: %@, error_code: %d
Error in stream task %llu, error: %@
Stream task %llu: %@ speaking text: '%@', %{public}@
Stream task %llu: Instrument metric: %@
Initializing fallback playback service
Using timestamp inside voiced for Presynthesized task
Created Presynthesized Task %llu
Speaking pre-synthesized audio: %@
Error in audio task %llu, error: %@
Audio task %llu: %@ speaking utterance '%@', %{public}@
Received inline streaming TTS with id %@, text: %@
Notification for %@ is on-going. Posting object immediately %@
Notification for %@ has not started. Cache object %@
Start notifying for: %@
No cached object found for notification %@.
%d cached objects found for notification: %@
Notify %@ with cached object %@
Remove notification %@
Prewarming: Invoked with request: '%@'
Unable to initialize Device Authentication session: %@
Device Authentication session is initialized
Unable to prewarm, error: %@
Can't prewarm engine with path '%@'
Prewarm finished. Latency: %.3f
Prewarming: Completed with request: '%@'
Can't create engine with path '%@'
Voice specific resources found.
Specified resource file '%@' does not exist at: '%@'
disableServerTTS is enabled by user default, disable server TTS
forceServerTTS is enabled by user default, force server TTS
forceServerTTS is enabled by speech request, force server TTS
Preinstalled cache is found, disable server TTS
Neural voice is found on device without fallback condition, disable server TTS
Short term cache is found for the text, use server TTS
Server TTS is disabled since '%{public}@' is not in the list of allowed apps
%s, %@
Unable to get power policy from Siri, error: %@
playbackService is initialized already.
Can't create VSAudioPlaybackService
Starting AudioQueue
Task %llu fetched voice %@
Error in device task %llu, error: %@
Device task %llu: %@ %@ utterance: '%@', %{public}@
Cached streamAudio in task %llu with hash %@ in memory
Cached audio in task %llu with hash %@ in memory
Error converting audio during caching. %@
Error converting stream audio during caching. %@
Caching is disabled. Skipping caching.
Unrecognized audio object, skip caching
Audio duration is too short: %.2f second, skip caching
Audio duration is too long: %.2f second, skip caching
Compressing audio for caching.
Audio compressed for caching.
Can't add audio cache, error: %@
Preinstalled cached synthesis %@ is found.
AssistantSiriAnalytics should always derive an identifier for SISchemaComponentName_COMPONENTNAME_TTS
Error AudioFileCreateWithURL: '%@', code: %@
Unable to begin OPUS decoder, %@
Error during decoding, %@
Error AudioFileWriteBytes: '%@', code: %@
Error AudioFileClose: '%@', code: %@
Start spinNextTask
Dispatch speaking task %llu
Starting task %llu
PresynthesisTask %llu requested to wait another speaking task %llu
New speak task %llu interrupts speaking task %llu
New speak task %llu waits for speaking task %llu
%llu interrupt task %llu
Speak task %llu is attached to eager task %llu
Dispatch synthesis task %llu
Finish spinNextTask
Starting text to phonemes task %llu
Finished text to phonemes task %llu
decoderStreamDescription formatID: %@, sample rate: %@
Unknown server audio format ID: %d
%s, invalid opus data
%s, Unknown format: %d
Invalid chunk size: %d at offset %d, bytes count = %d
Unable to convert OPUS to PCM. %@
Decoding opus for dumping.
Opus decoded for dumping.
OPTTSTextToSpeechResponse word timing info, offset: %ld, length: %ld, word: %@, sampleIndex: %d, timestamp: %.2f
Unable to madvise file '%@' MADV_DONTNEED, error: %s
Unable to munmap file '%@', error: %s
Unable to open file '%s', error: %s
Unable to get size of file '%s', error: %s
Unable to mmap '%s', error: %s
fcntl called on file '%@', size: %lu
VSSpeechSynthesisTask
VSSpeechEagerProtocol
VSSpeechSpeakableProtocol
VSSpeechTaskProtocol
NSObject
VSAudioMappedInfoAVSBAR
VSAudioMappedInfo
VSAudioPlaybackServiceAVSBAR
VSAudioPlaybackServiceProtocol
AFAudioPowerProviding
VSVoiceBooster
VSDurationEstimationTask
VSHMHomeManager
VSSpeechSpeakTask
VSSpeechServerTask
VSDeviceTTSCoreDelegate
VSOspreyTTSCoreDelegate
VSSpeechXPCHandler
VSSpeechXPCServiceProtocol
VSSpeechServiceDelegate
VSOspreyTTSCore
VSPostInstallService
VSVoicePreviewTask
AVAudioPlayerDelegate
VSSpeechCacheAudio
VSSpeechCacheItem
VSSpeechCache
OPTTSTTSRequestFeatureFlags
FLTBFBufferAccessor
NSCopying
OPTTSTextToSpeechVoice
OPTTSTextToSpeechResource
OPTTSTextToSpeechMeta
OPTTSTextToSpeechRequestMeta
OPTTSTextToSpeechRequestContext
OPTTSTextToSpeechRequestExperiment
OPTTSTextToSpeechRequest
OPTTSTextToSpeechRequest_ContextInfoEntry
OPTTSAudioDescription
OPTTSWordTimingInfo
OPTTSTextToSpeechResponse
OPTTSStartTextToSpeechStreamingRequest
OPTTSStartTextToSpeechStreamingRequest_ContextInfoEntry
OPTTSBeginTextToSpeechStreamingResponse
OPTTSPartialTextToSpeechStreamingResponse
OPTTSFinalTextToSpeechStreamingResponse
OPTTSTextToSpeechRouterStreamingStreamingRequest
OPTTSTextToSpeechRouterStreamingStreamingResponse
VSAceObjectUtility
VSHHManagementClient
VSAudioMappedInfoAT
VSOccasionalTimesObserver
VSAudioPlaybackServiceAT
VSAudioRouteInfo
VSAudioPlaybackService
VSDeviceTTSCore
VSCacheDeleteService
VSTimeoutCondition
VSDownloadService
OspreyTTSService
SpeechService
VSDiagnosticService
VSSiriServerConfiguration
VSShortTermCache
VSSiriInlineTTSStreamTask
VSSpeechPresynthesizedTask
VSSpeechAudioPowerService
VSInlineStreamService
OPTTSMutableTTSRequestFeatureFlags
OPTTSMutableTextToSpeechVoice
OPTTSMutableTextToSpeechResource
OPTTSMutableTextToSpeechMeta
OPTTSMutableTextToSpeechRequestMeta
OPTTSMutableTextToSpeechRequestContext
OPTTSMutableTextToSpeechRequestExperiment
OPTTSMutableTextToSpeechRequest
OPTTSMutableTextToSpeechRequest_ContextInfoEntry
OPTTSMutableAudioDescription
OPTTSMutableWordTimingInfo
OPTTSMutableTextToSpeechResponse
OPTTSMutableStartTextToSpeechStreamingRequest
OPTTSMutableStartTextToSpeechStreamingRequest_ContextInfoEntry
OPTTSMutableBeginTextToSpeechStreamingResponse
OPTTSMutablePartialTextToSpeechStreamingResponse
OPTTSMutableFinalTextToSpeechStreamingResponse
OPTTSMutableTextToSpeechRouterStreamingStreamingRequest
OPTTSMutableTextToSpeechRouterStreamingStreamingResponse
VSPrewarmService
VSServerTTSClient
Utilities
VSCachingService
VSSiriInstrumentation
VSStreamAudioMappedInfo
VSStreamAudioData
VSSpeechTaskQueue
VSTextToPhonemesTask
SAUIAudioData
VSHelpers
VSMemoryMap
neuralFallback
downloadOptionsWithBattery:
setVoiceType:
setErrorHandler:
mutableBytes
defaultRate
isReadableFileAtPath:
updateMeters
doubleValue
moveItemAtPath:toPath:error:
defaultPitch
setVoiceSettings:
requestMediaDataWhenReadyOnQueue:usingBlock:
setErrorCodes:
unsignedIntegerValue
initWithSourceASBD:
pathWithComponents:
domain
synthesisEndTimestamp
attributeForKey:
defaultManager
unsignedIntValue
vs_textifyEmojiWithLanguage:
setOutputPath:
standardInstance
setStartedOrChanged:
contentVersion
exceptionWithName:reason:userInfo:
setErrorCode:
setAllowsCellularAccess:
setOpusDataHandler:
path
migrateAssetsWithProgress:
defaultInstance
vs_substituteAudioWithLocalPath
requestCreatedTimestamp
eventMetadata
setStartTime:
unlock
previewType
setVoiceResourceAssetKey:
disableServerTTS
shouldWhisper
vs_stringFrom4CC:
isInternalBuild
shouldStreamAudioData
unloadResource:
estimatedTTSWordTimingForText:withLanguage:voiceName:
setActive:withOptions:error:
speechSynthesisVoice
disableOspreyStreaming
setObject:forKeyedSubscript:
setEnded:
vs_insertContextInfo:
defaultCenter
isHomePod
bytesPerPacket
setVoiceName:
setActive:error:
speechSynthesisResource
disableMobileAssetURLReset
mainRunLoop
getBytes:range:
vs_convertToSSML
setVoiceGender:
bytesPerFrame
arrayWithObjects:count:
isHomeHub
disableInlineStreamTTS
setObject:forKey:
synthesisBeginTimestamp
setSpeechEndTimestamp:
arrayWithCapacity:
setVoiceFootprint:
unaryRequestWithMethodName:requestData:requestBuilder:responseHandler:
setSpeechContext:
containsObject:
isFinished
outputs
generateTTSPhonemes:voicePath:phonemeSystem:error:
initWithProvider:queue:frequency:delegate:
decoderStreamDescription
disableDeviceRacing
bytesAtOffset:
setVoiceFallbackOccurred:
lowercaseString
volume
array
unarchivedObjectOfClasses:fromData:error:
initWithNSUUID:
errorWithDomain:code:userInfo:
setEagerRequestCreatedTimestampDiffs:
disableCompactVoiceFallback
bytes
decodeChunks:streamDescription:outError:
lowInactiveMemory
archivedDataWithRootObject:requiringSecureCoding:error:
setVoiceContext:
reportInstrumentMetrics:
setNeuralFallback:
errorMessage
typeStringFromType:
setDisableDeviceRacing:
isExecuting
genderFromString:
disableCache
decodeChunk:outError:
bundleWithIdentifier:
serverTTSTimeout
setSpeechBeginTimestamp:
longLongValue
voiceType
typeFromString:
appendData:
setNeuralAudioClick:
disableAssetUpdate
logWithEventContext:ttsIdentifier:
errorCode
concatenateWithAudio:
isEqualToString:
voiceSettings
bundlePath
signal
subscribedVoicesForClientID:accessoryID:
outputPath
appendBytes:length:
dateWithTimeIntervalSinceNow:
setNeuralAlignmentStall:
preinstalledAudioHashForLanguage:name:
isEqualToDictionary:
initWithInteger:
logWithEventContext:
disableAssetCleaning
subdataWithRange:
dateByAddingTimeInterval:
preheat
ttsSynthesisLatency
initWithInt:
JSONObjectWithData:options:error:
enumerateObjectsWithOptions:usingBlock:
setDelaysRateChangeUntilHasSufficientMediaData:
setVoiceAssetKey:
serverStreamingRequestWithMethodName:requestData:requestBuilder:streamingResponseHandler:completion:
directorySize:
stringWithUTF8String:
broadcast
setSourceOfTTS:
setMeteringEnabled:
date
logUtterance
setVoiceAccent:
renderers
ttsPolicy
setSource:
initWithFloat:
serverFirstPacketTimestamp
stringWithFormat:
logText
dataWithLength:
componentsSeparatedByString:
preferredDownloadForVoice:
setDateFormat:
ospreyEndpointURL
isCancelled
allocWithZone:
stringOfSourceOfTTS:
dataWithJSONObject:options:error:
removeVoiceResource:completion:
framesPerPacket
setCustomerPerceivedLatencyInSecond:
speechEndTimestamp
setUuid:
componentsJoinedByString:
stringFromDate:
allValues
bitsPerChannel
setLinkId:
dataWithContentsOfFile:
voiceResourceAssetKey
initWithDouble:
precomposedStringWithCanonicalMapping
enqueueSampleBuffer:
setUtterance:
URLWithString:
stringByReplacingOccurrencesOfString:withString:
beginUpdate
dataWithCapacity:
localizedInterstitialStringForKey:language:
powerProfile
speechBeginTimestamp
adjustWordTimingInfo:forContext:
beginEncoding
dataWithBytesNoCopy:length:freeWhenDone:
setServerStreamedAudioDuration:
stringByAppendingString:
addTimer:forMode:
dictionaryWithObjects:forKeys:count:
formatID
setLastTTSRequestDate:
localizedDescription
voicePath
removeObserver:
postNotificationName:object:
beginChunkDecoderForStreamDescription:
code
orderedSet
formatFlags
stringByAppendingPathExtension:
initWithData:encoding:
setServerLastPacketTimestamp:
dictionaryRepresentation
loadResourceAtPath:mimeType:error:
dataWithBytes:length:
voiceName
setUseCompression:
portType
isANECompilationPlatform
initWithCurrentProcess
forceServerTTS
clientID
stringByAppendingPathComponent:
dataUsingEncoding:
averagePowerForChannel:
setServerFirstPacketTimestamp:
setLanguages:
selectVoiceResourceAssetForLanguage:
opaqueSessionID
dictionaryMetrics
loadResource:error:
removeObjectAtIndex:
addRenderer:
setLanguageCode:
clientBundleIdentifier
dictionary
initWithContentsOfURL:fileTypeHint:error:
availableLanguages
selectVoiceForLang:name:type:gender:footprint:
voiceData
removeObject:
data
footprintStringFromFootprint:
addObserver:selector:name:object:
timerWithTimeInterval:target:selector:userInfo:repeats:
searchPathURL
voiceContext
setContentVersion:
setSearchPathURL:
linkId
footprintFromString:
initWithClient:accessory:voice:
addObjectsFromArray:
customResourceURLs
enqueue
setResourceVersion:
integerValue
cleanUnusedAssets
UTF8String
removeDirectory:
sourceOfTTS
setRequestedVoiceContext:
footprint
intValue
UUID
streamingPlaybackBufferSize
objectForKeyedSubscript:
lastTTSRequestDate
initWithBytesNoCopy:length:deallocator:
CMTimeValue
audioStartTimestampDiffs
sortedArrayUsingComparator:
addObject:
setComponent:
currentTime
shouldCache
setIsWarmStart:
cleanDirectory:withLRULimit:
setRequestReceivedTier1:
size
endUpdate
deviceUUID
lastPathComponent
initWithBytes:length:encoding:
voiceAssetsForSubscription:
audioSessionID
setIsSpeechRequest:
cleanDirectory:withDateOlderThan:
flush
setTimeoutIntervalForResource:
setRequestReceived:
setCompletionBlock:
initWithBool:
currentRoute
writeToFile:options:error:
floatValue
objectAtIndexedSubscript:
audioSession
remoteObjectProxy
setIsServerTimeout:
playerStreamDescription
lastObject
voiceAssetKey
endEncoding
setClientTraceIdentifier:
setTimeoutIntervalForRequest:
currentPowerPolicyWithError:
timeToSpeakLatency
object
writeToFile:atomically:
setRequestCreatedTimestamp:
setIsServerTTSRacing:
channelsPerFrame
languages
endChunkDecoding
timeIntervalSinceNow
numberWithLongLong:
setClientBundleIdentifier:
sampleRate
streamId
setIsServerTTS:
firstObject
languageCode
setTextToSynthesize:
encodeChunk:
timeIntervalSinceDate:
numberWithLong:
sharedStream
initWithArray:copyItems:
setTextRange:
setIsServerStreamTTS:
shouldWaitCurrentSpeaking
valueWithBytes:objCType:
addBoundaryTimeObserverForTimes:queue:usingBlock:
captureSnapshot
retryDeviceOnNetworkStall
setCategory:error:
streamBufferDuration
fileURLWithPathComponents:
descriptiveKey
numberWithInteger:
setRate:time:
wordTimingInfos
activeVoiceAssets
valueForKey:
setCancelled:
rate
enableAudioDump
cappedRealTimeFactor
textRange
numberWithInt:
fileURLWithPath:
setRate:
retrieveSessionWithID:
valueForEntitlement:
setIsCacheHitFromMemory:
setTarget:
emitMessage:
derivedIdentifierForComponentName:fromSourceIdentifier:
play
setCanUseServerTTS:
setPromptCount:
fileExistsAtPath:isDirectory:
numberWithDouble:
setIsCacheHitFromDisk:
isWatch
createNewXPCWrapperWithCompletion:
accessoryID
inactiveVoiceAssets
setSynthesizedAudioDurationInSecond:
installedVoiceResources
pitch
createFileAtPath:contents:attributes:
numberWithBool:
setSynthesisSource:
fileExistsAtPath:
sharedManager
isWarmStart
utterance
simulateNetworkStall
installedAssetsForType:voicename:language:gender:footprint:
setPowerProfile:
utf16TimingInfoWithUTF8Range:withText:
setInputTextLength:
whisper
stopRequestingMediaData
audioDuration
numOfPromptsTriggered
isVoiceReadyToUse
UUIDString
setSynthesisRealTimeFactor:
eagerRequestCreatedTimestampDiffs
initializeDeviceAuthenticationSessionWithCompletion:
cancelDownload:completion:
userInfo
resourceVersion
stopAtMarker:
sharedAVSystemController
createDirectoryAtPath:withIntermediateDirectories:attributes:error:
setSynthesisLatencyInSecond:
isSynthesisCached
userDefaultsKnowledgeStore
resourcePath
waitUntilFinished
purgeAsset:
setFootprint:
sha256hex
countByEnumeratingWithState:objects:count:
canUseServerTTS
initWithVoicePath:resourcePath:
requestedVoiceContext
setAudioStartTimestampDiffs:
synthesizeText:loggable:callback:
resourceMimeTypes
setFailed:
waitUntilDate:
identifier
initWithUnsignedInteger:
count
resourceList
setAudioSession:
isSimilarTo:
canLogRequestText
setSynthesisEndTimestamp:
hasPhaticResponses:
defaultVolume
neuralDidFallback
setWithObjects:
setSynthesisEffect:
useSSMLInput
copy
neuralAudioClick
convertLanguageCodeToSchemaLocale:
setPitch:
defaultVoiceNameForGender:
setWithArray:
isServerTimeout
setExperimentIdentifier:
promptCount
neuralAlignmentStall
peakPowerForChannel:
setAudioOutputRoute:
setSynthesisBeginTimestamp:
defaultVoiceGender
hasAudioClick
setVolume:
downloadVoiceResource:options:completion:
isServerTTS
defaultSessionConfiguration
setSubscribedVoices:forClientID:accessoryID:
resetCache
setExists:
setAudioDuration:
hasAlignmentStall
isSeedBuild
fallbackLanguageForLanguage:
contextInfo
audioBuffer
downloadVoiceAsset:options:progressUpdateHandler:
setVoiceVersion:
mutablePCMData
setEventMetadata:
isReadyForMoreMediaData
initWithRequest:
speechRequest:didStartWithMark:forRange:
synthesisRequest:didReceiveTimingInfo:
speechRequest:didReceiveTimingInfo:
speechRequest:didReportInstrumentMetrics:
speechRequestDidStart:
speechRequest:didStopWithSuccess:phonemesSpoken:error:
synthesisRequest:didFinishWithInstrumentMetrics:error:
isEqual:
class
self
performSelector:
performSelector:withObject:
performSelector:withObject:withObject:
isProxy
isKindOfClass:
isMemberOfClass:
conformsToProtocol:
respondsToSelector:
retain
release
autorelease
retainCount
zone
hash
superclass
description
debugDescription
TQ,R
T#,R
T@"NSString",R,C
start
suspend
resume
cancel
delegate
taskHash
instrumentMetrics
reportFinish
isSpeaking
reportInstrumentMetrics
reportSpeechStart
setObserverForWordTimings:
reportTimingInfo
request
voiceKey
setSiriInstrumentation:
audioPowerProvider
readyForEagerTask
setSpeakTask:
main
synthesize
speakTask
setReadyForEagerTask:
.cxx_destruct
_readyForEagerTask
_speakTask
T@"VSSpeechSpeakTask",&,N,V_speakTask
TB,N,V_readyForEagerTask
audioBytesRange
setAudioBytesRange:
packetCount
setPacketCount:
packetDescriptionsRange
setPacketDescriptionsRange:
T{_NSRange=QQ},N
TQ,N
endOfSiriTTSUtterance
setEndOfSiriTTSUtterance:
sampleBuffer
_endOfSiriTTSUtterance
_packetCount
_audioBytesRange
_packetDescriptionsRange
T{_NSRange=QQ},N,V_audioBytesRange
TQ,N,V_packetCount
T{_NSRange=QQ},N,V_packetDescriptionsRange
TB,N,V_endOfSiriTTSUtterance
init
handleMediaServerReset
dealloc
initWithAudioSessionID:asbd:
enqueue:packetCount:packetDescriptions:
flushAndStop
stop
pause
addBoundaryTimeObserverForTimes:usingBlock:
removeTimeObserver:
asbd
sessionID
setSessionID:
discontinuedDuringPlayback
error
setError:
T{AudioStreamBasicDescription=dIIIIIIII},R,N
TI,N
TB,R,N
T@"NSError",&,N
willBeginAccessPower
getAveragePower:andPeakPower:
didEndAccessPower
_play
createSampleBufferIdNeeded:
createSampleBuffer:
duration:
_startProvidingData
createSilenceEndBuffer
addEndOfDataAttachment
provideMoreData
stopWaiting
freeAudioQueue
setAsbd:
audioQueueBufferLock
setAudioQueueBufferLock:
renderer
setRenderer:
synchronizer
setSynchronizer:
dataQueue
setDataQueue:
stateLock
setStateLock:
state
setState:
mappedAudioQueuedTimeStamp
setMappedAudioQueuedTimeStamp:
rendererEnqueuedAudioDuration
setRendererEnqueuedAudioDuration:
outputRoute
setOutputRoute:
mappedData
setMappedData:
enqueuedMappedAudioInfo
setEnqueuedMappedAudioInfo:
startedProvidingData
setStartedProvidingData:
noRemainTasks
setNoRemainTasks:
_discontinuedDuringPlayback
_startedProvidingData
_sessionID
_error
_renderer
_synchronizer
_dataQueue
_state
_rendererEnqueuedAudioDuration
_outputRoute
_mappedData
_enqueuedMappedAudioInfo
_noRemainTasks
_mappedAudioQueuedTimeStamp
_asbd
_audioQueueBufferLock
_stateLock
T{_opaque_pthread_mutex_t=q[56c]},N,V_audioQueueBufferLock
T{AudioStreamBasicDescription=dIIIIIIII},N,V_asbd
T@"AVSampleBufferAudioRenderer",&,N,V_renderer
T@"AVSampleBufferRenderSynchronizer",&,N,V_synchronizer
T@"NSObject<OS_dispatch_queue>",&,N,V_dataQueue
T{_opaque_pthread_mutex_t=q[56c]},N,V_stateLock
Tq,N,V_state
T{?=qiIq},N,V_mappedAudioQueuedTimeStamp
Td,N,V_rendererEnqueuedAudioDuration
T@"NSString",&,N,V_outputRoute
T@"VSMappedData",&,N,V_mappedData
T@"NSMutableArray",&,N,V_enqueuedMappedAudioInfo
TB,N,V_startedProvidingData
T@"NSObject<OS_dispatch_semaphore>",&,N,V_noRemainTasks
TI,N,V_sessionID
TB,R,N,V_discontinuedDuringPlayback
T@"NSError",&,N,V_error
initWithStreamDescription:pcmBufferSize:
initialize
uninitialize
setVoiceBoostGainDecibels:
processData:
voiceBoostGainDecibels
pcmBufferSize
setPcmBufferSize:
floatConverter
setFloatConverter:
integerConverter
setIntegerConverter:
voiceBoostUnit
setVoiceBoostUnit:
audioTimeStamp
setAudioTimeStamp:
_voiceBoostGainDecibels
_pcmBufferSize
_floatConverter
_integerConverter
_voiceBoostUnit
_audioTimeStamp
TQ,N,V_pcmBufferSize
T^{OpaqueAudioConverter=},N,V_floatConverter
T^{OpaqueAudioConverter=},N,V_integerConverter
T^{OpaqueAudioComponentInstance=},N,V_voiceBoostUnit
T{AudioTimeStamp=dQdQ{SMPTETime=ssIIIssss}II},N,V_audioTimeStamp
Tf,N,V_voiceBoostGainDecibels
shortTermCachedEngines
shortTermCachedEngineForVoice:voiceResource:
setRequest:
estimatedDuration
setInstrumentMetrics:
deviceCore
setDeviceCore:
_request
_estimatedDuration
_instrumentMetrics
_deviceCore
T@"VSDeviceTTSCore",&,N,V_deviceCore
T@"VSSpeechRequest",&,N,V_request
Td,R,N,V_estimatedDuration
T@"VSInstrumentMetrics",&,N,V_instrumentMetrics
sharedInstance
transferPreinstallErrorMessagesOfLanguage:voiceName:forAccessoryID:
taskAuxiliaryQueue
synthesizeAndSpeak
timingInfos
setTimingInfos:
setDelegate:
engine
setEngine:
voiceBooster
setVoiceBooster:
playbackService
setPlaybackService:
voiceSelection
setVoiceSelection:
voiceResource
setVoiceResource:
cachingService
setCachingService:
prewarmService
setPrewarmService:
synthesisHasIssue
setSynthesisHasIssue:
siriInstrumentation
speechCache
setSpeechCache:
phonemes
setPhonemes:
compressedAudio
setCompressedAudio:
streamAudio
setStreamAudio:
setTaskAuxiliaryQueue:
neuralPlaybackSemaphore
setNeuralPlaybackSemaphore:
_synthesisHasIssue
_timingInfos
_delegate
_engine
_voiceBooster
_playbackService
_voiceSelection
_voiceResource
_cachingService
_prewarmService
_siriInstrumentation
_speechCache
_phonemes
_compressedAudio
_streamAudio
_taskAuxiliaryQueue
_neuralPlaybackSemaphore
T@"NSObject<OS_dispatch_semaphore>",&,N,V_neuralPlaybackSemaphore
T@"NSArray",&,N,V_timingInfos
T@"<VSSpeechServiceDelegate>",W,N,V_delegate
T@"VSSpeechEngine",&,N,V_engine
T@"VSVoiceBooster",&,N,V_voiceBooster
T@"VSAudioPlaybackService",&,N,V_playbackService
T@"VSVoiceAssetSelection",&,N,V_voiceSelection
T@"VSVoiceResourceAsset",&,N,V_voiceResource
T@"VSCachingService",&,N,V_cachingService
T@"VSPrewarmService",&,N,V_prewarmService
TB,N,V_synthesisHasIssue
T@"VSSiriInstrumentation",&,N,V_siriInstrumentation
T@"<VSSpeechCacheItem>",&,N,V_speechCache
T@"NSArray",&,N,V_phonemes
T@"VSAudioData",&,N,V_compressedAudio
T@"VSStreamAudioData",&,N,V_streamAudio
T@"NSObject<OS_dispatch_queue>",&,N,V_taskAuxiliaryQueue
synthesisCore:didReceiveAudio:
synthesisCore:didReceiveProcessingWordTimingInfo:
synthesisCore:didReceiveWordTimingInfo:
ospreyCore:didReceiveAudio:wordTimingInfo:
ospreyCore:didFinishWithError:
initWithRequest:shouldSpeak:
handleServerResponse:timingInfo:
speakRetryPhrase
fallbackToDeviceSynthesis
deviceTTSWaitTime
shouldRelyOnServerTTS
shouldDeferDeviceTTS
handleDeviceSynthesis:timingInfo:
enqueueAudioData:
eagerTaskHashForRequest:
proceedWithSpeechCache:
broadcastTimeoutCondition
proceedWithServerTTS
writeAudioIfNeeded:
shouldSpeak
setShouldSpeak:
isNeuralFallbackCondition
setIsNeuralFallbackCondition:
wordTimingInfo
setWordTimingInfo:
timeoutCondition
setTimeoutCondition:
setDeviceTTSWaitTime:
synthesisCore
setSynthesisCore:
useServerResponse
setUseServerResponse:
useDeviceSynthesis
setUseDeviceSynthesis:
speechStartReported
setSpeechStartReported:
isEagerCache
setIsEagerCache:
racingMutex
setRacingMutex:
serverAudio
setServerAudio:
deferredTTSTimingInfo
setDeferredTTSTimingInfo:
internalSettings
setInternalSettings:
ospreyCore
setOspreyCore:
serverTTSConfig
setServerTTSConfig:
_shouldSpeak
_isNeuralFallbackCondition
_useServerResponse
_useDeviceSynthesis
_speechStartReported
_isEagerCache
_wordTimingInfo
_deviceTTSWaitTime
_synthesisCore
_serverAudio
_deferredTTSTimingInfo
_internalSettings
_ospreyCore
_serverTTSConfig
_timeoutCondition
_racingMutex
TB,N,V_shouldSpeak
TB,N,V_isNeuralFallbackCondition
T@"NSArray",&,N,V_wordTimingInfo
T{_opaque_pthread_cond_t=q[40c]},N,V_timeoutCondition
Td,N,V_deviceTTSWaitTime
T@"VSSpeechServerTask",&,N,V_speakTask
T@"VSDeviceTTSCore",&,N,V_synthesisCore
TB,N,V_useServerResponse
TB,N,V_useDeviceSynthesis
TB,N,V_speechStartReported
TB,N,V_isEagerCache
T{_opaque_pthread_mutex_t=q[56c]},N,V_racingMutex
T@"VSAudioData",&,N,V_serverAudio
T@"NSArray",&,N,V_deferredTTSTimingInfo
T@"VSSpeechInternalSettings",&,N,V_internalSettings
T@"VSOspreyTTSCore",&,N,V_ospreyCore
T@"VSSiriServerConfiguration",&,N,V_serverTTSConfig
boolValue
isSiriClientBundleIdentifier:
updateWithConnectionIdentifier:keepActive:
prewarmIfNeededWithRequest:reply:
queryPhaticCapabilityWithRequest:reply:
estimateDurationWithRequest:reply:
startSpeechRequest:reply:
startSynthesisRequest:
pauseSpeechRequest:atMark:
continueSpeechRequest:
stopSpeechRequest:atMark:
startPresynthesizedAudioRequest:
cachePresynthesizedAudioRequest:
stopPresynthesizedAudioRequest:
getVoiceNamesForLanguage:reply:
getFootprintsForVoiceName:languageCode:reply:
getSpeechIsActiveReply:
getSpeechIsActiveForConnectionReply:
startVoicePreviewRequest:reply:
stopVoicePreview
startPhonemesRequest:phonemeSystem:reply:
beginAudioPowerUpdateWithReply:
endAudioPowerUpdate
cleanUnusedAssets:
getLocalVoicesForLanguage:reply:
getLocalVoiceResourcesReply:
setSubscribedVoiceAssets:withClientID:forAccessoryID:
getSubscribedVoiceAssetsWithClientID:forAccessoryID:reply:
getAllVoiceSubscriptionsWithReply:
triggerCellularDownloadedVoiceAssets:withClientID:
getVoiceResourceForLanguage:reply:
getVoiceInfoForLanguageCode:name:footprint:gender:type:reply:
forwardStreamObject:
invokeDaemon:
killDaemon
speechRequestDidPause:
speechRequestDidContinue:
synthesisRequest:didGenerateAudioChunk:
audioRequestDidStart:
audioRequest:didStopAtEnd:error:
audioRequest:didReportInstrumentMetrics:error:
previewRequestDidStartPlaying:
initWithConnection:
invalidate
performLanguageFallBackIfNeededWithRequest:
preprocessRequestBeforeSynthesis:
audioPowerUpdateQueue
audioPowerUpdater
connectionIdentifier
setConnectionIdentifier:
hubManagementClient
setHubManagementClient:
homeManager
setHomeManager:
setAudioPowerUpdateQueue:
setAudioPowerUpdater:
synthesizerTransaction
setSynthesizerTransaction:
_connection
_connectionIdentifier
_hubManagementClient
_homeManager
_audioPowerUpdateQueue
_audioPowerUpdater
_synthesizerTransaction
T@"NSString",&,N,V_connectionIdentifier
T@"VSHHManagementClient",&,N,V_hubManagementClient
T@"VSHMHomeManager",&,N,V_homeManager
T@"NSObject<OS_dispatch_queue>",&,N,V_audioPowerUpdateQueue
T@"AFAudioPowerUpdater",&,N,V_audioPowerUpdater
T@"NSObject<OS_os_transaction>",&,N,V_synthesizerTransaction
timeout
setDidReceiveAudio:
performRoundTripOspreyTTS
performStreamingOspreyTTS
waitUntilFinishedIfAudioReceivedWithin:
voice
setVoice:
serverTTSClient
setServerTTSClient:
serverConfig
setServerConfig:
bufferDurationLimit
setBufferDurationLimit:
didReceiveAudio
didReceiveAudioCondition
setDidReceiveAudioCondition:
delegateCallbackQueue
setDelegateCallbackQueue:
_didReceiveAudio
_voice
_serverTTSClient
_serverConfig
_bufferDurationLimit
_didReceiveAudioCondition
_delegateCallbackQueue
T@"VSServerTTSClient",&,N,V_serverTTSClient
T@"VSSiriServerConfiguration",&,N,V_serverConfig
Td,N,V_bufferDurationLimit
T@"VSTimeoutCondition",&,N,V_timeoutCondition
TB,N,V_didReceiveAudio
T@"NSCondition",&,N,V_didReceiveAudioCondition
T@"NSObject<OS_dispatch_queue>",&,N,V_delegateCallbackQueue
T@"VSSpeechRequest",R,N,V_request
T@"<VSOspreyTTSCoreDelegate>",W,N,V_delegate
T@"VSInstrumentMetrics",W,N,V_instrumentMetrics
T@"VSVoiceAsset",&,N,V_voice
registerPostInstallActivity
resetMobileAssetDefaults
clearSynthesisCache
previewAudioURLForLanguage:voiceName:previewType:
startVoicePreviewForLanguageCode:voiceName:previewType:startedPlaying:completion:
audioPlayerDidFinishPlaying:successfully:
audioPlayerDecodeErrorDidOccur:error:
audioPlayerBeginInterruption:
audioPlayerEndInterruption:withOptions:
audioPlayerEndInterruption:withFlags:
audioPlayerEndInterruption:
startVoicePreviewWithURL:startedPlaying:completion:
previewPlayer
setPreviewPlayer:
completion
setCompletion:
currentPreviewURL
setCurrentPreviewURL:
speakingQueue
setSpeakingQueue:
_previewPlayer
_completion
_currentPreviewURL
_speakingQueue
T@"AVAudioPlayer",&,N,V_previewPlayer
T@?,C,N,V_completion
T@"NSURL",C,N,V_currentPreviewURL
T@"NSObject<OS_dispatch_queue>",&,N,V_speakingQueue
serializedData
initWithKey:data:
initWithKey:audio:wordTimingInfo:voiceKey:voiceResourceKey:
magicVersion
voiceResourceKey
audio
setKey:
audioData
setAudioData:
packetDescriptions
setPacketDescriptions:
_magicVersion
_voiceKey
_voiceResourceKey
_audio
_key
_audioData
_packetDescriptions
T@"NSString",&,N,V_key
T@"NSData",&,N,V_audioData
Tq,N,V_packetCount
T@"NSData",&,N,V_packetDescriptions
Tq,R,N,V_magicVersion
T@"NSArray",R,N,V_timingInfos
T@"NSString",R,N,V_voiceKey
T@"NSString",R,N,V_voiceResourceKey
T@"VSAudioData",R,N,V_audio
defaultCacheStore
T@"VSSpeechCache",R
initWithStorePath:
addCache:
preinstalledCacheForText:language:name:
cacheDataForKey:
isPreinstalledCacheAvailableForRequest:
cleanCache
totalCacheSize
deleteCache
dirPath
setDirPath:
preinstalledCacheDir
setPreinstalledCacheDir:
_dirPath
_preinstalledCacheDir
T@"NSString",&,N,V_dirPath
T@"NSString",&,N,V_preinstalledCacheDir
flatbuffData
copyWithZone:
initWithFlatbuffData:
initAndVerifyWithFlatbuffData:
initWithFlatbuffData:root:
initWithFlatbuffData:root:verify:
fe_feature
fe_feature_only
addObjectToBuffer:
_storage
_data
_root
language
gender
name
version
quality
type
T@"NSString",R,N
resource
T@"OPTTSTextToSpeechVoice",R,N
T@"OPTTSTextToSpeechResource",R,N
channel_type
app_id
Tq,R,N
context_info
dialog_identifier
T@"NSArray",R,N
experiment_identifier
speech_id
session_id
text
audio_type
enable_word_timing_info
voice_name
preferred_voice_type
meta_info
context
experiment
feature_flags
T@"OPTTSTextToSpeechRequestMeta",R,N
T@"OPTTSTextToSpeechRequestContext",R,N
T@"OPTTSTextToSpeechRequestExperiment",R,N
T@"OPTTSTTSRequestFeatureFlags",R,N
value
sample_rate
format_id
format_flags
bytes_per_packet
frames_per_packet
bytes_per_frame
channels_per_frame
bits_per_channel
reserved
Td,R,N
TI,R,N
word
sample_idx
offset
length
timestamp
Tf,R,N
error_code
error_str
audio:
decoder_description
playback_description
word_timing_info
Ti,R,N
T@"NSData",R,N
T@"OPTTSAudioDescription",R,N
T@"OPTTSTextToSpeechMeta",R,N
stream_id
streaming_playback_buffer_size_in_seconds
current_pkt_number
total_pkt_number
content_type
contentAsOPTTSStartTextToSpeechStreamingRequest
T@"OPTTSStartTextToSpeechStreamingRequest",R,N
contentAsOPTTSBeginTextToSpeechStreamingResponse
contentAsOPTTSPartialTextToSpeechStreamingResponse
contentAsOPTTSFinalTextToSpeechStreamingResponse
T@"OPTTSBeginTextToSpeechStreamingResponse",R,N
T@"OPTTSPartialTextToSpeechStreamingResponse",R,N
T@"OPTTSFinalTextToSpeechStreamingResponse",R,N
vsDescription
cleanUnknownAccessoriesPreferences
isExistingAccessoryId:
initWithTimebase:times:queue:block:
_reallyInvalidate
_resetNextFireTime
timebase
_timerQueue
_timerSource
_invalid
_times
_nextFireTime
_block
_timebase
T^{OpaqueCMTimebase=},R,V_timebase
signalQueueRunningStateChange
dequeueAvailableMappedAudio
_enqueueAudioBytesLength:audioBytes:packetCount:packetDescriptions:
waitForAudioQueueStop
isAudioQueueStalled
isAudioQueueRunning
audioQueue
setAudioQueue:
setTimebase:
waitForStateChangeMutex
setWaitForStateChangeMutex:
stateChangeCondition
setStateChangeCondition:
enqueuedSampleCount
setEnqueuedSampleCount:
audioStartTimeStamp
setAudioStartTimeStamp:
audioQueueStartDate
setAudioQueueStartDate:
audioQueueFutureEndDate
setAudioQueueFutureEndDate:
playingBufferCount
setPlayingBufferCount:
dequeueCondition
setDequeueCondition:
_audioQueue
_enqueuedSampleCount
_audioQueueStartDate
_audioQueueFutureEndDate
_playingBufferCount
_dequeueCondition
_stateChangeCondition
_waitForStateChangeMutex
_audioStartTimeStamp
T^{OpaqueAudioQueue=},N,V_audioQueue
T^{OpaqueCMTimebase=},N,V_timebase
T{_opaque_pthread_mutex_t=q[56c]},N,V_waitForStateChangeMutex
T{_opaque_pthread_cond_t=q[40c]},N,V_stateChangeCondition
Td,N,V_enqueuedSampleCount
T{AudioTimeStamp=dQdQ{SMPTETime=ssIIIssss}II},N,V_audioStartTimeStamp
T@"NSDate",&,N,V_audioQueueStartDate
T@"NSDate",&,N,V_audioQueueFutureEndDate
TQ,N,V_playingBufferCount
T@"NSCondition",&,N,V_dequeueCondition
initWithRouteAttributes:
audioRouteName
isBluetoothRoute
isAppleProduct
routeInfo
_routeInfo
T@"NSDictionary",R,N,V_routeInfo
durationOfAudioDataLength:withAudioDescription:
bytesOfDuration:withAudioDescription:
initWithAudioSessionID:asbd:useAVSBAR:
setTimingObserver:
setBoundaryTimeObserverForTimingInfos:usingBlock:
outputRouteInfo
implementation
setImplementation:
playbackIntervalId
setPlaybackIntervalId:
timingObserver
_outputRouteInfo
_implementation
_playbackIntervalId
_timingObserver
T@"<VSAudioPlaybackServiceProtocol><AFAudioPowerProviding>",&,N,V_implementation
TQ,N,V_playbackIntervalId
T@,&,N,V_timingObserver
T@"VSAudioRouteInfo",R,N,V_outputRouteInfo
T@"NSError",R,N
selectedVoice
selectedVoiceResource
getCacheForHash:
voiceSelectionWithRequest:error:
voiceSelection_noRetry_WithRequest:error:
prepareForSynthesis
reportProcessingWordTimingInfo:
reportWordTimingInfo:
reportAudio:
setSelectedVoice:
setSelectedVoiceResource:
_selectedVoice
_selectedVoiceResource
T@"VSVoiceAssetSelection",&,N,V_selectedVoice
T@"VSVoiceResourceAsset",&,N,V_selectedVoiceResource
T@"<VSDeviceTTSCoreDelegate>",W,N,V_delegate
T@"VSAudioData",R,N,V_compressedAudio
T@"VSStreamAudioData",R,N,V_streamAudio
sharedService
purgeableAssetsWithInfo:urgency:
totalSizeOfAssets:
purgeable:urgency:
purge:urgency:
periodic:urgency:
purgeImpl:urgency:
registerCacheDelete
initWithTimeoutValue:
_waitForTimeInterval:
refresh
wait
timeoutValue
setTimeoutValue:
refreshTimeoutCondition
setRefreshTimeoutCondition:
shouldStop
setShouldStop:
_shouldStop
_timeoutValue
_refreshTimeoutCondition
T@"NSCondition",&,N,V_refreshTimeoutCondition
TB,N,V_shouldStop
Td,N,V_timeoutValue
downloadQueue
inProgressDownloadVoiceKeys
addInProgressDownloadVoiceKey:
removeInProgressDownloadVoiceKey:
initWithType:
initWithType:assetsManager:
updateVoicesAndVoiceResources
updateVoiceIfNeeded:
updateTrialVoiceResourceWithLanguage:
cancelDownloadForAssets:
assetsManager
setAssetsManager:
preferenceInterface
setPreferenceInterface:
updateLock
setUpdateLock:
_type
_assetsManager
_preferenceInterface
_updateLock
T@"VSMobileAssetsManager",&,N,V_assetsManager
T@"VSPreferencesInterface",&,N,V_preferenceInterface
T@"NSLock",&,N,V_updateLock
TQ,R,N,V_type
initWithURL:configuration:
ospreyServiceEndpointURL
roundTripTTS:responseHandler:
streamTTS:beginHandler:chunkHandler:endHandler:completion:
deviceID
setDeviceID:
_deviceID
T@"NSString",&,N,V_deviceID
gainDecibelWithVolume:
defaultService
initWithDirectory:
removeOldFiles
totalDiagnosticFileSize
removeDirectory
createDirectoryIfNeeded
dumpCompressedAudio:forRequest:
dumpStreamAudio:forRequest:
dumpInstrumentMetrics:withTimestamp:
collectTailspin:
audioDumpPath
setAudioDumpPath:
audioDumpFileAttributes
setAudioDumpFileAttributes:
_audioDumpPath
_audioDumpFileAttributes
T@"NSString",&,N,V_audioDumpPath
T@"NSDictionary",&,N,V_audioDumpFileAttributes
defaultConfig
dictForKey:
configForAppId:key:
timeoutForAppId:
deviceWaitTimeForAppId:
shouldDelayVoiceUpdate
experimentIdentifier
allowedAppID
knowledgeStore
setKnowledgeStore:
_knowledgeStore
T@"CKKnowledgeStore",&,N,V_knowledgeStore
timeToLiveTimerFired:
setObject:forKey:timeToLive:
removeObjectForKey:
objectForKey:
removeAllObjects
cache
setCache:
cacheTimer
setCacheTimer:
_cache
_cacheTimer
T@"NSCache",&,N,V_cache
T@"NSMutableDictionary",&,N,V_cacheTimer
handleStreamNotification:
initWithRequest:withStreamID:
handleBegin:
handleChunk:
handleEnd:
startPlayback
waitForNewData:
signalNewDataWithError:
streamID
setStreamID:
streamingResource
setStreamingResource:
streamingVoice
setStreamingVoice:
deviceTTSCore
setDeviceTTSCore:
playbackServices
setPlaybackServices:
finalTimingInfo
setFinalTimingInfo:
playbackBeginDate
setPlaybackBeginDate:
_streamID
_streamingResource
_streamingVoice
_deviceTTSCore
_playbackServices
_finalTimingInfo
_playbackBeginDate
T@"NSString",&,N,V_streamID
T@"SATTSSpeechSynthesisResource",&,N,V_streamingResource
T@"SATTSSpeechSynthesisVoice",&,N,V_streamingVoice
T@"VSDeviceTTSCore",&,N,V_deviceTTSCore
T@"VSAudioPlaybackService",&,N,V_playbackServices
T@"NSMutableArray",&,N,V_finalTimingInfo
T@"NSDate",&,N,V_playbackBeginDate
T@"VSPresynthesizedAudioRequest",R,N,V_request
T@"NSMutableData",&,N,V_audioData
sharedServices
getCurrentAudioPowerProvider
previousProvider
setPreviousProvider:
_previousProvider
T@"<AFAudioPowerProviding>",W,N,V_previousProvider
addInlineStreamRequest:
popInlineStreamRequestForSpeakRequest:
hasInlineStreamRequestForSpeakRequest:
enqueueStreamId:withObject:
startStreamingWithId:
removeStreamId:
queuedNotification
setQueuedNotification:
ongoingNotifications
setOngoingNotifications:
streamRequestQueue
setStreamRequestQueue:
lock
setLock:
recursiveLockAttr
setRecursiveLockAttr:
notifyQueue
setNotifyQueue:
_queuedNotification
_ongoingNotifications
_streamRequestQueue
_notifyQueue
_recursiveLockAttr
_lock
T@"NSMutableDictionary",&,N,V_queuedNotification
T@"NSMutableSet",&,N,V_ongoingNotifications
T@"NSMutableArray",&,N,V_streamRequestQueue
T{_opaque_pthread_mutex_t=q[56c]},N,V_lock
T{_opaque_pthread_mutexattr_t=q[8c]},N,V_recursiveLockAttr
T@"NSObject<OS_dispatch_queue>",&,N,V_notifyQueue
setFe_feature:
setFe_feature_only:
TB,N
setLanguage:
setGender:
setName:
setVersion:
setQuality:
setType:
T@"NSString",C,N
setResource:
T@"OPTTSTextToSpeechVoice",C,N
T@"OPTTSTextToSpeechResource",C,N
setChannel_type:
setApp_id:
Tq,N
setContext_info:
setDialog_identifier:
T@"NSArray",C,N
setExperiment_identifier:
setSpeech_id:
setSession_id:
setText:
setAudio_type:
setEnable_word_timing_info:
setVoice_name:
setPreferred_voice_type:
setMeta_info:
setContext:
setExperiment:
setFeature_flags:
T@"OPTTSTextToSpeechRequestMeta",C,N
T@"OPTTSTextToSpeechRequestContext",C,N
T@"OPTTSTextToSpeechRequestExperiment",C,N
T@"OPTTSTTSRequestFeatureFlags",C,N
setValue:
setSample_rate:
setFormat_id:
setFormat_flags:
setBytes_per_packet:
setFrames_per_packet:
setBytes_per_frame:
setChannels_per_frame:
setBits_per_channel:
setReserved:
Td,N
setWord:
setSample_idx:
setOffset:
setLength:
setTimestamp:
Tf,N
setError_code:
setError_str:
setAudio:
setDecoder_description:
setPlayback_description:
setWord_timing_info:
Ti,N
T@"NSData",C,N
T@"OPTTSAudioDescription",C,N
T@"OPTTSTextToSpeechMeta",C,N
setStream_id:
setStreaming_playback_buffer_size_in_seconds:
setCurrent_pkt_number:
setTotal_pkt_number:
setContent_type:
setContentAsOPTTSStartTextToSpeechStreamingRequest:
T@"OPTTSStartTextToSpeechStreamingRequest",C,N
setContentAsOPTTSBeginTextToSpeechStreamingResponse:
setContentAsOPTTSPartialTextToSpeechStreamingResponse:
setContentAsOPTTSFinalTextToSpeechStreamingResponse:
T@"OPTTSBeginTextToSpeechStreamingResponse",C,N
T@"OPTTSPartialTextToSpeechStreamingResponse",C,N
T@"OPTTSFinalTextToSpeechStreamingResponse",C,N
handleVoiceSelectionPurge:
setActiveSessionCount:
prewarmWithRequest:
_cachedEngineForVoice:resources:
cachedEngineForVoice:resources:
_engineForVoice:resources:
loadEngineForVoice:resources:
unloadCachedEngineWithVoice:
_loadVoiceResources:forEngine:
unloadEngine
waitUntilPrewarmFinish
activeSessionCount
cachedEngine
setCachedEngine:
loadedResources
setLoadedResources:
prewarmQueue
setPrewarmQueue:
_activeSessionCount
_cachedEngine
_loadedResources
_prewarmQueue
T@"VSSpeechEngine",&,N,V_cachedEngine
T@"VSVoiceResourceAsset",&,N,V_loadedResources
T@"NSObject<OS_dispatch_queue>",&,N,V_prewarmQueue
Tq,N,V_activeSessionCount
shouldUseServerTTSForRequest:
ospreyStartSynthesisRequest:responseHandler:completion:
ospreyStartStreamingRequest:dataHandler:metaInfoHandler:completion:
speakCachedAudio
startPlaybackServiceWithAudioSessionID:
waitUntilAudioFinished
pausePlayback
resumePlayback
fetchVoiceResource
fetchVoiceAsset
_fetchVoiceAsset_NoRetry
logFinish
enqueueCache
standardService
initWithCache:shortTermMemory:
compressAudio:
compressStreamAudio:
enqueueCacheWithHash:audio:timingInfo:voiceKey:voiceResourceKey:completion:
enqueueCacheWithHash:streamAudio:timingInfo:voiceKey:voiceResourceKey:completion:
_enqueueCacheWithHash:audioObject:timingInfo:voiceKey:voiceResourceKey:completion:
inMemoryCacheForHash:
_inMemoryCacheForHash:
onDiskCacheForHash:
_onDiskCacheForHash:
fetchCacheForTask:
enqueueShortTermCacheWithHash:audio:timingInfo:voiceKey:voiceResourceKey:completion:
shortTermCacheForHash:
popShortTermCacheForHash:
threadLock
setThreadLock:
inMemoryCaches
setInMemoryCaches:
cacheStore
setCacheStore:
shortTermCache
setShortTermCache:
cachingQueue
setCachingQueue:
_threadLock
_inMemoryCaches
_cacheStore
_shortTermCache
_cachingQueue
T@"NSLock",&,N,V_threadLock
T@"NSMutableArray",&,N,V_inMemoryCaches
T@"VSSpeechCache",&,N,V_cacheStore
T@"VSShortTermCache",&,N,V_shortTermCache
T@"NSObject<OS_dispatch_queue>",&,N,V_cachingQueue
schemaVoiceGenderFromGender:
schemaVoiceTypeFromType:
schemaFootprintFromFootprint:
outputRouteFromRouteInfo:
synthesisSourceFromSource:
sharedPowerLogger
instrumentVoicedProcessStartedPowerEvent
instrumentPowerEvent:ttsId:
initWithSiriRequestId:
makeRequestLinkEvent
instrumentRequestReceivedWithText:requestedVoiceType:requestedVoiceFootprint:isPrivate:
instrumentSpeechStartedWithSource:customerPerceivedLatency:audioOutputRoute:voiceType:voiceFootprint:voiceVersion:resourceVersion:isWhisper:
instrumentSpeechEndedWithAudioDuration:synthesisLatency:realTimeFactor:promptCount:errorCode:
instrumentSpeechFailedWithErrorCodes:
instrumentSpeechCancelled
instrumentVoiceFallbackOccurredWithVoice:resource:
siriRequestId
setSiriRequestId:
ttsId
setTtsId:
contextId
setContextId:
_siriRequestId
_ttsId
_contextId
T@"NSUUID",&,N,V_siriRequestId
T@"NSUUID",&,N,V_ttsId
T@"NSUUID",&,N,V_contextId
initWithASBD:
appendAudioData:packetCount:packetDescriptions:
enumerateAudioWithBlock:
duration
writeWaveToFilePath:
mappedAudioInfo
setMappedAudioInfo:
_mappedAudioInfo
T@"NSMutableArray",&,N,V_mappedAudioInfo
T{AudioStreamBasicDescription=dIIIIIIII},R,N,V_asbd
mainDeviceQueue
parallelQueueWithIdentifier:
cancelTasksWithDelegate:
addTask:
spinNextTask
currentTask
createdTimestampWithTask:
taskWithCreatedTimeStamp:
tasksWithDelegate:
cancelTask:
suspendCurrentTask
resumeCurrentTask
eagerTasks
setEagerTasks:
speakTasks
setSpeakTasks:
setCurrentTask:
threadMutex
setThreadMutex:
threadMutexAttr
setThreadMutexAttr:
lastSynthesisRequest
setLastSynthesisRequest:
lastSynthesisRequestCreatedTimeStamp
setLastSynthesisRequestCreatedTimeStamp:
_eagerTasks
_speakTasks
_currentTask
_lastSynthesisRequest
_lastSynthesisRequestCreatedTimeStamp
_threadMutexAttr
_threadMutex
T@"NSMutableArray",&,N,V_eagerTasks
T@"NSMutableArray",&,N,V_speakTasks
T@"NSOperation<VSSpeechTaskProtocol>",&,N,V_currentTask
T{_opaque_pthread_mutex_t=q[56c]},N,V_threadMutex
T{_opaque_pthread_mutexattr_t=q[8c]},N,V_threadMutexAttr
T@"VSSpeechRequest",&,N,V_lastSynthesisRequest
TQ,N,V_lastSynthesisRequestCreatedTimeStamp
phonemeSystem
setPhonemeSystem:
reply
setReply:
_phonemeSystem
_reply
Tq,N,V_phonemeSystem
T@?,C,N,V_reply
populateWithPCMData:
populateWithOpusData:
populatePCMDataWithSiriOpusSData:withOpusASBD:
writeToFilePath:
audioDataFromFile:error:
audioDataFromSAUIAudioData:
audioDataFromPresynthesisRequest:
pcmAudioDataFromOpusAudio:
audioDataWithASBD:rawData:
asbdFromDescription:
audioStreamBasicDescription
genderStringFromGender:
requestFromVSRequest:
vs_wordTimingInfos:withText:
vs_voice
vs_voiceResource
initWithFilePath:
mmap
madvise
filePath
fileSize
_filePath
_fileSize
T@"NSString",R,N,V_filePath
TQ,R,N,V_fileSize
T^v,R,N,V_mappedData
Ti,R,N,V_fd
B24@0:8@16
#16@0:8
@16@0:8
@24@0:8:16
@32@0:8:16@24
@40@0:8:16@24@32
B16@0:8
B24@0:8#16
B24@0:8:16
Vv16@0:8
Q16@0:8
^{_NSZone=}16@0:8
B24@0:8@"Protocol"16
@"NSString"16@0:8
v16@0:8
@"VSInstrumentMetrics"16@0:8
v24@0:8@16
v24@0:8@"NSArray"16
@"VSSpeechRequest"16@0:8
v24@0:8@"VSSiriInstrumentation"16
@"<AFAudioPowerProviding>"16@0:8
v24@0:8@"<VSSpeechSpeakableProtocol>"16
@24@0:8@16
v20@0:8B16
@"VSSpeechSpeakTask"
{_NSRange=QQ}16@0:8
v32@0:8{_NSRange=QQ}16
v24@0:8Q16
^{opaqueCMSampleBuffer=}
{_NSRange="location"Q"length"Q}
@60@0:8I16{AudioStreamBasicDescription=dIIIIIIII}20
v40@0:8@16q24@32
@32@0:8@16@?24
{AudioStreamBasicDescription=dIIIIIIII}16@0:8
I16@0:8
v20@0:8I16
v40@0:8@"NSData"16q24@"NSData"32
@"NSError"16@0:8
@32@0:8@"NSArray"16@?<v@?{?=qiIq}>24
v24@0:8@"NSError"16
B32@0:8^f16^f24
^{opaqueCMSampleBuffer=}24@0:8@16
d24@0:8@16
^{opaqueCMSampleBuffer=}16@0:8
v56@0:8{AudioStreamBasicDescription=dIIIIIIII}16
{_opaque_pthread_mutex_t=q[56c]}16@0:8
v80@0:8{_opaque_pthread_mutex_t=q[56c]}16
q16@0:8
v24@0:8q16
{?=qiIq}16@0:8
v40@0:8{?=qiIq}16
d16@0:8
v24@0:8d16
@"NSError"
@"AVSampleBufferAudioRenderer"
@"AVSampleBufferRenderSynchronizer"
@"NSObject<OS_dispatch_queue>"
@"NSString"
@"VSMappedData"
@"NSMutableArray"
@"NSObject<OS_dispatch_semaphore>"
{?="value"q"timescale"i"flags"I"epoch"q}
{AudioStreamBasicDescription="mSampleRate"d"mFormatID"I"mFormatFlags"I"mBytesPerPacket"I"mFramesPerPacket"I"mBytesPerFrame"I"mChannelsPerFrame"I"mBitsPerChannel"I"mReserved"I}
{_opaque_pthread_mutex_t="__sig"q"__opaque"[56c]}
@64@0:8{AudioStreamBasicDescription=dIIIIIIII}16Q56
v20@0:8f16
f16@0:8
^{OpaqueAudioConverter=}16@0:8
v24@0:8^{OpaqueAudioConverter=}16
^{OpaqueAudioComponentInstance=}16@0:8
v24@0:8^{OpaqueAudioComponentInstance=}16
{AudioTimeStamp=dQdQ{SMPTETime=ssIIIssss}II}16@0:8
v80@0:8{AudioTimeStamp=dQdQ{SMPTETime=ssIIIssss}II}16
^{OpaqueAudioConverter=}
^{OpaqueAudioComponentInstance=}
{AudioTimeStamp="mSampleTime"d"mHostTime"Q"mRateScalar"d"mWordClockTime"Q"mSMPTETime"{SMPTETime="mSubframes"s"mSubframeDivisor"s"mCounter"I"mType"I"mFlags"I"mHours"s"mMinutes"s"mSeconds"s"mFrames"s}"mFlags"I"mReserved"I}
@32@0:8@16@24
@"VSSpeechRequest"
@"VSInstrumentMetrics"
@"VSDeviceTTSCore"
v40@0:8@16@24@32
@"NSArray"
@"<VSSpeechServiceDelegate>"
@"VSSpeechEngine"
@"VSVoiceBooster"
@"VSAudioPlaybackService"
@"VSVoiceAssetSelection"
@"VSVoiceResourceAsset"
@"VSCachingService"
@"VSPrewarmService"
@"VSSiriInstrumentation"
@"<VSSpeechCacheItem>"
@"VSAudioData"
@"VSStreamAudioData"
v32@0:8@16@24
v32@0:8@"VSDeviceTTSCore"16@"VSAudioData"24
v32@0:8@"VSDeviceTTSCore"16@"NSArray"24
v40@0:8@"VSOspreyTTSCore"16@"VSAudioData"24@"NSArray"32
v32@0:8@"VSOspreyTTSCore"16@"NSError"24
@28@0:8@16B24
{_opaque_pthread_cond_t=q[40c]}16@0:8
v64@0:8{_opaque_pthread_cond_t=q[40c]}16
@"VSSpeechServerTask"
@"VSSpeechInternalSettings"
@"VSOspreyTTSCore"
@"VSSiriServerConfiguration"
{_opaque_pthread_cond_t="__sig"q"__opaque"[40c]}
Vv28@0:8@16B24
Vv32@0:8@16@?24
Vv24@0:8@16
Vv32@0:8@16q24
Vv40@0:8@16@24@?32
Vv24@0:8@?16
Vv40@0:8@16q24@?32
Vv40@0:8@16@24@32
Vv32@0:8@16@24
Vv64@0:8@16@24q32q40q48@?56
Vv28@0:8@"NSString"16B24
Vv32@0:8@"VSSpeechRequest"16@?<v@?@"NSError">24
Vv32@0:8@"VSSpeechRequest"16@?<v@?B>24
Vv32@0:8@"VSSpeechRequest"16@?<v@?d@"NSError">24
Vv32@0:8@"VSSpeechRequest"16@?<v@?>24
Vv24@0:8@"VSSpeechRequest"16
Vv32@0:8@"VSSpeechRequest"16q24
Vv24@0:8@"VSPresynthesizedAudioRequest"16
Vv32@0:8@"NSString"16@?<v@?@"NSArray">24
Vv40@0:8@"NSString"16@"NSString"24@?<v@?@"NSArray">32
Vv24@0:8@?<v@?B>16
Vv32@0:8@"VSPreviewRequest"16@?<v@?dB>24
Vv40@0:8@"VSSpeechRequest"16q24@?<v@?@"NSString"@"NSError">32
Vv24@0:8@?<v@?@"AFXPCWrapper">16
Vv24@0:8@?<v@?@"NSError">16
Vv32@0:8@"NSString"16@?<v@?@"NSArray"@"NSError">24
Vv24@0:8@?<v@?@"NSArray"@"NSError">16
Vv40@0:8@"NSArray"16@"NSString"24@"NSUUID"32
Vv40@0:8@"NSString"16@"NSUUID"24@?<v@?@"NSArray">32
Vv24@0:8@?<v@?@"NSArray">16
Vv32@0:8@"NSArray"16@"NSString"24
Vv32@0:8@"NSString"16@?<v@?@"VSVoiceResourceAsset">24
Vv64@0:8@"NSString"16@"NSString"24q32q40q48@?<v@?@"VSVoiceAsset"@"NSError">56
Vv24@0:8@"SATTSSpeechSynthesisStreaming"16
Vv24@0:8@?<v@?i>16
Vv48@0:8@16q24{_NSRange=QQ}32
Vv44@0:8@16B24@28@36
Vv36@0:8@16B24@28
Vv48@0:8@"VSSpeechRequest"16q24{_NSRange=QQ}32
Vv44@0:8@"VSSpeechRequest"16B24@"NSString"28@"NSError"36
Vv32@0:8@"VSSpeechRequest"16@"VSInstrumentMetrics"24
Vv32@0:8@"VSSpeechRequest"16@"NSArray"24
Vv32@0:8@"VSSpeechRequest"16@"VSAudioData"24
Vv40@0:8@"VSSpeechRequest"16@"VSInstrumentMetrics"24@"NSError"32
Vv36@0:8@"VSPresynthesizedAudioRequest"16B24@"NSError"28
Vv40@0:8@"VSPresynthesizedAudioRequest"16@"VSInstrumentMetrics"24@"NSError"32
Vv24@0:8@"VSPreviewRequest"16
@"NSXPCConnection"
@"VSHHManagementClient"
@"VSHMHomeManager"
@"AFAudioPowerUpdater"
@"NSObject<OS_os_transaction>"
@"<VSOspreyTTSCoreDelegate>"
@"VSVoiceAsset"
@"VSServerTTSClient"
@"VSTimeoutCondition"
@"NSCondition"
@40@0:8@16@24q32
v56@0:8@16@24q32@?40@?48
v28@0:8@16B24
v32@0:8@16Q24
v28@0:8@"AVAudioPlayer"16B24
v32@0:8@"AVAudioPlayer"16@"NSError"24
v24@0:8@"AVAudioPlayer"16
v32@0:8@"AVAudioPlayer"16Q24
v40@0:8@16@?24@?32
@?16@0:8
v24@0:8@?16
@"AVAudioPlayer"
@"NSURL"
@"NSData"16@0:8
@32@0:8@"NSString"16@"NSData"24
@56@0:8@16@24@32@40@48
@"NSData"
@40@0:8@16@24@32
@24@0:8^{_NSZone=}16
@32@0:8@16r^{TTSRequestFeatureFlags=[1C]}24
@36@0:8@16r^{TTSRequestFeatureFlags=[1C]}24B32
{Offset<siri::speech::schema_fb::TTSRequestFeatureFlags>=I}24@0:8^v16
@"NSMutableDictionary"
r^{TTSRequestFeatureFlags=[1C]}
@32@0:8@16r^{TextToSpeechVoice=[1C]}24
@36@0:8@16r^{TextToSpeechVoice=[1C]}24B32
{Offset<siri::speech::schema_fb::TextToSpeechVoice>=I}24@0:8^v16
r^{TextToSpeechVoice=[1C]}
@32@0:8@16r^{TextToSpeechResource=[1C]}24
@36@0:8@16r^{TextToSpeechResource=[1C]}24B32
{Offset<siri::speech::schema_fb::TextToSpeechResource>=I}24@0:8^v16
r^{TextToSpeechResource=[1C]}
@32@0:8@16r^{TextToSpeechMeta=[1C]}24
@36@0:8@16r^{TextToSpeechMeta=[1C]}24B32
{Offset<siri::speech::schema_fb::TextToSpeechMeta>=I}24@0:8^v16
r^{TextToSpeechMeta=[1C]}
@32@0:8@16r^{TextToSpeechRequestMeta=[1C]}24
@36@0:8@16r^{TextToSpeechRequestMeta=[1C]}24B32
{Offset<siri::speech::schema_fb::TextToSpeechRequestMeta>=I}24@0:8^v16
r^{TextToSpeechRequestMeta=[1C]}
@32@0:8@16r^{TextToSpeechRequestContext=[1C]}24
@36@0:8@16r^{TextToSpeechRequestContext=[1C]}24B32
{Offset<siri::speech::schema_fb::TextToSpeechRequestContext>=I}24@0:8^v16
r^{TextToSpeechRequestContext=[1C]}
@32@0:8@16r^{TextToSpeechRequestExperiment=[1C]}24
@36@0:8@16r^{TextToSpeechRequestExperiment=[1C]}24B32
{Offset<siri::speech::schema_fb::TextToSpeechRequestExperiment>=I}24@0:8^v16
r^{TextToSpeechRequestExperiment=[1C]}
@32@0:8@16r^{TextToSpeechRequest=[1C]}24
@36@0:8@16r^{TextToSpeechRequest=[1C]}24B32
{Offset<siri::speech::schema_fb::TextToSpeechRequest>=I}24@0:8^v16
r^{TextToSpeechRequest=[1C]}
@32@0:8@16r^{ContextInfoEntry=[1C]}24
@36@0:8@16r^{ContextInfoEntry=[1C]}24B32
{Offset<siri::speech::schema_fb::TextToSpeechRequest_::ContextInfoEntry>=I}24@0:8^v16
r^{ContextInfoEntry=[1C]}
@32@0:8@16r^{AudioDescription=[1C]}24
@36@0:8@16r^{AudioDescription=[1C]}24B32
{Offset<siri::speech::schema_fb::AudioDescription>=I}24@0:8^v16
r^{AudioDescription=[1C]}
@32@0:8@16r^{WordTimingInfo=[1C]}24
@36@0:8@16r^{WordTimingInfo=[1C]}24B32
{Offset<siri::speech::schema_fb::WordTimingInfo>=I}24@0:8^v16
r^{WordTimingInfo=[1C]}
@32@0:8@16r^{TextToSpeechResponse=[1C]}24
@36@0:8@16r^{TextToSpeechResponse=[1C]}24B32
i16@0:8
{Offset<siri::speech::schema_fb::TextToSpeechResponse>=I}24@0:8^v16
r^{TextToSpeechResponse=[1C]}
@32@0:8@16r^{StartTextToSpeechStreamingRequest=[1C]}24
@36@0:8@16r^{StartTextToSpeechStreamingRequest=[1C]}24B32
{Offset<siri::speech::schema_fb::StartTextToSpeechStreamingRequest>=I}24@0:8^v16
r^{StartTextToSpeechStreamingRequest=[1C]}
{Offset<siri::speech::schema_fb::StartTextToSpeechStreamingRequest_::ContextInfoEntry>=I}24@0:8^v16
@32@0:8@16r^{BeginTextToSpeechStreamingResponse=[1C]}24
@36@0:8@16r^{BeginTextToSpeechStreamingResponse=[1C]}24B32
{Offset<siri::speech::schema_fb::BeginTextToSpeechStreamingResponse>=I}24@0:8^v16
r^{BeginTextToSpeechStreamingResponse=[1C]}
@32@0:8@16r^{PartialTextToSpeechStreamingResponse=[1C]}24
@36@0:8@16r^{PartialTextToSpeechStreamingResponse=[1C]}24B32
{Offset<siri::speech::schema_fb::PartialTextToSpeechStreamingResponse>=I}24@0:8^v16
r^{PartialTextToSpeechStreamingResponse=[1C]}
@32@0:8@16r^{FinalTextToSpeechStreamingResponse=[1C]}24
@36@0:8@16r^{FinalTextToSpeechStreamingResponse=[1C]}24B32
{Offset<siri::speech::schema_fb::FinalTextToSpeechStreamingResponse>=I}24@0:8^v16
r^{FinalTextToSpeechStreamingResponse=[1C]}
@32@0:8@16r^{TextToSpeechRouterStreamingStreamingRequest=[1C]}24
@36@0:8@16r^{TextToSpeechRouterStreamingStreamingRequest=[1C]}24B32
{Offset<siri::speech::qss_fb::TextToSpeechRouterStreamingStreamingRequest>=I}24@0:8^v16
r^{TextToSpeechRouterStreamingStreamingRequest=[1C]}
@32@0:8@16r^{TextToSpeechRouterStreamingStreamingResponse=[1C]}24
@36@0:8@16r^{TextToSpeechRouterStreamingStreamingResponse=[1C]}24B32
{Offset<siri::speech::qss_fb::TextToSpeechRouterStreamingStreamingResponse>=I}24@0:8^v16
r^{TextToSpeechRouterStreamingStreamingResponse=[1C]}
@48@0:8^{OpaqueCMTimebase=}16@24@32@?40
^{OpaqueCMTimebase=}16@0:8
@"NSObject<OS_dispatch_source>"
^{OpaqueCMTimebase=}
@44@0:8I16r^v20q28r^v36
^{OpaqueAudioQueue=}16@0:8
v24@0:8^{OpaqueAudioQueue=}16
v24@0:8^{OpaqueCMTimebase=}16
^{OpaqueAudioQueue=}
@"NSDate"
@"NSDictionary"
d64@0:8Q16{AudioStreamBasicDescription=dIIIIIIII}24
Q64@0:8d16{AudioStreamBasicDescription=dIIIIIIII}24
@64@0:8I16{AudioStreamBasicDescription=dIIIIIIII}20B60
v32@0:8@16@?24
@"VSAudioRouteInfo"
@"<VSAudioPlaybackServiceProtocol><AFAudioPowerProviding>"
@32@0:8@16^@24
@"<VSDeviceTTSCoreDelegate>"
@28@0:8@16i24
q24@0:8@16
@24@0:8d16
B24@0:8d16
@24@0:8Q16
@32@0:8Q16@24
@"VSMobileAssetsManager"
@"VSPreferencesInterface"
@"NSLock"
v56@0:8@16@?24@?32@?40@?48
f24@0:8d16
v32@0:8@16q24
@"CKKnowledgeStore"
v40@0:8@16@24d32
@"NSCache"
@"SATTSSpeechSynthesisResource"
@"SATTSSpeechSynthesisVoice"
@"VSPresynthesizedAudioRequest"
@"NSMutableData"
@"<AFAudioPowerProviding>"
{_opaque_pthread_mutexattr_t=q[8c]}16@0:8
v32@0:8{_opaque_pthread_mutexattr_t=q[8c]}16
@"NSMutableSet"
{_opaque_pthread_mutexattr_t="__sig"q"__opaque"[8c]}
v20@0:8i16
v48@0:8@16@?24@?32@?40
v64@0:8@16@24@32@40@48@?56
@"VSSpeechCache"
@"VSShortTermCache"
i24@0:8q16
i24@0:8@16
v44@0:8@16q24q32B40
v76@0:8q16d24@32q40q48Q56Q64B72
v56@0:8d16d24d32Q40q48
@"NSUUID"
@56@0:8{AudioStreamBasicDescription=dIIIIIIII}16
v40@0:8@16Q24@32
Q24@0:8@16
@"NSOperation<VSSpeechTaskProtocol>"
B64@0:8@16{AudioStreamBasicDescription=dIIIIIIII}24
@64@0:8{AudioStreamBasicDescription=dIIIIIIII}16@56
{AudioStreamBasicDescription=dIIIIIIII}24@0:8@16
@24@0:8q16
^v16@0:8
