VoiceServicesErrorDomain
Synthesis is cancelled/interrupted.
v8@?0
@"NSError"16@?0@"VSSpeechSynthesisCallbackResult"8
hash
TQ,R
superclass
T#,R
description
T@"NSString",R,C
debugDescription
speakTask
T@"VSSpeechSpeakTask",&,N,V_speakTask
readyForEagerTask
TB,N,V_readyForEagerTask
asbd
T{AudioStreamBasicDescription=dIIIIIIII},N,V_asbd
pcmBufferSize
TQ,N,V_pcmBufferSize
floatConverter
T^{OpaqueAudioConverter=},N,V_floatConverter
integerConverter
T^{OpaqueAudioConverter=},N,V_integerConverter
voiceBoostUnit
T^{OpaqueAudioComponentInstance=},N,V_voiceBoostUnit
audioTimeStamp
T{AudioTimeStamp=dQdQ{SMPTETime=ssIIIssss}II},N,V_audioTimeStamp
voiceBoostGainDecibels
Tf,N,V_voiceBoostGainDecibels
cached_engine_%@_%@
Use `initWithRequest:`.
deviceCore
T@"VSDeviceTTSCore",&,N,V_deviceCore
request
T@"VSSpeechRequest",&,N,V_request
error
T@"NSError",&,N,V_error
estimatedDuration
Td,R,N,V_estimatedDuration
instrumentMetrics
T@"VSInstrumentMetrics",&,N,V_instrumentMetrics
%@ %@ %@ %@ %.2f %.2f %.2f %@ %@
Speech is cancelled/interrupted.
v20@?0B8@"NSString"12
timingInfos
T@"NSArray",&,N,V_timingInfos
delegate
T@"<VSSpeechServiceDelegate>",W,N,V_delegate
engine
T@"VSSpeechEngine",&,N,V_engine
voiceBooster
T@"VSVoiceBooster",&,N,V_voiceBooster
playbackService
T@"VSAudioPlaybackService",&,N,V_playbackService
voiceSelection
T@"VSVoiceAssetSelection",&,N,V_voiceSelection
voiceResource
T@"VSVoiceResourceAsset",&,N,V_voiceResource
cachingService
T@"VSCachingService",&,N,V_cachingService
synthesisDidFallback
TB,N,V_synthesisDidFallback
speechCache
T@"<VSSpeechCacheItem>",&,N,V_speechCache
phonemes
T@"NSArray",&,N,V_phonemes
compressedAudio
T@"VSAudioData",&,N,V_compressedAudio
streamAudio
T@"VSStreamAudioData",&,N,V_streamAudio
taskAuxiliaryQueue
T@"NSObject<OS_dispatch_queue>",&,N,V_taskAuxiliaryQueue
ServerTTSErrorDomain
Use `initWithRequest:shouldSpeak:`.
%@:%@:server
%@:server
%@ %@
v40@?0@"NSData"8Q16@"NSData"24^B32
Unable to create playback service
EagerTTS:%@:%@:%@:%@:%@:%@:%@:%@:%@
v24@?0@"VSAudioData"8@"NSArray"16
v16@?0@"NSError"8
Server TTS timeout
Task is cancelled.
osprey_round_trip
osprey_streaming
ace_round_trip
ace_inline_streaming
device
inline_tts
unknown
Cancelled
Finished
speaking
synthesizing
voice
(null)
is_eager
is_one_shot
is_time_out
is_device_tts
source_of_tts
shouldSpeak
TB,N,V_shouldSpeak
wordTimingInfo
T@"NSArray",&,N,V_wordTimingInfo
timeoutCondition
T{_opaque_pthread_cond_t=q[40c]},N,V_timeoutCondition
T@"VSSpeechServerTask",&,N,V_speakTask
synthesisCore
T@"VSDeviceTTSCore",&,N,V_synthesisCore
useServerResponse
TB,N,V_useServerResponse
useDeviceSynthesis
TB,N,V_useDeviceSynthesis
speechStartReported
TB,N,V_speechStartReported
isEagerCache
TB,N,V_isEagerCache
disableOsprey
TB,N,V_disableOsprey
aceChannelRetryTimes
Tq,N,V_aceChannelRetryTimes
serverTTSRetryTimes
Tq,N,V_serverTTSRetryTimes
racingMutex
T{_opaque_pthread_mutex_t=q[56c]},N,V_racingMutex
serverAudio
T@"VSAudioData",&,N,V_serverAudio
deferredTTSTimingInfo
T@"NSArray",&,N,V_deferredTTSTimingInfo
internalSettings
T@"VSSpeechInternalSettings",&,N,V_internalSettings
ospreyCore
T@"VSOspreyTTSCore",&,N,V_ospreyCore
serverTTSConfig
T@"VSSiriServerConfiguration",&,N,V_serverTTSConfig
\mrk=emo=whisper\
com.apple.voiced.can-dump-audio
com.apple.voiced.request.durationestimation
com.apple.voiced.request.speech
com.apple.voiced.request.presynthesis
com.apple.voiced.request.synthesis
VSAudioPowerUpdateQueue
-[VSSpeechXPCHandler beginAudioPowerUpdateWithReply:]
v16@?0@"AFXPCWrapper"8
-[VSSpeechXPCHandler endAudioPowerUpdate]
-[VSSpeechXPCHandler getVoiceInfoForLanguageCode:footprint:gender:type:reply:]
connectionIdentifier
T@"NSString",&,N,V_connectionIdentifier
audioPowerUpdateQueue
T@"NSObject<OS_dispatch_queue>",&,N,V_audioPowerUpdateQueue
audioPowerUpdater
T@"AFAudioPowerUpdater",&,N,V_audioPowerUpdater
Osprey round-trip TTS timed out
v28@?0@"VSVoiceAsset"8@"VSVoiceResourceAsset"16f24
Osprey streaming network stall
Osprey streaming TTS timed out
Osprey core is not able to provide audio in time
serverTTSClient
T@"VSServerTTSClient",&,N,V_serverTTSClient
serverConfig
T@"VSSiriServerConfiguration",&,N,V_serverConfig
bufferDurationLimit
Td,N,V_bufferDurationLimit
T@"VSTimeoutCondition",&,N,V_timeoutCondition
didReceiveAudio
TB,N,V_didReceiveAudio
didReceiveAudioCondition
T@"NSCondition",&,N,V_didReceiveAudioCondition
T@"VSSpeechRequest",R,N,V_request
T@"<VSOspreyTTSCoreDelegate>",W,N,V_delegate
T@"VSInstrumentMetrics",W,N,V_instrumentMetrics
T@"VSVoiceAsset",&,N,V_voice
com.apple.voiced.postInstall
VSPostInstallService
xpc activity state is not RUN.
v16@?0@"NSObject<OS_xpc_object>"8
com.apple.MobileAsset.VoiceServicesVocalizerVoice
com.apple.MobileAsset.VoiceServices.CustomVoice
com.apple.MobileAsset.VoiceServices.GryphonVoice
com.apple.MobileAsset.VoiceServices.VoiceResources
InvalidCache
Audio duration too short
duration %.2f second
T@"NSString",&,N,V_key
audioData
T@"NSData",&,N,V_audioData
packetCount
Tq,N,V_packetCount
packetDescriptions
T@"NSData",&,N,V_packetDescriptions
magicVersion
Tq,R,N,V_magicVersion
T@"NSArray",R,N,V_timingInfos
audio
T@"VSAudioData",R,N,V_audio
TTSResources/PreinstallCache/
VSSpeechCacheErrorDomain
com.apple.voiceservices
-[VSSpeechCache initWithStorePath:]
VoiceServices
Cache type name too long
-[VSSpeechCache addCache:]
male
female
%@_%@
dirPath
T@"NSString",&,N,V_dirPath
preinstalledCacheDir
T@"NSString",&,N,V_preinstalledCacheDir
mutableAudioData
T@"NSMutableData",&,N,V_mutableAudioData
mutableDescription
T@"NSMutableData",&,N,V_mutableDescription
T@"NSData",&,N
FlatBuffers 1.11.0
v24@?0^v8Q16
fe_feature
TB,R,N
fe_feature_only
language
T@"NSString",R,N
gender
name
version
quality
resource
T@"OPTTSTextToSpeechVoice",R,N
T@"OPTTSTextToSpeechResource",R,N
channel_type
Tq,R,N
app_id
context_info
T@"NSArray",R,N
dialog_identifier
experiment_identifier
meta_info
context
experiment
feature_flags
speech_id
session_id
text
audio_type
enable_word_timing_info
voice_name
preferred_voice_type
T@"OPTTSTextToSpeechRequestMeta",R,N
T@"OPTTSTextToSpeechRequestContext",R,N
T@"OPTTSTextToSpeechRequestExperiment",R,N
T@"OPTTSTTSRequestFeatureFlags",R,N
value
sample_rate
Td,R,N
format_id
TI,R,N
format_flags
bytes_per_packet
frames_per_packet
bytes_per_frame
channels_per_frame
bits_per_channel
reserved
word
sample_idx
offset
length
timestamp
Tf,R,N
decoder_description
playback_description
word_timing_info
v20@?0r*8I16
error_code
Ti,R,N
error_str
T@"NSData",R,N
T@"OPTTSAudioDescription",R,N
T@"OPTTSTextToSpeechMeta",R,N
stream_id
streaming_playback_buffer_size_in_seconds
current_pkt_number
total_pkt_number
content
content_type
contentAsOPTTSStartTextToSpeechStreamingRequest
T@"OPTTSStartTextToSpeechStreamingRequest",R,N
contentAsOPTTSBeginTextToSpeechStreamingResponse
T@"OPTTSBeginTextToSpeechStreamingResponse",R,N
contentAsOPTTSPartialTextToSpeechStreamingResponse
T@"OPTTSPartialTextToSpeechStreamingResponse",R,N
contentAsOPTTSFinalTextToSpeechStreamingResponse
T@"OPTTSFinalTextToSpeechStreamingResponse",R,N
Verifier
/System/Volumes/Data/SWE/iOS/BuildRoots/BuildRoot857/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneSimulator.platform/Developer/SDKs/iPhoneSimulator14.2.Internal.sdk/usr/local/include/flatbuffers/flatbuffers.h
size_ < FLATBUFFERS_MAX_BUFFER_SIZE
NotNested
!nested
!num_field_loc
ensure_space
cur_ >= scratch_ && scratch_ >= buf_
size() < FLATBUFFERS_MAX_BUFFER_SIZE
reallocate_downward
new_size > old_size
EndTable
nested
table_object_size < 0x10000
!ReadScalar<voffset_t>(buf_.data() + field_location->id)
data
cur_
scratch_end
scratch_
scratch_data
buf_
finished
ReferTo
off && off <= GetSize()
EndVector
Finish
strlen(file_identifier) == kFileIdentifierLength
allocator<T>::allocate(size_t n) 'n' exceeds maximum supported size
'%c%c%c%c', %.0fhz, %d bits, %d FPP, 
%@:%@
%@:%@:%@:%@:%@:%@
voiced_tts_playback_queue
audioBytesRange
T{_NSRange=QQ},N,V_audioBytesRange
TQ,N,V_packetCount
packetDescriptionsRange
T{_NSRange=QQ},N,V_packetDescriptionsRange
Error AudioQueueStart
YYYY-MM-dd hh:mm:ss:SSS
-[VSAudioPlaybackService getAveragePower:andPeakPower:]
audioQueue
T^{OpaqueAudioQueue=},N,V_audioQueue
state
Tq,N,V_state
waitForStateChangeMutex
T{_opaque_pthread_mutex_t=q[56c]},N,V_waitForStateChangeMutex
stateChangeCondition
T{_opaque_pthread_cond_t=q[40c]},N,V_stateChangeCondition
enqueuedSampleCount
Td,N,V_enqueuedSampleCount
audioStartTimeStamp
T{AudioTimeStamp=dQdQ{SMPTETime=ssIIIssss}II},N,V_audioStartTimeStamp
audioQueueStartDate
T@"NSDate",&,N,V_audioQueueStartDate
audioQueueFutureEndDate
T@"NSDate",&,N,V_audioQueueFutureEndDate
outputRoute
T@"NSString",&,N,V_outputRoute
mappedData
T@"VSMappedData",&,N,V_mappedData
enqueuedMappedAudioInfo
T@"NSMutableArray",&,N,V_enqueuedMappedAudioInfo
playingBufferCount
TQ,N,V_playingBufferCount
dequeueCondition
T@"NSCondition",&,N,V_dequeueCondition
sessionID
TI,N,V_sessionID
discontinuedDuringPlayback
TB,R,N,V_discontinuedDuringPlayback
com.apple.voiceservices.notification.voice-update
Missing utterance in the request (preprocessing missing?).
No voice available
Compact voice is explicitly disabled.
Voice is deleted already.
Can't create VSSpeechEngine
selectedVoice
T@"VSVoiceAssetSelection",&,N,V_selectedVoice
selectedVoiceResource
T@"VSVoiceResourceAsset",&,N,V_selectedVoiceResource
T@"<VSDeviceTTSCoreDelegate>",W,N,V_delegate
T@"VSAudioData",R,N,V_compressedAudio
T@"VSStreamAudioData",R,N,V_streamAudio
CACHE_DELETE_AMOUNT
CACHE_DELETE_VOLUME
com.apple.voiced.CacheDelete
r^{__CFDictionary=}20@?0i8r^{__CFDictionary=}12
refreshTimeoutCondition
T@"NSCondition",&,N,V_refreshTimeoutCondition
shouldStop
TB,N,V_shouldStop
timeoutValue
Td,N,V_timeoutValue
com.apple.voiced.downloadQueue
v20@?0d8f16
mobileAssetManager
T@"VSMobileAssetsManager",&,N,V_mobileAssetManager
preferenceInterface
T@"VSPreferencesInterface",&,N,V_preferenceInterface
type
TQ,R,N,V_type
https://dejavu.apple.com
https://seed-dejavu.siri.apple.com
https://carry-dejavu.siri.apple.com
deviceID
T@"NSString",&,N,V_deviceID
/private/var/mobile/Library/Logs/CrashReporter/VoiceServices/
mobile
-[VSDiagnosticService createDirectoryIfNeeded]
yyyy_MM_dd-HHmmss.SSS
TTS-%@
.tmp
.wav
audioDumpPath
T@"NSString",&,N,V_audioDumpPath
audioDumpFileAttributes
T@"NSDictionary",&,N,V_audioDumpFileAttributes
default
ServerTTSTimeoutV2
DeviceWaitTimeV2
DisableOsprey
TTSExperimentConfig
identifier
method
delayed
WhitelistedAppId
com.apple.springboard
com.apple.siri
com.apple.CarPlayApp
com.apple.Carousel
com.apple.AssistantServices
com.apple.MapsSupport
knowledgeStore
T@"CKKnowledgeStore",&,N,V_knowledgeStore
shouldDelayVoiceUpdate
whitelistedAppId
cache
T@"NSCache",&,N,V_cache
cacheTimer
T@"NSMutableDictionary",&,N,V_cacheTimer
Use `initWithRequest:withStreamID:`.
Unknown inline streaming error %d, %@
Missing utterance in the request (preprocessing missing?). Can't fallback to device TTS.
voice_resource
streamID
T@"NSString",&,N,V_streamID
deviceTTSCore
T@"VSDeviceTTSCore",&,N,V_deviceTTSCore
playbackServices
T@"VSAudioPlaybackService",&,N,V_playbackServices
finalTimingInfo
T@"NSMutableArray",&,N,V_finalTimingInfo
playbackBeginDate
T@"NSDate",&,N,V_playbackBeginDate
task: inprogress %@, request: %@
Can't create VSAudioPlaybackService
Can't decode audio data
T@"VSPresynthesizedAudioRequest",R,N,V_request
T@"NSMutableData",&,N,V_audioData
previousProvider
T@"<AFAudioPowerProviding>",W,N,V_previousProvider
com.apple.voiced.VSInlineStreamService
queuedNotification
T@"NSMutableDictionary",&,N,V_queuedNotification
ongoingNotifications
T@"NSMutableSet",&,N,V_ongoingNotifications
streamRequestQueue
T@"NSMutableArray",&,N,V_streamRequestQueue
lock
T{_opaque_pthread_mutex_t=q[56c]},N,V_lock
recursiveLockAttr
T{_opaque_pthread_mutexattr_t=q[8c]},N,V_recursiveLockAttr
notifyQueue
T@"NSObject<OS_dispatch_queue>",&,N,V_notifyQueue
TB,N
T@"NSString",C,N
T@"OPTTSTextToSpeechVoice",C,N
T@"OPTTSTextToSpeechResource",C,N
Tq,N
T@"NSArray",C,N
T@"OPTTSTextToSpeechRequestMeta",C,N
T@"OPTTSTextToSpeechRequestContext",C,N
T@"OPTTSTextToSpeechRequestExperiment",C,N
T@"OPTTSTTSRequestFeatureFlags",C,N
Td,N
TI,N
Tf,N
Ti,N
T@"NSData",C,N
T@"OPTTSAudioDescription",C,N
T@"OPTTSTextToSpeechMeta",C,N
T@"OPTTSStartTextToSpeechStreamingRequest",C,N
T@"OPTTSBeginTextToSpeechStreamingResponse",C,N
T@"OPTTSPartialTextToSpeechStreamingResponse",C,N
T@"OPTTSFinalTextToSpeechStreamingResponse",C,N
com.apple.voiceservices.notification.voice-purge
com.apple.voiced.prewarmQueue
Prewarm textify emoji
gryphon_frontend
VoiceServices/config
cachedEngine
T@"VSSpeechEngine",&,N,V_cachedEngine
loadedResources
T@"VSVoiceResourceAsset",&,N,V_loadedResources
prewarmQueue
T@"NSObject<OS_dispatch_queue>",&,N,V_prewarmQueue
com.apple.voiceservices.request
%@.%@.%@
client
%@.%@
locale
footprint
loggingPrefix
T@"NSString",&,N,V_loggingPrefix
v16@?0@"SABaseCommand"8
v20@?0B8@"NSError"12
-[VSServerTTSClient ospreyStartSynthesisRequest:responseHandler:completion:]_block_invoke
Unable to process audio data.
v24@?0@"OPTTSTextToSpeechResponse"8@"NSError"16
v16@?0@"OPTTSBeginTextToSpeechStreamingResponse"8
v16@?0@"OPTTSPartialTextToSpeechStreamingResponse"8
v16@?0@"OPTTSFinalTextToSpeechStreamingResponse"8
\vol=%d\%@
\rate=%d\%@
\pitch=%d\%@
-[VSServerTTSClient aceStartSynthesisRequest:responseHandler:completion:]_block_invoke
Unable to parse audio data
rate
pitch
volume
isEager
com.apple.voiced.cachingQueue
v32@?0@"NSData"8Q16@"NSData"24
gryphon
PreinstalledCache
premium
threadLock
T@"NSLock",&,N,V_threadLock
inMemoryCaches
T@"NSMutableArray",&,N,V_inMemoryCaches
cacheStore
T@"VSSpeechCache",&,N,V_cacheStore
shortTermCache
T@"VSShortTermCache",&,N,V_shortTermCache
cachingQueue
T@"NSObject<OS_dispatch_queue>",&,N,V_cachingQueue
VSVocalizerEngine
T{shared_ptr<SiriTTS::VoiceResource>=^{VoiceResource}^{__shared_weak_count}},V_resource
T@"NSString",&,N,V_text
stopMark
Tq,N,V_stopMark
callback
T@?,C,N,V_callback
wordTimings
T@"NSMutableArray",&,N,V_wordTimings
samplesProcessed
TQ,N,V_samplesProcessed
lastUTF8Offset
TQ,N,V_lastUTF8Offset
lastUTF16Offset
TQ,N,V_lastUTF16Offset
numOfPromptsTriggered
TQ,N,V_numOfPromptsTriggered
neuralDidFallback
TB,N,V_neuralDidFallback
hasAlignmentStall
TB,N,V_hasAlignmentStall
path
mimeType
TTSSynthesizer::load_voice_resource
unknown path
unknown mime-type
i12@?0i8
tts.neural.use_fallback
tts.metrics.alignment_stall
TTSSynthesizer::synthesize_text_with_markers_async
application/edct-bin-dictionary
application/x-vocalizer-rettt+text
voicePath
T@"NSString",&,N,V_voicePath
synthesizer
T^{TTSSynthesizer={unique_ptr<TTSSynthesizer::TTSSynthesizerInternal, std::__1::default_delete<TTSSynthesizer::TTSSynthesizerInternal> >={__compressed_pair<TTSSynthesizer::TTSSynthesizerInternal *, std::__1::default_delete<TTSSynthesizer::TTSSynthesizerInternal> >=^{TTSSynthesizerInternal}}}},N,V_synthesizer
currentCallbackResult
T@"VSSpeechSynthesisCallbackResult",&,N,V_currentCallbackResult
T{AudioStreamBasicDescription=dIIIIIIII},R,N,V_asbd
Tf,N,V_rate
Tf,N,V_pitch
Tf,N,V_volume
mappedAudioInfo
T@"NSMutableArray",&,N,V_mappedAudioInfo
com.apple.voiced.pthreadQueue
com.apple.voiced.speakingQueue
eagerTasks
T@"NSMutableArray",&,N,V_eagerTasks
speakTasks
T@"NSMutableArray",&,N,V_speakTasks
currentTask
T@"NSOperation<VSSpeechTaskProtocol>",&,N,V_currentTask
threadMutex
T{_opaque_pthread_mutex_t=q[56c]},N,V_threadMutex
threadMutexAttr
T{_opaque_pthread_mutexattr_t=q[8c]},N,V_threadMutexAttr
speakingQueue
T@"NSObject<OS_dispatch_queue>",&,N,V_speakingQueue
lastSynthesisRequest
T@"VSSpeechRequest",&,N,V_lastSynthesisRequest
lastSynthesisRequestCreatedTimeStamp
TQ,N,V_lastSynthesisRequestCreatedTimeStamp
audioDataFromFile:error:
AudioFileOpenURL
AudioFileGetProperty kAudioFilePropertyDataFormat
AudioFileGetProperty kAudioFilePropertyAudioDataByteCount
AudioFileGetProperty kAudioFilePropertyAudioDataPacketCount
+[VSAudioData(SAUIAudioData) audioDataWithASBD:rawData:]
filePath
T@"NSString",R,N,V_filePath
fileSize
TQ,R,N,V_fileSize
T^v,R,N,V_mappedData
Ti,R,N,V_fd
SynthesisTask done synthesize %lu characters, audio duration %f, error %@
Task %llu reported word time info
Device SpeakTask %llu: Instrument metric: %@
Task %llu started speaking
Device EagerTask %llu: Instrument metric: %@
Task %llu reported finish, error: %@
Error AudioUnitSetProperty _floatConverter %@
Error AudioUnitSetProperty _integerConverter %@
Error AudioComponentInstanceNew _voiceBoostUnit %@
Error AudioUnitSetProperty _voiceBoostUnit, kAudioUnitProperty_MaximumFramesPerSlice %@
Error AudioUnitSetProperty _voiceBoostUnit, kAudioUnitProperty_StreamFormat, kAudioUnitScope_Input %@
Error AudioUnitSetProperty _voiceBoostUnit, kAudioUnitProperty_StreamFormat, kAudioUnitScope_Output, %@
Error AudioUnitInitialize _voiceBoostUnit %@
Error AudioUnitSetParameter %@
Error AudioConverterConvertComplexBuffer _floatConverter %@
Error AudioUnitProcess _voiceBoostUnit %@
Error AudioConverterConvertComplexBuffer _integerConverter %@
Using timestamp inside voiced for Estimation task
Created Estimation task %llu
Unable to create engine for request %@
Estimated duration: %.2f, for utterance: %@
Using timestamp inside voiced for task
Created Task %llu (%p)
Starting speech task %llu
Short-term cached synthesis is found for text '%@'
Detected synthesis stall, starting tailspin
Finished tail spin, success:%d, file: %@
SpeakTask done synthesize %lu characters, audio duration %f, error %@
Device task %llu: Instrument metric: %@
Using requestCreatedTimestamp inside voiced for Server task
Created Server task %llu: shouldSpeak %{BOOL}d
forceOsprey is on
Received server TTS response. Use Server TTS.
Received device synthesis previously, ignore server TTS.
Encountered Osprey streaming network stall. Retry with device TTS.
Server network error: %@
Device fallback synthesis is disabled.
Start playing device synthesis instead.
Device racing is disabled.
Received server TTS previously, ignore device TTS
Received audio from device synthesis. Use device synthesis immediately.
Received audio from device synthesis, but it's deferred.
Inline server TTS is previously cached.
Eager server TTS is previously cached.
Server TTS Ace round-trip retry times: %ld
Device TTS wait time: %.2f
Unexpected code path in Osprey server task
Wait for network response, timeout value: %.3f
Server task %llu started speaking
Server task %llu: Instrument metric: %@
Error in server task %llu, error: %@
Server task %llu: %@ %@ utterance: '%@', %{public}@
%@ is not TTS language, fallback to %@
Use SSML input: %@
Utterance to synthesize for request %llu: '%@'
Overwriting volume with internal default: %.3f
Process is not entitled for dumping audio. Ignore outputPath
Update with connection identifier: %{public}@
Find on-going task: %@, ignoring prewarm request: %@
Server TTS is disabled in internal settings
Created stream speak task %llu
Created server speak task %llu
Created speak task %llu
Created presynthesized task %llu
Cache #PresynthesizedRequest %llu with text: %@
Cache #PresynthesizedRequest %llu skipped: no audio
Created server synthesis task %llu
Created synthesis task %llu
Stop speech request at mark: %d
client '%@' set auto download voice assets:%@
Start cellular download for %@
%s override voice info for server TTS platform, %@
Ignore stream object with nil stream ID: %@
Enqueue stream object %@, streamId: %@
Received invokeDaemon, I'm listening
Received killDaemon, shutting down
Simulate network stall is on, ignore audio object
Refresh timeout value as %.2f
Simulate network stall is on, ignore completion callback
Network stall in osprey streaming
Timeout in osprey streaming
Osprey core %p is cancelled
xpc activity com.apple.voiced.postInstall state: %d
Migration service finished.
xpc activity com.apple.voiced.postInstall, failed to set state to done.
VSPostInstallService registered xpc activity XPC_ACTIVITY_POST_INSTALL
Defaults disables reset, skip resetting MobileAsset default URL
Resetting MobileAsset default URL
Reading cache %@ error: %@
Error %s, %@
Cache type name too long %@
%@ is not TTS language, falling back to %@
Error in reading audio data from file: %@ error:%@
Error AudioQueueNewOutputWithAudioSession %@
Current audio output route: %@
Unable to set kAudioQueueProperty_ClientUID, errno: %@
AudioQueue initialized with session ID: %d
VSAudioPlaybackService %p init latency: %.3f
Error AudioQueueDispose %@
mediaserverd reset
Signal AudioQueue running state change
Error AudioQueueStart %@
VSAudioPlaybackService %p success AudioQueueStart
Error AudioQueueAllocateBuffer %@
Audio queue start sample time: %.0f
Error AudioQueueEnqueueBuffer %@
VSAudioPlaybackService %p enqueued audio buffer at sample time: %.2f, size: %ld, total enqueued samples: %.0f, discontinuity: %{BOOL}d
Base64 of first 64 bytes data: %@
Error AudioQueueFlush %@
Error AudioQueueStop %@
Error AudioQueuePause %@
VSAudioPlaybackServices %p success AudioQueuePause
Error AudioQueueReset %@
Error AudioQueueAddPropertyListener %@
Error AudioQueueRemovePropertyListener %@
Detected stall of audio queue, based on NSDate. Now: %@, supposed end time: %@, Tolerance: %.2f
Error AudioQueueGetProperty isRunning %@
Unable to enable kAudioQueueProperty_EnableLevelMetering, err: %@
Unable to disable kAudioQueueProperty_EnableLevelMetering, err: %@
Error: %s, errno: %@
VSAudioPlaybackServices %p played audio buffer at sample time: %f, size: %ld
Error AudioQueueFreeBuffer %@
On-disk cached synthesis %@ is found.
In-memory cached synthesis %@ is found.
Reset MobileAsset query cache and retry selecting voice
No voice available
Compact voice is explicitly disabled.
Voice is deleted at path '%@'
#CacheDelete asset cleaning is disabled in internal setting. Skip purgeable assets for urgency %d
#CacheDelete purgeable active voice asset: %@
#CacheDelete purgeable inactive voice asset: %@
#CacheDelete query purgeable size, urgency: %d / %d, info: %@
#CacheDelete purge, urgency: %d / %d, info: %@
#CacheDelete periodic purge, urgency: %d / %d, info: %@
Voice download is in progress, skip new download. %@
Updating target voice: %@
WatchOS skip downloading non-gryphon voice: %@
Voice update decision: shouldDownload:%d, canUseBattery:%d. Reason: triggerType:%d, compactVoiceSelected:%d, activeSiriUser:%d, serverExperimentDelay:%d
#MobileAsset #DownloadVoice downloading progress: %.2f, remainingTime: %.2f, voice: %@
#MobileAsset #DownloadVoice finished downloading voice: %@
No additional VoiceResources to queue. Current count is %lu items.
Adding %lu VoiceResources to download to a queue of %lu items.
Updating VoiceResource for %@...
Created audio dump directory %@
Unable to create intermediate audio dump at '%@'
Unable to create audio dump at '%@', error: %@
Audio save as %@
Unable to parse json for key '%@', error: %@
JSON for key '%@' is not dictionary
Added short term cache:%@ for key:'%@'
Removed short term cache for key:'%@'
Removed short term cache for all keys
Using timestamp inside voiced for Stream task
Created Stream task %llu: streamID %@
Simulate network stall is on, ignore object %@
Unknown streaming object: %@
Handle stream begin with streamId: %@, text: %@, decoder: %@
Handle stream chunk with streamId: %@
Reached buffer threshold. Start playing audio.
Handle stream end with streamId: %@, count: %@
Stream TTS network stall.
Inline streaming TTS timeout.
Streaming error: %@, error_code: %d
Error in stream task %llu, error: %@
Stream task %llu: %@ speaking text: '%@', %{public}@
Stream task %llu: Instrument metric: %@
Using timestamp inside voiced for Presynthesized task
Created Presynthesized Task %llu
Speaking pre-synthesized audio: %@
Can't create VSAudioPlaybackService
Error, %@
Pre-synthesized audio request stopped
Finished speaking pre-synthesized audio: %@
Task is cancelled by user: %@
Received inline streaming TTS with id %@, text: %@
Notification for %@ is on-going. Posting object immediately %@
Notification for %@ has not started. Cache object %@
Start notifying for: %@
No cached object found for notification %@.
%d cached objects found for notification: %@
Notify %@ with cached object %@
Remove notification %@
Prewarming: Invoked with request: '%@'
Unable to prewarm, error: %@
Can't prewarm engine with path '%@'
Prewarm finished. Latency: %.3f
Prewarming: Completed with request: '%@'
Can't create engine with path '%@'
Voice specific resources found.
Specified resource file '%@' does not exist at: '%@'
disableServerTTS is enabled by user default, disable server TTS
forceServerTTS is enabled by user default, force server TTS
forceServerTTS is enabled by speech request, force server TTS
Preinstalled cache is found, disable server TTS
Neural voice is found on device, disable server TTS
Short term cache is found for the text, use server TTS
Server TTS is disabled since '%@' is not in the whitelist
Received AceObject: %@
Voice count for %@_%@: %@
Voice: %@:%@:%@:%@:%@:%@
Error: %@
Sent AceObject: %@
%s, %@
#AFClientLite Request %llu received AceObject: %@, %s
Error: %@, %s
Server TTS word timing info size: %d
Server TTS word timing info, offset: %ld, length: %ld, word: %@, sampleIndex: %@, timestamp: %.2f
Synthesis completed, total packet number: %d, %s
#AFClientLite error: %@, %s
#AFClientLite Request %llu gets completion call. %s
playbackService is initialized already.
Starting AudioQueue
Error in device task %llu, error: %@
Device task %llu: %@ %@ utterance: '%@', %{public}@
Cached streamAudio in task %llu with hash %@ in memory
Cached audio in task %llu with hash %@ in memory
Error converting audio during caching. %@
Error converting stream audio during caching. %@
Caching is disabled. Skipping caching.
Unrecognized audio object, skip caching
Audio duration is too short: %.2f second, skip caching
Can't add audio cache, error: %@
Preinstalled cached synthesis %@ is found.
Exception: %s
voice path '%@', resource path '%@'
%d files under voice path
Failed to initialize synthesizer due to missing voice path.
Initializing engine with voice path: %@
Failed to initialize synthesizer: %s
Failed to initialize synthesizer: %zu
Error setPitch 0x%zx
Error setRate 0x%zx
Error setVolume 0x%zx
Unable to load resource '%@'
Url doesn't conform to RFC 1808 '%@'
Loading resource: %@, mime-type: %@
Unable to find mime-type for '%@'
Unknown voice resource handle to unload: %@
Engine preheating latency: %.3f
Error AudioFileCreateWithURL: '%@', code: %@
Unable to begin OPUS decoder, %@
Error during decoding, %@
Error AudioFileWriteBytes: '%@', code: %@
Error AudioFileClose: '%@', code: %@
Start spinNextTask
Dispatch speaking task %llu
Starting task %llu
PresynthesisTask %llu requested to wait another speaking task %llu
New speak task %llu interrupts speaking task %llu
%llu interrupt task %llu
Speak task %llu is attached to eager task %llu
Dispatch synthesis task %llu
Finish spinNextTask
Cancel task %p, %llu
decoderStreamDescription formatID: %@, sample rate: %@
Unknown server audio format ID: %d
%s, invalid opus data
%s, Unknown format: %d
Invalid chunk size: %d at offset %d, bytes count = %d
Unable to convert OPUS to PCM. %@
OPTTSTextToSpeechResponse word timing info, offset: %ld, length: %ld, word: %@, sampleIndex: %d, timestamp: %.2f
Unable to madvise file '%@' MADV_DONTNEED, error: %s
Unable to munmap file '%@', error: %s
Unable to open file '%s', error: %s
Unable to get size of file '%s', error: %s
Unable to mmap '%s', error: %s
fcntl called on file '%@', size: %lu
N11flatbuffers16DefaultAllocatorE
N11flatbuffers9AllocatorE
#<NSt3__110__function6__funcIU8__strongU13block_pointerFiN14TTSSynthesizer15CallbackMessageEENS_9allocatorIS6_EES4_EE
NSt3__110__function6__baseIFiN14TTSSynthesizer15CallbackMessageEEEE
U13block_pointerFiN14TTSSynthesizer15CallbackMessageEE
NSt3__110__function6__funcIZ51-[VSSpeechEngine synthesizeText:loggable:callback:]E3$_0NS_9allocatorIS2_EEFvPKvEEE
NSt3__110__function6__baseIFvPKvEEE
Z51-[VSSpeechEngine synthesizeText:loggable:callback:]E3$_0
NSt3__110__function6__funcIZ51-[VSSpeechEngine synthesizeText:loggable:callback:]E3$_1NS_9allocatorIS2_EEFvPKvEEE
Z51-[VSSpeechEngine synthesizeText:loggable:callback:]E3$_1
VSSpeechSynthesisTask
VSSpeechEagerProtocol
VSSpeechSpeakableProtocol
VSSpeechTaskProtocol
NSObject
VSVoiceBooster
VSDurationEstimationTask
VSSpeechSpeakTask
VSSpeechServerTask
VSDeviceTTSCoreDelegate
VSOspreyTTSCoreDelegate
VSSpeechXPCHandler
VSSpeechXPCServiceProtocol
VSSpeechServiceDelegate
VSOspreyTTSCore
VSPostInstallService
VSSpeechCacheAudio
VSSpeechCacheItem
VSSpeechCache
VSAudioData
NSCopying
OPTTSTTSRequestFeatureFlags
FLTBFBufferAccessor
OPTTSTextToSpeechVoice
OPTTSTextToSpeechResource
OPTTSTextToSpeechMeta
OPTTSTextToSpeechRequestMeta
OPTTSTextToSpeechRequestContext
OPTTSTextToSpeechRequestExperiment
OPTTSTextToSpeechRequest
OPTTSTextToSpeechRequest_ContextInfoEntry
OPTTSAudioDescription
OPTTSWordTimingInfo
OPTTSTextToSpeechResponse
OPTTSStartTextToSpeechStreamingRequest
OPTTSStartTextToSpeechStreamingRequest_ContextInfoEntry
OPTTSBeginTextToSpeechStreamingResponse
OPTTSPartialTextToSpeechStreamingResponse
OPTTSFinalTextToSpeechStreamingResponse
OPTTSTextToSpeechRouterStreamingStreamingRequest
OPTTSTextToSpeechRouterStreamingStreamingResponse
VSAceObjectUtility
VSAudioMappedInfo
VSAudioPlaybackService
AFAudioPowerProviding
VSDeviceTTSCore
VSCacheDeleteService
VSTimeoutCondition
VSDownloadService
OspreyTTSService
SpeechService
VSDiagnosticService
VSSiriServerConfiguration
VSShortTermCache
VSSiriInlineTTSStreamTask
VSSpeechPresynthesizedTask
VSSpeechAudioPowerService
VSInlineStreamService
OPTTSMutableTTSRequestFeatureFlags
OPTTSMutableTextToSpeechVoice
OPTTSMutableTextToSpeechResource
OPTTSMutableTextToSpeechMeta
OPTTSMutableTextToSpeechRequestMeta
OPTTSMutableTextToSpeechRequestContext
OPTTSMutableTextToSpeechRequestExperiment
OPTTSMutableTextToSpeechRequest
OPTTSMutableTextToSpeechRequest_ContextInfoEntry
OPTTSMutableAudioDescription
OPTTSMutableWordTimingInfo
OPTTSMutableTextToSpeechResponse
OPTTSMutableStartTextToSpeechStreamingRequest
OPTTSMutableStartTextToSpeechStreamingRequest_ContextInfoEntry
OPTTSMutableBeginTextToSpeechStreamingResponse
OPTTSMutablePartialTextToSpeechStreamingResponse
OPTTSMutableFinalTextToSpeechStreamingResponse
OPTTSMutableTextToSpeechRouterStreamingStreamingRequest
OPTTSMutableTextToSpeechRouterStreamingStreamingResponse
VSPrewarmService
VSAggdService
VSServerTTSClient
Utilities
VSCachingService
VSSpeechEngineVoiceResource
VSSpeechSynthesisCallbackResult
VSSpeechEngine
VSStreamAudioMappedInfo
VSStreamAudioData
VSSpeechTaskQueue
SAUIAudioData
VSHelpers
VSMemoryMap
initWithRequest:
speakTask
fetchVoiceResource
fetchVoiceAsset
request
text
languageCode
gender
estimatedTTSWordTimingForText:withLanguage:withGender:
setTimingInfos:
error
cachingService
fetchCacheForTask:
speechCache
synthesize
streamAudio
duration
instrumentMetrics
setAudioDuration:
reportTimingInfo
speakCachedAudio
speechBeginTimestamp
setSpeechEndTimestamp:
playbackService
discontinuedDuringPlayback
setErrorCode:
setReadyForEagerTask:
isCancelled
dictionaryWithObjects:forKeys:count:
errorWithDomain:code:userInfo:
setError:
reportInstrumentMetrics
defaultService
tallyTask:
dumpStreamAudio:
outputPath
path
writeWaveToFilePath:
shouldCache
synthesisDidFallback
enqueueCache
reportFinish
logFinish
prepareForSynthesis
audioSessionID
startPlaybackServiceWithAudioSessionID:
setSynthesisBeginTimestamp:
engine
utterance
canLogRequestText
state
mutablePCMData
voiceBooster
processData:
appendAudioData:packetCount:packetDescriptions:
enqueue:packetCount:packetDescriptions:
taskAuxiliaryQueue
reportSpeechStart
setSynthesisEndTimestamp:
neuralDidFallback
setSynthesisDidFallback:
numOfPromptsTriggered
setPromptCount:
timingInfos
wordTimingInfos
adjustWordTimingInfo
synthesizeText:loggable:callback:
length
waitUntilAudioFinished
delegate
synthesisRequest:didReceiveTimingInfo:
requestCreatedTimestamp
speechRequestDidReceiveTimingInfo:
speechRequestSuccessWithInstrumentMetrics:
setUtterance:
voiceAssetKey
setVoiceAssetKey:
voiceResourceAssetKey
setVoiceResourceAssetKey:
synthesisBeginTimestamp
synthesisEndTimestamp
setSpeechBeginTimestamp:
speechEndTimestamp
audioStartTimestampDiffs
setAudioStartTimestampDiffs:
audioDuration
isWarmStart
setIsWarmStart:
setEagerRequestCreatedTimeStampDiffs:
promptCount
errorCode
speechRequestDidStart
speechRequestDidStopWithSuccess:phonemesSpoken:error:
phonemes
componentsJoinedByString:
synthesisRequest:didFinishWithInstrumentMetrics:error:
isEqual:
class
self
performSelector:
performSelector:withObject:
performSelector:withObject:withObject:
isProxy
isKindOfClass:
isMemberOfClass:
conformsToProtocol:
respondsToSelector:
retain
release
autorelease
retainCount
zone
hash
superclass
description
debugDescription
start
suspend
resume
cancel
taskHash
isSpeaking
voiceKey
audioPowerProvider
readyForEagerTask
setSpeakTask:
main
.cxx_destruct
_readyForEagerTask
_speakTask
init
uninitialize
dealloc
vs_stringFrom4CC:
initialize
voiceBoostGainDecibels
pcmBufferSize
asbd
mutableBytes
dataWithLength:
initWithStreamDescription:pcmBufferSize:
setVoiceBoostGainDecibels:
setAsbd:
setPcmBufferSize:
floatConverter
setFloatConverter:
integerConverter
setIntegerConverter:
voiceBoostUnit
setVoiceBoostUnit:
audioTimeStamp
setAudioTimeStamp:
_voiceBoostGainDecibels
_pcmBufferSize
_floatConverter
_integerConverter
_voiceBoostUnit
_asbd
_audioTimeStamp
shortTermCachedEngines
stringWithFormat:
objectForKey:
exceptionWithName:reason:userInfo:
setRequestCreatedTimestamp:
deviceCore
selectedVoice
selectedVoiceResource
shortTermCachedEngineForVoice:voiceResource:
setEngine:
sharedManager
selectVoiceForLang:type:gender:footprint:
voicePath
initWithVoicePath:resourcePath:
setObject:forKey:timeToLive:
compressedAudio
setRequest:
estimatedDuration
setInstrumentMetrics:
setDeviceCore:
_request
_error
_estimatedDuration
_instrumentMetrics
_deviceCore
canUseServerTTS
setCanUseServerTTS:
clientBundleIdentifier
setClientBundleIdentifier:
standardService
voiceSelection
voiceResource
rate
pitch
volume
contextInfo
customResourceURLs
sha256hex
popShortTermCacheForHash:
logText
setIsServerTTS:
setIsCacheHitFromMemory:
setSourceOfTTS:
audio
setSpeechCache:
synthesizeAndSpeak
setIsSpeechRequest:
collectTailspin:
stopAtMarker:
stop
pausePlayback
resumePlayback
array
countByEnumeratingWithState:objects:count:
loadResource:error:
addObject:
voiceData
type
addObjectsFromArray:
hasAlignmentStall
setNeuralAlignmentStall:
unloadResource:
setPhonemes:
reportInstrumentMetrics:
setDelegate:
setVoiceBooster:
setPlaybackService:
setVoiceSelection:
setVoiceResource:
setCachingService:
setCompressedAudio:
setStreamAudio:
setTaskAuxiliaryQueue:
_synthesisDidFallback
_timingInfos
_delegate
_engine
_voiceBooster
_playbackService
_voiceSelection
_voiceResource
_cachingService
_speechCache
_phonemes
_compressedAudio
_streamAudio
_taskAuxiliaryQueue
standardInstance
defaultConfig
forceOsprey
disableOsprey
useDeviceSynthesis
synthesisCore
footprint
useServerResponse
setUseServerResponse:
ospreyCore
voice
genderStringFromGender:
wordTimingInfo
setWordTimingInfo:
playAudioData:
serverAudio
setServerAudio:
concatenateWithAudio:
localizedInterstitialStringForKey:language:gender:
copy
setText:
setUseDeviceSynthesis:
localizedDescription
setIsServerTimeout:
internalSettings
disableDeviceRacing
deferredTTSTimingInfo
setPacketCount:
setPacketDescriptions:
setAudioData:
enumerateAudioWithBlock:
domain
isEqualToString:
code
serverTTSConfig
deviceWaitTimeForAppId:
shouldDeferDeviceTTS
setDeferredTTSTimingInfo:
shouldSpeak
initWithAudioSessionID:asbd:
outputRoute
setAudioOutputRoute:
audioData
packetCount
packetDescriptions
speechStartReported
setSpeechStartReported:
numberWithLong:
voiceType
numberWithDouble:
experimentIdentifier
setExperimentIdentifier:
proceedWithSpeechCache:
eagerTaskHashForRequest:
setIsEagerCache:
proceedWithServerTTS
flushAndStop
tallySynthesisCore:
enqueueShortTermCacheWithHash:audio:timingInfo:completion:
writeAudioIfNeeded:
handleServerResponse:timingInfo:
broadcastTimeoutCondition
serverTTSRetryTimes
aceChannelRetryTimes
setServerTTSRetryTimes:
startSiriRoundTrip
aceStartSynthesisRequest:responseHandler:completion:
selectVoiceResourceAssetForLanguage:
defaultVoiceGender
setGender:
serverVoiceNameForGender:
setVoiceName:
setIsServerTTSRacing:
waitUntilFinished
waitUntilFinishedIfAudioReceivedWithin:
waitSiriRoundTripToFinish
speakRetryPhrase
fallbackToDeviceSynthesis
serverTTSTimeout
timeoutForAppId:
pause
isServerTimeout
isServerTTS
descriptiveKey
sourceOfTTS
isEagerCache
logUtterance
numberWithBool:
isSynthesisCached
numberWithInt:
stringOfSourceOfTTS:
dumpCompressedAudio:
writeToFilePath:
handleDeviceSynthesis:timingInfo:
synthesisCore:didReceiveAudio:
synthesisCore:didReceiveWordTimingInfo:
ospreyCore:didReceiveAudio:wordTimingInfo:
ospreyCore:didFinishWithError:
initWithRequest:shouldSpeak:
setShouldSpeak:
timeoutCondition
setTimeoutCondition:
setSynthesisCore:
setDisableOsprey:
setAceChannelRetryTimes:
racingMutex
setRacingMutex:
setInternalSettings:
setOspreyCore:
setServerTTSConfig:
_shouldSpeak
_useServerResponse
_useDeviceSynthesis
_speechStartReported
_isEagerCache
_disableOsprey
_wordTimingInfo
_synthesisCore
_aceChannelRetryTimes
_serverTTSRetryTimes
_serverAudio
_deferredTTSTimingInfo
_internalSettings
_ospreyCore
_serverTTSConfig
_timeoutCondition
_racingMutex
invalidate
sharedQueue
currentTask
availableLanguages
containsObject:
fallbackLanguageForLanguage:
setLanguageCode:
performLanguageFallBackIfNeededWithRequest:
vs_insertContextInfo:
vs_substituteAudioWithLocalPath
vs_textifyEmojiWithLanguage:
precomposedStringWithCanonicalMapping
whisper
stringByAppendingString:
useSSMLInput
vs_convertToSSML
defaultVolume
setVolume:
valueForEntitlement:
boolValue
setOutputPath:
sharedService
prewarmWithRequest:
hasPhaticResponses:
preprocessRequestBeforeSynthesis:
setCompletionBlock:
addTask:
defaultInstance
date
setLastTTSRequestDate:
disableServerTTS
popInlineStreamRequestForSpeakRequest:
identifier
initWithRequest:withStreamID:
shouldUseServerTTSForRequest:
addInlineStreamRequest:
audioDataFromPresynthesisRequest:
cancelCurrentTask
suspendCurrentTask
resumeCurrentTask
sharedServices
audioPowerUpdateQueue
initWithProvider:queue:frequency:delegate:
audioPowerUpdater
createNewXPCWrapperWithCompletion:
beginUpdate
endUpdate
setAudioPowerUpdater:
installedAssetsForType:voicename:language:gender:footprint:
name
remoteObjectProxy
speechRequestDidPause
speechRequestDidContinue
speechRequestMark:didStartForRange:
presynthesizedAudioRequestDidStart
presynthesizedAudioRequestSuccessWithInstrumentMetrics:error:
presynthesizedAudioRequestDidStopAtEnd:error:
cleanOldVoiceResources
cleanUnusedVoiceAssets
installedVoiceResources
autoDownloadedVoicesForClientID:
dictionaryRepresentation
isEqualToDictionary:
setAutoDownloadedVoices:withClientID:
initWithType:
cancelDownloadForAssets:
updateVoicesAndVoiceResources
cancelDownload:completion:
downloadOptionsWithBattery:
setAllowsCellularAccess:
downloadVoiceAsset:options:progressUpdateHandler:
languages
firstObject
arrayWithObjects:count:
setLanguages:
cancelAllResourceDownloads
queryVoices:reply:
streamId
enqueueStreamId:withObject:
updateWithConnectionIdentifier:
prewarmIfNeededWithRequest:reply:
queryPhaticCapabilityWithRequest:reply:
estimateDurationWithRequest:reply:
startSpeechRequest:reply:
startSynthesisRequest:
pauseSpeechRequestAtMark:
continueSpeechRequest
stopSpeechRequestAtMark:
startPresynthesizedAudioRequest:
cachePresynthesizedAudioRequest:
stopPresynthesizedAudioRequest
getVoiceNamesForLanguage:reply:
getFootprintsForVoiceName:languageCode:reply:
getSpeechIsActiveReply:
getSpeechIsActiveForConnectionReply:
beginAudioPowerUpdateWithReply:
endAudioPowerUpdate
cleanUnusedAssets:
getLocalVoicesForLanguage:reply:
getLocalVoiceResourcesReply:
setAutoDownloadedVoiceAssets:withClientID:
triggerCellularDownloadedVoiceAssets:withClientID:
getAutoDownloadedVoiceAssetsWithClientID:reply:
getVoiceResourceForLanguage:reply:
getVoiceInfoForLanguageCode:footprint:gender:type:reply:
cancelDownloads:
setLogToFile:
getLogToFile:
getTTSServerVoicesWithFilter:reply:
forwardStreamObject:
invokeDaemon:
killDaemon
initWithConnection:
connectionIdentifier
setConnectionIdentifier:
setAudioPowerUpdateQueue:
_connection
_connectionIdentifier
_audioPowerUpdateQueue
_audioPowerUpdater
serverConfig
didReceiveAudioCondition
lock
broadcast
unlock
timeout
initWithTimeoutValue:
serverTTSClient
isFinished
setDidReceiveAudio:
ospreyStartSynthesisRequest:responseHandler:completion:
wait
bufferDurationLimit
simulateNetworkStall
didReceiveAudio
count
objectAtIndexedSubscript:
dateByAddingTimeInterval:
timeIntervalSinceNow
setTimeoutValue:
timeoutValue
refresh
setVoice:
streamBufferDuration
setBufferDurationLimit:
ospreyStartStreamingRequest:dataHandler:metaInfoHandler:completion:
disableOspreyStreaming
performStreamingOspreyTTS
performRoundTripOspreyTTS
dateWithTimeIntervalSinceNow:
waitUntilDate:
setServerTTSClient:
setServerConfig:
setDidReceiveAudioCondition:
_didReceiveAudio
_voice
_serverTTSClient
_serverConfig
_bufferDurationLimit
_didReceiveAudioCondition
resetMobileAssetDefaults
migrateAssets
clearSynthesisCache
disableMobileAssetURLReset
defaultCacheStore
deleteCache
performPostInstallWithCompletion:
data
appendBytes:length:
appendData:
archivedDataWithRootObject:requiringSecureCoding:error:
getBytes:range:
subdataWithRange:
setWithObjects:
unarchivedObjectOfClasses:fromData:error:
serializedData
initWithKey:data:
initWithKey:audio:wordTimingInfo:
magicVersion
setKey:
_magicVersion
_audio
_key
_audioData
_packetCount
_packetDescriptions
bundleWithIdentifier:
bundlePath
stringByAppendingPathComponent:
defaultManager
createDirectoryAtPath:withIntermediateDirectories:attributes:error:
pathWithComponents:
initWithStorePath:
dataUsingEncoding:
dataWithCapacity:
setLength:
dirPath
writeToFile:options:error:
preinstalledAudioHashForLanguage:gender:
preinstalledCacheDir
stringByAppendingPathExtension:
isReadableFileAtPath:
audioDataFromFile:error:
dataWithContentsOfFile:
bytes
stringWithUTF8String:
preinstalledCacheForText:language:gender:
isHomePod
cleanDirectory:withLRULimit:
directorySize:
removeDirectory:
addCache:
cacheDataForKey:
isPreinstalledCacheAvailableForRequest:
cleanCache
totalCacheSize
setDirPath:
setPreinstalledCacheDir:
_dirPath
_preinstalledCacheDir
allocWithZone:
mutableAudioData
mutableDescription
setData:
copyWithZone:
totalFrames
setMutableAudioData:
setMutableDescription:
_mutableAudioData
_mutableDescription
initWithFlatbuffData:root:verify:
dictionary
fe_feature
fe_feature_only
addObjectToBuffer:
initWithBytesNoCopy:length:deallocator:
flatbuffData
initWithFlatbuffData:
initAndVerifyWithFlatbuffData:
initWithFlatbuffData:root:
_storage
_data
_root
initWithBytes:length:encoding:
language
UTF8String
version
quality
objectForKeyedSubscript:
setObject:forKeyedSubscript:
resource
channel_type
app_id
context_info
dialog_identifier
experiment_identifier
speech_id
session_id
audio_type
enable_word_timing_info
voice_name
preferred_voice_type
meta_info
context
experiment
feature_flags
value
sample_rate
format_id
format_flags
bytes_per_packet
frames_per_packet
bytes_per_frame
channels_per_frame
bits_per_channel
reserved
word
sample_idx
offset
timestamp
dataWithBytes:length:
error_code
error_str
audio:
decoder_description
playback_description
word_timing_info
stream_id
streaming_playback_buffer_size_in_seconds
current_pkt_number
total_pkt_number
content_type
contentAsOPTTSStartTextToSpeechStreamingRequest
contentAsOPTTSBeginTextToSpeechStreamingResponse
contentAsOPTTSPartialTextToSpeechStreamingResponse
contentAsOPTTSFinalTextToSpeechStreamingResponse
formatID
intValue
sampleRate
floatValue
bitsPerChannel
framesPerPacket
vsDescription
resourceVersion
contentVersion
audioBytesRange
setAudioBytesRange:
packetDescriptionsRange
setPacketDescriptionsRange:
_audioBytesRange
_packetDescriptionsRange
sharedInstance
opaqueSessionID
retrieveSessionWithID:
currentRoute
outputs
portType
defaultCenter
handleMediaServerReset
addObserver:selector:name:object:
removeObserver:
dequeueAvailableMappedAudio
removeObjectAtIndex:
bytesAtOffset:
_enqueueAudioBytesLength:audioBytes:packetCount:packetDescriptions:
internalBuild
base64EncodedDataWithOptions:
waitForAudioQueueStop
didEndAccessPower
removeAllObjects
signal
signalQueueRunningStateChange
isAudioQueueRunning
isAudioQueueStalled
timeIntervalSinceDate:
setDateFormat:
stringFromDate:
durationOfAudioDataLength:withAudioDescription:
bytesOfDuration:withAudioDescription:
willBeginAccessPower
getAveragePower:andPeakPower:
reset
audioQueue
setAudioQueue:
sessionID
setSessionID:
setOutputRoute:
setState:
waitForStateChangeMutex
setWaitForStateChangeMutex:
stateChangeCondition
setStateChangeCondition:
enqueuedSampleCount
setEnqueuedSampleCount:
audioStartTimeStamp
setAudioStartTimeStamp:
audioQueueStartDate
setAudioQueueStartDate:
audioQueueFutureEndDate
setAudioQueueFutureEndDate:
mappedData
setMappedData:
enqueuedMappedAudioInfo
setEnqueuedMappedAudioInfo:
playingBufferCount
setPlayingBufferCount:
dequeueCondition
setDequeueCondition:
_discontinuedDuringPlayback
_sessionID
_audioQueue
_outputRoute
_state
_enqueuedSampleCount
_audioQueueStartDate
_audioQueueFutureEndDate
_mappedData
_enqueuedMappedAudioInfo
_playingBufferCount
_dequeueCondition
_stateChangeCondition
_waitForStateChangeMutex
_audioStartTimeStamp
voiceSelectionWithRequest:error:
getCacheForHash:
reportWordTimingInfo:
reportAudio:
adjustWordTimingInfo:
enqueueCacheWithHash:streamAudio:timingInfo:completion:
isExecuting
inMemoryCacheForHash:
onDiskCacheForHash:
setIsCacheHitFromDisk:
voiceSelection_noRetry_WithRequest:error:
resetCache
disableCompactVoiceFallback
fileExistsAtPath:
cachedEngineForVoice:resources:
loadEngineForVoice:resources:
initWithASBD:
setPitch:
setRate:
gainDecibelWithVolume:
vs_markerStringForContext:
textRange
setTextRange:
setSelectedVoice:
setSelectedVoiceResource:
_selectedVoice
_selectedVoiceResource
disableAssetCleaning
longLongValue
activeVoiceAssets
inactiveVoiceAssets
totalSizeOfAssets:
size
purgeableVoiceAssetsWithInfo:urgency:
totalDiagnosticFileSize
numberWithLongLong:
purgeImpl:urgency:
purgeAsset:
removeDirectory
purgeable:urgency:
purge:urgency:
periodic:urgency:
registerCacheDelete
refreshTimeoutCondition
_waitForTimeInterval:
shouldStop
setRefreshTimeoutCondition:
setShouldStop:
_shouldStop
_timeoutValue
_refreshTimeoutCondition
inProgressDownloadVoiceKeys
removeObject:
preferenceInterface
orderedSet
mobileAssetManager
amendVoiceWithDefaultSettings:
updateVoiceIfNeeded:
updateVoiceResourcesWithPriorityForLanguages:
isWatch
lastTTSRequestDate
shouldDelayVoiceUpdate
addInProgressDownloadVoiceKey:
removeInProgressDownloadVoiceKey:
inProgressDownloadResourceKeys
orderedSetWithOrderedSet:
insertObject:atIndex:
minusOrderedSet:
downloadQueue
downloadNextInProgressDownloadResource
downloadVoiceResourceCatalogWithCompletion:
downloadVoiceResource:options:completion:
cancelResourceDownload:completion:
setMobileAssetManager:
setPreferenceInterface:
_type
_mobileAssetManager
_preferenceInterface
ospreyEndpointURL
isSeedBuild
isInternalBuild
ospreyServiceEndpointURL
roundTripTTS:responseHandler:
streamTTS:beginHandler:chunkHandler:endHandler:completion:
setConfigurationOptionUseCompression:
setConfigurationOptionUseAbsinthe:
deviceID
setDeviceID:
_deviceID
enableAudioDump
initWithDirectory:
audioDumpPath
cleanDirectory:withDateOlderThan:
fileExistsAtPath:isDirectory:
createDirectoryIfNeeded
audioDumpFileAttributes
createFileAtPath:contents:attributes:
moveItemAtPath:toPath:error:
removeOldFiles
setAudioDumpPath:
setAudioDumpFileAttributes:
_audioDumpPath
_audioDumpFileAttributes
userDefaultsKnowledgeStore
setKnowledgeStore:
knowledgeStore
valueForKey:
JSONObjectWithData:options:error:
dictForKey:
configForAppId:key:
componentsSeparatedByString:
whitelistedAppId
_knowledgeStore
cacheTimer
timeToLiveTimerFired:
timerWithTimeInterval:target:selector:userInfo:repeats:
cache
setObject:forKey:
mainRunLoop
addTimer:forMode:
userInfo
removeObjectForKey:
setCache:
setCacheTimer:
_cache
_cacheTimer
setIsServerStreamTTS:
streamID
removeStreamId:
object
handleBegin:
handleChunk:
handleEnd:
speechSynthesisVoice
speechSynthesisResource
decoderStreamDescription
streamingPlaybackBufferSize
asbdFromDescription:
setPlaybackServices:
playbackServices
signalNewDataWithError:
populateWithPCMData:
populateWithOpusData:
startPlayback
setPlaybackBeginDate:
playbackBeginDate
numberWithInteger:
errorMessage
handleStreamNotification:
startStreamingWithId:
waitForNewData:
retryDeviceOnNetworkStall
deviceTTSCore
finalTimingInfo
setStreamID:
setDeviceTTSCore:
setFinalTimingInfo:
_streamID
_deviceTTSCore
_playbackServices
_finalTimingInfo
_playbackBeginDate
getCurrentAudioPowerProvider
previousProvider
setPreviousProvider:
_previousProvider
streamRequestQueue
ongoingNotifications
notifyQueue
postNotificationName:object:
queuedNotification
setQueuedNotification:
setOngoingNotifications:
setStreamRequestQueue:
setLock:
recursiveLockAttr
setRecursiveLockAttr:
setNotifyQueue:
_queuedNotification
_ongoingNotifications
_streamRequestQueue
_notifyQueue
_recursiveLockAttr
_lock
initWithBool:
setFe_feature:
setFe_feature_only:
setLanguage:
setName:
setVersion:
setQuality:
setResource:
integerValue
initWithInteger:
setChannel_type:
setApp_id:
setContext_info:
setDialog_identifier:
setExperiment_identifier:
setSpeech_id:
setSession_id:
setAudio_type:
setEnable_word_timing_info:
setVoice_name:
setPreferred_voice_type:
setMeta_info:
setContext:
setExperiment:
setFeature_flags:
setValue:
doubleValue
initWithDouble:
unsignedIntegerValue
initWithUnsignedInteger:
setSample_rate:
setFormat_id:
setFormat_flags:
setBytes_per_packet:
setFrames_per_packet:
setBytes_per_frame:
setChannels_per_frame:
setBits_per_channel:
setReserved:
initWithFloat:
setWord:
setSample_idx:
setOffset:
setTimestamp:
initWithInt:
setError_code:
setError_str:
setAudio:
setDecoder_description:
setPlayback_description:
setWord_timing_info:
setStream_id:
setStreaming_playback_buffer_size_in_seconds:
setCurrent_pkt_number:
setTotal_pkt_number:
setContent_type:
setContentAsOPTTSStartTextToSpeechStreamingRequest:
setContentAsOPTTSBeginTextToSpeechStreamingResponse:
setContentAsOPTTSPartialTextToSpeechStreamingResponse:
setContentAsOPTTSFinalTextToSpeechStreamingResponse:
handleVoiceSelectionPurge:
cachedEngine
unloadEngine
prewarmQueue
_cachedEngineForVoice:resources:
_engineForVoice:resources:
preheat
resourceMimeTypes
searchPathURL
_loadVoiceResources:forEngine:
fileURLWithPathComponents:
fileURLWithPath:
setSearchPathURL:
resourceList
loadResourceAtPath:mimeType:error:
setCachedEngine:
setLoadedResources:
waitUntilPrewarmFinish
loadedResources
setPrewarmQueue:
_cachedEngine
_loadedResources
_prewarmQueue
initWithLoggingPrefix:
recordCategory:value:
masteredVersion
footprintStringFromFootprint:
loggingPrefix
setLoggingPrefix:
_loggingPrefix
forceServerTTS
shortTermCacheForHash:
genderStringFromVSGender:
setFilteredVoiceKey:
voiceKeyList
numberWithUnsignedInteger:
keyString
genderFromString:
setContentVersion:
footprintFromString:
setFootprint:
handleCommand:afterCurrentRequest:commandHandler:completion:
requestFromVSRequest:
audioStreamBasicDescription
audioDataWithASBD:rawData:
vs_wordTimingInfos:withText:
vs_voice
vs_voiceResource
setAudioType:
setStreaming:
setEnableAudioInfo:
voiceName
reason
reasonDescription
aceAudioData
audioDataFromSAUIAudioData:
aceAudioInfo
wordTimingInfoList
sampleIndex
setStartTime:
utf16TimingInfoWithUTF8Range:withText:
totalPacketNumber
_fetchVoiceAsset_NoRetry
eagerRequestCreatedTimeStampDiffs
enqueueCacheWithHash:audio:timingInfo:completion:
initWithCache:shortTermMemory:
initWithSourceASBD:
setErrorHandler:
setOpusDataHandler:
beginEncoding
encodeChunk:
endEncoding
_enqueueCacheWithHash:audioObject:timingInfo:completion:
disableCache
compressStreamAudio:
compressAudio:
threadLock
inMemoryCaches
cachingQueue
cacheStore
_inMemoryCacheForHash:
_onDiskCacheForHash:
shortTermCache
setThreadLock:
setInMemoryCaches:
setCacheStore:
setShortTermCache:
setCachingQueue:
_threadLock
_inMemoryCaches
_cacheStore
_shortTermCache
_cachingQueue
.cxx_construct
_resource
processMarkerBuffer
dataWithBytesNoCopy:length:freeWhenDone:
characterAtIndex:
utf8BytesForChar:
utf16OffsetFromUTF8:
initWithCallback:
synthesisCallback:
pcmData
sampleBuffer
markerBuffer
stopMark
setStopMark:
callback
setCallback:
wordTimings
setWordTimings:
samplesProcessed
setSamplesProcessed:
lastUTF8Offset
setLastUTF8Offset:
lastUTF16Offset
setLastUTF16Offset:
setNumOfPromptsTriggered:
setNeuralDidFallback:
setHasAlignmentStall:
_samples
_markers
_neuralDidFallback
_hasAlignmentStall
_text
_stopMark
_callback
_wordTimings
_samplesProcessed
_lastUTF8Offset
_lastUTF16Offset
_numOfPromptsTriggered
initializeWithResourcePath:
contentsOfDirectoryAtPath:error:
pathExtension
mimeForFileExtension:
currentCallbackResult
synthesizer
isUserCancelError:
setVoicePath:
setSynthesizer:
setCurrentCallbackResult:
_rate
_pitch
_volume
_voicePath
_synthesizer
_currentCallbackResult
beginChunkDecoderForStreamDescription:
decodeChunk:outError:
endChunkDecoding
mappedAudioInfo
setMappedAudioInfo:
_mappedAudioInfo
spinNextTask
isSimilarTo:
setCurrentTask:
setLastSynthesisRequest:
enqueue
_cancelTask:
eagerTasks
setEagerTasks:
speakTasks
setSpeakTasks:
threadMutex
setThreadMutex:
threadMutexAttr
setThreadMutexAttr:
speakingQueue
setSpeakingQueue:
lastSynthesisRequest
lastSynthesisRequestCreatedTimeStamp
setLastSynthesisRequestCreatedTimeStamp:
_eagerTasks
_speakTasks
_currentTask
_speakingQueue
_lastSynthesisRequest
_lastSynthesisRequestCreatedTimeStamp
_threadMutexAttr
_threadMutex
formatFlags
unsignedIntValue
bytesPerPacket
bytesPerFrame
channelsPerFrame
audioBuffer
playerStreamDescription
populatePCMDataWithSiriOpusSData:withOpusASBD:
decodeChunks:streamDescription:outError:
pcmAudioDataFromOpusAudio:
stringByReplacingOccurrencesOfString:withString:
mmap
initWithFilePath:
madvise
filePath
fileSize
_filePath
_fileSize
B24@0:8@16
#16@0:8
@16@0:8
@24@0:8:16
@32@0:8:16@24
@40@0:8:16@24@32
B16@0:8
B24@0:8#16
B24@0:8:16
Vv16@0:8
Q16@0:8
^{_NSZone=}16@0:8
B24@0:8@"Protocol"16
@"NSString"16@0:8
v16@0:8
@"VSInstrumentMetrics"16@0:8
@"VSSpeechRequest"16@0:8
@"<AFAudioPowerProviding>"16@0:8
v24@0:8@16
v24@0:8@"<VSSpeechSpeakableProtocol>"16
@24@0:8@16
v20@0:8B16
@"VSSpeechSpeakTask"
@64@0:8{AudioStreamBasicDescription=dIIIIIIII}16Q56
v20@0:8f16
f16@0:8
{AudioStreamBasicDescription=dIIIIIIII}16@0:8
v56@0:8{AudioStreamBasicDescription=dIIIIIIII}16
v24@0:8Q16
^{OpaqueAudioConverter=}16@0:8
v24@0:8^{OpaqueAudioConverter=}16
^{OpaqueAudioComponentInstance=}16@0:8
v24@0:8^{OpaqueAudioComponentInstance=}16
{AudioTimeStamp=dQdQ{SMPTETime=ssIIIssss}II}16@0:8
v80@0:8{AudioTimeStamp=dQdQ{SMPTETime=ssIIIssss}II}16
^{OpaqueAudioConverter=}
^{OpaqueAudioComponentInstance=}
{AudioStreamBasicDescription="mSampleRate"d"mFormatID"I"mFormatFlags"I"mBytesPerPacket"I"mFramesPerPacket"I"mBytesPerFrame"I"mChannelsPerFrame"I"mBitsPerChannel"I"mReserved"I}
{AudioTimeStamp="mSampleTime"d"mHostTime"Q"mRateScalar"d"mWordClockTime"Q"mSMPTETime"{SMPTETime="mSubframes"s"mSubframeDivisor"s"mCounter"I"mType"I"mFlags"I"mHours"s"mMinutes"s"mSeconds"s"mFrames"s}"mFlags"I"mReserved"I}
@32@0:8@16@24
d16@0:8
@"VSSpeechRequest"
@"NSError"
@"VSInstrumentMetrics"
@"VSDeviceTTSCore"
@"NSArray"
@"<VSSpeechServiceDelegate>"
@"VSSpeechEngine"
@"VSVoiceBooster"
@"VSAudioPlaybackService"
@"VSVoiceAssetSelection"
@"VSVoiceResourceAsset"
@"VSCachingService"
@"<VSSpeechCacheItem>"
@"VSAudioData"
@"VSStreamAudioData"
@"NSObject<OS_dispatch_queue>"
v32@0:8@16@24
v32@0:8@"VSDeviceTTSCore"16@"VSAudioData"24
v32@0:8@"VSDeviceTTSCore"16@"NSArray"24
v40@0:8@16@24@32
v40@0:8@"VSOspreyTTSCore"16@"VSAudioData"24@"NSArray"32
v32@0:8@"VSOspreyTTSCore"16@"NSError"24
@28@0:8@16B24
@24@0:8q16
{_opaque_pthread_cond_t=q[40c]}16@0:8
v64@0:8{_opaque_pthread_cond_t=q[40c]}16
q16@0:8
v24@0:8q16
{_opaque_pthread_mutex_t=q[56c]}16@0:8
v80@0:8{_opaque_pthread_mutex_t=q[56c]}16
@"VSSpeechServerTask"
@"VSSpeechInternalSettings"
@"VSOspreyTTSCore"
@"VSSiriServerConfiguration"
{_opaque_pthread_cond_t="__sig"q"__opaque"[40c]}
{_opaque_pthread_mutex_t="__sig"q"__opaque"[56c]}
Vv24@0:8@16
Vv32@0:8@16@?24
Vv24@0:8q16
Vv40@0:8@16@24@?32
Vv24@0:8@?16
Vv32@0:8@16@24
Vv56@0:8@16q24q32q40@?48
Vv20@0:8B16
Vv24@0:8@"NSString"16
Vv32@0:8@"VSSpeechRequest"16@?<v@?@"NSError">24
Vv32@0:8@"VSSpeechRequest"16@?<v@?B>24
Vv32@0:8@"VSSpeechRequest"16@?<v@?d@"NSError">24
Vv32@0:8@"VSSpeechRequest"16@?<v@?>24
Vv24@0:8@"VSSpeechRequest"16
Vv24@0:8@"VSPresynthesizedAudioRequest"16
Vv32@0:8@"NSString"16@?<v@?@"NSArray">24
Vv40@0:8@"NSString"16@"NSString"24@?<v@?@"NSArray">32
Vv24@0:8@?<v@?B>16
Vv24@0:8@?<v@?@"AFXPCWrapper">16
Vv24@0:8@?<v@?@"NSError">16
Vv32@0:8@"NSString"16@?<v@?@"NSArray"@"NSError">24
Vv24@0:8@?<v@?@"NSArray"@"NSError">16
Vv32@0:8@"NSArray"16@"NSString"24
Vv32@0:8@"NSString"16@?<v@?@"VSVoiceResourceAsset">24
Vv56@0:8@"NSString"16q24q32q40@?<v@?@"VSVoiceAsset">48
Vv24@0:8@?<v@?>16
Vv32@0:8@"VSVoiceAsset"16@?<v@?@"NSArray"@"NSError">24
Vv24@0:8@"SATTSSpeechSynthesisStreaming"16
Vv40@0:8q16{_NSRange=QQ}24
Vv36@0:8B16@20@28
Vv40@0:8@16@24@32
Vv28@0:8B16@20
Vv36@0:8B16@"NSString"20@"NSError"28
Vv24@0:8@"VSInstrumentMetrics"16
Vv24@0:8@"NSArray"16
Vv32@0:8@"VSSpeechRequest"16@"NSArray"24
Vv40@0:8@"VSSpeechRequest"16@"VSInstrumentMetrics"24@"NSError"32
Vv28@0:8B16@"NSError"20
Vv32@0:8@"VSInstrumentMetrics"16@"NSError"24
@"NSXPCConnection"
@"NSString"
@"AFAudioPowerUpdater"
v24@0:8d16
@"<VSOspreyTTSCoreDelegate>"
@"VSVoiceAsset"
@"VSServerTTSClient"
@"VSTimeoutCondition"
@"NSCondition"
v24@0:8@?16
@"NSData"16@0:8
@32@0:8@"NSString"16@"NSData"24
@40@0:8@16@24@32
@"NSData"
@40@0:8@16@24q32
@24@0:8^{_NSZone=}16
I16@0:8
@"NSMutableData"
@32@0:8@16r^{TTSRequestFeatureFlags=[1C]}24
@36@0:8@16r^{TTSRequestFeatureFlags=[1C]}24B32
{Offset<siri::speech::schema_fb::TTSRequestFeatureFlags>=I}24@0:8^{FlatBufferBuilder={vector_downward=^{Allocator}BQQQ***}ISBBQBB^{set<flatbuffers::Offset<flatbuffers::String>, flatbuffers::FlatBufferBuilder::StringOffsetCompare, std::__1::allocator<flatbuffers::Offset<flatbuffers::String> > >}}16
@"NSMutableDictionary"
r^{TTSRequestFeatureFlags=[1C]}
@32@0:8@16r^{TextToSpeechVoice=[1C]}24
@36@0:8@16r^{TextToSpeechVoice=[1C]}24B32
{Offset<siri::speech::schema_fb::TextToSpeechVoice>=I}24@0:8^{FlatBufferBuilder={vector_downward=^{Allocator}BQQQ***}ISBBQBB^{set<flatbuffers::Offset<flatbuffers::String>, flatbuffers::FlatBufferBuilder::StringOffsetCompare, std::__1::allocator<flatbuffers::Offset<flatbuffers::String> > >}}16
r^{TextToSpeechVoice=[1C]}
@32@0:8@16r^{TextToSpeechResource=[1C]}24
@36@0:8@16r^{TextToSpeechResource=[1C]}24B32
{Offset<siri::speech::schema_fb::TextToSpeechResource>=I}24@0:8^{FlatBufferBuilder={vector_downward=^{Allocator}BQQQ***}ISBBQBB^{set<flatbuffers::Offset<flatbuffers::String>, flatbuffers::FlatBufferBuilder::StringOffsetCompare, std::__1::allocator<flatbuffers::Offset<flatbuffers::String> > >}}16
r^{TextToSpeechResource=[1C]}
@32@0:8@16r^{TextToSpeechMeta=[1C]}24
@36@0:8@16r^{TextToSpeechMeta=[1C]}24B32
{Offset<siri::speech::schema_fb::TextToSpeechMeta>=I}24@0:8^{FlatBufferBuilder={vector_downward=^{Allocator}BQQQ***}ISBBQBB^{set<flatbuffers::Offset<flatbuffers::String>, flatbuffers::FlatBufferBuilder::StringOffsetCompare, std::__1::allocator<flatbuffers::Offset<flatbuffers::String> > >}}16
r^{TextToSpeechMeta=[1C]}
@32@0:8@16r^{TextToSpeechRequestMeta=[1C]}24
@36@0:8@16r^{TextToSpeechRequestMeta=[1C]}24B32
{Offset<siri::speech::schema_fb::TextToSpeechRequestMeta>=I}24@0:8^{FlatBufferBuilder={vector_downward=^{Allocator}BQQQ***}ISBBQBB^{set<flatbuffers::Offset<flatbuffers::String>, flatbuffers::FlatBufferBuilder::StringOffsetCompare, std::__1::allocator<flatbuffers::Offset<flatbuffers::String> > >}}16
r^{TextToSpeechRequestMeta=[1C]}
@32@0:8@16r^{TextToSpeechRequestContext=[1C]}24
@36@0:8@16r^{TextToSpeechRequestContext=[1C]}24B32
{Offset<siri::speech::schema_fb::TextToSpeechRequestContext>=I}24@0:8^{FlatBufferBuilder={vector_downward=^{Allocator}BQQQ***}ISBBQBB^{set<flatbuffers::Offset<flatbuffers::String>, flatbuffers::FlatBufferBuilder::StringOffsetCompare, std::__1::allocator<flatbuffers::Offset<flatbuffers::String> > >}}16
r^{TextToSpeechRequestContext=[1C]}
@32@0:8@16r^{TextToSpeechRequestExperiment=[1C]}24
@36@0:8@16r^{TextToSpeechRequestExperiment=[1C]}24B32
{Offset<siri::speech::schema_fb::TextToSpeechRequestExperiment>=I}24@0:8^{FlatBufferBuilder={vector_downward=^{Allocator}BQQQ***}ISBBQBB^{set<flatbuffers::Offset<flatbuffers::String>, flatbuffers::FlatBufferBuilder::StringOffsetCompare, std::__1::allocator<flatbuffers::Offset<flatbuffers::String> > >}}16
r^{TextToSpeechRequestExperiment=[1C]}
@32@0:8@16r^{TextToSpeechRequest=[1C]}24
@36@0:8@16r^{TextToSpeechRequest=[1C]}24B32
{Offset<siri::speech::schema_fb::TextToSpeechRequest>=I}24@0:8^{FlatBufferBuilder={vector_downward=^{Allocator}BQQQ***}ISBBQBB^{set<flatbuffers::Offset<flatbuffers::String>, flatbuffers::FlatBufferBuilder::StringOffsetCompare, std::__1::allocator<flatbuffers::Offset<flatbuffers::String> > >}}16
r^{TextToSpeechRequest=[1C]}
@32@0:8@16r^{ContextInfoEntry=[1C]}24
@36@0:8@16r^{ContextInfoEntry=[1C]}24B32
{Offset<siri::speech::schema_fb::TextToSpeechRequest_::ContextInfoEntry>=I}24@0:8^{FlatBufferBuilder={vector_downward=^{Allocator}BQQQ***}ISBBQBB^{set<flatbuffers::Offset<flatbuffers::String>, flatbuffers::FlatBufferBuilder::StringOffsetCompare, std::__1::allocator<flatbuffers::Offset<flatbuffers::String> > >}}16
r^{ContextInfoEntry=[1C]}
@32@0:8@16r^{AudioDescription=[1C]}24
@36@0:8@16r^{AudioDescription=[1C]}24B32
{Offset<siri::speech::schema_fb::AudioDescription>=I}24@0:8^{FlatBufferBuilder={vector_downward=^{Allocator}BQQQ***}ISBBQBB^{set<flatbuffers::Offset<flatbuffers::String>, flatbuffers::FlatBufferBuilder::StringOffsetCompare, std::__1::allocator<flatbuffers::Offset<flatbuffers::String> > >}}16
r^{AudioDescription=[1C]}
@32@0:8@16r^{WordTimingInfo=[1C]}24
@36@0:8@16r^{WordTimingInfo=[1C]}24B32
{Offset<siri::speech::schema_fb::WordTimingInfo>=I}24@0:8^{FlatBufferBuilder={vector_downward=^{Allocator}BQQQ***}ISBBQBB^{set<flatbuffers::Offset<flatbuffers::String>, flatbuffers::FlatBufferBuilder::StringOffsetCompare, std::__1::allocator<flatbuffers::Offset<flatbuffers::String> > >}}16
r^{WordTimingInfo=[1C]}
@32@0:8@16r^{TextToSpeechResponse=[1C]}24
@36@0:8@16r^{TextToSpeechResponse=[1C]}24B32
i16@0:8
{Offset<siri::speech::schema_fb::TextToSpeechResponse>=I}24@0:8^{FlatBufferBuilder={vector_downward=^{Allocator}BQQQ***}ISBBQBB^{set<flatbuffers::Offset<flatbuffers::String>, flatbuffers::FlatBufferBuilder::StringOffsetCompare, std::__1::allocator<flatbuffers::Offset<flatbuffers::String> > >}}16
r^{TextToSpeechResponse=[1C]}
@32@0:8@16r^{StartTextToSpeechStreamingRequest=[1C]}24
@36@0:8@16r^{StartTextToSpeechStreamingRequest=[1C]}24B32
{Offset<siri::speech::schema_fb::StartTextToSpeechStreamingRequest>=I}24@0:8^{FlatBufferBuilder={vector_downward=^{Allocator}BQQQ***}ISBBQBB^{set<flatbuffers::Offset<flatbuffers::String>, flatbuffers::FlatBufferBuilder::StringOffsetCompare, std::__1::allocator<flatbuffers::Offset<flatbuffers::String> > >}}16
r^{StartTextToSpeechStreamingRequest=[1C]}
{Offset<siri::speech::schema_fb::StartTextToSpeechStreamingRequest_::ContextInfoEntry>=I}24@0:8^{FlatBufferBuilder={vector_downward=^{Allocator}BQQQ***}ISBBQBB^{set<flatbuffers::Offset<flatbuffers::String>, flatbuffers::FlatBufferBuilder::StringOffsetCompare, std::__1::allocator<flatbuffers::Offset<flatbuffers::String> > >}}16
@32@0:8@16r^{BeginTextToSpeechStreamingResponse=[1C]}24
@36@0:8@16r^{BeginTextToSpeechStreamingResponse=[1C]}24B32
{Offset<siri::speech::schema_fb::BeginTextToSpeechStreamingResponse>=I}24@0:8^{FlatBufferBuilder={vector_downward=^{Allocator}BQQQ***}ISBBQBB^{set<flatbuffers::Offset<flatbuffers::String>, flatbuffers::FlatBufferBuilder::StringOffsetCompare, std::__1::allocator<flatbuffers::Offset<flatbuffers::String> > >}}16
r^{BeginTextToSpeechStreamingResponse=[1C]}
@32@0:8@16r^{PartialTextToSpeechStreamingResponse=[1C]}24
@36@0:8@16r^{PartialTextToSpeechStreamingResponse=[1C]}24B32
{Offset<siri::speech::schema_fb::PartialTextToSpeechStreamingResponse>=I}24@0:8^{FlatBufferBuilder={vector_downward=^{Allocator}BQQQ***}ISBBQBB^{set<flatbuffers::Offset<flatbuffers::String>, flatbuffers::FlatBufferBuilder::StringOffsetCompare, std::__1::allocator<flatbuffers::Offset<flatbuffers::String> > >}}16
r^{PartialTextToSpeechStreamingResponse=[1C]}
@32@0:8@16r^{FinalTextToSpeechStreamingResponse=[1C]}24
@36@0:8@16r^{FinalTextToSpeechStreamingResponse=[1C]}24B32
{Offset<siri::speech::schema_fb::FinalTextToSpeechStreamingResponse>=I}24@0:8^{FlatBufferBuilder={vector_downward=^{Allocator}BQQQ***}ISBBQBB^{set<flatbuffers::Offset<flatbuffers::String>, flatbuffers::FlatBufferBuilder::StringOffsetCompare, std::__1::allocator<flatbuffers::Offset<flatbuffers::String> > >}}16
r^{FinalTextToSpeechStreamingResponse=[1C]}
@32@0:8@16r^{TextToSpeechRouterStreamingStreamingRequest=[1C]}24
@36@0:8@16r^{TextToSpeechRouterStreamingStreamingRequest=[1C]}24B32
{Offset<siri::speech::qss_fb::TextToSpeechRouterStreamingStreamingRequest>=I}24@0:8^{FlatBufferBuilder={vector_downward=^{Allocator}BQQQ***}ISBBQBB^{set<flatbuffers::Offset<flatbuffers::String>, flatbuffers::FlatBufferBuilder::StringOffsetCompare, std::__1::allocator<flatbuffers::Offset<flatbuffers::String> > >}}16
r^{TextToSpeechRouterStreamingStreamingRequest=[1C]}
@32@0:8@16r^{TextToSpeechRouterStreamingStreamingResponse=[1C]}24
@36@0:8@16r^{TextToSpeechRouterStreamingStreamingResponse=[1C]}24B32
{Offset<siri::speech::qss_fb::TextToSpeechRouterStreamingStreamingResponse>=I}24@0:8^{FlatBufferBuilder={vector_downward=^{Allocator}BQQQ***}ISBBQBB^{set<flatbuffers::Offset<flatbuffers::String>, flatbuffers::FlatBufferBuilder::StringOffsetCompare, std::__1::allocator<flatbuffers::Offset<flatbuffers::String> > >}}16
r^{TextToSpeechRouterStreamingStreamingResponse=[1C]}
{_NSRange=QQ}16@0:8
v32@0:8{_NSRange=QQ}16
{_NSRange="location"Q"length"Q}
d64@0:8Q16{AudioStreamBasicDescription=dIIIIIIII}24
Q64@0:8d16{AudioStreamBasicDescription=dIIIIIIII}24
B32@0:8^f16^f24
@60@0:8I16{AudioStreamBasicDescription=dIIIIIIII}20
v40@0:8@16q24@32
@48@0:8q16r^v24q32r^v40
^{OpaqueAudioQueue=}16@0:8
v24@0:8^{OpaqueAudioQueue=}16
v20@0:8I16
^{OpaqueAudioQueue=}
@"NSDate"
@"VSMappedData"
@"NSMutableArray"
@32@0:8@16^@24
@"<VSDeviceTTSCoreDelegate>"
@28@0:8@16i24
q24@0:8@16
@24@0:8d16
B24@0:8d16
@24@0:8Q16
@"VSMobileAssetsManager"
@"VSPreferencesInterface"
v32@0:8@16@?24
v56@0:8@16@?24@?32@?40@?48
f24@0:8d16
@"NSDictionary"
d24@0:8@16
@"CKKnowledgeStore"
v40@0:8@16@24d32
@"NSCache"
@"VSPresynthesizedAudioRequest"
@"<AFAudioPowerProviding>"
{_opaque_pthread_mutexattr_t=q[8c]}16@0:8
v32@0:8{_opaque_pthread_mutexattr_t=q[8c]}16
@"NSMutableSet"
{_opaque_pthread_mutexattr_t="__sig"q"__opaque"[8c]}
v20@0:8i16
v40@0:8@16@?24@?32
v48@0:8@16@?24@?32@?40
v48@0:8@16@24@32@?40
@"NSLock"
@"VSSpeechCache"
@"VSShortTermCache"
{shared_ptr<SiriTTS::VoiceResource>=^{VoiceResource}^{__shared_weak_count}}16@0:8
v32@0:8{shared_ptr<SiriTTS::VoiceResource>=^{VoiceResource}^{__shared_weak_count}}16
{shared_ptr<SiriTTS::VoiceResource>="__ptr_"^{VoiceResource}"__cntrl_"^{__shared_weak_count}}
@24@0:8@?16
i20@0:8i16
^{vector<unsigned char, std::__1::allocator<unsigned char> >=**{__compressed_pair<unsigned char *, std::__1::allocator<unsigned char> >=*}}16@0:8
^{vector<TTSSynthesizer::Marker, std::__1::allocator<TTSSynthesizer::Marker> >=^{Marker}^{Marker}{__compressed_pair<TTSSynthesizer::Marker *, std::__1::allocator<TTSSynthesizer::Marker> >=^{Marker}}}16@0:8
Q20@0:8S16
Q24@0:8Q16
@?16@0:8
{vector<unsigned char, std::__1::allocator<unsigned char> >="__begin_"*"__end_"*"__end_cap_"{__compressed_pair<unsigned char *, std::__1::allocator<unsigned char> >="__value_"*}}
{vector<TTSSynthesizer::Marker, std::__1::allocator<TTSSynthesizer::Marker> >="__begin_"^{Marker}"__end_"^{Marker}"__end_cap_"{__compressed_pair<TTSSynthesizer::Marker *, std::__1::allocator<TTSSynthesizer::Marker> >="__value_"^{Marker}}}
@40@0:8@16@24^@32
@36@0:8@16B24@?28
^{TTSSynthesizer={unique_ptr<TTSSynthesizer::TTSSynthesizerInternal, std::__1::default_delete<TTSSynthesizer::TTSSynthesizerInternal> >={__compressed_pair<TTSSynthesizer::TTSSynthesizerInternal *, std::__1::default_delete<TTSSynthesizer::TTSSynthesizerInternal> >=^{TTSSynthesizerInternal}}}}16@0:8
v24@0:8^{TTSSynthesizer={unique_ptr<TTSSynthesizer::TTSSynthesizerInternal, std::__1::default_delete<TTSSynthesizer::TTSSynthesizerInternal> >={__compressed_pair<TTSSynthesizer::TTSSynthesizerInternal *, std::__1::default_delete<TTSSynthesizer::TTSSynthesizerInternal> >=^{TTSSynthesizerInternal}}}}16
^{TTSSynthesizer={unique_ptr<TTSSynthesizer::TTSSynthesizerInternal, std::__1::default_delete<TTSSynthesizer::TTSSynthesizerInternal> >={__compressed_pair<TTSSynthesizer::TTSSynthesizerInternal *, std::__1::default_delete<TTSSynthesizer::TTSSynthesizerInternal> >=^{TTSSynthesizerInternal}}}}
@"VSSpeechSynthesisCallbackResult"
@56@0:8{AudioStreamBasicDescription=dIIIIIIII}16
v40@0:8@16Q24@32
@"NSOperation<VSSpeechTaskProtocol>"
B64@0:8@16{AudioStreamBasicDescription=dIIIIIIII}24
@64@0:8{AudioStreamBasicDescription=dIIIIIIII}16@56
{AudioStreamBasicDescription=dIIIIIIII}24@0:8@16
^v16@0:8
