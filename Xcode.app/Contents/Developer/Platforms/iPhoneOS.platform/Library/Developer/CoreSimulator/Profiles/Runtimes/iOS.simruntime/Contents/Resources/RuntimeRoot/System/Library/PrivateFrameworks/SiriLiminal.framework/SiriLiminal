@(#)PROGRAM:SiriLiminal  PROJECT:CoreSpeech-
minibatch_token_type_ids
minibatch_attention_mask
minibatch_input_ids
minibatch_padding_mask
input_origin
v32@?0@"NSString"8@16^B24
-[SLODLDClassifierV1 processEncodedTokens:]
-[SLODLDClassifierV1 _constructFeatureDictionary:]
v32@?0@"NSNumber"8Q16^B24
-[SLODLDClassifierV1 _constructFeatureDictionary:]_block_invoke
v32@?0@"NSString"8@"MLFeatureDescription"16^B24
com.apple.SiriLiminal
Framework
v8@?0
::: Initializing SiriLiminal logging...
en_US_POSIX
yyyyMMdd-HHmmss
SLLogInitIfNeeded_block_invoke
gitrelno_unavailable
rules
patternMatch
replaceWith
Missing config for regex matcher %@
reason
-[SLRegexMatcher initWithConfigFile:]
-[SLRegexMatcher _compileRegexRulesForPatterns:]_block_invoke
v32@?0@"NSDictionary"8Q16^B24
v32@?0@"NSRegularExpression"8Q16^B24
-[SLRegexMatcher applyPreprocessingOnUtt:]
+[SLASRFeatureExtractor extractASRFaturesFrom:]
max:
min:
stddev:
average:
v32@?0@"AFSpeechUtterance"8Q16^B24
+[SLASRFeatureExtractor extractLRNNFaturesFrom:]
+[SLASRFeatureExtractor _getTokenConfidenceForPath:fromPhrases:]
v32@?0@"AFSpeechPhrase"8Q16^B24
+[SLASRFeatureExtractor _getTokenConfidenceForPath:fromPhrases:]_block_invoke
v32@?0@"AFSpeechToken"8Q16^B24
-[SLSentencePieceModule initWithConfigFile:]
-[SLSentencePieceModule dealloc]
+[SLUtils decodeJsonFromFile:]
version
inputType
pipeline
outputSpecs
preprocessing
tokenizer
model
regexMapConfig
spmModel
modelFile
outputNodeName
spmEncodeOptions
PostITN
PreITN
-[SLODLDConfigDecoder initWithConfigFile:]
vocabFile
unkToken
leadingText
truncationList
[SEP]
[CLS]
empty
domainProb
assetVersion
Missing config for Bert Classifier %@
Non en locales supported yet
default
Missing output name for Bert Classifier %@
hey siri
-[SLBertClassifier initWithConfig:error:locale:]
v32@?0@"NSString"8Q16^B24
-[SLBertClassifier _createInputIdsAndRunModel:]_block_invoke
-[SLBertClassifier _createInputIdsAndRunModel:]
outputTokens
attnMask
inputIds
tokenTypeIds
-[SLODLDProcessor initWithConfigFile:error:]
Unable to decode config file %@
-[SLODLDProcessor _setupPipelineComponentsUsingConfig:]
Unable to create tokenizer
Unable to create ODLD model
-[SLODLDProcessor processInputUtterance:]
-[SLODLDProcessor _getTokenizerForType:withConfig:]
SLInvocationType
SLVoiceTriggerEventInfo
SLAudioSourceOption
SLLanguageCode
Dictation
recognizerConfigs
checkerTimes
checkerConfig
checkerType
triggerEndSampleCount
triggerStartSampleCount
Missing keys in context
-[SLProgressiveCheckerContext initWithContext:error:]
Missing config for Progressive checker %@
-[SLProgressiveCheckerAnalyzer initWithConfig:withDelegate:error:]
EAR recognizer init failed for config: %@
-[SLProgressiveCheckerAnalyzer initWithConfig:withDelegate:error:]_block_invoke
Progressive Checker
v32@?0@"_EARSyncSpeechRecognizer"8Q16^B24
-[SLProgressiveCheckerAnalyzer _addAudio:]
-[SLProgressiveCheckerAnalyzer _addAudio:]_block_invoke
-[SLProgressiveCheckerAnalyzer _endAudio]
-[SLProgressiveCheckerAnalyzer _endAudio]_block_invoke
com.apple.sl
Version
InputOpsMap
OutputMap
ModelFile
SupportedInputOrigins
threshold
modelIndex
name
shape
numAsrRecords
topPathScores
topPathNumTokens
latticeMaxScores
latticeMinScores
latticeMeanScores
latticeVarScores
trailingSilence
lrnnScores
lrnnProcessed
inputOrigin
acousticFTMScore
timeFromPrevQuery
speakerIdScore
airpodsConnectedState
boronActivityScore
acousticSpeechActivityScore
attSiriPrevOutputState
multiModalDecisionStage
eosLikelihood
nldaScore
mitigatorScore
mitigatorDecision
mitigatorThreshold
numTokensTopPath
trailingSilenceDuration
theshold
Missing config for Ures %@
-[SLUresMitigator initWithConfig:error:]
Unable to create model with error %@
-[SLUresMitigator dealloc]
-[SLUresMitigator _createInputOriginThresholdMap:]_block_invoke
v32@?0@"NSString"8@"NSDictionary"16^B24
-[SLUresMitigator _createInputOriginThresholdMap:]
-[SLUresMitigator _processInputFeats:]_block_invoke
Failed to create feature with error %@
-[SLUresMitigator _processInputFeats:]
Failed to get output with error %@
requestMitigated
requestNotMitigated
score
decision
decisionValue
-[SLUresMitigator _convertNSArrayToMultiArray:withShape:]
-[SLUresMitigator _convertMultiArrayToNSArray:withShape:]
%s Unable to create feature dict with error %@ from %@
%s Expected 2D array, got %lu, not running model
%s Unable to extract shape of feature %@
%s Tokens: %@
%s inputIds: %@
%s attnMask: %@
%s ::: SL logging initialized (%s)
%s %@
%s Empty pattern, ignore
%s Unable to decode pattern %@ with err %@
%s Input to regex: %@
%s Output of regex: %@
%s Received nil recog candidate, nothing to extract
%s Extracted LRNN Score: %f from Model Version: %{public}@
%s Constructing tokens for speech path %{public}@
%s Adding score %{public}ld for token %{public}@
%s SPM model created
%s dealloc
%s ERR: metaData is nil, defaulting to NO for %{public}@
%s ERR: read metafile %{public}@ failed with %{public}@ - defaulting to NO
%s Decoded config at path: %@
%s VocabList size: %lu UnkToken %@
%s Truncation list: %@
%s Token not found, using unk token
%s Num Tokens - %lu  exceeded model input length - %lu
%s output: %@
%s Reading from configFile %@
%s Created ODLD Processor Pipelines
%s PreProcessorType: %lu PreProcessor %@
%s TokenizerType: %lu Tokenizer %@
%s Classifier %@
%s Empty input
%s output %f
%s Setting SPM encoder options %@ with status %ld
%s %{public}@
%s Created SLAcousticContext: %{public}@
%s Configured buffer size: %f samples, to be flushed after every %lu samples
%s Added checker to analyze %{public}lu samples with config file %{public}@
%s Failed to initialize SLProgressiveCheckerAnalyzer with error %{public}@
%s Initialized Progressive Checkers !
%s Unable to copy from circular buffer !
%s Flushed %lu samples to checker, samples since last flush %lu, total samples in buffer %lu
%s Calling endAudio after feeding %{public}lu samples to recognizer
%s Checker %lu fired, analyzed %{public}lu samples, token %{public}@,  confidence %{public}f
%s Flushed %lu samples to checker
%s All Checkers Finished, analyzed %{public}lu samples, token %{public}@, confidence %{public}f
%s Invalid config for %{public}@
%s Threshold map: %{public}@
%s AttFeature: %{public}@ --> %{public}@
%s Value for feature: %{public}@ isn't set, abort model run
%s Failed to convert array to MLMultiArray, not using feature %{public}@ for inference
%s Using software configured threshold: %.3f
%s Failed to convert NSArray with shape %{public}@ to MLMultiArray with err %{public}@
%s Mismatch in output shape, expected: %{public}@ got: %{public}@
SLODLDClassifierV1
SLODLDModelComponent
NSObject
SLRegexMatcher
SLODLDPreProcessingComponent
SLASRFeatures
SLLRNNFeatures
SLASRFeatureExtractor
SLSentencePieceModule
SLODLDTokenizerComponent
SLUtils
SLODLDProcessorResult
SLODLDConfigDecoder
SLBertClassifierResult
SLBertClassifier
SLODLDProcessor
SLProgressiveCheckerContext
SLProgressiveCheckerResult
SLProgressiveCheckerAnalyzer
SLUresMitigatorResult
SLUresMitigatorIpFeats
SLUresMitigator
T@"NSDictionary",R,N,V_features
JSONObjectWithData:options:error:
T@"NSString",&,N,V_resourcePath
T@"<SLODLDModelComponent>",&,N,V_model
TQ,N,V_topLatticePathTokenCount
T@"<SLODLDTokenizerComponent>",&,N,V_tokenizer
_createInputOriginThresholdMap:
T@"EARSentencePieceModule",&,V_processor
_extractedFeats
T@"MLModel",&,N,V_odldClassifier
_locale
T@"NSArray",&,N,V_latticePathMeanScores
_odldClassifier
T@"NSArray",&,N,V_latticePathVarScores
_replaceStrings
T@"NSArray",&,N,V_regexMatchingRules
_tokens
T@"NSArray",&,N,V_topLatticePathScores
confidenceScore
T@"NSArray",&,N,V_vocab
containsString:
T@"NSArray",R,N,V_domainProb
T@"NSDictionary",&,N,V_dictionary
getSPMModelFile
T@"NSDictionary",&,N,V_nldaMetaInfo
isProxy
T@"NSDictionary",R,N,V_extractedFeats
multiArrayValue
T@"NSNumber",&,N,V_acousticFTMScores
phrases
T@"NSNumber",&,N,V_eosLikelihood
release
T@"NSNumber",&,N,V_nldaScore
setOutputNodes:
T@"NSNumber",&,N,V_speakerIDScore
setUsesCPUOnly:
T@"NSString",&,N,V_unkToken
stringByAppendingPathComponent:
T@"NSString",R,N,V_configVersion
version
.cxx_destruct
T@"NSString",&,N,V_assetVersion
T#,R
T@"NSString",R,N,V_assetVersion
T@"<SLODLDPreProcessingComponent>",&,N,V_preprocessor
_config
T@"AFSpeechPackage",&,N,V_speechPackage
_detailedResult
T@"MLModel",&,N,V_bertModel
_invocationType
T@"NSArray",&,N,V_latticePathMaxScores
_normalizeText:
T@"NSArray",&,N,V_latticePathMinScores
_outputNodeName
T@"NSArray",&,N,V_outputNodes
_speakerIDScore
T@"NSArray",&,N,V_replaceStrings
analyzedSamples
T@"NSArray",&,N,V_truncationTokenList
containsObject:
T@"NSArray",R,N,V_detailedResult
dealloc
T@"NSArray",R,N,V_tokens
endTime
T@"NSDictionary",&,N,V_inputSpecs
interpretations
T@"NSDictionary",R,N,V_detailedResult
lowercaseString
T@"NSDictionary",R,N,V_vtei
numberWithBool:
T@"NSNumber",&,N,V_boronScore
prevStageOutput
T@"NSNumber",&,N,V_inputOrigin
setInputOrigin:
T@"NSNumber",&,N,V_prevStageOutput
setSnr:
T@"NSString",&,N,V_outputNodeName
T@"NSString",R,C
trailingSilence
T@"NSString",R,N,V_locale
T@"SLODLDConfigDecoder",&,N,V_config
TB,N,V_didDetectSpeechActivity
TB,N,V_isAirpodsConnected
TB,N,V_lrnnProcessed
TB,N,V_shouldAppendLeadingText
TB,R,N,V_didMitigate
TQ,N,V_decisionStage
TQ,N,V_maxInputCharsPerWord
TQ,N,V_maxNumTokens
TQ,N,V_numLeadingTokens
TQ,R
TQ,R,N,V_analyzedSamples
TQ,R,N,V_audioOption
TQ,R,N,V_invocationType
TQ,R,N,V_resultType
Td,N,V_timeSinceLastQuery
Td,R,N,V_decisionLevel
Tf,N,V_lrnnScore
Tf,N,V_snr
Tf,N,V_trailingSilence
Tf,R,N,V_odldScore
Tf,R,N,V_score
Tf,R,N,V_threshold
_acousticFTMScores
_activeRecognizers
_addAudio:
_analyzedSamples
_analyzedSamplesSoFar
_assetVersion
_audioOption
_bertModel
_boronScore
_checkerEndSamples
_checkerType
_circBuffer
_compileRegexRulesForPatterns:
_configVersion
_constructFeatureDictionary:
_context
_convert1dMLMultiArrayToNSArray:
_convertMultiArrayToNSArray:withShape:
_convertNSArrayToMultiArray:withShape:
_createInputIdsAndRunModel:
_decisionLevel
_decisionStage
_delegate
_dictionary
_didDetectSpeechActivity
_didMitigate
_domainProb
_endAudio
_eosLikelihood
_extractModelSpecs
_features
_getLastTokenForPath:fromPhrases:
_getPreprocessorForType:withConfig:
_getTokenConfidenceForPath:fromPhrases:
_getTokenizerForType:withConfig:
_getValueForKey:categoryKey:
_inputOpsMap
_inputOrigin
_inputSpecs
_isAirpodsConnected
_isCharPunctuation:
_latestScore
_latticePathMaxScores
_latticePathMeanScores
_latticePathMinScores
_latticePathVarScores
_lrnnProcessed
_lrnnScore
_maxInputCharsPerWord
_maxNumTokens
_model
_nldaMetaInfo
_nldaScore
_numLeadingTokens
_numSamplesAddedToBufferSinceLastFlush
_numSamplesInStride
_odldScore
_options
_outputMap
_outputNodes
_preprocessor
_prevStageOutput
_processInputFeats:
_processor
_queue
_readVocabFromFile:
_regexMatchingRules
_resourcePath
_resultType
_score
_setupPipelineComponentsUsingConfig:
_shouldAppendLeadingText
_snr
_speechPackage
_splitOnPunctuation:
_startNewRequestWithContext:
_threshold
_thresholdMap
_timeSinceLastQuery
_tokenizer
_topLatticePathScores
_topLatticePathTokenCount
_trailingSilence
_truncationTokenList
_unkToken
_uresModel
_version
_vocab
_vtei
_wordPieceTokenizer:
acousticFTMScores
addAudio:
addEntriesFromDictionary:
addObject:
addObjectsFromArray:
addSamples:numSamples:
analyzer:hasFinalResult:
analyzer:hasPartialResult:
appendString:
applyPreprocessingOnUtt:
array
arrayWithObject:
arrayWithObjects:count:
assetVersion
audioAnalytics
audioOption
autorelease
bertModel
boolValue
boronScore
bufferLength
bytes
characterAtIndex:
characterIsMember:
class
componentsSeparatedByCharactersInSet:
confidence
config
configVersion
conformsToProtocol:
copy
copyBufferWithNumSamplesCopiedIn:
copybufferFrom:to:
count
countByEnumeratingWithState:objects:count:
createErrorWithMsg:code:
dataWithContentsOfFile:
debugDescription
decisionLevel
decisionStage
decodeJsonFromFile:
description
detailedResult
dictionary
dictionaryRepresentation
dictionaryWithObjects:forKeys:count:
didDetectSpeechActivity
didMitigate
domainProb
doubleValue
encodeUtterance:
endAudio
enumerateKeysAndObjectsUsingBlock:
enumerateObjectsUsingBlock:
eosLikelihood
errorWithDomain:code:userInfo:
expressionForConstantValue:
expressionForFunction:arguments:
expressionValueWithObject:context:
extractASRFaturesFrom:
extractLRNNFaturesFrom:
extractedFeats
featureValueForName:
features
fileURLWithPath:
firstObject
floatValue
getBertModelFile
getBertModelOutputNodes
getBestSpeechRecognitionTextFromPackage:
getConfigVersion
getInputType
getOutputSpecs
getPreProcessorType
getRegexMapConfig
getSPMEncoderOptions
getTokenizerType
getTranscriptionForSpeechPackage:
hash
indexOfObject:
indexSetWithIndex:
init
initWithCapacity:
initWithConfig:error:
initWithConfig:error:locale:
initWithConfig:withDelegate:error:
initWithConfigFile:
initWithConfigFile:error:
initWithConfiguration:
initWithContext:error:
initWithDictionary:error:
initWithModelPath:
initWithNumChannels:recordingDuration:samplingRate:
initWithScore:assetVersion:extractedFeats:
initWithScore:decision:decisionLevel:detailedResults:extractedFeats:
initWithScore:ofType:analyzedSamples:detailedResults:
initWithScore:withVersion:tokens:features:
initWithShape:dataType:error:
inputDescriptionsByName
inputOrigin
inputSpecs
interpretationIndices
invocationType
isAirpodsConnected
isEqual:
isEqualToString:
isKindOfClass:
isMemberOfClass:
lastObject
latticeMitigatorResult
latticePathMaxScores
latticePathMeanScores
latticePathMinScores
latticePathVarScores
length
locale
localeWithLocaleIdentifier:
localizedDescription
lrnnProcessed
lrnnScore
maxInputCharsPerWord
maxNumTokens
model
modelDescription
modelWithContentsOfURL:configuration:error:
multiArrayConstraint
mutableCopy
newlineCharacterSet
nldaMetaInfo
nldaScore
numLeadingTokens
numberWithDouble:
numberWithFloat:
numberWithInteger:
numberWithUnsignedInteger:
objectAtIndex:
objectAtIndexedSubscript:
objectForKey:
objectForKeyedSubscript:
odldClassifier
odldScore
outputDescriptionsByName
outputNodeName
outputNodes
performSelector:
performSelector:withObject:
performSelector:withObject:withObject:
predictionFromFeatures:error:
predictionFromFeatures:options:error:
preprocessor
processEncodedTokens:
processInputFeats:completion:
processInputText:
processInputUtterance:
processSpeechPackage:
processor
recognition
regexMatchingRules
regularExpressionWithPattern:options:error:
removeObject:
removeObjectAtIndex:
removeObjectsAtIndexes:
removeSpaceAfter
removeSpaceBefore
replaceMatchesInString:options:range:withTemplate:
replaceStrings
reset
resetWithSamplingRate:language:taskType:userId:sessionId:deviceId:farField:audioSource:maxAudioBufferSizeSeconds:
resourcePath
respondsToSelector:
resultType
resultsWithAddedAudio:numberOfSamples:taskName:
resultsWithAddedFloatAudio:numberOfSamples:taskName:
resultsWithEndedAudio
retain
retainCount
sampleCount
score
self
setAcousticFTMScores:
setAssetVersion:
setBertModel:
setBoronScore:
setComputeUnits:
setConfig:
setDateFormat:
setDecisionStage:
setDictionary:
setDidDetectSpeechActivity:
setEncodeOptions:
setEosLikelihood:
setInputSpecs:
setIsAirpodsConnected:
setLatticePathMaxScores:
setLatticePathMeanScores:
setLatticePathMinScores:
setLatticePathVarScores:
setLocale:
setLrnnProcessed:
setLrnnScore:
setMaxInputCharsPerWord:
setMaxNumTokens:
setModel:
setNldaMetaInfo:
setNldaScore:
setNumLeadingTokens:
setObject:atIndexedSubscript:
setObject:forKey:
setObject:forKeyedSubscript:
setOdldClassifier:
setOutputNodeName:
setPreprocessor:
setPrevStageOutput:
setProcessor:
setRegexMatchingRules:
setReplaceStrings:
setResourcePath:
setSPMEncoderOptions:
setShouldAppendLeadingText:
setSpeakerIDScore:
setSpeechPackage:
setTimeSinceLastQuery:
setTokenizer:
setTopLatticePathScores:
setTopLatticePathTokenCount:
setTrailingSilence:
setTruncationTokenList:
setUnkToken:
setVocab:
shape
shouldAppendLeadingText
silenceStartTime
speakerIDScore
speechPackage
startNewRequestWithContext:
stringByAppendingString:
stringByDeletingLastPathComponent
stringWithContentsOfFile:encoding:error:
stringWithFormat:
substringWithRange:
superclass
text
threshold
timeSinceLastQuery
tokenName
tokenizer
tokens
topLatticePathScores
topLatticePathTokenCount
truncationTokenList
unkToken
unsignedIntValue
unsignedIntegerValue
utterances
valueForKey:
vocab
vtei
whitespaceCharacterSet
zone
B24@0:8@16
#16@0:8
@16@0:8
@24@0:8:16
@32@0:8:16@24
@40@0:8:16@24@32
B16@0:8
B24@0:8#16
B24@0:8:16
Vv16@0:8
Q16@0:8
^{_NSZone=}16@0:8
B24@0:8@"Protocol"16
@"NSString"16@0:8
@24@0:8@16
f24@0:8@16
@24@0:8@"NSString"16
f24@0:8@"NSArray"16
v16@0:8
v24@0:8@16
@"MLModel"
@"NSDictionary"
@"NSArray"
@"NSNumber"
@"NSString"24@0:8@"NSString"16
v24@0:8Q16
f16@0:8
v20@0:8f16
v20@0:8B16
@32@0:8@16@24
@"NSArray"24@0:8@"NSString"16
q24@0:8@16
@"EARSentencePieceModule"
@32@0:8@16Q24
@44@0:8f16@20@28@36
@"NSString"
@40@0:8@16@24@32
@40@0:8@16^@24@32
B20@0:8S16
@32@0:8@16^@24
@32@0:8Q16@24
@"SLODLDConfigDecoder"
@"<SLODLDPreProcessingComponent>"
@"<SLODLDTokenizerComponent>"
@"<SLODLDModelComponent>"
@44@0:8f16Q20Q28@36
@40@0:8@16@24^@32
@"NSMutableArray"
@"SLProgressiveCheckerContext"
@"NSObject<OS_dispatch_queue>"
@"<SLProgressiveCheckerAnalyzerDelegate>"
@"CSAudioCircularBuffer"
@48@0:8f16B20d24@32@40
d16@0:8
v24@0:8d16
@"AFSpeechPackage"
v32@0:8@16@?24
@"MLPredictionOptions"
@"NSMutableDictionary"
@(#)PROGRAM:SiriLiminal  PROJECT:CoreSpeech-
minibatch_token_type_ids
minibatch_attention_mask
minibatch_input_ids
minibatch_padding_mask
input_origin
v32@?0@"NSString"8@16^B24
-[SLODLDClassifierV1 processEncodedTokens:]
-[SLODLDClassifierV1 _constructFeatureDictionary:]
v32@?0@"NSNumber"8Q16^B24
-[SLODLDClassifierV1 _constructFeatureDictionary:]_block_invoke_2
v32@?0@"NSString"8@"MLFeatureDescription"16^B24
com.apple.SiriLiminal
Framework
v8@?0
::: Initializing SiriLiminal logging...
en_US_POSIX
yyyyMMdd-HHmmss
SLLogInitIfNeeded_block_invoke
gitrelno_unavailable
rules
patternMatch
replaceWith
Missing config for regex matcher %@
reason
-[SLRegexMatcher initWithConfigFile:]
-[SLRegexMatcher _compileRegexRulesForPatterns:]_block_invoke
v32@?0@"NSDictionary"8Q16^B24
v32@?0@"NSRegularExpression"8Q16^B24
-[SLRegexMatcher applyPreprocessingOnUtt:]
+[SLASRFeatureExtractor extractASRFaturesFrom:]
max:
min:
stddev:
average:
v32@?0@"AFSpeechUtterance"8Q16^B24
+[SLASRFeatureExtractor extractLRNNFaturesFrom:]
+[SLASRFeatureExtractor _getTokenConfidenceForPath:fromPhrases:]
v32@?0@"AFSpeechPhrase"8Q16^B24
+[SLASRFeatureExtractor _getTokenConfidenceForPath:fromPhrases:]_block_invoke_2
v32@?0@"AFSpeechToken"8Q16^B24
-[SLSentencePieceModule initWithConfigFile:]
-[SLSentencePieceModule dealloc]
+[SLUtils decodeJsonFromFile:]
version
inputType
pipeline
outputSpecs
preprocessing
tokenizer
model
regexMapConfig
spmModel
modelFile
outputNodeName
spmEncodeOptions
PostITN
PreITN
-[SLODLDConfigDecoder initWithConfigFile:]
vocabFile
unkToken
leadingText
truncationList
[SEP]
[CLS]
empty
domainProb
assetVersion
Missing config for Bert Classifier %@
Non en locales supported yet
default
Missing output name for Bert Classifier %@
hey siri
-[SLBertClassifier initWithConfig:error:locale:]
v32@?0@"NSString"8Q16^B24
-[SLBertClassifier _createInputIdsAndRunModel:]_block_invoke
-[SLBertClassifier _createInputIdsAndRunModel:]
outputTokens
attnMask
inputIds
tokenTypeIds
-[SLODLDProcessor initWithConfigFile:error:]
Unable to decode config file %@
-[SLODLDProcessor _setupPipelineComponentsUsingConfig:]
Unable to create tokenizer
Unable to create ODLD model
-[SLODLDProcessor processInputUtterance:]
-[SLODLDProcessor _getTokenizerForType:withConfig:]
SLInvocationType
SLVoiceTriggerEventInfo
SLAudioSourceOption
SLLanguageCode
Dictation
recognizerConfigs
checkerTimes
checkerConfig
checkerType
triggerEndSampleCount
triggerStartSampleCount
Missing keys in context
-[SLProgressiveCheckerContext initWithContext:error:]
Missing config for Progressive checker %@
-[SLProgressiveCheckerAnalyzer initWithConfig:withDelegate:error:]
EAR recognizer init failed for config: %@
-[SLProgressiveCheckerAnalyzer initWithConfig:withDelegate:error:]_block_invoke
Progressive Checker
v32@?0@"_EARSyncSpeechRecognizer"8Q16^B24
-[SLProgressiveCheckerAnalyzer _addAudio:]
-[SLProgressiveCheckerAnalyzer _addAudio:]_block_invoke
-[SLProgressiveCheckerAnalyzer _endAudio]
-[SLProgressiveCheckerAnalyzer _endAudio]_block_invoke_2
com.apple.sl
Version
InputOpsMap
OutputMap
ModelFile
SupportedInputOrigins
threshold
modelIndex
name
shape
numAsrRecords
topPathScores
topPathNumTokens
latticeMaxScores
latticeMinScores
latticeMeanScores
latticeVarScores
trailingSilence
lrnnScores
lrnnProcessed
inputOrigin
acousticFTMScore
timeFromPrevQuery
speakerIdScore
airpodsConnectedState
boronActivityScore
acousticSpeechActivityScore
attSiriPrevOutputState
multiModalDecisionStage
eosLikelihood
nldaScore
mitigatorScore
mitigatorDecision
mitigatorThreshold
numTokensTopPath
trailingSilenceDuration
theshold
Missing config for Ures %@
-[SLUresMitigator initWithConfig:error:]
Unable to create model with error %@
-[SLUresMitigator dealloc]
-[SLUresMitigator _createInputOriginThresholdMap:]_block_invoke
v32@?0@"NSString"8@"NSDictionary"16^B24
-[SLUresMitigator _createInputOriginThresholdMap:]
-[SLUresMitigator _processInputFeats:]_block_invoke
Failed to create feature with error %@
-[SLUresMitigator _processInputFeats:]
Failed to get output with error %@
requestMitigated
requestNotMitigated
score
decision
decisionValue
-[SLUresMitigator _convertNSArrayToMultiArray:withShape:]
-[SLUresMitigator _convertMultiArrayToNSArray:withShape:]
%s Unable to create feature dict with error %@ from %@
%s output: %@
%s Expected 2D array, got %lu, not running model
%s Unable to extract shape of feature %@
%s Tokens: %@
%s inputIds: %@
%s attnMask: %@
%s ::: SL logging initialized (%s)
%s %@
%s Empty pattern, ignore
%s Unable to decode pattern %@ with err %@
%s Input to regex: %@
%s Output of regex: %@
%s Received nil recog candidate, nothing to extract
%s Extracted LRNN Score: %f from Model Version: %{public}@
%s Constructing tokens for speech path %{public}@
%s Adding score %{public}ld for token %{public}@
%s SPM model created
%s dealloc
%s ERR: metaData is nil, defaulting to NO for %{public}@
%s ERR: read metafile %{public}@ failed with %{public}@ - defaulting to NO
%s Decoded config at path: %@
%s VocabList size: %lu UnkToken %@
%s Truncation list: %@
%s Token not found, using unk token
%s Num Tokens - %lu  exceeded model input length - %lu
%s Reading from configFile %@
%s Created ODLD Processor Pipelines
%s PreProcessorType: %lu PreProcessor %@
%s TokenizerType: %lu Tokenizer %@
%s Classifier %@
%s Empty input
%s output %f
%s Setting SPM encoder options %@ with status %ld
%s %{public}@
%s Created SLAcousticContext: %{public}@
%s Configured buffer size: %f samples, to be flushed after every %lu samples
%s Added checker to analyze %{public}lu samples with config file %{public}@
%s Failed to initialize SLProgressiveCheckerAnalyzer with error %{public}@
%s Initialized Progressive Checkers !
%s Unable to copy from circular buffer !
%s Flushed %lu samples to checker, samples since last flush %lu, total samples in buffer %lu
%s Calling endAudio after feeding %{public}lu samples to recognizer
%s Checker %lu fired, analyzed %{public}lu samples, token %{public}@,  confidence %{public}f
%s Flushed %lu samples to checker
%s All Checkers Finished, analyzed %{public}lu samples, token %{public}@, confidence %{public}f
%s Invalid config for %{public}@
%s Threshold map: %{public}@
%s AttFeature: %{public}@ --> %{public}@
%s Value for feature: %{public}@ isn't set, abort model run
%s Failed to convert array to MLMultiArray, not using feature %{public}@ for inference
%s Using software configured threshold: %.3f
%s Failed to convert NSArray with shape %{public}@ to MLMultiArray with err %{public}@
%s Mismatch in output shape, expected: %{public}@ got: %{public}@
SLODLDClassifierV1
SLODLDModelComponent
NSObject
SLRegexMatcher
SLODLDPreProcessingComponent
SLASRFeatures
SLLRNNFeatures
SLASRFeatureExtractor
SLSentencePieceModule
SLODLDTokenizerComponent
SLUtils
SLODLDProcessorResult
SLODLDConfigDecoder
SLBertClassifierResult
SLBertClassifier
SLODLDProcessor
SLProgressiveCheckerContext
SLProgressiveCheckerResult
SLProgressiveCheckerAnalyzer
SLUresMitigatorResult
SLUresMitigatorIpFeats
SLUresMitigator
T@"NSDictionary",R,N,V_features
JSONObjectWithData:options:error:
T@"NSString",&,N,V_resourcePath
T@"<SLODLDModelComponent>",&,N,V_model
TQ,N,V_topLatticePathTokenCount
T@"<SLODLDTokenizerComponent>",&,N,V_tokenizer
_createInputOriginThresholdMap:
T@"EARSentencePieceModule",&,V_processor
_extractedFeats
T@"MLModel",&,N,V_odldClassifier
_locale
T@"NSArray",&,N,V_latticePathMeanScores
_odldClassifier
T@"NSArray",&,N,V_latticePathVarScores
_replaceStrings
T@"NSArray",&,N,V_regexMatchingRules
_tokens
T@"NSArray",&,N,V_topLatticePathScores
confidenceScore
T@"NSArray",&,N,V_vocab
containsString:
T@"NSArray",R,N,V_domainProb
T@"NSDictionary",&,N,V_dictionary
getSPMModelFile
T@"NSDictionary",&,N,V_nldaMetaInfo
isProxy
T@"NSDictionary",R,N,V_extractedFeats
multiArrayValue
T@"NSNumber",&,N,V_acousticFTMScores
phrases
T@"NSNumber",&,N,V_eosLikelihood
release
T@"NSNumber",&,N,V_nldaScore
setOutputNodes:
T@"NSNumber",&,N,V_speakerIDScore
setUsesCPUOnly:
T@"NSString",&,N,V_unkToken
stringByAppendingPathComponent:
T@"NSString",R,N,V_configVersion
version
.cxx_destruct
T@"NSString",&,N,V_assetVersion
T#,R
T@"NSString",R,N,V_assetVersion
T@"<SLODLDPreProcessingComponent>",&,N,V_preprocessor
_config
T@"AFSpeechPackage",&,N,V_speechPackage
_detailedResult
T@"MLModel",&,N,V_bertModel
_invocationType
T@"NSArray",&,N,V_latticePathMaxScores
_normalizeText:
T@"NSArray",&,N,V_latticePathMinScores
_outputNodeName
T@"NSArray",&,N,V_outputNodes
_speakerIDScore
T@"NSArray",&,N,V_replaceStrings
analyzedSamples
T@"NSArray",&,N,V_truncationTokenList
containsObject:
T@"NSArray",R,N,V_detailedResult
dealloc
T@"NSArray",R,N,V_tokens
endTime
T@"NSDictionary",&,N,V_inputSpecs
interpretations
T@"NSDictionary",R,N,V_detailedResult
lowercaseString
T@"NSDictionary",R,N,V_vtei
numberWithBool:
T@"NSNumber",&,N,V_boronScore
prevStageOutput
T@"NSNumber",&,N,V_inputOrigin
setInputOrigin:
T@"NSNumber",&,N,V_prevStageOutput
setSnr:
T@"NSString",&,N,V_outputNodeName
T@"NSString",R,C
trailingSilence
T@"NSString",R,N,V_locale
T@"SLODLDConfigDecoder",&,N,V_config
TB,N,V_didDetectSpeechActivity
TB,N,V_isAirpodsConnected
TB,N,V_lrnnProcessed
TB,N,V_shouldAppendLeadingText
TB,R,N,V_didMitigate
TQ,N,V_decisionStage
TQ,N,V_maxInputCharsPerWord
TQ,N,V_maxNumTokens
TQ,N,V_numLeadingTokens
TQ,R
TQ,R,N,V_analyzedSamples
TQ,R,N,V_audioOption
TQ,R,N,V_invocationType
TQ,R,N,V_resultType
Td,N,V_timeSinceLastQuery
Td,R,N,V_decisionLevel
Tf,N,V_lrnnScore
Tf,N,V_snr
Tf,N,V_trailingSilence
Tf,R,N,V_odldScore
Tf,R,N,V_score
Tf,R,N,V_threshold
_acousticFTMScores
_activeRecognizers
_addAudio:
_analyzedSamples
_analyzedSamplesSoFar
_assetVersion
_audioOption
_bertModel
_boronScore
_checkerEndSamples
_checkerType
_circBuffer
_compileRegexRulesForPatterns:
_configVersion
_constructFeatureDictionary:
_context
_convert1dMLMultiArrayToNSArray:
_convertMultiArrayToNSArray:withShape:
_convertNSArrayToMultiArray:withShape:
_createInputIdsAndRunModel:
_decisionLevel
_decisionStage
_delegate
_dictionary
_didDetectSpeechActivity
_didMitigate
_domainProb
_endAudio
_eosLikelihood
_extractModelSpecs
_features
_getLastTokenForPath:fromPhrases:
_getPreprocessorForType:withConfig:
_getTokenConfidenceForPath:fromPhrases:
_getTokenizerForType:withConfig:
_getValueForKey:categoryKey:
_inputOpsMap
_inputOrigin
_inputSpecs
_isAirpodsConnected
_isCharPunctuation:
_latestScore
_latticePathMaxScores
_latticePathMeanScores
_latticePathMinScores
_latticePathVarScores
_lrnnProcessed
_lrnnScore
_maxInputCharsPerWord
_maxNumTokens
_model
_nldaMetaInfo
_nldaScore
_numLeadingTokens
_numSamplesAddedToBufferSinceLastFlush
_numSamplesInStride
_odldScore
_options
_outputMap
_outputNodes
_preprocessor
_prevStageOutput
_processInputFeats:
_processor
_queue
_readVocabFromFile:
_regexMatchingRules
_resourcePath
_resultType
_score
_setupPipelineComponentsUsingConfig:
_shouldAppendLeadingText
_snr
_speechPackage
_splitOnPunctuation:
_startNewRequestWithContext:
_threshold
_thresholdMap
_timeSinceLastQuery
_tokenizer
_topLatticePathScores
_topLatticePathTokenCount
_trailingSilence
_truncationTokenList
_unkToken
_uresModel
_version
_vocab
_vtei
_wordPieceTokenizer:
acousticFTMScores
addAudio:
addEntriesFromDictionary:
addObject:
addObjectsFromArray:
addSamples:numSamples:
analyzer:hasFinalResult:
analyzer:hasPartialResult:
appendString:
applyPreprocessingOnUtt:
array
arrayWithObject:
arrayWithObjects:count:
assetVersion
audioAnalytics
audioOption
autorelease
bertModel
boolValue
boronScore
bufferLength
bytes
characterAtIndex:
characterIsMember:
class
componentsSeparatedByCharactersInSet:
confidence
config
configVersion
conformsToProtocol:
copy
copyBufferWithNumSamplesCopiedIn:
copybufferFrom:to:
count
countByEnumeratingWithState:objects:count:
createErrorWithMsg:code:
dataWithContentsOfFile:
debugDescription
decisionLevel
decisionStage
decodeJsonFromFile:
description
detailedResult
dictionary
dictionaryRepresentation
dictionaryWithObjects:forKeys:count:
didDetectSpeechActivity
didMitigate
domainProb
doubleValue
encodeUtterance:
endAudio
enumerateKeysAndObjectsUsingBlock:
enumerateObjectsUsingBlock:
eosLikelihood
errorWithDomain:code:userInfo:
expressionForConstantValue:
expressionForFunction:arguments:
expressionValueWithObject:context:
extractASRFaturesFrom:
extractLRNNFaturesFrom:
extractedFeats
featureValueForName:
features
fileURLWithPath:
firstObject
floatValue
getBertModelFile
getBertModelOutputNodes
getBestSpeechRecognitionTextFromPackage:
getConfigVersion
getInputType
getOutputSpecs
getPreProcessorType
getRegexMapConfig
getSPMEncoderOptions
getTokenizerType
getTranscriptionForSpeechPackage:
hash
indexOfObject:
indexSetWithIndex:
init
initWithCapacity:
initWithConfig:error:
initWithConfig:error:locale:
initWithConfig:withDelegate:error:
initWithConfigFile:
initWithConfigFile:error:
initWithConfiguration:
initWithContext:error:
initWithDictionary:error:
initWithModelPath:
initWithNumChannels:recordingDuration:samplingRate:
initWithScore:assetVersion:extractedFeats:
initWithScore:decision:decisionLevel:detailedResults:extractedFeats:
initWithScore:ofType:analyzedSamples:detailedResults:
initWithScore:withVersion:tokens:features:
initWithShape:dataType:error:
inputDescriptionsByName
inputOrigin
inputSpecs
interpretationIndices
invocationType
isAirpodsConnected
isEqual:
isEqualToString:
isKindOfClass:
isMemberOfClass:
lastObject
latticeMitigatorResult
latticePathMaxScores
latticePathMeanScores
latticePathMinScores
latticePathVarScores
length
locale
localeWithLocaleIdentifier:
localizedDescription
lrnnProcessed
lrnnScore
maxInputCharsPerWord
maxNumTokens
model
modelDescription
modelWithContentsOfURL:configuration:error:
multiArrayConstraint
mutableCopy
newlineCharacterSet
nldaMetaInfo
nldaScore
numLeadingTokens
numberWithDouble:
numberWithFloat:
numberWithInteger:
numberWithUnsignedInteger:
objectAtIndex:
objectAtIndexedSubscript:
objectForKey:
objectForKeyedSubscript:
odldClassifier
odldScore
outputDescriptionsByName
outputNodeName
outputNodes
performSelector:
performSelector:withObject:
performSelector:withObject:withObject:
predictionFromFeatures:error:
predictionFromFeatures:options:error:
preprocessor
processEncodedTokens:
processInputFeats:completion:
processInputText:
processInputUtterance:
processSpeechPackage:
processor
recognition
regexMatchingRules
regularExpressionWithPattern:options:error:
removeObject:
removeObjectAtIndex:
removeObjectsAtIndexes:
removeSpaceAfter
removeSpaceBefore
replaceMatchesInString:options:range:withTemplate:
replaceStrings
reset
resetWithSamplingRate:language:taskType:userId:sessionId:deviceId:farField:audioSource:maxAudioBufferSizeSeconds:
resourcePath
respondsToSelector:
resultType
resultsWithAddedAudio:numberOfSamples:taskName:
resultsWithAddedFloatAudio:numberOfSamples:taskName:
resultsWithEndedAudio
retain
retainCount
sampleCount
score
self
setAcousticFTMScores:
setAssetVersion:
setBertModel:
setBoronScore:
setComputeUnits:
setConfig:
setDateFormat:
setDecisionStage:
setDictionary:
setDidDetectSpeechActivity:
setEncodeOptions:
setEosLikelihood:
setInputSpecs:
setIsAirpodsConnected:
setLatticePathMaxScores:
setLatticePathMeanScores:
setLatticePathMinScores:
setLatticePathVarScores:
setLocale:
setLrnnProcessed:
setLrnnScore:
setMaxInputCharsPerWord:
setMaxNumTokens:
setModel:
setNldaMetaInfo:
setNldaScore:
setNumLeadingTokens:
setObject:atIndexedSubscript:
setObject:forKey:
setObject:forKeyedSubscript:
setOdldClassifier:
setOutputNodeName:
setPreprocessor:
setPrevStageOutput:
setProcessor:
setRegexMatchingRules:
setReplaceStrings:
setResourcePath:
setSPMEncoderOptions:
setShouldAppendLeadingText:
setSpeakerIDScore:
setSpeechPackage:
setTimeSinceLastQuery:
setTokenizer:
setTopLatticePathScores:
setTopLatticePathTokenCount:
setTrailingSilence:
setTruncationTokenList:
setUnkToken:
setVocab:
shape
shouldAppendLeadingText
silenceStartTime
speakerIDScore
speechPackage
startNewRequestWithContext:
stringByAppendingString:
stringByDeletingLastPathComponent
stringWithContentsOfFile:encoding:error:
stringWithFormat:
substringWithRange:
superclass
text
threshold
timeSinceLastQuery
tokenName
tokenizer
tokens
topLatticePathScores
topLatticePathTokenCount
truncationTokenList
unkToken
unsignedIntValue
unsignedIntegerValue
utterances
valueForKey:
vocab
vtei
whitespaceCharacterSet
zone
B24@0:8@16
#16@0:8
@16@0:8
@24@0:8:16
@32@0:8:16@24
@40@0:8:16@24@32
B16@0:8
B24@0:8#16
B24@0:8:16
Vv16@0:8
Q16@0:8
^{_NSZone=}16@0:8
B24@0:8@"Protocol"16
@"NSString"16@0:8
@24@0:8@16
f24@0:8@16
@24@0:8@"NSString"16
f24@0:8@"NSArray"16
v16@0:8
v24@0:8@16
@"MLModel"
@"NSDictionary"
@"NSArray"
@"NSNumber"
@"NSString"24@0:8@"NSString"16
v24@0:8Q16
f16@0:8
v20@0:8f16
v20@0:8B16
@32@0:8@16@24
@"NSArray"24@0:8@"NSString"16
q24@0:8@16
@"EARSentencePieceModule"
@32@0:8@16Q24
@44@0:8f16@20@28@36
@"NSString"
@40@0:8@16@24@32
@40@0:8@16^@24@32
B20@0:8S16
@32@0:8@16^@24
@32@0:8Q16@24
@"SLODLDConfigDecoder"
@"<SLODLDPreProcessingComponent>"
@"<SLODLDTokenizerComponent>"
@"<SLODLDModelComponent>"
@44@0:8f16Q20Q28@36
@40@0:8@16@24^@32
@"NSMutableArray"
@"SLProgressiveCheckerContext"
@"NSObject<OS_dispatch_queue>"
@"<SLProgressiveCheckerAnalyzerDelegate>"
@"CSAudioCircularBuffer"
@48@0:8f16B20d24@32@40
d16@0:8
v24@0:8d16
@"AFSpeechPackage"
v32@0:8@16@?24
@"MLPredictionOptions"
@"NSMutableDictionary"
