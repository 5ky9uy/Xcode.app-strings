@(#)PROGRAM:SiriLiminal  PROJECT:CoreSpeech-
com.apple.SiriLiminal
Framework
v8@?0
::: Initializing SiriLiminal logging...
en_US_POSIX
yyyyMMdd-HHmmss
SLLogInitIfNeeded_block_invoke
gitrelno_unavailable
+[SLASRFeatureExtractor extractASRFaturesFrom:]
max:
min:
stddev:
average:
v32@?0@"AFSpeechUtterance"8Q16^B24
+[SLASRFeatureExtractor extractLRNNFaturesFrom:]
+[SLASRFeatureExtractor _getTokenConfidenceForPath:fromPhrases:]
v32@?0@"AFSpeechPhrase"8Q16^B24
+[SLASRFeatureExtractor _getTokenConfidenceForPath:fromPhrases:]_block_invoke
v32@?0@"AFSpeechToken"8Q16^B24
+[SLUtils decodeJsonFromFile:]
version
vocabFile
unkToken
modelFile
leadingText
outputNodeName
truncationList
[SEP]
[CLS]
empty
domainProb
assetVersion
Missing config for Bert Classifier %@
reason
Non en locales supported yet
default
Missing output name for Bert Classifier %@
hey siri
-[SLBertClassifier initWithConfig:error:locale:]
v32@?0@"NSString"8Q16^B24
-[SLBertClassifier _createInputIdsAndRunModel:]_block_invoke
-[SLBertClassifier _createInputIdsAndRunModel:]
minibatch_input_ids
minibatch_attention_mask
minibatch_token_type_ids
outputTokens
attnMask
inputIds
tokenTypeIds
SLInvocationType
SLVoiceTriggerEventInfo
SLAudioSourceOption
SLLanguageCode
Dictation
recognizerConfigs
checkerTimes
checkerConfig
checkerType
triggerEndSampleCount
triggerStartSampleCount
Missing keys in context
-[SLProgressiveCheckerContext initWithContext:error:]
Missing config for Progressive checker %@
v32@?0@"NSNumber"8Q16^B24
-[SLProgressiveCheckerAnalyzer initWithConfig:withDelegate:error:]
EAR recognizer init failed for config: %@
-[SLProgressiveCheckerAnalyzer initWithConfig:withDelegate:error:]_block_invoke
Progressive Checker
v32@?0@"_EARSyncSpeechRecognizer"8Q16^B24
-[SLProgressiveCheckerAnalyzer _addAudio:]
-[SLProgressiveCheckerAnalyzer _addAudio:]_block_invoke
-[SLProgressiveCheckerAnalyzer _endAudio]
-[SLProgressiveCheckerAnalyzer _endAudio]_block_invoke
com.apple.sl
Version
InputOpsMap
OutputMap
ModelFile
SupportedInputOrigins
threshold
modelIndex
name
shape
numAsrRecords
topPathScores
topPathNumTokens
latticeMaxScores
latticeMinScores
latticeMeanScores
latticeVarScores
trailingSilence
lrnnScores
lrnnProcessed
inputOrigin
acousticFTMScore
timeFromPrevQuery
speakerIdScore
airpodsConnectedState
boronActivityScore
acousticSpeechActivityScore
attSiriPrevOutputState
multiModalDecisionStage
eosLikelihood
nldaScore
mitigatorScore
mitigatorDecision
mitigatorThreshold
numTokensTopPath
trailingSilenceDuration
theshold
Missing config for Ures %@
-[SLUresMitigator initWithConfig:error:]
Unable to create model with error %@
-[SLUresMitigator dealloc]
-[SLUresMitigator _createInputOriginThresholdMap:]_block_invoke
v32@?0@"NSString"8@"NSDictionary"16^B24
-[SLUresMitigator _createInputOriginThresholdMap:]
-[SLUresMitigator _processInputFeats:]_block_invoke
Failed to create feature with error %@
-[SLUresMitigator _processInputFeats:]
Failed to get output with error %@
requestMitigated
requestNotMitigated
score
decision
decisionValue
-[SLUresMitigator _convertNSArrayToMultiArray:withShape:]
-[SLUresMitigator _convertMultiArrayToNSArray:withShape:]
%s ::: SL logging initialized (%s)
%s Received nil recog candidate, nothing to extract
%s Extracted LRNN Score: %f from Model Version: %{public}@
%s Constructing tokens for speech path %{public}@
%s Adding score %{public}ld for token %{public}@
%s ERR: metaData is nil, defaulting to NO for %{public}@
%s ERR: read metafile %{public}@ failed with %{public}@ - defaulting to NO
%s VocabList size: %lu UnkToken %@
%s Truncation list: %@
%s Token not found, using unk token
%s Tokens: %@
%s inputIds: %@
%s attnMask: %@
%s output: %@
%s %{public}@
%s Created SLAcousticContext: %{public}@
%s Configured buffer size: %f samples, to be flushed after every %lu samples
%s Added checker to analyze %{public}lu samples with config file %{public}@
%s Failed to initialize SLProgressiveCheckerAnalyzer with error %{public}@
%s Initialized Progressive Checkers !
%s Unable to copy from circular buffer !
%s Flushed %lu samples to checker, samples since last flush %lu, total samples in buffer %lu
%s Calling endAudio after feeding %{public}lu samples to recognizer
%s Checker %lu fired, analyzed %{public}lu samples, token %{public}@,  confidence %{public}f
%s Flushed %lu samples to checker
%s All Checkers Finished, analyzed %{public}lu samples, token %{public}@, confidence %{public}f
%s %@
%s Invalid config for %{public}@
%s Threshold map: %{public}@
%s AttFeature: %{public}@ --> %{public}@
%s Value for feature: %{public}@ isn't set, abort model run
%s Failed to convert array to MLMultiArray, not using feature %{public}@ for inference
%s Using software configured threshold: %.3f
%s Failed to convert NSArray with shape %{public}@ to MLMultiArray with err %{public}@
%s Mismatch in output shape, expected: %{public}@ got: %{public}@
SLASRFeatures
SLLRNNFeatures
SLASRFeatureExtractor
SLUtils
SLBertClassifierResult
SLBertClassifier
SLProgressiveCheckerContext
SLProgressiveCheckerResult
SLProgressiveCheckerAnalyzer
SLUresMitigatorResult
SLUresMitigatorIpFeats
SLUresMitigator
localeWithLocaleIdentifier:
setLocale:
setDateFormat:
latticePathMaxScores
setLatticePathMaxScores:
latticePathMinScores
setLatticePathMinScores:
latticePathMeanScores
setLatticePathMeanScores:
latticePathVarScores
setLatticePathVarScores:
topLatticePathScores
setTopLatticePathScores:
topLatticePathTokenCount
setTopLatticePathTokenCount:
setSnr:
trailingSilence
setTrailingSilence:
.cxx_destruct
_snr
_trailingSilence
_latticePathMaxScores
_latticePathMinScores
_latticePathMeanScores
_latticePathVarScores
_topLatticePathScores
_topLatticePathTokenCount
T@"NSArray",&,N,V_latticePathMaxScores
T@"NSArray",&,N,V_latticePathMinScores
T@"NSArray",&,N,V_latticePathMeanScores
T@"NSArray",&,N,V_latticePathVarScores
T@"NSArray",&,N,V_topLatticePathScores
TQ,N,V_topLatticePathTokenCount
Tf,N,V_snr
Tf,N,V_trailingSilence
lrnnScore
setLrnnScore:
lrnnProcessed
setLrnnProcessed:
_lrnnProcessed
_lrnnScore
Tf,N,V_lrnnScore
TB,N,V_lrnnProcessed
recognition
utterances
count
initWithCapacity:
phrases
_getTokenConfidenceForPath:fromPhrases:
expressionForConstantValue:
arrayWithObject:
expressionForFunction:arguments:
expressionValueWithObject:context:
addObject:
doubleValue
numberWithDouble:
floatValue
_getLastTokenForPath:fromPhrases:
enumerateObjectsUsingBlock:
audioAnalytics
endTime
silenceStartTime
latticeMitigatorResult
score
threshold
version
interpretationIndices
lastObject
unsignedIntegerValue
interpretations
objectAtIndex:
tokens
firstObject
array
dictionaryRepresentation
objectAtIndexedSubscript:
addObjectsFromArray:
confidenceScore
numberWithInteger:
text
countByEnumeratingWithState:objects:count:
removeSpaceBefore
removeSpaceAfter
appendString:
extractASRFaturesFrom:
extractLRNNFaturesFrom:
getBestSpeechRecognitionTextFromPackage:
dataWithContentsOfFile:
JSONObjectWithData:options:error:
decodeJsonFromFile:
init
dictionary
addEntriesFromDictionary:
setObject:forKey:
initWithScore:assetVersion:extractedFeats:
domainProb
assetVersion
extractedFeats
_domainProb
_assetVersion
_extractedFeats
T@"NSArray",R,N,V_domainProb
T@"NSString",R,N,V_assetVersion
T@"NSDictionary",R,N,V_extractedFeats
stringWithFormat:
dictionaryWithObjects:forKeys:count:
errorWithDomain:code:userInfo:
containsString:
stringByDeletingLastPathComponent
objectForKeyedSubscript:
stringByAppendingPathComponent:
_readVocabFromFile:
boolValue
fileURLWithPath:
setComputeUnits:
modelWithContentsOfURL:configuration:error:
processInputText:
_normalizeText:
_wordPieceTokenizer:
_createInputIdsAndRunModel:
stringWithContentsOfFile:encoding:error:
newlineCharacterSet
componentsSeparatedByCharactersInSet:
lowercaseString
length
substringWithRange:
whitespaceCharacterSet
characterAtIndex:
characterIsMember:
_isCharPunctuation:
isEqualToString:
numberWithUnsignedInteger:
arrayWithObjects:count:
initWithShape:dataType:error:
indexOfObject:
numberWithFloat:
setObject:atIndexedSubscript:
initWithDictionary:error:
predictionFromFeatures:error:
featureValueForName:
multiArrayValue
_convert1dMLMultiArrayToNSArray:
_splitOnPunctuation:
stringByAppendingString:
containsObject:
shape
initWithConfig:error:locale:
processSpeechPackage:
vocab
setVocab:
unkToken
setUnkToken:
maxInputCharsPerWord
setMaxInputCharsPerWord:
maxNumTokens
setMaxNumTokens:
bertModel
setBertModel:
setAssetVersion:
shouldAppendLeadingText
setShouldAppendLeadingText:
numLeadingTokens
setNumLeadingTokens:
outputNodeName
setOutputNodeName:
truncationTokenList
setTruncationTokenList:
_shouldAppendLeadingText
_vocab
_unkToken
_maxInputCharsPerWord
_maxNumTokens
_bertModel
_numLeadingTokens
_outputNodeName
_truncationTokenList
T@"NSArray",&,N,V_vocab
T@"NSString",&,N,V_unkToken
TQ,N,V_maxInputCharsPerWord
TQ,N,V_maxNumTokens
T@"MLModel",&,N,V_bertModel
T@"NSString",&,N,V_assetVersion
TB,N,V_shouldAppendLeadingText
TQ,N,V_numLeadingTokens
T@"NSString",&,N,V_outputNodeName
T@"NSArray",&,N,V_truncationTokenList
initWithContext:error:
audioOption
vtei
invocationType
locale
_audioOption
_vtei
_invocationType
_locale
TQ,R,N,V_audioOption
T@"NSDictionary",R,N,V_vtei
TQ,R,N,V_invocationType
T@"NSString",R,N,V_locale
initWithScore:ofType:analyzedSamples:detailedResults:
resultType
analyzedSamples
detailedResult
_score
_resultType
_analyzedSamples
_detailedResult
TQ,R,N,V_resultType
TQ,R,N,V_analyzedSamples
Tf,R,N,V_score
T@"NSArray",R,N,V_detailedResult
objectForKey:
unsignedIntValue
initWithNumChannels:recordingDuration:samplingRate:
initWithConfiguration:
localizedDescription
_startNewRequestWithContext:
_addAudio:
_endAudio
resetWithSamplingRate:language:taskType:userId:sessionId:deviceId:farField:audioSource:maxAudioBufferSizeSeconds:
reset
bytes
addSamples:numSamples:
sampleCount
bufferLength
copyBufferWithNumSamplesCopiedIn:
resultsWithAddedFloatAudio:numberOfSamples:taskName:
resultsWithAddedAudio:numberOfSamples:taskName:
resultsWithEndedAudio
removeObjectAtIndex:
confidence
tokenName
analyzer:hasPartialResult:
indexSetWithIndex:
removeObjectsAtIndexes:
copybufferFrom:to:
removeObject:
analyzer:hasFinalResult:
initWithConfig:withDelegate:error:
startNewRequestWithContext:
addAudio:
endAudio
_activeRecognizers
_context
_checkerEndSamples
_analyzedSamplesSoFar
_latestScore
_queue
_delegate
_checkerType
_circBuffer
_numSamplesAddedToBufferSinceLastFlush
_numSamplesInStride
initWithScore:decision:decisionLevel:detailedResults:extractedFeats:
didMitigate
decisionLevel
_didMitigate
_threshold
_decisionLevel
TB,R,N,V_didMitigate
Td,R,N,V_decisionLevel
Tf,R,N,V_threshold
T@"NSDictionary",R,N,V_detailedResult
speechPackage
setSpeechPackage:
inputOrigin
setInputOrigin:
acousticFTMScores
setAcousticFTMScores:
boronScore
setBoronScore:
speakerIDScore
setSpeakerIDScore:
didDetectSpeechActivity
setDidDetectSpeechActivity:
isAirpodsConnected
setIsAirpodsConnected:
timeSinceLastQuery
setTimeSinceLastQuery:
decisionStage
setDecisionStage:
prevStageOutput
setPrevStageOutput:
eosLikelihood
setEosLikelihood:
nldaMetaInfo
setNldaMetaInfo:
nldaScore
setNldaScore:
_didDetectSpeechActivity
_isAirpodsConnected
_speechPackage
_inputOrigin
_acousticFTMScores
_boronScore
_speakerIDScore
_timeSinceLastQuery
_decisionStage
_prevStageOutput
_eosLikelihood
_nldaMetaInfo
_nldaScore
T@"AFSpeechPackage",&,N,V_speechPackage
T@"NSNumber",&,N,V_inputOrigin
T@"NSNumber",&,N,V_acousticFTMScores
T@"NSNumber",&,N,V_boronScore
T@"NSNumber",&,N,V_speakerIDScore
TB,N,V_didDetectSpeechActivity
TB,N,V_isAirpodsConnected
Td,N,V_timeSinceLastQuery
TQ,N,V_decisionStage
T@"NSNumber",&,N,V_prevStageOutput
T@"NSNumber",&,N,V_eosLikelihood
T@"NSDictionary",&,N,V_nldaMetaInfo
T@"NSNumber",&,N,V_nldaScore
setUsesCPUOnly:
_createInputOriginThresholdMap:
dealloc
_processInputFeats:
enumerateKeysAndObjectsUsingBlock:
numberWithBool:
valueForKey:
_convertNSArrayToMultiArray:withShape:
predictionFromFeatures:options:error:
_convertMultiArrayToNSArray:withShape:
setObject:forKeyedSubscript:
getTranscriptionForSpeechPackage:
initWithConfig:error:
processInputFeats:completion:
_uresModel
_options
_inputOpsMap
_outputMap
_thresholdMap
_version
@16@0:8
v24@0:8@16
Q16@0:8
v24@0:8Q16
f16@0:8
v20@0:8f16
v16@0:8
@"NSArray"
B16@0:8
v20@0:8B16
@24@0:8@16
@32@0:8@16@24
@40@0:8@16@24@32
@"NSString"
@"NSDictionary"
@40@0:8@16^@24@32
B20@0:8S16
@"MLModel"
@32@0:8@16^@24
@44@0:8f16Q20Q28@36
@40@0:8@16@24^@32
@"NSMutableArray"
@"SLProgressiveCheckerContext"
@"NSObject<OS_dispatch_queue>"
@"<SLProgressiveCheckerAnalyzerDelegate>"
@"CSAudioCircularBuffer"
@48@0:8f16B20d24@32@40
d16@0:8
v24@0:8d16
@"AFSpeechPackage"
@"NSNumber"
v32@0:8@16@?24
@"MLPredictionOptions"
@"NSMutableDictionary"
@(#)PROGRAM:SiriLiminal  PROJECT:CoreSpeech-
com.apple.SiriLiminal
Framework
v8@?0
::: Initializing SiriLiminal logging...
en_US_POSIX
yyyyMMdd-HHmmss
SLLogInitIfNeeded_block_invoke
gitrelno_unavailable
+[SLASRFeatureExtractor extractASRFaturesFrom:]
max:
min:
stddev:
average:
v32@?0@"AFSpeechUtterance"8Q16^B24
+[SLASRFeatureExtractor extractLRNNFaturesFrom:]
+[SLASRFeatureExtractor _getTokenConfidenceForPath:fromPhrases:]
v32@?0@"AFSpeechPhrase"8Q16^B24
+[SLASRFeatureExtractor _getTokenConfidenceForPath:fromPhrases:]_block_invoke_2
v32@?0@"AFSpeechToken"8Q16^B24
+[SLUtils decodeJsonFromFile:]
version
vocabFile
unkToken
modelFile
leadingText
outputNodeName
truncationList
[SEP]
[CLS]
empty
domainProb
assetVersion
Missing config for Bert Classifier %@
reason
Non en locales supported yet
default
Missing output name for Bert Classifier %@
hey siri
-[SLBertClassifier initWithConfig:error:locale:]
v32@?0@"NSString"8Q16^B24
-[SLBertClassifier _createInputIdsAndRunModel:]_block_invoke
-[SLBertClassifier _createInputIdsAndRunModel:]
minibatch_input_ids
minibatch_attention_mask
minibatch_token_type_ids
outputTokens
attnMask
inputIds
tokenTypeIds
SLInvocationType
SLVoiceTriggerEventInfo
SLAudioSourceOption
SLLanguageCode
Dictation
recognizerConfigs
checkerTimes
checkerConfig
checkerType
triggerEndSampleCount
triggerStartSampleCount
Missing keys in context
-[SLProgressiveCheckerContext initWithContext:error:]
Missing config for Progressive checker %@
v32@?0@"NSNumber"8Q16^B24
-[SLProgressiveCheckerAnalyzer initWithConfig:withDelegate:error:]
EAR recognizer init failed for config: %@
-[SLProgressiveCheckerAnalyzer initWithConfig:withDelegate:error:]_block_invoke
Progressive Checker
v32@?0@"_EARSyncSpeechRecognizer"8Q16^B24
-[SLProgressiveCheckerAnalyzer _addAudio:]
-[SLProgressiveCheckerAnalyzer _addAudio:]_block_invoke
-[SLProgressiveCheckerAnalyzer _endAudio]
-[SLProgressiveCheckerAnalyzer _endAudio]_block_invoke_2
com.apple.sl
Version
InputOpsMap
OutputMap
ModelFile
SupportedInputOrigins
threshold
modelIndex
name
shape
numAsrRecords
topPathScores
topPathNumTokens
latticeMaxScores
latticeMinScores
latticeMeanScores
latticeVarScores
trailingSilence
lrnnScores
lrnnProcessed
inputOrigin
acousticFTMScore
timeFromPrevQuery
speakerIdScore
airpodsConnectedState
boronActivityScore
acousticSpeechActivityScore
attSiriPrevOutputState
multiModalDecisionStage
eosLikelihood
nldaScore
mitigatorScore
mitigatorDecision
mitigatorThreshold
numTokensTopPath
trailingSilenceDuration
theshold
Missing config for Ures %@
-[SLUresMitigator initWithConfig:error:]
Unable to create model with error %@
-[SLUresMitigator dealloc]
-[SLUresMitigator _createInputOriginThresholdMap:]_block_invoke
v32@?0@"NSString"8@"NSDictionary"16^B24
-[SLUresMitigator _createInputOriginThresholdMap:]
-[SLUresMitigator _processInputFeats:]_block_invoke
Failed to create feature with error %@
-[SLUresMitigator _processInputFeats:]
Failed to get output with error %@
requestMitigated
requestNotMitigated
score
decision
decisionValue
-[SLUresMitigator _convertNSArrayToMultiArray:withShape:]
-[SLUresMitigator _convertMultiArrayToNSArray:withShape:]
%s ::: SL logging initialized (%s)
%s Received nil recog candidate, nothing to extract
%s Extracted LRNN Score: %f from Model Version: %{public}@
%s Constructing tokens for speech path %{public}@
%s Adding score %{public}ld for token %{public}@
%s ERR: metaData is nil, defaulting to NO for %{public}@
%s ERR: read metafile %{public}@ failed with %{public}@ - defaulting to NO
%s VocabList size: %lu UnkToken %@
%s Truncation list: %@
%s Token not found, using unk token
%s Tokens: %@
%s inputIds: %@
%s attnMask: %@
%s output: %@
%s %{public}@
%s Created SLAcousticContext: %{public}@
%s Configured buffer size: %f samples, to be flushed after every %lu samples
%s Added checker to analyze %{public}lu samples with config file %{public}@
%s Failed to initialize SLProgressiveCheckerAnalyzer with error %{public}@
%s Initialized Progressive Checkers !
%s Unable to copy from circular buffer !
%s Flushed %lu samples to checker, samples since last flush %lu, total samples in buffer %lu
%s Calling endAudio after feeding %{public}lu samples to recognizer
%s Checker %lu fired, analyzed %{public}lu samples, token %{public}@,  confidence %{public}f
%s Flushed %lu samples to checker
%s All Checkers Finished, analyzed %{public}lu samples, token %{public}@, confidence %{public}f
%s %@
%s Invalid config for %{public}@
%s Threshold map: %{public}@
%s AttFeature: %{public}@ --> %{public}@
%s Value for feature: %{public}@ isn't set, abort model run
%s Failed to convert array to MLMultiArray, not using feature %{public}@ for inference
%s Using software configured threshold: %.3f
%s Failed to convert NSArray with shape %{public}@ to MLMultiArray with err %{public}@
%s Mismatch in output shape, expected: %{public}@ got: %{public}@
SLASRFeatures
SLLRNNFeatures
SLASRFeatureExtractor
SLUtils
SLBertClassifierResult
SLBertClassifier
SLProgressiveCheckerContext
SLProgressiveCheckerResult
SLProgressiveCheckerAnalyzer
SLUresMitigatorResult
SLUresMitigatorIpFeats
SLUresMitigator
JSONObjectWithData:options:error:
whitespaceCharacterSet
version
valueForKey:
utterances
unsignedIntegerValue
unsignedIntValue
tokens
tokenName
text
substringWithRange:
stringWithFormat:
stringWithContentsOfFile:encoding:error:
stringByDeletingLastPathComponent
stringByAppendingString:
stringByAppendingPathComponent:
silenceStartTime
shape
setUsesCPUOnly:
setObject:forKeyedSubscript:
setObject:forKey:
setObject:atIndexedSubscript:
setLocale:
setDateFormat:
setComputeUnits:
sampleCount
resultsWithEndedAudio
resultsWithAddedFloatAudio:numberOfSamples:taskName:
resultsWithAddedAudio:numberOfSamples:taskName:
resetWithSamplingRate:language:taskType:userId:sessionId:deviceId:farField:audioSource:maxAudioBufferSizeSeconds:
reset
removeSpaceBefore
removeSpaceAfter
removeObjectsAtIndexes:
removeObjectAtIndex:
removeObject:
recognition
predictionFromFeatures:options:error:
predictionFromFeatures:error:
phrases
objectForKeyedSubscript:
objectForKey:
objectAtIndexedSubscript:
objectAtIndex:
numberWithUnsignedInteger:
numberWithInteger:
numberWithFloat:
numberWithDouble:
numberWithBool:
newlineCharacterSet
multiArrayValue
modelWithContentsOfURL:configuration:error:
lowercaseString
localizedDescription
localeWithLocaleIdentifier:
length
latticeMitigatorResult
lastObject
isEqualToString:
interpretations
interpretationIndices
initWithShape:dataType:error:
initWithNumChannels:recordingDuration:samplingRate:
initWithDictionary:error:
initWithConfiguration:
initWithCapacity:
indexSetWithIndex:
indexOfObject:
floatValue
firstObject
fileURLWithPath:
featureValueForName:
expressionValueWithObject:context:
expressionForFunction:arguments:
expressionForConstantValue:
errorWithDomain:code:userInfo:
enumerateObjectsUsingBlock:
enumerateKeysAndObjectsUsingBlock:
endTime
doubleValue
dictionaryWithObjects:forKeys:count:
dictionary
dataWithContentsOfFile:
countByEnumeratingWithState:objects:count:
count
copybufferFrom:to:
copyBufferWithNumSamplesCopiedIn:
containsString:
containsObject:
confidenceScore
confidence
componentsSeparatedByCharactersInSet:
characterIsMember:
characterAtIndex:
bytes
bufferLength
boolValue
audioAnalytics
arrayWithObjects:count:
arrayWithObject:
array
appendString:
addSamples:numSamples:
addObjectsFromArray:
addObject:
addEntriesFromDictionary:
latticePathMaxScores
setLatticePathMaxScores:
latticePathMinScores
setLatticePathMinScores:
latticePathMeanScores
setLatticePathMeanScores:
latticePathVarScores
setLatticePathVarScores:
topLatticePathScores
setTopLatticePathScores:
topLatticePathTokenCount
setTopLatticePathTokenCount:
setSnr:
trailingSilence
setTrailingSilence:
.cxx_destruct
_snr
_trailingSilence
_latticePathMaxScores
_latticePathMinScores
_latticePathMeanScores
_latticePathVarScores
_topLatticePathScores
_topLatticePathTokenCount
T@"NSArray",&,N,V_latticePathMaxScores
T@"NSArray",&,N,V_latticePathMinScores
T@"NSArray",&,N,V_latticePathMeanScores
T@"NSArray",&,N,V_latticePathVarScores
T@"NSArray",&,N,V_topLatticePathScores
TQ,N,V_topLatticePathTokenCount
Tf,N,V_snr
Tf,N,V_trailingSilence
lrnnScore
setLrnnScore:
lrnnProcessed
setLrnnProcessed:
_lrnnProcessed
_lrnnScore
Tf,N,V_lrnnScore
TB,N,V_lrnnProcessed
extractASRFaturesFrom:
extractLRNNFaturesFrom:
_getLastTokenForPath:fromPhrases:
_getTokenConfidenceForPath:fromPhrases:
getBestSpeechRecognitionTextFromPackage:
decodeJsonFromFile:
init
initWithScore:assetVersion:extractedFeats:
dictionaryRepresentation
domainProb
assetVersion
extractedFeats
_domainProb
_assetVersion
_extractedFeats
T@"NSArray",R,N,V_domainProb
T@"NSString",R,N,V_assetVersion
T@"NSDictionary",R,N,V_extractedFeats
initWithConfig:error:locale:
processSpeechPackage:
processInputText:
_readVocabFromFile:
_normalizeText:
_createInputIdsAndRunModel:
_isCharPunctuation:
_splitOnPunctuation:
_wordPieceTokenizer:
_convert1dMLMultiArrayToNSArray:
vocab
setVocab:
unkToken
setUnkToken:
maxInputCharsPerWord
setMaxInputCharsPerWord:
maxNumTokens
setMaxNumTokens:
bertModel
setBertModel:
setAssetVersion:
shouldAppendLeadingText
setShouldAppendLeadingText:
numLeadingTokens
setNumLeadingTokens:
outputNodeName
setOutputNodeName:
truncationTokenList
setTruncationTokenList:
_shouldAppendLeadingText
_vocab
_unkToken
_maxInputCharsPerWord
_maxNumTokens
_bertModel
_numLeadingTokens
_outputNodeName
_truncationTokenList
T@"NSArray",&,N,V_vocab
T@"NSString",&,N,V_unkToken
TQ,N,V_maxInputCharsPerWord
TQ,N,V_maxNumTokens
T@"MLModel",&,N,V_bertModel
T@"NSString",&,N,V_assetVersion
TB,N,V_shouldAppendLeadingText
TQ,N,V_numLeadingTokens
T@"NSString",&,N,V_outputNodeName
T@"NSArray",&,N,V_truncationTokenList
initWithContext:error:
audioOption
vtei
invocationType
locale
_audioOption
_vtei
_invocationType
_locale
TQ,R,N,V_audioOption
T@"NSDictionary",R,N,V_vtei
TQ,R,N,V_invocationType
T@"NSString",R,N,V_locale
initWithScore:ofType:analyzedSamples:detailedResults:
resultType
analyzedSamples
score
detailedResult
_score
_resultType
_analyzedSamples
_detailedResult
TQ,R,N,V_resultType
TQ,R,N,V_analyzedSamples
Tf,R,N,V_score
T@"NSArray",R,N,V_detailedResult
analyzer:hasPartialResult:
analyzer:hasFinalResult:
initWithConfig:withDelegate:error:
startNewRequestWithContext:
addAudio:
endAudio
_startNewRequestWithContext:
_addAudio:
_endAudio
_activeRecognizers
_context
_checkerEndSamples
_analyzedSamplesSoFar
_latestScore
_queue
_delegate
_checkerType
_circBuffer
_numSamplesAddedToBufferSinceLastFlush
_numSamplesInStride
initWithScore:decision:decisionLevel:detailedResults:extractedFeats:
didMitigate
decisionLevel
threshold
_didMitigate
_threshold
_decisionLevel
TB,R,N,V_didMitigate
Td,R,N,V_decisionLevel
Tf,R,N,V_threshold
T@"NSDictionary",R,N,V_detailedResult
speechPackage
setSpeechPackage:
inputOrigin
setInputOrigin:
acousticFTMScores
setAcousticFTMScores:
boronScore
setBoronScore:
speakerIDScore
setSpeakerIDScore:
didDetectSpeechActivity
setDidDetectSpeechActivity:
isAirpodsConnected
setIsAirpodsConnected:
timeSinceLastQuery
setTimeSinceLastQuery:
decisionStage
setDecisionStage:
prevStageOutput
setPrevStageOutput:
eosLikelihood
setEosLikelihood:
nldaMetaInfo
setNldaMetaInfo:
nldaScore
setNldaScore:
_didDetectSpeechActivity
_isAirpodsConnected
_speechPackage
_inputOrigin
_acousticFTMScores
_boronScore
_speakerIDScore
_timeSinceLastQuery
_decisionStage
_prevStageOutput
_eosLikelihood
_nldaMetaInfo
_nldaScore
T@"AFSpeechPackage",&,N,V_speechPackage
T@"NSNumber",&,N,V_inputOrigin
T@"NSNumber",&,N,V_acousticFTMScores
T@"NSNumber",&,N,V_boronScore
T@"NSNumber",&,N,V_speakerIDScore
TB,N,V_didDetectSpeechActivity
TB,N,V_isAirpodsConnected
Td,N,V_timeSinceLastQuery
TQ,N,V_decisionStage
T@"NSNumber",&,N,V_prevStageOutput
T@"NSNumber",&,N,V_eosLikelihood
T@"NSDictionary",&,N,V_nldaMetaInfo
T@"NSNumber",&,N,V_nldaScore
dealloc
getTranscriptionForSpeechPackage:
initWithConfig:error:
processInputFeats:completion:
_createInputOriginThresholdMap:
_processInputFeats:
_convertNSArrayToMultiArray:withShape:
_convertMultiArrayToNSArray:withShape:
_uresModel
_options
_inputOpsMap
_outputMap
_thresholdMap
_version
@16@0:8
v24@0:8@16
Q16@0:8
v24@0:8Q16
f16@0:8
v20@0:8f16
v16@0:8
@"NSArray"
B16@0:8
v20@0:8B16
@24@0:8@16
@32@0:8@16@24
@40@0:8@16@24@32
@"NSString"
@"NSDictionary"
@40@0:8@16^@24@32
B20@0:8S16
@"MLModel"
@32@0:8@16^@24
@44@0:8f16Q20Q28@36
@40@0:8@16@24^@32
@"NSMutableArray"
@"SLProgressiveCheckerContext"
@"NSObject<OS_dispatch_queue>"
@"<SLProgressiveCheckerAnalyzerDelegate>"
@"CSAudioCircularBuffer"
@48@0:8f16B20d24@32@40
d16@0:8
v24@0:8d16
@"AFSpeechPackage"
@"NSNumber"
v32@0:8@16@?24
@"MLPredictionOptions"
@"NSMutableDictionary"
