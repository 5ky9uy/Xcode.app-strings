@(#)PROGRAM:SiriLiminal  PROJECT:CoreSpeech-
minibatch_token_type_ids
minibatch_attention_mask
minibatch_input_ids
minibatch_padding_mask
input_origin
v32@?0@"NSString"8@16^B24
-[SLODLDClassifierV1 processEncodedTokens:]
-[SLODLDClassifierV1 _constructFeatureDictionary:]
v32@?0@"NSNumber"8Q16^B24
-[SLODLDClassifierV1 _constructFeatureDictionary:]_block_invoke
v32@?0@"NSString"8@"MLFeatureDescription"16^B24
com.apple.SiriLiminal
Framework
v8@?0
::: Initializing SiriLiminal logging...
en_US_POSIX
yyyyMMdd-HHmmss
SLLogInitIfNeeded_block_invoke
gitrelno_unavailable
rules
patternMatch
replaceWith
Missing config for regex matcher %@
reason
-[SLRegexMatcher initWithConfigFile:]
-[SLRegexMatcher _compileRegexRulesForPatterns:]_block_invoke
v32@?0@"NSDictionary"8Q16^B24
v32@?0@"NSRegularExpression"8Q16^B24
-[SLRegexMatcher applyPreprocessingOnUtt:]
+[SLASRFeatureExtractor extractASRFaturesFrom:]
max:
min:
stddev:
average:
v32@?0@"AFSpeechUtterance"8Q16^B24
+[SLASRFeatureExtractor extractLRNNFaturesFrom:]
+[SLASRFeatureExtractor _getTokenConfidenceForPath:fromPhrases:]
v32@?0@"AFSpeechPhrase"8Q16^B24
+[SLASRFeatureExtractor _getTokenConfidenceForPath:fromPhrases:]_block_invoke
v32@?0@"AFSpeechToken"8Q16^B24
-[SLSentencePieceModule initWithConfigFile:]
-[SLSentencePieceModule dealloc]
+[SLUtils decodeJsonFromFile:]
version
inputType
pipeline
outputSpecs
preprocessing
tokenizer
model
regexMapConfig
spmModel
modelFile
outputNodeName
PostITN
PreITN
-[SLODLDConfigDecoder initWithConfigFile:]
vocabFile
unkToken
leadingText
truncationList
[SEP]
[CLS]
empty
domainProb
assetVersion
Missing config for Bert Classifier %@
Non en locales supported yet
default
Missing output name for Bert Classifier %@
hey siri
-[SLBertClassifier initWithConfig:error:locale:]
v32@?0@"NSString"8Q16^B24
-[SLBertClassifier _createInputIdsAndRunModel:]_block_invoke
-[SLBertClassifier _createInputIdsAndRunModel:]
outputTokens
attnMask
inputIds
tokenTypeIds
-[SLODLDProcessor initWithConfigFile:error:]
Unable to decode config file %@
-[SLODLDProcessor _setupPipelineComponentsUsingConfig:]
Unable to create tokenizer
Unable to create ODLD model
-[SLODLDProcessor processInputUtterance:]
SLInvocationType
SLVoiceTriggerEventInfo
SLAudioSourceOption
SLLanguageCode
Dictation
recognizerConfigs
checkerTimes
checkerConfig
checkerType
triggerEndSampleCount
triggerStartSampleCount
Missing keys in context
-[SLProgressiveCheckerContext initWithContext:error:]
Missing config for Progressive checker %@
-[SLProgressiveCheckerAnalyzer initWithConfig:withDelegate:error:]
EAR recognizer init failed for config: %@
-[SLProgressiveCheckerAnalyzer initWithConfig:withDelegate:error:]_block_invoke
Progressive Checker
v32@?0@"_EARSyncSpeechRecognizer"8Q16^B24
-[SLProgressiveCheckerAnalyzer _addAudio:]
-[SLProgressiveCheckerAnalyzer _addAudio:]_block_invoke
-[SLProgressiveCheckerAnalyzer _endAudio]
-[SLProgressiveCheckerAnalyzer _endAudio]_block_invoke
com.apple.sl
Version
InputOpsMap
OutputMap
ModelFile
SupportedInputOrigins
threshold
modelIndex
name
shape
numAsrRecords
topPathScores
topPathNumTokens
latticeMaxScores
latticeMinScores
latticeMeanScores
latticeVarScores
trailingSilence
lrnnScores
lrnnProcessed
inputOrigin
acousticFTMScore
timeFromPrevQuery
speakerIdScore
airpodsConnectedState
boronActivityScore
acousticSpeechActivityScore
attSiriPrevOutputState
multiModalDecisionStage
eosLikelihood
nldaScore
mitigatorScore
mitigatorDecision
mitigatorThreshold
numTokensTopPath
trailingSilenceDuration
theshold
Missing config for Ures %@
-[SLUresMitigator initWithConfig:error:]
Unable to create model with error %@
-[SLUresMitigator dealloc]
-[SLUresMitigator _createInputOriginThresholdMap:]_block_invoke
v32@?0@"NSString"8@"NSDictionary"16^B24
-[SLUresMitigator _createInputOriginThresholdMap:]
-[SLUresMitigator _processInputFeats:]_block_invoke
Failed to create feature with error %@
-[SLUresMitigator _processInputFeats:]
Failed to get output with error %@
requestMitigated
requestNotMitigated
score
decision
decisionValue
-[SLUresMitigator _convertNSArrayToMultiArray:withShape:]
-[SLUresMitigator _convertMultiArrayToNSArray:withShape:]
%s Unable to create feature dict with error %@ from %@
%s Expected 2D array, got %lu, not running model
%s Unable to extract shape of feature %@
%s Tokens: %@
%s inputIds: %@
%s attnMask: %@
%s ::: SL logging initialized (%s)
%s %@
%s Empty pattern, ignore
%s Unable to decode pattern %@ with err %@
%s Input to regex: %@
%s Output of regex: %@
%s Received nil recog candidate, nothing to extract
%s Extracted LRNN Score: %f from Model Version: %{public}@
%s Constructing tokens for speech path %{public}@
%s Adding score %{public}ld for token %{public}@
%s SPM model created
%s dealloc
%s ERR: metaData is nil, defaulting to NO for %{public}@
%s ERR: read metafile %{public}@ failed with %{public}@ - defaulting to NO
%s Decoded config at path: %@
%s VocabList size: %lu UnkToken %@
%s Truncation list: %@
%s Token not found, using unk token
%s Num Tokens - %lu  exceeded model input length - %lu
%s output: %@
%s Reading from configFile %@
%s Created ODLD Processor Pipelines
%s PreProcessorType: %lu PreProcessor %@
%s TokenizerType: %lu Tokenizer %@
%s Classifier %@
%s Empty input
%s output %f
%s %{public}@
%s Created SLAcousticContext: %{public}@
%s Configured buffer size: %f samples, to be flushed after every %lu samples
%s Added checker to analyze %{public}lu samples with config file %{public}@
%s Failed to initialize SLProgressiveCheckerAnalyzer with error %{public}@
%s Initialized Progressive Checkers !
%s Unable to copy from circular buffer !
%s Flushed %lu samples to checker, samples since last flush %lu, total samples in buffer %lu
%s Calling endAudio after feeding %{public}lu samples to recognizer
%s Checker %lu fired, analyzed %{public}lu samples, token %{public}@,  confidence %{public}f
%s Flushed %lu samples to checker
%s All Checkers Finished, analyzed %{public}lu samples, token %{public}@, confidence %{public}f
%s Invalid config for %{public}@
%s Threshold map: %{public}@
%s AttFeature: %{public}@ --> %{public}@
%s Value for feature: %{public}@ isn't set, abort model run
%s Failed to convert array to MLMultiArray, not using feature %{public}@ for inference
%s Using software configured threshold: %.3f
%s Failed to convert NSArray with shape %{public}@ to MLMultiArray with err %{public}@
%s Mismatch in output shape, expected: %{public}@ got: %{public}@
SLODLDClassifierV1
SLODLDModelComponent
NSObject
SLRegexMatcher
SLODLDPreProcessingComponent
SLASRFeatures
SLLRNNFeatures
SLASRFeatureExtractor
SLSentencePieceModule
SLODLDTokenizerComponent
SLUtils
SLODLDProcessorResult
SLODLDConfigDecoder
SLBertClassifierResult
SLBertClassifier
SLODLDProcessor
SLProgressiveCheckerContext
SLProgressiveCheckerResult
SLProgressiveCheckerAnalyzer
SLUresMitigatorResult
SLUresMitigatorIpFeats
SLUresMitigator
init
fileURLWithPath:
setComputeUnits:
modelWithContentsOfURL:configuration:error:
_extractModelSpecs
modelDescription
inputDescriptionsByName
outputDescriptionsByName
array
addObject:
enumerateKeysAndObjectsUsingBlock:
_constructFeatureDictionary:
initWithDictionary:error:
localizedDescription
predictionFromFeatures:error:
objectAtIndexedSubscript:
featureValueForName:
multiArrayValue
_convert1dMLMultiArrayToNSArray:
firstObject
floatValue
objectForKeyedSubscript:
multiArrayConstraint
shape
count
unsignedIntegerValue
initWithShape:dataType:error:
setObject:atIndexedSubscript:
numberWithFloat:
enumerateObjectsUsingBlock:
dictionary
isEqualToString:
setObject:forKey:
lastObject
isEqual:
class
self
performSelector:
performSelector:withObject:
performSelector:withObject:withObject:
isProxy
isKindOfClass:
isMemberOfClass:
conformsToProtocol:
respondsToSelector:
retain
release
autorelease
retainCount
zone
hash
superclass
description
debugDescription
TQ,R
T#,R
T@"NSString",R,C
initWithConfigFile:
processEncodedTokens:
odldClassifier
setOdldClassifier:
inputSpecs
setInputSpecs:
outputNodes
setOutputNodes:
inputOrigin
setInputOrigin:
.cxx_destruct
_odldClassifier
_inputSpecs
_outputNodes
_inputOrigin
T@"MLModel",&,N,V_odldClassifier
T@"NSDictionary",&,N,V_inputSpecs
T@"NSArray",&,N,V_outputNodes
T@"NSNumber",&,N,V_inputOrigin
localeWithLocaleIdentifier:
setLocale:
setDateFormat:
decodeJsonFromFile:
stringWithFormat:
dictionaryWithObjects:forKeys:count:
errorWithDomain:code:userInfo:
_compileRegexRulesForPatterns:
length
regularExpressionWithPattern:options:error:
mutableCopy
replaceMatchesInString:options:range:withTemplate:
applyPreprocessingOnUtt:
regexMatchingRules
setRegexMatchingRules:
replaceStrings
setReplaceStrings:
_regexMatchingRules
_replaceStrings
T@"NSArray",&,N,V_regexMatchingRules
T@"NSArray",&,N,V_replaceStrings
latticePathMaxScores
setLatticePathMaxScores:
latticePathMinScores
setLatticePathMinScores:
latticePathMeanScores
setLatticePathMeanScores:
latticePathVarScores
setLatticePathVarScores:
topLatticePathScores
setTopLatticePathScores:
topLatticePathTokenCount
setTopLatticePathTokenCount:
setSnr:
trailingSilence
setTrailingSilence:
_snr
_trailingSilence
_latticePathMaxScores
_latticePathMinScores
_latticePathMeanScores
_latticePathVarScores
_topLatticePathScores
_topLatticePathTokenCount
T@"NSArray",&,N,V_latticePathMaxScores
T@"NSArray",&,N,V_latticePathMinScores
T@"NSArray",&,N,V_latticePathMeanScores
T@"NSArray",&,N,V_latticePathVarScores
T@"NSArray",&,N,V_topLatticePathScores
TQ,N,V_topLatticePathTokenCount
Tf,N,V_snr
Tf,N,V_trailingSilence
lrnnScore
setLrnnScore:
lrnnProcessed
setLrnnProcessed:
_lrnnProcessed
_lrnnScore
Tf,N,V_lrnnScore
TB,N,V_lrnnProcessed
recognition
utterances
initWithCapacity:
phrases
_getTokenConfidenceForPath:fromPhrases:
expressionForConstantValue:
arrayWithObject:
expressionForFunction:arguments:
expressionValueWithObject:context:
doubleValue
numberWithDouble:
_getLastTokenForPath:fromPhrases:
audioAnalytics
endTime
silenceStartTime
latticeMitigatorResult
score
threshold
version
interpretationIndices
interpretations
objectAtIndex:
tokens
dictionaryRepresentation
addObjectsFromArray:
confidenceScore
numberWithInteger:
text
countByEnumeratingWithState:objects:count:
removeSpaceBefore
removeSpaceAfter
appendString:
extractASRFaturesFrom:
extractLRNNFaturesFrom:
getBestSpeechRecognitionTextFromPackage:
initWithModelPath:
dealloc
encodeUtterance:
processor
setProcessor:
_processor
T@"EARSentencePieceModule",&,V_processor
dataWithContentsOfFile:
JSONObjectWithData:options:error:
createErrorWithMsg:code:
copy
initWithScore:withVersion:tokens:features:
odldScore
configVersion
features
_odldScore
_configVersion
_tokens
_features
Tf,R,N,V_odldScore
T@"NSString",R,N,V_configVersion
T@"NSArray",R,N,V_tokens
T@"NSDictionary",R,N,V_features
stringByDeletingLastPathComponent
_getValueForKey:categoryKey:
stringByAppendingPathComponent:
getConfigVersion
getInputType
getOutputSpecs
getPreProcessorType
getTokenizerType
getRegexMapConfig
getSPMModelFile
getBertModelFile
getBertModelOutputNodes
resourcePath
setResourcePath:
setDictionary:
_resourcePath
_dictionary
T@"NSString",&,N,V_resourcePath
T@"NSDictionary",&,N,V_dictionary
addEntriesFromDictionary:
initWithScore:assetVersion:extractedFeats:
domainProb
assetVersion
extractedFeats
_domainProb
_assetVersion
_extractedFeats
T@"NSArray",R,N,V_domainProb
T@"NSString",R,N,V_assetVersion
T@"NSDictionary",R,N,V_extractedFeats
containsString:
_readVocabFromFile:
boolValue
processInputText:
_normalizeText:
_wordPieceTokenizer:
_createInputIdsAndRunModel:
stringWithContentsOfFile:encoding:error:
newlineCharacterSet
componentsSeparatedByCharactersInSet:
lowercaseString
substringWithRange:
whitespaceCharacterSet
characterAtIndex:
characterIsMember:
_isCharPunctuation:
numberWithUnsignedInteger:
arrayWithObjects:count:
indexOfObject:
_splitOnPunctuation:
stringByAppendingString:
containsObject:
initWithConfig:error:locale:
processSpeechPackage:
vocab
setVocab:
unkToken
setUnkToken:
maxInputCharsPerWord
setMaxInputCharsPerWord:
maxNumTokens
setMaxNumTokens:
bertModel
setBertModel:
setAssetVersion:
shouldAppendLeadingText
setShouldAppendLeadingText:
numLeadingTokens
setNumLeadingTokens:
outputNodeName
setOutputNodeName:
truncationTokenList
setTruncationTokenList:
_shouldAppendLeadingText
_vocab
_unkToken
_maxInputCharsPerWord
_maxNumTokens
_bertModel
_numLeadingTokens
_outputNodeName
_truncationTokenList
T@"NSArray",&,N,V_vocab
T@"NSString",&,N,V_unkToken
TQ,N,V_maxInputCharsPerWord
TQ,N,V_maxNumTokens
T@"MLModel",&,N,V_bertModel
T@"NSString",&,N,V_assetVersion
TB,N,V_shouldAppendLeadingText
TQ,N,V_numLeadingTokens
T@"NSString",&,N,V_outputNodeName
T@"NSArray",&,N,V_truncationTokenList
_setupPipelineComponentsUsingConfig:
_getPreprocessorForType:withConfig:
_getTokenizerForType:withConfig:
initWithConfigFile:error:
processInputUtterance:
config
setConfig:
preprocessor
setPreprocessor:
tokenizer
setTokenizer:
model
setModel:
_config
_preprocessor
_tokenizer
_model
T@"SLODLDConfigDecoder",&,N,V_config
T@"<SLODLDPreProcessingComponent>",&,N,V_preprocessor
T@"<SLODLDTokenizerComponent>",&,N,V_tokenizer
T@"<SLODLDModelComponent>",&,N,V_model
initWithContext:error:
audioOption
vtei
invocationType
locale
_audioOption
_vtei
_invocationType
_locale
TQ,R,N,V_audioOption
T@"NSDictionary",R,N,V_vtei
TQ,R,N,V_invocationType
T@"NSString",R,N,V_locale
initWithScore:ofType:analyzedSamples:detailedResults:
resultType
analyzedSamples
detailedResult
_score
_resultType
_analyzedSamples
_detailedResult
TQ,R,N,V_resultType
TQ,R,N,V_analyzedSamples
Tf,R,N,V_score
T@"NSArray",R,N,V_detailedResult
objectForKey:
unsignedIntValue
initWithNumChannels:recordingDuration:samplingRate:
initWithConfiguration:
_startNewRequestWithContext:
_addAudio:
_endAudio
resetWithSamplingRate:language:taskType:userId:sessionId:deviceId:farField:audioSource:maxAudioBufferSizeSeconds:
reset
bytes
addSamples:numSamples:
sampleCount
bufferLength
copyBufferWithNumSamplesCopiedIn:
resultsWithAddedFloatAudio:numberOfSamples:taskName:
resultsWithAddedAudio:numberOfSamples:taskName:
resultsWithEndedAudio
removeObjectAtIndex:
confidence
tokenName
analyzer:hasPartialResult:
indexSetWithIndex:
removeObjectsAtIndexes:
copybufferFrom:to:
removeObject:
analyzer:hasFinalResult:
initWithConfig:withDelegate:error:
startNewRequestWithContext:
addAudio:
endAudio
_activeRecognizers
_context
_checkerEndSamples
_analyzedSamplesSoFar
_latestScore
_queue
_delegate
_checkerType
_circBuffer
_numSamplesAddedToBufferSinceLastFlush
_numSamplesInStride
initWithScore:decision:decisionLevel:detailedResults:extractedFeats:
didMitigate
decisionLevel
_didMitigate
_threshold
_decisionLevel
TB,R,N,V_didMitigate
Td,R,N,V_decisionLevel
Tf,R,N,V_threshold
T@"NSDictionary",R,N,V_detailedResult
speechPackage
setSpeechPackage:
acousticFTMScores
setAcousticFTMScores:
boronScore
setBoronScore:
speakerIDScore
setSpeakerIDScore:
didDetectSpeechActivity
setDidDetectSpeechActivity:
isAirpodsConnected
setIsAirpodsConnected:
timeSinceLastQuery
setTimeSinceLastQuery:
decisionStage
setDecisionStage:
prevStageOutput
setPrevStageOutput:
eosLikelihood
setEosLikelihood:
nldaMetaInfo
setNldaMetaInfo:
nldaScore
setNldaScore:
_didDetectSpeechActivity
_isAirpodsConnected
_speechPackage
_acousticFTMScores
_boronScore
_speakerIDScore
_timeSinceLastQuery
_decisionStage
_prevStageOutput
_eosLikelihood
_nldaMetaInfo
_nldaScore
T@"AFSpeechPackage",&,N,V_speechPackage
T@"NSNumber",&,N,V_acousticFTMScores
T@"NSNumber",&,N,V_boronScore
T@"NSNumber",&,N,V_speakerIDScore
TB,N,V_didDetectSpeechActivity
TB,N,V_isAirpodsConnected
Td,N,V_timeSinceLastQuery
TQ,N,V_decisionStage
T@"NSNumber",&,N,V_prevStageOutput
T@"NSNumber",&,N,V_eosLikelihood
T@"NSDictionary",&,N,V_nldaMetaInfo
T@"NSNumber",&,N,V_nldaScore
setUsesCPUOnly:
_createInputOriginThresholdMap:
_processInputFeats:
numberWithBool:
valueForKey:
_convertNSArrayToMultiArray:withShape:
predictionFromFeatures:options:error:
_convertMultiArrayToNSArray:withShape:
setObject:forKeyedSubscript:
getTranscriptionForSpeechPackage:
initWithConfig:error:
processInputFeats:completion:
_uresModel
_options
_inputOpsMap
_outputMap
_thresholdMap
_version
B24@0:8@16
#16@0:8
@16@0:8
@24@0:8:16
@32@0:8:16@24
@40@0:8:16@24@32
B16@0:8
B24@0:8#16
B24@0:8:16
Vv16@0:8
Q16@0:8
^{_NSZone=}16@0:8
B24@0:8@"Protocol"16
@"NSString"16@0:8
@24@0:8@16
f24@0:8@16
@24@0:8@"NSString"16
f24@0:8@"NSArray"16
v16@0:8
v24@0:8@16
@"MLModel"
@"NSDictionary"
@"NSArray"
@"NSNumber"
@"NSString"24@0:8@"NSString"16
v24@0:8Q16
f16@0:8
v20@0:8f16
v20@0:8B16
@32@0:8@16@24
@"NSArray"24@0:8@"NSString"16
@"EARSentencePieceModule"
@32@0:8@16Q24
@44@0:8f16@20@28@36
@"NSString"
@40@0:8@16@24@32
@40@0:8@16^@24@32
B20@0:8S16
@32@0:8@16^@24
@32@0:8Q16@24
@"SLODLDConfigDecoder"
@"<SLODLDPreProcessingComponent>"
@"<SLODLDTokenizerComponent>"
@"<SLODLDModelComponent>"
@44@0:8f16Q20Q28@36
@40@0:8@16@24^@32
@"NSMutableArray"
@"SLProgressiveCheckerContext"
@"NSObject<OS_dispatch_queue>"
@"<SLProgressiveCheckerAnalyzerDelegate>"
@"CSAudioCircularBuffer"
@48@0:8f16B20d24@32@40
d16@0:8
v24@0:8d16
@"AFSpeechPackage"
v32@0:8@16@?24
@"MLPredictionOptions"
@"NSMutableDictionary"
@(#)PROGRAM:SiriLiminal  PROJECT:CoreSpeech-
minibatch_token_type_ids
minibatch_attention_mask
minibatch_input_ids
minibatch_padding_mask
input_origin
v32@?0@"NSString"8@16^B24
-[SLODLDClassifierV1 processEncodedTokens:]
-[SLODLDClassifierV1 _constructFeatureDictionary:]
v32@?0@"NSNumber"8Q16^B24
-[SLODLDClassifierV1 _constructFeatureDictionary:]_block_invoke_2
v32@?0@"NSString"8@"MLFeatureDescription"16^B24
com.apple.SiriLiminal
Framework
v8@?0
::: Initializing SiriLiminal logging...
en_US_POSIX
yyyyMMdd-HHmmss
SLLogInitIfNeeded_block_invoke
gitrelno_unavailable
rules
patternMatch
replaceWith
Missing config for regex matcher %@
reason
-[SLRegexMatcher initWithConfigFile:]
-[SLRegexMatcher _compileRegexRulesForPatterns:]_block_invoke
v32@?0@"NSDictionary"8Q16^B24
v32@?0@"NSRegularExpression"8Q16^B24
-[SLRegexMatcher applyPreprocessingOnUtt:]
+[SLASRFeatureExtractor extractASRFaturesFrom:]
max:
min:
stddev:
average:
v32@?0@"AFSpeechUtterance"8Q16^B24
+[SLASRFeatureExtractor extractLRNNFaturesFrom:]
+[SLASRFeatureExtractor _getTokenConfidenceForPath:fromPhrases:]
v32@?0@"AFSpeechPhrase"8Q16^B24
+[SLASRFeatureExtractor _getTokenConfidenceForPath:fromPhrases:]_block_invoke_2
v32@?0@"AFSpeechToken"8Q16^B24
-[SLSentencePieceModule initWithConfigFile:]
-[SLSentencePieceModule dealloc]
+[SLUtils decodeJsonFromFile:]
version
inputType
pipeline
outputSpecs
preprocessing
tokenizer
model
regexMapConfig
spmModel
modelFile
outputNodeName
PostITN
PreITN
-[SLODLDConfigDecoder initWithConfigFile:]
vocabFile
unkToken
leadingText
truncationList
[SEP]
[CLS]
empty
domainProb
assetVersion
Missing config for Bert Classifier %@
Non en locales supported yet
default
Missing output name for Bert Classifier %@
hey siri
-[SLBertClassifier initWithConfig:error:locale:]
v32@?0@"NSString"8Q16^B24
-[SLBertClassifier _createInputIdsAndRunModel:]_block_invoke
-[SLBertClassifier _createInputIdsAndRunModel:]
outputTokens
attnMask
inputIds
tokenTypeIds
-[SLODLDProcessor initWithConfigFile:error:]
Unable to decode config file %@
-[SLODLDProcessor _setupPipelineComponentsUsingConfig:]
Unable to create tokenizer
Unable to create ODLD model
-[SLODLDProcessor processInputUtterance:]
SLInvocationType
SLVoiceTriggerEventInfo
SLAudioSourceOption
SLLanguageCode
Dictation
recognizerConfigs
checkerTimes
checkerConfig
checkerType
triggerEndSampleCount
triggerStartSampleCount
Missing keys in context
-[SLProgressiveCheckerContext initWithContext:error:]
Missing config for Progressive checker %@
-[SLProgressiveCheckerAnalyzer initWithConfig:withDelegate:error:]
EAR recognizer init failed for config: %@
-[SLProgressiveCheckerAnalyzer initWithConfig:withDelegate:error:]_block_invoke
Progressive Checker
v32@?0@"_EARSyncSpeechRecognizer"8Q16^B24
-[SLProgressiveCheckerAnalyzer _addAudio:]
-[SLProgressiveCheckerAnalyzer _addAudio:]_block_invoke
-[SLProgressiveCheckerAnalyzer _endAudio]
-[SLProgressiveCheckerAnalyzer _endAudio]_block_invoke_2
com.apple.sl
Version
InputOpsMap
OutputMap
ModelFile
SupportedInputOrigins
threshold
modelIndex
name
shape
numAsrRecords
topPathScores
topPathNumTokens
latticeMaxScores
latticeMinScores
latticeMeanScores
latticeVarScores
trailingSilence
lrnnScores
lrnnProcessed
inputOrigin
acousticFTMScore
timeFromPrevQuery
speakerIdScore
airpodsConnectedState
boronActivityScore
acousticSpeechActivityScore
attSiriPrevOutputState
multiModalDecisionStage
eosLikelihood
nldaScore
mitigatorScore
mitigatorDecision
mitigatorThreshold
numTokensTopPath
trailingSilenceDuration
theshold
Missing config for Ures %@
-[SLUresMitigator initWithConfig:error:]
Unable to create model with error %@
-[SLUresMitigator dealloc]
-[SLUresMitigator _createInputOriginThresholdMap:]_block_invoke
v32@?0@"NSString"8@"NSDictionary"16^B24
-[SLUresMitigator _createInputOriginThresholdMap:]
-[SLUresMitigator _processInputFeats:]_block_invoke
Failed to create feature with error %@
-[SLUresMitigator _processInputFeats:]
Failed to get output with error %@
requestMitigated
requestNotMitigated
score
decision
decisionValue
-[SLUresMitigator _convertNSArrayToMultiArray:withShape:]
-[SLUresMitigator _convertMultiArrayToNSArray:withShape:]
%s Unable to create feature dict with error %@ from %@
%s output: %@
%s Expected 2D array, got %lu, not running model
%s Unable to extract shape of feature %@
%s Tokens: %@
%s inputIds: %@
%s attnMask: %@
%s ::: SL logging initialized (%s)
%s %@
%s Empty pattern, ignore
%s Unable to decode pattern %@ with err %@
%s Input to regex: %@
%s Output of regex: %@
%s Received nil recog candidate, nothing to extract
%s Extracted LRNN Score: %f from Model Version: %{public}@
%s Constructing tokens for speech path %{public}@
%s Adding score %{public}ld for token %{public}@
%s SPM model created
%s dealloc
%s ERR: metaData is nil, defaulting to NO for %{public}@
%s ERR: read metafile %{public}@ failed with %{public}@ - defaulting to NO
%s Decoded config at path: %@
%s VocabList size: %lu UnkToken %@
%s Truncation list: %@
%s Token not found, using unk token
%s Num Tokens - %lu  exceeded model input length - %lu
%s Reading from configFile %@
%s Created ODLD Processor Pipelines
%s PreProcessorType: %lu PreProcessor %@
%s TokenizerType: %lu Tokenizer %@
%s Classifier %@
%s Empty input
%s output %f
%s %{public}@
%s Created SLAcousticContext: %{public}@
%s Configured buffer size: %f samples, to be flushed after every %lu samples
%s Added checker to analyze %{public}lu samples with config file %{public}@
%s Failed to initialize SLProgressiveCheckerAnalyzer with error %{public}@
%s Initialized Progressive Checkers !
%s Unable to copy from circular buffer !
%s Flushed %lu samples to checker, samples since last flush %lu, total samples in buffer %lu
%s Calling endAudio after feeding %{public}lu samples to recognizer
%s Checker %lu fired, analyzed %{public}lu samples, token %{public}@,  confidence %{public}f
%s Flushed %lu samples to checker
%s All Checkers Finished, analyzed %{public}lu samples, token %{public}@, confidence %{public}f
%s Invalid config for %{public}@
%s Threshold map: %{public}@
%s AttFeature: %{public}@ --> %{public}@
%s Value for feature: %{public}@ isn't set, abort model run
%s Failed to convert array to MLMultiArray, not using feature %{public}@ for inference
%s Using software configured threshold: %.3f
%s Failed to convert NSArray with shape %{public}@ to MLMultiArray with err %{public}@
%s Mismatch in output shape, expected: %{public}@ got: %{public}@
SLODLDClassifierV1
SLODLDModelComponent
NSObject
SLRegexMatcher
SLODLDPreProcessingComponent
SLASRFeatures
SLLRNNFeatures
SLASRFeatureExtractor
SLSentencePieceModule
SLODLDTokenizerComponent
SLUtils
SLODLDProcessorResult
SLODLDConfigDecoder
SLBertClassifierResult
SLBertClassifier
SLODLDProcessor
SLProgressiveCheckerContext
SLProgressiveCheckerResult
SLProgressiveCheckerAnalyzer
SLUresMitigatorResult
SLUresMitigatorIpFeats
SLUresMitigator
replaceMatchesInString:options:range:withTemplate:
stringByDeletingLastPathComponent
copy
boolValue
initWithDictionary:error:
localeWithLocaleIdentifier:
expressionForConstantValue:
predictionFromFeatures:options:error:
addObjectsFromArray:
interpretations
numberWithDouble:
stringByAppendingString:
newlineCharacterSet
containsString:
removeSpaceBefore
predictionFromFeatures:error:
errorWithDomain:code:userInfo:
numberWithBool:
initWithConfiguration:
interpretationIndices
addObject:
setDateFormat:
containsObject:
stringByAppendingPathComponent:
removeSpaceAfter
mutableCopy
setObject:forKeyedSubscript:
phrases
audioAnalytics
length
addEntriesFromDictionary:
setComputeUnits:
removeObjectsAtIndexes:
floatValue
multiArrayValue
confidenceScore
arrayWithObjects:count:
whitespaceCharacterSet
setObject:forKey:
enumerateObjectsUsingBlock:
outputDescriptionsByName
tokenName
initWithCapacity:
inputDescriptionsByName
removeObjectAtIndex:
confidence
multiArrayConstraint
firstObject
arrayWithObject:
initWithShape:dataType:error:
setObject:atIndexedSubscript:
enumerateKeysAndObjectsUsingBlock:
objectForKeyedSubscript:
indexSetWithIndex:
sampleCount
dataWithContentsOfFile:
removeObject:
componentsSeparatedByCharactersInSet:
endTime
fileURLWithPath:
modelWithContentsOfURL:configuration:error:
array
objectForKey:
version
indexOfObject:
resultsWithEndedAudio
featureValueForName:
modelDescription
regularExpressionWithPattern:options:error:
valueForKey:
characterIsMember:
silenceStartTime
objectAtIndexedSubscript:
resultsWithAddedFloatAudio:numberOfSamples:taskName:
countByEnumeratingWithState:objects:count:
text
characterAtIndex:
recognition
shape
JSONObjectWithData:options:error:
appendString:
objectAtIndex:
latticeMitigatorResult
utterances
setLocale:
resultsWithAddedAudio:numberOfSamples:taskName:
substringWithRange:
count
setUsesCPUOnly:
bytes
numberWithUnsignedInteger:
doubleValue
lastObject
unsignedIntegerValue
copybufferFrom:to:
resetWithSamplingRate:language:taskType:userId:sessionId:deviceId:farField:audioSource:maxAudioBufferSizeSeconds:
stringWithFormat:
bufferLength
lowercaseString
expressionValueWithObject:context:
unsignedIntValue
initWithNumChannels:recordingDuration:samplingRate:
numberWithInteger:
isEqualToString:
copyBufferWithNumSamplesCopiedIn:
reset
stringWithContentsOfFile:encoding:error:
expressionForFunction:arguments:
localizedDescription
addSamples:numSamples:
initWithModelPath:
dictionaryWithObjects:forKeys:count:
numberWithFloat:
init
isEqual:
class
self
performSelector:
performSelector:withObject:
performSelector:withObject:withObject:
isProxy
isKindOfClass:
isMemberOfClass:
conformsToProtocol:
respondsToSelector:
retain
release
autorelease
retainCount
zone
hash
superclass
description
debugDescription
TQ,R
T#,R
T@"NSString",R,C
initWithConfigFile:
processEncodedTokens:
_extractModelSpecs
_constructFeatureDictionary:
_convert1dMLMultiArrayToNSArray:
odldClassifier
setOdldClassifier:
inputSpecs
setInputSpecs:
outputNodes
setOutputNodes:
inputOrigin
setInputOrigin:
.cxx_destruct
_odldClassifier
_inputSpecs
_outputNodes
_inputOrigin
T@"MLModel",&,N,V_odldClassifier
T@"NSDictionary",&,N,V_inputSpecs
T@"NSArray",&,N,V_outputNodes
T@"NSNumber",&,N,V_inputOrigin
applyPreprocessingOnUtt:
_compileRegexRulesForPatterns:
regexMatchingRules
setRegexMatchingRules:
replaceStrings
setReplaceStrings:
_regexMatchingRules
_replaceStrings
T@"NSArray",&,N,V_regexMatchingRules
T@"NSArray",&,N,V_replaceStrings
latticePathMaxScores
setLatticePathMaxScores:
latticePathMinScores
setLatticePathMinScores:
latticePathMeanScores
setLatticePathMeanScores:
latticePathVarScores
setLatticePathVarScores:
topLatticePathScores
setTopLatticePathScores:
topLatticePathTokenCount
setTopLatticePathTokenCount:
setSnr:
trailingSilence
setTrailingSilence:
_snr
_trailingSilence
_latticePathMaxScores
_latticePathMinScores
_latticePathMeanScores
_latticePathVarScores
_topLatticePathScores
_topLatticePathTokenCount
T@"NSArray",&,N,V_latticePathMaxScores
T@"NSArray",&,N,V_latticePathMinScores
T@"NSArray",&,N,V_latticePathMeanScores
T@"NSArray",&,N,V_latticePathVarScores
T@"NSArray",&,N,V_topLatticePathScores
TQ,N,V_topLatticePathTokenCount
Tf,N,V_snr
Tf,N,V_trailingSilence
lrnnScore
setLrnnScore:
lrnnProcessed
setLrnnProcessed:
_lrnnProcessed
_lrnnScore
Tf,N,V_lrnnScore
TB,N,V_lrnnProcessed
extractASRFaturesFrom:
extractLRNNFaturesFrom:
_getLastTokenForPath:fromPhrases:
_getTokenConfidenceForPath:fromPhrases:
getBestSpeechRecognitionTextFromPackage:
dealloc
encodeUtterance:
processor
setProcessor:
_processor
T@"EARSentencePieceModule",&,V_processor
decodeJsonFromFile:
createErrorWithMsg:code:
initWithScore:withVersion:tokens:features:
odldScore
configVersion
tokens
features
_odldScore
_configVersion
_tokens
_features
Tf,R,N,V_odldScore
T@"NSString",R,N,V_configVersion
T@"NSArray",R,N,V_tokens
T@"NSDictionary",R,N,V_features
getConfigVersion
getInputType
getOutputSpecs
getPreProcessorType
getTokenizerType
getRegexMapConfig
getSPMModelFile
getBertModelFile
getBertModelOutputNodes
_getValueForKey:categoryKey:
resourcePath
setResourcePath:
dictionary
setDictionary:
_resourcePath
_dictionary
T@"NSString",&,N,V_resourcePath
T@"NSDictionary",&,N,V_dictionary
initWithScore:assetVersion:extractedFeats:
dictionaryRepresentation
domainProb
assetVersion
extractedFeats
_domainProb
_assetVersion
_extractedFeats
T@"NSArray",R,N,V_domainProb
T@"NSString",R,N,V_assetVersion
T@"NSDictionary",R,N,V_extractedFeats
initWithConfig:error:locale:
processSpeechPackage:
processInputText:
_readVocabFromFile:
_normalizeText:
_createInputIdsAndRunModel:
_isCharPunctuation:
_splitOnPunctuation:
_wordPieceTokenizer:
vocab
setVocab:
unkToken
setUnkToken:
maxInputCharsPerWord
setMaxInputCharsPerWord:
maxNumTokens
setMaxNumTokens:
bertModel
setBertModel:
setAssetVersion:
shouldAppendLeadingText
setShouldAppendLeadingText:
numLeadingTokens
setNumLeadingTokens:
outputNodeName
setOutputNodeName:
truncationTokenList
setTruncationTokenList:
_shouldAppendLeadingText
_vocab
_unkToken
_maxInputCharsPerWord
_maxNumTokens
_bertModel
_numLeadingTokens
_outputNodeName
_truncationTokenList
T@"NSArray",&,N,V_vocab
T@"NSString",&,N,V_unkToken
TQ,N,V_maxInputCharsPerWord
TQ,N,V_maxNumTokens
T@"MLModel",&,N,V_bertModel
T@"NSString",&,N,V_assetVersion
TB,N,V_shouldAppendLeadingText
TQ,N,V_numLeadingTokens
T@"NSString",&,N,V_outputNodeName
T@"NSArray",&,N,V_truncationTokenList
initWithConfigFile:error:
_setupPipelineComponentsUsingConfig:
processInputUtterance:
_getPreprocessorForType:withConfig:
_getTokenizerForType:withConfig:
config
setConfig:
preprocessor
setPreprocessor:
tokenizer
setTokenizer:
model
setModel:
_config
_preprocessor
_tokenizer
_model
T@"SLODLDConfigDecoder",&,N,V_config
T@"<SLODLDPreProcessingComponent>",&,N,V_preprocessor
T@"<SLODLDTokenizerComponent>",&,N,V_tokenizer
T@"<SLODLDModelComponent>",&,N,V_model
initWithContext:error:
audioOption
vtei
invocationType
locale
_audioOption
_vtei
_invocationType
_locale
TQ,R,N,V_audioOption
T@"NSDictionary",R,N,V_vtei
TQ,R,N,V_invocationType
T@"NSString",R,N,V_locale
initWithScore:ofType:analyzedSamples:detailedResults:
resultType
analyzedSamples
score
detailedResult
_score
_resultType
_analyzedSamples
_detailedResult
TQ,R,N,V_resultType
TQ,R,N,V_analyzedSamples
Tf,R,N,V_score
T@"NSArray",R,N,V_detailedResult
analyzer:hasPartialResult:
analyzer:hasFinalResult:
initWithConfig:withDelegate:error:
startNewRequestWithContext:
addAudio:
endAudio
_startNewRequestWithContext:
_addAudio:
_endAudio
_activeRecognizers
_context
_checkerEndSamples
_analyzedSamplesSoFar
_latestScore
_queue
_delegate
_checkerType
_circBuffer
_numSamplesAddedToBufferSinceLastFlush
_numSamplesInStride
initWithScore:decision:decisionLevel:detailedResults:extractedFeats:
didMitigate
decisionLevel
threshold
_didMitigate
_threshold
_decisionLevel
TB,R,N,V_didMitigate
Td,R,N,V_decisionLevel
Tf,R,N,V_threshold
T@"NSDictionary",R,N,V_detailedResult
speechPackage
setSpeechPackage:
acousticFTMScores
setAcousticFTMScores:
boronScore
setBoronScore:
speakerIDScore
setSpeakerIDScore:
didDetectSpeechActivity
setDidDetectSpeechActivity:
isAirpodsConnected
setIsAirpodsConnected:
timeSinceLastQuery
setTimeSinceLastQuery:
decisionStage
setDecisionStage:
prevStageOutput
setPrevStageOutput:
eosLikelihood
setEosLikelihood:
nldaMetaInfo
setNldaMetaInfo:
nldaScore
setNldaScore:
_didDetectSpeechActivity
_isAirpodsConnected
_speechPackage
_acousticFTMScores
_boronScore
_speakerIDScore
_timeSinceLastQuery
_decisionStage
_prevStageOutput
_eosLikelihood
_nldaMetaInfo
_nldaScore
T@"AFSpeechPackage",&,N,V_speechPackage
T@"NSNumber",&,N,V_acousticFTMScores
T@"NSNumber",&,N,V_boronScore
T@"NSNumber",&,N,V_speakerIDScore
TB,N,V_didDetectSpeechActivity
TB,N,V_isAirpodsConnected
Td,N,V_timeSinceLastQuery
TQ,N,V_decisionStage
T@"NSNumber",&,N,V_prevStageOutput
T@"NSNumber",&,N,V_eosLikelihood
T@"NSDictionary",&,N,V_nldaMetaInfo
T@"NSNumber",&,N,V_nldaScore
getTranscriptionForSpeechPackage:
initWithConfig:error:
processInputFeats:completion:
_createInputOriginThresholdMap:
_processInputFeats:
_convertNSArrayToMultiArray:withShape:
_convertMultiArrayToNSArray:withShape:
_uresModel
_options
_inputOpsMap
_outputMap
_thresholdMap
_version
B24@0:8@16
#16@0:8
@16@0:8
@24@0:8:16
@32@0:8:16@24
@40@0:8:16@24@32
B16@0:8
B24@0:8#16
B24@0:8:16
Vv16@0:8
Q16@0:8
^{_NSZone=}16@0:8
B24@0:8@"Protocol"16
@"NSString"16@0:8
@24@0:8@16
f24@0:8@16
@24@0:8@"NSString"16
f24@0:8@"NSArray"16
v16@0:8
v24@0:8@16
@"MLModel"
@"NSDictionary"
@"NSArray"
@"NSNumber"
@"NSString"24@0:8@"NSString"16
v24@0:8Q16
f16@0:8
v20@0:8f16
v20@0:8B16
@32@0:8@16@24
@"NSArray"24@0:8@"NSString"16
@"EARSentencePieceModule"
@32@0:8@16Q24
@44@0:8f16@20@28@36
@"NSString"
@40@0:8@16@24@32
@40@0:8@16^@24@32
B20@0:8S16
@32@0:8@16^@24
@32@0:8Q16@24
@"SLODLDConfigDecoder"
@"<SLODLDPreProcessingComponent>"
@"<SLODLDTokenizerComponent>"
@"<SLODLDModelComponent>"
@44@0:8f16Q20Q28@36
@40@0:8@16@24^@32
@"NSMutableArray"
@"SLProgressiveCheckerContext"
@"NSObject<OS_dispatch_queue>"
@"<SLProgressiveCheckerAnalyzerDelegate>"
@"CSAudioCircularBuffer"
@48@0:8f16B20d24@32@40
d16@0:8
v24@0:8d16
@"AFSpeechPackage"
v32@0:8@16@?24
@"MLPredictionOptions"
@"NSMutableDictionary"
