@(#)PROGRAM:SiriLiminal  PROJECT:CoreSpeech-
com.apple.SiriLiminal
Framework
v8@?0
::: Initializing SiriLiminal logging...
en_US_POSIX
yyyyMMdd-HHmmss
SLLogInitIfNeeded_block_invoke
gitrelno_unavailable
+[SLASRFeatureExtractor extractASRFaturesFrom:]
max:
min:
stddev:
average:
v32@?0@"AFSpeechUtterance"8Q16^B24
+[SLASRFeatureExtractor extractLRNNFaturesFrom:]
+[SLASRFeatureExtractor _getTokenConfidenceForPath:fromPhrases:]
v32@?0@"AFSpeechPhrase"8Q16^B24
+[SLASRFeatureExtractor _getTokenConfidenceForPath:fromPhrases:]_block_invoke
v32@?0@"AFSpeechToken"8Q16^B24
+[SLUtils decodeJsonFromFile:]
SLInvocationType
SLVoiceTriggerEventInfo
SLAudioSourceOption
SLLanguageCode
Dictation
recognizerConfigs
checkerTimes
checkerConfig
triggerEndSampleCount
triggerStartSampleCount
Missing keys in context
reason
-[SLProgressiveCheckerContext initWithContext:error:]
Missing config for Progressive checker %@
v32@?0@"NSNumber"8Q16^B24
EAR recognizer init failed for config: %@
-[SLProgressiveCheckerAnalyzer initWithConfig:withDelegate:error:]_block_invoke
v32@?0@"NSString"8Q16^B24
-[SLProgressiveCheckerAnalyzer initWithConfig:withDelegate:error:]
Progressive Checker
-[SLProgressiveCheckerAnalyzer _startNewRequestWithContext:]_block_invoke
v32@?0@"_EARSyncSpeechRecognizer"8Q16^B24
-[SLProgressiveCheckerAnalyzer _addAudio:]_block_invoke
-[SLProgressiveCheckerAnalyzer _endAudio]_block_invoke
com.apple.sl
Version
InputOpsMap
OutputMap
ModelFile
name
shape
numAsrRecords
topPathScores
topPathNumTokens
latticeMaxScores
latticeMinScores
latticeMeanScores
latticeVarScores
trailingSilence
lrnnScores
lrnnProcessed
inputOrigin
acousticFTMScore
timeFromPrevQuery
speakerIdScore
airpodsConnectedState
boronActivityScore
acousticSpeechActivityScore
attSiriPrevOutputState
multiModalDecisionStage
eosLikelihood
mitigatorScore
mitigatorDecision
mitigatorThreshold
numTokensTopPath
trailingSilenceDuration
theshold
assetVersion
Missing config for Ures %@
-[SLUresMitigator initWithConfig:error:]
Unable to create model with error %@
-[SLUresMitigator _processInputFeats:]_block_invoke
v32@?0@"NSString"8@"NSDictionary"16^B24
Failed to create feature with error %@
-[SLUresMitigator _processInputFeats:]
Failed to get output with error %@
requestMitigated
requestNotMitigated
score
decision
decisionValue
-[SLUresMitigator _convertNSArrayToMultiArray:withShape:]
-[SLUresMitigator _convertMultiArrayToNSArray:withShape:]
%s ::: SL logging initialized (%s)
%s Received nil recog candidate, nothing to extract
%s Extracted LRNN Score: %f from Model Version: %{public}@
%s Constructing tokens for speech path %{public}@
%s Adding score %{public}ld for token %{public}@
%s ERR: metaData is nil, defaulting to NO for %{public}@
%s ERR: read metafile %{public}@ failed with %{public}@ - defaulting to NO
%s %{public}@
%s Created SLAcousticContext: %{public}@
%s Added checker to analyze %{public}lu samples with config file %{public}@
%s Failed to initialize SLProgressiveCheckerAnalyzer with error %{public}@
%s Initialized Progressive Checkers !
%s Updated checker endSamples with VTEI offset: %{public}lu to %{public}lu
%s Calling endAudio after feeding %{public}lu samples to recognizer
%s Checker %lu fired, analyzed %{public}lu samples, token %{public}@,  confidence %{public}f
%s All Checkers Finished, analyzed %{public}lu samples, token %{public}@, confidence %{public}f
%s %@
%s AttFeature: %{public}@ --> %{public}@
%s Value for feature: %{public}@ isn't set, abort model run
%s Failed to convert array to MLMultiArray, not using feature %{public}@ for inference
%s Failed to convert NSArray with shape %{public}@ to MLMultiArray with err %{public}@
%s Mismatch in output shape, expected: %{public}@ got: %{public}@
SLASRFeatures
SLLRNNFeatures
SLASRFeatureExtractor
SLUtils
SLProgressiveCheckerContext
SLProgressiveCheckerResult
SLProgressiveCheckerAnalyzer
SLUresMitigatorResult
SLUresMitigatorIpFeats
SLUresMitigator
localeWithLocaleIdentifier:
setLocale:
setDateFormat:
latticePathMaxScores
setLatticePathMaxScores:
latticePathMinScores
setLatticePathMinScores:
latticePathMeanScores
setLatticePathMeanScores:
latticePathVarScores
setLatticePathVarScores:
topLatticePathScores
setTopLatticePathScores:
topLatticePathTokenCount
setTopLatticePathTokenCount:
setSnr:
trailingSilence
setTrailingSilence:
.cxx_destruct
_snr
_trailingSilence
_latticePathMaxScores
_latticePathMinScores
_latticePathMeanScores
_latticePathVarScores
_topLatticePathScores
_topLatticePathTokenCount
T@"NSArray",&,N,V_latticePathMaxScores
T@"NSArray",&,N,V_latticePathMinScores
T@"NSArray",&,N,V_latticePathMeanScores
T@"NSArray",&,N,V_latticePathVarScores
T@"NSArray",&,N,V_topLatticePathScores
TQ,N,V_topLatticePathTokenCount
Tf,N,V_snr
Tf,N,V_trailingSilence
lrnnScore
setLrnnScore:
lrnnProcessed
setLrnnProcessed:
_lrnnProcessed
_lrnnScore
Tf,N,V_lrnnScore
TB,N,V_lrnnProcessed
recognition
utterances
count
initWithCapacity:
phrases
_getTokenConfidenceForPath:fromPhrases:
expressionForConstantValue:
arrayWithObject:
expressionForFunction:arguments:
expressionValueWithObject:context:
addObject:
doubleValue
numberWithDouble:
floatValue
_getLastTokenForPath:fromPhrases:
enumerateObjectsUsingBlock:
audioAnalytics
endTime
silenceStartTime
latticeMitigatorResult
score
threshold
version
interpretationIndices
lastObject
unsignedIntegerValue
interpretations
objectAtIndex:
tokens
firstObject
array
dictionaryRepresentation
objectAtIndexedSubscript:
addObjectsFromArray:
confidenceScore
numberWithInteger:
text
extractASRFaturesFrom:
extractLRNNFaturesFrom:
dataWithContentsOfFile:
JSONObjectWithData:options:error:
decodeJsonFromFile:
init
objectForKeyedSubscript:
stringWithFormat:
dictionaryWithObjects:forKeys:count:
errorWithDomain:code:userInfo:
initWithContext:error:
audioOption
vtei
invocationType
locale
_audioOption
_vtei
_invocationType
_locale
TQ,R,N,V_audioOption
T@"NSDictionary",R,N,V_vtei
TQ,R,N,V_invocationType
T@"NSString",R,N,V_locale
initWithScore:ofType:analyzedSamples:detailedResults:
resultType
analyzedSamples
detailedResult
_score
_resultType
_analyzedSamples
_detailedResult
TQ,R,N,V_resultType
TQ,R,N,V_analyzedSamples
Tf,R,N,V_score
T@"NSArray",R,N,V_detailedResult
stringByDeletingLastPathComponent
objectForKey:
numberWithUnsignedInteger:
stringByAppendingPathComponent:
initWithConfiguration:
localizedDescription
_startNewRequestWithContext:
_addAudio:
_endAudio
resetWithSamplingRate:language:taskType:userId:sessionId:deviceId:farField:audioSource:maxAudioBufferSizeSeconds:
setObject:atIndexedSubscript:
length
resultsWithAddedFloatAudio:numberOfSamples:taskName:
resultsWithAddedAudio:numberOfSamples:taskName:
resultsWithEndedAudio
removeObjectAtIndex:
confidence
tokenName
analyzer:hasPartialResult:
indexSetWithIndex:
removeObjectsAtIndexes:
analyzer:hasFinalResult:
initWithConfig:withDelegate:error:
startNewRequestWithContext:
addAudio:
endAudio
_activeRecognizers
_context
_checkerEndSamples
_analyzedSamplesSoFar
_latestScore
_queue
_delegate
_checkerType
initWithScore:decision:decisionLevel:detailedResults:extractedFeats:
didMitigate
decisionLevel
assetVersion
extractedFeats
_didMitigate
_threshold
_decisionLevel
_assetVersion
_extractedFeats
TB,R,N,V_didMitigate
Td,R,N,V_decisionLevel
Tf,R,N,V_threshold
T@"NSString",R,N,V_assetVersion
T@"NSDictionary",R,N,V_detailedResult
T@"NSDictionary",R,N,V_extractedFeats
speechPackage
setSpeechPackage:
inputOrigin
setInputOrigin:
acousticFTMScores
setAcousticFTMScores:
boronScore
setBoronScore:
speakerIDScore
setSpeakerIDScore:
didDetectSpeechActivity
setDidDetectSpeechActivity:
isAirpodsConnected
setIsAirpodsConnected:
timeSinceLastQuery
setTimeSinceLastQuery:
decisionStage
setDecisionStage:
prevStageOutput
setPrevStageOutput:
eosLikelihood
setEosLikelihood:
_didDetectSpeechActivity
_isAirpodsConnected
_speechPackage
_inputOrigin
_acousticFTMScores
_boronScore
_speakerIDScore
_timeSinceLastQuery
_decisionStage
_prevStageOutput
_eosLikelihood
T@"AFSpeechPackage",&,N,V_speechPackage
T@"NSNumber",&,N,V_inputOrigin
T@"NSNumber",&,N,V_acousticFTMScores
T@"NSNumber",&,N,V_boronScore
T@"NSNumber",&,N,V_speakerIDScore
TB,N,V_didDetectSpeechActivity
TB,N,V_isAirpodsConnected
Td,N,V_timeSinceLastQuery
TQ,N,V_decisionStage
T@"NSNumber",&,N,V_prevStageOutput
T@"NSNumber",&,N,V_eosLikelihood
fileURLWithPath:
setComputeUnits:
modelWithContentsOfURL:configuration:error:
setUsesCPUOnly:
_processInputFeats:
dictionary
isEqualToString:
arrayWithObjects:count:
numberWithFloat:
numberWithBool:
valueForKey:
_convertNSArrayToMultiArray:withShape:
setObject:forKey:
enumerateKeysAndObjectsUsingBlock:
initWithDictionary:error:
predictionFromFeatures:options:error:
featureValueForName:
multiArrayValue
_convertMultiArrayToNSArray:withShape:
initWithShape:dataType:error:
setObject:forKeyedSubscript:
shape
initWithConfig:error:
processInputFeats:completion:
_uresModel
_options
_inputOpsMap
_outputMap
_version
@16@0:8
v24@0:8@16
Q16@0:8
v24@0:8Q16
f16@0:8
v20@0:8f16
v16@0:8
@"NSArray"
B16@0:8
v20@0:8B16
@24@0:8@16
@32@0:8@16@24
@32@0:8@16^@24
@"NSDictionary"
@"NSString"
@44@0:8f16Q20Q28@36
@40@0:8@16@24^@32
@"NSMutableArray"
@"SLProgressiveCheckerContext"
@"NSObject<OS_dispatch_queue>"
@"<SLProgressiveCheckerAnalyzerDelegate>"
@48@0:8f16B20d24@32@40
d16@0:8
v24@0:8d16
@"AFSpeechPackage"
@"NSNumber"
v32@0:8@16@?24
@"MLModel"
@"MLPredictionOptions"
