@(#)PROGRAM:localspeechrecognition  PROJECT:SpeechRecognition-1
kLSRErrorDomain
+[LSRError new]
-[LSRError init]
app-lm.NGRAM
\unknown-first
\NT-appvocab
mini.json
%@-%@
-[LSRLanguageModel trainAppLmFromPlainTextAndWriteToAppDirectory:vocabFile:]
-[LSRLanguageModel ngramCountsSerializeData]
en_US_napg.json
dispatch.voc
vocdelta.voc
pg.voc
lexicon.enh
token_s.enh
mrec.psh
+[LSRLanguageModel createAppLmLmeProfileWithLanguage:modelRoot:customPronsData:newOovs:writeToVocabFile:]
v32@?0@"NSString"8Q16^B24
v24@?0@"NSString"8^B16
v32@?0@"NSString"8@"NSSet"16^B24
-[LSRLanguageModel trainAppLmFromNgramsSerializedData:customPronsData:language:writeToAppLmDir:vocabFile:]
-[LSRLanguageModel createSpeechProfileFromOovs:customProns:language:]
-[LSRLanguageModel trainAppLmFromNgramCountsArtifact:language:appLmArtifact:vocabFile:]
v32@?0@"_EAROovToken"8Q16^B24
com.apple.speech.localspeechrecognition
xpcservice
isTrial
modelRoot
QuasarDir
Language
v8@?0
-[LSRSpeechAssets installedLanguagesWithCompletion:]
+[LSRSpeechAssets assetConfigParameters:]
-[LSRSpeechAssets purgeAssetsForLanguage:completion:]
CESRTrialAssetManager
Unable to find class %s
CESRAssetConfig
v32@?0@"NSArray"8Q16^B24
-[LSRAudioDump initWithTaskIdentifier:]
-[LSRAudioDump appendBytes:]
-[LSRAudioDump close]
Library/Caches/com.apple.speech.localspeechrecognition/lsr_audio_dumps
%@-%04d%02d%02d-%02d%02d%02d.%d.wav
com.apple.assistant.dictation.prerecorded
voicemail-confidence-parameters
app-lm
+[_EARSpeechRecognizer(LSRConnection) _speechRecognizerWithLanguage:overrides:anyConfiguration:taskConfiguration:modelRoot:error:modelLoadTime:]
keepANEModelLoaded
itn_s.enh
Failed to create recognizer from=%@
-[LSRConnection initWithXPCConnection:]_block_invoke
-[LSRConnection dealloc]
-[LSRConnection _consumeSandboxExtensions:error:]
Error consuming sandbox extension: %s
file-write-create
-[LSRConnection _callerHasWritePriviledge:]
-[LSRConnection modelRootWithLanguage:modelOverrideURL:returningAssetType:error:]
-[LSRConnection _jitProfileWithLanguage:modelRoot:]
-[LSRConnection _requestContext]
none
-[LSRConnection prepareRecognizerWithLanguage:recognitionOverrides:modelOverrideURL:anyConfiguration:task:error:]
modelRoot is nil
Failed to access assets
_EARSpeechRecognizer is nil
Failed to initialize recognizer
-[LSRConnection preloadRecognizerForLanguage:task:recognitionOverrides:modelOverrideURL:completion:]
-[LSRConnection startRecordedAudioDictationWithParameters:]
Recognizer is busy
%@.%@
task name is nil
Failed to find task for recognizer
@"NSDictionary"8@?0
v32@?0@"NSString"8@"NSString"16^B24
-[LSRConnection addAudioPacket:]_block_invoke
-[LSRConnection stopSpeech]
-[LSRConnection cancelSpeech]
-[LSRConnection initializeWithSandboxExtensions:]_block_invoke
+[LSRConnection _scheduleCooldownTimer]
+[LSRConnection _cancelCooldownTimer]
+[LSRConnection _cooldownTimerFired]
-[LSRConnection downloadAssetsForLanguage:urgent:forceUpgrade:detailedProgress:]
v20@?0Q8B16
-[LSRConnection downloadAssetsForLanguage:urgent:forceUpgrade:detailedProgress:]_block_invoke
v24@?0@"NSString"8@"NSError"16
v20@?0d8B16
v16@?0Q8
com.apple.speech.localspeechrecognition.detailedProgressWait
-[LSRConnection fetchAssetsForLanguage:completion:]
v16@?0@"NSDictionary"8
-[LSRConnection configParametersForVoicemailWithLanguage:completion:]
-[LSRConnection configParametersForVoicemailWithLanguage:completion:]_block_invoke
-[LSRConnection initializeLmWithLocale:completion:]
Error initializing
-[LSRConnection initializeLmWithAssetPath:completion:]
-[LSRConnection addSentenceToNgramCounts:]
-[LSRConnection addSentenceToNgramCounts:completion:]
-[LSRConnection oovWordsAndFrequenciesWithCompletion:]
v24@?0@"NSURL"8@"NSURL"16
-[LSRConnection trainFromPlainTextAndWriteToDirectory:sandboxExtension:completion:]
SpeechProfile
-[LSRConnection trainAppLmFromNgramsSerializedData:customPronsData:language:writeToDirectory:sandboxExtension:completion:]
v16@?0@"NSURL"8
-[LSRConnection createPhraseCountsArtifactWithIdentifier:rawPhraseCountsPath:customPronunciationsPath:writeToDirectory:sandboxExtensions:completion:]
PhraseCountsFile
-[LSRConnection createNgramCountsArtifactFromPhraseCountArtifact:writeToDirectory:sandboxExtensions:completion:]
NgramCountsFile
-[LSRConnection trainAppLmFromNgramCountsArtifact:language:writeToDirectory:sandboxExtensions:completion:]
AppLmModelFile
-[LSRConnection purgeAssetsForLanguage:completion:]
-[LSRConnection speechRecognizer:didRecognizePartialResult:]_block_invoke
v32@?0@"_EARSpeechRecognitionToken"8Q16^B24
-[LSRConnection speechRecognizer:didRecognizeFinalResultPackage:]_block_invoke_2
-[LSRConnection speechRecognizer:didFinishRecognitionWithError:]_block_invoke
No speech detected
Recognition request was canceled
Failed to recognize the request
Unsupported EAR build
CheckPrerecordedConnectionAccess
Siri and Dictation are disabled
Task %@ not available for %@, supported tasks are %@
etiquette.json
ReplacementDictionaryForLanguage
ReplacementDictionaryForLanguage_block_invoke
v32@?0@"NSString"8@16^B24
voicemail_confidence_subtraction.json
sampling rate = %@
task type = %@
far field = %@
device ID = %@
Bluetooth device ID = %@
hasSPIAccess
_fileID
JSONObjectWithData:options:error:
_requestContext
T@"NSString",R,C
allKeys
URLByAppendingPathComponent:
confidenceScore
UTF8String
dealloc
UUIDString
downloadAssetsForLanguage:urgent:forceUpgrade:detailedProgress:
_appLmData
getLocalFileUrl
_assetQueryForLanguage:
initWithLocale:
_audioDump
isFinal
_auditToken
isValid
_bufferedAudioPackets
metrics
_cancelCooldownTimer
numberWithBool:
_consumeSandboxExtensions:error:
recognizedResultFromEARPackage:
_cooldownTimerFired
serviceListener
_extendedFile
setExtraLmList:
_firstAudioPacketTime
stringByAppendingPathComponent:
_initWithBestTranscription:rawTranscription:final:speechRecognitionMetadata:
validationError
_jitProfileWithLanguage:modelRoot:
.cxx_destruct
_modelAssetType
T#,R
addAudioPacket:
TQ,R
applicationName
URLByAppendingPathComponent:isDirectory:
containsObject:
UUID
discard
_UUID
_asrInitializationTime
initWithConfig:
_audioBuffer
interpretations
_audioDuration
isProxy
_bufferedAudioEnded
lexemes
_callerHasWritePriviledge:
_connection
oneBest
_consumedSandboxExtensions
release
_delegate
_fetchAssetPathForInstalledLanguage:assetType:triggerDownload:outError:
setInputFormat:
_firstWordTime
transcriptionFromSpeechPhrases:afAudioAnalytics:utteranceStart:
_jitLmeProcessingTime
version
_languageCode
_lastAudioPacketTime
_lastRecognitionResult
_lsrLanguageModel
_lsr_isInstalled
_lsr_language
_lsr_path
_lsr_quasarModelPath
_modelLoadTime
_modelRoot
_ncsRootPath
_recognitionBeginTime
_recognitionResultIsFinal
_recognitionResultPackageGenerated
_recognizer
_recognizerConfigFilePath
_requestParameters
_scheduleCooldownTimer
_setQueue:
_silStartFirstToken
_speechRecognizerWithLanguage:overrides:anyConfiguration:taskConfiguration:modelRoot:error:modelLoadTime:
_url
_userProfileWithModelRoot:language:
acousticFeatureValuePerFrame
acousticFeatures
activeConfiguration
activeConfigurationForEverything
addAudioSamples:count:
addEntriesFromDictionary:
addKeyValuePair:with:
addNgramCountWithType:content:
addObject:
addOovTokensFromSentence:
addProns:forWord:
addProns:forWord:completion:
addSentenceToNgramCounts:
addSentenceToNgramCounts:completion:
addSentenceWithType:uuid:content:
addWordWithParts:templateName:
appendBytes:
array
arrayByAddingObject:
arrayWithObjects:count:
assetConfigParameters:
assistantIsEnabled
attributes
audioAnalytics
auditToken
autorelease
bluetoothDeviceIdFilter
boolValue
buildLmWithConfig:root:data:dir:shouldStop:
bundleIdentifier
bytes
cancelRecognition
cancelSpeech
class
close
code
componentsSeparatedByCharactersInSet:
concatenateUtterances
confidence
configParametersForVoicemailWithLanguage:completion:
conformsToProtocol:
copy
count
countByEnumeratingWithState:objects:count:
createAppLmLmeProfileWithLanguage:modelRoot:customPronsData:newOovs:writeToVocabFile:
createDirectoryAtURL:withIntermediateDirectories:attributes:error:
createNgramCountsArtifactFromPhraseCountArtifact:saveTo:
createNgramCountsArtifactFromPhraseCountArtifact:writeToDirectory:sandboxExtensions:completion:
createPhraseCountArtifactWithIdentifier:rawPhraseCountsPath:customPronunciationsPath:saveTo:
createPhraseCountsArtifact:version:locale:rawPhraseCountsPath:customPronunciationsPath:saveTo:
createPhraseCountsArtifactWithIdentifier:rawPhraseCountsPath:customPronunciationsPath:writeToDirectory:sandboxExtensions:completion:
createSpeechProfileFromOovs:customProns:language:
currentConnection
dataProfile
dataWithContentsOfFile:
dataWithContentsOfFile:options:error:
debugDescription
defaultManager
description
deserializeModelData:
deserializeNgramCountsData:
deserializeNgramCountsData:completion:
detectMultipleUtterances
deviceIdFilter
dictationIsEnabled
dictionary
dictionaryWithDictionary:
dictionaryWithObjects:forKeys:count:
domain
doubleValue
downloadAssetOfType:language:urgent:forceUpgrade:progressHandler:completionHandler:
downloadStatusWithConfig:progressHandler:completionHandler:
dumpDirectory
dynamicLanguageModel
dynamicVocabulary
effectiveBoolValueForSetting:
emptyResultWithIsFinal:
enableAutoPunctuation
endAudio
enumerateKeysAndObjectsUsingBlock:
enumerateObjectsUsingBlock:
errorWithCode:
errorWithCode:description:
errorWithDomain:code:userInfo:
farFieldFilter
fetchAssetPathForInstalledLanguage:outError:
fetchAssetsForLanguage:completion:
fileExistsAtPath:
fileURLWithPath:
filenameForTaskIdentifier:
firstObject
frameDuration
frequency
generateNgramCountsSerializeDataWithCompletion:
generateNgramCountsWithConfig:root:data:
getProns
hasSpaceAfter
hash
indexOfObject:
init
initWithAcousticFeatureValue:frameDuration:
initWithAssetPath:
initWithCapacity:
initWithConfiguration:
initWithConfiguration:language:overrides:sdapiOverrides:generalVoc:emptyVoc:pgVoc:lexiconEnh:tokenEnh:paramsetHolder:
initWithConfiguration:ncsRoot:language:
initWithConfiguration:ncsRoot:recognizerConfigPath:
initWithConfiguration:overrides:overrideConfigFiles:generalVoc:lexiconEnh:itnEnh:language:activeConfiguration:modelLoadingOptions:enableSpeakerCodeTraining:supportEmojiRecognition:
initWithConfiguration:root:
initWithContentsOfFile:
initWithData:
initWithInterpretationIndices:confidenceScore:
initWithLanguage:
initWithLanguage:assetType:
initWithOrthography:pronunciations:tagName:frequency:
initWithPath:
initWithSpeechRecognitionFeatures:acousticFeatures:
initWithTaskIdentifier:
initWithTokenName:start:end:silenceStart:confidence:hasSpaceAfter:hasSpaceBefore:phoneSequence:ipaPhoneSequence:
initWithType:
initWithXPCConnection:
initialize
initializeLmWithAssetPath:completion:
initializeLmWithLocale:completion:
initializeWithSandboxExtensions:
inlineItemList
installationStatusForLanguagesForAssetType:includeDetailedStatus:error:
installedAssetWithConfig:regionId:triggerDownload:
installedLanguagesWithCompletion:
interpretationIndices
invalidate
ipaPhoneSequence
isEqual:
isEqualToDictionary:
isEqualToString:
isKindOfClass:
isMemberOfClass:
jitProfileFromContextualStrings:
language
lastObject
length
listener:shouldAcceptNewConnection:
lmeThreshold
lmeThresholdWithCompletion:
loadCustomPronData:ncsRoot:dataRoot:
loadLmFromDir:
loadOovs
localSpeechRecognitionDidDownloadAssetsWithProgress:isStalled:
localSpeechRecognitionDidFail:
localSpeechRecognitionDidFinishDownloadingAssets:error:
localSpeechRecognitionDidFinishRecognition:
localSpeechRecognitionDidProcessAudioDuration:
localSpeechRecognitionDidRecognizePartialResult:rawPartialResult:
localSpeechRecognitionDidSucceed
localizedDescription
logCoreAnalyticsEvent:withAnalytics:
mainBundle
maximumRecognitionDuration
metricsWithCompletion:
modelInfo
modelOverrideURL
modelRootWithLanguage:modelOverrideURL:returningAssetType:error:
narrowband
newlineCharacterSet
ngramCountsSerializeData
numberWithDouble:
numberWithInt:
numberWithInteger:
numberWithLongLong:
numberWithUnsignedInteger:
nvAsrPhoneSequenceForXsampaProns:
objectAtIndex:
objectAtIndexedSubscript:
objectForKey:
objectForKeyedSubscript:
onDeviceOnly
oovWordsAndFrequenciesInNgramCount
oovWordsAndFrequenciesWithCompletion:
oovsFromSentenceAddedToNgramCounts:
orderedOovs
orderedSetWithArray:
orthography
path
performSelector:
performSelector:withObject:
performSelector:withObject:withObject:
phoneSequence
preITNRecognition
preITNTokens
preloadRecognizerForLanguage:task:recognitionOverrides:modelOverrideURL:completion:
prepareRecognizerWithLanguage:recognitionOverrides:modelOverrideURL:anyConfiguration:task:error:
processInfo
promoteAssets
promoteAssetsForAssetType:
prons
pronunciationsForOrthography:
purgeAssetsForLanguage:completion:
purgeInstalledAssetForAssetType:language:regionId:error:
raise:format:
recognition
recognitionMetadataFromSpeechPhrases:afAudioAnalytics:utteranceStart:
recognitionOverrides
recognitionStatistics
releaseClients
remoteObjectProxy
removeAllObjects
removeItemAtURL:error:
reportFailureWithError:
requestIdentifier
respondsToSelector:
resume
retain
retainCount
returnTypes:
runRecognitionWithResultStream:language:task:samplingRate:
samplingRateFilter
sanitizedStringWithString:
self
serializedModelWithLanguage:modelData:oovs:
setActiveConfiguration:
setAssetsAsProvisional
setAssetsProvisionalForAssetType:
setBluetoothDeviceIdFilter:
setByAddingObject:
setCachedJitProfileBuilder:
setCachedLanguage:
setCachedModelRoot:
setConcatenateUtterances:
setConfidenceScore:
setDelegate:
setDetectUtterances:
setDeviceIdFilter:
setDisableAutoPunctuation:
setEnableSpeakerCodeTraining:
setEndTime:
setExportedInterface:
setExportedObject:
setFarFieldFilter:
setInterpretationIndices:
setInterpretations:
setInterruptionHandler:
setInvalidationHandler:
setIpaPhoneSequence:
setIsLowConfidence:
setJitProfileData:
setMaximumRecognitionDuration:
setObject:forKey:
setPhoneSequence:
setRecognitionConfidenceSubtraction:
setRecognitionReplacements:
setRecognizeEagerCandidates:
setRemoteObjectInterface:
setRemoveSpaceBefore:
setSamplingRateFilter:
setSilenceStartTime:
setSource:
setStartTime:
setTaskTypeFilter:
setText:
setTokens:
setUserProfileData:
setValue:forKey:
setWithObject:
setXsampaProns:forWord:
sharedConnection
sharedInstance
sharedPreferences
sigterm_queue
sigterm_source
silenceStart
speechRecognitionFeatures
speechRecognizer:didFinishRecognitionWithError:
speechRecognizer:didProcessAudioDuration:
speechRecognizer:didProduceEndpointFeaturesWithWordCount:trailingSilenceDuration:eosLikelihood:pauseCounts:silencePosterior:processedAudioDurationInMilliseconds:
speechRecognizer:didProduceLoggablePackage:
speechRecognizer:didRecognizeFinalResultCandidatePackage:
speechRecognizer:didRecognizeFinalResultPackage:
speechRecognizer:didRecognizeFinalResults:
speechRecognizer:didRecognizeFinalResults:tokenSausage:nBestChoices:
speechRecognizer:didRecognizePartialResult:
speechRecognizer:didRecognizePartialResultNbest:
speechRecognizer:didRecognizeRawEagerRecognitionCandidate:
speechRecognizer:didReportStatus:statusContext:
standardUserDefaults
start
startRecordedAudioDictationWithParameters:
state
stopSpeech
stringByDeletingLastPathComponent
stringByReplacingOccurrencesOfString:withString:
stringForKey:
stringValue
stringWithFormat:
superclass
supportedLanguagesWithAssetType:
switchToNewAssetsForAssetType:
systemUptime
task
taskIdentifier
taskTypeFilter
tasks
tokenName
tokenSausage
tokens
trainAppLmFromNgramCountsArtifact:language:appLmArtifact:vocabFile:
trainAppLmFromNgramCountsArtifact:language:writeToDirectory:sandboxExtensions:completion:
trainAppLmFromNgramsSerializedData:customPronsData:language:writeToAppLmDir:vocabFile:
trainAppLmFromNgramsSerializedData:customPronsData:language:writeToDirectory:sandboxExtension:completion:
trainAppLmFromPlainTextAndWriteToAppDirectory:vocabFile:
trainFromPlainTextAndWriteToDirectory:sandboxExtension:completion:
transcriptionsWithEARTokens:
transcriptionsWithTokens:
transitionArtifactAt:toStage:configPath:ncsRoot:dataRoot:estimationRoot:minimize:saveTo:
unsignedIntegerValue
updateAudioDuration:
urlForTaskIdentifier:
utteranceStart
valueForEntitlement:
wait
wakeUpWithCompletion:
writeToDirectory:
writeToURL:atomically:
zone
%s %s is not available, as LSRError is not designed to be instantiated. Returning nil.
%s Failed to train model successfully
%s Model trained successfully!
%s Writing to: %@
%s Write process failed
%s Failed to serialize ngram counts data
%s Successfully serialized ngram counts and OOVs
%s Failed to create user profile object
%s Failed to create user profile
%s Wrote profile data to file:%@
%s Failed to write LME
%s Successfully wrote LME: %@
%s No oovs in training data
%s Failed to train from ngram counts
%s Failed to write trained AppLm
%s Successfully wrote app-lm to path=%@
%s Exception while training.
%s Failed to train app-specific LM
%s Failed to read ngram counts artifact at: %@
%s Failed to load custom prons from artifact
%s Artifact contained invalid custom prons; skipping. %@
%s Could not get OOVs from artifact
_set_user_dir_suffix failed
: %{darwin.errno}d
%s Failed to query Trial with error=%@
%s Found %lu installed assets
%s Failed to read json file: %@
%s Failed to parse json config file:%@
%s Failed to create audio dump file. Error: %@, URL: %@
%s Failed to extend audio dump file. Error: %@, URL: %@
%s Failed to prewarm dump file. Error: %@, URL: %@
%s Created audio file at: %@
%s Failed to write to dump file. Error: %@, URL: %@
%s Closed audio dump file at: %@
%s Could not locate asset.
%s Failed to create recognizer from: %@
%s Created Recognizer in %lf sec
%s Handling SIGTERM
%s %@ cancelling instance %@
%s %@ deallocating
%s Consuming sandbox extensions: %@
%s Error consuming sandbox extension: %s
%s Caller has write priviledges for: %@
%s Caller does not have write priviledges for: %@
%s Fetch asset error %@
%s Asset path: %@
%s Could not locate asset
%s Using cached JIT profile builder for language=%@ modelRoot=%@
%s Initialized a new JIT profile builder for language=%@ modelRoot=%@
%s RequestContext: language:%@ task:%@ narrowband:%d appname:%@ enableAutoPunctuation:%d
%s Preparing recognizer...
%s %@ %@
%s Using cached recognizer for language=%@ modelRoot=%@ overrides=%@
%s Initializing a new recognizer for language=%@ modelRoot=%@ overrides=%@
%s Created _EARSpeechRecognizer successfully with modelInfo: %@
%s Failed to prepare recognizer in advance: %@
%s Starting..
%s Error: %@
%s %@
%s Audio dumps will be enabled for request: %@ with taskIdentifier: %@
%s Dump will not be created because dump identifier (%@) doesn't match the qualified task identifier (%@).
%s task name is nil
%s Inline LME input size=%lu
%s Failed to build jitData
%s Set JIT profile for the request
%s Failed to build jitProfile with error %@
%s Loaded Dynamic LM: %@
%s Loaded Dynamic Vocab: %@
%s Censor Speech enabled: %d
%s Changing active configuration from 
%@ to 
%s Adding audio packet: %ld
%s stopSpeech - %p
%s cancelSpeech - %p
%s Failed to consume sandbox extensions: %@
%s Successfully consumed sandbox extensions
%s _scheduleCooldownTimer
%s _cancelCooldownTimer
%s _cooldownTimerFired
%s Request not authorized
%s Failed to download Trial assets with error=%@
%s Failed to find voicemail config parameters
%s Failed to read json config parameters from file: %@
%s Failed to find asset path for language:%@
%s Initialize Language Model with locale=%@
%s Initialize Language Model with assetPath=%@
%s Adding sentence=%@
%s Adding oovs from sentence
%s oovs words and frequencies
%s Write failed error:%@
%s Write successfull
%s Starting training of App LM from ngram count.
%s Finished training of App LM from ngram count.
%s Recognized %lu partial result tokens.
%s package isFinal=%s
%s connection is %@, error %@
%s _recognitionBeginTime (%@) is greater than recognitionEndTime (%@)
%s audioDuration: %lf
%s recognitionDuration: %lf
%s responseTime(Audio finish to recognizer finish): %lf
%s modelLoadTime: %lf
%s TTFW: %lf
%s cpuRtf: %lf
%s jitLmeProcessingTime: %lf
%s asrInitializationTime: %lf
%s _recognitionResultIsFinal=%d _recognitionResultPackageGenerated=%d
%s Called from outside of an XPC connection
%s Could not read %@: %@
%s Could not parse %@: %@
%s %@ is wrong type: %@
%s %@ contains bogus key/value pair: %@ => %@
LSRError
LSRLanguageModel
ServiceDelegate
NSXPCListenerDelegate
NSObject
LSRAdditions
LSRSpeechAssets
LSRUtilities
LSRAudioDump
LSRConnection
_EARSpeechRecognitionResultStream
SFLSRProtocol
v16@0:8
@32@0:8q16@24
@24@0:8q16
@16@0:8
@32@0:8@16@24
B56@0:8@16@24@32@40@48
@24@0:8@16
v24@0:8@16
B32@0:8@16@24
v32@0:8^@16^@24
q16@0:8
v56@0:8@16@24@32^@40^@48
v48@0:8@16@24@32@40
v32@0:8@16@24
@40@0:8@16@24@32
v48@0:8@16@24^@32^@40
@"_EARAppLmData"
@"NSString"
B24@0:8@16
#16@0:8
@24@0:8:16
@32@0:8:16@24
@40@0:8:16@24@32
B16@0:8
B24@0:8#16
B24@0:8:16
Vv16@0:8
Q16@0:8
^{_NSZone=}16@0:8
B24@0:8@"Protocol"16
@"NSString"16@0:8
B32@0:8@"NSXPCListener"16@"NSXPCConnection"24
v24@0:8@?16
@32@0:8@16^@24
@44@0:8@16Q24B32^@36
v32@0:8@16@?24
@20@0:8B16
@"NSURL"
^{OpaqueAudioFileID=}
^{OpaqueExtAudioFile=}
@68@0:8@16@24B32@36@44^@52^d60
v32@0:8@16d24
v72@0:8@16q24q32d40@48d56q64
v40@0:8@16Q24@32
v32@0:8@"_EARSpeechRecognizer"16@"_EARSpeechRecognitionResult"24
v32@0:8@"_EARSpeechRecognizer"16@"NSError"24
v32@0:8@"_EARSpeechRecognizer"16@"NSArray"24
v48@0:8@"_EARSpeechRecognizer"16@"NSArray"24@"NSArray"32@"NSArray"40
v32@0:8@"_EARSpeechRecognizer"16@"_EARSpeechRecognitionResultPackage"24
v32@0:8@"_EARSpeechRecognizer"16d24
v32@0:8@"_EARSpeechRecognizer"16@"_EARSpeechRecognition"24
v72@0:8@"_EARSpeechRecognizer"16q24q32d40@"NSArray"48d56q64
v40@0:8@"_EARSpeechRecognizer"16Q24@"NSDictionary"32
Vv24@0:8@16
Vv56@0:8@16@24@32@40@?48
Vv24@0:8@?16
Vv36@0:8@16B24B28B32
Vv32@0:8@16@?24
Vv40@0:8@16@24@?32
Vv64@0:8@16@24@32@40@48@?56
Vv48@0:8@16@24@32@?40
Vv24@0:8@"SFRequestParameters"16
Vv24@0:8@"NSData"16
Vv56@0:8@"NSString"16@"NSString"24@"NSDictionary"32@"NSURL"40@?<v@?@"NSError">48
Vv24@0:8@?<v@?@"NSSet">16
v24@0:8@"NSArray"16
v32@0:8@"NSString"16@"NSDictionary"24
Vv36@0:8@"NSString"16B24B28B32
Vv32@0:8@"NSString"16@?<v@?@"NSString"@"NSError">24
Vv32@0:8@"NSString"16@?<v@?@"NSDictionary">24
Vv32@0:8@"NSString"16@?<v@?@"NSError">24
Vv32@0:8@"NSLocale"16@?<v@?@"NSError">24
Vv24@0:8@"NSString"16
Vv32@0:8@"NSString"16@?<v@?@"NSArray">24
Vv40@0:8@"NSSet"16@"NSString"24@?<v@?B>32
Vv24@0:8@?<v@?@"NSDictionary">16
Vv40@0:8@"NSURL"16@"NSString"24@?<v@?@"NSURL"@"NSURL">32
Vv24@0:8@?<v@?@"NSData">16
Vv32@0:8@"NSData"16@?<v@?@"NSDictionary">24
Vv24@0:8@?<v@?q>16
Vv24@0:8@?<v@?>16
Vv64@0:8@"NSData"16@"NSData"24@"NSString"32@"NSURL"40@"NSString"48@?<v@?@"NSURL"@"NSURL">56
Vv64@0:8@"NSString"16@"NSURL"24@"NSURL"32@"NSURL"40@"NSArray"48@?<v@?@"NSURL">56
Vv48@0:8@"NSURL"16@"NSURL"24@"NSArray"32@?<v@?@"NSURL">40
Vv56@0:8@"NSURL"16@"NSString"24@"NSURL"32@"NSArray"40@?<v@?@"NSURL"@"NSURL">48
B32@0:8@16^@24
{?=[8I]}16@0:8
@48@0:8@16@24^Q32^@40
B60@0:8@16@24@32B40@44^@52
@"_EARSpeechRecognizer"
@"_EARSpeechRecognitionAudioBuffer"
@"NSXPCConnection"
@"NSMutableArray"
@"SFSpeechRecognitionResult"
@"SFRequestParameters"
@"LSRAudioDump"
@"NSObject<OS_dispatch_queue>"
@"NSObject<OS_dispatch_source>"
@"LSRLanguageModel"
softlink:r:path:/System/Library/PrivateFrameworks/CoreEmbeddedSpeechRecognition.framework/CoreEmbeddedSpeechRecognition
softlink:r:path:/System/Library/PrivateFrameworks/CoreEmbeddedSpeechRecognition.framework/CoreEmbeddedSpeechRecognition
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
<key>application-identifier</key>
<string>com.apple.speech.localspeechrecognition</string>
<key>com.apple.application-identifier</key>
<string>com.apple.speech.localspeechrecognition</string>
<key>com.apple.private.assets.accessible-asset-types</key>
<array>
<string>com.apple.MobileAsset.EmbeddedSpeech</string>
<string>com.apple.MobileAsset.EmbeddedSpeechMac</string>
<string>com.apple.MobileAsset.Trial.Siri.SiriUnderstandingAsrAssistant</string>
<string>com.apple.MobileAsset.Trial.Siri.SiriUnderstandingAsrHammer</string>
<string>com.apple.MobileAsset.Trial.Siri.SiriDictationAssets</string>
</array>
<key>com.apple.private.assets.bypass-asset-types-check</key>
<true/>
<key>com.apple.private.sandbox.profile:embedded</key>
<string>temporary-sandbox</string>
<key>com.apple.private.tcc.manager.check-by-audit-token</key>
<array>
<string>kTCCServiceSpeechRecognition</string>
</array>
<key>com.apple.proactive.eventtracker</key>
<true/>
<key>com.apple.security.exception.files.absolute-path.read-only</key>
<array>
<string>/private/var/MobileAsset/</string>
</array>
<key>com.apple.security.exception.files.home-relative-path.read-write</key>
<array>
<string>/Library/Caches/com.apple.speech.localspeechrecognition/</string>
</array>
<key>com.apple.security.exception.iokit-user-client-class</key>
<array>
<string>IOSurfaceRootUserClient</string>
<string>H11ANEInDirectPathClient</string>
</array>
<key>com.apple.security.exception.mach-lookup.global-name</key>
<array>
<string>com.apple.appleneuralengine</string>
<string>com.apple.triald.namespace-management</string>
<string>com.apple.mobileasset.autoasset</string>
<string>com.apple.FileCoordination</string>
</array>
<key>com.apple.security.exception.shared-preference.read-only</key>
<array>
<string>com.apple.assistant.support</string>
<string>com.apple.speech.localspeechrecognition</string>
<string>com.apple.corevideo</string>
</array>
<key>com.apple.security.ts.mobile-keybag-access</key>
<true/>
<key>com.apple.security.ts.opengl-or-metal</key>
<true/>
<key>com.apple.security.ts.tmpdir</key>
<string>com.apple.speech.localspeechrecognition</string>
<key>com.apple.trial.client</key>
<array>
<string>372</string>
<string>401</string>
<string>751</string>
</array>
<key>platform-application</key>
<true/>
</dict>
</plist>
L0A
application-identifier'com.apple.speech.localspeechrecognition0K com.apple.application-identifier'com.apple.speech.localspeechrecognition0
7/com.apple.private.assets.accessible-asset-types0
$com.apple.MobileAsset.EmbeddedSpeech'com.apple.MobileAsset.EmbeddedSpeechMac>com.apple.MobileAsset.Trial.Siri.SiriUnderstandingAsrAssistant;com.apple.MobileAsset.Trial.Siri.SiriUnderstandingAsrHammer4com.apple.MobileAsset.Trial.Siri.SiriDictationAssets061com.apple.private.assets.bypass-asset-types-check
0?*com.apple.private.sandbox.profile:embedded
temporary-sandbox0T2com.apple.private.tcc.manager.check-by-audit-token0
kTCCServiceSpeechRecognition0% com.apple.proactive.eventtracker
0Y:com.apple.security.exception.files.absolute-path.read-only0
/private/var/MobileAsset/0~@com.apple.security.exception.files.home-relative-path.read-write0:8/Library/Caches/com.apple.speech.localspeechrecognition/0k4com.apple.security.exception.iokit-user-client-class03
IOSurfaceRootUserClient
H11ANEInDirectPathClient0
4com.apple.security.exception.mach-lookup.global-name0
com.apple.appleneuralengine%com.apple.triald.namespace-management
com.apple.mobileasset.autoasset
com.apple.FileCoordination0
8com.apple.security.exception.shared-preference.read-only0[
com.apple.assistant.support'com.apple.speech.localspeechrecognition
com.apple.corevideo0/*com.apple.security.ts.mobile-keybag-access
0*%com.apple.security.ts.opengl-or-metal
com.apple.security.ts.tmpdir'com.apple.speech.localspeechrecognition0)
com.apple.trial.client0
372
401
7510
platform-application
@(#)PROGRAM:localspeechrecognition  PROJECT:SpeechRecognition-1
kLSRErrorDomain
+[LSRError new]
-[LSRError init]
app-lm.NGRAM
\unknown-first
\NT-appvocab
mini.json
%@-%@
-[LSRLanguageModel trainAppLmFromPlainTextAndWriteToAppDirectory:vocabFile:]
-[LSRLanguageModel ngramCountsSerializeData]
en_US_napg.json
dispatch.voc
vocdelta.voc
pg.voc
lexicon.enh
token_s.enh
mrec.psh
+[LSRLanguageModel createAppLmLmeProfileWithLanguage:modelRoot:customPronsData:newOovs:writeToVocabFile:]
v32@?0@"NSString"8Q16^B24
v24@?0@"NSString"8^B16
v32@?0@"NSString"8@"NSSet"16^B24
-[LSRLanguageModel trainAppLmFromNgramsSerializedData:customPronsData:language:writeToAppLmDir:vocabFile:]
-[LSRLanguageModel createSpeechProfileFromOovs:customProns:language:]
-[LSRLanguageModel trainAppLmFromNgramCountsArtifact:language:appLmArtifact:vocabFile:]
v32@?0@"_EAROovToken"8Q16^B24
com.apple.speech.localspeechrecognition
xpcservice
isTrial
modelRoot
QuasarDir
Language
v8@?0
-[LSRSpeechAssets installedLanguagesWithCompletion:]
+[LSRSpeechAssets assetConfigParameters:]
-[LSRSpeechAssets purgeAssetsForLanguage:completion:]
CESRTrialAssetManager
Unable to find class %s
CESRAssetConfig
v32@?0@"NSArray"8Q16^B24
-[LSRAudioDump initWithTaskIdentifier:]
-[LSRAudioDump appendBytes:]
-[LSRAudioDump close]
Library/Caches/com.apple.speech.localspeechrecognition/lsr_audio_dumps
%@-%04d%02d%02d-%02d%02d%02d.%d.wav
com.apple.assistant.dictation.prerecorded
voicemail-confidence-parameters
app-lm
+[_EARSpeechRecognizer(LSRConnection) _speechRecognizerWithLanguage:overrides:anyConfiguration:taskConfiguration:modelRoot:error:modelLoadTime:]
keepANEModelLoaded
itn_s.enh
Failed to create recognizer from=%@
-[LSRConnection initWithXPCConnection:]_block_invoke
-[LSRConnection dealloc]
-[LSRConnection _consumeSandboxExtensions:error:]
Error consuming sandbox extension: %s
file-write-create
-[LSRConnection _callerHasWritePriviledge:]
-[LSRConnection modelRootWithLanguage:modelOverrideURL:returningAssetType:error:]
-[LSRConnection _jitProfileWithLanguage:modelRoot:]
-[LSRConnection _requestContext]
none
-[LSRConnection prepareRecognizerWithLanguage:recognitionOverrides:modelOverrideURL:anyConfiguration:task:error:]
modelRoot is nil
Failed to access assets
_EARSpeechRecognizer is nil
Failed to initialize recognizer
-[LSRConnection preloadRecognizerForLanguage:task:recognitionOverrides:modelOverrideURL:completion:]
-[LSRConnection startRecordedAudioDictationWithParameters:]
Recognizer is busy
%@.%@
task name is nil
Failed to find task for recognizer
@"NSDictionary"8@?0
v32@?0@"NSString"8@"NSString"16^B24
-[LSRConnection addAudioPacket:]_block_invoke
-[LSRConnection stopSpeech]
-[LSRConnection cancelSpeech]
-[LSRConnection initializeWithSandboxExtensions:]_block_invoke
+[LSRConnection _scheduleCooldownTimer]
+[LSRConnection _cancelCooldownTimer]
+[LSRConnection _cooldownTimerFired]
-[LSRConnection downloadAssetsForLanguage:urgent:forceUpgrade:detailedProgress:]
v20@?0Q8B16
-[LSRConnection downloadAssetsForLanguage:urgent:forceUpgrade:detailedProgress:]_block_invoke_4
v24@?0@"NSString"8@"NSError"16
v20@?0d8B16
v16@?0Q8
com.apple.speech.localspeechrecognition.detailedProgressWait
-[LSRConnection fetchAssetsForLanguage:completion:]
v16@?0@"NSDictionary"8
-[LSRConnection configParametersForVoicemailWithLanguage:completion:]
-[LSRConnection configParametersForVoicemailWithLanguage:completion:]_block_invoke
-[LSRConnection initializeLmWithLocale:completion:]
Error initializing
-[LSRConnection initializeLmWithAssetPath:completion:]
-[LSRConnection addSentenceToNgramCounts:]
-[LSRConnection addSentenceToNgramCounts:completion:]
-[LSRConnection oovWordsAndFrequenciesWithCompletion:]
v24@?0@"NSURL"8@"NSURL"16
-[LSRConnection trainFromPlainTextAndWriteToDirectory:sandboxExtension:completion:]
SpeechProfile
-[LSRConnection trainAppLmFromNgramsSerializedData:customPronsData:language:writeToDirectory:sandboxExtension:completion:]
v16@?0@"NSURL"8
-[LSRConnection createPhraseCountsArtifactWithIdentifier:rawPhraseCountsPath:customPronunciationsPath:writeToDirectory:sandboxExtensions:completion:]
PhraseCountsFile
-[LSRConnection createNgramCountsArtifactFromPhraseCountArtifact:writeToDirectory:sandboxExtensions:completion:]
NgramCountsFile
-[LSRConnection trainAppLmFromNgramCountsArtifact:language:writeToDirectory:sandboxExtensions:completion:]
AppLmModelFile
-[LSRConnection purgeAssetsForLanguage:completion:]
-[LSRConnection speechRecognizer:didRecognizePartialResult:]_block_invoke
v32@?0@"_EARSpeechRecognitionToken"8Q16^B24
-[LSRConnection speechRecognizer:didRecognizeFinalResultPackage:]_block_invoke
-[LSRConnection speechRecognizer:didFinishRecognitionWithError:]_block_invoke
No speech detected
Recognition request was canceled
Failed to recognize the request
Unsupported EAR build
CheckPrerecordedConnectionAccess
Siri and Dictation are disabled
Task %@ not available for %@, supported tasks are %@
etiquette.json
ReplacementDictionaryForLanguage
ReplacementDictionaryForLanguage_block_invoke
v32@?0@"NSString"8@16^B24
voicemail_confidence_subtraction.json
sampling rate = %@
task type = %@
far field = %@
device ID = %@
Bluetooth device ID = %@
hasSPIAccess
%s %s is not available, as LSRError is not designed to be instantiated. Returning nil.
%s Failed to train model successfully
%s Model trained successfully!
%s Writing to: %@
%s Write process failed
%s Failed to serialize ngram counts data
%s Successfully serialized ngram counts and OOVs
%s Failed to create user profile object
%s Failed to create user profile
%s Wrote profile data to file:%@
%s Failed to write LME
%s Successfully wrote LME: %@
%s No oovs in training data
%s Failed to train from ngram counts
%s Failed to write trained AppLm
%s Successfully wrote app-lm to path=%@
%s Exception while training.
%s Failed to train app-specific LM
%s Failed to read ngram counts artifact at: %@
%s Failed to load custom prons from artifact
%s Artifact contained invalid custom prons; skipping. %@
%s Could not get OOVs from artifact
_set_user_dir_suffix failed
: %{darwin.errno}d
%s Failed to query Trial with error=%@
%s Found %lu installed assets
%s Failed to read json file: %@
%s Failed to parse json config file:%@
%s Failed to create audio dump file. Error: %@, URL: %@
%s Failed to extend audio dump file. Error: %@, URL: %@
%s Failed to prewarm dump file. Error: %@, URL: %@
%s Created audio file at: %@
%s Failed to write to dump file. Error: %@, URL: %@
%s Closed audio dump file at: %@
%s Could not locate asset.
%s Failed to create recognizer from: %@
%s Created Recognizer in %lf sec
%s Handling SIGTERM
%s %@ cancelling instance %@
%s %@ deallocating
%s Consuming sandbox extensions: %@
%s Error consuming sandbox extension: %s
%s Caller has write priviledges for: %@
%s Caller does not have write priviledges for: %@
%s Fetch asset error %@
%s Asset path: %@
%s Could not locate asset
%s Using cached JIT profile builder for language=%@ modelRoot=%@
%s Initialized a new JIT profile builder for language=%@ modelRoot=%@
%s RequestContext: language:%@ task:%@ narrowband:%d appname:%@ enableAutoPunctuation:%d
%s Preparing recognizer...
%s %@ %@
%s Using cached recognizer for language=%@ modelRoot=%@ overrides=%@
%s Initializing a new recognizer for language=%@ modelRoot=%@ overrides=%@
%s Created _EARSpeechRecognizer successfully with modelInfo: %@
%s Failed to prepare recognizer in advance: %@
%s Starting..
%s Error: %@
%s %@
%s Audio dumps will be enabled for request: %@ with taskIdentifier: %@
%s Dump will not be created because dump identifier (%@) doesn't match the qualified task identifier (%@).
%s task name is nil
%s Inline LME input size=%lu
%s Failed to build jitData
%s Set JIT profile for the request
%s Failed to build jitProfile with error %@
%s Loaded Dynamic LM: %@
%s Loaded Dynamic Vocab: %@
%s Censor Speech enabled: %d
%s Changing active configuration from 
%@ to 
%s Adding audio packet: %ld
%s stopSpeech - %p
%s cancelSpeech - %p
%s Failed to consume sandbox extensions: %@
%s Successfully consumed sandbox extensions
%s _scheduleCooldownTimer
%s _cancelCooldownTimer
%s _cooldownTimerFired
%s Request not authorized
%s Failed to download Trial assets with error=%@
%s Failed to find voicemail config parameters
%s Failed to read json config parameters from file: %@
%s Failed to find asset path for language:%@
%s Initialize Language Model with locale=%@
%s Initialize Language Model with assetPath=%@
%s Adding sentence=%@
%s Adding oovs from sentence
%s oovs words and frequencies
%s Write failed error:%@
%s Write successfull
%s Starting training of App LM from ngram count.
%s Finished training of App LM from ngram count.
%s Recognized %lu partial result tokens.
%s package isFinal=%s
%s connection is %@, error %@
%s _recognitionBeginTime (%@) is greater than recognitionEndTime (%@)
%s audioDuration: %lf
%s recognitionDuration: %lf
%s responseTime(Audio finish to recognizer finish): %lf
%s modelLoadTime: %lf
%s TTFW: %lf
%s cpuRtf: %lf
%s jitLmeProcessingTime: %lf
%s asrInitializationTime: %lf
%s _recognitionResultIsFinal=%d _recognitionResultPackageGenerated=%d
%s Called from outside of an XPC connection
%s Could not read %@: %@
%s Could not parse %@: %@
%s %@ is wrong type: %@
%s %@ contains bogus key/value pair: %@ => %@
LSRError
LSRLanguageModel
ServiceDelegate
NSXPCListenerDelegate
NSObject
LSRAdditions
LSRSpeechAssets
LSRUtilities
LSRAudioDump
LSRConnection
_EARSpeechRecognitionResultStream
SFLSRProtocol
_fileID
JSONObjectWithData:options:error:
_requestContext
T@"NSString",R,C
allKeys
URLByAppendingPathComponent:
confidenceScore
UTF8String
dealloc
UUIDString
downloadAssetsForLanguage:urgent:forceUpgrade:detailedProgress:
_appLmData
getLocalFileUrl
_assetQueryForLanguage:
initWithLocale:
_audioDump
isFinal
_auditToken
isValid
_bufferedAudioPackets
metrics
_cancelCooldownTimer
numberWithBool:
_consumeSandboxExtensions:error:
recognizedResultFromEARPackage:
_cooldownTimerFired
serviceListener
_extendedFile
setExtraLmList:
_firstAudioPacketTime
stringByAppendingPathComponent:
_initWithBestTranscription:rawTranscription:final:speechRecognitionMetadata:
validationError
_jitProfileWithLanguage:modelRoot:
.cxx_destruct
_modelAssetType
T#,R
addAudioPacket:
TQ,R
applicationName
URLByAppendingPathComponent:isDirectory:
containsObject:
UUID
discard
_UUID
_asrInitializationTime
initWithConfig:
_audioBuffer
interpretations
_audioDuration
isProxy
_bufferedAudioEnded
lexemes
_callerHasWritePriviledge:
_connection
oneBest
_consumedSandboxExtensions
release
_delegate
_fetchAssetPathForInstalledLanguage:assetType:triggerDownload:outError:
setInputFormat:
_firstWordTime
transcriptionFromSpeechPhrases:afAudioAnalytics:utteranceStart:
_jitLmeProcessingTime
version
_languageCode
_lastAudioPacketTime
_lastRecognitionResult
_lsrLanguageModel
_lsr_isInstalled
_lsr_language
_lsr_path
_lsr_quasarModelPath
_modelLoadTime
_modelRoot
_ncsRootPath
_recognitionBeginTime
_recognitionResultIsFinal
_recognitionResultPackageGenerated
_recognizer
_recognizerConfigFilePath
_requestParameters
_scheduleCooldownTimer
_setQueue:
_silStartFirstToken
_speechRecognizerWithLanguage:overrides:anyConfiguration:taskConfiguration:modelRoot:error:modelLoadTime:
_url
_userProfileWithModelRoot:language:
acousticFeatureValuePerFrame
acousticFeatures
activeConfiguration
activeConfigurationForEverything
addAudioSamples:count:
addEntriesFromDictionary:
addKeyValuePair:with:
addNgramCountWithType:content:
addObject:
addOovTokensFromSentence:
addProns:forWord:
addProns:forWord:completion:
addSentenceToNgramCounts:
addSentenceToNgramCounts:completion:
addSentenceWithType:uuid:content:
addWordWithParts:templateName:
appendBytes:
array
arrayByAddingObject:
arrayWithObjects:count:
assetConfigParameters:
assistantIsEnabled
attributes
audioAnalytics
auditToken
autorelease
bluetoothDeviceIdFilter
boolValue
buildLmWithConfig:root:data:dir:shouldStop:
bundleIdentifier
bytes
cancelRecognition
cancelSpeech
class
close
code
componentsSeparatedByCharactersInSet:
concatenateUtterances
confidence
configParametersForVoicemailWithLanguage:completion:
conformsToProtocol:
copy
count
countByEnumeratingWithState:objects:count:
createAppLmLmeProfileWithLanguage:modelRoot:customPronsData:newOovs:writeToVocabFile:
createDirectoryAtURL:withIntermediateDirectories:attributes:error:
createNgramCountsArtifactFromPhraseCountArtifact:saveTo:
createNgramCountsArtifactFromPhraseCountArtifact:writeToDirectory:sandboxExtensions:completion:
createPhraseCountArtifactWithIdentifier:rawPhraseCountsPath:customPronunciationsPath:saveTo:
createPhraseCountsArtifact:version:locale:rawPhraseCountsPath:customPronunciationsPath:saveTo:
createPhraseCountsArtifactWithIdentifier:rawPhraseCountsPath:customPronunciationsPath:writeToDirectory:sandboxExtensions:completion:
createSpeechProfileFromOovs:customProns:language:
currentConnection
dataProfile
dataWithContentsOfFile:
dataWithContentsOfFile:options:error:
debugDescription
defaultManager
description
deserializeModelData:
deserializeNgramCountsData:
deserializeNgramCountsData:completion:
detectMultipleUtterances
deviceIdFilter
dictationIsEnabled
dictionary
dictionaryWithDictionary:
dictionaryWithObjects:forKeys:count:
domain
doubleValue
downloadAssetOfType:language:urgent:forceUpgrade:progressHandler:completionHandler:
downloadStatusWithConfig:progressHandler:completionHandler:
dumpDirectory
dynamicLanguageModel
dynamicVocabulary
effectiveBoolValueForSetting:
emptyResultWithIsFinal:
enableAutoPunctuation
endAudio
enumerateKeysAndObjectsUsingBlock:
enumerateObjectsUsingBlock:
errorWithCode:
errorWithCode:description:
errorWithDomain:code:userInfo:
farFieldFilter
fetchAssetPathForInstalledLanguage:outError:
fetchAssetsForLanguage:completion:
fileExistsAtPath:
fileURLWithPath:
filenameForTaskIdentifier:
firstObject
frameDuration
frequency
generateNgramCountsSerializeDataWithCompletion:
generateNgramCountsWithConfig:root:data:
getProns
hasSpaceAfter
hash
indexOfObject:
init
initWithAcousticFeatureValue:frameDuration:
initWithAssetPath:
initWithCapacity:
initWithConfiguration:
initWithConfiguration:language:overrides:sdapiOverrides:generalVoc:emptyVoc:pgVoc:lexiconEnh:tokenEnh:paramsetHolder:
initWithConfiguration:ncsRoot:language:
initWithConfiguration:ncsRoot:recognizerConfigPath:
initWithConfiguration:overrides:overrideConfigFiles:generalVoc:lexiconEnh:itnEnh:language:activeConfiguration:modelLoadingOptions:enableSpeakerCodeTraining:supportEmojiRecognition:
initWithConfiguration:root:
initWithContentsOfFile:
initWithData:
initWithInterpretationIndices:confidenceScore:
initWithLanguage:
initWithLanguage:assetType:
initWithOrthography:pronunciations:tagName:frequency:
initWithPath:
initWithSpeechRecognitionFeatures:acousticFeatures:
initWithTaskIdentifier:
initWithTokenName:start:end:silenceStart:confidence:hasSpaceAfter:hasSpaceBefore:phoneSequence:ipaPhoneSequence:
initWithType:
initWithXPCConnection:
initialize
initializeLmWithAssetPath:completion:
initializeLmWithLocale:completion:
initializeWithSandboxExtensions:
inlineItemList
installationStatusForLanguagesForAssetType:includeDetailedStatus:error:
installedAssetWithConfig:regionId:triggerDownload:
installedLanguagesWithCompletion:
interpretationIndices
invalidate
ipaPhoneSequence
isEqual:
isEqualToDictionary:
isEqualToString:
isKindOfClass:
isMemberOfClass:
jitProfileFromContextualStrings:
language
lastObject
length
listener:shouldAcceptNewConnection:
lmeThreshold
lmeThresholdWithCompletion:
loadCustomPronData:ncsRoot:dataRoot:
loadLmFromDir:
loadOovs
localSpeechRecognitionDidDownloadAssetsWithProgress:isStalled:
localSpeechRecognitionDidFail:
localSpeechRecognitionDidFinishDownloadingAssets:error:
localSpeechRecognitionDidFinishRecognition:
localSpeechRecognitionDidProcessAudioDuration:
localSpeechRecognitionDidRecognizePartialResult:rawPartialResult:
localSpeechRecognitionDidSucceed
localizedDescription
logCoreAnalyticsEvent:withAnalytics:
mainBundle
maximumRecognitionDuration
metricsWithCompletion:
modelInfo
modelOverrideURL
modelRootWithLanguage:modelOverrideURL:returningAssetType:error:
narrowband
newlineCharacterSet
ngramCountsSerializeData
numberWithDouble:
numberWithInt:
numberWithInteger:
numberWithLongLong:
numberWithUnsignedInteger:
nvAsrPhoneSequenceForXsampaProns:
objectAtIndex:
objectAtIndexedSubscript:
objectForKey:
objectForKeyedSubscript:
onDeviceOnly
oovWordsAndFrequenciesInNgramCount
oovWordsAndFrequenciesWithCompletion:
oovsFromSentenceAddedToNgramCounts:
orderedOovs
orderedSetWithArray:
orthography
path
performSelector:
performSelector:withObject:
performSelector:withObject:withObject:
phoneSequence
preITNRecognition
preITNTokens
preloadRecognizerForLanguage:task:recognitionOverrides:modelOverrideURL:completion:
prepareRecognizerWithLanguage:recognitionOverrides:modelOverrideURL:anyConfiguration:task:error:
processInfo
promoteAssets
promoteAssetsForAssetType:
prons
pronunciationsForOrthography:
purgeAssetsForLanguage:completion:
purgeInstalledAssetForAssetType:language:regionId:error:
raise:format:
recognition
recognitionMetadataFromSpeechPhrases:afAudioAnalytics:utteranceStart:
recognitionOverrides
recognitionStatistics
releaseClients
remoteObjectProxy
removeAllObjects
removeItemAtURL:error:
reportFailureWithError:
requestIdentifier
respondsToSelector:
resume
retain
retainCount
returnTypes:
runRecognitionWithResultStream:language:task:samplingRate:
samplingRateFilter
sanitizedStringWithString:
self
serializedModelWithLanguage:modelData:oovs:
setActiveConfiguration:
setAssetsAsProvisional
setAssetsProvisionalForAssetType:
setBluetoothDeviceIdFilter:
setByAddingObject:
setCachedJitProfileBuilder:
setCachedLanguage:
setCachedModelRoot:
setConcatenateUtterances:
setConfidenceScore:
setDelegate:
setDetectUtterances:
setDeviceIdFilter:
setDisableAutoPunctuation:
setEnableSpeakerCodeTraining:
setEndTime:
setExportedInterface:
setExportedObject:
setFarFieldFilter:
setInterpretationIndices:
setInterpretations:
setInterruptionHandler:
setInvalidationHandler:
setIpaPhoneSequence:
setIsLowConfidence:
setJitProfileData:
setMaximumRecognitionDuration:
setObject:forKey:
setPhoneSequence:
setRecognitionConfidenceSubtraction:
setRecognitionReplacements:
setRecognizeEagerCandidates:
setRemoteObjectInterface:
setRemoveSpaceBefore:
setSamplingRateFilter:
setSilenceStartTime:
setSource:
setStartTime:
setTaskTypeFilter:
setText:
setTokens:
setUserProfileData:
setValue:forKey:
setWithObject:
setXsampaProns:forWord:
sharedConnection
sharedInstance
sharedPreferences
sigterm_queue
sigterm_source
silenceStart
speechRecognitionFeatures
speechRecognizer:didFinishRecognitionWithError:
speechRecognizer:didProcessAudioDuration:
speechRecognizer:didProduceEndpointFeaturesWithWordCount:trailingSilenceDuration:eosLikelihood:pauseCounts:silencePosterior:processedAudioDurationInMilliseconds:
speechRecognizer:didProduceLoggablePackage:
speechRecognizer:didRecognizeFinalResultCandidatePackage:
speechRecognizer:didRecognizeFinalResultPackage:
speechRecognizer:didRecognizeFinalResults:
speechRecognizer:didRecognizeFinalResults:tokenSausage:nBestChoices:
speechRecognizer:didRecognizePartialResult:
speechRecognizer:didRecognizePartialResultNbest:
speechRecognizer:didRecognizeRawEagerRecognitionCandidate:
speechRecognizer:didReportStatus:statusContext:
standardUserDefaults
start
startRecordedAudioDictationWithParameters:
state
stopSpeech
stringByDeletingLastPathComponent
stringByReplacingOccurrencesOfString:withString:
stringForKey:
stringValue
stringWithFormat:
superclass
supportedLanguagesWithAssetType:
switchToNewAssetsForAssetType:
systemUptime
task
taskIdentifier
taskTypeFilter
tasks
tokenName
tokenSausage
tokens
trainAppLmFromNgramCountsArtifact:language:appLmArtifact:vocabFile:
trainAppLmFromNgramCountsArtifact:language:writeToDirectory:sandboxExtensions:completion:
trainAppLmFromNgramsSerializedData:customPronsData:language:writeToAppLmDir:vocabFile:
trainAppLmFromNgramsSerializedData:customPronsData:language:writeToDirectory:sandboxExtension:completion:
trainAppLmFromPlainTextAndWriteToAppDirectory:vocabFile:
trainFromPlainTextAndWriteToDirectory:sandboxExtension:completion:
transcriptionsWithEARTokens:
transcriptionsWithTokens:
transitionArtifactAt:toStage:configPath:ncsRoot:dataRoot:estimationRoot:minimize:saveTo:
unsignedIntegerValue
updateAudioDuration:
urlForTaskIdentifier:
utteranceStart
valueForEntitlement:
wait
wakeUpWithCompletion:
writeToDirectory:
writeToURL:atomically:
zone
v16@0:8
@32@0:8q16@24
@24@0:8q16
@16@0:8
@32@0:8@16@24
B56@0:8@16@24@32@40@48
@24@0:8@16
v24@0:8@16
B32@0:8@16@24
v32@0:8^@16^@24
q16@0:8
v56@0:8@16@24@32^@40^@48
v48@0:8@16@24@32@40
v32@0:8@16@24
@40@0:8@16@24@32
v48@0:8@16@24^@32^@40
@"_EARAppLmData"
@"NSString"
B24@0:8@16
#16@0:8
@24@0:8:16
@32@0:8:16@24
@40@0:8:16@24@32
B16@0:8
B24@0:8#16
B24@0:8:16
Vv16@0:8
Q16@0:8
^{_NSZone=}16@0:8
B24@0:8@"Protocol"16
@"NSString"16@0:8
B32@0:8@"NSXPCListener"16@"NSXPCConnection"24
v24@0:8@?16
@32@0:8@16^@24
@44@0:8@16Q24B32^@36
v32@0:8@16@?24
@20@0:8B16
@"NSURL"
^{OpaqueAudioFileID=}
^{OpaqueExtAudioFile=}
@68@0:8@16@24B32@36@44^@52^d60
v32@0:8@16d24
v72@0:8@16q24q32d40@48d56q64
v40@0:8@16Q24@32
v32@0:8@"_EARSpeechRecognizer"16@"_EARSpeechRecognitionResult"24
v32@0:8@"_EARSpeechRecognizer"16@"NSError"24
v32@0:8@"_EARSpeechRecognizer"16@"NSArray"24
v48@0:8@"_EARSpeechRecognizer"16@"NSArray"24@"NSArray"32@"NSArray"40
v32@0:8@"_EARSpeechRecognizer"16@"_EARSpeechRecognitionResultPackage"24
v32@0:8@"_EARSpeechRecognizer"16d24
v32@0:8@"_EARSpeechRecognizer"16@"_EARSpeechRecognition"24
v72@0:8@"_EARSpeechRecognizer"16q24q32d40@"NSArray"48d56q64
v40@0:8@"_EARSpeechRecognizer"16Q24@"NSDictionary"32
Vv24@0:8@16
Vv56@0:8@16@24@32@40@?48
Vv24@0:8@?16
Vv36@0:8@16B24B28B32
Vv32@0:8@16@?24
Vv40@0:8@16@24@?32
Vv64@0:8@16@24@32@40@48@?56
Vv48@0:8@16@24@32@?40
Vv24@0:8@"SFRequestParameters"16
Vv24@0:8@"NSData"16
Vv56@0:8@"NSString"16@"NSString"24@"NSDictionary"32@"NSURL"40@?<v@?@"NSError">48
Vv24@0:8@?<v@?@"NSSet">16
v24@0:8@"NSArray"16
v32@0:8@"NSString"16@"NSDictionary"24
Vv36@0:8@"NSString"16B24B28B32
Vv32@0:8@"NSString"16@?<v@?@"NSString"@"NSError">24
Vv32@0:8@"NSString"16@?<v@?@"NSDictionary">24
Vv32@0:8@"NSString"16@?<v@?@"NSError">24
Vv32@0:8@"NSLocale"16@?<v@?@"NSError">24
Vv24@0:8@"NSString"16
Vv32@0:8@"NSString"16@?<v@?@"NSArray">24
Vv40@0:8@"NSSet"16@"NSString"24@?<v@?B>32
Vv24@0:8@?<v@?@"NSDictionary">16
Vv40@0:8@"NSURL"16@"NSString"24@?<v@?@"NSURL"@"NSURL">32
Vv24@0:8@?<v@?@"NSData">16
Vv32@0:8@"NSData"16@?<v@?@"NSDictionary">24
Vv24@0:8@?<v@?q>16
Vv24@0:8@?<v@?>16
Vv64@0:8@"NSData"16@"NSData"24@"NSString"32@"NSURL"40@"NSString"48@?<v@?@"NSURL"@"NSURL">56
Vv64@0:8@"NSString"16@"NSURL"24@"NSURL"32@"NSURL"40@"NSArray"48@?<v@?@"NSURL">56
Vv48@0:8@"NSURL"16@"NSURL"24@"NSArray"32@?<v@?@"NSURL">40
Vv56@0:8@"NSURL"16@"NSString"24@"NSURL"32@"NSArray"40@?<v@?@"NSURL"@"NSURL">48
B32@0:8@16^@24
{?=[8I]}16@0:8
@48@0:8@16@24^Q32^@40
B60@0:8@16@24@32B40@44^@52
@"_EARSpeechRecognizer"
@"_EARSpeechRecognitionAudioBuffer"
@"NSXPCConnection"
@"NSMutableArray"
@"SFSpeechRecognitionResult"
@"SFRequestParameters"
@"LSRAudioDump"
@"NSObject<OS_dispatch_queue>"
@"NSObject<OS_dispatch_source>"
@"LSRLanguageModel"
softlink:r:path:/System/Library/PrivateFrameworks/CoreEmbeddedSpeechRecognition.framework/CoreEmbeddedSpeechRecognition
softlink:r:path:/System/Library/PrivateFrameworks/CoreEmbeddedSpeechRecognition.framework/CoreEmbeddedSpeechRecognition
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
<key>application-identifier</key>
<string>com.apple.speech.localspeechrecognition</string>
<key>com.apple.application-identifier</key>
<string>com.apple.speech.localspeechrecognition</string>
<key>com.apple.private.assets.accessible-asset-types</key>
<array>
<string>com.apple.MobileAsset.EmbeddedSpeech</string>
<string>com.apple.MobileAsset.EmbeddedSpeechMac</string>
<string>com.apple.MobileAsset.Trial.Siri.SiriUnderstandingAsrAssistant</string>
<string>com.apple.MobileAsset.Trial.Siri.SiriUnderstandingAsrHammer</string>
<string>com.apple.MobileAsset.Trial.Siri.SiriDictationAssets</string>
</array>
<key>com.apple.private.assets.bypass-asset-types-check</key>
<true/>
<key>com.apple.private.sandbox.profile:embedded</key>
<string>temporary-sandbox</string>
<key>com.apple.private.tcc.manager.check-by-audit-token</key>
<array>
<string>kTCCServiceSpeechRecognition</string>
</array>
<key>com.apple.proactive.eventtracker</key>
<true/>
<key>com.apple.security.exception.files.absolute-path.read-only</key>
<array>
<string>/private/var/MobileAsset/</string>
</array>
<key>com.apple.security.exception.files.home-relative-path.read-write</key>
<array>
<string>/Library/Caches/com.apple.speech.localspeechrecognition/</string>
</array>
<key>com.apple.security.exception.iokit-user-client-class</key>
<array>
<string>IOSurfaceRootUserClient</string>
<string>H11ANEInDirectPathClient</string>
</array>
<key>com.apple.security.exception.mach-lookup.global-name</key>
<array>
<string>com.apple.appleneuralengine</string>
<string>com.apple.triald.namespace-management</string>
<string>com.apple.mobileasset.autoasset</string>
<string>com.apple.FileCoordination</string>
</array>
<key>com.apple.security.exception.shared-preference.read-only</key>
<array>
<string>com.apple.assistant.support</string>
<string>com.apple.speech.localspeechrecognition</string>
<string>com.apple.corevideo</string>
</array>
<key>com.apple.security.ts.mobile-keybag-access</key>
<true/>
<key>com.apple.security.ts.opengl-or-metal</key>
<true/>
<key>com.apple.security.ts.tmpdir</key>
<string>com.apple.speech.localspeechrecognition</string>
<key>com.apple.trial.client</key>
<array>
<string>372</string>
<string>401</string>
<string>751</string>
</array>
<key>platform-application</key>
<true/>
</dict>
</plist>
L0A
application-identifier'com.apple.speech.localspeechrecognition0K com.apple.application-identifier'com.apple.speech.localspeechrecognition0
7/com.apple.private.assets.accessible-asset-types0
$com.apple.MobileAsset.EmbeddedSpeech'com.apple.MobileAsset.EmbeddedSpeechMac>com.apple.MobileAsset.Trial.Siri.SiriUnderstandingAsrAssistant;com.apple.MobileAsset.Trial.Siri.SiriUnderstandingAsrHammer4com.apple.MobileAsset.Trial.Siri.SiriDictationAssets061com.apple.private.assets.bypass-asset-types-check
0?*com.apple.private.sandbox.profile:embedded
temporary-sandbox0T2com.apple.private.tcc.manager.check-by-audit-token0
kTCCServiceSpeechRecognition0% com.apple.proactive.eventtracker
0Y:com.apple.security.exception.files.absolute-path.read-only0
/private/var/MobileAsset/0~@com.apple.security.exception.files.home-relative-path.read-write0:8/Library/Caches/com.apple.speech.localspeechrecognition/0k4com.apple.security.exception.iokit-user-client-class03
IOSurfaceRootUserClient
H11ANEInDirectPathClient0
4com.apple.security.exception.mach-lookup.global-name0
com.apple.appleneuralengine%com.apple.triald.namespace-management
com.apple.mobileasset.autoasset
com.apple.FileCoordination0
8com.apple.security.exception.shared-preference.read-only0[
com.apple.assistant.support'com.apple.speech.localspeechrecognition
com.apple.corevideo0/*com.apple.security.ts.mobile-keybag-access
0*%com.apple.security.ts.opengl-or-metal
com.apple.security.ts.tmpdir'com.apple.speech.localspeechrecognition0)
com.apple.trial.client0
372
401
7510
platform-application
