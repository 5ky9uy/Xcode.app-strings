@(#)PROGRAM:localspeechrecognition  PROJECT:SpeechRecognition-1
kLSRErrorDomain
+[LSRError new]
-[LSRError init]
app-lm.NGRAM
\unknown-first
\NT-appvocab
mini.json
%@-%@
-[LSRLanguageModel trainAppLmFromPlainTextAndWriteToAppDirectory:vocabFile:]
-[LSRLanguageModel ngramCountsSerializeData]
en_US_napg.json
dispatch.voc
vocdelta.voc
pg.voc
lexicon.enh
token_s.enh
mrec.psh
+[LSRLanguageModel createAppLmLmeProfileWithLanguage:modelRoot:customPronsData:newOovs:writeToVocabFile:]
v32@?0@"NSString"8Q16^B24
v24@?0@"NSString"8^B16
v32@?0@"NSString"8@"NSSet"16^B24
-[LSRLanguageModel trainAppLmFromNgramsSerializedData:customPronsData:language:writeToAppLmDir:vocabFile:]
-[LSRLanguageModel createSpeechProfileFromOovs:customProns:language:]
-[LSRLanguageModel trainAppLmFromNgramCountsArtifact:language:appLmArtifact:vocabFile:]
v32@?0@"_EAROovToken"8Q16^B24
com.apple.speech.localspeechrecognition
xpcservice
isTrial
modelRoot
assetType
QuasarDir
Language
v8@?0
-[LSRSpeechAssets installedLanguagesWithCompletion:]
+[LSRSpeechAssets assetConfigParameters:]
-[LSRSpeechAssets purgeMAAssetForLanguage:outError:]
-[LSRSpeechAssets purgeAssetsForLanguage:completion:]
CESRTrialAssetManager
Unable to find class %s
CESRAssetConfig
v32@?0@"NSArray"8Q16^B24
-[LSRAudioDump initWithTaskIdentifier:]
-[LSRAudioDump appendBytes:]
-[LSRAudioDump close]
Library/Caches/com.apple.speech.localspeechrecognition/lsr_audio_dumps
%@-%04d%02d%02d-%02d%02d%02d.wav
com.apple.assistant.dictation.prerecorded
voicemail-confidence-parameters
app-lm
+[_EARSpeechRecognizer(LSRConnection) _speechRecognizerWithLanguage:overrides:anyConfiguration:taskConfiguration:modelRoot:error:modelLoadTime:]
keepANEModelLoaded
itn_s.enh
Failed to create recognizer from=%@
-[LSRConnection initWithXPCConnection:]_block_invoke
-[LSRConnection dealloc]
-[LSRConnection _consumeSandboxExtensions:error:]
Error consuming sandbox extension: %s
file-write-create
-[LSRConnection _callerHasWritePriviledge:]
-[LSRConnection modelRootWithLanguage:modelOverrideURL:returningAssetType:error:]
-[LSRConnection _jitProfileWithLanguage:modelRoot:]
-[LSRConnection _requestContext]
none
-[LSRConnection prepareRecognizerWithLanguage:recognitionOverrides:modelOverrideURL:anyConfiguration:task:error:]
modelRoot is nil
Failed to access assets
_EARSpeechRecognizer is nil
Failed to initialize recognizer
-[LSRConnection preloadRecognizerForLanguage:task:recognitionOverrides:modelOverrideURL:completion:]
-[LSRConnection startRecordedAudioDictationWithParameters:]
Recognizer is busy
%@.%@
task name is nil
Failed to find task for recognizer
@"NSDictionary"8@?0
v32@?0@"NSString"8@"NSString"16^B24
-[LSRConnection addAudioPacket:]_block_invoke
-[LSRConnection stopSpeech]
-[LSRConnection cancelSpeech]
-[LSRConnection initializeWithSandboxExtensions:]_block_invoke
+[LSRConnection _scheduleCooldownTimer]
+[LSRConnection _cancelCooldownTimer]
+[LSRConnection _cooldownTimerFired]
-[LSRConnection downloadAssetsForLanguage:urgent:forceUpgrade:detailedProgress:]
v20@?0Q8B16
-[LSRConnection downloadAssetsForLanguage:urgent:forceUpgrade:detailedProgress:]_block_invoke
v24@?0@"NSString"8@"NSError"16
v20@?0d8B16
v16@?0Q8
com.apple.speech.localspeechrecognition.detailedProgressWait
-[LSRConnection fetchAssetsForLanguage:completion:]
v16@?0@"NSDictionary"8
-[LSRConnection configParametersForVoicemailWithLanguage:completion:]
-[LSRConnection configParametersForVoicemailWithLanguage:completion:]_block_invoke
-[LSRConnection initializeLmWithLocale:completion:]
Error initializing
-[LSRConnection initializeLmWithAssetPath:completion:]
-[LSRConnection addSentenceToNgramCounts:]
-[LSRConnection addSentenceToNgramCounts:completion:]
-[LSRConnection oovWordsAndFrequenciesWithCompletion:]
v24@?0@"NSURL"8@"NSURL"16
-[LSRConnection trainFromPlainTextAndWriteToDirectory:sandboxExtension:completion:]
SpeechProfile
-[LSRConnection trainAppLmFromNgramsSerializedData:customPronsData:language:writeToDirectory:sandboxExtension:completion:]
v16@?0@"NSURL"8
-[LSRConnection createPhraseCountsArtifactWithIdentifier:rawPhraseCountsPath:customPronunciationsPath:writeToDirectory:sandboxExtensions:completion:]
PhraseCountsFile
-[LSRConnection createNgramCountsArtifactFromPhraseCountArtifact:writeToDirectory:sandboxExtensions:completion:]
NgramCountsFile
-[LSRConnection trainAppLmFromNgramCountsArtifact:language:writeToDirectory:sandboxExtensions:completion:]
AppLmModelFile
-[LSRConnection purgeAssetsForLanguage:completion:]
-[LSRConnection speechRecognizer:didRecognizePartialResult:]_block_invoke
v32@?0@"_EARSpeechRecognitionToken"8Q16^B24
-[LSRConnection speechRecognizer:didRecognizeFinalResultPackage:]_block_invoke_2
-[LSRConnection speechRecognizer:didFinishRecognitionWithError:]_block_invoke
No speech detected
Recognition request was canceled
Failed to recognize the request
Unsupported EAR build
CheckPrerecordedConnectionAccess
Siri and Dictation are disabled
Task %@ not available for %@, supported tasks are %@
etiquette.json
ReplacementDictionaryForLanguage
ReplacementDictionaryForLanguage_block_invoke
v32@?0@"NSString"8@16^B24
voicemail_confidence_subtraction.json
sampling rate = %@
task type = %@
far field = %@
device ID = %@
Bluetooth device ID = %@
hasSPIAccess
initWithCapacity:
setObject:forKey:
count
errorWithDomain:code:userInfo:
errorWithCode:description:
initialize
errorWithCode:
init
objectForKey:
stringWithFormat:
stringByReplacingOccurrencesOfString:withString:
fetchAssetPathForInstalledLanguage:outError:
length
initWithAssetPath:
stringByAppendingPathComponent:
initWithConfiguration:ncsRoot:recognizerConfigPath:
initWithConfiguration:
modelInfo
language
UUID
UUIDString
addSentenceWithType:uuid:content:
setXsampaProns:forWord:
addOovTokensFromSentence:
orderedOovs
countByEnumeratingWithState:objects:count:
frequency
numberWithInteger:
orthography
copy
setInputFormat:
buildLmWithConfig:root:data:dir:shouldStop:
path
writeToDirectory:
initWithConfig:
generateNgramCountsWithConfig:root:data:
initWithConfiguration:root:
oovWordsAndFrequenciesInNgramCount
allKeys
serializedModelWithLanguage:modelData:oovs:
deserializeModelData:
lmeThreshold
metrics
initWithConfiguration:language:overrides:sdapiOverrides:generalVoc:emptyVoc:pgVoc:lexiconEnh:tokenEnh:paramsetHolder:
_userProfileWithModelRoot:language:
enumerateObjectsUsingBlock:
initWithData:
lexemes
dictionaryWithDictionary:
initWithLanguage:
nvAsrPhoneSequenceForXsampaProns:
addObject:
enumerateKeysAndObjectsUsingBlock:
sanitizedStringWithString:
pronunciationsForOrthography:
initWithOrthography:pronunciations:tagName:frequency:
arrayWithObjects:count:
addWordWithParts:templateName:
dataProfile
writeToURL:atomically:
objectForKeyedSubscript:
newlineCharacterSet
componentsSeparatedByCharactersInSet:
stringByDeletingLastPathComponent
createAppLmLmeProfileWithLanguage:modelRoot:customPronsData:newOovs:writeToVocabFile:
addNgramCountWithType:content:
createPhraseCountsArtifact:version:locale:rawPhraseCountsPath:customPronunciationsPath:saveTo:
transitionArtifactAt:toStage:configPath:ncsRoot:dataRoot:estimationRoot:minimize:saveTo:
initWithPath:
loadCustomPronData:ncsRoot:dataRoot:
isValid
getProns
prons
validationError
loadOovs
createSpeechProfileFromOovs:customProns:language:
initWithLocale:
addSentenceToNgramCounts:
addProns:forWord:
oovsFromSentenceAddedToNgramCounts:
trainAppLmFromPlainTextAndWriteToAppDirectory:vocabFile:
ngramCountsSerializeData
deserializeNgramCountsData:
trainAppLmFromNgramsSerializedData:customPronsData:language:writeToAppLmDir:vocabFile:
createPhraseCountArtifactWithIdentifier:rawPhraseCountsPath:customPronunciationsPath:saveTo:
createNgramCountsArtifactFromPhraseCountArtifact:saveTo:
trainAppLmFromNgramCountsArtifact:language:appLmArtifact:vocabFile:
.cxx_destruct
_appLmData
_recognizerConfigFilePath
_ncsRootPath
_languageCode
initWithXPCConnection:
setExportedInterface:
setExportedObject:
setRemoteObjectInterface:
resume
isEqual:
class
self
performSelector:
performSelector:withObject:
performSelector:withObject:withObject:
isProxy
isKindOfClass:
isMemberOfClass:
conformsToProtocol:
respondsToSelector:
retain
release
autorelease
retainCount
zone
hash
superclass
description
debugDescription
TQ,R
T#,R
T@"NSString",R,C
listener:shouldAcceptNewConnection:
serviceListener
setDelegate:
attributes
getLocalFileUrl
_lsr_path
state
_lsr_language
_lsr_quasarModelPath
_lsr_isInstalled
initWithType:
returnTypes:
addKeyValuePair:with:
isUnifiedAssetNamespaceEnabled
CESRDictationAssetType
installationStatusForLanguagesForAssetType:includeDetailedStatus:error:
_fetchAssetPathForInstalledLanguage:assetType:triggerDownload:outError:
sharedPreferences
assistantIsEnabled
dictionary
dictionaryWithObjects:forKeys:count:
addEntriesFromDictionary:
numberWithUnsignedLong:
sharedInstance
switchToNewAssetsForAssetType:
initWithLanguage:assetType:
supportedLanguagesWithAssetType:
containsObject:
installedAssetWithConfig:regionId:triggerDownload:
purgeMAAssetForLanguage:outError:
numberWithBool:
initWithContentsOfFile:
JSONObjectWithData:options:error:
_assetQueryForLanguage:
queryMetaDataSync
results
purgeSync
purgeInstalledAssetForAssetType:language:regionId:error:
releaseClients
setAssetsProvisionalForAssetType:
promoteAssetsForAssetType:
assetConfigParameters:
installedLanguagesWithCompletion:
fetchAssetInfoForSpeechRequestForInstalledLanguage:outError:
purgeAssetsForLanguage:completion:
setAssetsAsProvisionalForAssetType:
_CESRDictationAssetType
TQ,R,N,V_CESRDictationAssetType
transcriptionsWithTokens:
audioAnalytics
recognition
tokenSausage
interpretationIndices
utteranceStart
transcriptionFromSpeechPhrases:afAudioAnalytics:utteranceStart:
recognitionMetadataFromSpeechPhrases:afAudioAnalytics:utteranceStart:
preITNRecognition
isFinal
_initWithBestTranscription:rawTranscription:final:speechRecognitionMetadata:
initWithTokenName:start:end:silenceStart:confidence:hasSpaceAfter:hasSpaceBefore:phoneSequence:ipaPhoneSequence:
transcriptionsWithEARTokens:
sharedConnection
effectiveBoolValueForSetting:
recognizedResultFromEARPackage:
emptyResultWithIsFinal:
tokenName
setText:
phoneSequence
setPhoneSequence:
start
setStartTime:
silenceStart
setSilenceStartTime:
setEndTime:
setRemoveSpaceBefore:
hasSpaceAfter
confidence
setConfidenceScore:
ipaPhoneSequence
setIpaPhoneSequence:
acousticFeatures
setValue:forKey:
speechRecognitionFeatures
initWithSpeechRecognitionFeatures:acousticFeatures:
acousticFeatureValuePerFrame
frameDuration
initWithAcousticFeatureValue:frameDuration:
orderedSetWithArray:
array
initWithInterpretationIndices:confidenceScore:
setSource:
objectAtIndex:
objectAtIndexedSubscript:
unsignedIntegerValue
setTokens:
indexOfObject:
numberWithUnsignedInteger:
arrayByAddingObject:
setInterpretationIndices:
confidenceScore
firstObject
lastObject
setInterpretations:
setIsLowConfidence:
interpretations
tokens
urlForTaskIdentifier:
defaultManager
dumpDirectory
createDirectoryAtURL:withIntermediateDirectories:attributes:error:
close
dealloc
bytes
removeItemAtURL:error:
filenameForTaskIdentifier:
URLByAppendingPathComponent:
fileURLWithPath:
initWithTaskIdentifier:
appendBytes:
discard
_url
_fileID
_extendedFile
processInfo
systemUptime
activeConfigurationForEverything
setSamplingRateFilter:
setTaskTypeFilter:
setFarFieldFilter:
setDeviceIdFilter:
setBluetoothDeviceIdFilter:
setWithObject:
initWithConfiguration:overrides:overrideConfigFiles:generalVoc:lexiconEnh:itnEnh:language:activeConfiguration:modelLoadingOptions:enableSpeakerCodeTraining:supportEmojiRecognition:
setDetectUtterances:
setRecognizeEagerCandidates:
setConcatenateUtterances:
setEnableSpeakerCodeTraining:
_speechRecognizerWithLanguage:overrides:anyConfiguration:taskConfiguration:modelRoot:error:modelLoadTime:
_setQueue:
_UUID
cancelRecognition
removeAllObjects
invalidate
setInterruptionHandler:
setInvalidationHandler:
remoteObjectProxy
UTF8String
numberWithLongLong:
auditToken
_auditToken
localizedDescription
unsignedLongValue
isEqualToString:
initWithConfiguration:ncsRoot:language:
task
narrowband
applicationName
enableAutoPunctuation
modelRootWithLanguage:modelOverrideURL:returningAssetType:error:
isEqualToDictionary:
_scheduleCooldownTimer
prepareRecognizerWithLanguage:recognitionOverrides:modelOverrideURL:anyConfiguration:task:error:
onDeviceOnly
reportFailureWithError:
taskIdentifier
standardUserDefaults
stringForKey:
requestIdentifier
recognitionOverrides
modelOverrideURL
maximumRecognitionDuration
setMaximumRecognitionDuration:
detectMultipleUtterances
setDisableAutoPunctuation:
inlineItemList
_jitProfileWithLanguage:modelRoot:
jitProfileFromContextualStrings:
setJitProfileData:
dynamicLanguageModel
loadLmFromDir:
setExtraLmList:
dynamicVocabulary
dataWithContentsOfFile:
setUserProfileData:
setRecognitionReplacements:
setRecognitionConfidenceSubtraction:
activeConfiguration
farFieldFilter
setByAddingObject:
samplingRateFilter
taskTypeFilter
setActiveConfiguration:
runRecognitionWithResultStream:language:task:samplingRate:
_requestContext
version
addAudioSamples:count:
updateAudioDuration:
endAudio
_cancelCooldownTimer
_consumeSandboxExtensions:error:
_cooldownTimerFired
code
stringValue
domain
_delegate
localSpeechRecognitionDidFail:
localSpeechRecognitionDidFinishDownloadingAssets:error:
downloadStatusWithConfig:progressHandler:completionHandler:
localSpeechRecognitionDidDownloadAssetsWithProgress:isStalled:
downloadAssetOfType:language:urgent:forceUpgrade:progressHandler:completionHandler:
wait
fileExistsAtPath:
URLByAppendingPathComponent:isDirectory:
preITNTokens
localSpeechRecognitionDidRecognizePartialResult:rawPartialResult:
oneBest
numberWithInt:
concatenateUtterances
localSpeechRecognitionDidFinishRecognition:
numberWithDouble:
recognitionStatistics
doubleValue
localSpeechRecognitionDidSucceed
localSpeechRecognitionDidProcessAudioDuration:
raise:format:
speechRecognizer:didRecognizePartialResult:
speechRecognizer:didFinishRecognitionWithError:
speechRecognizer:didRecognizeFinalResults:
speechRecognizer:didRecognizeFinalResults:tokenSausage:nBestChoices:
speechRecognizer:didRecognizeFinalResultPackage:
speechRecognizer:didProcessAudioDuration:
speechRecognizer:didRecognizeRawEagerRecognitionCandidate:
speechRecognizer:didProduceEndpointFeaturesWithWordCount:trailingSilenceDuration:eosLikelihood:pauseCounts:silencePosterior:processedAudioDurationInMilliseconds:
speechRecognizer:didRecognizePartialResultNbest:
speechRecognizer:didProduceLoggablePackage:
speechRecognizer:didRecognizeFinalResultCandidatePackage:
startRecordedAudioDictationWithParameters:
stopSpeech
cancelSpeech
addAudioPacket:
preloadRecognizerForLanguage:task:recognitionOverrides:modelOverrideURL:completion:
initializeWithSandboxExtensions:
logCoreAnalyticsEvent:withAnalytics:
downloadAssetsForLanguage:urgent:forceUpgrade:detailedProgress:
fetchAssetsForLanguage:completion:
configParametersForVoicemailWithLanguage:completion:
setAssetsAsProvisional
promoteAssets
initializeLmWithLocale:completion:
initializeLmWithAssetPath:completion:
addSentenceToNgramCounts:completion:
addProns:forWord:completion:
oovWordsAndFrequenciesWithCompletion:
trainFromPlainTextAndWriteToDirectory:sandboxExtension:completion:
generateNgramCountsSerializeDataWithCompletion:
deserializeNgramCountsData:completion:
lmeThresholdWithCompletion:
metricsWithCompletion:
wakeUpWithCompletion:
trainAppLmFromNgramsSerializedData:customPronsData:language:writeToDirectory:sandboxExtension:completion:
createPhraseCountsArtifactWithIdentifier:rawPhraseCountsPath:customPronunciationsPath:writeToDirectory:sandboxExtensions:completion:
createNgramCountsArtifactFromPhraseCountArtifact:writeToDirectory:sandboxExtensions:completion:
trainAppLmFromNgramCountsArtifact:language:writeToDirectory:sandboxExtensions:completion:
_callerHasWritePriviledge:
setCachedLanguage:
setCachedModelRoot:
setCachedJitProfileBuilder:
_recognizer
_audioBuffer
_connection
_bufferedAudioPackets
_bufferedAudioEnded
_lastRecognitionResult
_recognitionResultPackageGenerated
_recognitionResultIsFinal
_consumedSandboxExtensions
_requestParameters
_audioDump
_modelRoot
_modelAssetType
_audioDuration
_recognitionBeginTime
_lastAudioPacketTime
_modelLoadTime
_firstAudioPacketTime
_firstWordTime
_silStartFirstToken
_jitLmeProcessingTime
_asrInitializationTime
sigterm_queue
sigterm_source
_lsrLanguageModel
currentConnection
valueForEntitlement:
boolValue
dictationIsEnabled
tasks
dataWithContentsOfFile:options:error:
deviceIdFilter
bluetoothDeviceIdFilter
%s %s is not available, as LSRError is not designed to be instantiated. Returning nil.
%s Failed to train model successfully
%s Model trained successfully!
%s Writing to: %@
%s Write process failed
%s Failed to serialize ngram counts data
%s Successfully serialized ngram counts and OOVs
%s Failed to create user profile object
%s Failed to create user profile
%s Wrote profile data to file:%@
%s Failed to write LME
%s Successfully wrote LME: %@
%s No oovs in training data
%s Failed to train from ngram counts
%s Failed to write trained AppLm
%s Successfully wrote app-lm to path=%@
%s Exception while training.
%s Failed to train app-specific LM
%s Failed to read ngram counts artifact at: %@
%s Failed to load custom prons from artifact
%s Artifact contained invalid custom prons; skipping. %@
%s Could not get OOVs from artifact
_set_user_dir_suffix failed
: %{darwin.errno}d
%s Failed to query Trial with error=%@
%s Found %lu installed assets
%s Failed to read json file: %@
%s Failed to parse json config file:%@
%s Failed to query MobileAsset with error=%ld
%s Found %lu assets for language=%@
%s Purging asset: %@, language %@
%s Purging failed
%s Failed to create audio dump file. Error: %@, URL: %@
%s Failed to extend audio dump file. Error: %@, URL: %@
%s Failed to prewarm dump file. Error: %@, URL: %@
%s Created audio file at: %@
%s Failed to write to dump file. Error: %@, URL: %@
%s Closed audio dump file at: %@
%s Could not locate asset.
%s Failed to create recognizer from: %@
%s Created Recognizer in %lf sec
%s Handling SIGTERM
%s %@ cancelling instance %@
%s %@ deallocating
%s Consuming sandbox extensions: %@
%s Error consuming sandbox extension: %s
%s Caller has write priviledges for: %@
%s Caller does not have write priviledges for: %@
%s Fetch asset error %@
%s Asset path: %@
%s Could not locate asset
%s Using cached JIT profile builder for language=%@ modelRoot=%@
%s Initialized a new JIT profile builder for language=%@ modelRoot=%@
%s RequestContext: language:%@ task:%@ narrowband:%d appname:%@ enableAutoPunctuation:%d
%s Preparing recognizer...
%s %@ %@
%s Using cached recognizer for language=%@ modelRoot=%@ overrides=%@
%s Initializing a new recognizer for language=%@ modelRoot=%@ overrides=%@
%s Created _EARSpeechRecognizer successfully with modelInfo: %@
%s Failed to prepare recognizer in advance: %@
%s Starting..
%s Error: %@
%s %@
%s Audio dumps will be enabled for request: %@ with taskIdentifier: %@
%s Dump will not be created because dump identifier (%@) doesn't match the qualified task identifier (%@).
%s task name is nil
%s Inline LME input size=%lu
%s Failed to build jitData
%s Set JIT profile for the request
%s Failed to build jitProfile with error %@
%s Loaded Dynamic LM: %@
%s Loaded Dynamic Vocab: %@
%s Censor Speech enabled: %d
%s Changing active configuration from 
%@ to 
%s Adding audio packet: %ld
%s stopSpeech - %p
%s cancelSpeech - %p
%s Failed to consume sandbox extensions: %@
%s Successfully consumed sandbox extensions
%s _scheduleCooldownTimer
%s _cancelCooldownTimer
%s _cooldownTimerFired
%s Request not authorized
%s Failed to download Trial assets with error=%@
%s Failed to find voicemail config parameters
%s Failed to read json config parameters from file: %@
%s Failed to find asset path for language:%@
%s Initialize Language Model with locale=%@
%s Initialize Language Model with assetPath=%@
%s Adding sentence=%@
%s Adding oovs from sentence
%s oovs words and frequencies
%s Write failed error:%@
%s Write successfull
%s Starting training of App LM from ngram count.
%s Finished training of App LM from ngram count.
%s Recognized %lu partial result tokens.
%s package isFinal=%s
%s connection is %@, error %@
%s _recognitionBeginTime (%@) is greater than recognitionEndTime (%@)
%s audioDuration: %lf
%s recognitionDuration: %lf
%s responseTime(Audio finish to recognizer finish): %lf
%s modelLoadTime: %lf
%s TTFW: %lf
%s cpuRtf: %lf
%s jitLmeProcessingTime: %lf
%s asrInitializationTime: %lf
%s _recognitionResultIsFinal=%d _recognitionResultPackageGenerated=%d
%s Called from outside of an XPC connection
%s Could not read %@: %@
%s Could not parse %@: %@
%s %@ is wrong type: %@
%s %@ contains bogus key/value pair: %@ => %@
LSRError
LSRLanguageModel
ServiceDelegate
NSXPCListenerDelegate
NSObject
LSRAdditions
LSRSpeechAssets
LSRUtilities
LSRAudioDump
LSRConnection
_EARSpeechRecognitionResultStream
SFLSRProtocol
v16@0:8
@32@0:8q16@24
@24@0:8q16
@16@0:8
@32@0:8@16@24
B56@0:8@16@24@32@40@48
@24@0:8@16
v24@0:8@16
B32@0:8@16@24
v32@0:8^@16^@24
q16@0:8
v56@0:8@16@24@32^@40^@48
v48@0:8@16@24@32@40
v32@0:8@16@24
@40@0:8@16@24@32
v48@0:8@16@24^@32^@40
@"_EARAppLmData"
@"NSString"
B24@0:8@16
#16@0:8
@24@0:8:16
@32@0:8:16@24
@40@0:8:16@24@32
B16@0:8
B24@0:8#16
B24@0:8:16
Vv16@0:8
Q16@0:8
^{_NSZone=}16@0:8
B24@0:8@"Protocol"16
@"NSString"16@0:8
B32@0:8@"NSXPCListener"16@"NSXPCConnection"24
v24@0:8@?16
@32@0:8@16^@24
@44@0:8@16Q24B32^@36
v32@0:8@16^@24
v32@0:8@16@?24
v24@0:8Q16
@20@0:8B16
@"NSURL"
^{OpaqueAudioFileID=}
^{OpaqueExtAudioFile=}
@68@0:8@16@24B32@36@44^@52^d60
v32@0:8@16d24
v72@0:8@16q24q32d40@48d56q64
v32@0:8@"_EARSpeechRecognizer"16@"_EARSpeechRecognitionResult"24
v32@0:8@"_EARSpeechRecognizer"16@"NSError"24
v32@0:8@"_EARSpeechRecognizer"16@"NSArray"24
v48@0:8@"_EARSpeechRecognizer"16@"NSArray"24@"NSArray"32@"NSArray"40
v32@0:8@"_EARSpeechRecognizer"16@"_EARSpeechRecognitionResultPackage"24
v32@0:8@"_EARSpeechRecognizer"16d24
v32@0:8@"_EARSpeechRecognizer"16@"_EARSpeechRecognition"24
v72@0:8@"_EARSpeechRecognizer"16q24q32d40@"NSArray"48d56q64
Vv24@0:8@16
Vv56@0:8@16@24@32@40@?48
Vv24@0:8@?16
Vv36@0:8@16B24B28B32
Vv32@0:8@16@?24
Vv40@0:8@16@24@?32
Vv64@0:8@16@24@32@40@48@?56
Vv48@0:8@16@24@32@?40
Vv24@0:8@"SFRequestParameters"16
Vv24@0:8@"NSData"16
Vv56@0:8@"NSString"16@"NSString"24@"NSDictionary"32@"NSURL"40@?<v@?@"NSError">48
Vv24@0:8@?<v@?@"NSSet">16
v24@0:8@"NSArray"16
v32@0:8@"NSString"16@"NSDictionary"24
Vv36@0:8@"NSString"16B24B28B32
Vv32@0:8@"NSString"16@?<v@?@"NSString"@"NSError">24
Vv32@0:8@"NSString"16@?<v@?@"NSDictionary">24
Vv32@0:8@"NSString"16@?<v@?@"NSError">24
Vv32@0:8@"NSLocale"16@?<v@?@"NSError">24
Vv24@0:8@"NSString"16
Vv32@0:8@"NSString"16@?<v@?@"NSArray">24
Vv40@0:8@"NSSet"16@"NSString"24@?<v@?B>32
Vv24@0:8@?<v@?@"NSDictionary">16
Vv40@0:8@"NSURL"16@"NSString"24@?<v@?@"NSURL"@"NSURL">32
Vv24@0:8@?<v@?@"NSData">16
Vv32@0:8@"NSData"16@?<v@?@"NSDictionary">24
Vv24@0:8@?<v@?q>16
Vv24@0:8@?<v@?>16
Vv64@0:8@"NSData"16@"NSData"24@"NSString"32@"NSURL"40@"NSString"48@?<v@?@"NSURL"@"NSURL">56
Vv64@0:8@"NSString"16@"NSURL"24@"NSURL"32@"NSURL"40@"NSArray"48@?<v@?@"NSURL">56
Vv48@0:8@"NSURL"16@"NSURL"24@"NSArray"32@?<v@?@"NSURL">40
Vv56@0:8@"NSURL"16@"NSString"24@"NSURL"32@"NSArray"40@?<v@?@"NSURL"@"NSURL">48
B32@0:8@16^@24
{?=[8I]}16@0:8
@48@0:8@16@24^Q32^@40
B60@0:8@16@24@32B40@44^@52
@"_EARSpeechRecognizer"
@"_EARSpeechRecognitionAudioBuffer"
@"NSXPCConnection"
@"NSMutableArray"
@"SFSpeechRecognitionResult"
@"SFRequestParameters"
@"LSRAudioDump"
@"NSObject<OS_dispatch_queue>"
@"NSObject<OS_dispatch_source>"
@"LSRLanguageModel"
softlink:r:path:/System/Library/PrivateFrameworks/CoreEmbeddedSpeechRecognition.framework/CoreEmbeddedSpeechRecognition
softlink:r:path:/System/Library/PrivateFrameworks/CoreEmbeddedSpeechRecognition.framework/CoreEmbeddedSpeechRecognition
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
<key>application-identifier</key>
<string>com.apple.speech.localspeechrecognition</string>
<key>com.apple.application-identifier</key>
<string>com.apple.speech.localspeechrecognition</string>
<key>com.apple.private.assets.accessible-asset-types</key>
<array>
<string>com.apple.MobileAsset.EmbeddedSpeech</string>
<string>com.apple.MobileAsset.EmbeddedSpeechMac</string>
<string>com.apple.MobileAsset.Trial.Siri.SiriUnderstandingAsrAssistant</string>
<string>com.apple.MobileAsset.Trial.Siri.SiriUnderstandingAsrHammer</string>
<string>com.apple.MobileAsset.Trial.Siri.SiriDictationAssets</string>
</array>
<key>com.apple.private.assets.bypass-asset-types-check</key>
<true/>
<key>com.apple.private.sandbox.profile:embedded</key>
<string>temporary-sandbox</string>
<key>com.apple.private.tcc.manager.check-by-audit-token</key>
<array>
<string>kTCCServiceSpeechRecognition</string>
</array>
<key>com.apple.proactive.eventtracker</key>
<true/>
<key>com.apple.security.exception.files.absolute-path.read-only</key>
<array>
<string>/private/var/MobileAsset/</string>
</array>
<key>com.apple.security.exception.files.home-relative-path.read-write</key>
<array>
<string>/Library/Caches/com.apple.speech.localspeechrecognition/</string>
</array>
<key>com.apple.security.exception.iokit-user-client-class</key>
<array>
<string>IOSurfaceRootUserClient</string>
<string>H11ANEInDirectPathClient</string>
</array>
<key>com.apple.security.exception.mach-lookup.global-name</key>
<array>
<string>com.apple.appleneuralengine</string>
<string>com.apple.triald.namespace-management</string>
<string>com.apple.mobileasset.autoasset</string>
<string>com.apple.FileCoordination</string>
</array>
<key>com.apple.security.exception.shared-preference.read-only</key>
<array>
<string>com.apple.assistant.support</string>
<string>com.apple.speech.localspeechrecognition</string>
<string>com.apple.corevideo</string>
</array>
<key>com.apple.security.ts.mobile-keybag-access</key>
<true/>
<key>com.apple.security.ts.opengl-or-metal</key>
<true/>
<key>com.apple.security.ts.tmpdir</key>
<string>com.apple.speech.localspeechrecognition</string>
<key>com.apple.trial.client</key>
<array>
<string>372</string>
<string>401</string>
<string>751</string>
</array>
<key>platform-application</key>
<true/>
</dict>
</plist>
@(#)PROGRAM:localspeechrecognition  PROJECT:SpeechRecognition-1
kLSRErrorDomain
+[LSRError new]
-[LSRError init]
app-lm.NGRAM
\unknown-first
\NT-appvocab
mini.json
%@-%@
-[LSRLanguageModel trainAppLmFromPlainTextAndWriteToAppDirectory:vocabFile:]
-[LSRLanguageModel ngramCountsSerializeData]
en_US_napg.json
dispatch.voc
vocdelta.voc
pg.voc
lexicon.enh
token_s.enh
mrec.psh
+[LSRLanguageModel createAppLmLmeProfileWithLanguage:modelRoot:customPronsData:newOovs:writeToVocabFile:]
v32@?0@"NSString"8Q16^B24
v24@?0@"NSString"8^B16
v32@?0@"NSString"8@"NSSet"16^B24
-[LSRLanguageModel trainAppLmFromNgramsSerializedData:customPronsData:language:writeToAppLmDir:vocabFile:]
-[LSRLanguageModel createSpeechProfileFromOovs:customProns:language:]
-[LSRLanguageModel trainAppLmFromNgramCountsArtifact:language:appLmArtifact:vocabFile:]
v32@?0@"_EAROovToken"8Q16^B24
com.apple.speech.localspeechrecognition
xpcservice
isTrial
modelRoot
assetType
QuasarDir
Language
v8@?0
-[LSRSpeechAssets installedLanguagesWithCompletion:]
+[LSRSpeechAssets assetConfigParameters:]
-[LSRSpeechAssets purgeMAAssetForLanguage:outError:]
-[LSRSpeechAssets purgeAssetsForLanguage:completion:]
CESRTrialAssetManager
Unable to find class %s
CESRAssetConfig
v32@?0@"NSArray"8Q16^B24
-[LSRAudioDump initWithTaskIdentifier:]
-[LSRAudioDump appendBytes:]
-[LSRAudioDump close]
Library/Caches/com.apple.speech.localspeechrecognition/lsr_audio_dumps
%@-%04d%02d%02d-%02d%02d%02d.wav
com.apple.assistant.dictation.prerecorded
voicemail-confidence-parameters
app-lm
+[_EARSpeechRecognizer(LSRConnection) _speechRecognizerWithLanguage:overrides:anyConfiguration:taskConfiguration:modelRoot:error:modelLoadTime:]
keepANEModelLoaded
itn_s.enh
Failed to create recognizer from=%@
-[LSRConnection initWithXPCConnection:]_block_invoke
-[LSRConnection dealloc]
-[LSRConnection _consumeSandboxExtensions:error:]
Error consuming sandbox extension: %s
file-write-create
-[LSRConnection _callerHasWritePriviledge:]
-[LSRConnection modelRootWithLanguage:modelOverrideURL:returningAssetType:error:]
-[LSRConnection _jitProfileWithLanguage:modelRoot:]
-[LSRConnection _requestContext]
none
-[LSRConnection prepareRecognizerWithLanguage:recognitionOverrides:modelOverrideURL:anyConfiguration:task:error:]
modelRoot is nil
Failed to access assets
_EARSpeechRecognizer is nil
Failed to initialize recognizer
-[LSRConnection preloadRecognizerForLanguage:task:recognitionOverrides:modelOverrideURL:completion:]
-[LSRConnection startRecordedAudioDictationWithParameters:]
Recognizer is busy
%@.%@
task name is nil
Failed to find task for recognizer
@"NSDictionary"8@?0
v32@?0@"NSString"8@"NSString"16^B24
-[LSRConnection addAudioPacket:]_block_invoke
-[LSRConnection stopSpeech]
-[LSRConnection cancelSpeech]
-[LSRConnection initializeWithSandboxExtensions:]_block_invoke
+[LSRConnection _scheduleCooldownTimer]
+[LSRConnection _cancelCooldownTimer]
+[LSRConnection _cooldownTimerFired]
-[LSRConnection downloadAssetsForLanguage:urgent:forceUpgrade:detailedProgress:]
v20@?0Q8B16
-[LSRConnection downloadAssetsForLanguage:urgent:forceUpgrade:detailedProgress:]_block_invoke_4
v24@?0@"NSString"8@"NSError"16
v20@?0d8B16
v16@?0Q8
com.apple.speech.localspeechrecognition.detailedProgressWait
-[LSRConnection fetchAssetsForLanguage:completion:]
v16@?0@"NSDictionary"8
-[LSRConnection configParametersForVoicemailWithLanguage:completion:]
-[LSRConnection configParametersForVoicemailWithLanguage:completion:]_block_invoke
-[LSRConnection initializeLmWithLocale:completion:]
Error initializing
-[LSRConnection initializeLmWithAssetPath:completion:]
-[LSRConnection addSentenceToNgramCounts:]
-[LSRConnection addSentenceToNgramCounts:completion:]
-[LSRConnection oovWordsAndFrequenciesWithCompletion:]
v24@?0@"NSURL"8@"NSURL"16
-[LSRConnection trainFromPlainTextAndWriteToDirectory:sandboxExtension:completion:]
SpeechProfile
-[LSRConnection trainAppLmFromNgramsSerializedData:customPronsData:language:writeToDirectory:sandboxExtension:completion:]
v16@?0@"NSURL"8
-[LSRConnection createPhraseCountsArtifactWithIdentifier:rawPhraseCountsPath:customPronunciationsPath:writeToDirectory:sandboxExtensions:completion:]
PhraseCountsFile
-[LSRConnection createNgramCountsArtifactFromPhraseCountArtifact:writeToDirectory:sandboxExtensions:completion:]
NgramCountsFile
-[LSRConnection trainAppLmFromNgramCountsArtifact:language:writeToDirectory:sandboxExtensions:completion:]
AppLmModelFile
-[LSRConnection purgeAssetsForLanguage:completion:]
-[LSRConnection speechRecognizer:didRecognizePartialResult:]_block_invoke
v32@?0@"_EARSpeechRecognitionToken"8Q16^B24
-[LSRConnection speechRecognizer:didRecognizeFinalResultPackage:]_block_invoke
-[LSRConnection speechRecognizer:didFinishRecognitionWithError:]_block_invoke
No speech detected
Recognition request was canceled
Failed to recognize the request
Unsupported EAR build
CheckPrerecordedConnectionAccess
Siri and Dictation are disabled
Task %@ not available for %@, supported tasks are %@
etiquette.json
ReplacementDictionaryForLanguage
ReplacementDictionaryForLanguage_block_invoke
v32@?0@"NSString"8@16^B24
voicemail_confidence_subtraction.json
sampling rate = %@
task type = %@
far field = %@
device ID = %@
Bluetooth device ID = %@
hasSPIAccess
%s %s is not available, as LSRError is not designed to be instantiated. Returning nil.
%s Failed to train model successfully
%s Model trained successfully!
%s Writing to: %@
%s Write process failed
%s Failed to serialize ngram counts data
%s Successfully serialized ngram counts and OOVs
%s Failed to create user profile object
%s Failed to create user profile
%s Wrote profile data to file:%@
%s Failed to write LME
%s Successfully wrote LME: %@
%s No oovs in training data
%s Failed to train from ngram counts
%s Failed to write trained AppLm
%s Successfully wrote app-lm to path=%@
%s Exception while training.
%s Failed to train app-specific LM
%s Failed to read ngram counts artifact at: %@
%s Failed to load custom prons from artifact
%s Artifact contained invalid custom prons; skipping. %@
%s Could not get OOVs from artifact
_set_user_dir_suffix failed
: %{darwin.errno}d
%s Failed to query Trial with error=%@
%s Found %lu installed assets
%s Failed to read json file: %@
%s Failed to parse json config file:%@
%s Failed to query MobileAsset with error=%ld
%s Found %lu assets for language=%@
%s Purging asset: %@, language %@
%s Purging failed
%s Failed to create audio dump file. Error: %@, URL: %@
%s Failed to extend audio dump file. Error: %@, URL: %@
%s Failed to prewarm dump file. Error: %@, URL: %@
%s Created audio file at: %@
%s Failed to write to dump file. Error: %@, URL: %@
%s Closed audio dump file at: %@
%s Could not locate asset.
%s Failed to create recognizer from: %@
%s Created Recognizer in %lf sec
%s Handling SIGTERM
%s %@ cancelling instance %@
%s %@ deallocating
%s Consuming sandbox extensions: %@
%s Error consuming sandbox extension: %s
%s Caller has write priviledges for: %@
%s Caller does not have write priviledges for: %@
%s Fetch asset error %@
%s Asset path: %@
%s Could not locate asset
%s Using cached JIT profile builder for language=%@ modelRoot=%@
%s Initialized a new JIT profile builder for language=%@ modelRoot=%@
%s RequestContext: language:%@ task:%@ narrowband:%d appname:%@ enableAutoPunctuation:%d
%s Preparing recognizer...
%s %@ %@
%s Using cached recognizer for language=%@ modelRoot=%@ overrides=%@
%s Initializing a new recognizer for language=%@ modelRoot=%@ overrides=%@
%s Created _EARSpeechRecognizer successfully with modelInfo: %@
%s Failed to prepare recognizer in advance: %@
%s Starting..
%s Error: %@
%s %@
%s Audio dumps will be enabled for request: %@ with taskIdentifier: %@
%s Dump will not be created because dump identifier (%@) doesn't match the qualified task identifier (%@).
%s task name is nil
%s Inline LME input size=%lu
%s Failed to build jitData
%s Set JIT profile for the request
%s Failed to build jitProfile with error %@
%s Loaded Dynamic LM: %@
%s Loaded Dynamic Vocab: %@
%s Censor Speech enabled: %d
%s Changing active configuration from 
%@ to 
%s Adding audio packet: %ld
%s stopSpeech - %p
%s cancelSpeech - %p
%s Failed to consume sandbox extensions: %@
%s Successfully consumed sandbox extensions
%s _scheduleCooldownTimer
%s _cancelCooldownTimer
%s _cooldownTimerFired
%s Request not authorized
%s Failed to download Trial assets with error=%@
%s Failed to find voicemail config parameters
%s Failed to read json config parameters from file: %@
%s Failed to find asset path for language:%@
%s Initialize Language Model with locale=%@
%s Initialize Language Model with assetPath=%@
%s Adding sentence=%@
%s Adding oovs from sentence
%s oovs words and frequencies
%s Write failed error:%@
%s Write successfull
%s Starting training of App LM from ngram count.
%s Finished training of App LM from ngram count.
%s Recognized %lu partial result tokens.
%s package isFinal=%s
%s connection is %@, error %@
%s _recognitionBeginTime (%@) is greater than recognitionEndTime (%@)
%s audioDuration: %lf
%s recognitionDuration: %lf
%s responseTime(Audio finish to recognizer finish): %lf
%s modelLoadTime: %lf
%s TTFW: %lf
%s cpuRtf: %lf
%s jitLmeProcessingTime: %lf
%s asrInitializationTime: %lf
%s _recognitionResultIsFinal=%d _recognitionResultPackageGenerated=%d
%s Called from outside of an XPC connection
%s Could not read %@: %@
%s Could not parse %@: %@
%s %@ is wrong type: %@
%s %@ contains bogus key/value pair: %@ => %@
LSRError
LSRLanguageModel
ServiceDelegate
NSXPCListenerDelegate
NSObject
LSRAdditions
LSRSpeechAssets
LSRUtilities
LSRAudioDump
LSRConnection
_EARSpeechRecognitionResultStream
SFLSRProtocol
countByEnumeratingWithState:objects:count:
writeToDirectory:
wait
version
valueForEntitlement:
validationError
utteranceStart
unsignedLongValue
unsignedIntegerValue
transitionArtifactAt:toStage:configPath:ncsRoot:dataRoot:estimationRoot:minimize:saveTo:
transcriptionsWithTokens:
transcriptionFromSpeechPhrases:afAudioAnalytics:utteranceStart:
tokens
tokenSausage
tokenName
tasks
taskTypeFilter
taskIdentifier
task
systemUptime
switchToNewAssetsForAssetType:
supportedLanguagesWithAssetType:
stringWithFormat:
stringValue
stringForKey:
stringByReplacingOccurrencesOfString:withString:
stringByDeletingLastPathComponent
stringByAppendingPathComponent:
state
start
standardUserDefaults
speechRecognitionFeatures
silenceStart
sharedPreferences
sharedConnection
setXsampaProns:forWord:
setWithObject:
setValue:forKey:
setUserProfileData:
setTokens:
setText:
setTaskTypeFilter:
setStartTime:
setSource:
setSilenceStartTime:
setSamplingRateFilter:
setRemoveSpaceBefore:
setRemoteObjectInterface:
setRecognizeEagerCandidates:
setRecognitionReplacements:
setRecognitionConfidenceSubtraction:
setPhoneSequence:
setObject:forKey:
setMaximumRecognitionDuration:
setJitProfileData:
setIsLowConfidence:
setIpaPhoneSequence:
setInvalidationHandler:
setInterruptionHandler:
setInterpretations:
setInterpretationIndices:
setInputFormat:
setFarFieldFilter:
setExtraLmList:
setExportedObject:
setExportedInterface:
setEndTime:
setEnableSpeakerCodeTraining:
setDisableAutoPunctuation:
setDeviceIdFilter:
setDetectUtterances:
setDelegate:
setConfidenceScore:
setConcatenateUtterances:
setByAddingObject:
setBluetoothDeviceIdFilter:
setAssetsProvisionalForAssetType:
setActiveConfiguration:
serviceListener
serializedModelWithLanguage:modelData:oovs:
sanitizedStringWithString:
samplingRateFilter
runRecognitionWithResultStream:language:task:samplingRate:
returnTypes:
resume
results
requestIdentifier
removeItemAtURL:error:
removeAllObjects
remoteObjectProxy
recognitionStatistics
recognitionOverrides
recognitionMetadataFromSpeechPhrases:afAudioAnalytics:utteranceStart:
recognition
raise:format:
queryMetaDataSync
purgeSync
pronunciationsForOrthography:
prons
processInfo
preITNTokens
preITNRecognition
phoneSequence
path
orthography
orderedSetWithArray:
orderedOovs
oneBest
onDeviceOnly
objectForKeyedSubscript:
objectForKey:
objectAtIndexedSubscript:
objectAtIndex:
nvAsrPhoneSequenceForXsampaProns:
numberWithUnsignedLong:
numberWithUnsignedInteger:
numberWithLongLong:
numberWithInteger:
numberWithInt:
numberWithDouble:
numberWithBool:
newlineCharacterSet
narrowband
modelOverrideURL
modelInfo
maximumRecognitionDuration
localizedDescription
localSpeechRecognitionDidSucceed
localSpeechRecognitionDidRecognizePartialResult:rawPartialResult:
localSpeechRecognitionDidProcessAudioDuration:
localSpeechRecognitionDidFinishRecognition:
localSpeechRecognitionDidFinishDownloadingAssets:error:
localSpeechRecognitionDidFail:
localSpeechRecognitionDidDownloadAssetsWithProgress:isStalled:
loadOovs
loadLmFromDir:
loadCustomPronData:ncsRoot:dataRoot:
lexemes
length
lastObject
language
jitProfileFromContextualStrings:
isValid
isUnifiedAssetNamespaceEnabled
isFinal
isEqualToString:
isEqualToDictionary:
ipaPhoneSequence
invalidate
interpretations
interpretationIndices
installedAssetWithConfig:regionId:triggerDownload:
installationStatusForLanguagesForAssetType:includeDetailedStatus:error:
inlineItemList
initWithType:
initWithTokenName:start:end:silenceStart:confidence:hasSpaceAfter:hasSpaceBefore:phoneSequence:ipaPhoneSequence:
initWithSpeechRecognitionFeatures:acousticFeatures:
initWithPath:
initWithOrthography:pronunciations:tagName:frequency:
initWithLanguage:assetType:
initWithLanguage:
initWithInterpretationIndices:confidenceScore:
initWithData:
initWithContentsOfFile:
initWithConfiguration:root:
initWithConfiguration:overrides:overrideConfigFiles:generalVoc:lexiconEnh:itnEnh:language:activeConfiguration:modelLoadingOptions:enableSpeakerCodeTraining:supportEmojiRecognition:
initWithConfiguration:ncsRoot:recognizerConfigPath:
initWithConfiguration:ncsRoot:language:
_UUID
UUIDString
URLByAppendingPathComponent:isDirectory:
URLByAppendingPathComponent:
JSONObjectWithData:options:error:
UUID
UTF8String
_initWithBestTranscription:rawTranscription:final:speechRecognitionMetadata:
_setQueue:
acousticFeatureValuePerFrame
acousticFeatures
activeConfiguration
activeConfigurationForEverything
addAudioSamples:count:
addEntriesFromDictionary:
addKeyValuePair:with:
addNgramCountWithType:content:
addObject:
addOovTokensFromSentence:
addSentenceWithType:uuid:content:
addWordWithParts:templateName:
allKeys
applicationName
array
arrayByAddingObject:
arrayWithObjects:count:
assistantIsEnabled
attributes
audioAnalytics
auditToken
bluetoothDeviceIdFilter
buildLmWithConfig:root:data:dir:shouldStop:
bytes
cancelRecognition
code
componentsSeparatedByCharactersInSet:
concatenateUtterances
confidence
confidenceScore
containsObject:
copy
count
writeToURL:atomically:
createDirectoryAtURL:withIntermediateDirectories:attributes:error:
createPhraseCountsArtifact:version:locale:rawPhraseCountsPath:customPronunciationsPath:saveTo:
currentConnection
dataProfile
dataWithContentsOfFile:
dataWithContentsOfFile:options:error:
defaultManager
deserializeModelData:
detectMultipleUtterances
deviceIdFilter
dictationIsEnabled
dictionary
dictionaryWithDictionary:
dictionaryWithObjects:forKeys:count:
domain
doubleValue
downloadAssetOfType:language:urgent:forceUpgrade:progressHandler:completionHandler:
dynamicLanguageModel
dynamicVocabulary
effectiveBoolValueForSetting:
enableAutoPunctuation
endAudio
enumerateKeysAndObjectsUsingBlock:
enumerateObjectsUsingBlock:
errorWithDomain:code:userInfo:
farFieldFilter
fileExistsAtPath:
fileURLWithPath:
firstObject
frameDuration
frequency
generateNgramCountsWithConfig:root:data:
getLocalFileUrl
getProns
hasSpaceAfter
indexOfObject:
initWithAcousticFeatureValue:frameDuration:
initWithCapacity:
initWithConfig:
initWithConfiguration:
initWithConfiguration:language:overrides:sdapiOverrides:generalVoc:emptyVoc:pgVoc:lexiconEnh:tokenEnh:paramsetHolder:
initialize
errorWithCode:description:
errorWithCode:
init
_userProfileWithModelRoot:language:
createAppLmLmeProfileWithLanguage:modelRoot:customPronsData:newOovs:writeToVocabFile:
initWithLocale:
initWithAssetPath:
addSentenceToNgramCounts:
addProns:forWord:
oovsFromSentenceAddedToNgramCounts:
oovWordsAndFrequenciesInNgramCount
trainAppLmFromPlainTextAndWriteToAppDirectory:vocabFile:
ngramCountsSerializeData
deserializeNgramCountsData:
lmeThreshold
metrics
trainAppLmFromNgramsSerializedData:customPronsData:language:writeToAppLmDir:vocabFile:
createPhraseCountArtifactWithIdentifier:rawPhraseCountsPath:customPronunciationsPath:saveTo:
createNgramCountsArtifactFromPhraseCountArtifact:saveTo:
createSpeechProfileFromOovs:customProns:language:
trainAppLmFromNgramCountsArtifact:language:appLmArtifact:vocabFile:
.cxx_destruct
_appLmData
_recognizerConfigFilePath
_ncsRootPath
_languageCode
isEqual:
class
self
performSelector:
performSelector:withObject:
performSelector:withObject:withObject:
isProxy
isKindOfClass:
isMemberOfClass:
conformsToProtocol:
respondsToSelector:
retain
release
autorelease
retainCount
zone
hash
superclass
description
debugDescription
TQ,R
T#,R
T@"NSString",R,C
listener:shouldAcceptNewConnection:
_lsr_language
_lsr_path
_lsr_quasarModelPath
_lsr_isInstalled
purgeInstalledAssetForAssetType:language:regionId:error:
sharedInstance
_assetQueryForLanguage:
assetConfigParameters:
installedLanguagesWithCompletion:
fetchAssetInfoForSpeechRequestForInstalledLanguage:outError:
fetchAssetPathForInstalledLanguage:outError:
_fetchAssetPathForInstalledLanguage:assetType:triggerDownload:outError:
purgeMAAssetForLanguage:outError:
purgeAssetsForLanguage:completion:
releaseClients
setAssetsAsProvisionalForAssetType:
promoteAssetsForAssetType:
CESRDictationAssetType
_CESRDictationAssetType
TQ,R,N,V_CESRDictationAssetType
transcriptionsWithEARTokens:
recognizedResultFromEARPackage:
emptyResultWithIsFinal:
dealloc
urlForTaskIdentifier:
dumpDirectory
filenameForTaskIdentifier:
initWithTaskIdentifier:
appendBytes:
close
discard
_url
_fileID
_extendedFile
_speechRecognizerWithLanguage:overrides:anyConfiguration:taskConfiguration:modelRoot:error:modelLoadTime:
downloadStatusWithConfig:progressHandler:completionHandler:
_scheduleCooldownTimer
_cancelCooldownTimer
_cooldownTimerFired
speechRecognizer:didRecognizePartialResult:
speechRecognizer:didFinishRecognitionWithError:
speechRecognizer:didRecognizeFinalResults:
speechRecognizer:didRecognizeFinalResults:tokenSausage:nBestChoices:
speechRecognizer:didRecognizeFinalResultPackage:
speechRecognizer:didProcessAudioDuration:
speechRecognizer:didRecognizeRawEagerRecognitionCandidate:
speechRecognizer:didProduceEndpointFeaturesWithWordCount:trailingSilenceDuration:eosLikelihood:pauseCounts:silencePosterior:processedAudioDurationInMilliseconds:
speechRecognizer:didRecognizePartialResultNbest:
speechRecognizer:didProduceLoggablePackage:
speechRecognizer:didRecognizeFinalResultCandidatePackage:
startRecordedAudioDictationWithParameters:
stopSpeech
cancelSpeech
addAudioPacket:
preloadRecognizerForLanguage:task:recognitionOverrides:modelOverrideURL:completion:
initializeWithSandboxExtensions:
logCoreAnalyticsEvent:withAnalytics:
downloadAssetsForLanguage:urgent:forceUpgrade:detailedProgress:
fetchAssetsForLanguage:completion:
configParametersForVoicemailWithLanguage:completion:
setAssetsAsProvisional
promoteAssets
initializeLmWithLocale:completion:
initializeLmWithAssetPath:completion:
addSentenceToNgramCounts:completion:
addProns:forWord:completion:
oovWordsAndFrequenciesWithCompletion:
trainFromPlainTextAndWriteToDirectory:sandboxExtension:completion:
generateNgramCountsSerializeDataWithCompletion:
deserializeNgramCountsData:completion:
lmeThresholdWithCompletion:
metricsWithCompletion:
wakeUpWithCompletion:
trainAppLmFromNgramsSerializedData:customPronsData:language:writeToDirectory:sandboxExtension:completion:
createPhraseCountsArtifactWithIdentifier:rawPhraseCountsPath:customPronunciationsPath:writeToDirectory:sandboxExtensions:completion:
createNgramCountsArtifactFromPhraseCountArtifact:writeToDirectory:sandboxExtensions:completion:
trainAppLmFromNgramCountsArtifact:language:writeToDirectory:sandboxExtensions:completion:
initWithXPCConnection:
_delegate
_consumeSandboxExtensions:error:
_auditToken
_callerHasWritePriviledge:
modelRootWithLanguage:modelOverrideURL:returningAssetType:error:
_jitProfileWithLanguage:modelRoot:
_requestContext
prepareRecognizerWithLanguage:recognitionOverrides:modelOverrideURL:anyConfiguration:task:error:
updateAudioDuration:
reportFailureWithError:
setCachedLanguage:
setCachedModelRoot:
setCachedJitProfileBuilder:
_recognizer
_audioBuffer
_connection
_bufferedAudioPackets
_bufferedAudioEnded
_lastRecognitionResult
_recognitionResultPackageGenerated
_recognitionResultIsFinal
_consumedSandboxExtensions
_requestParameters
_audioDump
_modelRoot
_modelAssetType
_audioDuration
_recognitionBeginTime
_lastAudioPacketTime
_modelLoadTime
_firstAudioPacketTime
_firstWordTime
_silStartFirstToken
_jitLmeProcessingTime
_asrInitializationTime
sigterm_queue
sigterm_source
_lsrLanguageModel
boolValue
v16@0:8
@32@0:8q16@24
@24@0:8q16
@16@0:8
@32@0:8@16@24
B56@0:8@16@24@32@40@48
@24@0:8@16
v24@0:8@16
B32@0:8@16@24
v32@0:8^@16^@24
q16@0:8
v56@0:8@16@24@32^@40^@48
v48@0:8@16@24@32@40
v32@0:8@16@24
@40@0:8@16@24@32
v48@0:8@16@24^@32^@40
@"_EARAppLmData"
@"NSString"
B24@0:8@16
#16@0:8
@24@0:8:16
@32@0:8:16@24
@40@0:8:16@24@32
B16@0:8
B24@0:8#16
B24@0:8:16
Vv16@0:8
Q16@0:8
^{_NSZone=}16@0:8
B24@0:8@"Protocol"16
@"NSString"16@0:8
B32@0:8@"NSXPCListener"16@"NSXPCConnection"24
v24@0:8@?16
@32@0:8@16^@24
@44@0:8@16Q24B32^@36
v32@0:8@16^@24
v32@0:8@16@?24
v24@0:8Q16
@20@0:8B16
@"NSURL"
^{OpaqueAudioFileID=}
^{OpaqueExtAudioFile=}
@68@0:8@16@24B32@36@44^@52^d60
v32@0:8@16d24
v72@0:8@16q24q32d40@48d56q64
v32@0:8@"_EARSpeechRecognizer"16@"_EARSpeechRecognitionResult"24
v32@0:8@"_EARSpeechRecognizer"16@"NSError"24
v32@0:8@"_EARSpeechRecognizer"16@"NSArray"24
v48@0:8@"_EARSpeechRecognizer"16@"NSArray"24@"NSArray"32@"NSArray"40
v32@0:8@"_EARSpeechRecognizer"16@"_EARSpeechRecognitionResultPackage"24
v32@0:8@"_EARSpeechRecognizer"16d24
v32@0:8@"_EARSpeechRecognizer"16@"_EARSpeechRecognition"24
v72@0:8@"_EARSpeechRecognizer"16q24q32d40@"NSArray"48d56q64
Vv24@0:8@16
Vv56@0:8@16@24@32@40@?48
Vv24@0:8@?16
Vv36@0:8@16B24B28B32
Vv32@0:8@16@?24
Vv40@0:8@16@24@?32
Vv64@0:8@16@24@32@40@48@?56
Vv48@0:8@16@24@32@?40
Vv24@0:8@"SFRequestParameters"16
Vv24@0:8@"NSData"16
Vv56@0:8@"NSString"16@"NSString"24@"NSDictionary"32@"NSURL"40@?<v@?@"NSError">48
Vv24@0:8@?<v@?@"NSSet">16
v24@0:8@"NSArray"16
v32@0:8@"NSString"16@"NSDictionary"24
Vv36@0:8@"NSString"16B24B28B32
Vv32@0:8@"NSString"16@?<v@?@"NSString"@"NSError">24
Vv32@0:8@"NSString"16@?<v@?@"NSDictionary">24
Vv32@0:8@"NSString"16@?<v@?@"NSError">24
Vv32@0:8@"NSLocale"16@?<v@?@"NSError">24
Vv24@0:8@"NSString"16
Vv32@0:8@"NSString"16@?<v@?@"NSArray">24
Vv40@0:8@"NSSet"16@"NSString"24@?<v@?B>32
Vv24@0:8@?<v@?@"NSDictionary">16
Vv40@0:8@"NSURL"16@"NSString"24@?<v@?@"NSURL"@"NSURL">32
Vv24@0:8@?<v@?@"NSData">16
Vv32@0:8@"NSData"16@?<v@?@"NSDictionary">24
Vv24@0:8@?<v@?q>16
Vv24@0:8@?<v@?>16
Vv64@0:8@"NSData"16@"NSData"24@"NSString"32@"NSURL"40@"NSString"48@?<v@?@"NSURL"@"NSURL">56
Vv64@0:8@"NSString"16@"NSURL"24@"NSURL"32@"NSURL"40@"NSArray"48@?<v@?@"NSURL">56
Vv48@0:8@"NSURL"16@"NSURL"24@"NSArray"32@?<v@?@"NSURL">40
Vv56@0:8@"NSURL"16@"NSString"24@"NSURL"32@"NSArray"40@?<v@?@"NSURL"@"NSURL">48
B32@0:8@16^@24
{?=[8I]}16@0:8
@48@0:8@16@24^Q32^@40
B60@0:8@16@24@32B40@44^@52
@"_EARSpeechRecognizer"
@"_EARSpeechRecognitionAudioBuffer"
@"NSXPCConnection"
@"NSMutableArray"
@"SFSpeechRecognitionResult"
@"SFRequestParameters"
@"LSRAudioDump"
@"NSObject<OS_dispatch_queue>"
@"NSObject<OS_dispatch_source>"
@"LSRLanguageModel"
softlink:r:path:/System/Library/PrivateFrameworks/CoreEmbeddedSpeechRecognition.framework/CoreEmbeddedSpeechRecognition
softlink:r:path:/System/Library/PrivateFrameworks/CoreEmbeddedSpeechRecognition.framework/CoreEmbeddedSpeechRecognition
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
<key>application-identifier</key>
<string>com.apple.speech.localspeechrecognition</string>
<key>com.apple.application-identifier</key>
<string>com.apple.speech.localspeechrecognition</string>
<key>com.apple.private.assets.accessible-asset-types</key>
<array>
<string>com.apple.MobileAsset.EmbeddedSpeech</string>
<string>com.apple.MobileAsset.EmbeddedSpeechMac</string>
<string>com.apple.MobileAsset.Trial.Siri.SiriUnderstandingAsrAssistant</string>
<string>com.apple.MobileAsset.Trial.Siri.SiriUnderstandingAsrHammer</string>
<string>com.apple.MobileAsset.Trial.Siri.SiriDictationAssets</string>
</array>
<key>com.apple.private.assets.bypass-asset-types-check</key>
<true/>
<key>com.apple.private.sandbox.profile:embedded</key>
<string>temporary-sandbox</string>
<key>com.apple.private.tcc.manager.check-by-audit-token</key>
<array>
<string>kTCCServiceSpeechRecognition</string>
</array>
<key>com.apple.proactive.eventtracker</key>
<true/>
<key>com.apple.security.exception.files.absolute-path.read-only</key>
<array>
<string>/private/var/MobileAsset/</string>
</array>
<key>com.apple.security.exception.files.home-relative-path.read-write</key>
<array>
<string>/Library/Caches/com.apple.speech.localspeechrecognition/</string>
</array>
<key>com.apple.security.exception.iokit-user-client-class</key>
<array>
<string>IOSurfaceRootUserClient</string>
<string>H11ANEInDirectPathClient</string>
</array>
<key>com.apple.security.exception.mach-lookup.global-name</key>
<array>
<string>com.apple.appleneuralengine</string>
<string>com.apple.triald.namespace-management</string>
<string>com.apple.mobileasset.autoasset</string>
<string>com.apple.FileCoordination</string>
</array>
<key>com.apple.security.exception.shared-preference.read-only</key>
<array>
<string>com.apple.assistant.support</string>
<string>com.apple.speech.localspeechrecognition</string>
<string>com.apple.corevideo</string>
</array>
<key>com.apple.security.ts.mobile-keybag-access</key>
<true/>
<key>com.apple.security.ts.opengl-or-metal</key>
<true/>
<key>com.apple.security.ts.tmpdir</key>
<string>com.apple.speech.localspeechrecognition</string>
<key>com.apple.trial.client</key>
<array>
<string>372</string>
<string>401</string>
<string>751</string>
</array>
<key>platform-application</key>
<true/>
</dict>
</plist>
