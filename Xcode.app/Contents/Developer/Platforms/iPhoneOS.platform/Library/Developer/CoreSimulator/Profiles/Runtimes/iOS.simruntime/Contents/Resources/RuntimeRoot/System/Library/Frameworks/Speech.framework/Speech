@(#)PROGRAM:Speech  PROJECT:SpeechRecognition-1
MbP?
@mcpl
_acousticFeaturePerFrame
_frameDuration
, featureValues=%@, frameDuration=%f
_jitter
_shimmer
_pitch
_voicing
, jitter=%@, shimmer=%@, pitch=%@, voicing=%@
speakingRate
averagePauseDuration
jitter
shimmer
pitch
voicing
v32@?0@"NSString"8@"NSString"16^B24
com.apple.Speech.Task.Internal
v8@?0
User denied access to speech recognition
v24@?0@"SFSpeechRecognitionResult"8@"NSError"16
v32@?0@"AFSpeechToken"8Q16^B24
v32@?0@"AFSpeechInterpretation"8Q16^B24
v24@?0@"NSSet"8@"NSSet"16
Cannot make recognizer for %@. Supported locale identifiers are %@
%@-%@
mini.json
Cannot create model
Model trained successfully!
Writting to: %@
Write process failed
Model training failed
SpeechProfile
Path where to write the file was not provided. Aborting!!
There was a problem writing data to the file! Error:  %@
Ngram counts and OOV list serialized and saved to: %@
grapheme
phoneme
Invalid prons: %@ for word: %@
_bestTranscription
_rawTranscription
_final
_speechRecognitionMetadata
 final=%d, bestTranscription=%@, speechRecognitionMetadata=%@
v32@?0@"NSString"8Q16^B24
v32@?0@"SFTranscriptionSegment"8Q16^B24
v12@?0C8
v16@?0@"NSNotification"8
v16@?0@"NSArray"8
Result handler must be non-null
%@ queue must not be nil
, substringRange=%@, timestamp=%@, duration=%@, confidence=%@, substring=%@, alternativeSubstrings=%@, phoneSequence=%@, ipaPhoneSequence=%@, voiceAnalytics=%@
_substring
_substringRange.location
_substringRange.length
_timestamp
_duration
_confidence
_alternativeSubstrings
_alternativeConfidences
_phoneSequence
_ipaPhoneSequence
_voiceAnalytics
SFTranscriptionSegment.m
hi-IN
hi-IN-translit
Translit
SFSpeechPreecordedRequest
Use -[SFSpeechURLRecognitionRequest initWithURL:]
Could not add output for %@
B8@?0
com.apple.SFSpeechAudioBufferRecognitionRequest
SFSpeechRecognitionRequest.m
%@ cannot be re-used
Invalid audio format
@"AVAudioBuffer"20@?0I8^q12
Could not drain converter %@
Could not run audio converter %@
CMBlockBufferCopyDataBytes could not copy data: %d
_speakingRate
_avgPauseDuration
, formattedString=%@, segments=%@, speakingRate=%f, averagePauseDuration=%f
_segments
_formattedString
_averagePauseDuration
_speechStartTimestamp
_speechDuration
, speakingRate=%@, averagePauseDuration=%@, speechStartTimestamp=%@, speechDuration=%@, voiceAnalytics=%@
com.apple.MobileAsset.EmbeddedSpeechMac
QuasarDir
Language
Failed to query MobileAsset with error=%ld
Found %lu assets for language=%@
Assets are installed for language=%@
No asset installed for language=%@
No asset found for language=%@
SFAcousticFeature
NSCopying
NSSecureCoding
NSCoding
SFVoiceAnalytics
_SFSearchResult
SFSpeechRecognitionTask
AFDictationDelegate
NSObject
SFSpeechRecognitionBufferDelegate
_SFSpeechRecognitionBlockTask
_SFSpeechRecognitionDelegateTask
SFSpeechLanguageModel
SFSpeechRecognitionResult
SFSpeechRecognizer
CXCallObserverDelegate
SFTranscriptionSegment
SFUtilities
_SFSearchRequest
SFSpeechRecognitionRequest
SFSpeechURLRecognitionRequest
SFSpeechAudioBufferRecognitionRequest
SFTranscription
SFSpeechRecognitionMetadata
SRSampling
SFAdditions
SFSpeechAssetManager
encodeObject:forKey:
encodeDouble:forKey:
init
arrayWithObjects:count:
setWithArray:
decodeObjectOfClasses:forKey:
decodeDoubleForKey:
copy
description
stringByAppendingFormat:
supportsSecureCoding
copyWithZone:
encodeWithCoder:
initWithCoder:
TB,R
_initWithAcousticFeatureValue:frameDuration:
acousticFeatureValuePerFrame
frameDuration
.cxx_destruct
_acousticFeatureValuePerFrame
_frameDuration
T@"NSArray",R,C,N,V_acousticFeatureValuePerFrame
Td,R,N,V_frameDuration
decodeObjectOfClass:forKey:
_initWithJitter:shimmer:pitch:voicing:
jitter
shimmer
pitch
voicing
_jitter
_shimmer
_pitch
_voicing
T@"SFAcousticFeature",R,C,N,V_jitter
T@"SFAcousticFeature",R,C,N,V_shimmer
T@"SFAcousticFeature",R,C,N,V_pitch
T@"SFAcousticFeature",R,C,N,V_voicing
statusCode
intValue
headers
enumerateKeysAndObjectsUsingBlock:
_responseWithCFURLResponse:
result
searchType
isEqualToString:
JSONObjectWithData:options:error:
dataWithJSONObject:options:error:
initWithData:encoding:
stringByAppendingString:
initWithVoiceSearchResult:
response
data
_response
_data
_searchType
T@"NSHTTPURLResponse",R,N,V_response
T@"NSData",R,N,V_data
Tq,R,N,V_searchType
UUID
UUIDString
taskHint
_startedConnectionWithLanguageCode:delegate:taskHint:requestIdentifier:
stopSpeechWithOptions:
cancelSpeech
peakPower
averagePower
code
dictionaryWithObjects:forKeys:count:
errorWithDomain:code:userInfo:
addRecordedSpeechSampleData:
array
string
countByEnumeratingWithState:objects:count:
removeSpaceBefore
removeSpaceAfter
appendString:
text
length
startTime
silenceStartTime
confidenceScore
_initWithSubstring:range:timestamp:duration:confidence:alternativeSubstrings:alternativeConfidences:phoneSequence:ipaPhoneSequence:voiceAnalytics:
addObject:
_initWithSegments:formattedString:speakingRate:averagePauseDuration:
recognition
phrases
audioAnalytics
utteranceStart
rawRecognition
isFinal
_initWithBestTranscription:rawTranscription:final:speechRecognitionMetadata:
transcriptionsWithTokens:
recognizedResultFromPackage:
isEqual:
class
self
performSelector:
performSelector:withObject:
performSelector:withObject:withObject:
isProxy
isKindOfClass:
isMemberOfClass:
conformsToProtocol:
respondsToSelector:
retain
release
autorelease
retainCount
zone
hash
superclass
debugDescription
TQ,R
T#,R
T@"NSString",R,C
dictationConnectionSpeechRecordingWillBegin:
dictationConnectionSpeechRecordingDidBegin:
dictationConnection:speechRecordingDidBeginWithOptions:
dictationConnection:didBeginLocalRecognitionWithModelInfo:
dictationConnectionSpeechRecordingDidEnd:
dictationConnectionSpeechRecordingDidCancel:
dictationConnection:speechRecordingDidFail:
dictationConnection:speechRecognitionDidFail:
dictationConnection:didDetectLanguage:confidenceScores:
dictationConnection:didDetectLanguage:confidenceScores:isConfident:
dictationConnection:didRecognizeMultilingualSpeech:
dictationConnection:languageDetectorFailedWithError:
dictationConnection:didRecognizePhrases:languageModel:correctionIdentifier:
dictationConnection:didRecognizePhrases:languageModel:correctionIdentifier:replacingPreviousPhrasesCount:
dictationConnection:didRecognizeTokens:languageModel:
dictationConnection:didRecognizePartialResult:
dictationConnection:didProcessAudioDuration:
dictationConnectionSpeechRecognitionDidSucceed:
dictationConnection:didRecognizeTranscriptionObjects:languageModel:
dictationConnnectionDidChangeAvailability:
dictationConnection:didFinishWritingAudioFile:error:
dictationConnection:didReceiveSearchResults:recognizedText:stable:final:
dictationConnection:didRecognizePackage:
stopSpeech
_initWithRequest:queue:languageCode:taskHint:
state
finish
cancel
_taskHint
isFinishing
isCancelled
error
requestIdentifier
_dictationConnection
_externalQueue
_languageCode
_request
_internalQueue
_completed
_running
_finishing
_cancelled
_error
_requestIdentifier
Tq,R,N,V_taskHint
T@"NSString",R,C,N,V_requestIdentifier
Tq,R,N
finishing
TB,R,N,GisFinishing,V_finishing
cancelled
TB,R,N,GisCancelled,V_cancelled
T@"NSError",R,C,N,V_error
addOperationWithBlock:
_fireResultHandlerWithResult:error:
_finalizeResultHandler
shouldReportPartialResults
_initWithBestTranscription:rawTranscription:final:
_initWithRequest:queue:languageCode:taskHint:resultHandler:
_resultHandler
_hasFiredFinalResult
_searchRequest
speechRecognitionTaskWasCancelled:
speechRecognitionTask:didFinishSuccessfully:
_tellDelegateDidFinishSuccessfully:
speechRecognitionTaskFinishedReadingAudio:
speechRecognitionTask:didReceiveSearchResults:recognizedText:stable:final:
bestTranscription
formattedString
speechRecognitionTask:didFinishRecognition:
speechRecognitionTask:didHypothesizeTranscription:
speechRecognitionTask:didProcessAudioDuration:
numberWithInteger:
setObject:forKey:
_initWithRequest:queue:languageCode:taskHint:delegate:
_delegate
_recognitionResultToReportAfterFinalSearchResults
_selfReference
_waitForVoiceSearchResult
_hasSentRealSearchResults
interpretations
firstObject
tokens
numberWithDouble:
phoneSequence
ipaPhoneSequence
enumerateObjectsUsingBlock:
stringWithString:
speechRecognitionFeatures
objectForKey:
doubleValue
acousticFeatures
count
subarrayWithRange:
_initWithSpeechStartTimestamp:speechDuration:voiceAnalytics:speakingRate:averagePauseDuration:
supportedLocalesWithCompletion:
localeIdentifier
stringByReplacingOccurrencesOfString:withString:
containsObject:
localeWithLocaleIdentifier:
stringWithFormat:
fetchAssetPathForInstalledLanguage:
initWithAssetPath:
stringByAppendingPathComponent:
initWithConfiguration:ncsRoot:recognizerConfigPath:
addSentenceWithType:uuid:content:
addSentence:
addOovTokensFromSentence:
outOfVocabularyWordsAndFrequencies
allKeys
orderedOovs
frequency
orthography
initWithConfiguration:
trainWithData:
writeToDirectory:
generateLmeData:
initWithConfig:
setInputFormat:
generateNgramCounts:
language
outOfVocabularyWords
serializedModelWithLanguage:modelData:oovs:
writeToURL:options:error:
deserializeModelData:
initWithFilePath:
lexemes
objectForKeyedSubscript:
componentsSeparatedByString:
addProns:forWord:
allObjects
raise:format:
setXsampaProns:forWord:
lmeThreshold
metrics
initialize
supportedLocales
initWithLocale:
addSentences:
addOovsFromSentence:
trainFromPlainTextAndWriteToDirectory:
generateNgramsSerializeDataAndWriteToFile:
addPronsFromFile:
locale
_appLmData
_recognizerConfigFilePath
_model
_oovDict
_locale
T@"NSLocale",R,C,N,V_locale
T@"NSArray",R,C,N
T@"NSDictionary",R,C,N
decodeBoolForKey:
encodeBool:forKey:
segments
alternativeSubstrings
alternativeConfidences
mutableCopy
substring
insertObject:atIndex:
confidence
numberWithFloat:
addObjectsFromArray:
substringRange
removeObjectAtIndex:
objectAtIndex:
timestamp
duration
floatValue
stringByReplacingCharactersInRange:withString:
expandTranscription:
_initWithBestTranscription:final:
transcriptions
rawTranscriptions
speechRecognitionMetadata
rawTranscription
_transcriptions
_rawTranscriptions
_final
_bestTranscription
_speechRecognitionMetadata
_rawTranscription
T@"SFTranscription",R,C,N,V_rawTranscription
T@"SFTranscription",R,C,N,V_bestTranscription
final
TB,R,N,GisFinal,V_final
T@"SFSpeechRecognitionMetadata",R,N,V_speechRecognitionMetadata
authorizationStatus
currentLocale
mainQueue
setDelegate:
beginAvailabilityMonitoring
defaultCenter
_informDelegateOfPreferencesChange
addObserverForName:object:queue:usingBlock:
setDelegate:queue:
endSession
cancelAvailabilityMonitoring
removeObserver:
dealloc
dictationIsEnabled
dictationIsAvailableForLanguage:synchronous:
_isAvailableForForcedOfflineRecognition
forcedOfflineDictationIsAvailableForLanguage:
requestOfflineDictationSupportForLanguage:completion:
getForcedOfflineDictationSupportedLanguagesWithCompletion:
_informDelegateOfAvailabilityChange
speechRecognizer:availabilityDidChange:
isAvailable
requestAuthorization:
_fetchSupportedForcedOfflineLocalesWithCompletion:
callObserver:callChanged:
supportsOnDeviceRecognition
_requestOfflineDictationSupportWithCompletion:
_isInternalTaskHint:
recognitionTaskWithRequest:resultHandler:
recognitionTaskWithRequest:delegate:
setQueue:
setSupportsOnDeviceRecognition:
delegate
defaultTaskHint
setDefaultTaskHint:
queue
_callObserver
_facetimeObserver
_foregroundObserver
_preferencesObserver
_supportsOnDeviceRecognition
_defaultTaskHint
_queue
_availableForForcedOfflineRecognition
TB,R,N,G_isAvailableForForcedOfflineRecognition
available
TB,R,N,GisAvailable
TB,N,V_supportsOnDeviceRecognition
T@"<SFSpeechRecognizerDelegate>",W,N,V_delegate
Tq,N,V_defaultTaskHint
T@"NSOperationQueue",&,N,V_queue
encodeInteger:forKey:
decodeIntegerForKey:
decodeObjectForKey:
currentHandler
handleFailureInMethod:object:file:lineNumber:description:
voiceAnalytics
_confidence
_substring
_timestamp
_duration
_alternativeSubstrings
_voiceAnalytics
_alternativeConfidences
_phoneSequence
_ipaPhoneSequence
_substringRange
T@"NSArray",R,N,V_alternativeConfidences
T@"NSString",R,N,V_phoneSequence
T@"NSString",R,N,V_ipaPhoneSequence
T@"NSString",R,C,N,V_substring
T{_NSRange=QQ},R,N,V_substringRange
Td,R,N,V_timestamp
Td,R,N,V_duration
Tf,R,N,V_confidence
T@"NSArray",R,N,V_alternativeSubstrings
T@"SFVoiceAnalytics",R,N,V_voiceAnalytics
setWithCapacity:
searchTypes
setSearchTypes:
headerFields
setHeaderFields:
queryParameters
setQueryParameters:
_searchTypes
_headerFields
_queryParameters
Tq,N,V_searchTypes
T@"NSDictionary",C,N,V_headerFields
T@"NSDictionary",C,N,V_queryParameters
setDetectMultipleUtterances:
_setForceOfflineRecognition:
_forceOfflineRecognition
mainBundle
bundleIdentifier
setApplicationName:
infoDictionary
setApplicationVersion:
setInlineItemList:
setRequestIdentifier:
setVoiceTriggerEventInfo:
setMaximumRecognitionDuration:
setDetectUtterances:
setVoiceSearchTypeOptions:
setVoiceSearchQueryParameters:
setVoiceSearchHeaderFields:
setKeyboardType:
setTaskHint:
setInteractionIdentifier:
setForceOfflineRecognition:
setRecognitionOverrides:
setModelOverrideURL:
initWithActivationEvent:
automaticallyEndpoint
setUseAutomaticEndpointing:
setUseStreamingDictation:
processInfo
systemUptime
setActivationEventTime:
_setSearchRequests:
_searchRequests
_powerMeteringAvailable
setRequiresOnDeviceRecognition:
requiresOnDeviceRecognition
_dictationOptionsWithTaskHint:requestIdentifier:
_speechRequestOptions
_maximumRecognitionDuration
_setMaximumRecognitionDuration:
_setSearchRequest:
_voiceTriggerEventInfo
_setVoiceTriggerEventInfo:
_recognitionOverrides
_setRecognitionOverrides:
_modelOverrideURL
_setModelOverrideURL:
setShouldReportPartialResults:
contextualStrings
setContextualStrings:
interactionIdentifier
detectMultipleUtterances
_shouldReportPartialResults
_detectMultipleUtterances
_contextualStrings
_interactionIdentifier
T@"_SFSearchRequest",&,N,G_searchRequest,S_setSearchRequest:,V_searchRequest
TB,N,V_detectMultipleUtterances
TB,N,G_forceOfflineRecognition,S_setForceOfflineRecognition:,V_forceOfflineRecognition
T@"NSDictionary",&,N,G_voiceTriggerEventInfo,S_setVoiceTriggerEventInfo:,V_voiceTriggerEventInfo
Td,N,G_maximumRecognitionDuration,S_setMaximumRecognitionDuration:,V_maximumRecognitionDuration
T@"NSDictionary",&,N,G_recognitionOverrides,S_setRecognitionOverrides:,V_recognitionOverrides
T@"NSURL",&,N,G_modelOverrideURL,S_setModelOverrideURL:,V_modelOverrideURL
Tq,N,V_taskHint
TB,N,V_shouldReportPartialResults
T@"NSArray",C,N,V_contextualStrings
T@"NSString",C,N,V_interactionIdentifier
TB,N
setFieldLabel:
assetWithURL:
caseInsensitiveCompare:
setKeyboardIdentifier:
setOriginalAudioFileURL:
tracksWithMediaType:
formatDescriptions
startRecordedAudioDictationWithOptions:forLanguage:narrowband:
assetReaderWithAsset:error:
numberWithUnsignedInteger:
assetReaderAudioMixOutputWithAudioTracks:audioSettings:
canAddOutput:
addOutput:
startReading
copyNextSampleBuffer
initWithURL:
_URL
T@"NSURL",R,C,N,V_URL
initWithStreamDescription:
startRecordedAudioDictationWithOptions:forLanguage:
_appendAudioPCMBuffer:
_appendAudioSampleBuffer:
_endAudio
nativeAudioFormat
format
_drainAndClearAudioConverter
int16ChannelData
frameLength
dataWithBytes:length:
_convertAndFeedPCMBuffer:
initWithPCMFormat:frameCapacity:
setFrameLength:
convertToBuffer:error:withInputFromBlock:
inputFormat
initFromFormat:toFormat:
setSampleRateConverterQuality:
mutableAudioBufferList
appendAudioPCMBuffer:
appendAudioSampleBuffer:
endAudio
_bufferDelegate
_queuedBuffers
_converter
_audioEnded
T@"AVAudioFormat",R,N
initWithLength:
mutableBytes
speakingRate
averagePauseDuration
_formattedString
_segments
_speakingRate
_averagePauseDuration
T@"NSString",R,C,N,V_formattedString
T@"NSArray",R,C,N,V_segments
Td,R,N,V_speakingRate
Td,R,N,V_averagePauseDuration
unarchivedObjectOfClass:fromData:error:
archivedDataWithRootObject:requiringSecureCoding:error:
initWithBinarySampleRepresentation:metadata:timestamp:
binarySampleRepresentation
speechStartTimestamp
speechDuration
_speechStartTimestamp
_speechDuration
Td,R,N,V_speechStartTimestamp
Td,R,N,V_speechDuration
getLocalFileUrl
path
_sf_path
attributes
_sf_quasarModelPath
_sf_isInstalled
initWithType:
returnTypes:
addKeyValuePair:with:
_assetQueryForLanguage:
queryMetaDataSync
results
fetchAssetsWithLanguage:completion:
B16@0:8
@24@0:8^{_NSZone=}16
v24@0:8@16
@24@0:8@16
v24@0:8@"NSCoder"16
@24@0:8@"NSCoder"16
@32@0:8@16d24
@16@0:8
d16@0:8
v16@0:8
@"NSArray"
@48@0:8@16@24@32@40
@"SFAcousticFeature"
q16@0:8
@"NSHTTPURLResponse"
@"NSData"
B24@0:8@16
#16@0:8
@24@0:8:16
@32@0:8:16@24
@40@0:8:16@24@32
B24@0:8#16
B24@0:8:16
Vv16@0:8
Q16@0:8
^{_NSZone=}16@0:8
B24@0:8@"Protocol"16
@"NSString"16@0:8
v32@0:8@16@24
v40@0:8@16@24@32
v44@0:8@16@24@32B40
v48@0:8@16@24@32@40
v56@0:8@16@24@32@40Q48
v32@0:8@16d24
v48@0:8@16@24@32B40B44
v24@0:8@"AFDictationConnection"16
v32@0:8@"AFDictationConnection"16@"AFDictationOptions"24
v32@0:8@"AFDictationConnection"16@"NSString"24
v32@0:8@"AFDictationConnection"16@"NSError"24
v40@0:8@"AFDictationConnection"16@"NSString"24@"NSDictionary"32
v44@0:8@"AFDictationConnection"16@"NSString"24@"NSDictionary"32B40
v32@0:8@"AFDictationConnection"16@"SASMultilingualSpeechRecognized"24
v48@0:8@"AFDictationConnection"16@"NSArray"24@"NSString"32@40
v56@0:8@"AFDictationConnection"16@"NSArray"24@"NSString"32@40Q48
v40@0:8@"AFDictationConnection"16@"NSArray"24@"NSString"32
v32@0:8@"AFDictationConnection"16@"SASSpeechPartialResult"24
v32@0:8@"AFDictationConnection"16d24
v40@0:8@"AFDictationConnection"16@"NSFileHandle"24@"NSError"32
v48@0:8@"AFDictationConnection"16@"NSArray"24@"NSString"32B40B44
v32@0:8@"AFDictationConnection"16@"AFSpeechPackage"24
v24@0:8@"NSData"16
@48@0:8@16@24@32q40
f16@0:8
@"AFDictationConnection"
@"NSOperationQueue"
@"NSString"
@"SFSpeechRecognitionRequest"
@"NSObject<OS_dispatch_queue>"
@"NSError"
@56@0:8@16@24@32q40@?48
@56@0:8@16@24@32q40@48
v20@0:8B16
@"<_SFSpeechRecognitionTaskDelegatePrivate>"
@"SFSpeechRecognitionResult"
@"_SFSpeechRecognitionDelegateTask"
B32@0:8@16@24
@"_EARAppLmData"
@"_EARLmModel"
@"NSMutableDictionary"
@"NSLocale"
@28@0:8@16B24
@36@0:8@16@24B32
@44@0:8@16@24B32@36
@"SFTranscription"
@"SFSpeechRecognitionMetadata"
v24@0:8@?16
v32@0:8@"CXCallObserver"16@"CXCall"24
B24@0:8q16
@32@0:8@16@?24
@32@0:8@16@24
v24@0:8q16
@"CXCallObserver"
@"<NSObject>"
@"<SFSpeechRecognizerDelegate>"
@100@0:8@16{_NSRange=QQ}24d40d48f56@60@68@76@84@92
{_NSRange=QQ}16@0:8
@"SFVoiceAnalytics"
{_NSRange="location"Q"length"Q}
@"NSDictionary"
@48@0:8@16@24q32@40
@32@0:8q16@24
v24@0:8d16
@"_SFSearchRequest"
@"NSURL"
v24@0:8^{opaqueCMSampleBuffer=}16
@"<SFSpeechRecognitionBufferDelegate>"
@"NSMutableArray"
@"AVAudioConverter"
@48@0:8@16@24d32d40
@40@0:8@16@24d32
@40@0:8@"NSData"16@"NSDictionary"24d32
@"NSData"16@0:8
@56@0:8d16d24@32d40d48
v32@0:8@16@?24
mcpl
