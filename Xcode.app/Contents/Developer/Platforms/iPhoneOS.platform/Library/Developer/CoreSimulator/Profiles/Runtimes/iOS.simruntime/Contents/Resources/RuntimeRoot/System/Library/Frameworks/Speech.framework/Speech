@(#)PROGRAM:Speech  PROJECT:SpeechRecognition-1
MbP?
@mcpl
_acousticFeaturePerFrame
_frameDuration
, featureValues=%@, frameDuration=%f
_jitter
_shimmer
_pitch
_voicing
, jitter=%@, shimmer=%@, pitch=%@, voicing=%@
v32@?0@"NSString"8@"NSString"16^B24
com.apple.Speech.Task.Internal
v8@?0
User denied access to speech recognition
-[SFSpeechRecognitionTask handleSpeechRecognitionDidFailWithError:]_block_invoke
-[SFSpeechRecognitionTask dictationConnection:speechRecordingDidFail:]_block_invoke
-[SFSpeechRecognitionTask localSpeechRecognitionClient:speechRecordingDidFail:]_block_invoke
v24@?0@"SFSpeechRecognitionResult"8@"NSError"16
textNotEmpty
isFinal
, applicationName=%@, applicationVersion=%@, inlineItemList=%@, requestIdentifier=%@ task=%@ language=%@ narrowband=%d recognitionOverrides=%@ modelOverrideURL=%@ maximumRecognitionDuration=%f dynamicLanguageModel=%@ dynamicVocabulary=%@ detectMultipleUtterances=%d onDeviceOnly=%d enableAutoPunctuation=%d
SFRequestParameters::applicationName
SFRequestParameters::applicationVersion
SFRequestParameters::inlineItemList
SFRequestParameters::requestIdentifier
SFRequestParameters::taskIdentifier
SFRequestParameters::task
SFRequestParameters::language
SFRequestParameters::narrowband
SFRequestParameters::recognitionOverrides
SFRequestParameters::modelOverrideURL
SFRequestParameters::maximumRecognitionDuration
SFRequestParameters::dynamicLanguageModel
SFRequestParameters::dynamicVocabulary
SFRequestParameters::detectMultipleUtterances
SFRequestParameters::onDeviceOnly
SFRequestParameters::enableAutoPunctuation
v24@?0@"NSSet"8@"NSSet"16
Cannot make recognizer for %@. Supported locale identifiers are %@
%@-%@
v16@?0@"NSError"8
v16@?0@"NSArray"8
v16@?0@"NSDictionary"8
v24@?0@"NSURL"8@"NSURL"16
Write failed error:%@
Write successfull
Failed to generate ngram counts
There was a problem writing data to the file! Error:  %@
Ngram counts and OOV list serialized and saved to: %@
v16@?0@"NSData"8
grapheme
phoneme
Invalid prons: %@ for word: %@
v12@?0B8
v16@?0q8
Could not find ngrams serialized data file: %@ 
Failed to read the file: %@
Successfully read the file: %@
Failed to read the pronunciation file: %@
Failed to create directory error:%@
Created directory: %@
Failed to initialize language model: %@
Could not issue sandbox extension for path:%@
No write directory provided
No rawPhraseCountsPath provided
No customPronunciationsPath provided but will continue..
v16@?0@"NSURL"8
No phraseCountArtifact provided
_bestTranscription
_rawTranscription
_final
_speechRecognitionMetadata
 final=%d, bestTranscription=%@, speechRecognitionMetadata=%@
v32@?0@"NSString"8Q16^B24
v32@?0@"SFTranscriptionSegment"8Q16^B24
v12@?0C8
v16@?0@"NSNotification"8
Required assets are not available for Locale:%@
v24@?0@"NSString"8@"NSError"16
v16@?0@"NSSet"8
Result handler must be non-null
%@ queue must not be nil
, substringRange=%@, timestamp=%@, duration=%@, confidence=%@, substring=%@, alternativeSubstrings=%@, phoneSequence=%@, ipaPhoneSequence=%@, voiceAnalytics=%@
_substring
_substringRange.location
_substringRange.length
_timestamp
_duration
_confidence
_alternativeSubstrings
_alternativeConfidences
_phoneSequence
_ipaPhoneSequence
_voiceAnalytics
SFTranscriptionSegment.m
com.apple.speechapi.RequestStarted
com.apple.speechapi.StopSpeech
com.apple.speechapi.CancelSpeech
com.apple.speechapi.ResultPackage
com.apple.speechapi.RequestCompleted
com.apple.speechapi.RequestFailed
com.apple.speechapi.RequestPerformance
com.apple.speechapi.AssetRequested
com.apple.speechapi.AssetRequestCompleted
unknown.%ld
com.apple.speech.localspeechrecognition
SFLocalSpeechRecognitionClient
v20@?0Q8B16
+N9mZUAHooNvMiQnjeTJ8g
InternalBuild
hi-IN
hi-IN-translit
Translit
speakingRate
averagePauseDuration
jitter
shimmer
pitch
voicing
Dictation
VoiceMail
Captioning
dumpedTaskIdentifier
success
language
task
narrowband
appname
ondevice
modelVersion
audioDuration
ttfw
responseTime
modelLoadTime
recognitionDuration
jitLmeProcessingTime
asrInitializationTime
cpuRtf
errorCode
errorDomain
v32@?0@"AFSpeechToken"8Q16^B24
v32@?0@"AFSpeechInterpretation"8Q16^B24
Siri
trial_dictation_asset_delivery
Failed to access path: %@ method:%@
+[SFUtilities issueReadWriteSandboxExtensionForDirectoryPath:error:]
com.apple.app-sandbox.read-write
+[SFUtilities issueReadSandboxExtensionForFilePath:error:]
com.apple.app-sandbox.read
/var/mobile/Library/Caches
/private/var/mobile/Library/Caches
+[SFUtilities lsrCacheDirPath]
SFSpeechPreecordedRequest
Use -[SFSpeechURLRecognitionRequest initWithURL:]
Could not add output for %@
B8@?0
none
com.apple.SFSpeechAudioBufferRecognitionRequest
SFSpeechRecognitionRequest.m
%@ cannot be re-used
Invalid audio format
@"AVAudioBuffer"20@?0I8^q12
Could not drain converter %@
Could not run audio converter %@
CMBlockBufferCopyDataBytes could not copy data: %d
_speakingRate
_avgPauseDuration
, formattedString=%@, segments=%@, speakingRate=%f, averagePauseDuration=%f
_segments
_formattedString
QuasarDir
Language
Failed to query MobileAsset with error=%ld
Found %lu assets for language=%@
Assets are installed for language=%@
No asset installed for language=%@
No asset found for language=%@
LSRConnection
SpeechFramework
_averagePauseDuration
_speechStartTimestamp
_speechDuration
, speakingRate=%@, averagePauseDuration=%@, speechStartTimestamp=%@, speechDuration=%@, voiceAnalytics=%@
%s Ignoring subsequent recongition error: %@
%s Ignoring subsequent recording error: %@
%s Ignoring subsequent local speech recording error: %@
%@ Interrupted
%@ Invalidated
Dealloc-ing
Local speech recognizer restarted while already recognizing
Received an error while accessing %@ service: %@
%s %@: path=%@
%s %@: sandbox_extension_issue_file() returned NULL. path=%@
%s %@: Inaccessible file (%@) : error=%@
%s %@: sandboxExtension=%@
%s Failed to create directory %@
%s Write successful %@
SFAcousticFeature
NSCopying
NSSecureCoding
NSCoding
SFVoiceAnalytics
_SFSearchResult
SFSpeechRecognitionTask
AFDictationDelegate
NSObject
SFSpeechRecognitionBufferDelegate
SFLocalSpeechRecognitionDelegate
_SFSpeechRecognitionBlockTask
_SFSpeechRecognitionDelegateTask
SFRequestParameters
SFLSRProtocol
SFLSRDelegate
SFSpeechLanguageModel
SFSpeechRecognitionResult
SRSampling
SFSpeechRecognizer
CXCallObserverDelegate
SFTranscriptionSegment
SFLocalSpeechRecognitionClient
SFUtilities
_SFSearchRequest
SFSpeechRecognitionRequest
SFSpeechURLRecognitionRequest
SFSpeechAudioBufferRecognitionRequest
SFTranscription
SFAdditions
SFSpeechAssetManager
SFSpeechRecognitionMetadata
encodeObject:forKey:
encodeDouble:forKey:
init
arrayWithObjects:count:
setWithArray:
decodeObjectOfClasses:forKey:
decodeDoubleForKey:
copy
description
stringByAppendingFormat:
supportsSecureCoding
copyWithZone:
encodeWithCoder:
initWithCoder:
TB,R
_initWithAcousticFeatureValue:frameDuration:
acousticFeatureValuePerFrame
frameDuration
.cxx_destruct
_acousticFeatureValuePerFrame
_frameDuration
T@"NSArray",R,C,N,V_acousticFeatureValuePerFrame
Td,R,N,V_frameDuration
decodeObjectOfClass:forKey:
_initWithJitter:shimmer:pitch:voicing:
jitter
shimmer
pitch
voicing
_jitter
_shimmer
_pitch
_voicing
T@"SFAcousticFeature",R,C,N,V_jitter
T@"SFAcousticFeature",R,C,N,V_shimmer
T@"SFAcousticFeature",R,C,N,V_pitch
T@"SFAcousticFeature",R,C,N,V_voicing
statusCode
intValue
headers
enumerateKeysAndObjectsUsingBlock:
_responseWithCFURLResponse:
result
searchType
isEqualToString:
JSONObjectWithData:options:error:
dataWithJSONObject:options:error:
initWithData:encoding:
stringByAppendingString:
initWithVoiceSearchResult:
response
data
_response
_data
_searchType
T@"NSHTTPURLResponse",R,N,V_response
T@"NSData",R,N,V_data
Tq,R,N,V_searchType
UUID
UUIDString
taskHint
requiresOnDeviceRecognition
isSpeechXPCEnabled
_forceUseSiriProcess
_searchRequest
installedLanguages
containsObject:
taskIdentifier
_startedLocalConnectionWithLanguageCode:delegate:taskHint:requestIdentifier:taskIdentifier:
_startedConnectionWithLanguageCode:delegate:taskHint:requestIdentifier:
stopSpeechWithOptions:
stopSpeech
cancelSpeech
peakPower
averagePower
code
dictionaryWithObjects:forKeys:count:
errorWithDomain:code:userInfo:
_afDictationRequestParams
logCoreAnalyticsEvent:withAnalytics:
handleSpeechRecognitionDidFailWithError:
invalidate
addRecordedSpeechSampleData:
addAudioPacket:
initialize
isEqual:
class
self
performSelector:
performSelector:withObject:
performSelector:withObject:withObject:
isProxy
isKindOfClass:
isMemberOfClass:
conformsToProtocol:
respondsToSelector:
retain
release
autorelease
retainCount
zone
hash
superclass
debugDescription
TQ,R
T#,R
T@"NSString",R,C
dictationConnectionSpeechRecordingWillBegin:
dictationConnectionSpeechRecordingDidBegin:
dictationConnection:speechRecordingDidBeginWithOptions:
dictationConnection:didBeginLocalRecognitionWithModelInfo:
dictationConnectionSpeechRecordingDidEnd:
dictationConnectionSpeechRecordingDidCancel:
dictationConnection:speechRecordingDidFail:
dictationConnection:speechRecognitionDidFail:
dictationConnection:didDetectLanguage:confidenceScores:
dictationConnection:didDetectLanguage:confidenceScores:isConfident:
dictationConnection:didRecognizeMultilingualSpeech:
dictationConnection:languageDetectorFailedWithError:
dictationConnection:didRecognizePhrases:languageModel:correctionIdentifier:
dictationConnection:didRecognizePhrases:languageModel:correctionIdentifier:replacingPreviousPhrasesCount:
dictationConnection:didRecognizeTokens:languageModel:
dictationConnection:didRecognizeTokens:nluResult:languageModel:
dictationConnection:didRecognizePartialResult:
dictationConnection:didProcessAudioDuration:
dictationConnectionSpeechRecognitionDidSucceed:
dictationConnection:didRecognizeTranscriptionObjects:languageModel:
dictationConnnectionDidChangeAvailability:
dictationConnection:didFinishWritingAudioFile:error:
dictationConnection:didReceiveSearchResults:recognizedText:stable:final:
dictationConnection:didRecognizePackage:
dictationConnection:didRecognizePackage:nluResult:
dictationConnectionDidPauseRecognition:
dictationConnection:didRecognizeFinalResultCandidatePackage:
localSpeechRecognitionClientSpeechRecordingDidCancel:
localSpeechRecognitionClientSpeechRecognitionDidSucceed:
localSpeechRecognitionClient:speechRecordingDidFail:
localSpeechRecognitionClient:didRecognizePartialResult:rawPartialResult:
localSpeechRecognitionClient:didProcessAudioDuration:
localSpeechRecognitionClient:didFinishRecognition:
localSpeechRecognitionClient:speechRecognitionDidFail:
_initWithRequest:queue:languageCode:taskHint:
state
finish
cancel
error
_taskHint
isFinishing
isCancelled
requestIdentifier
_dictationConnection
_sflsrClient
_externalQueue
_languageCode
_request
_internalQueue
_error
_completed
_running
_finishing
_cancelled
_requestIdentifier
Tq,R,N,V_taskHint
T@"NSString",R,C,N,V_requestIdentifier
Tq,R,N
finishing
TB,R,N,GisFinishing,V_finishing
cancelled
TB,R,N,GisCancelled,V_cancelled
T@"NSError",R,C,N
isFinal
addOperationWithBlock:
_fireResultHandlerWithResult:error:
_finalizeResultHandler
recognizedResultFromPackage:
dictionary
addEntriesFromDictionary:
bestTranscription
formattedString
length
numberWithInt:
numberWithBool:
shouldReportPartialResults
transcriptionsWithTokens:
_initWithBestTranscription:rawTranscription:final:
_initWithRequest:queue:languageCode:taskHint:resultHandler:
_resultHandler
_hasFiredFinalResult
speechRecognitionTaskWasCancelled:
speechRecognitionTask:didHypothesizeTranscription:
speechRecognitionTask:didReceiveSearchResults:recognizedText:stable:final:
speechRecognitionTask:didFinishRecognition:
speechRecognitionTask:didProcessAudioDuration:
speechRecognitionTask:didFinishSuccessfully:
_handleSpeechRecordingDidCancel
_tellDelegateDidFinishSuccessfully:
speechRecognitionTaskFinishedReadingAudio:
_handleDidFinishRecognition:
_handleDidRecognizePartialResult:
_handleDidProcessAudioDuration:
countByEnumeratingWithState:objects:count:
numberWithInteger:
setObject:forKey:
_initWithRequest:queue:languageCode:taskHint:delegate:
_delegate
_recognitionResultToReportAfterFinalSearchResults
_selfReference
_waitForVoiceSearchResult
_hasSentRealSearchResults
encodeBool:forKey:
decodeBoolForKey:
applicationName
setApplicationName:
applicationVersion
setApplicationVersion:
inlineItemList
setInlineItemList:
setRequestIdentifier:
setTaskIdentifier:
language
setLanguage:
task
setTask:
narrowband
setNarrowband:
recognitionOverrides
setRecognitionOverrides:
modelOverrideURL
setModelOverrideURL:
maximumRecognitionDuration
setMaximumRecognitionDuration:
dynamicLanguageModel
setDynamicLanguageModel:
dynamicVocabulary
setDynamicVocabulary:
detectMultipleUtterances
setDetectMultipleUtterances:
onDeviceOnly
setOnDeviceOnly:
enableAutoPunctuation
setEnableAutoPunctuation:
_narrowband
_detectMultipleUtterances
_onDeviceOnly
_enableAutoPunctuation
_applicationName
_applicationVersion
_inlineItemList
_taskIdentifier
_language
_task
_recognitionOverrides
_modelOverrideURL
_maximumRecognitionDuration
_dynamicLanguageModel
_dynamicVocabulary
T@"NSString",C,N,V_applicationName
T@"NSString",C,N,V_applicationVersion
T@"NSArray",C,N,V_inlineItemList
T@"NSString",C,N,V_requestIdentifier
T@"NSString",C,N,V_taskIdentifier
T@"NSString",C,N,V_language
T@"NSString",C,N,V_task
TB,N,V_narrowband
T@"NSDictionary",C,N,V_recognitionOverrides
T@"NSURL",C,N,V_modelOverrideURL
Td,N,V_maximumRecognitionDuration
T@"NSURL",C,N,V_dynamicLanguageModel
T@"NSURL",C,N,V_dynamicVocabulary
TB,N,V_detectMultipleUtterances
TB,N,V_onDeviceOnly
TB,N,V_enableAutoPunctuation
startRecordedAudioDictationWithParameters:
preloadRecognizerForLanguage:task:recognitionOverrides:modelOverrideURL:completion:
installedLanguagesWithCompletion:
initializeWithSandboxExtensions:
downloadAssetsForLanguage:urgent:forceUpgrade:detailedProgress:
fetchAssetsForLanguage:completion:
configParametersForVoicemailWithLanguage:completion:
purgeAssetsForLanguage:completion:
setAssetsAsProvisional
promoteAssets
initializeLmWithLocale:completion:
initializeLmWithAssetPath:completion:
addSentenceToNgramCounts:
addSentenceToNgramCounts:completion:
addProns:forWord:completion:
oovWordsAndFrequenciesWithCompletion:
trainFromPlainTextAndWriteToDirectory:sandboxExtension:completion:
generateNgramCountsSerializeDataWithCompletion:
deserializeNgramCountsData:completion:
lmeThresholdWithCompletion:
metricsWithCompletion:
wakeUpWithCompletion:
trainAppLmFromNgramsSerializedData:customPronsData:language:writeToDirectory:sandboxExtension:completion:
createPhraseCountsArtifactWithIdentifier:rawPhraseCountsPath:customPronunciationsPath:writeToDirectory:sandboxExtensions:completion:
createNgramCountsArtifactFromPhraseCountArtifact:writeToDirectory:sandboxExtensions:completion:
trainAppLmFromNgramCountsArtifact:language:writeToDirectory:sandboxExtensions:completion:
interfaceWithProtocol:
setClasses:forSelector:argumentIndex:ofReply:
localSpeechRecognitionDidRecognizePartialResult:rawPartialResult:
localSpeechRecognitionDidProcessAudioDuration:
localSpeechRecognitionDidFinishRecognition:
localSpeechRecognitionDidFail:
localSpeechRecognitionDidSucceed
localSpeechRecognitionDidDownloadAssetsWithProgress:isStalled:
localSpeechRecognitionDidFinishDownloadingAssets:error:
setWithObject:
supportedLocalesWithCompletion:
localeIdentifier
stringByReplacingOccurrencesOfString:withString:
localeWithLocaleIdentifier:
objectForKey:
stringWithFormat:
assetPathForLanguage:
initWithAssetPath:
dealloc
addSentence:
outOfVocabularyWordsAndFrequencies
allKeys
defaultManager
createDirectoryAtURL:withIntermediateDirectories:attributes:error:
path
issueReadWriteSandboxExtensionForDirectoryPath:error:
writeToURL:options:error:
initWithFilePath:
lexemes
objectForKeyedSubscript:
componentsSeparatedByString:
addProns:forWord:
allObjects
raise:format:
fileExistsAtPath:
dataWithContentsOfURL:
array
addObject:
issueReadSandboxExtensionForFilePath:error:
createPhraseCountsArtifactWithIdentifier:rawPhraseCountsPath:customPronunciationsPath:writeDirectory:
createNgramCountsArtifactFromPhraseCountArtifact:writeDirectory:
supportedLocales
trainAppLmFromNgramsSerializedDataFile:customPronsFile:language:writeToDirectory:modelOverride:completion:
trainAppLmFromNgramCountsArtifact:language:writeToDirectory:modelOverride:completion:
initWithLocale:
addSentences:
addOovsFromSentence:
outOfVocabularyWords
trainFromPlainTextAndWriteToDirectory:completion:
trainFromPlainTextAndWriteToDirectory:
generateNgramsSerializeDataAndWriteToFile:
deserializeModelData:
addPronsFromFile:
lmeThreshold
metrics
createNgramCountsArtifactWithIdentifier:rawPhraseCountsPath:customPronunciationsPath:writeDirectory:
locale
_lsrClient
_locale
T@"NSLocale",R,C,N,V_locale
T@"NSArray",R,C,N
T@"NSDictionary",R,C,N
_initWithBestTranscription:rawTranscription:final:speechRecognitionMetadata:
segments
alternativeSubstrings
alternativeConfidences
mutableCopy
substring
insertObject:atIndex:
confidence
numberWithFloat:
subarrayWithRange:
addObjectsFromArray:
substringRange
removeObjectAtIndex:
objectAtIndex:
timestamp
duration
floatValue
phoneSequence
ipaPhoneSequence
_initWithSubstring:range:timestamp:duration:confidence:alternativeSubstrings:alternativeConfidences:phoneSequence:ipaPhoneSequence:voiceAnalytics:
count
stringByReplacingCharactersInRange:withString:
_initWithSegments:formattedString:speakingRate:averagePauseDuration:
enumerateObjectsUsingBlock:
expandTranscription:
unarchivedObjectOfClass:fromData:error:
archivedDataWithRootObject:requiringSecureCoding:error:
initWithBinarySampleRepresentation:metadata:timestamp:
binarySampleRepresentation
_initWithBestTranscription:final:
transcriptions
rawTranscriptions
speechRecognitionMetadata
rawTranscription
_transcriptions
_rawTranscriptions
_final
_bestTranscription
_speechRecognitionMetadata
_rawTranscription
T@"SFTranscription",R,C,N,V_rawTranscription
T@"SFTranscription",R,C,N,V_bestTranscription
final
TB,R,N,GisFinal,V_final
T@"SFSpeechRecognitionMetadata",R,N,V_speechRecognitionMetadata
authorizationStatus
currentLocale
mainQueue
setDelegate:
beginAvailabilityMonitoring
defaultCenter
_informDelegateOfPreferencesChange
addObserverForName:object:queue:usingBlock:
setDelegate:queue:
endSession
cancelAvailabilityMonitoring
removeObserver:
dictationIsAvailableForLanguage:synchronous:
_isAvailableForForcedOfflineRecognition
forcedOfflineDictationIsAvailableForLanguage:
fetchAssetsForLanguage:urgent:progress:completion:
requestOfflineDictationSupportForLanguage:completion:
getForcedOfflineDictationSupportedLanguagesWithCompletion:
taskNameFromTaskHint:
supportsOnDeviceRecognition
_informDelegateOfAvailabilityChange
speechRecognizer:availabilityDidChange:
isAvailable
dictationIsEnabled
requestAuthorization:
_fetchSupportedForcedOfflineLocalesWithCompletion:
callObserver:callChanged:
_requestOfflineDictationSupportWithCompletion:
_prepareToRecognizeWithTaskHint:completion:
_isInternalTaskHint:
recognitionTaskWithRequest:resultHandler:
recognitionTaskWithRequest:delegate:
setQueue:
setSupportsOnDeviceRecognition:
delegate
defaultTaskHint
setDefaultTaskHint:
queue
_callObserver
_facetimeObserver
_foregroundObserver
_preferencesObserver
_supportsOnDeviceRecognition
_defaultTaskHint
_queue
_availableForForcedOfflineRecognition
TB,R,N,G_isAvailableForForcedOfflineRecognition
available
TB,R,N,GisAvailable
TB,N,V_supportsOnDeviceRecognition
T@"<SFSpeechRecognizerDelegate>",W,N,V_delegate
Tq,N,V_defaultTaskHint
T@"NSOperationQueue",&,N,V_queue
numberWithDouble:
encodeInteger:forKey:
decodeIntegerForKey:
currentHandler
handleFailureInMethod:object:file:lineNumber:description:
voiceAnalytics
_confidence
_substring
_timestamp
_duration
_alternativeSubstrings
_voiceAnalytics
_alternativeConfidences
_phoneSequence
_ipaPhoneSequence
_substringRange
T@"NSArray",R,N,V_alternativeConfidences
T@"NSString",R,N,V_phoneSequence
T@"NSString",R,N,V_ipaPhoneSequence
T@"NSString",R,C,N,V_substring
T{_NSRange=QQ},R,N,V_substringRange
Td,R,N,V_timestamp
Td,R,N,V_duration
Tf,R,N,V_confidence
T@"NSArray",R,N,V_alternativeSubstrings
T@"SFVoiceAnalytics",R,N,V_voiceAnalytics
initWithFormat:
initWithServiceName:
setRemoteObjectInterface:
setExportedInterface:
setExportedObject:
setInterruptionHandler:
setInvalidationHandler:
resume
synchronousRemoteObjectProxyWithErrorHandler:
remoteObjectProxyWithErrorHandler:
_serviceProxyWithErrorHandler:
_asynchronousServiceProxyWithErrorHandler:
initWithDelegate:
installedLanguagesWithSynchronousConnection:completion:
downloadAssetsForLanguage:urgent:forceUpgrade:progress:completionHandler:
downloadAssetsForLanguage:urgent:forceUpgrade:detailedProgress:completionHandler:
_lsrConnection
_downloadProgress
_downloadCompletion
_recognitionActive
T@"<SFLocalSpeechRecognitionDelegate>",R,W,N,V_delegate
setWithCapacity:
string
interpretations
firstObject
tokens
startTime
silenceStartTime
removeSpaceBefore
removeSpaceAfter
appendString:
text
confidenceScore
stringWithString:
speechRecognitionFeatures
doubleValue
_initWithSpeechStartTimestamp:speechDuration:voiceAnalytics:speakingRate:averagePauseDuration:
recognition
phrases
audioAnalytics
utteranceStart
transcriptionFromSpeechPhrases:afAudioAnalytics:utteranceStart:
rawRecognition
recognitionMetadataFromSpeechPhrases:afAudioAnalytics:utteranceStart:
canAccessPathAt:methodName:error:
UTF8String
stringWithUTF8String:
hasPrefix:
stringByAppendingPathComponent:
createDirectoryAtPath:withIntermediateDirectories:attributes:error:
isCacheDirInternal
lsrCacheDirPath
acousticFeatures
searchTypes
setSearchTypes:
headerFields
setHeaderFields:
queryParameters
setQueryParameters:
_searchTypes
_headerFields
_queryParameters
Tq,N,V_searchTypes
T@"NSDictionary",C,N,V_headerFields
T@"NSDictionary",C,N,V_queryParameters
_setForceOfflineRecognition:
_forceOfflineRecognition
mainBundle
bundleIdentifier
infoDictionary
setVoiceTriggerEventInfo:
setDetectUtterances:
setVoiceSearchTypeOptions:
setVoiceSearchQueryParameters:
setVoiceSearchHeaderFields:
setKeyboardType:
setTaskHint:
setAutoPunctuation:
setForceOfflineRecognition:
caseInsensitiveCompare:
addsPunctuation
initWithActivationEvent:
automaticallyEndpoint
setUseAutomaticEndpointing:
setUseStreamingDictation:
processInfo
systemUptime
setActivationEventTime:
_setSearchRequests:
_searchRequests
_powerMeteringAvailable
setRequiresOnDeviceRecognition:
_dictationOptionsWithTaskHint:requestIdentifier:
_requestParametersWithTaskHint:requestIdentifier:taskIdentifier:narrowband:language:
_sandboxExtensionsWithError:
_speechRequestOptions
_setMaximumRecognitionDuration:
_setSearchRequest:
_voiceTriggerEventInfo
_setVoiceTriggerEventInfo:
_setRecognitionOverrides:
_setModelOverrideURL:
_setAFDictationRequestParams:
_setDynamicLanguageModel:
_setDynamicVocabulary:
_setForceUseSiriProcess:
setShouldReportPartialResults:
contextualStrings
setContextualStrings:
interactionIdentifier
setInteractionIdentifier:
setAddsPunctuation:
_shouldReportPartialResults
_addsPunctuation
_contextualStrings
_interactionIdentifier
T@"NSDictionary",&,N,G_afDictationRequestParams,S_setAFDictationRequestParams:,V_afDictationRequestParams
T@"_SFSearchRequest",&,N,G_searchRequest,S_setSearchRequest:,V_searchRequest
TB,N,G_forceOfflineRecognition,S_setForceOfflineRecognition:,V_forceOfflineRecognition
T@"NSDictionary",&,N,G_voiceTriggerEventInfo,S_setVoiceTriggerEventInfo:,V_voiceTriggerEventInfo
Td,N,G_maximumRecognitionDuration,S_setMaximumRecognitionDuration:,V_maximumRecognitionDuration
T@"NSDictionary",&,N,G_recognitionOverrides,S_setRecognitionOverrides:,V_recognitionOverrides
T@"NSURL",&,N,G_modelOverrideURL,S_setModelOverrideURL:,V_modelOverrideURL
T@"NSURL",&,N,G_dynamicLanguageModel,S_setDynamicLanguageModel:,V_dynamicLanguageModel
T@"NSURL",&,N,G_dynamicVocabulary,S_setDynamicVocabulary:,V_dynamicVocabulary
TB,N,G_forceUseSiriProcess,S_setForceUseSiriProcess:,V_forceUseSiriProcess
T@"NSString",&,N,V_taskIdentifier
Tq,N,V_taskHint
TB,N,V_shouldReportPartialResults
T@"NSArray",C,N,V_contextualStrings
T@"NSString",C,N,V_interactionIdentifier
TB,N
TB,N,V_addsPunctuation
assetReaderWithAsset:error:
numberWithUnsignedInteger:
assetReaderAudioMixOutputWithAudioTracks:audioSettings:
canAddOutput:
addOutput:
startReading
copyNextSampleBuffer
setFieldLabel:
setKeyboardIdentifier:
setOriginalAudioFileURL:
assetWithURL:
tracksWithMediaType:
startRecordedAudioDictationWithOptions:forLanguage:narrowband:
_handlePreRecordedAudioWithAsset:audioTracks:narrowband:addSpeechDataBlock:stopSpeechBlock:cancelSpeechWithErrorBlock:
initWithURL:
_URL
T@"NSURL",R,C,N,V_URL
initWithStreamDescription:
_appendAudioPCMBuffer:
_endAudio
startRecordedAudioDictationWithOptions:forLanguage:
_handleAudioBuffersWithDelegate:
nativeAudioFormat
format
_drainAndClearAudioConverter
int16ChannelData
frameLength
dataWithBytes:length:
_convertAndFeedPCMBuffer:
initWithPCMFormat:frameCapacity:
setFrameLength:
convertToBuffer:error:withInputFromBlock:
inputFormat
initFromFormat:toFormat:
setSampleRateConverterQuality:
mutableAudioBufferList
appendAudioPCMBuffer:
appendAudioSampleBuffer:
endAudio
_bufferDelegate
_queuedBuffers
_converter
_audioEnded
T@"AVAudioFormat",R,N
initWithLength:
mutableBytes
formatDescriptions
speakingRate
averagePauseDuration
_formattedString
_segments
_speakingRate
_averagePauseDuration
T@"NSString",R,C,N,V_formattedString
T@"NSArray",R,C,N,V_segments
Td,R,N,V_speakingRate
Td,R,N,V_averagePauseDuration
getLocalFileUrl
_sf_path
attributes
_sf_quasarModelPath
_sf_isInstalled
initWithType:
returnTypes:
addKeyValuePair:with:
_assetQueryForLanguage:
queryMetaDataSync
results
fetchAssetsForLanguage:urgent:forceUpgrade:progress:completion:
fetchMAAssetPathForInstalledLanguage:
fetchAssetsForLanguage:progress:completion:
fetchAssetsForLanguage:urgent:forceUpgrade:detailedProgress:completion:
configParametersForVoicemailWithLanguage:
purgeAssetsForLanguage:error:
speechStartTimestamp
speechDuration
_speechStartTimestamp
_speechDuration
Td,R,N,V_speechStartTimestamp
Td,R,N,V_speechDuration
B16@0:8
@24@0:8^{_NSZone=}16
v24@0:8@16
@24@0:8@16
v24@0:8@"NSCoder"16
@24@0:8@"NSCoder"16
@32@0:8@16d24
@16@0:8
d16@0:8
v16@0:8
@"NSArray"
@48@0:8@16@24@32@40
@"SFAcousticFeature"
q16@0:8
@"NSHTTPURLResponse"
@"NSData"
B24@0:8@16
#16@0:8
@24@0:8:16
@32@0:8:16@24
@40@0:8:16@24@32
B24@0:8#16
B24@0:8:16
Vv16@0:8
Q16@0:8
^{_NSZone=}16@0:8
B24@0:8@"Protocol"16
@"NSString"16@0:8
v32@0:8@16@24
v40@0:8@16@24@32
v44@0:8@16@24@32B40
v48@0:8@16@24@32@40
v56@0:8@16@24@32@40Q48
v32@0:8@16d24
v48@0:8@16@24@32B40B44
v24@0:8@"AFDictationConnection"16
v32@0:8@"AFDictationConnection"16@"AFDictationOptions"24
v32@0:8@"AFDictationConnection"16@"NSString"24
v32@0:8@"AFDictationConnection"16@"NSError"24
v40@0:8@"AFDictationConnection"16@"NSString"24@"NSDictionary"32
v44@0:8@"AFDictationConnection"16@"NSString"24@"NSDictionary"32B40
v32@0:8@"AFDictationConnection"16@"SASMultilingualSpeechRecognized"24
v48@0:8@"AFDictationConnection"16@"NSArray"24@"NSString"32@40
v56@0:8@"AFDictationConnection"16@"NSArray"24@"NSString"32@40Q48
v40@0:8@"AFDictationConnection"16@"NSArray"24@"NSString"32
v48@0:8@"AFDictationConnection"16@"NSArray"24@"AFDictationNLUResult"32@"NSString"40
v32@0:8@"AFDictationConnection"16@"SASSpeechPartialResult"24
v32@0:8@"AFDictationConnection"16d24
v40@0:8@"AFDictationConnection"16@"NSFileHandle"24@"NSError"32
v48@0:8@"AFDictationConnection"16@"NSArray"24@"NSString"32B40B44
v32@0:8@"AFDictationConnection"16@"AFSpeechPackage"24
v40@0:8@"AFDictationConnection"16@"AFSpeechPackage"24@"AFDictationNLUResult"32
v24@0:8@"NSData"16
v24@0:8@"SFLocalSpeechRecognitionClient"16
v32@0:8@"SFLocalSpeechRecognitionClient"16@"NSError"24
v40@0:8@"SFLocalSpeechRecognitionClient"16@"SFTranscription"24@"SFTranscription"32
v32@0:8@"SFLocalSpeechRecognitionClient"16d24
v32@0:8@"SFLocalSpeechRecognitionClient"16@"SFSpeechRecognitionResult"24
@48@0:8@16@24@32q40
f16@0:8
@"AFDictationConnection"
@"SFLocalSpeechRecognitionClient"
@"NSOperationQueue"
@"NSString"
@"SFSpeechRecognitionRequest"
@"NSObject<OS_dispatch_queue>"
@"NSError"
@56@0:8@16@24@32q40@?48
@56@0:8@16@24@32q40@48
v24@0:8d16
v20@0:8B16
@"<_SFSpeechRecognitionTaskDelegatePrivate>"
@"SFSpeechRecognitionResult"
@"_SFSpeechRecognitionDelegateTask"
@"NSDictionary"
@"NSURL"
Vv24@0:8@16
Vv56@0:8@16@24@32@40@?48
Vv24@0:8@?16
Vv36@0:8@16B24B28B32
Vv32@0:8@16@?24
Vv40@0:8@16@24@?32
Vv64@0:8@16@24@32@40@48@?56
Vv48@0:8@16@24@32@?40
Vv24@0:8@"SFRequestParameters"16
Vv24@0:8@"NSData"16
Vv56@0:8@"NSString"16@"NSString"24@"NSDictionary"32@"NSURL"40@?<v@?@"NSError">48
Vv24@0:8@?<v@?@"NSSet">16
v24@0:8@"NSArray"16
v32@0:8@"NSString"16@"NSDictionary"24
Vv36@0:8@"NSString"16B24B28B32
Vv32@0:8@"NSString"16@?<v@?@"NSString"@"NSError">24
Vv32@0:8@"NSString"16@?<v@?@"NSDictionary">24
Vv32@0:8@"NSString"16@?<v@?@"NSError">24
Vv32@0:8@"NSLocale"16@?<v@?@"NSError">24
Vv24@0:8@"NSString"16
Vv32@0:8@"NSString"16@?<v@?@"NSArray">24
Vv40@0:8@"NSSet"16@"NSString"24@?<v@?B>32
Vv24@0:8@?<v@?@"NSDictionary">16
Vv40@0:8@"NSURL"16@"NSString"24@?<v@?@"NSURL"@"NSURL">32
Vv24@0:8@?<v@?@"NSData">16
Vv32@0:8@"NSData"16@?<v@?@"NSDictionary">24
Vv24@0:8@?<v@?q>16
Vv24@0:8@?<v@?>16
Vv64@0:8@"NSData"16@"NSData"24@"NSString"32@"NSURL"40@"NSString"48@?<v@?@"NSURL"@"NSURL">56
Vv64@0:8@"NSString"16@"NSURL"24@"NSURL"32@"NSURL"40@"NSArray"48@?<v@?@"NSURL">56
Vv48@0:8@"NSURL"16@"NSURL"24@"NSArray"32@?<v@?@"NSURL">40
Vv56@0:8@"NSURL"16@"NSString"24@"NSURL"32@"NSArray"40@?<v@?@"NSURL"@"NSURL">48
Vv32@0:8@16@24
Vv24@0:8d16
Vv28@0:8Q16B24
Vv32@0:8@"SFTranscription"16@"SFTranscription"24
Vv24@0:8@"SFSpeechRecognitionResult"16
Vv24@0:8@"NSError"16
Vv32@0:8@"NSString"16@"NSError"24
v64@0:8@16@24@32@40@48@?56
v56@0:8@16@24@32@40@?48
v32@0:8@16@?24
B32@0:8@16@24
@32@0:8@16@24
@"NSLocale"
@40@0:8@16@24d32
@40@0:8@"NSData"16@"NSDictionary"24d32
@"NSData"16@0:8
@28@0:8@16B24
@36@0:8@16@24B32
@44@0:8@16@24B32@36
@"SFTranscription"
@"SFSpeechRecognitionMetadata"
v24@0:8@?16
v32@0:8@"CXCallObserver"16@"CXCall"24
v32@0:8q16@?24
B24@0:8q16
@32@0:8@16@?24
v24@0:8q16
@"CXCallObserver"
@"<NSObject>"
@"<SFSpeechRecognizerDelegate>"
@100@0:8@16{_NSRange=QQ}24d40d48f56@60@68@76@84@92
{_NSRange=QQ}16@0:8
@"SFVoiceAnalytics"
{_NSRange="location"Q"length"Q}
@24@0:8@?16
v28@0:8B16@?20
v48@0:8@16B24B28@?32@?40
v40@0:8@16@24@?32
v48@0:8@16@24@32@?40
@"NSXPCConnection"
@"<SFLocalSpeechRecognitionDelegate>"
@24@0:8q16
B40@0:8@16@24^@32
@32@0:8@16^@24
@48@0:8@16@24q32@40
@56@0:8@16@24q32@40@48
@32@0:8q16@24
@52@0:8q16@24@32B40@44
@24@0:8^@16
@"_SFSearchRequest"
v60@0:8@16@24B32@?36@?44@?52
v24@0:8^{opaqueCMSampleBuffer=}16
@"<SFSpeechRecognitionBufferDelegate>"
@"NSMutableArray"
@"AVAudioConverter"
@48@0:8@16@24d32d40
v40@0:8@16@?24@?32
v44@0:8@16B24@?28@?36
v32@0:8@16^@24
@56@0:8d16d24@32d40d48
mcpl
@(#)PROGRAM:Speech  PROJECT:SpeechRecognition-1
MbP?
@mcpl
_acousticFeaturePerFrame
_frameDuration
, featureValues=%@, frameDuration=%f
_jitter
_shimmer
_pitch
_voicing
, jitter=%@, shimmer=%@, pitch=%@, voicing=%@
v32@?0@"NSString"8@"NSString"16^B24
com.apple.Speech.Task.Internal
v8@?0
User denied access to speech recognition
-[SFSpeechRecognitionTask handleSpeechRecognitionDidFailWithError:]_block_invoke
-[SFSpeechRecognitionTask dictationConnection:speechRecordingDidFail:]_block_invoke
-[SFSpeechRecognitionTask localSpeechRecognitionClient:speechRecordingDidFail:]_block_invoke
v24@?0@"SFSpeechRecognitionResult"8@"NSError"16
textNotEmpty
isFinal
, applicationName=%@, applicationVersion=%@, inlineItemList=%@, requestIdentifier=%@ task=%@ language=%@ narrowband=%d recognitionOverrides=%@ modelOverrideURL=%@ maximumRecognitionDuration=%f dynamicLanguageModel=%@ dynamicVocabulary=%@ detectMultipleUtterances=%d onDeviceOnly=%d enableAutoPunctuation=%d
SFRequestParameters::applicationName
SFRequestParameters::applicationVersion
SFRequestParameters::inlineItemList
SFRequestParameters::requestIdentifier
SFRequestParameters::taskIdentifier
SFRequestParameters::task
SFRequestParameters::language
SFRequestParameters::narrowband
SFRequestParameters::recognitionOverrides
SFRequestParameters::modelOverrideURL
SFRequestParameters::maximumRecognitionDuration
SFRequestParameters::dynamicLanguageModel
SFRequestParameters::dynamicVocabulary
SFRequestParameters::detectMultipleUtterances
SFRequestParameters::onDeviceOnly
SFRequestParameters::enableAutoPunctuation
v24@?0@"NSSet"8@"NSSet"16
Cannot make recognizer for %@. Supported locale identifiers are %@
%@-%@
v16@?0@"NSError"8
v16@?0@"NSArray"8
v16@?0@"NSDictionary"8
v24@?0@"NSURL"8@"NSURL"16
Write failed error:%@
Write successfull
Failed to generate ngram counts
There was a problem writing data to the file! Error:  %@
Ngram counts and OOV list serialized and saved to: %@
v16@?0@"NSData"8
grapheme
phoneme
Invalid prons: %@ for word: %@
v12@?0B8
v16@?0q8
Could not find ngrams serialized data file: %@ 
Failed to read the file: %@
Successfully read the file: %@
Failed to read the pronunciation file: %@
Failed to create directory error:%@
Created directory: %@
Failed to initialize language model: %@
Could not issue sandbox extension for path:%@
No write directory provided
No rawPhraseCountsPath provided
No customPronunciationsPath provided but will continue..
v16@?0@"NSURL"8
No phraseCountArtifact provided
_bestTranscription
_rawTranscription
_final
_speechRecognitionMetadata
 final=%d, bestTranscription=%@, speechRecognitionMetadata=%@
v32@?0@"NSString"8Q16^B24
v32@?0@"SFTranscriptionSegment"8Q16^B24
v12@?0C8
v16@?0@"NSNotification"8
Required assets are not available for Locale:%@
v24@?0@"NSString"8@"NSError"16
v16@?0@"NSSet"8
Result handler must be non-null
%@ queue must not be nil
, substringRange=%@, timestamp=%@, duration=%@, confidence=%@, substring=%@, alternativeSubstrings=%@, phoneSequence=%@, ipaPhoneSequence=%@, voiceAnalytics=%@
_substring
_substringRange.location
_substringRange.length
_timestamp
_duration
_confidence
_alternativeSubstrings
_alternativeConfidences
_phoneSequence
_ipaPhoneSequence
_voiceAnalytics
SFTranscriptionSegment.m
com.apple.speechapi.RequestStarted
com.apple.speechapi.StopSpeech
com.apple.speechapi.CancelSpeech
com.apple.speechapi.ResultPackage
com.apple.speechapi.RequestCompleted
com.apple.speechapi.RequestFailed
com.apple.speechapi.RequestPerformance
com.apple.speechapi.AssetRequested
com.apple.speechapi.AssetRequestCompleted
unknown.%ld
com.apple.speech.localspeechrecognition
SFLocalSpeechRecognitionClient
v20@?0Q8B16
+N9mZUAHooNvMiQnjeTJ8g
InternalBuild
hi-IN
hi-IN-translit
Translit
speakingRate
averagePauseDuration
jitter
shimmer
pitch
voicing
Dictation
VoiceMail
Captioning
dumpedTaskIdentifier
success
language
task
narrowband
appname
ondevice
modelVersion
audioDuration
ttfw
responseTime
modelLoadTime
recognitionDuration
jitLmeProcessingTime
asrInitializationTime
cpuRtf
errorCode
errorDomain
v32@?0@"AFSpeechToken"8Q16^B24
v32@?0@"AFSpeechInterpretation"8Q16^B24
Siri
trial_dictation_asset_delivery
Failed to access path: %@ method:%@
+[SFUtilities issueReadWriteSandboxExtensionForDirectoryPath:error:]
com.apple.app-sandbox.read-write
+[SFUtilities issueReadSandboxExtensionForFilePath:error:]
com.apple.app-sandbox.read
/var/mobile/Library/Caches
/private/var/mobile/Library/Caches
+[SFUtilities lsrCacheDirPath]
SFSpeechPreecordedRequest
Use -[SFSpeechURLRecognitionRequest initWithURL:]
Could not add output for %@
B8@?0
none
com.apple.SFSpeechAudioBufferRecognitionRequest
SFSpeechRecognitionRequest.m
%@ cannot be re-used
Invalid audio format
@"AVAudioBuffer"20@?0I8^q12
Could not drain converter %@
Could not run audio converter %@
CMBlockBufferCopyDataBytes could not copy data: %d
_speakingRate
_avgPauseDuration
, formattedString=%@, segments=%@, speakingRate=%f, averagePauseDuration=%f
_segments
_formattedString
QuasarDir
Language
Failed to query MobileAsset with error=%ld
Found %lu assets for language=%@
Assets are installed for language=%@
No asset installed for language=%@
No asset found for language=%@
LSRConnection
SpeechFramework
_averagePauseDuration
_speechStartTimestamp
_speechDuration
, speakingRate=%@, averagePauseDuration=%@, speechStartTimestamp=%@, speechDuration=%@, voiceAnalytics=%@
%s Ignoring subsequent recongition error: %@
%s Ignoring subsequent recording error: %@
%s Ignoring subsequent local speech recording error: %@
%@ Interrupted
%@ Invalidated
Dealloc-ing
Local speech recognizer restarted while already recognizing
Received an error while accessing %@ service: %@
%s %@: path=%@
%s %@: sandbox_extension_issue_file() returned NULL. path=%@
%s %@: Inaccessible file (%@) : error=%@
%s %@: sandboxExtension=%@
%s Failed to create directory %@
%s Write successful %@
SFAcousticFeature
NSCopying
NSSecureCoding
NSCoding
SFVoiceAnalytics
_SFSearchResult
SFSpeechRecognitionTask
AFDictationDelegate
NSObject
SFSpeechRecognitionBufferDelegate
SFLocalSpeechRecognitionDelegate
_SFSpeechRecognitionBlockTask
_SFSpeechRecognitionDelegateTask
SFRequestParameters
SFLSRProtocol
SFLSRDelegate
SFSpeechLanguageModel
SFSpeechRecognitionResult
SRSampling
SFSpeechRecognizer
CXCallObserverDelegate
SFTranscriptionSegment
SFLocalSpeechRecognitionClient
SFUtilities
_SFSearchRequest
SFSpeechRecognitionRequest
SFSpeechURLRecognitionRequest
SFSpeechAudioBufferRecognitionRequest
SFTranscription
SFAdditions
SFSpeechAssetManager
SFSpeechRecognitionMetadata
setDetectUtterances:
setInvalidationHandler:
countByEnumeratingWithState:objects:count:
encodeInteger:forKey:
mutableCopy
setVoiceSearchTypeOptions:
objectForKeyedSubscript:
statusCode
audioAnalytics
int16ChannelData
intValue
dictationIsAvailableForLanguage:synchronous:
stringWithString:
initWithActivationEvent:
mutableBytes
fileExistsAtPath:
dataWithContentsOfURL:
unarchivedObjectOfClass:fromData:error:
attributes
setInterruptionHandler:
count
initFromFormat:toFormat:
getForcedOfflineDictationSupportedLanguagesWithCompletion:
initWithStreamDescription:
objectForKey:
encodeDouble:forKey:
removeObserver:
setOriginalAudioFileURL:
dataWithBytes:length:
setVoiceSearchQueryParameters:
copyNextSampleBuffer
mutableAudioBufferList
startTime
addOutput:
stringWithFormat:
objectAtIndex:
setFrameLength:
assetWithURL:
encodeBool:forKey:
processInfo
frameLength
_responseWithCFURLResponse:
cancelAvailabilityMonitoring
numberWithUnsignedInteger:
setVoiceSearchHeaderFields:
removeObjectAtIndex:
canAddOutput:
initWithServiceName:
systemUptime
setDelegate:queue:
currentLocale
remoteObjectProxyWithErrorHandler:
stringByReplacingOccurrencesOfString:withString:
initWithPCMFormat:frameCapacity:
assetReaderWithAsset:error:
isEqualToString:
copy
returnTypes:
setUseStreamingDictation:
defaultManager
setForceOfflineRecognition:
convertToBuffer:error:withInputFromBlock:
addOperationWithBlock:
UUIDString
startRecordedAudioDictationWithOptions:forLanguage:narrowband:
currentHandler
infoDictionary
stringByReplacingCharactersInRange:withString:
setFieldLabel:
speechRecognitionFeatures
allObjects
startRecordedAudioDictationWithOptions:forLanguage:
synchronousRemoteObjectProxyWithErrorHandler:
assetReaderAudioMixOutputWithAudioTracks:audioSettings:
mainBundle
numberWithInteger:
setObject:forKey:
setUseAutomaticEndpointing:
defaultCenter
bundleIdentifier
mainQueue
phrases
addObserverForName:object:queue:usingBlock:
UTF8String
resume
results
setClasses:forSelector:argumentIndex:ofReply:
stringByAppendingString:
silenceStartTime
allKeys
initWithLength:
numberWithInt:
containsObject:
headers
startReading
inputFormat
setAutoPunctuation:
addObjectsFromArray:
insertObject:atIndex:
decodeObjectOfClasses:forKey:
stringByAppendingPathComponent:
formatDescriptions
errorWithDomain:code:userInfo:
tracksWithMediaType:
recognition
numberWithFloat:
setExportedObject:
confidenceScore
addObject:
addKeyValuePair:with:
initWithFormat:
doubleValue
decodeObjectOfClass:forKey:
enumerateObjectsUsingBlock:
format
numberWithDouble:
rawRecognition
arrayWithObjects:count:
stringByAppendingFormat:
setExportedInterface:
result
decodeIntegerForKey:
hasPrefix:
beginAvailabilityMonitoring
string
tokens
setWithObject:
setSampleRateConverterQuality:
setWithCapacity:
array
initWithFilePath:
forcedOfflineDictationIsAvailableForLanguage:
lexemes
path
enumerateKeysAndObjectsUsingBlock:
dictionaryWithObjects:forKeys:count:
numberWithBool:
raise:format:
setKeyboardType:
endSession
addEntriesFromDictionary:
decodeDoubleForKey:
archivedDataWithRootObject:requiringSecureCoding:error:
interpretations
localeWithLocaleIdentifier:
writeToURL:options:error:
createDirectoryAtURL:withIntermediateDirectories:attributes:error:
setActivationEventTime:
requestOfflineDictationSupportForLanguage:completion:
length
stopSpeechWithOptions:
UUID
floatValue
componentsSeparatedByString:
subarrayWithRange:
createDirectoryAtPath:withIntermediateDirectories:attributes:error:
queryMetaDataSync
dictionary
setKeyboardIdentifier:
setWithArray:
localeIdentifier
decodeBoolForKey:
removeSpaceBefore
utteranceStart
JSONObjectWithData:options:error:
text
setVoiceTriggerEventInfo:
initWithData:encoding:
encodeObject:forKey:
handleFailureInMethod:object:file:lineNumber:description:
appendString:
dictationIsEnabled
getLocalFileUrl
interfaceWithProtocol:
stringWithUTF8String:
code
acousticFeatures
caseInsensitiveCompare:
setRemoteObjectInterface:
firstObject
initWithType:
removeSpaceAfter
dataWithJSONObject:options:error:
init
description
supportsSecureCoding
copyWithZone:
encodeWithCoder:
initWithCoder:
TB,R
_initWithAcousticFeatureValue:frameDuration:
acousticFeatureValuePerFrame
frameDuration
.cxx_destruct
_acousticFeatureValuePerFrame
_frameDuration
T@"NSArray",R,C,N,V_acousticFeatureValuePerFrame
Td,R,N,V_frameDuration
_initWithJitter:shimmer:pitch:voicing:
jitter
shimmer
pitch
voicing
_jitter
_shimmer
_pitch
_voicing
T@"SFAcousticFeature",R,C,N,V_jitter
T@"SFAcousticFeature",R,C,N,V_shimmer
T@"SFAcousticFeature",R,C,N,V_pitch
T@"SFAcousticFeature",R,C,N,V_voicing
initWithVoiceSearchResult:
response
data
searchType
_response
_data
_searchType
T@"NSHTTPURLResponse",R,N,V_response
T@"NSData",R,N,V_data
Tq,R,N,V_searchType
initialize
isEqual:
class
self
performSelector:
performSelector:withObject:
performSelector:withObject:withObject:
isProxy
isKindOfClass:
isMemberOfClass:
conformsToProtocol:
respondsToSelector:
retain
release
autorelease
retainCount
zone
hash
superclass
debugDescription
TQ,R
T#,R
T@"NSString",R,C
dictationConnectionSpeechRecordingWillBegin:
dictationConnectionSpeechRecordingDidBegin:
dictationConnection:speechRecordingDidBeginWithOptions:
dictationConnection:didBeginLocalRecognitionWithModelInfo:
dictationConnectionSpeechRecordingDidEnd:
dictationConnectionSpeechRecordingDidCancel:
dictationConnection:speechRecordingDidFail:
dictationConnection:speechRecognitionDidFail:
dictationConnection:didDetectLanguage:confidenceScores:
dictationConnection:didDetectLanguage:confidenceScores:isConfident:
dictationConnection:didRecognizeMultilingualSpeech:
dictationConnection:languageDetectorFailedWithError:
dictationConnection:didRecognizePhrases:languageModel:correctionIdentifier:
dictationConnection:didRecognizePhrases:languageModel:correctionIdentifier:replacingPreviousPhrasesCount:
dictationConnection:didRecognizeTokens:languageModel:
dictationConnection:didRecognizeTokens:nluResult:languageModel:
dictationConnection:didRecognizePartialResult:
dictationConnection:didProcessAudioDuration:
dictationConnectionSpeechRecognitionDidSucceed:
dictationConnection:didRecognizeTranscriptionObjects:languageModel:
dictationConnnectionDidChangeAvailability:
dictationConnection:didFinishWritingAudioFile:error:
dictationConnection:didReceiveSearchResults:recognizedText:stable:final:
dictationConnection:didRecognizePackage:
dictationConnection:didRecognizePackage:nluResult:
dictationConnectionDidPauseRecognition:
dictationConnection:didRecognizeFinalResultCandidatePackage:
addRecordedSpeechSampleData:
stopSpeech
localSpeechRecognitionClientSpeechRecordingDidCancel:
localSpeechRecognitionClientSpeechRecognitionDidSucceed:
localSpeechRecognitionClient:speechRecordingDidFail:
localSpeechRecognitionClient:didRecognizePartialResult:rawPartialResult:
localSpeechRecognitionClient:didProcessAudioDuration:
localSpeechRecognitionClient:didFinishRecognition:
localSpeechRecognitionClient:speechRecognitionDidFail:
_initWithRequest:queue:languageCode:taskHint:
state
finish
cancel
error
peakPower
averagePower
handleSpeechRecognitionDidFailWithError:
_taskHint
isFinishing
isCancelled
requestIdentifier
_dictationConnection
_sflsrClient
_externalQueue
_languageCode
_request
_internalQueue
_error
_completed
_running
_finishing
_cancelled
_requestIdentifier
Tq,R,N,V_taskHint
T@"NSString",R,C,N,V_requestIdentifier
Tq,R,N
finishing
TB,R,N,GisFinishing,V_finishing
cancelled
TB,R,N,GisCancelled,V_cancelled
T@"NSError",R,C,N
_initWithRequest:queue:languageCode:taskHint:resultHandler:
_fireResultHandlerWithResult:error:
_finalizeResultHandler
_resultHandler
_hasFiredFinalResult
speechRecognitionTaskWasCancelled:
speechRecognitionTask:didHypothesizeTranscription:
speechRecognitionTask:didReceiveSearchResults:recognizedText:stable:final:
speechRecognitionTask:didFinishRecognition:
speechRecognitionTask:didProcessAudioDuration:
speechRecognitionTask:didFinishSuccessfully:
speechRecognitionTaskFinishedReadingAudio:
_initWithRequest:queue:languageCode:taskHint:delegate:
_handleSpeechRecordingDidCancel
_handleDidRecognizePartialResult:
_handleDidFinishRecognition:
_handleDidProcessAudioDuration:
_tellDelegateDidFinishSuccessfully:
_delegate
_recognitionResultToReportAfterFinalSearchResults
_selfReference
_waitForVoiceSearchResult
_hasSentRealSearchResults
applicationName
setApplicationName:
applicationVersion
setApplicationVersion:
inlineItemList
setInlineItemList:
setRequestIdentifier:
taskIdentifier
setTaskIdentifier:
language
setLanguage:
task
setTask:
narrowband
setNarrowband:
recognitionOverrides
setRecognitionOverrides:
modelOverrideURL
setModelOverrideURL:
maximumRecognitionDuration
setMaximumRecognitionDuration:
dynamicLanguageModel
setDynamicLanguageModel:
dynamicVocabulary
setDynamicVocabulary:
detectMultipleUtterances
setDetectMultipleUtterances:
onDeviceOnly
setOnDeviceOnly:
enableAutoPunctuation
setEnableAutoPunctuation:
_narrowband
_detectMultipleUtterances
_onDeviceOnly
_enableAutoPunctuation
_applicationName
_applicationVersion
_inlineItemList
_taskIdentifier
_language
_task
_recognitionOverrides
_modelOverrideURL
_maximumRecognitionDuration
_dynamicLanguageModel
_dynamicVocabulary
T@"NSString",C,N,V_applicationName
T@"NSString",C,N,V_applicationVersion
T@"NSArray",C,N,V_inlineItemList
T@"NSString",C,N,V_requestIdentifier
T@"NSString",C,N,V_taskIdentifier
T@"NSString",C,N,V_language
T@"NSString",C,N,V_task
TB,N,V_narrowband
T@"NSDictionary",C,N,V_recognitionOverrides
T@"NSURL",C,N,V_modelOverrideURL
Td,N,V_maximumRecognitionDuration
T@"NSURL",C,N,V_dynamicLanguageModel
T@"NSURL",C,N,V_dynamicVocabulary
TB,N,V_detectMultipleUtterances
TB,N,V_onDeviceOnly
TB,N,V_enableAutoPunctuation
startRecordedAudioDictationWithParameters:
cancelSpeech
addAudioPacket:
preloadRecognizerForLanguage:task:recognitionOverrides:modelOverrideURL:completion:
installedLanguagesWithCompletion:
initializeWithSandboxExtensions:
logCoreAnalyticsEvent:withAnalytics:
downloadAssetsForLanguage:urgent:forceUpgrade:detailedProgress:
fetchAssetsForLanguage:completion:
configParametersForVoicemailWithLanguage:completion:
purgeAssetsForLanguage:completion:
setAssetsAsProvisional
promoteAssets
initializeLmWithLocale:completion:
initializeLmWithAssetPath:completion:
addSentenceToNgramCounts:
addSentenceToNgramCounts:completion:
addProns:forWord:completion:
oovWordsAndFrequenciesWithCompletion:
trainFromPlainTextAndWriteToDirectory:sandboxExtension:completion:
generateNgramCountsSerializeDataWithCompletion:
deserializeNgramCountsData:completion:
lmeThresholdWithCompletion:
metricsWithCompletion:
wakeUpWithCompletion:
trainAppLmFromNgramsSerializedData:customPronsData:language:writeToDirectory:sandboxExtension:completion:
createPhraseCountsArtifactWithIdentifier:rawPhraseCountsPath:customPronunciationsPath:writeToDirectory:sandboxExtensions:completion:
createNgramCountsArtifactFromPhraseCountArtifact:writeToDirectory:sandboxExtensions:completion:
trainAppLmFromNgramCountsArtifact:language:writeToDirectory:sandboxExtensions:completion:
localSpeechRecognitionDidRecognizePartialResult:rawPartialResult:
localSpeechRecognitionDidProcessAudioDuration:
localSpeechRecognitionDidFinishRecognition:
localSpeechRecognitionDidFail:
localSpeechRecognitionDidSucceed
localSpeechRecognitionDidDownloadAssetsWithProgress:isStalled:
localSpeechRecognitionDidFinishDownloadingAssets:error:
dealloc
supportedLocales
trainAppLmFromNgramsSerializedDataFile:customPronsFile:language:writeToDirectory:modelOverride:completion:
trainAppLmFromNgramCountsArtifact:language:writeToDirectory:modelOverride:completion:
initWithLocale:
initWithAssetPath:
addSentence:
addSentences:
addOovsFromSentence:
outOfVocabularyWords
outOfVocabularyWordsAndFrequencies
trainFromPlainTextAndWriteToDirectory:completion:
trainFromPlainTextAndWriteToDirectory:
generateNgramsSerializeDataAndWriteToFile:
deserializeModelData:
addPronsFromFile:
addProns:forWord:
lmeThreshold
metrics
createPhraseCountsArtifactWithIdentifier:rawPhraseCountsPath:customPronunciationsPath:writeDirectory:
createNgramCountsArtifactFromPhraseCountArtifact:writeDirectory:
createNgramCountsArtifactWithIdentifier:rawPhraseCountsPath:customPronunciationsPath:writeDirectory:
locale
_lsrClient
_locale
T@"NSLocale",R,C,N,V_locale
T@"NSArray",R,C,N
T@"NSDictionary",R,C,N
initWithBinarySampleRepresentation:metadata:timestamp:
binarySampleRepresentation
_initWithBestTranscription:final:
_initWithBestTranscription:rawTranscription:final:
_initWithBestTranscription:rawTranscription:final:speechRecognitionMetadata:
expandTranscription:
transcriptions
rawTranscriptions
bestTranscription
isFinal
speechRecognitionMetadata
rawTranscription
_transcriptions
_rawTranscriptions
_final
_bestTranscription
_speechRecognitionMetadata
_rawTranscription
T@"SFTranscription",R,C,N,V_rawTranscription
T@"SFTranscription",R,C,N,V_bestTranscription
final
TB,R,N,GisFinal,V_final
T@"SFSpeechRecognitionMetadata",R,N,V_speechRecognitionMetadata
speechRecognizer:availabilityDidChange:
authorizationStatus
requestAuthorization:
_fetchSupportedForcedOfflineLocalesWithCompletion:
callObserver:callChanged:
isAvailable
supportsOnDeviceRecognition
_isAvailableForForcedOfflineRecognition
_requestOfflineDictationSupportWithCompletion:
_prepareToRecognizeWithTaskHint:completion:
_isInternalTaskHint:
recognitionTaskWithRequest:resultHandler:
recognitionTaskWithRequest:delegate:
setQueue:
_informDelegateOfAvailabilityChange
_informDelegateOfPreferencesChange
setSupportsOnDeviceRecognition:
delegate
setDelegate:
defaultTaskHint
setDefaultTaskHint:
queue
_callObserver
_facetimeObserver
_foregroundObserver
_preferencesObserver
_supportsOnDeviceRecognition
_defaultTaskHint
_queue
_availableForForcedOfflineRecognition
TB,R,N,G_isAvailableForForcedOfflineRecognition
available
TB,R,N,GisAvailable
TB,N,V_supportsOnDeviceRecognition
T@"<SFSpeechRecognizerDelegate>",W,N,V_delegate
Tq,N,V_defaultTaskHint
T@"NSOperationQueue",&,N,V_queue
_initWithSubstring:range:timestamp:duration:confidence:alternativeSubstrings:alternativeConfidences:phoneSequence:ipaPhoneSequence:voiceAnalytics:
substring
substringRange
timestamp
duration
confidence
alternativeSubstrings
voiceAnalytics
alternativeConfidences
phoneSequence
ipaPhoneSequence
_confidence
_substring
_timestamp
_duration
_alternativeSubstrings
_voiceAnalytics
_alternativeConfidences
_phoneSequence
_ipaPhoneSequence
_substringRange
T@"NSArray",R,N,V_alternativeConfidences
T@"NSString",R,N,V_phoneSequence
T@"NSString",R,N,V_ipaPhoneSequence
T@"NSString",R,C,N,V_substring
T{_NSRange=QQ},R,N,V_substringRange
Td,R,N,V_timestamp
Td,R,N,V_duration
Tf,R,N,V_confidence
T@"NSArray",R,N,V_alternativeSubstrings
T@"SFVoiceAnalytics",R,N,V_voiceAnalytics
initWithDelegate:
invalidate
_serviceProxyWithErrorHandler:
_asynchronousServiceProxyWithErrorHandler:
installedLanguagesWithSynchronousConnection:completion:
downloadAssetsForLanguage:urgent:forceUpgrade:progress:completionHandler:
downloadAssetsForLanguage:urgent:forceUpgrade:detailedProgress:completionHandler:
_lsrConnection
_downloadProgress
_downloadCompletion
_recognitionActive
T@"<SFLocalSpeechRecognitionDelegate>",R,W,N,V_delegate
supportedLocalesWithCompletion:
taskNameFromTaskHint:
transcriptionFromSpeechPhrases:afAudioAnalytics:utteranceStart:
recognitionMetadataFromSpeechPhrases:afAudioAnalytics:utteranceStart:
transcriptionsWithTokens:
recognizedResultFromPackage:
isSpeechXPCEnabled
canAccessPathAt:methodName:error:
issueReadWriteSandboxExtensionForDirectoryPath:error:
issueReadSandboxExtensionForFilePath:error:
isCacheDirInternal
lsrCacheDirPath
searchTypes
setSearchTypes:
headerFields
setHeaderFields:
queryParameters
setQueryParameters:
_searchTypes
_headerFields
_queryParameters
Tq,N,V_searchTypes
T@"NSDictionary",C,N,V_headerFields
T@"NSDictionary",C,N,V_queryParameters
_setSearchRequests:
_searchRequests
automaticallyEndpoint
_powerMeteringAvailable
setRequiresOnDeviceRecognition:
requiresOnDeviceRecognition
_startedConnectionWithLanguageCode:delegate:taskHint:requestIdentifier:
_startedLocalConnectionWithLanguageCode:delegate:taskHint:requestIdentifier:taskIdentifier:
_dictationOptionsWithTaskHint:requestIdentifier:
_requestParametersWithTaskHint:requestIdentifier:taskIdentifier:narrowband:language:
_sandboxExtensionsWithError:
_speechRequestOptions
_setMaximumRecognitionDuration:
_forceOfflineRecognition
_setForceOfflineRecognition:
_searchRequest
_setSearchRequest:
_voiceTriggerEventInfo
_setVoiceTriggerEventInfo:
_setRecognitionOverrides:
_setModelOverrideURL:
_afDictationRequestParams
_setAFDictationRequestParams:
_setDynamicLanguageModel:
_setDynamicVocabulary:
_forceUseSiriProcess
_setForceUseSiriProcess:
taskHint
setTaskHint:
shouldReportPartialResults
setShouldReportPartialResults:
contextualStrings
setContextualStrings:
interactionIdentifier
setInteractionIdentifier:
addsPunctuation
setAddsPunctuation:
_shouldReportPartialResults
_addsPunctuation
_contextualStrings
_interactionIdentifier
T@"NSDictionary",&,N,G_afDictationRequestParams,S_setAFDictationRequestParams:,V_afDictationRequestParams
T@"_SFSearchRequest",&,N,G_searchRequest,S_setSearchRequest:,V_searchRequest
TB,N,G_forceOfflineRecognition,S_setForceOfflineRecognition:,V_forceOfflineRecognition
T@"NSDictionary",&,N,G_voiceTriggerEventInfo,S_setVoiceTriggerEventInfo:,V_voiceTriggerEventInfo
Td,N,G_maximumRecognitionDuration,S_setMaximumRecognitionDuration:,V_maximumRecognitionDuration
T@"NSDictionary",&,N,G_recognitionOverrides,S_setRecognitionOverrides:,V_recognitionOverrides
T@"NSURL",&,N,G_modelOverrideURL,S_setModelOverrideURL:,V_modelOverrideURL
T@"NSURL",&,N,G_dynamicLanguageModel,S_setDynamicLanguageModel:,V_dynamicLanguageModel
T@"NSURL",&,N,G_dynamicVocabulary,S_setDynamicVocabulary:,V_dynamicVocabulary
TB,N,G_forceUseSiriProcess,S_setForceUseSiriProcess:,V_forceUseSiriProcess
T@"NSString",&,N,V_taskIdentifier
Tq,N,V_taskHint
TB,N,V_shouldReportPartialResults
T@"NSArray",C,N,V_contextualStrings
T@"NSString",C,N,V_interactionIdentifier
TB,N
TB,N,V_addsPunctuation
initWithURL:
_handlePreRecordedAudioWithAsset:audioTracks:narrowband:addSpeechDataBlock:stopSpeechBlock:cancelSpeechWithErrorBlock:
_URL
T@"NSURL",R,C,N,V_URL
nativeAudioFormat
_handleAudioBuffersWithDelegate:
_appendAudioPCMBuffer:
appendAudioPCMBuffer:
_drainAndClearAudioConverter
_convertAndFeedPCMBuffer:
appendAudioSampleBuffer:
_endAudio
endAudio
_bufferDelegate
_queuedBuffers
_converter
_audioEnded
T@"AVAudioFormat",R,N
_initWithSegments:formattedString:speakingRate:averagePauseDuration:
formattedString
segments
speakingRate
averagePauseDuration
_formattedString
_segments
_speakingRate
_averagePauseDuration
T@"NSString",R,C,N,V_formattedString
T@"NSArray",R,C,N,V_segments
Td,R,N,V_speakingRate
Td,R,N,V_averagePauseDuration
_sf_path
_sf_quasarModelPath
_sf_isInstalled
_assetQueryForLanguage:
fetchMAAssetPathForInstalledLanguage:
installedLanguages
fetchAssetsForLanguage:progress:completion:
fetchAssetsForLanguage:urgent:progress:completion:
fetchAssetsForLanguage:urgent:forceUpgrade:progress:completion:
fetchAssetsForLanguage:urgent:forceUpgrade:detailedProgress:completion:
assetPathForLanguage:
configParametersForVoicemailWithLanguage:
purgeAssetsForLanguage:error:
_initWithSpeechStartTimestamp:speechDuration:voiceAnalytics:speakingRate:averagePauseDuration:
speechStartTimestamp
speechDuration
_speechStartTimestamp
_speechDuration
Td,R,N,V_speechStartTimestamp
Td,R,N,V_speechDuration
B16@0:8
@24@0:8^{_NSZone=}16
v24@0:8@16
@24@0:8@16
v24@0:8@"NSCoder"16
@24@0:8@"NSCoder"16
@32@0:8@16d24
@16@0:8
d16@0:8
v16@0:8
@"NSArray"
@48@0:8@16@24@32@40
@"SFAcousticFeature"
q16@0:8
@"NSHTTPURLResponse"
@"NSData"
B24@0:8@16
#16@0:8
@24@0:8:16
@32@0:8:16@24
@40@0:8:16@24@32
B24@0:8#16
B24@0:8:16
Vv16@0:8
Q16@0:8
^{_NSZone=}16@0:8
B24@0:8@"Protocol"16
@"NSString"16@0:8
v32@0:8@16@24
v40@0:8@16@24@32
v44@0:8@16@24@32B40
v48@0:8@16@24@32@40
v56@0:8@16@24@32@40Q48
v32@0:8@16d24
v48@0:8@16@24@32B40B44
v24@0:8@"AFDictationConnection"16
v32@0:8@"AFDictationConnection"16@"AFDictationOptions"24
v32@0:8@"AFDictationConnection"16@"NSString"24
v32@0:8@"AFDictationConnection"16@"NSError"24
v40@0:8@"AFDictationConnection"16@"NSString"24@"NSDictionary"32
v44@0:8@"AFDictationConnection"16@"NSString"24@"NSDictionary"32B40
v32@0:8@"AFDictationConnection"16@"SASMultilingualSpeechRecognized"24
v48@0:8@"AFDictationConnection"16@"NSArray"24@"NSString"32@40
v56@0:8@"AFDictationConnection"16@"NSArray"24@"NSString"32@40Q48
v40@0:8@"AFDictationConnection"16@"NSArray"24@"NSString"32
v48@0:8@"AFDictationConnection"16@"NSArray"24@"AFDictationNLUResult"32@"NSString"40
v32@0:8@"AFDictationConnection"16@"SASSpeechPartialResult"24
v32@0:8@"AFDictationConnection"16d24
v40@0:8@"AFDictationConnection"16@"NSFileHandle"24@"NSError"32
v48@0:8@"AFDictationConnection"16@"NSArray"24@"NSString"32B40B44
v32@0:8@"AFDictationConnection"16@"AFSpeechPackage"24
v40@0:8@"AFDictationConnection"16@"AFSpeechPackage"24@"AFDictationNLUResult"32
v24@0:8@"NSData"16
v24@0:8@"SFLocalSpeechRecognitionClient"16
v32@0:8@"SFLocalSpeechRecognitionClient"16@"NSError"24
v40@0:8@"SFLocalSpeechRecognitionClient"16@"SFTranscription"24@"SFTranscription"32
v32@0:8@"SFLocalSpeechRecognitionClient"16d24
v32@0:8@"SFLocalSpeechRecognitionClient"16@"SFSpeechRecognitionResult"24
@48@0:8@16@24@32q40
f16@0:8
@"AFDictationConnection"
@"SFLocalSpeechRecognitionClient"
@"NSOperationQueue"
@"NSString"
@"SFSpeechRecognitionRequest"
@"NSObject<OS_dispatch_queue>"
@"NSError"
@56@0:8@16@24@32q40@?48
@56@0:8@16@24@32q40@48
v24@0:8d16
v20@0:8B16
@"<_SFSpeechRecognitionTaskDelegatePrivate>"
@"SFSpeechRecognitionResult"
@"_SFSpeechRecognitionDelegateTask"
@"NSDictionary"
@"NSURL"
Vv24@0:8@16
Vv56@0:8@16@24@32@40@?48
Vv24@0:8@?16
Vv36@0:8@16B24B28B32
Vv32@0:8@16@?24
Vv40@0:8@16@24@?32
Vv64@0:8@16@24@32@40@48@?56
Vv48@0:8@16@24@32@?40
Vv24@0:8@"SFRequestParameters"16
Vv24@0:8@"NSData"16
Vv56@0:8@"NSString"16@"NSString"24@"NSDictionary"32@"NSURL"40@?<v@?@"NSError">48
Vv24@0:8@?<v@?@"NSSet">16
v24@0:8@"NSArray"16
v32@0:8@"NSString"16@"NSDictionary"24
Vv36@0:8@"NSString"16B24B28B32
Vv32@0:8@"NSString"16@?<v@?@"NSString"@"NSError">24
Vv32@0:8@"NSString"16@?<v@?@"NSDictionary">24
Vv32@0:8@"NSString"16@?<v@?@"NSError">24
Vv32@0:8@"NSLocale"16@?<v@?@"NSError">24
Vv24@0:8@"NSString"16
Vv32@0:8@"NSString"16@?<v@?@"NSArray">24
Vv40@0:8@"NSSet"16@"NSString"24@?<v@?B>32
Vv24@0:8@?<v@?@"NSDictionary">16
Vv40@0:8@"NSURL"16@"NSString"24@?<v@?@"NSURL"@"NSURL">32
Vv24@0:8@?<v@?@"NSData">16
Vv32@0:8@"NSData"16@?<v@?@"NSDictionary">24
Vv24@0:8@?<v@?q>16
Vv24@0:8@?<v@?>16
Vv64@0:8@"NSData"16@"NSData"24@"NSString"32@"NSURL"40@"NSString"48@?<v@?@"NSURL"@"NSURL">56
Vv64@0:8@"NSString"16@"NSURL"24@"NSURL"32@"NSURL"40@"NSArray"48@?<v@?@"NSURL">56
Vv48@0:8@"NSURL"16@"NSURL"24@"NSArray"32@?<v@?@"NSURL">40
Vv56@0:8@"NSURL"16@"NSString"24@"NSURL"32@"NSArray"40@?<v@?@"NSURL"@"NSURL">48
Vv32@0:8@16@24
Vv24@0:8d16
Vv28@0:8Q16B24
Vv32@0:8@"SFTranscription"16@"SFTranscription"24
Vv24@0:8@"SFSpeechRecognitionResult"16
Vv24@0:8@"NSError"16
Vv32@0:8@"NSString"16@"NSError"24
v64@0:8@16@24@32@40@48@?56
v56@0:8@16@24@32@40@?48
v32@0:8@16@?24
B32@0:8@16@24
@32@0:8@16@24
@"NSLocale"
@40@0:8@16@24d32
@40@0:8@"NSData"16@"NSDictionary"24d32
@"NSData"16@0:8
@28@0:8@16B24
@36@0:8@16@24B32
@44@0:8@16@24B32@36
@"SFTranscription"
@"SFSpeechRecognitionMetadata"
v24@0:8@?16
v32@0:8@"CXCallObserver"16@"CXCall"24
v32@0:8q16@?24
B24@0:8q16
@32@0:8@16@?24
v24@0:8q16
@"CXCallObserver"
@"<NSObject>"
@"<SFSpeechRecognizerDelegate>"
@100@0:8@16{_NSRange=QQ}24d40d48f56@60@68@76@84@92
{_NSRange=QQ}16@0:8
@"SFVoiceAnalytics"
{_NSRange="location"Q"length"Q}
@24@0:8@?16
v28@0:8B16@?20
v48@0:8@16B24B28@?32@?40
v40@0:8@16@24@?32
v48@0:8@16@24@32@?40
@"NSXPCConnection"
@"<SFLocalSpeechRecognitionDelegate>"
@24@0:8q16
B40@0:8@16@24^@32
@32@0:8@16^@24
@48@0:8@16@24q32@40
@56@0:8@16@24q32@40@48
@32@0:8q16@24
@52@0:8q16@24@32B40@44
@24@0:8^@16
@"_SFSearchRequest"
v60@0:8@16@24B32@?36@?44@?52
v24@0:8^{opaqueCMSampleBuffer=}16
@"<SFSpeechRecognitionBufferDelegate>"
@"NSMutableArray"
@"AVAudioConverter"
@48@0:8@16@24d32d40
v40@0:8@16@?24@?32
v44@0:8@16B24@?28@?36
v32@0:8@16^@24
@56@0:8d16d24@32d40d48
mcpl
