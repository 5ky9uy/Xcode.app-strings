@(#)PROGRAM:Speech  PROJECT:SpeechRecognition-1
MbP?
@mcpl
_acousticFeaturePerFrame
_frameDuration
, featureValues=%@, frameDuration=%f
_jitter
_shimmer
_pitch
_voicing
, jitter=%@, shimmer=%@, pitch=%@, voicing=%@
v32@?0@"NSString"8@"NSString"16^B24
com.apple.Speech.Task.Internal
v8@?0
User denied access to speech recognition
-[SFSpeechRecognitionTask handleSpeechRecognitionDidFailWithError:]_block_invoke
-[SFSpeechRecognitionTask dictationConnection:speechRecordingDidFail:]_block_invoke
-[SFSpeechRecognitionTask localSpeechRecognitionClient:speechRecordingDidFail:]_block_invoke
v24@?0@"SFSpeechRecognitionResult"8@"NSError"16
textNotEmpty
isFinal
, applicationName=%@, applicationVersion=%@, inlineItemList=%@, requestIdentifier=%@ task=%@ language=%@ narrowband=%d recognitionOverrides=%@ modelOverrideURL=%@ maximumRecognitionDuration=%f dynamicLanguageModel=%@ dynamicVocabulary=%@ detectMultipleUtterances=%d onDeviceOnly=%d enableAutoPunctuation=%d
SFRequestParameters::applicationName
SFRequestParameters::applicationVersion
SFRequestParameters::inlineItemList
SFRequestParameters::requestIdentifier
SFRequestParameters::taskIdentifier
SFRequestParameters::task
SFRequestParameters::language
SFRequestParameters::narrowband
SFRequestParameters::recognitionOverrides
SFRequestParameters::modelOverrideURL
SFRequestParameters::maximumRecognitionDuration
SFRequestParameters::dynamicLanguageModel
SFRequestParameters::dynamicVocabulary
SFRequestParameters::detectMultipleUtterances
SFRequestParameters::onDeviceOnly
SFRequestParameters::enableAutoPunctuation
v24@?0@"NSSet"8@"NSSet"16
Cannot make recognizer for %@. Supported locale identifiers are %@
%@-%@
v16@?0@"NSError"8
v16@?0@"NSArray"8
v16@?0@"NSDictionary"8
v24@?0@"NSURL"8@"NSURL"16
Write failed error:%@
Write successfull
Failed to generate ngram counts
There was a problem writing data to the file! Error:  %@
Ngram counts and OOV list serialized and saved to: %@
v16@?0@"NSData"8
grapheme
phoneme
Invalid prons: %@ for word: %@
v12@?0B8
v16@?0q8
Could not find ngrams serialized data file: %@ 
Failed to read the file: %@
Successfully read the file: %@
Failed to read the pronunciation file: %@
Failed to create directory error:%@
Created directory: %@
Failed to initialize language model: %@
Could not issue sandbox extension for path:%@
No write directory provided
No rawPhraseCountsPath provided
No customPronunciationsPath provided but will continue..
v16@?0@"NSURL"8
No phraseCountArtifact provided
_bestTranscription
_rawTranscription
_final
_speechRecognitionMetadata
 final=%d, bestTranscription=%@, speechRecognitionMetadata=%@
v32@?0@"NSString"8Q16^B24
v32@?0@"SFTranscriptionSegment"8Q16^B24
v12@?0C8
v16@?0@"NSNotification"8
Required assets are not available for Locale:%@
v24@?0@"NSString"8@"NSError"16
v16@?0@"NSSet"8
Result handler must be non-null
%@ queue must not be nil
, substringRange=%@, timestamp=%@, duration=%@, confidence=%@, substring=%@, alternativeSubstrings=%@, phoneSequence=%@, ipaPhoneSequence=%@, voiceAnalytics=%@
_substring
_substringRange.location
_substringRange.length
_timestamp
_duration
_confidence
_alternativeSubstrings
_alternativeConfidences
_phoneSequence
_ipaPhoneSequence
_voiceAnalytics
SFTranscriptionSegment.m
com.apple.speechapi.RequestStarted
com.apple.speechapi.StopSpeech
com.apple.speechapi.CancelSpeech
com.apple.speechapi.ResultPackage
com.apple.speechapi.RequestCompleted
com.apple.speechapi.RequestFailed
com.apple.speechapi.RequestPerformance
com.apple.speechapi.AssetRequested
com.apple.speechapi.AssetRequestCompleted
unknown.%ld
com.apple.speech.localspeechrecognition
SFLocalSpeechRecognitionClient
v20@?0Q8B16
+N9mZUAHooNvMiQnjeTJ8g
InternalBuild
hi-IN
hi-IN-translit
Translit
speakingRate
averagePauseDuration
jitter
shimmer
pitch
voicing
Dictation
VoiceMail
Captioning
dumpedTaskIdentifier
success
language
task
narrowband
appname
ondevice
modelVersion
audioDuration
ttfw
responseTime
modelLoadTime
recognitionDuration
jitLmeProcessingTime
asrInitializationTime
cpuRtf
errorCode
errorDomain
v32@?0@"AFSpeechToken"8Q16^B24
v32@?0@"AFSpeechInterpretation"8Q16^B24
Siri
trial_dictation_asset_delivery
Failed to access path: %@ method:%@
+[SFUtilities issueReadWriteSandboxExtensionForDirectoryPath:error:]
com.apple.app-sandbox.read-write
+[SFUtilities issueReadSandboxExtensionForFilePath:error:]
com.apple.app-sandbox.read
/var/mobile/Library/Caches
/private/var/mobile/Library/Caches
+[SFUtilities lsrCacheDirPath]
SFSpeechPreecordedRequest
Use -[SFSpeechURLRecognitionRequest initWithURL:]
Could not add output for %@
B8@?0
none
com.apple.SFSpeechAudioBufferRecognitionRequest
SFSpeechRecognitionRequest.m
%@ cannot be re-used
Invalid audio format
@"AVAudioBuffer"20@?0I8^q12
Could not drain converter %@
Could not run audio converter %@
CMBlockBufferCopyDataBytes could not copy data: %d
_speakingRate
_avgPauseDuration
, formattedString=%@, segments=%@, speakingRate=%f, averagePauseDuration=%f
_segments
_formattedString
Language
Failed to set purgeability for assets.
CESRTrialAssetManager
Class getCESRTrialAssetManagerClass(void)_block_invoke
SFSpeechAssetManager.m
Unable to find class %s
void *CoreEmbeddedSpeechRecognitionLibrary(void)
LSRConnection
SpeechFramework
_averagePauseDuration
_speechStartTimestamp
_speechDuration
, speakingRate=%@, averagePauseDuration=%@, speechStartTimestamp=%@, speechDuration=%@, voiceAnalytics=%@
%s Ignoring subsequent recongition error: %@
%s Ignoring subsequent recording error: %@
%s Ignoring subsequent local speech recording error: %@
%@ Interrupted
%@ Invalidated
Dealloc-ing
Local speech recognizer restarted while already recognizing
Received an error while accessing %@ service: %@
%s %@: path=%@
%s %@: sandbox_extension_issue_file() returned NULL. path=%@
%s %@: Inaccessible file (%@) : error=%@
%s %@: sandboxExtension=%@
%s Failed to create directory %@
%s Write successful %@
softlink:r:path:/System/Library/PrivateFrameworks/CoreEmbeddedSpeechRecognition.framework/CoreEmbeddedSpeechRecognition
SFAcousticFeature
NSCopying
NSSecureCoding
NSCoding
SFVoiceAnalytics
_SFSearchResult
SFSpeechRecognitionTask
AFDictationDelegate
NSObject
SFSpeechRecognitionBufferDelegate
SFLocalSpeechRecognitionDelegate
_SFSpeechRecognitionBlockTask
_SFSpeechRecognitionDelegateTask
SFRequestParameters
SFLSRProtocol
SFLSRDelegate
SFSpeechLanguageModel
SFSpeechRecognitionResult
SRSampling
SFSpeechRecognizer
CXCallObserverDelegate
SFTranscriptionSegment
SFLocalSpeechRecognitionClient
SFUtilities
_SFSearchRequest
SFSpeechRecognitionRequest
SFSpeechURLRecognitionRequest
SFSpeechAudioBufferRecognitionRequest
SFTranscription
SFSpeechAssetManager
SFSpeechRecognitionMetadata
T@"SFSpeechRecognitionMetadata",R,N,V_speechRecognitionMetadata
JSONObjectWithData:options:error:
TB,R,N,GisCancelled,V_cancelled
T@"<SFLocalSpeechRecognitionDelegate>",R,W,N,V_delegate
Tq,N,V_taskHint
T@"AVAudioFormat",R,N
_bufferDelegate
T@"NSArray",C,N,V_inlineItemList
_jitter
T@"NSArray",R,C,N,V_acousticFeatureValuePerFrame
_setMaximumRecognitionDuration:
T@"NSArray",R,N,V_alternativeConfidences
_substringRange
T@"NSData",R,N,V_data
_transcriptions
T@"NSDictionary",&,N,G_voiceTriggerEventInfo,S_setVoiceTriggerEventInfo:,V_voiceTriggerEventInfo
addAudioPacket:
T@"NSDictionary",C,N,V_queryParameters
allKeys
T@"NSDictionary",R,C,N
confidenceScore
T@"NSHTTPURLResponse",R,N,V_response
dealloc
T@"NSOperationQueue",&,N,V_queue
dictationConnection:didRecognizeTokens:nluResult:languageModel:
T@"NSString",C,N,V_applicationName
fetchAssetsForLanguage:urgent:forceUpgrade:progress:completion:
T@"NSString",C,N,V_interactionIdentifier
getLocalFileUrl
T@"NSString",C,N,V_requestIdentifier
initWithFormat:
T@"NSString",C,N,V_taskIdentifier
initWithLocale:
T@"NSString",R,C,N,V_formattedString
isFinal
T@"NSString",R,C,N,V_substring
lexemes
T@"NSString",R,N,V_phoneSequence
metrics
T@"NSURL",&,N,G_dynamicVocabulary,S_setDynamicVocabulary:,V_dynamicVocabulary
phrases
T@"NSURL",C,N,V_dynamicLanguageModel
release
T@"NSURL",C,N,V_modelOverrideURL
results
T@"SFAcousticFeature",R,C,N,V_jitter
setFrameLength:
T@"SFAcousticFeature",R,C,N,V_shimmer
setSearchTypes:
T@"SFTranscription",R,C,N,V_bestTranscription
shimmer
T@"SFVoiceAnalytics",R,N,V_voiceAnalytics
supportedLocalesWithCompletion:
TB,N
voicing
.cxx_destruct
TB,N,V_detectMultipleUtterances
T#,R
TB,R,N,GisFinishing,V_finishing
T@"<SFSpeechRecognizerDelegate>",W,N,V_delegate
T@"NSArray",C,N,V_contextualStrings
_inlineItemList
T@"NSArray",R,C,N
_locale
T@"NSArray",R,C,N,V_segments
_speechDuration
T@"NSArray",R,N,V_alternativeSubstrings
_taskIdentifier
T@"NSDictionary",&,N,G_recognitionOverrides,S_setRecognitionOverrides:,V_recognitionOverrides
_voiceAnalytics
T@"NSDictionary",C,N,V_headerFields
addsPunctuation
T@"NSDictionary",C,N,V_recognitionOverrides
applicationName
T@"NSError",R,C,N
containsObject:
T@"NSLocale",R,C,N,V_locale
defaultTaskHint
T@"NSString",&,N,V_taskIdentifier
downloadAssetsForLanguage:urgent:forceUpgrade:detailedProgress:
T@"NSString",C,N,V_applicationVersion
formattedString
T@"NSString",C,N,V_language
headers
T@"NSString",C,N,V_task
initWithLength:
T@"NSString",R,C
interpretations
T@"NSString",R,C,N,V_requestIdentifier
isProxy
T@"NSString",R,N,V_ipaPhoneSequence
lsrCacheDirPath
T@"NSURL",&,N,G_dynamicLanguageModel,S_setDynamicLanguageModel:,V_dynamicLanguageModel
numberWithBool:
T@"NSURL",&,N,G_modelOverrideURL,S_setModelOverrideURL:,V_modelOverrideURL
queryParameters
T@"NSURL",C,N,V_dynamicVocabulary
removeObserver:
T@"NSURL",R,C,N,V_URL
T@"SFAcousticFeature",R,C,N,V_pitch
setRequiresOnDeviceRecognition:
T@"SFAcousticFeature",R,C,N,V_voicing
setSupportsOnDeviceRecognition:
T@"SFTranscription",R,C,N,V_rawTranscription
stringByAppendingPathComponent:
T@"_SFSearchRequest",&,N,G_searchRequest,S_setSearchRequest:,V_searchRequest
transcriptionFromSpeechPhrases:afAudioAnalytics:utteranceStart:
TB,N,G_forceOfflineRecognition,S_setForceOfflineRecognition:,V_forceOfflineRecognition
TB,N,G_forceUseSiriProcess,S_setForceUseSiriProcess:,V_forceUseSiriProcess
TB,N,V_addsPunctuation
TB,N,V_enableAutoPunctuation
TB,N,V_narrowband
TB,N,V_onDeviceOnly
TB,N,V_shouldReportPartialResults
TB,N,V_supportsOnDeviceRecognition
TB,R
TB,R,N,G_isAvailableForForcedOfflineRecognition
TB,R,N,GisAvailable
TB,R,N,GisFinal,V_final
TQ,R
Td,N,G_maximumRecognitionDuration,S_setMaximumRecognitionDuration:,V_maximumRecognitionDuration
Td,N,V_maximumRecognitionDuration
Td,R,N,V_averagePauseDuration
Td,R,N,V_duration
Td,R,N,V_frameDuration
Td,R,N,V_speakingRate
Td,R,N,V_speechDuration
Td,R,N,V_speechStartTimestamp
Td,R,N,V_timestamp
Tf,R,N,V_confidence
Tq,N,V_defaultTaskHint
Tq,N,V_searchTypes
Tq,R,N
Tq,R,N,V_searchType
Tq,R,N,V_taskHint
T{_NSRange=QQ},R,N,V_substringRange
UTF8String
UUID
UUIDString
_URL
_acousticFeatureValuePerFrame
_addsPunctuation
_afDictationRequestParams
_alternativeConfidences
_alternativeSubstrings
_appendAudioPCMBuffer:
_applicationName
_applicationVersion
_assetQueryForLanguage:
_audioEnded
_availableForForcedOfflineRecognition
_averagePauseDuration
_bestTranscription
_callObserver
_cancelled
_completed
_confidence
_contextualStrings
_converter
_data
_defaultTaskHint
_delegate
_detectMultipleUtterances
_dictationConnection
_downloadCompletion
_downloadProgress
_duration
_dynamicLanguageModel
_dynamicVocabulary
_enableAutoPunctuation
_error
_externalQueue
_facetimeObserver
_fetchSupportedForcedOfflineLocalesWithCompletion:
_final
_finishing
_forceOfflineRecognition
_forceUseSiriProcess
_foregroundObserver
_formattedString
_frameDuration
_hasFiredFinalResult
_hasSentRealSearchResults
_headerFields
_initWithAcousticFeatureValue:frameDuration:
_initWithBestTranscription:rawTranscription:final:speechRecognitionMetadata:
_initWithJitter:shimmer:pitch:voicing:
_initWithRequest:queue:languageCode:taskHint:
_initWithRequest:queue:languageCode:taskHint:delegate:
_initWithRequest:queue:languageCode:taskHint:resultHandler:
_initWithSegments:formattedString:speakingRate:averagePauseDuration:
_initWithSpeechStartTimestamp:speechDuration:voiceAnalytics:speakingRate:averagePauseDuration:
_initWithSubstring:range:timestamp:duration:confidence:alternativeSubstrings:alternativeConfidences:phoneSequence:ipaPhoneSequence:voiceAnalytics:
_interactionIdentifier
_internalQueue
_ipaPhoneSequence
_isAvailableForForcedOfflineRecognition
_language
_languageCode
_lsrClient
_lsrConnection
_maximumRecognitionDuration
_modelOverrideURL
_narrowband
_onDeviceOnly
_phoneSequence
_pitch
_preferencesObserver
_prepareToRecognizeWithTaskHint:completion:
_queryParameters
_queue
_queuedBuffers
_rawTranscription
_rawTranscriptions
_recognitionActive
_recognitionOverrides
_recognitionResultToReportAfterFinalSearchResults
_request
_requestIdentifier
_requestOfflineDictationSupportWithCompletion:
_response
_responseWithCFURLResponse:
_resultHandler
_running
_searchRequest
_searchType
_searchTypes
_segments
_selfReference
_setDynamicLanguageModel:
_setDynamicVocabulary:
_setForceOfflineRecognition:
_setForceUseSiriProcess:
_setModelOverrideURL:
_setRecognitionOverrides:
_setSearchRequest:
_setVoiceTriggerEventInfo:
_sflsrClient
_shimmer
_shouldReportPartialResults
_speakingRate
_speechRecognitionMetadata
_speechStartTimestamp
_startedConnectionWithLanguageCode:delegate:taskHint:requestIdentifier:
_startedLocalConnectionWithLanguageCode:delegate:taskHint:requestIdentifier:taskIdentifier:
_substring
_supportsOnDeviceRecognition
_task
_taskHint
_timestamp
_voiceTriggerEventInfo
_voicing
_waitForVoiceSearchResult
acousticFeatureValuePerFrame
acousticFeatures
addEntriesFromDictionary:
addKeyValuePair:with:
addObject:
addObjectsFromArray:
addObserverForName:object:queue:usingBlock:
addOovsFromSentence:
addOperationWithBlock:
addOutput:
addProns:forWord:
addProns:forWord:completion:
addPronsFromFile:
addRecordedSpeechSampleData:
addSentence:
addSentenceToNgramCounts:
addSentenceToNgramCounts:completion:
addSentences:
allObjects
alternativeConfidences
alternativeSubstrings
appendAudioPCMBuffer:
appendAudioSampleBuffer:
appendString:
applicationVersion
archivedDataWithRootObject:requiringSecureCoding:error:
array
arrayWithObjects:count:
assetPathForLanguage:
assetReaderAudioMixOutputWithAudioTracks:audioSettings:
assetReaderWithAsset:error:
assetWithURL:
attributes
audioAnalytics
authorizationStatus
autorelease
available
averagePauseDuration
averagePower
beginAvailabilityMonitoring
bestTranscription
binarySampleRepresentation
bundleIdentifier
callObserver:callChanged:
canAccessPathAt:methodName:error:
canAddOutput:
cancel
cancelAvailabilityMonitoring
cancelSpeech
cancelled
caseInsensitiveCompare:
class
code
componentsSeparatedByString:
confidence
configParametersForVoicemailWithLanguage:
configParametersForVoicemailWithLanguage:completion:
conformsToProtocol:
contextualStrings
convertToBuffer:error:withInputFromBlock:
copy
copyNextSampleBuffer
copyWithZone:
count
countByEnumeratingWithState:objects:count:
createDirectoryAtPath:withIntermediateDirectories:attributes:error:
createDirectoryAtURL:withIntermediateDirectories:attributes:error:
createNgramCountsArtifactFromPhraseCountArtifact:writeDirectory:
createNgramCountsArtifactFromPhraseCountArtifact:writeToDirectory:sandboxExtensions:completion:
createNgramCountsArtifactWithIdentifier:rawPhraseCountsPath:customPronunciationsPath:writeDirectory:
createPhraseCountsArtifactWithIdentifier:rawPhraseCountsPath:customPronunciationsPath:writeDirectory:
createPhraseCountsArtifactWithIdentifier:rawPhraseCountsPath:customPronunciationsPath:writeToDirectory:sandboxExtensions:completion:
currentHandler
currentLocale
data
dataWithBytes:length:
dataWithContentsOfURL:
dataWithJSONObject:options:error:
debugDescription
decodeBoolForKey:
decodeDoubleForKey:
decodeIntegerForKey:
decodeObjectOfClass:forKey:
decodeObjectOfClasses:forKey:
defaultCenter
defaultManager
delegate
description
deserializeModelData:
deserializeNgramCountsData:completion:
detectMultipleUtterances
dictationConnection:didBeginLocalRecognitionWithModelInfo:
dictationConnection:didDetectLanguage:confidenceScores:
dictationConnection:didDetectLanguage:confidenceScores:isConfident:
dictationConnection:didFinishWritingAudioFile:error:
dictationConnection:didProcessAudioDuration:
dictationConnection:didReceiveSearchResults:recognizedText:stable:final:
dictationConnection:didRecognizeFinalResultCandidatePackage:
dictationConnection:didRecognizeMultilingualSpeech:
dictationConnection:didRecognizePackage:
dictationConnection:didRecognizePackage:nluResult:
dictationConnection:didRecognizePartialResult:
dictationConnection:didRecognizePhrases:languageModel:correctionIdentifier:
dictationConnection:didRecognizePhrases:languageModel:correctionIdentifier:replacingPreviousPhrasesCount:
dictationConnection:didRecognizeTokens:languageModel:
dictationConnection:didRecognizeTranscriptionObjects:languageModel:
dictationConnection:languageDetectorFailedWithError:
dictationConnection:speechRecognitionDidFail:
dictationConnection:speechRecordingDidBeginWithOptions:
dictationConnection:speechRecordingDidFail:
dictationConnectionDidPauseRecognition:
dictationConnectionSpeechRecognitionDidSucceed:
dictationConnectionSpeechRecordingDidBegin:
dictationConnectionSpeechRecordingDidCancel:
dictationConnectionSpeechRecordingDidEnd:
dictationConnectionSpeechRecordingWillBegin:
dictationConnnectionDidChangeAvailability:
dictationIsAvailableForLanguage:synchronous:
dictationIsEnabled
dictionary
dictionaryWithObjects:forKeys:count:
doubleValue
downloadAssetsForLanguage:urgent:forceUpgrade:detailedProgress:completionHandler:
downloadAssetsForLanguage:urgent:forceUpgrade:progress:completionHandler:
duration
dynamicLanguageModel
dynamicVocabulary
enableAutoPunctuation
encodeBool:forKey:
encodeDouble:forKey:
encodeInteger:forKey:
encodeObject:forKey:
encodeWithCoder:
endAudio
endSession
enumerateKeysAndObjectsUsingBlock:
enumerateObjectsUsingBlock:
error
errorWithDomain:code:userInfo:
fetchAssetsForLanguage:completion:
fetchAssetsForLanguage:progress:completion:
fetchAssetsForLanguage:urgent:forceUpgrade:detailedProgress:completion:
fetchAssetsForLanguage:urgent:progress:completion:
fileExistsAtPath:
final
finish
finishing
firstObject
floatValue
forcedOfflineDictationIsAvailableForLanguage:
format
formatDescriptions
frameDuration
frameLength
generateNgramCountsSerializeDataWithCompletion:
generateNgramsSerializeDataAndWriteToFile:
getForcedOfflineDictationSupportedLanguagesWithCompletion:
handleFailureInFunction:file:lineNumber:description:
handleFailureInMethod:object:file:lineNumber:description:
hasPrefix:
hash
headerFields
infoDictionary
init
initFromFormat:toFormat:
initWithActivationEvent:
initWithAssetPath:
initWithBinarySampleRepresentation:metadata:timestamp:
initWithCoder:
initWithData:encoding:
initWithDelegate:
initWithFilePath:
initWithPCMFormat:frameCapacity:
initWithServiceName:
initWithStreamDescription:
initWithType:
initWithURL:
initWithVoiceSearchResult:
initialize
initializeLmWithAssetPath:completion:
initializeLmWithLocale:completion:
initializeWithSandboxExtensions:
inlineItemList
inputFormat
insertObject:atIndex:
installedLanguages
installedLanguagesWithCompletion:
installedLanguagesWithSynchronousConnection:completion:
int16ChannelData
intValue
interactionIdentifier
interfaceWithProtocol:
invalidate
ipaPhoneSequence
isAvailable
isCacheDirInternal
isCancelled
isEqual:
isEqualToString:
isFinishing
isKindOfClass:
isMemberOfClass:
isSpeechXPCEnabled
issueReadSandboxExtensionForFilePath:error:
issueReadWriteSandboxExtensionForDirectoryPath:error:
jitter
language
length
lmeThreshold
lmeThresholdWithCompletion:
localSpeechRecognitionClient:didFinishRecognition:
localSpeechRecognitionClient:didProcessAudioDuration:
localSpeechRecognitionClient:didRecognizePartialResult:rawPartialResult:
localSpeechRecognitionClient:speechRecognitionDidFail:
localSpeechRecognitionClient:speechRecordingDidFail:
localSpeechRecognitionClientSpeechRecognitionDidSucceed:
localSpeechRecognitionClientSpeechRecordingDidCancel:
localSpeechRecognitionDidDownloadAssetsWithProgress:isStalled:
localSpeechRecognitionDidFail:
localSpeechRecognitionDidFinishDownloadingAssets:error:
localSpeechRecognitionDidFinishRecognition:
localSpeechRecognitionDidProcessAudioDuration:
localSpeechRecognitionDidRecognizePartialResult:rawPartialResult:
localSpeechRecognitionDidSucceed
locale
localeIdentifier
localeWithLocaleIdentifier:
logCoreAnalyticsEvent:withAnalytics:
mainBundle
mainQueue
maximumRecognitionDuration
metricsWithCompletion:
modelOverrideURL
mutableAudioBufferList
mutableBytes
mutableCopy
narrowband
nativeAudioFormat
numberWithDouble:
numberWithFloat:
numberWithInt:
numberWithInteger:
numberWithUnsignedInteger:
objectAtIndex:
objectForKey:
objectForKeyedSubscript:
onDeviceOnly
oovWordsAndFrequenciesWithCompletion:
outOfVocabularyWords
outOfVocabularyWordsAndFrequencies
path
peakPower
performSelector:
performSelector:withObject:
performSelector:withObject:withObject:
phoneSequence
pitch
preloadRecognizerForLanguage:task:recognitionOverrides:modelOverrideURL:completion:
processInfo
promoteAssets
purgeAssetsForLanguage:completion:
purgeAssetsForLanguage:error:
queryMetaDataSync
queue
raise:format:
rawRecognition
rawTranscription
rawTranscriptions
recognition
recognitionMetadataFromSpeechPhrases:afAudioAnalytics:utteranceStart:
recognitionOverrides
recognitionTaskWithRequest:delegate:
recognitionTaskWithRequest:resultHandler:
recognizedResultFromPackage:
remoteObjectProxyWithErrorHandler:
removeObjectAtIndex:
removeSpaceAfter
removeSpaceBefore
requestAuthorization:
requestIdentifier
requestOfflineDictationSupportForLanguage:completion:
requiresOnDeviceRecognition
respondsToSelector:
response
result
resume
retain
retainCount
returnTypes:
searchType
searchTypes
segments
self
setActivationEventTime:
setAddsPunctuation:
setApplicationName:
setApplicationVersion:
setAssetsAsProvisional
setAssetsPurgeability:exceptLanguages:assetType:
setAssetsPurgeability:exceptLanguages:error:
setAssetsPurgeability:forLanguages:assetType:
setAssetsPurgeability:forLanguages:error:
setAutoPunctuation:
setClasses:forSelector:argumentIndex:ofReply:
setContextualStrings:
setDefaultTaskHint:
setDelegate:
setDelegate:queue:
setDetectMultipleUtterances:
setDetectUtterances:
setDynamicLanguageModel:
setDynamicVocabulary:
setEnableAutoPunctuation:
setExportedInterface:
setExportedObject:
setFieldLabel:
setForceOfflineRecognition:
setHeaderFields:
setInlineItemList:
setInteractionIdentifier:
setInterruptionHandler:
setInvalidationHandler:
setKeyboardIdentifier:
setKeyboardType:
setLanguage:
setMaximumRecognitionDuration:
setModelOverrideURL:
setNarrowband:
setObject:forKey:
setOnDeviceOnly:
setOriginalAudioFileURL:
setQueryParameters:
setQueue:
setRecognitionOverrides:
setRemoteObjectInterface:
setRequestIdentifier:
setSampleRateConverterQuality:
setShouldReportPartialResults:
setTask:
setTaskHint:
setTaskIdentifier:
setUseAutomaticEndpointing:
setUseStreamingDictation:
setVoiceSearchHeaderFields:
setVoiceSearchQueryParameters:
setVoiceSearchTypeOptions:
setVoiceTriggerEventInfo:
setWithArray:
setWithCapacity:
setWithObject:
sharedInstance
shouldReportPartialResults
silenceStartTime
speakingRate
speechDuration
speechRecognitionFeatures
speechRecognitionMetadata
speechRecognitionTask:didFinishRecognition:
speechRecognitionTask:didFinishSuccessfully:
speechRecognitionTask:didHypothesizeTranscription:
speechRecognitionTask:didProcessAudioDuration:
speechRecognitionTask:didReceiveSearchResults:recognizedText:stable:final:
speechRecognitionTaskFinishedReadingAudio:
speechRecognitionTaskWasCancelled:
speechRecognizer:availabilityDidChange:
speechStartTimestamp
startReading
startRecordedAudioDictationWithOptions:forLanguage:
startRecordedAudioDictationWithOptions:forLanguage:narrowband:
startRecordedAudioDictationWithParameters:
startTime
state
statusCode
stopSpeech
stopSpeechWithOptions:
string
stringByAppendingFormat:
stringByAppendingString:
stringByReplacingCharactersInRange:withString:
stringByReplacingOccurrencesOfString:withString:
stringWithFormat:
stringWithString:
stringWithUTF8String:
subarrayWithRange:
substring
substringRange
superclass
supportedLocales
supportsOnDeviceRecognition
supportsSecureCoding
synchronousRemoteObjectProxyWithErrorHandler:
systemUptime
task
taskHint
taskIdentifier
taskNameFromTaskHint:
text
timestamp
tokens
tracksWithMediaType:
trainAppLmFromNgramCountsArtifact:language:writeToDirectory:modelOverride:completion:
trainAppLmFromNgramCountsArtifact:language:writeToDirectory:sandboxExtensions:completion:
trainAppLmFromNgramsSerializedData:customPronsData:language:writeToDirectory:sandboxExtension:completion:
trainAppLmFromNgramsSerializedDataFile:customPronsFile:language:writeToDirectory:modelOverride:completion:
trainFromPlainTextAndWriteToDirectory:
trainFromPlainTextAndWriteToDirectory:completion:
trainFromPlainTextAndWriteToDirectory:sandboxExtension:completion:
transcriptions
transcriptionsWithTokens:
unarchivedObjectOfClass:fromData:error:
utteranceStart
voiceAnalytics
wakeUpWithCompletion:
writeToURL:options:error:
zone
B16@0:8
@24@0:8^{_NSZone=}16
v24@0:8@16
@24@0:8@16
v24@0:8@"NSCoder"16
@24@0:8@"NSCoder"16
@32@0:8@16d24
@16@0:8
d16@0:8
v16@0:8
@"NSArray"
@48@0:8@16@24@32@40
@"SFAcousticFeature"
q16@0:8
@"NSHTTPURLResponse"
@"NSData"
B24@0:8@16
#16@0:8
@24@0:8:16
@32@0:8:16@24
@40@0:8:16@24@32
B24@0:8#16
B24@0:8:16
Vv16@0:8
Q16@0:8
^{_NSZone=}16@0:8
B24@0:8@"Protocol"16
@"NSString"16@0:8
v32@0:8@16@24
v40@0:8@16@24@32
v44@0:8@16@24@32B40
v48@0:8@16@24@32@40
v56@0:8@16@24@32@40Q48
v32@0:8@16d24
v48@0:8@16@24@32B40B44
v24@0:8@"AFDictationConnection"16
v32@0:8@"AFDictationConnection"16@"AFDictationOptions"24
v32@0:8@"AFDictationConnection"16@"NSString"24
v32@0:8@"AFDictationConnection"16@"NSError"24
v40@0:8@"AFDictationConnection"16@"NSString"24@"NSDictionary"32
v44@0:8@"AFDictationConnection"16@"NSString"24@"NSDictionary"32B40
v32@0:8@"AFDictationConnection"16@"SASMultilingualSpeechRecognized"24
v48@0:8@"AFDictationConnection"16@"NSArray"24@"NSString"32@40
v56@0:8@"AFDictationConnection"16@"NSArray"24@"NSString"32@40Q48
v40@0:8@"AFDictationConnection"16@"NSArray"24@"NSString"32
v48@0:8@"AFDictationConnection"16@"NSArray"24@"AFDictationNLUResult"32@"NSString"40
v32@0:8@"AFDictationConnection"16@"SASSpeechPartialResult"24
v32@0:8@"AFDictationConnection"16d24
v40@0:8@"AFDictationConnection"16@"NSFileHandle"24@"NSError"32
v48@0:8@"AFDictationConnection"16@"NSArray"24@"NSString"32B40B44
v32@0:8@"AFDictationConnection"16@"AFSpeechPackage"24
v40@0:8@"AFDictationConnection"16@"AFSpeechPackage"24@"AFDictationNLUResult"32
v24@0:8@"NSData"16
v24@0:8@"SFLocalSpeechRecognitionClient"16
v32@0:8@"SFLocalSpeechRecognitionClient"16@"NSError"24
v40@0:8@"SFLocalSpeechRecognitionClient"16@"SFTranscription"24@"SFTranscription"32
v32@0:8@"SFLocalSpeechRecognitionClient"16d24
v32@0:8@"SFLocalSpeechRecognitionClient"16@"SFSpeechRecognitionResult"24
@48@0:8@16@24@32q40
f16@0:8
@"AFDictationConnection"
@"SFLocalSpeechRecognitionClient"
@"NSOperationQueue"
@"NSString"
@"SFSpeechRecognitionRequest"
@"NSObject<OS_dispatch_queue>"
@"NSError"
@56@0:8@16@24@32q40@?48
@56@0:8@16@24@32q40@48
@"<_SFSpeechRecognitionTaskDelegatePrivate>"
@"SFSpeechRecognitionResult"
@"_SFSpeechRecognitionDelegateTask"
v20@0:8B16
v24@0:8d16
@"NSDictionary"
@"NSURL"
Vv24@0:8@16
Vv56@0:8@16@24@32@40@?48
Vv24@0:8@?16
Vv36@0:8@16B24B28B32
Vv32@0:8@16@?24
Vv40@0:8@16@24@?32
Vv64@0:8@16@24@32@40@48@?56
Vv48@0:8@16@24@32@?40
Vv24@0:8@"SFRequestParameters"16
Vv24@0:8@"NSData"16
Vv56@0:8@"NSString"16@"NSString"24@"NSDictionary"32@"NSURL"40@?<v@?@"NSError">48
Vv24@0:8@?<v@?@"NSSet">16
v24@0:8@"NSArray"16
v32@0:8@"NSString"16@"NSDictionary"24
Vv36@0:8@"NSString"16B24B28B32
Vv32@0:8@"NSString"16@?<v@?@"NSString"@"NSError">24
Vv32@0:8@"NSString"16@?<v@?@"NSDictionary">24
Vv32@0:8@"NSString"16@?<v@?@"NSError">24
Vv32@0:8@"NSLocale"16@?<v@?@"NSError">24
Vv24@0:8@"NSString"16
Vv32@0:8@"NSString"16@?<v@?@"NSArray">24
Vv40@0:8@"NSSet"16@"NSString"24@?<v@?B>32
Vv24@0:8@?<v@?@"NSDictionary">16
Vv40@0:8@"NSURL"16@"NSString"24@?<v@?@"NSURL"@"NSURL">32
Vv24@0:8@?<v@?@"NSData">16
Vv32@0:8@"NSData"16@?<v@?@"NSDictionary">24
Vv24@0:8@?<v@?q>16
Vv24@0:8@?<v@?>16
Vv64@0:8@"NSData"16@"NSData"24@"NSString"32@"NSURL"40@"NSString"48@?<v@?@"NSURL"@"NSURL">56
Vv64@0:8@"NSString"16@"NSURL"24@"NSURL"32@"NSURL"40@"NSArray"48@?<v@?@"NSURL">56
Vv48@0:8@"NSURL"16@"NSURL"24@"NSArray"32@?<v@?@"NSURL">40
Vv56@0:8@"NSURL"16@"NSString"24@"NSURL"32@"NSArray"40@?<v@?@"NSURL"@"NSURL">48
Vv32@0:8@16@24
Vv24@0:8d16
Vv28@0:8Q16B24
Vv32@0:8@"SFTranscription"16@"SFTranscription"24
Vv24@0:8@"SFSpeechRecognitionResult"16
Vv24@0:8@"NSError"16
Vv32@0:8@"NSString"16@"NSError"24
v64@0:8@16@24@32@40@48@?56
v56@0:8@16@24@32@40@?48
v32@0:8@16@?24
B32@0:8@16@24
@32@0:8@16@24
@"NSLocale"
@40@0:8@16@24d32
@40@0:8@"NSData"16@"NSDictionary"24d32
@"NSData"16@0:8
@44@0:8@16@24B32@36
@"SFTranscription"
@"SFSpeechRecognitionMetadata"
v24@0:8@?16
v32@0:8@"CXCallObserver"16@"CXCall"24
v32@0:8q16@?24
@32@0:8@16@?24
v24@0:8q16
@"CXCallObserver"
@"<NSObject>"
@"<SFSpeechRecognizerDelegate>"
@100@0:8@16{_NSRange=QQ}24d40d48f56@60@68@76@84@92
{_NSRange=QQ}16@0:8
@"SFVoiceAnalytics"
{_NSRange="location"Q"length"Q}
v28@0:8B16@?20
v48@0:8@16B24B28@?32@?40
v40@0:8@16@24@?32
v48@0:8@16@24@32@?40
@"NSXPCConnection"
@"<SFLocalSpeechRecognitionDelegate>"
@24@0:8q16
B40@0:8@16@24^@32
@32@0:8@16^@24
@48@0:8@16@24q32@40
@56@0:8@16@24q32@40@48
@"_SFSearchRequest"
v24@0:8^{opaqueCMSampleBuffer=}16
@"<SFSpeechRecognitionBufferDelegate>"
@"NSMutableArray"
@"AVAudioConverter"
@48@0:8@16@24d32d40
v40@0:8@16@?24@?32
v44@0:8@16B24@?28@?36
v32@0:8@16^@24
v36@0:8B16@20^@28
@56@0:8d16d24@32d40d48
mcpl
@(#)PROGRAM:Speech  PROJECT:SpeechRecognition-1
MbP?
@mcpl
_acousticFeaturePerFrame
_frameDuration
, featureValues=%@, frameDuration=%f
_jitter
_shimmer
_pitch
_voicing
, jitter=%@, shimmer=%@, pitch=%@, voicing=%@
v32@?0@"NSString"8@"NSString"16^B24
com.apple.Speech.Task.Internal
v8@?0
User denied access to speech recognition
-[SFSpeechRecognitionTask handleSpeechRecognitionDidFailWithError:]_block_invoke
-[SFSpeechRecognitionTask dictationConnection:speechRecordingDidFail:]_block_invoke
-[SFSpeechRecognitionTask localSpeechRecognitionClient:speechRecordingDidFail:]_block_invoke
v24@?0@"SFSpeechRecognitionResult"8@"NSError"16
textNotEmpty
isFinal
, applicationName=%@, applicationVersion=%@, inlineItemList=%@, requestIdentifier=%@ task=%@ language=%@ narrowband=%d recognitionOverrides=%@ modelOverrideURL=%@ maximumRecognitionDuration=%f dynamicLanguageModel=%@ dynamicVocabulary=%@ detectMultipleUtterances=%d onDeviceOnly=%d enableAutoPunctuation=%d
SFRequestParameters::applicationName
SFRequestParameters::applicationVersion
SFRequestParameters::inlineItemList
SFRequestParameters::requestIdentifier
SFRequestParameters::taskIdentifier
SFRequestParameters::task
SFRequestParameters::language
SFRequestParameters::narrowband
SFRequestParameters::recognitionOverrides
SFRequestParameters::modelOverrideURL
SFRequestParameters::maximumRecognitionDuration
SFRequestParameters::dynamicLanguageModel
SFRequestParameters::dynamicVocabulary
SFRequestParameters::detectMultipleUtterances
SFRequestParameters::onDeviceOnly
SFRequestParameters::enableAutoPunctuation
v24@?0@"NSSet"8@"NSSet"16
Cannot make recognizer for %@. Supported locale identifiers are %@
%@-%@
v16@?0@"NSError"8
v16@?0@"NSArray"8
v16@?0@"NSDictionary"8
v24@?0@"NSURL"8@"NSURL"16
Write failed error:%@
Write successfull
Failed to generate ngram counts
There was a problem writing data to the file! Error:  %@
Ngram counts and OOV list serialized and saved to: %@
v16@?0@"NSData"8
grapheme
phoneme
Invalid prons: %@ for word: %@
v12@?0B8
v16@?0q8
Could not find ngrams serialized data file: %@ 
Failed to read the file: %@
Successfully read the file: %@
Failed to read the pronunciation file: %@
Failed to create directory error:%@
Created directory: %@
Failed to initialize language model: %@
Could not issue sandbox extension for path:%@
No write directory provided
No rawPhraseCountsPath provided
No customPronunciationsPath provided but will continue..
v16@?0@"NSURL"8
No phraseCountArtifact provided
_bestTranscription
_rawTranscription
_final
_speechRecognitionMetadata
 final=%d, bestTranscription=%@, speechRecognitionMetadata=%@
v32@?0@"NSString"8Q16^B24
v32@?0@"SFTranscriptionSegment"8Q16^B24
v12@?0C8
v16@?0@"NSNotification"8
Required assets are not available for Locale:%@
v24@?0@"NSString"8@"NSError"16
v16@?0@"NSSet"8
Result handler must be non-null
%@ queue must not be nil
, substringRange=%@, timestamp=%@, duration=%@, confidence=%@, substring=%@, alternativeSubstrings=%@, phoneSequence=%@, ipaPhoneSequence=%@, voiceAnalytics=%@
_substring
_substringRange.location
_substringRange.length
_timestamp
_duration
_confidence
_alternativeSubstrings
_alternativeConfidences
_phoneSequence
_ipaPhoneSequence
_voiceAnalytics
SFTranscriptionSegment.m
com.apple.speechapi.RequestStarted
com.apple.speechapi.StopSpeech
com.apple.speechapi.CancelSpeech
com.apple.speechapi.ResultPackage
com.apple.speechapi.RequestCompleted
com.apple.speechapi.RequestFailed
com.apple.speechapi.RequestPerformance
com.apple.speechapi.AssetRequested
com.apple.speechapi.AssetRequestCompleted
unknown.%ld
com.apple.speech.localspeechrecognition
SFLocalSpeechRecognitionClient
v20@?0Q8B16
+N9mZUAHooNvMiQnjeTJ8g
InternalBuild
hi-IN
hi-IN-translit
Translit
speakingRate
averagePauseDuration
jitter
shimmer
pitch
voicing
Dictation
VoiceMail
Captioning
dumpedTaskIdentifier
success
language
task
narrowband
appname
ondevice
modelVersion
audioDuration
ttfw
responseTime
modelLoadTime
recognitionDuration
jitLmeProcessingTime
asrInitializationTime
cpuRtf
errorCode
errorDomain
v32@?0@"AFSpeechToken"8Q16^B24
v32@?0@"AFSpeechInterpretation"8Q16^B24
Siri
trial_dictation_asset_delivery
Failed to access path: %@ method:%@
+[SFUtilities issueReadWriteSandboxExtensionForDirectoryPath:error:]
com.apple.app-sandbox.read-write
+[SFUtilities issueReadSandboxExtensionForFilePath:error:]
com.apple.app-sandbox.read
/var/mobile/Library/Caches
/private/var/mobile/Library/Caches
+[SFUtilities lsrCacheDirPath]
SFSpeechPreecordedRequest
Use -[SFSpeechURLRecognitionRequest initWithURL:]
Could not add output for %@
B8@?0
none
com.apple.SFSpeechAudioBufferRecognitionRequest
SFSpeechRecognitionRequest.m
%@ cannot be re-used
Invalid audio format
@"AVAudioBuffer"20@?0I8^q12
Could not drain converter %@
Could not run audio converter %@
CMBlockBufferCopyDataBytes could not copy data: %d
_speakingRate
_avgPauseDuration
, formattedString=%@, segments=%@, speakingRate=%f, averagePauseDuration=%f
_segments
_formattedString
Language
Failed to set purgeability for assets.
CESRTrialAssetManager
Class getCESRTrialAssetManagerClass(void)_block_invoke
SFSpeechAssetManager.m
Unable to find class %s
void *CoreEmbeddedSpeechRecognitionLibrary(void)
LSRConnection
SpeechFramework
_averagePauseDuration
_speechStartTimestamp
_speechDuration
, speakingRate=%@, averagePauseDuration=%@, speechStartTimestamp=%@, speechDuration=%@, voiceAnalytics=%@
%s Ignoring subsequent recongition error: %@
%s Ignoring subsequent recording error: %@
%s Ignoring subsequent local speech recording error: %@
%@ Interrupted
%@ Invalidated
Dealloc-ing
Local speech recognizer restarted while already recognizing
Received an error while accessing %@ service: %@
%s %@: path=%@
%s %@: sandbox_extension_issue_file() returned NULL. path=%@
%s %@: Inaccessible file (%@) : error=%@
%s %@: sandboxExtension=%@
%s Failed to create directory %@
%s Write successful %@
softlink:r:path:/System/Library/PrivateFrameworks/CoreEmbeddedSpeechRecognition.framework/CoreEmbeddedSpeechRecognition
SFAcousticFeature
NSCopying
NSSecureCoding
NSCoding
SFVoiceAnalytics
_SFSearchResult
SFSpeechRecognitionTask
AFDictationDelegate
NSObject
SFSpeechRecognitionBufferDelegate
SFLocalSpeechRecognitionDelegate
_SFSpeechRecognitionBlockTask
_SFSpeechRecognitionDelegateTask
SFRequestParameters
SFLSRProtocol
SFLSRDelegate
SFSpeechLanguageModel
SFSpeechRecognitionResult
SRSampling
SFSpeechRecognizer
CXCallObserverDelegate
SFTranscriptionSegment
SFLocalSpeechRecognitionClient
SFUtilities
_SFSearchRequest
SFSpeechRecognitionRequest
SFSpeechURLRecognitionRequest
SFSpeechAudioBufferRecognitionRequest
SFTranscription
SFSpeechAssetManager
SFSpeechRecognitionMetadata
T@"SFSpeechRecognitionMetadata",R,N,V_speechRecognitionMetadata
JSONObjectWithData:options:error:
TB,R,N,GisCancelled,V_cancelled
T@"<SFLocalSpeechRecognitionDelegate>",R,W,N,V_delegate
Tq,N,V_taskHint
T@"AVAudioFormat",R,N
_bufferDelegate
T@"NSArray",C,N,V_inlineItemList
_jitter
T@"NSArray",R,C,N,V_acousticFeatureValuePerFrame
_setMaximumRecognitionDuration:
T@"NSArray",R,N,V_alternativeConfidences
_substringRange
T@"NSData",R,N,V_data
_transcriptions
T@"NSDictionary",&,N,G_voiceTriggerEventInfo,S_setVoiceTriggerEventInfo:,V_voiceTriggerEventInfo
addAudioPacket:
T@"NSDictionary",C,N,V_queryParameters
allKeys
T@"NSDictionary",R,C,N
confidenceScore
T@"NSHTTPURLResponse",R,N,V_response
dealloc
T@"NSOperationQueue",&,N,V_queue
dictationConnection:didRecognizeTokens:nluResult:languageModel:
T@"NSString",C,N,V_applicationName
fetchAssetsForLanguage:urgent:forceUpgrade:progress:completion:
T@"NSString",C,N,V_interactionIdentifier
headers
T@"NSString",C,N,V_requestIdentifier
initWithLength:
T@"NSString",C,N,V_taskIdentifier
interpretations
T@"NSString",R,C,N,V_formattedString
isProxy
T@"NSString",R,C,N,V_substring
lsrCacheDirPath
T@"NSString",R,N,V_phoneSequence
numberWithBool:
T@"NSURL",&,N,G_dynamicVocabulary,S_setDynamicVocabulary:,V_dynamicVocabulary
queryParameters
T@"NSURL",C,N,V_dynamicLanguageModel
removeObserver:
T@"NSURL",C,N,V_modelOverrideURL
setFrameLength:
T@"SFAcousticFeature",R,C,N,V_jitter
setSearchTypes:
T@"SFAcousticFeature",R,C,N,V_shimmer
shimmer
T@"SFTranscription",R,C,N,V_bestTranscription
supportedLocalesWithCompletion:
T@"SFVoiceAnalytics",R,N,V_voiceAnalytics
voicing
.cxx_destruct
TB,N,V_detectMultipleUtterances
T#,R
TB,R,N,GisFinishing,V_finishing
T@"<SFSpeechRecognizerDelegate>",W,N,V_delegate
T@"NSArray",C,N,V_contextualStrings
_inlineItemList
T@"NSArray",R,C,N
_locale
T@"NSArray",R,C,N,V_segments
_speechDuration
T@"NSArray",R,N,V_alternativeSubstrings
_taskIdentifier
T@"NSDictionary",&,N,G_recognitionOverrides,S_setRecognitionOverrides:,V_recognitionOverrides
_voiceAnalytics
T@"NSDictionary",C,N,V_headerFields
addsPunctuation
T@"NSDictionary",C,N,V_recognitionOverrides
applicationName
T@"NSError",R,C,N
containsObject:
T@"NSLocale",R,C,N,V_locale
defaultTaskHint
T@"NSString",&,N,V_taskIdentifier
downloadAssetsForLanguage:urgent:forceUpgrade:detailedProgress:
T@"NSString",C,N,V_applicationVersion
formattedString
T@"NSString",C,N,V_language
initWithFormat:
T@"NSString",C,N,V_task
initWithLocale:
T@"NSString",R,C
isFinal
T@"NSString",R,C,N,V_requestIdentifier
lexemes
T@"NSString",R,N,V_ipaPhoneSequence
metrics
T@"NSURL",&,N,G_dynamicLanguageModel,S_setDynamicLanguageModel:,V_dynamicLanguageModel
phrases
T@"NSURL",&,N,G_modelOverrideURL,S_setModelOverrideURL:,V_modelOverrideURL
release
T@"NSURL",C,N,V_dynamicVocabulary
T@"NSURL",R,C,N,V_URL
setRequiresOnDeviceRecognition:
T@"SFAcousticFeature",R,C,N,V_pitch
setSupportsOnDeviceRecognition:
T@"SFAcousticFeature",R,C,N,V_voicing
stringByAppendingPathComponent:
T@"SFTranscription",R,C,N,V_rawTranscription
transcriptionFromSpeechPhrases:afAudioAnalytics:utteranceStart:
T@"_SFSearchRequest",&,N,G_searchRequest,S_setSearchRequest:,V_searchRequest
TB,N
TB,N,G_forceOfflineRecognition,S_setForceOfflineRecognition:,V_forceOfflineRecognition
TB,N,G_forceUseSiriProcess,S_setForceUseSiriProcess:,V_forceUseSiriProcess
TB,N,V_addsPunctuation
TB,N,V_enableAutoPunctuation
TB,N,V_narrowband
TB,N,V_onDeviceOnly
TB,N,V_shouldReportPartialResults
TB,N,V_supportsOnDeviceRecognition
TB,R
TB,R,N,G_isAvailableForForcedOfflineRecognition
TB,R,N,GisAvailable
TB,R,N,GisFinal,V_final
TQ,R
Td,N,G_maximumRecognitionDuration,S_setMaximumRecognitionDuration:,V_maximumRecognitionDuration
Td,N,V_maximumRecognitionDuration
Td,R,N,V_averagePauseDuration
Td,R,N,V_duration
Td,R,N,V_frameDuration
Td,R,N,V_speakingRate
Td,R,N,V_speechDuration
Td,R,N,V_speechStartTimestamp
Td,R,N,V_timestamp
Tf,R,N,V_confidence
Tq,N,V_defaultTaskHint
Tq,N,V_searchTypes
Tq,R,N
Tq,R,N,V_searchType
Tq,R,N,V_taskHint
T{_NSRange=QQ},R,N,V_substringRange
UTF8String
UUID
UUIDString
_URL
_acousticFeatureValuePerFrame
_addsPunctuation
_afDictationRequestParams
_alternativeConfidences
_alternativeSubstrings
_appendAudioPCMBuffer:
_applicationName
_applicationVersion
_assetQueryForLanguage:
_audioEnded
_availableForForcedOfflineRecognition
_averagePauseDuration
_bestTranscription
_callObserver
_cancelled
_completed
_confidence
_contextualStrings
_converter
_data
_defaultTaskHint
_delegate
_detectMultipleUtterances
_dictationConnection
_downloadCompletion
_downloadProgress
_duration
_dynamicLanguageModel
_dynamicVocabulary
_enableAutoPunctuation
_error
_externalQueue
_facetimeObserver
_fetchSupportedForcedOfflineLocalesWithCompletion:
_final
_finishing
_forceOfflineRecognition
_forceUseSiriProcess
_foregroundObserver
_formattedString
_frameDuration
_hasFiredFinalResult
_hasSentRealSearchResults
_headerFields
_initWithAcousticFeatureValue:frameDuration:
_initWithBestTranscription:rawTranscription:final:speechRecognitionMetadata:
_initWithJitter:shimmer:pitch:voicing:
_initWithRequest:queue:languageCode:taskHint:
_initWithRequest:queue:languageCode:taskHint:delegate:
_initWithRequest:queue:languageCode:taskHint:resultHandler:
_initWithSegments:formattedString:speakingRate:averagePauseDuration:
_initWithSpeechStartTimestamp:speechDuration:voiceAnalytics:speakingRate:averagePauseDuration:
_initWithSubstring:range:timestamp:duration:confidence:alternativeSubstrings:alternativeConfidences:phoneSequence:ipaPhoneSequence:voiceAnalytics:
_interactionIdentifier
_internalQueue
_ipaPhoneSequence
_isAvailableForForcedOfflineRecognition
_language
_languageCode
_lsrClient
_lsrConnection
_maximumRecognitionDuration
_modelOverrideURL
_narrowband
_onDeviceOnly
_phoneSequence
_pitch
_preferencesObserver
_prepareToRecognizeWithTaskHint:completion:
_queryParameters
_queue
_queuedBuffers
_rawTranscription
_rawTranscriptions
_recognitionActive
_recognitionOverrides
_recognitionResultToReportAfterFinalSearchResults
_request
_requestIdentifier
_requestOfflineDictationSupportWithCompletion:
_response
_responseWithCFURLResponse:
_resultHandler
_running
_searchRequest
_searchType
_searchTypes
_segments
_selfReference
_setDynamicLanguageModel:
_setDynamicVocabulary:
_setForceOfflineRecognition:
_setForceUseSiriProcess:
_setModelOverrideURL:
_setRecognitionOverrides:
_setSearchRequest:
_setVoiceTriggerEventInfo:
_sflsrClient
_shimmer
_shouldReportPartialResults
_speakingRate
_speechRecognitionMetadata
_speechStartTimestamp
_startedConnectionWithLanguageCode:delegate:taskHint:requestIdentifier:
_startedLocalConnectionWithLanguageCode:delegate:taskHint:requestIdentifier:taskIdentifier:
_substring
_supportsOnDeviceRecognition
_task
_taskHint
_timestamp
_voiceTriggerEventInfo
_voicing
_waitForVoiceSearchResult
acousticFeatureValuePerFrame
acousticFeatures
addEntriesFromDictionary:
addKeyValuePair:with:
addObject:
addObjectsFromArray:
addObserverForName:object:queue:usingBlock:
addOovsFromSentence:
addOperationWithBlock:
addOutput:
addProns:forWord:
addProns:forWord:completion:
addPronsFromFile:
addRecordedSpeechSampleData:
addSentence:
addSentenceToNgramCounts:
addSentenceToNgramCounts:completion:
addSentences:
allObjects
alternativeConfidences
alternativeSubstrings
appendAudioPCMBuffer:
appendAudioSampleBuffer:
appendString:
applicationVersion
archivedDataWithRootObject:requiringSecureCoding:error:
array
arrayWithObjects:count:
assetPathForLanguage:
assetReaderAudioMixOutputWithAudioTracks:audioSettings:
assetReaderWithAsset:error:
assetWithURL:
audioAnalytics
authorizationStatus
autorelease
available
averagePauseDuration
averagePower
beginAvailabilityMonitoring
bestTranscription
binarySampleRepresentation
bundleIdentifier
callObserver:callChanged:
canAccessPathAt:methodName:error:
canAddOutput:
cancel
cancelAvailabilityMonitoring
cancelSpeech
cancelled
caseInsensitiveCompare:
class
code
componentsSeparatedByString:
confidence
configParametersForVoicemailWithLanguage:
configParametersForVoicemailWithLanguage:completion:
conformsToProtocol:
contextualStrings
convertToBuffer:error:withInputFromBlock:
copy
copyNextSampleBuffer
copyWithZone:
count
countByEnumeratingWithState:objects:count:
createDirectoryAtPath:withIntermediateDirectories:attributes:error:
createDirectoryAtURL:withIntermediateDirectories:attributes:error:
createNgramCountsArtifactFromPhraseCountArtifact:writeDirectory:
createNgramCountsArtifactFromPhraseCountArtifact:writeToDirectory:sandboxExtensions:completion:
createNgramCountsArtifactWithIdentifier:rawPhraseCountsPath:customPronunciationsPath:writeDirectory:
createPhraseCountsArtifactWithIdentifier:rawPhraseCountsPath:customPronunciationsPath:writeDirectory:
createPhraseCountsArtifactWithIdentifier:rawPhraseCountsPath:customPronunciationsPath:writeToDirectory:sandboxExtensions:completion:
currentHandler
currentLocale
data
dataWithBytes:length:
dataWithContentsOfURL:
dataWithJSONObject:options:error:
debugDescription
decodeBoolForKey:
decodeDoubleForKey:
decodeIntegerForKey:
decodeObjectOfClass:forKey:
decodeObjectOfClasses:forKey:
defaultCenter
defaultManager
delegate
description
deserializeModelData:
deserializeNgramCountsData:completion:
detectMultipleUtterances
dictationConnection:didBeginLocalRecognitionWithModelInfo:
dictationConnection:didDetectLanguage:confidenceScores:
dictationConnection:didDetectLanguage:confidenceScores:isConfident:
dictationConnection:didFinishWritingAudioFile:error:
dictationConnection:didProcessAudioDuration:
dictationConnection:didReceiveSearchResults:recognizedText:stable:final:
dictationConnection:didRecognizeFinalResultCandidatePackage:
dictationConnection:didRecognizeMultilingualSpeech:
dictationConnection:didRecognizePackage:
dictationConnection:didRecognizePackage:nluResult:
dictationConnection:didRecognizePartialResult:
dictationConnection:didRecognizePhrases:languageModel:correctionIdentifier:
dictationConnection:didRecognizePhrases:languageModel:correctionIdentifier:replacingPreviousPhrasesCount:
dictationConnection:didRecognizeTokens:languageModel:
dictationConnection:didRecognizeTranscriptionObjects:languageModel:
dictationConnection:languageDetectorFailedWithError:
dictationConnection:speechRecognitionDidFail:
dictationConnection:speechRecordingDidBeginWithOptions:
dictationConnection:speechRecordingDidFail:
dictationConnectionDidPauseRecognition:
dictationConnectionSpeechRecognitionDidSucceed:
dictationConnectionSpeechRecordingDidBegin:
dictationConnectionSpeechRecordingDidCancel:
dictationConnectionSpeechRecordingDidEnd:
dictationConnectionSpeechRecordingWillBegin:
dictationConnnectionDidChangeAvailability:
dictationIsAvailableForLanguage:synchronous:
dictationIsEnabled
dictionary
dictionaryWithObjects:forKeys:count:
doubleValue
downloadAssetsForLanguage:urgent:forceUpgrade:detailedProgress:completionHandler:
downloadAssetsForLanguage:urgent:forceUpgrade:progress:completionHandler:
duration
dynamicLanguageModel
dynamicVocabulary
enableAutoPunctuation
encodeBool:forKey:
encodeDouble:forKey:
encodeInteger:forKey:
encodeObject:forKey:
encodeWithCoder:
endAudio
endSession
enumerateKeysAndObjectsUsingBlock:
enumerateObjectsUsingBlock:
error
errorWithDomain:code:userInfo:
fetchAssetsForLanguage:completion:
fetchAssetsForLanguage:progress:completion:
fetchAssetsForLanguage:urgent:forceUpgrade:detailedProgress:completion:
fetchAssetsForLanguage:urgent:progress:completion:
fileExistsAtPath:
final
finish
finishing
firstObject
floatValue
forcedOfflineDictationIsAvailableForLanguage:
format
formatDescriptions
frameDuration
frameLength
generateNgramCountsSerializeDataWithCompletion:
generateNgramsSerializeDataAndWriteToFile:
getForcedOfflineDictationSupportedLanguagesWithCompletion:
handleFailureInFunction:file:lineNumber:description:
handleFailureInMethod:object:file:lineNumber:description:
hasPrefix:
hash
headerFields
infoDictionary
init
initFromFormat:toFormat:
initWithAssetPath:
initWithBinarySampleRepresentation:metadata:timestamp:
initWithCoder:
initWithData:encoding:
initWithDelegate:
initWithFilePath:
initWithPCMFormat:frameCapacity:
initWithServiceName:
initWithStreamDescription:
initWithType:
initWithURL:
initWithVoiceSearchResult:
initialize
initializeLmWithAssetPath:completion:
initializeLmWithLocale:completion:
initializeWithSandboxExtensions:
inlineItemList
inputFormat
insertObject:atIndex:
installedLanguages
installedLanguagesWithCompletion:
installedLanguagesWithSynchronousConnection:completion:
int16ChannelData
intValue
interactionIdentifier
interfaceWithProtocol:
invalidate
ipaPhoneSequence
isAvailable
isCacheDirInternal
isCancelled
isEqual:
isEqualToString:
isFinishing
isKindOfClass:
isMemberOfClass:
isSpeechXPCEnabled
issueReadSandboxExtensionForFilePath:error:
issueReadWriteSandboxExtensionForDirectoryPath:error:
jitter
language
length
lmeThreshold
lmeThresholdWithCompletion:
localSpeechRecognitionClient:didFinishRecognition:
localSpeechRecognitionClient:didProcessAudioDuration:
localSpeechRecognitionClient:didRecognizePartialResult:rawPartialResult:
localSpeechRecognitionClient:speechRecognitionDidFail:
localSpeechRecognitionClient:speechRecordingDidFail:
localSpeechRecognitionClientSpeechRecognitionDidSucceed:
localSpeechRecognitionClientSpeechRecordingDidCancel:
localSpeechRecognitionDidDownloadAssetsWithProgress:isStalled:
localSpeechRecognitionDidFail:
localSpeechRecognitionDidFinishDownloadingAssets:error:
localSpeechRecognitionDidFinishRecognition:
localSpeechRecognitionDidProcessAudioDuration:
localSpeechRecognitionDidRecognizePartialResult:rawPartialResult:
localSpeechRecognitionDidSucceed
locale
localeIdentifier
localeWithLocaleIdentifier:
logCoreAnalyticsEvent:withAnalytics:
mainBundle
mainQueue
maximumRecognitionDuration
metricsWithCompletion:
modelOverrideURL
mutableAudioBufferList
mutableBytes
mutableCopy
narrowband
nativeAudioFormat
numberWithDouble:
numberWithFloat:
numberWithInt:
numberWithInteger:
numberWithUnsignedInteger:
objectAtIndex:
objectForKey:
objectForKeyedSubscript:
onDeviceOnly
oovWordsAndFrequenciesWithCompletion:
outOfVocabularyWords
outOfVocabularyWordsAndFrequencies
path
peakPower
performSelector:
performSelector:withObject:
performSelector:withObject:withObject:
phoneSequence
pitch
preloadRecognizerForLanguage:task:recognitionOverrides:modelOverrideURL:completion:
promoteAssets
purgeAssetsForLanguage:completion:
purgeAssetsForLanguage:error:
queue
raise:format:
rawRecognition
rawTranscription
rawTranscriptions
recognition
recognitionMetadataFromSpeechPhrases:afAudioAnalytics:utteranceStart:
recognitionOverrides
recognitionTaskWithRequest:delegate:
recognitionTaskWithRequest:resultHandler:
recognizedResultFromPackage:
remoteObjectProxyWithErrorHandler:
removeObjectAtIndex:
removeSpaceAfter
removeSpaceBefore
requestAuthorization:
requestIdentifier
requestOfflineDictationSupportForLanguage:completion:
requiresOnDeviceRecognition
respondsToSelector:
response
result
resume
retain
retainCount
returnTypes:
searchType
searchTypes
segments
self
setAddsPunctuation:
setApplicationName:
setApplicationVersion:
setAssetsAsProvisional
setAssetsPurgeability:exceptLanguages:assetType:
setAssetsPurgeability:exceptLanguages:error:
setAssetsPurgeability:forLanguages:assetType:
setAssetsPurgeability:forLanguages:error:
setAutoPunctuation:
setClasses:forSelector:argumentIndex:ofReply:
setContextualStrings:
setDefaultTaskHint:
setDelegate:
setDelegate:queue:
setDetectMultipleUtterances:
setDetectUtterances:
setDynamicLanguageModel:
setDynamicVocabulary:
setEnableAutoPunctuation:
setExportedInterface:
setExportedObject:
setFieldLabel:
setForceOfflineRecognition:
setHeaderFields:
setInlineItemList:
setInteractionIdentifier:
setInterruptionHandler:
setInvalidationHandler:
setKeyboardIdentifier:
setKeyboardType:
setLanguage:
setMaximumRecognitionDuration:
setModelOverrideURL:
setNarrowband:
setObject:forKey:
setOnDeviceOnly:
setOriginalAudioFileURL:
setQueryParameters:
setQueue:
setRecognitionOverrides:
setRemoteObjectInterface:
setRequestIdentifier:
setSampleRateConverterQuality:
setShouldReportPartialResults:
setTask:
setTaskHint:
setTaskIdentifier:
setVoiceSearchHeaderFields:
setVoiceSearchQueryParameters:
setVoiceSearchTypeOptions:
setVoiceTriggerEventInfo:
setWithArray:
setWithCapacity:
setWithObject:
sharedInstance
shouldReportPartialResults
silenceStartTime
speakingRate
speechDuration
speechRecognitionFeatures
speechRecognitionMetadata
speechRecognitionTask:didFinishRecognition:
speechRecognitionTask:didFinishSuccessfully:
speechRecognitionTask:didHypothesizeTranscription:
speechRecognitionTask:didProcessAudioDuration:
speechRecognitionTask:didReceiveSearchResults:recognizedText:stable:final:
speechRecognitionTaskFinishedReadingAudio:
speechRecognitionTaskWasCancelled:
speechRecognizer:availabilityDidChange:
speechStartTimestamp
startReading
startRecordedAudioDictationWithOptions:forLanguage:
startRecordedAudioDictationWithOptions:forLanguage:narrowband:
startRecordedAudioDictationWithParameters:
startTime
state
statusCode
stopSpeech
stopSpeechWithOptions:
string
stringByAppendingFormat:
stringByAppendingString:
stringByReplacingCharactersInRange:withString:
stringByReplacingOccurrencesOfString:withString:
stringWithFormat:
stringWithString:
stringWithUTF8String:
subarrayWithRange:
substring
substringRange
superclass
supportedLocales
supportsOnDeviceRecognition
supportsSecureCoding
synchronousRemoteObjectProxyWithErrorHandler:
task
taskHint
taskIdentifier
taskNameFromTaskHint:
text
timestamp
tokens
tracksWithMediaType:
trainAppLmFromNgramCountsArtifact:language:writeToDirectory:modelOverride:completion:
trainAppLmFromNgramCountsArtifact:language:writeToDirectory:sandboxExtensions:completion:
trainAppLmFromNgramsSerializedData:customPronsData:language:writeToDirectory:sandboxExtension:completion:
trainAppLmFromNgramsSerializedDataFile:customPronsFile:language:writeToDirectory:modelOverride:completion:
trainFromPlainTextAndWriteToDirectory:
trainFromPlainTextAndWriteToDirectory:completion:
trainFromPlainTextAndWriteToDirectory:sandboxExtension:completion:
transcriptions
transcriptionsWithTokens:
unarchivedObjectOfClass:fromData:error:
utteranceStart
voiceAnalytics
wakeUpWithCompletion:
writeToURL:options:error:
zone
B16@0:8
@24@0:8^{_NSZone=}16
v24@0:8@16
@24@0:8@16
v24@0:8@"NSCoder"16
@24@0:8@"NSCoder"16
@32@0:8@16d24
@16@0:8
d16@0:8
v16@0:8
@"NSArray"
@48@0:8@16@24@32@40
@"SFAcousticFeature"
q16@0:8
@"NSHTTPURLResponse"
@"NSData"
B24@0:8@16
#16@0:8
@24@0:8:16
@32@0:8:16@24
@40@0:8:16@24@32
B24@0:8#16
B24@0:8:16
Vv16@0:8
Q16@0:8
^{_NSZone=}16@0:8
B24@0:8@"Protocol"16
@"NSString"16@0:8
v32@0:8@16@24
v40@0:8@16@24@32
v44@0:8@16@24@32B40
v48@0:8@16@24@32@40
v56@0:8@16@24@32@40Q48
v32@0:8@16d24
v48@0:8@16@24@32B40B44
v24@0:8@"AFDictationConnection"16
v32@0:8@"AFDictationConnection"16@"AFDictationOptions"24
v32@0:8@"AFDictationConnection"16@"NSString"24
v32@0:8@"AFDictationConnection"16@"NSError"24
v40@0:8@"AFDictationConnection"16@"NSString"24@"NSDictionary"32
v44@0:8@"AFDictationConnection"16@"NSString"24@"NSDictionary"32B40
v32@0:8@"AFDictationConnection"16@"SASMultilingualSpeechRecognized"24
v48@0:8@"AFDictationConnection"16@"NSArray"24@"NSString"32@40
v56@0:8@"AFDictationConnection"16@"NSArray"24@"NSString"32@40Q48
v40@0:8@"AFDictationConnection"16@"NSArray"24@"NSString"32
v48@0:8@"AFDictationConnection"16@"NSArray"24@"AFDictationNLUResult"32@"NSString"40
v32@0:8@"AFDictationConnection"16@"SASSpeechPartialResult"24
v32@0:8@"AFDictationConnection"16d24
v40@0:8@"AFDictationConnection"16@"NSFileHandle"24@"NSError"32
v48@0:8@"AFDictationConnection"16@"NSArray"24@"NSString"32B40B44
v32@0:8@"AFDictationConnection"16@"AFSpeechPackage"24
v40@0:8@"AFDictationConnection"16@"AFSpeechPackage"24@"AFDictationNLUResult"32
v24@0:8@"NSData"16
v24@0:8@"SFLocalSpeechRecognitionClient"16
v32@0:8@"SFLocalSpeechRecognitionClient"16@"NSError"24
v40@0:8@"SFLocalSpeechRecognitionClient"16@"SFTranscription"24@"SFTranscription"32
v32@0:8@"SFLocalSpeechRecognitionClient"16d24
v32@0:8@"SFLocalSpeechRecognitionClient"16@"SFSpeechRecognitionResult"24
@48@0:8@16@24@32q40
f16@0:8
@"AFDictationConnection"
@"SFLocalSpeechRecognitionClient"
@"NSOperationQueue"
@"NSString"
@"SFSpeechRecognitionRequest"
@"NSObject<OS_dispatch_queue>"
@"NSError"
@56@0:8@16@24@32q40@?48
@56@0:8@16@24@32q40@48
@"<_SFSpeechRecognitionTaskDelegatePrivate>"
@"SFSpeechRecognitionResult"
@"_SFSpeechRecognitionDelegateTask"
v20@0:8B16
v24@0:8d16
@"NSDictionary"
@"NSURL"
Vv24@0:8@16
Vv56@0:8@16@24@32@40@?48
Vv24@0:8@?16
Vv36@0:8@16B24B28B32
Vv32@0:8@16@?24
Vv40@0:8@16@24@?32
Vv64@0:8@16@24@32@40@48@?56
Vv48@0:8@16@24@32@?40
Vv24@0:8@"SFRequestParameters"16
Vv24@0:8@"NSData"16
Vv56@0:8@"NSString"16@"NSString"24@"NSDictionary"32@"NSURL"40@?<v@?@"NSError">48
Vv24@0:8@?<v@?@"NSSet">16
v24@0:8@"NSArray"16
v32@0:8@"NSString"16@"NSDictionary"24
Vv36@0:8@"NSString"16B24B28B32
Vv32@0:8@"NSString"16@?<v@?@"NSString"@"NSError">24
Vv32@0:8@"NSString"16@?<v@?@"NSDictionary">24
Vv32@0:8@"NSString"16@?<v@?@"NSError">24
Vv32@0:8@"NSLocale"16@?<v@?@"NSError">24
Vv24@0:8@"NSString"16
Vv32@0:8@"NSString"16@?<v@?@"NSArray">24
Vv40@0:8@"NSSet"16@"NSString"24@?<v@?B>32
Vv24@0:8@?<v@?@"NSDictionary">16
Vv40@0:8@"NSURL"16@"NSString"24@?<v@?@"NSURL"@"NSURL">32
Vv24@0:8@?<v@?@"NSData">16
Vv32@0:8@"NSData"16@?<v@?@"NSDictionary">24
Vv24@0:8@?<v@?q>16
Vv24@0:8@?<v@?>16
Vv64@0:8@"NSData"16@"NSData"24@"NSString"32@"NSURL"40@"NSString"48@?<v@?@"NSURL"@"NSURL">56
Vv64@0:8@"NSString"16@"NSURL"24@"NSURL"32@"NSURL"40@"NSArray"48@?<v@?@"NSURL">56
Vv48@0:8@"NSURL"16@"NSURL"24@"NSArray"32@?<v@?@"NSURL">40
Vv56@0:8@"NSURL"16@"NSString"24@"NSURL"32@"NSArray"40@?<v@?@"NSURL"@"NSURL">48
Vv32@0:8@16@24
Vv24@0:8d16
Vv28@0:8Q16B24
Vv32@0:8@"SFTranscription"16@"SFTranscription"24
Vv24@0:8@"SFSpeechRecognitionResult"16
Vv24@0:8@"NSError"16
Vv32@0:8@"NSString"16@"NSError"24
v64@0:8@16@24@32@40@48@?56
v56@0:8@16@24@32@40@?48
v32@0:8@16@?24
B32@0:8@16@24
@32@0:8@16@24
@"NSLocale"
@40@0:8@16@24d32
@40@0:8@"NSData"16@"NSDictionary"24d32
@"NSData"16@0:8
@44@0:8@16@24B32@36
@"SFTranscription"
@"SFSpeechRecognitionMetadata"
v24@0:8@?16
v32@0:8@"CXCallObserver"16@"CXCall"24
v32@0:8q16@?24
@32@0:8@16@?24
v24@0:8q16
@"CXCallObserver"
@"<NSObject>"
@"<SFSpeechRecognizerDelegate>"
@100@0:8@16{_NSRange=QQ}24d40d48f56@60@68@76@84@92
{_NSRange=QQ}16@0:8
@"SFVoiceAnalytics"
{_NSRange="location"Q"length"Q}
v28@0:8B16@?20
v48@0:8@16B24B28@?32@?40
v40@0:8@16@24@?32
v48@0:8@16@24@32@?40
@"NSXPCConnection"
@"<SFLocalSpeechRecognitionDelegate>"
@24@0:8q16
B40@0:8@16@24^@32
@32@0:8@16^@24
@48@0:8@16@24q32@40
@56@0:8@16@24q32@40@48
@"_SFSearchRequest"
v24@0:8^{opaqueCMSampleBuffer=}16
@"<SFSpeechRecognitionBufferDelegate>"
@"NSMutableArray"
@"AVAudioConverter"
@48@0:8@16@24d32d40
v40@0:8@16@?24@?32
v44@0:8@16B24@?28@?36
v32@0:8@16^@24
v36@0:8B16@20^@28
@56@0:8d16d24@32d40d48
mcpl
