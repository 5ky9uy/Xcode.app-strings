@(#)PROGRAM:RTCV-core-iOS  PROJECT:Vision-3.0.59
@(#)PROGRAM:RTCV-cvabm-iOS  PROJECT:Vision-3.0.59
@@(#)PROGRAM:RTCV-sim-iOS  PROJECT:Vision-3.0.59
@(#)PROGRAM:RTCV-taptotrack-cyprus-iOS  PROJECT:Vision-3.0.59
=ffffff
@(#)PROGRAM:TemporalRegistration-iOS  PROJECT:Vision-3.0.59
333333
f@333333
I@xwwwww
?xwwwww
333333
zd?333333
o@UUUUUU
@333333
?333?
Q8>ff
Cq=J?\
Dfff?
=33s?
<ff&?
hSq=
p}?33S?
 @33
B33XB
 BmQv9
Ga==
q=J?
FaceIDModel_v1_d
PFvPvE
NSt3__120__shared_ptr_pointerIPhPFvPvENS_9allocatorIhEEEE
NSt3__120__shared_ptr_pointerIPfPFvPvENS_9allocatorIfEEEE
N6vision3mod26ImageDescriptorBufferJointE
NSt3__115basic_stringbufIcNS_11char_traitsIcEENS_9allocatorIcEEEE
:D7V
"nN%
AA)Z
>Y1\
?*Ral!
WXp?
>J6h
iQ~V?
iN^d
iN^d
N} y
?9(a
g|_\
&4I,)
|F"4
_Cp\
vj.7
ip[[
'Hlw
?VF#
uoEb
?\='
@ut\
?'/2
h?RD
"nN%
t><K
!sePmp
|zlK
c\qq
0Xr
9x&4
^Cp\
`!sePm
?aobHN
.5B?S
i3NCT
"nN%
|a2U0
%Tpx
"nN%
~NA~
lscz
AA)Z
r?jl@
<e5]O
! _B
?`vO
^Cp\F
@Y32
d:tz
N} y
?Uka
SrNl
-:Yj
tBFe@
@!"5
O7+]3n@
h8en
;a@O
?!=E
"LQ.
OVW
"nN%
0Xr
W zR&
/5B?S
|a2U0*
?EUwv
E|'f
i3NCT
1zn!
"nN%
#gaO;|
[@h=|
2uWv
 |(
?K[\
NSt3__120__shared_ptr_emplaceIN6vision3mod15FaceFrontalizerENS_9allocatorIS3_EEEE
NSt3__120__shared_ptr_emplaceIN6vision3mod28ConcreteFaceQualityPredictorENS_9allocatorIS3_EEEE
N6vision3mod29ImageDescriptorBufferAbstractE
N6vision3mod24ObjectTrackerCorrelationE
N5apple6vision37GreedyClusteringHacksWrapperRevision1E
N5apple6vision28GreedyClusteringHacksWrapperE
N5apple6vision37GreedyClusteringHacksWrapperRevision2E
N5apple6vision45GreedyClusteringHacksWrapperRevision2ConcreteE
NSt3__120__shared_ptr_emplaceIN5apple6vision45GreedyClusteringHacksWrapperRevision2ConcreteENS_9allocatorIS3_EEEE
N5apple6vision45GreedyClusteringHacksWrapperRevision1ConcreteE
NSt3__120__shared_ptr_emplaceIN6vision3mod25GreedyClustererHacks_rev1ENS_9allocatorIS3_EEEE
NSt3__120__shared_ptr_emplaceIN5apple6vision45GreedyClusteringHacksWrapperRevision1ConcreteENS_9allocatorIS3_EEEE
NSt3__114default_deleteIN6vision3mod28ImageDescriptorBufferFloat32EEE
NSt3__120__shared_ptr_pointerIPN6vision3mod28ImageDescriptorBufferFloat32ENS_14default_deleteIS3_EENS_9allocatorIS3_EEEE
N6vision3mod5ERT2DIfEE
NSt3__114default_deleteIA_fEE
NSt3__120__shared_ptr_pointerIPfNS_14default_deleteIA_fEENS_9allocatorIfEEEE
N6vision3mod23Transformation2DPrivateI16_Geometry2D_RST_EE
N6vision3mod23Transformation2DPrivateI19_Geometry2D_Affine_EE
N6vision3mod16Transformation2DE
NSt3__120__shared_ptr_emplaceIN6vision3mod33ObjectDetector_DCNFaceDetector_v2ENS_9allocatorIS3_EEEE
N6vision3mod23Transformation2DPrivateIA9_fEE
>333>
6>%I
N6vision3mod32ImageDescriptorProcessorAbstractE
N6vision3mod23ImageClassifierAbstractE
N6vision3mod23ImageClassifierEspressoE
NSt3__120__shared_ptr_emplaceIN6vision3mod23ImageClassifierEspresso9private_tENS_9allocatorIS4_EEEE
NSt3__120__shared_ptr_emplaceIN6vision3mod18LandmarkAttributesENS_9allocatorIS3_EEEE
ZN4cvml4util31binserialized_table_of_contents9blob_infoC1EPKhRK26_BinSerializer_blobHeader_EUlS4_E_
NSt3__120__shared_ptr_pointerIPKhZN4cvml4util31binserialized_table_of_contents9blob_infoC1ES2_RK26_BinSerializer_blobHeader_EUlS2_E_NS_9allocatorIS1_EEEE
NSt3__114default_deleteIN6vision3mod21ObjectTrackerAbstractEEE
NSt3__120__shared_ptr_pointerIPN6vision3mod21ObjectTrackerAbstractENS_14default_deleteIS3_EENS_9allocatorIS3_EEEE
N6vision3mod33ObjectDetector_DCNFaceDetector_v2E
N6vision3mod22ObjectDetectorAbstractE
NSt3__120__shared_ptr_emplaceIN6vision3mod33ObjectDetector_DCNFaceDetector_v24privENS_9allocatorIS4_EEEE
N6vision3mod29ImageDescriptor_EspressoSceneE
NSt3__120__shared_ptr_emplaceIN6vision3mod29ImageDescriptor_EspressoSceneENS_9allocatorIS3_EEEE
NSt3__120__shared_ptr_emplaceIN6vision3mod30ObjectDetector_DCNFaceDetectorENS_9allocatorIS3_EEEE
N6vision3mod28ImageDescriptor_EspressoFaceE
NSt3__120__shared_ptr_emplaceIN6vision3mod28ImageDescriptor_EspressoFaceENS_9allocatorIS3_EEEE
N6vision3mod28ImageDescriptor_EspressoJunkE
NSt3__120__shared_ptr_emplaceIN6vision3mod28ImageDescriptor_EspressoJunkENS_9allocatorIS3_EEEE
NSt3__114default_deleteIN6vision3mod18LandmarkAttributesEEE
NSt3__120__shared_ptr_pointerIPN6vision3mod18LandmarkAttributesENS_14default_deleteIS3_EENS_9allocatorIS3_EEEE
f024800L
NSt3__113basic_filebufIcNS_11char_traitsIcEEEE
NSt3__114basic_ifstreamIcNS_11char_traitsIcEEEE
N6vision3mod28ImageDescriptorBufferFloat32E
NSt3__120__shared_ptr_emplaceIN6vision3mod28ImageDescriptorBufferFloat32ENS_9allocatorIS3_EEEE
NSt3__120__shared_ptr_emplaceIN6vision3mod16FaceSegmenterDNNENS_9allocatorIS3_EEEE
NSt3__121__empty_non_own_stateIcEE
NSt3__115__has_one_stateIcEE
NSt3__16__nodeIcEE
NSt3__117__owns_two_statesIcEE
NSt3__116__owns_one_stateIcEE
NSt3__111__alternateIcEE
NSt3__117__repeat_one_loopIcEE
NSt3__16__loopIcEE
NSt3__110__l_anchorIcEE
NSt3__110__r_anchorIcEE
NSt3__126__end_marked_subexpressionIcEE
NSt3__128__begin_marked_subexpressionIcEE
NSt3__112__match_charIcEE
NSt3__120__match_char_collateIcNS_12regex_traitsIcEEEE
NSt3__118__match_char_icaseIcNS_12regex_traitsIcEEEE
NSt3__120__bracket_expressionIcNS_12regex_traitsIcEEEE
NSt3__111__match_anyIcEE
NSt3__110__back_refIcEE
NSt3__118__back_ref_collateIcNS_12regex_traitsIcEEEE
NSt3__116__back_ref_icaseIcNS_12regex_traitsIcEEEE
NSt3__123__match_any_but_newlineIcEE
NSt3__111__lookaheadIcNS_12regex_traitsIcEEEE
NSt3__115__word_boundaryIcNS_12regex_traitsIcEEEE
NSt3__114default_deleteINS_13__empty_stateIcEEEE
NSt3__120__shared_ptr_pointerIPNS_13__empty_stateIcEENS_14default_deleteIS2_EENS_9allocatorIS2_EEEE
NSt3__113__empty_stateIcEE
NSt3__111__end_stateIcEE
NSt3__120__shared_ptr_emplaceIN6vision3mod33ImageClassifier_HierarchicalModelENS_9allocatorIS3_EEEE
?n'O
%5>q
f|:>
?T9W=
>tM>9!
v%?-"
=NSt3__114default_deleteIN6vision3mod20GreedyClustererHacksEEE
NSt3__120__shared_ptr_pointerIPN6vision3mod20GreedyClustererHacksENS_14default_deleteIS3_EENS_9allocatorIS3_EEEE
NSt3__114default_deleteIN4cvml4util12BackedBufferINS2_15RAMBackingStoreEEEEE
NSt3__120__shared_ptr_pointerIPN4cvml4util12BackedBufferINS2_15RAMBackingStoreEEENS_14default_deleteIS5_EENS_9allocatorIS5_EEEE
NSt3__114default_deleteIN4cvml4util15RAMBackingStoreEEE
NSt3__120__shared_ptr_pointerIPN4cvml4util15RAMBackingStoreENS_14default_deleteIS3_EENS_9allocatorIS3_EEEE
NSt3__114default_deleteIKN4cvml4util12BackedBufferINS2_15RAMBackingStoreEEEEE
NSt3__120__shared_ptr_pointerIPKN4cvml4util12BackedBufferINS2_15RAMBackingStoreEEENS_14default_deleteIS6_EENS_9allocatorIS6_EEEE
N6vision3mod24GreedyClustererWithTorsoE
NSt3__120__shared_ptr_emplaceINS_6vectorINS_5tupleIJjjfEEENS_9allocatorIS3_EEEENS4_IS6_EEEE
NSt3__120__shared_ptr_emplaceINS_6vectorIhNS_9allocatorIhEEEENS2_IS4_EEEE
NSt3__120__shared_ptr_emplaceINS_6vectorIjNS_9allocatorIjEEEENS2_IS4_EEEE
N6vision3mod29GreedyClustererFacesWithTorsoE
N6vision3mod14FaceClusteringE
NSt3__120__shared_ptr_emplaceIN6vision3mod24GreedyClustererWithTorso9private_tENS_9allocatorIS4_EEEE
NSt3__120__shared_ptr_emplaceIN6vision3mod29GreedyClustererFacesWithTorsoENS_9allocatorIS3_EEEE
N6vision3mod32ImageDescriptorProcessorEspressoE
NSt3__120__shared_ptr_emplaceIN6vision3mod32ImageDescriptorProcessorEspresso9private_tENS_9allocatorIS4_EEEE
NSt3__120__shared_ptr_emplaceIN6vision3mod13FaceRegionMapENS_9allocatorIS3_EEEE
~|zxvuusqpnmkjiggfecba`_^]]\[ZYXWVUTTSRQQPONNMMLKKJIIHGGGFFEDDCCBBBAA@@??>>===<<;;:::999888776666555444333322211110000////.....----,,,,+++++*****)))))((((('''''''&&&&&&%%%%%%$$$$$$$########""""""!!!!!!!!!        X|E
+>>%I
>N6vision3mod30ObjectDetector_DCNFaceDetectorE
NSt3__120__shared_ptr_emplaceIN6vision3mod30ObjectDetector_DCNFaceDetector4privENS_9allocatorIS4_EEEE
N6vision3mod15ObjectTrackerExE
yA$H
ITCA
vA,2
q@,2
5mAv<
9Bh6
DNSt3__120__shared_ptr_emplaceIN6vision3mod11FaceIDModelENS_9allocatorIS3_EEEE
NSt3__120__shared_ptr_emplaceIN6vision3mod22ImageClassifierGlimmerENS_9allocatorIS3_EEEE
NSt3__117bad_function_callE
ZN6vision3mod12broadcastAddIdLm16EEEvRKNS0_10CVMLMatrixIT_XT0_EEERKNS0_10CVMLVectorIS3_XT0_EEEiRS4_bEUlddE_
NSt3__110__function6__funcIZN6vision3mod12broadcastAddIdLm16EEEvRKNS3_10CVMLMatrixIT_XT0_EEERKNS3_10CVMLVectorIS6_XT0_EEEiRS7_bEUlddE_NS_9allocatorISF_EEFdddEEE
NSt3__110__function6__baseIFdddEEE
ZN6vision3mod14broadcastMinusIdLm16EEEvRKNS0_10CVMLMatrixIT_XT0_EEERKNS0_10CVMLVectorIS3_XT0_EEEiRS4_bEUlddE_
NSt3__110__function6__funcIZN6vision3mod14broadcastMinusIdLm16EEEvRKNS3_10CVMLMatrixIT_XT0_EEERKNS3_10CVMLVectorIS6_XT0_EEEiRS7_bEUlddE_NS_9allocatorISF_EEFdddEEE
N6vision3mod32FeatureSignSparseCoder_bad_allocE
N6vision3mod31ColorGaborImageDescriptorBufferE
NSt3__120__shared_ptr_emplaceIN6vision3mod31ColorGaborImageDescriptorBufferENS_9allocatorIS3_EEEE
N6vision3mod34ColorGaborImageDescriptorProcessorE
NSt3__16__bindIRFxRKNS_3mapINS_12basic_stringIcNS_11char_traitsIcEENS_9allocatorIcEEEENS_6vectorIfNS5_IfEEEENS_4lessIS7_EENS5_INS_4pairIKS7_SA_EEEEEERKSA_RSA_EJRKNS_12placeholders4__phILi1EEERKNSQ_ILi2EEERKNSQ_ILi3EEEEEE
NSt3__118__weak_result_typeIPFxRKNS_3mapINS_12basic_stringIcNS_11char_traitsIcEENS_9allocatorIcEEEENS_6vectorIfNS5_IfEEEENS_4lessIS7_EENS5_INS_4pairIKS7_SA_EEEEEERKSA_RSA_EEE
NSt3__110__function6__funcINS_6__bindIRFxRKNS_3mapINS_12basic_stringIcNS_11char_traitsIcEENS_9allocatorIcEEEENS_6vectorIfNS7_IfEEEENS_4lessIS9_EENS7_INS_4pairIKS9_SC_EEEEEERKSC_RSC_EJRKNS_12placeholders4__phILi1EEERKNSS_ILi2EEERKNSS_ILi3EEEEEENS7_IS12_EESP_EE
NSt3__110__function6__baseIFxRKNS_3mapINS_12basic_stringIcNS_11char_traitsIcEENS_9allocatorIcEEEENS_6vectorIfNS6_IfEEEENS_4lessIS8_EENS6_INS_4pairIKS8_SB_EEEEEERKSB_RSB_EEE
NSt3__16__bindIRFmmEJRKNS_12placeholders4__phILi1EEEEEE
NSt3__118__weak_result_typeIPFmmEEE
NSt3__114unary_functionImmEE
NSt3__110__function6__funcINS_6__bindIRFmmEJRKNS_12placeholders4__phILi1EEEEEENS_9allocatorISA_EES3_EE
NSt3__110__function6__baseIFmmEEE
NSt3__16__bindIRFxP7__sFILEPKcRNS_3mapINS_12basic_stringIcNS_11char_traitsIcEENS_9allocatorIcEEEENS_6vectorIfNS9_IfEEEENS_4lessISB_EENS9_INS_4pairIKSB_SE_EEEEEEEJRKNS_12placeholders4__phILi1EEERKNSQ_ILi2EEERKNSQ_ILi3EEEEEE
NSt3__118__weak_result_typeIPFxP7__sFILEPKcRNS_3mapINS_12basic_stringIcNS_11char_traitsIcEENS_9allocatorIcEEEENS_6vectorIfNS9_IfEEEENS_4lessISB_EENS9_INS_4pairIKSB_SE_EEEEEEEEE
NSt3__110__function6__funcINS_6__bindIRFxP7__sFILEPKcRNS_3mapINS_12basic_stringIcNS_11char_traitsIcEENS_9allocatorIcEEEENS_6vectorIfNSB_IfEEEENS_4lessISD_EENSB_INS_4pairIKSD_SG_EEEEEEEJRKNS_12placeholders4__phILi1EEERKNSS_ILi2EEERKNSS_ILi3EEEEEENSB_IS12_EESP_EE
NSt3__110__function6__baseIFxP7__sFILEPKcRNS_3mapINS_12basic_stringIcNS_11char_traitsIcEENS_9allocatorIcEEEENS_6vectorIfNSA_IfEEEENS_4lessISC_EENSA_INS_4pairIKSC_SF_EEEEEEEEE
NSt3__119basic_ostringstreamIcNS_11char_traitsIcEENS_9allocatorIcEEEE
N6vision3mod12_GLOBAL__N_119BoxAlignerExceptionE
?2w-!
?O#-
?o/i
?`vO
?+0du
_{fI
tYLl>
?A+0du
++MJ
?S?o*Ra
?G=D
? <
?S\U
|a2U
?G8-x
??5^
?o*Ral!
tYLl
?gDio
?h?RD
?|DL
?pB!
FZ*o
?@M-[
#bJ$Q
?p%;6
FZ*oG
?+5{
?%]3
_{fI
?`vO
h"lx
?%]3
?U0*
?pB!
?Mg'
?a2U0*
o%;6
?,+MJA
?Ral!
?o*Ral!
?8J^
?Dio
?:#J{
?q $
$#ga
?{Ic
?scz
?o*Ral!
?U0*
?S?o*Ra
?Dio
8b->
?o/i
R?o*
?G8-x
?G8-x
?2ZGU
?Z*oG8-
?MJA
R?o*R
?`vO
_{fI
FZ*o
$#gaO
?{fI
?N6vision3mod19LandmarkDetectorERTE
NSt3__120__shared_ptr_emplaceIN6vision3mod19LandmarkDetectorERTENS_9allocatorIS3_EEEE
NSt3__120__shared_ptr_emplaceIN4cvml4util31binserialized_table_of_contentsENS_9allocatorIS3_EEEE
Q9RI
Q9RI
7:RI
Q9RI
d*;4
>;_)K;
DX;B`e;
{r;l
k;B`e;
Q;_)K;
#;RI
7:RI
Q9RI
d*;4
7;_)K;
DX;B`e;
{r;o
{r;B`e;
DX;_)K;4
d*;RI
7:RI
Q9RI
7;_)K;
y;KY
3"<U
-<X94<
b<B`e<
4o<E
{r<!
ew<l
{r<E
h<B`e<e
#9<X94<
3"<RI
^;_)K;4
7:RI
Q9RI
k;KY
Q:RI
Q9RI
k:RI
DX;$
d*<4
?F<4
DX;4
Q:RI
>;B`e;'
#<X94<
z<KY
$=yX(=V
7=5^:=[
2D=]mE=
?F=]mE=
2D=\
<=5^:=
-2=2
+=yX(=
D<X94<
;B`e;
>;RI
{r<&
=R' =T
b=B`e=B>h=h
4o= Aq=
{r=F
{r= Aq=
(m=h
j=B>h=B`e=e
WJ=9
E6=|
%=R' =Qk
k:RI
7;B`e;
?$=V
b=B>h=D
s=Gry=
\~=n
\~=Gry=F
m=B>h=e
?$=v
;B`e;4
DX;'
3"<4
=R' =U
(=W[1=Z
B=_)K=a
S=d]\=
d=d]\=a
S=_)K=\
9=W[1=U
(=R' =P
j<<b
ew=I
{r<b
j<<>
ew<M
ew<>
=yX(=
A=;pN=
h=k+v=n
=k+v=
[=;pN=
5=yX(=
ew<b
7:RI
{r<M
7x=J{
/>:#
O/<N
=yX(=Y
U=B`e=
9#>/
$>0L&>
d*>1
'>0L&>/
Yu=B`e=
?F=Y
7=yX(=u
7;KY
=z6+=
/;=:
K=d]\=D
m=I.
V,>i
9>lx:>
<>6<=>
=>6<=>
;>lx:>l
u1>i
V,>
m=d]\=:
/;=z6+=
;_)K;
{r<)\
q,=6<==
Ga=F
s=J{
u >/
!N>`vO>
|P>`vO>
2D>7
~;>~
N=6<==
;_)K;o
<X94<
=z6+=6<==
ew=o
G!>ff&>
+>io0>F%5>
9>[B>>%uB>
qJ>_
QZ>?
+e>x
F>%uB>[B>>
9>F%5>io0>
+>ff&>
P=6<==z6+=u
/]<X94<r
Q9RI
{r<N
=yX(=
x=p_
/>aT
X>RI]>
Ga>f
n>W[q>}
=y>lxz>Zd{>
|>HP|>HP|>
|>Zd{>lxz>
s>W[q>
rh>f
Ga>RI]>P
/>o
/;=yX(=
y;RI
d*;KY
d*<b
ew=p_
2>Gr9>[
sW>v
8g>1
|>n4
3b>v
?>Gr9>
%>d;
Ga=:
d*<o
?F=d]\=F
8V>?
c>gDi>
t>5^z>$
>5^z>
4o>gDi>
s=d]\=
<;p
:_)K;RI
=yX(=
m=J{
P>>yX>
_>>yX>
kI>n
v>=yX(=
;_)K;
T<KY
L=B`e=I.
..>}
6>I.?>
h>iop>~
x>iop>
DX>r
G>I.?>}
c>]
=B`e=
;B`e;X9
c<)\
>B>(>W[1>lx:>
C>lx:>W[1>B>(>-!
;B`e;o
5=;pN=
>R' >z
!=>]
|P>]
g3>z
)>R' >b
xi=;pN=
{r;'
>R' >
m4>m
ZS>-
-r>HP|>
>HP|>
J*>R' >
?F<N
h=J{
H?>:#J>+
U>:#J>
m4>z
)>-!
=R' =
sW=k+v=(
>B>(>
g3>m
>>:#J>=
U>w-a>
l>w-a>=
U>:#J>m
g3>B>(>
=k+v=
:=R' =
#<B`
d*;u
Sc=n
%>W[1>
U>w-a>
\m>~
\m>w-a>+
!=>W[1>
Jj<1
d*;RI
=W[1=
..>lx:>]
ZS>]
F>lx:>
AO=W[1=
5<HP
7;RI
|P>-
:_)K;|
?$=\
Sc=o
/>>y
2>I.?>_
6Z>_
L>I.?>
%>>y
/>n4
Sc=\
;_)K;4
+=_)K=
(m=p
q,>#
q?_
q?h
q,>d;
(m=_)K=V
ew=r
\m>HP|>
c?r
>HP|>
%>>y
ew=a
2=tF
:B`e;
9=d]\=I
=d]\=Z
9=Qk
u`<RI
;B`e;
=R' =
d=&S
:p>7
R&?t
@=R' =
l=q=
~*>Gr9>p
H>>yX>
f%?=
+?-C,?
,?-C,?
h>>yX>p
H>Gr9>
{r<U
:0>[
_>iop>I
;.?{
;.?$
@"?$
>iop>
G!><
;X94<
fU=$
>ff&>
y'?u
c,?r
0?EG2?}
7?^K8?Y
8?^K8?
+5?}
3?EG2?
c,?u
y'?j
8V>9
5>ff&>
fU=|
<X94<B`
?n4 ?
,?I./?
1?I./?D
#?n4 ?
E6=tF
u >io0>
t>J{
$?B>(?
1?X94?
6?X94?
+?B>(?
@>io0>
@=B>h=
$>F%5>
sW>gDi>
V?E
%?gD)?
,?R'0?
S3?0L6?
>?R'@?
8E?a
B?R'@?_
9?0L6?
S3?R'0?
,?gD)?"
V?0
~{>gDi>
F>F%5>/
=B>h=
B<HP
41?4
=?io@?
H?q=J?
~K?(~L?6<M?H
M?6<M?(~L?
~K?q=J?U
B?io@?d
41?h
4o>v
WJ=F
s=Nb
->[B>>r
4!?"
1?]m5?
I<?Di??aTB?
O?2UP?
aQ?2UP?
E?aTB?Di??
8?]m5?
3b>r
O>[B>>V
=yX(=
N=Gry=*
0>%uB>
8g>5^z>
?n4 ?
$?gD)?h
F?GrI?
HN?2UP?
&R?F
V?p_W?
W?p_W?
&R?2UP?
K?GrI?}
-?gD)?
$?n4 ?
>5^z>
T>%uB>|
=Gry=
N=yX(=
Q<KY
S#>4
#?B>(?
41?]m5?c
A?o
X?GrY?
Z??W[?
[?h"\?h"\?
[??W[?
Z?GrY?~
\=?c
9?]m5?
,?B>(?
S#><
ZS=V
Y<RI
sW=n
qJ>RI]>
+?R'0?4
D?yXH?q
Q?tFT?}
Z?HP\?
_?R'`?
h`?R'`?$
]?HP\?h
V?tFT?n
K?yXH?
4?R'0?
p>RI]>
qJ>~
sW=2
<RI
~;>_
|@?o
D?yXH?
UO?A
R?]mU?~
c?=,d?Ttd?Ttd?=,d?
X?]mU?A
K?yXH?o
Ga>_
~;>
y'?D
1?0L6?
:?Di??
UO?X
.^?Nb`?aTb?
d?]me?
eg?+
Dh?+
f?]me?
d?aTb?Nb`?
UO?q
C?Di??
:?0L6?
V,>,
<B`e<
b='1
rh>[
*?I./?X94?
=?aTB?}
U?gDY?
I\?gDY?
F?aTB?d
9?X94?I./?u
rh>O
/>G
<B`e<
=5^:=B`e=q=
k>n4
;?io@?
E?GrI?
Q?]mU?
&b?O
(l?mVm??5n?
n??5n?mVm?
X?]mU?n
M?GrI?
E?io@?
=B`e=5^:=
<=B>h=
q?aT
:P?tFT?~
m?-!o?
Np?-!o?
X?tFT?
a!>r
=B>h=[
4o<z6
9#>X
\>W[q>
f%?
;?R'@?
g?U0j?
n?2Up?N
q?2Up?
l?U0j?
E?R'@?C
>W[q>?
(m=M
G!?=
,?EG2?
K?2UP?O
\?Nb`?}
?t?]mu?KYv?
v?KYv?]mu?
?t?<
c?Nb`?$
T?2UP?
7?EG2?$
G!?Zd
{r<V}
>0L&>l
;.?}
&R?+
^?aTb?
t?KYv?
w?>yx?
y?,ey?,ey?
y?>yx?
w?KYv?
e?aTb?
J9?}
9>0L&>tF
4o=\
B=Qk
2D= Aq=
'>lx:>
!N>x
8E?q=J?
X?HP\?
m?2Up?<
v?'1x?
ky?'1x?
r?2Up?
H`?HP\?~
O?q=J?
+5?{
!N>lx:>
= Aq=
t<RI
=]mE=
;>`vO>x
~K?2UP?
U?GrY?
a?]me?5
(l?-!o?N
?t?KYv?'1x?^
{?(~|?
|?(~|?
y?'1x?KYv?
?t?N
q?-!o?
(l?5
h?]me?
]?GrY?
U?2UP?
=y>x
d>`vO>
{r=]mE=
u<RI
ew<3
?F=F
+e>lxz>B>
eG?(~L?
aQ?4
j?mVm?
Np?S
s?]mu?
/|?q
w?]mu?S
Np?mVm?
aQ?(~L?
>lxz>
^)>=
?F=RI
)>6<=>
e>Zd{>9
R&?-C,?
$H?6<M?
V??W[?$
eg?c
j??5n?
s?KYv?>yx?
Wz?>yx?KYv?
4q??5n?c
_??W[?
&R?6<M?
2?-C,?
A ?:#
>Zd{>
4Q>6<=>
Yu=*
2?^K8?
R?p_W?
[?R'`?=,d?+
z?(~|?H
}?(~|?
g?=,d?R'`?
[?p_W?
=?^K8?\
d*>+
f>HP|>L7
W?h"\?
h`?Ttd?
Dh?6
>w?,ey?
C{?,ey?
Dh?Ttd?
h`?h"\?
!>?Y
>HP|>B
 <X9
f>HP|>L7
W?h"\?
h`?Ttd?
Dh?6
>w?,ey?
C{?,ey?
Dh?Ttd?
h`?h"\?
!>?Y
>HP|>B
 <X9
Yu=*
2?^K8?
R?p_W?
[?R'`?=,d?+
z?(~|?H
}?(~|?
g?=,d?R'`?
[?p_W?
=?^K8?\
d*>+
)>6<=>
e>Zd{>9
R&?-C,?
$H?6<M?
V??W[?$
eg?c
j??5n?
s?KYv?>yx?
Wz?>yx?KYv?
4q??5n?c
_??W[?
&R?6<M?
2?-C,?
A ?:#
>Zd{>
4Q>6<=>
ew<3
?F=F
+e>lxz>B>
eG?(~L?
aQ?4
j?mVm?
Np?S
s?]mu?
/|?q
w?]mu?S
Np?mVm?
aQ?(~L?
>lxz>
^)>=
?F=RI
=]mE=
;>`vO>x
~K?2UP?
U?GrY?
a?]me?5
(l?-!o?N
?t?KYv?'1x?^
{?(~|?
|?(~|?
y?'1x?KYv?
?t?N
q?-!o?
(l?5
h?]me?
]?GrY?
U?2UP?
=y>x
d>`vO>
{r=]mE=
u<RI
2D= Aq=
'>lx:>
!N>x
8E?q=J?
X?HP\?
m?2Up?<
v?'1x?
ky?'1x?
r?2Up?
H`?HP\?~
O?q=J?
+5?{
!N>lx:>
= Aq=
t<RI
{r<V}
>0L&>l
;.?}
&R?+
^?aTb?
t?KYv?
w?>yx?
y?,ey?,ey?
y?>yx?
w?KYv?
e?aTb?
J9?}
9>0L&>tF
4o=\
B=Qk
(m=M
G!?=
,?EG2?
K?2UP?O
\?Nb`?}
?t?]mu?KYv?
v?KYv?]mu?
?t?<
c?Nb`?$
T?2UP?
7?EG2?$
G!?Zd
4o<z6
9#>X
\>W[q>
f%?
;?R'@?
g?U0j?
n?2Up?N
q?2Up?
l?U0j?
E?R'@?C
>W[q>?
<=B>h=
q?aT
:P?tFT?~
m?-!o?
Np?-!o?
X?tFT?
a!>r
=B>h=[
=5^:=B`e=q=
k>n4
;?io@?
E?GrI?
Q?]mU?
&b?O
(l?mVm??5n?
n??5n?mVm?
X?]mU?n
M?GrI?
E?io@?
=B`e=5^:=
<B`e<
b='1
rh>[
*?I./?X94?
=?aTB?}
U?gDY?
I\?gDY?
F?aTB?d
9?X94?I./?u
rh>O
/>G
<B`e<
y'?D
1?0L6?
:?Di??
UO?X
.^?Nb`?aTb?
d?]me?
eg?+
Dh?+
f?]me?
d?aTb?Nb`?
UO?q
C?Di??
:?0L6?
V,>,
~;>_
|@?o
D?yXH?
UO?A
R?]mU?~
c?=,d?Ttd?Ttd?=,d?
X?]mU?A
K?yXH?o
Ga>_
~;>
Y<RI
sW=n
qJ>RI]>
+?R'0?4
D?yXH?q
Q?tFT?}
Z?HP\?
_?R'`?
h`?R'`?$
]?HP\?h
V?tFT?n
K?yXH?
4?R'0?
p>RI]>
qJ>~
sW=2
<RI
S#>4
#?B>(?
41?]m5?c
A?o
X?GrY?
Z??W[?
[?h"\?h"\?
[??W[?
Z?GrY?~
\=?c
9?]m5?
,?B>(?
S#><
ZS=V
=yX(=
N=Gry=*
0>%uB>
8g>5^z>
?n4 ?
$?gD)?h
F?GrI?
HN?2UP?
&R?F
V?p_W?
W?p_W?
&R?2UP?
K?GrI?}
-?gD)?
$?n4 ?
>5^z>
T>%uB>|
=Gry=
N=yX(=
Q<KY
WJ=F
s=Nb
->[B>>r
4!?"
1?]m5?
I<?Di??aTB?
O?2UP?
aQ?2UP?
E?aTB?Di??
8?]m5?
3b>r
O>[B>>V
41?4
=?io@?
H?q=J?
~K?(~L?6<M?H
M?6<M?(~L?
~K?q=J?U
B?io@?d
41?h
4o>v
@=B>h=
$>F%5>
sW>gDi>
V?E
%?gD)?
,?R'0?
S3?0L6?
>?R'@?
8E?a
B?R'@?_
9?0L6?
S3?R'0?
,?gD)?"
V?0
~{>gDi>
F>F%5>/
=B>h=
B<HP
u >io0>
t>J{
$?B>(?
1?X94?
6?X94?
+?B>(?
@>io0>
?n4 ?
,?I./?
1?I./?D
#?n4 ?
E6=tF
;X94<
fU=$
>ff&>
y'?u
c,?r
0?EG2?}
7?^K8?Y
8?^K8?
+5?}
3?EG2?
c,?u
y'?j
8V>9
5>ff&>
fU=|
<X94<B`
:0>[
_>iop>I
;.?{
;.?$
@"?$
>iop>
G!><
l=q=
~*>Gr9>p
H>>yX>
f%?=
+?-C,?
,?-C,?
h>>yX>p
H>Gr9>
{r<U
=R' =
d=&S
:p>7
R&?t
@=R' =
:B`e;
9=d]\=I
=d]\=Z
9=Qk
u`<RI
;B`e;
ew=r
\m>HP|>
c?r
>HP|>
%>>y
ew=a
2=tF
+=_)K=
(m=p
q,>#
q?_
q?h
q,>d;
(m=_)K=V
:_)K;|
?$=\
Sc=o
/>>y
2>I.?>_
6Z>_
L>I.?>
%>>y
/>n4
Sc=\
;_)K;4
|P>-
=W[1=
..>lx:>]
ZS>]
F>lx:>
AO=W[1=
5<HP
7;RI
d*;u
Sc=n
%>W[1>
U>w-a>
\m>~
\m>w-a>+
!=>W[1>
Jj<1
d*;RI
=R' =
sW=k+v=(
>B>(>
g3>m
>>:#J>=
U>w-a>
l>w-a>=
U>:#J>m
g3>B>(>
=k+v=
:=R' =
#<B`
h=J{
H?>:#J>+
U>:#J>
m4>z
)>-!
>R' >
m4>m
ZS>-
-r>HP|>
>HP|>
J*>R' >
?F<N
5=;pN=
>R' >z
!=>]
|P>]
g3>z
)>R' >b
xi=;pN=
{r;'
;B`e;X9
c<)\
>B>(>W[1>lx:>
C>lx:>W[1>B>(>-!
;B`e;o
T<KY
L=B`e=I.
..>}
6>I.?>
h>iop>~
x>iop>
DX>r
G>I.?>}
c>]
=B`e=
:_)K;RI
=yX(=
m=J{
P>>yX>
_>>yX>
kI>n
v>=yX(=
;_)K;
?F=d]\=F
8V>?
c>gDi>
t>5^z>$
>5^z>
4o>gDi>
s=d]\=
<;p
d*;KY
d*<b
ew=p_
2>Gr9>[
sW>v
8g>1
|>n4
3b>v
?>Gr9>
%>d;
Ga=:
d*<o
Q9RI
{r<N
=yX(=
x=p_
/>aT
X>RI]>
Ga>f
n>W[q>}
=y>lxz>Zd{>
|>HP|>HP|>
|>Zd{>lxz>
s>W[q>
rh>f
Ga>RI]>P
/>o
/;=yX(=
y;RI
<X94<
=z6+=6<==
ew=o
G!>ff&>
+>io0>F%5>
9>[B>>%uB>
qJ>_
QZ>?
+e>x
F>%uB>[B>>
9>F%5>io0>
+>ff&>
P=6<==z6+=u
/]<X94<r
;_)K;
{r<)\
q,=6<==
Ga=F
s=J{
u >/
!N>`vO>
|P>`vO>
2D>7
~;>~
N=6<==
;_)K;o
7;KY
=z6+=
/;=:
K=d]\=D
m=I.
V,>i
9>lx:>
<>6<=>
=>6<=>
;>lx:>l
u1>i
V,>
m=d]\=:
/;=z6+=
=yX(=Y
U=B`e=
9#>/
$>0L&>
d*>1
'>0L&>/
Yu=B`e=
?F=Y
7=yX(=u
{r<M
7x=J{
/>:#
O/<N
=yX(=
A=;pN=
h=k+v=n
=k+v=
[=;pN=
5=yX(=
ew<b
7:RI
j<<>
ew<M
ew<>
j<<b
ew=I
{r<b
DX;'
3"<4
=R' =U
(=W[1=Z
B=_)K=a
S=d]\=
d=d]\=a
S=_)K=\
9=W[1=U
(=R' =P
7;B`e;
?$=V
b=B>h=D
s=Gry=
\~=n
\~=Gry=F
m=B>h=e
?$=v
;B`e;4
{r<&
=R' =T
b=B`e=B>h=h
4o= Aq=
{r=F
{r= Aq=
(m=h
j=B>h=B`e=e
WJ=9
E6=|
%=R' =Qk
k:RI
>;B`e;'
#<X94<
z<KY
$=yX(=V
7=5^:=[
2D=]mE=
?F=]mE=
2D=\
<=5^:=
-2=2
+=yX(=
D<X94<
;B`e;
>;RI
DX;$
d*<4
?F<4
DX;4
Q:RI
Q9RI
k:RI
Q9RI
k;KY
Q:RI
Q9RI
7;_)K;
y;KY
3"<U
-<X94<
b<B`e<
4o<E
{r<!
ew<l
{r<E
h<B`e<e
#9<X94<
3"<RI
^;_)K;4
7:RI
Q9RI
d*;4
7;_)K;
DX;B`e;
{r;o
{r;B`e;
DX;_)K;4
d*;RI
7:RI
Q9RI
d*;4
>;_)K;
DX;B`e;
{r;l
k;B`e;
Q;_)K;
#;RI
7:RI
Q9RI
7:RI
Q9RI
N6vision3mod32ImageDescriptorAugmenterAbstractE
N6vision3mod28ImageDescriptorAugmenterFlipE
N6vision3mod22ImageClassifierGlimmerE
NSt3__120__shared_ptr_emplaceIN6vision3mod22ImageClassifierGlimmer9private_tENS_9allocatorIS4_EEEE
N6vision3mod20ObjectTrackerOptionsE
N6vision3mod21ObjectTrackerAbstractE
NSt3__114default_deleteIN6vision3mod20ObjectTrackerOptionsEEE
NSt3__120__shared_ptr_pointerIPN6vision3mod20ObjectTrackerOptionsENS_14default_deleteIS3_EENS_9allocatorIS3_EEEE
NSt3__120__shared_ptr_emplaceIN6vision3mod28ImageDescriptorAugmenterFlipENS_9allocatorIS3_EEEE
NSt3__114default_deleteIA_hEE
NSt3__120__shared_ptr_pointerIPhNS_14default_deleteIA_hEENS_9allocatorIhEEEE
NSt3__120__shared_ptr_emplaceINS_6vectorINS_10shared_ptrIN6vision3mod30ImageAnalyzer_CustomClassifierEEENS_9allocatorIS6_EEEENS7_IS9_EEEE
NSt3__114default_deleteIN6vision3mod30ImageAnalyzer_CustomClassifierEEE
NSt3__120__shared_ptr_pointerIPN6vision3mod30ImageAnalyzer_CustomClassifierENS_14default_deleteIS3_EENS_9allocatorIS3_EEEE
x<NSt3__120__shared_ptr_emplaceIN6vision3mod23ImageClassifierEspressoENS_9allocatorIS3_EEEE
?N6vision3mod13CVMLCancellerE
N6vision3mod28ImageDescriptorAugmenterNoOpE
NSt3__120__shared_ptr_emplaceIN6vision3mod28ImageDescriptorAugmenterNoOpENS_9allocatorIS3_EEEE
NSt3__120__shared_ptr_emplaceIN6vision3mod30ConcreteFaceprintAndAttributesENS_9allocatorIS3_EEEE
NSt3__114default_deleteIN6vision3mod29ImageDescriptorBufferAbstractEEE
NSt3__120__shared_ptr_pointerIPN6vision3mod29ImageDescriptorBufferAbstractENS_14default_deleteIS3_EENS_9allocatorIS3_EEEE
NSt3__120__shared_ptr_emplaceIN6vision3mod19LandmarkDetectorDNNENS_9allocatorIS3_EEEE
N6vision3mod15GreedyClustererE
NSt3__120__shared_ptr_emplaceINS_6vectorINS_4pairImmEENS_9allocatorIS3_EEEENS4_IS6_EEEE
NSt3__120__shared_ptr_emplaceINS_6vectorImNS_9allocatorImEEEENS2_IS4_EEEE
N6vision3mod20GreedyClustererFacesE
NSt3__120__shared_ptr_emplaceIN6vision3mod15GreedyClusterer9private_tENS_9allocatorIS4_EEEE
NSt3__120__shared_ptr_emplaceIN6vision3mod20GreedyClustererFacesENS_9allocatorIS3_EEEE
17VNNSDataStreambuf
24VNNSMutableDataStreambuf
&:/1
mVA=
\L;&
*RY;4|
KdC<
+/l?
rTD?
9FI?
W@+/l?
,A?P
?+/l?
+/l?@
q@4J?
9Q>`^
w=?k+K
F?]"!@R
?bD&>xzr?
uw?-
Fs?)
'I^?`
l?6a
>xq%
$@t=
;?`1S>
>M;{?
?;+n?
{f=~C
G]Z?
_=E3
wi@^ 
?k+H
7kV@
ZN6vision3mod28readBinSerializedModelValuesEP7__sFILEPKcRKNS0_26BinSerializedModelFileInfoEbRNS0_11ModelValuesEE3$_1
NSt3__120__shared_ptr_pointerIPhZN6vision3mod28readBinSerializedModelValuesEP7__sFILEPKcRKNS3_26BinSerializedModelFileInfoEbRNS3_11ModelValuesEE3$_1NS_9allocatorIhEEEE
ZN6vision3mod28readBinSerializedModelValuesEP7__sFILEPKcRKNS0_26BinSerializedModelFileInfoEbRNS0_11ModelValuesEE3$_0
NSt3__120__shared_ptr_pointerIPfZN6vision3mod28readBinSerializedModelValuesEP7__sFILEPKcRKNS3_26BinSerializedModelFileInfoEbRNS3_11ModelValuesEE3$_0NS_9allocatorIfEEEE
NSt3__120__shared_ptr_emplaceI17espresso_buffer_tNS_9allocatorIS1_EEEE
N6vision3mod32ImageDescriptor_EspressoSmartCamE
NSt3__120__shared_ptr_emplaceIN6vision3mod32ImageDescriptor_EspressoSmartCamENS_9allocatorIS3_EEEE
NSt3__120__shared_ptr_emplaceIN6vision3mod18FaceBoxPoseAlignerIaEENS_9allocatorIS4_EEEE
N6vision3mod17RPNTrackerOptionsE
N6vision3mod16ObjectTrackerRPNE
@(#)PROGRAM:Vision  PROJECT:Vision-3.0.59
/dev/null
< %-8s > 
Emergency
Alert
Critical
Error
Warning
Notice
Info
Verbose
Pixel format (%d) not supported!
Pixel format (%d), model (%d), or stretch (%d) not supported!
Invalid source image!
Invalid instanceResult buffer!
Output buffer size incorrect!
Invalid output buffer rowBytes (%d)!
rtcv::simCropResize failed!
Invalid exemplarResult buffer!
TtResult modelInfo.numModels (%d) out of bounds!
Incorrect trk node state version (%u vs %u)
Numbers of net outputs (%d) more than limit!
Numbers of net outputs (%d) isn't correct!
GeomTransform_constructor: unknown transform model (%d)
GeomTransform_minSupportPoints: unknown transform model (%d), reset to RIGID
GeomTransform_changeCoordinateSystem failed
GeomTransform_setModel: unknown new model (%d) use the old model (%d)
GeomTransform_estimate: unknown transform model (%d) use RIGID
GeomTransform_numTestsToDo: unknown transform model (%d) use RIGID
RigidTransform_estimate: not symmetric positive definite matrix
RigidTransform_estimate: the %ld-th argument is wrong in sposv_ call
AffineTransform_estimate: not symmetric positive definite matrix
AffineTransform_estimate: the %ld-th argument is wrong in sposv_ call
HomographyTransform_estimate:The %ld-th eigenvector failed to converge
HomographyTransform_estimate:The %ld-th eigenvector failed to converge
HomographyTransform_estimate: the %ld-th argument is wrong in ssyevx_ call
HomographyTransform_estimate: the %ld-th argument is wrong in ssyevx_ call
HomographyTransform_estimate: the 9-th parameter is too small pp[8]=%f 
IPDetector_constructor: Cannot allocate mFltImage 
IPDetector_constructor: Cannot allocate box filter
IPDetector_constructor: Cannot allocate mTmpBuffer 
IPDetector_constructor: Cannot allocate mCornerVec 
IPDetector_constructor: Cannot allocate mBX, mBY 
IPDetector_response: box filter failed
v16@?0i8i12
v8@?0
histogram equalization queue
v16@?0Q8
boxFilter_uint8_init: box filter failed when request minimum size err=%d
boxFilter_uint8: box filter failed err=%d
 invMatrix failed INFO1 = %ld
 invMatrix failed INFO2 = %ld
%s : -[EAGLContext setParameter:...] failed with error %d
gl_UtilsCreateContext
%s : calloc failed
ImageRegistrationCreateContext
%s : CFDictionaryCreateMutable failed
%s : CFArrayCreateMutable failed
%s : HistEqCreateContext failed
%s : RegistrationEngine_constructor failed
imageRegQueue
%s : dispatch_queue_create failed
%s : NULL input parameters
ImageRegistrationUnregisterOneImage
ImageRegister
%s : Need at least one non-reference image
%s : CFArrayGetCount(imageRegCtx->availExtraBuffers) != 0
%s : Could not locate scratch buffers
%s : CPU histogram equalization failed
HistogramEqualization
%s : invalid histogram equalization method
HistEqualizeCPU
%s : Unsupported image width,height
%s : Couldn't lock output buffer
%s : Couldn't lock input buffer
%s : CFArrayGetCount(imageRegCtx->availExtraBuffers) returned %d
GetUnusedExtraBuffer
GeomTransform_estimate failed
Pyramid_loadImage: incompatible size in pyramid (%lu!=%lu) or (%lu!=%lu)
v40@?0^Q8^Q16Q24Q32
WARNING: insufficient number of external corners provided (only %hu corners provided but minumum is %d)
Registration could not detect more that %d inlier corners at the highest resolution.
position
%s SHADER COMPILATION FAILED
VERTEX
FRAGMENT
<unknown>
Shader compile log:
<OOM with failed shader compile log>
PROGRAM LINK FAILED
Program link log:
<OOM with failed program link log>
%s : GL program failed to compile/link
HistEqCreateContext
source
textureScale
textureBias
attribute vec2 position;
varying vec2 texcoord;
void main()
  texcoord = 0.5 * (position + 1.0);
  gl_Position = vec4(position.x, position.y, 0.0, 1.0);
precision mediump float;
uniform sampler2D source;
uniform sampler2D lut;
uniform float textureScale;
uniform float textureBias;
varying highp vec2 texcoord;
void main()
  vec4 texColor = texture2D(source, texcoord);
  texColor = textureScale * texColor + textureBias;
  gl_FragColor.xyzw = vec4(texture2D(lut, vec2(texColor.x, 0.0)).x,
                           texture2D(lut, vec2(texColor.y, 0.0)).x,
                           texture2D(lut, vec2(texColor.z, 0.0)).x,
                           texture2D(lut, vec2(texColor.w, 0.0)).x);
libcompiler_rt abort
_availability_version_check
kCFAllocatorNull
CFDataCreateWithBytesNoCopy
CFPropertyListCreateWithData
CFPropertyListCreateFromXMLData
CFStringCreateWithCStringNoCopy
CFDictionaryGetValue
CFGetTypeID
CFStringGetTypeID
CFStringGetCString
CFRelease
/System/Library/CoreServices/SystemVersion.plist
IPHONE_SIMULATOR_ROOT
ProductVersion
%d.%d.%d
__cpu_indicator_init
/BuildRoot/Library/Caches/com.apple.xbs/Sources/clang/clang-1100.2.32.4/src/projects/compiler-rt/lib/builtins/cpu_model.c
__cpu_model.__cpu_type < CPU_TYPE_MAX
__cpu_model.__cpu_subtype < CPU_SUBTYPE_MAX
VNSaliencyHeatmapBBoxGeneratorProcessingOptionSaliencyDetectorType
%@:%@
saliency_attention_box_head_i4fgq3rswb_fp16.espresso
saliency_objectness_boxes_head_ecvqeduzc7_46800_fp16.espresso
bbox
error occured when running model, unexpected output received
error occured when running model
Model runtime error, Unable to bind input buffer
Model runtime error, Unable to bind output buffer
unable to lock base address of pixelBuffer
Internal error:  detection of Human Heads should be handled by ANFD Detector Compound Request
VNFaceAnalyzerMultiDetectorProcessingOptionInputFaceObservation
face quality of %f is out of range
Could not run FaceQuality Espresso network.
Failed to scale and crop face rectangle
Failure to create face quality predictor.
face_quality_v1.0.espresso
CVML module = %@
com.apple.cvml.%@
logFolderURL
T@"NSURL",R,V_logFolderURL
logFileURL
T@"NSURL",R,V_logFileURL
logEnabled
TB,R,V_logEnabled
fileNameBase
T@"NSString",R,V_fileNameBase
1 Lookup
0 Lookup
%lld
%@Faces: 
ClusterId: %lld
Level %@ cluster map:
%@_%@.log
VNClusteringLog
yyyy-MM-dd_HH-mm-ss-SSS
en_US_POSIX
CVML_debug_enable_cluster_log
%@, 
Final list of suggestions face IDs (results):
Group %d suggestions: 
Connected groups of suggestions face IDs (connectedSuggestedInputs):
filtered suggestions to include only those that are part of input query (suggestedInputsLists)
all sugestions for given input query (suggestionLists)
%@Suggestions: 
ClusterId: %@   
Suggested face IDs: %@:
%@can be returned: %@
faceId: %@
Input query - face IDs with flags (clusterIdsWithFlags):
VNSuggestionLog
Greedy clusterer failed with unknown error
Greedy clusterer failed with error: %s
Greedy clusterer failed with error: %llu
Internal error in maximumFaceIdInModelAndReturnError
Clustering request was canceled, error: %llu
Clustering not properly initialized
Parameter validation failed for getDistanceBetweenLevel1Clusters
B24@?0@"NSNumber"8@"NSMutableOrderedSet"16
getLevel1ClusteredIdsGroupedByLevel0ClustersForFaceId: Level-0 cluster size is zero for a faceId (%d) from Level-1 cluster.
getLevel1ClusteredIdsGroupedByLevel0ClustersForFaceId: Level-1 cluster size is zero for a valid clusterID descriptor (%lld)
getLevel1ClusteredIdsGroupedByLevel0ClustersForFaceId: There is no level-1 cluster that contains faceId = %d
getLevel1ClusteredIdsGroupedByLevel0ClustersForFaceId: m_ClusteringImpl_const (%llu) and/or faceId (%@) are/is not initialized
Internal error querying similar faces
Error initializing cluster state
vision_clustering_kid_threshold
vision_clustering_baby_threshold
vision_clustering_torso_threshold
vision_clustering_sl_threshold
vision_clustering_threshold
VNRequestOptionClusteringAlgorithm must be set to either VNClusteringAlgorithm_Greedy or VNClusteringAlgorithm_GreedyWithTorso
Invalid cache file path: %@
RestoreClusteringState is a required parameter
Cache file path is a required parameter
  clusterId: %ld, %s
clusters:
  %d: prevcount=%d, curcount=%d, shouldUpdateRep = %d
Clusters:
Faces to add array must be the same size as that of the grouping identifiers array.
Faces to add must be accompanied by grouping identifiers when performing clustering in torso mode.
adding faces (%lu): %s
Clustering with greedy algorithm
faceId
Ti,VfaceId
faceRect
T{CGRect={CGPoint=dd}{CGSize=dd}},VfaceRect
framesSinceLast
Ti,VframesSinceLast
maxScore
Tf,VmaxScore
minScore
Tf,VminScore
numScores
Ti,VnumScores
swFaceId
Ti,VswFaceId
swCenter
T{CGPoint=dd},VswCenter
swSize
T{CGSize=dd},VswSize
swLastFrameSeen
Ti,VswLastFrameSeen
hwFaceId
Ti,VhwFaceId
hwCenter
T{CGPoint=dd},VhwCenter
hwSize
T{CGSize=dd},VhwSize
hwLastFrameSeen
Ti,VhwLastFrameSeen
curConfig
T@"NSMutableDictionary",&,V_curConfig
faceIdMapping
T@"NSMutableDictionary",&,V_faceIdMapping
renameMapping
T@"NSMutableDictionary",&,V_renameMapping
faceIdCounter
Ti,V_faceIdCounter
faceInfoArray
T@"NSMutableArray",&,V_faceInfoArray
numFramesSinceFullFaceCore
Ti,V_numFramesSinceFullFaceCore
numFramesNoFaces
Ti,V_numFramesNoFaces
faceTimestampArray
T@"NSMutableArray",&,V_faceTimestampArray
latestImageTimestamp
Td,V_latestImageTimestamp
lastFaceIndex
Ti,V_lastFaceIndex
timeFaceDetectionDone
Td,VtimeFaceDetectionDone
timeBlinkDetectionDone
Td,VtimeBlinkDetectionDone
forceFaceDetectionEnable
TB,VforceFaceDetectionEnable
forceFaceDetailsEnable
TB,V_forceFaceDetailsEnable
latestFaceTimestamp
Td,VlatestFaceTimestamp
Ti,V_version
SmileLevel
RightEyeBlinkLevel
RightEyeHeight
RightEyeWidth
RightEyeY
RightEyeX
LeftEyeBlinkLevel
LeftEyeHeight
LeftEyeWidth
LeftEyeY
LeftEyeX
PitchAngle
YawAngle
RollAngle
Height
Width
Timestamp
timescale
epoch
value
RegionList
Regions
AccumulatedFaceMetadata
Rect
v24@?0@"VNRequest"8@"NSError"16
q24@?0@"VNFaceObservation"8@"VNFaceObservation"16
testMaxInnerDistance
Tf,VtestMaxInnerDistance
testInOutRatio
Tf,VtestInOutRatio
testMaxPeakRegistrationError
Tf,VtestMaxPeakRegistrationError
testMeanPeakRegistrationError
Tf,VtestMeanPeakRegistrationError
testMinRegionOfInterestSize
Tf,VtestMinRegionOfInterestSize
testMaxRegistrationErrorSkewness
Tf,VtestMaxRegistrationErrorSkewness
testMaxRegistrationErrorIntegral
Tf,VtestMaxRegistrationErrorIntegral
testAverageCameraTravelDistance
Tf,VtestAverageCameraTravelDistance
testAverageRegistrationErrorSkewness
Tf,VtestAverageRegistrationErrorSkewness
testBeginningVsEndAEMatrixVsAverageAdjacentAEMatrix
Tf,VtestBeginningVsEndAEMatrixVsAverageAdjacentAEMatrix
svmParameters
T^{__SVMParameters=[7{__SVMScaleOffset=ff}]ddii^{BurstSupportVector}^{BurstSupportVector}},V_svmParameters
VNFaceAnalyzerMultiDetectorObservationGroupsForRequests
VNFaceAnalyzerMultiDetectorProcessingOptionCreateFaceprint
VNFaceAnalyzerMultiDetectorProcessingOptionCreateFaceAnalyzer
VNFaceAnalyzerMultiDetectorProcessingOptionFaceprintRequestRevision
VNFaceAnalyzerMultiDetectorProcessingOptionFaceAnalyzerRequestRevision
VNFaceAnalyzerMultiDetectorProcessingOptionFaceObservations
VNFaceAnalyzerMultiDetectorProcessingOptionFaceprintForceFaceprintCreation
Could not run Espresso network. Error = %s
could not lock cropped image
facerec_fp2.2_fa1.1_r2_optimize.espresso
Unable to initialize frontalizer.
Failure to create face multi-headed classifier.
VNClusterOptionVectorMapReadOnlyFlag
Failed to create clusterer; Error = %@
Invalid Faceprint revision: %lu
Invalid Clusterer type: %@
Invalid Clusterer cache directory: %@
Failed to locate age classifier model file
unable to locate facerec-v2.2-bgr-att-308_chk-0050__0__faceBoxPoseAligner-current__faceDetectorV2-current__ageClassifier
facerec-v2.2-bgr-att-308_chk-0050__0__faceBoxPoseAligner-current__faceDetectorV2-current__ageClassifier
Suggestions request has been cancelled
Clustering request has been cancelled
label
VNSceneClassifierCreationOptionCustomHierarchy
VNSceneClassifierOptionMaximumLeafLabels
VNSceneClassifierOptionMaximumHierarchicalLabels
espressoModelImageprintClass
T#,R
returnAllResultsOptionKey
T@"NSString",R
hash
TQ,R
superclass
description
T@"NSString",R,C
debugDescription
VN_scene_classifier_debug_intermediates/
VN_DEBUG_DUMP_SCENE_INTERMEDIATES
scene-classifier-relationships
scene-classifier-labels
scene-classifier
scene-descriptor
Invalid data for input image descriptor option
Invalid data for input sceneprint descriptors
Could not compute image descriptor for image
json
%@ was initialized with request revision %lu but is being performed with request revision %lu
custom hierarchy created for revision %lu cannot be used with a detector for revision %lu
The image is invalid
VisionErrorDomain
Invalid argument passeed in
Invalid requestRevision requested
URL can't be nil
request
T@"VNRequest",R,N,V_request
T@"NSError",R,N,V_error
( %@, %@ )
observationsCacheKey
T@"<NSObject><NSCopying>",R,N,V_observationsCacheKey
parentRequest
T@"VNRequest",R,N,V_parentRequest
orderedChildRequests
T@"NSArray",R,C,N,V_orderedChildRequests
( %@, [ %@ ] )
orderedRequests
T@"NSArray",C,N
originalRequests
T@"NSArray",R,C,N
performedRequests
%@ hit %@
%@ looked up %@
%@ cached %@
, failed with %@
performed %@
performing %@
%@ created
%@ #%lu (%p)
device
T@"<MTLDevice>",R,N,V_device
commandQueue
T@"<MTLCommandQueue>",R,N,V_commandQueue
library
T@"<MTLLibrary>",R,N,V_library
Cannot create an MTLCommandQueue
Cannot create an MTLLibrary
Cannot locate MLTLibrary
metallib
default
Cannot create an MTLDevice
algParam
nodeParam
leafParam
regLookup
ERT - p : %d
testFeature
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-3.0.59/Libraries/cvml-Core/ERT/ERT2D.cpp
p.n > 0
ERT - node params are unavailable
ERT - reg lookup is unavailable
ERT - maxCacheSize: %d
ERT - cacheIndexL : %d
ERT - cacheIndexR : %d
ERT - anchorIndex : %d
com.apple.vis
%@ (%@)
: %s
%@ does not support private revision %lu
%@ does not support %@
The current configuration of %@ is not supported
processing with %@ is not supported
%@ requires the GPU for processing
argument %@ has an invalid value of %@
"%@"
 - %@
option %@ has an invalid value of %@
missing option %@
%@ does not support operation
VNRequest
request was cancelled
request %@ was cancelled
T@"NSString",C,N,V_burstFrameIdentifier
imageProperties
T@"NSDictionary",C,N,V_imageProperties
burst context is not available
%@:%@:%@
VNBarcodeSymbologyAztec
VNBarcodeSymbologyCode39
VNBarcodeSymbologyCode39Checksum
VNBarcodeSymbologyCode39FullASCII
VNBarcodeSymbologyCode39FullASCIIChecksum
VNBarcodeSymbologyCode93
VNBarcodeSymbologyCode93i
VNBarcodeSymbologyCode128
VNBarcodeSymbologyDataMatrix
VNBarcodeSymbologyEAN8
VNBarcodeSymbologyEAN13
VNBarcodeSymbologyI2of5
VNBarcodeSymbologyI2of5Checksum
VNBarcodeSymbologyITF14
VNBarcodeSymbologyPDF417
VNBarcodeSymbologyQR
VNBarcodeSymbologyUPCE
Failed to align a detected bounding box
could not locate the face detection model file
noMapping
1DAffineMapping
1DLogisticMapping
1DPairwiseAffineMapping
postProcessorType
imageCropAndScaleOption
TQ,N,V_imageCropAndScaleOption
TQ,N
%@ %@
VNClassifyJunkImageRequestPrivateRevisionStillCapturePipeline
leftEyeOpen
TB,VleftEyeOpen
rightEyeOpen
TB,VrightEyeOpen
smiling
TB,Vsmiling
leftEyeBlinkScore
Tf,VleftEyeBlinkScore
rightEyeBlinkScore
Tf,VrightEyeBlinkScore
smileScore
Tf,VsmileScore
hasLeftEye
TB,VhasLeftEye
hasRightEye
TB,VhasRightEye
foundByFaceCore
TB,VfoundByFaceCore
normalizedFaceRect
T{CGRect={CGPoint=dd}{CGSize=dd}},VnormalizedFaceRect
focusScore
Tf,VfocusScore
faceScore
Tf,VfaceScore
leftEyeRect
T{CGRect={CGPoint=dd}{CGSize=dd}},VleftEyeRect
rightEyeRect
T{CGRect={CGPoint=dd}{CGSize=dd}},VrightEyeRect
FCRLeftEyeFeaturesOffset
Ti,VFCRLeftEyeFeaturesOffset
FCRRightEyeFeaturesOffset
Ti,VFCRRightEyeFeaturesOffset
FCRSmileFeaturesOffset
Ti,VFCRSmileFeaturesOffset
FCRBlinkFeaturesSize
Ti,VFCRBlinkFeaturesSize
FCRSmileFeaturesSize
Ti,VFCRSmileFeaturesSize
FCRSmileAndBlinkFeatures
T@"NSMutableArray",&,VFCRSmileAndBlinkFeatures
hwFaceRect
T{CGRect={CGPoint=dd}{CGSize=dd}},V_hwFaceRect
normalizedFocusScore
Tf,VnormalizedFocusScore
normalizedSigma
Tf,VnormalizedSigma
hasRollAngle
TB,VhasRollAngle
hasYawAngle
TB,VhasYawAngle
hasPitchAngle
TB,V_hasPitchAngle
rollAngle
Tf,VrollAngle
yawAngle
Tf,VyawAngle
pitchAngle
Tf,V_pitchAngle
timestamp
Td,Vtimestamp
isSyncedWithImage
TB,V_isSyncedWithImage
smallFace
TB,VsmallFace
Image_ImageROIGridStartX
Image_ImageROIGridStartY
Image_ImageROIGridEndX
Image_ImageROIGridEndY
imageId
T@"NSString",&,V_imageId
Ti,Vorientation
faceStatArray
T@"NSMutableArray",&,V_faceStatArray
exclude
TB,Vexclude
AEStable
TB,VAEStable
AEAverage
Ti,VAEAverage
AETarget
Ti,VAETarget
AFStable
TB,VAFStable
temporalOrder
Ti,VtemporalOrder
avgHorzDiffY
Tf,VavgHorzDiffY
blurExtent
Tf,VblurExtent
imageScore
Tf,VimageScore
actionScore
Tf,VactionScore
timeReceived
Td,VtimeReceived
maxSkewness
Tf,VmaxSkewness
registrationErrorX
Tf,VregistrationErrorX
registrationErrorY
Tf,VregistrationErrorY
registrationErrorIntegral
Tf,VregistrationErrorIntegral
actionClusteringScore
Tf,VactionClusteringScore
hasRegistrationData
TB,VhasRegistrationData
facesRoiRect
T{CGRect={CGPoint=dd}{CGSize=dd}},VfacesRoiRect
numHWFaces
Ti,VnumHWFaces
emotionallyRejected
TB,VemotionallyRejected
doLimitedSharpnessAndBlur
TB,VdoLimitedSharpnessAndBlur
Tf,Vtx
Tf,Vty
isGarbage
TB,VisGarbage
roiSize
Tf,VroiSize
AEDelta
Ti,V_AEDelta
ERROR: unknown transformation
VNImageOptionImageOrientation
VNImageOptionProperties
VNImageOptionCameraPixelFocalLength
VNImageOptionCameraIntrinsics
VNImageOptionCIContext
VNRequestHandlerCleanupOption_AllPipelines
VNRequestHandlerCleanupOption_FacePipeline
VNRequestHandlerCleanupOption_ScenePipeline
VNRequestHandlerCleanupOption_SmartCamPipeline
VNRequestHandlerCleanupOption_JunkPipeline
VNCleanupLevel_Complete
VNCleanupLevel_Partial
VNRequestHandlerCleanupOption_ImageBuffers
VNCleanupPurgeAll
imageSpecifier
orientation
could not create an image buffer
com.apple.VNRequestHandler
modelContextObject
T@"NSObject",&,N,V_modelContextObject
burstAnalysisLoggingCallback
T@?,C,N,V_burstAnalysisLoggingCallback
VNSequenceRequestHandlerOption_LoggingCallback
VNPhotosRequestHandler
model request handler not available
registration of region of interest %@ (%@) cannot be performed on reference image of size %@
failed to create image registration context
failed to create GL context
failed to warp image
failed to create a %lu x %lu pixel buffer of type '%c%c%c%c'
sceneObservation
T@"VNSceneObservation",&,N,V_sceneObservation
customHierarchy
T@"VNClassificationCustomHierarchy",&,N,V_customHierarchy
maximumLeafObservations
TQ,N,V_maximumLeafObservations
maximumHierarchicalObservations
TQ,N,V_maximumHierarchicalObservations
 leaf=%lu hierarchy=%lu
T@"VNSceneObservation",R,&,N
T@"VNClassificationCustomHierarchy",R,C,N
supportedImageSizeSet
T@"NSArray",R
the custom hierarchy is for request revision %lu, not %lu
%@ cannot be performed with compatibility revision %lu and sceneprints generated by revision %lu
%@ does not provide this information
%@ is not supported for %@
knownSceneClassifications
T@"NSArray",R,N
0x%08X
'%c%c%c%c'
failed to write to data stream
tag %@ did not provide any data
tag %@ has a data overflow to %lu bytes
unexpected end of data stream
encountered unexpected length of %u, instead of %u
could not decode object of class %@
Error while computing blur score: %s
Inconsistent platform
This method should not be invoked directly. Derived classes are responsible for providing correct implementation
inputObservation
T@"VNDetectedObjectObservation",&,N,V_inputObservation
trackingLevel
TQ,N,V_trackingLevel
lastFrame
TB,N,GisLastFrame,V_lastFrame
Internal error: unexpeted tracked object bounding box size
Internal error: internal type conversion failed
%@:Trk=%@
metalDevice
T@"<MTLDevice>",R,V_metalDevice
wisdomParams
T@"NSDictionary",R,V_wisdomParams
useGPU
TB,R,V_useGPU
Requested Metal Device is not supported: %@
VNDetectorProcessOption_InputImageBuffers
VNDetectorProcessOption_ScenePrints
VNDetectorProcessOption_ModelBackingStore
VNDetectorProcessOption_ImageCropAndScaleOption
VNDetectorProcessOption_Canceller
VNDetectorProcessOption_RequestRevision
synchronizationQueue
T@"NSObject<OS_dispatch_queue>",&,N,V_synchronizationQueue
configurationOptions
T@"NSDictionary",R,C,V_configurationOptions
metalContext
T@"VNMetalContext",R,N,V_metalContext
processingQueue
T@"NSObject<OS_dispatch_queue>",R,N,V_processingQueue
backingStore
TQ,R,N,V_backingStore
TQ,R,N,V_requestRevision
TQ,R,N
Cannot initialize Metal Context
Cannot create Metal Context for non-GPU targeting device
%@ does not implement %@
unable to create processing queue
com.apple.VN.processingQueue.%@
:%@=%@
AnnealContour
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-3.0.59/Libraries/RectangleDetector/QuadDetect/ContourUtilities.c
sPtrPrev != sPtr
sPtr != sPtrNext
rPtr != NULL
annealSegments
sPrevPtr != sPtr
newCurr != NULL
createBridgeSegment
mPnts == nPnts
reverseContour
currCPtr != NULL
mergeEndpointSearch4
code1 != -1
code2 != -1
mergeContourEnd
cPtr2->active
PCOORD_EQUAL(sPtr2->pary[sPtr2->nPnts-1], pc3)
(code == 1) || (code == 2)
CVML_status module %d error %lld
not implemented error
internal error
unexpected null pointer
invalid parameter
memory allocation error
vImage related error
delegate error
missing option
invalid option
unknown option
I/O error
data inconsistency error
invalid data type
invalid ID
hash already in use
platform endianess not supported
LAPACK error
division by zero
singular point configuration error
out of bounds
invalid format
OpenGL error
warping error
inconsistent state error
missing positional parameter
error with projection computation
video error
too few IDs to build VIP model
computation kill request was issued
batch size violation
nominal distance not changed
no saved state to revert
initialization error
feature extraction error
small sparsity error
incorrect binserializer key
Espresso error
General error
Not supported error
CVML Module %lld
CVMLEngine
PhotosProcessorCLI
ImageProcessorCLI
ClusteringCLI
MPCmdlineClientCLI
ImageClassifierCLI
FaceProcessorCLI
AppleNetParser
BinSerializerProcessor
ThirdParty
ImageWarper
VideoTools
ImageTools
Generic
FaceQuality
FaceprintAndAttributes
FaceAttributes
ImageAnalyzer
FaceSegmenter
BoostedClassifier
FaceID
SparseCoding
Kmeans
SRCClassifier
ObjectTracker
ObjectDetector
FaceRegionMap
HumanDetector
Clustering
SimilarityMatrix
ImageRegistration
VIPIdentification
ImageProcessing
ImageClassifier
ImageDescriptor
FaceboxAligner
MomentProcessor
LandmarkDetector
ImageQuality
ImageGrouping
Geometry3D
Geometry2D
FaceWarper
FaceFrontalizer
FaceDescriptor
Face3D
BinSerializer
v32@?0@"NSNumber"8@"NSMutableArray"16^B24
v32@?0@"<NSCopying>"8@"VNImageAnalyzerCompoundRequestGroupingConfiguration"16^B24
[%g,%g,%g,%g]:%@:%u:%u:%c
detectorType
T@"NSString",C,N,V_detectorType
detectorConfigurationOptions
T@"NSDictionary",C,N,V_detectorConfigurationOptions
originalRequestConfigurations
T@"NSArray",C,N,V_originalRequestConfigurations
v32@?0Q8@"NSArray"16^B24
cannot process %@ in a single operation
regionLabels
T@"NSArray",C,V_regionLabels
supportsSecureCoding
TB,R
Unknown
pixelToRgnMap
rgnMapRowBytes
rgnMapH
rgnMapW
rgnMapData
alignH
alignW
alignY
alignX
userH
userW
userY
userX
requestRevision
VNFaceRegionMapVersion
region map data has length of %lu instead of the expected %lu
unknown coding version
com.apple.VN.createGaborFilterBankGCDQueueName
com.apple.VN.extractGaborDescriptorGCDQueueName
com.apple.VN.gaborReadySyncQueueName
com.apple.VN.gaborDescriptorReadySyncQueueName
VNFaceLandmarkDetectorProcessOption_InputFaceObservations
VNFaceLandmarkDetectorProcessOption_CalculateLandmarkScore
VNFaceLandmarkDetectorProcessOption_LoadRefinersModel
Invalid parameters passed to blink score computation
Could not compute landmark score, error code = %lld
Invalid parameters passed to landmark score computation
Cannot create intermediate image buffer
Invalid image buffer size
landmarkRefinerAndPupil_v2
Model file info populated incorrectly
void cvml::util::binserialized_contents::init_model_values(const cvml::util::binserialized_table_of_contents &, const char *, const vision::mod::BinSerializedModelFileInfo &)
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-3.0.59/VisionKitFramework/VN/algorithm_util/binserialized_mapped_file_contents.h
model file is corrupt
void cvml::util::binserialized_table_of_contents::init(const void *const, size_t)
Could not read landmark refiner model data
Invalid landmark refiner model resource path
VNDetectFaceLandmarksRequest
Unknown constellation type: %lu
T@"NSMutableArray",R,N,V_originalRequests
%@:%u:%u
%f,%f,%f,%f:%u:%u
VNImageBufferOption_DownscaleCGInterpolationQuality
VNImageBufferOption_UpscaleCGInterpolationQuality
VNImageBufferOption_FeatureOrientationRelativeToUpRight
VNImageBufferAugmentationApplePipeline
VNImageBufferAugmentationBlur
VNImageBufferAugmentationNoise
VNImageBufferAugmentationRotation
VNImageBufferAugmentationFlip
VNImageBufferAugmentationFlipVertical
VNImageBufferAugmentationFlipHorizontal
VNImageBufferAugmentationShear
VNImageBufferAugmentationExposure
VNImageBufferAugmentationRandomCrop
VNImageBufferAugmentationOptionMaxRange
VNImageBufferAugmentationOptionMinRange
VNImageBufferAugmentationOptionNumberOfBuffers
VNImageBufferAugmentationOptionRandomSeed
-[VNImageBufferManager purgeAllCaches]
ERROR while purging caches %s | %@
%@_intermediate_%04u.png
Request
-[VN
 internalPerformRevision:inContext:error:]
The augmentationOptions do not conatain any of the VNImageBufferAugmentation keys
RandomCrop produced an invalid crop for width %f height %f
CIMultiplyBlendMode
CIColorMonochrome
CIRandomGenerator
CIStraightenFilter
CIExposureAdjust
CIDiscBlur
unable to create the cropped buffer - error: %d
invalid chunk increment of %lu x %lu
invalid chunk size of %ld x %ld
invalid ROI size of %f x %f
Could not create buffer with format %@ (%ld)
0x%x
Operation failed due to attempt to crop zero or near zero dimensioned area
Unable to create a CGBitmapContext
Unable to create CGImage for scaling
Unable to crop image from source buffer
Missing target buffer for crop operations
%@ cannot be called with nil options
Extracting ROI from an image failed
unable to create the Y plane wrapper buffer
this release call should not be used with anything but a referencing pixelbuffer %s
void CVPixelBufferReleaseReferencingPixelBufferCallback(void * _Nullable, const void * _Nullable)
Failed to transfer inBuffer to inputBufferForRotation. Error %d
unable to create %@ x %@ pixel buffer with format %@
Failed to transfer inBuffer to croppedBuffer. Error %d
unable to create the interim YUV buffer
Failed to create image for processing due to invalid requested buffer dimensions
Failed to create image for processing
inputFaceObservations
espressoModelName
T@"NSString",R,V_espressoModelName
Failed to create inference engine resources
VNRequestOptionPreferBackgroundProcessing
VNRequestOptionProcessingDevice
cacheOptionsKeys
Option value for option key %@ is a mandatory parameter
com.apple.VN.serializeRPNTrackingQueue
rpn_track_v3.espresso
rpn_template_v3.espresso
com.apple.Tracker.rpnTrackQueuee
com.apple.Tracker.rpnInitQueue
6ziz6uinva_opt.espresso
failed to obtain the data
v24@?0Q8^B16
ToOutputStream:options:md5Context:error:
%@%lu%@
writeVersion
writeReadOnlyVersion
v32@?0@"VNFaceObservation"8Q16^B24
 <dirty>
 modified on %@
VNRequestWarningImageTooSmall
VNRequestWarningImageTooSmallForFaceObservations
VNRequestWarningImageMinimumLongDimension
VNRequestWarningImageMinimumShortDimension
VNRequestWarningBlinkDetectionFailure
cancellationSemaphore
T@"NSObject<OS_dispatch_semaphore>",&,V_cancellationSemaphore
T@"NSDictionary",R,C,N,V_options
cancellationTriggered
modelFileBackingStore
preferredMetalContext
T@"<MTLDevice>",&,N
metalContextPriority
detectionLevel
processingDevice
T@"VNProcessingDevice",C,N
preferBackgroundProcessing
TB,N
usesCPUOnly
results
T@"NSArray",R,C,N,V_results
completionHandler
T@?,R,C,N,V_completionHandler
revision
TQ,N,V_revision
 usesCPUOnly
 preferBackgroundProcessing
imageBuffer
cancellation is not currently available
%@ does not support cancellation
-[%@ %@] has not been implemented
com.apple.%@
%@-%lu:MTL=%@:Det=%lu
supportedRevisions
T@"NSIndexSet",R,C,N
defaultRevision
currentRevision
Vision
All elements in the %@ array must be of class %@ (%@)
The %@ option was expected to be a %@, but was instead a %@ (%@)
The %@ required option was not found
unable to prepare %@
requestClass
T#,R,N,V_requestClass
resolvedRevision
TQ,N,V_resolvedRevision
TQ,N,V_detectionLevel
T@"VNProcessingDevice",&,N,V_processingDevice
TQ,N,V_metalContextPriority
TB,N,V_preferBackgroundProcessing
TQ,N,V_modelFileBackingStore
    %@=%@
minimumDimension
TQ,R,N,V_minimumDimension
maximumDimension
TQ,R,N,V_maximumDimension
TQ,R,N,V_idealDimension
idealDimension
maxDimension
minDimension
pixelsWideRange
T@"VNSizeRange",R,N,V_pixelsWideRange
pixelsHighRange
T@"VNSizeRange",R,N,V_pixelsHighRange
aspectRatioHandling
TQ,R,N,V_aspectRatioHandling
idealImageFormat
TI,R,N,V_idealImageFormat
TI,R,N,V_idealOrientation
TB,R,N,GisOrientationAgnostic,V_orientationAgnostic
orientationAgnostic
idealOrientation
aspectHandling
highRange
wideRange
idealFormat
T@"VNFaceprint",R,N,V_faceprint
T@"VNTorsoprint",R,N,V_torsoprint
validTorsoprint
TB,R,N,GisValidTorsoprint
Unexpected size of serialized state of the object of type %@
state cannot be nil
Failed to initialize VNFaceTorsoprint object
Unexpected size of deserialized state of the object of type %@
Serialized and calculated MD5s don't match
Wrong type of print object
FTp_tp
FTp_fp
FTp_rev
FTp_algorithmVersion
FTp_VNFaceTorsoprint
FTp_labelsAndConfidence
FTp_length
FTp_elementsType
FTp_elementsCount
FTp_data
apple_scenes
Error creating fp32, unable to unlock base address
Error converting pixel buffer to fp32
Error creating fp32, unable to lock base address
Error creating fp32 buffer
Attempt to request fp32 format for null pixel buffer
VNFaceDetector error aligning a detected bounding box
VN Face detector debug intermediates written to: %@
error
rect
imageURL
height
width
_raw_bbox_crop.png
%@_face_%ld
<binary-data>
_raw_bboxes.json
_fd_image.png
_fd_image.vdump
VN_facedetector_debug_intermediates/
failed to lock face pixel buffer
model8c_.espresso.net
model8c_.espresso
context
T@"VNMPContext",&,N,V_context
-[VNMomentProcessor computeClusteringTreeForImageDescriptors:assumeDescriptorsAreSorted:error:]
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-3.0.59/Libraries/MomentProcessor/Moments/MomentProcessor.mm
node
T^v,V_node
freeNodeOnDealloc
TB,V_freeNodeOnDealloc
-[VNMomentProcessor computeNaturalClusteringOfImageDescriptors:error:]
-[VNMomentProcessor computeClusteringOfImageDescriptors:intoKGroups:error:]
-[VNMomentProcessor processImagesFromDataProvider:error:]
-[VNMomentProcessor initWithOptions:error:]
q24@?0@"VNMPImageDescriptor"8@"VNMPImageDescriptor"16
VNGenerateAttentionBasedSaliencyImageRequestPrivateRevisionStillCapturePipeline
objectType
Tq,V_objectType
bounds
T{CGRect={CGPoint=dd}{CGSize=dd}},V_bounds
center
T{CGPoint=dd},R
Tf,V_confidence
: %@
T@"NSDictionary",R,C,N
%@;opt=%@
%@ must be overridden
image buffer is no longer available
imageData
ciImage
cgImage
pixelBuffer
object
unsupported serialized state version %u
Serialized state payload data checksum mismatch
Serialized state data length is invalid
Serialized state data is an unsupported version (%lu)
Error deserializing VNFaceprint
Input data is neither VNFaceprint nor CVMLFaceprint. NSKeyedUnarchiver error = %@
facePrint
faceprint
CVMLFaceprint
Input data is not a VNFaceprint
Attempt to deserialize nil
type
VNFaceprint
fp_av
fp_lac
fp_l
fp_et
fp_ec
boundingBox
T{CGRect={CGPoint=dd}{CGSize=dd}},R,N
narrowedBoundingBox
salientObjects
Unexpected nil value for bounding box
NBBSH
NBBSW
NBBOY
NBBOX
BBSH
BBSW
BBOY
BBOX
OISH
OISW
VNSaliencyImageObservation
VNClassifyImageAestheticsRequestPrivateRevisionStillCapturePipeline
VNRectangleTracking_BottomLeftTracker
VNRectangleTracking_BottomRightTracker
VNRectangleTracking_TopLeftTracker
VNRectangleTracking_TopRightTracker
Internal error: Tracking of one of the corners failed, confidence = %f; threshold = %f
VNRectangleObservation object is expected to initialize Rectangle Tracker
Internal error: Resetting tracker failed with error: %llu
Internal error: Tracker is not initialized
Internal error: tracking of one or more of the rectangle corners failed
Internal error: wrong type of a corner tracker object created
v32@?0@"NSString"8@"VNObjectTracker"16^B24
Internal error: Setting input rectangles to one of the rectangle corners failed
Internal error: initialization of internal object
Internal error: wrong type of a corner tracker allocated
No objects to track passed to the tracker
includeClusters
TB,N,V_includeClusters
includeAllImageIdentifiers
TB,N,V_includeAllImageIdentifiers
includeAllImageStats
TB,N,V_includeAllImageStats
%@:%u%u%u
faceDescriptor-current
facerec
VN_smartcam_classifier_debug_intermediates/
VN_DEBUG_DUMP_SMARTCAM_INTERMEDIATES
smartcam-classifier-relationships
smartcam-classifier-labels
smartcam-classifier
smartcam-descriptor
model_junk_12_espresso
VNFaceExpressionDetectorProcessOption_InputFaceObservations
unexpected exception
Corrupt face mark data
VNFaceExpressionDetector face does not have landmark points
Could not create face expression module
unknown exception thrown
Could not read expressions model data
algorithmVersion
VNSceneprint
labelsAndConfidence
length
elementsType
elementsCount
VNDetectorProcessingQueueOption
 (%s)
failure with status %lld
NSException
encountered an unexpected condition: %@
encountered unknown exception
encountered an unexpected condition: %s
dividerScore
Tf,VdividerScore
trueLocalMaximum
Ti,VtrueLocalMaximum
leftImage
Ti,VleftImage
actionAmount
Tf,VactionAmount
noiseThreshold
Tf,VnoiseThreshold
highNoiseThreshold
Tf,VhighNoiseThreshold
VNTextRecognitionOptionNone
VNTextRecognitionOptionASCIICharacterSet
VNTextRecognitionOptionEnglishCharacterSet
VNTextRecognitionOptionDanishCharacterSet
VNTextRecognitionOptionDutchCharacterSet
VNTextRecognitionOptionFrenchCharacterSet
VNTextRecognitionOptionGermanCharacterSet
VNTextRecognitionOptionIcelandicCharacterSet
VNTextRecognitionOptionItalianCharacterSet
VNTextRecognitionOptionNorwegianCharacterSet
VNTextRecognitionOptionPortugueseCharacterSet
VNTextRecognitionOptionSpanishCharacterSet
VNTextRecognitionOptionSwedishCharacterSet
reportCharacterBoxes
TB,N,V_reportCharacterBoxes
algorithm
TQ,N,V_algorithm
minimumCharacterPixelHeight
TQ,N,V_minimumCharacterPixelHeight
detectDiacritics
TB,N,V_detectDiacritics
minimizeFalseDetections
TB,N,V_minimizeFalseDetections
textRecognition
T@"NSString",C,N,V_textRecognition
T@"NSString",C,N
invalid VNRequestOptionDetectTextRectanglesRequiredVersion algorithm value of %lu
Text detector object was not created
ASCII
com.apple.vis.VNPersonsModel
configuration
T@"VNPersonsModelConfiguration",R,C,N
function unavailable
<%@: %p> %lu identities
configuration has already been resolved to %@ and cannot be set to %@
cannot create model with version %u
TQ,R,N,V_version
lastModificationDate
T@"NSDate",R,C,N,V_lastModificationDate
lastModDate
version
T@"NSIndexSet",C,N,V_acceptableVersions
acceptableVersions
B28@?0I8@"NSObject"12^@20
%@ read as %@
com.apple.vis.VNPersonsModelLoader
i12@?0I8
model data cannot be verified due to mismatched checksums
cannot read model version %u
readObjectForVersion%uTag:fromInputStream:intoObjectDictionary:md5Context:error:
cannot accept model version %lu
Tag:fromInputStream:intoObjectDictionary:md5Context:error:
readObjectForVersion
maximumFaceprintsPerIdentity
TQ,N,V_maximumFaceprintsPerIdentity
maximumIdentities
TQ,N,V_maximumIdentities
TQ,N,V_faceprintRequestRevision
faceprintRequestRevision
faceprintsPerIdentity
maxIdentities
       faceprintRequestRevision = %@
   maximumFaceprintsPerIdentity = %lu
              maximumIdentities = %lu
maximumAllowableIdentities
unknown model kind '%@'
faceObservation
T@"VNFaceObservation",R,N,V_faceObservation
predictedPersonUniqueIdentifier
T@"<NSObject><NSCopying><NSSecureCoding>",R,C,N,V_predictedPersonUniqueIdentifier
Tf,R,N,V_confidence
 '%@' confidence %f
personUIDClass
personUID
TQ,N,V_version
readOnly
TB,N,V_readOnly
readonly
incompatible faceprint revision
faceprint is not avilable from the observation
operation was cancelled
VNFaceLandmarkDetectorType
VNFaceDetectorType
VNFaceBoxAlignerType
VNFaceGeometryEstimatorType
VNFaceRegionMapGeneratorType
VNFaceExpressionDetectorType
VNFaceprintGeneratorDetectorType
VNFaceSegmentGeneratorType
VNTorsoprintGeneratorDetectorType
VNFaceQualityGeneratorType
VNRectangleDetectorType
VNJunkIdentifierType
VNSceneClassifierType
VNSingleHeadSceneprintGeneratorType
VNSmartCamClassifierType
VNImageprintGeneratorType
VNSmartCamCombinedAestheticsAndSaliencyDetectorType
VNObjectnessBasedSaliencyDetectorType
VNObjectnessBasedSaliency544x544DetectorType
VNSaliencyHeatmapBoundingBoxGeneratorType
VNHorizonDetectorType
VNImageAnalyzerMultiDetectorType
VNFaceAnalyzerMultiDetectorType
VNANFDMultiDetectorType
v32@?0@"<NSObject><NSCopying>"8@"VNDetector"16^B24
B32@?0@"<NSObject><NSCopying>"8@"VNDetector"16^B24
v32@?0@"NSString"8@"NSDictionary"16^B24
com.apple.VN.
unknown detector type '%@'
%@ must override %@
previous request results
T@"VNClassificationObservation",C,N,V_mostLikelyLabel
allLabelsWithConfidences
T@"NSArray",C,N,V_allLabelsWithConfidences
FAC_LAC
FAC_label
facrRev
VNFaceAttributeCategoryVersion
ageCategory
T@"VNFaceAttributeCategory",&,V_ageCategory
genderCategory
T@"VNFaceAttributeCategory",&,V_genderCategory
eyesCategory
T@"VNFaceAttributeCategory",&,V_eyesCategory
smilingCategory
T@"VNFaceAttributeCategory",&,V_smilingCategory
faceHairCategory
T@"VNFaceAttributeCategory",&,V_faceHairCategory
hairColorCategory
T@"VNFaceAttributeCategory",&,V_hairColorCategory
baldCategory
T@"VNFaceAttributeCategory",&,V_baldCategory
glassesCategory
T@"VNFaceAttributeCategory",&,V_glassesCategory
makeupCategory
T@"VNFaceAttributeCategory",&,V_makeupCategory
makeupEyesCategory
T@"VNFaceAttributeCategory",&,V_makeupEyesCategory
makeupLipsCategory
T@"VNFaceAttributeCategory",&,V_makeupLipsCategory
makeupLips_Cat
makeupEyes_Cat
glasses_Cat
bald_Cat
haircolor_Cat
facehair_Cat
smiling_Cat
eyes_Cat
gender_Cat
age_Cat
farRev
VNFaceAttributesVersion
makeup_Cat
Descriptor count = 
Descriptor length = 
 bytes
 = [
sc_av
VNSmartCamprint
sc_lac
sc_l
sc_et
sc_ec
sc_d
yyyy:MM:dd HH:mm:ss
WARNING: failed to compute ranking %@
The %d argument had an illegal value.
The %d-th diagonal element of the triangular factor of A is zero, so that A does not have full rank; the least squares solution could not be computed.
unknown
VNFaceSegmentGeneratorProcessOption_InputFaceObservations
VNFaceSegmentGeneratorProcessOption_FaceBoundingBoxExpansionRatio
cannot map face segments
rowBytes
Expected labelConfidence map of %lu x %lu and got %lu x %lu
unable to lock pixel buffer
Input face aspect ratio > %f cannot be processed
One of the dimensions of the input face image is zero
faceSemantics_v1.1.espresso
Unexpected request revision
Invalid parameter (numberOfSupportedFaceSegments)
Invalid parameter (size)
Failed to create Face Segmenter object
scenenet_sceneprint_r9_opt_int8.espresso
Unexpected espresso result
Could not run network
Could not bind image to Espresso network
Could not bind output aesthetics scores
Tf,N
filterThreshold
T@"NSArray",&,N,V_filterThreshold
nmsThreshold
Tf,N,V_nmsThreshold
osfsThreshold
Tf,N,V_osfsThreshold
osfsSizeRatio
Tf,N,V_osfsSizeRatio
olmcsThreshold
Tf,N,V_olmcsThreshold
olmcsMergeCountDelta
Ti,N,V_olmcsMergeCountDelta
smartThreshold
Tf,N,V_smartThreshold
smartDistanceFactor
Tf,N,V_smartDistanceFactor
B24@?0@"ShotflowDetection"8@"NSDictionary"16
smartDistance
mergesCount
shotflowNetworkClass
T#,R,D
filterThresholds
T@"NSArray",R,D
networkThreshold
Tf,R
modelName
T@"NSString",R,D
inputLayerName
inputImageSize
T{CGSize=dd},R
inputImageMinDimension
inputImageMaxDimension
inputImageAspectRatio
supportedLabelKeys
T@"NSSet",R
origDim
pcaDim
\s*([^\s]+)\s*->\s*([^\s]+)\s*
espresso-descriptor
espresso-classifier
espresso-classifier-labels
espresso-classifier-relationships
Could not compute image descriptor for image. Error: %s
Image buffer is not initialized
Cannot create observation object
Cannot create image print
Cannot calculate classification image descriptor
Could not compute raw labels and confidence for image
%@ with a %@ is not supported
could not locate %@ in %@
resource key "%@" is not available
%@ must implement +classifierResourceTypesToNames
could not locate the resource file "%@"
%@ must implement %@
acbsBarcodeInfo
T@"NSDictionary",C,N,SsetACBSBarcodeInfo:,V_acbsBarcodeInfo
T@"NSString",R,C,N,V_symbology
T@"CIBarcodeDescriptor",R,N,V_barcodeDescriptor
payloadStringValue
T@"NSString",R,C,N
 (%@)
ACBSBarcodeInfo
barcodeDescriptor
symbology
locateMode
symbologies
failed to analyze image
@"NSString"8@?0
barcode location is not available
unable to create a barcode descriptor for %@
VNDetectBarcodesRequest
creation of a barcode descriptor for %@ is not supported
_new%@DescriptorForACBSBarcodeInfo:
unknown barcode type of '%@'
barcode type is not available
%@ is not a supported barcode symbology
barcode detection requires at least one element in the symbologies property
VNDetectBarcodesLocateModeRegularIntervalVertical
VNDetectBarcodesLocateModeRegularIntervalHorizontal
VNDetectBarcodesLocateModeCenterThreeHorizontalAndCoverageAndDiagonals
VNDetectBarcodesLocateModeCenterThreeVerticalAndCoverageAndDiagonals
VNDetectBarcodesLocateModeCenterFiveEachDirection
VNDetectBarcodesLocateModeCenterOneEachDirection
VNDetectBarcodesLocateModeCenterThreeHorizontalCrossed
VNDetectBarcodesLocateModeCenterThreeHorizontal
VNDetectBarcodesLocateModeCenterOneHorizontalThick
VNDetectBarcodesLocateModeCenterOneHorizontal
VNDetectBarcodesLocateModeCenterThreeVerticalCrossed
VNDetectBarcodesLocateModeCenterThreeVertical
VNDetectBarcodesLocateModeCenterOneVerticalThick
VNDetectBarcodesLocateModeCenterOneVertical
VNDetectBarcodesLocateModeCenterThreeEachDirection
VNDetectBarcodesLocateModeFastSearch
VNDetectBarcodesLocateModeCenterThreeEachDirectionAndCoverageAndDiagonals
availableLocateModes
supportedSymbologies
T@"NSArray",C,N,V_symbologies
T@"NSString",C,N,V_locateMode
v32@?0@"NSString"8@"NSString"16^B24
ageClassifier_W
ageClassifier_b
VNObjectTrackerRevision1Type
VNObjectTrackerRevision2Type
VNRectangleTrackerType
Internal error: Exceeded maximum allowed number of Trackers for a tracker type: %@
Cannot create a Tracker with unknown tracker type: %@
A tracker cannot be created without specifying a unique tracker key
com.apple.VN.trackersCollectionManagementQueue
com.apple.VN.trackingProcessingQueue
v32@?0@"NSString"8#16^B24
v32@?0@"NSUUID"8@"VNTracker"16^B24
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-3.0.59/Libraries/cvml-Core/Clustering/GreedyWithTorso/GreedyWithTorsoClustering.cpp
rotationAngle
Tf,V_rotationAngle
Tf,V_yawAngle
labelKey
Ti,V_labelKey
T^{vImage_Buffer=^vQQQ},R,V_image
imageCVPixelBuffer
T^{__CVBuffer=},R,V_imageCVPixelBuffer
imageFilePath
T@"NSString",&,V_imageFilePath
freeImageInDealloc
TB,V_freeImageInDealloc
externalImageId
T@"NSString",R,V_externalImageId
exifTimestamp
Tq,R,V_exifTimestamp
-[VNMPImageData initWithCVPixelBufferImage:externalImageId:andExifTimestampValue:error:]
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-3.0.59/Libraries/MomentProcessor/Moments/MPImageData.m
-[VNMPImageData initWithVImage:externalImageId:andExifTimestampValue:error:]
ERROR: The input image does not seem to be 8888
no operation point data is available for "%@"
unknown classification identifier "%@"
detectorPreferredImageSize
T@"VNSupportedImageSize",&,N,V_detectorPreferredImageSize
detectorWantsAnisotropicScaling
TB,N,V_detectorWantsAnisotropicScaling
detectorExecutionTimeInterval
Td,N,V_detectorExecutionTimeInterval
T@"VNSupportedImageSize",C,N
Td,N
unable to determine preferred image size for detection
Failed to center square crop the input image
Failed to scale the input image
refineMouthRegion
TB,N,V_refineMouthRegion
refineLeftEyeRegion
TB,N,V_refineLeftEyeRegion
refineRightEyeRegion
TB,N,V_refineRightEyeRegion
performBlinkDetection
TB,N,V_performBlinkDetection
cascadeStepCount
T@"NSNumber",&,N,V_cascadeStepCount
constellation
TQ,N,V_constellation
T@"NSNumber",&,N
VNDetectFaceLandmarksRequest revision %lu doesn't support constellation %lu
VNRequestFaceLandmarksConstellation76Points
VNRequestFaceLandmarksConstellation65Points
VNRequestFaceLandmarksConstellationNotDefined
T@"VNMPImageDescriptor",&,N,V_descriptor
TQ,N,V_type
serializedLength
could not compute faceprint distance
unexpected error while calculating distance between faceprints
faceprints with invalid data supplied
otherImageprint cannot be nil
ipReqRev
ipType
MPImDesc
VNIp
Failed to initialize VNImageprint object
Failed to deserialize requestRevision
Invalid format of VNImageprint serialized state
VNFaceGeometryEstimatorInitOption_ImageSize
VNFaceGeometryEstimatorInitOption_CameraFocalLength
VNFaceGeometryEstimatorProcessOption_EstimatePoseOnly
VNFaceGeometryEstimatorProcessOption_InputFaceObservation
Failed to estimate face geometry
eigenshape
_%llu
emp_data
emp_elementsCount
emp_elementsType
emp_length
emp_labelsAndConfidence
VNEspressoModelImageprint
emp_algorithmVersionCodingKey
emp_algorithmVersion
T@"NSData",&,V_descriptorData
TQ,V_elementCount
lengthInBytes
TQ,V_lengthInBytes
distanceMode
Tq,V_distanceMode
T@"NSDictionary",C,V_labelsAndConfidence
T@"NSString",C,V_version
elementType
confidenceScoreType
TQ,R,N,V_confidenceScoreType
Unexpected error while calculating distance between VNEspressoModelImageprint(s)
VNEspressoModelImageprint(s) with invalid data supplied
Unknown distance funtion requested
VNEspressoModelImageprint(s) with different versions supplied
VNEspressoModelImageprint(s) with different length supplied
nil VNEspressoModelImageprint(s) supplied
Failed to initialize 'print' object
Memory allocation failure
Invalid format of %@ serialized state
Inconsistent intenal state
labelsConfidences
elementCount
descriptorByteLength
descriptorData
unable to create %@ %@ descriptor with length %@
expected descriptor length of %@ does not match the encoded length of %@
invalid element type of %@
descriptor length is unavailable
descriptor data is unavailable
unknown request revision
VNEspressoModelImageprintRequestRevision
currentCodingVersion
TI,R,D
codingTypesToCodingKeys
T@"NSDictionary",R,C,D
serializationMagicNumber
TQ,R,D
currentSerializationVersion
currentVersion
Tf,R,N,V_aestheticScore
wellFramedSubjectScore
Tf,R,N,V_wellFramedSubjectScore
Tf,R,N
wellChosenBackgroundScore
Tf,R,N,V_wellChosenBackgroundScore
tastefullyBlurredScore
Tf,R,N,V_tastefullyBlurredScore
sharplyFocusedSubjectScore
Tf,R,N,V_sharplyFocusedSubjectScore
wellTimedShotScore
Tf,R,N,V_wellTimedShotScore
pleasantLightingScore
Tf,R,N,V_pleasantLightingScore
pleasantReflectionsScore
Tf,R,N,V_pleasantReflectionsScore
harmoniousColorScore
Tf,R,N,V_harmoniousColorScore
livelyColorScore
Tf,R,N,V_livelyColorScore
pleasantSymmetryScore
Tf,R,N,V_pleasantSymmetryScore
pleasantPatternScore
Tf,R,N,V_pleasantPatternScore
immersivenessScore
Tf,R,N,V_immersivenessScore
pleasantPerspectiveScore
Tf,R,N,V_pleasantPerspectiveScore
pleasantPostProcessingScore
Tf,R,N,V_pleasantPostProcessingScore
noiseScore
Tf,R,N,V_noiseScore
failureScore
Tf,R,N,V_failureScore
pleasantCompositionScore
Tf,R,N,V_pleasantCompositionScore
interestingSubjectScore
Tf,R,N,V_interestingSubjectScore
intrusiveObjectPresenceScore
Tf,R,N,V_intrusiveObjectPresenceScore
pleasantCameraTiltScore
Tf,R,N,V_pleasantCameraTiltScore
lowKeyLightingScore
Tf,R,N,V_lowKeyLightingScore
  %@=%@
LOWKEY
HCOL
CTILT
INTRUSIVE
INTREST
PCOMP
FAIL
NOISE
PPOST
PPERS
PPAT
PSYM
LCOL
PREF
PLHT
OAES
VNImageAestheticsObservation
Score
aestheticScore
wellChosenSubjectScore
VNFaceRegionMapGeneratorProcessOption_InputFaceObservations
faceRegionMap-current
Smile
Disgust
Neutral
Surprise
Scream
Suspicious
T@"NSUUID",&,N,SsetUUID:,V_uuid
Tf,N,V_confidence
%@ %@ requestRevision=%lu confidence=%f
uuid
VNObservation
T{CGRect={CGPoint=dd}{CGSize=dd}},N,V_boundingBox
T@"NSUUID",C,V_identifier
%@ boundingBox=%@
VNDetectedObjectObservation
T@"VNFaceLandmarks2D",&,N,V_landmarks
hasBBoxBeenAligned
TB,N,V_hasBBoxBeenAligned
alignedBoundingBox
T{_Geometry2D_rect2D_={_Geometry2D_point2D_=ff}{_Geometry2D_size2D_=ff}},N,V_alignedBoundingBox
Tf,N,V_alignedRotationAngle
landmarkPoints
T@"NSData",&,N,V_landmarkPoints
landmarkPoints65
T@"NSData",&,N,V_landmarkPoints65
TQ,N,V_landmarksConstellation
landmarkPrecisionEstimatesPerPoint
T@"NSArray",&,N,V_landmarkPrecisionEstimatesPerPoint
landmarkOcclusionFlagsPerPoint
T@"NSArray",&,N,V_landmarkOcclusionFlagsPerPoint
landmarkPoints3d
T@"NSData",&,N,V_landmarkPoints3d
poseData
T@"NSData",&,N,V_poseData
faceIdConfidence
Tf,N,V_faceIdConfidence
T@"NSData",&,N,V_alignedMeanShape
T@"VNTorsoprint",&,N
T@"NSNumber",&,N,V_roll
T@"NSNumber",&,N,V_yaw
landmarksRequestRevision
TQ,N,V_landmarksRequestRevision
landmarks3DRequestRevision
TQ,N,V_landmarks3DRequestRevision
unalignedBoundingBox
T{CGRect={CGPoint=dd}{CGSize=dd}},R,N,V_unalignedBoundingBox
landmarks3d
T@"VNFaceLandmarks3D",R,N
T@"VNFaceLandmarks2D",R,N
T@"VNFaceRegionMap",R,N,V_faceRegionMap
T@"VNFaceAttributes",R,N,V_faceAttributes
expressionsAndConfidence
T{?=[4]},R,N
poseQuaternion
T{?=},R,N
nameConfidence
TQ,N,V_faceId
T@"VNFaceprint",&,N,V_faceprint
T@"VNFaceTorsoprint",&,N
faceSegments
T@"VNFaceSegments",R,N,V_faceSegments
Tf,R,N,GfaceJunkinessIndex
Tf,R,N,GfaceOrientationIndex
T@"NSNumber",R,N,V_faceCaptureQuality
 VNFaceLandmarks2D [%@, confidence=%f]
 ID=%lu
faceLM3DRequestRev
faceLMRequestRev
faceOrientationIndex
faceJunkinessIndex
faceCaptureQuality
landmarksScore
blinkScore
alignedMeanShape
blinking
faceSmntcSegments
faceAttributes
faceRegionMap
roll
alignedRotAngle
hasAlignedBBox
faceTorsoprint
torsoprint
faceID
faceIDConfidence
expressions
pose
landmarks3D
landmarksConstellation
occlusions
precisionEstimates
landmarks65
unalignedBBH
unalignedBBW
unalignedBBY
unalignedBBX
alignedBBH
alignedBBW
alignedBBY
alignedBBX
VNFaceObservation
invalid pose data
face orientation
exifOrientation cannot be null
T@"VNImageRegistrationSignature",&,N,V_referenceImageSignature
T@"VNImageRegistrationSignature",&,N,V_floatingImageSignature
T{CGAffineTransform=dddddd},N
%@ is not supported by %@
floatingImageSignature
referenceImageSignature
T{CGAffineTransform=dddddd},N,V_alignmentTransform
alignmentTransform
T{?=[3]},N,V_warpTransform
warpTransform
blurScore
T@"NSNumber",&,N,V_blurScore
exposureScore
T@"NSNumber",&,N,V_exposureScore
exposure
imageprint
T@"VNImageprint",&,N,V_imageprint
imageprintValid
TB,R,N,V_imageprintValid
imageprintVersion
T@"NSString",R,C,N,V_imageprintVersion
rawImageprintDescriptor
T@"NSData",R,N
VNImageprint
VNImageprintObservation
descriptor
Failed creating a new VNImageprintObservation object
nil imageprint supplied
T@"NSString",R,C,N,V_identifier
%@ cannot provide operation points
 (P/R)
 "%@"
operationPoints
identifier
labels
T@"NSArray",R,C,N,V_labels
T@"MLFeatureValue",R,C,N,V_featureValue
T@"NSString",R,C,N,V_featureName
%@ "%@" - "%@" (%f)
featureName
featureValue
T^{__CVBuffer=},R,N,V_pixelBuffer
vnpbo_pbdict
topLeft
T{CGPoint=dd},R,N,V_topLeft
topRight
T{CGPoint=dd},R,N,V_topRight
bottomLeft
T{CGPoint=dd},R,N,V_bottomLeft
bottomRight
T{CGPoint=dd},R,N,V_bottomRight
T{CGAffineTransform=dddddd},N,V_transform
Td,N,V_angle
transform
angle
objects
T@"NSArray",&,N,V_objects
clusterId
TQ,N,V_clusterId
totalObjectCount
TQ,N,V_totalObjectCount
shouldUpdateRepresentative
TB,N,V_shouldUpdateRepresentative
suggestedIdsForRepresentative
T@"NSArray",&,N,V_suggestedIdsForRepresentative
representativenessById
T@"NSDictionary",&,N,V_representativenessById
vncRepnessById
vncRepIds
vncRepUpdate
vncTotObjCount
vncCId
vncObjects
vncluster
  representativenessById = %@;
  suggestedIdsForRep = %@;
  shouldUpdateRep = %d;
  objects = %@;
  totalObjCount = %lu;
  clusterId = %lu;
T@"NSArray",&,N,V_clusters
suggestionsForCluster
T@"NSArray",&,N,V_suggestionsForCluster
T@"NSData",&,N,V_clusterState
clusteredFaceIds
T@"NSSet",&,N,V_clusteredFaceIds
groupedClusteredFaceIdsForCluster
T@"NSArray",&,N,V_groupedClusteredFaceIdsForCluster
distance
T@"NSNumber",&,N,V_distance
distancesById
T@"NSDictionary",&,N,V_distancesById
distancesByID
level0Distance
groupedClusteredFaceIDs
clusteredFaceIDs
clusterState
suggestions
clusters
T@"NSData",R
One or more of the feature prints are empty
The observations do not have a feature print that match each others format
The revision of the observations do not match
sceneprints
T@"NSArray",R,N,V_sceneprints
sceneprintVersion
T@"NSString",R,C,N,V_sceneprintVersion
descriptors
algo
VNSceneObservation
Undefined
smartCamprints
T@"NSArray",C,N,V_smartCamprints
smartCamprintVersion
T@"NSString",R,C,N,V_smartCamprintVersion
sc_descriptors
sc_algo
VNSmartCamObservation
allImageIdentifiers
T@"NSArray",&,N,V_allImageIdentifiers
bestImageIdentifiers
T@"NSArray",&,N,V_bestImageIdentifiers
allImageStats
T@"NSDictionary",&,N,V_allImageStats
coverImageIdentifier
T@"NSString",&,N,V_coverImageIdentifier
TB,N,V_isAction
TB,N,V_isPortrait
isPortrait
isAction
coverImage
imageStats
bestImages
allImages
T@"NSArray",C,N,V_textObjects
T@"NSString",R,C,N,V_text
textObjects
text
characterBoxes
'%@' is not a valid matrix_float4x4 encoding
4x4:|
4x4:|%g %g %g %g %g %g %g %g %g %g %g %g %g %g %g %g|
'%@' is not a valid matrix_float3x3 encoding
3x3:|
3x3:|%g %g %g %g %g %g %g %g %g|
'%@' is not a valid CGAffineTransform encoding
[%g %g %g %g %g %g]
regionOfInterest
T{CGRect={CGPoint=dd}{CGSize=dd}},N,V_regionOfInterest
T@"NSArray",C,N,V_inputFaceObservations
T{CGRect={CGPoint=dd}{CGSize=dd}},N
B32@?0@"VNFaceObservation"8Q16^B24
 inputFaceObservations=[%@]
%@ ROI=%@
The region of interest [%g, %g, %g, %g] is not within the normalized bounds of [0 0 1 1]
Unspecified error
face
com.apple.espresso.mainqueue
pixelBuffer is not in correct format. (Required format is one component, 32-float)
pixelBuffer cannot be null
Unable to generate smoothed float-32 image buffer
{CGRect={CGPoint=dd}{CGSize=dd}}
Internal error:  detection of Animals should be handled by ANFD Detector Compound Request
VNImageGrouperProcessOption_Threshold
VNImageGrouperProcessOption_AdjustDistancesWithTimestamp
Input imageprint array contains an item that is not an imageprint
Cannot create VNMomentProcessor object
vnpbo_bpr
vnpbo_bytes
%@_%zu
vnpbo_attach
vnpbo_attribs
vnpbo_pixelFormat
vnpbo_height
vnpbo_width
tplTracker_FFT_3324
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-3.0.59/Libraries/cvml-Core/ObjectTracker/temporalEx/tplTrackerFFT.c
(outputIndex >= 0) && (outputIndex < 72)
tplTracker_IFFT_3324
Internal error: processing of Face Attributes should be handled by Face Analyzer Compound Request
faceIDModel
unable to serialize the face ID model (status = %@)
serialNumberToIdentifier
maximumElementsPerID
Face ID model data deserialization failed with code %@
v32@?0@"NSNumber"8@"<NSObject><NSCopying><NSSecureCoding>"16^B24
q24@?0@"VNPersonsModelPrediction"8@"VNPersonsModelPrediction"16
A prediction for an unknown identity with serial number %@ and confidence %f was provided
v32@?0@"<NSObject><NSCopying><NSSecureCoding>"8@"NSNumber"16^B24
mismatched faceprint request revision for observation at index %lu, person at index %lu
invalid face observation at index %lu for person at index %lu
heal
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-3.0.59/Libraries/RectangleDetector/QuadDetect/Contours.c
(endpoint2 == 4)|| (endpoint2 == 8)
(endpoint2 == 8)|| (endpoint2 == 6)
(endpoint2 == 6)|| (endpoint2 == 2)
(endpoint2 == 2)|| (endpoint2 == 4)
healCenters
ady == 2
AdjacentContourHeal
PCOORD_EQUAL(joinPoint, LAST_PCOORD(cPtrN))
PCOORD_EQUAL(joinPoint, FIRST_PCOORD(cPtrN))
testJoin
(orn >= 0) && (orn <= 8)
orn != 4
landmarks
landmarks_63
landmarks_76
confidence
occlusion
output matrix size too small
matrix size too small for output
broadcast op: dimension mismatch
unknown axis value
dimensions of data points mismatch
output distance matrix too small
matrix size mismatch
empty cumsum vector
vector length < rows
vector length < cols
row index out of range
col index out of range
aligned buffer allocation of 
 exceeded calculated size of 
matrix vector size mismatch
vector size too small for output
3.000 - Oct 20, 2016
burst_mode_logging
staccato_mode_logging
burst_max_pending_frames
burst_disable_analysis
burst_force_face_detection
burst_force_face_detail_detection
burst_dummy_analysis
burst_disable_facecore
burst_use_fixed_image
burst_fixed_image_filename
burst_dump_yuv
staccato_yuv_dump
burst_use_version
BurstSet_AlgorithmVersion
Image_FacesArray
Image_ISPFacesArray
Image_ImageScore
Image_Timestamp
Image_YUVData
ImageYUVWidth
ImageYUVHeight
ImageYData
ImageUVData
ImageYUVBytesPerRow
Image_TimeReceived
Image_TimeQueued
Image_TimeConverted
Image_TimeStartedAnalysis
Image_TimeStartedFaceDetection
Image_TimeDoneFaceDetection
Image_TimeDoneFaceBlinkDetection
Image_TimeDoneFaceFocusScore
Image_TimeDoneAnalysis
ImageFace_ID
ImageFaceX
ImageFaceY
ImageFaceW
ImageFaceH
ImageFaceFocusScore
ImageFaceLeftEyeOpen
ImageFaceRightEyeOpen
ImageFaceSmiling
ImageFaceLeftEyePosX
ImageFaceLeftEyePosY
ImageFaceRightEyePosX
ImageFaceRightEyePosY
ImageFaceTimestamp
ImageFaceRollAngle
ImageFaceYawAngle
ImageFacePitchAngle
ImageFaceLeftEyeBlinkScore
ImageFaceRightEyeBlinkScore
ImageFaceSmileScore
ImageFaceSmallFace
loggingCallback
T@?,C,N,V_loggingCallback
faceAnalysisContext
T@"BurstImageFaceAnalysisContext",&,V_faceAnalysisContext
overrideImage
T@"VNImageBuffer",&,V_overrideImage
overrideProps
T@"NSDictionary",&,V_overrideProps
clusterArray
T@"NSMutableArray",&,V_clusterArray
Ti,V_temporalOrder
faceIDCounts
T@"NSCountedSet",&,V_faceIDCounts
T@"NSMutableArray",&,V_allImageIdentifiers
statsByImageIdentifier
T@"NSMutableDictionary",&,V_statsByImageIdentifier
clusterByImageIdentifier
T@"NSMutableDictionary",&,V_clusterByImageIdentifier
burstLogFileName
T@"NSString",&,V_burstLogFileName
actionClassifier
T@"BurstActionClassifier",&,V_actionClassifier
maxNumPendingFrames
Ti,V_maxNumPendingFrames
enableAnalysis
TB,V_enableAnalysis
dummyAnalysisCount
Ti,V_dummyAnalysisCount
enableFaceCore
TB,V_enableFaceCore
enableDumpYUV
TB,V_enableDumpYUV
burstCoverSelection
T@"NSString",&,V_burstCoverSelection
burstId
T@"NSString",&,V_burstId
bestImageIdentifiersArray
T@"NSArray",&,V_bestImageIdentifiersArray
versionString
T@"NSString",&,V_versionString
  <CVMLBurst> Trying to write xml file to '%s'
BurstSet_CoverImage
BurstSet_IsPortrait
BurstSet_IsAction
BurstSet_Setting_EnableDumpYUV
BurstSet_Setting_DummyAnalysisCount
BurstSet_Setting_DisableFaceCore
BurstSet_Setting_DisableAnalysis
BurstSet_Setting_MaxNumPendingFrames
BurstDoc_LogFile
BurstDoc_AllImageStats
BurstDoc_AllImageIdentifiers
BurstDoc_BestImageIds
BurstSet_TimeDone
BurstSet_TimeDoneCapturing
too many analysis frames pending
analysis is disabled
Image_AEMatrix
Image_Orientation
Image_AFStable
Image_AEStable
Image_AETarget
Image_AEAverage
Image_Height
Image_Width
Image_FaceRectROI
burstimage_%06d.yuv
counter.bin
foundByISP
SmileFeaturesSize
BlinkFeaturesSize
SmileFeaturesOffset
RightEyeFeaturesOffset
LeftEyeFeaturesOffset
com.apple.burstAnalyzer
kern.osversion
com.apple.staccato_dump
dd-MM-yyyy'_'HH-mm-ss'_burstLog.txt'
/var/mobile/Library/Caches/com.apple.camera
com.apple.camera
burstSets
Internal error:  detection of Human Torsos should be handled by ANFD Detector Compound Request
network
T{?=^vi},R,V_network
plan
T^v,R,V_plan
T^v,R,V_context
T@"NSString",R,V_modelName
Model name: %@, network: %p, plan: %p: context: %p
Could not feed-forward buffer data becuase of compatibility of source and destination buffers
Could not bind input buffer to Espresso network
Could not bind output buffer to Espresso network
Espresso network cannot be nil
Could not enable Montreal feature in Espresso
Unknown Espresso buffer type
espresso buffer image with row bytes size of %ld cannot be rendered into a pixel buffer with %lu bytes per row
Unsupported espresso buffer storage type (%lu)
could not lock pixel buffer
espresso buffer image with dimensions %ld x %ld cannot be rendered into a pixel buffer with dimensions %ld x %ld
Error allocating %lu x %lu CVPixelBuffer with format %lu
Unsupported pixel format %lu
Could not build Espresso plan
Could not declare Espresso network output buffer: %@
Could not declare Espresso network input buffer: %@
Could not create/add network to Espresso plan
Could not create Espresso plan
Could not create Espresso context
Invalid inputs specified to Espresso Plan Builder
unable to locate resource "%@" of type "%@" in %@
unable to locate resource bundle
Unable to create vImage_Buffer from CIImage
Unable to create CGImage from CIImage
Unable to create CIImage
^ +| +$)
true
ERTFaceBox::ERTDefaultFeatureValue
Error: 
 failed to load from ERT model file!
ERROR: ERTDefaultFeatureValue failed to load from ERT model file!
ERTFaceBox::ERTDefaultPixelValue
ERROR: ERTDefaultPixelValue failed to load from ERT model file!
ERTFaceBox::ERTNumXYPairs
ERTFaceBox::ERTXYPairs
 unexpected size of value
ERTFaceBox::ERTGlobalShift
ERTFaceBox::ERTNumCascadeStages
ERTFaceBox::ERTNumTrees
ERTFaceBox::ERTNumPredictions
ERTFaceBox::ERTNodesThresholds
ERTFaceBox::ERTNodesPredictions
ERTFaceBox::ERTNodesFeatureIDs
ERTFaceBox::ERTNodesLeafFlags
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-3.0.59/Libraries/RectangleDetector/QuadDetect/Spans.c
spl != NULL
addSpan
VNFaceLandmarkDetectorProcessOption_RefineLeftEye
VNFaceLandmarkDetectorProcessOption_RefineRightEye
VNFaceLandmarkDetectorProcessOption_RefineMouth
VNFaceLandmarkDetectorProcessOption_BlinkDetection
VNFaceLandmarkDetectorProcessOption_CascadeStepCount
VN facelandmark2d debug intermediates written to: %@
outputLandmarks
inputMeanShape
aligned
_info.json
_output_intermediate_landmarks.png
_input_intermediate_meanShape.png
_input_intermediate_image.png
_input_intermediate_image.vdump
VN_facelandmark2d_debug_intermediates/
VN_DEBUG_DUMP_FACE_LANDMARK_2D_INTERMEDIATES
Landmark Detector did not provide any data
landmarks_v2
mouth
lefteye
righteye
Landmark algorithm is not initialized
Could not read landmark model data
Invalid landmark model resource path
isValid
TB,R,N,V_isValid
needConversionBGRA2YUVA
TB,N,V_needConversionBGRA2YUVA
Ti,R,N,V_width
Ti,R,N,V_height
nscales
Ti,R,N,V_nscales
streamFrameCount
Ti,R,N,V_streamFrameCount
nwarpings
Ti,N,V_nwarpings
useNonLocalRegularization
TB,N,V_useNonLocalRegularization
nlreg_radius
Ti,N,V_nlreg_radius
nlreg_padding
Ti,N,V_nlreg_padding
nlreg_sigma_l
Tf,N,V_nlreg_sigma_l
nlreg_sigma_c
Tf,N,V_nlreg_sigma_c
nlreg_sigma_w
Tf,N,V_nlreg_sigma_w
LKT:ComputeFlow level %d
lkt_bgra2yuva
lkt_zero_flow
lkt_downscale_2x
lkt_features
lkt_features_derivatives
lkt_solver_prepare_matrices
lkt_solver_box7_y
lkt_solver_box_x_and_Axb
lkt_nlreg_hegbf
The number of scales specified is too large
LKT::Pyramid
unexpected optical flow estimation failure
could not bind pixel buffer to texture
LKT:waitUntilCompleted
Odd image dimensions are not supported
.json
VN_junk_classifier_debug_intermediates
Trying to run junk classifier when the classifier failed to initialize
junk
junk-classifier-labels-current
junk-classifier-current
junk-descriptor-current
VN_DEBUG_DUMP_JUNK_INTERMEDIATES
input__0
add3__0
 VNRequestOptionRectangleMaximumNumber value is out of bounds: %d
 VNRequestOptionRectangleMinimumConfidence value is out of bounds: %f
 VNRequestOptionRectangleMinimumSize value is out of bounds: %f
 VNRequestOptionRectangleQuadratureTolerance value is out of bounds: %f
 VNRequestOptionRectangleMinimumAspectRatio value, %f is greater than VNRequestOptionRectangleMaximumAspectRatio value, %f
 VNRequestOptionRectangleMaximumAspectRatio value is out of bounds: %f
 VNRequestOptionRectangleMinimumAspectRatio value is out of bounds: %f
 PixelFocalLength value is out of bounds: %f
 VNRequestOptionInputRegionOfInterest value malformed: %@
 VNRequestOptionRectangleDetectorRequiredVersion value is out of bounds: %d
VNTorsoprintGeneratorProcessOption_InputFaceObservations
Error in calculating torso bounding box dimensions
faceOrientationRelativeToUpright
Memory for torso bouding box is not allocated
torso_print__0
Espresso output results dimensions are incorrect
Could not run Espresso network
bodynet_v1.0.espresso
TQ,N,V_maximumIntermediateSideLength
TQ,N,V_blurDeterminationMethod
blurDeterminationMethod
maximumIntermediateSideLength
VNFaceDetectorInitOption_MinFaceSize
VNFaceDetectorInitOption_EnableLowMemoryMode
%@_face_%d
VN_DEBUG_DUMP_FACE_DETECT_INTERMEDIATES
VNDetectFaceRectanglesRequest
faceBoundingBox
T{CGRect={CGPoint=dd}{CGSize=dd}},V_faceBoundingBox
pointCount
TQ,V_pointCount
%@ faceBoundingBox=%@ pointCount=%lu requestRevision=%lu
FLMReg_Rev
FLMReg_PtCnt
FLMReg_BBH
FLMReg_BBW
FLMReg_BBY
FLMReg_BBX
VNFaceLandmarkRegion
points
Tr^,R,V_points
occlusionFlagsPerPoint
T@"NSArray",R,V_occlusionFlagsPerPoint
normalizedPoints
Tr^{CGPoint=dd},R
precisionEstimatesPerPoint
T@"NSArray",R,V_precisionEstimatesPerPoint
{CGSize=dd}
FLMs_PtsOcc
FLMs_PtsAE
FLMReg2D_PtsData
VNFaceLandmarkRegion2D
[VNFaceLandmarkRegion2D -initWithRequestRevision:faceBoundingBox:] is not available, use class designated initializers
Tr^,V_points
FLMReg3D_PtsData
VNFaceLandmarkRegion3D
[VNFaceLandmarkRegion3D -initWithRequestRevision:faceBoundingBox:] is not available, use class designated initializers
pointsData
T@"NSData",&,V_pointsData
alignedBBox
T{_Geometry2D_rect2D_={_Geometry2D_point2D_=ff}{_Geometry2D_size2D_=ff}},V_alignedBBox
userFacingBBox
T{CGRect={CGPoint=dd}{CGSize=dd}},V_userFacingBBox
Tf,R,V_confidence
%@ pointCount=%lu requestRevision=%lu
Can't use abstract base class. Must be VNVaceLandmark2D or 3D
FLMs_Rev
FLMs_UsrFacingBBoxH
FLMs_UsrFacingBBoxW
FLMs_UsrFacingBBoxY
FLMs_UsrFacingBBoxX
FLMs_AlgnBBoxH
FLMs_AlgnBBoxW
FLMs_AlgnBBoxY
FLMs_AlgnBBoxX
FLMs_PtsData
FLMs_PtsCnt
FLMs_Conf
VNFaceLandmarks
TQ,V_constellation
T@"NSArray",&,V_precisionEstimatesPerPoint
T@"NSArray",&,V_occlusionFlagsPerPoint
allPoints
T@"VNFaceLandmarkRegion2D",R,V_allPoints
faceContour
T@"VNFaceLandmarkRegion2D",R,V_faceContour
leftEye
T@"VNFaceLandmarkRegion2D",R,V_leftEye
rightEye
T@"VNFaceLandmarkRegion2D",R,V_rightEye
leftEyebrow
T@"VNFaceLandmarkRegion2D",R,V_leftEyebrow
rightEyebrow
T@"VNFaceLandmarkRegion2D",R,V_rightEyebrow
nose
T@"VNFaceLandmarkRegion2D",R,V_nose
noseCrest
T@"VNFaceLandmarkRegion2D",R,V_noseCrest
medianLine
T@"VNFaceLandmarkRegion2D",R,V_medianLine
outerLips
T@"VNFaceLandmarkRegion2D",R,V_outerLips
innerLips
T@"VNFaceLandmarkRegion2D",R,V_innerLips
leftPupil
T@"VNFaceLandmarkRegion2D",R,V_leftPupil
rightPupil
T@"VNFaceLandmarkRegion2D",R,V_rightPupil
FLMs2D_PtsOcc
FLMs2D_PtsAE
FLMs2D_CType
VNFaceLandmarks2D
T@"VNFaceLandmarkRegion3D",R,V_allPoints
T@"VNFaceLandmarkRegion3D",R,V_faceContour
T@"VNFaceLandmarkRegion3D",R,V_leftEye
T@"VNFaceLandmarkRegion3D",R,V_rightEye
T@"VNFaceLandmarkRegion3D",R,V_leftEyebrow
T@"VNFaceLandmarkRegion3D",R,V_rightEyebrow
T@"VNFaceLandmarkRegion3D",R,V_nose
T@"VNFaceLandmarkRegion3D",R,V_noseCrest
T@"VNFaceLandmarkRegion3D",R,V_medianLine
T@"VNFaceLandmarkRegion3D",R,V_outerLips
T@"VNFaceLandmarkRegion3D",R,V_innerLips
VNFaceLandmarks3D
straightLineLSQ
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-3.0.59/Libraries/RectangleDetector/QuadDetect/Leq.c
maxDev != -1.f
straightLineWLSQ
cluster IDs
face observations
All elements in the %@ array must be a VNRequest subclass (%@)
All elements in the %@ array must be a Class object (%@)
The %@ array has %lu items, which is more than the maximum allowed of %lu
The %@ array has %lu items, which is less than the required count of %lu
%@ is nil
array
expectedAncestoralClass
%@ was given %@
zero-dimensioned image (%ld x %ld)
Unexpected landmarks constellation (%d) while processing Face Landmarks
Internal error while processing Face Landmarks
landmarksflow-gwkf986dmy_63053_plus_8dtz95rnyx_quantized.espresso
identifierCount
allIdentifiers
T@"NSSet",R,C
T@"NSData",&,V_faceprint
T@"NSString",C,V_key
platform
TI,V_platform
profile
TI,V_profile
faceprintInputPath
T@"NSString",C,V_faceprintInputPath
CVMLFaceprint_ProfileCodingKey
CVMLFaceprint_PlatformCodingKey
CVMLFaceprint_KeyCodingKey
CVMLFaceprint_FaceprintCodingKey
CVMLFaceprint_CodingVersionCodingKey
CVMLImageprintObservation
CVMLImageprintObservation_ImageprintDescriptorColorGaborVersion
CVMLImageprintObservation_ImageprintTypeCodingKey
CVMLImageprintObservation_UUIDCodingKey
CVMLImageprintObservation_ImageprintDescriptorCodingKey
CVMLImageprintObservation_VersionCodingKey
CVMLImageprintObservation_ObjectCodingKey
CVMLImageprintObservation_ImageprintTypeColorGabor
MPImageDescriptor
descriptorId
Tq,R,V_descriptorId
quality
Tf,R,V_quality
colorGaborDescriptor
T^v,R,V_colorGaborDescriptor
sceneClassifierDescriptor
T^v,R,V_sceneClassifierDescriptor
imageRegistrationDescriptor
T^v,R,V_imageRegistrationDescriptor
previousLeafId
Tq,V_previousLeafId
nextLeafId
Tq,V_nextLeafId
nextLeafDescriptorDistance
Tf,V_nextLeafDescriptorDistance
previousLeafDescriptorDistance
Tf,V_previousLeafDescriptorDistance
nextLeafTimestampDistance
Tq,V_nextLeafTimestampDistance
previousLeafTimestampDistance
Tq,V_previousLeafTimestampDistance
nextLeafTotalDistance
Tf,V_nextLeafTotalDistance
previousLeafTotalDistance
Tf,V_previousLeafTotalDistance
rawColorGaborDescriptor
T@"NSData",R,V_rawColorGaborDescriptor
T@"NSString",R,V_imageFilePath
ERROR: invalid image Id format
ERROR: state cannot be nil
MPImageDescriptor_ColorGaborImageDescriptorBuffer_stride
MPImageDescriptor_ColorGaborImageDescriptorBuffer_count
MPImageDescriptor_ColorGaborImageDescriptorBuffer_data
MPImageDescriptor_ColorGaborImageDescriptorBuffer_lengthInBytes
MPImageDescriptor_ColorGaborImageDescriptorBuffer_type
MPImageDescriptor_quality
MPImageDescriptor_exifTimestamp
MPImageDescriptor_externalImageId
CVMLObservation
CVMLObservation_ConfidenceCodingKey
CVMLObservation_CodingVersionCodingKey
Tp_algorithmVersion
Tp_VNTorsoprint
Tp_labelsAndConfidence
Tp_length
Tp_elementsType
Tp_elementsCount
Tp_data
featureNames
T@"NSSet",R,N
v16@?0^v8
cachingIdentifier
T@"<NSObject><NSCopying>",R,C,N
model
T@"MLModel",&,V_model
modelType
Ti,V_modelType
inputImageKey
T@"NSString",&,V_inputImageKey
predictedFeatureKey
T@"NSString",&,V_predictedFeatureKey
predictedProbabilitiesKey
T@"NSString",R,V_predictedProbabilitiesKey
boundingBoxOutputDescription
T@"MLObjectBoundingBoxOutputDescription",R,V_boundingBoxOutputDescription
inputImageWidth
TQ,R,V_inputImageWidth
inputImageHeight
TQ,R,V_inputImageHeight
inputImageFormat
TI,R,V_inputImageFormat
scenePrintRevision
TQ,R,V_scenePrintRevision
inputScenePrintKey
T@"NSString",&,V_inputScenePrintKey
inputScenePrintMLMultiArrayDataType
Tq,R,V_inputScenePrintMLMultiArrayDataType
inputImageFeatureName
featureProvider
T@"<MLFeatureProvider>",&,N,V_featureProvider
%@:UUID=%@
The requested FeaturePrint.scene is not available. Requested revision: %lu
The model does not have a valid input feature of type image
T@"VNCoreMLModel",R,V_model
could not obtain a feature value for key "%@"
q24@?0@"VNClassificationObservation"8@"VNClassificationObservation"16
v32@?0@8Q16^B24
The confidence scores don't line up with the labls.
The outputs of the model are of unexpected types.
The inputImageFeatureName does not point to a MLFeatureTypeImage input.
No valid VNCoreMLModel found in passed in options
VNImageAnalyzerMultiDetectorSceneNetV3R8
no data source available
plist
Identifier
no %@ is defined at %@
recall
precision
threshold
operation point map data for "%@" is corrupt
unable to open %@
invalid recall table for "%@"
missing recall table for "%@"
invalid precision table for "%@"
missing precision table for "%@"
missing F2 for "%@"
missing threshold for "%@"
could not locate the %@ resource for operation points identifier "%@"
unknown operation points identifier "%@"
scenenet_op-v8d.plist
MinConfidenceForHierarchical
hierarchicalLabelsAndConfidence
MinConfidenceForClassificationRaw
debugID
.vdump
.png
_tile_
imageID
numTiles
augmentationMode
scalingFactor
VN Image Classifier debug intermediates written to: %@
_source_scaled.vdump
_source_scaled.png
B8@?0
timeStamp
T@"NSNumber",C,N,V_timeStamp
Attempt to create an imageprint failed
Attempt to create an image print request without a timestamp
FaceRegionMap::Width
FaceRegionMap::Height
FaceRegionMap::Data
FaceRegionMap::Triangles
FaceRegionMap::NormalizedLandmarks
Background
Left eye
Right eye
Left eyebrow
Right eyebrow
Root of nose
Nose
Chin
Lower left cheek
Lower right cheek
Between mouth and nose
Left cheek
Right cheek
Left temple
Right temple
Between eyebrows
Above left eye
Above right eye
Upper lip
Lower lip
Between lips
Forehead
Tip of nose
NtCreatePixelBuffer
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-3.0.59/Libraries/RTCV/frameworks/AppleNeuralTracker/ntModel.cpp
ret == kCVReturnSuccess
net_exempler_reg
net_exempler_cls
r1_kernel
cls1_kernel
classification_x_corr
regress_adjust
EspressoNetUnload
status == ESPRESSO_STATUS_SUCCESS
image
EspressoNetExemplarRun
instance_image
EspressoNetInstanceRun
outputBufferWidth
TQ,R,N,V_outputBufferWidth
outputBufferHeight
TQ,R,N,V_outputBufferHeight
outputBufferData
T@"NSData",R,N,V_outputBufferData
numberOfFaceSegments
TQ,R,N,V_numberOfFaceSegments
faceSegmentLabelToProbabilityMap
T@"NSDictionary",R,N,V_faceSegmentLabelToProbabilityMap
T{CGRect={CGPoint=dd}{CGSize=dd}},R,N,V_boundingBox
Cannot create CVPixelBuffer object. Error = %d
Cannot copy face segment probability map. Error = %d
Cannot create CVPixelBuffer object: region parameter is out of range
Cannot create CVPixelBuffer object: faceSegment parameter is out of range
Cannot create CVPixelBuffer object: faceSegments is out of range
Cannot create CVPixelBuffer object
fsLblToProbMap
fsBBoxSzH
fsBBoxSzW
fsBBoxOrgY
fsBBoxOrgX
fsNumOfSgmnts
fsData
fsHeight
fsWidth
fsRev
VNFaceSegmentsVersion
faceSegmentIndexToFlagMap
T@"NSDictionary",R
faceSegmentToSegmentMaskGrayLevelDictionary
Object identifier is not initialized in detected object observation
VNDetectedObjectObservation object is expected to initialize Object Tracker
burstImages
T@"NSMutableArray",&,V_burstImages
imageProps
T@"NSMutableDictionary",&,V_imageProps
completionBlock
T@?,C,V_completionBlock
imagePixelBuffer
T^{__CVBuffer=},V_imagePixelBuffer
baseAddress
Tr^v,R,N
resourcePath
T@"NSString",&,N
void cvml::util::mapped_model_file::advise(int) const
Error %s when executing %s in file %s:%d
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-3.0.59/VisionKitFramework/VN/algorithm_util/mapped_model_file.h
mmap MAP_FAILED
cvml::util::mapped_model_file::mapped_model_file(const char *, bool)
syslog_assert_failed
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-3.0.59/VisionKitFramework/VN/algorithm_util/common_defines.h
false
void cvml::util::mapped_model_file::open_file(const char *)
Tq,R,V_internalNonSerializedDescriptorId
-[VNMPImageDescriptor computeQualityForImageData:andQualityCriteria:context:error:]
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-3.0.59/Libraries/MomentProcessor/Moments/MPImageDescriptor.mm
error != nil
ERROR: all ranking criteria failed
ERROR: image data property is not initialized
-[VNMPImageDescriptor computeDescriptorForImageData:context:error:]
Invalid state format
state parameter cannot be nil
Failed to initialize VNMPImageDescriptor object
MPImageDescriptor cannot be serialized without being created
VNMPImageDescriptor_ColorGaborImageDescriptorBuffer_stride
VNMPImageDescriptor_ColorGaborImageDescriptorBuffer_count
VNMPImageDescriptor_ColorGaborImageDescriptorBuffer_data
VNMPImageDescriptor_ColorGaborImageDescriptorBuffer_lengthInBytes
VNMPImageDescriptor_ColorGaborImageDescriptorBuffer_type
VNMPImageDescriptor_quality
VNMPImageDescriptor_exifTimestamp
ERROR: Could not compute image quality
ERROR: Could not compute image registration features
ERROR: Could not compute the convnet-based image descriptor
ERROR: Could not compute the image descriptor
faceBoundingBoxExpansionRatio
beginRangeFaceBoundingBoxExpansionRatio
endRangeFaceBoundingBoxExpansionRatio
defaultFaceBoundingBoxExpansionRatio
Cannot generate face segments
VNDetectFaceRectanglesRequestPrivateRevisionANFD2Detector
T@?,C,N
burst frame '%@' failed to be added
operation timeout
v24@?0@"NSString"8@"NSError"16
burstFrameIdentifier
%@%s
##INVALID##
VNFaceprintGeneratorType
VNFaceprintGeneratorTypeEspressoCPU
VNFaceprintGeneratorProcessOption_InputFaceObservations
modelPath
T@"NSString",R,D,N
clusteringConfidence
Ti,R,D,N
numberOfChannels
TQ,R,D,N
imageType
pixelFormat
TI,R,D,N
magnifiedBBoxScaleFactor
Tf,R,D,N
VN_DEBUG_DUMP_FACEPRINTER_INTERMEDIATES
TQ,R,N,V_length
useLowPriorityMode
TB,R,N,V_useLowPriorityMode
VN Faceprinter debug intermediate written to: %@
_bbox_crop.png
_bbox.json
_pre_frontalized.png
_frontalized.png
_frontalized.vdump
_pre_frontalized.vdump
VN_faceprinter_debug_intermediates/
Could not compute face descriptor due to internal error
Preprocessing of data for faceprinting failed
Intermediate image buffer for face printing has zero dimension(s)
Could not find face descriptor model resource!
Unsupported descriptor type
Descriptor type not specified!
returnAllResults
TB,N,V_returnAllResults
VNImageAnalyzerMultiDetectorInitializationOptionModel
VNImageAnalyzerMultiDetectorInitializationOptionPreferLowerGPUMemoryUtilization
VNImageAnalyzerMultiDetectorProcessingOptionSkipInputImageScaling
VNImageAnalyzerMultiDetectorProcessingOptionImageCropAndScale
VNImageAnalyzerMultiDetectorProcessingOptionCreateSceneprint
VNImageAnalyzerMultiDetectorProcessingOptionSceneprintRequestClass
VNImageAnalyzerMultiDetectorProcessingOptionSceneprintRequestRevision
VNImageAnalyzerMultiDetectorProcessingOptionSceneprintIncludeLabelsAndConfidences
VNImageAnalyzerMultiDetectorProcessingOptionSceneprintObservationsArray
VNImageAnalyzerMultiDetectorProcessingOptionCreateCompressedSceneprint
VNImageAnalyzerMultiDetectorProcessingOptionCompressedSceneprintRequestClass
VNImageAnalyzerMultiDetectorProcessingOptionCompressedSceneprintRequestRevision
VNImageAnalyzerMultiDetectorProcessingOptionCompressedSceneprintObservationsArray
VNImageAnalyzerMultiDetectorProcessingOptionClassifyScene
VNImageAnalyzerMultiDetectorProcessingOptionSceneRequestClass
VNImageAnalyzerMultiDetectorProcessingOptionSceneRequestRevision
VNImageAnalyzerMultiDetectorProcessingOptionSceneBlacklist
VNImageAnalyzerMultiDetectorProcessingOptionSceneMaximumLeafLabels
VNImageAnalyzerMultiDetectorProcessingOptionSceneMaximumHierarchicalLabels
VNImageAnalyzerMultiDetectorProcessingOptionSceneMinimumConfidence
VNImageAnalyzerMultiDetectorProcessingOptionSceneObservationsArray
VNImageAnalyzerMultiDetectorProcessingOptionSceneClassificationCustomHierarchy
VNImageAnalyzerMultiDetectorProcessingOptionClassifyJunk
VNImageAnalyzerMultiDetectorProcessingOptionJunkRequestRevision
VNImageAnalyzerMultiDetectorProcessingOptionJunkObservationsArray
VNImageAnalyzerMultiDetectorProcessingOptionClassifyAesthetics
VNImageAnalyzerMultiDetectorProcessingOptionAestheticsRequestRevision
VNImageAnalyzerMultiDetectorProcessingOptionAestheticsObservationsArray
VNImageAnalyzerMultiDetectorProcessingOptionGenerateSaliencyHeatMap
VNImageAnalyzerMultiDetectorProcessingOptionSaliencyRequestRevision
VNImageAnalyzerMultiDetectorProcessingOptionSaliencyObservationsArray
VNImageAnalyzerMultiDetectorProcessingOptionClassifyPipelineImageCorrectionNeed1
VNImageAnalyzerMultiDetectorProcessingOptionPipelineImageCorrectionNeed1RequestRevision
VNImageAnalyzerMultiDetectorProcessingOptionPipelineImageCorrectionNeed1ObservationsArray
unordered_map::at: key not found
v64@?0{CGRect={CGPoint=dd}{CGSize=dd}}8Q40Q48^B56
^{__CVBuffer=}16@?0^@8
inner/sceneprint
classification/labels
smartCam_v3.8.1_quantized.espresso
flatten
softmax
failed to create image analyzer
smartcam_labels_basic-v3b.txt
scenenet_labels_basic-v8d.csv
scenenet_aesthetic_labels_basic-v8e
scenenet_sc2.4_sa1.4_ae1.6_r13.5_opt_int8_fp16-sp_ae_sa_only.espresso
scenenet_sc2.4_sa1.4_ae1.6_r13.4_opt_int8_asymetric.espresso
unable to access image
failed to create saliency heat map image
subject_framing
background
blur
subject_sharpness
timing
lightning
reflections
color_harmony
color_brightness
symmetry
repetition
immersive_feeling
perspective
post_processing
noise
failure
composition
interestingness
object_intrusion
tilt
low_light
could not create a compressed sceneprint from tensor vector with %lu elements (%lu bytes)
could not create a sceneprint from tensor vector with %lu elements (%lu bytes)
sceneprint compression mode %lu is not supported
pca256
scenenet_sc2.4_sa1.4_ae1.4_r9_opt_int8_pca256.pcadata
perspective_gate
The required classification label was not generated
labels-pers/probabilities
Pooling
perspective_p3.0
perspective_labels-v1a.txt
perspective.r13.p3.0.espresso
junk classification not supported for %@
VNImageAnalyzerMultiDetectorModel%lu
VNImageAnalyzerMultiDetectorModelSmartCamNetV2
VNImageAnalyzerMultiDetectorModelSceneNetV3StillCapturePipeline
VNImageAnalyzerMultiDetectorModelSceneNetV3
VNImageAnalyzerMultiDetectorModelUndefined
hierarchical/probabilities
junk_hierarchical-classifier_j7
junk_hierarchical.labels_higher_order-v2e.txt
junk_hierarchical.r13.j7.espresso
leaf/probabilities
junk_leaf-classifier_j7
junk_leaf.labels_basic-v2e.txt
junk_leaf.r13.j7.espresso
a hierarchical model for detector model %lu is not supported
smartcam_relationships-v3b.txt
scenenet_relationships-v8d.txt
boomerang
bubble_soap
cider
doll_house
electronic_toy
equipment
logo
logo_other
pinata
raw_cardboard
raw_other
raw_plastic
shoe_other
skysurfing
slipper
sport_other
swing_dancing
tablet
toy_organizer
ammunition
blackjack
blade
body_part
brassiere
firearm
holiday
magic
menorah
minaret
missile
oktoberfest
pistol
primate
projectile
ramadan_lantern
raw_metal
religion
revolver
rifle
sauna
shotgun
tank_army
temple_exterior
thanksgiving
underwear
underwear_male
weapon
%@.%lu
initializeCannyEdgeContext
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-3.0.59/Libraries/CannyEdge/cannyEdge.c
context->blockAddress
thresholdAndConnectCandidateEdges
context->edgeStackSize <= context->width*context->height
smartcam_onlyfc
floatVectorSumProd
T^f,VfloatVectorSumProd
pulseVectorHeightCharBox
T*,VpulseVectorHeightCharBox
pulseVectorHeightCharBoxAdaptive
T*,VpulseVectorHeightCharBoxAdaptive
charBoxFlags
T^Q,VcharBoxFlags
charboxROIFullVectorRowStart
T^S,VcharboxROIFullVectorRowStart
charboxROIFullVectorHeight2
T^S,VcharboxROIFullVectorHeight2
allocationSize
TI,VallocationSize
mTop
Tf,VmTop
mBottom
Tf,VmBottom
bTop
Tf,VbTop
bBottom
Tf,VbBottom
posUL
Tf,VposUL
posLL
Tf,VposLL
posUR
Tf,VposUR
posLR
Tf,VposLR
medianHeightTop
TS,VmedianHeightTop
medianHeightBottom
TS,VmedianHeightBottom
loopBigBox
Ts,VloopBigBox
loopBigBoxPrev
Ts,VloopBigBoxPrev
filterWalkUpDownCount
TS,VfilterWalkUpDownCount
CCTextDetector_EnableDebug
CCTextDetector_DebugPathname
CCTextDetector_RequestRevision
computeZCVectorHighProbability
TB,V_computeZCVectorHighProbability
midRow
Ti,V_midRow
minHeight
TI,V_minHeight
maxHeight
TI,V_maxHeight
startMaxFind
TI,V_startMaxFind
stopMaxFind
TI,V_stopMaxFind
mmHeightCard
Tf,V_mmHeightCard
mmWidthCard
Tf,V_mmWidthCard
pixelHeightCard
TI,V_pixelHeightCard
pixelWidthCard
TI,V_pixelWidthCard
minBoxWidth
TI,V_minBoxWidth
maxBoxWidth
TI,V_maxBoxWidth
startNormal
TI,V_startNormal
stopNormal
TI,V_stopNormal
startSensitized
TI,V_startSensitized
stopSensitized
TI,V_stopSensitized
charBoxContext
T@"CCCharBoxContext",&,V_charBoxContext
TC,V_ii
profileNormal
TC,V_profileNormal
debugMatlab
TB,V_debugMatlab
debugOut
TB,V_debugOut
debugFilename
T@"NSString",C,V_debugFilename
q24@?0@"VNTextObservation"8@"VNTextObservation"16
textBoxRevisedNormalized
stubBoxNormalized
CCTextDetector internal error
textBoxRevised
stubBoxMM
{{%2.4f,%2.4f},{%2.4f,%2.4f}}
{{%i,%i},{%i,%i}}
stubBox
connectedComponents.png
{{1,1},{1,1}}
charConfidence
charBoxMM
charBox
textBoxMM
textBox
uOutImage.png
selectedTextOutImageArray.png
adaptiveOutImage.png
textOutSecondPassImage.png
textOutFirstPassImage.png
/var/mobile/Media/DCIM/ccOutDebug/
inverseVotingImage.png
votingImage.png
creditCardSubsampleImage.png
map::at:  key not found
v32@?0@8@16^B24
VNEspressoModelFileBasedDetectorOption_InputBlobNames
VNEspressoModelFileBasedDetectorOption_OutputBlobNames
espressoContext
T^v,R,N,V_espressoContext
espressoPlan
T^v,R,N,V_espressoPlan
espressoNetwork
T{?=^vi},R,N,V_espressoNetwork
networkRequiredInputImageWidth
TQ,R,N,V_networkRequiredInputImageWidth
networkRequiredInputImageHeight
TQ,R,N,V_networkRequiredInputImageHeight
could not obtain the dimensions of "%@"
Unexpected network input image size
%@ did not provide a valid Espresso model input image dimensions blob name
%@ did not provide a valid Espresso model file name
recognitionLanguages
T@"NSArray",C,N,V_recognitionLanguages
customWords
T@"NSArray",C,N,V_customWords
recognitionLevel
Tq,N,V_recognitionLevel
usesLanguageCorrection
TB,N,V_usesLanguageCorrection
minimumTextHeight
Tf,N,V_minimumTextHeight
en_US
Tq,N
progressHandler
T@?,C,N,VprogressHandler
indeterminate
TB,R,Vindeterminate
T@"CRImageReaderOutput",R,C,V_crOutput
string
 "%@" - (%f)
crOutput
ImageReader object was not created
VNRecognizeTextRequest error - invalid orientation
VNRecognizeTextRequest produced an internal error
v24@?0d8@"NSError"16
CRImageReaderMinimumTextHeight
%@:%@:%p:%ld:%d:%f
flatten_output
softmax_glasses_output
softmax_eyes_output
softmax_gender_output
softmax_age_output
softmax_smile_output
softmax_bald_output
softmax_haircolor_output
softmax_facehair_output
softmax_makeupeyes_output
softmax_makeuplips_output
glasses_0_prescription
glasses_1_sunglasses
glasses_2_not
age_0_baby
age_1_child
age_2_youngadult
age_3_adult
age_4_senior
bald_0_not
bald_1
eyes_0_closed
eyes_1_open
facehair_0_beard
facehair_1_goatee
facehair_2_moustache
facehair_3_stubble
facehair_4_unsure
haircolor_0_black
haircolor_1_blonde
haircolor_2_brown
haircolor_3_gray
haircolor_4_red
haircolor_5_white
gender_0_male
gender_1_female
smile_0_not
smile_1
makeupeyes_0_unsure
makeupeyes_1_no
makeupeyes_2_yes
makeuplips_0_unsure
makeuplips_1_no
makeuplips_2_yes
T{CGRect={CGPoint=dd}{CGSize=dd}},N,V_box
defaultBox
T{CGRect={CGPoint=dd}{CGSize=dd}},N,V_defaultBox
scale
Ti,N,V_scale
Ti,N,V_mergesCount
Tf,N,V_rotationAngle
Tf,N,V_yawAngle
hasLabel
TB,N,V_hasLabel
Ti,N,V_label
boxCenter
T{CGPoint=dd},R,N
distanceToDefaultBox
requiredVersion
TQ,N,V_requiredVersion
minimumAspectRatio
Tf,N,V_minimumAspectRatio
maximumAspectRatio
Tf,N,V_maximumAspectRatio
quadratureTolerance
Tf,N,V_quadratureTolerance
minimumSize
Tf,N,V_minimumSize
minimumConfidence
Tf,N,V_minimumConfidence
maximumObservations
TQ,N,V_maximumObservations
face model data is unavailable
useCenterTileOnly
TB,N,V_useCenterTileOnly
VNCreateSceneprintRequestPrivateRevisionStillCapturePipeline
VNCreateSceneprintPrivateRevision256DimensionPCAStillCapturePipeline
VNCreateSceneprintPrivateRevision256DimensionPCA
VNCreateSceneprintPrivateRevision128DimensionPCA
VNCreateSceneprintPrivateRevision64DimensionPCA
 returnAllResults
 useCenterTileOnly
VNFaceLandmarkDetectorDNNProcessOption_Constellation
inputBlobNames
outputBlobNames
landmarkDetectorDNNVersion
TC,R,D,N
T^{vector<_Geometry2D_point2D_, std::__1::allocator<_Geometry2D_point2D_> >=^{_Geometry2D_point2D_}^{_Geometry2D_point2D_}{__compressed_pair<_Geometry2D_point2D_ *, std::__1::allocator<_Geometry2D_point2D_> >=^{_Geometry2D_point2D_}}},R
Could not run Landmark Detector. Error = %s
Unexpected number of occlusion entries for Landmark points (%lu) while processing Face Landmarks request. Expected: %lu
Unexpected number of error estimates for Landmark points (%lu) while processing Face Landmarks request. Expected: %lu
Unexpected number of Landmark points (%lu) while processing Face Landmarks request. Expected: %lu
Could not compute Landmarks using Landmark Detector due to internal error
Unsupported constellation type.
requests
request classes
q24@?0@"VNRequest"8@"VNRequest"16
The requests parameter must be an array of VNRequests
VNANFDMultiDetectorProcessingOption_HumanFaceDetectorRequestRevision
VNANFDMultiDetectorProcessingOption_HumanHeadDetectorRequestRevision
VNANFDMultiDetectorProcessingOption_HumanDetectorRequestRevision
VNANFDMultiDetectorProcessingOption_AnimalRecognitionRequestRevision
requestRevisionKey
T@"NSString",R,V_requestRevisionKey
originalRequestResultsIndex
TQ,R,V_originalRequestResultsIndex
detectorClass
faceDetectorChunkAspectRatio
detectedObjectClassToRequestClass
detectedAnimalObjectClassToAnimalName
detectedObjectRequestClassToRequestInfo
Unexpected number of aligned faces: %lu, should be 1
v16@?0@"NSMutableArray"8
B56@?0@"VNImageBuffer"8{CGRect={CGPoint=dd}{CGSize=dd}}16^@48
Not supported object type: %d
Unexpected zero dimension for image crop
The request info is not found for request class %@
Failure to create multi-headed object detector.
.cmap
getMergeableClusters
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-3.0.59/Libraries/cvml-Core/Clustering/Greedy/GreedyClustering.cpp
ci == clusterID
computeInitialMergingList
L0 > mergingTo
addDescriptors
elementSize == ELEMENT_SIZE
relationships
could not decode additional relationships
SCRDL
SCRR
%@: Revision %lu, %@
additional relationships must have at least one child identifier
v32@?0@"NSString"8@"NSArray"16^B24
The classification identifier '%@' does not exist in the hierarchy
%@ must provide an implementation for %@
inputImageprints
T@"NSArray",C,N,V_inputImageprints
clusteringDistanceThreshold
Tf,N,V_clusteringDistanceThreshold
T@"NSArray",R,C,N,V_inputFaceObservations
Input faces not provided to face rectangle aligner
VNClusteringAlgorithm_Greedy
VNClusteringAlgorithm_GreedyWithTorso
T@"NSString",C,N,V_type
cachePath
T@"NSString",C,N,V_cachePath
state
T@"NSData",&,N,V_state
Tf,N,V_threshold
torsoThreshold
Tf,N,V_torsoThreshold
TQ,N,V_requestRevision
ageClassifierBabyThreshold
Tf,N,V_ageClassifierBabyThreshold
ageClassifierKidThreshold
Tf,N,V_ageClassifierKidThreshold
Failed to retrieve kid threshold value
Kid threshold pointer is a mandatory parameter
Failed to retrieve baby threshold value
Baby threshold pointer is a mandatory parameter
options parameter cannot be nil
unsupported cluster algorithm type
splitIntoMonotonicSpans
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-3.0.59/Libraries/RectangleDetector/QuadDetect/SegmentUtilities.c
cPtr->nPnts > 1
!closedP
idx <= monoLength
mergeSegment
sPtr2 != NULL
modelCachingIdentifier
T@"<NSObject><NSCopying>",C,N,V_modelCachingIdentifier
T@"VNCoreMLModel",R,N,V_model
The VNCoreMLTransform request failed
no sceneprints defined in observation
sceneprint could not be generated
Failed to initialize VNCoreMLTransformer
%@:imageCropAndScaleOption=%lu:Model=%@
%@ %@ model=%@
debugMode
Ti,V_debugMode
timerMode
Ti,V_timerMode
clusterSplitDistanceType
Ti,V_clusterSplitDistanceType
qualityCriteriaList
T@"NSArray",&,V_qualityCriteriaList
useTimestampAdjustedDistances
TB,V_useTimestampAdjustedDistances
performClustersPostprocessing
TB,V_performClustersPostprocessing
performSceneClassification
TB,V_performSceneClassification
roiAreaThreshold
Tf,V_roiAreaThreshold
inliersRatioThreshold
Tf,V_inliersRatioThreshold
numberOfKeypointsToConsider
Ti,V_numberOfKeypointsToConsider
naturalClusteringDistanceThreshold
Tf,V_naturalClusteringDistanceThreshold
meanShape
/dev/urandom
convertYUV420ToRGBA8888: src must be YUV420 format!
convertYUV420ToRGBA8888: invalid dst size of %lu x %lu
convertYUV420ToRGBA8888: failed to allocate %lu bytes
public.png
preferredSmallSide
TQ,R,N,V_preferredSmallSide
f8@?0
objectness_%ld
offsets_%ld
logits_%ld
logits_neg_%ld
logits_pos_%ld
allocator<T>::allocate(size_t n) 'n' exceeds maximum supported size
numberMaxoutLayers
ratios
Tr^{vector<float, std::__1::allocator<float> >=^f^f{__compressed_pair<float *, std::__1::allocator<float> >=^f}},R,D
objectnessFilterThresholds
cellStartsX
cellStartsY
mumberBinsNegativeMaxout
mumberPosClasses
strides
Tr^{vector<float, std::__1::allocator<float> >=^f^f{__compressed_pair<float *, std::__1::allocator<float> >=^f}},R
defaultBoxesSides
inputScale
inputBiasRGB
T{tuple<float, float, float>={__tuple_impl<std::__1::__tuple_indices<0, 1, 2>, float, float, float>=fff}},R
inputBGR
numberBinsRoll
numberBinsYaw
T{CGSize=dd},R,D
poseSquare
TB,R,D
hasObjectnessOutputs
nonSquareRollDefault
nonSquareYawDefault
importantClasses
Tr^{vector<unsigned long, std::__1::allocator<unsigned long> >=^Q^Q{__compressed_pair<unsigned long *, std::__1::allocator<unsigned long> >=^Q}},R,D
logits_yaw_%ld
logits_roll_%ld
shotflow-8k6zuzd9wy_46860_opt_quantized.espresso
smartcam
scorPdiffParameters
exprParameters
blinkParametersApp
smileBlinkParametersGeo
exprParamsv1
lmarkQuality
pupilMeanStd
pupil
VNRequestOptionInputFaces
VNRequestOptionAddObjectsToClustering
VNRequestOptionAddObjectGroupIdsToClustering
VNRequestOptionRemoveObjectsFromClustering
VNRequestOptionFaceprintRevision
VNRequestOptionClusteringAgeClassifierFilePath
VNRequestOptionClusteringAgeClassifierBabyThreshold
VNRequestOptionClusteringAgeClassifierKidThreshold
VNRequestOptionForceFaceprintCreation
VNRequestOptionCacheFolderPath
VNRequestOptionRectangleMinimumAspectRatio
VNRequestOptionRectangleMaximumAspectRatio
VNRequestOptionRectangleQuadratureTolerance
VNRequestOptionRectangleMinimumSize
VNRequestOptionRectangleMinimumConfidence
VNRequestOptionRectangleMaximumNumber
VNRequestOptionInputThreshold
VNRequestOptionInputTorsoThreshold
VNRequestOptionInputTimestamp
VNRequestOptionInputImageprints
VNRequestOptionInputRegionOfInterest
VNRequestOptionMetalContextPriority
VNRequestOptionMaximumIntermediateSideLength
VNRequestOptionClusteringAlgorithm
VNRequestOptionRestoreClusteringState
VNRequestOptionDetectionLevel
VNRequestOptionDetectionLevel_Accurate
VNRequestOptionDetectionLevel_Balanced
VNRequestOptionDetectionLevel_Fast
VNRequestOptionRectangleDetectorRequiredVersion
VNRequestOptionInputSceneprints
VNRequestOptionInputSceneClassificationAllResults
VNRequestOptionImageCropAndScale
VNRequestOptionInputSmartCamClassificationAllResults
VNRequestOptionSmartCamClassificationCenterTileOnly
VNRequestOptionBlurMethod
VNRequestOptionBurstAllImageIdentifiers
VNRequestOptionBurstImageStats
VNRequestOptionBurstClusters
VNRequestOptionReportCharacterBoxes
VNRequestOptionBarcodeSymbologies
VNRequestOptionTrackingType
VNRequestOptionTrackingTypeFast
VNRequestOptionTrackingTypeAccurate
VNRequestOptionTrackingTypeRPN
VNRequestOptionDetectTextRectanglesRequiredVersion
VNRequestOptionTextRecognition
VNRequestOptionTextRecognitionAdditionalCharacters
VNRequestOptionMinimumLinePixelHeight
forceFaceprintCreation
TB,N,V_forceFaceprintCreation
Cannot create 'print' object
solo_landmarks_s9min6ugm8_opt.espresso
TB,V_forceFaceprintCreation
VNFaceBBoxAlignerProcessOption_InputFaceObservations
Error aligning face bounds.  Bounds are likely out of bounds
Could not create memory efficient crop for bbox alignment
Invalid face bounds supplied to face aligner
VNAlignBBox recieved a zero dimensioned image
VN aligner debug intermediates written to: %@
alignedRotationAngle
_meanShape.png
_aligned_bbox_crop.png
_bboxes.json
_aligner_image.png
_aligner_image.vdump
VN_facealigner_debug_intermediates/
Could not read face box aligner model
faceBoxPoseAligner-current
Could not map face box aligner model
VN_DEBUG_DUMP_FACE_ALIGNER_INTERMEDIATES
apple_scenes_onlyfc
VNTrackingOption_ProcessingQueue
VNTrackingOption_TrackerKey
VNTrackingOption_TrackerType
VNTrackingOption_TrackingLevel
VNTrackingOption_InputBBox
VNTrackingOption_RequestRevision
VNTrackingOption_CVPixelBufferFormat
VNTrackingOption_InputImageMaxWidth
VNTrackingOption_InputImageMaxHeight
isTracking
trackedFrameNumber
Tq,V_trackedFrameNumber
lastTrackedBBox
T{CGRect={CGPoint=dd}{CGSize=dd}},V_lastTrackedBBox
T@"NSUUID",R,V_key
level
T@"NSString",R,V_level
trackedFrameCVPixelBufferFormat
TI,V_trackedFrameCVPixelBufferFormat
Internal error: Tracking objects failed with error: %llu
Internal error: No frame to track objects was passed to the tracker
Internal error: Conversion to Tracker coordinate system failed
Internal error: failed to initialize object IDs to rectangles dictionary
Internal error: Setting objects to track failed with error: %llu
Internal error: Tracker is busy with previous tracking requests. It needs to be reset to restart tracking sequence
trackerObservationClass
VNSmartCamCombinedAestheticsAndSaliencyDetectorOptionClassifyAesthetics
VNSmartCamCombinedAestheticsAndSaliencyDetectorOptionGenerateSaliencyHeatMap
combined_classification_smartCamfanet_0-0011_saliency_bhutc68tnd_aesthetics_3eqm2xn28k.espresso
aesthetics/scores
aesthetics/attributes
saliency/map
data
Failed to create observation
Error allocating VNImageAestheticsObservation
Unexpected attribute scores input types provided
No attribute data returned
Unexpected aesthetic score input types provided
No aesthetic score data returned
Could not bind output aesthetics attributes
+N9mZUAHooNvMiQnjeTJ8g
T@"NSArray",R,C,N,V_originalRequests
   +-- %@
v32@?0@"NSString"8@16^B24
%@ does not override %@
TQ,N,V_levelCount
warpCount
TQ,N,V_warpCount
enableFiltering
TB,N,V_enableFiltering
TQ,N,V_filterSize
TQ,N,V_filterSamplingDensity
filterLumaWeight
Tf,N,V_filterLumaWeight
filterChromaWeight
Tf,N,V_filterChromaWeight
filterOcclusionWeight
Tf,N,V_filterOcclusionWeight
filterSamplingDensity
filterSize
levelCount
optical flow cannot be performed on images with different dimensions
failed to allocate the vector result buffer (CVReturn=%ld)
v20@?0@"<NSObject><NSCopying>"8B16
no image is available
no request performer available
gazeflow-mcbdnde3m8_225900_opt_quantized_w_conv_u16_fc_fp16.espresso
mask
unable to access image pixel buffer
network input blob name is not available
Could not bind objectness heat map
pumtc2j5f7_wide_u8.espresso
The request class %@ shall have it's results populated in the results array
B16@?0^@8
VNGenerateObjectnessBasedSaliencyImageRequestPrivateRevision544x544Input128x128Output
colSumSq
colSum
colProjections
rowSumSq
rowSum
rowProjections
inconsistent column data
inconsistent row data
Error while trying to allocate VNImageRegistrationSignature object
nil buffer passed into initWithImageBuffer
delegate
T@"<VNPersonsModelDataDelegate>",W,N,V_delegate
T@"NSDate",R,N,V_lastModificationDate
TQ,R,N,V_faceprintRequestRevision
identity serial numbers have been exhausted
The model has reached the maximum identity limit of %lu
VNImageprintGeneratorProcessOption_Timestamp
no valid initial image specifier was provided
ConnectedComponents
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-3.0.59/Libraries/RectangleDetector/QuadDetect/ConnectedComponents.c
pneighbor(*(Pcoord *)dequeFirst(dbLookup[mnid]), pix)
pneighbor(*(Pcoord *)dequeFirst(dbLookup[nid]), pix)
pneighbor(*(Pcoord *)dequeFirst(dbLookup[mid]), pix)
pneighbor(*(Pcoord *)dequeFirst(dbLookup[pid]), pix)
loc != -1
cidcnt < MAX_CONTOURS
eraseContourPixels
deque != NULL
projectionRows_planar8UtoF
projectionCols_planar8UtoF
allocSegments
/BuildRoot/Library/Caches/com.apple.xbs/Sources/Vision_Sim/Vision-3.0.59/Libraries/RectangleDetector/QuadDetect/Segments.c
sdb->nSegments <= sdb->maxSegments
VNElementType(%lu)
VNElementTypeDouble
VNElementTypeFloat
VNElementTypeUnknown
VNImageCropAndScaleOption(%lu)
VNImageCropAndScaleOptionScaleFill
VNImageCropAndScaleOptionScaleFit
VNImageCropAndScaleOptionCenterCrop
(%g, %g)
%g x %g
[%@ %@]
[%g, %g, %g, %g]
feature orientation
image orientation
Face crop orientation is a mandatory parameter
Oriented Face BBox is a mandatory parameter
Scale parameter cannot be zero
options
q24@?0@"VNObservation"8@"VNObservation"16
%@Revision%lu
VNSaliencyHeatmapBoundingBoxGenerator
VNDetectHumanHeadRectanglesRequest
Revisioning
VNFaceQualityGenerator
CVML_Error
VNClusteringLogger
VNSuggestionsLogger
VNGreedyClusteringReadOnly
VNClusteringCancelling
VNClusteringReadOnly
VNGreedyClusteringReadWrite
VNClusteringWritable
BurstFaceConfigEntry
BurstFaceScoreEntry
BurstFaceInfo
BurstImageFaceAnalysisContext
BurstActionClassifier
VNFaceAnalyzerMultiDetector
VNClustererContextBase
VNClustererReadOnlyContext
VNClustererModelQuerying
VNClustererReadWriteContext
VNClustererModelBuilding
VNSceneClassifier
VNDetectorIdealImageSizeProviding
NSObject
_VNRequestForensicsRequestAndErrorTuple
_VNRequestForensicsRequestAndObservationsCacheKeyTuple
_VNRequestForensicsParentChildRequests
VNRequestForensics
LKTMetalContext
VNError
VNAppendBurstSequenceFrameRequest
VNFaceDetectorRevision2
VNClassifyJunkImageRequestConfiguration
VNClassifyJunkImageRequest
BurstFaceStat
NSCopying
BurstImageStat
VNSequenceRequestHandler
VNRequestCancelling
VNRequestWarming
VNImageRequestHandler
VNPhotosRequestHandler
VNPhotosRequestHandlerSupport
VNHomographicImageRegistrationRequest
VNSceneClassificationRequestConfiguration
VNSceneClassificationRequest
VNImageIdealImageSizeProviding
VNBlurSignature
VNBlurMeasure
VNTrackingRequest
VNMetalContext
VNDetector
VNRequestRevisionProviding
VNDetectorKeyProviding
VNImageAnalyzerCompoundRequestGroupingConfiguration
VNImageAnalyzerCompoundRequestGroupingConfigurations
VNImageAnalyzerCompoundRequestConfiguration
VNImageAnalyzerCompoundRequest
VNFaceRegionMap
NSSecureCoding
NSCoding
VNANEProcessingDevice
VNRuntimeUtilities
VNCanceller
VNFaceLandmarkDetector
VNFaceAnalyzerCompoundRequestConfiguration
VNFaceAnalyzerCompoundRequestConfigurationGroups
VNFaceAnalyzerCompoundRequest
VNFaceAnalyzerFaceObservationGrouping
VNImageBufferManager
VNImageSourceManager
VNImageBuffer
VNCreateTorsoprintRequest
VNFaceObservationAccepting
VNRPNTrackerEspressoModelCacheManager
VNObjectTrackerRevision2
VNMutablePersonsModel
VNPersonsModelDataDelegate
VNRequest
VNSequencedRequestSupporting
VNRequestConfiguration
VNSizeRange
VNSupportedImageSize
VNFaceTorsoprint
VNOpticalFlowObservation
VNFaceDetectorRevision1
 0$1
VNMomentProcessor
VNMPClusteringTreeNodeWrapper
VNGenerateAttentionBasedSaliencyImageRequestConfiguration
VNGenerateAttentionBasedSaliencyImageRequest
VNEspressoDetectedObject
_VNPixelBufferSpecifier
VNImageSpecifier
_VNDataImageSpecifier
_VNURLImageSpecifier
_VNCIImageSpecifier
_VNCGImageSpecifier
OptionsDictionaryCompatability
VNFaceprint
SaliencyExtrema
VNSaliencyImageObservation
VNImageSaliencyObservation
VNClassifyImageAestheticsRequestConfiguration
VNClassifyImageAestheticsRequest
VNRectangleTracker
VNBurstAnalysisResultsRequestConfiguration
VNBurstAnalysisResultsRequest
VNFaceprintGeneratorRevision1
VNSmartCamClassifier
VNFaceExpressionDetector
VNSceneprint
VNSceneFeaturePrint
BurstClusterDivider
VNDetectTextRectanglesRequestConfiguration
VNDetectTextRectanglesRequest
_VNPersonsModelDataSourceBasedDataProvider
VNPersonsModelFaceModelDataProvider
VNPersonsModel
VNPersonsModelInformation
VNPersonsModelReadOptions
VNPersonsModelConfiguration
VNPersonsModelPrediction
VNPersonsModelWriteOptions
VNPersonsModelAdditions
VNDetectorManager
ConvenienceMethods
VNClassifyImageRequestConfiguration
VNClassifyImageRequest
VNImageRegistrationRequest
VNFaceAttributeCategory
VNFaceAttributes
VNWarningRecorder
VNSmartCamprint
VNMPUtils
VNOperationPointsProvider
VNOperationPointsProviding
VNMPImageQuality
VNFaceSegmentGenerator
VNSingleHeadSceneprintGenerator
ShotflowDetector
ShotflowDetectorANFDv1
ShotflowDetectorANFDv2
VNEspressoModelClassifier
VNBarcodeObservation
VNDetectBarcodesRequest
VNDetectBarcodesRequestConfiguration
VNTrackerManager
VNGenerateImageFeaturePrintRequestConfiguration
VNGenerateImageFeaturePrintRequest
ANFDDetectedObject
VNMPImageData
_VNUnspecifiedOperationPoints
VNOperationPoints
VNClassifyPipelineImageCorrectionNeed1Request
VNNOPRequestConfiguration
VNNOPRequest
VNDetectFaceLandmarksRequestConfiguration
VNDetectFaceLandmarksRequest
VNImageprint
VNSerializing
VNSerializingInternal
VNFaceGeometryEstimator
VNMetalProcessingDevice
VNEspressoModelImageprint
VNImageAestheticsObservation
VNFaceRegionMapGenerator
VNObservation
VNDetectedObjectObservation
VNFaceObservation
VNImageAlignmentObservation
VNImageTranslationAlignmentObservation
VNImageHomographicAlignmentObservation
VNImageScoreObservation
VNImageprintObservation
VNImageBlurObservation
VNImageBrightnessObservation
VNClassificationObservation
VNRecognizedObjectObservation
VNCoreMLFeatureValueObservation
VNPixelBufferObservation
VNRectangleObservation
VNHorizonObservation
VNCluster
VNClusterObservation
VNFeaturePrintObservation
VNSceneObservation
VNSmartCamObservation
VNBurstObservation
VNRecognizedTextObservation
_VNTextObservationCharacterBox
VNTextObservation
VisionAdditions
VNHorizonDetector
VNDetectFacePoseRequest
VNImageBasedRequestConfiguration
VNImageBasedRequest
VNFaceObservationAcceptingInternal
VNVersionParser
VNHeatMapExtrema
VNHeatMapUtilities
VNGenerateImageSaliencyRequestConfiguration
VNGenerateImageSaliencyRequest
VNRecognizeAnimalsRequest
VNANERuntimeDirectProcessingDevice
VNImageGrouper
VNCVPixelBufferConversionHelpers
VNClassifyFaceAttributesRequest
VNPersonsModelFaceModel
VNPersonsModelFaceModelAdditions
VNDetectAnimalRectanglesRequest
BurstImageSetInternal
VNDetectHumanRectanglesRequest
VNMPImageGrouping
VNEspressoResources
VNEspressoHelpers
ImageProcessing_CoreImageUtils
VNDetectFace3DLandmarksRequest
VNFaceLandmarkDetectorRevision1
LKTGPU
VNJunkIdentifier
VNRectangleDetector
VNCPUProcessingDevice
VNTorsoprintGenerator
VNImageBlurScoreRequestConfiguration
VNImageBlurScoreRequest
VNFaceDetector
VNFaceLandmarkRegion
VNFaceLandmarkRegion2D
VNFaceLandmarkRegion3D
VNFaceLandmarks
VNFaceLandmarks2D
VNFaceLandmarks3D
VNValidationUtilities
VNFaceLandmarkDetectorRevision3
VNBlacklist
CVMLFaceprint_LegacySupportDoNotChange
CVMLObservation_LegacySupportDoNotChange
CVMLImageprintObservation_LegacySupportDoNotChange
MPImageDescriptor_LegacySupportDoNotChange
VNTorsoprint
VNMLFeatureProvider
MLFeatureProvider
VNCoreMLModel
VNCoreMLTransformer
VNSceneTaxonomyOperationPoints
VNImageClassifier
VNCreateImageprintRequestConfiguration
VNCreateImageprintRequest
VNFaceSegments
VNObjectTracker
BurstThumbnailCluster
VNModelFileImpl
VNModelFile
VNModelFilesCache
VNMPImageDescriptor
VNGenerateFaceSegmentsRequestConfiguration
VNGenerateFaceSegmentsRequest
VNDetectFaceRectanglesRequest
VNDetectFaceCaptureQualityRequest
VNBurstContext
VNTranslationalImageRegistrationRequest
VNFaceprintGenerator
VNCreateSmartCamprintRequestConfiguration
VNCreateSmartCamprintRequest
_VNImageAnalyzerMultiDetectorSceneOperationPointsProvider
VNImageAnalyzerMultiDetector
_VNImageAnalyzerMultiDetectorSceneOperationPointsCache
VNCreateFaceRegionMapRequest
CCCharBoxContext
CCTextDetector
VNObjectTrackerRevision1
VNObservationsCache
VNEspressoModelFileBasedDetector
VNRecognizeTextRequestConfiguration
VNRecognizeTextRequest
VNRequestProgressProviding
VNRecognizedText
VNDebugHelpers
ShotflowDetection
VNDetectRectanglesRequestConfiguration
VNDetectRectanglesRequest
VNReadOnlyPersonsModel
VNPersonsModelDataSource
VNCreateSceneprintRequestConfiguration
VNCreateSceneprintRequest
VNFaceLandmarkDetectorDNN
VNRequestPerformer
VNTrackerProviding
VNIdentifyJunkRequest
VNANFDMultiDetectorOriginalRequestInfo
VNANFDMultiDetector
VNClassificationCustomHierarchy
_VNImageAnalyzerMultiDetectorClassificationCustomHierarchy
_VNSceneClassifierClassificationCustomHierarchy
VNGroupImagesByTimeAndContentRequest
VNProcessingDevice
VNAlignFaceRectangleRequestConfiguration
VNAlignFaceRectangleRequest
VNClustererOptions
VNClustererQueryOptions
VNClustererBuilderOptions
VNClustererQuery
VNClustererBuilder
VNCoreMLRequestConfiguration
VNCoreMLRequest
VNMPContext
ShotflowNetwork
ShotflowNetworkANFDv1
ShotflowNetworkANFDv2
VNDetectFaceExpressionsRequest
VNBrightnessMeasure
VNTrackRectangleRequest
VNCreateFaceTorsoprintRequest
VNFaceLandmarkDetectorRevision2
VNCreateFaceprintRequestConfiguration
VNCreateFaceprintRequest
VNTrackObjectRequest
VNFaceBBoxAligner
VNTracker
VNImageExposureScoreRequest
VNSmartCamCombinedAestheticsAndSaliencyDetector
VNCompoundRequest
VNUniqueObservationClassCompoundRequest
VNHomologousObservationClassCompoundRequest
VNGenerateOpticalFlowRequest
VNANERuntimeProcessingDevice
VNRequestPerformingContext
VNImageBufferProviding
VNGenerateObjectnessBasedSaliencyDetector
VNGenerateObjectnessBasedSaliency544x544Detector
VNANFDDetectorCompoundRequestConfiguration
VNANFDDetectorCompoundRequestConfigurationGroups
VNANFDDetectorCompoundRequest
VNGenerateObjectnessBasedSaliencyImageRequestConfiguration
VNGenerateObjectnessBasedSaliencyImageRequest
VNImageRegistrationSignature
VNImageRegistration
VNPersonsModelData
VNMPImageSharpness
VNImageprintGenerator
VNTargetedImageRequest
VNDetectHorizonRequest
objectForKey:
count
objectAtIndex:
cStringUsingEncoding:
stringWithUTF8String:
dictionaryWithContentsOfFile:
setObject:forKey:
initWithAPI:properties:
setParameter:to:
setCurrentContext:
recordDefaultConfigurationOptionsInDictionary:
configurationOptionKeysForDetectorKey
keyForDetectorWithConfigurationOptions:
initWithFormat:
isEqualToString:
objectForKeyedSubscript:
copy
addObject:
removeObject:
mutableCopy
setObject:forKeyedSubscript:
arrayWithObjects:count:
_observationsForOneComponent32FloatPixelBuffer:options:regionOfInterest:error:
_createScaledOneComponent32FloatPixelBufferFromImageBuffer:options:error:
espressoModelFileNameForConfigurationOptions:
espressoModelInputImageDimensionsBlobNameForConfigurationOptions:
supportsProcessingDevice:
processWithOptions:regionOfInterest:warningRecorder:error:
imageWithCVPixelBuffer:
initWithCapacity:
UTF8String
warmUpRequestPerformer:error:
defaultProcessingDeviceForRevision:
dependencyProcessingOrdinality
internalPerformRevision:inContext:error:
revisionAvailability
_mFaceQualityPredictor
_mNetworkOptions
completeInitializationAndReturnError:
.cxx_destruct
.cxx_construct
numberWithFloat:
stringWithFormat:
pathForResource:ofType:
createNSErrorWithStatus:andMessage:
createNSExceptionWithStatus:andMessage:
exceptionWithName:reason:userInfo:
dictionaryWithObjects:forKeys:count:
localizedStringForKey:value:table:
mainBundle
errorWithDomain:code:userInfo:
_logEnabled
_logFolderURL
_logFileURL
_fileNameBase
initWithOptions:logEnabled:logFileNameBase:
initWithOptions:logEnabled:
resetFileNameURLWithCurentDateTime
logString:
logClusterMap:level:
logClusterMapL0:
logClusterLookupMapL0:
logClusterMapL1:
logClusterLookupMapL1:
logFolderURL
logFileURL
logEnabled
fileNameBase
appendFormat:
padStringWithSpaces:toSize:
appendString:toLogFile:
path
URLByAppendingPathComponent:
currentDateTime
isLogEnabled
stringFromDate:
date
setDateFormat:
setLocale:
localeWithLocaleIdentifier:
replaceCharactersInRange:withString:
insertString:atIndex:
stringWithCapacity:
stringWithString:
closeFile
writeData:
dataUsingEncoding:
stringByAppendingString:
seekToEndOfFile
writeToFile:atomically:encoding:error:
fileHandleForUpdatingAtPath:
fileExistsAtPath:isDirectory:
stringByDeletingLastPathComponent
defaultManager
boolForKey:
standardUserDefaults
logInputFaceIdsWithFlags:
logSuggestons:description:
logAllSuggestons:
logFilteredByInputQuerySuggestons:
logConnectedGroups:
logFinalSuggestionsList:
appendString:
deleteCharactersInRange:
countByEnumeratingWithState:objects:count:
objectAtIndexedSubscript:
allKeys
_clusteringLogger
_suggestionsLogger
_cacheFolderPath
_thresholdN
_thresholdSL
_thresholdTorso
_type
_state
_vectorMapReadOnlyFlag
_faceprintRevision
_ageClassifierFilePath
_ageClassifierBabyThreshold
_ageClassifierKidThreshold
m_ClusteringImpl_const
cancelClustering:
getRepresentativenessForFaces:error:
clustererModelFileNamesFromState:storedInPath:error:
nonGroupedGroupID
suggestionsForClusterIdsWithFlags:affinityThreshold:error:
getClusterState:
getClusteredIds:
getLevel1ClusteredIdsGroupedByLevel0ClustersForFaceId:error:
getDistanceBetweenLevel0ClustersWithFaceId:andFaceId:error:
getDistanceBetweenLevel1Clusters:error:
getAllClustersFromStateAndReturnError:
getClustersForClusterIds:options:error:
getDistances:to:error:
maximumFaceIdInModelAndReturnError:
_parseOptions:error:
initializeLogging
initWithOptions:error:
setGreedyClustererFaces_const:
convertUpdatePairsToClusters:
getLevel0ClusteredIdsForFaceId:error:
numberWithLongLong:
numberWithInt:
numberWithUnsignedInteger:
unsignedIntegerValue
addFaceObservations:toFaceDescriptorBuffer:
dictionary
enumerateObjectsUsingBlock:
unsignedLongValue
firstObject
minusOrderedSet:
orderedSetWithCapacity:
intValue
longLongValue
unsignedIntValue
dataWithBytes:length:
isEqualToSet:
allObjects
minusSet:
unionSet:
allValues
setWithObject:
containsObject:
numberWithBool:
dictionaryWithCapacity:
arrayWithObject:
bytes
fileURLWithPath:
boolValue
floatValue
addFaceObservations:withGroupingIdentifiers:toFaceDescriptorBuffer:
lastPathComponent
getUUIDBytes:
initWithUUIDBytes:
m_ClusteringImpl
getClustersWithOptions:error:
_cancellableUpdate:facesToMove:requestRevision:
faceId
faceRect
framesSinceLast
initWithRect:withFaceId:
setFaceId:
setFaceRect:
setFramesSinceLast:
maxScore
minScore
sumScores
sumSqScores
numScores
initWithScore:
addScore:
computeAverage
computeStandardDeviation
setMaxScore:
setMinScore:
setNumScores:
swFaceId
swLastFrameSeen
hwFaceId
hwLastFrameSeen
swCenter
swSize
hwCenter
hwSize
init
hwFaceRect
swFaceRect
overlapWithHwRect:
overlapWithSwRect:
setSwFaceId:
setSwCenter:
setSwSize:
setSwLastFrameSeen:
setHwFaceId:
setHwCenter:
setHwSize:
setHwLastFrameSeen:
forceFaceDetectionEnable
_forceFaceDetailsEnable
_faceIdCounter
_numFramesSinceFullFaceCore
_numFramesNoFaces
_lastFaceIndex
_version
timeBlinkDetectionDone
timeFaceDetectionDone
latestFaceTimestamp
_curConfig
_faceIdMapping
_renameMapping
_faceInfoArray
_faceTimestampArray
_latestImageTimestamp
initWithVersion:
padRoiRect:paddingX:paddingY:
calculateFaceCoreROI:imageStat:needSWFaceDetection:
findOverlappingFaceStat:imageStat:
_filterFacesToProcess:imageSize:imageStat:
findFacesInImage:imageStat:
calculateFaceFocusInImage:imageStat:
calcFaceScores:
adjustFaceIdsForImageStat:
addFaceToArray:
extractFacesFromMetadata:
addFacesToImageStat:imageSize:
dumpFaceInfoArray
setTimeBlinkDetectionDone:
setTimeFaceDetectionDone:
setForceFaceDetectionEnable:
setLatestFaceTimestamp:
curConfig
setCurConfig:
faceIdMapping
setFaceIdMapping:
renameMapping
setRenameMapping:
faceIdCounter
setFaceIdCounter:
faceInfoArray
setFaceInfoArray:
numFramesSinceFullFaceCore
setNumFramesSinceFullFaceCore:
numFramesNoFaces
setNumFramesNoFaces:
faceTimestampArray
setFaceTimestampArray:
latestImageTimestamp
setLatestImageTimestamp:
lastFaceIndex
setLastFaceIndex:
forceFaceDetailsEnable
setForceFaceDetailsEnable:
version
setVersion:
doubleValue
insertObject:atIndex:
removeObjectForKey:
numberWithDouble:
dictionaryWithDictionary:
unsignedLongLongValue
arrayWithArray:
removeObjectAtIndex:
addEntriesFromDictionary:
arrayWithCapacity:
getValue:
value:withObjCType:
array
subarrayWithRange:
sortedArrayUsingComparator:
hasBeenScaled
testVector
testAverageCameraTravelDistance
testMaxRegistrationErrorIntegral
testMaxPeakRegistrationError
testMeanPeakRegistrationError
testBeginningVsEndAEMatrixVsAverageAdjacentAEMatrix
testInOutRatio
testMaxInnerDistance
testAverageRegistrationErrorSkewness
testMinRegionOfInterestSize
testMaxRegistrationErrorSkewness
_svmParameters
scaleVector
computeKernelValueWithSupportVector:
predictResult
isBurstAction
setTestAverageCameraTravelDistance:
setTestMaxRegistrationErrorIntegral:
setTestMaxPeakRegistrationError:
setTestMeanPeakRegistrationError:
setTestBeginningVsEndAEMatrixVsAverageAdjacentAEMatrix:
setTestInOutRatio:
setTestMaxInnerDistance:
setTestAverageRegistrationErrorSkewness:
setTestMinRegionOfInterestSize:
setTestMaxRegistrationErrorSkewness:
svmParameters
setSvmParameters:
_mMultiHeadedFaceClassifier
_mFaceFrontalizerImpl
_faceVImageBuffer
_mFaceFrontalizerWorkingBuffer
_addFaceAnalysisResultsFromMap:toFaceAttributeObject:withRequestRevision:
integerValue
mutableBytes
initWithLength:
_threshold
_torsoThreshold
_cacheDirectoryPath
_readOnly
_requestRevision
_checkInitInputs:cachePath:checkType:requestRevision:error:
initWithType:cachePath:state:readOnly:threshold:requestRevision:error:
initWithType:cachePath:state:readOnly:threshold:torsoThreshold:requestRevision:error:
_createGreedyClusterer:state:error:
_initializeGreedyClustererOptions:
localizedDescription
alloc
containsIndex:
_faceprintRevision1ModelPathAndReturnError:
_faceprintRevision2ModelPathAndReturnError:
_ageClassifierPathForFaceprintRequestRevision:error:
_clusterer
representativenessForFaces:error:
distanceBetweenFacesWithFaceprint:andFaceprint:error:
distanceBetweenFacesWithFaceObservation:andFaceObservation:error:
allClusteredFaceIdsAndReturnError:
clusteredFaceIdsForClusterContainingFaceId:error:
getAllClustersAndReturnError:
l1ClusteredFaceIdsGroupedByL0ClustersForClustersContainingFaceIds:error:
distanceBetweenClustersWithFaceId:andFaceId:error:
distanceBetweenLevel1Clusters:error:
suggestionsForClustersWithFaceIds:affinityThreshold:canceller:error:
initWithType:cachePath:state:threshold:requestRevision:error:
initWithType:cachePath:state:threshold:torsoThreshold:requestRevision:error:
_ageClassifierModelFilePath
updateModelByAddingFaces:canceller:error:
updateModelByAddingFaces:withGroupingIdentifiers:canceller:error:
updateModelByRemovingFaces:canceller:error:
resetModelState:error:
saveAndReturnCurrentModelState:
updateModelByAddingFaces:andRemovingFaces:canceller:error:
updateModelByAddingFaces:withGroupingIdentifiers:andRemovingFaces:canceller:error:
initWithType:cachePath:state:threshold:babyThreshold:kidThreshold:requestRevision:error:
initWithType:cachePath:state:threshold:torsoThreshold:babyThreshold:kidThreshold:requestRevision:error:
supportedImageSizeSetForOptions:
debugDescription
isEqual:
class
self
performSelector:
performSelector:withObject:
performSelector:withObject:withObject:
isProxy
isKindOfClass:
isMemberOfClass:
conformsToProtocol:
respondsToSelector:
retain
release
autorelease
retainCount
zone
hash
superclass
description
createClassifierWithDescriptor:classifierAbsolutePath:computePlatform:computePath:labelsFilename:options:
createDescriprorProcessorWithModelPath:nBatch:computePlatform:computePath:options:
classifierResourceTypesToNamesForRevision:
espressoModelImageprintClass
returnAllResultsOptionKey
createObservationWithDescriptors:forRequestRevision:
initDumpDebugIntermediates:debugInfo:
createDirectoryAtURL:withIntermediateDirectories:attributes:error:
_sceneClassifierHierarchicalModel
isSceneprinterCompatibleWithSceneprinterCreatedWithOptions:error:
initImageDescriptorBuffer:descriptorBuffer:error:
labelOperationPointsForRequestRevision:error:
writeToFile:atomically:
dataWithJSONObject:options:error:
stringByAppendingPathComponent:
stringByAppendingPathExtension:
stringValue
numberWithUnsignedLongLong:
stringByDeletingPathExtension
enumerateIndexesUsingBlock:
_request
_error
initWithRequest:error:
request
error
_observationsCacheKey
initWithRequest:observationsCacheKey:
observationsCacheKey
_parentRequest
_orderedChildRequests
initWithParentRequest:orderedChildRequests:
parentRequest
orderedChildRequests
componentsJoinedByString:
_originalRequests
_orderedRequests
_implicitRequests
_performedRequests
_cachedRequestResults
_checkedCachedResultsOnBehalfOfRequest
_locatedCachedResultsOnBehalfOfRequest
_ledger
_requestToHumanReadableLabelMap
_humanReadableLabelForRequest:
initWithOriginalRequests:
setOrderedRequests:
performingOrderedDependentRequests:onBehalfOfRequest:
performingRequest:
performedRequest:withError:
request:cachedResultsWithObservationsCacheKey:
cachedObservationsWithKey:wereCheckedOnBehalfOfRequest:
cachedObservationsWithKey:wereLocatedOnBehalfOfRequest:
originalRequests
orderedRequests
performedRequests
keyUsedToCacheResultsOfRequest:
requestsThatLookedUpCachedResultsKey:
_childRequestsImplicitlyPerformedOnBehalfOfParentRequest:
requestsImplicitlyPerformedOnBehalfOfRequest:
requestThatProvidedObservationsForRequest:
resultsObtainedFromObservationsCacheForRequest:
strongToStrongObjectsMapTable
_device
_commandQueue
_library
initWithDevice:error:
bindPixelBufferToMTL2DTexture:pixelFormat:plane:error:
bindPixelBufferToMTL2DTexture:pixelFormat:textureSize:plane:error:
textureBytesPerRow:format:
device
commandQueue
library
bytesPerPixelForTextureFormat:
minimumLinearTextureAlignmentForPixelFormat:
newCommandQueue
newLibraryWithFile:error:
metalContextForDevice:error:
makeTextureCoherent:texture:
errorWithCode:message:
errorWithCode:message:underlyingError:
errorForCancellationOfRequest:
errorForMemoryAllocationFailure
errorForMemoryAllocationFailureWithLocalizedDescription:
errorForInternalErrorWithLocalizedDescription:
errorForInternalErrorWithLocalizedDescription:underlyingError:
errorForInvalidFormatErrorWithLocalizedDescription:
errorForUnimplementedFunctionWithLocalizedDescription:
errorForOutOfBoundsErrorWithLocalizedDescription:
errorForInvalidOperationWithLocalizedDescription:
errorForInvalidOperationForRequestClass:revision:
errorForMissingOptionNamed:
errorForInvalidOption:named:
errorForInvalidOption:named:localizedDescription:
errorForInvalidArgumentWithLocalizedDescription:
errorForInvalidArgument:named:
errorForInvalidModelWithLocalizedDescription:
errorForInvalidModelWithLocalizedDescription:underlyingError:
errorForOperationFailedErrorWithLocalizedDescription:
errorForUnknownErrorErrorWithLocalizedDescription:
errorForGPURequiredByRequest:
errorForUnsupportedProcessingDevice:
errorForUnsupportedConfigurationOfRequest:
errorForUnsupportedRevision:ofRequest:
errorForUnsupportedRevision:ofRequestClass:
errorForUnsupportedPrivateRevision:ofRequest:
errorForUnsupportedPrivateRevision:ofRequestClass:
errorForDataUnavailableWithLocalizedDescription:
errorForEspressoReturnStatus:localizedDescription:
errorForCVReturnCode:localizedDescription:
errorForOSStatus:localizedDescription:
logInternalError:
localizedFailureReason
code
dictionaryWithObjectsAndKeys:
stringByAppendingFormat:
_burstFrameIdentifier
_imageProperties
initWithTargetedCVPixelBuffer:options:completionHandler:
initWithTargetedCVPixelBuffer:options:
initWithTargetedCGImage:options:completionHandler:
initWithTargetedCGImage:options:
initWithTargetedCIImage:options:completionHandler:
initWithTargetedCIImage:options:
initWithTargetedImageURL:options:completionHandler:
initWithTargetedImageURL:options:
initWithTargetedImageData:options:completionHandler:
initWithTargetedImageData:options:
allowsCachingOfResults
willAcceptCachedResultsFromRequestWithConfiguration:
sequencedRequestPreviousObservationsKey
applyConfigurationOfRequest:
internalPerformInContext:error:
burstFrameIdentifier
setBurstFrameIdentifier:
imageProperties
setImageProperties:
UUIDString
UUID
_faceDetector
_faceBBoxAligner
purgeIntermediates
setObject:atIndexedSubscript:
addObjectsFromArray:
initWithObjectsAndKeys:
_imageCropAndScaleOption
initWithRequestClass:
copyWithZone:
imageCropAndScaleOption
setImageCropAndScaleOption:
_applicableDetectorAndOptions:error:
newDefaultDetectorOptionsForRequestRevision:
resultsSortingComparator
supportedImageSizeSet
knownClassificationsForRevision:error:
configurationClass
supportsPrivateRevision:
descriptionForPrivateRevision:
leftEyeOpen
rightEyeOpen
smiling
foundByFaceCore
hasLeftEye
hasRightEye
hasRollAngle
hasYawAngle
smallFace
_hasPitchAngle
_isSyncedWithImage
normalizedSigma
focusScore
normalizedFocusScore
faceScore
leftEyeBlinkScore
rightEyeBlinkScore
smileScore
FCRLeftEyeFeaturesOffset
FCRRightEyeFeaturesOffset
FCRSmileFeaturesOffset
FCRBlinkFeaturesSize
FCRSmileFeaturesSize
rollAngle
yawAngle
_pitchAngle
FCRSmileAndBlinkFeatures
timestamp
normalizedFaceRect
leftEyeRect
rightEyeRect
_hwFaceRect
initWithFaceStat:
setLeftEyeOpen:
setRightEyeOpen:
setSmiling:
setFoundByFaceCore:
setNormalizedFaceRect:
setNormalizedSigma:
setFocusScore:
setNormalizedFocusScore:
setFaceScore:
setHasLeftEye:
setHasRightEye:
setLeftEyeRect:
setRightEyeRect:
setLeftEyeBlinkScore:
setRightEyeBlinkScore:
setSmileScore:
setFCRLeftEyeFeaturesOffset:
setFCRRightEyeFeaturesOffset:
setFCRSmileFeaturesOffset:
setFCRBlinkFeaturesSize:
setFCRSmileFeaturesSize:
setFCRSmileAndBlinkFeatures:
setHasRollAngle:
setHasYawAngle:
setRollAngle:
setYawAngle:
setTimestamp:
setSmallFace:
setHwFaceRect:
hasPitchAngle
setHasPitchAngle:
pitchAngle
setPitchAngle:
isSyncedWithImage
setIsSyncedWithImage:
allocWithZone:
colorHistogram
numEntries
aeMatrix
dissimilarity
projectionSignature
sharpnessGrid
gridWidth
gridHeight
gridROI
smoothedROI
maxSkewness
roiSize
exclude
AEStable
AFStable
hasRegistrationData
emotionallyRejected
doLimitedSharpnessAndBlur
isGarbage
orientation
AEAverage
AETarget
temporalOrder
avgHorzDiffY
blurExtent
imageScore
actionScore
registrationErrorX
registrationErrorY
registrationErrorIntegral
actionClusteringScore
numHWFaces
_AEDelta
timeReceived
_imageId
_faceStatArray
facesRoiRect
allocateMeanStdPingPongBuffers::::
assignMeanStdBuffers:
initWithIdentifier:
dealloc
computeImageColorHistogram:
getSharpnessAndBlurLimits
computeImageSharpnessOnGrid:
computeBlurStatsOnGrid:
computeSmoothedGridROI:nextStat:
updateROI:
flagAsGarbage
performRegistration:deltaCol:deltaRow:
computeImageProjections:
canRegister
writeGridROI:
computeImageData:faceIDCounts:
collapseSharpnessGrid
computeFacialFocusScoreSum
computeRuleOfThreeDistance
computeSmilePercentage
computeImageDistance:
computeAEMatrixDifference:
setAEMatrix:
computeAEMatrix:
computeScore:
compareImageStats:
compareImageOrder:
setOrientation:
setExclude:
setAEStable:
setAEAverage:
setAETarget:
setAFStable:
setTemporalOrder:
setAvgHorzDiffY:
setBlurExtent:
setImageScore:
setActionScore:
setTimeReceived:
setMaxSkewness:
setRegistrationErrorX:
setRegistrationErrorY:
setHasRegistrationData:
setRegistrationErrorIntegral:
setActionClusteringScore:
setFacesRoiRect:
setNumHWFaces:
setEmotionallyRejected:
setDoLimitedSharpnessAndBlur:
setTx:
setTy:
setIsGarbage:
setRoiSize:
imageId
setImageId:
faceStatArray
setFaceStatArray:
AEDelta
setAEDelta:
getBytes:length:
_requestPerformer
cancelAllRequests
prepareForPerformingRequestsOfClass:error:
prepareForPerformingRequests:error:
_performRequests:onUnvettedImageBuffer:gatheredForensics:error:
performRequests:onCVPixelBuffer:error:
performRequests:onCVPixelBuffer:orientation:error:
performRequests:onCVPixelBuffer:gatheredForensics:error:
performRequests:onCVPixelBuffer:orientation:gatheredForensics:error:
performRequests:onCGImage:error:
performRequests:onCGImage:orientation:error:
performRequests:onCGImage:gatheredForensics:error:
performRequests:onCGImage:orientation:gatheredForensics:error:
performRequests:onCIImage:error:
performRequests:onCIImage:orientation:error:
performRequests:onCIImage:gatheredForensics:error:
performRequests:onCIImage:orientation:gatheredForensics:error:
performRequests:onImageURL:error:
performRequests:onImageURL:orientation:error:
performRequests:onImageURL:gatheredForensics:error:
performRequests:onImageURL:orientation:gatheredForensics:error:
performRequests:onImageData:error:
performRequests:onImageData:orientation:error:
performRequests:onImageData:orientation:gatheredForensics:error:
performRequests:onImageSpecifier:error:
performRequests:onImageSpecifier:gatheredForensics:error:
imageBufferAndReturnError:
initWithData:options:
numberWithUnsignedInt:
initWithURL:options:
initWithCIImage:options:
initWithCGImage:options:
initWithCVPixelBuffer:options:
requestForcedCleanup
forcedCleanup
requestForcedCleanupWithOptions:
requestForcedCleanupWithOptions:completion:
forcedCleanupWithOptions:
asyncProcessingDispatchQueue
_options
_imageSpecifier
_observationsCache
initWithImageSpecifier:
initWithCVPixelBuffer:orientation:options:
initWithCGImage:orientation:options:
initWithCIImage:orientation:options:
initWithURL:orientation:options:
initWithData:orientation:options:
performRequests:error:
performRequests:gatheredForensics:error:
_burstAnalysisLoggingCallback
_modelContextObject
burstAnalysisLoggingCallback
setBurstAnalysisLoggingCallback:
modelContextObject
setModelContextObject:
setModelRequestHandler:
modelRequestHandlerAndReturnError:
_createHomographicPixelBufferFromImageBuffer:cropRect:options:error:
_createN:CVPixelBuffers:withPixelFormat:width:height:error:
_calculateHomographicWarpTransform:ofFloatingImagePixelBuffer:ontoReferenceImagePixelBuffer:usingImageRegistrationContext:glContext:seededWithPreviousWarpTransform:error:
wantsSequencedRequestObservationsRecording
_sceneObservation
_customHierarchy
_maximumLeafObservations
_maximumHierarchicalObservations
sceneObservation
setSceneObservation:
customHierarchy
setCustomHierarchy:
maximumLeafObservations
setMaximumLeafObservations:
maximumHierarchicalObservations
setMaximumHierarchicalObservations:
initWithSceneObservation:
initWithSceneObservation:completionHandler:
_errorForUnimplementedSelector:forRevision:
_classificationOperationPointsAndReturnError:
_setCustomHierarchy:
setRevision:
defineCustomHierarchy:error:
defineCustomHierarchyWithRelationships:error:
enumerateKeysAndObjectsUsingBlock:
_imageAnalyzerMultiDetectorForRevision:detectionLevel:processingDevice:requestBackingStore:appliedDetectorOptions:error:
_sceneClassifierForRevision:requestBackingStore:appliedDetectorOptions:error:
_knownVNSceneClassifierLabelsForRevision:requestBackingStore:error:
_knownVNImageAnalyzerMultiDetectorSceneClassificationObservationsForRevision:requestBackingStore:error:
dependentRequestCompatability
knownSceneClassifications
knownSceneClassificationsForRevision:error:
sortedArrayUsingSelector:
compare:
archivedDataWithRootObject:requiringSecureCoding:error:
read:maxLength:
write:maxLength:
unarchivedObjectOfClass:fromData:error:
reason
streamError
_signatureData
initWithSignatureData:
setSignatureData:
getSignatureData
computeBlurSignatureForGrayscaleImage:error:
computeBlurScore:onGrayscaleImage:andWithRegionOfInterestInImageCoordinates:andRegionOfInterestInsetFactor:error:
computeBlurScore:onGrayscaleImage:insetFactor:error:
computeBlurScore:usingBlurSignature:insetFactor:imageROI:error:
computeEdgeWidthBlurScore:onGrayscaleImage:error:
computeApproximateBlurScore:onGrayscaleImage:sampledPixelsCount:insetFactor:error:
computeApproximateBlurScore:onRGBAImage:sampledPixelsCount:insetFactor:error:
dataWithBytesNoCopy:length:freeWhenDone:
wipe_layers_blobs
initWithPlatform:
set_priority:low_priority_max_ms_per_command_buffer:gpu_priority:
initWithJSFile:context:computePath:
initWithJSFile:binSerializerId:context:computePath:
recordDefaultOptionsInDictionary:
frameCVPixelBufferFormatForRequestRevision:
trackerTypeForRequestRevision:error:
raise:format:
_inputObservation
_trackingLevel
_lastFrame
_trackingLevelOptionFromTrackingLevelEnum
setTrackingLevel:
initWithDetectedObjectObservation:
initWithDetectedObjectObservation:completionHandler:
newDefaultRequestInstance
setInputObservation:
inputObservation
_resetTrackerIfNeeded:trackerProvider:options:error:
trackingLevel
isLastFrame
setLastFrame:
numberWithUnsignedLong:
_useGPU
_metalDevice
_wisdomParams
initWithMetalDevice:
metalDevice
wisdomParams
useGPU
name
_configurationOptions
_processingQueue
_metalContext
_backingStore
_synchronizationQueue
requestRevision
initWithConfigurationOptions:
warmUpWithOptions:error:
processInSynchronizationQueueUsingQualityOfServiceClass:options:regionOfInterest:warningRecorder:error:
currentQueueIsSynchronizationQueue
validateImageBuffer:error:
validatedImageBufferFromOptions:error:
needsMetalContext
newMetalContextForConfigurationOptions:error:
getOptionalCanceller:inOptions:error:
requiredCancellerInOptions:error:
validatedProcessingDeviceInOptions:error:
updateConfigurationOptionsWithObject:forKey:
configurationOptions
processingQueue
metalContext
backingStore
synchronizationQueue
setSynchronizationQueue:
detectorName
fullyPopulateConfigurationOptions:
detectorClassForConfigurationOptions:error:
detectorWithConfigurationOptions:error:
detectorKeyComponentForDetectorConfigurationOptionKey:value:
initWithArray:
scanCharactersFromSet:intoString:
scanUpToCharactersFromSet:intoString:
isAtEnd
scannerWithString:
uppercaseLetterCharacterSet
substringFromIndex:
hasPrefix:
_kindToOriginalRequestsMapping
_detectorConfigurationOptions
setDetectorConfigurationOption:value:
addOriginalRequest:forKind:
detectorConfigurationOptions
enumerateOriginalRequestsByKindUsingBlock:
processingDevice
preferBackgroundProcessing
_groupingConfigurations
groupingConfigurationForRequest:kind:
allGroupingConfigurations
groupingConfigurationsCount
methodForSelector:
_detectorType
_originalRequestConfigurations
detectorType
setDetectorType:
setDetectorConfigurationOptions:
originalRequestConfigurations
setOriginalRequestConfigurations:
_groupingConfiguration
initWithDetectorType:groupingConfiguration:
_configuredDetectorForRequestRevision:appliedConfigurationOptions:error:
isEqualToDictionary:
_addCompoundRequestsToArray:forModel:withGroupingConfigurations:
compoundRequestsForOriginalRequests:withPerformingContext:error:
_regionMap
_userBBox
_internalAlignedBBox
_deallocateBuffer
_pixelValueToRegionLabelMap
_regionLabels
supportsSecureCoding
encodeWithCoder:
initWithCoder:
initWithRequestRevision:regionMap:deallocateBuffer:userBBox:alignedBBox:valueToLabelMap:
getRegionLabels
regionNameAtNormalizedAlignedFaceCoordinate:
regionNameAtImageCoordinate:imageSize:
regionNameAtNormalizedFaceCoordinate:
regionLabels
setRegionLabels:
numberWithUnsignedChar:
encodeInteger:forKey:
encodeObject:forKey:
decodeIntegerForKey:
failWithError:
decodeObjectOfClass:forKey:
targetsANE
espressoStorageType
object:overridesSelector:
instanceMethodForSelector:
instancesRespondToSelector:
_signallingBlock
_lock
_signalled
_releaseSignallingBlock
tryToPerformBlock:usingSignallingBlock:
reset
signalCancellation
wasSignalled
_faceAttributesPupilRefiner
mLandmarkRefinerModelFileHandle
modelFilesWereMemmapped
loadRefinersAndReturnError:
createLumaPixelBufferFrom:forFaceBBox:initializeVImage:initializeRect2D:initializeIgnoreCropAndScaleFlag:initializeLumaScaleFromOriginal:options:error:
calculatePupilLocationAndUpdateLandmarkPoints:
computeLandmarksScoreOnImage:withFaceBoundingBox:andLandmarks:error:
postprocessLandmarkResultsForLandmarks:imageBuffer:outputFace:options:warningRecorder:error:
normalizedFaceBBoxForLandmarks:
detectBlinkOnFaceImage:faceObservation:lumaRec2DInImageCoordinates:landmarks:warningRecorder:error:
computeCentroidUsingPoints:indicies:numberOfIndicies:
landmarksMeshPartsForConstellation:
setValue:forKey:
_generalConfigurations
_observationGroupConfigurations
configurationForRequest:withObservationGroup:
allConfigurations
initWithDetectorType:configuration:
_determineFacesToProcessFrom:outputFacesThatNeedNoProcessing:outputfacesThatNeedAttributes:isFaceprintRequest:isAttributeRequest:
detectionLevel
assignOriginalRequestsResultsFromObservations:obtainedInPerformingContext:
addToGroupingsRequest:withFaceObservations:
_observationGroupsToRequestMapping
requestsForGroup:
mainCIContext
mainCIContextMetalDevice
lowPriorityCIContext
lowPriorityCIContextMetalDevice
activeImageBuffers
bufferTableLock
purgeAllCaches
addImageBuffer:
removeBuffer:
sharedCIContextWithOptions:
unlock
contextWithOptions:
contextWithMTLDevice:options:
lock
purgeCachedRepresentations
nextObject
objectEnumerator
weakObjectsHashTable
manager
_getOrientationLock
_loadSubSample1Lock
_loadSubSample2Lock
_loadSubSample4Lock
_loadSubSample8Lock
_imageSourceSubsample1
_imageSourceSubsample2
_imageSourceSubsample4
_imageSourceSubsample8
_imageURL
_imageData
_orientation
initWithImageURL:
initWithImageData:
_obtainCreatedCGImageSourceRefAtAddress:forSubSampleFactor:protectedWithUnfairLock:operatingInLowPriority:
obtainImageSourceRef
obtainImageSourceRefWithSubSampleFactor:andLowPriorityHint:
imageURL
imageData
exifOrientation
_origPixelBuffer
_pixelBufferReps
_origCIImage
_passedInCIContext
_imageSourceManager
_origImageWidth
_origImageHeight
initWithOptions:
originalPixelBuffer
bufferWithWidth:height:format:options:error:
_cropCVPixelBuffer:outBuffer:width:height:format:cropRect:performCrop:options:error:
_calculateTargetRectFromCropRect:
_isRectOutOfBounds:
_cropCIImage:outBuffer:width:height:format:cropRect:performCrop:options:error:
_cropImageSourceManager:outBuffer:width:height:format:cropRect:performCrop:options:error:
_retrieveBufferFromCacheIfFoundWithWidth:height:format:
croppedBufferWithWidth:height:format:cropRect:options:error:
cropAndScaleBufferWithWidth:height:cropRect:format:imageCropAndScaleOption:options:error:calculatedNormalizedOriginOffset:calculatedScaleX:calculatedScaleY:
calculateOrientationCorrectedImageDimensions
width
height
getPixelFocalLengthIfAvailable:
getCameraOpticalCenterIfAvailable:
getCameraIntrinsicsAvailable:
fileURL
processInChunksOfSize:overlapFraction:options:roi:handler:error:
makeClippedRectAgainstImageExtentUsingOriginalRect:
createBufferWithMaxSideLengthOf:andPixelFormat:andOptions:error:
createCroppedBufferWithMaxSideLengthOf:andCropBounds:andPixelFormat:andOptions:error:
_baseCIImage
_baseCVPixelBuffer
augmentedBuffersWithWidth:height:format:options:augmentationOptions:error:
augmentedCroppedBuffersWithWidth:height:format:cropRect:options:augmentationOptions:error:
_optionsWithOverridingOptions:
_useCoreImageForFormat:
_dumpIntermediateImage:withOptions:
createDirectoryAtPath:withIntermediateDirectories:attributes:error:
hasSuffix:
substringWithRange:
rangeOfString:options:range:
rangeOfString:options:
callStackSymbols
imageByApplyingCGOrientation:
imageByCroppingToRect:
colorWithRed:green:blue:alpha:
imageByApplyingTransform:
outputImage
imageByClampingToExtent
filterWithName:
ioSurfaceBackedPixelBufferAttributes
firstObjectCommonWithArray:
render:toCVPixelBuffer:
imageWithContentsOfURL:
imageWithCVImageBuffer:
extent
copyColorspaceForFormat:bitmapInfo:
computeCenterCropRectFromCropRect:inImageSize:calculatedScaleX:calculatedScaleY:
imageWithData:options:
imageWithContentsOfURL:options:
render:toCVPixelBuffer:bounds:colorSpace:
imageByCompositingOverImage:
imageByClampingToRect:
clearImage
imageByApplyingOrientation:
imageByApplyingTransform:highQualityDownsample:
imageTransformForCGOrientation:
imageWithCGImage:
_helpReadOrientationFromOptionsDictionary:
imageWithData:
inputFaceObservations
setInputFaceObservations:
_determineFacesToProcessFrom:outputFacesThatNeedNoProcessing:outputFacesThatNeedAlignment:outputFacesThatNeedTorsoprints:
_rpnEspressoResourcesKeyToEspressoResourcesCache
_rpnEspressoResourcesKeyToEspressoResourcesCacheLock
_espressoModelName
initWithRPNEspressoModelName:
espressoResourcesFromOptions:error:
espressoModelName
cacheKeyFromOptions:error:
cacheOptionsKeys
serializeRPNTrackingQueue
rpnTrackQueue
rpnInitQueue
rpnTrackEspressoResourcesCacheManager
rpnInitEspressoResourcesCacheManager
rpnTrackerInitProcessingQueueName
rpnTrackerTrackProcessingQueueName
rpnTrackerInitModelName
rpnTrackerTrackModelName
detect:face:sublandmark:doFaceRectFix:
newface
pathExtension
initWithNetworkAtPath:contextObjC:platform:computePath:
_modelData
_faceModel_DO_NOT_ACCESS_DIRECTLY
personsModelDataWasModified:
initWithConfiguration:
_modelWasModified
_lastModificationDate
upToDateFaceModelWithCanceller:error:
_writeVersion1InformationToOutputStream:md5Context:error:
_writeVersion1ConfigurationToOutputStream:md5Context:error:
writeVersion1ToOutputStream:options:md5Context:error:
_writeReadOnlyVersion:toOutputStream:options:md5Context:error:
writeReadOnlyVersion1ToOutputStream:options:md5Context:error:
_getModelWritingImplementation:selector:forVersion:readOnly:
_getModelWritingImplementation:selector:version:forOptions:
writeToStream:options:error:
_writeToUnopenedStream:options:error:
dataWithOptions:error:
writeToURL:options:error:
addFaceObservations:toPersonWithUniqueIdentifier:error:
removeFaceObservations:fromPersonWithUniqueIdentifier:error:
removeAllFaceObservationsFromPersonWithUniqueIdentifier:error:
removePersonWithUniqueIdentifier:error:
initWithURL:append:
propertyForKey:
initToMemory
close
open
enumerateIndexesWithOptions:usingBlock:
initWithIndex:
supportedWriteVersions
enumerateObjectsAtIndexes:options:usingBlock:
initWithIndexesInRange:
setLength:
appendData:
appendBytes:length:
localizedStringFromDate:dateStyle:timeStyle:
configurationFromLoadedObjects:error:
_version1ModelWithObjects:error:
newModelFromVersion:objects:error:
addIndexes:
keyEnumerator
_completionHandler
_configuration
_warningRecorder
_canceller
_cancellationTriggered
_cancellationSemaphore
_cancellationQueue
_revision
_results
recordWarning:value:
valueForWarning:
_defaultProcessingDevice
_updateProcessingDeviceOption
configuration
initWithCompletionHandler:
setValue:forPrivateOption:
valueForPrivateOption:
setValue:forRequestOption:
hasCancellationHook
newDefaultDetectorOptions
copyStateOfRequest:
performInContext:error:
validateConfigurationAndReturnError:
internalCancelInContext:error:
setResults:
setSortedResults:
warnings
cancel
cancellerAndReturnError:
cancellationTriggered
cancellationTriggeredAndReturnError:
setPreferBackgroundProcessing:
modelFileBackingStore
setModelFileBackingStore:
preferredMetalContext
setPreferredMetalContext:
usesCPUOnly
setUsesCPUOnly:
metalContextPriority
setMetalContextPriority:
setProcessingDevice:
setDetectionLevel:
validateImageBuffer:ofNonZeroWidth:andHeight:error:
_setResolvedRevision:
setPrivateRevision:error:
revision
resolvedRevision
compatibleRevisionForDependentRequest:
results
completionHandler
cancellationSemaphore
setCancellationSemaphore:
options
compatibleRevisionForDependentRequestOfClass:beingPerformedByRevision:
setResolvedRevision:
resolvedRevisionForRevision:
null
sortedArrayWithOptions:usingComparator:
supportedRevisions
defaultRevision
newConfigurationInstance
requestClass
initialize
defaultRequestInstanceWarmUpPerformer:error:
getOptionalObject:ofClass:forKey:inOptions:error:
getRequiredObject:ofClass:forKey:inOptions:error:
getDoubleValue:forKey:inOptions:error:
getDoubleValue:forKey:inOptions:withDefaultValue:error:
getFloatValue:forKey:inOptions:error:
getFloatValue:forKey:inOptions:withDefaultValue:error:
getOptionalArray:forKey:inOptions:withElementsOfClass:error:
getOptionalInputFacesArray:inOptions:error:
_defaultRevisionForBuildVersion:
_introspectionBuiltSupportedRevisions
currentRevision
lastIndex
addIndex:
_preferBackgroundProcessing
_requestClass
_resolvedRevision
_detectionLevel
_processingDevice
_metalContextPriority
_modelFileBackingStore
_allPropertyNames
valueForKey:
sortUsingSelector:
initWithUTF8String:
_minimumDimension
_maximumDimension
_idealDimension
initWithMinimumDimension:maximumDimension:idealDimension:
isAllowedDimension:
minimumDimension
maximumDimension
idealDimension
_cachedCalculatedHash
_orientationAgnostic
_idealImageFormat
_idealOrientation
_pixelsWideRange
_pixelsHighRange
_aspectRatioHandling
initWithIdealFormat:pixelsWideRange:pixelsHighRange:aspectRatioHandling:idealOrientation:orientationAgnostic:
isAllowedPixelsWide:pixelsHigh:
pixelsWideRange
pixelsHighRange
aspectRatioHandling
idealImageFormat
idealOrientation
isOrientationAgnostic
decodeBoolForKey:
decodeInt32ForKey:
encodeBool:forKey:
encodeInt32:forKey:
_faceprint
_torsoprint
initWithData:elementCount:elementType:lengthInBytes:labelsAndConfidence:requestRevision:
initWithData:elementCount:elementType:lengthInBytes:labelsAndConfidence:validTorsoprint:requestRevision:
initWithFaceprint:torsoPrint:requestRevision:
computeDistance:withDistanceFunction:error:
isValidTorsoprint
initWithState:error:
initWithState:startingAtByteOffset:error:
serializeStateIntoData:startingAtByteOffset:error:
serializeStateAndReturnError:
serializedLength
faceprint
torsoprint
dataWithLength:
currentVersion
currentCodingVersion
codingTypesToCodingKeys
currentSerializationVersion
getPixelBufferFP32FormattedAndReturnError:
initWithDevice:andWisdomParams:
_context
processImagesFromDataProvider:error:
computeClusteringOfImageDescriptors:intoKGroups:error:
computeNaturalClusteringOfImageDescriptors:error:
computeClusteringTreeForImageDescriptors:error:
computeClusteringTreeForImageDescriptors:assumeDescriptorsAreSorted:error:
convertClusterNodesListToDescriptorsList:
performClustersPostprocessing:error:
computeClusteringForClusteringTree:intoKGroups:error:
computeClusteringForClusteringTree:usingThreshold:error:
computeNaturalClusteringForClusteringTree:error:
getKey:fromDictionary:withDefault:
context
setContext:
node
initWithNode:freeNodeOnDealloc:
_freeNodeOnDealloc
_node
nodeId
descriptor
left
right
distance
avgDistance
leafsCount
getLeafNodes
setNode:
freeNodeOnDealloc
setFreeNodeOnDealloc:
sortImageDescriptorsChronologically:
getNextImage:
_smartCamCombinedModelImageSaliencyObservationsForRevision:performedInContext:error:
_bounds
_confidence
_objectType
initWithObjectType:boundingBox:confidence:
center
objectType
setObjectType:
bounds
setBounds:
confidence
setConfidence:
_pixelBuffer
initWithCVPixelBuffer:
pixelBuffer
newImageBufferWithOptions:error:
initInternal
_imageBuffer
imageSpecifierWithCVPixelBuffer:error:
imageSpecifierWithCVPixelBuffer:orientation:error:
imageSpecifierWithCVPixelBuffer:orientation:options:error:
imageSpecifierWithCGImage:error:
imageSpecifierWithCGImage:orientation:error:
imageSpecifierWithCGImage:orientation:options:error:
imageSpecifierWithCIImage:error:
imageSpecifierWithCIImage:orientation:error:
imageSpecifierWithCIImage:orientation:options:error:
imageSpecifierWithURL:error:
imageSpecifierWithURL:orientation:error:
imageSpecifierWithURL:orientation:options:error:
imageSpecifierWithData:error:
imageSpecifierWithData:orientation:error:
imageSpecifierWithData:orientation:options:error:
initWithData:
_data
data
initWithURL:
_url
initWithCIImage:
_ciImage
ciImage
initWithCGImage:
_cgImage
cgImage
imageSpecifierWithObject:error:
isSubclassOfClass:
finishDecoding
setClass:forClassName:
initForReadingFromData:error:
containsValueForKey:
serializationMagicNumber
confidenceTypeForRevision:
_extrema
_extremeValues
updateExtrema:x:y:
computeRectFromExtremaUsingThreshold:vImage:
_mOriginalImageSize
_mSalientRegion
_mHighlySalientRegion
_mComputeBoundingBoxesLock
_mSalientObjects
initWithRequestRevision:rawSaliencyImage:originalImageSize:
createSaliencyImageAndReturnError:
salientObjectsAndReturnError:
calculateSaliencyBoundingBoxesForDetectorType:configurationOptions:regionOfInterest:warningRecorder:error:
_computeBoundingBoxes
salientObjects
boundingBox
narrowedBoundingBox
getValue:size:
encodeDouble:forKey:
decodeDoubleForKey:
_smartCamCombinedModelImageAestheticsClassificationsForRevision:performedInContext:error:
_cornerTrackersImpl
_rectangleTrackingProcessingQueue
setTrackedObjects:inFrame:error:
trackInFrame:error:
reset:
isTracking
_parseInputObservations:imageBuffer:error:
_convertCornerObservationsToRectangleObservation:error:
_trackingRectAroundPoint:trackingRectSize:
trackedCorners
trackerObservationClass
_includeClusters
_includeAllImageIdentifiers
_includeAllImageStats
includeClusters
setIncludeClusters:
includeAllImageIdentifiers
setIncludeAllImageIdentifiers:
includeAllImageStats
setIncludeAllImageStats:
modelPath
clusteringConfidence
numberOfChannels
imageType
pixelFormat
faceDescriptorCreator
faceBoundingBox:
cropFaceBoundingBoxFrom:cropBounds:error:
frontalizer
getFaceJunkClassifier
magnifiedBBoxScaleFactor
m_FaceAttributesImpl
m_LandmarkRefinerModelFileHandle
expressionTypeFromString:
createExpressionAndConfidencesDictionaryFromScores:
createExpressionDetectionDictionaryFromScores:
bundleForClass:
dividerScore
trueLocalMaximum
leftImage
actionAmount
noiseThreshold
highNoiseThreshold
compareDividers:
compareIndices:
compareActionAmounts:
setDividerScore:
setLeftImage:
setTrueLocalMaximum:
setActionAmount:
setNoiseThreshold:
setHighNoiseThreshold:
_reportCharacterBoxes
_detectDiacritics
_minimizeFalseDetections
_algorithm
_minimumCharacterPixelHeight
_textRecognition
reportCharacterBoxes
setReportCharacterBoxes:
algorithm
setAlgorithm:
minimumCharacterPixelHeight
setMinimumCharacterPixelHeight:
detectDiacritics
setDetectDiacritics:
minimizeFalseDetections
setMinimizeFalseDetections:
textRecognition
setTextRecognition:
_detectCreditCardTextWithRequestPerformingContext:requestRevision:error:
_detectTextWithRequestPerformingContext:requestRevision:error:
subFeatures
corners
detectFeaturesInBuffer:withRegionOfInterest:error:
setReturnSubFeatures:
setRecognitionLanguage:
setMinimumCharacterHeight:
initWithDimensions:
_personsModel
_dataSource
faceModelPersonsCount
faceModelUniqueIdentifierOfPersonAtIndex:
faceModelIndexOfPersonWithUniqueIdentifier:
faceModelNumberOfFaceObservationsForPersonAtIndex:
faceModelFaceObservationAtIndex:forPersonAtIndex:
initWithPersonsModel:dataSource:
_lastDataChangeSequenceNumber
initWithConfiguration:dataSource:
updateInternalConfigurationWithModelFaceprintRequestRevision:error:
predictPersonFromFaceObservation:limit:canceller:error:
personCount
personUniqueIdentifiers
faceCountForPersonWithUniqueIdentifier:
faceCountsForPersonsWithUniqueIdentifiers:
faceCountsForAllPersons
VNPersonsModelFaceprintWithRequestRevision:error:
faceprintRequestRevision
setFaceprintRequestRevision:
_modelClassForKind:error:
versionNumbersEncodedInClass:withMethodNamePrefix:suffix:
supportedReadVersions
_readModelObjectsFromStream:options:actionBlock:progressBlock:modelClass:version:error:
_modelFromStream:options:error:
modelFromStream:options:error:
_modelFromUnopenedStream:options:error:
modelFromData:options:error:
modelFromURL:options:error:
_modelInformationFromUnopenedStream:error:
informationForModelWithData:error:
informationForModelWithURL:error:
readObjectForVersion1Tag:fromInputStream:intoObjectDictionary:md5Context:error:
subdataWithRange:
initWithDomain:code:userInfo:
initWithVersion:lastModificationDate:
lastModificationDate
isEqualToDate:
distantPast
_acceptableVersions
acceptableVersions
setAcceptableVersions:
setMaximumFaceprintsPerIdentity:
setMaximumIdentities:
_maximumIdentities
_maximumFaceprintsPerIdentity
_faceprintRequestRevision
maximumFaceprintsPerIdentity
maximumIdentities
maximumAllowableFaceprintsPerIdentity
maximumAllowableIdentities
_faceObservation
_predictedPersonUniqueIdentifier
initWithFaceObservation:predictedPersonUniqueIdentifier:confidence:
faceObservation
predictedPersonUniqueIdentifier
encodeFloat:forKey:
decodeFloatForKey:
readOnly
setReadOnly:
_activeDetectorsCacheLock
_activeDetectorsCache
_detectorTypeToSynchronizationQueueLookupLock
_detectorTypeToSynchronizationQueueLookup
_cachedMetalDeviceWisdomParameters
_flushMetalDeviceWisdomParametersCache
_detectorClassForDetectorType:error:
_synchronizationQueueForDetectorType:
_specialCaseLookUpOfExistingDetector:forType:configuredWithOptions:error:
_detectorOfClass:type:configuredWithOptions:error:
_detectorClassForDetectorType:options:detectorCreationOptions:error:
wisdomParametersForMetalDeviceWithName:
detectorOfType:options:error:
detectorClassForDetectorType:options:
_removeCachedDetectorClasses:
_removeCachedDetectorTypes:
_removeAllCachedDetectors
loadedDetectors
_forcedCleanupFacePipelineWithLevel:
_forcedCleanupScenePipelineWithLevel:
_forcedCleanupSmartCamPipelineWithLevel:
_forcedCleanupJunkPipelineWithLevel:
removeAllObjects
removeObjectsForKeys:
keysOfEntriesPassingTest:
containsString:
saliencyHeatmapBoundingBoxGeneratorForDetector:withOptions:error:
_cachedFloatingImageBuffer
_cachedFloatingImageSignature
cachedFloatingImageBufferReturningError:
cachedFloatingImageRegistrationSignatureReturningError:
getReferenceImageBuffer:registrationSignature:forRequestPerformingContext:options:error:
_mostLikelyLabel
_allLabelsWithConfidences
_computeLabel
setAllLabelsWithConfidences:
initWithRequestRevision:
label
setLabel:
allLabelsWithConfidences
decodeObjectOfClasses:forKey:
initWithObjects:
_ageCategory
_genderCategory
_eyesCategory
_smilingCategory
_faceHairCategory
_hairColorCategory
_baldCategory
_glassesCategory
_makeupCategory
_makeupEyesCategory
_makeupLipsCategory
ageCategory
setAgeCategory:
genderCategory
setGenderCategory:
eyesCategory
setEyesCategory:
smilingCategory
setSmilingCategory:
faceHairCategory
setFaceHairCategory:
hairColorCategory
setHairColorCategory:
baldCategory
setBaldCategory:
glassesCategory
setGlassesCategory:
makeupCategory
setMakeupCategory:
makeupEyesCategory
setMakeupEyesCategory:
makeupLipsCategory
setMakeupLipsCategory:
_warnings
hasWarnings
setWarnings:
recordWarnings:
getHostTime
getHostTimeInNanos
createErrorWithCode:andMessage:
freeVImageBuffer:
parseExifTimestamp:
timeIntervalSince1970
dateFromString:
_operationPoints
operationPointsAndReturnError:
initWithOperationPoints:
computeImageQuality:forCriteria:error:
_faceSegmenterDNN
_getFaceSegmenterInputImageSize:forRequestRevision:error:
_getNumberOfSupportedFaceSegments:forRequestRevision:error:
_fillFaceSegmentLabelToProbabilityMap:error:
initWithBytesNoCopy:length:freeWhenDone:
initWithBytesNoCopy:length:
_faceSegmenterMaximumInputImageAspectRatio
_analyzePixelBuffer:sceneprintOutputBuffer:options:error:
_analyzeRegionOfInterest:sceneprintOutputBuffer:options:warningRecorder:error:
_observationsForSceneprintOutput:requestRevision:error:
_nmsThreshold
_network
_osfsThreshold
_osfsSizeRatio
_olmcsThreshold
_olmcsMergeCountDelta
_smartThreshold
_smartDistanceFactor
_filterThreshold
initWithNetwork:filterThreshold:
initWithNetwork:
threshold
setThreshold:
overlappingSmallFacesSuppression:
overlappingLowMergeCountSuppression:
mergeBoxes:
smartMergeBoxes:
nmsBoxes:
sortBoxes:filterThresholdIndex:
filterBoxes:
detect:inputIsBGR:
processBoxes:withHeight:andWidth:
detectAndProcessObjects:inputIsBGR:
enforceSquareFaces:withHeight:andWidth:
nmsThreshold
setNmsThreshold:
filterThreshold
setFilterThreshold:
osfsThreshold
setOsfsThreshold:
osfsSizeRatio
setOsfsSizeRatio:
olmcsThreshold
setOlmcsThreshold:
olmcsMergeCountDelta
setOlmcsMergeCountDelta:
smartThreshold
setSmartThreshold:
smartDistanceFactor
setSmartDistanceFactor:
filteredArrayUsingPredicate:
predicateWithBlock:
sortedArrayUsingDescriptors:
sortDescriptorWithKey:ascending:
filterThresholds
inputLayerName
modelName
inputImageSize
inputImageMinDimension
inputImageMaxDimension
inputImageAspectRatio
supportedLabelKeys
processingDeviceDetectorWithModelPath:networkThreshold:filterThreshold:preferredDeviceID:engineID:storageType:
processingDeviceDetectorWithModelPath:preferredDeviceID:engineID:storageType:
processingDeviceDetectorWithEspressoNetwork:espressoPlan:networkThreshold:filterThreshold:
processingDeviceDetectorWithEspressoNetwork:espressoPlan:
networkThreshold
getSuggestedImageSize:
shotflowNetworkClass
mergeHeadsBoxes:
mDescriptorProcessor
mClassifier
_blacklistedIdentifiers
calculateImageDescriptors:regionOfInterest:warningRecorder:canceller:error:
_calculateImageDescriptors:regionOfInterest:warningRecorder:canceller:descriptorBuffer:debugIntermediatesDumpPath:outputDebugDictionary:error:
getLabels
blacklistedIdentifiers
setWithCapacity:
createHierarchicalModelForRequestRevision:error:
convertRelationships:toStdRelationships:
defaultCStringEncoding
_cachedPayloadStringValue
_symbology
_barcodeDescriptor
_acbsBarcodeInfo
initWithRequestRevision:symbology:descriptor:topLeft:bottomLeft:bottomRight:topRight:
initWithRequestRevision:symbology:descriptor:boundingBox:
payloadStringValue
symbology
barcodeDescriptor
acbsBarcodeInfo
setACBSBarcodeInfo:
indexOfObject:
_newVNBarcodeSymbologyQRDescriptorForACBSBarcodeInfo:
_newVNBarcodeSymbologyAztecDescriptorForACBSBarcodeInfo:
_newVNBarcodeSymbologyPDF417DescriptorForACBSBarcodeInfo:
_ACBarcodeRecognizerLocateMode
_createACBSConfigAndReturnError:
_getCornerPointsFromCodeLocationPoints:bottomLeft:topLeft:topRight:bottomRight:
newBarcodeObservationForACBSBarcodeInfo:imageWidth:imageHeight:roiCroppingPixelRect:requestRevision:error:
_barcodesDetectedInImageBuffer:usingACBSConfig:requestRevision:error:
symbologies
setSymbologies:
locateMode
setLocateMode:
barcodeSymbologyForACBSBarcodeType:
initWithKeyOptions:valueOptions:capacity:
initWithPayload:isCompact:rowCount:columnCount:
initWithPayload:isCompact:layerCount:dataCodewordCount:
initWithPayload:symbolVersion:maskPattern:errorCorrectionLevel:
unsignedCharValue
ACBSBarcodeTypeForBarcodeSymbology:
_allBarcodeSymbologies
supportedSymbologies
availableLocateModes
_symbologies
_locateMode
_trackerTypeToClassDictionary
_trackerClassToNameMapTable
_liveTrackerCounter
_trackingProcessingQueue
_trackersCollectionManagementQueue
_liveTrackerCounterLimit
_trackers
trackerWithOptions:error:
_maximumTrackersOfType:
_getTracker:
_createTracker:type:options:error:
releaseTracker:
numberWithInteger:
releaseManager
releaseAllTrackers
_rotationAngle
_yawAngle
_labelKey
initWithObjectType:boundingBox:confidence:rotationAngle:yawAngle:labelKey:
rotationAngle
setRotationAngle:
labelKey
setLabelKey:
_freeImageInDealloc
_image
_imageCVPixelBuffer
_imageFilePath
_externalImageId
_exifTimestamp
initWithVImage:externalImageId:andExifTimestampString:error:
initWithVImage:externalImageId:andExifTimestampValue:error:
initWithCVPixelBufferImage:externalImageId:andExifTimestampString:error:
initWithCVPixelBufferImage:externalImageId:andExifTimestampValue:error:
image
imageCVPixelBuffer
imageFilePath
setImageFilePath:
freeImageInDealloc
setFreeImageInDealloc:
externalImageId
exifTimestamp
getDefaultConfidence:forClassificationIdentifier:error:
getConfidence:forClassificationIdentifier:withPrecision:error:
getPrecision:forClassificationIdentifier:confidence:error:
getConfidence:forClassificationIdentifier:withRecall:error:
getRecall:forClassificationIdentifier:confidence:error:
errorForUnknownClassificationIdentifier:
errorForUnimplementedMethod:
unspecifiedOperationPoints
loadFromURL:error:
_applicableDetectorAndGetConfigurationOptions:error:
_detectorWantsAnisotropicScaling
_detectorPreferredImageSize
_detectorExecutionTimeInterval
detectorPreferredImageSize
setDetectorPreferredImageSize:
detectorWantsAnisotropicScaling
setDetectorWantsAnisotropicScaling:
detectorExecutionTimeInterval
setDetectorExecutionTimeInterval:
_actualSizeForDesiredSize:ofSourceImageWidth:height:
_createScaledImagePixelBufferFromCropRect:ofImageBuffer:inPixelFormat:forDetectorInputImageSize:usingAnisotropicScaling:options:error:
_createScaledImagePixelBufferFromImageBuffer:inPixelFormat:forDetectorInputImageSize:usingAnisotropicScaling:options:error:
_performNOPForRevision:inContext:detectorCompletionSemaphore:error:
_refineMouthRegion
_refineLeftEyeRegion
_refineRightEyeRegion
_performBlinkDetection
_cascadeStepCount
_constellation
refineMouthRegion
setRefineMouthRegion:
refineLeftEyeRegion
setRefineLeftEyeRegion:
refineRightEyeRegion
setRefineRightEyeRegion:
performBlinkDetection
setPerformBlinkDetection:
cascadeStepCount
setCascadeStepCount:
constellation
setConstellation:
revision:supportsConstellation:
_determineFacesToProcessFrom:outputFacesThatNeedNoProcessing:outputFacesThatNeedAlignment:outputFacesThatNeedLandmarks:
_descriptor
initWithImageDescriptor:type:requestRevision:
distanceToImageprint:error:
setDescriptor:
type
setType:
layers_size
initWithNetworkContext:
mCameraCalibrationMatrix
initWithBytes:length:
targetsGPU
espressoDeviceID
espressoEngine
registryID
allDevices
_labelsAndConfidence
_elementType
_descriptorData
_elementCount
_lengthInBytes
_confidenceScoreType
_distanceMode
elementType
_initWithClassKeyMappedCoder:
initWithCoder:forCodingVersion:
descriptorData
setDescriptorData:
elementCount
setElementCount:
lengthInBytes
setLengthInBytes:
labelsAndConfidence
setLabelsAndConfidence:
confidenceScoreType
distanceMode
setDistanceMode:
setWithObjects:
_aestheticScore
_wellFramedSubjectScore
_wellChosenBackgroundScore
_tastefullyBlurredScore
_sharplyFocusedSubjectScore
_wellTimedShotScore
_pleasantLightingScore
_pleasantReflectionsScore
_harmoniousColorScore
_livelyColorScore
_pleasantSymmetryScore
_pleasantPatternScore
_immersivenessScore
_pleasantPerspectiveScore
_pleasantPostProcessingScore
_noiseScore
_failureScore
_pleasantCompositionScore
_interestingSubjectScore
_intrusiveObjectPresenceScore
_pleasantCameraTiltScore
_lowKeyLightingScore
initWithRequestRevision:overallAestheticScore:wellFramedSubjectScore:wellChosenBackgroundScore:tastefullyBlurredScore:sharplyFocusedSubjectScore:wellTimedShotScore:pleasantLightingScore:pleasantReflectionsScore:harmoniousColorScore:livelyColorScore:pleasantSymmetryScore:pleasantPatternScore:immersivenessScore:pleasantPerspectiveScore:pleasantPostProcessingScore:noiseScore:failureScore:pleasantCompositionScore:interestingSubjectScore:intrusiveObjectPresenceScore:pleasantCameraTiltScore:lowKeyLightingScore:
wellChosenSubjectScore
_scoresDictionary
aestheticScore
wellFramedSubjectScore
wellChosenBackgroundScore
tastefullyBlurredScore
sharplyFocusedSubjectScore
wellTimedShotScore
pleasantLightingScore
pleasantReflectionsScore
harmoniousColorScore
livelyColorScore
pleasantSymmetryScore
pleasantPatternScore
immersivenessScore
pleasantPerspectiveScore
pleasantPostProcessingScore
noiseScore
failureScore
pleasantCompositionScore
interestingSubjectScore
intrusiveObjectPresenceScore
pleasantCameraTiltScore
lowKeyLightingScore
allScorePropertyNames
mFaceRegionMapAlgorithmImpl
_uuid
uuid
setUUID:
vn_encodeCodingVersion:forKey:
vn_decodeCodingVersionForKey:
_boundingBox
_identifier
excludesBoundingBoxFromCoding
setBoundingBoxFromQuadrilateralPointsAtTopLeft:topRight:bottomRight:bottomLeft:
setBoundingBox:
identifier
setIdentifier:
observationWithBoundingBox:
observationWithRequestRevision:boundingBox:
_cachedLandmarks
_cachedLandmarks65
_cachedLandmarks3d
_faceRegionMap
_faceAttributes
_faceTorsoprint
_faceSegments
_landmarkScore
_isBlinking
_blinkScore
_expressionsAndScores
_faceJunkinessIndex
_faceOrientationIndex
_alignedBoundingBox
_unalignedBoundingBox
_landmarkPoints
_landmarkPoints65
_landmarksConstellation
_landmarkPrecisionEstimatesPerPoint
_landmarkOcclusionFlagsPerPoint
_landmarkPoints3d
_poseData
_faceIdConfidence
_faceId
_hasBBoxBeenAligned
_alignedRotationAngle
_roll
_yaw
_alignedMeanShape
_faceCaptureQuality
_landmarksRequestRevision
_landmarks3DRequestRevision
_landmarks
expressionsAndConfidence
nameConfidence
landmarks
landmarks65
landmarks3d
pose
poseQuaternion
getComputedRectifyingTransform:
setIsBlinking:
setBlinkScore:
isBlinking
blinkScore
alignedBoundingBoxAsCGRect
setExpressionsAndScores:
expressionsAndScores
expressionsAndDetections
setLandmarkScore:
landmarkScore
setFaceRegionMap:
setFaceAttributes:
setFaceSegments:
setFaceCaptureQuality:
faceOrientationIndex
faceJunkinessIndex
setFaceJunkinessIndex:
setFaceOrientationIndex:
setTorsoprint:
faceTorsoprint
setFaceTorsoprint:
setUnalignedBoundingBox:
setRoll:
setYaw:
getFaceEXIFOrientation:error:
faceCaptureQuality
roll
setLandmarks:
hasBBoxBeenAligned
setHasBBoxBeenAligned:
alignedBoundingBox
setAlignedBoundingBox:
alignedRotationAngle
setAlignedRotationAngle:
landmarkPoints
setLandmarkPoints:
landmarkPoints65
setLandmarkPoints65:
landmarksConstellation
setLandmarksConstellation:
landmarkPrecisionEstimatesPerPoint
setLandmarkPrecisionEstimatesPerPoint:
landmarkOcclusionFlagsPerPoint
setLandmarkOcclusionFlagsPerPoint:
landmarkPoints3d
setLandmarkPoints3d:
poseData
setPoseData:
faceIdConfidence
setFaceIdConfidence:
alignedMeanShape
setAlignedMeanShape:
landmarksRequestRevision
setLandmarksRequestRevision:
landmarks3DRequestRevision
setLandmarks3DRequestRevision:
unalignedBoundingBox
faceRegionMap
faceAttributes
setFaceprint:
faceSegments
_exifOrientationFromFaceRollAngle:exifOrientation:error:
faceObservationWithBoundingBox:andAlignedBoundingBox:
faceObservationWithRequestRevision:boundingBox:andAlignedBoundingBox:
faceObservationWithRequestRevision:unalignedBoundingBox:alignedBoundingBox:
faceObservationWithBoundingBox:faceprint:
faceObservationWithRequestRevision:boundingBox:faceprint:
faceObservationWithRequestRevision:boundingBox:roll:yaw:
faceObservationWithRequestRevision:boundingBox:alignedBoundingBox:roll:yaw:
computeYawPitchRollFromPoseMatrix:outputYaw:outputPitch:outputRoll:
_referenceImageSignature
_floatingImageSignature
alignmentTransform
setAlignmentTransform:
referenceImageSignature
setReferenceImageSignature:
floatingImageSignature
setFloatingImageSignature:
_alignmentTransform
vn_encodeCGAffineTransform:forKey:
vn_decodeCGAffineTransformForKey:
_warpTransform
warpTransform
setWarpTransform:
vn_encode3x3Matrix:forKey:
vn_decode3x3MatrixForKey:
_blurScore
_exposureScore
blurScore
setBlurScore:
exposureScore
setExposureScore:
_imageprintValid
_imageprint
_imageprintVersion
calculateDistanceFromImageprintObservation:
isImageprintValid
initWithRawImageprintDescriptor:
rawImageprintDescriptor
imageprint
setImageprint:
imageprintValid
imageprintVersion
observationWithImageprint:error:
initWithUUIDString:
blurMeasure
brightness
_operationPointsProvider
initWithRequestRevision:identifier:confidence:
initWithRequestRevision:identifier:confidence:operationPointsProvider:
hasPrecisionRecallCurve
hasMinimumRecall:forPrecision:
hasMinimumPrecision:forRecall:
_labels
initWithRequestRevision:boundingBox:confidence:labels:
labels
_featureValue
_featureName
initWithRequestRevision:featureName:featureValue:
featureValue
featureName
initWithRequestRevision:featureName:CVPixelBuffer:
_topLeft
_bottomLeft
_bottomRight
_topRight
initWithTopLeft:bottomLeft:bottomRight:topRight:
initWithRequestRevision:topLeft:bottomLeft:bottomRight:topRight:
initWithBoundingBox:
initWithRequestRevision:boundingBox:
topLeft
bottomLeft
bottomRight
topRight
rectangleObservationWithRequestRevision:topLeft:bottomLeft:bottomRight:topRight:
_transform
_angle
transform
setTransform:
angle
setAngle:
_shouldUpdateRepresentative
_objects
_clusterId
_totalObjectCount
_suggestedIdsForRepresentative
_representativenessById
objects
setObjects:
clusterId
setClusterId:
totalObjectCount
setTotalObjectCount:
shouldUpdateRepresentative
setShouldUpdateRepresentative:
suggestedIdsForRepresentative
setSuggestedIdsForRepresentative:
representativenessById
setRepresentativenessById:
_clusters
_suggestionsForCluster
_clusterState
_clusteredFaceIds
_groupedClusteredFaceIdsForCluster
_distance
_distancesById
clusters
setClusters:
suggestionsForCluster
setSuggestionsForCluster:
clusterState
setClusterState:
clusteredFaceIds
setClusteredFaceIds:
groupedClusteredFaceIdsForCluster
setGroupedClusteredFaceIdsForCluster:
setDistance:
distancesById
setDistancesById:
computeDistance:toFeaturePrintObservation:error:
computeDistanceToFeaturePrintObservation:error:
_sceneprints
_sceneprintVersion
initWithRequestRevision:sceneprints:
sceneprints
sceneprintVersion
sceneprintCurrentVersion
observationWithSceneprints:
_smartCamprints
_smartCamprintVersion
initWithRequestRevision:smartCamprints:
smartCamprints
setSmartCamprints:
smartCamprintVersion
smartCamprintCurrentVersion
observationWithSmartCamprints:
_allImageIdentifiers
_bestImageIdentifiers
_allImageStats
_coverImageIdentifier
_isAction
_isPortrait
allImageIdentifiers
setAllImageIdentifiers:
bestImageIdentifiers
setBestImageIdentifiers:
allImageStats
setAllImageStats:
coverImageIdentifier
setCoverImageIdentifier:
isAction
setIsAction:
isPortrait
setIsPortrait:
_textObjects
_text
topCandidates:
setText:
textObjects
setTextObjects:
text
_characterBoxes
characterBoxes
setCharacterBoxes:
scanFloat:
scanString:intoString:
scanDouble:
vn_encode4x4Matrix:forKey:
vn_decode4x4MatrixForKey:
_determineFacesToProcessFrom:outputFacesThatNeedNoProcessing:outputFacesThatNeedProcessing:outputFacesThatNeed2DLandmarks:
_inputFaceObservations
_regionOfInterest
regionOfInterest
setRegionOfInterest:
getOptionalValidatedInputFaceObservations:clippedToRegionOfInterest:error:
validatedInputFaceObservationsClippedToRegionOfInterest:error:
isFullCoverageRegionOfInterest
regionOfInterestNonIntegralPixelRectForWidth:height:
regionOfInterestPixelRectForWidth:height:
_faceObservationsForRegionOfInterestContainingFaceObservations:
objectsAtIndexes:
indexesOfObjectsPassingTest:
_isSeparatedString:equalToString:atIndex:usingSeparator:
isMajorVersion:equalToMajorVersion:
isMinorVersion:equalToMinorVersion:
componentsSeparatedByString:
wipeLayersMemory
getFacesFromNetworkResultOriginalWidth:originalHeight:
processVimageNoRotation:tex:doBGRA2RGBA:
autoResizeForAspectRatio:useLowPriorityMode:gpuPriority:
is_memory_tight
shared
setIs_memory_tight:
setContextMetal:
setContextCpu:
autoSetupNetBaseName:weights:scaleConfig:setupMode:computePath:autoAspectRatio:forceReset:useLowPriorityMode:gpuPriority:
setForceMaxNScales:
smoothedFloat32ImageBuffer:fromImageBuffer:originalImageSize:sigmaX:sigmaY:nStd:
maximumValueFromFloat32ImageBuffer:
significantRegionsFromFloat32ImageBuffer:threshold:
significantRegionsFromFloat32ImageBuffer:threshold:relativeToMaximum:
significantRegionsFromFloat32PixelBuffer:threshold:relativeToMaximum:error:
boundingBoxesFromFloat32ImageBuffer:thresholds:error:
boundingBoxesFromFloat32ImageBuffer:thresholds:relativeToMaximum:applySmoothing:originalImageSize:sigmaX:sigmaY:nStd:error:
boundingBoxesFromFloat32PixelBuffer:thresholds:relativeToMaximum:applySmoothing:originalImageSize:sigmaX:sigmaY:nStd:error:
knownAnimalIdentifiersForRevision:error:
groupImageprints:withOptions:error:
createDictionaryRepresentationOfCVPixelBuffer:
createCVPixelBufferRefFromDictionaryRepresentation:
isCVPixelBuffer:equalToCVPixelBuffer:
computeHashForCVPixelBuffer:
_determineFacesToProcessFrom:outputFacesThatNeedNoProcessing:outputFacesThatNeedAlignment:outputfacesThatNeedAttributes:
_serialNumberToPersonUniqueIdentifierMapTable
_faceIDModel
_maximumElementsPerID
initWithFaceIDModel:faceprintRequestRevision:maximumElementsPerID:personUniqueIdentifierToSerialNumberMapping:
_personPredictionsForFace:withDescriptor:limit:faceIDCanceller:error:
personPredictionsForFace:withDescriptor:limit:canceller:error:
dictionaryRepresentation
encodeInt:forKey:
decodeIntForKey:
removeObjectsInRange:
sortUsingComparator:
_concatenateFaceprintImageDescriptorBuffer:withFaceprints:forIdentityWithSerialNumber:faceprintLabels:
modelBuiltFromConfiguration:dataProvider:canceller:error:
faceIDModelMaximumElementsPerID
knownAnimalDetectorsForRevision:
_burstAnalyzerDispatchQueue
_pendingFramesSemaphore
_yuvdumpDispatchQueue
_temporalOrder
_maxNumPendingFrames
_enableAnalysis
_dummyAnalysisCount
_enableFaceCore
_enableDumpYUV
_burstLogFileHandle
_currentClusterIndexToProcess
_clusterArray
_faceAnalysisContext
_faceIDCounts
_burstId
_loggingCallback
_overrideImage
_overrideProps
_statsByImageIdentifier
_clusterByImageIdentifier
_burstLogFileName
_actionClassifier
_burstCoverSelection
_bestImageIdentifiersArray
_versionString
burstDocumentDirectory
computeActionSelectionThreshold
_reorientROIRect:imageSize:orientation:
_reorientFaceRects:imageSize:orientation:
processClusters:
_addImageInternal:properties:identifier:imageProps:completionBlock:
addImage:properties:identifier:completionBlock:
computeEmotion:
performEmotionalRejectionOnCluster:
computeCameraTravelDistance
computeBeginningVsEndAEMatrixDiffVsAverageAdjacent
computeAllImageScores
findBestImage:useActionScores:
selectCoverPhotoFromMultiple:burstSize:
imageClusterForIdentifier:
allImageClusters
isFaceDetectionForced
secondsSinceStart
faceIDCounts
setFaceIDCounts:
faceAnalysisContext
setFaceAnalysisContext:
clusterArray
setClusterArray:
maxNumPendingFrames
setMaxNumPendingFrames:
enableAnalysis
setEnableAnalysis:
dummyAnalysisCount
setDummyAnalysisCount:
enableFaceCore
setEnableFaceCore:
enableDumpYUV
setEnableDumpYUV:
burstId
setBurstId:
loggingCallback
setLoggingCallback:
overrideImage
setOverrideImage:
overrideProps
setOverrideProps:
statsByImageIdentifier
setStatsByImageIdentifier:
clusterByImageIdentifier
setClusterByImageIdentifier:
burstLogFileName
setBurstLogFileName:
actionClassifier
setActionClassifier:
burstCoverSelection
setBurstCoverSelection:
bestImageIdentifiersArray
setBestImageIdentifiersArray:
versionString
setVersionString:
lastObject
countForObject:
fileExistsAtPath:
persistentDomainForName:
stringWithCString:encoding:
defaultVersionString
computeHierarchicalClusteringOfImageDescriptors:results:context:
getDistanceForClusterNode:splitDistanceType:
computeClusteringIntoKGroups:orUsingDistanceThreshold:forHierarchicalTree:context:
computeClusteringIntoKGroups:forHierarchicalTree:context:
computeClusteringUsingDistanceThreshold:forHierarchicalTree:context:
computeNaturalClusteringForHierarchicalTree:context:
computeTimestampAdjustedDistanceForBaseDistance:andTimestampDiff:
computeTotalDistanceForDescriptorDistance:timestampDiff:useTimestampAdjustment:
_plan
_modelName
initWithModelName:network:plan:context:
free
network
plan
pathForEspressoResource:ofType:error:
pathForEspressoResourceWithFilename:error:
pathForEspressoNetworkModelFileWithName:error:
espressoDeviceIDForMetalDevice:
createSingleNetworkPlanFromResourceName:usingProcessingDevice:lowPriorityMode:inputBlobNames:outputBlobNames:espressoContext:espressoPlan:espressoNetwork:error:
createSingleNetworkPlanFromResourceName:usingProcessingDevice:lowPriorityMode:inputBlobNames:outputBlobNames:explicitNetworkLayersStorageType:espressoContext:espressoPlan:espressoNetwork:error:
tearDownEspressoContext:andPlan:
createCVPixelBufferWithPixelFormat:fromImageInEspressoBuffer:error:
renderEspressoBufferImage:intoCVPixelBuffer:error:
pixelValueSizeInBytesForBuffer:error:
enableMontrealAndReturnError:
feedForwardEspressoBufferForNetwork:fromBufferWithName:toBufferWithName:firstFrame:error:
createFromImage:withType:error:
createCGImageFromCIImage:error:
createVImageBufferFromCIImage:error:
createCGImage:fromRect:
contentsOfDirectoryAtPath:error:
removeItemAtPath:error:
setWithArray:
shouldDumpDebugIntermediates
dumpDebugIntermediatesWithImageBuffer:lumaIntermediate:alignedBBoxInLumaIntermediateCoordinates:meanShapeInLumaIntermediate:landmarkPointsInLumaIntermediate:
mFaceLandmarkAlgorithmImpl
mFaceLandmarkMouthRefinerImpl
mFaceLandmarkRightEyeRefinerImpl
mFaceLandmarkLeftEyeRefinerImpl
mCoreLandmarkModelFileHandle
_loadEspressoModelWithConfigurationOptions:error:
cascadeStepCountInOriginalModel
cascadeStepCountLoaded
_mtlContext
_computePipelines
_maxThreadExecutionWidth
_pyramid_size
_I_tex
_I_u32_alias_tex
_G0_pxbuf
_G1_pxbuf
_G0_tex
_G1_tex
_C0_pxbuf
_C1_pxbuf
_C0_tex
_C1_tex
_Adiagb_buf
_Ixy_buf
_w_pxbuf
_w_tex
_uv_pxbuf
_uv_tex
_uv_u32_alias_tex
_current_frame_index
_streamFrameCount
_uv_tex_user_ref
_isValid
_needConversionBGRA2YUVA
_useNonLocalRegularization
_width
_height
_nscales
_nwarpings
_nlreg_radius
_nlreg_padding
_nlreg_sigma_l
_nlreg_sigma_c
_nlreg_sigma_w
initWithMetalContext:width:height:nscales:error:
waitUntilCompleted
setOutputUV:error:
estimateFlowFromReference:target:error:
estimateFlowStream:error:
_initMemory:height:nscales:error:
_setupPipelines
_setupBufferAndReturnError:
_computeOpticalFlow
_createImagePyramidWithCommandBuffer:in_pixelbuf:I_idx:error:
_zeroFlowWithCommandBuffer:uv_tex:
_downscale2XWithCommandBuffer:in_u32_alias_tex:out_u32_alias_tex:
_computeFeaturesWithCommandBuffer:in_tex:out_tex:
_computeFeaturesDerivativesWithCommandBuffer:in_tex:out_tex:
_doSolverWithCommandBuffer:scale:scale_xy_inv:coeff:in_uv_tex:out_uv_tex:out_w_tex:
_doNLRegularizationWithCommandBuffer:in_uv_tex:join_tex:w_tex:out_uv_tex:
isValid
needConversionBGRA2YUVA
setNeedConversionBGRA2YUVA:
nscales
streamFrameCount
nwarpings
setNwarpings:
useNonLocalRegularization
setUseNonLocalRegularization:
nlreg_radius
setNlreg_radius:
nlreg_padding
setNlreg_padding:
nlreg_sigma_l
setNlreg_sigma_l:
nlreg_sigma_c
setNlreg_sigma_c:
nlreg_sigma_w
setNlreg_sigma_w:
endEncoding
dispatchThreadgroups:threadsPerThreadgroup:
maxTotalThreadsPerThreadgroup
threadExecutionWidth
setBytes:length:atIndex:
setTexture:atIndex:
setComputePipelineState:
computeCommandEncoder
setBuffer:offset:atIndex:
replaceRegion:mipmapLevel:withBytes:bytesPerRow:
commit
commandBuffer
newTextureViewWithPixelFormat:
newTextureWithDescriptor:
setUsage:
texture2DDescriptorWithPixelFormat:width:height:mipmapped:
newBufferWithLength:options:
newComputePipelineStateWithFunction:error:
newFunctionWithName:
mJunkDescriptorImpl
mJunkClassifierImpl
fileURLWithPath:isDirectory:
_perMeshPtr
targetsCPU
_torsoprintInputImageSizeForFaceOrientation:
_torsoprintDescriptorSize
_minimumTorsoInsideInputImageThreshold
_magnifiedBBoxScaleFactor
_mEspressoContext
_mEspressoPlan
_mEspressoNetwork
_calculateTorsoBBoxFromFaceBBox:insideImageWithSize:faceOrientationRelativeToUpright:torsoBBox:error:
_maximumIntermediateSideLength
_blurDeterminationMethod
maximumIntermediateSideLength
setMaximumIntermediateSideLength:
blurDeterminationMethod
setBlurDeterminationMethod:
printDebugInfo:facesDataRaw:faceDetectorBGRAImage:tempImage:
_faceBoundingBox
_pointCount
initWithRequestRevision:faceBoundingBox:
faceBoundingBox
setFaceBoundingBox:
pointCount
setPointCount:
_sizedPointsCache
_points
_precisionEstimatesPerPoint
_occlusionFlagsPerPoint
pointAtIndex:
normalizedPoints
pointsInImageOfSize:
initWithRequestRevision:faceBoundingBox:points:pointCount:precisionEstimatesPerPoint:occlusionFlagsPerPoint:
points
precisionEstimatesPerPoint
occlusionFlagsPerPoint
initWithRequestRevision:faceBoundingBox:points:pointCount:
setPoints:
_pointsData
_alignedBBox
_userFacingBBox
_createPointArray:count:
isUserFacingBBoxEquivalentToAlignedBBox
initWithRequestRevision:pointsData:pointCount:userFacingBBox:alignedBBox:landmarkScore:
pointsData
setPointsData:
alignedBBox
setAlignedBBox:
userFacingBBox
setUserFacingBBox:
_allPoints
_faceContour
_leftEye
_rightEye
_leftEyebrow
_rightEyebrow
_nose
_noseCrest
_medianLine
_outerLips
_innerLips
_leftPupil
_rightPupil
initWithRequestRevision:pointsData:pointCount:constellation:precisionEstimatesPerPoint:occlusionFlagsPerPoint:userFacingBBox:alignedBBox:landmarkScore:
allPoints
faceContour
_createFaceLandmarks2DRegionFromPointIndexes:andPointCount:
leftEye
rightEye
leftEyebrow
rightEyebrow
nose
noseCrest
medianLine
outerLips
innerLips
leftPupil
rightPupil
setPrecisionEstimatesPerPoint:
setOcclusionFlagsPerPoint:
_createNSArrayFrom:withPointIndices:andPointCount:
validateNonZeroImageWidth:height:componentNameProvidingBlock:error:
validateArray:named:hasElementsOfClass:requiredMinimumCount:allowedMaximumCount:error:
validateClassArray:named:hasElementsAncestoredFromClass:requiredMinimumCount:allowedMaximumCount:error:
_validateFaceObservations:withMinimumCount:forOptionalRequest:error:
validateRequiredFaceObservations:error:
validateRequiredFaceObservations:forRequest:error:
validateOptionalFaceObservations:error:
validateOptionalFaceObservations:forRequest:error:
validateRequiredClusterIDs:error:
requiredObjectOfClass:forKey:inOptions:error:
getBOOLValue:forKey:inOptions:error:
getBOOLValue:forKey:inOptions:withDefaultValue:error:
getNSUIntegerValue:forKey:inOptions:error:
getNSUIntegerValue:forKey:inOptions:withDefaultValue:error:
getArray:forKey:inOptions:withElementsOfClass:requiredMinimumCount:allowedMaximumCount:error:
requiredArrayForKey:inOptions:withElementsOfClass:error:
requiredInputFacesArrayInOptions:error:
requiredFaceObservationInOptions:withOptionName:error:
getRequiredRequestRevision:forKey:inOptions:error:
getRequiredRequestRevision:inOptions:error:
getRequiredRequestRevision:fromSupportedRevisionsForRequestClass:forKey:inOptions:error:
getRequiredRequestRevision:fromSupportedRevisionsForRequestClass:inOptions:error:
getOptionalRequestRevision:forKey:inOptions:error:
getOptionalRequestRevision:inOptions:error:
getOptionalRequestRevision:fromSupportedRevisionsForRequestClass:forKey:inOptions:error:
getOptionalRequestRevision:fromSupportedRevisionsForRequestClass:inOptions:error:
landmarkDetectorDNNVersion
_identifiers
initWithIdentifiers:
identifierCount
allIdentifiers
containsIdentifier:
blacklistFromUTF8StringArray:
_platform
_profile
_key
_faceprintInputPath
setKey:
platform
setPlatform:
profile
setProfile:
faceprintInputPath
setFaceprintInputPath:
initWithData:forKey:
_imageprintDescriptor
_imageprintType
serializeAsVNImageprintStateAndReturnError:
_quality
_nextLeafDescriptorDistance
_previousLeafDescriptorDistance
_nextLeafTotalDistance
_previousLeafTotalDistance
_descriptorId
_colorGaborDescriptor
_sceneClassifierDescriptor
_imageRegistrationDescriptor
_previousLeafId
_nextLeafId
_nextLeafTimestampDistance
_previousLeafTimestampDistance
_rawColorGaborDescriptor
descriptorId
quality
colorGaborDescriptor
sceneClassifierDescriptor
imageRegistrationDescriptor
previousLeafId
setPreviousLeafId:
nextLeafId
setNextLeafId:
nextLeafDescriptorDistance
setNextLeafDescriptorDistance:
previousLeafDescriptorDistance
setPreviousLeafDescriptorDistance:
nextLeafTimestampDistance
setNextLeafTimestampDistance:
previousLeafTimestampDistance
setPreviousLeafTimestampDistance:
nextLeafTotalDistance
setNextLeafTotalDistance:
previousLeafTotalDistance
setPreviousLeafTotalDistance:
rawColorGaborDescriptor
shortValue
longValue
dataUsingEncoding:allowLossyConversion:
initWithData:encoding:
emptyVNTorsoprintForRevision:
_imageInputKey
_buffer
_scenePrint
_scenePrintMLMultiArrayDataType
_featureProvider
featureValueForName:
featureNames
initWithBuffer:forKey:originalFeatureProvider:
initWithScenePrint:dataType:forKey:originalFeatureProvider:
featureValueFromScenePrint:dataType:
featureValueWithPixelBuffer:
featureValueWithMultiArray:
initWithDataPointer:shape:dataType:strides:deallocator:error:
_uuidStringForCacheIdentifier
_modelType
_inputImageFormat
_model
_inputImageKey
_predictedFeatureKey
_predictedProbabilitiesKey
_boundingBoxOutputDescription
_inputImageWidth
_inputImageHeight
_scenePrintRevision
_inputScenePrintKey
_inputScenePrintMLMultiArrayDataType
initWithMLModel:error:
setInputImageFeatureName:
inputImageFeatureName
setupInputImageFromModelDescription:
_updateModelWithFlexibleImageConstraintUsingWidth:height:
predictWithCVPixelBuffer:options:error:
predictWithScenePrint:options:error:
cachingIdentifier
featureProvider
setFeatureProvider:
model
setModel:
modelType
setModelType:
inputImageKey
setInputImageKey:
predictedFeatureKey
setPredictedFeatureKey:
predictedProbabilitiesKey
boundingBoxOutputDescription
inputImageWidth
inputImageHeight
inputImageFormat
scenePrintRevision
inputScenePrintKey
setInputScenePrintKey:
inputScenePrintMLMultiArrayDataType
predictionFromFeatures:options:error:
pixelsHigh
pixelsWide
allowedImageSizeClosestToPixelsWide:pixelsHigh:preferDownScaling:preferInputAspectRatio:
sizeConstraint
imageConstraint
inputDescriptionsByName
modelDescription
pixelFormatType
dataType
multiArrayConstraint
postVisionFeaturePrintModel
visionFeaturePrintInfo
predictedProbabilitiesName
outputDescriptionsByName
objectBoundingBoxOutputDescription
predictedFeatureName
modelForMLModel:error:
initWithOptions:model:error:
imageBufferValue
labelNames
shape
multiArrayValue
confidenceFeatureName
coordinatesFeatureName
dictionaryValue
_frameworkOperationPointsIdentifier
_nonframeworkDataURL
_labelToOperationPointsDataIndexMap
_operationPointsDataArray
_cachedHashValue
initWithLabelToOperationPointsDataIndexMap:operationPointsDataArray:
_operationPointsDataForClassificationIdentifier:error:
_allClassificationIdentifiers
_propertyListRepresentation
loadFromPropertyList:error:
loadFromIdentifier:error:
URLForIdentifier:error:
propertyListWithStream:options:format:error:
inputStreamWithURL:
URLForResource:withExtension:
computeImageDescriptorsWithImage:regionOfInterest:usingDescriptorProcessor:tileCount:augmentationMode:scalingImage:resultantDescriptorBuffer:debugIntermediatesDumpPath:outputDebugDictionary:options:metalContext:canceller:error:
computeLabelsAndConfidence:usingDescriptorBuffer:populateLabelsAndConfidence:options:metalContext:error:
classifyImageWithDescriptors:usingImageClassifier:andMinConfidenceForClassification:outputDebugDictionary:options:metalContext:error:
classifyImageHierarchicallyWithDescriptors:usingImageClassifier:hierarchicalClassifier:minimumClassificationConfidence:minimumClassificationConfidenceRatio:maximumLeafLabels:maximumHierarchicalLabels:outputDebugDictionary:options:metalContext:error:
_timeStamp
timeStamp
setTimeStamp:
_outputBufferWidth
_outputBufferHeight
_outputBufferData
_numberOfFaceSegments
_faceSegmentLabelToProbabilityMap
_probabilityNormSums
initWithRequestRevision:outputBufferWidth:outputBufferHeight:outputBufferData:numberOfFaceSegments:faceSegmentBBox:faceSegmentLabelToProbabilityMap:
createMaskImageOfFaceSegments:error:
createProbabilityImageOfFaceSegment:error:
createProbabilityImageOfFaceSegment:region:normalize:error:
_normalizeRegion:
_makeFaceSegmentProbabilityDataImageBuffer:rect:
_createFaceSegmentProabilityDataPixelBufferWithSize:error:
_calculateProbabilityNormalSumsForRect:
outputBufferWidth
outputBufferHeight
outputBufferData
numberOfFaceSegments
faceSegmentLabelToProbabilityMap
faceSegmentToSegmentMaskGrayLevelDictionary
faceSegmentIndexToFlagMap
_imagePixelBuffer
_completionBlock
_burstImages
_imageProps
initWithImageData:dict:identifier:imageProps:completionBlock:
addItemsFromCluster:
computeMergeCost:::
imagePixelBuffer
setImagePixelBuffer:
completionBlock
setCompletionBlock:
burstImages
setBurstImages:
imageProps
setImageProps:
m_impl
resourcePath
setResourcePath:
advise:
baseAddress
length
initWithMappedModel:
load:
unload:
purgeAll
sharedInstance
_internalNonSerializedDescriptorId
initWithImageData:context:error:
initWithImageData:andQualityCriteria:context:error:
initWithImageData:andCustomQualityScore:context:error:
computeConvnetDescriptorForImageData:context:error:
computeDescriptorForImageData:context:error:
computeRegistrationFeaturesForImageData:context:error:
computeQualityForImageData:andQualityCriteria:context:error:
distanceFromDescriptor:
computeFinalDescriptorBasedDistanceForColorDistance:andSceneClassifierDistance:
initWithRawColorGaborDescriptor:
numberWithShort:
numberWithLong:
_faceBoundingBoxExpansionRatio
setFaceBoundingBoxExpansionRatio:
faceBoundingBoxExpansionRatio
expansionRatioWithinValidRange:
defaultFaceBoundingBoxExpansionRatio
beginRangeFaceBoundingBoxExpansionRatio
endRangeFaceBoundingBoxExpansionRatio
_determineFacesToProcessFrom:outputFacesThatNeedNoProcessing:outputFacesThatNeedAlignment:outputfacesThatNeedSegments:
_determineFacesToProcessFrom:outputFacesThatNeedNoProcessing:outputFacesThatNeedFaceQuality:
_burstSet
addBurstFrameWithIdentifier:fromImageBuffer:withProperties:error:
allClusters
copyItemAtPath:toPath:error:
moveItemAtPath:toPath:error:
m_FaceDescriptorImpl
m_FaceFrontalizerImpl
m_DescriptorAugmenter
m_FaceFrontalizerWorkingBuffer
m_FaceFrontalizerImageBuffer
_useLowPriorityMode
_length
isFaceprinterCompatibleWithFaceprinterCreatedWithOptions:
printDebugInfoFor:imageBuffer:originalImageLumaCrop:faceBBoxInLumaCropCoordinates:magnifiedBBoxInLumaCropCoordinates:
useLowPriorityMode
_returnAllResults
returnAllResults
setReturnAllResults:
_operationPointsCache
initWithOperationPointsCache:requestRevision:
sceneLabelOperationPointsForRequestRevision:error:
_defaultSceneClassificationHierarchicalModel
_imageAnalyzer
_imageAnalyzerJunkCustomClassifiers
_imageAnalyzerPCA256
_pipelineImageCorrectionNeed1CustomClassifier_DO_NOT_ACCESS_DIRECTLY
_cachedAllSceneClassificationsFromLastAnalysis
_getImageCropAndScaleOption:networkRequiredInputImageSize:forOptions:
_createScaledImagePixelBufferFromImageBuffer:forNetworkInputImageSize:imageCropAndScaleOption:options:error:
_tileRect:horizontally:vertically:windowAspectRatio:overlapPercentage:usingBlock:
_analysisTypeForScene:junk:sceneprint:includingLabelsAndConfidences:compressedSceneprint:aesthetics:saliencyHeatMap:pipelineImageCorrectionNeed1:
_populateLeafSceneObservations:hierarchySceneObservations:forLastAnalysisWithOptions:error:
_sceneObservationsForLastAnalysisWithOptions:error:
_loadCustomClassifierWithModelName:labelsFileName:classifierName:inputBlobName:outputBlobName:espressoEngine:espressoPlanFlags:espressoStorageType:espressoDeviceID:error:
_loadJunkCustomClassifiersForRequestRevision:options:error:
_junkCustomClassifiersForRequestRevision:options:error:
_junkObservationsForLastAnalysisWithOptions:error:
_loadPipelineImageCorrectionNeed1CustomClassifierForRequestRevision:options:error:
_pipelineImageCorrectionNeed1CustomClassifierForRequestRevision:options:error:
_pipelineImageCorrectionNeed1ForLastAnalysisWithOptions:error:
_sceneprintCompressorForCompression:error:
_sceneprintObservationsForLastAnalysisOfSceneprint:includingLabelsAndConfidencesInSceneprint:compressedSceneprint:options:error:
_aestheticsObservationsForLastAnalysisWithOptions:error:
_saliencyObservationsForLastAnalysisWithRegionOfInterest:originalImageSize:options:warningRecorder:error:
_performAnalysis:on32BGRAImageInPixelBufferProvidedByBlock:error:
_lastAnalysisSceneClassifications
_observationsForTopN:lastAnalysisSceneLabelsWithMinimumConfidence:excludeObfuscatedLabels:labelBlackList:sceneRequestRevisionNumber:operationPointsProvider:
_observationsForTopN:sceneClassificationsInMap:withMinimumConfidence:labelBlackList:sceneRequestRevisionNumber:operationPointsProvider:
_observationsForScene:junk:sceneprint:includingLabelsAndConfidences:compressedSceneprint:aesthetics:saliencyHeatMap:pipelineImageCorrectionNeed1:of32BGRAImageInPixelBufferProvidedByBlock:withOptions:originalImageSize:regionOfInterest:warningRecorder:error:
_getLeafSceneObservations:hierarchySceneObservations:of32BGRAImageInPixelBufferProvidedByBlock:withOptions:error:
_createScaledImagePixelBufferFromCropRect:ofImageBuffer:forNetworkInputImageSize:imageCropAndScaleOption:options:error:
supportedImageSizeSetForProcessingOptions:
allKnownSceneClassificationsWithOptions:error:
allJunkClassificationObservationsForOptions:error:
arrayByAddingObjectsFromArray:
indexSetWithIndexesInRange:
createHierarchicalModelForMultiDetectorModel:requestRevision:error:
initWithDetectorModel:
_detectorModel
_sceneLabelOperationPointsForRequestRevision
indexOfObject:inSortedRange:options:usingComparator:
modelForRequestClass:revision:detectionLevel:
blacklistForModel:
anyObject
isSubsetOfSet:
medianHeightTop
medianHeightBottom
loopBigBox
loopBigBoxPrev
filterWalkUpDownCount
allocationSize
mTop
mBottom
bTop
bBottom
posUL
posLL
posUR
posLR
floatVectorSumProd
pulseVectorHeightCharBox
pulseVectorHeightCharBoxAdaptive
charBoxFlags
charboxROIFullVectorRowStart
charboxROIFullVectorHeight2
setFlag:atIndex:
clearFlag:atIndex:
checkFlag:atIndex:
copyFlagValue:toTarget:atIndex:
resetBoxBounds
makeAllocationsForWidth:
releaseAllocations
setFloatVectorSumProd:
setPulseVectorHeightCharBox:
setPulseVectorHeightCharBoxAdaptive:
setCharBoxFlags:
setCharboxROIFullVectorRowStart:
setCharboxROIFullVectorHeight2:
setAllocationSize:
setMTop:
setMBottom:
setBTop:
setBBottom:
setPosUL:
setPosLL:
setPosUR:
setPosLR:
setMedianHeightTop:
setMedianHeightBottom:
setLoopBigBox:
setLoopBigBoxPrev:
setFilterWalkUpDownCount:
_getFilter_callCount
_computeZCVectorHighProbability
_profileNormal
_debugMatlab
_debugOut
_midRow
_minHeight
_maxHeight
_startMaxFind
_stopMaxFind
_mmHeightCard
_mmWidthCard
_pixelHeightCard
_pixelWidthCard
_minBoxWidth
_maxBoxWidth
_startNormal
_stopNormal
_startSensitized
_stopSensitized
_charBoxContext
_debugFilename
initializeForImage:
_allocateVImageWithWidth:height:rowBytes:imageOut:
_freeVImage:
_generateVotingImage:votingImage:useLowLightEnhancement:
examinePulseWindow:prodBoostNormalized:pwContext:minHeight:maxHeight:thresholdSet:
generatePulses:minHeight:maxHeight:thresholdSet:prodBoostNormalized:pulseVectorFlag:
_computeColumnSumsOverRange:sampleImageAddress:rowSumOut:rowDerivOut:
_allocateSumDerivVectors:size:
_freeSumDerivVectors:
_computeProdBoostNormalizedResult:size:binOverride:
_getFilterResultOutBothSumDeriv:floatVectorResult:bufferHeight:minHeight:maxHeight:config:bytesPerRow:thresholdSet:sampleImageAddressRef:
_getFilterResultOut:defaultRows:bufferHeight:minHeight:maxHeight:startRange:stopRange:startMaxFind:stopMaxFind:bytesPerRow:thresholdSet:sampleImageAddressRef:sampleImageFloatAddressRef:pulseVectorFlag:
_generatePulseAggregate:pulseVectorMain:pulseVectorResult:metricSelection:bufferHeight:bufferWidth:
_generatePulseAggregateSmallStubs:pulseVectorResult:bufferHeight:bufferWidth:
generateDominantPulse:rowLocationsRef:debugOut:bufferHeight:bufferWidth:
generateHistogramBounds:rgbVector2Ref:numPixels1:numPixels2:minMaxRGB:lowHighRGB:
_generateBinarizationForImage:textOut:firstOrSecondPassIndicator:minMaxRGB:
createLumaImage:lumaImage:numCropRows:rowStartLocation:
createLumaImageAlternative:lumaImageAlternative:numCropRows:rowStartLocation:
getVotingHistogram:colorProfileContext:startCC:rowStartLocation:
getLumaHistogram:startCC:colorProfileContext:
computeNumCropCols:width:start:
computeMainStub:numCropRows:numCropColsOut:maxY:start:
determineColorProfileType:
allocateColorProfileContext:width:height:rowBytes:
freeColorProfileContext:
_generateAndApplyColorProfileForImage:votingImage:textOut:minMaxRGB:lowHighRGB:numCropRows:rowStartLocation:rowStopLocation:sumTextOutFirstPass:useLowLightEnhancement:
_generateAdaptiveBinarization:adaptImage:useLowLightEnhancement:
_generateSmoothedImage:uImage:
_generateBoxes:pulseVector:uImage:countBigBoxOut:bigBoxes:bigBoxesA:useLowLightEnhancement:
_generateCC:ccBigBoxes:textOut:countBigBox:bufferHeight:
groupEndpoints:finalCharBoxCoordCount:
computeIndependentTopAndBottomComponents:finalCharBoxCoordCount:
computeDependentTopAndBottomComponents:finalCharBoxCoordCount:
_generateCRCharBoxInformation_TrackBox:finalCharBoxCoordCount:
computePulseVectorSum:start:stop:imageHeight:imageWidth:bottomHeight:topHeight:
tightenBox:startTop:startBottom:startPosition:stopPosition:imageHeight:halfWalk:
_generateCRCharBoxInformation_spaceBoxRemovalTightenBox:singleVotingImageAddressRef:adaptOut:textOut:zcStartLeft:zcStopRight:finalCoordinatesForStub:finalCoordinatesForStubRevised:finalCharBoxCoordCount:useLowLightEnhancement:
_generateCRCharBoxInformation_spaceBoxRemovalTruthFilter:magicThresh:zcStartLeft:zcStopRight:isSpaceBoxAccepted:histCompliancePercent:useLowLightEnhancement:
_generateCRCharBoxInformation_spaceBoxRemovalHistogram:zcStartLeft:zcStopRight:rowStartLocation2:lowHighRGB:histCompliancePercent:varSpaceBox:
_generateCRCharBoxInformation_spaceBoxRemovalMagicThresh:magicMinHeight:magicMaxHeight:rowStartLocation2:magicThresh:magicThreshPrev:useLowLightEnhancement:
_extractCharBoxCuts:heightConstraint:medianHeightTopVector:medianHeightBottomVector:isAdaptive:
_generateFilteredPulseVector:target:height:
_generateZeroCrossingVector:zcVectorFlag:width:
calculateSumProd:prodROIRef:prodROIRotateRef:rowStartLocation2:height:
calculateSumProdAlternative:prodROIRef:prodROIRotateRef:rowStartLocation2:height:
extractCharBoxHeightInfo:ccCharBoxesFiltered:ccYTopLocationsForSort:ccYBottomLocationsForSort:aggregateGreenBoxesForStubCount:imageWidth:
charBoxesFromTextBoxes:bigBoxes:ccYTopLocationsForSort:ccYBottomLocationsForSort:
extractBoxesForStub:bigBoxesAdapt:countBigBox:rowStartLocation2:rowStopLocation2:heightConstraint:imageWidth:height:ccCharBoxesAggr:ccCharBoxesFiltered:useLowLightEnhancement:
_generateCRCharBoxInformation_zcFinalVectorOriginate:textOut:adaptOut:bigBoxes:bigBoxesAdapt:ccCharBoxesAggr:ccCharBoxesFiltered:height:rowStartLocation2:rowStopLocation2:singleVotingImageAddressRef:countBigBox:filterWalkUpDownCount:useLowLightEnhancement:
_generateCRCharBoxInformation_zcFinalVectorHighProbability:zcFinalRange:
fillInOneVector:checkFlag:checkHeight:
_generateCRCharBoxInformation_zcFinalVectorFillIn:
_allocateCRCharBoxContext:
_freeCRCharBoxContext
_generateCRCharBoxInformation_extendTextBoxes:countBigBox:rowStartLocation2:finalCharBoxCoordCount:finalCoordinatesForStubRevised:width:height:bigBoxIndicator:
_generateCRCharBoxInformation:inputImage:singleVotingImageAddressRef:bigBoxes:bigBoxesAdapt:textOut:adaptOut:lowHighRGB:countBigBox:useLowLightEnhancement:
_generatePulseVectorOutputs:votingImage:rowLocationsRef:
textBoxesForBuffer:error:
textBoxesForImage:error:
charBoxContext
setCharBoxContext:
computeZCVectorHighProbability
setComputeZCVectorHighProbability:
midRow
setMidRow:
minHeight
setMinHeight:
maxHeight
setMaxHeight:
startMaxFind
setStartMaxFind:
stopMaxFind
setStopMaxFind:
mmHeightCard
setMmHeightCard:
mmWidthCard
setMmWidthCard:
pixelHeightCard
setPixelHeightCard:
pixelWidthCard
setPixelWidthCard:
minBoxWidth
setMinBoxWidth:
maxBoxWidth
setMaxBoxWidth:
startNormal
setStartNormal:
stopNormal
setStopNormal:
startSensitized
setStartSensitized:
stopSensitized
setStopSensitized:
setIi:
profileNormal
setProfileNormal:
debugMatlab
setDebugMatlab:
debugOut
setDebugOut:
debugFilename
setDebugFilename:
removeObjectsInArray:
observationsForKey:
setObservations:forKey:
observationsForRequest:testedKeyHandler:
_espressoContext
_espressoPlan
_espressoNetwork
_networkRequiredInputImageWidth
_networkRequiredInputImageHeight
espressoModelNetworkLayersStorageTypeForConfigurationOptions:
getWidth:height:ofEspressoModelNetworkBlobNamed:error:
espressoContext
espressoPlan
espressoNetwork
networkRequiredInputImageWidth
networkRequiredInputImageHeight
_usesLanguageCorrection
_minimumTextHeight
_recognitionLanguages
_customWords
_recognitionLevel
recognitionLanguages
setRecognitionLanguages:
customWords
setCustomWords:
recognitionLevel
setRecognitionLevel:
usesLanguageCorrection
setUsesLanguageCorrection:
minimumTextHeight
setMinimumTextHeight:
indeterminate
progressHandler
setProgressHandler:
_crOutput
initWithRequestRevision:CRImageReaderOutput:
string
boundingBoxForRange:error:
crOutput
pointValue
cornersForCharacterRange:error:
candidates
resultsForPixelBuffer:options:error:
resultsForPixelBuffer:roi:options:error:withProgressHandler:
smallestImageSizeForTextWithRelativeHeight:originalImageSize:
setOutputObjectTypes:
isEqualToArray:
supportedRecognitionLanguagesForTextRecognitionLevel:revision:error:
supportedLanguagesForOptions:revision:error:
computeNormalizedCosineDistanceOfFaceprint:toFaceprint:
doesAreaOverlapSignificantlyBetweenRect:andOtherRect:
doesAreaOverlapBetweenRect:andOtherRect:withOverlapRatioGreaterThan:
_area
_hasLabel
_mergesCount
_scale
_label
_box
_defaultBox
initWithBox:defaultBox:confidence:scale:rotationAngle:yawAngle:
initWithBox:defaultBox:confidence:scale:rotationAngle:yawAngle:mergesCount:
initWithBox:defaultBox:confidence:scale:rotationAngle:yawAngle:hasLabel:label:
initWithBox:defaultBox:confidence:scale:rotationAngle:yawAngle:mergesCount:hasLabel:label:
boxCenter
distanceToDefaultBox
smartDistance
overlap:
intersectionOverArea:
isOverlappingSmallFace:withOverlapThreshold:withSizeRatio:
isOverlappingLowMergeDet:withOverlapThreshold:withMergeCountDelta:
setBox:
defaultBox
setDefaultBox:
mergesCount
setMergesCount:
scale
setScale:
hasLabel
setHasLabel:
_minimumAspectRatio
_maximumAspectRatio
_quadratureTolerance
_minimumSize
_minimumConfidence
_requiredVersion
_maximumObservations
requiredVersion
setRequiredVersion:
minimumAspectRatio
setMinimumAspectRatio:
maximumAspectRatio
setMaximumAspectRatio:
quadratureTolerance
setQuadratureTolerance:
minimumSize
setMinimumSize:
minimumConfidence
setMinimumConfidence:
maximumObservations
setMaximumObservations:
lastDataChangeSequenceNumberForPersonsModel:
numberOfPersonsInPersonsModel:
personsModel:uniqueIdentifierOfPersonAtIndex:
personsModel:indexOfPersonWithUniqueIdentifier:
personsModel:numberOfFaceObservationsForPersonAtIndex:
personsModel:faceObservationAtIndex:forPersonAtIndex:
initWithConfiguration:faceModel:
_useCenterTileOnly
useCenterTileOnly
setUseCenterTileOnly:
_detectorTypeForRequestRevision:options:error:
_applicableDetectorForRequestRevision:applicableDetectorOptions:error:
_applicableDetectorAndReturnError:
inputBlobNames
outputBlobNames
_requestConstellationToDetectorConstellationMap
_landmarkDetector
getConstellation:cvmlConstellation:fromOptions:error:
getLandmarkPoints:forConstellation:error:
getLandmarkErrorEstimates:forConstellation:error:
getLandmarkOcclusionFlags:forConstellation:error:
translateAndNormalizeLandmarkPointsWrtLLCofAlignedFaceBBox:imageBuffer:outputFace:error:
releaseResources
_requestLock
_requestsInFlight
_requestsPending
_sequencedRequestObservations
_trackerKeys
_validateAndPrepareRequests:error:
_dependencyAnalyzedRequestsForRequests:withPerformingContext:error:
_orderedRequestsForRequests:
_performOrderedRequests:inContext:error:
performRequests:inContext:onBehalfOfRequest:error:
performRequests:inContext:error:
performDependentRequests:inContext:onBehalfOfRequest:error:
recordSequencedObservationsForRequest:
previousSequencedObservationsForRequest:
removeObjectIdenticalTo:
_requestRevisionKey
_originalRequestResultsIndex
initWithProcessingOptionRequestRevisionKey:originalRequestResultsIndex:
requestRevisionKey
originalRequestResultsIndex
detectorClass
faceDetectorChunkAspectRatio
detectedObjectClassToRequestClass
detectedObjectRequestClassToRequestInfo
detectedAnimalObjectClassToAnimalName
knownAnimalIdentifiers
_printDebugInfo:detectedObjectsRaw:faceDetectorBGRAImage:tempImage:
_mMultiHeadedANFDDetector
_alignFace:imageBuffer:warningRecorder:error:
_hierarchicalModel_DO_NOT_ACCESS_DIRECTLY
_additionalRelationships
_sceneClassificationRequestRevision
_sceneClassificationRequestDetectionLevel
initWithSceneClassificationRequestRevision:detectionLevel:
hierarchicalModelAndReturnError:
newHierarchicalModelAndReturnError:
_addRelationships:error:
relationships
customHierarchyWithAdditionalParent:children:error:
customHierarchyWithAdditionalRelationships:error:
requestDetectionLevel
initWithArray:copyItems:
customHierarchyForRequest:error:
_inputImageprints
_clusteringDistanceThreshold
initWithImageprintObservations:clusteringDistanceThreshold:
initWithImageprintObservations:clusteringDistanceThreshold:completionHandler:
inputImageprints
setInputImageprints:
clusteringDistanceThreshold
setClusteringDistanceThreshold:
_lockStaticObjectsAccessLock
_unlockStaticObjectsAccessLock
defaultDevice
defaultCPUDevice
defaultMetalDevice
deviceForMetalDevice:
defaultANEDevice
directANEDevice
initWithFaceObservations:
initWithFaceObservations:completionHandler:
_determineFacesToProcessFrom:outputFacesThatNeedNoProcessing:outputFacesThatNeedProcessing:
_cachePath
initWithType:cachePath:state:threshold:requestRevision:
initWithType:cachePath:state:threshold:torsoThreshold:requestRevision:
initWithType:cachePath:state:threshold:
initWithType:cachePath:state:threshold:torsoThreshold:
cachePath
setCachePath:
state
setState:
torsoThreshold
setTorsoThreshold:
setRequestRevision:
ageClassifierBabyThreshold
setAgeClassifierBabyThreshold:
ageClassifierKidThreshold
setAgeClassifierKidThreshold:
_defaultAgeClassifierKidThreshold:forFaceprintRequestRevision:error:
_defaultAgeClassifierBabyThreshold:forFaceprintRequestRevision:error:
clustererQueryWithOptions:error:
clustererBuilderWithOptions:error:
_modelCachingIdentifier
updateWithPropertiesOfModel:
modelCachingIdentifier
setModelCachingIdentifier:
initWithModel:
initWithModel:completionHandler:
_useTimestampAdjustedDistances
_performClustersPostprocessing
_performSceneClassification
_debugMode
_timerMode
_clusterSplitDistanceType
_roiAreaThreshold
_inliersRatioThreshold
_numberOfKeypointsToConsider
_naturalClusteringDistanceThreshold
_qualityCriteriaList
debugMode
setDebugMode:
timerMode
setTimerMode:
clusterSplitDistanceType
setClusterSplitDistanceType:
qualityCriteriaList
setQualityCriteriaList:
useTimestampAdjustedDistances
setUseTimestampAdjustedDistances:
performClustersPostprocessing
setPerformClustersPostprocessing:
performSceneClassification
setPerformSceneClassification:
roiAreaThreshold
setRoiAreaThreshold:
inliersRatioThreshold
setInliersRatioThreshold:
numberOfKeypointsToConsider
setNumberOfKeypointsToConsider:
naturalClusteringDistanceThreshold
setNaturalClusteringDistanceThreshold:
_logitsPosOutputs
_logitsNegOutputs
_offsetsOutputs
_objectnessOutputs
_rollOutputs
_yawOutputs
_currentNetworkWidth
_currentNetworkHeight
_releaseEspressoContext
_releaseEspressoPlan
isAnchorSquare
_defaultBoxSizes
_preferredSmallSide
initWithModelPath:espressoEngineID:espressoDeviceID:espressoStorageType:threshold:
initWithEspressoNetwork:espressoPlan:threshold:
initializeEspressoResourcesWithModelPath:espressoEngineID:espressoDeviceID:espressoStorageType:
initializeBuffers
setInputShape:height:
runNetwork:inputIsBGR:
processVImage:inputIsBGR:
resizeAndProcessVImage:inputIsBGR:
preferredSmallSide
nonSquareYawDefault
nonSquareRollDefault
importantClasses
poseSquare
mumberPosClasses
mumberBinsNegativeMaxout
numberBinsYaw
numberBinsRoll
ratios
cellStartsX
cellStartsY
strides
objectnessFilterThresholds
numberMaxoutLayers
inputBGR
inputScale
inputBiasRGB
hasObjectnessOutputs
defaultBoxesSides
processingDeviceNetworkWithModelPath:threshold:preferredDeviceID:engineID:storageType:
processingDeviceDetectorWithEspressoNetwork:espressoPlan:threshold:
computeBrightnessScore:onImage:error:
initWithRectangleObservation:
initWithRectangleObservation:completionHandler:
_forceFaceprintCreation
setForceFaceprintCreation:
forceFaceprintCreation
_determineFacesToProcessFrom:outputFacesThatNeedNoProcessing:outputFacesThatNeedAlignment:outputFacesThatNeedFaceprints:
mFaceBoxPoseAlignerImpl
_modelFilesWereMemmapped
mFaceBoxAlignerModelFileHandle
dumpDebugIntermediatesWithImageBuffer:lumaIntermediate:rawBBoxInLumaIntermediateCoordinates:alignedBBoxInLumaIntermediateCoordinates:meanShapeInLumaIntermediateCoordinates:rotationAngle:
processWithOptions:warningRecorder:error:
mTrackerImpl
_trackedFrameCVPixelBufferFormat
_trackedFrameNumber
_level
_lastTrackedBBox
_createTrackerWithLevel:options:error:
_postProcessTrackingResults:trackerResults:error:
_visionBBoxToTrackerBBox:trackedObjects:imageSize:results:error:
_updateTrackerWithModifiedBBoxForImageBuffer:error:
trackedFrameCVPixelBufferFormat
setTrackedFrameCVPixelBufferFormat:
trackedFrameNumber
setTrackedFrameNumber:
lastTrackedBBox
setLastTrackedBBox:
level
VNTrackerOptionToTrackerType:
_classifyAesthetics:generateSaliencyHeatMap:for32BGRAImageInPixelBuffer:withOptions:modelInputImageSize:originalImageSize:regionOfInterest:warningRecorder:error:
_cachedDependencyProcessingOrdinality
compoundResults
recordWarningsInOriginalRequests
compoundRequestProcessingDeviceFromOriginalRequestsProcessingDevice:
_requestToObservationClassMap
initWithSubrequestsAndUniqueObservationClasses:
initWithSubrequests:uniqueObservationClasses:
initWithOriginalRequests:requestToObservationClassMap:
_requestsClassMapping
initWithSubrequests:
originalRequestsOfClass:
_enableFiltering
_filterLumaWeight
_filterChromaWeight
_filterOcclusionWeight
_levelCount
_warpCount
_filterSize
_filterSamplingDensity
initWithTargetedImageSpecifier:completionHandler:
_createLKTPixelBufferFromPixelRegionOfInterest:inImageBuffer:error:
_createLKTVectorResultPixelBufferForImageWidth:height:error:
_initializedLKTMetalContextAndReturnError:
_calculateLKTVectorResult:fromPixelBuffer:toPixelBuffer:ofWidth:height:error:
_validateParameters:
levelCount
setLevelCount:
warpCount
setWarpCount:
enableFiltering
setEnableFiltering:
filterSize
setFilterSize:
filterSamplingDensity
setFilterSamplingDensity:
filterLumaWeight
setFilterLumaWeight:
filterChromaWeight
setFilterChromaWeight:
filterOcclusionWeight
setFilterOcclusionWeight:
_qosClass
_requestPerformer_DO_NOT_DIRECTLY_ACCESS
_imageBuffer_DO_NOT_DIRECTLY_ACCESS
_requestToObservationsCacheKeyMap
_requestForensics
initWithRequestPerformer:imageBuffer:forensics:observationsCache:
initWithRequestPerformer:imageBuffer:forensics:observationsCache:qosClass:
_observationsCacheKeyForRequest:
requestPerformerAndReturnError:
performDependentRequests:onBehalfOfRequest:error:
requestForensics
qosClass
cacheObservationsForRequest:
cachedObservationsForRequest:
_observationsForImageIn32BGRAPixelBuffer:withOptions:originalImageSize:error:
_regionOfInterestConfigurations
configurationForRequest:
_detectorTypeForRevision:error:
_detectorForRevision:getAppliedDetectorOptions:error:
_objectnessObservationsForRevision:performedInContext:error:
mSignature_
initWithImageBuffer:andOptions:error:
signature
computeTransform:forRegisteringImageSignature:withSignature:andOptions:minimumOverlap:error:
_personUniqueIdentifiers
_personUniqueIdentifierToSerialNumberMapping
_serialNumberToFaceObservationsMapping
_availablePersonSerialNumbers
_delegate
_requestNewIdentitySerialNumberAndReturnError:
_uniqueFaceObservationsWithRegistrationState:forFaceObservations:withExpectedFaceprintRequestRevision:ofPersonWithUniqueIdentifier:error:
_addUniqueFaceObservations:toPersonWithUniqueIdentifier:error:
_removeAllFaceObservationsFromIdentityWithSerialNumber:
_removePersonWithUniqueIdentifier:
_removeExistingFaceObservations:fromIdentityWithSerialNumber:
_removeExistingFaceObservations:fromPersonWithUniqueIdentifier:
_accessToMutableFaceObservationsForPersonAtIndex:
delegate
setDelegate:
intersectSet:
removeIndex:
firstIndex
computeSharpnessQualityForImage:result:
_targetedImageSpecifier
initWithTargetedCVPixelBuffer:
initWithTargetedCVPixelBuffer:completionHandler:
initWithTargetedCVPixelBuffer:orientation:options:
initWithTargetedCVPixelBuffer:orientation:options:completionHandler:
initWithTargetedCGImage:
initWithTargetedCGImage:completionHandler:
initWithTargetedCGImage:orientation:options:
initWithTargetedCGImage:orientation:options:completionHandler:
initWithTargetedCIImage:
initWithTargetedCIImage:completionHandler:
initWithTargetedCIImage:orientation:options:
initWithTargetedCIImage:orientation:options:completionHandler:
initWithTargetedImageURL:
initWithTargetedImageURL:completionHandler:
initWithTargetedImageURL:orientation:options:
initWithTargetedImageURL:orientation:options:completionHandler:
initWithTargetedImageData:
initWithTargetedImageData:completionHandler:
initWithTargetedImageData:orientation:options:
initWithTargetedImageData:orientation:options:completionHandler:
targetedImageSpecifier
requiredTargetedImageSpecifierReturningError:
compare:options:
v24@0:8@16
@16@0:8
@24@0:8@16
@72@0:8^{__CVBuffer=}16@24{CGRect={CGPoint=dd}{CGSize=dd}}32^@64
^{__CVBuffer=}40@0:8@16@24^@32
B24@0:8@16
@72@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24@56^@64
B32@0:8@16^@24
@24@0:8Q16
q16@0:8
B40@0:8Q16@24^@32
r^{?=Q{?=ii}{?=ii}{?=ii}}16@0:8
{shared_ptr<vision::mod::FaceQualityPredictor>="__ptr_"^{FaceQualityPredictor}"__cntrl_"^{__shared_weak_count}}
{FaceQualityOptions="preferred_device_id"i"espresso_engine_id"i"storage_type"i}
B24@0:8^@16
v16@0:8
@32@0:8q16@24
@"NSURL"
@"NSString"
@36@0:8@16B24@28
@28@0:8@16B24
v32@0:8r^{map<long long, std::__1::vector<long long, std::__1::allocator<long long> >, std::__1::less<long long>, std::__1::allocator<std::__1::pair<const long long, std::__1::vector<long long, std::__1::allocator<long long> > > > >={__tree<std::__1::__value_type<long long, std::__1::vector<long long, std::__1::allocator<long long> > >, std::__1::__map_value_compare<long long, std::__1::__value_type<long long, std::__1::vector<long long, std::__1::allocator<long long> > >, std::__1::less<long long>, true>, std::__1::allocator<std::__1::__value_type<long long, std::__1::vector<long long, std::__1::allocator<long long> > > > >=^{__tree_end_node<std::__1::__tree_node_base<void *> *>}{__compressed_pair<std::__1::__tree_end_node<std::__1::__tree_node_base<void *> *>, std::__1::allocator<std::__1::__tree_node<std::__1::__value_type<long long, std::__1::vector<long long, std::__1::allocator<long long> > >, void *> > >={__tree_end_node<std::__1::__tree_node_base<void *> *>=^{__tree_node_base<void *>}}}{__compressed_pair<unsigned long, std::__1::__map_value_compare<long long, std::__1::__value_type<long long, std::__1::vector<long long, std::__1::allocator<long long> > >, std::__1::less<long long>, true> >=Q}}}16@24
v24@0:8r^{map<long long, std::__1::vector<long long, std::__1::allocator<long long> >, std::__1::less<long long>, std::__1::allocator<std::__1::pair<const long long, std::__1::vector<long long, std::__1::allocator<long long> > > > >={__tree<std::__1::__value_type<long long, std::__1::vector<long long, std::__1::allocator<long long> > >, std::__1::__map_value_compare<long long, std::__1::__value_type<long long, std::__1::vector<long long, std::__1::allocator<long long> > >, std::__1::less<long long>, true>, std::__1::allocator<std::__1::__value_type<long long, std::__1::vector<long long, std::__1::allocator<long long> > > > >=^{__tree_end_node<std::__1::__tree_node_base<void *> *>}{__compressed_pair<std::__1::__tree_end_node<std::__1::__tree_node_base<void *> *>, std::__1::allocator<std::__1::__tree_node<std::__1::__value_type<long long, std::__1::vector<long long, std::__1::allocator<long long> > >, void *> > >={__tree_end_node<std::__1::__tree_node_base<void *> *>=^{__tree_node_base<void *>}}}{__compressed_pair<unsigned long, std::__1::__map_value_compare<long long, std::__1::__value_type<long long, std::__1::vector<long long, std::__1::allocator<long long> > >, std::__1::less<long long>, true> >=Q}}}16
B16@0:8
v32@0:8@16@24
@32@0:8@16q24
@"VNClusteringLogger"
@"VNSuggestionsLogger"
@"NSData"
{shared_ptr<const vision::mod::FaceClustering>="__ptr_"^{FaceClustering}"__cntrl_"^{__shared_weak_count}}
@"NSArray"36@0:8@"NSDictionary"16f24^@28
@"NSData"24@0:8^@16
@"NSSet"24@0:8^@16
@"NSArray"32@0:8@"NSNumber"16^@24
@"NSNumber"40@0:8@"NSNumber"16@"NSNumber"24^@32
@"NSDictionary"32@0:8@"NSArray"16^@24
@"NSArray"24@0:8^@16
@"NSArray"40@0:8@"NSArray"16@"NSDictionary"24^@32
@"NSDictionary"40@0:8@"NSArray"16@"NSArray"24^@32
@"NSNumber"24@0:8^@16
@"NSArray"40@0:8@"NSData"16@"NSString"24^@32
@"NSUUID"16@0:8
@32@0:8@16^@24
@40@0:8@16@24^@32
@36@0:8@16f24^@28
@24@0:8^@16
v32@0:8{shared_ptr<const vision::mod::FaceClustering>=^{FaceClustering}^{__shared_weak_count}}16
@24@0:8^{vector<std::__1::pair<long long, long long>, std::__1::allocator<std::__1::pair<long long, long long> > >=^{pair<long long, long long>}^{pair<long long, long long>}{__compressed_pair<std::__1::pair<long long, long long> *, std::__1::allocator<std::__1::pair<long long, long long> > >=^{pair<long long, long long>}}}16
v32@0:8@16^{ImageDescriptorBufferFloat32=^^?{vector<long long, std::__1::allocator<long long> >=^q^q{__compressed_pair<long long *, std::__1::allocator<long long> >=^q}}{map<long long, int, std::__1::less<long long>, std::__1::allocator<std::__1::pair<const long long, int> > >={__tree<std::__1::__value_type<long long, int>, std::__1::__map_value_compare<long long, std::__1::__value_type<long long, int>, std::__1::less<long long>, true>, std::__1::allocator<std::__1::__value_type<long long, int> > >=^{__tree_end_node<std::__1::__tree_node_base<void *> *>}{__compressed_pair<std::__1::__tree_end_node<std::__1::__tree_node_base<void *> *>, std::__1::allocator<std::__1::__tree_node<std::__1::__value_type<long long, int>, void *> > >={__tree_end_node<std::__1::__tree_node_base<void *> *>=^{__tree_node_base<void *>}}}{__compressed_pair<unsigned long, std::__1::__map_value_compare<long long, std::__1::__value_type<long long, int>, std::__1::less<long long>, true> >=Q}}}^vQQQBQi^f}24
v40@0:8@16@24^{ImageDescriptorBufferJoint=^^?{vector<long long, std::__1::allocator<long long> >=^q^q{__compressed_pair<long long *, std::__1::allocator<long long> >=^q}}{map<long long, int, std::__1::less<long long>, std::__1::allocator<std::__1::pair<const long long, int> > >={__tree<std::__1::__value_type<long long, int>, std::__1::__map_value_compare<long long, std::__1::__value_type<long long, int>, std::__1::less<long long>, true>, std::__1::allocator<std::__1::__value_type<long long, int> > >=^{__tree_end_node<std::__1::__tree_node_base<void *> *>}{__compressed_pair<std::__1::__tree_end_node<std::__1::__tree_node_base<void *> *>, std::__1::allocator<std::__1::__tree_node<std::__1::__value_type<long long, int>, void *> > >={__tree_end_node<std::__1::__tree_node_base<void *> *>=^{__tree_node_base<void *>}}}{__compressed_pair<unsigned long, std::__1::__map_value_compare<long long, std::__1::__value_type<long long, int>, std::__1::less<long long>, true> >=Q}}}^vQQQBQi^f{vector<bool, std::__1::allocator<bool> >=^QQ{__compressed_pair<unsigned long, std::__1::allocator<unsigned long> >=Q}}{vector<vision::mod::DescriptorItemSideInfo, std::__1::allocator<vision::mod::DescriptorItemSideInfo> >=^{DescriptorItemSideInfo}^{DescriptorItemSideInfo}{__compressed_pair<vision::mod::DescriptorItemSideInfo *, std::__1::allocator<vision::mod::DescriptorItemSideInfo> >=^{DescriptorItemSideInfo}}}}32
{shared_ptr<vision::mod::FaceClustering>="__ptr_"^{FaceClustering}"__cntrl_"^{__shared_weak_count}}
@"NSArray"32@0:8@"NSDictionary"16^@24
q40@0:8^{ImageDescriptorBufferAbstract=^^?{vector<long long, std::__1::allocator<long long> >=^q^q{__compressed_pair<long long *, std::__1::allocator<long long> >=^q}}{map<long long, int, std::__1::less<long long>, std::__1::allocator<std::__1::pair<const long long, int> > >={__tree<std::__1::__value_type<long long, int>, std::__1::__map_value_compare<long long, std::__1::__value_type<long long, int>, std::__1::less<long long>, true>, std::__1::allocator<std::__1::__value_type<long long, int> > >=^{__tree_end_node<std::__1::__tree_node_base<void *> *>}{__compressed_pair<std::__1::__tree_end_node<std::__1::__tree_node_base<void *> *>, std::__1::allocator<std::__1::__tree_node<std::__1::__value_type<long long, int>, void *> > >={__tree_end_node<std::__1::__tree_node_base<void *> *>=^{__tree_node_base<void *>}}}{__compressed_pair<unsigned long, std::__1::__map_value_compare<long long, std::__1::__value_type<long long, int>, std::__1::less<long long>, true> >=Q}}}^vQQQB}16^{vector<std::__1::pair<long long, long long>, std::__1::allocator<std::__1::pair<long long, long long> > >=^{pair<long long, long long>}^{pair<long long, long long>}{__compressed_pair<std::__1::pair<long long, long long> *, std::__1::allocator<std::__1::pair<long long, long long> > >=^{pair<long long, long long>}}}24Q32
{CGRect="origin"{CGPoint="x"d"y"d}"size"{CGSize="width"d"height"d}}
@52@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16i48
i16@0:8
v20@0:8i16
{CGRect={CGPoint=dd}{CGSize=dd}}16@0:8
v48@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16
@20@0:8f16
v20@0:8f16
f16@0:8
{CGPoint="x"d"y"d}
{CGSize="width"d"height"d}
f48@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16
{CGPoint=dd}16@0:8
v32@0:8{CGPoint=dd}16
{CGSize=dd}16@0:8
v32@0:8{CGSize=dd}16
@"NSMutableDictionary"
@"NSMutableArray"
{CGRect={CGPoint=dd}{CGSize=dd}}56@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16f48f52
{CGRect={CGPoint=dd}{CGSize=dd}}64@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16@48^B56
@56@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16@48
@48@0:8@16{CGSize=dd}24@40
i32@0:8^{__CVBuffer=}16@24
v32@0:8^{__CVBuffer=}16@24
v40@0:8@16{CGSize=dd}24
d16@0:8
v24@0:8d16
v20@0:8B16
[7d]
^{__SVMParameters=[7{__SVMScaleOffset=ff}]ddii^{BurstSupportVector}^{BurstSupportVector}}
@20@0:8i16
d24@0:8r^{BurstSupportVector=d[7d]}16
^{__SVMParameters=[7{__SVMScaleOffset=ff}]ddii^{BurstSupportVector}^{BurstSupportVector}}16@0:8
v24@0:8^{__SVMParameters=[7{__SVMScaleOffset=ff}]ddii^{BurstSupportVector}^{BurstSupportVector}}16
{shared_ptr<vision::mod::FaceprintAndAttributes>="__ptr_"^{FaceprintAndAttributes}"__cntrl_"^{__shared_weak_count}}
{shared_ptr<vision::mod::FaceFrontalizer>="__ptr_"^{FaceFrontalizer}"__cntrl_"^{__shared_weak_count}}
{vImage_Buffer="data"^v"height"Q"width"Q"rowBytes"Q}
@"NSMutableData"
v40@0:8^{map<std::__1::basic_string<char>, float, std::__1::less<std::__1::basic_string<char> >, std::__1::allocator<std::__1::pair<const std::__1::basic_string<char>, float> > >={__tree<std::__1::__value_type<std::__1::basic_string<char>, float>, std::__1::__map_value_compare<std::__1::basic_string<char>, std::__1::__value_type<std::__1::basic_string<char>, float>, std::__1::less<std::__1::basic_string<char> >, true>, std::__1::allocator<std::__1::__value_type<std::__1::basic_string<char>, float> > >=^{__tree_end_node<std::__1::__tree_node_base<void *> *>}{__compressed_pair<std::__1::__tree_end_node<std::__1::__tree_node_base<void *> *>, std::__1::allocator<std::__1::__tree_node<std::__1::__value_type<std::__1::basic_string<char>, float>, void *> > >={__tree_end_node<std::__1::__tree_node_base<void *> *>=^{__tree_node_base<void *>}}}{__compressed_pair<unsigned long, std::__1::__map_value_compare<std::__1::basic_string<char>, std::__1::__value_type<std::__1::basic_string<char>, float>, std::__1::less<std::__1::basic_string<char> >, true> >=Q}}}16@24Q32
@"NSNumber"
B56@0:8@16@24@32Q40^@48
@64@0:8@16@24@32B40f44Q48^@56
@68@0:8@16@24@32B40f44f48Q52^@60
@40@0:8#16@24^@32
@32@0:8Q16^@24
@"<VNClusteringReadOnly><VNClusteringCancelling>"
@"NSArray"32@0:8@"NSArray"16^@24
@"NSArray"44@0:8@"NSDictionary"16f24@"VNCanceller"28^@36
@"NSNumber"40@0:8@"VNFaceprint"16@"VNFaceprint"24^@32
@"NSNumber"40@0:8@"VNFaceObservation"16@"VNFaceObservation"24^@32
@44@0:8@16f24@28^@36
@60@0:8@16@24@32f40Q44^@52
@64@0:8@16@24@32f40f44Q48^@56
@"<VNClusteringReadOnly><VNClusteringWritable><VNClusteringCancelling>"
B32@0:8@"NSData"16^@24
@"NSArray"48@0:8@"NSArray"16@"NSArray"24@"VNCanceller"32^@40
@"NSArray"56@0:8@"NSArray"16@"NSArray"24@"NSArray"32@"VNCanceller"40^@48
@"NSArray"40@0:8@"NSArray"16@"VNCanceller"24^@32
@48@0:8@16@24@32^@40
@56@0:8@16@24@32@40^@48
@68@0:8@16@24@32f40f44f48Q52^@60
@72@0:8@16@24@32f40f44f48f52Q56^@64
@"NSArray"24@0:8@"NSDictionary"16
#16@0:8
@24@0:8:16
@32@0:8:16@24
@40@0:8:16@24@32
B24@0:8#16
B24@0:8@"Protocol"16
B24@0:8:16
Vv16@0:8
Q16@0:8
^{_NSZone=}16@0:8
@"NSString"16@0:8
{shared_ptr<vision::mod::ImageClassifierAbstract>=^{ImageClassifierAbstract}^{__shared_weak_count}}88@0:8{shared_ptr<vision::mod::ImageDescriptorProcessorAbstract>=^{ImageDescriptorProcessorAbstract}^{__shared_weak_count}}16r*32i40i44r*48{Options=BQ@@}56
{shared_ptr<vision::mod::ImageDescriptorProcessorAbstract>=^{ImageDescriptorProcessorAbstract}^{__shared_weak_count}}68@0:8r*16i24i28i32{Options=BQ@@}36
@32@0:8@16Q24
v32@0:8^@16^@24
{shared_ptr<vision::mod::ImageClassifier_HierarchicalModel>="__ptr_"^{ImageClassifier_HierarchicalModel}"__cntrl_"^{__shared_weak_count}}
B40@0:8@16^{shared_ptr<vision::mod::ImageDescriptorBufferAbstract>=^{ImageDescriptorBufferAbstract}^{__shared_weak_count}}24^@32
@"VNRequest"
@"NSError"
@32@0:8@16@24
@"<NSObject><NSCopying>"
@"NSArray"
@"NSMapTable"
@"<MTLDevice>"
@"<MTLCommandQueue>"
@"<MTLLibrary>"
@48@0:8^{__CVBuffer=}16Q24Q32^@40
@64@0:8^{__CVBuffer=}16Q24{CGSize=dd}32Q48^@56
Q32@0:8Q16Q24
Q24@0:8Q16
@40@0:8q16@24@32
@32@0:8#16Q24
@40@0:8@16@24@32
@32@0:8Q16@24
@32@0:8Q16#24
@28@0:8i16@20
@"NSDictionary"
@40@0:8^{__CVBuffer=}16@24@?32
@32@0:8^{__CVBuffer=}16@24
@40@0:8^{CGImage=}16@24@?32
@32@0:8^{CGImage=}16@24
@40@0:8@16@24@?32
{shared_ptr<vision::mod::ObjectDetector_DCNFaceDetector_v2>="__ptr_"^{ObjectDetector_DCNFaceDetector_v2}"__cntrl_"^{__shared_weak_count}}
@"VNFaceBBoxAligner"
@24@0:8#16
@24@0:8^{_NSZone=}16
v24@0:8Q16
@32@0:8^@16^@24
@?16@0:8
B24@0:8Q16
[1024f]
[256S]
{FastRegistration_Signatures="piRow"^f"nPiRow"Q"piRowTable"{Projections_meanStdTable="sumTable"^f"sumSqTable"^f}"piCol"^f"nPiCol"Q"piColTable"{Projections_meanStdTable="sumTable"^f"sumSqTable"^f}"_memoryContainer"*}
^{SharpnessGridElement_t=CCf}
{GridROI_t="startX"i"startY"i"endX"i"endY"i}
v48@0:8^^f16^^f24^^f32^^f40
v24@0:8^f16
v48@0:8{vImage_Buffer=^vQQQ}16
{GridROI_t=iiii}16@0:8
f32@0:8@16@24
v32@0:8{GridROI_t=iiii}16
v40@0:8@16^f24^f32
f24@0:8@16
i24@0:8@16
v24@0:8^{__CVBuffer=}16
^S16@0:8
f20@0:8f16
q24@0:8@16
^f16@0:8
@"VNRequestPerformer"
B32@0:8@"NSArray"16^@24
B48@0:8@16@24^@32^@40
B40@0:8@16^{__CVBuffer=}24^@32
B44@0:8@16^{__CVBuffer=}24I32^@36
B48@0:8@16^{__CVBuffer=}24^@32^@40
B52@0:8@16^{__CVBuffer=}24I32^@36^@44
B40@0:8@16^{CGImage=}24^@32
B44@0:8@16^{CGImage=}24I32^@36
B48@0:8@16^{CGImage=}24^@32^@40
B52@0:8@16^{CGImage=}24I32^@36^@44
B40@0:8@16@24^@32
B44@0:8@16@24I32^@36
B52@0:8@16@24I32^@36^@44
v32@0:8@16@?24
@"VNImageSpecifier"
@"VNObservationsCache"
@36@0:8^{__CVBuffer=}16I24@28
@36@0:8^{CGImage=}16I24@28
@36@0:8@16I24@28
B40@0:8@16^@24^@32
@"NSObject"
v24@0:8@?16
^{__CVBuffer=}72@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24@56^@64
B60@0:8Q16^^{__CVBuffer}24I32Q36Q44^@52
B72@0:8^{?=[3]}16^{__CVBuffer=}24^{__CVBuffer=}32^{ImageRegistrationCtx_s=}40^v48r^{?=[3]}56^@64
@"VNSceneObservation"
@"VNClassificationCustomHierarchy"
@"NSArray"16@0:8
@32@0:8@16@?24
@32@0:8:16Q24
@48@0:8Q16Q24^@32^@40
@64@0:8Q16Q24@32Q40^@48^@56
@40@0:8Q16Q24^@32
r^{?=Q#Q}16@0:8
@24@0:8^v16
v24@0:8^v16
^v16@0:8
@32@0:8^{__CVBuffer=}16^@24
B76@0:8^f16^{__CVBuffer=}24{CGRect={CGPoint=dd}{CGSize=dd}}32f64^@68
B44@0:8^f16^{__CVBuffer=}24f32^@36
B76@0:8^f16@24f32{CGRect={CGPoint=dd}{CGSize=dd}}36^@68
B40@0:8^f16^{__CVBuffer=}24^@32
B48@0:8^f16^{__CVBuffer=}24i32f36^@40
I24@0:8Q16
@"VNDetectedObjectObservation"
@"NSObject<OS_dispatch_queue>"
@"VNMetalContext"
@"<NSObject><NSCopying>"24@0:8@"NSDictionary"16
@76@0:8I16@20{CGRect={CGPoint=dd}{CGSize=dd}}28@60^@68
B40@0:8^@16@24^@32
#32@0:8@16^@24
v32@0:8@16Q24
@"VNImageAnalyzerCompoundRequestGroupingConfiguration"
@40@0:8Q16^@24^@32
v40@0:8@16Q24@32
{_Geometry2D_rect2D_="origin"{_Geometry2D_point2D_="x"f"y"f}"size"{_Geometry2D_size2D_="height"f"width"f}}
v24@0:8@"NSCoder"16
@24@0:8@"NSCoder"16
@92@0:8Q16^{vImage_Buffer=^vQQQ}24B32{CGRect={CGPoint=dd}{CGSize=dd}}36{_Geometry2D_rect2D_={_Geometry2D_point2D_=ff}{_Geometry2D_size2D_=ff}}68@84
@32@0:8{CGPoint=dd}16
@48@0:8{CGPoint=dd}16{CGSize=dd}32
B32@0:8@16:24
{os_unfair_lock_s="_os_unfair_lock_opaque"I}
B32@0:8@?16@?24
{shared_ptr<vision::mod::LandmarkAttributes>="__ptr_"^{LandmarkAttributes}"__cntrl_"^{__shared_weak_count}}
@"<VNModelFile>"
^{__CVBuffer=}88@0:8@16{_Geometry2D_rect2D_={_Geometry2D_point2D_=ff}{_Geometry2D_size2D_=ff}}24^{vImage_Buffer=^vQQQ}40^{_Geometry2D_rect2D_={_Geometry2D_point2D_=ff}{_Geometry2D_size2D_=ff}}48^B56^f64@72^@80
v24@0:8^{vector<_Geometry2D_point2D_, std::__1::allocator<_Geometry2D_point2D_> >=^{_Geometry2D_point2D_}^{_Geometry2D_point2D_}{__compressed_pair<_Geometry2D_point2D_ *, std::__1::allocator<_Geometry2D_point2D_> >=^{_Geometry2D_point2D_}}}16
@48@0:8r^{vImage_Buffer=^vQQQ}16r^{_Geometry2D_rect2D_={_Geometry2D_point2D_=ff}{_Geometry2D_size2D_=ff}}24r^v32^@40
B64@0:8r^{vector<_Geometry2D_point2D_, std::__1::allocator<_Geometry2D_point2D_> >=^{_Geometry2D_point2D_}^{_Geometry2D_point2D_}{__compressed_pair<_Geometry2D_point2D_ *, std::__1::allocator<_Geometry2D_point2D_> >=^{_Geometry2D_point2D_}}}16@24@32@40@48^@56
{CGRect={CGPoint=dd}{CGSize=dd}}24@0:8@16
B64@0:8r^{vImage_Buffer=^vQQQ}16@24^{_Geometry2D_rect2D_={_Geometry2D_point2D_=ff}{_Geometry2D_size2D_=ff}}32r^v40@48^@56
r^{_LandmarkDetector_faceMeshParts_=ii[23i]i[23i]i[23i]i[23i]i[23i]i[23i]i[23i]i[23i]i[23i]i[23i]i[23i]i[23i]i[23i]i[23i]i[23i]i[23i]i[23i]}24@0:8Q16
{_Geometry2D_point2D_=ff}36@0:8r^{_Geometry2D_point2D_=ff}16r^i24i32
v48@0:8@16@24@32B40B44
@"CIContext"
@"NSHashTable"
@"NSLock"
^{CGImageSource=}
^{CGImageSource=}40@0:8^^{CGImageSource}16I24^{os_unfair_lock_s=I}28B36
^{CGImageSource=}16@0:8
^{CGImageSource=}24@0:8I16B20
^{__CVBuffer=}
^{__CFArray=}
@"CIImage"
@"VNImageSourceManager"
^{__CVBuffer=}16@0:8
^{__CVBuffer=}52@0:8Q16Q24I32@36^@44
B104@0:8^{__CVBuffer=}16^^{__CVBuffer}24Q32Q40I48{CGRect={CGPoint=dd}{CGSize=dd}}52B84@88^@96
{CGRect={CGPoint=dd}{CGSize=dd}}48@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16
B48@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16
B104@0:8@16^^{__CVBuffer}24Q32Q40I48{CGRect={CGPoint=dd}{CGSize=dd}}52B84@88^@96
^{__CVBuffer=}36@0:8Q16Q24I32
^{__CVBuffer=}84@0:8Q16Q24I32{CGRect={CGPoint=dd}{CGSize=dd}}36@68^@76
^{__CVBuffer=}116@0:8Q16Q24{CGRect={CGPoint=dd}{CGSize=dd}}32I64Q68@76^@84^{CGPoint=dd}92^d100^d108
B24@0:8^f16
B24@0:8^{CGPoint=dd}16
B24@0:8^{?=[3]}16
B84@0:8Q16f24@28{CGRect={CGPoint=dd}{CGSize=dd}}36@?68^@76
^{__CVBuffer=}44@0:8Q16I24@28^@36
^{__CVBuffer=}76@0:8Q16{CGRect={CGPoint=dd}{CGSize=dd}}24I56@60^@68
@60@0:8Q16Q24I32@36@44^@52
@92@0:8Q16Q24I32{CGRect={CGPoint=dd}{CGSize=dd}}36@68@76^@84
B20@0:8I16
r^{__CFDictionary=}16@0:8
{CGRect={CGPoint=dd}{CGSize=dd}}80@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{CGSize=dd}48^d64^d72
^{CGColorSpace=}28@0:8I16^I20
v24@0:8@"NSArray"16
v48@0:8@16@24@32@40
@"VNPersonsModelData"
@"VNPersonsModelFaceModel"
v24@0:8@"VNPersonsModelData"16
B40@0:8@16^{CC_MD5state_st=IIIIII[16I]i}24^@32
B48@0:8@16@24^{CC_MD5state_st=IIIIII[16I]i}32^@40
B56@0:8Q16@24@32^{CC_MD5state_st=IIIIII[16I]i}40^@48
B44@0:8^^?16^:24Q32B40
B48@0:8^^?16^:24^Q32@40
@40@0:8Q16@24^@32
@"VNRequestConfiguration"
@"VNWarningRecorder"
@"VNCanceller"
@"NSObject<OS_dispatch_semaphore>"
@"<NSObject><NSCopying>"16@0:8
v32@0:8@"NSString"16@24
@24@0:8@"NSString"16
@24@0:8@?16
B48@0:8@16^Q24^Q32^@40
B32@0:8Q16^@24
Q24@0:8@16
B56@0:8^@16#24@32@40^@48
B48@0:8^d16@24@32^@40
B56@0:8^d16@24@32d40^@48
B48@0:8^f16@24@32^@40
B52@0:8^f16@24@32f40^@44
B56@0:8^@16@24@32#40^@48
Q20@0:8i16
Q32@0:8#16Q24
@"VNProcessingDevice"
@40@0:8Q16Q24Q32
@"VNSizeRange"
@52@0:8I16@20@28Q36I44B48
B32@0:8Q16Q24
I16@0:8
@"VNFaceprint"
@"VNTorsoprint"
@64@0:8r^v16Q24Q32Q40@48Q56
@68@0:8r^v16Q24Q32Q40@48B56Q60
@40@0:8@16@24Q32
@40@0:8@16Q24^@32
Q40@0:8@16Q24^@32
^{__CVBuffer=}24@0:8^@16
{shared_ptr<vision::mod::ObjectDetector_DCNFaceDetector>="__ptr_"^{ObjectDetector_DCNFaceDetector}"__cntrl_"^{__shared_weak_count}}
@"VNMPContext"
@36@0:8@16i24^@28
@36@0:8@16B24^@28
@40@0:8{vector<MPClusteringTreeNode *, std::__1::allocator<MPClusteringTreeNode *> >=^^{MPClusteringTreeNode}^^{MPClusteringTreeNode}{__compressed_pair<MPClusteringTreeNode **, std::__1::allocator<MPClusteringTreeNode *> >=^^{MPClusteringTreeNode}}}16
@28@0:8^v16B24
@60@0:8q16{CGRect={CGPoint=dd}{CGSize=dd}}24f56
v24@0:8q16
@24@0:8^{__CVBuffer=}16
@"VNImageBuffer"
@36@0:8^{__CVBuffer=}16I24^@28
@44@0:8^{__CVBuffer=}16I24@28^@36
@32@0:8^{CGImage=}16^@24
@36@0:8^{CGImage=}16I24^@28
@44@0:8^{CGImage=}16I24@28^@36
@36@0:8@16I24^@28
@44@0:8@16I24@28^@36
^{CGImage=}
@24@0:8^{CGImage=}16
^{CGImage=}16@0:8
[4{CGPoint="x"d"y"d}]
[4f]
v28@0:8f16i20i24
{CGRect={CGPoint=dd}{CGSize=dd}}52@0:8f16{vImage_Buffer=^vQQQ}20
@48@0:8Q16^{__CVBuffer=}24{CGSize=dd}32
B80@0:8@16@24{CGRect={CGPoint=dd}{CGSize=dd}}32@64^@72
^?16@0:8
^{__CVBuffer=}64@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24^@56
16@0:8
B40@0:8@16Q24^@32
@"VNPersonsModel"
@"<VNPersonsModelDataSource>"
@"<NSObject><NSCopying><NSSecureCoding>"24@0:8Q16
Q24@0:8@"<NSObject><NSCopying><NSSecureCoding>"16
@"VNFaceObservation"32@0:8Q16Q24
@32@0:8Q16Q24
@"VNPersonsModelConfiguration"
@48@0:8@16Q24@32^@40
#28@0:8I16^@20
@40@0:8#16@24@32
@72@0:8@16@24@?32@?40^#48^Q56^@64
B52@0:8I16@20@28^{CC_MD5state_st=IIIIII[16I]i}36^@44
@"NSDate"
@"NSIndexSet"
@"VNFaceObservation"
@"<NSObject><NSCopying><NSSecureCoding>"
@36@0:8@16@24f32
B48@0:8^@16@24@32^@40
@48@0:8#16@24@32^@40
#48@0:8@16@24^@32^@40
#32@0:8@16@24
@"VNImageRegistrationSignature"
B56@0:8^@16^@24@32@40^@48
@"VNClassificationObservation"
@"VNFaceAttributeCategory"
v24@0:8^{vImage_Buffer=^vQQQ}16
@"VNOperationPoints"
@"VNOperationPoints"24@0:8^@16
f40@0:8^{vImage_Buffer=^vQQQ}16@24^@32
{shared_ptr<vision::mod::FaceSegmenterDNN>="__ptr_"^{FaceSegmenterDNN}"__cntrl_"^{__shared_weak_count}}
B40@0:8^{CGSize=dd}16Q24^@32
B40@0:8^Q16Q24^@32
B48@0:8^{__CVBuffer=}16^{?=^v^v[4Q][4Q]QQQQQQQQQQi}24@32^@40
B80@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16^{?=^v^v[4Q][4Q]QQQQQQQQQQi}48@56@64^@72
@40@0:8r^{?=^v^v[4Q][4Q]QQQQQQQQQQi}16Q24^@32
@"ShotflowNetwork"
@28@0:8r^{vImage_Buffer=^vQQQ}16B24
@32@0:8@16f24f28
@48@0:8@16f24@28i36i40i44
@36@0:8@16i24i28i32
@52@0:8{?=^vi}16^v32f40@44
@40@0:8{?=^vi}16^v32
{CGSize=dd}32@0:8{CGSize=dd}16
{shared_ptr<vision::mod::ImageDescriptorProcessorAbstract>="__ptr_"^{ImageDescriptorProcessorAbstract}"__cntrl_"^{__shared_weak_count}}
{shared_ptr<vision::mod::ImageClassifierAbstract>="__ptr_"^{ImageClassifierAbstract}"__cntrl_"^{__shared_weak_count}}
@"NSSet"
@80@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24@56@64^@72
B104@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24@56@64^{shared_ptr<vision::mod::ImageDescriptorBufferAbstract>=^{ImageDescriptorBufferAbstract}^{__shared_weak_count}}72@80@88^@96
{shared_ptr<vision::mod::ImageClassifier_HierarchicalModel>=^{ImageClassifier_HierarchicalModel}^{__shared_weak_count}}32@0:8Q16^@24
v32@0:8@16^{vector<std::__1::pair<std::__1::basic_string<char>, std::__1::basic_string<char> >, std::__1::allocator<std::__1::pair<std::__1::basic_string<char>, std::__1::basic_string<char> > > >=^{pair<std::__1::basic_string<char>, std::__1::basic_string<char> >}^{pair<std::__1::basic_string<char>, std::__1::basic_string<char> >}{__compressed_pair<std::__1::pair<std::__1::basic_string<char>, std::__1::basic_string<char> > *, std::__1::allocator<std::__1::pair<std::__1::basic_string<char>, std::__1::basic_string<char> > > >=^{pair<std::__1::basic_string<char>, std::__1::basic_string<char> >}}}24
@"CIBarcodeDescriptor"
@104@0:8Q16@24@32{CGPoint=dd}40{CGPoint=dd}56{CGPoint=dd}72{CGPoint=dd}88
@72@0:8Q16@24@32{CGRect={CGPoint=dd}{CGSize=dd}}40
^{ACBSConfig=}24@0:8^@16
B56@0:8@16^{CGPoint=dd}24^{CGPoint=dd}32^{CGPoint=dd}40^{CGPoint=dd}48
@88@0:8@16Q24Q32{CGRect={CGPoint=dd}{CGSize=dd}}40Q72^@80
@48@0:8@16^{ACBSConfig=}24Q32^@40
@72@0:8q16{CGRect={CGPoint=dd}{CGSize=dd}}24f56f60f64i68
^{vImage_Buffer=^vQQQ}
@48@0:8^{vImage_Buffer=^vQQQ}16@24@32^@40
@48@0:8^{vImage_Buffer=^vQQQ}16@24q32^@40
@48@0:8^{__CVBuffer=}16@24@32^@40
@48@0:8^{__CVBuffer=}16@24q32^@40
^{vImage_Buffer=^vQQQ}16@0:8
B40@0:8^f16@24^@32
B44@0:8^f16@24f32^@36
@"VNSupportedImageSize"
{CGSize=dd}40@0:8@16Q24Q32
^{__CVBuffer=}96@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16@48I56{CGSize=dd}60B76@80^@88
^{__CVBuffer=}64@0:8@16I24{CGSize=dd}28B44@48^@56
B48@0:8Q16@24@32^@40
@"VNMPImageDescriptor"
@32@0:8@"NSData"16^@24
Q40@0:8@"NSMutableData"16Q24^@32
@40@0:8@"NSData"16Q24^@32
@40@0:8@16Q24Q32
[9f]
@28@0:8@16I24
@112@0:8Q16f24f28f32f36f40f44f48f52f56f60f64f68f72f76f80f84f88f92f96f100f104f108
{shared_ptr<vision::mod::FaceRegionMap>="__ptr_"^{FaceRegionMap}"__cntrl_"^{__shared_weak_count}}
@"NSUUID"
v80@0:8{CGPoint=dd}16{CGPoint=dd}32{CGPoint=dd}48{CGPoint=dd}64
@48@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16
@56@0:8Q16{CGRect={CGPoint=dd}{CGSize=dd}}24
@"VNFaceLandmarks2D"
@"VNFaceLandmarks3D"
@"VNFaceRegionMap"
@"VNFaceAttributes"
@"VNFaceTorsoprint"
@"VNFaceSegments"
{?=[4]}16@0:8
{?=}16@0:8
B24@0:8^{CGAffineTransform=dddddd}16
B32@0:8^i16^@24
{_Geometry2D_rect2D_={_Geometry2D_point2D_=ff}{_Geometry2D_size2D_=ff}}16@0:8
v32@0:8{_Geometry2D_rect2D_={_Geometry2D_point2D_=ff}{_Geometry2D_size2D_=ff}}16
B36@0:8f16^i20^@28
@80@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{CGRect={CGPoint=dd}{CGSize=dd}}48
@88@0:8Q16{CGRect={CGPoint=dd}{CGSize=dd}}24{CGRect={CGPoint=dd}{CGSize=dd}}56
@64@0:8Q16{CGRect={CGPoint=dd}{CGSize=dd}}24@56
@72@0:8Q16{CGRect={CGPoint=dd}{CGSize=dd}}24@56@64
@104@0:8Q16{CGRect={CGPoint=dd}{CGSize=dd}}24{CGRect={CGPoint=dd}{CGSize=dd}}56@88@96
B104@0:8{?=[4]}16^f80^f88^f96
{CGAffineTransform=dddddd}16@0:8
v64@0:8{CGAffineTransform=dddddd}16
{CGAffineTransform="a"d"b"d"c"d"d"d"tx"d"ty"d}
{?="columns"[3]}
{?=[3]}16@0:8
v64@0:8{?=[3]}16
@"VNImageprint"
@"<VNOperationPointsProviding>"
@36@0:8Q16@24f32
@44@0:8Q16@24f32@36
B24@0:8f16f20
@68@0:8Q16{CGRect={CGPoint=dd}{CGSize=dd}}24f56@60
@"MLFeatureValue"
@40@0:8Q16@24@32
@40@0:8Q16@24^{__CVBuffer=}32
@80@0:8{CGPoint=dd}16{CGPoint=dd}32{CGPoint=dd}48{CGPoint=dd}64
@88@0:8Q16{CGPoint=dd}24{CGPoint=dd}40{CGPoint=dd}56{CGPoint=dd}72
f32@0:8@16^@24
v28@0:8I16@20
I24@0:8@16
v72@0:8{CGAffineTransform=dddddd}16@64
{CGAffineTransform=dddddd}24@0:8@16
v72@0:8{?=[3]}16@64
{?=[3]}24@0:8@16
v88@0:8{?=[4]}16@80
{?=[4]}24@0:8@16
B36@0:8^@16B24^@28
@"NSArray"28@0:8B16^@20
@28@0:8B16^@20
{CGRect={CGPoint=dd}{CGSize=dd}}32@0:8Q16Q24
B48@0:8@16@24Q32@40
B32@0:8@16@24
B108@0:8{vImage_Buffer=^vQQQ}16{vImage_Buffer=^vQQQ}48{CGSize=dd}80f96f100f104
f48@0:8{vImage_Buffer=^vQQQ}16
@52@0:8{vImage_Buffer=^vQQQ}16f48
@56@0:8{vImage_Buffer=^vQQQ}16f48B52
@40@0:8^{__CVBuffer=}16f24B28^@32
@64@0:8{vImage_Buffer=^vQQQ}16@48^@56
@100@0:8{vImage_Buffer=^vQQQ}16@48B56B60{CGSize=dd}64f80f84f88^@92
@76@0:8^{__CVBuffer=}16@24B32B36{CGSize=dd}40f56f60f64^@68
^{__CVBuffer=}24@0:8@16
B32@0:8^{__CVBuffer=}16^{__CVBuffer=}24
Q24@0:8^{__CVBuffer=}16
{shared_ptr<vision::mod::FaceIDModel>="__ptr_"^{FaceIDModel}"__cntrl_"^{__shared_weak_count}}
@56@0:8{shared_ptr<vision::mod::FaceIDModel>=^{FaceIDModel}^{__shared_weak_count}}16Q32Q40@48
@56@0:8@16r^{ImageDescriptorBufferFloat32=^^?{vector<long long, std::__1::allocator<long long> >=^q^q{__compressed_pair<long long *, std::__1::allocator<long long> >=^q}}{map<long long, int, std::__1::less<long long>, std::__1::allocator<std::__1::pair<const long long, int> > >={__tree<std::__1::__value_type<long long, int>, std::__1::__map_value_compare<long long, std::__1::__value_type<long long, int>, std::__1::less<long long>, true>, std::__1::allocator<std::__1::__value_type<long long, int> > >=^{__tree_end_node<std::__1::__tree_node_base<void *> *>}{__compressed_pair<std::__1::__tree_end_node<std::__1::__tree_node_base<void *> *>, std::__1::allocator<std::__1::__tree_node<std::__1::__value_type<long long, int>, void *> > >={__tree_end_node<std::__1::__tree_node_base<void *> *>=^{__tree_node_base<void *>}}}{__compressed_pair<unsigned long, std::__1::__map_value_compare<long long, std::__1::__value_type<long long, int>, std::__1::less<long long>, true> >=Q}}}^vQQQBQi^f}24Q32^{CVMLCanceller=^^?Bi}40^@48
@56@0:8@16r^{ImageDescriptorBufferFloat32=^^?{vector<long long, std::__1::allocator<long long> >=^q^q{__compressed_pair<long long *, std::__1::allocator<long long> >=^q}}{map<long long, int, std::__1::less<long long>, std::__1::allocator<std::__1::pair<const long long, int> > >={__tree<std::__1::__value_type<long long, int>, std::__1::__map_value_compare<long long, std::__1::__value_type<long long, int>, std::__1::less<long long>, true>, std::__1::allocator<std::__1::__value_type<long long, int> > >=^{__tree_end_node<std::__1::__tree_node_base<void *> *>}{__compressed_pair<std::__1::__tree_end_node<std::__1::__tree_node_base<void *> *>, std::__1::allocator<std::__1::__tree_node<std::__1::__value_type<long long, int>, void *> > >={__tree_end_node<std::__1::__tree_node_base<void *> *>=^{__tree_node_base<void *>}}}{__compressed_pair<unsigned long, std::__1::__map_value_compare<long long, std::__1::__value_type<long long, int>, std::__1::less<long long>, true> >=Q}}}^vQQQBQi^f}24Q32@40^@48
{shared_ptr<vision::mod::ImageDescriptorBufferFloat32>=^{ImageDescriptorBufferFloat32}^{__shared_weak_count}}52@0:8{shared_ptr<vision::mod::ImageDescriptorBufferFloat32>=^{ImageDescriptorBufferFloat32}^{__shared_weak_count}}16@32i40^{vector<int, std::__1::allocator<int> >=^i^i{__compressed_pair<int *, std::__1::allocator<int> >=^i}}44
^{__sFILE=*iiss{__sbuf=*i}i^v^?^?^?^?{__sbuf=*i}^{__sFILEX}i[3C][1C]{__sbuf=*i}iq}
@"BurstImageFaceAnalysisContext"
@"NSCountedSet"
@"BurstActionClassifier"
{CGRect={CGPoint=dd}{CGSize=dd}}68@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{CGSize=dd}48i64
v44@0:8@16{CGSize=dd}24i40
v56@0:8@16@24@32@40@?48
v48@0:8@16@24@32@?40
v28@0:8@16i24
q40@0:8@16^^{MPClusteringTreeNode}24@32
f28@0:8^{MPClusteringTreeNode=@iffi^{MPClusteringTreeNode}^{MPClusteringTreeNode}^{MPClusteringTreeNode}^{MPClusteringTreeNode}}16i24
{vector<MPClusteringTreeNode *, std::__1::allocator<MPClusteringTreeNode *> >=^^{MPClusteringTreeNode}^^{MPClusteringTreeNode}{__compressed_pair<MPClusteringTreeNode **, std::__1::allocator<MPClusteringTreeNode *> >=^^{MPClusteringTreeNode}}}40@0:8i16f20^{MPClusteringTreeNode=@iffi^{MPClusteringTreeNode}^{MPClusteringTreeNode}^{MPClusteringTreeNode}^{MPClusteringTreeNode}}24@32
{vector<MPClusteringTreeNode *, std::__1::allocator<MPClusteringTreeNode *> >=^^{MPClusteringTreeNode}^^{MPClusteringTreeNode}{__compressed_pair<MPClusteringTreeNode **, std::__1::allocator<MPClusteringTreeNode *> >=^^{MPClusteringTreeNode}}}36@0:8i16^{MPClusteringTreeNode=@iffi^{MPClusteringTreeNode}^{MPClusteringTreeNode}^{MPClusteringTreeNode}^{MPClusteringTreeNode}}20@28
{vector<MPClusteringTreeNode *, std::__1::allocator<MPClusteringTreeNode *> >=^^{MPClusteringTreeNode}^^{MPClusteringTreeNode}{__compressed_pair<MPClusteringTreeNode **, std::__1::allocator<MPClusteringTreeNode *> >=^^{MPClusteringTreeNode}}}36@0:8f16^{MPClusteringTreeNode=@iffi^{MPClusteringTreeNode}^{MPClusteringTreeNode}^{MPClusteringTreeNode}^{MPClusteringTreeNode}}20@28
{vector<MPClusteringTreeNode *, std::__1::allocator<MPClusteringTreeNode *> >=^^{MPClusteringTreeNode}^^{MPClusteringTreeNode}{__compressed_pair<MPClusteringTreeNode **, std::__1::allocator<MPClusteringTreeNode *> >=^^{MPClusteringTreeNode}}}32@0:8^{MPClusteringTreeNode=@iffi^{MPClusteringTreeNode}^{MPClusteringTreeNode}^{MPClusteringTreeNode}^{MPClusteringTreeNode}}16@24
f28@0:8f16q20
f32@0:8f16q20B28
{?="plan"^v"network_index"i}
@56@0:8@16{?=^vi}24^v40^v48
{?=^vi}16@0:8
B84@0:8@16@24B32@36@44^^v52^^v60^{?=^vi}68^@76
B88@0:8@16@24B32@36@44i52^^v56^^v64^{?=^vi}72^@80
v32@0:8^v16^v24
^{__CVBuffer=}36@0:8I16r^{?=^v^v[4Q][4Q]QQQQQQQQQQi}20^@28
B40@0:8r^{?=^v^v[4Q][4Q]QQQQQQQQQQi}16^{__CVBuffer=}24^@32
Q32@0:8r^{?=^v^v[4Q][4Q]QQQQQQQQQQi}16^@24
B52@0:8^{?=^vi}16@24@32B40^@44
@36@0:8^{vImage_Buffer=^vQQQ}16i24^@28
^{CGImage=}32@0:8@16^@24
{vImage_Buffer=^vQQQ}32@0:8@16^@24
v64@0:8@16^{__CVBuffer=}24{_Geometry2D_rect2D_={_Geometry2D_point2D_=ff}{_Geometry2D_size2D_=ff}}32r^{vector<_Geometry2D_point2D_, std::__1::allocator<_Geometry2D_point2D_> >=^{_Geometry2D_point2D_}^{_Geometry2D_point2D_}{__compressed_pair<_Geometry2D_point2D_ *, std::__1::allocator<_Geometry2D_point2D_> >=^{_Geometry2D_point2D_}}}48r^{vector<_Geometry2D_point2D_, std::__1::allocator<_Geometry2D_point2D_> >=^{_Geometry2D_point2D_}^{_Geometry2D_point2D_}{__compressed_pair<_Geometry2D_point2D_ *, std::__1::allocator<_Geometry2D_point2D_> >=^{_Geometry2D_point2D_}}}56
{shared_ptr<vision::mod::LandmarkDetectorERT>="__ptr_"^{LandmarkDetectorERT}"__cntrl_"^{__shared_weak_count}}
@"LKTMetalContext"
[9@"<MTLComputePipelineState>"]
[10{CGSize="width"d"height"d}]
[2[10@"<MTLTexture>"]]
[10@"<MTLTexture>"]
[2@"<MTLBuffer>"]
[2^{__CVBuffer}]
@"<MTLTexture>"
@44@0:8@16i24i28i32^@36
B32@0:8^{__CVBuffer=}16^@24
i40@0:8^{__CVBuffer=}16^{__CVBuffer=}24^@32
i32@0:8^{__CVBuffer=}16^@24
B36@0:8i16i20i24^@28
B44@0:8@16^{__CVBuffer=}24i32^@36
i32@0:8@16@24
i40@0:8@16@24@32
i68@0:8@16i242836@44@52@60
i56@0:8@16@24@32@40@48
{CGSize=dd}20@0:8i16
B84@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{CGSize=dd}48i64^{CGRect={CGPoint=dd}{CGSize=dd}}68^@76
v72@0:8@16^{vector<vision::mod::DetectedObject, std::__1::allocator<vision::mod::DetectedObject> >=^{DetectedObject}^{DetectedObject}{__compressed_pair<vision::mod::DetectedObject *, std::__1::allocator<vision::mod::DetectedObject> >=^{DetectedObject}}}24^{__CVBuffer=}32{vImage_Buffer=^vQQQ}40
24@0:8Q16
r^{CGPoint=dd}16@0:8
r^{CGPoint=dd}32@0:8{CGSize=dd}16
@88@0:8Q16{CGRect={CGPoint=dd}{CGSize=dd}}24^56Q64@72@80
r^16@0:8
@72@0:8Q16{CGRect={CGPoint=dd}{CGSize=dd}}24^56Q64
v24@0:8r^16
^v32@0:8r^i16Q24
@92@0:8Q16@24Q32{CGRect={CGPoint=dd}{CGSize=dd}}40{_Geometry2D_rect2D_={_Geometry2D_point2D_=ff}{_Geometry2D_size2D_=ff}}72f88
@"VNFaceLandmarkRegion2D"
@116@0:8Q16@24Q32Q40@48@56{CGRect={CGPoint=dd}{CGSize=dd}}64{_Geometry2D_rect2D_={_Geometry2D_point2D_=ff}{_Geometry2D_size2D_=ff}}96f112
@32@0:8r^i16Q24
@40@0:8@16r^i24Q32
@"VNFaceLandmarkRegion3D"
B48@0:8Q16Q24@?32^@40
B64@0:8@16@24#32Q40Q48^@56
B48@0:8@16Q24@32^@40
B48@0:8^B16@24@32^@40
B52@0:8^B16@24@32B40^@44
B48@0:8^Q16@24@32^@40
B56@0:8^Q16@24@32Q40^@48
B72@0:8^@16@24@32#40Q48Q56^@64
@48@0:8@16@24#32^@40
B40@0:8^Q16@24^@32
B56@0:8^Q16#24@32@40^@48
B48@0:8^Q16#24@32^@40
C16@0:8
@24@0:8r^*16
v20@0:8I16
@"MPImageDescriptor_LegacySupportDoNotChange"
@"VNSceneprint"
@"<MLFeatureProvider>"
@"MLFeatureValue"24@0:8@"NSString"16
@"NSSet"16@0:8
@40@0:8^{__CVBuffer=}16@24@32
@48@0:8@16^q24@32@40
@"MLModel"
@"MLObjectBoundingBoxOutputDescription"
v32@0:8q16q24
@40@0:8^{__CVBuffer=}16@24^@32
@"VNCoreMLModel"
^{?=ff[9{?=ff}][9{?=ff}]}
@32@0:8@16^{?=ff[9{?=ff}][9{?=ff}]}24
r^{?=ff[9{?=ff}][9{?=ff}]}32@0:8@16^@24
B132@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24^{ImageDescriptorProcessorAbstract=^^?}56i64I68B72^{ImageDescriptorBufferAbstract=^^?{vector<long long, std::__1::allocator<long long> >=^q^q{__compressed_pair<long long *, std::__1::allocator<long long> >=^q}}{map<long long, int, std::__1::less<long long>, std::__1::allocator<std::__1::pair<const long long, int> > >={__tree<std::__1::__value_type<long long, int>, std::__1::__map_value_compare<long long, std::__1::__value_type<long long, int>, std::__1::less<long long>, true>, std::__1::allocator<std::__1::__value_type<long long, int> > >=^{__tree_end_node<std::__1::__tree_node_base<void *> *>}{__compressed_pair<std::__1::__tree_end_node<std::__1::__tree_node_base<void *> *>, std::__1::allocator<std::__1::__tree_node<std::__1::__value_type<long long, int>, void *> > >={__tree_end_node<std::__1::__tree_node_base<void *> *>=^{__tree_node_base<void *>}}}{__compressed_pair<unsigned long, std::__1::__map_value_compare<long long, std::__1::__value_type<long long, int>, std::__1::less<long long>, true> >=Q}}}^vQQQB}76@84@92@100@108@116^@124
B64@0:8^{ImageClassifierAbstract=^^?{vector<std::__1::basic_string<char>, std::__1::allocator<std::__1::basic_string<char> > >=^{basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >}^{basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >}{__compressed_pair<std::__1::basic_string<char> *, std::__1::allocator<std::__1::basic_string<char> > >=^{basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >}}}{unordered_map<std::__1::basic_string<char>, float, std::__1::hash<std::__1::basic_string<char> >, std::__1::equal_to<std::__1::basic_string<char> >, std::__1::allocator<std::__1::pair<const std::__1::basic_string<char>, float> > >={__hash_table<std::__1::__hash_value_type<std::__1::basic_string<char>, float>, std::__1::__unordered_map_hasher<std::__1::basic_string<char>, std::__1::__hash_value_type<std::__1::basic_string<char>, float>, std::__1::hash<std::__1::basic_string<char> >, true>, std::__1::__unordered_map_equal<std::__1::basic_string<char>, std::__1::__hash_value_type<std::__1::basic_string<char>, float>, std::__1::equal_to<std::__1::basic_string<char> >, true>, std::__1::allocator<std::__1::__hash_value_type<std::__1::basic_string<char>, float> > >={unique_ptr<std::__1::__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char>, float>, void *> *> *[], std::__1::__bucket_list_deallocator<std::__1::allocator<std::__1::__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char>, float>, void *> *> *> > >={__compressed_pair<std::__1::__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char>, float>, void *> *> **, std::__1::__bucket_list_deallocator<std::__1::allocator<std::__1::__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char>, float>, void *> *> *> > >=^^{__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char>, float>, void *> *>}{__bucket_list_deallocator<std::__1::allocator<std::__1::__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char>, float>, void *> *> *> >={__compressed_pair<unsigned long, std::__1::allocator<std::__1::__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char>, float>, void *> *> *> >=Q}}}}{__compressed_pair<std::__1::__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char>, float>, void *> *>, std::__1::allocator<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char>, float>, void *> > >={__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char>, float>, void *> *>=^{__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char>, float>, void *> *>}}}{__compressed_pair<unsigned long, std::__1::__unordered_map_hasher<std::__1::basic_string<char>, std::__1::__hash_value_type<std::__1::basic_string<char>, float>, std::__1::hash<std::__1::basic_string<char> >, true> >=Q}{__compressed_pair<float, std::__1::__unordered_map_equal<std::__1::basic_string<char>, std::__1::__hash_value_type<std::__1::basic_string<char>, float>, std::__1::equal_to<std::__1::basic_string<char> >, true> >=f}}}{vector<std::__1::basic_string<char>, std::__1::allocator<std::__1::basic_string<char> > >=^{basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >}^{basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >}{__compressed_pair<std::__1::basic_string<char> *, std::__1::allocator<std::__1::basic_string<char> > >=^{basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >}}}iffii}16^{ImageDescriptorBufferAbstract=^^?{vector<long long, std::__1::allocator<long long> >=^q^q{__compressed_pair<long long *, std::__1::allocator<long long> >=^q}}{map<long long, int, std::__1::less<long long>, std::__1::allocator<std::__1::pair<const long long, int> > >={__tree<std::__1::__value_type<long long, int>, std::__1::__map_value_compare<long long, std::__1::__value_type<long long, int>, std::__1::less<long long>, true>, std::__1::allocator<std::__1::__value_type<long long, int> > >=^{__tree_end_node<std::__1::__tree_node_base<void *> *>}{__compressed_pair<std::__1::__tree_end_node<std::__1::__tree_node_base<void *> *>, std::__1::allocator<std::__1::__tree_node<std::__1::__value_type<long long, int>, void *> > >={__tree_end_node<std::__1::__tree_node_base<void *> *>=^{__tree_node_base<void *>}}}{__compressed_pair<unsigned long, std::__1::__map_value_compare<long long, std::__1::__value_type<long long, int>, std::__1::less<long long>, true> >=Q}}}^vQQQB}24^{vector<std::__1::unordered_map<std::__1::basic_string<char>, float, std::__1::hash<std::__1::basic_string<char> >, std::__1::equal_to<std::__1::basic_string<char> >, std::__1::allocator<std::__1::pair<const std::__1::basic_string<char>, float> > >, std::__1::allocator<std::__1::unordered_map<std::__1::basic_string<char>, float, std::__1::hash<std::__1::basic_string<char> >, std::__1::equal_to<std::__1::basic_string<char> >, std::__1::allocator<std::__1::pair<const std::__1::basic_string<char>, float> > > > >=^{unordered_map<std::__1::basic_string<char>, float, std::__1::hash<std::__1::basic_string<char> >, std::__1::equal_to<std::__1::basic_string<char> >, std::__1::allocator<std::__1::pair<const std::__1::basic_string<char>, float> > >}^{unordered_map<std::__1::basic_string<char>, float, std::__1::hash<std::__1::basic_string<char> >, std::__1::equal_to<std::__1::basic_string<char> >, std::__1::allocator<std::__1::pair<const std::__1::basic_string<char>, float> > >}{__compressed_pair<std::__1::unordered_map<std::__1::basic_string<char>, float, std::__1::hash<std::__1::basic_string<char> >, std::__1::equal_to<std::__1::basic_string<char> >, std::__1::allocator<std::__1::pair<const std::__1::basic_string<char>, float> > > *, std::__1::allocator<std::__1::unordered_map<std::__1::basic_string<char>, float, std::__1::hash<std::__1::basic_string<char> >, std::__1::equal_to<std::__1::basic_string<char> >, std::__1::allocator<std::__1::pair<const std::__1::basic_string<char>, float> > > > >=^{unordered_map<std::__1::basic_string<char>, float, std::__1::hash<std::__1::basic_string<char> >, std::__1::equal_to<std::__1::basic_string<char> >, std::__1::allocator<std::__1::pair<const std::__1::basic_string<char>, float> > >}}}32@40@48^@56
@68@0:8r^{ImageDescriptorBufferAbstract=^^?{vector<long long, std::__1::allocator<long long> >=^q^q{__compressed_pair<long long *, std::__1::allocator<long long> >=^q}}{map<long long, int, std::__1::less<long long>, std::__1::allocator<std::__1::pair<const long long, int> > >={__tree<std::__1::__value_type<long long, int>, std::__1::__map_value_compare<long long, std::__1::__value_type<long long, int>, std::__1::less<long long>, true>, std::__1::allocator<std::__1::__value_type<long long, int> > >=^{__tree_end_node<std::__1::__tree_node_base<void *> *>}{__compressed_pair<std::__1::__tree_end_node<std::__1::__tree_node_base<void *> *>, std::__1::allocator<std::__1::__tree_node<std::__1::__value_type<long long, int>, void *> > >={__tree_end_node<std::__1::__tree_node_base<void *> *>=^{__tree_node_base<void *>}}}{__compressed_pair<unsigned long, std::__1::__map_value_compare<long long, std::__1::__value_type<long long, int>, std::__1::less<long long>, true> >=Q}}}^vQQQB}16^{ImageClassifierAbstract=^^?{vector<std::__1::basic_string<char>, std::__1::allocator<std::__1::basic_string<char> > >=^{basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >}^{basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >}{__compressed_pair<std::__1::basic_string<char> *, std::__1::allocator<std::__1::basic_string<char> > >=^{basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >}}}{unordered_map<std::__1::basic_string<char>, float, std::__1::hash<std::__1::basic_string<char> >, std::__1::equal_to<std::__1::basic_string<char> >, std::__1::allocator<std::__1::pair<const std::__1::basic_string<char>, float> > >={__hash_table<std::__1::__hash_value_type<std::__1::basic_string<char>, float>, std::__1::__unordered_map_hasher<std::__1::basic_string<char>, std::__1::__hash_value_type<std::__1::basic_string<char>, float>, std::__1::hash<std::__1::basic_string<char> >, true>, std::__1::__unordered_map_equal<std::__1::basic_string<char>, std::__1::__hash_value_type<std::__1::basic_string<char>, float>, std::__1::equal_to<std::__1::basic_string<char> >, true>, std::__1::allocator<std::__1::__hash_value_type<std::__1::basic_string<char>, float> > >={unique_ptr<std::__1::__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char>, float>, void *> *> *[], std::__1::__bucket_list_deallocator<std::__1::allocator<std::__1::__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char>, float>, void *> *> *> > >={__compressed_pair<std::__1::__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char>, float>, void *> *> **, std::__1::__bucket_list_deallocator<std::__1::allocator<std::__1::__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char>, float>, void *> *> *> > >=^^{__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char>, float>, void *> *>}{__bucket_list_deallocator<std::__1::allocator<std::__1::__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char>, float>, void *> *> *> >={__compressed_pair<unsigned long, std::__1::allocator<std::__1::__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char>, float>, void *> *> *> >=Q}}}}{__compressed_pair<std::__1::__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char>, float>, void *> *>, std::__1::allocator<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char>, float>, void *> > >={__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char>, float>, void *> *>=^{__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char>, float>, void *> *>}}}{__compressed_pair<unsigned long, std::__1::__unordered_map_hasher<std::__1::basic_string<char>, std::__1::__hash_value_type<std::__1::basic_string<char>, float>, std::__1::hash<std::__1::basic_string<char> >, true> >=Q}{__compressed_pair<float, std::__1::__unordered_map_equal<std::__1::basic_string<char>, std::__1::__hash_value_type<std::__1::basic_string<char>, float>, std::__1::equal_to<std::__1::basic_string<char> >, true> >=f}}}{vector<std::__1::basic_string<char>, std::__1::allocator<std::__1::basic_string<char> > >=^{basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >}^{basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >}{__compressed_pair<std::__1::basic_string<char> *, std::__1::allocator<std::__1::basic_string<char> > >=^{basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >}}}iffii}24f32@36@44@52^@60
@96@0:8r^{ImageDescriptorBufferAbstract=^^?{vector<long long, std::__1::allocator<long long> >=^q^q{__compressed_pair<long long *, std::__1::allocator<long long> >=^q}}{map<long long, int, std::__1::less<long long>, std::__1::allocator<std::__1::pair<const long long, int> > >={__tree<std::__1::__value_type<long long, int>, std::__1::__map_value_compare<long long, std::__1::__value_type<long long, int>, std::__1::less<long long>, true>, std::__1::allocator<std::__1::__value_type<long long, int> > >=^{__tree_end_node<std::__1::__tree_node_base<void *> *>}{__compressed_pair<std::__1::__tree_end_node<std::__1::__tree_node_base<void *> *>, std::__1::allocator<std::__1::__tree_node<std::__1::__value_type<long long, int>, void *> > >={__tree_end_node<std::__1::__tree_node_base<void *> *>=^{__tree_node_base<void *>}}}{__compressed_pair<unsigned long, std::__1::__map_value_compare<long long, std::__1::__value_type<long long, int>, std::__1::less<long long>, true> >=Q}}}^vQQQB}16^{ImageClassifierAbstract=^^?{vector<std::__1::basic_string<char>, std::__1::allocator<std::__1::basic_string<char> > >=^{basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >}^{basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >}{__compressed_pair<std::__1::basic_string<char> *, std::__1::allocator<std::__1::basic_string<char> > >=^{basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >}}}{unordered_map<std::__1::basic_string<char>, float, std::__1::hash<std::__1::basic_string<char> >, std::__1::equal_to<std::__1::basic_string<char> >, std::__1::allocator<std::__1::pair<const std::__1::basic_string<char>, float> > >={__hash_table<std::__1::__hash_value_type<std::__1::basic_string<char>, float>, std::__1::__unordered_map_hasher<std::__1::basic_string<char>, std::__1::__hash_value_type<std::__1::basic_string<char>, float>, std::__1::hash<std::__1::basic_string<char> >, true>, std::__1::__unordered_map_equal<std::__1::basic_string<char>, std::__1::__hash_value_type<std::__1::basic_string<char>, float>, std::__1::equal_to<std::__1::basic_string<char> >, true>, std::__1::allocator<std::__1::__hash_value_type<std::__1::basic_string<char>, float> > >={unique_ptr<std::__1::__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char>, float>, void *> *> *[], std::__1::__bucket_list_deallocator<std::__1::allocator<std::__1::__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char>, float>, void *> *> *> > >={__compressed_pair<std::__1::__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char>, float>, void *> *> **, std::__1::__bucket_list_deallocator<std::__1::allocator<std::__1::__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char>, float>, void *> *> *> > >=^^{__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char>, float>, void *> *>}{__bucket_list_deallocator<std::__1::allocator<std::__1::__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char>, float>, void *> *> *> >={__compressed_pair<unsigned long, std::__1::allocator<std::__1::__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char>, float>, void *> *> *> >=Q}}}}{__compressed_pair<std::__1::__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char>, float>, void *> *>, std::__1::allocator<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char>, float>, void *> > >={__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char>, float>, void *> *>=^{__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char>, float>, void *> *>}}}{__compressed_pair<unsigned long, std::__1::__unordered_map_hasher<std::__1::basic_string<char>, std::__1::__hash_value_type<std::__1::basic_string<char>, float>, std::__1::hash<std::__1::basic_string<char> >, true> >=Q}{__compressed_pair<float, std::__1::__unordered_map_equal<std::__1::basic_string<char>, std::__1::__hash_value_type<std::__1::basic_string<char>, float>, std::__1::equal_to<std::__1::basic_string<char> >, true> >=f}}}{vector<std::__1::basic_string<char>, std::__1::allocator<std::__1::basic_string<char> > >=^{basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >}^{basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >}{__compressed_pair<std::__1::basic_string<char> *, std::__1::allocator<std::__1::basic_string<char> > >=^{basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >}}}iffii}24^{ImageClassifier_HierarchicalModel=^{ImageClassfier_Graph}}32f40f44Q48Q56@64@72@80^@88
{vector<float, std::__1::allocator<float> >="__begin_"^f"__end_"^f"__end_cap_"{__compressed_pair<float *, std::__1::allocator<float> >="__value_"^f}}
@96@0:8Q16Q24Q32@40Q48{CGRect={CGPoint=dd}{CGSize=dd}}56@88
^{__CVBuffer=}32@0:8Q16^@24
^{__CVBuffer=}68@0:8Q16{CGRect={CGPoint=dd}{CGSize=dd}}24B56^@60
{vImage_Buffer=^vQQQ}56@0:8@16{CGRect={CGPoint=dd}{CGSize=dd}}24
^{__CVBuffer=}40@0:8{CGSize=dd}16^@32
@56@0:8^{__CVBuffer=}16@24@32@40@?48
f36@0:8@16^i24i32
r^{mapped_model_file=i^vQ}
r^v16@0:8
v24@0:8@"NSString"16
@24@0:8r^{mapped_model_file=i^vQ}16
{unique_ptr<cvml::util::model_file_cache, std::__1::default_delete<cvml::util::model_file_cache> >="__ptr_"{__compressed_pair<cvml::util::model_file_cache *, std::__1::default_delete<cvml::util::model_file_cache> >="__value_"^{model_file_cache}}}
B48@0:8@16@24@32^@40
f24@0:8f16f20
B20@0:8f16
v40@0:8@16@24@32
@"BurstImageSetInternal"
{shared_ptr<vision::mod::ImageDescriptorAugmenterFlip>="__ptr_"^{ImageDescriptorAugmenterFlip}"__cntrl_"^{__shared_weak_count}}
{shared_ptr<unsigned char>="__ptr_"*"__cntrl_"^{__shared_weak_count}}
v112@0:8^{__CVBuffer=}16@24{vImage_Buffer=^vQQQ}32{CGRect={CGPoint=dd}{CGSize=dd}}64{_Geometry2D_rect2D_={_Geometry2D_point2D_=ff}{_Geometry2D_size2D_=ff}}96
@"_VNImageAnalyzerMultiDetectorSceneOperationPointsCache"
{unique_ptr<vision::mod::ImageAnalyzer, std::__1::default_delete<vision::mod::ImageAnalyzer> >="__ptr_"{__compressed_pair<vision::mod::ImageAnalyzer *, std::__1::default_delete<vision::mod::ImageAnalyzer> >="__value_"^{ImageAnalyzer}}}
{unordered_map<unsigned long, std::__1::shared_ptr<std::__1::vector<std::__1::shared_ptr<vision::mod::ImageAnalyzer_CustomClassifier>, std::__1::allocator<std::__1::shared_ptr<vision::mod::ImageAnalyzer_CustomClassifier> > > >, std::__1::hash<unsigned long>, std::__1::equal_to<unsigned long>, std::__1::allocator<std::__1::pair<const unsigned long, std::__1::shared_ptr<std::__1::vector<std::__1::shared_ptr<vision::mod::ImageAnalyzer_CustomClassifier>, std::__1::allocator<std::__1::shared_ptr<vision::mod::ImageAnalyzer_CustomClassifier> > > > > > >="__table_"{__hash_table<std::__1::__hash_value_type<unsigned long, std::__1::shared_ptr<std::__1::vector<std::__1::shared_ptr<vision::mod::ImageAnalyzer_CustomClassifier>, std::__1::allocator<std::__1::shared_ptr<vision::mod::ImageAnalyzer_CustomClassifier> > > > >, std::__1::__unordered_map_hasher<unsigned long, std::__1::__hash_value_type<unsigned long, std::__1::shared_ptr<std::__1::vector<std::__1::shared_ptr<vision::mod::ImageAnalyzer_CustomClassifier>, std::__1::allocator<std::__1::shared_ptr<vision::mod::ImageAnalyzer_CustomClassifier> > > > >, std::__1::hash<unsigned long>, true>, std::__1::__unordered_map_equal<unsigned long, std::__1::__hash_value_type<unsigned long, std::__1::shared_ptr<std::__1::vector<std::__1::shared_ptr<vision::mod::ImageAnalyzer_CustomClassifier>, std::__1::allocator<std::__1::shared_ptr<vision::mod::ImageAnalyzer_CustomClassifier> > > > >, std::__1::equal_to<unsigned long>, true>, std::__1::allocator<std::__1::__hash_value_type<unsigned long, std::__1::shared_ptr<std::__1::vector<std::__1::shared_ptr<vision::mod::ImageAnalyzer_CustomClassifier>, std::__1::allocator<std::__1::shared_ptr<vision::mod::ImageAnalyzer_CustomClassifier> > > > > > >="__bucket_list_"{unique_ptr<std::__1::__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<unsigned long, std::__1::shared_ptr<std::__1::vector<std::__1::shared_ptr<vision::mod::ImageAnalyzer_CustomClassifier>, std::__1::allocator<std::__1::shared_ptr<vision::mod::ImageAnalyzer_CustomClassifier> > > > >, void *> *> *[], std::__1::__bucket_list_deallocator<std::__1::allocator<std::__1::__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<unsigned long, std::__1::shared_ptr<std::__1::vector<std::__1::shared_ptr<vision::mod::ImageAnalyzer_CustomClassifier>, std::__1::allocator<std::__1::shared_ptr<vision::mod::ImageAnalyzer_CustomClassifier> > > > >, void *> *> *> > >="__ptr_"{__compressed_pair<std::__1::__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<unsigned long, std::__1::shared_ptr<std::__1::vector<std::__1::shared_ptr<vision::mod::ImageAnalyzer_CustomClassifier>, std::__1::allocator<std::__1::shared_ptr<vision::mod::ImageAnalyzer_CustomClassifier> > > > >, void *> *> **, std::__1::__bucket_list_deallocator<std::__1::allocator<std::__1::__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<unsigned long, std::__1::shared_ptr<std::__1::vector<std::__1::shared_ptr<vision::mod::ImageAnalyzer_CustomClassifier>, std::__1::allocator<std::__1::shared_ptr<vision::mod::ImageAnalyzer_CustomClassifier> > > > >, void *> *> *> > >="__value_"^^{__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<unsigned long, std::__1::shared_ptr<std::__1::vector<std::__1::shared_ptr<vision::mod::ImageAnalyzer_CustomClassifier>, std::__1::allocator<std::__1::shared_ptr<vision::mod::ImageAnalyzer_CustomClassifier> > > > >, void *> *>}"__value_"{__bucket_list_deallocator<std::__1::allocator<std::__1::__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<unsigned long, std::__1::shared_ptr<std::__1::vector<std::__1::shared_ptr<vision::mod::ImageAnalyzer_CustomClassifier>, std::__1::allocator<std::__1::shared_ptr<vision::mod::ImageAnalyzer_CustomClassifier> > > > >, void *> *> *> >="__data_"{__compressed_pair<unsigned long, std::__1::allocator<std::__1::__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<unsigned long, std::__1::shared_ptr<std::__1::vector<std::__1::shared_ptr<vision::mod::ImageAnalyzer_CustomClassifier>, std::__1::allocator<std::__1::shared_ptr<vision::mod::ImageAnalyzer_CustomClassifier> > > > >, void *> *> *> >="__value_"Q}}}}"__p1_"{__compressed_pair<std::__1::__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<unsigned long, std::__1::shared_ptr<std::__1::vector<std::__1::shared_ptr<vision::mod::ImageAnalyzer_CustomClassifier>, std::__1::allocator<std::__1::shared_ptr<vision::mod::ImageAnalyzer_CustomClassifier> > > > >, void *> *>, std::__1::allocator<std::__1::__hash_node<std::__1::__hash_value_type<unsigned long, std::__1::shared_ptr<std::__1::vector<std::__1::shared_ptr<vision::mod::ImageAnalyzer_CustomClassifier>, std::__1::allocator<std::__1::shared_ptr<vision::mod::ImageAnalyzer_CustomClassifier> > > > >, void *> > >="__value_"{__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<unsigned long, std::__1::shared_ptr<std::__1::vector<std::__1::shared_ptr<vision::mod::ImageAnalyzer_CustomClassifier>, std::__1::allocator<std::__1::shared_ptr<vision::mod::ImageAnalyzer_CustomClassifier> > > > >, void *> *>="__next_"^{__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<unsigned long, std::__1::shared_ptr<std::__1::vector<std::__1::shared_ptr<vision::mod::ImageAnalyzer_CustomClassifier>, std::__1::allocator<std::__1::shared_ptr<vision::mod::ImageAnalyzer_CustomClassifier> > > > >, void *> *>}}}"__p2_"{__compressed_pair<unsigned long, std::__1::__unordered_map_hasher<unsigned long, std::__1::__hash_value_type<unsigned long, std::__1::shared_ptr<std::__1::vector<std::__1::shared_ptr<vision::mod::ImageAnalyzer_CustomClassifier>, std::__1::allocator<std::__1::shared_ptr<vision::mod::ImageAnalyzer_CustomClassifier> > > > >, std::__1::hash<unsigned long>, true> >="__value_"Q}"__p3_"{__compressed_pair<float, std::__1::__unordered_map_equal<unsigned long, std::__1::__hash_value_type<unsigned long, std::__1::shared_ptr<std::__1::vector<std::__1::shared_ptr<vision::mod::ImageAnalyzer_CustomClassifier>, std::__1::allocator<std::__1::shared_ptr<vision::mod::ImageAnalyzer_CustomClassifier> > > > >, std::__1::equal_to<unsigned long>, true> >="__value_"f}}}
{unique_ptr<vision::mod::ImageAnalyzer_PCA, std::__1::default_delete<vision::mod::ImageAnalyzer_PCA> >="__ptr_"{__compressed_pair<vision::mod::ImageAnalyzer_PCA *, std::__1::default_delete<vision::mod::ImageAnalyzer_PCA> >="__value_"^{ImageAnalyzer_PCA}}}
{shared_ptr<vision::mod::ImageAnalyzer_CustomClassifier>="__ptr_"^{ImageAnalyzer_CustomClassifier}"__cntrl_"^{__shared_weak_count}}
{vector<std::__1::tuple<std::__1::basic_string<char>, float, bool>, std::__1::allocator<std::__1::tuple<std::__1::basic_string<char>, float, bool> > >="__begin_"^{tuple<std::__1::basic_string<char>, float, bool>}"__end_"^{tuple<std::__1::basic_string<char>, float, bool>}"__end_cap_"{__compressed_pair<std::__1::tuple<std::__1::basic_string<char>, float, bool> *, std::__1::allocator<std::__1::tuple<std::__1::basic_string<char>, float, bool> > >="__value_"^{tuple<std::__1::basic_string<char>, float, bool>}}}
v40@0:8^Q16^{_Geometry2D_size2D_=ff}24@32
^{__CVBuffer=}56@0:8@16{_Geometry2D_size2D_=ff}24Q32@40^@48
v80@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16B48B52d56d64@?72
I48@0:8B16B20B24B28B32B36B40B44
{shared_ptr<vision::mod::ImageAnalyzer_CustomClassifier>=^{ImageAnalyzer_CustomClassifier}^{__shared_weak_count}}80@0:8@16@24@32@40@48i56i60i64i68^@72
{shared_ptr<std::__1::vector<std::__1::shared_ptr<vision::mod::ImageAnalyzer_CustomClassifier>, std::__1::allocator<std::__1::shared_ptr<vision::mod::ImageAnalyzer_CustomClassifier> > > >=^{vector<std::__1::shared_ptr<vision::mod::ImageAnalyzer_CustomClassifier>, std::__1::allocator<std::__1::shared_ptr<vision::mod::ImageAnalyzer_CustomClassifier> > >}^{__shared_weak_count}}40@0:8Q16@24^@32
{shared_ptr<vision::mod::ImageAnalyzer_CustomClassifier>=^{ImageAnalyzer_CustomClassifier}^{__shared_weak_count}}40@0:8Q16@24^@32
^{ImageAnalyzer_PCA={vector<float, std::__1::allocator<float> >=^f^f{__compressed_pair<float *, std::__1::allocator<float> >=^f}}QQ}32@0:8Q16^@24
@44@0:8B16B20B24@28^@36
@88@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{CGSize=dd}48@64@72^@80
B36@0:8I16@?20^@28
^{vector<std::__1::tuple<std::__1::basic_string<char>, float, bool>, std::__1::allocator<std::__1::tuple<std::__1::basic_string<char>, float, bool> > >=^{tuple<std::__1::basic_string<char>, float, bool>}^{tuple<std::__1::basic_string<char>, float, bool>}{__compressed_pair<std::__1::tuple<std::__1::basic_string<char>, float, bool> *, std::__1::allocator<std::__1::tuple<std::__1::basic_string<char>, float, bool> > >=^{tuple<std::__1::basic_string<char>, float, bool>}}}16@0:8
@56@0:8r^Q16f24B28@32Q40@48
@60@0:8r^Q16r^{unordered_map<std::__1::basic_string<char>, float, std::__1::hash<std::__1::basic_string<char> >, std::__1::equal_to<std::__1::basic_string<char> >, std::__1::allocator<std::__1::pair<const std::__1::basic_string<char>, float> > >={__hash_table<std::__1::__hash_value_type<std::__1::basic_string<char>, float>, std::__1::__unordered_map_hasher<std::__1::basic_string<char>, std::__1::__hash_value_type<std::__1::basic_string<char>, float>, std::__1::hash<std::__1::basic_string<char> >, true>, std::__1::__unordered_map_equal<std::__1::basic_string<char>, std::__1::__hash_value_type<std::__1::basic_string<char>, float>, std::__1::equal_to<std::__1::basic_string<char> >, true>, std::__1::allocator<std::__1::__hash_value_type<std::__1::basic_string<char>, float> > >={unique_ptr<std::__1::__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char>, float>, void *> *> *[], std::__1::__bucket_list_deallocator<std::__1::allocator<std::__1::__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char>, float>, void *> *> *> > >={__compressed_pair<std::__1::__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char>, float>, void *> *> **, std::__1::__bucket_list_deallocator<std::__1::allocator<std::__1::__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char>, float>, void *> *> *> > >=^^{__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char>, float>, void *> *>}{__bucket_list_deallocator<std::__1::allocator<std::__1::__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char>, float>, void *> *> *> >={__compressed_pair<unsigned long, std::__1::allocator<std::__1::__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char>, float>, void *> *> *> >=Q}}}}{__compressed_pair<std::__1::__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char>, float>, void *> *>, std::__1::allocator<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char>, float>, void *> > >={__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char>, float>, void *> *>=^{__hash_node_base<std::__1::__hash_node<std::__1::__hash_value_type<std::__1::basic_string<char>, float>, void *> *>}}}{__compressed_pair<unsigned long, std::__1::__unordered_map_hasher<std::__1::basic_string<char>, std::__1::__hash_value_type<std::__1::basic_string<char>, float>, std::__1::hash<std::__1::basic_string<char> >, true> >=Q}{__compressed_pair<float, std::__1::__unordered_map_equal<std::__1::basic_string<char>, std::__1::__hash_value_type<std::__1::basic_string<char>, float>, std::__1::equal_to<std::__1::basic_string<char> >, true> >=f}}}24f32@36Q44@52
@128@0:8B16B20B24B28B32B36B40B44@?48@56{CGSize=dd}64{CGRect={CGPoint=dd}{CGSize=dd}}80@112^@120
B56@0:8^@16^@24@?32@40^@48
^{__CVBuffer=}88@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16@48{_Geometry2D_size2D_=ff}56Q64@72^@80
Q40@0:8#16Q24Q32
{shared_ptr<vision::mod::ImageClassifier_HierarchicalModel>=^{ImageClassifier_HierarchicalModel}^{__shared_weak_count}}40@0:8Q16Q24^@32
v28@0:8Q16I24
I28@0:8Q16I24
v36@0:8Q16Q24I32
*16@0:8
v24@0:8*16
^Q16@0:8
v24@0:8^Q16
v24@0:8^S16
S16@0:8
v20@0:8S16
s16@0:8
v20@0:8s16
@"CCCharBoxContext"
i48@0:8Q16Q24Q32^{vImage_Buffer=^vQQQ}40
i88@0:8{vImage_Buffer=^vQQQ}16{vImage_Buffer=^vQQQ}48*80
v56@0:8S16^f20^{__CCPulseWindowContext=^{__CCRange}SSsB}28C36C40{ThresholdSet_t=fff}44
i56@0:8S16C20C24{ThresholdSet_t=fff}28^f40Q48
v48@0:8^{__CCRange=SS}16*24^i32^i40
i28@0:8^{__CCSumDerivVectors=^f^f^f^f^fffii}16i24
v24@0:8^{__CCSumDerivVectors=^f^f^f^f^fffii}16
v32@0:8^{__CCSumDerivVectors=^f^f^f^f^fffii}16i24C28
i72@0:8C16^f20S28C32C36^{__CCFilterSumDerivConfig={__CCRange=SS}{__CCRange=SS}BBQQ}40S48{ThresholdSet_t=fff}52*64
i96@0:8^f16S24S28C32C36S40S44I48I52S56{ThresholdSet_t=fff}60*72^f80Q88
v52@0:8Q16Q24Q32C40S44S48
v40@0:8Q16Q24S32S36
v44@0:8Q16^S24C32S36S40
i56@0:8^{__rgbaColor=CCCC}16^{__rgbaColor=CCCC}24I32I36^{__rgbMinMaxU8=CCCCCC}40^{__rgbMinMaxFloat=ffffff}48
I92@0:8{vImage_Buffer=^vQQQ}16{vImage_Buffer=^vQQQ}48C80^{__rgbMinMaxFloat=ffffff}84
f88@0:8{vImage_Buffer=^vQQQ}16{vImage_Buffer=^vQQQ}48S80S84
v64@0:8{vImage_Buffer=^vQQQ}16^{__CCColorProfileContext={vImage_Buffer=^vQQQ}{vImage_Buffer=^vQQQ}{vImage_Buffer=^vQQQ}{vImage_Buffer=^vQQQ}iiSS}48S56S60
v60@0:8{vImage_Buffer=^vQQQ}16S48^{__CCColorProfileContext={vImage_Buffer=^vQQQ}{vImage_Buffer=^vQQQ}{vImage_Buffer=^vQQQ}{vImage_Buffer=^vQQQ}iiSS}52
S40@0:8^f16Q24^S32
i72@0:8{vImage_Buffer=^vQQQ}16S48^S52f60^S64
v24@0:8^{__CCColorProfileContext={vImage_Buffer=^vQQQ}{vImage_Buffer=^vQQQ}{vImage_Buffer=^vQQQ}{vImage_Buffer=^vQQQ}iiSS}16
i40@0:8^{__CCColorProfileContext={vImage_Buffer=^vQQQ}{vImage_Buffer=^vQQQ}{vImage_Buffer=^vQQQ}{vImage_Buffer=^vQQQ}iiSS}16S24S28Q32
i152@0:8{vImage_Buffer=^vQQQ}16{vImage_Buffer=^vQQQ}48{vImage_Buffer=^vQQQ}80^{__rgbMinMaxU8=CCCCCC}112^{__rgbMinMaxFloat=ffffff}120S128S132S136^I140C148
i84@0:8{vImage_Buffer=^vQQQ}16{vImage_Buffer=^vQQQ}48C80
i80@0:8{vImage_Buffer=^vQQQ}16{vImage_Buffer=^vQQQ}48
i92@0:8@16Q24{vImage_Buffer=^vQQQ}32^Q64^{__CCBigBox=SSSS[50{__CCCharBox=SSSSS}]}72^{__CCBigBox=SSSS[50{__CCCharBox=SSSSS}]}80C88
v72@0:8@16^{__CCBigBox=SSSS[50{__CCCharBox=SSSSS}]}24{vImage_Buffer=^vQQQ}32C64S68
v32@0:8^{__CCBox=SSSS}16^Q24
i32@0:8^{__CCBox=SSSS}16^Q24
i56@0:8*16S24S28Q32Q40S48S52
{__CCRange=SS}76@0:8{vImage_Buffer=^vQQQ}16S48S52S56S60Q64S72
v156@0:8{vImage_Buffer=^vQQQ}16*48{vImage_Buffer=^vQQQ}56{vImage_Buffer=^vQQQ}88S120S124^{__CCBox=SSSS}128^{__CCBox=SSSS}136^Q144C152
v60@0:8^f16^f24S32S36^B40^f48C56
v84@0:8{vImage_Buffer=^vQQQ}16S48S52S56^{__rgbMinMaxFloat=ffffff}60^f68^f76
i80@0:8{vImage_Buffer=^vQQQ}16f48f52S56^f60^f68C76
I48@0:8^{__CCBigBox=SSSS[50{__CCCharBox=SSSSS}]}16C24^S28^S36B44
v40@0:8Q16Q24Q32
v72@0:8{vImage_Buffer=^vQQQ}16^f48^f56S64S68
v56@0:8^{__CCCharBox=SSSSS}16^{__CCCharBox=SSSSS}24^S32^S40S48S52
S48@0:8^{__CCCharBox=SSSSS}16^{__CCBigBox=SSSS[50{__CCCharBox=SSSSS}]}24^S32^S40
i76@0:8^{__CCBigBox=SSSS[50{__CCCharBox=SSSSS}]}16^{__CCBigBox=SSSS[50{__CCCharBox=SSSSS}]}24C32S36S40C44S48S52^{__CCCharBox=SSSSS}56^{__CCCharBox=SSSSS}64C72
i180@0:8{vImage_Buffer=^vQQQ}16{vImage_Buffer=^vQQQ}48{vImage_Buffer=^vQQQ}80^{__CCBigBox=SSSS[50{__CCCharBox=SSSSS}]}112^{__CCBigBox=SSSS[50{__CCCharBox=SSSSS}]}120^{__CCCharBox=SSSSS}128^{__CCCharBox=SSSSS}136S144S148S152*156C164^S168C176
i52@0:8{vImage_Buffer=^vQQQ}16S48
v60@0:8{vImage_Buffer=^vQQQ}16Q48B56
i24@0:8Q16
i68@0:8^{__CCBigBox=SSSS[50{__CCCharBox=SSSSS}]}16C24S28Q32^{__CCBox=SSSS}40Q48S56*60
i160@0:8@16{vImage_Buffer=^vQQQ}24*56^{__CCBigBox=SSSS[50{__CCCharBox=SSSSS}]}64^{__CCBigBox=SSSS[50{__CCCharBox=SSSSS}]}72{vImage_Buffer=^vQQQ}80{vImage_Buffer=^vQQQ}112^{__rgbMinMaxFloat=ffffff}144C152C156
i88@0:8{vImage_Buffer=^vQQQ}16{vImage_Buffer=^vQQQ}48^S80
@56@0:8{vImage_Buffer=^vQQQ}16^@48
v20@0:8C16
B48@0:8^Q16^Q24@32^@40
@?<v@?@"VNRequest"d@"NSError">16@0:8
v24@0:8@?<v@?@"VNRequest"d@"NSError">16
@"CRImageReaderOutput"
@40@0:8{_NSRange=QQ}16^@32
@40@0:8q16Q24^@32
B80@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{CGRect={CGPoint=dd}{CGSize=dd}}48
B88@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{CGRect={CGPoint=dd}{CGSize=dd}}48d80
@96@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{CGRect={CGPoint=dd}{CGSize=dd}}48f80i84f88f92
@100@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{CGRect={CGPoint=dd}{CGSize=dd}}48f80i84f88f92i96
@104@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{CGRect={CGPoint=dd}{CGSize=dd}}48f80i84f88f92B96i100
@108@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16{CGRect={CGPoint=dd}{CGSize=dd}}48f80i84f88f92i96B100i104
B32@0:8@16f24f28
B32@0:8@16f24i28
Q24@0:8@"VNPersonsModel"16
@"<NSObject><NSCopying><NSSecureCoding>"32@0:8@"VNPersonsModel"16Q24
Q32@0:8@"VNPersonsModel"16@"<NSObject><NSCopying><NSSecureCoding>"24
Q32@0:8@"VNPersonsModel"16Q24
@"VNFaceObservation"40@0:8@"VNPersonsModel"16Q24Q32
Q32@0:8@16@24
Q32@0:8@16Q24
r^{map<unsigned long, int, std::__1::less<unsigned long>, std::__1::allocator<std::__1::pair<const unsigned long, int> > >={__tree<std::__1::__value_type<unsigned long, int>, std::__1::__map_value_compare<unsigned long, std::__1::__value_type<unsigned long, int>, std::__1::less<unsigned long>, true>, std::__1::allocator<std::__1::__value_type<unsigned long, int> > >=^{__tree_end_node<std::__1::__tree_node_base<void *> *>}{__compressed_pair<std::__1::__tree_end_node<std::__1::__tree_node_base<void *> *>, std::__1::allocator<std::__1::__tree_node<std::__1::__value_type<unsigned long, int>, void *> > >={__tree_end_node<std::__1::__tree_node_base<void *> *>=^{__tree_node_base<void *>}}}{__compressed_pair<unsigned long, std::__1::__map_value_compare<unsigned long, std::__1::__value_type<unsigned long, int>, std::__1::less<unsigned long>, true> >=Q}}}16@0:8
{shared_ptr<vision::mod::LandmarkDetectorDNN>="__ptr_"^{LandmarkDetectorDNN}"__cntrl_"^{__shared_weak_count}}
{vector<_Geometry2D_point2D_, std::__1::allocator<_Geometry2D_point2D_> >="__begin_"^{_Geometry2D_point2D_}"__end_"^{_Geometry2D_point2D_}"__end_cap_"{__compressed_pair<_Geometry2D_point2D_ *, std::__1::allocator<_Geometry2D_point2D_> >="__value_"^{_Geometry2D_point2D_}}}
^{vector<_Geometry2D_point2D_, std::__1::allocator<_Geometry2D_point2D_> >=^{_Geometry2D_point2D_}^{_Geometry2D_point2D_}{__compressed_pair<_Geometry2D_point2D_ *, std::__1::allocator<_Geometry2D_point2D_> >=^{_Geometry2D_point2D_}}}16@0:8
B48@0:8^Q16^i24@32^@40
B36@0:8^{vector<_Geometry2D_point2D_, std::__1::allocator<_Geometry2D_point2D_> >=^{_Geometry2D_point2D_}^{_Geometry2D_point2D_}{__compressed_pair<_Geometry2D_point2D_ *, std::__1::allocator<_Geometry2D_point2D_> >=^{_Geometry2D_point2D_}}}16i24^@28
B36@0:8^{vector<float, std::__1::allocator<float> >=^f^f{__compressed_pair<float *, std::__1::allocator<float> >=^f}}16i24^@28
B36@0:8^{vector<bool, std::__1::allocator<bool> >=^QQ{__compressed_pair<unsigned long, std::__1::allocator<unsigned long> >=Q}}16i24^@28
B48@0:8^{vector<_Geometry2D_point2D_, std::__1::allocator<_Geometry2D_point2D_> >=^{_Geometry2D_point2D_}^{_Geometry2D_point2D_}{__compressed_pair<_Geometry2D_point2D_ *, std::__1::allocator<_Geometry2D_point2D_> >=^{_Geometry2D_point2D_}}}16@24@32^@40
@"NSMutableSet"
@"VNTracker"32@0:8@"NSDictionary"16^@24
v24@0:8@"VNTracker"16
v72@0:8@16@24^{__CVBuffer=}32{vImage_Buffer=^vQQQ}40
@"ShotflowDetectorANFDv2"
{shared_ptr<vision::mod::ImageClassifier_HierarchicalModel>=^{ImageClassifier_HierarchicalModel}^{__shared_weak_count}}24@0:8^@16
@28@0:8@16f24
@36@0:8@16f24@?28
@52@0:8@16@24@32f40Q44
@56@0:8@16@24@32f40f44Q48
@44@0:8@16@24@32f40
@48@0:8@16@24@32f40f44
B40@0:8^f16Q24^@32
@"<VNClustererModelQuerying>"
@"<VNClustererModelQuerying><VNClustererModelBuilding>"
{vector<std::__1::shared_ptr<espresso_buffer_t>, std::__1::allocator<std::__1::shared_ptr<espresso_buffer_t> > >="__begin_"^{shared_ptr<espresso_buffer_t>}"__end_"^{shared_ptr<espresso_buffer_t>}"__end_cap_"{__compressed_pair<std::__1::shared_ptr<espresso_buffer_t> *, std::__1::allocator<std::__1::shared_ptr<espresso_buffer_t> > >="__value_"^{shared_ptr<espresso_buffer_t>}}}
[10B]
[6[10[2f]]]
@40@0:8@16i24i28i32f36
@44@0:8{?=^vi}16^v32f40
v36@0:8@16i24i28i32
i32@0:8Q16Q24
v52@0:8{vImage_Buffer=^vQQQ}16B48
@52@0:8{vImage_Buffer=^vQQQ}16B48
r^{vector<float, std::__1::allocator<float> >=^f^f{__compressed_pair<float *, std::__1::allocator<float> >=^f}}16@0:8
{tuple<float, float, float>={__tuple_impl<std::__1::__tuple_indices<0, 1, 2>, float, float, float>=fff}}16@0:8
@40@0:8@16f24i28i32i36
r^{vector<unsigned long, std::__1::allocator<unsigned long> >=^Q^Q{__compressed_pair<unsigned long *, std::__1::allocator<unsigned long> >=^Q}}16@0:8
{shared_ptr<vision::mod::FaceBoxPoseAligner<signed char> >="__ptr_"^{FaceBoxPoseAligner<signed char>}"__cntrl_"^{__shared_weak_count}}
v76@0:8@16^{__CVBuffer=}24{_Geometry2D_rect2D_={_Geometry2D_point2D_=ff}{_Geometry2D_size2D_=ff}}32{_Geometry2D_rect2D_={_Geometry2D_point2D_=ff}{_Geometry2D_size2D_=ff}}48r^{vector<_Geometry2D_point2D_, std::__1::allocator<_Geometry2D_point2D_> >=^{_Geometry2D_point2D_}^{_Geometry2D_point2D_}{__compressed_pair<_Geometry2D_point2D_ *, std::__1::allocator<_Geometry2D_point2D_> >=^{_Geometry2D_point2D_}}}64f72
{shared_ptr<vision::mod::ObjectTrackerAbstract>="__ptr_"^{ObjectTrackerAbstract}"__cntrl_"^{__shared_weak_count}}
^{ObjectTrackerAbstract=^^?^{ObjectDetectorAbstract}{shared_ptr<vision::mod::ObjectTrackerOptions>=^{ObjectTrackerOptions}^{__shared_weak_count}}}40@0:8@16^{ObjectTrackerOptions=^^?@i}24^@32
B64@0:8@16^{vector<vision::mod::DetectedObject, std::__1::allocator<vision::mod::DetectedObject> >=^{DetectedObject}^{DetectedObject}{__compressed_pair<vision::mod::DetectedObject *, std::__1::allocator<vision::mod::DetectedObject> >=^{DetectedObject}}}24{CGSize=dd}32@48^@56
@120@0:8B16B20^{__CVBuffer=}24@32{CGSize=dd}40{CGSize=dd}56{CGRect={CGPoint=dd}{CGSize=dd}}72@104^@112
^{__CVBuffer=}64@0:8{CGRect={CGPoint=dd}{CGSize=dd}}16@48^@56
^{__CVBuffer=}40@0:8Q16Q24^@32
B64@0:8^{__CVBuffer=}16^{__CVBuffer=}24^{__CVBuffer=}32Q40Q48^@56
@"VNRequestForensics"
@"VNImageBuffer"24@0:8^@16
@48@0:8@16@24@32@40
@52@0:8@16@24@32@40I48
@56@0:8^{__CVBuffer=}16@24{CGSize=dd}32^@48
r^{FastRegistration_Signatures=^fQ{Projections_meanStdTable=^f^f}^fQ{Projections_meanStdTable=^f^f}*}16@0:8
B60@0:8^{CGAffineTransform=dddddd}16@24@32@40f48^@52
@"NSMutableIndexSet"
@"<VNPersonsModelDataDelegate>"
@52@0:8B16@20Q28@36^@44
q32@0:8^{vImage_Buffer=^vQQQ}16^f24
@32@0:8^{__CVBuffer=}16@?24
@44@0:8^{__CVBuffer=}16I24@28@?36
@32@0:8^{CGImage=}16@?24
@44@0:8^{CGImage=}16I24@28@?36
@44@0:8@16I24@28@?36
Me!>
`Y?*
K/>n
4?Rc
As?@
[?>Y
5?+3A?&S
>;9W?@1
^y>?
>3PI>
j?}Y
?}wG?X
&>CVK?
q'?ep0?
xj>t
>: i?
6:>0
>=`Z?
=aR<>=
aK=e
>A*q?C
t=+jp>B
->g,&?"R;?
>?k}A>}
G?c%
O?Ig
>obp?
$X?7pG>
>h@=>H
\'?`
7?BZg?
{>?q
g?> 
=6rQ?
8?6v
y?F$&?~
L?6X
GS?R
>FA?6Wq?
{?OX
>9{3?1
*?<h6>
?X:/>
XQ?$~
>S\%>o
v#?Y
@#?o+u?
{v>U
*X?S]
>c~j?'
>5)E?
>:u)?w1E?
>}y)?
W?:U?<L
B%?-|
?sc~?
>}>2?
=?!"
6=JCA?
.?m<4?
I>O\R?
Dc?#I
D?R(
LA?:
>R`U?
\?@h}?:
?=`b?
L>h\
??yYS?
>WCB>mq
>k,a=T
a?V)}?
?]m}?
>$%M?
Oi>-
d?6x{?
>|HX?DnV>
>W!E>w
>~t*>+
OU=0cn?
E\?+5o?
)?kb
}?C<
uQ?1A
=Nd6>
Ch=R
Nx>(-\=c)
`I>K9
"~=p
=8hS?.
\)<]pF=
$?OZ
>9`'?
r=?}vp>5&<?-yx? 
R>x~
rj>?V$?
>L?;
E?oI2?
>b1*>
>?pB
>:X'?
C?AcB?
=?Vb>?
>BAY>T
~'> ~*?
d?%=
t?`w
@?5(j>yt
^$?v
-Z?mq
ms?@O
?P$
333?
