N4bnns14graph_compiler2op17binary_relationalE
N4bnns14graph_compiler2op17binary_arithmeticE
N4bnns14graph_compiler2op16unary_arithmeticE
a_*ik, b_*kj -> c_*ij
INTERNAL: bwd
^9giP9giP9giP9giP9
N4bnns14graph_compiler2op6concatE
NSt3__110__function6__funcIZNK4bnns14graph_compiler8Function11generate_irEvE3$_0NS_9allocatorIS5_EEFiPKNS3_9OperationEEEE
NSt3__110__function6__baseIFiPKN4bnns14graph_compiler9OperationEEEE
ZNK4bnns14graph_compiler8Function11generate_irEvE3$_0
N4bnns14graph_compiler2op10prefix_sumE
N4bnns14graph_compiler2op4tileE
N4bnns20GraphOptionsInternalE
N4bnns14graph_compiler6TensorE
N4bnns14graph_compiler14TensorConstantE
N4bnns14graph_compiler24TensorConstantPalletizedE
N4bnns14graph_compiler19TensorStringLiteralE
N4bnns14graph_compiler10TensorViewE
NSt3__110__function6__funcIZN4bnns14graph_compiler8Function24layout_persistent_memoryEvE3$_1NS_9allocatorIS5_EEFiPNS3_9OperationEEEE
NSt3__110__function6__baseIFiPN4bnns14graph_compiler9OperationEEEE
ZN4bnns14graph_compiler8Function24layout_persistent_memoryEvE3$_1
NSt3__110__function6__funcIZN4bnns14graph_compiler8Function6layoutEbNS2_12layout_alg_tEE3$_2NS_9allocatorIS6_EEFiPNS3_9OperationEEEE
ZN4bnns14graph_compiler8Function6layoutEbNS_12layout_alg_tEE3$_2
NSt3__110__function6__funcIZN4bnns14graph_compiler8Function6layoutEbNS2_12layout_alg_tEE3$_3NS_9allocatorIS6_EEFiPNS3_9OperationEEEE
ZN4bnns14graph_compiler8Function6layoutEbNS_12layout_alg_tEE3$_3
N4bnns14graph_compiler2op31conv_transpose_manipulate_inputE
N4bnns14graph_compiler2op22elementwise_activationE
NSt3__110__function6__funcIZN4bnns14graph_compilerL24function_fusion_opt_passERNS3_8FunctionEE3$_1NS_9allocatorIS6_EEFiPNS3_9OperationEEEE
ZN4bnns14graph_compilerL24function_fusion_opt_passERNS0_8FunctionEE3$_1
NSt3__110__function6__funcIZN4bnns14graph_compiler15fusion_opt_passERNS3_7ProgramEE3$_0NS_9allocatorIS6_EEFiPNS3_9OperationEEEE
ZN4bnns14graph_compiler15fusion_opt_passERNS0_7ProgramEE3$_0
NSt3__117bad_function_callE
N4bnns14graph_compiler2op4lstmE
NSt3__110__function6__funcIZN4bnns14graph_compilerL26function_aliasing_opt_passERNS3_8FunctionEE3$_0NS_9allocatorIS6_EEFiPNS3_9OperationEEEE
ZN4bnns14graph_compilerL26function_aliasing_opt_passERNS0_8FunctionEE3$_0
N4bnns12GraphOptionsE
NSt3__120__shared_ptr_pointerIPN3MIL10MILContextENS_14default_deleteIS2_EENS_9allocatorIS2_EEEE
NSt3__114default_deleteIN3MIL10MILContextEEE
N4bnns14graph_compiler2op11crop_resizeE
N4bnns14graph_compiler2op9gru_fusedE
NSt3__110__function6__funcIZN4bnns14graph_compiler7pattern21make_grufused_patternEvE3$_0NS_9allocatorIS5_EEFbPKNS3_6TensorEPKNS3_8FunctionERNS3_18PatternMatchResultEEEE
NSt3__110__function6__baseIFbPKN4bnns14graph_compiler6TensorEPKNS3_8FunctionERNS3_18PatternMatchResultEEEE
ZN4bnns14graph_compiler7pattern21make_grufused_patternEvE3$_0
NSt3__120__shared_ptr_pointerIPN4bnns14graph_compiler7PatternENS_10shared_ptrIS3_E27__shared_ptr_default_deleteIS3_S3_EENS_9allocatorIS3_EEEE
NSt3__110shared_ptrIN4bnns14graph_compiler7PatternEE27__shared_ptr_default_deleteIS3_S3_EE
NSt3__110__function6__funcIZN4bnns14graph_compiler7pattern21make_grufused_patternEvE3$_1NS_9allocatorIS5_EEFbPKNS3_6TensorEPKNS3_8FunctionERNS3_18PatternMatchResultEEEE
ZN4bnns14graph_compiler7pattern21make_grufused_patternEvE3$_1
NSt3__110__function6__funcIZN4bnns14graph_compiler7pattern21make_grufused_patternEvE3$_2NS_9allocatorIS5_EEFbPKNS3_6TensorEPKNS3_8FunctionERNS3_18PatternMatchResultEEEE
ZN4bnns14graph_compiler7pattern21make_grufused_patternEvE3$_2
NSt3__110__function6__funcIZN4bnns14graph_compiler7pattern21make_grufused_patternEvE3$_3NS_9allocatorIS5_EEFbPKNS3_6TensorEPKNS3_8FunctionERNS3_18PatternMatchResultEEEE
ZN4bnns14graph_compiler7pattern21make_grufused_patternEvE3$_3
NSt3__110__function6__funcIZN4bnns14graph_compiler7pattern21make_grufused_patternEvE3$_4NS_9allocatorIS5_EEFbPNS3_8FunctionERKNS3_18PatternMatchResultEEEE
NSt3__110__function6__baseIFbPN4bnns14graph_compiler8FunctionERKNS3_18PatternMatchResultEEEE
ZN4bnns14graph_compiler7pattern21make_grufused_patternEvE3$_4
_h2m
N4bnns14graph_compiler2op6reduceE
?N4bnns2IR3gen16TensorFirstMajorE
N4bnns2IR3gen6TensorE
N4bnns2IR3gen20TensorViewFirstMajorE
N4bnns14graph_compiler2op3gruE
TANH_APPROXIMATION
N4bnns14graph_compiler2op7permuteE
N4bnns14graph_compiler2op6gatherE
?fff
N4bnns14graph_compiler8FunctionE
NSt3__110__function6__funcIZNK4bnns14graph_compiler8Function8validateEvE3$_0NS_9allocatorIS5_EEFiPKNS3_9OperationEEEE
ZNK4bnns14graph_compiler8Function8validateEvE3$_0
NSt3__110__function6__funcIZN4bnns14graph_compiler8Function11repack_dataEvE3$_1NS_9allocatorIS5_EEFiPNS3_9OperationEEEE
ZN4bnns14graph_compiler8Function11repack_dataEvE3$_1
NSt3__110__function6__funcIZNK4bnns14graph_compiler8Function18reachable_literalsEvE3$_3NS_9allocatorIS5_EEFiPKNS3_9OperationEEEE
ZNK4bnns14graph_compiler8Function18reachable_literalsEvE3$_3
NSt3__110__function6__funcIZNK4bnns14graph_compiler8Function28reachable_underlying_tensorsEvE3$_4NS_9allocatorIS5_EEFiPKNS3_9OperationEEEE
ZNK4bnns14graph_compiler8Function28reachable_underlying_tensorsEvE3$_4
NSt3__110__function6__funcIZNK4bnns14graph_compiler8Function16hasDynamicShapesEvE3$_5NS_9allocatorIS5_EEFiPKNS3_9OperationEEEE
ZNK4bnns14graph_compiler8Function16hasDynamicShapesEvE3$_5
?N4bnns14graph_compiler2op8range_1dE
333333
?N4bnns14graph_compiler2op4copyE
N4bnns14graph_compiler2op7reshapeE
N4bnns14graph_compiler2op8max_poolE
N4bnns14graph_compiler2op9base_poolE
N4bnns14graph_compiler2op8avg_poolE
N4bnns14graph_compiler2op7l2_poolE
N4bnns14graph_compiler2op5shapeE
N4bnns14graph_compiler2op6matvecE
NSt3__120__shared_ptr_pointerIPhNS_10shared_ptrIA_hE27__shared_ptr_default_deleteIS3_hEENS_9allocatorIhEEEE
NSt3__110shared_ptrIA_hE27__shared_ptr_default_deleteIS1_hEE
NSt3__110__function6__funcIZN4bnns14graph_compiler7pattern16make_any_patternERKNS_12basic_stringIcNS_11char_traitsIcEENS_9allocatorIcEEEEE3$_0NS8_ISD_EEFbPKNS3_6TensorEPKNS3_8FunctionERNS3_18PatternMatchResultEEEE
ZN4bnns14graph_compiler7pattern16make_any_patternERKNSt3__112basic_stringIcNS2_11char_traitsIcEENS2_9allocatorIcEEEEE3$_0
NSt3__110__function6__funcIZN4bnns14graph_compiler7pattern21make_constant_patternERKNS_12basic_stringIcNS_11char_traitsIcEENS_9allocatorIcEEEEE3$_1NS8_ISD_EEFbPKNS3_6TensorEPKNS3_8FunctionERNS3_18PatternMatchResultEEEE
ZN4bnns14graph_compiler7pattern21make_constant_patternERKNSt3__112basic_stringIcNS2_11char_traitsIcEENS2_9allocatorIcEEEEE3$_1
N4bnns14graph_compiler2op5stackE
NSt3__110__function6__funcIZN4bnns5graphL33execute_typed_arithmetic_unary_opIDF16_EEiRKNS2_13GraphInternalEPKvPK34bnns_graph_arithmetic_unary_paramsEUlPDF16_PKDF16_mE_NS_9allocatorISG_EEFvSD_SF_mEEE
NSt3__110__function6__baseIFvPDF16_PKDF16_mEEE
ZN4bnns5graphL33execute_typed_arithmetic_unary_opIDF16_EEiRKNS_13GraphInternalEPKvPK34bnns_graph_arithmetic_unary_paramsEUlPDF16_PKDF16_mE_
NSt3__110__function6__funcIZN4bnns5graphL33execute_typed_arithmetic_unary_opIDF16_EEiRKNS2_13GraphInternalEPKvPK34bnns_graph_arithmetic_unary_paramsEUlPDF16_PKDF16_mE0_NS_9allocatorISG_EEFvSD_SF_mEEE
ZN4bnns5graphL33execute_typed_arithmetic_unary_opIDF16_EEiRKNS_13GraphInternalEPKvPK34bnns_graph_arithmetic_unary_paramsEUlPDF16_PKDF16_mE0_
NSt3__110__function6__funcIZN4bnns5graphL33execute_typed_arithmetic_unary_opIDF16_EEiRKNS2_13GraphInternalEPKvPK34bnns_graph_arithmetic_unary_paramsEUlPDF16_PKDF16_mE1_NS_9allocatorISG_EEFvSD_SF_mEEE
ZN4bnns5graphL33execute_typed_arithmetic_unary_opIDF16_EEiRKNS_13GraphInternalEPKvPK34bnns_graph_arithmetic_unary_paramsEUlPDF16_PKDF16_mE1_
NSt3__110__function6__funcIZN4bnns5graphL33execute_typed_arithmetic_unary_opIDF16_EEiRKNS2_13GraphInternalEPKvPK34bnns_graph_arithmetic_unary_paramsEUlPDF16_PKDF16_mE2_NS_9allocatorISG_EEFvSD_SF_mEEE
ZN4bnns5graphL33execute_typed_arithmetic_unary_opIDF16_EEiRKNS_13GraphInternalEPKvPK34bnns_graph_arithmetic_unary_paramsEUlPDF16_PKDF16_mE2_
NSt3__110__function6__funcIZN4bnns5graphL33execute_typed_arithmetic_unary_opIfEEiRKNS2_13GraphInternalEPKvPK34bnns_graph_arithmetic_unary_paramsEUlPfPKfmE_NS_9allocatorISG_EEFvSD_SF_mEEE
NSt3__110__function6__baseIFvPfPKfmEEE
ZN4bnns5graphL33execute_typed_arithmetic_unary_opIfEEiRKNS_13GraphInternalEPKvPK34bnns_graph_arithmetic_unary_paramsEUlPfPKfmE_
NSt3__110__function6__funcIZN4bnns5graphL33execute_typed_arithmetic_unary_opIfEEiRKNS2_13GraphInternalEPKvPK34bnns_graph_arithmetic_unary_paramsEUlPfPKfmE0_NS_9allocatorISG_EEFvSD_SF_mEEE
ZN4bnns5graphL33execute_typed_arithmetic_unary_opIfEEiRKNS_13GraphInternalEPKvPK34bnns_graph_arithmetic_unary_paramsEUlPfPKfmE0_
NSt3__110__function6__funcIZN4bnns5graphL33execute_typed_arithmetic_unary_opIfEEiRKNS2_13GraphInternalEPKvPK34bnns_graph_arithmetic_unary_paramsEUlPfPKfmE1_NS_9allocatorISG_EEFvSD_SF_mEEE
ZN4bnns5graphL33execute_typed_arithmetic_unary_opIfEEiRKNS_13GraphInternalEPKvPK34bnns_graph_arithmetic_unary_paramsEUlPfPKfmE1_
NSt3__110__function6__funcIZN4bnns5graphL33execute_typed_arithmetic_unary_opIfEEiRKNS2_13GraphInternalEPKvPK34bnns_graph_arithmetic_unary_paramsEUlPfPKfmE2_NS_9allocatorISG_EEFvSD_SF_mEEE
ZN4bnns5graphL33execute_typed_arithmetic_unary_opIfEEiRKNS_13GraphInternalEPKvPK34bnns_graph_arithmetic_unary_paramsEUlPfPKfmE2_
N4bnns14graph_compiler2op6randomE
y4#@y4#@y4#@y4#@
r1?V
CB6"9B6"9B6"9B6"9N
U?DH
!rr{>
!rr{>
}76R
YVpA
YVpA
YVpA
<7p7M}
<7p7M}
<7p7M}
:I~D
:I~D
:I~D
:I~D
?G/d?
*?9s
)>e\
N4bnns14graph_compiler2op4topkE
NSt3__110__function6__funcIZN4bnns14graph_compiler8Function8scheduleEvE3$_0NS_9allocatorIS5_EEFiPNS3_9OperationEEEE
ZN4bnns14graph_compiler8Function8scheduleEvE3$_0
?N4bnns14graph_compiler2op4convE
N4bnns14graph_compiler2op3nopE
N4bnns14graph_compiler9OperationE
N4bnns14graph_compiler2op7softmaxE
N4bnns14graph_compiler2op6resizeE
N4bnns14graph_compiler2op7scatterE
NSt3__110__function6__funcIZN4bnns14graph_compilerL27function_constexpr_opt_passERNS3_8FunctionEE3$_0NS_9allocatorIS6_EEFiPNS3_9OperationEEEE
ZN4bnns14graph_compilerL27function_constexpr_opt_passERNS0_8FunctionEE3$_0
N4bnns14graph_compiler2op9gumbelmaxE
NSt3__110__function6__funcIZN4bnns14graph_compiler7pattern25make_gumbelmax_pattern_v0EvE3$_0NS_9allocatorIS5_EEFbPKNS3_6TensorEPKNS3_8FunctionERNS3_18PatternMatchResultEEEE
ZN4bnns14graph_compiler7pattern25make_gumbelmax_pattern_v0EvE3$_0
NSt3__110__function6__funcIZN4bnns14graph_compiler7pattern25make_gumbelmax_pattern_v0EvE3$_1NS_9allocatorIS5_EEFbPNS3_8FunctionERKNS3_18PatternMatchResultEEEE
ZN4bnns14graph_compiler7pattern25make_gumbelmax_pattern_v0EvE3$_1
NSt3__110__function6__funcIZN4bnns14graph_compiler7pattern25make_gumbelmax_pattern_v1EvE3$_2NS_9allocatorIS5_EEFbPKNS3_6TensorEPKNS3_8FunctionERNS3_18PatternMatchResultEEEE
ZN4bnns14graph_compiler7pattern25make_gumbelmax_pattern_v1EvE3$_2
NSt3__110__function6__funcIZN4bnns14graph_compiler7pattern25make_gumbelmax_pattern_v1EvE3$_3NS_9allocatorIS5_EEFbPNS3_8FunctionERKNS3_18PatternMatchResultEEEE
ZN4bnns14graph_compiler7pattern25make_gumbelmax_pattern_v1EvE3$_3
N4bnns14graph_compiler2op6matmulE
^9giP9
NSt3__110__function6__funcIZN4bnns14graph_compiler14local_opt_passERNS3_8FunctionEE3$_0NS_9allocatorIS6_EEFiPNS3_9OperationEEEE
ZN4bnns14graph_compiler14local_opt_passERNS0_8FunctionEE3$_0
N4bnns14graph_compiler2op11dot_productE
N4bnns14graph_compiler2op3rnnE
N4bnns14graph_compiler2op3padE
N4bnns14graph_compiler2op7shuffleE
N4bnns14graph_compiler2op5sliceE
EnumeratedShapes
N4bnns14graph_compiler2op13NormalizationE
N4bnns14graph_compiler2op10batch_normE
N4bnns14graph_compiler2op13instance_normE
N4bnns14graph_compiler2op10layer_normE
N4bnns14graph_compiler2op12channel_normE
NSt3__110__function6__funcIZN4bnns14graph_compiler7pattern25make_channel_norm_patternEbE3$_0NS_9allocatorIS5_EEFbPKNS3_6TensorEPKNS3_8FunctionERNS3_18PatternMatchResultEEEE
ZN4bnns14graph_compiler7pattern25make_channel_norm_patternEbE3$_0
NSt3__110__function6__funcIZN4bnns14graph_compiler7pattern25make_channel_norm_patternEbE3$_1NS_9allocatorIS5_EEFbPKNS3_6TensorEPKNS3_8FunctionERNS3_18PatternMatchResultEEEE
ZN4bnns14graph_compiler7pattern25make_channel_norm_patternEbE3$_1
NSt3__110__function6__funcIZN4bnns14graph_compiler7pattern25make_channel_norm_patternEbE3$_2NS_9allocatorIS5_EEFbPKNS3_6TensorEPKNS3_8FunctionERNS3_18PatternMatchResultEEEE
ZN4bnns14graph_compiler7pattern25make_channel_norm_patternEbE3$_2
NSt3__110__function6__funcIZN4bnns14graph_compiler7pattern25make_channel_norm_patternEbE3$_3NS_9allocatorIS5_EEFbPNS3_8FunctionERKNS3_18PatternMatchResultEEEE
ZN4bnns14graph_compiler7pattern25make_channel_norm_patternEbE3$_3
NSt3__110__function6__funcIZN4bnns14graph_compiler7pattern25make_channel_norm_patternEbE3$_4NS_9allocatorIS5_EEFbPNS3_8FunctionERKNS3_18PatternMatchResultEEEE
ZN4bnns14graph_compiler7pattern25make_channel_norm_patternEbE3$_4
logical_and
logical_or
logical_xor
logical_nand
logical_nor
greater
greater_equal
less
less_equal
not_equal
equal
logical_not
input0
input1
output
Relational op does not support dynamic shaped inputs
Relational from 
 to 
 is not supported
map::at:  key not found
basic_string
undefined
floor_div
maximum
minimum
real_div
binary_arithmetic only supports fp16 and fp32 operands
manifest_literal
internal.manifest_literal
literal(for:
input
Replaced op 
 with constexpr result
vector
acos
acosh
asin
asinh
atan
atanh
ceil
clip
cosh
exp2
floor
inverse
round
rsqrt
sign
sinh
sqrt
square
tanh
threshold
, epsilon=
BNNS Tile: descriptor is illegal
BNNS Tile: data type check failed
BNNS Tile: output tensor size is not a multiple of input
BNNS Tile: type %u not supported
Shouldn't call get_data_size with Indexed2 or Indexed4, switch to get_data_bits
BNNS Dropout: Error layer_params is NULL
BNNS Dropout: rate must be in the range [0.0, 1.0].
BNNS Dropout: Unsupported input data type
BNNS Dropout: Input and Output data types must be the same
BNNS Dropout: Error memory allocation failed
BNNS Dropout: Error input descriptor is invalid
BNNS Dropout: Error output descriptor is invalid
BNNS Dropout: Input and output descriptors must have the same shape.
BNNS Dropout: Tensors with dimension greater than %u are not supported.
BNNS Dropout: Error filter is NULL
BNNS Dropout: Error wrong filter type, filter is not Dropout
BNNS Dropout: input pointer is NULL
BNNS Dropout: batch_size>1 and in_stride is 0
BNNS Dropout: output pointer is NULL
BNNS Dropout: batch_size>1 and out_stride is 0
BNNS BatchNorm: Error filter is NULL
BNNS Dropout: input_delta layout, shape and stride must match layer_params->i_desc
BNNS Dropout: output_delta layout, shape and stride must match layer_params->o_desc
Unsupported target %d
v16@?0Q8
v24@?0Q8^v16
BNNS Activation Gumbel Max: noise buffer size (%zu) isn't power of 2 or smaller than 16
BNNS Tensor Contraction: inputB pointer must not be NULL unless B is a weight or the operation is quadratic.
BNNS Tensor Contraction: After adding batch dimension, inputA tensor has too many indices
BNNS Tensor Contraction: After adding batch dimension, inputB tensor has too many indices
BNNS Tensor Contraction: After adding batch dimension, output tensor has too many indices
BNNS Tensor Contraction: inB_delta calculation is non-sensical for a contraction of a tensor with itself.
BNNS Tensor Contraction: inA_delta calculation requires inA to be non-NULL
BNNS Tensor Contraction: inA_delta calculation requires out_delta (and out_delta->data) to be non-NULL
inA_delta
BNNS Tensor Contraction: After adding batch dimension, out_delta tensor has too many indices
BNNS Tensor Contraction: After adding batch dimension, inputA_delta tensor has too many indices
BNNS Tensor Contraction: inA_delta calculation requires inB to be non-NULL
BNNS Tensor Contraction: inB_delta calculation requires inA to be non-NULL
BNNS Tensor Contraction: inB_delta calculation requires out_delta (and out_delta->data) to be non-NULL
inB_delta
BNNS Tensor Contraction: After adding batch dimension, inputB_delta tensor has too many indices
BNNS Tensor Contraction: beta must be 0.0 or 1.0 (beta=%.2f)
BNNS Tensor Contraction: invalid op string "%s"
BNNS Tensor Contraction: both inputA and inputB cannot be weights "%s"
BNNS Tensor Contraction: Failed to allocate memory for filter
BNNS Tensor Contraction: inputA descriptor is illegal "%s"
BNNS Tensor Contraction: inputB descriptor is illegal "%s"
BNNS Tensor Contraction: ouput descriptor is illegal "%s"
Data type combination not supported
Specified opaque layout is not supported for use with specified contraction operation
BNNS Tensor Contraction: %s shape does not match inputA shape in dimension %ld (%zu vs %zu)
BNNS Tensor Contraction: out_delta shape does not match output shape in dimension %ld (%zu vs %zu)
BNNS Tensor Contraction: Wildcard index '*' must appear on both sides of operation or neither: "%s"
BNNS Tensor Contraction: Wildcard index '*' must appear at most once per index set: "%s"
BNNS Tensor Contraction: Wildcard index '*' must appear consistently only at start or end of index sets: "%s"
BNNS Tensor Contraction: number of indices from operation (%zu) does not match dimension of input A descriptor (%zu) "%s"
BNNS Tensor Contraction: number of indices from operation (%zu) does not match dimension of input B descriptor (%zu) "%s"
BNNS Tensor Contraction: number of indices from operation (%zu) does not match dimension of output descriptor (%zu) "%s"
BNNS Tensor Contraction: '%c', index %zu of inputA has size %zu and index %td of output has size %zu, but sizes are required to match
BNNS Tensor Contraction: '%c', index %d of inputA has size %zu and index %td of inputB has size %zu, but sizes are required to match
BNNS Tensor Contraction: index '%c' of input A does not match any index of inputB or output "%s"
BNNS Tensor Contraction: '%c', index %d of inputB has size %zu and index %td of output has size %zu, but sizes are required to match
BNNS Tensor Contraction: index '%c' of input B does not match any index of inputA or output "%s"
BNNS Tensor Contraction: index '%c' of output does not match any index of inputA or inputB "%s"
BNNS Tensor Contraction: Wildcard index '*' can only appear as first or last index of "%s_%s"
BNNS Tensor Contraction: repeated indices "%s_%s"
v16@?0^v8
v24@?0Q8Q16
BNNS SoftMax: sum result shouldn't be 0, something is wrong
BNNS SoftMax FP16: sum result shouldn't be 0, something is wrong
BNNS SoftMax BF16: sum result shouldn't be 0, something is wrong
BNNS Graph Execute: missing persistent memory, please allocate memory based on BNNSGraphGetExecutionContextSize
  %s
   %s [id=0x%llx] datahash = 0x%x
exec op %zu of %zu
  final generator of %s [id=0x%llx] abs_diff=%.2e rel_diff=%.2e
  final generator of %s [id=0x%llx] <not in expected_values map>
BNNS Graph Execute: Unrecognized kernel id %u
WARNING: Asked to run with signposts, but signposts are disabled for BNNS
BNNS signposts can be enabled by running the following commands:
sudo cp /System/Library/Preferences/Logging/Subsystems/com.apple.accelerate.bnns.plist /Library/Preferences/Logging/Subsystems/com.apple.accelerate.bnns.plist
sudo plutil -replace DEFAULT-OPTIONS.Signpost-Enabled -bool YES /Library/Preferences/Logging/Subsystems/com.apple.accelerate.bnns.plist
Input datahashes:
 = concat(values=[
], axis=
, interleave=
view(of:
pad op only supports FP16, FP32 and INT32 operands
pad op does not support dynamic shaped inputs
pad op requires output type to match input type
malloc activation_grad failed
apply_specialized_convolution_PKT_0: o_height 1 not supported
apply_specialized_convolution_PKT_1: o_height less then 4 isn't supported
apply_specialized_convolution_PKT_2: o_height 1 not supported
apply_specialized_convolution_PKT_3: o_height 1 not supported
apply_specialized_convolution_PKT_3: o_height 2 not supported
BNNS Copy Filter: in and out must not be NULL.
BNNS Copysum Filter: unsupported data type conversion
internal bwd copysum
BNNS Tensor Contraction: Input must not be NULL
BNNS Tensor Contraction: Output must not be NULL
BNNS Tensor Contraction: index '%c' appears multiple times on right-hand side, but with different sizes
BNNS Tensor Contraction: '%c', index %zu of input has size %zu and index %d of output has size %zu, but sizes are required to match
BNNS Tensor Contraction: index '%c' appears multiple times on left-hand side, but with different sizes
BNNS Tensor Contraction: Unsupported data type conversion %sfrom %s to %s
(in summation) 
(in copy) 
BNNSTranspose: src and dest have a different number of dimensions.
BNNSTranspose: specified axes must be within range for supplied tensor (axes: %zu, %zu) dimension of tensor is %zu
internal_transpose
internal_copy
int1
int2
int4
int8
int16
int32
int64
uint1
uint2
uint4
uint8
uint16
uint32
uint64
bf16
fp16
fp32
indexed8
indexed4
indexed2
indexed1
bool
unknown
BNNS Fully Connected Sparsify: failed to allocate scratch buffer
BNNS Convolution Sparse: failed to allocate memory to sparsify
BNNS Convolution Sparsify: output allocation failed
BNNS CONVOLUTIONS VERSION2: Error minimum float w_pack weight packing value is 32
BNNS Fully Conneceted Sparsify: output allocation failed
BNNS Sparse Fully Connected Apply: output data type not supported
BNNS Convolution Sparsify: out descriptor can't be NULL
BNNS Convolution Sparse: failed to allocate memory to sparsify COO
BNNS Convolution Sparse: Number of nonzeros is 0
BNNS Sparse Convolution create: could not allocate 
BNNS Sparse Convolution Create: asymmetric padding for Sparse Convolution is not supported
BNNS Sparse Convolution Create: groups for Sparse Convolution is not supported
BNNS Sparse Convolution Create: stride > 1 for Sparse Convolution is not supported
internal gemm: unsupported type combination
: BNNS Graph prefix_sum does not support specified type/minimum_compute_type combination
: BNNS Graph only supports INT32, FP16 and FP32 operands for prefix_sum
: BNNS Graph prefix_sum required input and output types to match
 , axis=
, exclusive = 
, reverse = 
, reps=
tile op does not support dynamic sizes
tile requires input and output ranks to match
tile rank exceeds maximum supported dimension
tile requires reps >= 1
tile output size inconsistent with inputs
tile should have been replaced with copy
BNNS Fully Connected Sparsify: workspace size is too small
BNNS Fully Connected Sparsify: failed to allocate workspace
BNNS Fully Connected Sparsify: not enough memory in scratch memory to encode sparsity
BNNS Dequantize: input and output can't be null
BNNS Dequantize: in place conversion not supported
BNNS Dequantize: only __fp16 output is supported
BNNS Dequantize: lut is null and needed for Indexed input type
BNNS Dequantize: input type not supported
BNNS Pose Init: blob memory is NULL. please allocate buffer of BNNSPose_C_allocation_size_bytes and 256B alignment
BNNS Pose Execute: blob memory is NULL
BNNS Pose Execute: failed to run layer
encoder_0_bias
encoder_0_weight
encoder_2_bias
encoder_2_weight
decoder_1_bias
decoder_1_weight
decoder_3_bias
decoder_3_weight
decoder_5_bias
decoder_5_weight
concat_0
concat_1
concat_2
concat_3
concat_4
concat_5
MIL program has more than one function
const
bnns::PoseInitFromMIL(): MIL file does not contain expected tensor '%s'
unexpected tensor sizes
BNNS Graph Compile: Unsupported MIL data type
unordered_map::at: key not found
BNNS Layer Norm Apply: failed to allocate scratch memory
BNNS Instance Norm Apply: input stride (%zu) and output stride (%zu) aren't set correctly
BNNS Instance Norm Apply: failed to allocate scratch memory
BNNS Layer Norm Apply: input stride (%zu) and output stride (%zu) aren't set correctly
BNNS Group Norm Apply: failed to allocate scratch memory
BNNS Fully Connected Tiny Apply: unexpecetd weights layout
MIL program has more than one function, and no specific function specified for compilation
BNNS Fully Connected Apply: allocation of work buffer failed
BNNS Fully Connected: Unexpected result, should be at least 1
BNNS Fully Connected: Unexpected result, at least 1 batch should remain
BNNS Fully Connected: Unexpected result, at least 1 output should remain
BNNS Fully Connected: Failed to fit in memory limit, using generic code
BNNS FUlly: allocation failed
BNNS Fully Connected: weights conversion buffer isn't allocated.
BNNS Fully Connected: inputs conversion buffer isn't allocated.
BNNS Fully Connected Backward: output delta is NULL
BNNS Fully Connected Backward: only float32 or bfloat16 output delta are supported
BNNS Fully Connected Backward: input delta only support float32 or bfloat16
BNNS Fully Connected Backward: weights delta only support float32 or bfloat16
BNNS Fully Connected Backward: bias delta only support float32 or bfloat16
BNNS Fully Connected Backward: operation is currently unsupported.  We only support sparse weights for inference.
BNNS Fully Connected Backward: output delta BNNSDataLayoutImageCHW isn't supported
BNNS Fully Connected Backward: failed to allocate output delta scratch buffer
BNNS Fully Connected Backward: apply activation backward failed
BNNS Fully Connected Backward: input delta BNNSDataLayoutImageCHW isn't supported
BNNS Fully Connected Backward: failed to allocate input delta scratch buffer
BNNS Fully Connected Backward: failed to compute input delta
BNNS Fully Connected Backward: user requested weights delta but didn't supply original inputs
BNNS Fully Connected Backward: weights sizes doesn't match weights delta sizes
BNNS Fully Connected Backward: unsupported weights delta layout
BNNS Fully Connected: Weight type not supported
BNNS Fully Connected Create: failed to allocated memory (%zu bytes))
BNNS_FULLY: input doesn't support Indexed data type
BNNS_FULLY: output only support fp32 data type
BNNS Fully Connected: output data type not supported for conversion
BNNS Fully Connected: input must be a 1D array
BNNS Fully Connected: input descriptor is illegal
BNNS Fully Connected: output must be a 1D array
BNNS Fully Connected: output descriptor is illegal
BNNS Fully Connected: weights row size should match input vector size
BNNS Fully Connected: weights number of rows should match output vector size
BNNS Fully Connected: weights must be a 2D array
BNNS Fully Connected: weights descriptor is illegal
BNNS Fully Connected: Pointer to weight data must be non-NULL
BNNS Fully Connected: Output data types supported in this version: Float32, Float16, BFloat16
BNNS Fully Connected: Bias data types supported in this version: Float32
BNNS Fully Connected: Float16 output isn't supported with bias or activation function
BNNS Fully Connected: Input/Weight data types supported in this version: Float32, Float16, BFloat16, Int8, Int16
BNNS Fully Connected: Data type of weights and input should match
BNNS Fully Connected: size computation wraparound -I %zu -O %zu
BNNS Fully Connected: bias size computation wraparound -O %zu bias data type %x
Attempted to copy a view without explicit shallow copy flag
_view
BNNS InTopK: invalid axis value %zu (input tensor has %zu dimensions)
BNNS InTopK: Unsupported test_indices data type for this operation (supported data types are: int32)
BNNS InTopK: Unsupported output data type for this operation (supported data types are: boolean)
BNNS InTopK: input tensor with axis removed is not congruent with test_indices tensor
BNNS InTopK: input tensor with axis removed is not congruent with output tensor
BNNS InTopK: Unsupported input data type for this operation (supported data types are: fp32, fp16)
BNNS TopK: input descriptor is illegal
BNNS TopK: best_values descriptor is illegal
BNNS TopK: best_values data type has to match input data type for this operation
BNNS TopK: Unsupported best_indices data type for this operation (supported data types are: int32)
BNNS TopK: invalid axis value %zu (input tensor has %zu dimensions)
BNNS TopK: best_values tensor is not consistent with input tensor
BNNS TopK: best_values tensor size[%zu]=%zu should be K=%zu
BNNS TopK: best_indices tensor is not consistent with input tensor
BNNS TopK: best_indices tensor size[%zu]=%zu should be K=%zu
BNNS TopK: Unsupported input data type for this operation (supported data types are: fp32, fp16)
malloc
orange
yellow
green
blue
indigo
violet
lilac
pink
cerulean
ultraviolet
infrared
Unoptimized layout option used. Workspace will be super-massive.
Context '
': workspace size 
, lower bound (LOAD) 
Unoptimized runtime layout used. Workspace will be super-massive.
layout_color
 = conv_transpose_manipulate_input(x=
relu
relu6
leaky_relu
sigmoid
scaled_tanh
linear_activation
clamped_relu
sigmoid_hard
softplus
softsign
thresholded_relu
prelu
silu
gelu
gelu_approx_tanh
gelu_approx_sigmoid
elementwise_activation only supports fp16 and fp32 operands
alpha
, alpha=
, beta=
bias
last_h
last_c
= lstm(x=
, initial_h=
initial_h
, weight_ih=
weight_ih
, weight_hh=
weight_hh
, bias=
<none>
, direction=
, output_sequence=
true
false
, recurrent_activation=
, cell_activation=
, activation=
initial_c
peephole
weight_ih_back
weight_hh_back
bias_back
peephole_back
scratch(for:
forward
reverse
bidirectional
UNDEFINED
peehole
peehole_back
BNNS unsupported sgd variant
BNNS Optimizer Adam Apply: Adam beta1 and beta2 must be positive
BNNS Optimizer Adam Apply: Adam beta1 and beta2 must be strictly less than 1
BNNS Optimizer RMSProp Apply: RMSProp alpha must be in (0,1)
BNNS Optimizer Apply: Error unsupported optimizer function
BNNS Optimizer Apply: Error OptimizerAlgFields is NULL
BNNS Optimizer Apply: parameter array pointer is NULL
BNNS Optimizer Apply: gradient array pointer is NULL
BNNS Optimizer Apply: gradient pointer number %zu is NULL
BNNS Optimizer Apply: Error in parameter %zu: parameter descriptor must be contiguous such that stride[0]=1 and for every N>0 stride[N]=stride[N-1]*size[N-1]
BNNS Optimizer Apply: accumulator array pointer is NULL
BNNS Optimizer Apply: parameter pointer number %zu is NULL
BNNS Optimizer Apply: accumulator pointer number %zu is NULL
BNNS Optimizer Apply: Error in parameter %zu: Parameter and Gradient descriptors must have the same sizes and strides
BNNS Optimizer Apply: Error in parameter %zu: Parameter and Accumulator descriptors must have the same sizes and strides
BNNS Optimizer Apply: Error in Parameter %zu: Parameter data type is not Float32 / Float16 / Bfloat16
BNNS Optimizer Apply: Error in Gradient %zu: Gradient data type is not Float32 / Float16 / Bfloat16
BNNS Optimizer Apply: Error in Accumulator %zu: Accumulator data type is not Float32 / Float16 / BFloat16
BNNS Optimizer Apply: Error time_step is not valid for Adam optimizer. minimal time step value is 1
BNNS Optimizer Apply: Adam optimizer require accumulators pointer to be valid
BNNS Optimizer Apply: accumulator1 pointer number %zu is NULL
BNNS Optimizer Apply: accumulator2 pointer number %zu is NULL
BNNS Optimizer Apply: accumulator3 pointer number %zu is NULL
BNNS Optimizer Apply: Error in parameter %zu: Parameter and Accumulator1 descriptors must have the same sizes and strides
BNNS Optimizer Apply: Error in parameter %zu: Parameter and Accumulator2 descriptors must have the same sizes and strides
BNNS Optimizer Apply: Error in parameter %zu: Parameter and Accumulator3 descriptors must have the same sizes and strides
BNNS Optimizer Apply: Error in accumulator1 %zu: Accumulator data type is not Float32 / Float16 / Bfloat16
BNNS Optimizer Apply: Error in accumulator2 %zu: Accumulator data type is not Float32 / Float16 / Bfloat16
BNNS Optimizer Apply: Error in accumulator3 %zu: Accumulator data type is not Float32 / Float16 / Bfloat16
BNNS Optimizer Apply: RMSProp optimizer require accumulators pointer to be valid
BNNS Optimizer Apply: accumulator_n pointer number %zu is NULL
BNNS Optimizer Apply: accumulator_g pointer number %zu is NULL
BNNS Optimizer Apply: momentum pointer number %zu is NULL
BNNS Optimizer Apply: Error in accumulator_n %zu: Accumulator data type is not Float32 / Float16 / Bfloat16
BNNS Optimizer Apply: Error in accumulator_g %zu: Accumulator data type is not Float32 / Float16 / Bfloat16
BNNS Optimizer Apply: Error in momentum %zu: Accumulator data type is not Float32 / Float16 / Bfloat16
BNNS Optimizer: unsupported optimizer function
BNNS ClipByValue: Error dimensions of input tensor (%zu) and output tensor (%zu) must match
BNNS ClipByValue: Error only supports fp32
BNNS ClipByValue: Error sizes and strides of input tensor and output tensor must match
BNNS ClipByNorm: Error dimensions of input tensor (%zu) and output tensor (%zu) must match
BNNS ClipByNorm: Error only supports fp32
BNNS ClipByNorm: Error sizes and strides of input tensor and output tensor must match
BNNS ClipByGlobalNorm: src[%zu] and dest[%zu] have a different number of dimensions.
BNNS ClipByGlobalNorm: src[%zu] and dest[%zu] must contain fp32 data.
BNNS ClipByGlobalNorm: src[%zu] and dest[%zu] have different sizes or strides.
BNNS ComputeNorm: Error only supports BNNSL2Norm
BNNS ComputeNorm: Error only supports fp32
BNNS ComputeNorm: Error non-axis dimensions of input tensor (%zu) and output tensor (%zu) must match
BNNS ComputeNorm: Error non-axis sizes of input tensor and output tensor must match
BNNS ComputeNorm Backward: Error only supports BNNSL2Norm
BNNS ComputeNorm Backward: Error only supports fp32
BNNS ComputeNorm Backward: Error non-axis dimensions of input tensor (%zu) and output tensor (%zu) must match
BNNS ComputeNorm Backward: Error non-axis sizes of input tensor and output tensor must match
BNNS Optimizer Init: Unsupported layout
BNNS Sparse Fully Connected: input must be a 1D array
BNNS Sparse Fully Connected: input descriptor is illegal
BNNS Sparse Fully Connected: output must be a 1D array
BNNS Sparse Fully Connected: output descriptor is illegal
BNNS Sparse Fully Connected: invalid matrix layout
BNNS Sparse Fully Connected: invalid block row count: %u
BNNS Sparse Fully Connected: invalid block col count: %u
BNNS Sparse Fully Connected: input data types supported in this version: Float16
BNNS Sparse Fully Connected: output data types supported in this version: Float32, Float16
BNNS Sparse Fully Connected: weights data types supported in this version: Float16
BNNS Sparse Fully Connected: unsupported matrix block size %u x %u
BNNS Sparse Fully Connected: Allocation of the setup structure failed
BNNS Convolution: malloc failed
BNNS Graph: Invalid representation
BNNS Graph: Unsupported BNNS IR Version
BNNS Graph: Static size specified in sizes[%zu][%zu] does not match input MIL size
BNNS Graph: Missing required dynamic size for %zu-th argument
BNNS Graph: Called batch size interface, but graph requires a non-batch dynamic size for argument %zu
/System/Library/PrivateFrameworks/MIL.framework/MIL
BNNS cannot compile graph: MIL framework is not available
filename provided is empty
BNNS GraphGetWorkspaceSize passed invalid graph
BNNS GraphGetWorkspaceSize passed graph with unsupported ir_version %u
BNNS GraphGetArgPosition: Argument '%s' not found
BNNS: BNNSGraphContextExecute called before dynamic shapes have been supplied
BNNS GraphGetSize passed invalid graph
BNNS GraphGetSize passed graph with unsupported ir_version %u
BNNS GraphGetArgPosition passed invalid graph
BNNS GraphGetArgPosition passed graph with unsupported ir_version %u
BNNS IR describes a graph with dynamic sizes, BNNSGraphContextExecute() must be used instead of BNNSGraphExecute().
BNNS IR describes a graph with persistent memory requirement, BNNSGraphContextExecute() must be used instead of BNNSGraphExecute().
BNNS Compare: in0 is NULL
BNNS Compare: in1 is NULL
BNNS Compare: out is NULL
BNNS Compare: Unsupported I/O tensor data types.
BNNS Compare: Mismatch in input tensor data types.
BNNS Compare: Invalid operation %d for data type %s.
BNNS Compare: Broadcast failed.
BNNS Compare: I/O tensor dimension %lu mismatch.
BNNS Compare: Partially overlapping input and output tensors are not supported.
BNNS Compare: Unsupported I/O tensor layout.
BNNS Compare: I/O tensor layout mismatch.
BNNS Compare: input size[%zu]=%zu does not equal output size of %zu or 1
BNNS Compare: Unsupported operator.
, ), 
BNNS crop_resize: input and output data types don't match.
BNNS crop_resize: input and roi data types don't match.
hinput
input_u
input_r
input_e
recurrent_u
recurrent_r
recurrent_e
 = grufused(input_u=
, input_r=
, input_e=
, recurrent_u
, recurrent_r
, recurrent_e
, hidden_input=
internal.grufused
BNNS Fully Connected Apply: failed to allocated scratch memory
BNNS Fully Connected Init: missing data table for indexed data type
BNNS Fully Connected Apply: Ilegal compute block
BNNS Fully Connected Create: failed to create layer context
BNNS Fully Connected Create: allocation failed
BNNS Fully Connected Direct Apply: input pointer is NULL
BNNS Fully Connected Direct Apply: output pointer is NULL
BNNS Fully Connected Direct Apply: failed to create layer context
BNNS Fully Connected Init: failed to create context
BNNS Fully Connected Init: input descriptor CHW layout isn't set properly
BNNS Fully Connected Init: input descriptor isn't set properly
BNNS Fully Connected Init: input descriptor failed validation
BNNS Fully Connected Init: output descriptor CHW layout isn't set properly
BNNS Fully Connected Init: output descriptor isn't set properly
BNNS Fully Connected Init: output descriptor failed validation
BNNS Fully Connected Init: weights descriptor row major size[0] doesn't match input vector size
BNNS Fully Connected Init: weights descriptor row major size[1] doesn't match output vector size
BNNS Fully Connected Init: weights descriptor column major size[0] doesn't match output vector size
BNNS Fully Connected Init: weights descriptor column major size[1] doesn't match input vector size
BNNS Fully Connected Init: weights descriptor isn't set properly
BNNS Fully Connected Init: weights descriptor failed validation
BNNS Fully Connected Init BNNSNDArrayDescriptor: stride[%zu] is too small
BNNS Fully Connected Apply: failed to apply filter
BNNS Random Generator Create: Unsupported method %u.
BNNS Random Generator Create: Failed to allocate memory
BNNS Random Fill Uniform Float: Invalid descriptor
BNNS Random Fill Uniform Float: Range bounds are not exactly representable as bf16
BNNS Random Fill Uniform Float: Range (%g, %g) is empty
BNNS Random Fill Normal Float: Mean and standard deviation are not exactly representable as bf16
BNNS Random Fill Normal Float: Standard deviation (%g) is not greater than zero
BNNS Random Fill Float: Distribution type not supported
BNNS Random Fill Uniform Float: Range bounds are not exactly representable as fp16
BNNS Random Fill Normal Float: Mean and standard deviation are not exactly representable as fp16
BNNS Random Fill Float: Unsupported data type
BNNS Random Fill Uniform Integer: Invalid descriptor
BNNS Random Fill Uniform Integer:%s range (%d, %d) is empty
 (clipped)
BNNS Random Fill Uniform Integer: range (%lld, %lld) is empty
BNNS Random Fill Uniform Integer:%s range (%llu, %lld) is empty
BNNS Random Fill Uniform Integer: Unsupported data type
BNNS Random: CCCryptorCreateWithMode() failed
BNNS Random Fill Uniform: Invalid generator
Failed to create random data
reduce_max
reduce_mean
reduce_min
reduce_prod
reduce_sum
reduce_sum_square
reduce_log_sum
reduce_log_sum_exp
reduce_argmax
reduce_argmin
reduce_l1_norm
reduce_l2_norm
reduce op requires arg reductions to have output type int32
reduce op requires input and output types to match
reduce op only supports inputs of type fp16, fp32 or int32
BNNS Loss: Error unsupported loss function
BNNS Loss: Error Input width is 0
BNNS Loss: Error Input must be BNNSDataTypeFloat32 or BNNSDataTypeBFloat16 type
BNNS Loss Error: Input must be contiguous. stride0 must be 1 or 0
BNNS Loss Error: Input tensors must be contiguous.
BNNS Loss: Error unknown reduction function
BNNS Loss: Error output data type must be BNNSDataTypeFloat32 or BNNSDataTypeBFloat16
BNNS Loss: Error output size>1 is only allowed with reduction BNNSLossReductionNone.
BNNS Yolo loss: Error out width (size[0]) must be 1 as yolo loss is always reduced
BNNS Loss: Error anchors is NULL
BNNS Loss: Error number of grid rows is 0
BNNS Loss: Error number of grid columns is 0
BNNS Loss: Error number of anchor boxes is 0
BNNS Loss: Error anchor box size <= 5
BNNS Loss: Error object minimum iou is negative
BNNS Loss: Error no object maximum iou is negative
BNNS Loss: Error input descriptor width (%zu) different from expected width of grid_size*num_anchors*(5+num_+classes) (%zu)
BNNS Loss: memory allocation failed
BNNS Loss: Error filter is NULL
BNNS Loss: Error batch_size is 0
BNNS Loss: Error in pointer is NULL
BNNS Loss: Error in_stride is smaller than i_desc.size[0] or Tensor size
BNNS Loss: Error labels is NULL
BNNS Loss: Error labels_stride is smaller than i_desc.size[0] or Tensor size
BNNS Loss: Error weights_size>0 but weights pointer is NULL
BNNS Loss: Error out is NULL
BNNS Loss: Error in_delta descriptor is ilegal
BNNS Loss: Error in_delta must be contiguous, such that stride[0] is 0 or 1
BNNS Loss Error: data type not supported for softmax
BNNS Yolo loss: input and output data type must be BNNSDataTypeFloat32
BNNS Loss Error: data type not supported
BNNS Loss Error: unsupported loss function
BNNS Loss Backward: Error filter is NULL
BNNS Loss Backward: Error batch_size is 0
BNNS Loss Backward: Error in is NULL
BNNS Loss Backward: Error in_stride is smaller than i_desc.size[0]
BNNS Loss Backward: Error labels is NULL
BNNS Loss Backward: Error labels_stride is smaller than i_desc.size[0]
BNNS Loss Backward: Error weights_size>0 but weights pointer is NULL
BNNS Loss Backward: Error in_delta descriptor is illegal
BNNS Loss Backward: Error in_delta must be contiguous, such that stride[0] is 0 or 1
BNNS Loss Backward: Error in_delta is NULL
BNNS Loss Backward: Error out_delta descriptor is illegal
BNNS Loss Backward: Error out_delta must be contiguous, such that stride[0] is 0 or 1
BNNS Loss Backward: Error out_delta is NULL
BNNS Loss: data type not supported
BNNS Loss: unsupported loss backward function
BNNS Yolo loss: Error yolo reduction type must be BNNSLossReductionSum
BNNS Yolo loss: Error input descriptor width does not match grid_rows*grid_columns*anchors*(5+classes)
BNNS Yolo loss: alloc failed
BNNS Yolo loss: malloc failed
BNNS softmax cross entropy loss: malloc for pDx2 failed
BNNS softmax cross entropy loss: malloc for norm_x failed
BNNS softmax cross entropy loss: malloc for loss failed
BNNS Loss: Error reduction BNNSLossReductionNonZeroWeightMean all weights are zero
BNNS loss: malloc for loss failed
BNNS categorical cross entropy loss: malloc for loss failed
BNNS softmax cross entropy loss: malloc for temporary_softmax_forward_output failed
BNNS Create Depthwise Convolution: incompatible numbers of channels between images and convolution parameters
BNNS Create Depthwise Convolution: unsupported weight format
BNNS Create Depthwise Convolution: input must be BNNSDataLayoutImageCHW layout 
BNNS Create Depthwise Convolution: input descriptor is illegal
BNNS Create Depthwise Convolution: output must be BNNSDataLayoutImageCHW layout 
BNNS Create Depthwise Convolution: output descriptor is illegal
BNNS Depthwise Convolution Create: weights descriptor is illegal
BNNS Depthwise Convolution Create: does not support asymmetric padding. left and right pad must match, up and down pad must match
BNNS Create Depthwise Convolution: only float32 is supported
BNNS Create Depthwise Convolution: channel multiplier mismatch
BNNS Create Depthwise Convolution: malloc failed
BNNS Create Depthwise Convolution: weights malloc failed 
invalid argument
BNNS DEPTHWISE CONV: input batch stride doesn't make sense (%zu < %zu x %zu)
BNNS DEPTHWISE CONV: output batch stride doesn't make sense (%zu < %zu x %zu)
  params = {
    .use_dispatch = %c, .num_contiguous_regions = %llu, .regions_per_block = %llu, .elements_per_block = %llu
    .type = %s, .contig_size = %llu, .non_contig_rank = %llu,
    .non_contig_size = {
%s%llu
}, .non_contig_dest_stride = {
}, .non_contig_src_stride = {
= rnn(x=
BNNS LSTM Direct Apply: filter initialization failed
BNNS LSTM Direct Apply: training cache capacity isn't sufficient
BNNS LSTM Direct Apply: Dropout is only supported in the presence of a training cache
BNNS LSTM Direct Apply: failed to allocate scratch buffer
BNNS LSTM Direct Apply: failed to thread workspace buffer
BNNS LSTM Direct Apply Backward: BNNSNDArrayFlagBackpropAccumulate is only supported on layer_delta->input_descriptor.data_desc.
BNNS LSTM Direct Apply Backward: layer_params initialization failed
BNNS LSTM Direct Apply Backward: layer_delta initialization failed
BNNS LSTM APPLY BACKWARD: Use of dropout without training cache is not supported.
BNNS LSTM Direct Apply Backward: failed to allocate training cache buffer
BNNS LSTM Direct Apply Backward: failed to compute forward intermediate results
BNNS LSTM Direct Apply Backward: training cache capacity isn't sufficient
BNNS LSTM Direct Apply Backward: failed to allocate scratch buffer
BNNS LSTM init: hidden_size must be greater than zero
BNNS LSTM init: input descriptor.data isn't set correctly (size don't match seq_len/batch_size/input_size)
BNNS LSTM init: input descriptor.data isn't set correctly (size don't match batch_size/seq_len/input_size)
BNNS LSTM init: input descriptor.data isn't set correctly (size don't match input_size/batch_size/seq_len)
BNNS LSTM init: input descriptor.hidden isn't set correctly (size don't match hidden_size/batch_size/num_directions/num_layers)
BNNS LSTM init: input descriptor.cell_state isn't set correctly (size don't match hidden_size/batch_size/num_directions/num_layers)
BNNS LSTM init: output descriptor.data isn't set correctly (size don't match seq_len/batch_size/hidden_size*num_directions)
BNNS LSTM init: output descriptor.data isn't set correctly (size don't match batch_size/seq_len/hidden_size*num_directions)
BNNS LSTM init: output descriptor.data isn't set correctly (size don't match hidden_size/num_directions/batch_size/seq_len)
BNNS LSTM init: output descriptor.hidden isn't set correctly (size don't match hidden_size/batch_size/num_directions/num_layers)
BNNS LSTM init: output descriptor.cell_state isn't set correctly (size don't match hidden_size/batch_size/num_directions/num_layers)
BNNS LSTM init: forget_gate isn't set correctly (size don't match hidden_size/batch_size/num_directions/num_layers)
BNNS LSTM init: input_gate isn't set correctly (size don't match hidden_size/batch_size/num_directions/num_layers)
BNNS LSTM init: candidate_gate isn't set correctly (size don't match hidden_size/batch_size/num_directions/num_layers)
BNNS LSTM init: output_gate isn't set correctly (size don't match hidden_size/batch_size/num_directions/num_layers)
BNNS LSTM: Data type not supported, fail
softmax
conv
conv_transpose
cast
crop
upsample_nearest_neighbor
upsample_bilinear
resize_bilinear
resize_nearest_neighbor
crop_resize
linear
matmul
avg_pool
max_pool
l2_pool
batch_norm
instance_norm
layer_norm
random_categorical
lstm
scatter
scatter_along_axis
scatter_nd
cumsum
identity
range_1d
shape
tile
topk
concat
depth_to_space
expand_dims
gather
gather_along_axis
gather_nd
pixel_shuffle
pixel_unshuffle
reshape
slice_by_index
slice_by_size
space_to_depth
split
squeeze
stack
transpose
constexpr_cast
constexpr_lut_to_dense
: Failed to parse x argument
: Failed to parse y argument
values
axis
interleave
m2a_attr
palletize_int4
Failed to palletize %s
: Failed to parse Output 
output_dtype
source_val
: only 4-bit palletization of fp16 is supported
indices
%s: only 1d and 2d convolutions are supported
weights
weight
strides
dilations
pad_type
%s: Unsupported conv pad type '%s'
groups
output_shape
crop_height
: Failed to parse `crop_height`
crop_width
: Failed to parse `crop_width`
: Failed to parse roi argument
target_height
target_width
normalized_coordinates
spatial_scale
box_coordinate_mode
CORNERS_HEIGHT_FIRST
CORNERS_WIDTH_FIRST
CENTER_SIZE_HEIGHT_FIRST
CENTER_SIZE_WIDTH_FIRST
sampling_mode
STRICT_ALIGN_CORNERS
ALIGN_CORNERS
DEFAULT
OFFSET_CORNERS
UNALIGN_CORNERS
exclusive
: Failed to parse weight argument
squeeze(of:
, for: matvec(
internal.squeeze
pre_expand(of:
internal.expand_dims
matrix
mode
constant
reflect
replicate
: Unsupported pad mode '
constant_val
: Unsupported data type for constant_val
start
step
align_corners
scale_factor_height
scale_factor_width
UPSAMPLE scale_factor data type not supported
UPSAMPLE need scale_factor
target_size_height
target_size_width
RESIZE need target_size
: BNNS does not support peephole LSTM
direction
: BNNS does not support direction = 
output_sequence
recurrent_activation
cell_activation
activation
: BNNS does not support LSTM clip
: Direction 
 not supported
: Activation type 
begin
stride
begin_mask
end_mask
squeeze_mask
: BNNS does not support non-constant squeeze_mask
size
num_splits
split_sizes
: split size 
 not divisiable by number of outputs 
: Input Tensor rank 
doesn't match Output Tensor rank 
reps
ascending
output_values
output_indices
undefined shuffle operator.
n_threads
perm
epsilon
beta
SIGMOID_APPROXIMATION
: Failed to parse alpha argument
: Failed to parse shape argument
: Unsupported mode '
seed
axes
keep_dims
mean
: Failed to parse mean argument
variance
: Failed to parse variance argument
gamma
: axes are not contiguous
: Unsupported gather type
: Failed to parse input argument
: Failed to parse indices argument
updates
: Failed to parse updates argument
: Unsupported scatter type
transpose_x
transpose_y
: only 1d and 2d avg pooling are supported
kernel_sizes
: Unsupported avg_pool pad type '
exclude_padding_from_average
ceil_mode
: ceil_mode must be false
: input width and padding don't have enough elements to generate output
: input height or padding don't have enough rows to generate output
: only 1d and 2d max pooling are supported
: Unsupported pad type '
: only 1d and 2d l2 pooling are supported
minimum_compute_type
: BNNS does not support dynamic shaped literal tensors
string
BNNS Graph Compile: Unsupported data type '%s'
BNNS Graph Compile: Unsupported data type
: Failed to parse 
 argument
: BNNS does not support 
 type 
Can't cast this scalar :(
permute only supports fp16 and fp32 operands
constexpr_replaced(
BNNS: Model was compiled for hardware not available on this device
gather only supports fp16, fp32 and int32 inputs
gather only supports fp16 and fp32 outputs
BNNS Convolution Create: Client needs to support weights ptr with low_mem
BNNS Convolution Create: malloc
BNNS Convolution Create: failed to create convolution context
BNNS Convolution Create: failed to divide work between convolution contexts to fit in memory
BNNS Convolution Create: failed to create single generic convolution
BNNS Convolution Apply: invalid argument
BNNS Convolution Apply: failed to allocate input auxiliary buffer
BNNS Convolution: allocation of contexts failed
BNNS LOW MEM CONVOLUTION: failed to create winograd convolution
BNNS LOW MEM CONVOLUTION: malloc failed
BNNS LOW MEM CONVOLUTION: incompatible BNNS_low_memory_context id
BNNS LOW MEM CONVOLUTION: convolution failed
BNNS LOW MEM CONVOLUTION: input aux buffer wasn't allocated
BNNS LOW MEM CONVOLUTION:: context type not supported
BNNS Convolutions Apply: unexpected input data type
BNNS Convolution Create: apply convolution failed
BNNS graph batchnorm: Unrecognized data type
BNNS graph layernorm: Unrecognized data type
BNNS graph channelnorm: Unrecognized data type
BNNS Quantization Filter: layer_params is NULL
BNNS Quantization: input layout and output layout must match
BNNS Quantization: unsupported input/output layouts
BNNS Quantization: input descriptor error
BNNS Quantization: input descriptor data is NULL
BNNS Quantization: output descriptor error
BNNS Quantization: output descriptor data is NULL
BNNS Quantization: invalid quantizer function
BNNS Quantization: BNNSQuantize function supported for the following input descriptor data types: BNNSDataTypeFloat32, BNNSDataTypeFloat16, BNNSDataTypeBFloat16, BNNSDataTypeInt32
BNNS Quantization: BNNSQuantize function supported for the following output descriptor data types: BNNSDataTypeInt8, BNNSDataTypeUInt8, BNNSDataTypeInt16, BNNSDataTypeUInt16, BNNSDataTypeInt32, BNNSDataTypeUInt32
BNNS Quantization: BNNSDeQuantize function supported for the following input descriptor data types: BNNSDataTypeInt8, BNNSDataTypeUInt8, BNNSDataTypeInt16, BNNSDataTypeUInt16, BNNSDataTypeInt32, BNNSDataTypeUInt32
BNNS Quantization: BNNSDeQuantize function supported for the following output descriptor data types: BNNSDataTypeFloat32, BNNSDataTypeFloat16, BNNSDataTypeBFloat16, BNNSDataTypeInt32
BNNS Quantization: invalid axis_mask, number of mask set bits must be lower or equal to 1
BNNS Quantization: axis_mask bits are set beyond batch dimension
BNNS Quantization: Error dimensions of input tensor (%zu) and output tensor (%zu) must match
BNNS Quantization: Error shape of input tensor and output tensor must match
BNNS Quantization Filter: scale layout must be BNNSDataLayoutVector
BNNS Quantization: scale descriptor error
BNNS Quantization Filter: bias layout must be BNNSDataLayoutVector
BNNS Quantization: bias descriptor error
BNNS Quantization: bias vector size does not match axis mask, bias size[0]=%zu, expected %zu
BNNS Quantization: scale vector size does not match axis mask, scale size[0]=%zu, expected %zu
BNNS Fused Convolution and Normalization: Error convolution_layer_params is NULL
BNNS Fused Convolution and Quantization: Error quantization_layer_params is NULL
BNNS Fused Compute and Quantization: Error Convolution output layout and Quantization input layout must match
BNNS Fused Compute and Quantization: Error Convolution output data type and Quantization input data type must match
BNNS Fused Compute and Quantization: Error Convolution output descriptor sizes and Quantization input descriptor sizes must match
BNNS Fused Compute and Quantization: Error Convolution output descriptor strides and Quantization input descriptor strides must match
BNNS Fused Compute and Quantization: Error memory allocation failed
BNNS Fused Compute and Quantization create filter failed: compute filter type error
BNNS Fused Compute and Quantization create filter failed
BNNS Quantization filter failed
BNNS Fused Fully Connected and Quantization: Error fully_layer_params is NULL
BNNS Fused Fully Connected and Quantization: Error quantization_layer_params is NULL
BNNS Fused Compute and Quantization: Error Fully Connected output layout and Quantization input layout must match
BNNS Fused Compute and Quantization: Error Fully Connected output data type and Quantization input data type must match
BNNS Fused Compute and Quantization: Error Fully Connected output descriptor sizes and Quantization input descriptor sizes must match
BNNS Fused Compute and Quantization: Error Fully Connected output descriptor strides and Quantization input descriptor strides must match
BNNS Quantization: unsupported data type
BNNS Quantization: unsupported quantization input data type
BNNS Quantization: unsupported quantization output data type
BNNS Quantization: unsupported dequantization input data type
BNNS Quantization: unsupported dequantization output data type
BNNS Transposed Convolution Create: input data type isn't supported
BNNS Transposed Convolution Create: weight data isn't supported
BNNS Transposed Convolution Create: output data type isn't supported
BNNS Transpsoed Vector Convolution Apply: activation gradient auxilary allocation failed
BNNS Transposed Convolution Apply: Filter is NULL
BNNS Transposed Convolution Apply: failed to allocate memory to manipulate input
BNNS Transposed Convolution Apply: unknown weight layout
BNNS Transposed Convolution Create: layer_params is NULL
BNNS Transposed Convolution Create: output channels must divide by groups
BNNS Transposed Convolution Create: input channels must divide by groups
BNNS Transposed Convolution Create: low-mem isn't supported
BNNS Transposed Convolution Create: packed weights aren't supported
BNNS Transposed Convolution Create: failed to create forward path
BNNS Transposed Convolution Create: failed to allocate wrapper memory
BNNS Transposed Convolution Create: transposed convolution groups only supported with BNNSDataLayoutConvolutionWeightsOIHW or BNNSDataLayoutConvolutionWeightsOIHrWr layouts
BNNS Transposed Convolution Create: weights allocation failed
BNNS Create Layer Transposed Convolution: input channels must be divisible by groups
BNNS Create Layer Transposed Convolution: output channels must be divisible by groups
BNNS Create Layer Transposed Convolution: bias must match output channels
BNNS Create Layer Transposed Convolution: failed to allocate memory to copy weights and bias
BNNS Transposed Convolution Create: malloc failed
BNNS Transposed Convolution: weights allocation failed
BNNS Transposed Convolution: bias allocation failed
BNNS Transposed Convolution Reorder Weights: NULL weight object
BNNS Transposed Convolution Reorder Weights: dst weight size too small
BNNS Transposed Convolution Reorder Weights: NULL weight array
BNNS Transposed Convolution Rotate Weights: NULL weight object
BNNS Transposed Convolution Rotate Weights: dst weight size too small
BNNS Transposed Convolution Rotate Weights: NULL weight array
BNNS Transposed Convolution Reorder and Rotate Weights: NULL weight object
BNNS Transposed Convolution Reorder and Rotate Weights: dst weight size too small
BNNS Transposed Convolution Reorder and Rotate Weights: NULL weight array
transposed convolution input manipulation failed
transposed convolution input manipulation failed - nothing to do, should not have allocated dst_input buffer
BNNS: unsupported activation gradient data type
BNNS Transposed Convlution Backward: dy padding failed
BNNS Transposed Convlution Backward: no padding is needed, dy padding failed
BNNS Create Convolution: fp32 weights allocation failed
BNNS Create Convolution: int16 weights allocation failed
BNNS Create Convolution: failed to allocate memory to copy the weights
BNNS Create Convolution: single descriptors allocation failed
BNNS Convolution Create: allocation failed
failed to upconvert or copy weights
failed to allocate none generic format
BNNS Convolution Create: forward pass check (swapping input and output)  - input size computation wraparound input (%zu x %zu), output (%zu x %zu), stride (%zu x %zu), asymmetric padding (%zu,%zu,%zu,%zu)
BNNS Convolution Create: forward pass check (swapping input and output)  - input size computation wraparound input (%zu x %zu), output (%zu x %zu), stride (%zu x %zu), padding (%zu x %zu)
BNNS Convolution Create: forward pass check (swapping input and output) - output(%zu x %zu x %zu) is larger than input(%zu x %zu x %zu) with padding(%zu x %zu).
 kernel (%zu x %zu) and stride (%zu x %zu)
v8@?0
com.apple.accelerate.bnns
layer
BNNS Fully Connected Sparsify: out descriptor can't be NULL
BNNS Fully Connected Sparse: failed to allocate memory to sparsify COO
BNNS Fully Connected Sparse: insufficient memory allocated memory to sparsify COO
BNNS Fully Connected Sparse: Number of nonzeros is 0
BNNS Fully Connected Sparsify: in descriptor can't be NULL
BNNS Normalization: layer_params is NULL
BNNS Normalization: filter memory allocation failed
BNNS Fused Convolution and Normalization: Error normalization_layer_params is NULL
BNNS Fused Convolution and Normalization: Error convolution output descriptor and normalization input descriptor must have the same sizes and strides
BNNS Fused Convolution and Normalization: Error memory allocation failed
BNNS Fused Convolution and Normalization create filter failed: Convolution type error
BNNS Fused Convolution and Normalization create filter failed
BNNS Fused Fully Connected and Normalization: Error fully_layer_params is NULL
BNNS Fused Fully Connected and Normalization: Error batch_normalization_layer_params is NULL
BNNS Fused Fully Connected and Normalization: Error fully connected output descriptor and normalization input descriptor must have the same sizes and strides
BNNS Fused Fully Connected and Normalization: Error memory allocation failed
BNNS Fused Fully Connected and Normalization create filter failed
BNNS Fused Arithmetic and Normalization: Error arithmetic_layer_params is NULL
BNNS Fused Arithmetic and Normalization: Error normalization_layer_params is NULL
BNNS Fused Arithmetic and Normalization: unsupported arithmetic function
BNNS Fused Arithmetic and Normalization: Error arithmetic output descriptor and normalization input descriptor must have the same sizes and strides
BNNS Fused Arithmetic and Normalization: Error memory allocation failed
BNNS Fused Arithmetic and Normalization create filter failed
BNNS Normalization: input pointer is NULL
BNNS Normalization: batch_size>1 and in_stride is 0
BNNS Normalization: output pointer is NULL
BNNS Normalization: batch_size>1 and out_stride is 0
BNNS Normalization: batch_size too large. Backprop cache size limits batch_size <= %zu.
BNNS Normalization: x_hat allocation failed
BNNS Normalization: filter id not supported
BNNS Normalization: inverse variance allocation failed
BNNS Normalization Apply Backward: filter is NULL
BNNS Normalization Apply Backward: Normalization beta delta pointer is NULL. backward computation of all active parameters must be done in a single function call.
BNNS Normalization Apply Backward: Normalization gamma delta pointer is NULL. backward computation of all active parameters must be done in a single function call.
BNNS Normalization Apply Backward: make sure to run normalization forward with training flag enabled before running normalization backward
BNNS Normalization Apply Backward: cannot compute activation backward, output is NULL
BNNS Normalization Apply Backward: Fusion of normalization with activation is unsupported for given activation function
BNNS Normalization Apply Backward: cannot compute input delta because input delta data pointer is NULL
BNNS Normalization Apply Backward: Error normalization input delta and input must have the same sizes
BNNS Normalization Apply Backward: in delta should be contiguous and same size as input size
BNNS Normalization Apply Backward: cannot compute beta delta because beta delta data pointer is NULL
BNNS Normalization Apply Backward: BNNSNDArrayFlagBackpropAccumulate is no supported for beta_delta.
BNNS Normalization Apply Backward: Error normalization input delta and input must have the same sizes and strides
BNNS Normalization Apply Backward: Error beta_delta descriptor must match the shape of input up to the normalizaton axis
BNNS Normalization Apply Backward: beta delta should be contiguous and same size as number of input channels
BNNS Normalization Apply Backward: cannot compute gamma delta because gamma delta data pointer is NULL
BNNS Normalization Apply Backward: BNNSNDArrayFlagBackpropAccumulate is no supported for gamma_delta.
BNNS Layer Normalization: Error gamma_delta descriptor must match the shape of input up to the normalizaton axis
BNNS Normalization Apply Backward: gamma delta should be contiguous and same size as number of input channels
BNNS Normalization Apply Backward: out delta data is NULL
BNNS Normalization Apply Backward: Error Normalization output delta and output must have the same sizes
BNNS Normalization Apply Backward: out delta should be contiguous and same size as input size
BNNS Normalization Apply Backward: normalization type not supported
BNNS Normalization Set State: Backprop cache must be in BNNSDataLayoutVector
BNNS Normalization Set State: Backprop cache must be in BNNSDataTypeFloat32
BNNS Normalization Set State: Backprop cache must be in contiguous
BNNS Normalization Set State: Backprop cache pointer is NULL
BNNS Normalization Set State: Backprop cache too small, must be >= %zu fp32 to allow max_batch_size >= 1
BNNS Normalization: Only elementwise activation function is supported
BNNS Normalization: Moving Mean must be BNNSDataTypeFloat32
BNNS Normalization: Moving Mean isn't set properly
BNNS Normalization: Moving Variance must be BNNSDataTypeFloat32
BNNS Normalization: Moving Variance isn't set properly
BNNS Normalization Create: normalization type not supported
BNNS Normalization: input descriptor isn't set properly
BNNS Normalization: Input must be in BNNSDataLayoutImageCHW layout
BNNS Normalization: output descriptor isn't set properly
BNNS Normalization: Output must be in BNNSDataLayoutImageCHW layout
BNNS Normalization: Gamma descriptor isn't set properly
BNNS Normalization: Beta descriptor isn't set properly
BNNS Normalization: Error input descriptor must have stride[0]=1
BNNS Normalization: Error output descriptor must have stride[0]=1
BNNS Normalization: Error input size (%zu) different from output size (%zu)
BNNS Normalization: Error normalization_axis must be 0, 1, or 2
BNNS Layer Normalization: Gamma descriptor must match the shape of input up to the normalizaton axis
BNNS Normalization: Beta descriptor must match the shape of input up to the normalizaton axis
BNNS Normalization: Gamma descriptor size[0] must be the same as number of input channels
BNNS Normalization: Beta descriptor size[0] must be the same as number of input channels
BNNS Normalization: Moving Mean descriptor size[0] must be the same as number of input channels
BNNS Normalization: Moving Variance descriptor size[0] must be the same as number of input channels
BNNS Normalization: Error momentum must be between 0 and 1
BNNS CONVOLUTIONS VERSION2: malloc failed
BNNS CONVOLUTIONS VERSION2: malloc failed
BNNS CONVOLUTIONS VERSION2: failed to allocate bias conversion memory
BNNS CONVOLUTIONS VERSION2: failed to allocate scratch memory
BNNS CONVOLUTIONS VERSION2: invalid argument
BNNS CONVOLUTIONS VERSION2: input batch stride doesn't make sense (%zu < %zu x %zu)
BNNS CONVOLUTIONS VERSION2: output batch stride doesn't make sense (%zu < %zu x %zu)
BNNS CONVOLUTIONS VERSION2: unsupported weight packing
BNNS CONVOLUTIONS VERSION2: failed to allocated memory to convert bias
BNNS CONVOLUTIONS VERSION2: input padding memory allocation failed
BNNS CONVOLUTIONS VERSION2: output repack malloc failed
BNNS CONVOLUTIONS VERSION2: weight repack malloc failed
BNNS Padding Create: layer_params is NULL
BNNS Padding Create: Padding is not supported beyond 4-D tensors.
BNNS Padding Create: Undefined padding mode.
BNNS Padding Create: Unsupported data layout.
BNNS Padding Create: input and output desciptors have differing numbers of dimensions.
BNNS Padding Create: Input dimension is too small for the padding size.
BNNS Padding Create: Input size + padding sizes doesn't match output size in dimension %zu.
BNNS Padding Create: input descriptor is illegal
BNNS Padding Create: output descriptor is illegal
BNNS Padding Create: Unsupported data type.
BNNS Padding Create: I/O data type mismatch.
BNNS Padding Create: memory allocation failed
BNNS Padding Apply: filter is NULL
BNNS Padding Apply: wrong filter type, filter is not Padding.
BNNS Padding Apply: input pointer is NULL
BNNS Padding Apply: batch_size > 1 and in_stride is 0
BNNS Padding Apply: output pointer is NULL
BNNS Padding Apply: batch_size > 1 and out_stride is 0
BNNS Padding Apply Backward: filter is NULL
BNNS Padding Apply Backward: wrong filter type, filter is not padding
BNNS Padding Apply Backward: input descriptor is NULL
BNNS Padding Apply Backward: input data pointer is NULL
BNNS Padding Apply Backward: batch_size>1 and in_stride is 0
BNNS Padding Apply Backward: output delta descriptor is NULL
BNNS Padding Apply Backward: output delta data pointer is NULL
BNNS Padding Apply Backward: batch_size>1 and out_delta_stride is 0
[range_1d] type not supported.
BNNS MIL Matvec Create: unexpected compute path
BNNS Pooling: Error wrong filter type, filter is not pooling
out_delta is NULL
out_delta->data is NULL
BNNSFilterCreateLayerSparseEmbedding: layer_params must not be NULL
BNNSSparseGetRepresentation: filter must not be NULL
BNNSSparseGetRepresentation: Invalid filter
BNNSSparseSetRepresentation: filter must not be NULL
BNNSSparseSetRepresentation: Invalid filter
BNNSSparseEmbeddingGetDense: filter must not be NULL
BNNSSparseEmbeddingGetDense: Invalid filter
BNNS Fused Filter Multi-Input: Error filter is NULL
BNNS Fused Filter Backward Multi-Input: Error filter is NULL
BNNS Fused Filter Backward Multi-Input: output delta is NULL
BNNS Arithmetic Backward Multi-Input: failed to allocate memory
x_ijf*, x_ijc* -> G_fc*
BNNSFilterApplyTwoInputBatch: invalid argument
Tensor Contraction filter only expects one input
invalid filter or incorrect number of inputs
BNNSFilterApplyBackwardTwoInputBatch: invalid argument
Two-input Tensor Contraction filter has no weights
Tensor Contraction filter has no bias
Unsupported filter
BNNS GetStateSize : filter is NULL
BNNS SetState: Error filter is NULL
BNNS SetState: Source filter state is NULL
BNNS SetState: Target filter is NOT Dropout/Normalization filter
BNNS GetState: Error filter is NULL
BNNS GetState: Target filter state is NULL
BNNS GetState: Source filter is NOT Dropout filter
BNNS LayerNorm: Error Normalization axis must be 0, 1, or 2.
BNNS GroupNorm: Error The number of input channels must be divisible by the number of groups.
BNNS Fused Filter: Error number_of_fused_filters is not 2. currently supporting only 2 fused filters
BNNS Fused Filter: Error filter_type is NULL
BNNS Fused Filter: Error layer_params is NULL
BNNS Fused Filter: Error currently supporting first filter of BNNSConvolution, BNNSFullyConnected, BNNSTransposedConvolution, or BNNSArithmetic type only
BNNS Fused Filter: Error currently supporting second filter of BNNSBatchNorm, BNNSInstanceNorm, BNNSLayerNorm, BNNSGroupNorm and BNNSQuantization types only
BNNS Fused Filter: Error layer_params[0] is NULL
BNNS Fused Filter: Error layer_params[1] is NULL
BNNS Fused Filter: Error unknown fused filter type
BNNS Normalization: Error filter is NULL
BNNS Normalization: Error wrong filter type, filter is not normalization
BNNS Fused Filter: Error filter is NULL
BNNS Arithmetic Filter: Error filter is NULL
BNNS Permute Filter: filter is NULL
BNNS Permute Filter: input delta is NULL
BNNS Permute Filter: output delta is NULL
BNNS Permute Filter: input delta data pointer is NULL
BNNS Permute Filter: output delta data pointer is NULL
BNNS Permute Filter: inplace gradient is not supported
Tensor Contraction filter has no bias: bias_delta must be NULL.
Tensor Contraction filter expects more than one input
Resize filter has no bias: bias_delta must be NULL.
Resize filter has no weights: weights_delta must be NULL.
Resize filter: backward pass requires in_delta and in_delta->data to be non-NULL.
BNNS Copysum filter has no bias: bias_delta must be NULL.
BNNS Copysum filter has no weights: weights_delta must be NULL.
BNNS Copysum filter: backward pass requires in_delta and in_delta->data to be non-NULL.
BNNS Reduction filter has no bias: bias_delta must be NULL.
BNNS Padding: backward pass requires in_delta and in_delta->data to be non-NULL.
BNNS Padding filter has no bias: bias_delta must be NULL.
BNNS Embedding: in must not be NULL.
BNNS Embedding: in_delta must be NULL.
BNNS Embedding: backward pass requires weights_delta and weights_delta->data to be non-NULL.
BNNS Embedding: bias_delta must be NULL.
BNNS Normalization Backward: Error filter is NULL
BNNS Fused Filter Backward: Error filter is NULL
BNNS Fused Filter Backward: output delta is NULL
BNNS Fused Filter Backward: Error delta_parameters is NULL
BNNS Arithmetic Backward: failed to allocate memory
BNNSDirectApplyConvolutionBatch not supported 
BNNSDirectApplyTransposedConvolutionBatch not supported 
BNNSDirectApplyPoolingBatch not supported 
BNNSDirectApplyLossBatch not supported
BNNS Apply: filter or output can't be null
BNNS Apply: Tensor Contraction filter expects more than one input
BNNS Apply: Loss filter apply must be called with BNNSLossFilterApplyBatch
BNNS Apply: Batchnorm filter apply must be called with BNNSNormalizationFilterApplyBatch
BNNS Apply: Instance Norm filter apply must be called with BNNSNormalizationFilterApplyBatch
BNNS Apply: Layer Norm filter apply must be called with BNNSNormalizationFilterApplyBatch
BNNS Apply: Group Norm filter apply must be called with BNNSNormalizationFilterApplyBatch
BNNS Apply: Fused filter apply must be called with BNNSFusedFilterApplyBatch
BNNS Apply: invalid filter
BNNS DESTROY: invalid filter
Convolution
FullyConnected
Pooling
Activation
SparseConvolution
SparseFullyConnected
Loss
Batchnorm
Instancenorm
Layernorm
Groupnorm
Fused{Compute,Normalization}
Dropout
Contraction
Resize
LSTM
Arithmetic
Copysum
MultiheadAttention
Reduction
Padding
Embedding
Fused{Compute,Quantization}
Fused{Arithmetic,Normalization}
Fused{Affine Grid, Grid Sample}
Unknown
BNNS Padding + Convolution Apply Backward: input is null
BNNS Padding + Convolution Apply Backward: allocation failed
BNNS Padding + Convolution Apply Backward: Padding failed
BNNS Padding + Convolution Apply Backward: Convolution backward failed
BNNS Padding + Convolution Apply Backward: Padding backward failed
malloc failed
Convolution failed
BNNS LSTM Apply: Unsupported data type
anon_context
_literal
BNNS Graph: 
sorted_outputs = 
outputs = 
missing from sorted_outputs:
missing from outputs:
BNNS Band Part: illegal input descriptor
BNNS Band Part: illegal output descriptor
BNNS Band Part: input and output data type or dimension do not match or the dimension is less than two
BNNS Band Part: input and output sizes do not match
BNNS Convolution Variant: input type %s not supported!!!
BNNS Convolution Variant: compute type %s not supported!!!
BNNS GRU Fused Gates: data type not supported
BNNS GRU Fused Gates: Output pointer is null
BNNS GRU Fused Gates: Input pointer is null
BNNS GRU Fused Gates: Recurrent pointer is null
BNNS GRU Fused Gates: Hidden Input pointer is null
BNNS GRU Fused Gates: Input vector size %zu foesn't match output vector size %zu
BNNS GRU Fused Gates: Recurrent vector size %zu foesn't match output vector size %zu
BNNS GRU Fused Gates: Hidden Input vector size %zu foesn't match output vector size %zu
BNNS GRU Fused Gates: number of gates for Input %zu and Recurrent %zu doesn't match
BNNS GRU Fused Gates: Input stride %zu is smaller than number of elements %zu
BNNS GRU Fused Gates: Recurrent stride %zu is smaller than number of elements %zu
BNNS GRU Fused Gates: Output supported data types are fp32 or fp16
BNNS GRU Fused Gates: Input data type doesn't match output data type
BNNS GRU Fused Gates: Recurrent data type doesn't match output data type
BNNS GRU Fused Gates: Hidden Input data type doesn't match output data typen
BNNS GRU Fused Gates: GRU fused gates doesn't support %zu gates
BNNS GRU Fused Gates: Hidden Input batch size %zu doesn't match output batch size %zu
BNNS GRU Fused Gates: Input batch size %zu doesn't match output batch size %zu
BNNS GRU Fused Gates: Hidden Output batch stride %zu is small then a single vector stride
BNNS GRU Fused Gates: Hidden Input batch stride %zu is small then a single vector stride
BNNS GRU Fused Gates: Input batch stride %zu is small then a single vector stride
BNNS GRU Fused Gates: Recurrent batch stride %zu is small then a single vector stride
range_1d does not support dynamic shaped input
range_1d only supports fp16, fp32 and int32 output types
range_1d requires the types of start, end, step and output to match
BNNS CONVOLUTIONS VERSION2: unsupported src type to pack
BNNS Convolution: unsupported src type to pack
BNNS Convolution Compute Weights delta: failed to compute
BNNS Convolution Apply Backward: activation_grad allocation failed
weight delta backward failed
BNNS Prefix sum: unsupported data type
BNNS Convolution: int32 repack allocation failed
BNNS Convolution: Float32 repack allocation failed
copy from 
BNNS Activation Apply: Error input/output/activation pointers must be non-NULL
BNNS Activation Apply: Input and output tensor doesn't match
BNNS Activation Apply: unable to allocate memory to compute activation
BNNS Activation Apply: failed to allocate memory
BNNS Apply Activation Backward: BNNSActivationFunctionIntegerLinearSaturate isn't supported
BNNS Apply Activation Backward: BNNSActivationFunctionIntegerLinearSaturatePerChannel isn't supported
BNNS Apply Activation Backward: failed to allocate temporary y
BNNS Apply Activation Backward: unsupported activation backward
BNNS Activation backward: malloc failed
BNNS Activation: number of memory descriptor is lower than expected
BNNS Activation: claiming pre allocated memory failed
BNNS Activation: memory allocation failed
BNNS Activation: Failed to init filter
BNNS Activation Apply Backward: At least one of in or out must be non-NULL
BNNS Activation Apply Backward: invalid filter
BNNS Activation Apply Backward: out_delta must not be NULL
BNNS Activation Backward: preallocated memory isn't supported
BNNS Activation Apply Backward: Input is required
BNNS Activation Backward: Weights Delta not supported
BNNS Activation Apply Backward: Bias Delta not supported
BNNS Activation Apply Backward: in place operation for none matching data sizes isn't supported
BNNS Activation Apply Backward: Input and output tensor doesn't match
BNNS Activation Apply Backward: In Delta tensor doesn't match
BNNS Activation Apply Backward: Out Delta tensor doesn't match
BNNS Activation Apply Backward: Weights Delta tensor (%zu x %zu) doesn't make sense
BNNS Activation Apply Backward: unable to allocate memory to compute backward with missing original output
BNNS Activation Apply Backward: failed to allocate memory
BNNS Activation Apply Backward: failed to init index counter
BNNS Activation: invalid argument
BNNS Activation: in-place activation layer is allowed only for output types with the same or smaller storage size
BNNS Activation: unsupported types for conversion
BNNS Activation: val doesn't make sense
BNNS Activation Apply: activation function not supported
BNNS Activation Init: filter is null
BNNS Activation Init: input descriptor is illegal
BNNS Activation Init: output descriptor is illegal
BNNS Activation Init: layout doesn't match
BNNS Activation Init: memory allocation failed
BNNS Activation Init: dim %zu input size %zu != %zu output size
BNNS Activation Init: alpha can't be +/-inf or Nan for Gumbel or Gumbel Max
BNNS Activation Init: beta can't be +/-inf or Nan or zero or negative for Gumbel or Gumbel Max
BNNS Activation Init: BNNSActivationFunctionPReLUPerChannel is only valid with data layout BNNSDataLayoutImageCHW
BNNS Activation Init: invalid activation function
BNNS SoftMax: sum result shouldn't be 0
BNNS Activation Apply Backward: Softmax backward require original output
BNNS Activation Apply Backward: original inputs or original outputs must be available
BNNS Activation Apply Backward: activation function not supported
BNNS Activation: Error NULL input/output pointer
BNNS Activation: input and output dimensions do not match
BNNS Activation: input and output sizes do not match
Unsupported layout: %d
BNNS Multihead Attention Apply: key_mask must be a 1D tensor.
BNNS Multihead Attention Apply: key_mask must have type BNNSDataTypeBoolean.
BNNS Multihead Attention Apply: key_mask must have size exactly source_length (key_mask.size[0] = %zu, source_length = %zu).
BNNS Multihead Attention Apply: 2D add_to_attention must have shape (target_length, source_length) = (%zu, %zu), but passed tensor has shape (%zu, %zu).
BNNS Multihead Attention Apply: 3D add_to_attention must have shape (num_heads, target_length, source_length) = (%zu, %zu, %zu), but passed tensor has shape (%zu, %zu, %zu).
BNNS Multihead Attention Apply: 4D add_to_attention must have shape (batch_size, num_heads, target_length, source_length) = (%zu, %zu, %zu, %zu), but passed tensor has shape (%zu, %zu, %zu, %zu).
BNNS Multihead Attention Apply: unsupported layout for argument add_to_attention.
BNNS Multihead Attention Apply: add_to_attention must have type BNNSDataTypeBoolean or BNNSDataTypeFloat32.
BNNS Multihead Attention Apply: If backprop_cache is non-NULL, backprop_cache_size must also be non-NULL
BNNS Multihead Attention Apply: If workspace is non-NULL, workspace_size must also be non-NULL
BNNS Multihead Attention Apply: Supplied workspace is too small (size %ld, but required %ld)
Multihead Attention Backward: Only support for a full-size backprop_cache has been implemented.
Multihead Attention Backward: If workspace is non-NULL, workspace_size must also be non-NULL
Multihead Attention Backward: Insufficient workspace supplied (%zu bytes). Require %zu bytes.
value_delta_copy_out
value_shadow_delta
value_delta
key_delta_copy_out
key_shadow_delta
key_delta
query_delta_copy_out
query_shadow_delta
query_delta
layer_params->query.target_desc
layer_params->key.target_desc
layer_params->query.weights
layer_params->key.weights
layer_params->value.weights
(d_model, d_key, num_heads)
(source_length, k_dim)
(k_dim, d_key, num_heads)
layer_params->value.target_desc
(source_length, v_dim)
(v_dim, d_value, num_heads)
layer_params->output.target_desc
(target_length, d_model)
layer_params->output.weights
(num_heads*d_value, d_model)
BNNS Multihead Attention Create: layer_params->key_attn_bias.data and layer_params->value.attn_bias.data must both be NULL or must both be not NULL
layer_params->key_attn_bias
(d_key, num_heads)
layer_params->value_attn_bias
(d_value, num_heads)
BNNS Multihead Attention Create: fp16 and bf16 may not be mixed or unsupported data type
layer_params->query.bias
layer_params->key.bias
layer_params->value.bias
layer_params->output.bias
(d_model)
BNNS MHA: layer_params->query.weights.layout=%u is not compatible with low memory flag.
BNNS MHA: layer_params->key.weights.layout=%u is not compatible with low memory flag.
BNNS MHA: layer_params->value.weights.layout=%u is not compatible with low memory flag.
BNNS Multihead Attention Create: Unsupported layout for query.weights
BNNS Multihead Attention Create: Unsupported layout for key.weights
BNNS Multihead Attention Create: Unsupported layout for value.weights
BNNS MULTIHEAD ATTENTION GetPointer: Layer was created with dropout=0.0, cannot return pointer to dropout value.
BNNS MULTIHEAD ATTENTION GetPointer: Unsupported target %d
BNNS Multihead Attention Create: Expected %s to have exactly %zu dimensions, but it has %zu
BNNS Multihead Attention Create: Expected %s to have shape %s = (
%s%zu
) but has actual shape (
BNNSGather: Only 8, 16, 32 and 64 bit integer types may be used as indices data type
BNNSGather: Only 16 and 32 bit floating point and 8, 16 and 32 bit integer types may be used as input data type
BNNSGather: Only 16 and 32 bit floating point and 8, 16 and 32 bit integer types may be used as output data type
BNNSGather: indices must be a 1d array or have the same shape as output
BNNSGather: input and output tensors must have the same number of dimensions
BNNSGather: output.size[%zu]=%zu must have same size as indices.size[0]=%zu if indices is 1D
BNNSGather: output.size[%zu]=%zu must have same size as indices.size[%zu]=%zu if indices is not 1D
BNNSGather: output.size[%zu]=%zu must have same size as input.size[%zu]=%zu
BNNSScatter: Only add and set ops are supported
BNNSScatter: Only 8, 16, 32 and 64 bit integer types may be used as indices data type
BNNSScatter: Only 16 and 32 bit floating point and 8, 16 and 32 bit integer types may be used as input data type
BNNSScatter: Only 16 and 32 bit floating point and 8, 16 and 32 bit integer types may be used as output data type
BNNSScatter: indices must be a 1d array or have the same shape as input
BNNSScatter: input and output tensors must have the same number of dimensions
BNNSScatter: input.size[%zu]=%zu must have same size as indices.size[0]=%zu if indices is 1D
BNNSScatter: input.size[%zu]=%zu must have same size as indices.size[%zu]=%zu
BNNSScatter: input.size[%zu]=%zu must have same size as output.size[%zu]=%zu
BNNSGatherND: Only 8, 16, 32 and 64 bit integer types may be used as indices data type
BNNSGatherND: Only 16 and 32 bit floating point and 8, 16 and 32 bit integer types may be used as input data type
BNNSGatherND: Only 16 and 32 bit floating point and 8, 16 and 32 bit integer types may be used as output data type
BNNSGatherND: indices.size[%zu]=%zu must be less than or equal to input.ndim=%zu
BNNSScatterND: Only 8, 16, 32 and 64 bit integer types may be used as indices data type
BNNSScatterND: Only 16 and 32 bit floating point and 8, 16 and 32 bit integer types may be used as input data type
BNNSScatterND: Only 16 and 32 bit floating point and 8, 16 and 32 bit integer types may be used as output data type
BNNSScatterND: indices.size[%zu]=%zu must be less than or equal to output.ndim=%zu
internal scatter copy
dict-item
o-item
BNNSGatherND: output tensor (shape [%s]) must have shape matching first n-1 dimensions of indices + shape of any dimensions of input not specified in final dimension of indices (i.e. [%s]).
BNNSScatterND: input tensor (shape [%s]) must have shape matching first n-1 dimensions of indices + shape of any dimensions of output not specified in final dimension of indices (i.e. [%s]).
internal gatherND copy
src-item
dest-item
internal scatterND copy
 = reshape(from=
.... %s refused on principle
BNNS Graph Compile: Allocation failed in construction of maxpool op
, kernel_size=
, kernel_stride=
, pad=
BNNS Graph Compile: Allocation failed in construction of avgpool op
BNNS Graph Compile: Allocation failed in construction of l2pool op
shape only supports int32 outputs
BNNS_pruned
bnns_fc_sparse_opaque(
, for=
MIL BNNS Matvec Repack Data: couldn't sparsify weights
repacked(
matvec op does not support dynamic shaped inputs
matvec op detected unexpected tensor shapes
matvec op requires final dimension to have stride 1
matvec op detected incompatible tensor shape for bias
matvec op requires input types to be FP16 or FP32
matvec op requires output type to be FP16 or FP32
matvec(A=
, x=
, transpose=
pre_bias(
transposed(
matvec_concat_matrix(
matvec_concat_bias(
matvec_concat_output(
matvec_output_slice.
BNNS SPECIALIZED CONVOLUTION Create: failed to alloacted memory
%s: 
BNNS Permute Filter: layer_params is NULL
BNNS Permute Filter: illegal input descriptor
BNNS Permute Filter: illegal output descriptor
BNNS Permute Filter: input and output descriptor layout dimension does not match. input dimm = %zu output dimm = %zu
BNNS Permute Filter: permutation array index %zu has illegal value of %zu
BNNS Permute Filter: permutation array is missing axis %zu
BNNS Permute Filter: permutation array axis %zu appears %zu times, each axis should appear exactly once
0123456789abcdefghijklmnopqrstuvwxyz
BNNS Shuffle: input and output data type do not match
BNNS Shuffle: input or output data is NULL
BNNS Shuffle: input and output dimensions should be 4
BNNS Shuffle: input and output sizes are not valid
BNNS Shuffle: input and output sizes do not match
BNNS Pooling Filter: filter is NULL
BNNS Pooling Filter: input is NULL
BNNS Pooling Filter: output is NULL
BNNS Pooling: input batch stride doesn't make sense (%zu < %zu x %zu)
BNNS Pooling: dilation supported only in BNNSPoolingFunctionMax/UnMax
BNNS Pooling UnMax: indices array is NULL
BNNS Pooling: case not implemented
BNNS Pooling Apply: allocation of work buffer failed
BNNS Pooling Backward: cannot run backward without output delta
BNNS Pooling Backward: only float32 bfloat16 output delta are supported
BNNS Pooling Backward: only float32 bfloat16 input delta are supported
BNNS Pooling Backward: only float32 bfloat16 bias delta are supported
BNNS Pooling Backward: unsupported pooling function
BNNS Pooling Backward: invalid argument
BNNS Pooling Backward: Fusion of compute with activation is unsupported for given activation function
BNNS Pooling Backward: failed to allocate memory
BNNS Pooling Backward: failed to apply activation backward
BNNS Pooling: layer parameters is NULL
BNNS Pooling: input must be a 3D array
BNNS Pooling: input descriptor is illegal
BNNS Pooling: output must be a 3D array
BNNS Pooling: output descriptor is illegal
BNNS Pooling: input/output channel counts do not match
BNNS Pooling: input/output types do not match
BNNS Pooling: invalid kernel dimensions, should be greater than 0
BNNS Pooling: dilation only supported for BNNSPoolingFunctionMax/UnMax
BNNS_POOLING: input size computation wraparound input (%zu x %zu), output (%zu x %zu), stride (%zu x %zu), padding (left %zu right %zu up %zu down %zu)
BNNS_POOLING: input size computation wraparound input (%zu x %zu), output (%zu x %zu), stride (%zu x %zu), padding (%zu x %zu)
BNNS_POOLING: output(%zu x %zu) is larger than input(%zu x %zu) with padding(left %zu right %zu up %zu down %zu).
 stride (%zu x %zu)
BNNS_POOLING: output(%zu x %zu) is larger than input(%zu x %zu) with padding(%zu x %zu).
 stride (%zu x %zu)
BNNS_POOLING: invalid pooling function
BNNS_POOLING: supported input/output data types: float32 float16 bfloat16
BNNS_POOLING: slow path: stride not in {1,2}
BNNS_POOLING: slow path: kernel size not in {2,3,4}
BNNS Pooling: Internal Memory size %zu doesn't match expected %d
BNNS Pooling: claiming pre allocated memory failed
BNNS Pooling: failed to allocate context
BNNS Pooling Backward: only float32 bfloat16 delta are supported
BNNS Pooling Backward: wrong pooling function called
 = stack(values=[
KERNEL_ARITHMETIC_UNARY
KERNEL_ARITHMETIC_BINARY
KERNEL_ARITHMETIC_BINARY_DYNAMIC
KERNEL_ARITHMETIC_BINARY_AMX2
KERNEL_COPY_GENERIC
KERNEL_COPY_AMX1
KERNEL_COPY_AMX2
KERNEL_COPY_AMX3
KERNEL_COPY_DYNAMIC
KERNEL_COPY_PERMUTE_DYNAMIC
KERNEL_COPY_SLICE_BY_INDEX_DYNAMIC
KERNEL_COPY_SLICE_BY_SIZE_DYNAMIC
KERNEL_CAST_GENERIC
KERNEL_CAST_AMX1
KERNEL_CAST_AMX2
KERNEL_CAST_AMX3
KERNEL_CAST_DYNAMIC
KERNEL_TRANSPOSE2D_GENERIC
KERNEL_TRANSPOSE2D_GENERIC_DYNAMIC
KERNEL_CONV_CREATE_APPLY
KERNEL_CONV_CUSTOMIZED_VOICE_ISOLATION_FP32_S1x1_O12x1x16N_I12x1xM_K1x1_P0x0x0x0_GK
KERNEL_CONV_VERSION_2
KERNEL_CONV_FC
KERNEL_CONV_AMX_SPECIAL
KERNEL_CONV_AMX1
KERNEL_CONV_WINOGRAD
KERNEL_CONV_SPECIAL
KERNEL_CONV_LOW_MEM
KERNEL_CONV_VERSION_2_DYNAMIC
KERNEL_CONV_TRANS_EXPAND_INPUT
KERNEL_CONV_TRANS_VECTOR
KERNEL_ELEMENTWISE_ACTIVATION
KERNEL_ELEMENTWISE_ACTIVATION_DYNAMIC
KERNEL_BATCHNORM
KERNEL_INSTANCENORM
KERNEL_LAYERNORM
KERNEL_REDUCE
KERNEL_MATVEC_GENERIC
KERNEL_MATVEC_WAVERNN_0
KERNEL_SPARSE_MATVEC_GENERIC
KERNEL_SPARSE_MATVEC_WAVERNN_0
KERNEL_SPARSE_MATVEC_WAVERNN_1
KERNEL_DOT_PRODUCT_GENERIC
KERNEL_MATMUL_INTEL64
KERNEL_MATMUL_NEON
KERNEL_MATMUL_AMX1
KERNEL_MATMUL_AMX2
KERNEL_MATMUL_AMX3
KERNEL_PAD_GENERIC
KERNEL_PAD_DYNAMIC_GENERIC
KERNEL_TOPK
KERNEL_GUMBELMAX
KERNEL_GRU_FUSED
KERNEL_GATHER
KERNEL_SCATTER
KERNEL_RELATIONAL
KERNEL_RESIZE
KERNEL_SOFTMAX
KERNEL_TILE_GENERIC
KERNEL_RNN
KERNEL_LSTM
KERNEL_GRU
KERNEL_MISC_SHAPE
KERNEL_MAXPOOL
KERNEL_AVGPOOL
KERNEL_L2POOL
KERNEL_SHUFFLE_GENERIC
UNKOWN_KERNEL(
=== IR dump from memory at %p ===
*** HEADER ***
  .ir_version = %s
  .total_file_size = %llu bytes
  .data_section_offset = %llu
  .debug_info_offset = %llu
  .workspace_size = %llu bytes
  .has_dynamic_sizes = %c
  .attributes = {
*** OUTPUTS ***
'%s' -> 0x%llx
*** INPUTS ***
*** OPERATIONS ***
op%5zu %s: {
%s0x%llx
} -> {
} param_offset=%llu param_size=%llu
*** TENSORS ***
0x%llx:
 workspace_offset=%llu
 view_of=0x%llx offset=%llu
 constant_offset=%llu
 user_input
 user_output
 type=%s, shape=[
 %llu
 rank=%u
=== END OF DUMP ===
strict digraph {
%s [label = "%s", shape="box"%s];
tensor_
constant_
%s [label = "%s %s", style=filled, color=lightgray];
%s -> %s;
input_
%s [label = "%s"];
%s -> %s [label = "%s  %s" ];
%s -> %s [label = "%s %s" ];
output_
arm64
amx1
unknown(
,style=filled,color=".7 .3 1.0"
BNNS Reduction Create: Invalid reduction function %u
BNNS Reduction Create: weight data type does not match input
BNNS Reduction Create: i_desc and o_desc must have the same number of dimensions (%zu vs %zu)
BNNS Reduction Create: i_desc and w_desc must have the same number of dimensions (%zu vs %zu)
BNNS Reduction Create: i_desc and w_desc dimensions must have the same size (dimension %zu: %zu vs %zu)
BNNS Reduction Create: i_desc and o_desc dimensions must have the same size, or o_desc.size[d] must be 1 to indicate reduction (dimension %zu: %zu vs %zu)
BNNS Reduction Create: ArgMin/ArgMax reduction is only supported on a single axis
BNNS Reduction Apply: Both input and output must non-NULL
BNNS Reduction Backward: input_delta must have the same number of dimensions as i_desc
BNNS Reduction Backward: input_delta must have the same shape as i_desc
BNNS Reduction Backward: output_delta must have the same number of dimensions as o_desc
BNNS Reduction Backward: output_delta must have the same shape as o_desc
BNNS Reduction Backward: output_delta must not be NULL
BNNS Reduction Backward: weights_delta must have the same number of dimensions as w_desc
BNNS Reduction Backward: weights_delta must have the same shape as w_desc
BNNS Reduction Direct Apply: Size of the dimensions of the output can not be greater than the size of the input
BNNS Reduction Direct Apply: ArgMin/ArgMax reduction is only supported on a single axis
BNNS Reduction: Unsupported data type combination for %u
BNNS Reduce Apply Generic: Size mismatch in dimension %zu: input %zu, output %zu
Backward usage is not supported for reduction type %u
BNNS Reduction Backward: Reduce function %u requires input to be supplied for backward computation
BNNS Reduction Backward: Reduce function %u requires output to be supplied for backward computation
BNNS Reduction Backward: Calculation of weights_delta requires input to be supplied
Size mismatch in dimension %zu: input %zu, output %zu
BNNS Arithmetic: Cannot find binary kernel
BNNS Arithmetic: Attempting to use amx2 kernel on non-amx hardware
BNNS Sparse Fully Connected: out of scratch memory to encode
BNNS Fully Conneceted Sparsify: Sparse data is greater than dense prune, developer should use dense prune
BNNS Sparse Fully Connected: failed to apply activation
: BNNS Graph random_categorical generate_ir() not yet implemented
BNNS Arithmetic Filter Internal: unsupported arithmetic with epsilon function
BNNS Arithmetic Filter: layer_params is NULL
BNNS Arithmetic Filter: unsupported arithmetic function
BNNS Arithmetic Filter: memory allocation failed
BNNS Arithmetic Filter: Failed to init filter
BNNS Arithmetic Filter: filter is NULL
BNNS Arithmetic Filter: wrong filter type, this function should only be used for a filter created with BNNSFilterCreateLayerArithmetic
BNNS Arithmetic Filter: in pointer is NULL
BNNS Arithmetic Filter: batch_size>1 and in_stride is 0
BNNS Arithmetic Filter Internal: expected number of inputs call failed
BNNS Arithmetic Filter: wrong number_of_inputs, number_of_inputs=%zu, expecting %zu
BNNS Arithmetic Filter: in[%zu] is NULL
BNNS Arithmetic Filter: in_stride[%zu] is 0
BNNS Arithmetic Filter: output pointer is NULL
BNNS Arithmetic Filter: batch_size>1 and out_stride is 0
BNNS Arithmetic Filter: only BNNSDataTypeFloat32/BFloat16/Float16 is supported
BNNS Arithmetic Filter: only BNNSDataTypeFloat32/BNNSDataTypeBFloat16 is supported
BNNS Arithmetic Filter Internal: unsupported arithmetic function
BNNS Arithmetic Filter: out delta is not valid
BNNS Arithmetic Filter: in_delta is not valid
BNNS Arithmetic Filter: in is NULL, but it is required for backward compute
BNNS Arithmetic Filter: in_delta descriptor %zu is not valid
BNNS Arithmetic Filter: cannot compute activation backward, output is NULL
BNNS Arithmetic Filter: Fusion of arithmetic with activation is unsupported for given activation function
BNNS Arithmetic Filter Backward Internal: unsupported arithmetic function
BNNS Arithmetic Filter Backward: failed to apply activation backward
BNNS Arithmetic Filter: in_delta[0] must be NULL for BNNSArithmeticSelect
BNNS Arithmetic Filter: arithmetic_function_fields is NULL
BNNS Arithmetic Filter: malloc failed
BNNS Arithmetic Filter: unsupported out_type/in_type
BNNS Arithmetic Filter: in, out descriptor check failed
BNNS Arithmetic Filter: in1,in2,out descriptor check failed
BNNS Arithmetic Filter: in1,in2,in3,out descriptor check failed
BNNS Arithmetic: Activation creation failed
BNNS Arithmetic Filter: input size[%zu]=%zu does not equal max input,output size of %zu or 1
BNNS Arithmetic Filter: input%zu and input%zu have the same data pointer, but descriptors are different
BNNS Arithmetic Filter: input%zu and input%zu have the same data pointer, but different BNNSDescriptorType
BNNS Arithmetic Filter: descriptors for input that is being processed in-place and output must exactly match
BNNS Arithmetic Filter: input is being processed in-place but input and output descriptor types do not match
BNNS Arithmetic Filter: output dimension smaller than input is not supported. batch size>1, input type is BNNSSample but out_type is not BNNSSample
BNNS Arithmetic Filter: backward compute for chosen arithmetic function does not allow inplace gradient. out delta data must not be the same as input delta data
BNNS Arithmetic Filter: in_delta%zu is NULL but in%zu type is not BNNSConstant
BNNS Arithmetic Filter: in_delta%zu->data is NULL but in%zu type is not BNNSConstant
BNNS Arithmetic Filter: in%zu type is BNNSSample but in_delta_stride[%zu] is 0
BNNS Arithmetic Filter: arithmetic input_delta and input descriptors must have the same sizes, strides and data types
BNNS Arithmetic Filter: input%zu delta data pointer is not NULL. cannot compute gradient for BNNSConstant
BNNS Arithmetic Filter: arithmetic output_delta and output descriptors must have the same sizes, strides and data types
BNNS Arithmetic Filter: input%zu delta and input%zu delta has the same data pointer, but descriptors are different
BNNS Arithmetic Filter: input%zu delta and input%zu delta has the same data pointer, but different data types
BNNS Arithmetic Filter: input%zu delta and input%zu delta has the same data pointer, but different in_delta_stride
BNNS Arithmetic Filter: forward input pointer points to the same array but input gradient pointers point to different arrays
BNNS Arithmetic Filter: descriptors for out_delta that is being processed in-place and in1_delta must exactly match
BNNS Arithmetic Filter: input1 delta is being processed in-place but input type and output type are different
BNNS Arithmetic Filter: input1 delta and output delta has the same data pointer (inplace), but different in_delta_stride
BNNS Arithmetic Filter: input1 delta and output delta has the same data pointer (inplace), but in1_delta.flags indicates to accumulate result
BNNS Arithmetic Filter: output dimension smaller than input is not supported. batch size>1, an input type is BNNSSample but out_type is not BNNSSample
 = topk (
topk does not support dynamic sizes
topk from
BNNS Crop Resize: data types do not match
BNNS Crop Resize: dimension of input, roi, or output is not correct
BNNS Crop Resize: size of roi is not [N, 4] or [N, 5]
BNNS Crop Resize: size of output is not [N, B, C, OH, OW] or [N, 1, C, OH, OW]
BNNS Crop Resize: only linear method is supported now
BNNS Crop Resize: Unsupported data type
BNNS Crop Resize: sampling mode not supported
BNNS Embedding Create: Only 8, 16, 32 and 64 bit integer types may be used as input data type.
BNNS Embedding Create: Only 16 and 32 bit floating point and 8, 16 and 32 bit integer types may be used as dictionary data type.
BNNS Embedding Create: o_desc dimension must be dim(i_desc) + dim(dictionary) - 1.
BNNS Embedding Create: First dim(dictionary)-1 dimensions of o_desc and dictionary must have consistent shapes.
BNNS Embedding Create: Last dim(i_desc) dimensions of o_desc and i_desc must have consistent shapes.
internal embedding copy
internal embedding copy back
BNNS Embedding Apply: Input value %zu is out of range [0, %zu)
BNNS Embedding ApplyBackward: weights_delta must have the same shape as layer_params->dictionary
BNNS Embedding ApplyBackward: out_delta must have the same shape as layer_params->o_desc
BNNS Embedding ApplyBackward: Input value %zu out of range
BNNS Sparse Embedding Create: Only 8, 16, 32 and 64 bit integer types may be used as input data type.
BNNS Sparse Embedding Create: Only 16 and 32 bit floating point and 8, 16 and 32 bit integer types may be used as dictionary data type.
BNNS Sparse Embedding Create: o_desc dimension must be dim(i_desc) + dim(dictionary) - 1.
BNNS Sparse Embedding Create: First dim(dictionary)-1 dimensions of o_desc and dictionary must have consistent shapes.
BNNS Sparse Embedding Create: Last dim(i_desc) dimensions of o_desc and i_desc must have consistent shapes.
BNNS Sparse Embedding Create: Unsupported optimization function
BNNS Sparse Embedding Create: Dictionary must be of type BNNSDataTypeFloat32
BNNS Sparse Embedding Apply: Filter is no longer valid
BNNS Sparse Embedding Apply: Input value %zu is out of range [0, %zu)
BNNS Sparse Embedding Apply Backwards: Filter is no longer valid
BNNS Embedding Backwards Sparse: Accumulation of sparse weights gradient is not supported.
BNNS Embedding Backwards Sparse: out_delta must have the same shape as layer_params->o_desc
BNNS Embedding Backwards Sparse: dictionary_delta->indices must have a single dimension
BNNS Embedding Backwards Sparse: dictionary_delta->indices.size[0] must be 1
BNNS Embedding Backwards Sparse: dictionary_delta->sparse_dimension_size[0] must be num_embeddings
BNNS Embedding Backwards Sparse: dictionary_delta->values shape must match dictionary item shape
BNNS Embedding Backwards Sparse: For in-place indices, dictionary_delta->count must equal the total size of layer_params->i_desc * batch_size
BNNS Embedding Backwards Sparse: For in-place indices, input must be contiguous
BNNS Embedding Backwards Sparse: For in-place values, dictionary_delta->count must equal the total size of out_delta * batch_size
BNNS Embedding Backwards Sparse: For in-place values, out_delta must be contiguous in its indexing dimensions
BNNS Sparse Embedding Get Representation: Filter is no longer valid
BNNS Sparse Embedding Get Representation: Unexpected value for num_accumulators=%zu (expected %zu)
BNNS Sparse Embedding Get Representation: num_opt_field_changes!=NULL but opt_field_changes_indices is NULL
BNNS Sparse Embedding Get Representation: num_opt_field_changes!=NULL but opt_field_changes is NULL
BNNS Sparse Embedding Get Representation: num_opt_field_changes=NULL, but optimization fields of filter have previosuly been updated
BNNS Sparse Embedding Set Representation: Filter is no longer valid
BNNS Embedding Set Sparse Optimizer Context: Expected %zu accumulators
BNNS Embedding Set Sparse Optimizer Context: values[0]->indices must have dimension 1
BNNS Embedding Set Sparse Optimizer Context: values[0]->indices.size[0] must be 1
BNNS Embedding Set Sparse Optimizer Context: Expected values indices descriptors to be identical (that is point to the same data)
BNNS Embedding Set Sparse Optimizer Context: Expected values indices descriptors to be identical
dest
BNNS Sparse Embedding Optimizer Step: Filter is no longer valid
BNNS Embedding Sparse Optimizer Step: Expected dictionary_delta->indices to have a single dimension
BNNS Embedding Sparse Optimizer Step: Expected dictionary_delta->values to match shape of dictionary
BNNS Embedding Sparse Optimizer Step: Dictionary gradient items must be contiguous
BNNS Embedding Sparse Optimizer Step: Dictionary items must be contiguous
BNNS Embedding Sparse Optimizer Step: grad.data_type must be BNNSDataTypeFloat32
BNNS Sparse Embedding Get Dense: Filter is no longer valid
BNNS Sparse Embedding Get Dense: Unexpected value for num_accumulators=%zu (expected %zu)
BNNS Sparse Embedding Get Dense: Unexpected number of dimensions for accumulator[%zu]
BNNS Sparse Embedding Get Dense: Unsupported data type for accumulator[%zu]
BNNS Sparse Embedding Get Dense: Unexpected shape for accumulator[%zu]
BNNS Sparse Embedding Get Dense: accumulator[%zu] must be contiguous in dictionary item dimensions
BNNSActivationFunctionIntegerLinearSaturate isn't supported in apply_bias_and_activation
BNNSActivationFunctionIntegerLinearSaturatePerChannel isn't supported in apply_bias_and_activation
BNNSActivationFunctionIntegerLinearSaturate isn't supported in apply_bias_and_activation_fp16
BNNSActivationFunctionIntegerLinearSaturatePerChannel isn't supported in apply_bias_and_activation_fp16
BNNSActivationFunctionIntegerLinearSaturate isn't supported in apply_bias_and_activation_bf16
BNNSActivationFunctionIntegerLinearSaturatePerChannel isn't supported in apply_bias_and_activation_bf16
allocation failed, size=%zu
reallocation failed, size=%zu
allocation failed, size=%zu, align=%zu
BNNS: unexpected data type, failing
BNNS: Data type not supprted, fail
BNNS: layout not supported
BNNS: active dimension must be greater than 0
BNNS: dimension %zu stride %zu is lower then previous dimension actual size %zu * %zu (size*stride)
BNNS: Opaque data structure doesn't match descriptor information
BNNS: Opaque data structure has incorrect information
BNNS Convolution: Attempting to use amx_special kernel on non-amx hardware
Unsupported data type for execute_conv_manipulate_input_op
Conv repack for amx general case not supported on this arch
BNNS Convolution: Repacking to CONV_WEIGHT_REPACK_AMX1 on non-AMX hardware is not yet supported
BNNS Convolution: Repacking to CONV_WEIGHT_REPACK_AMX_GENERAL_* on non-AMX hardware is not yet supported
BNNS Convolution: Unrecognoized repack format
BNNS Create Layer Convolution: failed to create forward convolution filter
BNNS Create Layer Convolution: failed to create wrapper filter
BNNS Create Layer Convolution: failed to allocate memory to copy weights and bias
BNNS Convolution Create: failed to allocate weights buffer
BNNS Convolution Create: failed to allocate bias buffer
BNNS Convolution Create: failed to allocate memory
BNNS Convolution Create: failed to allocate weights descriptor
BNNS Convolution Create: convert weights
BNNS Convolution Create: failed to prepare blocks
BNNS Convolution Create: failed to create compute blocks
BNNS: memory usage exceeded capacity
BNNS PreAllocated Memory: memory usage exceeded capacity
BNNS PreAllocated Memory: failed to claim scratch memory
BNNS Affine Resize: layer parameter is not supported
BNNS Affine Resize: input data type or size is not supported
BNNS Affine Resize: affine data type or size is not supported
BNNS Affine Resize: output data type or size is not supported
BNNS Create Convolution Winograd: malloc failed
BNNS Convolutions Winograd: weights allocation failed
BNNS Convolutions Winograd: bias allocation failed
BNNS Apply Convolution Winograd: memory allocation failed
weight packing must be at least 32 and a power of 2
conv op only supports fp16 and fp32 operands
pad(of:
bnns_internal.pad
conv_trans_manipulate_input(of:
bnns_internal.conv_transpose_manipulate_input
conv_transpose_manipulate_input
slice(of:
conv_input_slice(
, weights=
, strides=[
], pad=[
], dilation=[
], groups=
UNKNOWN
OPAQUE
STRING
operation does not support dynamic shaped input
operation only supports fp16 and fp32 operands
softmax(x=
, axis=
BNNS Graph Compile: Allocation failed in construction of softmax op
UPSAMPLE_NEAREST_NEIGHBOR
UPSAMPLE_BILINEAR
RESIZE_NEAREST_NEIGHBOR
RESIZE_BILINEAR
, target_size_height = 
, target_size_width = 
scatter does not support dynamic shaped input
scatter only supports fp16, fp32 types for input and updates, and int32 type for indices
scatter only supports fp16 and fp32 outputs
Rejected constexpr elimination of op 
 as it increased data footprint by 
 bytes
Constexpr elimination of op 
 would decrease weight footprint, but is not yet supported
Unsupported repack operation from layout=%u to layout=%u
BNNSCopy: Unsupported repack operation from layout=%u to layout=%u
BNNSCopy: src->size[%zu]=%zu is not equal to dest->size[%zu]=%zu
BNNSCopy: Cannot perform type conversion as part of opaque repack
temp_desc
BNNS Graph Compile: Allocation failed in construction of gumbelmax op
 = gumbelmax(input=
persistent_memory(for:
internal.gumbelmax
copy(
, fp32)
, fp16)
matmul op does not support dynamic shaped inputs
matmul op detected incompatible tensor shapes
matmul op detected unexpected output shape
matmul op requires final dimension to have stride 1
matmul op detected incompatible tensor shape for bias
matmul op requires input types to be FP16 or FP32
matmul op requires output type to be FP16 or FP32
matmul(x=
, y=
matmul_concat_weight(
mtmul_concat_bias(
matmul_concat_output(
matmul_output_slice.
Unsupported minimum compute type
 %.6f
-- slice 
,:,: --
BNNS: Data type not supported, fail
    first big diff at (
 %zu
) observed = %e [offset=%zu] expected = %e [offset=%zu]
BNNS Graph Initialize Persistent Memory: Unsupported kernel id %u
dot_product only supports fp16 and fp32 operands
dot_product(
hw.physicalcpu
pad op only supports FP16 and FP32 operands
pad op only supports rank <= 4
pad op doesn't support mode="replicate"
pad op doesn't support mode="reflect"
, pad=[
], mode=
BNNS Dequantize: shouldn't have reached fp16 convert path
BNNSShuffle: shuffle operator not supported.
 = slice_by_index(x=
, begin=[
], end=[
], stride=[
slice only supports fp16, fp32 and int32 operands
slice requires input and output types to match
BNNS Graph Shape Deduction: Unsupported kernel id %u
BNNS Normalization Apply Backward: input gradient data type not supported
BNNS batch norm forward: malloc for mean failed
BNNS batch norm forward: malloc for var failed
BNNS batch norm forward: malloc for isqrtvar failed
BNNS Batchnorm Backward Apply: delta allocation failed
BNNS Batchnorm Backward Apply: activation_grad allocation failed
BNNS instance norm backward: malloc for dbeta failed
BNNS instance norm backward: malloc for dgamma failed
BNNS Layer Norm Apply Backward: malloc activation_grad failed
BNNS Layer Norm Apply Backward: dx hat allocation failed
BNNS Group Norm Apply Backward: failed to allocate memory
BNNS Group Norm Apply Backward: failed to allocate activation grad memory
BNNS Normalization Apply Backward: malloc activation_scratch failed
BNNS instance norm backward: malloc for dbeta and dgamma failed
' is not a tensor
': variadic tensors are not supported by mil2bnns
FlexibleShapeInformation
BNNS: Failed to parse FlexibleShapeInformation, expected tuple
BNNS: Failed to parse FlexibleShapeInformation, expected tuple<tuple>
BNNS: Failed to parse FlexibleShapeInformation, expected type info tuple to have exactly two entries
BNNS: Failed to parse FlexibleShapeInformation, expected first element of type info tuple to be a string
BNNS: Failed to parse FlexibleShapeInformation, expected second element of type info tuple to be a dictionary
BNNS: Failed to parse FlexibleShapeInformation, expected dictionary key to be of type string
BNNS: Failed to parse FlexibleShapeInformation, expected default shape to be vector of int32
BNNS: Failed to find matching default size to input for FlexibleShapeInformation provided for '%s'
BNNS: input tensor '%s' and its FlexibleShapeInformation default size have different ranks
BNNS: Failed to parse FlexibleShapeInformation, default shapes with negative values are not supported
BNNS: Failed to parse FlexibleShapeInformation, expected range values to be supplied as list
BNNS: Failed to find matching range dims to input for FlexibleShapeInformation provided for '%s'
BNNS: input tensor '%s' and its FlexibleShapeInformation range dims have different ranks
BNNS: Failed to parse FlexibleShapeInformation, expected range values to be supplied as list of int32 tensors
BNNS: FlexibleShapeInformation of type EnumeratedShapes is currently ignored
BNNS: Failed to parse FlexibleShapeInformation, unexpected information type
BNNS Graph Compile: Attempted to parse unsupported op '%s' (reversed name '%s')
BNNS Convolution Apply: input batch stride doesn't make sense (%zu < %zu x %zu)
BNNS Convolution Apply: output batch stride doesn't make sense (%zu < %zu x %zu)
BNNS Convolution Apply: failed to allocate weights in apply
BNNS Convolution Backward: BNNSNDArrayFlagBackpropAccumulate has not yet been implemented for weights_delta or bias_delta.
BNNS Convolution Backward: cannot run backward without output delta
BNNS Convolution Backward: [out delta/in delta/weight delta/bias delta] - data type check failed. unsupported data type
BNNS Convolution Backward: internal error, incorrect wrapper
BNNS Convolution forward output data type does not match output delta data type
BNNS Convolution forward input data type does not match input delta data type
BNNS Convolution forward weights data type does not match weights delta data type
BNNS Convolution weight packing is not supported backward
BNNS Convolution Backward: BNNSFlagsUseClientPtr must be enabled during training
BNNS Convolution Backward: invalid argument
BNNS Convolution Backward: Fusion of compute with activation is unsupported for given activation function
BNNS Convolution Backward: failed to allocate memory
BNNS Convolution Backward: failed to apply activation backward
BNNS Conv: Unsupported weight format for Input delta compute
BNNS Convolution Apply Backward: could not create transposed convolution filter
BNNS Convolution Apply Backward: Convolution input delta computation failed
BNNS Transposed Convlution Apply: failed to allocate weights buffer
BNNS Transposed Convlution Apply: failed to allocate backward weights buffer
BNNS Transposed Convlution Apply: failed to allocate out delta buffer
BNNS Transposed Convlution Apply: failed to manipulate output delta
BNNS Transposed Convlution Apply: could not create convolution filter
BNNS Convolution Apply Backward: allocation failed
BNNS Convolution Apply Backward: Transposed Convolution Backward Compute failed
transposed convolution parameter error
transposed convolution backward failed
Convolution bias delta failed
BNNS Create Layer Convolution: fused padd convolution allocation failed
BNNS Create Layer Convolution: padding layer creation failed
BNNS Create Layer Convolution: padding descriptor creation failed
BNNS Create Layer Convolution: input channels must be divisible by groups
BNNS Create Layer Convolution: output channels must be divisible by groups
BNNS Create Layer Convolution: bias must match output channels
BNNS Create Layer Convolution: grouped convolution weight layout must be BNNSDataLayoutConvolutionWeightsOIHW
BNNS Convolution Create: incompatible numbers of channels between images and convolution parameters
BNNS Convolution Create: unsupported weight format
BNNS Convolution Create: input must be a 3D array
BNNS Convolution Create: input descriptor is illegal
BNNS Conv: output must be a 3D array
BNNS Convolution Create: output descriptor is illegal
BNNS Convolution Create:  weights descriptor is illegal
BNNS Convolution Create: failed to create dilated convolution
BNNS Convolution Create: failed to create packed weights convolution
BNNS Convolution Create: failed to create grouped convolution
BNNS Grouped Convolution apply: groups should not be used if groups <= 1
BNNS Grouped Convolution apply: weights are null 
BNNS Grouped Convolution apply: failed to create context
BNNS Grouped Convolution apply: unsupported convolution filter type 
BNNS Convolution Apply Backward:  filter is NULL
BNNS Convolution Apply Backward: out_delta is NULL
BNNS Convolution weight delta: malloc failed
BNNS Convolutions Apply: unexpected output data type
BNNS Convolution Backward: out_delta descriptor memory layout is not contiguous
BNNS Convolution bias delta: malloc failed
BNNS Convolution bias delta: upconverting tensors failed
BNNS Convolution: malloc activation_grad failed
Convolution backward: in_delta, weights_delta, bias_delta descriptor pointers are all NULL
BNNS Convolution Backward: in_delta descriptor memory layout is not contiguous
BNNS Convolution Backward: weights_delta descriptor memory layout is not contiguous
BNNS Convolution Backward: bias_delta descriptor memory layout is not contiguous
BNNS Convolution Backward: out_stride and out_delta_stride do not match
BNNS Convolution Backward: out_delta_stride [%zu] does not match out_delta descriptor memory size [%zu]
BNNS Convolution Backward: in_stride and in_delta_stride do not match
BNNS Convolution Backward: in_delta_stride [%zu] does not match in_delta descriptor memory size [%zu]
BNNS gradient computation for Transposed Convolution with groups is not supported
addend
_repacked
rsqrt(
 + epsilon)
rsqrtvar
 = batch_norm(input=
, mean=
, addend=
, rsqrtvar=
, variance=
, gamma=
, eps=
 = instance_norm(input=
 = layer_norm(input=
, axes=
 = channel_norm(input=
bessel_correction_factor
x_minus_mean_squared
internal.channel_norm
BNNS: unsupported resize forward data type
BNNS Resize: input_delta has %zu dimensions but must match input that has %zu
BNNS Resize: output_delta has %zu dimensions but must match output that has %zu
BNNS Resize: input_delta size[%zu]=%zu but must match input size[%zu]=%zu
BNNS Resize: output_delta size[%zu]=%zu but must match output size[%zu]=%zu
BNNS: unsupported resize backward data type
BNNS Resize: Unsupported data type
BNNS Resize: Input and output have different number of dimensions
BNNS Resize: resize must be in same direction for all direction (request %d downsample and %d upsample)
BNNS Resize: Linear interpolation requires resize in at most two dimensions, but %d dimensions are resized
BNNS Resize: Unsupported interpolation method %d
Downsampling not supported for dimension > 2.
BNNS Resize: downsampling has no support for 3 or more dimensions.
BNNS Resize: downsampling has no support for 3 or higher dimension.
BNNS CONVOLUTIONS CUSTOMIZED: malloc failed
BNNS CONVOLUTIONS CUSTOMIZED: malloc failed
BNNS CONVOLUTIONS CUSTOMIZED: failed to allocate bias memory
BNNS CONVOLUTIONS CUSTOMIZED: failed to allocate input repack memory
%zu: %s
execute_op
%{name=layer}s %{name=direction}s
BNNSDirectApply
Pooling
%{name=direction}s
Quantizer
ClipByValue
ClipByGlobalNorm
ComputeNorm
SparseEmbedding
SparseEmbeddingOptimzierStep
RandomFill
Filter
MultiheadAttention
Loss
Arithmetic
Permute
Normalization
FusedFilter
BNNS Signposts Enabled=%c
!$N4bnns14graph_compiler2op17binary_relationalE
N4bnns14graph_compiler2op17binary_arithmeticE
NSt3__120__shared_ptr_pointerIPhNS_10shared_ptrIA_hE27__shared_ptr_default_deleteIS3_hEENS_9allocatorIhEEEE
NSt3__110shared_ptrIA_hE27__shared_ptr_default_deleteIS1_hEE
NSt3__114default_deleteIA_hEE
G #&)
N4bnns14graph_compiler2op16unary_arithmeticE
"%)-
QTX[_c
!msy
a_*ik, b_*kj -> c_*ij
INTERNAL: bwd
N4bnns14graph_compiler2op6concatE
nMnnnP
FaIaaaL
NSt3__110__function6__funcIZNK4bnns14graph_compiler8Function11generate_irEvE3$_0NS_9allocatorIS5_EEFiPKNS3_9OperationEEEE
NSt3__110__function6__baseIFiPKN4bnns14graph_compiler9OperationEEEE
ZNK4bnns14graph_compiler8Function11generate_irEvE3$_0
N4bnns14graph_compiler2op10prefix_sumE
N4bnns14graph_compiler2op4tileE
#58:=@
3XV[^bQ
"%*/
!,7Z
*/6;CK
N4bnns20GraphOptionsInternalE
N4bnns14graph_compiler6TensorE
N4bnns14graph_compiler14TensorConstantE
N4bnns14graph_compiler24TensorConstantPalletizedE
N4bnns14graph_compiler19TensorStringLiteralE
N4bnns14graph_compiler10TensorViewE
NSt3__110__function6__funcIZN4bnns14graph_compiler8Function24layout_persistent_memoryEvE3$_1NS_9allocatorIS5_EEFiPNS3_9OperationEEEE
NSt3__110__function6__baseIFiPN4bnns14graph_compiler9OperationEEEE
ZN4bnns14graph_compiler8Function24layout_persistent_memoryEvE3$_1
NSt3__110__function6__funcIZN4bnns14graph_compiler8Function6layoutEbNS2_12layout_alg_tEE3$_2NS_9allocatorIS6_EEFiPNS3_9OperationEEEE
ZN4bnns14graph_compiler8Function6layoutEbNS_12layout_alg_tEE3$_2
NSt3__110__function6__funcIZN4bnns14graph_compiler8Function6layoutEbNS2_12layout_alg_tEE3$_3NS_9allocatorIS6_EEFiPNS3_9OperationEEEE
ZN4bnns14graph_compiler8Function6layoutEbNS_12layout_alg_tEE3$_3
N4bnns14graph_compiler2op31conv_transpose_manipulate_inputE
N4bnns14graph_compiler2op22elementwise_activationE
NSt3__110__function6__funcIZN4bnns14graph_compilerL24function_fusion_opt_passERNS3_8FunctionEE3$_1NS_9allocatorIS6_EEFiPNS3_9OperationEEEE
ZN4bnns14graph_compilerL24function_fusion_opt_passERNS0_8FunctionEE3$_1
NSt3__110__function6__funcIZN4bnns14graph_compiler15fusion_opt_passERNS3_7ProgramEE3$_0NS_9allocatorIS6_EEFiPNS3_9OperationEEEE
ZN4bnns14graph_compiler15fusion_opt_passERNS0_7ProgramEE3$_0
NSt3__117bad_function_callE
N4bnns14graph_compiler2op4lstmE
NSt3__110__function6__funcIZN4bnns14graph_compilerL26function_aliasing_opt_passERNS3_8FunctionEE3$_0NS_9allocatorIS6_EEFiPNS3_9OperationEEEE
ZN4bnns14graph_compilerL26function_aliasing_opt_passERNS0_8FunctionEE3$_0
JMQU
:LOQTW
1mkpsw
AQaq
N4bnns12GraphOptionsE
NSt3__120__shared_ptr_pointerIPN3MIL10MILContextENS_14default_deleteIS2_EENS_9allocatorIS2_EEEE
NSt3__114default_deleteIN3MIL10MILContextEEE
cks{
N4bnns14graph_compiler2op11crop_resizeE
N4bnns14graph_compiler2op9gru_fusedE
NSt3__110__function6__funcIZN4bnns14graph_compiler7pattern21make_grufused_patternEvE3$_0NS_9allocatorIS5_EEFbPKNS3_6TensorEPKNS3_8FunctionERNS3_18PatternMatchResultEEEE
NSt3__110__function6__baseIFbPKN4bnns14graph_compiler6TensorEPKNS3_8FunctionERNS3_18PatternMatchResultEEEE
ZN4bnns14graph_compiler7pattern21make_grufused_patternEvE3$_0
NSt3__120__shared_ptr_pointerIPN4bnns14graph_compiler7PatternENS_10shared_ptrIS3_E27__shared_ptr_default_deleteIS3_S3_EENS_9allocatorIS3_EEEE
NSt3__110shared_ptrIN4bnns14graph_compiler7PatternEE27__shared_ptr_default_deleteIS3_S3_EE
NSt3__114default_deleteIN4bnns14graph_compiler7PatternEEE
NSt3__110__function6__funcIZN4bnns14graph_compiler7pattern21make_grufused_patternEvE3$_1NS_9allocatorIS5_EEFbPKNS3_6TensorEPKNS3_8FunctionERNS3_18PatternMatchResultEEEE
ZN4bnns14graph_compiler7pattern21make_grufused_patternEvE3$_1
NSt3__110__function6__funcIZN4bnns14graph_compiler7pattern21make_grufused_patternEvE3$_2NS_9allocatorIS5_EEFbPKNS3_6TensorEPKNS3_8FunctionERNS3_18PatternMatchResultEEEE
ZN4bnns14graph_compiler7pattern21make_grufused_patternEvE3$_2
NSt3__110__function6__funcIZN4bnns14graph_compiler7pattern21make_grufused_patternEvE3$_3NS_9allocatorIS5_EEFbPKNS3_6TensorEPKNS3_8FunctionERNS3_18PatternMatchResultEEEE
ZN4bnns14graph_compiler7pattern21make_grufused_patternEvE3$_3
NSt3__110__function6__funcIZN4bnns14graph_compiler7pattern21make_grufused_patternEvE3$_4NS_9allocatorIS5_EEFbPNS3_8FunctionERKNS3_18PatternMatchResultEEEE
NSt3__110__function6__baseIFbPN4bnns14graph_compiler8FunctionERKNS3_18PatternMatchResultEEEE
ZN4bnns14graph_compiler7pattern21make_grufused_patternEvE3$_4
N4bnns14graph_compiler2op6reduceE
N4bnns2IR3gen16TensorFirstMajorE
N4bnns2IR3gen6TensorE
N4bnns2IR3gen20TensorViewFirstMajorE
nMnnnP
FaIaaaL
N4bnns14graph_compiler2op3gruE
*BL?
SC33SSSSSSSC
SSSS
N4bnns14graph_compiler2op7permuteE
N4bnns14graph_compiler2op6gatherE
LOSW
=ORTWZ
.mkpsw
*5LLL@R
!&///+
?fff
(6D7
pAd)d)d)d)d)d)d)d)b1b1b1b1b1b1b1b1
;>ADGJMPSVY\_b
),k,n
 #&),/258;>
J(s|((
[^adgjmpsvy|
^adgjmpsvy|
 js  v   
 nw  z   
`cfilorux{~
ilorux{~
X[^adgjmpsvy|
????>
00000J____^
jjjjj
cfilorux{~
_behknqtwz}
"u~""
N4bnns14graph_compiler8FunctionE
NSt3__110__function6__funcIZNK4bnns14graph_compiler8Function8validateEvE3$_0NS_9allocatorIS5_EEFiPKNS3_9OperationEEEE
ZNK4bnns14graph_compiler8Function8validateEvE3$_0
NSt3__110__function6__funcIZN4bnns14graph_compiler8Function11repack_dataEvE3$_1NS_9allocatorIS5_EEFiPNS3_9OperationEEEE
ZN4bnns14graph_compiler8Function11repack_dataEvE3$_1
NSt3__110__function6__funcIZNK4bnns14graph_compiler8Function18reachable_literalsEvE3$_3NS_9allocatorIS5_EEFiPKNS3_9OperationEEEE
ZNK4bnns14graph_compiler8Function18reachable_literalsEvE3$_3
NSt3__110__function6__funcIZNK4bnns14graph_compiler8Function28reachable_underlying_tensorsEvE3$_4NS_9allocatorIS5_EEFiPKNS3_9OperationEEEE
ZNK4bnns14graph_compiler8Function28reachable_underlying_tensorsEvE3$_4
NSt3__110__function6__funcIZNK4bnns14graph_compiler8Function16hasDynamicShapesEvE3$_5NS_9allocatorIS5_EEFiPKNS3_9OperationEEEE
ZNK4bnns14graph_compiler8Function16hasDynamicShapesEvE3$_5
+@+++C
hekeeen
bVeVVVh
hVhhhY
Y}\}}}_
@[C[[[F#
hVhhhY
Y}\}}}_
@[C[[[Fi
@L@@@O
VSYSSS\
E3H333K
"%)-
"&*/
 %,BGLSou{
"%)-
"&*/
?CHJOT[puz
?N4bnns14graph_compiler2op8range_1dE
333333
?N4bnns14graph_compiler2op4copyE
N4bnns14graph_compiler2op7reshapeE
N4bnns14graph_compiler2op8max_poolE
N4bnns14graph_compiler2op9base_poolE
N4bnns14graph_compiler2op8avg_poolE
N4bnns14graph_compiler2op7l2_poolE
N4bnns14graph_compiler2op5shapeE
N4bnns14graph_compiler2op6matvecE
TW\`
imtw
;=,?
.;?$&
<7>A
%GmU5
3LU5
U5U5
%GlU5U5
3KU5U5
JU5U5
WZ_bglsv{
UX]`ejqty~
"',37=C
(3:EP]hw
"',37=C
NSt3__110__function6__funcIZN4bnns14graph_compiler7pattern16make_any_patternERKNS_12basic_stringIcNS_11char_traitsIcEENS_9allocatorIcEEEEE3$_0NS8_ISD_EEFbPKNS3_6TensorEPKNS3_8FunctionERNS3_18PatternMatchResultEEEE
ZN4bnns14graph_compiler7pattern16make_any_patternERKNSt3__112basic_stringIcNS2_11char_traitsIcEENS2_9allocatorIcEEEEE3$_0
NSt3__110__function6__funcIZN4bnns14graph_compiler7pattern21make_constant_patternERKNS_12basic_stringIcNS_11char_traitsIcEENS_9allocatorIcEEEEE3$_1NS8_ISD_EEFbPKNS3_6TensorEPKNS3_8FunctionERNS3_18PatternMatchResultEEEE
ZN4bnns14graph_compiler7pattern21make_constant_patternERKNSt3__112basic_stringIcNS2_11char_traitsIcEENS2_9allocatorIcEEEEE3$_1
N4bnns14graph_compiler2op5stackE
hVhhhY
@[C[[[F
2266
NSt3__110__function6__funcIZN4bnns5graphL33execute_typed_arithmetic_unary_opIDF16_EEiRKNS2_13GraphInternalEPKvPK34bnns_graph_arithmetic_unary_paramsEUlPDF16_PKDF16_mE_NS_9allocatorISG_EEFvSD_SF_mEEE
NSt3__110__function6__baseIFvPDF16_PKDF16_mEEE
ZN4bnns5graphL33execute_typed_arithmetic_unary_opIDF16_EEiRKNS_13GraphInternalEPKvPK34bnns_graph_arithmetic_unary_paramsEUlPDF16_PKDF16_mE_
NSt3__110__function6__funcIZN4bnns5graphL33execute_typed_arithmetic_unary_opIDF16_EEiRKNS2_13GraphInternalEPKvPK34bnns_graph_arithmetic_unary_paramsEUlPDF16_PKDF16_mE0_NS_9allocatorISG_EEFvSD_SF_mEEE
ZN4bnns5graphL33execute_typed_arithmetic_unary_opIDF16_EEiRKNS_13GraphInternalEPKvPK34bnns_graph_arithmetic_unary_paramsEUlPDF16_PKDF16_mE0_
NSt3__110__function6__funcIZN4bnns5graphL33execute_typed_arithmetic_unary_opIDF16_EEiRKNS2_13GraphInternalEPKvPK34bnns_graph_arithmetic_unary_paramsEUlPDF16_PKDF16_mE1_NS_9allocatorISG_EEFvSD_SF_mEEE
ZN4bnns5graphL33execute_typed_arithmetic_unary_opIDF16_EEiRKNS_13GraphInternalEPKvPK34bnns_graph_arithmetic_unary_paramsEUlPDF16_PKDF16_mE1_
NSt3__110__function6__funcIZN4bnns5graphL33execute_typed_arithmetic_unary_opIDF16_EEiRKNS2_13GraphInternalEPKvPK34bnns_graph_arithmetic_unary_paramsEUlPDF16_PKDF16_mE2_NS_9allocatorISG_EEFvSD_SF_mEEE
ZN4bnns5graphL33execute_typed_arithmetic_unary_opIDF16_EEiRKNS_13GraphInternalEPKvPK34bnns_graph_arithmetic_unary_paramsEUlPDF16_PKDF16_mE2_
NSt3__110__function6__funcIZN4bnns5graphL33execute_typed_arithmetic_unary_opIfEEiRKNS2_13GraphInternalEPKvPK34bnns_graph_arithmetic_unary_paramsEUlPfPKfmE_NS_9allocatorISG_EEFvSD_SF_mEEE
NSt3__110__function6__baseIFvPfPKfmEEE
ZN4bnns5graphL33execute_typed_arithmetic_unary_opIfEEiRKNS_13GraphInternalEPKvPK34bnns_graph_arithmetic_unary_paramsEUlPfPKfmE_
NSt3__110__function6__funcIZN4bnns5graphL33execute_typed_arithmetic_unary_opIfEEiRKNS2_13GraphInternalEPKvPK34bnns_graph_arithmetic_unary_paramsEUlPfPKfmE0_NS_9allocatorISG_EEFvSD_SF_mEEE
ZN4bnns5graphL33execute_typed_arithmetic_unary_opIfEEiRKNS_13GraphInternalEPKvPK34bnns_graph_arithmetic_unary_paramsEUlPfPKfmE0_
NSt3__110__function6__funcIZN4bnns5graphL33execute_typed_arithmetic_unary_opIfEEiRKNS2_13GraphInternalEPKvPK34bnns_graph_arithmetic_unary_paramsEUlPfPKfmE1_NS_9allocatorISG_EEFvSD_SF_mEEE
ZN4bnns5graphL33execute_typed_arithmetic_unary_opIfEEiRKNS_13GraphInternalEPKvPK34bnns_graph_arithmetic_unary_paramsEUlPfPKfmE1_
NSt3__110__function6__funcIZN4bnns5graphL33execute_typed_arithmetic_unary_opIfEEiRKNS2_13GraphInternalEPKvPK34bnns_graph_arithmetic_unary_paramsEUlPfPKfmE2_NS_9allocatorISG_EEFvSD_SF_mEEE
ZN4bnns5graphL33execute_typed_arithmetic_unary_opIfEEiRKNS_13GraphInternalEPKvPK34bnns_graph_arithmetic_unary_paramsEUlPfPKfmE2_
:?GKQW
JOVZ_e
oszb}
e^gbil
N4bnns14graph_compiler2op6randomE
#(-27<AFKPU{
UUUUUU
#(-27<AFKPU{
UUUUUU
#(-27<AFKPU{
UUUUUU
"%(+0369;@EH999999KNQVa999d
"%(+0369;@EH999999KNQVa999d
"%(+0369;@EH999999KNQVa999d
!rr{>
!rr{>
}76R
YVpA
YVpA
YVpA
<7p7M}
<7p7M}
<7p7M}
:I~D
:I~D
:I~D
:I~D
*?9s
?G/d?
)>e\
N4bnns14graph_compiler2op4topkE
NSt3__110__function6__funcIZN4bnns14graph_compiler8Function8scheduleEvE3$_0NS_9allocatorIS5_EEFiPNS3_9OperationEEEE
ZN4bnns14graph_compiler8Function8scheduleEvE3$_0
N4bnns14graph_compiler2op4convE
OjRjjjUN4bnns14graph_compiler2op3nopE
N4bnns14graph_compiler9OperationE
 &*06
dhmr
Iejnsx
N4bnns14graph_compiler2op7softmaxE
N4bnns14graph_compiler2op6resizeE
N4bnns14graph_compiler2op7scatterE
NSt3__110__function6__funcIZN4bnns14graph_compilerL27function_constexpr_opt_passERNS3_8FunctionEE3$_0NS_9allocatorIS6_EEFiPNS3_9OperationEEEE
ZN4bnns14graph_compilerL27function_constexpr_opt_passERNS0_8FunctionEE3$_0
N4bnns14graph_compiler2op9gumbelmaxE
NSt3__110__function6__funcIZN4bnns14graph_compiler7pattern25make_gumbelmax_pattern_v0EvE3$_0NS_9allocatorIS5_EEFbPKNS3_6TensorEPKNS3_8FunctionERNS3_18PatternMatchResultEEEE
ZN4bnns14graph_compiler7pattern25make_gumbelmax_pattern_v0EvE3$_0
NSt3__110__function6__funcIZN4bnns14graph_compiler7pattern25make_gumbelmax_pattern_v0EvE3$_1NS_9allocatorIS5_EEFbPNS3_8FunctionERKNS3_18PatternMatchResultEEEE
ZN4bnns14graph_compiler7pattern25make_gumbelmax_pattern_v0EvE3$_1
NSt3__110__function6__funcIZN4bnns14graph_compiler7pattern25make_gumbelmax_pattern_v1EvE3$_2NS_9allocatorIS5_EEFbPKNS3_6TensorEPKNS3_8FunctionERNS3_18PatternMatchResultEEEE
ZN4bnns14graph_compiler7pattern25make_gumbelmax_pattern_v1EvE3$_2
NSt3__110__function6__funcIZN4bnns14graph_compiler7pattern25make_gumbelmax_pattern_v1EvE3$_3NS_9allocatorIS5_EEFbPNS3_8FunctionERKNS3_18PatternMatchResultEEEE
ZN4bnns14graph_compiler7pattern25make_gumbelmax_pattern_v1EvE3$_3
N4bnns14graph_compiler2op6matmulE
NSt3__110__function6__funcIZN4bnns14graph_compiler14local_opt_passERNS3_8FunctionEE3$_0NS_9allocatorIS6_EEFiPNS3_9OperationEEEE
ZN4bnns14graph_compiler14local_opt_passERNS0_8FunctionEE3$_0
N4bnns14graph_compiler2op11dot_productE
N4bnns14graph_compiler2op3rnnE
N4bnns14graph_compiler2op3padE

N4bnns14graph_compiler2op7shuffleE
N4bnns14graph_compiler2op5sliceE
N4bnns14graph_compiler2op13NormalizationE
N4bnns14graph_compiler2op10batch_normE
N4bnns14graph_compiler2op13instance_normE
N4bnns14graph_compiler2op10layer_normE
N4bnns14graph_compiler2op12channel_normE
NSt3__110__function6__funcIZN4bnns14graph_compiler7pattern25make_channel_norm_patternEbE3$_0NS_9allocatorIS5_EEFbPKNS3_6TensorEPKNS3_8FunctionERNS3_18PatternMatchResultEEEE
ZN4bnns14graph_compiler7pattern25make_channel_norm_patternEbE3$_0
NSt3__110__function6__funcIZN4bnns14graph_compiler7pattern25make_channel_norm_patternEbE3$_1NS_9allocatorIS5_EEFbPKNS3_6TensorEPKNS3_8FunctionERNS3_18PatternMatchResultEEEE
ZN4bnns14graph_compiler7pattern25make_channel_norm_patternEbE3$_1
NSt3__110__function6__funcIZN4bnns14graph_compiler7pattern25make_channel_norm_patternEbE3$_2NS_9allocatorIS5_EEFbPKNS3_6TensorEPKNS3_8FunctionERNS3_18PatternMatchResultEEEE
ZN4bnns14graph_compiler7pattern25make_channel_norm_patternEbE3$_2
NSt3__110__function6__funcIZN4bnns14graph_compiler7pattern25make_channel_norm_patternEbE3$_3NS_9allocatorIS5_EEFbPNS3_8FunctionERKNS3_18PatternMatchResultEEEE
ZN4bnns14graph_compiler7pattern25make_channel_norm_patternEbE3$_3
NSt3__110__function6__funcIZN4bnns14graph_compiler7pattern25make_channel_norm_patternEbE3$_4NS_9allocatorIS5_EEFbPNS3_8FunctionERKNS3_18PatternMatchResultEEEE
ZN4bnns14graph_compiler7pattern25make_channel_norm_patternEbE3$_4
logical_and
logical_or
logical_xor
logical_nand
logical_nor
greater
greater_equal
less
less_equal
not_equal
equal
logical_not
input0
input1
output
Relational op does not support dynamic shaped inputs
Relational from 
 to 
 is not supported
map::at:  key not found
basic_string
undefined
floor_div
maximum
minimum
real_div
binary_arithmetic only supports fp16 and fp32 operands
manifest_literal
internal.manifest_literal
literal(for:
input
Replaced op 
 with constexpr result
vector
acos
acosh
asin
asinh
atan
atanh
ceil
clip
cosh
exp2
floor
inverse
round
rsqrt
sign
sinh
sqrt
square
tanh
threshold
, epsilon=
%s: 
BNNS Tile: descriptor is illegal
BNNS Tile: data type check failed
BNNS Tile: output tensor size is not a multiple of input
BNNS Tile: type %u not supported
Shouldn't call get_data_size with Indexed2 or Indexed4, switch to get_data_bits
BNNS Dropout: Error layer_params is NULL
BNNS Dropout: rate must be in the range [0.0, 1.0].
BNNS Dropout: Unsupported input data type
BNNS Dropout: Input and Output data types must be the same
BNNS Dropout: Error memory allocation failed
BNNS Dropout: Error input descriptor is invalid
BNNS Dropout: Error output descriptor is invalid
BNNS Dropout: Input and output descriptors must have the same shape.
BNNS Dropout: Tensors with dimension greater than %u are not supported.
BNNS Dropout: Error filter is NULL
BNNS Dropout: Error wrong filter type, filter is not Dropout
BNNS Dropout: input pointer is NULL
BNNS Dropout: batch_size>1 and in_stride is 0
BNNS Dropout: output pointer is NULL
BNNS Dropout: batch_size>1 and out_stride is 0
BNNS BatchNorm: Error filter is NULL
BNNS Dropout: input_delta layout, shape and stride must match layer_params->i_desc
BNNS Dropout: output_delta layout, shape and stride must match layer_params->o_desc
Unsupported target %d
v16@?0Q8
v24@?0Q8^v16
BNNS Activation Gumbel Max: noise buffer size (%zu) isn't power of 2 or smaller than 16
BNNS Tensor Contraction: inputB pointer must not be NULL unless B is a weight or the operation is quadratic.
BNNS Tensor Contraction: After adding batch dimension, inputA tensor has too many indices
BNNS Tensor Contraction: After adding batch dimension, inputB tensor has too many indices
BNNS Tensor Contraction: After adding batch dimension, output tensor has too many indices
BNNS Tensor Contraction: inB_delta calculation is non-sensical for a contraction of a tensor with itself.
BNNS Tensor Contraction: inA_delta calculation requires inA to be non-NULL
BNNS Tensor Contraction: inA_delta calculation requires out_delta (and out_delta->data) to be non-NULL
inA_delta
BNNS Tensor Contraction: After adding batch dimension, out_delta tensor has too many indices
BNNS Tensor Contraction: After adding batch dimension, inputA_delta tensor has too many indices
BNNS Tensor Contraction: inA_delta calculation requires inB to be non-NULL
BNNS Tensor Contraction: inB_delta calculation requires inA to be non-NULL
BNNS Tensor Contraction: inB_delta calculation requires out_delta (and out_delta->data) to be non-NULL
inB_delta
BNNS Tensor Contraction: After adding batch dimension, inputB_delta tensor has too many indices
BNNS Tensor Contraction: beta must be 0.0 or 1.0 (beta=%.2f)
BNNS Tensor Contraction: invalid op string "%s"
BNNS Tensor Contraction: both inputA and inputB cannot be weights "%s"
BNNS Tensor Contraction: Failed to allocate memory for filter
BNNS Tensor Contraction: inputA descriptor is illegal "%s"
BNNS Tensor Contraction: inputB descriptor is illegal "%s"
BNNS Tensor Contraction: ouput descriptor is illegal "%s"
Data type combination not supported
Specified opaque layout is not supported for use with specified contraction operation
BNNS Tensor Contraction: %s shape does not match inputA shape in dimension %ld (%zu vs %zu)
BNNS Tensor Contraction: out_delta shape does not match output shape in dimension %ld (%zu vs %zu)
BNNS Tensor Contraction: Wildcard index '*' must appear on both sides of operation or neither: "%s"
BNNS Tensor Contraction: Wildcard index '*' must appear at most once per index set: "%s"
BNNS Tensor Contraction: Wildcard index '*' must appear consistently only at start or end of index sets: "%s"
BNNS Tensor Contraction: number of indices from operation (%zu) does not match dimension of input A descriptor (%zu) "%s"
BNNS Tensor Contraction: number of indices from operation (%zu) does not match dimension of input B descriptor (%zu) "%s"
BNNS Tensor Contraction: number of indices from operation (%zu) does not match dimension of output descriptor (%zu) "%s"
BNNS Tensor Contraction: '%c', index %zu of inputA has size %zu and index %td of output has size %zu, but sizes are required to match
BNNS Tensor Contraction: '%c', index %d of inputA has size %zu and index %td of inputB has size %zu, but sizes are required to match
BNNS Tensor Contraction: index '%c' of input A does not match any index of inputB or output "%s"
BNNS Tensor Contraction: '%c', index %d of inputB has size %zu and index %td of output has size %zu, but sizes are required to match
BNNS Tensor Contraction: index '%c' of input B does not match any index of inputA or output "%s"
BNNS Tensor Contraction: index '%c' of output does not match any index of inputA or inputB "%s"
BNNS Tensor Contraction: Wildcard index '*' can only appear as first or last index of "%s_%s"
BNNS Tensor Contraction: repeated indices "%s_%s"
v16@?0^v8
v24@?0Q8Q16
BNNS SoftMax: sum result shouldn't be 0, something is wrong
BNNS SoftMax FP16: sum result shouldn't be 0, something is wrong
BNNS SoftMax BF16: sum result shouldn't be 0, something is wrong
BNNS Graph Execute: missing persistent memory, please allocate memory based on BNNSGraphGetExecutionContextSize
  %s
   %s [id=0x%llx] datahash = 0x%x
exec op %zu of %zu
  final generator of %s [id=0x%llx] abs_diff=%.2e rel_diff=%.2e
  final generator of %s [id=0x%llx] <not in expected_values map>
BNNS Graph Execute: Unrecognized kernel id %u
WARNING: Asked to run with signposts, but signposts are disabled for BNNS
BNNS signposts can be enabled by running the following commands:
sudo cp /System/Library/Preferences/Logging/Subsystems/com.apple.accelerate.bnns.plist /Library/Preferences/Logging/Subsystems/com.apple.accelerate.bnns.plist
sudo plutil -replace DEFAULT-OPTIONS.Signpost-Enabled -bool YES /Library/Preferences/Logging/Subsystems/com.apple.accelerate.bnns.plist
Input datahashes:
 = concat(values=[
], axis=
, interleave=
view(of:
pad op only supports FP16, FP32 and INT32 operands
pad op does not support dynamic shaped inputs
pad op requires output type to match input type
malloc activation_grad failed
apply_specialized_convolution_PKT_0: o_height 1 not supported
apply_specialized_convolution_PKT_1: o_height less then 4 isn't supported
apply_specialized_convolution_PKT_2: o_height 1 not supported
apply_specialized_convolution_PKT_3: o_height 1 not supported
apply_specialized_convolution_PKT_3: o_height 2 not supported
BNNS Copy Filter: in and out must not be NULL.
BNNS Copysum Filter: unsupported data type conversion
internal bwd copysum
BNNS Tensor Contraction: Input must not be NULL
BNNS Tensor Contraction: Output must not be NULL
BNNS Tensor Contraction: index '%c' appears multiple times on right-hand side, but with different sizes
BNNS Tensor Contraction: '%c', index %zu of input has size %zu and index %d of output has size %zu, but sizes are required to match
BNNS Tensor Contraction: index '%c' appears multiple times on left-hand side, but with different sizes
BNNS Tensor Contraction: Unsupported data type conversion %sfrom %s to %s
(in summation) 
(in copy) 
BNNSTranspose: src and dest have a different number of dimensions.
BNNSTranspose: specified axes must be within range for supplied tensor (axes: %zu, %zu) dimension of tensor is %zu
ijklmnpq
internal_transpose
internal_copy
int1
int2
int4
int8
int16
int32
int64
uint1
uint2
uint4
uint8
uint16
uint32
uint64
bf16
fp16
fp32
indexed8
indexed4
indexed2
indexed1
bool
unknown
BNNS Fully Connected Sparsify: failed to allocate scratch buffer
BNNS Convolution Sparse: failed to allocate memory to sparsify
BNNS Convolution Sparsify: output allocation failed
BNNS CONVOLUTIONS VERSION2: Error minimum float w_pack weight packing value is 32
BNNS Fully Conneceted Sparsify: output allocation failed
BNNS Sparse Fully Connected Apply: output data type not supported
BNNS Convolution Sparsify: out descriptor can't be NULL
BNNS Convolution Sparse: failed to allocate memory to sparsify COO
BNNS Convolution Sparse: Number of nonzeros is 0
BNNS Sparse Convolution create: could not allocate 
BNNS Sparse Convolution Create: asymmetric padding for Sparse Convolution is not supported
BNNS Sparse Convolution Create: groups for Sparse Convolution is not supported
BNNS Sparse Convolution Create: stride > 1 for Sparse Convolution is not supported
: BNNS Graph prefix_sum does not support specified type/minimum_compute_type combination
: BNNS Graph only supports INT32, FP16 and FP32 operands for prefix_sum
: BNNS Graph prefix_sum required input and output types to match
 = prefix_sum(
 , axis=
, exclusive = 
, reverse = 
, reps=
tile op does not support dynamic sizes
tile requires input and output ranks to match
tile rank exceeds maximum supported dimension
tile requires reps >= 1
tile output size inconsistent with inputs
tile should have been replaced with copy
BNNS Fully Connected Sparsify: workspace size is too small
BNNS Fully Connected Sparsify: failed to allocate workspace
BNNS Fully Connected Sparsify: not enough memory in scratch memory to encode sparsity
BNNS Dequantize: input and output can't be null
BNNS Dequantize: in place conversion not supported
BNNS Dequantize: only __fp16 output is supported
BNNS Dequantize: lut is null and needed for Indexed input type
BNNS Dequantize: input type not supported
BNNS Pose Init: blob memory is NULL. please allocate buffer of BNNSPose_C_allocation_size_bytes and 256B alignment
BNNS Pose Execute: blob memory is NULL
BNNS Pose Execute: failed to run layer
encoder_0_bias
encoder_0_weight
encoder_2_bias
encoder_2_weight
decoder_1_bias
decoder_1_weight
decoder_3_bias
decoder_3_weight
decoder_5_bias
decoder_5_weight
concat_0
concat_1
concat_2
concat_3
concat_4
concat_5
MIL program has more than one function
const
bnns::PoseInitFromMIL(): MIL file does not contain expected tensor '%s'
unexpected tensor sizes
BNNS Graph Compile: Unsupported MIL data type
unordered_map::at: key not found
BNNS Dequantize: shouldn't have reached fp16 convert path
BNNS Layer Norm Apply: failed to allocate scratch memory
BNNS Instance Norm Apply: input stride (%zu) and output stride (%zu) aren't set correctly
BNNS Instance Norm Apply: failed to allocate scratch memory
BNNS Layer Norm Apply: input stride (%zu) and output stride (%zu) aren't set correctly
BNNS Group Norm Apply: failed to allocate scratch memory
BNNS Fully Connected Tiny Apply: unexpecetd weights layout
MIL program has more than one function, and no specific function specified for compilation
BNNS Fully Connected Apply: allocation of work buffer failed
BNNS Fully Connected: Unexpected result, should be at least 1
BNNS Fully Connected: Unexpected result, at least 1 batch should remain
BNNS Fully Connected: Unexpected result, at least 1 output should remain
BNNS Fully Connected: Failed to fit in memory limit, using generic code
BNNS FUlly: allocation failed
BNNS Fully Connected: weights conversion buffer isn't allocated.
BNNS Fully Connected: inputs conversion buffer isn't allocated.
BNNS Fully Connected Backward: output delta is NULL
BNNS Fully Connected Backward: only float32 or bfloat16 output delta are supported
BNNS Fully Connected Backward: input delta only support float32 or bfloat16
BNNS Fully Connected Backward: weights delta only support float32 or bfloat16
BNNS Fully Connected Backward: bias delta only support float32 or bfloat16
BNNS Fully Connected Backward: operation is currently unsupported.  We only support sparse weights for inference.
BNNS Fully Connected Backward: output delta BNNSDataLayoutImageCHW isn't supported
BNNS Fully Connected Backward: failed to allocate output delta scratch buffer
BNNS Fully Connected Backward: apply activation backward failed
BNNS Fully Connected Backward: input delta BNNSDataLayoutImageCHW isn't supported
BNNS Fully Connected Backward: failed to allocate input delta scratch buffer
BNNS Fully Connected Backward: failed to compute input delta
BNNS Fully Connected Backward: user requested weights delta but didn't supply original inputs
BNNS Fully Connected Backward: weights sizes doesn't match weights delta sizes
BNNS Fully Connected Backward: unsupported weights delta layout
BNNS Fully Connected: Weight type not supported
BNNS Fully Connected Create: failed to allocated memory (%zu bytes))
BNNS_FULLY: input doesn't support Indexed data type
BNNS_FULLY: output only support fp32 data type
BNNS Fully Connected: output data type not supported for conversion
BNNS Fully Connected: input must be a 1D array
BNNS Fully Connected: input descriptor is illegal
BNNS Fully Connected: output must be a 1D array
BNNS Fully Connected: output descriptor is illegal
BNNS Fully Connected: weights row size should match input vector size
BNNS Fully Connected: weights number of rows should match output vector size
BNNS Fully Connected: weights must be a 2D array
BNNS Fully Connected: weights descriptor is illegal
BNNS Fully Connected: Pointer to weight data must be non-NULL
BNNS Fully Connected: Output data types supported in this version: Float32, Float16, BFloat16
BNNS Fully Connected: Bias data types supported in this version: Float32
BNNS Fully Connected: Float16 output isn't supported with bias or activation function
BNNS Fully Connected: Input/Weight data types supported in this version: Float32, Float16, BFloat16, Int8, Int16
BNNS Fully Connected: Data type of weights and input should match
BNNS Fully Connected: size computation wraparound -I %zu -O %zu
BNNS Fully Connected: bias size computation wraparound -O %zu bias data type %x
Attempted to copy a view without explicit shallow copy flag
_view
BNNS InTopK: invalid axis value %zu (input tensor has %zu dimensions)
BNNS InTopK: Unsupported test_indices data type for this operation (supported data types are: int32)
BNNS InTopK: Unsupported output data type for this operation (supported data types are: boolean)
BNNS InTopK: input tensor with axis removed is not congruent with test_indices tensor
BNNS InTopK: input tensor with axis removed is not congruent with output tensor
BNNS InTopK: Unsupported input data type for this operation (supported data types are: fp32, fp16)
BNNS TopK: input descriptor is illegal
BNNS TopK: best_values descriptor is illegal
BNNS TopK: best_values data type has to match input data type for this operation
BNNS TopK: Unsupported best_indices data type for this operation (supported data types are: int32)
BNNS TopK: invalid axis value %zu (input tensor has %zu dimensions)
BNNS TopK: best_values tensor is not consistent with input tensor
BNNS TopK: best_values tensor size[%zu]=%zu should be K=%zu
BNNS TopK: best_indices tensor is not consistent with input tensor
BNNS TopK: best_indices tensor size[%zu]=%zu should be K=%zu
BNNS TopK: Unsupported input data type for this operation (supported data types are: fp32, fp16)
malloc
orange
yellow
green
blue
indigo
violet
lilac
pink
cerulean
ultraviolet
infrared
Unoptimized layout option used. Workspace will be super-massive.
Context '
': workspace size 
, lower bound (LOAD) 
Unoptimized runtime layout used. Workspace will be super-massive.
layout_color
user_input
user_output
 = conv_transpose_manipulate_input(x=
relu
relu6
leaky_relu
sigmoid
scaled_tanh
linear_activation
clamped_relu
sigmoid_hard
softplus
softsign
thresholded_relu
prelu
silu
gelu
gelu_approx_tanh
gelu_approx_sigmoid
elementwise_activation only supports fp16 and fp32 operands
alpha
, alpha=
, beta=
bias
last_h
last_c
= lstm(x=
, initial_h=
initial_h
, weight_ih=
weight_ih
, weight_hh=
weight_hh
, bias=
<none>
, direction=
, output_sequence=
true
false
, recurrent_activation=
, cell_activation=
, activation=
initial_c
peephole
weight_ih_back
weight_hh_back
bias_back
peephole_back
scratch(for:
forward
reverse
bidirectional
UNDEFINED
peehole
peehole_back
BNNS unsupported sgd variant
BNNS Optimizer Adam Apply: Adam beta1 and beta2 must be positive
BNNS Optimizer Adam Apply: Adam beta1 and beta2 must be strictly less than 1
BNNS Optimizer RMSProp Apply: RMSProp alpha must be in (0,1)
BNNS Optimizer Apply: Error unsupported optimizer function
BNNS Optimizer Apply: Error OptimizerAlgFields is NULL
BNNS Optimizer Apply: parameter array pointer is NULL
BNNS Optimizer Apply: gradient array pointer is NULL
BNNS Optimizer Apply: gradient pointer number %zu is NULL
BNNS Optimizer Apply: Error in parameter %zu: parameter descriptor must be contiguous such that stride[0]=1 and for every N>0 stride[N]=stride[N-1]*size[N-1]
BNNS Optimizer Apply: accumulator array pointer is NULL
BNNS Optimizer Apply: parameter pointer number %zu is NULL
BNNS Optimizer Apply: accumulator pointer number %zu is NULL
BNNS Optimizer Apply: Error in parameter %zu: Parameter and Gradient descriptors must have the same sizes and strides
BNNS Optimizer Apply: Error in parameter %zu: Parameter and Accumulator descriptors must have the same sizes and strides
BNNS Optimizer Apply: Error in Parameter %zu: Parameter data type is not Float32 / Float16 / Bfloat16
BNNS Optimizer Apply: Error in Gradient %zu: Gradient data type is not Float32 / Float16 / Bfloat16
BNNS Optimizer Apply: Error in Accumulator %zu: Accumulator data type is not Float32 / Float16 / BFloat16
BNNS Optimizer Apply: Error time_step is not valid for Adam optimizer. minimal time step value is 1
BNNS Optimizer Apply: Adam optimizer require accumulators pointer to be valid
BNNS Optimizer Apply: accumulator1 pointer number %zu is NULL
BNNS Optimizer Apply: accumulator2 pointer number %zu is NULL
BNNS Optimizer Apply: accumulator3 pointer number %zu is NULL
BNNS Optimizer Apply: Error in parameter %zu: Parameter and Accumulator1 descriptors must have the same sizes and strides
BNNS Optimizer Apply: Error in parameter %zu: Parameter and Accumulator2 descriptors must have the same sizes and strides
BNNS Optimizer Apply: Error in parameter %zu: Parameter and Accumulator3 descriptors must have the same sizes and strides
BNNS Optimizer Apply: Error in accumulator1 %zu: Accumulator data type is not Float32 / Float16 / Bfloat16
BNNS Optimizer Apply: Error in accumulator2 %zu: Accumulator data type is not Float32 / Float16 / Bfloat16
BNNS Optimizer Apply: Error in accumulator3 %zu: Accumulator data type is not Float32 / Float16 / Bfloat16
BNNS Optimizer Apply: RMSProp optimizer require accumulators pointer to be valid
BNNS Optimizer Apply: accumulator_n pointer number %zu is NULL
BNNS Optimizer Apply: accumulator_g pointer number %zu is NULL
BNNS Optimizer Apply: momentum pointer number %zu is NULL
BNNS Optimizer Apply: Error in accumulator_n %zu: Accumulator data type is not Float32 / Float16 / Bfloat16
BNNS Optimizer Apply: Error in accumulator_g %zu: Accumulator data type is not Float32 / Float16 / Bfloat16
BNNS Optimizer Apply: Error in momentum %zu: Accumulator data type is not Float32 / Float16 / Bfloat16
BNNS Optimizer: unsupported optimizer function
BNNS ClipByValue: Error dimensions of input tensor (%zu) and output tensor (%zu) must match
BNNS ClipByValue: Error only supports fp32
BNNS ClipByValue: Error sizes and strides of input tensor and output tensor must match
BNNS ClipByNorm: Error dimensions of input tensor (%zu) and output tensor (%zu) must match
BNNS ClipByNorm: Error only supports fp32
BNNS ClipByNorm: Error sizes and strides of input tensor and output tensor must match
BNNS ClipByGlobalNorm: src[%zu] and dest[%zu] have a different number of dimensions.
BNNS ClipByGlobalNorm: src[%zu] and dest[%zu] must contain fp32 data.
BNNS ClipByGlobalNorm: src[%zu] and dest[%zu] have different sizes or strides.
BNNS ComputeNorm: Error only supports BNNSL2Norm
BNNS ComputeNorm: Error only supports fp32
BNNS ComputeNorm: Error non-axis dimensions of input tensor (%zu) and output tensor (%zu) must match
BNNS ComputeNorm: Error non-axis sizes of input tensor and output tensor must match
BNNS ComputeNorm Backward: Error only supports BNNSL2Norm
BNNS ComputeNorm Backward: Error only supports fp32
BNNS ComputeNorm Backward: Error non-axis dimensions of input tensor (%zu) and output tensor (%zu) must match
BNNS ComputeNorm Backward: Error non-axis sizes of input tensor and output tensor must match
BNNS Optimizer Init: Unsupported layout
BNNS Sparse Fully Connected: input must be a 1D array
BNNS Sparse Fully Connected: input descriptor is illegal
BNNS Sparse Fully Connected: output must be a 1D array
BNNS Sparse Fully Connected: output descriptor is illegal
BNNS Sparse Fully Connected: invalid matrix layout
BNNS Sparse Fully Connected: invalid block row count: %u
BNNS Sparse Fully Connected: invalid block col count: %u
BNNS Sparse Fully Connected: input data types supported in this version: Float16
BNNS Sparse Fully Connected: output data types supported in this version: Float32, Float16
BNNS Sparse Fully Connected: weights data types supported in this version: Float16
BNNS Sparse Fully Connected: unsupported matrix block size %u x %u
BNNS Sparse Fully Connected: Allocation of the setup structure failed
BNNS Convolution: malloc failed
BNNS Graph: Invalid representation
BNNS Graph: Unsupported BNNS IR Version
BNNS Graph: Static size specified in sizes[%zu][%zu] does not match input MIL size
BNNS Graph: Missing required dynamic size for %zu-th argument
BNNS Graph: Called batch size interface, but graph requires a non-batch dynamic size for argument %zu
/System/Library/PrivateFrameworks/MIL.framework/MIL
BNNS cannot compile graph: MIL framework is not available
filename provided is empty
BNNS GraphGetWorkspaceSize passed invalid graph
BNNS GraphGetWorkspaceSize passed graph with unsupported ir_version %u
BNNS GraphGetArgPosition: Argument '%s' not found
BNNS: BNNSGraphContextExecute called before dynamic shapes have been supplied
BNNS GraphGetSize passed invalid graph
BNNS GraphGetSize passed graph with unsupported ir_version %u
BNNS GraphGetArgPosition passed invalid graph
BNNS GraphGetArgPosition passed graph with unsupported ir_version %u
BNNS IR describes a graph with dynamic sizes, BNNSGraphContextExecute() must be used instead of BNNSGraphExecute().
BNNS IR describes a graph with persistent memory requirement, BNNSGraphContextExecute() must be used instead of BNNSGraphExecute().
BNNS Compare: in0 is NULL
BNNS Compare: in1 is NULL
BNNS Compare: out is NULL
BNNS Compare: Unsupported I/O tensor data types.
BNNS Compare: Mismatch in input tensor data types.
BNNS Compare: Invalid operation %d for data type %s.
BNNS Compare: Broadcast failed.
BNNS Compare: I/O tensor dimension %lu mismatch.
BNNS Compare: Partially overlapping input and output tensors are not supported.
BNNS Compare: Unsupported I/O tensor layout.
BNNS Compare: I/O tensor layout mismatch.
BNNS Compare: input size[%zu]=%zu does not equal output size of %zu or 1
BNNS Compare: Unsupported operator.
 = crop_resize(
, ), 
BNNS crop_resize: input and output data types don't match.
BNNS crop_resize: input and roi data types don't match.
hinput
input_u
input_r
input_e
recurrent_u
recurrent_r
recurrent_e
 = grufused(input_u=
, input_r=
, input_e=
, recurrent_u
, recurrent_r
, recurrent_e
, hidden_input=
internal.grufused
BNNS Fully Connected Apply: failed to allocated scratch memory
BNNS Fully Connected Init: missing data table for indexed data type
BNNS Fully Connected Apply: Ilegal compute block
BNNS Fully Connected Create: failed to create layer context
BNNS Fully Connected Create: allocation failed
BNNS Fully Connected Direct Apply: input pointer is NULL
BNNS Fully Connected Direct Apply: output pointer is NULL
BNNS Fully Connected Direct Apply: failed to create layer context
BNNS Fully Connected Init: failed to create context
BNNS Fully Connected Init: input descriptor CHW layout isn't set properly
BNNS Fully Connected Init: input descriptor isn't set properly
BNNS Fully Connected Init: input descriptor failed validation
BNNS Fully Connected Init: output descriptor CHW layout isn't set properly
BNNS Fully Connected Init: output descriptor isn't set properly
BNNS Fully Connected Init: output descriptor failed validation
BNNS Fully Connected Init: weights descriptor row major size[0] doesn't match input vector size
BNNS Fully Connected Init: weights descriptor row major size[1] doesn't match output vector size
BNNS Fully Connected Init: weights descriptor column major size[0] doesn't match output vector size
BNNS Fully Connected Init: weights descriptor column major size[1] doesn't match input vector size
BNNS Fully Connected Init: weights descriptor isn't set properly
BNNS Fully Connected Init: weights descriptor failed validation
BNNS Fully Connected Init BNNSNDArrayDescriptor: stride[%zu] is too small
BNNS Fully Connected Apply: failed to apply filter
BNNS Random Generator Create: Unsupported method %u.
BNNS Random Generator Create: Failed to allocate memory
BNNS Random Fill Uniform Float: Invalid descriptor
BNNS Random Fill Uniform Float: Range bounds are not exactly representable as bf16
BNNS Random Fill Uniform Float: Range (%g, %g) is empty
BNNS Random Fill Normal Float: Mean and standard deviation are not exactly representable as bf16
BNNS Random Fill Normal Float: Standard deviation (%g) is not greater than zero
BNNS Random Fill Float: Distribution type not supported
BNNS Random Fill Uniform Float: Range bounds are not exactly representable as fp16
BNNS Random Fill Normal Float: Mean and standard deviation are not exactly representable as fp16
BNNS Random Fill Float: Unsupported data type
BNNS Random Fill Uniform Integer: Invalid descriptor
BNNS Random Fill Uniform Integer:%s range (%d, %d) is empty
 (clipped)
BNNS Random Fill Uniform Integer: range (%lld, %lld) is empty
BNNS Random Fill Uniform Integer:%s range (%llu, %lld) is empty
BNNS Random Fill Uniform Integer: Unsupported data type
BNNS Random: CCCryptorCreateWithMode() failed
BNNS Random Fill Uniform: Invalid generator
Failed to create random data
reduce_max
reduce_mean
reduce_min
reduce_prod
reduce_sum
reduce_sum_square
reduce_log_sum
reduce_log_sum_exp
reduce_argmax
reduce_argmin
reduce_l1_norm
reduce_l2_norm
reduce op requires arg reductions to have output type int32
reduce op requires input and output types to match
reduce op only supports inputs of type fp16, fp32 or int32
BNNS Loss: Error unsupported loss function
BNNS Loss: Error Input width is 0
BNNS Loss: Error Input must be BNNSDataTypeFloat32 or BNNSDataTypeBFloat16 type
BNNS Loss Error: Input must be contiguous. stride0 must be 1 or 0
BNNS Loss Error: Input tensors must be contiguous.
BNNS Loss: Error unknown reduction function
BNNS Loss: Error output data type must be BNNSDataTypeFloat32 or BNNSDataTypeBFloat16
BNNS Loss: Error output size>1 is only allowed with reduction BNNSLossReductionNone.
BNNS Yolo loss: Error out width (size[0]) must be 1 as yolo loss is always reduced
BNNS Loss: Error anchors is NULL
BNNS Loss: Error number of grid rows is 0
BNNS Loss: Error number of grid columns is 0
BNNS Loss: Error number of anchor boxes is 0
BNNS Loss: Error anchor box size <= 5
BNNS Loss: Error object minimum iou is negative
BNNS Loss: Error no object maximum iou is negative
BNNS Loss: Error input descriptor width (%zu) different from expected width of grid_size*num_anchors*(5+num_+classes) (%zu)
BNNS Loss: memory allocation failed
BNNS Loss: Error filter is NULL
BNNS Loss: Error batch_size is 0
BNNS Loss: Error in pointer is NULL
BNNS Loss: Error in_stride is smaller than i_desc.size[0] or Tensor size
BNNS Loss: Error labels is NULL
BNNS Loss: Error labels_stride is smaller than i_desc.size[0] or Tensor size
BNNS Loss: Error weights_size>0 but weights pointer is NULL
BNNS Loss: Error out is NULL
BNNS Loss: Error in_delta descriptor is ilegal
BNNS Loss: Error in_delta must be contiguous, such that stride[0] is 0 or 1
BNNS Loss Error: data type not supported for softmax
BNNS Yolo loss: input and output data type must be BNNSDataTypeFloat32
BNNS Loss Error: data type not supported
BNNS Loss Error: unsupported loss function
BNNS Loss Backward: Error filter is NULL
BNNS Loss Backward: Error batch_size is 0
BNNS Loss Backward: Error in is NULL
BNNS Loss Backward: Error in_stride is smaller than i_desc.size[0]
BNNS Loss Backward: Error labels is NULL
BNNS Loss Backward: Error labels_stride is smaller than i_desc.size[0]
BNNS Loss Backward: Error weights_size>0 but weights pointer is NULL
BNNS Loss Backward: Error in_delta descriptor is illegal
BNNS Loss Backward: Error in_delta must be contiguous, such that stride[0] is 0 or 1
BNNS Loss Backward: Error in_delta is NULL
BNNS Loss Backward: Error out_delta descriptor is illegal
BNNS Loss Backward: Error out_delta must be contiguous, such that stride[0] is 0 or 1
BNNS Loss Backward: Error out_delta is NULL
BNNS Loss: data type not supported
BNNS Loss: unsupported loss backward function
BNNS Yolo loss: Error yolo reduction type must be BNNSLossReductionSum
BNNS Yolo loss: Error input descriptor width does not match grid_rows*grid_columns*anchors*(5+classes)
BNNS Yolo loss: alloc failed
BNNS Yolo loss: malloc failed
BNNS softmax cross entropy loss: malloc for pDx2 failed
BNNS softmax cross entropy loss: malloc for norm_x failed
BNNS softmax cross entropy loss: malloc for loss failed
BNNS Loss: Error reduction BNNSLossReductionNonZeroWeightMean all weights are zero
BNNS loss: malloc for loss failed
BNNS categorical cross entropy loss: malloc for loss failed
BNNS softmax cross entropy loss: malloc for temporary_softmax_forward_output failed
BNNS Create Depthwise Convolution: incompatible numbers of channels between images and convolution parameters
BNNS Create Depthwise Convolution: unsupported weight format
BNNS Create Depthwise Convolution: input must be BNNSDataLayoutImageCHW layout 
BNNS Create Depthwise Convolution: input descriptor is illegal
BNNS Create Depthwise Convolution: output must be BNNSDataLayoutImageCHW layout 
BNNS Create Depthwise Convolution: output descriptor is illegal
BNNS Depthwise Convolution Create: weights descriptor is illegal
BNNS Depthwise Convolution Create: does not support asymmetric padding. left and right pad must match, up and down pad must match
BNNS Create Depthwise Convolution: only float32 is supported
BNNS Create Depthwise Convolution: channel multiplier mismatch
BNNS Create Depthwise Convolution: malloc failed
BNNS Create Depthwise Convolution: weights malloc failed 
invalid argument
BNNS DEPTHWISE CONV: input batch stride doesn't make sense (%zu < %zu x %zu)
BNNS DEPTHWISE CONV: output batch stride doesn't make sense (%zu < %zu x %zu)
  params = {
    .use_dispatch = %c, .num_contiguous_regions = %llu, .regions_per_block = %llu, .elements_per_block = %llu
    .type = %s, .contig_size = %llu, .non_contig_rank = %llu,
    .non_contig_size = {
%s%llu
}, .non_contig_dest_stride = {
}, .non_contig_src_stride = {
= rnn(x=
BNNS LSTM Direct Apply: filter initialization failed
BNNS LSTM Direct Apply: training cache capacity isn't sufficient
BNNS LSTM Direct Apply: Dropout is only supported in the presence of a training cache
BNNS LSTM Direct Apply: failed to allocate scratch buffer
BNNS LSTM Direct Apply: failed to thread workspace buffer
BNNS LSTM Direct Apply Backward: BNNSNDArrayFlagBackpropAccumulate is only supported on layer_delta->input_descriptor.data_desc.
BNNS LSTM Direct Apply Backward: layer_params initialization failed
BNNS LSTM Direct Apply Backward: layer_delta initialization failed
BNNS LSTM APPLY BACKWARD: Use of dropout without training cache is not supported.
BNNS LSTM Direct Apply Backward: failed to allocate training cache buffer
BNNS LSTM Direct Apply Backward: failed to compute forward intermediate results
BNNS LSTM Direct Apply Backward: training cache capacity isn't sufficient
BNNS LSTM Direct Apply Backward: failed to allocate scratch buffer
BNNS LSTM init: hidden_size must be greater than zero
BNNS LSTM init: input descriptor.data isn't set correctly (size don't match seq_len/batch_size/input_size)
BNNS LSTM init: input descriptor.data isn't set correctly (size don't match batch_size/seq_len/input_size)
BNNS LSTM init: input descriptor.data isn't set correctly (size don't match input_size/batch_size/seq_len)
BNNS LSTM init: input descriptor.hidden isn't set correctly (size don't match hidden_size/batch_size/num_directions/num_layers)
BNNS LSTM init: input descriptor.cell_state isn't set correctly (size don't match hidden_size/batch_size/num_directions/num_layers)
BNNS LSTM init: output descriptor.data isn't set correctly (size don't match seq_len/batch_size/hidden_size*num_directions)
BNNS LSTM init: output descriptor.data isn't set correctly (size don't match batch_size/seq_len/hidden_size*num_directions)
BNNS LSTM init: output descriptor.data isn't set correctly (size don't match hidden_size/num_directions/batch_size/seq_len)
BNNS LSTM init: output descriptor.hidden isn't set correctly (size don't match hidden_size/batch_size/num_directions/num_layers)
BNNS LSTM init: output descriptor.cell_state isn't set correctly (size don't match hidden_size/batch_size/num_directions/num_layers)
BNNS LSTM init: forget_gate isn't set correctly (size don't match hidden_size/batch_size/num_directions/num_layers)
BNNS LSTM init: input_gate isn't set correctly (size don't match hidden_size/batch_size/num_directions/num_layers)
BNNS LSTM init: candidate_gate isn't set correctly (size don't match hidden_size/batch_size/num_directions/num_layers)
BNNS LSTM init: output_gate isn't set correctly (size don't match hidden_size/batch_size/num_directions/num_layers)
BNNS LSTM: Data type not supported, fail
softmax
conv
conv_transpose
cast
crop
upsample_nearest_neighbor
upsample_bilinear
resize_bilinear
resize_nearest_neighbor
crop_resize
linear
matmul
avg_pool
max_pool
l2_pool
batch_norm
instance_norm
layer_norm
random_categorical
lstm
scatter
scatter_along_axis
scatter_nd
cumsum
identity
range_1d
shape
tile
topk
concat
depth_to_space
expand_dims
gather
gather_along_axis
gather_nd
pixel_shuffle
pixel_unshuffle
reshape
slice_by_index
slice_by_size
space_to_depth
split
squeeze
stack
transpose
constexpr_cast
constexpr_lut_to_dense
: Failed to parse x argument
: Failed to parse y argument
values
axis
interleave
m2a_attr
palletize_int4
Failed to palletize %s
: Failed to parse Output 
output_dtype
source_val
: only 4-bit palletization of fp16 is supported
indices
%s: only 1d and 2d convolutions are supported
weights
weight
strides
dilations
pad_type
%s: Unsupported conv pad type '%s'
groups
output_shape
crop_height
: Failed to parse `crop_height`
crop_width
: Failed to parse `crop_width`
: Failed to parse roi argument
target_height
target_width
normalized_coordinates
spatial_scale
box_coordinate_mode
CORNERS_HEIGHT_FIRST
CORNERS_WIDTH_FIRST
CENTER_SIZE_HEIGHT_FIRST
CENTER_SIZE_WIDTH_FIRST
sampling_mode
STRICT_ALIGN_CORNERS
ALIGN_CORNERS
DEFAULT
OFFSET_CORNERS
UNALIGN_CORNERS
exclusive
: Failed to parse weight argument
squeeze(of:
, for: matvec(
internal.squeeze
pre_expand(of:
internal.expand_dims
matrix
mode
constant
reflect
replicate
: Unsupported pad mode '
constant_val
: Unsupported data type for constant_val
start
step
align_corners
scale_factor_height
scale_factor_width
UPSAMPLE scale_factor data type not supported
UPSAMPLE need scale_factor
target_size_height
target_size_width
RESIZE need target_size
: BNNS does not support peephole LSTM
direction
: BNNS does not support direction = 
output_sequence
recurrent_activation
cell_activation
activation
: BNNS does not support LSTM clip
: Direction 
 not supported
: Activation type 
begin
stride
begin_mask
end_mask
squeeze_mask
: BNNS does not support non-constant squeeze_mask
size
num_splits
split_sizes
: split size 
 not divisiable by number of outputs 
: Input Tensor rank 
doesn't match Output Tensor rank 
reps
ascending
output_values
output_indices
undefined shuffle operator.
n_threads
perm
epsilon
beta
SIGMOID_APPROXIMATION
: Failed to parse alpha argument
: Failed to parse shape argument
: Unsupported mode '
seed
axes
keep_dims
mean
: Failed to parse mean argument
variance
: Failed to parse variance argument
gamma
: axes are not contiguous
: Unsupported gather type
: Failed to parse input argument
: Failed to parse indices argument
updates
: Failed to parse updates argument
: Unsupported scatter type
transpose_x
transpose_y
: only 1d and 2d avg pooling are supported
kernel_sizes
: Unsupported avg_pool pad type '
exclude_padding_from_average
ceil_mode
: ceil_mode must be false
: input width and padding don't have enough elements to generate output
: input height or padding don't have enough rows to generate output
: only 1d and 2d max pooling are supported
: Unsupported pad type '
: only 1d and 2d l2 pooling are supported
minimum_compute_type
: BNNS does not support dynamic shaped literal tensors
string
BNNS Graph Compile: Unsupported data type '%s'
BNNS Graph Compile: Unsupported data type
: Failed to parse 
 argument
: BNNS does not support 
 type 
Can't cast this scalar :(
permute only supports fp16 and fp32 operands
 = permute(
constexpr_replaced(
Cross compilation not currently supported
BNNS: Model was compiled for hardware not available on this device
gather only supports fp16, fp32 and int32 inputs
gather only supports fp16 and fp32 outputs
BNNS Convolution Create: Client needs to support weights ptr with low_mem
BNNS Convolution Create: malloc
BNNS Convolution Create: failed to create convolution context
BNNS Convolution Create: failed to divide work between convolution contexts to fit in memory
BNNS Convolution Create: failed to create single generic convolution
BNNS Convolution Apply: invalid argument
BNNS Convolution Apply: failed to allocate input auxiliary buffer
BNNS Convolution: allocation of contexts failed
BNNS LOW MEM CONVOLUTION: failed to create winograd convolution
BNNS LOW MEM CONVOLUTION: malloc failed
BNNS LOW MEM CONVOLUTION: incompatible BNNS_low_memory_context id
BNNS LOW MEM CONVOLUTION: convolution failed
BNNS LOW MEM CONVOLUTION: input aux buffer wasn't allocated
BNNS LOW MEM CONVOLUTION:: context type not supported
BNNS Convolutions Apply: unexpected input data type
BNNS Convolution Create: apply convolution failed
BNNS graph batchnorm: Unrecognized data type
BNNS graph layernorm: Unrecognized data type
BNNS graph channelnorm: Unrecognized data type
BNNS Quantization Filter: layer_params is NULL
BNNS Quantization: input layout and output layout must match
BNNS Quantization: unsupported input/output layouts
BNNS Quantization: input descriptor error
BNNS Quantization: input descriptor data is NULL
BNNS Quantization: output descriptor error
BNNS Quantization: output descriptor data is NULL
BNNS Quantization: invalid quantizer function
BNNS Quantization: BNNSQuantize function supported for the following input descriptor data types: BNNSDataTypeFloat32, BNNSDataTypeFloat16, BNNSDataTypeBFloat16, BNNSDataTypeInt32
BNNS Quantization: BNNSQuantize function supported for the following output descriptor data types: BNNSDataTypeInt8, BNNSDataTypeUInt8, BNNSDataTypeInt16, BNNSDataTypeUInt16, BNNSDataTypeInt32, BNNSDataTypeUInt32
BNNS Quantization: BNNSDeQuantize function supported for the following input descriptor data types: BNNSDataTypeInt8, BNNSDataTypeUInt8, BNNSDataTypeInt16, BNNSDataTypeUInt16, BNNSDataTypeInt32, BNNSDataTypeUInt32
BNNS Quantization: BNNSDeQuantize function supported for the following output descriptor data types: BNNSDataTypeFloat32, BNNSDataTypeFloat16, BNNSDataTypeBFloat16, BNNSDataTypeInt32
BNNS Quantization: invalid axis_mask, number of mask set bits must be lower or equal to 1
BNNS Quantization: axis_mask bits are set beyond batch dimension
BNNS Quantization: Error dimensions of input tensor (%zu) and output tensor (%zu) must match
BNNS Quantization: Error shape of input tensor and output tensor must match
BNNS Quantization Filter: scale layout must be BNNSDataLayoutVector
BNNS Quantization: scale descriptor error
BNNS Quantization Filter: bias layout must be BNNSDataLayoutVector
BNNS Quantization: bias descriptor error
BNNS Quantization: bias vector size does not match axis mask, bias size[0]=%zu, expected %zu
BNNS Quantization: scale vector size does not match axis mask, scale size[0]=%zu, expected %zu
BNNS Fused Convolution and Normalization: Error convolution_layer_params is NULL
BNNS Fused Convolution and Quantization: Error quantization_layer_params is NULL
BNNS Fused Compute and Quantization: Error Convolution output layout and Quantization input layout must match
BNNS Fused Compute and Quantization: Error Convolution output data type and Quantization input data type must match
BNNS Fused Compute and Quantization: Error Convolution output descriptor sizes and Quantization input descriptor sizes must match
BNNS Fused Compute and Quantization: Error Convolution output descriptor strides and Quantization input descriptor strides must match
BNNS Fused Compute and Quantization: Error memory allocation failed
BNNS Fused Compute and Quantization create filter failed: compute filter type error
BNNS Fused Compute and Quantization create filter failed
BNNS Quantization filter failed
BNNS Fused Fully Connected and Quantization: Error fully_layer_params is NULL
BNNS Fused Fully Connected and Quantization: Error quantization_layer_params is NULL
BNNS Fused Compute and Quantization: Error Fully Connected output layout and Quantization input layout must match
BNNS Fused Compute and Quantization: Error Fully Connected output data type and Quantization input data type must match
BNNS Fused Compute and Quantization: Error Fully Connected output descriptor sizes and Quantization input descriptor sizes must match
BNNS Fused Compute and Quantization: Error Fully Connected output descriptor strides and Quantization input descriptor strides must match
BNNS Quantization: unsupported data type
BNNS Quantization: unsupported quantization input data type
BNNS Quantization: unsupported quantization output data type
BNNS Quantization: unsupported dequantization input data type
BNNS Quantization: unsupported dequantization output data type
BNNS Transposed Convolution Create: input data type isn't supported
BNNS Transposed Convolution Create: weight data isn't supported
BNNS Transposed Convolution Create: output data type isn't supported
BNNS Transpsoed Vector Convolution Apply: activation gradient auxilary allocation failed
BNNS Transposed Convolution Apply: Filter is NULL
BNNS Transposed Convolution Apply: failed to allocate memory to manipulate input
BNNS Transposed Convolution Apply: unknown weight layout
BNNS Transposed Convolution Create: layer_params is NULL
BNNS Transposed Convolution Create: output channels must divide by groups
BNNS Transposed Convolution Create: input channels must divide by groups
BNNS Transposed Convolution Create: low-mem isn't supported
BNNS Transposed Convolution Create: packed weights aren't supported
BNNS Transposed Convolution Create: failed to create forward path
BNNS Transposed Convolution Create: failed to allocate wrapper memory
BNNS Transposed Convolution Create: transposed convolution groups only supported with BNNSDataLayoutConvolutionWeightsOIHW or BNNSDataLayoutConvolutionWeightsOIHrWr layouts
BNNS Transposed Convolution Create: weights allocation failed
BNNS Create Layer Transposed Convolution: input channels must be divisible by groups
BNNS Create Layer Transposed Convolution: output channels must be divisible by groups
BNNS Create Layer Transposed Convolution: bias must match output channels
BNNS Create Layer Transposed Convolution: failed to allocate memory to copy weights and bias
BNNS Transposed Convolution Create: malloc failed
BNNS Transposed Convolution: weights allocation failed
BNNS Transposed Convolution: bias allocation failed
BNNS Transposed Convolution Reorder Weights: NULL weight object
BNNS Transposed Convolution Reorder Weights: dst weight size too small
BNNS Transposed Convolution Reorder Weights: NULL weight array
BNNS Transposed Convolution Rotate Weights: NULL weight object
BNNS Transposed Convolution Rotate Weights: dst weight size too small
BNNS Transposed Convolution Rotate Weights: NULL weight array
BNNS Transposed Convolution Reorder and Rotate Weights: NULL weight object
BNNS Transposed Convolution Reorder and Rotate Weights: dst weight size too small
BNNS Transposed Convolution Reorder and Rotate Weights: NULL weight array
transposed convolution input manipulation failed
transposed convolution input manipulation failed - nothing to do, should not have allocated dst_input buffer
BNNS: unsupported activation gradient data type
BNNS Transposed Convlution Backward: dy padding failed
BNNS Transposed Convlution Backward: no padding is needed, dy padding failed
BNNS Create Convolution: fp32 weights allocation failed
BNNS Create Convolution: int16 weights allocation failed
BNNS Create Convolution: failed to allocate memory to copy the weights
BNNS Create Convolution: single descriptors allocation failed
BNNS Convolution Create: allocation failed
failed to upconvert or copy weights
failed to allocate none generic format
BNNS Convolution Create: forward pass check (swapping input and output)  - input size computation wraparound input (%zu x %zu), output (%zu x %zu), stride (%zu x %zu), asymmetric padding (%zu,%zu,%zu,%zu)
BNNS Convolution Create: forward pass check (swapping input and output)  - input size computation wraparound input (%zu x %zu), output (%zu x %zu), stride (%zu x %zu), padding (%zu x %zu)
BNNS Convolution Create: forward pass check (swapping input and output) - output(%zu x %zu x %zu) is larger than input(%zu x %zu x %zu) with padding(%zu x %zu).
 kernel (%zu x %zu) and stride (%zu x %zu)
v8@?0
com.apple.accelerate.bnns
layer
BNNS Fully Connected Sparsify: out descriptor can't be NULL
BNNS Fully Connected Sparse: failed to allocate memory to sparsify COO
BNNS Fully Connected Sparse: insufficient memory allocated memory to sparsify COO
BNNS Fully Connected Sparse: Number of nonzeros is 0
BNNS Fully Connected Sparsify: in descriptor can't be NULL
BNNS Normalization: layer_params is NULL
BNNS Normalization: filter memory allocation failed
BNNS Fused Convolution and Normalization: Error normalization_layer_params is NULL
BNNS Fused Convolution and Normalization: Error convolution output descriptor and normalization input descriptor must have the same sizes and strides
BNNS Fused Convolution and Normalization: Error memory allocation failed
BNNS Fused Convolution and Normalization create filter failed: Convolution type error
BNNS Fused Convolution and Normalization create filter failed
BNNS Fused Fully Connected and Normalization: Error fully_layer_params is NULL
BNNS Fused Fully Connected and Normalization: Error batch_normalization_layer_params is NULL
BNNS Fused Fully Connected and Normalization: Error fully connected output descriptor and normalization input descriptor must have the same sizes and strides
BNNS Fused Fully Connected and Normalization: Error memory allocation failed
BNNS Fused Fully Connected and Normalization create filter failed
BNNS Fused Arithmetic and Normalization: Error arithmetic_layer_params is NULL
BNNS Fused Arithmetic and Normalization: Error normalization_layer_params is NULL
BNNS Fused Arithmetic and Normalization: unsupported arithmetic function
BNNS Fused Arithmetic and Normalization: Error arithmetic output descriptor and normalization input descriptor must have the same sizes and strides
BNNS Fused Arithmetic and Normalization: Error memory allocation failed
BNNS Fused Arithmetic and Normalization create filter failed
BNNS Normalization: input pointer is NULL
BNNS Normalization: batch_size>1 and in_stride is 0
BNNS Normalization: output pointer is NULL
BNNS Normalization: batch_size>1 and out_stride is 0
BNNS Normalization: batch_size too large. Backprop cache size limits batch_size <= %zu.
BNNS Normalization: x_hat allocation failed
BNNS Normalization: filter id not supported
BNNS Normalization: inverse variance allocation failed
BNNS Normalization Apply Backward: filter is NULL
BNNS Normalization Apply Backward: Normalization beta delta pointer is NULL. backward computation of all active parameters must be done in a single function call.
BNNS Normalization Apply Backward: Normalization gamma delta pointer is NULL. backward computation of all active parameters must be done in a single function call.
BNNS Normalization Apply Backward: make sure to run normalization forward with training flag enabled before running normalization backward
BNNS Normalization Apply Backward: cannot compute activation backward, output is NULL
BNNS Normalization Apply Backward: Fusion of normalization with activation is unsupported for given activation function
BNNS Normalization Apply Backward: cannot compute input delta because input delta data pointer is NULL
BNNS Normalization Apply Backward: Error normalization input delta and input must have the same sizes
BNNS Normalization Apply Backward: in delta should be contiguous and same size as input size
BNNS Normalization Apply Backward: cannot compute beta delta because beta delta data pointer is NULL
BNNS Normalization Apply Backward: BNNSNDArrayFlagBackpropAccumulate is no supported for beta_delta.
BNNS Normalization Apply Backward: Error normalization input delta and input must have the same sizes and strides
BNNS Normalization Apply Backward: Error beta_delta descriptor must match the shape of input up to the normalizaton axis
BNNS Normalization Apply Backward: beta delta should be contiguous and same size as number of input channels
BNNS Normalization Apply Backward: cannot compute gamma delta because gamma delta data pointer is NULL
BNNS Normalization Apply Backward: BNNSNDArrayFlagBackpropAccumulate is no supported for gamma_delta.
BNNS Layer Normalization: Error gamma_delta descriptor must match the shape of input up to the normalizaton axis
BNNS Normalization Apply Backward: gamma delta should be contiguous and same size as number of input channels
BNNS Normalization Apply Backward: out delta data is NULL
BNNS Normalization Apply Backward: Error Normalization output delta and output must have the same sizes
BNNS Normalization Apply Backward: out delta should be contiguous and same size as input size
BNNS Normalization Apply Backward: normalization type not supported
BNNS Normalization Set State: Backprop cache must be in BNNSDataLayoutVector
BNNS Normalization Set State: Backprop cache must be in BNNSDataTypeFloat32
BNNS Normalization Set State: Backprop cache must be in contiguous
BNNS Normalization Set State: Backprop cache pointer is NULL
BNNS Normalization Set State: Backprop cache too small, must be >= %zu fp32 to allow max_batch_size >= 1
BNNS Normalization: Only elementwise activation function is supported
BNNS Normalization: Moving Mean must be BNNSDataTypeFloat32
BNNS Normalization: Moving Mean isn't set properly
BNNS Normalization: Moving Variance must be BNNSDataTypeFloat32
BNNS Normalization: Moving Variance isn't set properly
BNNS Normalization Create: normalization type not supported
BNNS Normalization: input descriptor isn't set properly
BNNS Normalization: Input must be in BNNSDataLayoutImageCHW layout
BNNS Normalization: output descriptor isn't set properly
BNNS Normalization: Output must be in BNNSDataLayoutImageCHW layout
BNNS Normalization: Gamma descriptor isn't set properly
BNNS Normalization: Beta descriptor isn't set properly
BNNS Normalization: Error input descriptor must have stride[0]=1
BNNS Normalization: Error output descriptor must have stride[0]=1
BNNS Normalization: Error input size (%zu) different from output size (%zu)
BNNS Normalization: Error normalization_axis must be 0, 1, or 2
BNNS Layer Normalization: Gamma descriptor must match the shape of input up to the normalizaton axis
BNNS Normalization: Beta descriptor must match the shape of input up to the normalizaton axis
BNNS Normalization: Gamma descriptor size[0] must be the same as number of input channels
BNNS Normalization: Beta descriptor size[0] must be the same as number of input channels
BNNS Normalization: Moving Mean descriptor size[0] must be the same as number of input channels
BNNS Normalization: Moving Variance descriptor size[0] must be the same as number of input channels
BNNS Normalization: Error momentum must be between 0 and 1
BNNS CONVOLUTIONS VERSION2: malloc failed
BNNS CONVOLUTIONS VERSION2: malloc failed
BNNS CONVOLUTIONS VERSION2: failed to allocate bias conversion memory
BNNS CONVOLUTIONS VERSION2: failed to allocate scratch memory
BNNS CONVOLUTIONS VERSION2: invalid argument
BNNS CONVOLUTIONS VERSION2: input batch stride doesn't make sense (%zu < %zu x %zu)
BNNS CONVOLUTIONS VERSION2: output batch stride doesn't make sense (%zu < %zu x %zu)
BNNS CONVOLUTIONS VERSION2: unsupported weight packing
BNNS CONVOLUTIONS VERSION2: failed to allocated memory to convert bias
BNNS CONVOLUTIONS VERSION2: input padding memory allocation failed
BNNS CONVOLUTIONS VERSION2: output repack malloc failed
BNNS CONVOLUTIONS VERSION2: weight repack malloc failed
BNNS Padding Create: layer_params is NULL
BNNS Padding Create: Padding is not supported beyond 4-D tensors.
BNNS Padding Create: Undefined padding mode.
BNNS Padding Create: Unsupported data layout.
BNNS Padding Create: input and output desciptors have differing numbers of dimensions.
BNNS Padding Create: Input dimension is too small for the padding size.
BNNS Padding Create: Input size + padding sizes doesn't match output size in dimension %zu.
BNNS Padding Create: input descriptor is illegal
BNNS Padding Create: output descriptor is illegal
BNNS Padding Create: Unsupported data type.
BNNS Padding Create: I/O data type mismatch.
BNNS Padding Create: memory allocation failed
BNNS Padding Apply: filter is NULL
BNNS Padding Apply: wrong filter type, filter is not Padding.
BNNS Padding Apply: input pointer is NULL
BNNS Padding Apply: batch_size > 1 and in_stride is 0
BNNS Padding Apply: output pointer is NULL
BNNS Padding Apply: batch_size > 1 and out_stride is 0
BNNS Padding Apply Backward: filter is NULL
BNNS Padding Apply Backward: wrong filter type, filter is not padding
BNNS Padding Apply Backward: input descriptor is NULL
BNNS Padding Apply Backward: input data pointer is NULL
BNNS Padding Apply Backward: batch_size>1 and in_stride is 0
BNNS Padding Apply Backward: output delta descriptor is NULL
BNNS Padding Apply Backward: output delta data pointer is NULL
BNNS Padding Apply Backward: batch_size>1 and out_delta_stride is 0
[range_1d] type not supported.
BNNS MIL Matvec Create: unexpected compute path
BNNS Pooling: Error wrong filter type, filter is not pooling
filter is NULL
out_delta is NULL
out_delta->data is NULL
BNNSFilterCreateLayerSparseEmbedding: layer_params must not be NULL
BNNSSparseGetRepresentation: filter must not be NULL
BNNSSparseGetRepresentation: Invalid filter
BNNSSparseSetRepresentation: filter must not be NULL
BNNSSparseSetRepresentation: Invalid filter
BNNSSparseEmbeddingGetDense: filter must not be NULL
BNNSSparseEmbeddingGetDense: Invalid filter
BNNS Fused Filter Multi-Input: Error filter is NULL
BNNS Fused Filter Backward Multi-Input: Error filter is NULL
BNNS Fused Filter Backward Multi-Input: output delta is NULL
BNNS Arithmetic Backward Multi-Input: failed to allocate memory
x_ijf*, x_ijc* -> G_fc*
BNNSFilterApplyTwoInputBatch: invalid argument
Tensor Contraction filter only expects one input
invalid filter or incorrect number of inputs
BNNSFilterApplyBackwardTwoInputBatch: invalid argument
Two-input Tensor Contraction filter has no weights
Tensor Contraction filter has no bias
Invalid filter
Unsupported filter
BNNS GetStateSize : filter is NULL
BNNS SetState: Error filter is NULL
BNNS SetState: Source filter state is NULL
BNNS SetState: Target filter is NOT Dropout/Normalization filter
BNNS GetState: Error filter is NULL
BNNS GetState: Target filter state is NULL
BNNS GetState: Source filter is NOT Dropout filter
BNNS LayerNorm: Error Normalization axis must be 0, 1, or 2.
BNNS GroupNorm: Error The number of input channels must be divisible by the number of groups.
BNNS Fused Filter: Error number_of_fused_filters is not 2. currently supporting only 2 fused filters
BNNS Fused Filter: Error filter_type is NULL
BNNS Fused Filter: Error layer_params is NULL
BNNS Fused Filter: Error currently supporting first filter of BNNSConvolution, BNNSFullyConnected, BNNSTransposedConvolution, or BNNSArithmetic type only
BNNS Fused Filter: Error currently supporting second filter of BNNSBatchNorm, BNNSInstanceNorm, BNNSLayerNorm, BNNSGroupNorm and BNNSQuantization types only
BNNS Fused Filter: Error layer_params[0] is NULL
BNNS Fused Filter: Error layer_params[1] is NULL
BNNS Fused Filter: Error unknown fused filter type
BNNS Normalization: Error filter is NULL
BNNS Normalization: Error wrong filter type, filter is not normalization
BNNS Fused Filter: Error filter is NULL
BNNS Arithmetic Filter: Error filter is NULL
BNNS Permute Filter: filter is NULL
BNNS Permute Filter: input delta is NULL
BNNS Permute Filter: output delta is NULL
BNNS Permute Filter: input delta data pointer is NULL
BNNS Permute Filter: output delta data pointer is NULL
BNNS Permute Filter: inplace gradient is not supported
Tensor Contraction filter has no bias: bias_delta must be NULL.
Tensor Contraction filter expects more than one input
Resize filter has no bias: bias_delta must be NULL.
Resize filter has no weights: weights_delta must be NULL.
Resize filter: backward pass requires in_delta and in_delta->data to be non-NULL.
BNNS Copysum filter has no bias: bias_delta must be NULL.
BNNS Copysum filter has no weights: weights_delta must be NULL.
BNNS Copysum filter: backward pass requires in_delta and in_delta->data to be non-NULL.
BNNS Reduction filter has no bias: bias_delta must be NULL.
BNNS Padding: backward pass requires in_delta and in_delta->data to be non-NULL.
BNNS Padding filter has no bias: bias_delta must be NULL.
BNNS Embedding: in must not be NULL.
BNNS Embedding: in_delta must be NULL.
BNNS Embedding: backward pass requires weights_delta and weights_delta->data to be non-NULL.
BNNS Embedding: bias_delta must be NULL.
BNNS Normalization Backward: Error filter is NULL
BNNS Fused Filter Backward: Error filter is NULL
BNNS Fused Filter Backward: output delta is NULL
BNNS Fused Filter Backward: Error delta_parameters is NULL
BNNS Arithmetic Backward: failed to allocate memory
BNNSDirectApplyConvolutionBatch not supported 
BNNSDirectApplyTransposedConvolutionBatch not supported 
BNNSDirectApplyPoolingBatch not supported 
BNNSDirectApplyLossBatch not supported
BNNS Apply: filter or output can't be null
BNNS Apply: Tensor Contraction filter expects more than one input
BNNS Apply: Loss filter apply must be called with BNNSLossFilterApplyBatch
BNNS Apply: Batchnorm filter apply must be called with BNNSNormalizationFilterApplyBatch
BNNS Apply: Instance Norm filter apply must be called with BNNSNormalizationFilterApplyBatch
BNNS Apply: Layer Norm filter apply must be called with BNNSNormalizationFilterApplyBatch
BNNS Apply: Group Norm filter apply must be called with BNNSNormalizationFilterApplyBatch
BNNS Apply: Fused filter apply must be called with BNNSFusedFilterApplyBatch
BNNS Apply: invalid filter
BNNS DESTROY: invalid filter
Convolution
FullyConnected
Pooling
Activation
SparseConvolution
SparseFullyConnected
Loss
Batchnorm
Instancenorm
Layernorm
Groupnorm
Fused{Compute,Normalization}
Dropout
Contraction
Resize
LSTM
Arithmetic
Copysum
MultiheadAttention
Reduction
Padding
Embedding
Fused{Compute,Quantization}
Fused{Arithmetic,Normalization}
Fused{Affine Grid, Grid Sample}
Unknown
BNNS Padding + Convolution Apply Backward: input is null
BNNS Padding + Convolution Apply Backward: allocation failed
BNNS Padding + Convolution Apply Backward: Padding failed
BNNS Padding + Convolution Apply Backward: Convolution backward failed
BNNS Padding + Convolution Apply Backward: Padding backward failed
malloc failed
Padding failed
Convolution failed
BNNS LSTM Apply: Unsupported data type
anon_context
_literal
BNNS Graph: 
sorted_outputs = 
outputs = 
missing from sorted_outputs:
missing from outputs:
BNNS Band Part: illegal input descriptor
BNNS Band Part: illegal output descriptor
BNNS Band Part: input and output data type or dimension do not match or the dimension is less than two
BNNS Band Part: input and output sizes do not match
BNNS Convolution Variant: input type %s not supported!!!
BNNS Convolution Variant: compute type %s not supported!!!
BNNS GRU Fused Gates: data type not supported
BNNS GRU Fused Gates: Output pointer is null
BNNS GRU Fused Gates: Input pointer is null
BNNS GRU Fused Gates: Recurrent pointer is null
BNNS GRU Fused Gates: Hidden Input pointer is null
BNNS GRU Fused Gates: Input vector size %zu foesn't match output vector size %zu
BNNS GRU Fused Gates: Recurrent vector size %zu foesn't match output vector size %zu
BNNS GRU Fused Gates: Hidden Input vector size %zu foesn't match output vector size %zu
BNNS GRU Fused Gates: number of gates for Input %zu and Recurrent %zu doesn't match
BNNS GRU Fused Gates: Input stride %zu is smaller than number of elements %zu
BNNS GRU Fused Gates: Recurrent stride %zu is smaller than number of elements %zu
BNNS GRU Fused Gates: Output supported data types are fp32 or fp16
BNNS GRU Fused Gates: Input data type doesn't match output data type
BNNS GRU Fused Gates: Recurrent data type doesn't match output data type
BNNS GRU Fused Gates: Hidden Input data type doesn't match output data typen
BNNS GRU Fused Gates: GRU fused gates doesn't support %zu gates
BNNS GRU Fused Gates: Hidden Input batch size %zu doesn't match output batch size %zu
BNNS GRU Fused Gates: Input batch size %zu doesn't match output batch size %zu
BNNS GRU Fused Gates: Hidden Output batch stride %zu is small then a single vector stride
BNNS GRU Fused Gates: Hidden Input batch stride %zu is small then a single vector stride
BNNS GRU Fused Gates: Input batch stride %zu is small then a single vector stride
BNNS GRU Fused Gates: Recurrent batch stride %zu is small then a single vector stride
Unsupported gemm type combination %s x %s -> %s
range_1d does not support dynamic shaped input
range_1d only supports fp16, fp32 and int32 output types
range_1d requires the types of start, end, step and output to match
 = range(
BNNS CONVOLUTIONS VERSION2: unsupported src type to pack
BNNS Convolution: unsupported src type to pack
BNNS Convolution Compute Weights delta: failed to compute
BNNS Convolution Apply Backward: activation_grad allocation failed
weight delta backward failed
BNNS Prefix sum: unsupported data type
BNNS Convolution: int32 repack allocation failed
BNNS Convolution: Float16 repack allocation failed
BNNS Convolution: Float32 repack allocation failed
 = copy(from=
copy from 
BNNS Activation Apply: Error input/output/activation pointers must be non-NULL
BNNS Activation Apply: Input and output tensor doesn't match
BNNS Activation Apply: unable to allocate memory to compute activation
BNNS Activation Apply: failed to allocate memory
BNNS Apply Activation Backward: BNNSActivationFunctionIntegerLinearSaturate isn't supported
BNNS Apply Activation Backward: BNNSActivationFunctionIntegerLinearSaturatePerChannel isn't supported
BNNS Apply Activation Backward: failed to allocate temporary y
BNNS Apply Activation Backward: unsupported activation backward
BNNS Activation backward: malloc failed
BNNS Activation: number of memory descriptor is lower than expected
BNNS Activation: claiming pre allocated memory failed
BNNS Activation: memory allocation failed
BNNS Activation: Failed to init filter
BNNS Activation Apply Backward: At least one of in or out must be non-NULL
BNNS Activation Apply Backward: invalid filter
BNNS Activation Apply Backward: out_delta must not be NULL
BNNS Activation Backward: preallocated memory isn't supported
BNNS Activation Apply Backward: Input is required
BNNS Activation Backward: Weights Delta not supported
BNNS Activation Apply Backward: Bias Delta not supported
BNNS Activation Apply Backward: in place operation for none matching data sizes isn't supported
BNNS Activation Apply Backward: Input and output tensor doesn't match
BNNS Activation Apply Backward: In Delta tensor doesn't match
BNNS Activation Apply Backward: Out Delta tensor doesn't match
BNNS Activation Apply Backward: Weights Delta tensor (%zu x %zu) doesn't make sense
BNNS Activation Apply Backward: unable to allocate memory to compute backward with missing original output
BNNS Activation Apply Backward: failed to allocate memory
BNNS Activation Apply Backward: failed to init index counter
BNNS Activation: invalid argument
BNNS Activation: in-place activation layer is allowed only for output types with the same or smaller storage size
BNNS Activation: unsupported types for conversion
BNNS Activation: val doesn't make sense
BNNS Activation Apply: activation function not supported
BNNS Activation Init: filter is null
BNNS Activation Init: input descriptor is illegal
BNNS Activation Init: output descriptor is illegal
BNNS Activation Init: layout doesn't match
BNNS Activation Init: memory allocation failed
BNNS Activation Init: dim %zu input size %zu != %zu output size
BNNS Activation Init: alpha can't be +/-inf or Nan for Gumbel or Gumbel Max
BNNS Activation Init: beta can't be +/-inf or Nan or zero or negative for Gumbel or Gumbel Max
BNNS Activation Init: BNNSActivationFunctionPReLUPerChannel is only valid with data layout BNNSDataLayoutImageCHW
BNNS Activation Init: invalid activation function
BNNS SoftMax: sum result shouldn't be 0
BNNS Activation Apply Backward: Softmax backward require original output
BNNS Activation Apply Backward: original inputs or original outputs must be available
BNNS Activation Apply Backward: activation function not supported
BNNS Activation: Error NULL input/output pointer
BNNS Activation: input and output dimensions do not match
BNNS Activation: input and output sizes do not match
Unsupported layout: %d
BNNS Multihead Attention Apply: key_mask must be a 1D tensor.
BNNS Multihead Attention Apply: key_mask must have type BNNSDataTypeBoolean.
BNNS Multihead Attention Apply: key_mask must have size exactly source_length (key_mask.size[0] = %zu, source_length = %zu).
BNNS Multihead Attention Apply: 2D add_to_attention must have shape (target_length, source_length) = (%zu, %zu), but passed tensor has shape (%zu, %zu).
BNNS Multihead Attention Apply: 3D add_to_attention must have shape (num_heads, target_length, source_length) = (%zu, %zu, %zu), but passed tensor has shape (%zu, %zu, %zu).
BNNS Multihead Attention Apply: 4D add_to_attention must have shape (batch_size, num_heads, target_length, source_length) = (%zu, %zu, %zu, %zu), but passed tensor has shape (%zu, %zu, %zu, %zu).
BNNS Multihead Attention Apply: unsupported layout for argument add_to_attention.
BNNS Multihead Attention Apply: add_to_attention must have type BNNSDataTypeBoolean or BNNSDataTypeFloat32.
BNNS Multihead Attention Apply: If backprop_cache is non-NULL, backprop_cache_size must also be non-NULL
BNNS Multihead Attention Apply: If workspace is non-NULL, workspace_size must also be non-NULL
BNNS Multihead Attention Apply: Supplied workspace is too small (size %ld, but required %ld)
Multihead Attention Backward: Only support for a full-size backprop_cache has been implemented.
Multihead Attention Backward: If workspace is non-NULL, workspace_size must also be non-NULL
Multihead Attention Backward: Insufficient workspace supplied (%zu bytes). Require %zu bytes.
value_delta_copy_out
value_shadow_delta
value_delta
key_delta_copy_out
key_shadow_delta
key_delta
query_delta_copy_out
query_shadow_delta
query_delta
layer_params->query.target_desc
layer_params->key.target_desc
layer_params->query.weights
layer_params->key.weights
layer_params->value.weights
(d_model, d_key, num_heads)
(source_length, k_dim)
(k_dim, d_key, num_heads)
layer_params->value.target_desc
(source_length, v_dim)
(v_dim, d_value, num_heads)
layer_params->output.target_desc
(target_length, d_model)
layer_params->output.weights
(num_heads*d_value, d_model)
BNNS Multihead Attention Create: layer_params->key_attn_bias.data and layer_params->value.attn_bias.data must both be NULL or must both be not NULL
layer_params->key_attn_bias
(d_key, num_heads)
layer_params->value_attn_bias
(d_value, num_heads)
BNNS Multihead Attention Create: fp16 and bf16 may not be mixed or unsupported data type
layer_params->query.bias
layer_params->key.bias
layer_params->value.bias
layer_params->output.bias
(d_model)
BNNS MHA: layer_params->query.weights.layout=%u is not compatible with low memory flag.
BNNS MHA: layer_params->key.weights.layout=%u is not compatible with low memory flag.
BNNS MHA: layer_params->value.weights.layout=%u is not compatible with low memory flag.
BNNS Multihead Attention Create: Unsupported layout for query.weights
BNNS Multihead Attention Create: Unsupported layout for key.weights
BNNS Multihead Attention Create: Unsupported layout for value.weights
BNNS MULTIHEAD ATTENTION GetPointer: Layer was created with dropout=0.0, cannot return pointer to dropout value.
BNNS MULTIHEAD ATTENTION GetPointer: Unsupported target %d
BNNS Multihead Attention Create: Expected %s to have exactly %zu dimensions, but it has %zu
BNNS Multihead Attention Create: Expected %s to have shape %s = (
%s%zu
) but has actual shape (
BNNSGather: Only 8, 16, 32 and 64 bit integer types may be used as indices data type
BNNSGather: Only 16 and 32 bit floating point and 8, 16 and 32 bit integer types may be used as input data type
BNNSGather: Only 16 and 32 bit floating point and 8, 16 and 32 bit integer types may be used as output data type
BNNSGather: indices must be a 1d array or have the same shape as output
BNNSGather: input and output tensors must have the same number of dimensions
BNNSGather: output.size[%zu]=%zu must have same size as indices.size[0]=%zu if indices is 1D
BNNSGather: output.size[%zu]=%zu must have same size as indices.size[%zu]=%zu if indices is not 1D
BNNSGather: output.size[%zu]=%zu must have same size as input.size[%zu]=%zu
BNNSScatter: Only add and set ops are supported
BNNSScatter: Only 8, 16, 32 and 64 bit integer types may be used as indices data type
BNNSScatter: Only 16 and 32 bit floating point and 8, 16 and 32 bit integer types may be used as input data type
BNNSScatter: Only 16 and 32 bit floating point and 8, 16 and 32 bit integer types may be used as output data type
BNNSScatter: indices must be a 1d array or have the same shape as input
BNNSScatter: input and output tensors must have the same number of dimensions
BNNSScatter: input.size[%zu]=%zu must have same size as indices.size[0]=%zu if indices is 1D
BNNSScatter: input.size[%zu]=%zu must have same size as indices.size[%zu]=%zu
BNNSScatter: input.size[%zu]=%zu must have same size as output.size[%zu]=%zu
BNNSGatherND: Only 8, 16, 32 and 64 bit integer types may be used as indices data type
BNNSGatherND: Only 16 and 32 bit floating point and 8, 16 and 32 bit integer types may be used as input data type
BNNSGatherND: Only 16 and 32 bit floating point and 8, 16 and 32 bit integer types may be used as output data type
BNNSGatherND: indices.size[%zu]=%zu must be less than or equal to input.ndim=%zu
BNNSScatterND: Only 8, 16, 32 and 64 bit integer types may be used as indices data type
BNNSScatterND: Only 16 and 32 bit floating point and 8, 16 and 32 bit integer types may be used as input data type
BNNSScatterND: Only 16 and 32 bit floating point and 8, 16 and 32 bit integer types may be used as output data type
BNNSScatterND: indices.size[%zu]=%zu must be less than or equal to output.ndim=%zu
internal scatter copy
dict-item
o-item
BNNSGatherND: output tensor (shape [%s]) must have shape matching first n-1 dimensions of indices + shape of any dimensions of input not specified in final dimension of indices (i.e. [%s]).
BNNSScatterND: input tensor (shape [%s]) must have shape matching first n-1 dimensions of indices + shape of any dimensions of output not specified in final dimension of indices (i.e. [%s]).
internal gatherND copy
src-item
dest-item
internal scatterND copy
 = reshape(from=
.... %s refused on principle
BNNS Graph Compile: Allocation failed in construction of maxpool op
 = max_pool(
, kernel_size=
, kernel_stride=
, pad=
BNNS Graph Compile: Allocation failed in construction of avgpool op
 = avg_pool(
BNNS Graph Compile: Allocation failed in construction of l2pool op
 = l2_pool(
 = shape(
shape only supports int32 outputs
BNNS_pruned
bnns_fc_sparse_opaque(
, for=
MIL BNNS Matvec Repack Data: couldn't sparsify weights
repacked(
matvec op does not support dynamic shaped inputs
matvec op detected unexpected tensor shapes
matvec op requires final dimension to have stride 1
matvec op detected incompatible tensor shape for bias
matvec op requires input types to be FP16 or FP32
matvec op requires output type to be FP16 or FP32
matvec(A=
, x=
, transpose=
pre_bias(
internal.add
transposed(
matvec_concat_matrix(
matvec_concat_bias(
matvec_concat_output(
matvec_output_slice.
BNNS SPECIALIZED CONVOLUTION Create: failed to alloacted memory
BNNS Permute Filter: layer_params is NULL
BNNS Permute Filter: illegal input descriptor
BNNS Permute Filter: illegal output descriptor
BNNS Permute Filter: input and output descriptor layout dimension does not match. input dimm = %zu output dimm = %zu
BNNS Permute Filter: permutation array index %zu has illegal value of %zu
BNNS Permute Filter: permutation array is missing axis %zu
BNNS Permute Filter: permutation array axis %zu appears %zu times, each axis should appear exactly once
0123456789abcdefghijklmnopqrstuvwxyz
 -> out_
BNNS Shuffle: input and output data type do not match
BNNS Shuffle: input or output data is NULL
BNNS Shuffle: input and output dimensions should be 4
BNNS Shuffle: input and output sizes are not valid
BNNS Shuffle: input and output sizes do not match
BNNS Pooling Filter: filter is NULL
BNNS Pooling Filter: input is NULL
BNNS Pooling Filter: output is NULL
BNNS Pooling: input batch stride doesn't make sense (%zu < %zu x %zu)
BNNS Pooling: dilation supported only in BNNSPoolingFunctionMax/UnMax
BNNS Pooling UnMax: indices array is NULL
BNNS Pooling: case not implemented
BNNS Pooling Apply: allocation of work buffer failed
BNNS Pooling Backward: cannot run backward without output delta
BNNS Pooling Backward: only float32 bfloat16 output delta are supported
BNNS Pooling Backward: only float32 bfloat16 input delta are supported
BNNS Pooling Backward: only float32 bfloat16 bias delta are supported
BNNS Pooling Backward: unsupported pooling function
BNNS Pooling Backward: invalid argument
BNNS Pooling Backward: Fusion of compute with activation is unsupported for given activation function
BNNS Pooling Backward: failed to allocate memory
BNNS Pooling Backward: failed to apply activation backward
BNNS Pooling: layer parameters is NULL
BNNS Pooling: input must be a 3D array
BNNS Pooling: input descriptor is illegal
BNNS Pooling: output must be a 3D array
BNNS Pooling: output descriptor is illegal
BNNS Pooling: input/output channel counts do not match
BNNS Pooling: input/output types do not match
BNNS Pooling: invalid kernel dimensions, should be greater than 0
BNNS Pooling: dilation only supported for BNNSPoolingFunctionMax/UnMax
BNNS_POOLING: input size computation wraparound input (%zu x %zu), output (%zu x %zu), stride (%zu x %zu), padding (left %zu right %zu up %zu down %zu)
BNNS_POOLING: input size computation wraparound input (%zu x %zu), output (%zu x %zu), stride (%zu x %zu), padding (%zu x %zu)
BNNS_POOLING: output(%zu x %zu) is larger than input(%zu x %zu) with padding(left %zu right %zu up %zu down %zu).
 stride (%zu x %zu)
BNNS_POOLING: output(%zu x %zu) is larger than input(%zu x %zu) with padding(%zu x %zu).
 stride (%zu x %zu)
BNNS_POOLING: invalid pooling function
BNNS_POOLING: supported input/output data types: float32 float16 bfloat16
BNNS_POOLING: slow path: stride not in {1,2}
BNNS_POOLING: slow path: kernel size not in {2,3,4}
BNNS Pooling: Internal Memory size %zu doesn't match expected %d
BNNS Pooling: claiming pre allocated memory failed
BNNS Pooling: failed to allocate context
BNNS Pooling Backward: only float32 bfloat16 delta are supported
BNNS Pooling Backward: wrong pooling function called
 = stack(values=[
KERNEL_ARITHMETIC_UNARY
KERNEL_ARITHMETIC_BINARY
KERNEL_ARITHMETIC_BINARY_DYNAMIC
KERNEL_ARITHMETIC_BINARY_AMX2
KERNEL_COPY_GENERIC
KERNEL_COPY_AMX1
KERNEL_COPY_AMX2
KERNEL_COPY_AMX3
KERNEL_COPY_DYNAMIC
KERNEL_COPY_PERMUTE_DYNAMIC
KERNEL_COPY_SLICE_BY_INDEX_DYNAMIC
KERNEL_COPY_SLICE_BY_SIZE_DYNAMIC
KERNEL_CAST_GENERIC
KERNEL_CAST_AMX1
KERNEL_CAST_AMX2
KERNEL_CAST_AMX3
KERNEL_CAST_DYNAMIC
KERNEL_TRANSPOSE2D_GENERIC
KERNEL_TRANSPOSE2D_GENERIC_DYNAMIC
KERNEL_CONV_CREATE_APPLY
KERNEL_CONV_CUSTOMIZED_VOICE_ISOLATION_FP32_S1x1_O12x1x16N_I12x1xM_K1x1_P0x0x0x0_GK
KERNEL_CONV_VERSION_2
KERNEL_CONV_FC
KERNEL_CONV_AMX_SPECIAL
KERNEL_CONV_AMX1
KERNEL_CONV_WINOGRAD
KERNEL_CONV_SPECIAL
KERNEL_CONV_LOW_MEM
KERNEL_CONV_VERSION_2_DYNAMIC
KERNEL_CONV_TRANS_EXPAND_INPUT
KERNEL_CONV_TRANS_VECTOR
KERNEL_ELEMENTWISE_ACTIVATION
KERNEL_ELEMENTWISE_ACTIVATION_DYNAMIC
KERNEL_BATCHNORM
KERNEL_INSTANCENORM
KERNEL_LAYERNORM
KERNEL_REDUCE
KERNEL_MATVEC_GENERIC
KERNEL_MATVEC_WAVERNN_0
KERNEL_SPARSE_MATVEC_GENERIC
KERNEL_SPARSE_MATVEC_WAVERNN_0
KERNEL_SPARSE_MATVEC_WAVERNN_1
KERNEL_DOT_PRODUCT_GENERIC
KERNEL_MATMUL_INTEL64
KERNEL_MATMUL_NEON
KERNEL_MATMUL_AMX1
KERNEL_MATMUL_AMX2
KERNEL_MATMUL_AMX3
KERNEL_PAD_GENERIC
KERNEL_PAD_DYNAMIC_GENERIC
KERNEL_TOPK
KERNEL_GUMBELMAX
KERNEL_GRU_FUSED
KERNEL_GATHER
KERNEL_SCATTER
KERNEL_RELATIONAL
KERNEL_RESIZE
KERNEL_SOFTMAX
KERNEL_TILE_GENERIC
KERNEL_RNN
KERNEL_LSTM
KERNEL_GRU
KERNEL_MISC_SHAPE
KERNEL_MAXPOOL
KERNEL_AVGPOOL
KERNEL_L2POOL
KERNEL_SHUFFLE_GENERIC
UNKOWN_KERNEL(
=== IR dump from memory at %p ===
*** HEADER ***
  .ir_version = %s
  .total_file_size = %llu bytes
  .data_section_offset = %llu
  .debug_info_offset = %llu
  .workspace_size = %llu bytes
  .has_dynamic_sizes = %c
  .attributes = {
*** OUTPUTS ***
'%s' -> 0x%llx
*** INPUTS ***
*** OPERATIONS ***
op%5zu %s: {
%s0x%llx
} -> {
} param_offset=%llu param_size=%llu
*** TENSORS ***
0x%llx:
 workspace_offset=%llu
 view_of=0x%llx offset=%llu
 constant_offset=%llu
 user_input
 user_output
 type=%s, shape=[
 %llu
 rank=%u
=== END OF DUMP ===
strict digraph {
%s [label = "%s", shape="box"%s];
tensor_
constant_
%s [label = "%s %s", style=filled, color=lightgray];
%s -> %s;
input_
%s [label = "%s"];
%s -> %s [label = "%s  %s" ];
%s -> %s [label = "%s %s" ];
output_
arm64
amx1
unknown(
,style=filled,color=".7 .3 1.0"
BNNS Reduction Create: Invalid reduction function %u
BNNS Reduction Create: weight data type does not match input
BNNS Reduction Create: i_desc and o_desc must have the same number of dimensions (%zu vs %zu)
BNNS Reduction Create: i_desc and w_desc must have the same number of dimensions (%zu vs %zu)
BNNS Reduction Create: i_desc and w_desc dimensions must have the same size (dimension %zu: %zu vs %zu)
BNNS Reduction Create: i_desc and o_desc dimensions must have the same size, or o_desc.size[d] must be 1 to indicate reduction (dimension %zu: %zu vs %zu)
BNNS Reduction Create: ArgMin/ArgMax reduction is only supported on a single axis
BNNS Reduction Apply: Both input and output must non-NULL
BNNS Reduction Backward: input_delta must have the same number of dimensions as i_desc
BNNS Reduction Backward: input_delta must have the same shape as i_desc
BNNS Reduction Backward: output_delta must have the same number of dimensions as o_desc
BNNS Reduction Backward: output_delta must have the same shape as o_desc
BNNS Reduction Backward: output_delta must not be NULL
BNNS Reduction Backward: weights_delta must have the same number of dimensions as w_desc
BNNS Reduction Backward: weights_delta must have the same shape as w_desc
BNNS Reduction Direct Apply: Size of the dimensions of the output can not be greater than the size of the input
BNNS Reduction Direct Apply: ArgMin/ArgMax reduction is only supported on a single axis
BNNS Reduction: Unsupported data type combination for %u
BNNS Reduce Apply Generic: Size mismatch in dimension %zu: input %zu, output %zu
Backward usage is not supported for reduction type %u
BNNS Reduction Backward: Reduce function %u requires input to be supplied for backward computation
BNNS Reduction Backward: Reduce function %u requires output to be supplied for backward computation
BNNS Reduction Backward: Calculation of weights_delta requires input to be supplied
Size mismatch in dimension %zu: input %zu, output %zu
BNNS Arithmetic: Cannot find binary kernel
BNNS Arithmetic: Attempting to use amx2 kernel on non-amx hardware
BNNS Sparse Fully Connected: out of scratch memory to encode
BNNS Fully Conneceted Sparsify: Sparse data is greater than dense prune, developer should use dense prune
BNNS Sparse Fully Connected: failed to apply activation
: BNNS Graph random_categorical generate_ir() not yet implemented
BNNS Arithmetic Filter Internal: unsupported arithmetic with epsilon function
BNNS Arithmetic Filter: layer_params is NULL
BNNS Arithmetic Filter: unsupported arithmetic function
BNNS Arithmetic Filter: memory allocation failed
BNNS Arithmetic Filter: Failed to init filter
BNNS Arithmetic Filter: filter is NULL
BNNS Arithmetic Filter: wrong filter type, this function should only be used for a filter created with BNNSFilterCreateLayerArithmetic
BNNS Arithmetic Filter: in pointer is NULL
BNNS Arithmetic Filter: batch_size>1 and in_stride is 0
BNNS Arithmetic Filter Internal: expected number of inputs call failed
BNNS Arithmetic Filter: wrong number_of_inputs, number_of_inputs=%zu, expecting %zu
BNNS Arithmetic Filter: in[%zu] is NULL
BNNS Arithmetic Filter: in_stride[%zu] is 0
BNNS Arithmetic Filter: output pointer is NULL
BNNS Arithmetic Filter: batch_size>1 and out_stride is 0
BNNS Arithmetic Filter: only BNNSDataTypeFloat32/BFloat16/Float16 is supported
BNNS Arithmetic Filter: only BNNSDataTypeFloat32/BNNSDataTypeBFloat16 is supported
BNNS Arithmetic Filter Internal: unsupported arithmetic function
BNNS Arithmetic Filter: out delta is not valid
BNNS Arithmetic Filter: in_delta is not valid
BNNS Arithmetic Filter: in is NULL, but it is required for backward compute
BNNS Arithmetic Filter: in_delta descriptor %zu is not valid
BNNS Arithmetic Filter: cannot compute activation backward, output is NULL
BNNS Arithmetic Filter: Fusion of arithmetic with activation is unsupported for given activation function
BNNS Arithmetic Filter Backward Internal: unsupported arithmetic function
BNNS Arithmetic Filter Backward: failed to apply activation backward
BNNS Arithmetic Filter: in_delta[0] must be NULL for BNNSArithmeticSelect
BNNS Arithmetic Filter: arithmetic_function_fields is NULL
BNNS Arithmetic Filter: malloc failed
BNNS Arithmetic Filter: unsupported out_type/in_type
BNNS Arithmetic Filter: in, out descriptor check failed
BNNS Arithmetic Filter: in1,in2,out descriptor check failed
BNNS Arithmetic Filter: in1,in2,in3,out descriptor check failed
BNNS Arithmetic: Activation creation failed
BNNS Arithmetic Filter: input size[%zu]=%zu does not equal max input,output size of %zu or 1
BNNS Arithmetic Filter: input%zu and input%zu have the same data pointer, but descriptors are different
BNNS Arithmetic Filter: input%zu and input%zu have the same data pointer, but different BNNSDescriptorType
BNNS Arithmetic Filter: descriptors for input that is being processed in-place and output must exactly match
BNNS Arithmetic Filter: input is being processed in-place but input and output descriptor types do not match
BNNS Arithmetic Filter: output dimension smaller than input is not supported. batch size>1, input type is BNNSSample but out_type is not BNNSSample
BNNS Arithmetic Filter: backward compute for chosen arithmetic function does not allow inplace gradient. out delta data must not be the same as input delta data
BNNS Arithmetic Filter: in_delta%zu is NULL but in%zu type is not BNNSConstant
BNNS Arithmetic Filter: in_delta%zu->data is NULL but in%zu type is not BNNSConstant
BNNS Arithmetic Filter: in%zu type is BNNSSample but in_delta_stride[%zu] is 0
BNNS Arithmetic Filter: arithmetic input_delta and input descriptors must have the same sizes, strides and data types
BNNS Arithmetic Filter: input%zu delta data pointer is not NULL. cannot compute gradient for BNNSConstant
BNNS Arithmetic Filter: arithmetic output_delta and output descriptors must have the same sizes, strides and data types
BNNS Arithmetic Filter: input%zu delta and input%zu delta has the same data pointer, but descriptors are different
BNNS Arithmetic Filter: input%zu delta and input%zu delta has the same data pointer, but different data types
BNNS Arithmetic Filter: input%zu delta and input%zu delta has the same data pointer, but different in_delta_stride
BNNS Arithmetic Filter: forward input pointer points to the same array but input gradient pointers point to different arrays
BNNS Arithmetic Filter: descriptors for out_delta that is being processed in-place and in1_delta must exactly match
BNNS Arithmetic Filter: input1 delta is being processed in-place but input type and output type are different
BNNS Arithmetic Filter: input1 delta and output delta has the same data pointer (inplace), but different in_delta_stride
BNNS Arithmetic Filter: input1 delta and output delta has the same data pointer (inplace), but in1_delta.flags indicates to accumulate result
BNNS Arithmetic Filter: output dimension smaller than input is not supported. batch size>1, an input type is BNNSSample but out_type is not BNNSSample
 = topk (
topk does not support dynamic sizes
topk from
BNNS Crop Resize: data types do not match
BNNS Crop Resize: dimension of input, roi, or output is not correct
BNNS Crop Resize: size of roi is not [N, 4] or [N, 5]
BNNS Crop Resize: size of output is not [N, B, C, OH, OW] or [N, 1, C, OH, OW]
BNNS Crop Resize: only linear method is supported now
BNNS Crop Resize: Unsupported data type
BNNS Crop Resize: sampling mode not supported
BNNS Embedding Create: Only 8, 16, 32 and 64 bit integer types may be used as input data type.
BNNS Embedding Create: Only 16 and 32 bit floating point and 8, 16 and 32 bit integer types may be used as dictionary data type.
BNNS Embedding Create: o_desc dimension must be dim(i_desc) + dim(dictionary) - 1.
BNNS Embedding Create: First dim(dictionary)-1 dimensions of o_desc and dictionary must have consistent shapes.
BNNS Embedding Create: Last dim(i_desc) dimensions of o_desc and i_desc must have consistent shapes.
internal embedding copy
internal embedding copy back
BNNS Embedding Apply: Input value %zu is out of range [0, %zu)
BNNS Embedding ApplyBackward: weights_delta must have the same shape as layer_params->dictionary
BNNS Embedding ApplyBackward: out_delta must have the same shape as layer_params->o_desc
BNNS Embedding ApplyBackward: Input value %zu out of range
BNNS Sparse Embedding Create: Only 8, 16, 32 and 64 bit integer types may be used as input data type.
BNNS Sparse Embedding Create: Only 16 and 32 bit floating point and 8, 16 and 32 bit integer types may be used as dictionary data type.
BNNS Sparse Embedding Create: o_desc dimension must be dim(i_desc) + dim(dictionary) - 1.
BNNS Sparse Embedding Create: First dim(dictionary)-1 dimensions of o_desc and dictionary must have consistent shapes.
BNNS Sparse Embedding Create: Last dim(i_desc) dimensions of o_desc and i_desc must have consistent shapes.
BNNS Sparse Embedding Create: Unsupported optimization function
BNNS Sparse Embedding Create: Dictionary must be of type BNNSDataTypeFloat32
BNNS Sparse Embedding Apply: Filter is no longer valid
BNNS Sparse Embedding Apply: Input value %zu is out of range [0, %zu)
BNNS Sparse Embedding Apply Backwards: Filter is no longer valid
BNNS Embedding Backwards Sparse: Accumulation of sparse weights gradient is not supported.
BNNS Embedding Backwards Sparse: out_delta must have the same shape as layer_params->o_desc
BNNS Embedding Backwards Sparse: dictionary_delta->indices must have a single dimension
BNNS Embedding Backwards Sparse: dictionary_delta->indices.size[0] must be 1
BNNS Embedding Backwards Sparse: dictionary_delta->sparse_dimension_size[0] must be num_embeddings
BNNS Embedding Backwards Sparse: dictionary_delta->values shape must match dictionary item shape
BNNS Embedding Backwards Sparse: For in-place indices, dictionary_delta->count must equal the total size of layer_params->i_desc * batch_size
BNNS Embedding Backwards Sparse: For in-place indices, input must be contiguous
BNNS Embedding Backwards Sparse: For in-place values, dictionary_delta->count must equal the total size of out_delta * batch_size
BNNS Embedding Backwards Sparse: For in-place values, out_delta must be contiguous in its indexing dimensions
BNNS Sparse Embedding Get Representation: Filter is no longer valid
BNNS Sparse Embedding Get Representation: Unexpected value for num_accumulators=%zu (expected %zu)
BNNS Sparse Embedding Get Representation: num_opt_field_changes!=NULL but opt_field_changes_indices is NULL
BNNS Sparse Embedding Get Representation: num_opt_field_changes!=NULL but opt_field_changes is NULL
BNNS Sparse Embedding Get Representation: num_opt_field_changes=NULL, but optimization fields of filter have previosuly been updated
BNNS Sparse Embedding Set Representation: Filter is no longer valid
BNNS Embedding Set Sparse Optimizer Context: Expected %zu accumulators
BNNS Embedding Set Sparse Optimizer Context: values[0]->indices must have dimension 1
BNNS Embedding Set Sparse Optimizer Context: values[0]->indices.size[0] must be 1
BNNS Embedding Set Sparse Optimizer Context: Expected values indices descriptors to be identical (that is point to the same data)
BNNS Embedding Set Sparse Optimizer Context: Expected values indices descriptors to be identical
dest
BNNS Sparse Embedding Optimizer Step: Filter is no longer valid
BNNS Embedding Sparse Optimizer Step: Expected dictionary_delta->indices to have a single dimension
BNNS Embedding Sparse Optimizer Step: Expected dictionary_delta->values to match shape of dictionary
BNNS Embedding Sparse Optimizer Step: Dictionary gradient items must be contiguous
BNNS Embedding Sparse Optimizer Step: Dictionary items must be contiguous
BNNS Embedding Sparse Optimizer Step: grad.data_type must be BNNSDataTypeFloat32
BNNS Sparse Embedding Get Dense: Filter is no longer valid
BNNS Sparse Embedding Get Dense: Unexpected value for num_accumulators=%zu (expected %zu)
BNNS Sparse Embedding Get Dense: Unexpected number of dimensions for accumulator[%zu]
BNNS Sparse Embedding Get Dense: Unsupported data type for accumulator[%zu]
BNNS Sparse Embedding Get Dense: Unexpected shape for accumulator[%zu]
BNNS Sparse Embedding Get Dense: accumulator[%zu] must be contiguous in dictionary item dimensions
BNNS batch norm forward: malloc for mean failed
BNNS batch norm forward: malloc for var failed
BNNS batch norm forward: malloc for isqrtvar failed
BNNS Batchnorm Backward Apply: delta allocation failed
BNNS Batchnorm Backward Apply: activation_grad allocation failed
BNNS instance norm backward: malloc for dbeta failed
BNNS instance norm backward: malloc for dgamma failed
BNNS Layer Norm Apply Backward: malloc activation_grad failed
BNNS Layer Norm Apply Backward: dx hat allocation failed
BNNS Group Norm Apply Backward: failed to allocate memory
BNNS Group Norm Apply Backward: failed to allocate activation grad memory
BNNSActivationFunctionIntegerLinearSaturate isn't supported in apply_bias_and_activation
BNNSActivationFunctionIntegerLinearSaturatePerChannel isn't supported in apply_bias_and_activation
BNNSActivationFunctionIntegerLinearSaturate isn't supported in apply_bias_and_activation_fp16
BNNSActivationFunctionIntegerLinearSaturatePerChannel isn't supported in apply_bias_and_activation_fp16
BNNSActivationFunctionIntegerLinearSaturate isn't supported in apply_bias_and_activation_bf16
BNNSActivationFunctionIntegerLinearSaturatePerChannel isn't supported in apply_bias_and_activation_bf16
allocation failed, size=%zu
reallocation failed, size=%zu
allocation failed, size=%zu, align=%zu
BNNS: unexpected data type, failing
BNNS: Data type not supprted, fail
BNNS: layout not supported
BNNS: active dimension must be greater than 0
BNNS: dimension %zu stride %zu is lower then previous dimension actual size %zu * %zu (size*stride)
BNNS: Opaque data structure doesn't match descriptor information
BNNS: Opaque data structure has incorrect information
BNNS Convolution: Attempting to use amx_special kernel on non-amx hardware
Unsupported data type for execute_conv_manipulate_input_op
Conv repack for amx general case not supported on this arch
BNNS Convolution: Repacking to CONV_WEIGHT_REPACK_AMX1 on non-AMX hardware is not yet supported
BNNS Convolution: Repacking to CONV_WEIGHT_REPACK_AMX_GENERAL_* on non-AMX hardware is not yet supported
BNNS Convolution: Unrecognoized repack format
BNNS Create Layer Convolution: failed to create forward convolution filter
BNNS Create Layer Convolution: failed to create wrapper filter
BNNS Create Layer Convolution: failed to allocate memory to copy weights and bias
BNNS Convolution Create: failed to allocate weights buffer
BNNS Convolution Create: failed to allocate bias buffer
BNNS Convolution Create: failed to allocate memory
BNNS Convolution Create: failed to allocate weights descriptor
BNNS Convolution Create: convert weights
BNNS Convolution Create: failed to prepare blocks
BNNS Convolution Create: failed to create compute blocks
BNNS: memory usage exceeded capacity
BNNS PreAllocated Memory: memory usage exceeded capacity
BNNS PreAllocated Memory: failed to claim scratch memory
BNNS Affine Resize: layer parameter is not supported
BNNS Affine Resize: input data type or size is not supported
BNNS Affine Resize: affine data type or size is not supported
BNNS Affine Resize: output data type or size is not supported
BNNS Create Convolution Winograd: malloc failed
BNNS Convolutions Winograd: weights allocation failed
BNNS Convolutions Winograd: bias allocation failed
BNNS Apply Convolution Winograd: memory allocation failed
weight packing must be at least 32 and a power of 2
conv op only supports fp16 and fp32 operands
pad(of:
bnns_internal.pad
conv_trans_manipulate_input(of:
bnns_internal.conv_transpose_manipulate_input
conv_transpose_manipulate_input
internal.slice
slice(of:
conv_input_slice(
 = conv(x=
, weights=
, strides=[
], pad=[
], dilation=[
], groups=
UNKNOWN
OPAQUE
STRING
internal.nop
operation does not support dynamic shaped input
operation only supports fp16 and fp32 operands
softmax(x=
, axis=
BNNS Graph Compile: Allocation failed in construction of softmax op
 not supported.
UPSAMPLE_NEAREST_NEIGHBOR
UPSAMPLE_BILINEAR
RESIZE_NEAREST_NEIGHBOR
RESIZE_BILINEAR
, target_size_height = 
, target_size_width = 
scatter does not support dynamic shaped input
scatter only supports fp16, fp32 types for input and updates, and int32 type for indices
scatter only supports fp16 and fp32 outputs
Rejected constexpr elimination of op 
 as it increased data footprint by 
 bytes
Constexpr elimination of op 
 would decrease weight footprint, but is not yet supported
Unsupported repack operation from layout=%u to layout=%u
BNNSCopy: Unsupported repack operation from layout=%u to layout=%u
BNNSCopy: src->size[%zu]=%zu is not equal to dest->size[%zu]=%zu
BNNSCopy: Cannot perform type conversion as part of opaque repack
temp_desc
BNNS Graph Compile: Allocation failed in construction of gumbelmax op
 = gumbelmax(input=
persistent_memory(for:
gumbelmax
internal.gumbelmax
copy(
, fp32)
internal.copy
, fp16)
matmul op does not support dynamic shaped inputs
matmul op detected incompatible tensor shapes
matmul op detected unexpected output shape
matmul op requires final dimension to have stride 1
matmul op detected incompatible tensor shape for bias
matmul op requires input types to be FP16 or FP32
matmul op requires output type to be FP16 or FP32
matmul(x=
, y=
matmul_concat_weight(
mtmul_concat_bias(
matmul_concat_output(
matmul_output_slice.
Unsupported minimum compute type
 %.6f
-- slice 
,:,: --
BNNS: Data type not supported, fail
    first big diff at (
 %zu
) observed = %e [offset=%zu] expected = %e [offset=%zu]
BNNS Graph Initialize Persistent Memory: Unsupported kernel id %u
dot_product only supports fp16 and fp32 operands
dot_product(
hw.cpufamily
hw.cpusubfamily
hw.physicalcpu
hw.perflevel0.l2cachesize
hw.perflevel1.l2cachesize
pad op only supports FP16 and FP32 operands
pad op only supports rank <= 4
pad op doesn't support mode="replicate"
pad op doesn't support mode="reflect"
 = pad(x=
, pad=[
], mode=
BNNS Convolution ARM64: only stride 1x1 and 2x2 are supported
 = shuffle(
BNNSShuffle: shuffle operator not supported.
 = slice_by_index(x=
, begin=[
], end=[
], stride=[
slice only supports fp16, fp32 and int32 operands
slice requires input and output types to match
BNNS Graph Shape Deduction: Unsupported kernel id %u
BNNS Normalization Apply Backward: input gradient data type not supported
BNNS Normalization Apply Backward: malloc activation_scratch failed
BNNS instance norm backward: malloc for dbeta and dgamma failed
' is not a tensor
': variadic tensors are not supported by mil2bnns
FlexibleShapeInformation
BNNS: Failed to parse FlexibleShapeInformation, expected tuple
BNNS: Failed to parse FlexibleShapeInformation, expected tuple<tuple>
BNNS: Failed to parse FlexibleShapeInformation, expected type info tuple to have exactly two entries
BNNS: Failed to parse FlexibleShapeInformation, expected first element of type info tuple to be a string
BNNS: Failed to parse FlexibleShapeInformation, expected second element of type info tuple to be a dictionary
BNNS: Failed to parse FlexibleShapeInformation, expected dictionary key to be of type string
BNNS: Failed to parse FlexibleShapeInformation, expected default shape to be vector of int32
BNNS: Failed to find matching default size to input for FlexibleShapeInformation provided for '%s'
BNNS: input tensor '%s' and its FlexibleShapeInformation default size have different ranks
BNNS: Failed to parse FlexibleShapeInformation, default shapes with negative values are not supported
BNNS: Failed to parse FlexibleShapeInformation, expected range values to be supplied as list
BNNS: Failed to find matching range dims to input for FlexibleShapeInformation provided for '%s'
BNNS: input tensor '%s' and its FlexibleShapeInformation range dims have different ranks
BNNS: Failed to parse FlexibleShapeInformation, expected range values to be supplied as list of int32 tensors
BNNS: FlexibleShapeInformation of type EnumeratedShapes is currently ignored
BNNS: Failed to parse FlexibleShapeInformation, unexpected information type
BNNS Graph Compile: Attempted to parse unsupported op '%s' (reversed name '%s')
BNNS Convolution Apply: input batch stride doesn't make sense (%zu < %zu x %zu)
BNNS Convolution Apply: output batch stride doesn't make sense (%zu < %zu x %zu)
BNNS Convolution Apply: failed to allocate weights in apply
BNNS Convolution Backward: BNNSNDArrayFlagBackpropAccumulate has not yet been implemented for weights_delta or bias_delta.
BNNS Convolution Backward: cannot run backward without output delta
BNNS Convolution Backward: [out delta/in delta/weight delta/bias delta] - data type check failed. unsupported data type
BNNS Convolution Backward: internal error, incorrect wrapper
BNNS Convolution forward output data type does not match output delta data type
BNNS Convolution forward input data type does not match input delta data type
BNNS Convolution forward weights data type does not match weights delta data type
BNNS Convolution weight packing is not supported backward
BNNS Convolution Backward: BNNSFlagsUseClientPtr must be enabled during training
BNNS Convolution Backward: invalid argument
BNNS Convolution Backward: Fusion of compute with activation is unsupported for given activation function
BNNS Convolution Backward: failed to allocate memory
BNNS Convolution Backward: failed to apply activation backward
BNNS Conv: Unsupported weight format for Input delta compute
BNNS Convolution Apply Backward: could not create transposed convolution filter
BNNS Convolution Apply Backward: Convolution input delta computation failed
BNNS Transposed Convlution Apply: failed to allocate weights buffer
BNNS Transposed Convlution Apply: failed to allocate backward weights buffer
BNNS Transposed Convlution Apply: failed to allocate out delta buffer
BNNS Transposed Convlution Apply: failed to manipulate output delta
BNNS Transposed Convlution Apply: could not create convolution filter
BNNS Convolution Apply Backward: allocation failed
BNNS Convolution Apply Backward: Transposed Convolution Backward Compute failed
transposed convolution parameter error
transposed convolution backward failed
Convolution bias delta failed
BNNS Create Layer Convolution: fused padd convolution allocation failed
BNNS Create Layer Convolution: padding layer creation failed
BNNS Create Layer Convolution: padding descriptor creation failed
BNNS Create Layer Convolution: input channels must be divisible by groups
BNNS Create Layer Convolution: output channels must be divisible by groups
BNNS Create Layer Convolution: bias must match output channels
BNNS Create Layer Convolution: grouped convolution weight layout must be BNNSDataLayoutConvolutionWeightsOIHW
BNNS Convolution Create: incompatible numbers of channels between images and convolution parameters
BNNS Convolution Create: unsupported weight format
BNNS Convolution Create: input must be a 3D array
BNNS Convolution Create: input descriptor is illegal
BNNS Conv: output must be a 3D array
BNNS Convolution Create: output descriptor is illegal
BNNS Convolution Create:  weights descriptor is illegal
BNNS Convolution Create: failed to create dilated convolution
BNNS Convolution Create: failed to create packed weights convolution
BNNS Convolution Create: failed to create grouped convolution
BNNS Grouped Convolution apply: groups should not be used if groups <= 1
BNNS Grouped Convolution apply: weights are null 
BNNS Grouped Convolution apply: failed to create context
BNNS Grouped Convolution apply: unsupported convolution filter type 
BNNS Convolution Apply Backward:  filter is NULL
BNNS Convolution Apply Backward: out_delta is NULL
BNNS Convolution weight delta: malloc failed
BNNS Convolutions Apply: unexpected output data type
BNNS Convolution Backward: out_delta descriptor memory layout is not contiguous
BNNS Convolution bias delta: malloc failed
BNNS Convolution bias delta: upconverting tensors failed
BNNS Convolution: malloc activation_grad failed
Convolution backward: in_delta, weights_delta, bias_delta descriptor pointers are all NULL
BNNS Convolution Backward: in_delta descriptor memory layout is not contiguous
BNNS Convolution Backward: weights_delta descriptor memory layout is not contiguous
BNNS Convolution Backward: bias_delta descriptor memory layout is not contiguous
BNNS Convolution Backward: out_stride and out_delta_stride do not match
BNNS Convolution Backward: out_delta_stride [%zu] does not match out_delta descriptor memory size [%zu]
BNNS Convolution Backward: in_stride and in_delta_stride do not match
BNNS Convolution Backward: in_delta_stride [%zu] does not match in_delta descriptor memory size [%zu]
BNNS gradient computation for Transposed Convolution with groups is not supported
addend
_repacked
rsqrt(
 + epsilon)
rsqrtvar
 = batch_norm(input=
, mean=
, addend=
, rsqrtvar=
, variance=
, gamma=
, eps=
 = instance_norm(input=
 = layer_norm(input=
, axes=
 = channel_norm(input=
bessel_correction_factor
x_minus_mean_squared
channel_norm
internal.channel_norm
BNNS: unsupported resize forward data type
BNNS Resize: input_delta has %zu dimensions but must match input that has %zu
BNNS Resize: output_delta has %zu dimensions but must match output that has %zu
BNNS Resize: input_delta size[%zu]=%zu but must match input size[%zu]=%zu
BNNS Resize: output_delta size[%zu]=%zu but must match output size[%zu]=%zu
BNNS: unsupported resize backward data type
BNNS Resize: Unsupported data type
BNNS Resize: Input and output have different number of dimensions
BNNS Resize: resize must be in same direction for all direction (request %d downsample and %d upsample)
BNNS Resize: Linear interpolation requires resize in at most two dimensions, but %d dimensions are resized
BNNS Resize: Unsupported interpolation method %d
Downsampling not supported for dimension > 2.
BNNS Resize: downsampling has no support for 3 or more dimensions.
BNNS Resize: downsampling has no support for 3 or higher dimension.
BNNS CONVOLUTIONS CUSTOMIZED: malloc failed
BNNS CONVOLUTIONS CUSTOMIZED: malloc failed
BNNS CONVOLUTIONS CUSTOMIZED: failed to allocate bias memory
BNNS CONVOLUTIONS CUSTOMIZED: failed to allocate input repack memory
%zu: %s
execute_op
%{name=layer}s %{name=direction}s
BNNSDirectApply
Filter
Pooling
%{name=direction}s
Quantizer
ClipByValue
ClipByGlobalNorm
ComputeNorm
SparseEmbedding
SparseEmbeddingOptimzierStep
RandomFill
MultiheadAttention
Loss
Arithmetic
Permute
Normalization
FusedFilter
BNNS Signposts Enabled=%c
C$|Y
P/'
D'|E
