N4bnns14graph_compiler2op17binary_arithmeticE
N4bnns14graph_compiler2op16unary_arithmeticE
a_*ik, b_*kj -> c_*ij
INTERNAL: bwd
^9giP9giP9giP9giP9
N4bnns14graph_compiler2op6concatE
NSt3__110__function6__funcIZNK4bnns14graph_compiler2op6concat15local_optimizerEvE3$_0NS_9allocatorIS6_EEFbRNS3_7ContextEEEE
NSt3__110__function6__baseIFbRN4bnns14graph_compiler7ContextEEEE
ZNK4bnns14graph_compiler2op6concat15local_optimizerEvE3$_0
NSt3__110__function6__funcIZNK4bnns14graph_compiler8Function11generate_irEvE3$_0NS_9allocatorIS5_EEFiPKNS3_9OperationEEEE
NSt3__110__function6__baseIFiPKN4bnns14graph_compiler9OperationEEEE
ZNK4bnns14graph_compiler8Function11generate_irEvE3$_0
N4bnns14graph_compiler2op4tileE
N4bnns20GraphOptionsInternalE
N4bnns14graph_compiler6TensorE
N4bnns14graph_compiler13TensorLiteralE
N4bnns14graph_compiler23TensorLiteralPalletizedE
N4bnns14graph_compiler19TensorStringLiteralE
N4bnns14graph_compiler10TensorViewE
NSt3__120__shared_ptr_pointerIPhNS_10shared_ptrIA_hE27__shared_ptr_default_deleteIS3_hEENS_9allocatorIhEEEE
NSt3__110shared_ptrIA_hE27__shared_ptr_default_deleteIS1_hEE
NSt3__110__function6__funcIZN4bnns14graph_compiler8Function6layoutEbNS2_12layout_alg_tEE3$_1NS_9allocatorIS6_EEFiPNS3_9OperationEEEE
NSt3__110__function6__baseIFiPN4bnns14graph_compiler9OperationEEEE
ZN4bnns14graph_compiler8Function6layoutEbNS_12layout_alg_tEE3$_1
NSt3__110__function6__funcIZN4bnns14graph_compiler8Function6layoutEbNS2_12layout_alg_tEE3$_2NS_9allocatorIS6_EEFiPNS3_9OperationEEEE
ZN4bnns14graph_compiler8Function6layoutEbNS_12layout_alg_tEE3$_2
N4bnns14graph_compiler2op22elementwise_activationE
NSt3__110__function6__funcIZN4bnns14graph_compiler15fusion_opt_passERNS3_7ProgramEE3$_0NS_9allocatorIS6_EEFiPNS3_9OperationEEEE
ZN4bnns14graph_compiler15fusion_opt_passERNS0_7ProgramEE3$_0
NSt3__117bad_function_callE
N4bnns14graph_compiler7ViewMapE
N4bnns14graph_compiler13DirectViewMapE
N4bnns14graph_compiler13ExpandViewMapE
N4bnns14graph_compiler14SqueezeViewMapE
N4bnns14graph_compiler12SliceViewMapE
N4bnns12GraphOptionsE
_h2m
????????????????
?N4bnns2IR3gen16TensorFirstMajorE
N4bnns2IR3gen6TensorE
N4bnns2IR3gen20TensorViewFirstMajorE
N4bnns14graph_compiler2op7permuteE
?fff
N4bnns14graph_compiler7ContextE
N4bnns14graph_compiler7ProgramE
NSt3__110__function6__funcIZNK4bnns14graph_compiler7Context8validateEvE3$_0NS_9allocatorIS5_EEFiPKNS3_9OperationEEEE
ZNK4bnns14graph_compiler7Context8validateEvE3$_0
NSt3__110__function6__funcIZN4bnns14graph_compiler7Context11repack_dataEvE3$_1NS_9allocatorIS5_EEFiPNS3_9OperationEEEE
ZN4bnns14graph_compiler7Context11repack_dataEvE3$_1
NSt3__110__function6__funcIZNK4bnns14graph_compiler8Function18reachable_literalsEvE3$_3NS_9allocatorIS5_EEFiPKNS3_9OperationEEEE
ZNK4bnns14graph_compiler8Function18reachable_literalsEvE3$_3
NSt3__110__function6__funcIZNK4bnns14graph_compiler8Function28reachable_underlying_tensorsEvE3$_4NS_9allocatorIS5_EEFiPKNS3_9OperationEEEE
ZNK4bnns14graph_compiler8Function28reachable_underlying_tensorsEvE3$_4
N4bnns14graph_compiler2op4copyE
NSt3__110__function6__funcIZNK4bnns14graph_compiler2op4copy15local_optimizerEvE3$_0NS_9allocatorIS6_EEFbRNS3_7ContextEEEE
ZNK4bnns14graph_compiler2op4copy15local_optimizerEvE3$_0
N4bnns14graph_compiler2op7reshapeE
N4bnns14graph_compiler2op5stackE
NSt3__110__function6__funcIZNK4bnns14graph_compiler2op5stack15local_optimizerEvE3$_0NS_9allocatorIS6_EEFbRNS3_7ContextEEEE
ZNK4bnns14graph_compiler2op5stack15local_optimizerEvE3$_0
NSt3__110__function6__funcIZN4bnns5graphL33execute_typed_arithmetic_unary_opIDF16_EEiRKNS2_13GraphInternalEPKvPK34bnns_graph_arithmetic_unary_paramsEUlPDF16_PKDF16_mE_NS_9allocatorISG_EEFvSD_SF_mEEE
NSt3__110__function6__baseIFvPDF16_PKDF16_mEEE
ZN4bnns5graphL33execute_typed_arithmetic_unary_opIDF16_EEiRKNS_13GraphInternalEPKvPK34bnns_graph_arithmetic_unary_paramsEUlPDF16_PKDF16_mE_
NSt3__110__function6__funcIZN4bnns5graphL33execute_typed_arithmetic_unary_opIDF16_EEiRKNS2_13GraphInternalEPKvPK34bnns_graph_arithmetic_unary_paramsEUlPDF16_PKDF16_mE0_NS_9allocatorISG_EEFvSD_SF_mEEE
ZN4bnns5graphL33execute_typed_arithmetic_unary_opIDF16_EEiRKNS_13GraphInternalEPKvPK34bnns_graph_arithmetic_unary_paramsEUlPDF16_PKDF16_mE0_
NSt3__110__function6__funcIZN4bnns5graphL33execute_typed_arithmetic_unary_opIfEEiRKNS2_13GraphInternalEPKvPK34bnns_graph_arithmetic_unary_paramsEUlPfPKfmE_NS_9allocatorISG_EEFvSD_SF_mEEE
NSt3__110__function6__baseIFvPfPKfmEEE
ZN4bnns5graphL33execute_typed_arithmetic_unary_opIfEEiRKNS_13GraphInternalEPKvPK34bnns_graph_arithmetic_unary_paramsEUlPfPKfmE_
NSt3__110__function6__funcIZN4bnns5graphL33execute_typed_arithmetic_unary_opIfEEiRKNS2_13GraphInternalEPKvPK34bnns_graph_arithmetic_unary_paramsEUlPfPKfmE0_NS_9allocatorISG_EEFvSD_SF_mEEE
ZN4bnns5graphL33execute_typed_arithmetic_unary_opIfEEiRKNS_13GraphInternalEPKvPK34bnns_graph_arithmetic_unary_paramsEUlPfPKfmE0_
y4#@y4#@y4#@y4#@
r1?V
B6"9B6"9B6"9B6"9N
U?DH
!rr{>
!rr{>
}76R
YVpA
YVpA
YVpA
<7p7M}
<7p7M}
<7p7M}
:I~D
:I~D
:I~D
:I~D
?G/d?
*?9s
)>e\
?N4bnns14graph_compiler2op4convE
N4bnns14graph_compiler9OperationE
^9giP9
NSt3__110__function6__funcIZN4bnns14graph_compiler14local_opt_passERNS3_7ContextEE3$_0NS_9allocatorIS6_EEFiPNS3_9OperationEEEE
ZN4bnns14graph_compiler14local_opt_passERNS0_7ContextEE3$_0
N4bnns14graph_compiler2op5sliceE
NSt3__110__function6__funcIZNK4bnns14graph_compiler2op5slice15local_optimizerEvE3$_0NS_9allocatorIS6_EEFbRNS3_7ContextEEEE
ZNK4bnns14graph_compiler2op5slice15local_optimizerEvE3$_0
N4bnns14graph_compiler8FunctionE
floor_div
maximum
minimum
real_div
input0
input1
output
undefined
allocator<T>::allocate(size_t n) 'n' exceeds maximum supported size
map::at:  key not found
acos
acosh
asin
asinh
atan
atanh
ceil
cosh
exp2
floor
inverse
round
rsqrt
sinh
sign
sqrt
square
tanh
input
, epsilon=
BNNS Tile: descriptor is illegal
BNNS Tile: data type check failed
BNNS Tile: output tensor size is not a multiple of input
BNNS Tile: type %u not supported
Shouldn't call get_data_size with Indexed2 or Indexed4, switch to get_data_bits
BNNS Dropout: Error layer_params is NULL
BNNS Dropout: rate must be in the range [0.0, 1.0].
BNNS Dropout: Unsupported input data type
BNNS Dropout: Input and Output data types must be the same
BNNS Dropout: Error memory allocation failed
BNNS Dropout: Error input descriptor is invalid
BNNS Dropout: Error output descriptor is invalid
BNNS Dropout: Input and output descriptors must have the same shape.
BNNS Dropout: Tensors with dimension greater than %u are not supported.
BNNS Dropout: Error filter is NULL
BNNS Dropout: Error wrong filter type, filter is not Dropout
BNNS Dropout: input pointer is NULL
BNNS Dropout: batch_size>1 and in_stride is 0
BNNS Dropout: output pointer is NULL
BNNS Dropout: batch_size>1 and out_stride is 0
BNNS BatchNorm: Error filter is NULL
BNNS Dropout: input_delta layout, shape and stride must match layer_params->i_desc
BNNS Dropout: output_delta layout, shape and stride must match layer_params->o_desc
Unsupported target %d
v16@?0Q8
v24@?0Q8^v16
BNNS Acitvation Gumbel Max: noise buffer size (%zu) isn't power of 2 or smaller than 16
BNNS Tensor Contraction: inputB pointer must not be NULL unless B is a weight or the operation is quadratic.
BNNS Tensor Contraction: After adding batch dimension, inputA tensor has too many indices
BNNS Tensor Contraction: After adding batch dimension, inputB tensor has too many indices
BNNS Tensor Contraction: After adding batch dimension, output tensor has too many indices
BNNS Tensor Contraction: inB_delta calculation is non-sensical for a contraction of a tensor with itself.
BNNS Tensor Contraction: inA_delta calculation requires inA to be non-NULL
BNNS Tensor Contraction: inA_delta calculation requires out_delta (and out_delta->data) to be non-NULL
inA_delta
BNNS Tensor Contraction: After adding batch dimension, out_delta tensor has too many indices
BNNS Tensor Contraction: After adding batch dimension, inputA_delta tensor has too many indices
BNNS Tensor Contraction: inA_delta calculation requires inB to be non-NULL
BNNS Tensor Contraction: inB_delta calculation requires inA to be non-NULL
BNNS Tensor Contraction: inB_delta calculation requires out_delta (and out_delta->data) to be non-NULL
inB_delta
BNNS Tensor Contraction: After adding batch dimension, inputB_delta tensor has too many indices
BNNS Tensor Contraction: beta must be 0.0 or 1.0 (beta=%.2f)
BNNS Tensor Contraction: invalid op string "%s"
BNNS Tensor Contraction: both inputA and inputB cannot be weights "%s"
BNNS Tensor Contraction: Failed to allocate memory for filter
BNNS Tensor Contraction: inputA descriptor is illegal "%s"
BNNS Tensor Contraction: inputB descriptor is illegal "%s"
BNNS Tensor Contraction: ouput descriptor is illegal "%s"
Data type combination not supported
Specified opaque layout is not supported for use with specified contraction operation
BNNS Tensor Contraction: %s shape does not match inputA shape in dimension %ld (%zu vs %zu)
BNNS Tensor Contraction: out_delta shape does not match output shape in dimension %ld (%zu vs %zu)
BNNS Tensor Contraction: Wildcard index '*' must appear on both sides of operation or neither: "%s"
BNNS Tensor Contraction: Wildcard index '*' must appear at most once per index set: "%s"
BNNS Tensor Contraction: Wildcard index '*' must appear consistently only at start or end of index sets: "%s"
BNNS Tensor Contraction: number of indices from operation (%td) does not match dimension of input A descriptor (%zu) "%s"
BNNS Tensor Contraction: number of indices from operation (%td) does not match dimension of input B descriptor (%zu) "%s"
BNNS Tensor Contraction: number of indices from operation (%td) does not match dimension of output descriptor (%zu) "%s"
BNNS Tensor Contraction: '%c', index %zu of inputA has size %zu and index %td of output has size %zu, but sizes are required to match
BNNS Tensor Contraction: '%c', index %d of inputA has size %zu and index %td of inputB has size %zu, but sizes are required to match
BNNS Tensor Contraction: index '%c' of input A does not match any index of inputB or output "%s"
BNNS Tensor Contraction: '%c', index %d of inputB has size %zu and index %td of output has size %zu, but sizes are required to match
BNNS Tensor Contraction: index '%c' of input B does not match any index of inputA or output "%s"
BNNS Tensor Contraction: index '%c' of output does not match any index of inputA or inputB "%s"
BNNS Tensor Contraction: Wildcard index '*' can only appear as first or last index of "%s_%s"
BNNS Tensor Contraction: repeated indices "%s_%s"
v16@?0^v8
v24@?0Q8Q16
BNNS SoftMax: sum result shouldn't be 0, something is wrong
BNNS SoftMax FP16: sum result shouldn't be 0, something is wrong
BNNS SoftMax BF16: sum result shouldn't be 0, something is wrong
exec op %zu of %zu
  %s
  final generator of %s abs_diff=%.2e rel_diff=%.2e
  final generator of %s <not in expected_values map>
BNNS Graph Execute: Unrecognized kernel id %u
 = concat(values=[
], axis=
, interleave=
view(of:
malloc activation_grad failed
apply_specialized_convolution_PKT_0: o_height 1 not supported
apply_specialized_convolution_PKT_1: o_height less then 4 isn't supported
apply_specialized_convolution_PKT_2: o_height 1 not supported
apply_specialized_convolution_PKT_3: o_height 1 not supported
apply_specialized_convolution_PKT_3: o_height 2 not supported
BNNS Copy Filter: in and out must not be NULL.
BNNS Copysum Filter: unsupported data type conversion
internal bwd copysum
BNNS Tensor Contraction: Input must not be NULL
BNNS Tensor Contraction: Output must not be NULL
BNNS Tensor Contraction: index '%c' appears multiple times on right-hand side, but with different sizes
BNNS Tensor Contraction: '%c', index %zu of input has size %zu and index %d of output has size %zu, but sizes are required to match
BNNS Tensor Contraction: index '%c' appears multiple times on left-hand side, but with different sizes
BNNS Tensor Contraction: Unsupported data type conversion %sfrom %s to %s
(in summation) 
(in copy) 
BNNSTranspose: src and dest have a different number of dimensions.
BNNSTranspose: specified axes must be within range for supplied tensor (axes: %zu, %zu) dimension of tensor is %zu
internal_transpose
internal_copy
int1
int2
int4
int8
int16
int32
int64
uint1
uint2
uint4
uint8
uint16
uint32
uint64
bf16
fp16
fp32
indexed8
indexed4
indexed2
indexed1
bool
unknown
BNNS CONVOLUTIONS VERSION2: Error minimum float w_pack weight packing value is 32
BNNS Fully Conneceted Sparsify: output allocation failed
BNNS Sparse Fully Connected Apply: output data type not supported
BNNS Fully Connected Sparsify: tried structured 5:8 but number of non zeros is vey low %zu
BNNS Fully Connected Sparse: using batch single compute for batch_size %zu, will be probably better to use batch_multi code path
BNNS Fully Connected Sparsify: tried structured 14:16 but number of non zeros is vey low %zu
internal gemm: unsupported type combination
 = tile(
, reps=
BNNS Fully Connected Sparsify: workspace size is too small
BNNS Fully Connected Sparsify: failed to allocate workspace
BNNS Fully Connected Sparsify: not enough memory in scratch memory to encode sparsity
BNNSGather: Only 8, 16, 32 and 64 bit integer types may be used as indices data type
BNNSGather: Only 16 and 32 bit floating point and 8, 16 and 32 bit integer types may be used as input data type
BNNSGather: Only 16 and 32 bit floating point and 8, 16 and 32 bit integer types may be used as output data type
BNNSGather: indices must be a 1d array or have the same shape as output
BNNSGather: input and output tensors must have the same number of dimensions
BNNSGather: output.size[%zu]=%zu must have same size as indices.size[0]=%zu if indices is 1D
BNNSGather: output.size[%zu]=%zu must have same size as indices.size[%zu]=%zu if indices is not 1D
BNNSGather: output.size[%zu]=%zu must have same size as input.size[%zu]=%zu
BNNSScatter: Only add and set ops are supported
BNNSScatter: Only 8, 16, 32 and 64 bit integer types may be used as indices data type
BNNSScatter: Only 16 and 32 bit floating point and 8, 16 and 32 bit integer types may be used as input data type
BNNSScatter: Only 16 and 32 bit floating point and 8, 16 and 32 bit integer types may be used as output data type
BNNSScatter: indices must be a 1d array or have the same shape as input
BNNSScatter: input and output tensors must have the same number of dimensions
BNNSScatter: input.size[%zu]=%zu must have same size as indices.size[0]=%zu if indices is 1D
BNNSScatter: input.size[%zu]=%zu must have same size as indices.size[%zu]=%zu
BNNSScatter: input.size[%zu]=%zu must have same size as output.size[%zu]=%zu
BNNSGatherND: Only 8, 16, 32 and 64 bit integer types may be used as indices data type
BNNSGatherND: Only 16 and 32 bit floating point and 8, 16 and 32 bit integer types may be used as input data type
BNNSGatherND: Only 16 and 32 bit floating point and 8, 16 and 32 bit integer types may be used as output data type
BNNSGatherND: indices.size[%zu]=%zu must be less than or equal to input.ndim=%zu
BNNSScatterND: Only 8, 16, 32 and 64 bit integer types may be used as indices data type
BNNSScatterND: Only 16 and 32 bit floating point and 8, 16 and 32 bit integer types may be used as input data type
BNNSScatterND: Only 16 and 32 bit floating point and 8, 16 and 32 bit integer types may be used as output data type
BNNSScatterND: indices.size[%zu]=%zu must be less than or equal to output.ndim=%zu
internal scatter copy
dict-item
o-item
%s%zu
BNNSGatherND: output tensor (shape [%s]) must have shape matching first n-1 dimensions of indices + shape of any dimensions of input not specified in final dimension of indices (i.e. [%s]).
BNNSScatterND: input tensor (shape [%s]) must have shape matching first n-1 dimensions of indices + shape of any dimensions of output not specified in final dimension of indices (i.e. [%s]).
internal gatherND copy
src-item
dest-item
internal scatterND copy
BNNS Dequantize: input and output can't be null
BNNS Dequantize: in place conversion not supported
BNNS Dequantize: only __fp16 output is supported
BNNS Dequantize: lut is null and needed for Indexed input type
BNNS Dequantize: input type not supported
BNNS Layer Norm Apply: failed to allocate scratch memory
BNNS Batchnorm Apply Backward: activation allocation failed
BNNS Batchnorm Apply Backward: failed to apply activation backward
BNNS Instance Norm Apply: input stride (%zu) and output stride (%zu) aren't set correctly
BNNS Instance Norm Apply: failed to allocate scratch memory
BNNS Instance Norm Apply Backward: activation allocation failed
BNNS Instance Norm Apply Backward: failed to apply activation backward
BNNS Layer Norm Apply: input stride (%zu) and output stride (%zu) aren't set correctly
BNNS Layer Norm Apply Backward: activation allocation failed
BNNS Layer Norm Apply Backward: failed to apply activation backward
BNNS Group Norm Apply: failed to allocate scratch memory
BNNS Group Norm Apply Backward: activation allocation failed
BNNS Group Norm Apply Backward: failed to apply activation backward
BNNS Fully Connected Tiny Apply: unexpecetd weights layout
MIL program has more than one function, and no specific function specified for compilation
BNNS Fully Connected Apply: allocation of work buffer failed
BNNS Fully Connected: Unexpected result, should be at least 1
BNNS Fully Connected: Unexpected result, at least 1 batch should remain
BNNS Fully Connected: Unexpected result, at least 1 output should remain
BNNS Fully Connected: Failed to fit in memory limit, using generic code
BNNS FUlly: allocation failed
BNNS Fully Connected: weights conversion buffer isn't allocated.
BNNS Fully Connected: inputs conversion buffer isn't allocated.
BNNS Fully Connected Backward: output delta is NULL
BNNS Fully Connected Backward: only float32 or bfloat16 output delta are supported
BNNS Fully Connected Backward: input delta only support float32 or bfloat16
BNNS Fully Connected Backward: weights delta only support float32 or bfloat16
BNNS Fully Connected Backward: bias delta only support float32 or bfloat16
BNNS Fully Connected Backward: operation is currently unsupported.  We only support sparse weights for inference.
BNNS Fully Connected Backward: output delta BNNSDataLayoutImageCHW isn't supported
BNNS Fully Connected Backward: failed to allocate output delta scratch buffer
BNNS Fully Connected Backward: apply activation backward failed
BNNS Fully Connected Backward: input delta BNNSDataLayoutImageCHW isn't supported
BNNS Fully Connected Backward: failed to allocate input delta scratch buffer
BNNS Fully Connected Backward: failed to compute input delta
BNNS Fully Connected Backward: user requested weights delta but didn't supply original inputs
BNNS Fully Connected Backward: weights sizes doesn't match weights delta sizes
BNNS Fully Connected Backward: unsupported weights delta layout
BNNS Fully Connected Backward: no bias, bias delta ignored
BNNS Fully Connected: Weight type not supported
BNNS Fully Connected Create: failed to allocated memory (%zu bytes))
BNNS_FULLY: input doesn't support Indexed data type
BNNS_FULLY: output only support fp32 data type
BNNS Fully Connected: using generic code to convert data 1 element each time
BNNS Fully Connected: output data type not supported for conversion
BNNS Fully Connected: input must be a 1D array
BNNS Fully Connected: input descriptor is illegal
BNNS Fully Connected: output must be a 1D array
BNNS Fully Connected: output descriptor is illegal
BNNS Fully Connected: weights row size should match input vector size
BNNS Fully Connected: weights number of rows should match output vector size
BNNS Fully Connected: weights must be a 2D array
BNNS Fully Connected: weights descriptor is illegal
BNNS Fully Connected: Pointer to weight data must be non-NULL
BNNS Fully Connected: Output data types supported in this version: Float32, Float16, BFloat16
BNNS Fully Connected: Bias data types supported in this version: Float32
BNNS Fully Connected: Float16 output isn't supported with bias or activation function
BNNS Fully Connected: Input/Weight data types supported in this version: Float32, Float16, BFloat16, Int8, Int16
BNNS Fully Connected: Data type of weights and input should match
BNNS Fully Connected: size computation wraparound -I %zu -O %zu
BNNS Fully Connected: bias size computation wraparound -O %zu bias data type %x
Attempted to copy a view without explicit shallow copy flag
_view
BNNS InTopK: invalid axis value %zu (input tensor has %zu dimensions)
BNNS InTopK: Unsupported test_indices data type for this operation (supported data types are: int32)
BNNS InTopK: Unsupported output data type for this operation (supported data types are: boolean)
BNNS InTopK: input tensor with axis removed is not congruent with test_indices tensor
BNNS InTopK: input tensor with axis removed is not congruent with output tensor
BNNS InTopK: Unsupported input data type for this operation (supported data types are: fp32, fp16)
BNNS TopK: input descriptor is illegal
BNNS TopK: best_values descriptor is illegal
BNNS TopK: best_values data type has to match input data type for this operation
BNNS TopK: Unsupported best_indices data type for this operation (supported data types are: int32)
BNNS TopK: invalid axis value %zu (input tensor has %zu dimensions)
BNNS TopK: best_values tensor is not consistent with input tensor
BNNS TopK: best_values tensor size[%zu]=%zu should be K=%zu
BNNS TopK: best_indices tensor is not consistent with input tensor
BNNS TopK: best_indices tensor size[%zu]=%zu should be K=%zu
BNNS TopK: Unsupported input data type for this operation (supported data types are: fp32, fp16)
malloc
orange
yellow
green
blue
indigo
violet
lilac
pink
cerulean
ultraviolet
infrared
Unoptimized layout option used. Workspace will be super-massive.
Context '
': workspace size 
, lower bound (LOAD) 
layout_color
literal
user_input
user_output
relu
leaky_relu
sigmoid
scaled_tanh
linear_activation
clamped_relu
sigmoid_hard
softplus
softsign
thresholded_relu
silu
, alpha=
, beta=
BNNS unsupported sgd variant
BNNS Optimizer Adam Apply: Adam beta1 and beta2 must be positive
BNNS Optimizer Adam Apply: Adam beta1 and beta2 must be strictly less than 1
BNNS Optimizer RMSProp Apply: RMSProp alpha must be in (0,1)
BNNS Optimizer Apply: Error unsupported optimizer function
BNNS Optimizer Apply: Error OptimizerAlgFields is NULL
BNNS Optimizer Apply: parameter array pointer is NULL
BNNS Optimizer Apply: gradient array pointer is NULL
BNNS Optimizer Apply: gradient pointer number %zu is NULL
BNNS Optimizer Apply: Error in parameter %zu: parameter descriptor must be contiguous such that stride[0]=1 and for every N>0 stride[N]=stride[N-1]*size[N-1]
BNNS Optimizer Apply: accumulator array pointer is NULL
BNNS Optimizer Apply: parameter pointer number %zu is NULL
BNNS Optimizer Apply: accumulator pointer number %zu is NULL
BNNS Optimizer Apply: Error in parameter %zu: Parameter and Gradient descriptors must have the same sizes and strides
BNNS Optimizer Apply: Error in parameter %zu: Parameter and Accumulator descriptors must have the same sizes and strides
BNNS Optimizer Apply: Error in Parameter %zu: Parameter data type is not Float32 / Float16 / Bfloat16
BNNS Optimizer Apply: Error in Gradient %zu: Gradient data type is not Float32 / Float16 / Bfloat16
BNNS Optimizer Apply: Error in Accumulator %zu: Accumulator data type is not Float32 / Float16 / BFloat16
BNNS Optimizer Apply: Error time_step is not valid for Adam optimizer. minimal time step value is 1
BNNS Optimizer Apply: Adam optimizer require accumulators pointer to be valid
BNNS Optimizer Apply: accumulator1 pointer number %zu is NULL
BNNS Optimizer Apply: accumulator2 pointer number %zu is NULL
BNNS Optimizer Apply: accumulator3 pointer number %zu is NULL
BNNS Optimizer Apply: Error in parameter %zu: Parameter and Accumulator1 descriptors must have the same sizes and strides
BNNS Optimizer Apply: Error in parameter %zu: Parameter and Accumulator2 descriptors must have the same sizes and strides
BNNS Optimizer Apply: Error in parameter %zu: Parameter and Accumulator3 descriptors must have the same sizes and strides
BNNS Optimizer Apply: Error in accumulator1 %zu: Accumulator data type is not Float32 / Float16 / Bfloat16
BNNS Optimizer Apply: Error in accumulator2 %zu: Accumulator data type is not Float32 / Float16 / Bfloat16
BNNS Optimizer Apply: Error in accumulator3 %zu: Accumulator data type is not Float32 / Float16 / Bfloat16
BNNS Optimizer Apply: RMSProp optimizer require accumulators pointer to be valid
BNNS Optimizer Apply: accumulator_n pointer number %zu is NULL
BNNS Optimizer Apply: accumulator_g pointer number %zu is NULL
BNNS Optimizer Apply: momentum pointer number %zu is NULL
BNNS Optimizer Apply: Error in accumulator_n %zu: Accumulator data type is not Float32 / Float16 / Bfloat16
BNNS Optimizer Apply: Error in accumulator_g %zu: Accumulator data type is not Float32 / Float16 / Bfloat16
BNNS Optimizer Apply: Error in momentum %zu: Accumulator data type is not Float32 / Float16 / Bfloat16
BNNS Optimizer: unsupported optimizer function
BNNS ClipByValue: Error dimensions of input tensor (%zu) and output tensor (%zu) must match
BNNS ClipByValue: Error only supports fp32
BNNS ClipByValue: Error sizes and strides of input tensor and output tensor must match
BNNS ClipByNorm: Error dimensions of input tensor (%zu) and output tensor (%zu) must match
BNNS ClipByNorm: Error only supports fp32
BNNS ClipByNorm: Error sizes and strides of input tensor and output tensor must match
BNNS ClipByGlobalNorm: src[%zu] and dest[%zu] have a different number of dimensions.
BNNS ClipByGlobalNorm: src[%zu] and dest[%zu] must contain fp32 data.
BNNS ClipByGlobalNorm: src[%zu] and dest[%zu] have different sizes or strides.
BNNS ComputeNorm: Error only supports BNNSL2Norm
BNNS ComputeNorm: Error only supports fp32
BNNS ComputeNorm: Error non-axis dimensions of input tensor (%zu) and output tensor (%zu) must match
BNNS ComputeNorm: Error non-axis sizes of input tensor and output tensor must match
BNNS ComputeNorm Backward: Error only supports BNNSL2Norm
BNNS ComputeNorm Backward: Error only supports fp32
BNNS ComputeNorm Backward: Error non-axis dimensions of input tensor (%zu) and output tensor (%zu) must match
BNNS ComputeNorm Backward: Error non-axis sizes of input tensor and output tensor must match
BNNS Optimizer Init: Unsupported layout
BNNS Sparse Fully Connected: input must be a 1D array
BNNS Sparse Fully Connected: input descriptor is illegal
BNNS Sparse Fully Connected: output must be a 1D array
BNNS Sparse Fully Connected: output descriptor is illegal
BNNS Sparse Fully Connected: invalid matrix layout
BNNS Sparse Fully Connected: invalid block row count: %u
BNNS Sparse Fully Connected: invalid block col count: %u
BNNS Sparse Fully Connected: input data types supported in this version: Float16
BNNS Sparse Fully Connected: output data types supported in this version: Float32, Float16
BNNS Sparse Fully Connected: weights data types supported in this version: Float16
BNNS Sparse Fully Connected: unsupported matrix block size %u x %u
BNNS Sparse Fully Connected: Allocation of the setup structure failed
BNNS Convolution: malloc failed
BNNS Graph: Invalid representation
BNNS Graph: Unsupported BNNS IR Version
/System/Library/PrivateFrameworks/MIL.framework/MIL
BNNS cannot compile graph: MIL framework is not available
BNNS GraphGetSize passed invalid graph
BNNS GraphGetSize passed graph with unsupported ir_version %u
BNNS GraphGetWorkspaceSize passed invalid graph
BNNS GraphGetWorkspaceSize passed graph with unsupported ir_version %u
BNNS GraphGetArgPosition passed invalid graph
BNNS GraphGetArgPosition passed graph with unsupported ir_version %u
BNNS GraphGetArgPosition: Argument '%s' not found
BNNS Compare: in0 is NULL
BNNS Compare: in1 is NULL
BNNS Compare: out is NULL
BNNS Compare: Unsupported I/O tensor data types.
BNNS Compare: Mismatch in input tensor data types.
BNNS Compare: Invalid operation %d for data type %s.
BNNS Compare: Broadcast failed.
BNNS Compare: I/O tensor dimension %lu mismatch.
BNNS Compare: Partially overlapping input and output tensors are not supported.
BNNS Compare: Unsupported I/O tensor layout.
BNNS Compare: I/O tensor layout mismatch.
BNNS Compare: input size[%zu]=%zu does not equal output size of %zu or 1
BNNS Compare: Unsupported operator.
BNNS Fully Connected Choose: inputs, weights, outputs and bias must be contigeous in memory (stride[0] <= 1)
BNNS Fully Connected Apply: failed to allocated scratch memory
BNNS Fully Connected Init: missing data table for indexed data type
BNNS Fully Connected Apply: Ilegal compute block
BNNS Fully Connected Create: failed to create layer context
BNNS Fully Connected Create: allocation failed
BNNS Fully Connected Direct Apply: input pointer is NULL
BNNS Fully Connected Direct Apply: output pointer is NULL
BNNS Fully Connected Direct Apply: failed to create layer context
BNNS Fully Connected Init: failed to create context
BNNS Fully Connected Init: input descriptor CHW layout isn't set properly
BNNS Fully Connected Init: input descriptor isn't set properly
BNNS Fully Connected Init: input descriptor failed validation
BNNS Fully Connected Init: output descriptor CHW layout isn't set properly
BNNS Fully Connected Init: output descriptor isn't set properly
BNNS Fully Connected Init: output descriptor failed validation
BNNS Fully Connected Init: weights descriptor row major size[0] doesn't match input vector size
BNNS Fully Connected Init: weights descriptor row major size[1] doesn't match output vector size
BNNS Fully Connected Init: weights descriptor column major size[0] doesn't match output vector size
BNNS Fully Connected Init: weights descriptor column major size[1] doesn't match input vector size
BNNS Fully Connected Init: weights descriptor isn't set properly
BNNS Fully Connected Init: weights descriptor failed validation
BNNS Fully Connected Init BNNSNDArrayDescriptor: stride[%zu] is too small
BNNS Fully Connected Apply: failed to apply filter
BNNS Random Generator Create: Unsupported method %u.
BNNS Random Generator Create: Failed to allocate memory
BNNS Random Fill Uniform Float: Invalid descriptor
BNNS Random Fill Uniform Float: Range bounds are not exactly representable as bf16
BNNS Random Fill Uniform Float: Range (%g, %g) is empty
BNNS Random Fill Normal Float: Mean and standard deviation are not exactly representable as bf16
BNNS Random Fill Normal Float: Standard deviation (%g) is not greater than zero
BNNS Random Fill Float: Distribution type not supported
BNNS Random Fill Uniform Float: Range bounds are not exactly representable as fp16
BNNS Random Fill Normal Float: Mean and standard deviation are not exactly representable as fp16
BNNS Random Fill Float: Unsupported data type
BNNS Random Fill Uniform Integer: Invalid descriptor
BNNS Random Fill Uniform Integer:%s range (%d, %d) is empty
 (clipped)
BNNS Random Fill Uniform Integer: range (%lld, %lld) is empty
BNNS Random Fill Uniform Integer:%s range (%llu, %lld) is empty
BNNS Random Fill Uniform Integer: Unsupported data type
BNNS Random: CCCryptorCreateWithMode() failed
BNNS Random Fill Uniform: Invalid generator
Failed to create random data
BNNS Loss: Error unsupported loss function
BNNS Loss: Error Input width is 0
BNNS Loss: Error Input must be BNNSDataTypeFloat32 or BNNSDataTypeBFloat16 type
BNNS Loss Error: Input must be contiguous. stride0 must be 1 or 0
BNNS Loss Error: Input tensors must be contiguous.
BNNS Loss: Error unknown reduction function
BNNS Loss: Error output data type must be BNNSDataTypeFloat32 or BNNSDataTypeBFloat16
BNNS Loss: Error output size>1 is only allowed with reduction BNNSLossReductionNone.
BNNS Yolo loss: Error out width (size[0]) must be 1 as yolo loss is always reduced
BNNS Loss: Error anchors is NULL
BNNS Loss: Error number of grid rows is 0
BNNS Loss: Error number of grid columns is 0
BNNS Loss: Error number of anchor boxes is 0
BNNS Loss: Error anchor box size <= 5
BNNS Loss: Error object minimum iou is negative
BNNS Loss: Error no object maximum iou is negative
BNNS Loss: Error input descriptor width (%zu) different from expected width of grid_size*num_anchors*(5+num_+classes) (%zu)
BNNS Loss: memory allocation failed
BNNS Loss: Error filter is NULL
BNNS Loss: Error batch_size is 0
BNNS Loss: Error in pointer is NULL
BNNS Loss: Error in_stride is smaller than i_desc.size[0] or Tensor size
BNNS Loss: Error labels is NULL
BNNS Loss: Error labels_stride is smaller than i_desc.size[0] or Tensor size
BNNS Loss: Error weights_size>0 but weights pointer is NULL
BNNS Loss: Warning weight_size==0 but weight pointer is not NULL. Weight pointer is ignored.
BNNS Loss: Error out is NULL
BNNS Loss: Error in_delta descriptor is ilegal
BNNS Loss: Error in_delta must be contiguous, such that stride[0] is 0 or 1
BNNS Loss Error: weight_size value must be 0,1 batch_size, or batch_size*input_width for softmax
BNNS Loss Error: data type not supported for softmax
BNNS Loss: Error yolo weight_size value must be 0, use yolo specific weight factors during filter create
BNNS Yolo loss: input and output data type must be BNNSDataTypeFloat32
BNNS Loss Error: weight_size value must be 0,1 batch_size, or batch_size*input_width for sigmoid/categorical cross entropy, mse, mae, log, hinge, and huber
BNNS Loss Error: data type not supported
BNNS Loss Error: unsupported loss function
BNNS Loss Backward: Error filter is NULL
BNNS Loss Backward: Error batch_size is 0
BNNS Loss Backward: Error in is NULL
BNNS Loss Backward: Error in_stride is smaller than i_desc.size[0]
BNNS Loss Backward: Error labels is NULL
BNNS Loss Backward: Error labels_stride is smaller than i_desc.size[0]
BNNS Loss Backward: Error weights_size>0 but weights pointer is NULL
BNNS Loss Warning: weight_size==0 but weight pointer is not NULL. Weight pointer is ignored.
BNNS Loss Backward Error: weight_size value must be 0,1 batch_size, or batch_size*input_width for softmax/sigmoid/categorical cross entropy, mse, mae, log, hinge, and huber
BNNS Loss Backward: Error in_delta descriptor is illegal
BNNS Loss Backward: Error in_delta must be contiguous, such that stride[0] is 0 or 1
BNNS Loss Backward: Error in_delta is NULL
BNNS Loss Backward: Error out_delta descriptor is illegal
BNNS Loss Backward: Error out_delta must be contiguous, such that stride[0] is 0 or 1
BNNS Loss Backward: Error out_delta is NULL
BNNS Loss: data type not supported
BNNS Loss: unsupported loss backward function
BNNS Yolo loss: Error yolo reduction type must be BNNSLossReductionSum
BNNS Yolo loss: Error input descriptor width does not match grid_rows*grid_columns*anchors*(5+classes)
BNNS Yolo loss: alloc failed
BNNS Yolo loss: malloc failed
BNNS softmax cross entropy loss: malloc for pDx2 failed
BNNS softmax cross entropy loss: malloc for norm_x failed
BNNS Loss Warning: reduction BNNSLossReductionWeightedMean sum of weights is zero
BNNS Loss: Error reduction BNNSLossReductionNonZeroWeightMean all weights are zero
BNNS softmax cross entropy loss: malloc for temporary_softmax_forward_output failed
BNNS Create Depthwise Convolution: incompatible numbers of channels between images and convolution parameters
BNNS Create Depthwise Convolution: unsupported weight format
BNNS Create Depthwise Convolution: input must be BNNSDataLayoutImageCHW layout 
BNNS Create Depthwise Convolution: input descriptor is illegal
BNNS Create Depthwise Convolution: output must be BNNSDataLayoutImageCHW layout 
BNNS Create Depthwise Convolution: output descriptor is illegal
BNNS Depthwise Convolution Create: weights descriptor is illegal
BNNS Depthwise Convolution Create: does not support asymmetric padding. left and right pad must match, up and down pad must match
BNNS Create Depthwise Convolution: only float32 is supported
BNNS Create Depthwise Convolution: channel multiplier mismatch
BNNS Create Depthwise Convolution: malloc failed
BNNS Create Depthwise Convolution: weights malloc failed 
invalid argument
BNNS DEPTHWISE CONV: input batch stride doesn't make sense (%zu < %zu x %zu)
BNNS DEPTHWISE CONV: output batch stride doesn't make sense (%zu < %zu x %zu)
BNNS LSTM Direct Apply: filter initialization failed
BNNS LSTM Direct Apply: training cache capacity isn't sufficient
BNNS LSTM Direct Apply: Dropout is only supported in the presence of a training cache
BNNS LSTM Direct Apply: failed to allocate scratch buffer
BNNS LSTM Direct Apply: failed to thread workspace buffer
BNNS LSTM Direct Apply Backward: BNNSNDArrayFlagBackpropAccumulate is only supported on layer_delta->input_descriptor.data_desc.
BNNS LSTM Direct Apply Backward: layer_params initialization failed
BNNS LSTM Direct Apply Backward: layer_delta initialization failed
BNNS LSTM APPLY BACKWARD: Use of dropout without training cache is not supported.
BNNS LSTM APPLY BACKWARD: forward pass intermediate results weren't cached, recomputing forward pass
BNNS LSTM Direct Apply Backward: failed to allocate training cache buffer
BNNS LSTM Direct Apply Backward: failed to compute forward intermediate results
BNNS LSTM Direct Apply Backward: training cache capacity isn't sufficient
BNNS LSTM Direct Apply Backward: failed to allocate scratch buffer
BNNS LSTM init: hidden_size must be greater than zero
BNNS LSTM init: input descriptor.data isn't set correctly (size don't match seq_len/batch_size/input_size)
BNNS LSTM init: input descriptor.data isn't set correctly (size don't match batch_size/seq_len/input_size)
BNNS LSTM init: input descriptor.data isn't set correctly (size don't match input_size/batch_size/seq_len)
BNNS LSTM init: input descriptor.hidden isn't set correctly (size don't match hidden_size/batch_size/num_directions/num_layers)
BNNS LSTM init: input descriptor.cell_state isn't set correctly (size don't match hidden_size/batch_size/num_directions/num_layers)
BNNS LSTM init: output descriptor.data isn't set correctly (size don't match seq_len/batch_size/hidden_size*num_directions)
BNNS LSTM init: output descriptor.data isn't set correctly (size don't match batch_size/seq_len/hidden_size*num_directions)
BNNS LSTM init: output descriptor.data isn't set correctly (size don't match hidden_size/num_directions/batch_size/seq_len)
BNNS LSTM init: output descriptor.hidden isn't set correctly (size don't match hidden_size/batch_size/num_directions/num_layers)
BNNS LSTM init: output descriptor.cell_state isn't set correctly (size don't match hidden_size/batch_size/num_directions/num_layers)
BNNS LSTM init: forget_gate isn't set correctly (size don't match hidden_size/batch_size/num_directions/num_layers)
BNNS LSTM init: input_gate isn't set correctly (size don't match hidden_size/batch_size/num_directions/num_layers)
BNNS LSTM init: candidate_gate isn't set correctly (size don't match hidden_size/batch_size/num_directions/num_layers)
BNNS LSTM init: output_gate isn't set correctly (size don't match hidden_size/batch_size/num_directions/num_layers)
BNNS LSTM: Data type not supported, fail
prelu
conv
conv_transpose
cast
const
identity
tile
concat
expand_dims
reshape
slice_by_index
slice_by_size
squeeze
stack
transpose
constexpr_lut_to_dense
: Failed to parse x argument
: Failed to parse y argument
values
axis
interleave
m2a_attr
palletize_int4
Failed to palletize %s
: only 4-bit palletization of fp16 is supported
indices
%s: only 1d and 2d convolutions are supported
weight
weights
strides
pad_type
valid
custom
%s: Unsupported conv pad type '%s'
dilations
groups
bias
begin
stride
begin_mask
end_mask
squeeze_mask
size
reps
perm
epsilon
beta
alpha
_anon_
BNNS Graph Compile: Unsupported MIL data type
unordered_map::at: key not found
Can't cast this scalar :(
 = permute(
Replaced op 
 with constexpr result
BNNS Convolution Create: Client needs to support weights ptr with low_mem
BNNS Convolution Create: malloc
BNNS Convolution Create: failed to create convolution context
BNNS Convolution Create: failed to divide work between convolution contexts to fit in memory
BNNS Convolution Create: failed to create single generic convolution
BNNS Convolution Apply: invalid argument
BNNS Convolution Apply: failed to allocate input auxiliary buffer
BNNS Convolution: Winograd weights memory doesn't fit in memory
BNNS Convolution: unable to create Winograd that fit in memory
BNNS Convolution: allocation of contexts failed
BNNS LOW MEM CONVOLUTION: failed to create winograd convolution
BNNS LOW MEM CONVOLUTION: malloc failed
BNNS LOW MEM CONVOLUTION: incompatible BNNS_low_memory_context id
BNNS LOW MEM CONVOLUTION: convolution failed
BNNS LOW MEM CONVOLUTION: input aux buffer wasn't allocated
BNNS LOW MEM CONVOLUTION:: context type not supported
BNNS Convolutions Apply: unexpected input data type
BNNS Convolution Create: apply convolution failed
BNNS Quantization Filter: layer_params is NULL
BNNS Quantization: input layout and output layout must match
BNNS Quantization: unsupported input/output layouts
BNNS Quantization: input descriptor error
BNNS Quantization: input descriptor data is NULL
BNNS Quantization: output descriptor error
BNNS Quantization: output descriptor data is NULL
BNNS Quantization: invalid quantizer function
BNNS Quantization: BNNSQuantize function supported for the following input descriptor data types: BNNSDataTypeFloat32, BNNSDataTypeFloat16, BNNSDataTypeBFloat16, BNNSDataTypeInt32
BNNS Quantization: BNNSQuantize function supported for the following output descriptor data types: BNNSDataTypeInt8, BNNSDataTypeUInt8, BNNSDataTypeInt16, BNNSDataTypeUInt16, BNNSDataTypeInt32, BNNSDataTypeUInt32
BNNS Quantization: BNNSDeQuantize function supported for the following input descriptor data types: BNNSDataTypeInt8, BNNSDataTypeUInt8, BNNSDataTypeInt16, BNNSDataTypeUInt16, BNNSDataTypeInt32, BNNSDataTypeUInt32
BNNS Quantization: BNNSDeQuantize function supported for the following output descriptor data types: BNNSDataTypeFloat32, BNNSDataTypeFloat16, BNNSDataTypeBFloat16, BNNSDataTypeInt32
BNNS Quantization: invalid axis_mask, number of mask set bits must be lower or equal to 1
BNNS Quantization: axis_mask bits are set beyond batch dimension
BNNS Quantization: Error dimensions of input tensor (%zu) and output tensor (%zu) must match
BNNS Quantization: Error shape of input tensor and output tensor must match
BNNS Quantization Filter: scale layout must be BNNSDataLayoutVector
BNNS Quantization: scale descriptor error
BNNS Quantization Filter: bias layout must be BNNSDataLayoutVector
BNNS Quantization: bias descriptor error
BNNS Quantization: bias vector size does not match axis mask, bias size[0]=%zu, expected %zu
BNNS Quantization: scale vector size does not match axis mask, scale size[0]=%zu, expected %zu
BNNS Fused Convolution and Normalization: Error convolution_layer_params is NULL
BNNS Fused Convolution and Quantization: Error quantization_layer_params is NULL
BNNS Fused Compute and Quantization: Error Convolution output layout and Quantization input layout must match
BNNS Fused Compute and Quantization: Error Convolution output data type and Quantization input data type must match
BNNS Fused Compute and Quantization: Error Convolution output descriptor sizes and Quantization input descriptor sizes must match
BNNS Fused Compute and Quantization: Error Convolution output descriptor strides and Quantization input descriptor strides must match
BNNS Fused Compute and Quantization: Error memory allocation failed
BNNS Fused Compute and Quantization create filter failed: compute filter type error
BNNS Fused Compute and Quantization create filter failed
BNNS Quantization filter failed
BNNS Fused Fully Connected and Quantization: Error fully_layer_params is NULL
BNNS Fused Fully Connected and Quantization: Error quantization_layer_params is NULL
BNNS Fused Compute and Quantization: Error Fully Connected output layout and Quantization input layout must match
BNNS Fused Compute and Quantization: Error Fully Connected output data type and Quantization input data type must match
BNNS Fused Compute and Quantization: Error Fully Connected output descriptor sizes and Quantization input descriptor sizes must match
BNNS Fused Compute and Quantization: Error Fully Connected output descriptor strides and Quantization input descriptor strides must match
BNNS Quantization: unsupported data type
BNNS Quantization: unsupported quantization input data type
BNNS Quantization: unsupported quantization output data type
BNNS Quantization: unsupported dequantization input data type
BNNS Quantization: unsupported dequantization output data type
BNNS Transposed Convolution Create: input data type isn't supported
BNNS Transposed Convolution Create: weight data isn't supported
BNNS Transposed Convolution Create: output data type isn't supported
BNNS Transposed Convolution Apply: Filter is NULL
BNNS Transposed Convolution Apply: failed to allocate memory to manipulate input
BNNS Transposed Convolution Apply: unknown weight layout
BNNS Transposed Convolution Create: layer_params is NULL
BNNS Transposed Convolution Create: output channels must divide by groups
BNNS Transposed Convolution Create: input channels must divide by groups
BNNS Transposed Convolution Create: low-mem isn't supported
BNNS Transposed Convolution Create: packed weights aren't supported
BNNS Transposed Convolution Create: failed to create forward path
BNNS Transposed Convolution Create: failed to allocate wrapper memory
BNNS Transposed Convolution Create: transposed convolution groups only supported with BNNSDataLayoutConvolutionWeightsOIHW or BNNSDataLayoutConvolutionWeightsOIHrWr layouts
BNNS Transposed Convolution Create: weights allocation failed
BNNS Create Layer Transposed Convolution: input channels must be divisible by groups
BNNS Create Layer Transposed Convolution: output channels must be divisible by groups
BNNS Create Layer Transposed Convolution: bias must match output channels
BNNS Create Layer Transposed Convolution: failed to allocate memory to copy weights and bias
BNNS Transpsoed Vector Convolution Apply: activation gradient auxilary allocation failed
BNNS Transposed Convolution Create: malloc failed
BNNS Transposed Convolution: weights allocation failed
BNNS Transposed Convolution: bias allocation failed
BNNS Transposed Convolution Reorder Weights: NULL weight object
BNNS Transposed Convolution Reorder Weights: dst weight size too small
BNNS Transposed Convolution Reorder Weights: NULL weight array
BNNS Transposed Convolution Rotate Weights: NULL weight object
BNNS Transposed Convolution Rotate Weights: dst weight size too small
BNNS Transposed Convolution Rotate Weights: NULL weight array
BNNS Transposed Convolution Reorder and Rotate Weights: NULL weight object
BNNS Transposed Convolution Reorder and Rotate Weights: dst weight size too small
BNNS Transposed Convolution Reorder and Rotate Weights: NULL weight array
transposed convolution input manipulation failed
transposed convolution input manipulation failed - nothing to do, should not have allocated dst_input buffer
BNNS: unsupported activation gradient data type
BNNS Transposed Convlution Backward: dy padding failed
BNNS Transposed Convlution Backward: no padding is needed, dy padding failed
BNNS Create Convolution: fp32 weights allocation failed
BNNS Create Convolution: int16 weights allocation failed
BNNS Create Convolution: failed to allocate memory to copy the weights
BNNS Create Convolution: single descriptors allocation failed
BNNS Convolution Create: allocation failed
failed to upconvert or copy weights
failed to allocate none generic format
BNNS Convolution Create: forward pass check (swapping input and output)  - input size computation wraparound input (%zu x %zu), output (%zu x %zu), stride (%zu x %zu), asymmetric padding (%zu,%zu,%zu,%zu)
BNNS Convolution Create: forward pass check (swapping input and output)  - input size computation wraparound input (%zu x %zu), output (%zu x %zu), stride (%zu x %zu), padding (%zu x %zu)
BNNS Convolution Create: forward pass check (swapping input and output) - output(%zu x %zu x %zu) is larger than input(%zu x %zu x %zu) with padding(%zu x %zu).
 kernel (%zu x %zu) and stride (%zu x %zu)
v8@?0
com.apple.accelerate.bnns
layer
BNNS Fully Connected Sparsify: out descriptor can't be NULL
BNNS Fully Connected Sparse: failed to allocate memory to sparsify COO
BNNS Fully Connected Sparse: insufficient memory allocated memory to sparsify COO
BNNS Fully Connected Sparse: Number of nonzeros is 0
BNNS Fully Connected Sparse COO: probably wrong stride size %zu
BNNS Fully Connected Sparsify: in descriptor can't be NULL
BNNS Normalization: layer_params is NULL
BNNS Normalization: filter memory allocation failed
BNNS Fused Convolution and Normalization: Error normalization_layer_params is NULL
BNNS Fused Convolution and Normalization: Error convolution output descriptor and normalization input descriptor must have the same sizes and strides
BNNS Fused Convolution and Normalization: Error memory allocation failed
BNNS Fused Convolution and Normalization create filter failed: Convolution type error
BNNS Fused Convolution and Normalization create filter failed
BNNS Fused Fully Connected and Normalization: Error fully_layer_params is NULL
BNNS Fused Fully Connected and Normalization: Error batch_normalization_layer_params is NULL
BNNS Fused Fully Connected and Normalization: Error fully connected output descriptor and normalization input descriptor must have the same sizes and strides
BNNS Fused Fully Connected and Normalization: Error memory allocation failed
BNNS Fused Fully Connected and Normalization create filter failed
BNNS Fused Arithmetic and Normalization: Error arithmetic_layer_params is NULL
BNNS Fused Arithmetic and Normalization: Error normalization_layer_params is NULL
BNNS Fused Arithmetic and Normalization: unsupported arithmetic function
BNNS Fused Arithmetic and Normalization: Error arithmetic output descriptor and normalization input descriptor must have the same sizes and strides
BNNS Fused Arithmetic and Normalization: Error memory allocation failed
BNNS Fused Arithmetic and Normalization create filter failed
BNNS Normalization: input pointer is NULL
BNNS Normalization: batch_size>1 and in_stride is 0
BNNS Normalization: output pointer is NULL
BNNS Normalization: batch_size>1 and out_stride is 0
BNNS Normalization: batch_size too large. Backprop cache size limits batch_size <= %zu.
BNNS Normalization: x_hat allocation failed
BNNS Normalization: filter id not supported
BNNS Normalization: inverse variance allocation failed
BNNS Normalization Apply Backward: filter is NULL
BNNS Normalization Apply Backward: Normalization beta delta pointer is NULL. backward computation of all active parameters must be done in a single function call.
BNNS Normalization Apply Backward: Normalization gamma delta pointer is NULL. backward computation of all active parameters must be done in a single function call.
BNNS Normalization Apply Backward: make sure to run normalization forward with training flag enabled before running normalization backward
BNNS Normalization Apply Backward: cannot compute activation backward, output is NULL
BNNS Normalization Apply Backward: Fusion of normalization with activation is unsupported for given activation function
BNNS Normalization Apply Backward: cannot compute input delta because input delta data pointer is NULL
BNNS Normalization Apply Backward: Error normalization input delta and input must have the same sizes
BNNS Normalization Apply Backward: in delta should be contiguous and same size as input size
BNNS Normalization Apply Backward: cannot compute beta delta because beta delta data pointer is NULL
BNNS Normalization Apply Backward: BNNSNDArrayFlagBackpropAccumulate is no supported for beta_delta.
BNNS Normalization Apply Backward: Error normalization input delta and input must have the same sizes and strides
BNNS Normalization Apply Backward: Error beta_delta descriptor must match the shape of input up to the normalizaton axis
BNNS Normalization Apply Backward: beta delta should be contiguous and same size as number of input channels
BNNS Normalization Apply Backward: cannot compute gamma delta because gamma delta data pointer is NULL
BNNS Normalization Apply Backward: BNNSNDArrayFlagBackpropAccumulate is no supported for gamma_delta.
BNNS Layer Normalization: Error gamma_delta descriptor must match the shape of input up to the normalizaton axis
BNNS Normalization Apply Backward: gamma delta should be contiguous and same size as number of input channels
BNNS Normalization Apply Backward: out delta data is NULL
BNNS Normalization Apply Backward: Error Normalization output delta and output must have the same sizes
BNNS Normalization Apply Backward: out delta should be contiguous and same size as input size
BNNS Normalization Apply Backward: normalization type not supported
BNNS Normalization Set State: Backprop cache must be in BNNSDataLayoutVector
BNNS Normalization Set State: Backprop cache must be in BNNSDataTypeFloat32
BNNS Normalization Set State: Backprop cache must be in contiguous
BNNS Normalization Set State: Backprop cache pointer is NULL
BNNS Normalization Set State: Backprop cache too small, must be >= %zu fp32 to allow max_batch_size >= 1
BNNS Normalization: Only elementwise activation function is supported
BNNS Normalization: Moving Mean must be BNNSDataTypeFloat32
BNNS Normalization: Moving Mean isn't set properly
BNNS Normalization: Moving Variance must be BNNSDataTypeFloat32
BNNS Normalization: Moving Variance isn't set properly
BNNS Normalization Create: normalization type not supported
BNNS Normalization: input descriptor isn't set properly
BNNS Normalization: Input must be in BNNSDataLayoutImageCHW layout
BNNS Normalization: output descriptor isn't set properly
BNNS Normalization: Output must be in BNNSDataLayoutImageCHW layout
BNNS Normalization: Gamma descriptor isn't set properly
BNNS Normalization: Beta descriptor isn't set properly
BNNS Normalization: Error input descriptor must have stride[0]=1
BNNS Normalization: Error output descriptor must have stride[0]=1
BNNS Normalization: Error input size (%zu) different from output size (%zu)
BNNS Normalization: Error normalization_axis must be 0, 1, or 2
BNNS Layer Normalization: Gamma descriptor must match the shape of input up to the normalizaton axis
BNNS Normalization: Beta descriptor must match the shape of input up to the normalizaton axis
BNNS Normalization: Gamma descriptor size[0] must be the same as number of input channels
BNNS Normalization: Beta descriptor size[0] must be the same as number of input channels
BNNS Normalization: Moving Mean descriptor size[0] must be the same as number of input channels
BNNS Normalization: Moving Variance descriptor size[0] must be the same as number of input channels
BNNS Normalization: Error momentum must be between 0 and 1
BNNS Normalization Warning: epsilon is zero, it may cause division by zero
BNNS Normalization Warning: momentum is zero
BNNS CONVOLUTIONS VERSION2: layer param is NULL
BNNS CONVOLUTIONS VERSION2: unsupported input data type
BNNS CONVOLUTIONS VERSION2: unsupported output data type
BNNS CONVOLUTIONS VERSION2: unsupported weight data type
BNNS CONVOLUTIONS VERSION2: unsupported bias data type
BNNS CONVOLUTIONS VERSION2: int/out/weight/bias descriptor element stride (stride[0]) must be 1
BNNS CONVOLUTIONS VERSION2: 2D conv doesn't fit in memory limit
BNNS CONVOLUTIONS VERSION2: 0 width/height/channel
BNNS CONVOLUTIONS VERSION2: unsupported activation
BNNS CONVOLUTIONS VERSION2: output bias or scale is not supported
BNNS CONVOLUTIONS VERSION2: malloc failed
BNNS CONVOLUTIONS VERSION2: malloc failed
BNNS CONVOLUTIONS VERSION2: failed to allocate bias conversion memory
BNNS CONVOLUTIONS VERSION2: failed to allocate scratch memory
BNNS CONVOLUTIONS VERSION2: invalid argument
BNNS CONVOLUTIONS VERSION2: input batch stride doesn't make sense (%zu < %zu x %zu)
BNNS CONVOLUTIONS VERSION2: output batch stride doesn't make sense (%zu < %zu x %zu)
BNNS CONVOLUTIONS VERSION2: unsupported weight packing
BNNS CONVOLUTIONS VERSION2: failed to allocated memory to convert bias
BNNS CONVOLUTIONS VERSION2: input padding memory allocation failed
BNNS CONVOLUTIONS VERSION2: output repack malloc failed
BNNS CONVOLUTIONS VERSION2: weight repack malloc failed
BNNS Padding Create: layer_params is NULL
BNNS Padding Create: Padding is not supported beyond 4-D tensors.
BNNS Padding Create: Undefined padding mode.
BNNS Padding Create: Unsupported data layout.
BNNS Padding Create: input and output desciptors have differing numbers of dimensions.
BNNS Padding Create: Input dimension is too small for the padding size.
BNNS Padding Create: Input size + padding sizes doesn't match output size in dimension %zu.
BNNS Padding Create: input descriptor is illegal
BNNS Padding Create: output descriptor is illegal
BNNS Padding Create: Unsupported data type.
BNNS Padding Create: I/O data type mismatch.
BNNS Padding Create: memory allocation failed
BNNS Padding Apply: filter is NULL
BNNS Padding Apply: wrong filter type, filter is not Padding.
BNNS Padding Apply: input pointer is NULL
BNNS Padding Apply: batch_size > 1 and in_stride is 0
BNNS Padding Apply: output pointer is NULL
BNNS Padding Apply: batch_size > 1 and out_stride is 0
BNNS Padding Apply Backward: filter is NULL
BNNS Padding Apply Backward: wrong filter type, filter is not padding
BNNS Padding Apply Backward: input descriptor is NULL
BNNS Padding Apply Backward: input data pointer is NULL
BNNS Padding Apply Backward: batch_size>1 and in_stride is 0
BNNS Padding Apply Backward: output delta descriptor is NULL
BNNS Padding Apply Backward: output delta data pointer is NULL
BNNS Padding Apply Backward: batch_size>1 and out_delta_stride is 0
BNNS Pooling: Error wrong filter type, filter is not pooling
out_delta is NULL
out_delta->data is NULL
BNNSFilterCreateLayerSparseEmbedding: layer_params must not be NULL
BNNSSparseGetRepresentation: filter must not be NULL
BNNSSparseGetRepresentation: Invalid filter
BNNSSparseSetRepresentation: filter must not be NULL
BNNSSparseSetRepresentation: Invalid filter
BNNSSparseEmbeddingGetDense: filter must not be NULL
BNNSSparseEmbeddingGetDense: Invalid filter
BNNS Fused Filter Multi-Input: Error filter is NULL
BNNS Fused Filter Multi-Input Warning: Batch size is 0, nothing to do
BNNS Fused Filter Multi-Input: Error - only fused arithmetic and normalization is supported.
BNNS Fused Filter Multi-Input: Error arithmetic filter apply failed
BNNS Fused Filter Backward Multi-Input: Error filter is NULL
BNNS Fused Filter Backward Multi-Input Warning: Batch size is 0, nothing to do
BNNS Fused Filter Backward Multi-Input: output delta is NULL
BNNS Fused Filter Backward Multi-Input: Error - only fused arithmetic and normalization is supported.
BNNS Fused Filter Backward Multi-Input: Error normalization beta delta pointer is NULL. backward computation of all active parameters must be done in a single function call. 
BNNS Fused Filter Backward Multi-Input: Error normalization gamma delta pointer is NULL. backward computation of all active parameters must be done in a single function call. 
BNNS Arithmetic Backward Multi-Input: failed to allocate memory
BNNS Fused Filter Backward Multi-Input: Error normalization backward failed
x_ijf*, x_ijc* -> G_fc*
BNNSFilterApplyTwoInputBatch: invalid argument
Tensor Contraction filter only expects one input
invalid filter or incorrect number of inputs
BNNSFilterApplyBackwardTwoInputBatch: invalid argument
Two-input Tensor Contraction filter has no weights
Tensor Contraction filter has no bias
Unsupported filter
BNNS GetStateSize : filter is NULL
BNNS SetState: Error filter is NULL
BNNS SetState: Source filter state is NULL
BNNS SetState: Target filter is NOT Dropout/Normalization filter
BNNS GetState: Error filter is NULL
BNNS GetState: Target filter state is NULL
BNNS GetState: Source filter is NOT Dropout filter
BNNS LayerNorm: Error Normalization axis must be 0, 1, or 2.
BNNS GroupNorm: Error The number of input channels must be divisible by the number of groups.
BNNS Fused Filter: Error number_of_fused_filters is not 2. currently supporting only 2 fused filters
BNNS Fused Filter: Error filter_type is NULL
BNNS Fused Filter: Error layer_params is NULL
BNNS Fused Filter: Error currently supporting first filter of BNNSConvolution, BNNSFullyConnected, BNNSTransposedConvolution, or BNNSArithmetic type only
BNNS Fused Filter: Error currently supporting second filter of BNNSBatchNorm, BNNSInstanceNorm, BNNSLayerNorm, BNNSGroupNorm and BNNSQuantization types only
BNNS Fused Filter: Error layer_params[0] is NULL
BNNS Fused Filter: Error layer_params[1] is NULL
BNNS Fused Filter: Error unknown fused filter type
BNNS Normalization: Error filter is NULL
BNNS Normalization: Error wrong filter type, filter is not normalization
BNNS Fused Filter: Error filter is NULL
BNNS Fused Filter Warning: Batch size is 0, nothing to do
BNNS Fused Filter: Error wrong filter, this function should only be used for a filter created with BNNSFilterCreateFusedLayer
BNNS Fused Filter: Error first filter apply failed
BNNS Fused Filter: Error malloc failed
BNNS Arithmetic Filter: Error filter is NULL
BNNS Arithmetic Filter Warning: Batch size is 0, nothing to do
BNNS Arithmetic Filter: Error wrong filter, this function should only be used for a filter created with BNNSFilterCreateLayerArithmetic
BNNS Permute Filter: filter is NULL
BNNS Permute Filter Warning: Batch size is 0, nothing to do
BNNS Permute Filter: input delta is NULL
BNNS Permute Filter: output delta is NULL
BNNS Permute Filter: input delta data pointer is NULL
BNNS Permute Filter: output delta data pointer is NULL
BNNS Permute Filter: inplace gradient is not supported
Tensor Contraction filter has no bias: bias_delta must be NULL.
Tensor Contraction filter expects more than one input
Resize filter has no bias: bias_delta must be NULL.
Resize filter has no weights: weights_delta must be NULL.
Resize filter: backward pass requires in_delta and in_delta->data to be non-NULL.
BNNS Copysum filter has no bias: bias_delta must be NULL.
BNNS Copysum filter has no weights: weights_delta must be NULL.
BNNS Copysum filter: backward pass requires in_delta and in_delta->data to be non-NULL.
BNNS Reduction filter has no bias: bias_delta must be NULL.
BNNS Padding: backward pass requires in_delta and in_delta->data to be non-NULL.
BNNS Padding filter has no bias: bias_delta must be NULL.
BNNS Embedding: in must not be NULL.
BNNS Embedding: in_delta must be NULL.
BNNS Embedding: backward pass requires weights_delta and weights_delta->data to be non-NULL.
BNNS Embedding: bias_delta must be NULL.
BNNS Normalization Backward: Error filter is NULL
BNNS Normalization Backward Warning: Batch size is 0, nothing to do
BNNS Normalization Backward: Error wrong filter, this function should only be used for a filter created with BNNSFilterCreateLayerNormalization
BNNS Fused Filter Backward: Error filter is NULL
BNNS Fused Filter Backward Warning: Batch size is 0, nothing to do
BNNS Fused Filter Backward: output delta is NULL
BNNS Fused Filter Backward: Error delta_parameters is NULL
BNNS Fused Filter Backward: Error wrong filter, this function should only be used for a filter created with BNNSFilterCreateFusedLayer
BNNS Fused Filter Backward: Error - fused compute and quantization gradient is not support 
BNNS Fused Filter Backward: Error Weight delta pointer is NULL. backward computation of all active parameters must be done in a single function call. 
BNNS Fused Filter Backward: Error bias delta pointer is NULL. backward computation of all active parameters must be done in a single function call. 
BNNS Fused Filter Backward: Error normalization beta delta pointer is NULL. backward computation of all active parameters must be done in a single function call. 
BNNS Fused Filter Backward: Error normalization gamma delta pointer is NULL. backward computation of all active parameters must be done in a single function call. 
BNNS Arithmetic Backward: failed to allocate memory
BNNS Fused Filter Backward: Error normalization backward failed
BNNSDirectApplyConvolutionBatch not supported 
BNNSDirectApplyTransposedConvolutionBatch not supported 
BNNSDirectApplyPoolingBatch not supported 
BNNSDirectApplyLossBatch not supported
BNNS Apply: filter or output can't be null
BNNS Apply: Tensor Contraction filter expects more than one input
BNNS Apply: Loss filter apply must be called with BNNSLossFilterApplyBatch
BNNS Apply: Batchnorm filter apply must be called with BNNSNormalizationFilterApplyBatch
BNNS Apply: Instance Norm filter apply must be called with BNNSNormalizationFilterApplyBatch
BNNS Apply: Layer Norm filter apply must be called with BNNSNormalizationFilterApplyBatch
BNNS Apply: Group Norm filter apply must be called with BNNSNormalizationFilterApplyBatch
BNNS Apply: Fused filter apply must be called with BNNSFusedFilterApplyBatch
BNNS Apply: invalid filter
BNNS DESTROY: invalid filter
Convolution
FullyConnected
Pooling
Activation
SparseFullyConnected
Loss
Batchnorm
Instancenorm
Layernorm
Groupnorm
Fused{Compute,Normalization}
Dropout
Contraction
Resize
LSTM
Arithmetic
Copysum
MultiheadAttention
Reduction
Padding
Embedding
Fused{Compute,Quantization}
Fused{Arithmetic,Normalization}
Fused{Affine Grid, Grid Sample}
Unknown
BNNS Padding + Convolution Apply Backward: input is null
BNNS Padding + Convolution Apply Backward: allocation failed
BNNS Padding + Convolution Apply Backward: Padding failed
BNNS Padding + Convolution Apply Backward: Convolution backward failed
BNNS Padding + Convolution Apply Backward: Padding backward failed
malloc failed
Convolution failed
anon_context
manifest_literal
internal.manifest_literal
_literal
program
sorted_outputs = 
outputs = 
missing from sorted_outputs:
missing from outputs:
BNNS Band Part: illegal input descriptor
BNNS Band Part: illegal output descriptor
BNNS Band Part: input and output data type or dimension do not match or the dimension is less than two
BNNS Band Part: input and output sizes do not match
BNNS Convolution Variant: input type %s not supported!!!
BNNS Convolution Variant: compute type %s not supported!!!
BNNS GRU Fused Gates: Output pointer is null
BNNS GRU Fused Gates: Input pointer is null
BNNS GRU Fused Gates: Recurrent pointer is null
BNNS GRU Fused Gates: Hidden Input pointer is null
BNNS GRU Fused Gates: Input vector size %zu foesn't match output vector size %zu
BNNS GRU Fused Gates: Recurrent vector size %zu foesn't match output vector size %zu
BNNS GRU Fused Gates: Hidden Input vector size %zu foesn't match output vector size %zu
BNNS GRU Fused Gates: number of gates for Input %zu and Recurrent %zu doesn't match
BNNS GRU Fused Gates: Input stride %zu is smaller than number of elements %zu
BNNS GRU Fused Gates: Recurrent stride %zu is smaller than number of elements %zu
BNNS GRU Fused Gates: Output supported data types are fp32 or fp16
BNNS GRU Fused Gates: Input data type doesn't match output data type
BNNS GRU Fused Gates: Recurrent data type doesn't match output data type
BNNS GRU Fused Gates: Hidden Input data type doesn't match output data typen
BNNS GRU Fused Gates: GRU fused gates doesn't support %zu gates
BNNS GRU Fused Gates: Hidden Input batch size %zu doesn't match output batch size %zu
BNNS GRU Fused Gates: Input batch size %zu doesn't match output batch size %zu
BNNS GRU Fused Gates: Hidden Output batch stride %zu is small then a single vector stride
BNNS GRU Fused Gates: Hidden Input batch stride %zu is small then a single vector stride
BNNS GRU Fused Gates: Input batch stride %zu is small then a single vector stride
BNNS GRU Fused Gates: Recurrent batch stride %zu is small then a single vector stride
BNNS CONVOLUTIONS VERSION2: unsupported src type to pack
BNNS Convolution Compute Weights delta: failed to compute
BNNS Convolution Apply Backward: activation_grad allocation failed
weight delta backward failed
BNNS Convolution: int32 repack allocation failed
BNNS Convolution: Float32 repack allocation failed
 = copy(from=
: BNNS Graph copy from 
 -> 
 is not supported
BNNS Activation Apply: Error input/output/activation pointers must be non-NULL
BNNS Activation Apply: Input and output tensor doesn't match
BNNS Activation Apply: unable to allocate memory to compute activation
BNNS Activation Apply: failed to allocate memory
BNNS Apply Activation Backward: BNNSActivationFunctionIntegerLinearSaturate isn't supported
BNNS Apply Activation Backward: BNNSActivationFunctionIntegerLinearSaturatePerChannel isn't supported
BNNS Apply Activation Backward: failed to allocate temporary y
BNNS Apply Activation Backward: unsupported activation backward
BNNS Activation backward: malloc failed
BNNS Activation: number of memory descriptor is lower than expected
BNNS Activation: claiming pre allocated memory failed
BNNS Activation: memory allocation failed
BNNS Activation: Failed to init filter
BNNS Activation Apply Backward: At least one of in or out must be non-NULL
BNNS Activation Apply Backward: invalid filter
BNNS Activation Apply Backward: out_delta must not be NULL
BNNS Activation Backward: preallocated memory isn't supported
BNNS Activation Apply Backward: Input is required
BNNS Activation Backward: Weights Delta not supported
BNNS Activation Apply Backward: Bias Delta not supported
BNNS Activation Apply Backward: in place operation for none matching data sizes isn't supported
BNNS Activation Apply Backward: Input and output tensor doesn't match
BNNS Activation Apply Backward: In Delta tensor doesn't match
BNNS Activation Apply Backward: Out Delta tensor doesn't match
BNNS Activation Apply Backward: Weights Delta tensor (%zu x %zu) doesn't make sense
BNNS Activation Apply Backward: unable to allocate memory to compute backward with missing original output
BNNS Activation Apply Backward: unable to allocate memory to compute activation, using slower compute path
BNNS Activation Apply Backward: failed to allocate memory
BNNS Activation Apply Backward: failed to init index counter
BNNS Activation: invalid argument
BNNS Activation: in-place activation layer is allowed only for output types with the same or smaller storage size
BNNS Activation: unsupported types for conversion
BNNS Activation: val doesn't make sense
BNNS Activation Apply: activation function not supported
BNNS Activation Init: filter is null
BNNS Activation Init: input descriptor is illegal
BNNS Activation Init: output descriptor is illegal
BNNS Activation Init: layout doesn't match
BNNS Activation Init: memory allocation failed
BNNS Activation Init: dim %zu input size %zu != %zu output size
BNNS Activation Init: alpha can't be +/-inf or Nan for Gumbel or Gumbel Max
BNNS Activation Init: beta can't be +/-inf or Nan or zero or negative for Gumbel or Gumbel Max
BNNS Activation Init: BNNSActivationFunctionPReLUPerChannel is only valid with data layout BNNSDataLayoutImageCHW
BNNS Activation Init: invalid activation function
BNNS SoftMax: sum result shouldn't be 0
BNNS Activation Apply Backward: Softmax backward require original output
BNNS Activation Apply Backward: original inputs or original outputs must be available
BNNS Activation Apply Backward: activation function not supported
BNNS Activation: Error NULL input/output pointer
BNNS Activation: input and output dimensions do not match
BNNS Activation: input and output sizes do not match
Unsupported layout: %d
BNNS Multihead Attention Apply: key_mask must be a 1D tensor.
BNNS Multihead Attention Apply: key_mask must have type BNNSDataTypeBoolean.
BNNS Multihead Attention Apply: key_mask must have size exactly source_length (key_mask.size[0] = %zu, source_length = %zu).
BNNS Multihead Attention Apply: 2D add_to_attention must have shape (target_length, source_length) = (%zu, %zu), but passed tensor has shape (%zu, %zu).
BNNS Multihead Attention Apply: 3D add_to_attention must have shape (num_heads, target_length, source_length) = (%zu, %zu, %zu), but passed tensor has shape (%zu, %zu, %zu).
BNNS Multihead Attention Apply: 4D add_to_attention must have shape (batch_size, num_heads, target_length, source_length) = (%zu, %zu, %zu, %zu), but passed tensor has shape (%zu, %zu, %zu, %zu).
BNNS Multihead Attention Apply: unsupported layout for argument add_to_attention.
BNNS Multihead Attention Apply: add_to_attention must have type BNNSDataTypeBoolean or BNNSDataTypeFloat32.
BNNS Multihead Attention Apply: If backprop_cache is non-NULL, backprop_cache_size must also be non-NULL
BNNS Multihead Attention Apply: If workspace is non-NULL, workspace_size must also be non-NULL
BNNS Multihead Attention Apply: Supplied workspace is too small (size %ld, but required %ld)
Multihead Attention Backward: Only support for a full-size backprop_cache has been implemented.
Multihead Attention Backward: If workspace is non-NULL, workspace_size must also be non-NULL
Multihead Attention Backward: Insufficient workspace supplied (%zu bytes). Require %zu bytes.
value_delta_copy_out
value_shadow_delta
value_delta
key_delta_copy_out
key_shadow_delta
key_delta
query_delta_copy_out
query_shadow_delta
query_delta
layer_params->query.target_desc
layer_params->key.target_desc
layer_params->query.weights
layer_params->key.weights
layer_params->value.weights
(d_model, d_key, num_heads)
(source_length, k_dim)
(k_dim, d_key, num_heads)
layer_params->value.target_desc
(source_length, v_dim)
(v_dim, d_value, num_heads)
layer_params->output.target_desc
(target_length, d_model)
layer_params->output.weights
(num_heads*d_value, d_model)
BNNS Multihead Attention Create: layer_params->key_attn_bias.data and layer_params->value.attn_bias.data must both be NULL or must both be not NULL
layer_params->key_attn_bias
(d_key, num_heads)
layer_params->value_attn_bias
(d_value, num_heads)
BNNS Multihead Attention Create: fp16 and bf16 may not be mixed or unsupported data type
layer_params->query.bias
layer_params->key.bias
layer_params->value.bias
layer_params->output.bias
(d_model)
BNNS MHA: layer_params->query.weights.layout=%u is not compatible with low memory flag.
BNNS MHA: layer_params->key.weights.layout=%u is not compatible with low memory flag.
BNNS MHA: layer_params->value.weights.layout=%u is not compatible with low memory flag.
BNNS Multihead Attention Create: Unsupported layout for query.weights
BNNS Multihead Attention Create: Unsupported layout for key.weights
BNNS Multihead Attention Create: Unsupported layout for value.weights
BNNS MULTIHEAD ATTENTION GetPointer: Layer was created with dropout=0.0, cannot return pointer to dropout value.
BNNS MULTIHEAD ATTENTION GetPointer: Unsupported target %d
BNNS Multihead Attention Create: Expected %s to have exactly %zu dimensions, but it has %zu
BNNS Multihead Attention Create: Expected %s to have shape %s = (
) but has actual shape (
 = reshape(from=
BNNS SPECIALIZED CONVOLUTION Create: failed to alloacted memory
%s: 
BNNS Permute Filter: layer_params is NULL
BNNS Permute Filter: illegal input descriptor
BNNS Permute Filter: illegal output descriptor
BNNS Permute Filter: input and output descriptor layout dimension does not match. input dimm = %zu output dimm = %zu
BNNS Permute Filter: permutation array index %zu has illegal value of %zu
BNNS Permute Filter: permutation array is missing axis %zu
BNNS Permute Filter: permutation array axis %zu appears %zu times, each axis should appear exactly once
0123456789abcdefghijklmnopqrstuvwxyz
BNNS Shuffle: input and output data type do not match
BNNS Shuffle: input or output data is NULL
BNNS Shuffle: input and output dimensions should be 4
BNNS Shuffle: input and output sizes are not valid
BNNS Shuffle: input and output sizes do not match
BNNS Pooling Filter: filter is NULL
BNNS Pooling Filter: input is NULL
BNNS Pooling Filter: output is NULL
BNNS Pooling Filter Warning: Batch size is 0, nothing to do
BNNS Pooling: input batch stride doesn't make sense (%zu < %zu x %zu)
BNNS Pooling: dilation supported only in BNNSPoolingFunctionMax/UnMax
BNNS Pooling UnMax: indices array is NULL
Pooling layer filter running slow path: stride=%zu,%zu kernel=%zu,%zu
BNNS Pooling: case not implemented
BNNS Pooling Apply: allocation of work buffer failed
BNNS Pooling Backward: cannot run backward without output delta
BNNS Pooling Backward: only float32 bfloat16 output delta are supported
BNNS Pooling Backward: only float32 bfloat16 input delta are supported
BNNS Pooling Backward: only float32 bfloat16 bias delta are supported
BNNS Pooling Backward: unsupported pooling function
BNNS Pooling Backward: invalid argument
BNNS Pooling Backward: Fusion of compute with activation is unsupported for given activation function
BNNS Pooling Backward: failed to allocate memory
BNNS Pooling Backward: failed to apply activation backward
BNNS Pooling: layer parameters is NULL
BNNS Pooling: input must be a 3D array
BNNS Pooling: input descriptor is illegal
BNNS Pooling: output must be a 3D array
BNNS Pooling: output descriptor is illegal
BNNS Pooling: input/output channel counts do not match
BNNS Pooling: input/output types do not match
BNNS Pooling: invalid kernel dimensions, should be greater than 0
BNNS Pooling: optimized code supports kernel width/height up to 16
BNNS Pooling: dilation only supported for BNNSPoolingFunctionMax/UnMax
BNNS_POOLING: input size computation wraparound input (%zu x %zu), output (%zu x %zu), stride (%zu x %zu), padding (left %zu right %zu up %zu down %zu)
BNNS_POOLING: input size computation wraparound input (%zu x %zu), output (%zu x %zu), stride (%zu x %zu), padding (%zu x %zu)
BNNS_POOLING: output(%zu x %zu) is larger than input(%zu x %zu) with padding(left %zu right %zu up %zu down %zu).
 stride (%zu x %zu)
BNNS_POOLING: output(%zu x %zu) is larger than input(%zu x %zu) with padding(%zu x %zu).
 stride (%zu x %zu)
BNNS_POOLING: invalid pooling function
BNNS_POOLING: supported input/output data types: float32 float16 bfloat16
BNNS_POOLING: slow path: stride not in {1,2}
BNNS_POOLING: slow path: kernel size not in {2,3,4}
BNNS Pooling: Internal Memory size %zu doesn't match expected %d
BNNS Pooling: claiming pre allocated memory failed
BNNS Pooling: failed to allocate context
BNNS Pooling Backward: only float32 bfloat16 delta are supported
BNNS Pooling Backward: wrong pooling function called
 = stack(values=[
BNNS Reduction Create: Invalid reduction function %u
BNNS Reduction Create: weight data type does not match input
BNNS Reduction Create: i_desc and o_desc must have the same number of dimensions (%zu vs %zu)
BNNS Reduction Create: i_desc and w_desc must have the same number of dimensions (%zu vs %zu)
BNNS Reduction Create: i_desc and w_desc dimensions must have the same size (dimension %zu: %zu vs %zu)
BNNS Reduction Create: i_desc and o_desc dimensions must have the same size, or o_desc.size[d] must be 1 to indicate reduction (dimension %zu: %zu vs %zu)
BNNS Reduction Create: ArgMin/ArgMax reduction is only supported on a single axis
BNNS Reduction Apply: Both input and output must non-NULL
BNNS Reduction Backward: input_delta must have the same number of dimensions as i_desc
BNNS Reduction Backward: input_delta must have the same shape as i_desc
BNNS Reduction Backward: output_delta must have the same number of dimensions as o_desc
BNNS Reduction Backward: output_delta must have the same shape as o_desc
BNNS Reduction Backward: output_delta must not be NULL
BNNS Reduction Backward: weights_delta must have the same number of dimensions as w_desc
BNNS Reduction Backward: weights_delta must have the same shape as w_desc
BNNS Reduction Direct Apply: Size of the dimensions of the output can not be greater than the size of the input
BNNS Reduction Direct Apply: ArgMin/ArgMax reduction is only supported on a single axis
BNNS Reduction: Unsupported data type combination for %u
BNNS Reduce Apply Generic: Size mismatch in dimension %zu: input %zu, output %zu
Backward usage is not supported for reduction type %u
BNNS Reduction Backward: Reduce function %u requires input to be supplied for backward computation
BNNS Reduction Backward: Reduce function %u requires output to be supplied for backward computation
BNNS Reduction Backward: Calculation of weights_delta requires input to be supplied
Size mismatch in dimension %zu: input %zu, output %zu
BNNS Sparse Fully Connected: out of scratch memory to encode
BNNS Fully Conneceted Sparsify: Sparse data is greater than dense prune, developer should use dense prune
BNNS Sparse Fully Connected: failed to apply activation
BNNS Arithmetic Filter Internal: unsupported arithmetic with epsilon function
BNNS Arithmetic Filter: layer_params is NULL
BNNS Arithmetic Filter: unsupported arithmetic function
BNNS Arithmetic Filter: memory allocation failed
BNNS Arithmetic Filter: Failed to init filter
BNNS Arithmetic Filter: filter is NULL
BNNS Arithmetic Filter: wrong filter type, this function should only be used for a filter created with BNNSFilterCreateLayerArithmetic
BNNS Arithmetic Filter: in pointer is NULL
BNNS Arithmetic Filter: batch_size>1 and in_stride is 0
BNNS Arithmetic Filter Internal: expected number of inputs call failed
BNNS Arithmetic Filter: wrong number_of_inputs, number_of_inputs=%zu, expecting %zu
BNNS Arithmetic Filter: in[%zu] is NULL
BNNS Arithmetic Filter: in_stride[%zu] is 0
BNNS Arithmetic Filter: output pointer is NULL
BNNS Arithmetic Filter: batch_size>1 and out_stride is 0
BNNS Arithmetic Filter: only BNNSDataTypeFloat32/BFloat16/Float16 is supported
BNNS Arithmetic Filter: only BNNSDataTypeFloat32/BNNSDataTypeBFloat16 is supported
BNNS Arithmetic Filter Internal: unsupported arithmetic function
BNNS Arithmetic Filter: out delta is not valid
BNNS Arithmetic Filter: in_delta is not valid
BNNS Arithmetic Filter: in is NULL, but it is required for backward compute
BNNS Arithmetic Filter: in_delta descriptor %zu is not valid
BNNS Arithmetic Filter: cannot compute activation backward, output is NULL
BNNS Arithmetic Filter: Fusion of arithmetic with activation is unsupported for given activation function
BNNS Arithmetic Filter Backward Internal: unsupported arithmetic function
BNNS Arithmetic Filter Backward: failed to apply activation backward
BNNS Arithmetic Filter: input type is BNNSConstant, no gradient to compute
BNNS Arithmetic Filter: in_delta[0] must be NULL for BNNSArithmeticSelect
BNNS Arithmetic Filter: arithmetic_function_fields is NULL
BNNS Arithmetic Filter: malloc failed
BNNS Arithmetic Filter: unsupported out_type/in_type
BNNS Arithmetic Filter: in, out descriptor check failed
BNNS Arithmetic Filter: in1,in2,out descriptor check failed
BNNS Arithmetic Filter: in1,in2,in3,out descriptor check failed
BNNS Arithmetic: Activation creation failed
BNNS Arithmetic Filter: input size[%zu]=%zu does not equal max input,output size of %zu or 1
BNNS Arithmetic Filter: input%zu and input%zu have the same data pointer, but descriptors are different
BNNS Arithmetic Filter: input%zu and input%zu have the same data pointer, but different BNNSDescriptorType
BNNS Arithmetic Filter: descriptors for input that is being processed in-place and output must exactly match
BNNS Arithmetic Filter: input is being processed in-place but input and output descriptor types do not match
BNNS Arithmetic Filter: output dimension smaller than input is not supported. batch size>1, input type is BNNSSample but out_type is not BNNSSample
BNNS Arithmetic Filter: backward compute for chosen arithmetic function does not allow inplace gradient. out delta data must not be the same as input delta data
BNNS Arithmetic Filter: in_delta%zu is NULL but in%zu type is not BNNSConstant
BNNS Arithmetic Filter: in_delta%zu->data is NULL but in%zu type is not BNNSConstant
BNNS Arithmetic Filter: in%zu type is BNNSSample but in_delta_stride[%zu] is 0
BNNS Arithmetic Filter: arithmetic input_delta and input descriptors must have the same sizes, strides and data types
BNNS Arithmetic Filter: input%zu delta data pointer is not NULL. cannot compute gradient for BNNSConstant
BNNS Arithmetic Filter: arithmetic output_delta and output descriptors must have the same sizes, strides and data types
BNNS Arithmetic Filter: input%zu delta and input%zu delta has the same data pointer, but descriptors are different
BNNS Arithmetic Filter: input%zu delta and input%zu delta has the same data pointer, but different data types
BNNS Arithmetic Filter: input%zu delta and input%zu delta has the same data pointer, but different in_delta_stride
BNNS Arithmetic Filter: forward input pointer points to the same array but input gradient pointers point to different arrays
BNNS Arithmetic Filter: descriptors for out_delta that is being processed in-place and in1_delta must exactly match
BNNS Arithmetic Filter: input1 delta is being processed in-place but input type and output type are different
BNNS Arithmetic Filter: input1 delta and output delta has the same data pointer (inplace), but different in_delta_stride
BNNS Arithmetic Filter: input1 delta and output delta has the same data pointer (inplace), but in1_delta.flags indicates to accumulate result
BNNS Arithmetic Filter: output dimension smaller than input is not supported. batch size>1, an input type is BNNSSample but out_type is not BNNSSample
BNNS Fully Connected Sparsify: failed to allocate scratch buffer
BNNS Crop Resize: data types do not match
BNNS Crop Resize: dimension of input, roi, or output is not correct
BNNS Crop Resize: size of roi is not [N, 4] or [N, 5]
BNNS Crop Resize: size of output is not [N, B, C, OH, OW] or [N, 1, C, OH, OW]
BNNS Crop Resize: only linear method is supported now
BNNS Crop Resize: Unsupported data type
BNNS Crop Resize: sampling mode not supported
BNNS Embedding Create: Only 8, 16, 32 and 64 bit integer types may be used as input data type.
BNNS Embedding Create: Only 16 and 32 bit floating point and 8, 16 and 32 bit integer types may be used as dictionary data type.
BNNS Embedding Create: o_desc dimension must be dim(i_desc) + dim(dictionary) - 1.
BNNS Embedding Create: First dim(dictionary)-1 dimensions of o_desc and dictionary must have consistent shapes.
BNNS Embedding Create: Last dim(i_desc) dimensions of o_desc and i_desc must have consistent shapes.
internal embedding copy
internal embedding copy back
BNNS Embedding Apply: Input value %zu is out of range [0, %zu)
BNNS Embedding ApplyBackward: weights_delta must have the same shape as layer_params->dictionary
BNNS Embedding ApplyBackward: out_delta must have the same shape as layer_params->o_desc
BNNS Embedding ApplyBackward: Input value %zu out of range
BNNS Sparse Embedding Create: Only 8, 16, 32 and 64 bit integer types may be used as input data type.
BNNS Sparse Embedding Create: Only 16 and 32 bit floating point and 8, 16 and 32 bit integer types may be used as dictionary data type.
BNNS Sparse Embedding Create: o_desc dimension must be dim(i_desc) + dim(dictionary) - 1.
BNNS Sparse Embedding Create: First dim(dictionary)-1 dimensions of o_desc and dictionary must have consistent shapes.
BNNS Sparse Embedding Create: Last dim(i_desc) dimensions of o_desc and i_desc must have consistent shapes.
BNNS Sparse Embedding Create: Unsupported optimization function
BNNS Sparse Embedding Create: Dictionary must be of type BNNSDataTypeFloat32
BNNS Sparse Embedding Apply: Filter is no longer valid
BNNS Sparse Embedding Apply: Input value %zu is out of range [0, %zu)
BNNS Sparse Embedding Apply Backwards: Filter is no longer valid
BNNS Embedding Backwards Sparse: Accumulation of sparse weights gradient is not supported.
BNNS Embedding Backwards Sparse: out_delta must have the same shape as layer_params->o_desc
BNNS Embedding Backwards Sparse: dictionary_delta->indices must have a single dimension
BNNS Embedding Backwards Sparse: dictionary_delta->indices.size[0] must be 1
BNNS Embedding Backwards Sparse: dictionary_delta->sparse_dimension_size[0] must be num_embeddings
BNNS Embedding Backwards Sparse: dictionary_delta->values shape must match dictionary item shape
BNNS Embedding Backwards Sparse: For in-place indices, dictionary_delta->count must equal the total size of layer_params->i_desc * batch_size
BNNS Embedding Backwards Sparse: For in-place indices, input must be contiguous
BNNS Embedding Backwards Sparse: For in-place values, dictionary_delta->count must equal the total size of out_delta * batch_size
BNNS Embedding Backwards Sparse: For in-place values, out_delta must be contiguous in its indexing dimensions
BNNS Sparse Embedding Get Representation: Filter is no longer valid
BNNS Sparse Embedding Get Representation: Unexpected value for num_accumulators=%zu (expected %zu)
BNNS Sparse Embedding Get Representation: num_opt_field_changes!=NULL but opt_field_changes_indices is NULL
BNNS Sparse Embedding Get Representation: num_opt_field_changes!=NULL but opt_field_changes is NULL
BNNS Sparse Embedding Get Representation: num_opt_field_changes=NULL, but optimization fields of filter have previosuly been updated
BNNS Sparse Embedding Set Representation: Filter is no longer valid
BNNS Embedding Set Sparse Optimizer Context: Expected %zu accumulators
BNNS Embedding Set Sparse Optimizer Context: values[0]->indices must have dimension 1
BNNS Embedding Set Sparse Optimizer Context: values[0]->indices.size[0] must be 1
BNNS Embedding Set Sparse Optimizer Context: Expected values indices descriptors to be identical (that is point to the same data)
BNNS Embedding Set Sparse Optimizer Context: Expected values indices descriptors to be identical
dest
BNNS Sparse Embedding Optimizer Step: Filter is no longer valid
BNNS Embedding Sparse Optimizer Step: Expected dictionary_delta->indices to have a single dimension
BNNS Embedding Sparse Optimizer Step: Expected dictionary_delta->values to match shape of dictionary
BNNS Embedding Sparse Optimizer Step: Dictionary gradient items must be contiguous
BNNS Embedding Sparse Optimizer Step: Dictionary items must be contiguous
BNNS Embedding Sparse Optimizer Step: grad.data_type must be BNNSDataTypeFloat32
BNNS Sparse Embedding Get Dense: Filter is no longer valid
BNNS Sparse Embedding Get Dense: Unexpected value for num_accumulators=%zu (expected %zu)
BNNS Sparse Embedding Get Dense: Unexpected number of dimensions for accumulator[%zu]
BNNS Sparse Embedding Get Dense: Unsupported data type for accumulator[%zu]
BNNS Sparse Embedding Get Dense: Unexpected shape for accumulator[%zu]
BNNS Sparse Embedding Get Dense: accumulator[%zu] must be contiguous in dictionary item dimensions
BNNSActivationFunctionIntegerLinearSaturate isn't supported in apply_bias_and_activation
BNNSActivationFunctionIntegerLinearSaturatePerChannel isn't supported in apply_bias_and_activation
BNNSActivationFunctionIntegerLinearSaturate isn't supported in apply_bias_and_activation_fp16
BNNSActivationFunctionIntegerLinearSaturatePerChannel isn't supported in apply_bias_and_activation_fp16
BNNSActivationFunctionIntegerLinearSaturate isn't supported in apply_bias_and_activation_bf16
BNNSActivationFunctionIntegerLinearSaturatePerChannel isn't supported in apply_bias_and_activation_bf16
allocation failed, size=%zu
reallocation failed, size=%zu
allocation failed, size=%zu, align=%zu
BNNS: unexpected data type, failing
BNNS: Data type not supprted, fail
BNNS: layout not supported
BNNS: active dimension must be greater than 0
BNNS: dimension %zu stride %zu is lower then previous dimension actual size %zu * %zu (size*stride)
BNNS: Opaque data structure doesn't match descriptor information
BNNS: Opaque data structure has incorrect information
BNNS: memory usage exceeded capacity
BNNS PreAllocated Memory: memory usage exceeded capacity
BNNS PreAllocated Memory: failed to claim scratch memory
BNNS Affine Resize: layer parameter is not supported
BNNS Affine Resize: input data type or size is not supported
BNNS Affine Resize: affine data type or size is not supported
BNNS Affine Resize: output data type or size is not supported
BNNS Create Convolution Winograd: malloc failed
BNNS Convolutions Winograd: weights allocation failed
BNNS Convolutions Winograd: bias allocation failed
BNNS Apply Convolution Winograd: memory allocation failed
weight packing must be at least 32 and a power of 2
repacked(
, for=
conv_input_slice(
 = conv(x=
, weights=
, bias=
<none>
, strides=[
], pad=[
], dilation=[
], groups=
UNKNOWN
OPAQUE
STRING
: BNNS Graph only supports FP16 and FP32 operands for this function
Rejected constexpr elimination of op 
 as it increased data footprint by 
 bytes
Constexpr elimination of op 
 would decrease weight footprint, but is not yet supported
Unsupported repack operation from layout=%u to layout=%u
BNNSCopy: Unsupported repack operation from layout=%u to layout=%u
BNNSCopy: src->size[%zu]=%zu is not equal to dest->size[%zu]=%zu
BNNSCopy: Cannot perform type conversion as part of opaque repack
temp_desc
hw.physicalcpu
BNNS Dequantize: shouldn't have reached fp16 convert path
 = slice_by_index(x=
, begin=[
], end=[
], stride=[
BNNS Normalization Apply Backward: input gradient data type not supported
BNNS batch norm forward: malloc for mean failed
BNNS batch norm forward: malloc for var failed
BNNS batch norm forward: malloc for isqrtvar failed
BNNS Batchnorm Backward Apply: delta allocation failed
BNNS Batchnorm Backward Apply: activation_grad allocation failed
BNNS instance norm backward: malloc for dbeta failed
BNNS instance norm backward: malloc for dgamma failed
BNNS Layer Norm Apply Backward: malloc activation_grad failed
BNNS Layer Norm Apply Backward: dx hat allocation failed
BNNS Group Norm Apply Backward: failed to allocate memory
BNNS Group Norm Apply Backward: failed to allocate activation grad memory
BNNS Normalization Apply Backward: malloc activation_scratch failed
BNNS instance norm backward: malloc for dbeta and dgamma failed
' is not a tensor
' has a non-constant dimension
BNNS Graph Compile: Unsupported data type
BNNS Graph Compile: Attempted to parse unsupported op '%s' (reversed name '%s'
BNNS Convolution Apply: input batch stride doesn't make sense (%zu < %zu x %zu)
BNNS Convolution Apply: output batch stride doesn't make sense (%zu < %zu x %zu)
BNNS Convolution Apply: failed to allocate weights in apply
BNNS Convolution Backward: BNNSNDArrayFlagBackpropAccumulate has not yet been implemented for weights_delta or bias_delta.
BNNS Convolution Backward: cannot run backward without output delta
BNNS Convolution Backward: [out delta/in delta/weight delta/bias delta] - data type check failed. unsupported data type
BNNS Convolution Backward: internal error, incorrect wrapper
BNNS Convolution forward output data type does not match output delta data type
BNNS Convolution forward input data type does not match input delta data type
BNNS Convolution forward weights data type does not match weights delta data type
BNNS Convolution weight packing is not supported backward
BNNS Convolution Backward: BNNSFlagsUseClientPtr must be enabled during training
BNNS Convolution Backward: invalid argument
BNNS Convolution Backward: Fusion of compute with activation is unsupported for given activation function
BNNS Convolution Backward: failed to allocate memory
BNNS Convolution Backward: failed to apply activation backward
BNNS Conv: Unsupported weight format for Input delta compute
BNNS Convolution Apply Backward: could not create transposed convolution filter
BNNS Convolution Apply Backward: Convolution input delta computation failed
BNNS Transposed Convlution Apply: failed to allocate weights buffer
BNNS Transposed Convlution Apply: failed to allocate backward weights buffer
BNNS Transposed Convlution Apply: failed to allocate out delta buffer
BNNS Transposed Convlution Apply: failed to manipulate output delta
BNNS Transposed Convlution Apply: could not create convolution filter
BNNS Convolution Apply Backward: allocation failed
BNNS Convolution Apply Backward: Transposed Convolution Backward Compute failed
transposed convolution parameter error
transposed convolution backward failed
Convolution bias delta failed
BNNS Create Layer Convolution: fused padd convolution allocation failed
BNNS Create Layer Convolution: padding layer creation failed
BNNS Create Layer Convolution: padding descriptor creation failed
BNNS Create Layer Convolution: input channels must be divisible by groups
BNNS Create Layer Convolution: output channels must be divisible by groups
BNNS Create Layer Convolution: bias must match output channels
BNNS Create Layer Convolution: grouped convolution weight layout must be BNNSDataLayoutConvolutionWeightsOIHW
BNNS Create Layer Convolution: failed to allocate memory to copy weights and bias
BNNS Create Layer Convolution: failed to create forward convolution filter
BNNS Create Layer Convolution: failed to create wrapper filter
BNNS Convolution Create: incompatible numbers of channels between images and convolution parameters
BNNS Convolution Create: unsupported weight format
BNNS Convolution Create: input must be a 3D array
BNNS Convolution Create: input descriptor is illegal
BNNS Conv: output must be a 3D array
BNNS Convolution Create: output descriptor is illegal
BNNS Convolution Create:  weights descriptor is illegal
BNNS Convolution Create: not allowed to delay allocation to apply if weights ptr isn't maintained by Client
BNNS Convolution Create: failed to create dilated convolution
BNNS Convolution Create: failed to create packed weights convolution
BNNS Convolution Create: failed to create grouped convolution
BNNS Convolution Create: failed to allocate weights buffer
BNNS Convolution Create: failed to allocate bias buffer
BNNS Convolution Create: input data type is not supported
BNNS Convolution Create: output data type is not supported
BNNS Convolution Create: weight data type is not supported
BNNS Convolution Create: int8/uint8 output supported only with int8/uint8 inputs and weights
BNNS Convolution Create: int16/uint16 output is not supported
BNNS Convolution Create: uint32 output is not supported
BNNS Convolution Create: convolution doesn't support indexed weights
BNNS Convolution Create: failed to allocate memory
BNNS Convolution Create: failed to allocate weights descriptor
BNNS Convolution Create: convert weights
BNNS Convolution Create: failed to prepare blocks
BNNS Convolution Create: failed to create compute blocks
BNNS Grouped Convolution apply: groups should not be used if groups <= 1
BNNS Grouped Convolution apply: weights are null 
BNNS Grouped Convolution apply: failed to create context
BNNS Grouped Convolution apply: unsupported convolution filter type 
BNNS Convolution Apply Backward:  filter is NULL
BNNS Convolution Apply Backward: out_delta is NULL
BNNS Convolution weight delta: malloc failed
BNNS Convolutions Apply: unexpected output data type
BNNS Convolution Backward: out_delta descriptor memory layout is not contiguous
BNNS Convolution bias delta: malloc failed
BNNS Convolution bias delta: upconverting tensors failed
BNNS Convolution: malloc activation_grad failed
Convolution backward: in_delta, weights_delta, bias_delta descriptor pointers are all NULL
BNNS Convolution Backward: in_delta descriptor memory layout is not contiguous
BNNS Convolution Backward: weights_delta descriptor memory layout is not contiguous
BNNS Convolution Backward: bias_delta descriptor memory layout is not contiguous
BNNS Convolution Backward: out_stride and out_delta_stride do not match
BNNS Convolution Backward: out_delta_stride [%zu] does not match out_delta descriptor memory size [%zu]
BNNS Convolution Backward: in_stride and in_delta_stride do not match
BNNS Convolution Backward: in_delta_stride [%zu] does not match in_delta descriptor memory size [%zu]
BNNS gradient computation for Transposed Convolution with groups is not supported
BNNS: unsupported resize forward data type
BNNS Resize: input_delta has %zu dimensions but must match input that has %zu
BNNS Resize: output_delta has %zu dimensions but must match output that has %zu
BNNS Resize: input_delta size[%zu]=%zu but must match input size[%zu]=%zu
BNNS Resize: output_delta size[%zu]=%zu but must match output size[%zu]=%zu
BNNS: unsupported resize backward data type
BNNS Resize: Unsupported data type
BNNS Resize: Input and output have different number of dimensions
BNNS Resize: resize must be in same direction for all direction (request %d downsample and %d upsample)
BNNS Resize: Linear interpolation requires resize in at most two dimensions, but %d dimensions are resized
BNNS Resize: Unsupported interpolation method %d
Downsampling not supported for dimension > 2.
BNNS Resize: downsampling has no support for 3 or more dimensions.
BNNS Resize: downsampling has no support for 3 or higher dimension.
BNNS CONVOLUTIONS CUSTOMIZED: layer param is NULL
BNNS CONVOLUTIONS CUSTOMIZED: malloc failed
BNNS CONVOLUTIONS CUSTOMIZED: malloc failed
BNNS CONVOLUTIONS CUSTOMIZED: failed to allocate bias memory
BNNS CONVOLUTIONS CUSTOMIZED: failed to allocate input repack memory
%{name=layer}s %{name=direction}s
BNNSDirectApply
Pooling
%{name=direction}s
Quantizer
ClipByValue
ClipByGlobalNorm
ComputeNorm
SparseEmbedding
SparseEmbeddingOptimzierStep
RandomFill
Filter
MultiheadAttention
Loss
Arithmetic
Permute
Normalization
FusedFilter
BNNS Signposts Enabled=%c
J<XJ<
N4bnns14graph_compiler2op17binary_arithmeticE
9!$'*
BN4bnns14graph_compiler2op16unary_arithmeticE
"%)-
OSX\af
a_*ik, b_*kj -> c_*ij
INTERNAL: bwd
N4bnns14graph_compiler2op6concatE
NSt3__110__function6__funcIZNK4bnns14graph_compiler2op6concat15local_optimizerEvE3$_0NS_9allocatorIS6_EEFbRNS3_7ContextEEEE
NSt3__110__function6__baseIFbRN4bnns14graph_compiler7ContextEEEE
ZNK4bnns14graph_compiler2op6concat15local_optimizerEvE3$_0
m[mmm^
D_G___J
NSt3__110__function6__funcIZNK4bnns14graph_compiler8Function11generate_irEvE3$_0NS_9allocatorIS5_EEFiPKNS3_9OperationEEEE
NSt3__110__function6__baseIFiPKN4bnns14graph_compiler9OperationEEEE
ZNK4bnns14graph_compiler8Function11generate_irEvE3$_0
N4bnns14graph_compiler2op4tileE
#58:=@
1VTY\`Q
"%*/
!,7Z
*/6;CK
N4bnns20GraphOptionsInternalE
N4bnns14graph_compiler6TensorE
N4bnns14graph_compiler13TensorLiteralE
N4bnns14graph_compiler23TensorLiteralPalletizedE
N4bnns14graph_compiler19TensorStringLiteralE
N4bnns14graph_compiler10TensorViewE
NSt3__120__shared_ptr_pointerIPhNS_10shared_ptrIA_hE27__shared_ptr_default_deleteIS3_hEENS_9allocatorIhEEEE
NSt3__110shared_ptrIA_hE27__shared_ptr_default_deleteIS1_hEE
NSt3__114default_deleteIA_hEE
NSt3__110__function6__funcIZN4bnns14graph_compiler8Function6layoutEbNS2_12layout_alg_tEE3$_1NS_9allocatorIS6_EEFiPNS3_9OperationEEEE
NSt3__110__function6__baseIFiPN4bnns14graph_compiler9OperationEEEE
ZN4bnns14graph_compiler8Function6layoutEbNS_12layout_alg_tEE3$_1
NSt3__110__function6__funcIZN4bnns14graph_compiler8Function6layoutEbNS2_12layout_alg_tEE3$_2NS_9allocatorIS6_EEFiPNS3_9OperationEEEE
ZN4bnns14graph_compiler8Function6layoutEbNS_12layout_alg_tEE3$_2
N4bnns14graph_compiler2op22elementwise_activationE
NSt3__110__function6__funcIZN4bnns14graph_compiler15fusion_opt_passERNS3_7ProgramEE3$_0NS_9allocatorIS6_EEFiPNS3_9OperationEEEE
ZN4bnns14graph_compiler15fusion_opt_passERNS0_7ProgramEE3$_0
NSt3__117bad_function_callE
LOSW
=ORTWZ
.mkpswN4bnns14graph_compiler7ViewMapE
N4bnns14graph_compiler13DirectViewMapE
N4bnns14graph_compiler13ExpandViewMapE
N4bnns14graph_compiler14SqueezeViewMapE
N4bnns14graph_compiler12SliceViewMapE
N4bnns12GraphOptionsE
bjrz
N4bnns2IR3gen16TensorFirstMajorE
N4bnns2IR3gen6TensorE
N4bnns2IR3gen20TensorViewFirstMajorE
N4bnns14graph_compiler2op7permuteE
LOSW
=ORTWZ
.mkpsw2
"'000,
?fff
(6D7
;>ADGJMPSVY\_
),h,k
n(s|((
[^adgjmpsvy|
^adgjmpsvy|
`cfilorux{~
ilorux{~
X[^adgjmpsvy|
????>
00000J____^
jjjjj
cfilorux{~
_behknqtwz}
N4bnns14graph_compiler7ContextE
N4bnns14graph_compiler7ProgramE
NSt3__110__function6__funcIZNK4bnns14graph_compiler7Context8validateEvE3$_0NS_9allocatorIS5_EEFiPKNS3_9OperationEEEE
ZNK4bnns14graph_compiler7Context8validateEvE3$_0
NSt3__110__function6__funcIZN4bnns14graph_compiler7Context11repack_dataEvE3$_1NS_9allocatorIS5_EEFiPNS3_9OperationEEEE
ZN4bnns14graph_compiler7Context11repack_dataEvE3$_1
NSt3__110__function6__funcIZNK4bnns14graph_compiler8Function18reachable_literalsEvE3$_3NS_9allocatorIS5_EEFiPKNS3_9OperationEEEE
ZNK4bnns14graph_compiler8Function18reachable_literalsEvE3$_3
NSt3__110__function6__funcIZNK4bnns14graph_compiler8Function28reachable_underlying_tensorsEvE3$_4NS_9allocatorIS5_EEFiPKNS3_9OperationEEEE
ZNK4bnns14graph_compiler8Function28reachable_underlying_tensorsEvE3$_4
+@+++C
hekeeen
bVeVVVh
hVhhhY
Y}\}}}_
@[C[[[Fe
hVhhhY
Y}\}}}_
@[C[[[Fi
@L@@@O
VSYSSS\
E3H333K
"%)-
 #'+0
 %,BGLSou{
"%)-
"&*/
?CHJOT[puz
"%(+
N4bnns14graph_compiler2op4copyE
NSt3__110__function6__funcIZNK4bnns14graph_compiler2op4copy15local_optimizerEvE3$_0NS_9allocatorIS6_EEFbRNS3_7ContextEEEE
ZNK4bnns14graph_compiler2op4copy15local_optimizerEvE3$_0
N4bnns14graph_compiler2op7reshapeE
TW\`
hlsv
;=,?
.;?#%
;6=@
WZ_bglsv{
UX]`ejqty~
"',37=C
(3:EP]hw
"',37=C
N4bnns14graph_compiler2op5stackE
NSt3__110__function6__funcIZNK4bnns14graph_compiler2op5stack15local_optimizerEvE3$_0NS_9allocatorIS6_EEFbRNS3_7ContextEEEE
ZNK4bnns14graph_compiler2op5stack15local_optimizerEvE3$_0
2266
%PTX
NSt3__110__function6__funcIZN4bnns5graphL33execute_typed_arithmetic_unary_opIDF16_EEiRKNS2_13GraphInternalEPKvPK34bnns_graph_arithmetic_unary_paramsEUlPDF16_PKDF16_mE_NS_9allocatorISG_EEFvSD_SF_mEEE
NSt3__110__function6__baseIFvPDF16_PKDF16_mEEE
ZN4bnns5graphL33execute_typed_arithmetic_unary_opIDF16_EEiRKNS_13GraphInternalEPKvPK34bnns_graph_arithmetic_unary_paramsEUlPDF16_PKDF16_mE_
NSt3__110__function6__funcIZN4bnns5graphL33execute_typed_arithmetic_unary_opIDF16_EEiRKNS2_13GraphInternalEPKvPK34bnns_graph_arithmetic_unary_paramsEUlPDF16_PKDF16_mE0_NS_9allocatorISG_EEFvSD_SF_mEEE
ZN4bnns5graphL33execute_typed_arithmetic_unary_opIDF16_EEiRKNS_13GraphInternalEPKvPK34bnns_graph_arithmetic_unary_paramsEUlPDF16_PKDF16_mE0_
NSt3__110__function6__funcIZN4bnns5graphL33execute_typed_arithmetic_unary_opIfEEiRKNS2_13GraphInternalEPKvPK34bnns_graph_arithmetic_unary_paramsEUlPfPKfmE_NS_9allocatorISG_EEFvSD_SF_mEEE
NSt3__110__function6__baseIFvPfPKfmEEE
ZN4bnns5graphL33execute_typed_arithmetic_unary_opIfEEiRKNS_13GraphInternalEPKvPK34bnns_graph_arithmetic_unary_paramsEUlPfPKfmE_
NSt3__110__function6__funcIZN4bnns5graphL33execute_typed_arithmetic_unary_opIfEEiRKNS2_13GraphInternalEPKvPK34bnns_graph_arithmetic_unary_paramsEUlPfPKfmE0_NS_9allocatorISG_EEFvSD_SF_mEEE
ZN4bnns5graphL33execute_typed_arithmetic_unary_opIfEEiRKNS_13GraphInternalEPKvPK34bnns_graph_arithmetic_unary_paramsEUlPfPKfmE0_
8=EIOU
KPW[`f
rv}e
e^gbil
FFFFx{F~
zzzz
#(-27<AFKPU{
UUUUUU
#(-27<AFKPU{
UUUUUU
#(-27<AFKPU{
UUUUUU
!rr{>
!rr{>
}76R
YVpA
YVpA
YVpA
<7p7M}
<7p7M}
<7p7M}
:I~D
:I~D
:I~D
:I~D
*?9s
?G/d?
)>e\
N4bnns14graph_compiler2op4convE
OjRjjjUN4bnns14graph_compiler9OperationE
 &*06
dhmr
Iejnsx
NSt3__110__function6__funcIZN4bnns14graph_compiler14local_opt_passERNS3_7ContextEE3$_0NS_9allocatorIS6_EEFiPNS3_9OperationEEEE
ZN4bnns14graph_compiler14local_opt_passERNS0_7ContextEE3$_0

N4bnns14graph_compiler2op5sliceE
NSt3__110__function6__funcIZNK4bnns14graph_compiler2op5slice15local_optimizerEvE3$_0NS_9allocatorIS6_EEFbRNS3_7ContextEEEE
ZNK4bnns14graph_compiler2op5slice15local_optimizerEvE3$_0
N4bnns14graph_compiler8FunctionE
floor_div
maximum
minimum
real_div
input0
input1
output
undefined
allocator<T>::allocate(size_t n) 'n' exceeds maximum supported size
map::at:  key not found
acos
acosh
asin
asinh
atan
atanh
ceil
cosh
exp2
floor
inverse
round
rsqrt
sinh
sign
sqrt
square
tanh
input
, epsilon=
%s: 
BNNS Tile: descriptor is illegal
BNNS Tile: data type check failed
BNNS Tile: output tensor size is not a multiple of input
BNNS Tile: type %u not supported
Shouldn't call get_data_size with Indexed2 or Indexed4, switch to get_data_bits
BNNS Dropout: Error layer_params is NULL
BNNS Dropout: rate must be in the range [0.0, 1.0].
BNNS Dropout: Unsupported input data type
BNNS Dropout: Input and Output data types must be the same
BNNS Dropout: Error memory allocation failed
BNNS Dropout: Error input descriptor is invalid
BNNS Dropout: Error output descriptor is invalid
BNNS Dropout: Input and output descriptors must have the same shape.
BNNS Dropout: Tensors with dimension greater than %u are not supported.
BNNS Dropout: Error filter is NULL
BNNS Dropout: Error wrong filter type, filter is not Dropout
BNNS Dropout: input pointer is NULL
BNNS Dropout: batch_size>1 and in_stride is 0
BNNS Dropout: output pointer is NULL
BNNS Dropout: batch_size>1 and out_stride is 0
BNNS BatchNorm: Error filter is NULL
BNNS Dropout: input_delta layout, shape and stride must match layer_params->i_desc
BNNS Dropout: output_delta layout, shape and stride must match layer_params->o_desc
Unsupported target %d
v16@?0Q8
v24@?0Q8^v16
BNNS Acitvation Gumbel Max: noise buffer size (%zu) isn't power of 2 or smaller than 16
BNNS Tensor Contraction: inputB pointer must not be NULL unless B is a weight or the operation is quadratic.
BNNS Tensor Contraction: After adding batch dimension, inputA tensor has too many indices
BNNS Tensor Contraction: After adding batch dimension, inputB tensor has too many indices
BNNS Tensor Contraction: After adding batch dimension, output tensor has too many indices
BNNS Tensor Contraction: inB_delta calculation is non-sensical for a contraction of a tensor with itself.
BNNS Tensor Contraction: inA_delta calculation requires inA to be non-NULL
BNNS Tensor Contraction: inA_delta calculation requires out_delta (and out_delta->data) to be non-NULL
inA_delta
BNNS Tensor Contraction: After adding batch dimension, out_delta tensor has too many indices
BNNS Tensor Contraction: After adding batch dimension, inputA_delta tensor has too many indices
BNNS Tensor Contraction: inA_delta calculation requires inB to be non-NULL
BNNS Tensor Contraction: inB_delta calculation requires inA to be non-NULL
BNNS Tensor Contraction: inB_delta calculation requires out_delta (and out_delta->data) to be non-NULL
inB_delta
BNNS Tensor Contraction: After adding batch dimension, inputB_delta tensor has too many indices
BNNS Tensor Contraction: beta must be 0.0 or 1.0 (beta=%.2f)
BNNS Tensor Contraction: invalid op string "%s"
BNNS Tensor Contraction: both inputA and inputB cannot be weights "%s"
BNNS Tensor Contraction: Failed to allocate memory for filter
BNNS Tensor Contraction: inputA descriptor is illegal "%s"
BNNS Tensor Contraction: inputB descriptor is illegal "%s"
BNNS Tensor Contraction: ouput descriptor is illegal "%s"
Data type combination not supported
Specified opaque layout is not supported for use with specified contraction operation
BNNS Tensor Contraction: %s shape does not match inputA shape in dimension %ld (%zu vs %zu)
BNNS Tensor Contraction: out_delta shape does not match output shape in dimension %ld (%zu vs %zu)
BNNS Tensor Contraction: Wildcard index '*' must appear on both sides of operation or neither: "%s"
BNNS Tensor Contraction: Wildcard index '*' must appear at most once per index set: "%s"
BNNS Tensor Contraction: Wildcard index '*' must appear consistently only at start or end of index sets: "%s"
BNNS Tensor Contraction: number of indices from operation (%td) does not match dimension of input A descriptor (%zu) "%s"
BNNS Tensor Contraction: number of indices from operation (%td) does not match dimension of input B descriptor (%zu) "%s"
BNNS Tensor Contraction: number of indices from operation (%td) does not match dimension of output descriptor (%zu) "%s"
BNNS Tensor Contraction: '%c', index %zu of inputA has size %zu and index %td of output has size %zu, but sizes are required to match
BNNS Tensor Contraction: '%c', index %d of inputA has size %zu and index %td of inputB has size %zu, but sizes are required to match
BNNS Tensor Contraction: index '%c' of input A does not match any index of inputB or output "%s"
BNNS Tensor Contraction: '%c', index %d of inputB has size %zu and index %td of output has size %zu, but sizes are required to match
BNNS Tensor Contraction: index '%c' of input B does not match any index of inputA or output "%s"
BNNS Tensor Contraction: index '%c' of output does not match any index of inputA or inputB "%s"
BNNS Tensor Contraction: Wildcard index '*' can only appear as first or last index of "%s_%s"
BNNS Tensor Contraction: repeated indices "%s_%s"
v16@?0^v8
v24@?0Q8Q16
BNNS SoftMax: sum result shouldn't be 0, something is wrong
BNNS SoftMax FP16: sum result shouldn't be 0, something is wrong
BNNS SoftMax BF16: sum result shouldn't be 0, something is wrong
exec op %zu of %zu
  %s
  final generator of %s abs_diff=%.2e rel_diff=%.2e
  final generator of %s <not in expected_values map>
BNNS Graph Execute: Unrecognized kernel id %u
 = concat(values=[
], axis=
, interleave=
view(of:
malloc activation_grad failed
apply_specialized_convolution_PKT_0: o_height 1 not supported
apply_specialized_convolution_PKT_1: o_height less then 4 isn't supported
apply_specialized_convolution_PKT_2: o_height 1 not supported
apply_specialized_convolution_PKT_3: o_height 1 not supported
apply_specialized_convolution_PKT_3: o_height 2 not supported
BNNS Copy Filter: in and out must not be NULL.
BNNS Copysum Filter: unsupported data type conversion
internal bwd copysum
BNNS Tensor Contraction: Input must not be NULL
BNNS Tensor Contraction: Output must not be NULL
BNNS Tensor Contraction: index '%c' appears multiple times on right-hand side, but with different sizes
BNNS Tensor Contraction: '%c', index %zu of input has size %zu and index %d of output has size %zu, but sizes are required to match
BNNS Tensor Contraction: index '%c' appears multiple times on left-hand side, but with different sizes
BNNS Tensor Contraction: Unsupported data type conversion %sfrom %s to %s
(in summation) 
(in copy) 
BNNSTranspose: src and dest have a different number of dimensions.
BNNSTranspose: specified axes must be within range for supplied tensor (axes: %zu, %zu) dimension of tensor is %zu
ijklmnpq
internal_transpose
internal_copy
int1
int2
int4
int8
int16
int32
int64
uint1
uint2
uint4
uint8
uint16
uint32
uint64
bf16
fp16
fp32
indexed8
indexed4
indexed2
indexed1
bool
unknown
BNNS CONVOLUTIONS VERSION2: Error minimum float w_pack weight packing value is 32
BNNS Fully Conneceted Sparsify: output allocation failed
BNNS Sparse Fully Connected Apply: output data type not supported
BNNS Fully Connected Sparsify: tried structured 5:8 but number of non zeros is vey low %zu
BNNS Fully Connected Sparse: using batch single compute for batch_size %zu, will be probably better to use batch_multi code path
BNNS Fully Connected Sparsify: tried structured 14:16 but number of non zeros is vey low %zu
 = tile(
, reps=
BNNS Fully Connected Sparsify: workspace size is too small
BNNS Fully Connected Sparsify: failed to allocate workspace
BNNS Fully Connected Sparsify: not enough memory in scratch memory to encode sparsity
BNNSGather: Only 8, 16, 32 and 64 bit integer types may be used as indices data type
BNNSGather: Only 16 and 32 bit floating point and 8, 16 and 32 bit integer types may be used as input data type
BNNSGather: Only 16 and 32 bit floating point and 8, 16 and 32 bit integer types may be used as output data type
BNNSGather: indices must be a 1d array or have the same shape as output
BNNSGather: input and output tensors must have the same number of dimensions
BNNSGather: output.size[%zu]=%zu must have same size as indices.size[0]=%zu if indices is 1D
BNNSGather: output.size[%zu]=%zu must have same size as indices.size[%zu]=%zu if indices is not 1D
BNNSGather: output.size[%zu]=%zu must have same size as input.size[%zu]=%zu
BNNSScatter: Only add and set ops are supported
BNNSScatter: Only 8, 16, 32 and 64 bit integer types may be used as indices data type
BNNSScatter: Only 16 and 32 bit floating point and 8, 16 and 32 bit integer types may be used as input data type
BNNSScatter: Only 16 and 32 bit floating point and 8, 16 and 32 bit integer types may be used as output data type
BNNSScatter: indices must be a 1d array or have the same shape as input
BNNSScatter: input and output tensors must have the same number of dimensions
BNNSScatter: input.size[%zu]=%zu must have same size as indices.size[0]=%zu if indices is 1D
BNNSScatter: input.size[%zu]=%zu must have same size as indices.size[%zu]=%zu
BNNSScatter: input.size[%zu]=%zu must have same size as output.size[%zu]=%zu
BNNSGatherND: Only 8, 16, 32 and 64 bit integer types may be used as indices data type
BNNSGatherND: Only 16 and 32 bit floating point and 8, 16 and 32 bit integer types may be used as input data type
BNNSGatherND: Only 16 and 32 bit floating point and 8, 16 and 32 bit integer types may be used as output data type
BNNSGatherND: indices.size[%zu]=%zu must be less than or equal to input.ndim=%zu
BNNSScatterND: Only 8, 16, 32 and 64 bit integer types may be used as indices data type
BNNSScatterND: Only 16 and 32 bit floating point and 8, 16 and 32 bit integer types may be used as input data type
BNNSScatterND: Only 16 and 32 bit floating point and 8, 16 and 32 bit integer types may be used as output data type
BNNSScatterND: indices.size[%zu]=%zu must be less than or equal to output.ndim=%zu
internal scatter copy
dict-item
o-item
%s%zu
BNNSGatherND: output tensor (shape [%s]) must have shape matching first n-1 dimensions of indices + shape of any dimensions of input not specified in final dimension of indices (i.e. [%s]).
BNNSScatterND: input tensor (shape [%s]) must have shape matching first n-1 dimensions of indices + shape of any dimensions of output not specified in final dimension of indices (i.e. [%s]).
internal gatherND copy
src-item
dest-item
internal scatterND copy
BNNS Dequantize: input and output can't be null
BNNS Dequantize: in place conversion not supported
BNNS Dequantize: only __fp16 output is supported
BNNS Dequantize: lut is null and needed for Indexed input type
BNNS Dequantize: input type not supported
BNNS Dequantize: shouldn't have reached fp16 convert path
BNNS Layer Norm Apply: failed to allocate scratch memory
BNNS Batchnorm Apply Backward: activation allocation failed
BNNS Batchnorm Apply Backward: failed to apply activation backward
BNNS Instance Norm Apply: input stride (%zu) and output stride (%zu) aren't set correctly
BNNS Instance Norm Apply: failed to allocate scratch memory
BNNS Instance Norm Apply Backward: activation allocation failed
BNNS Instance Norm Apply Backward: failed to apply activation backward
BNNS Layer Norm Apply: input stride (%zu) and output stride (%zu) aren't set correctly
BNNS Layer Norm Apply Backward: activation allocation failed
BNNS Layer Norm Apply Backward: failed to apply activation backward
BNNS Group Norm Apply: failed to allocate scratch memory
BNNS Group Norm Apply Backward: activation allocation failed
BNNS Group Norm Apply Backward: failed to apply activation backward
BNNS Fully Connected Tiny Apply: unexpecetd weights layout
MIL program has more than one function, and no specific function specified for compilation
BNNS Fully Connected Apply: allocation of work buffer failed
BNNS Fully Connected: Unexpected result, should be at least 1
BNNS Fully Connected: Unexpected result, at least 1 batch should remain
BNNS Fully Connected: Unexpected result, at least 1 output should remain
BNNS Fully Connected: Failed to fit in memory limit, using generic code
BNNS FUlly: allocation failed
BNNS Fully Connected: weights conversion buffer isn't allocated.
BNNS Fully Connected: inputs conversion buffer isn't allocated.
BNNS Fully Connected Backward: output delta is NULL
BNNS Fully Connected Backward: only float32 or bfloat16 output delta are supported
BNNS Fully Connected Backward: input delta only support float32 or bfloat16
BNNS Fully Connected Backward: weights delta only support float32 or bfloat16
BNNS Fully Connected Backward: bias delta only support float32 or bfloat16
BNNS Fully Connected Backward: operation is currently unsupported.  We only support sparse weights for inference.
BNNS Fully Connected Backward: output delta BNNSDataLayoutImageCHW isn't supported
BNNS Fully Connected Backward: failed to allocate output delta scratch buffer
BNNS Fully Connected Backward: apply activation backward failed
BNNS Fully Connected Backward: input delta BNNSDataLayoutImageCHW isn't supported
BNNS Fully Connected Backward: failed to allocate input delta scratch buffer
BNNS Fully Connected Backward: failed to compute input delta
BNNS Fully Connected Backward: user requested weights delta but didn't supply original inputs
BNNS Fully Connected Backward: weights sizes doesn't match weights delta sizes
BNNS Fully Connected Backward: unsupported weights delta layout
BNNS Fully Connected Backward: no bias, bias delta ignored
BNNS Fully Connected: Weight type not supported
BNNS Fully Connected Create: failed to allocated memory (%zu bytes))
BNNS_FULLY: input doesn't support Indexed data type
BNNS_FULLY: output only support fp32 data type
BNNS Fully Connected: using generic code to convert data 1 element each time
BNNS Fully Connected: output data type not supported for conversion
BNNS Fully Connected: input must be a 1D array
BNNS Fully Connected: input descriptor is illegal
BNNS Fully Connected: output must be a 1D array
BNNS Fully Connected: output descriptor is illegal
BNNS Fully Connected: weights row size should match input vector size
BNNS Fully Connected: weights number of rows should match output vector size
BNNS Fully Connected: weights must be a 2D array
BNNS Fully Connected: weights descriptor is illegal
BNNS Fully Connected: Pointer to weight data must be non-NULL
BNNS Fully Connected: Output data types supported in this version: Float32, Float16, BFloat16
BNNS Fully Connected: Bias data types supported in this version: Float32
BNNS Fully Connected: Float16 output isn't supported with bias or activation function
BNNS Fully Connected: Input/Weight data types supported in this version: Float32, Float16, BFloat16, Int8, Int16
BNNS Fully Connected: Data type of weights and input should match
BNNS Fully Connected: size computation wraparound -I %zu -O %zu
BNNS Fully Connected: bias size computation wraparound -O %zu bias data type %x
Attempted to copy a view without explicit shallow copy flag
_view
BNNS InTopK: invalid axis value %zu (input tensor has %zu dimensions)
BNNS InTopK: Unsupported test_indices data type for this operation (supported data types are: int32)
BNNS InTopK: Unsupported output data type for this operation (supported data types are: boolean)
BNNS InTopK: input tensor with axis removed is not congruent with test_indices tensor
BNNS InTopK: input tensor with axis removed is not congruent with output tensor
BNNS InTopK: Unsupported input data type for this operation (supported data types are: fp32, fp16)
BNNS TopK: input descriptor is illegal
BNNS TopK: best_values descriptor is illegal
BNNS TopK: best_values data type has to match input data type for this operation
BNNS TopK: Unsupported best_indices data type for this operation (supported data types are: int32)
BNNS TopK: invalid axis value %zu (input tensor has %zu dimensions)
BNNS TopK: best_values tensor is not consistent with input tensor
BNNS TopK: best_values tensor size[%zu]=%zu should be K=%zu
BNNS TopK: best_indices tensor is not consistent with input tensor
BNNS TopK: best_indices tensor size[%zu]=%zu should be K=%zu
BNNS TopK: Unsupported input data type for this operation (supported data types are: fp32, fp16)
malloc
orange
yellow
green
blue
indigo
violet
lilac
pink
cerulean
ultraviolet
infrared
Unoptimized layout option used. Workspace will be super-massive.
Context '
': workspace size 
, lower bound (LOAD) 
layout_color
literal
user_input
user_output
relu
leaky_relu
sigmoid
scaled_tanh
linear_activation
clamped_relu
sigmoid_hard
softplus
softsign
thresholded_relu
silu
, alpha=
, beta=
BNNS unsupported sgd variant
BNNS Optimizer Adam Apply: Adam beta1 and beta2 must be positive
BNNS Optimizer Adam Apply: Adam beta1 and beta2 must be strictly less than 1
BNNS Optimizer RMSProp Apply: RMSProp alpha must be in (0,1)
BNNS Optimizer Apply: Error unsupported optimizer function
BNNS Optimizer Apply: Error OptimizerAlgFields is NULL
BNNS Optimizer Apply: parameter array pointer is NULL
BNNS Optimizer Apply: gradient array pointer is NULL
BNNS Optimizer Apply: gradient pointer number %zu is NULL
BNNS Optimizer Apply: Error in parameter %zu: parameter descriptor must be contiguous such that stride[0]=1 and for every N>0 stride[N]=stride[N-1]*size[N-1]
BNNS Optimizer Apply: accumulator array pointer is NULL
BNNS Optimizer Apply: parameter pointer number %zu is NULL
BNNS Optimizer Apply: accumulator pointer number %zu is NULL
BNNS Optimizer Apply: Error in parameter %zu: Parameter and Gradient descriptors must have the same sizes and strides
BNNS Optimizer Apply: Error in parameter %zu: Parameter and Accumulator descriptors must have the same sizes and strides
BNNS Optimizer Apply: Error in Parameter %zu: Parameter data type is not Float32 / Float16 / Bfloat16
BNNS Optimizer Apply: Error in Gradient %zu: Gradient data type is not Float32 / Float16 / Bfloat16
BNNS Optimizer Apply: Error in Accumulator %zu: Accumulator data type is not Float32 / Float16 / BFloat16
BNNS Optimizer Apply: Error time_step is not valid for Adam optimizer. minimal time step value is 1
BNNS Optimizer Apply: Adam optimizer require accumulators pointer to be valid
BNNS Optimizer Apply: accumulator1 pointer number %zu is NULL
BNNS Optimizer Apply: accumulator2 pointer number %zu is NULL
BNNS Optimizer Apply: accumulator3 pointer number %zu is NULL
BNNS Optimizer Apply: Error in parameter %zu: Parameter and Accumulator1 descriptors must have the same sizes and strides
BNNS Optimizer Apply: Error in parameter %zu: Parameter and Accumulator2 descriptors must have the same sizes and strides
BNNS Optimizer Apply: Error in parameter %zu: Parameter and Accumulator3 descriptors must have the same sizes and strides
BNNS Optimizer Apply: Error in accumulator1 %zu: Accumulator data type is not Float32 / Float16 / Bfloat16
BNNS Optimizer Apply: Error in accumulator2 %zu: Accumulator data type is not Float32 / Float16 / Bfloat16
BNNS Optimizer Apply: Error in accumulator3 %zu: Accumulator data type is not Float32 / Float16 / Bfloat16
BNNS Optimizer Apply: RMSProp optimizer require accumulators pointer to be valid
BNNS Optimizer Apply: accumulator_n pointer number %zu is NULL
BNNS Optimizer Apply: accumulator_g pointer number %zu is NULL
BNNS Optimizer Apply: momentum pointer number %zu is NULL
BNNS Optimizer Apply: Error in accumulator_n %zu: Accumulator data type is not Float32 / Float16 / Bfloat16
BNNS Optimizer Apply: Error in accumulator_g %zu: Accumulator data type is not Float32 / Float16 / Bfloat16
BNNS Optimizer Apply: Error in momentum %zu: Accumulator data type is not Float32 / Float16 / Bfloat16
BNNS Optimizer: unsupported optimizer function
BNNS ClipByValue: Error dimensions of input tensor (%zu) and output tensor (%zu) must match
BNNS ClipByValue: Error only supports fp32
BNNS ClipByValue: Error sizes and strides of input tensor and output tensor must match
BNNS ClipByNorm: Error dimensions of input tensor (%zu) and output tensor (%zu) must match
BNNS ClipByNorm: Error only supports fp32
BNNS ClipByNorm: Error sizes and strides of input tensor and output tensor must match
BNNS ClipByGlobalNorm: src[%zu] and dest[%zu] have a different number of dimensions.
BNNS ClipByGlobalNorm: src[%zu] and dest[%zu] must contain fp32 data.
BNNS ClipByGlobalNorm: src[%zu] and dest[%zu] have different sizes or strides.
BNNS ComputeNorm: Error only supports BNNSL2Norm
BNNS ComputeNorm: Error only supports fp32
BNNS ComputeNorm: Error non-axis dimensions of input tensor (%zu) and output tensor (%zu) must match
BNNS ComputeNorm: Error non-axis sizes of input tensor and output tensor must match
BNNS ComputeNorm Backward: Error only supports BNNSL2Norm
BNNS ComputeNorm Backward: Error only supports fp32
BNNS ComputeNorm Backward: Error non-axis dimensions of input tensor (%zu) and output tensor (%zu) must match
BNNS ComputeNorm Backward: Error non-axis sizes of input tensor and output tensor must match
BNNS Optimizer Init: Unsupported layout
BNNS Sparse Fully Connected: input must be a 1D array
BNNS Sparse Fully Connected: input descriptor is illegal
BNNS Sparse Fully Connected: output must be a 1D array
BNNS Sparse Fully Connected: output descriptor is illegal
BNNS Sparse Fully Connected: invalid matrix layout
BNNS Sparse Fully Connected: invalid block row count: %u
BNNS Sparse Fully Connected: invalid block col count: %u
BNNS Sparse Fully Connected: input data types supported in this version: Float16
BNNS Sparse Fully Connected: output data types supported in this version: Float32, Float16
BNNS Sparse Fully Connected: weights data types supported in this version: Float16
BNNS Sparse Fully Connected: unsupported matrix block size %u x %u
BNNS Sparse Fully Connected: Allocation of the setup structure failed
BNNS Convolution: malloc failed
BNNS Graph: Invalid representation
BNNS Graph: Unsupported BNNS IR Version
/System/Library/PrivateFrameworks/MIL.framework/MIL
BNNS cannot compile graph: MIL framework is not available
BNNS GraphGetSize passed invalid graph
BNNS GraphGetSize passed graph with unsupported ir_version %u
BNNS GraphGetWorkspaceSize passed invalid graph
BNNS GraphGetWorkspaceSize passed graph with unsupported ir_version %u
BNNS GraphGetArgPosition passed invalid graph
BNNS GraphGetArgPosition passed graph with unsupported ir_version %u
BNNS GraphGetArgPosition: Argument '%s' not found
BNNS Compare: in0 is NULL
BNNS Compare: in1 is NULL
BNNS Compare: out is NULL
BNNS Compare: Unsupported I/O tensor data types.
BNNS Compare: Mismatch in input tensor data types.
BNNS Compare: Invalid operation %d for data type %s.
BNNS Compare: Broadcast failed.
BNNS Compare: I/O tensor dimension %lu mismatch.
BNNS Compare: Partially overlapping input and output tensors are not supported.
BNNS Compare: Unsupported I/O tensor layout.
BNNS Compare: I/O tensor layout mismatch.
BNNS Compare: input size[%zu]=%zu does not equal output size of %zu or 1
BNNS Compare: Unsupported operator.
BNNS Fully Connected Choose: inputs, weights, outputs and bias must be contigeous in memory (stride[0] <= 1)
BNNS Fully Connected Apply: failed to allocated scratch memory
BNNS Fully Connected Init: missing data table for indexed data type
BNNS Fully Connected Apply: Ilegal compute block
BNNS Fully Connected Create: failed to create layer context
BNNS Fully Connected Create: allocation failed
BNNS Fully Connected Direct Apply: input pointer is NULL
BNNS Fully Connected Direct Apply: output pointer is NULL
BNNS Fully Connected Direct Apply: failed to create layer context
BNNS Fully Connected Init: failed to create context
BNNS Fully Connected Init: input descriptor CHW layout isn't set properly
BNNS Fully Connected Init: input descriptor isn't set properly
BNNS Fully Connected Init: input descriptor failed validation
BNNS Fully Connected Init: output descriptor CHW layout isn't set properly
BNNS Fully Connected Init: output descriptor isn't set properly
BNNS Fully Connected Init: output descriptor failed validation
BNNS Fully Connected Init: weights descriptor row major size[0] doesn't match input vector size
BNNS Fully Connected Init: weights descriptor row major size[1] doesn't match output vector size
BNNS Fully Connected Init: weights descriptor column major size[0] doesn't match output vector size
BNNS Fully Connected Init: weights descriptor column major size[1] doesn't match input vector size
BNNS Fully Connected Init: weights descriptor isn't set properly
BNNS Fully Connected Init: weights descriptor failed validation
BNNS Fully Connected Init BNNSNDArrayDescriptor: stride[%zu] is too small
BNNS Fully Connected Apply: failed to apply filter
BNNS Random Generator Create: Unsupported method %u.
BNNS Random Generator Create: Failed to allocate memory
BNNS Random Fill Uniform Float: Invalid descriptor
BNNS Random Fill Uniform Float: Range bounds are not exactly representable as bf16
BNNS Random Fill Uniform Float: Range (%g, %g) is empty
BNNS Random Fill Normal Float: Mean and standard deviation are not exactly representable as bf16
BNNS Random Fill Normal Float: Standard deviation (%g) is not greater than zero
BNNS Random Fill Float: Distribution type not supported
BNNS Random Fill Uniform Float: Range bounds are not exactly representable as fp16
BNNS Random Fill Normal Float: Mean and standard deviation are not exactly representable as fp16
BNNS Random Fill Float: Unsupported data type
BNNS Random Fill Uniform Integer: Invalid descriptor
BNNS Random Fill Uniform Integer:%s range (%d, %d) is empty
 (clipped)
BNNS Random Fill Uniform Integer: range (%lld, %lld) is empty
BNNS Random Fill Uniform Integer:%s range (%llu, %lld) is empty
BNNS Random Fill Uniform Integer: Unsupported data type
BNNS Random: CCCryptorCreateWithMode() failed
BNNS Random Fill Uniform: Invalid generator
Failed to create random data
BNNS Loss: Error unsupported loss function
BNNS Loss: Error Input width is 0
BNNS Loss: Error Input must be BNNSDataTypeFloat32 or BNNSDataTypeBFloat16 type
BNNS Loss Error: Input must be contiguous. stride0 must be 1 or 0
BNNS Loss Error: Input tensors must be contiguous.
BNNS Loss: Error unknown reduction function
BNNS Loss: Error output data type must be BNNSDataTypeFloat32 or BNNSDataTypeBFloat16
BNNS Loss: Error output size>1 is only allowed with reduction BNNSLossReductionNone.
BNNS Yolo loss: Error out width (size[0]) must be 1 as yolo loss is always reduced
BNNS Loss: Error anchors is NULL
BNNS Loss: Error number of grid rows is 0
BNNS Loss: Error number of grid columns is 0
BNNS Loss: Error number of anchor boxes is 0
BNNS Loss: Error anchor box size <= 5
BNNS Loss: Error object minimum iou is negative
BNNS Loss: Error no object maximum iou is negative
BNNS Loss: Error input descriptor width (%zu) different from expected width of grid_size*num_anchors*(5+num_+classes) (%zu)
BNNS Loss: memory allocation failed
BNNS Loss: Error filter is NULL
BNNS Loss: Error batch_size is 0
BNNS Loss: Error in pointer is NULL
BNNS Loss: Error in_stride is smaller than i_desc.size[0] or Tensor size
BNNS Loss: Error labels is NULL
BNNS Loss: Error labels_stride is smaller than i_desc.size[0] or Tensor size
BNNS Loss: Error weights_size>0 but weights pointer is NULL
BNNS Loss: Warning weight_size==0 but weight pointer is not NULL. Weight pointer is ignored.
BNNS Loss: Error out is NULL
BNNS Loss: Error in_delta descriptor is ilegal
BNNS Loss: Error in_delta must be contiguous, such that stride[0] is 0 or 1
BNNS Loss Error: weight_size value must be 0,1 batch_size, or batch_size*input_width for softmax
BNNS Loss Error: data type not supported for softmax
BNNS Loss: Error yolo weight_size value must be 0, use yolo specific weight factors during filter create
BNNS Yolo loss: input and output data type must be BNNSDataTypeFloat32
BNNS Loss Error: weight_size value must be 0,1 batch_size, or batch_size*input_width for sigmoid/categorical cross entropy, mse, mae, log, hinge, and huber
BNNS Loss Error: data type not supported
BNNS Loss Error: unsupported loss function
BNNS Loss Backward: Error filter is NULL
BNNS Loss Backward: Error batch_size is 0
BNNS Loss Backward: Error in is NULL
BNNS Loss Backward: Error in_stride is smaller than i_desc.size[0]
BNNS Loss Backward: Error labels is NULL
BNNS Loss Backward: Error labels_stride is smaller than i_desc.size[0]
BNNS Loss Backward: Error weights_size>0 but weights pointer is NULL
BNNS Loss Warning: weight_size==0 but weight pointer is not NULL. Weight pointer is ignored.
BNNS Loss Backward Error: weight_size value must be 0,1 batch_size, or batch_size*input_width for softmax/sigmoid/categorical cross entropy, mse, mae, log, hinge, and huber
BNNS Loss Backward: Error in_delta descriptor is illegal
BNNS Loss Backward: Error in_delta must be contiguous, such that stride[0] is 0 or 1
BNNS Loss Backward: Error in_delta is NULL
BNNS Loss Backward: Error out_delta descriptor is illegal
BNNS Loss Backward: Error out_delta must be contiguous, such that stride[0] is 0 or 1
BNNS Loss Backward: Error out_delta is NULL
BNNS Loss: data type not supported
BNNS Loss: unsupported loss backward function
BNNS Yolo loss: Error yolo reduction type must be BNNSLossReductionSum
BNNS Yolo loss: Error input descriptor width does not match grid_rows*grid_columns*anchors*(5+classes)
BNNS Yolo loss: alloc failed
BNNS Yolo loss: malloc failed
BNNS softmax cross entropy loss: malloc for pDx2 failed
BNNS softmax cross entropy loss: malloc for norm_x failed
BNNS Loss Warning: reduction BNNSLossReductionWeightedMean sum of weights is zero
BNNS Loss: Error reduction BNNSLossReductionNonZeroWeightMean all weights are zero
BNNS softmax cross entropy loss: malloc for temporary_softmax_forward_output failed
BNNS Create Depthwise Convolution: incompatible numbers of channels between images and convolution parameters
BNNS Create Depthwise Convolution: unsupported weight format
BNNS Create Depthwise Convolution: input must be BNNSDataLayoutImageCHW layout 
BNNS Create Depthwise Convolution: input descriptor is illegal
BNNS Create Depthwise Convolution: output must be BNNSDataLayoutImageCHW layout 
BNNS Create Depthwise Convolution: output descriptor is illegal
BNNS Depthwise Convolution Create: weights descriptor is illegal
BNNS Depthwise Convolution Create: does not support asymmetric padding. left and right pad must match, up and down pad must match
BNNS Create Depthwise Convolution: only float32 is supported
BNNS Create Depthwise Convolution: channel multiplier mismatch
BNNS Create Depthwise Convolution: malloc failed
BNNS Create Depthwise Convolution: weights malloc failed 
invalid argument
BNNS DEPTHWISE CONV: input batch stride doesn't make sense (%zu < %zu x %zu)
BNNS DEPTHWISE CONV: output batch stride doesn't make sense (%zu < %zu x %zu)
BNNS LSTM Direct Apply: filter initialization failed
BNNS LSTM Direct Apply: training cache capacity isn't sufficient
BNNS LSTM Direct Apply: Dropout is only supported in the presence of a training cache
BNNS LSTM Direct Apply: failed to allocate scratch buffer
BNNS LSTM Direct Apply: failed to thread workspace buffer
BNNS LSTM Direct Apply Backward: BNNSNDArrayFlagBackpropAccumulate is only supported on layer_delta->input_descriptor.data_desc.
BNNS LSTM Direct Apply Backward: layer_params initialization failed
BNNS LSTM Direct Apply Backward: layer_delta initialization failed
BNNS LSTM APPLY BACKWARD: Use of dropout without training cache is not supported.
BNNS LSTM APPLY BACKWARD: forward pass intermediate results weren't cached, recomputing forward pass
BNNS LSTM Direct Apply Backward: failed to allocate training cache buffer
BNNS LSTM Direct Apply Backward: failed to compute forward intermediate results
BNNS LSTM Direct Apply Backward: training cache capacity isn't sufficient
BNNS LSTM Direct Apply Backward: failed to allocate scratch buffer
BNNS LSTM init: hidden_size must be greater than zero
BNNS LSTM init: input descriptor.data isn't set correctly (size don't match seq_len/batch_size/input_size)
BNNS LSTM init: input descriptor.data isn't set correctly (size don't match batch_size/seq_len/input_size)
BNNS LSTM init: input descriptor.data isn't set correctly (size don't match input_size/batch_size/seq_len)
BNNS LSTM init: input descriptor.hidden isn't set correctly (size don't match hidden_size/batch_size/num_directions/num_layers)
BNNS LSTM init: input descriptor.cell_state isn't set correctly (size don't match hidden_size/batch_size/num_directions/num_layers)
BNNS LSTM init: output descriptor.data isn't set correctly (size don't match seq_len/batch_size/hidden_size*num_directions)
BNNS LSTM init: output descriptor.data isn't set correctly (size don't match batch_size/seq_len/hidden_size*num_directions)
BNNS LSTM init: output descriptor.data isn't set correctly (size don't match hidden_size/num_directions/batch_size/seq_len)
BNNS LSTM init: output descriptor.hidden isn't set correctly (size don't match hidden_size/batch_size/num_directions/num_layers)
BNNS LSTM init: output descriptor.cell_state isn't set correctly (size don't match hidden_size/batch_size/num_directions/num_layers)
BNNS LSTM init: forget_gate isn't set correctly (size don't match hidden_size/batch_size/num_directions/num_layers)
BNNS LSTM init: input_gate isn't set correctly (size don't match hidden_size/batch_size/num_directions/num_layers)
BNNS LSTM init: candidate_gate isn't set correctly (size don't match hidden_size/batch_size/num_directions/num_layers)
BNNS LSTM init: output_gate isn't set correctly (size don't match hidden_size/batch_size/num_directions/num_layers)
BNNS LSTM: Data type not supported, fail
prelu
conv
conv_transpose
cast
const
identity
tile
concat
expand_dims
reshape
slice_by_index
slice_by_size
squeeze
stack
transpose
constexpr_lut_to_dense
: Failed to parse x argument
: Failed to parse y argument
values
axis
interleave
m2a_attr
palletize_int4
Failed to palletize %s
: only 4-bit palletization of fp16 is supported
indices
%s: only 1d and 2d convolutions are supported
weight
weights
strides
pad_type
valid
custom
%s: Unsupported conv pad type '%s'
dilations
groups
bias
begin
stride
begin_mask
end_mask
squeeze_mask
size
reps
perm
epsilon
beta
alpha
_anon_
BNNS Graph Compile: Unsupported MIL data type
unordered_map::at: key not found
Can't cast this scalar :(
 = permute(
Replaced op 
 with constexpr result
BNNS Convolution Create: Client needs to support weights ptr with low_mem
BNNS Convolution Create: malloc
BNNS Convolution Create: failed to create convolution context
BNNS Convolution Create: failed to divide work between convolution contexts to fit in memory
BNNS Convolution Create: failed to create single generic convolution
BNNS Convolution Apply: invalid argument
BNNS Convolution Apply: failed to allocate input auxiliary buffer
BNNS Convolution: Winograd weights memory doesn't fit in memory
BNNS Convolution: unable to create Winograd that fit in memory
BNNS Convolution: allocation of contexts failed
BNNS LOW MEM CONVOLUTION: failed to create winograd convolution
BNNS LOW MEM CONVOLUTION: malloc failed
BNNS LOW MEM CONVOLUTION: incompatible BNNS_low_memory_context id
BNNS LOW MEM CONVOLUTION: convolution failed
BNNS LOW MEM CONVOLUTION: input aux buffer wasn't allocated
BNNS LOW MEM CONVOLUTION:: context type not supported
BNNS Convolutions Apply: unexpected input data type
BNNS Convolution Create: apply convolution failed
BNNS Quantization Filter: layer_params is NULL
BNNS Quantization: input layout and output layout must match
BNNS Quantization: unsupported input/output layouts
BNNS Quantization: input descriptor error
BNNS Quantization: input descriptor data is NULL
BNNS Quantization: output descriptor error
BNNS Quantization: output descriptor data is NULL
BNNS Quantization: invalid quantizer function
BNNS Quantization: BNNSQuantize function supported for the following input descriptor data types: BNNSDataTypeFloat32, BNNSDataTypeFloat16, BNNSDataTypeBFloat16, BNNSDataTypeInt32
BNNS Quantization: BNNSQuantize function supported for the following output descriptor data types: BNNSDataTypeInt8, BNNSDataTypeUInt8, BNNSDataTypeInt16, BNNSDataTypeUInt16, BNNSDataTypeInt32, BNNSDataTypeUInt32
BNNS Quantization: BNNSDeQuantize function supported for the following input descriptor data types: BNNSDataTypeInt8, BNNSDataTypeUInt8, BNNSDataTypeInt16, BNNSDataTypeUInt16, BNNSDataTypeInt32, BNNSDataTypeUInt32
BNNS Quantization: BNNSDeQuantize function supported for the following output descriptor data types: BNNSDataTypeFloat32, BNNSDataTypeFloat16, BNNSDataTypeBFloat16, BNNSDataTypeInt32
BNNS Quantization: invalid axis_mask, number of mask set bits must be lower or equal to 1
BNNS Quantization: axis_mask bits are set beyond batch dimension
BNNS Quantization: Error dimensions of input tensor (%zu) and output tensor (%zu) must match
BNNS Quantization: Error shape of input tensor and output tensor must match
BNNS Quantization Filter: scale layout must be BNNSDataLayoutVector
BNNS Quantization: scale descriptor error
BNNS Quantization Filter: bias layout must be BNNSDataLayoutVector
BNNS Quantization: bias descriptor error
BNNS Quantization: bias vector size does not match axis mask, bias size[0]=%zu, expected %zu
BNNS Quantization: scale vector size does not match axis mask, scale size[0]=%zu, expected %zu
BNNS Fused Convolution and Normalization: Error convolution_layer_params is NULL
BNNS Fused Convolution and Quantization: Error quantization_layer_params is NULL
BNNS Fused Compute and Quantization: Error Convolution output layout and Quantization input layout must match
BNNS Fused Compute and Quantization: Error Convolution output data type and Quantization input data type must match
BNNS Fused Compute and Quantization: Error Convolution output descriptor sizes and Quantization input descriptor sizes must match
BNNS Fused Compute and Quantization: Error Convolution output descriptor strides and Quantization input descriptor strides must match
BNNS Fused Compute and Quantization: Error memory allocation failed
BNNS Fused Compute and Quantization create filter failed: compute filter type error
BNNS Fused Compute and Quantization create filter failed
BNNS Quantization filter failed
BNNS Fused Fully Connected and Quantization: Error fully_layer_params is NULL
BNNS Fused Fully Connected and Quantization: Error quantization_layer_params is NULL
BNNS Fused Compute and Quantization: Error Fully Connected output layout and Quantization input layout must match
BNNS Fused Compute and Quantization: Error Fully Connected output data type and Quantization input data type must match
BNNS Fused Compute and Quantization: Error Fully Connected output descriptor sizes and Quantization input descriptor sizes must match
BNNS Fused Compute and Quantization: Error Fully Connected output descriptor strides and Quantization input descriptor strides must match
BNNS Quantization: unsupported data type
BNNS Quantization: unsupported quantization input data type
BNNS Quantization: unsupported quantization output data type
BNNS Quantization: unsupported dequantization input data type
BNNS Quantization: unsupported dequantization output data type
BNNS Transposed Convolution Create: input data type isn't supported
BNNS Transposed Convolution Create: weight data isn't supported
BNNS Transposed Convolution Create: output data type isn't supported
BNNS Transposed Convolution Apply: Filter is NULL
BNNS Transposed Convolution Apply: failed to allocate memory to manipulate input
BNNS Transposed Convolution Apply: unknown weight layout
BNNS Transposed Convolution Create: layer_params is NULL
BNNS Transposed Convolution Create: output channels must divide by groups
BNNS Transposed Convolution Create: input channels must divide by groups
BNNS Transposed Convolution Create: low-mem isn't supported
BNNS Transposed Convolution Create: packed weights aren't supported
BNNS Transposed Convolution Create: failed to create forward path
BNNS Transposed Convolution Create: failed to allocate wrapper memory
BNNS Transposed Convolution Create: transposed convolution groups only supported with BNNSDataLayoutConvolutionWeightsOIHW or BNNSDataLayoutConvolutionWeightsOIHrWr layouts
BNNS Transposed Convolution Create: weights allocation failed
BNNS Create Layer Transposed Convolution: input channels must be divisible by groups
BNNS Create Layer Transposed Convolution: output channels must be divisible by groups
BNNS Create Layer Transposed Convolution: bias must match output channels
BNNS Create Layer Transposed Convolution: failed to allocate memory to copy weights and bias
BNNS Transpsoed Vector Convolution Apply: activation gradient auxilary allocation failed
BNNS Transposed Convolution Create: malloc failed
BNNS Transposed Convolution: weights allocation failed
BNNS Transposed Convolution: bias allocation failed
BNNS Transposed Convolution Reorder Weights: NULL weight object
BNNS Transposed Convolution Reorder Weights: dst weight size too small
BNNS Transposed Convolution Reorder Weights: NULL weight array
BNNS Transposed Convolution Rotate Weights: NULL weight object
BNNS Transposed Convolution Rotate Weights: dst weight size too small
BNNS Transposed Convolution Rotate Weights: NULL weight array
BNNS Transposed Convolution Reorder and Rotate Weights: NULL weight object
BNNS Transposed Convolution Reorder and Rotate Weights: dst weight size too small
BNNS Transposed Convolution Reorder and Rotate Weights: NULL weight array
transposed convolution input manipulation failed
transposed convolution input manipulation failed - nothing to do, should not have allocated dst_input buffer
BNNS: unsupported activation gradient data type
BNNS Transposed Convlution Backward: dy padding failed
BNNS Transposed Convlution Backward: no padding is needed, dy padding failed
BNNS Create Convolution: fp32 weights allocation failed
BNNS Create Convolution: int16 weights allocation failed
BNNS Create Convolution: failed to allocate memory to copy the weights
BNNS Create Convolution: single descriptors allocation failed
BNNS Convolution Create: allocation failed
failed to upconvert or copy weights
failed to allocate none generic format
BNNS Convolution Create: forward pass check (swapping input and output)  - input size computation wraparound input (%zu x %zu), output (%zu x %zu), stride (%zu x %zu), asymmetric padding (%zu,%zu,%zu,%zu)
BNNS Convolution Create: forward pass check (swapping input and output)  - input size computation wraparound input (%zu x %zu), output (%zu x %zu), stride (%zu x %zu), padding (%zu x %zu)
BNNS Convolution Create: forward pass check (swapping input and output) - output(%zu x %zu x %zu) is larger than input(%zu x %zu x %zu) with padding(%zu x %zu).
 kernel (%zu x %zu) and stride (%zu x %zu)
v8@?0
com.apple.accelerate.bnns
layer
BNNS Fully Connected Sparsify: out descriptor can't be NULL
BNNS Fully Connected Sparse: failed to allocate memory to sparsify COO
BNNS Fully Connected Sparse: insufficient memory allocated memory to sparsify COO
BNNS Fully Connected Sparse: Number of nonzeros is 0
BNNS Fully Connected Sparse COO: probably wrong stride size %zu
BNNS Fully Connected Sparsify: in descriptor can't be NULL
BNNS Normalization: layer_params is NULL
BNNS Normalization: filter memory allocation failed
BNNS Fused Convolution and Normalization: Error normalization_layer_params is NULL
BNNS Fused Convolution and Normalization: Error convolution output descriptor and normalization input descriptor must have the same sizes and strides
BNNS Fused Convolution and Normalization: Error memory allocation failed
BNNS Fused Convolution and Normalization create filter failed: Convolution type error
BNNS Fused Convolution and Normalization create filter failed
BNNS Fused Fully Connected and Normalization: Error fully_layer_params is NULL
BNNS Fused Fully Connected and Normalization: Error batch_normalization_layer_params is NULL
BNNS Fused Fully Connected and Normalization: Error fully connected output descriptor and normalization input descriptor must have the same sizes and strides
BNNS Fused Fully Connected and Normalization: Error memory allocation failed
BNNS Fused Fully Connected and Normalization create filter failed
BNNS Fused Arithmetic and Normalization: Error arithmetic_layer_params is NULL
BNNS Fused Arithmetic and Normalization: Error normalization_layer_params is NULL
BNNS Fused Arithmetic and Normalization: unsupported arithmetic function
BNNS Fused Arithmetic and Normalization: Error arithmetic output descriptor and normalization input descriptor must have the same sizes and strides
BNNS Fused Arithmetic and Normalization: Error memory allocation failed
BNNS Fused Arithmetic and Normalization create filter failed
BNNS Normalization: input pointer is NULL
BNNS Normalization: batch_size>1 and in_stride is 0
BNNS Normalization: output pointer is NULL
BNNS Normalization: batch_size>1 and out_stride is 0
BNNS Normalization: batch_size too large. Backprop cache size limits batch_size <= %zu.
BNNS Normalization: x_hat allocation failed
BNNS Normalization: filter id not supported
BNNS Normalization: inverse variance allocation failed
BNNS Normalization Apply Backward: filter is NULL
BNNS Normalization Apply Backward: Normalization beta delta pointer is NULL. backward computation of all active parameters must be done in a single function call.
BNNS Normalization Apply Backward: Normalization gamma delta pointer is NULL. backward computation of all active parameters must be done in a single function call.
BNNS Normalization Apply Backward: make sure to run normalization forward with training flag enabled before running normalization backward
BNNS Normalization Apply Backward: cannot compute activation backward, output is NULL
BNNS Normalization Apply Backward: Fusion of normalization with activation is unsupported for given activation function
BNNS Normalization Apply Backward: cannot compute input delta because input delta data pointer is NULL
BNNS Normalization Apply Backward: Error normalization input delta and input must have the same sizes
BNNS Normalization Apply Backward: in delta should be contiguous and same size as input size
BNNS Normalization Apply Backward: cannot compute beta delta because beta delta data pointer is NULL
BNNS Normalization Apply Backward: BNNSNDArrayFlagBackpropAccumulate is no supported for beta_delta.
BNNS Normalization Apply Backward: Error normalization input delta and input must have the same sizes and strides
BNNS Normalization Apply Backward: Error beta_delta descriptor must match the shape of input up to the normalizaton axis
BNNS Normalization Apply Backward: beta delta should be contiguous and same size as number of input channels
BNNS Normalization Apply Backward: cannot compute gamma delta because gamma delta data pointer is NULL
BNNS Normalization Apply Backward: BNNSNDArrayFlagBackpropAccumulate is no supported for gamma_delta.
BNNS Layer Normalization: Error gamma_delta descriptor must match the shape of input up to the normalizaton axis
BNNS Normalization Apply Backward: gamma delta should be contiguous and same size as number of input channels
BNNS Normalization Apply Backward: out delta data is NULL
BNNS Normalization Apply Backward: Error Normalization output delta and output must have the same sizes
BNNS Normalization Apply Backward: out delta should be contiguous and same size as input size
BNNS Normalization Apply Backward: normalization type not supported
BNNS Normalization Set State: Backprop cache must be in BNNSDataLayoutVector
BNNS Normalization Set State: Backprop cache must be in BNNSDataTypeFloat32
BNNS Normalization Set State: Backprop cache must be in contiguous
BNNS Normalization Set State: Backprop cache pointer is NULL
BNNS Normalization Set State: Backprop cache too small, must be >= %zu fp32 to allow max_batch_size >= 1
BNNS Normalization: Only elementwise activation function is supported
BNNS Normalization: Moving Mean must be BNNSDataTypeFloat32
BNNS Normalization: Moving Mean isn't set properly
BNNS Normalization: Moving Variance must be BNNSDataTypeFloat32
BNNS Normalization: Moving Variance isn't set properly
BNNS Normalization Create: normalization type not supported
BNNS Normalization: input descriptor isn't set properly
BNNS Normalization: Input must be in BNNSDataLayoutImageCHW layout
BNNS Normalization: output descriptor isn't set properly
BNNS Normalization: Output must be in BNNSDataLayoutImageCHW layout
BNNS Normalization: Gamma descriptor isn't set properly
BNNS Normalization: Beta descriptor isn't set properly
BNNS Normalization: Error input descriptor must have stride[0]=1
BNNS Normalization: Error output descriptor must have stride[0]=1
BNNS Normalization: Error input size (%zu) different from output size (%zu)
BNNS Normalization: Error normalization_axis must be 0, 1, or 2
BNNS Layer Normalization: Gamma descriptor must match the shape of input up to the normalizaton axis
BNNS Normalization: Beta descriptor must match the shape of input up to the normalizaton axis
BNNS Normalization: Gamma descriptor size[0] must be the same as number of input channels
BNNS Normalization: Beta descriptor size[0] must be the same as number of input channels
BNNS Normalization: Moving Mean descriptor size[0] must be the same as number of input channels
BNNS Normalization: Moving Variance descriptor size[0] must be the same as number of input channels
BNNS Normalization: Error momentum must be between 0 and 1
BNNS Normalization Warning: epsilon is zero, it may cause division by zero
BNNS Normalization Warning: momentum is zero
BNNS CONVOLUTIONS VERSION2: layer param is NULL
BNNS CONVOLUTIONS VERSION2: unsupported input data type
BNNS CONVOLUTIONS VERSION2: unsupported output data type
BNNS CONVOLUTIONS VERSION2: unsupported weight data type
BNNS CONVOLUTIONS VERSION2: unsupported bias data type
BNNS CONVOLUTIONS VERSION2: int/out/weight/bias descriptor element stride (stride[0]) must be 1
BNNS CONVOLUTIONS VERSION2: 2D conv doesn't fit in memory limit
BNNS CONVOLUTIONS VERSION2: 0 width/height/channel
BNNS CONVOLUTIONS VERSION2: unsupported activation
BNNS CONVOLUTIONS VERSION2: output bias or scale is not supported
BNNS CONVOLUTIONS VERSION2: malloc failed
BNNS CONVOLUTIONS VERSION2: malloc failed
BNNS CONVOLUTIONS VERSION2: failed to allocate bias conversion memory
BNNS CONVOLUTIONS VERSION2: failed to allocate scratch memory
BNNS CONVOLUTIONS VERSION2: invalid argument
BNNS CONVOLUTIONS VERSION2: input batch stride doesn't make sense (%zu < %zu x %zu)
BNNS CONVOLUTIONS VERSION2: output batch stride doesn't make sense (%zu < %zu x %zu)
BNNS CONVOLUTIONS VERSION2: unsupported weight packing
BNNS CONVOLUTIONS VERSION2: failed to allocated memory to convert bias
BNNS CONVOLUTIONS VERSION2: input padding memory allocation failed
BNNS CONVOLUTIONS VERSION2: output repack malloc failed
BNNS CONVOLUTIONS VERSION2: weight repack malloc failed
BNNS Padding Create: layer_params is NULL
BNNS Padding Create: Padding is not supported beyond 4-D tensors.
BNNS Padding Create: Undefined padding mode.
BNNS Padding Create: Unsupported data layout.
BNNS Padding Create: input and output desciptors have differing numbers of dimensions.
BNNS Padding Create: Input dimension is too small for the padding size.
BNNS Padding Create: Input size + padding sizes doesn't match output size in dimension %zu.
BNNS Padding Create: input descriptor is illegal
BNNS Padding Create: output descriptor is illegal
BNNS Padding Create: Unsupported data type.
BNNS Padding Create: I/O data type mismatch.
BNNS Padding Create: memory allocation failed
BNNS Padding Apply: filter is NULL
BNNS Padding Apply: wrong filter type, filter is not Padding.
BNNS Padding Apply: input pointer is NULL
BNNS Padding Apply: batch_size > 1 and in_stride is 0
BNNS Padding Apply: output pointer is NULL
BNNS Padding Apply: batch_size > 1 and out_stride is 0
BNNS Padding Apply Backward: filter is NULL
BNNS Padding Apply Backward: wrong filter type, filter is not padding
BNNS Padding Apply Backward: input descriptor is NULL
BNNS Padding Apply Backward: input data pointer is NULL
BNNS Padding Apply Backward: batch_size>1 and in_stride is 0
BNNS Padding Apply Backward: output delta descriptor is NULL
BNNS Padding Apply Backward: output delta data pointer is NULL
BNNS Padding Apply Backward: batch_size>1 and out_delta_stride is 0
BNNS Pooling: Error wrong filter type, filter is not pooling
filter is NULL
out_delta is NULL
out_delta->data is NULL
BNNSFilterCreateLayerSparseEmbedding: layer_params must not be NULL
BNNSSparseGetRepresentation: filter must not be NULL
BNNSSparseGetRepresentation: Invalid filter
BNNSSparseSetRepresentation: filter must not be NULL
BNNSSparseSetRepresentation: Invalid filter
BNNSSparseEmbeddingGetDense: filter must not be NULL
BNNSSparseEmbeddingGetDense: Invalid filter
BNNS Fused Filter Multi-Input: Error filter is NULL
BNNS Fused Filter Multi-Input Warning: Batch size is 0, nothing to do
BNNS Fused Filter Multi-Input: Error - only fused arithmetic and normalization is supported.
BNNS Fused Filter Multi-Input: Error arithmetic filter apply failed
BNNS Fused Filter Backward Multi-Input: Error filter is NULL
BNNS Fused Filter Backward Multi-Input Warning: Batch size is 0, nothing to do
BNNS Fused Filter Backward Multi-Input: output delta is NULL
BNNS Fused Filter Backward Multi-Input: Error - only fused arithmetic and normalization is supported.
BNNS Fused Filter Backward Multi-Input: Error normalization beta delta pointer is NULL. backward computation of all active parameters must be done in a single function call. 
BNNS Fused Filter Backward Multi-Input: Error normalization gamma delta pointer is NULL. backward computation of all active parameters must be done in a single function call. 
BNNS Arithmetic Backward Multi-Input: failed to allocate memory
BNNS Fused Filter Backward Multi-Input: Error normalization backward failed
x_ijf*, x_ijc* -> G_fc*
BNNSFilterApplyTwoInputBatch: invalid argument
Tensor Contraction filter only expects one input
invalid filter or incorrect number of inputs
BNNSFilterApplyBackwardTwoInputBatch: invalid argument
Two-input Tensor Contraction filter has no weights
Tensor Contraction filter has no bias
Invalid filter
Unsupported filter
BNNS GetStateSize : filter is NULL
BNNS SetState: Error filter is NULL
BNNS SetState: Source filter state is NULL
BNNS SetState: Target filter is NOT Dropout/Normalization filter
BNNS GetState: Error filter is NULL
BNNS GetState: Target filter state is NULL
BNNS GetState: Source filter is NOT Dropout filter
BNNS LayerNorm: Error Normalization axis must be 0, 1, or 2.
BNNS GroupNorm: Error The number of input channels must be divisible by the number of groups.
BNNS Fused Filter: Error number_of_fused_filters is not 2. currently supporting only 2 fused filters
BNNS Fused Filter: Error filter_type is NULL
BNNS Fused Filter: Error layer_params is NULL
BNNS Fused Filter: Error currently supporting first filter of BNNSConvolution, BNNSFullyConnected, BNNSTransposedConvolution, or BNNSArithmetic type only
BNNS Fused Filter: Error currently supporting second filter of BNNSBatchNorm, BNNSInstanceNorm, BNNSLayerNorm, BNNSGroupNorm and BNNSQuantization types only
BNNS Fused Filter: Error layer_params[0] is NULL
BNNS Fused Filter: Error layer_params[1] is NULL
BNNS Fused Filter: Error unknown fused filter type
BNNS Normalization: Error filter is NULL
BNNS Normalization: Error wrong filter type, filter is not normalization
BNNS Fused Filter: Error filter is NULL
BNNS Fused Filter Warning: Batch size is 0, nothing to do
BNNS Fused Filter: Error wrong filter, this function should only be used for a filter created with BNNSFilterCreateFusedLayer
BNNS Fused Filter: Error first filter apply failed
BNNS Fused Filter: Error malloc failed
BNNS Arithmetic Filter: Error filter is NULL
BNNS Arithmetic Filter Warning: Batch size is 0, nothing to do
BNNS Arithmetic Filter: Error wrong filter, this function should only be used for a filter created with BNNSFilterCreateLayerArithmetic
BNNS Permute Filter: filter is NULL
BNNS Permute Filter Warning: Batch size is 0, nothing to do
BNNS Permute Filter: input delta is NULL
BNNS Permute Filter: output delta is NULL
BNNS Permute Filter: input delta data pointer is NULL
BNNS Permute Filter: output delta data pointer is NULL
BNNS Permute Filter: inplace gradient is not supported
Tensor Contraction filter has no bias: bias_delta must be NULL.
Tensor Contraction filter expects more than one input
Resize filter has no bias: bias_delta must be NULL.
Resize filter has no weights: weights_delta must be NULL.
Resize filter: backward pass requires in_delta and in_delta->data to be non-NULL.
BNNS Copysum filter has no bias: bias_delta must be NULL.
BNNS Copysum filter has no weights: weights_delta must be NULL.
BNNS Copysum filter: backward pass requires in_delta and in_delta->data to be non-NULL.
BNNS Reduction filter has no bias: bias_delta must be NULL.
BNNS Padding: backward pass requires in_delta and in_delta->data to be non-NULL.
BNNS Padding filter has no bias: bias_delta must be NULL.
BNNS Embedding: in must not be NULL.
BNNS Embedding: in_delta must be NULL.
BNNS Embedding: backward pass requires weights_delta and weights_delta->data to be non-NULL.
BNNS Embedding: bias_delta must be NULL.
BNNS Normalization Backward: Error filter is NULL
BNNS Normalization Backward Warning: Batch size is 0, nothing to do
BNNS Normalization Backward: Error wrong filter, this function should only be used for a filter created with BNNSFilterCreateLayerNormalization
BNNS Fused Filter Backward: Error filter is NULL
BNNS Fused Filter Backward Warning: Batch size is 0, nothing to do
BNNS Fused Filter Backward: output delta is NULL
BNNS Fused Filter Backward: Error delta_parameters is NULL
BNNS Fused Filter Backward: Error wrong filter, this function should only be used for a filter created with BNNSFilterCreateFusedLayer
BNNS Fused Filter Backward: Error - fused compute and quantization gradient is not support 
BNNS Fused Filter Backward: Error Weight delta pointer is NULL. backward computation of all active parameters must be done in a single function call. 
BNNS Fused Filter Backward: Error bias delta pointer is NULL. backward computation of all active parameters must be done in a single function call. 
BNNS Fused Filter Backward: Error normalization beta delta pointer is NULL. backward computation of all active parameters must be done in a single function call. 
BNNS Fused Filter Backward: Error normalization gamma delta pointer is NULL. backward computation of all active parameters must be done in a single function call. 
BNNS Arithmetic Backward: failed to allocate memory
BNNS Fused Filter Backward: Error normalization backward failed
BNNSDirectApplyConvolutionBatch not supported 
BNNSDirectApplyTransposedConvolutionBatch not supported 
BNNSDirectApplyPoolingBatch not supported 
BNNSDirectApplyLossBatch not supported
BNNS Apply: filter or output can't be null
BNNS Apply: Tensor Contraction filter expects more than one input
BNNS Apply: Loss filter apply must be called with BNNSLossFilterApplyBatch
BNNS Apply: Batchnorm filter apply must be called with BNNSNormalizationFilterApplyBatch
BNNS Apply: Instance Norm filter apply must be called with BNNSNormalizationFilterApplyBatch
BNNS Apply: Layer Norm filter apply must be called with BNNSNormalizationFilterApplyBatch
BNNS Apply: Group Norm filter apply must be called with BNNSNormalizationFilterApplyBatch
BNNS Apply: Fused filter apply must be called with BNNSFusedFilterApplyBatch
BNNS Apply: invalid filter
BNNS DESTROY: invalid filter
Convolution
FullyConnected
Pooling
Activation
SparseFullyConnected
Loss
Batchnorm
Instancenorm
Layernorm
Groupnorm
Fused{Compute,Normalization}
Dropout
Contraction
Resize
LSTM
Arithmetic
Copysum
MultiheadAttention
Reduction
Padding
Embedding
Fused{Compute,Quantization}
Fused{Arithmetic,Normalization}
Fused{Affine Grid, Grid Sample}
Unknown
BNNS Padding + Convolution Apply Backward: input is null
BNNS Padding + Convolution Apply Backward: allocation failed
BNNS Padding + Convolution Apply Backward: Padding failed
BNNS Padding + Convolution Apply Backward: Convolution backward failed
BNNS Padding + Convolution Apply Backward: Padding backward failed
malloc failed
Padding failed
Convolution failed
anon_context
manifest_literal
internal.manifest_literal
_literal
program
sorted_outputs = 
outputs = 
missing from sorted_outputs:
missing from outputs:
BNNS Band Part: illegal input descriptor
BNNS Band Part: illegal output descriptor
BNNS Band Part: input and output data type or dimension do not match or the dimension is less than two
BNNS Band Part: input and output sizes do not match
BNNS Convolution Variant: input type %s not supported!!!
BNNS Convolution Variant: compute type %s not supported!!!
BNNS GRU Fused Gates: Output pointer is null
BNNS GRU Fused Gates: Input pointer is null
BNNS GRU Fused Gates: Recurrent pointer is null
BNNS GRU Fused Gates: Hidden Input pointer is null
BNNS GRU Fused Gates: Input vector size %zu foesn't match output vector size %zu
BNNS GRU Fused Gates: Recurrent vector size %zu foesn't match output vector size %zu
BNNS GRU Fused Gates: Hidden Input vector size %zu foesn't match output vector size %zu
BNNS GRU Fused Gates: number of gates for Input %zu and Recurrent %zu doesn't match
BNNS GRU Fused Gates: Input stride %zu is smaller than number of elements %zu
BNNS GRU Fused Gates: Recurrent stride %zu is smaller than number of elements %zu
BNNS GRU Fused Gates: Output supported data types are fp32 or fp16
BNNS GRU Fused Gates: Input data type doesn't match output data type
BNNS GRU Fused Gates: Recurrent data type doesn't match output data type
BNNS GRU Fused Gates: Hidden Input data type doesn't match output data typen
BNNS GRU Fused Gates: GRU fused gates doesn't support %zu gates
BNNS GRU Fused Gates: Hidden Input batch size %zu doesn't match output batch size %zu
BNNS GRU Fused Gates: Input batch size %zu doesn't match output batch size %zu
BNNS GRU Fused Gates: Hidden Output batch stride %zu is small then a single vector stride
BNNS GRU Fused Gates: Hidden Input batch stride %zu is small then a single vector stride
BNNS GRU Fused Gates: Input batch stride %zu is small then a single vector stride
BNNS GRU Fused Gates: Recurrent batch stride %zu is small then a single vector stride
Unsupported gemm type combination %s x %s -> %s
BNNS CONVOLUTIONS VERSION2: unsupported src type to pack
BNNS Convolution: unsupported src type to pack
BNNS Convolution Compute Weights delta: failed to compute
BNNS Convolution Apply Backward: activation_grad allocation failed
weight delta backward failed
BNNS Convolution: int32 repack allocation failed
BNNS Convolution: Float16 repack allocation failed
BNNS Convolution: Float32 repack allocation failed
 = copy(from=
: BNNS Graph copy from 
 -> 
 is not supported
BNNS Activation Apply: Error input/output/activation pointers must be non-NULL
BNNS Activation Apply: Input and output tensor doesn't match
BNNS Activation Apply: unable to allocate memory to compute activation
BNNS Activation Apply: failed to allocate memory
BNNS Apply Activation Backward: BNNSActivationFunctionIntegerLinearSaturate isn't supported
BNNS Apply Activation Backward: BNNSActivationFunctionIntegerLinearSaturatePerChannel isn't supported
BNNS Apply Activation Backward: failed to allocate temporary y
BNNS Apply Activation Backward: unsupported activation backward
BNNS Activation backward: malloc failed
BNNS Activation: number of memory descriptor is lower than expected
BNNS Activation: claiming pre allocated memory failed
BNNS Activation: memory allocation failed
BNNS Activation: Failed to init filter
BNNS Activation Apply Backward: At least one of in or out must be non-NULL
BNNS Activation Apply Backward: invalid filter
BNNS Activation Apply Backward: out_delta must not be NULL
BNNS Activation Backward: preallocated memory isn't supported
BNNS Activation Apply Backward: Input is required
BNNS Activation Backward: Weights Delta not supported
BNNS Activation Apply Backward: Bias Delta not supported
BNNS Activation Apply Backward: in place operation for none matching data sizes isn't supported
BNNS Activation Apply Backward: Input and output tensor doesn't match
BNNS Activation Apply Backward: In Delta tensor doesn't match
BNNS Activation Apply Backward: Out Delta tensor doesn't match
BNNS Activation Apply Backward: Weights Delta tensor (%zu x %zu) doesn't make sense
BNNS Activation Apply Backward: unable to allocate memory to compute backward with missing original output
BNNS Activation Apply Backward: unable to allocate memory to compute activation, using slower compute path
BNNS Activation Apply Backward: failed to allocate memory
BNNS Activation Apply Backward: failed to init index counter
BNNS Activation: invalid argument
BNNS Activation: in-place activation layer is allowed only for output types with the same or smaller storage size
BNNS Activation: unsupported types for conversion
BNNS Activation: val doesn't make sense
BNNS Activation Apply: activation function not supported
BNNS Activation Init: filter is null
BNNS Activation Init: input descriptor is illegal
BNNS Activation Init: output descriptor is illegal
BNNS Activation Init: layout doesn't match
BNNS Activation Init: memory allocation failed
BNNS Activation Init: dim %zu input size %zu != %zu output size
BNNS Activation Init: alpha can't be +/-inf or Nan for Gumbel or Gumbel Max
BNNS Activation Init: beta can't be +/-inf or Nan or zero or negative for Gumbel or Gumbel Max
BNNS Activation Init: BNNSActivationFunctionPReLUPerChannel is only valid with data layout BNNSDataLayoutImageCHW
BNNS Activation Init: invalid activation function
BNNS SoftMax: sum result shouldn't be 0
BNNS Activation Apply Backward: Softmax backward require original output
BNNS Activation Apply Backward: original inputs or original outputs must be available
BNNS Activation Apply Backward: activation function not supported
BNNS Activation: Error NULL input/output pointer
BNNS Activation: input and output dimensions do not match
BNNS Activation: input and output sizes do not match
Unsupported layout: %d
BNNS Multihead Attention Apply: key_mask must be a 1D tensor.
BNNS Multihead Attention Apply: key_mask must have type BNNSDataTypeBoolean.
BNNS Multihead Attention Apply: key_mask must have size exactly source_length (key_mask.size[0] = %zu, source_length = %zu).
BNNS Multihead Attention Apply: 2D add_to_attention must have shape (target_length, source_length) = (%zu, %zu), but passed tensor has shape (%zu, %zu).
BNNS Multihead Attention Apply: 3D add_to_attention must have shape (num_heads, target_length, source_length) = (%zu, %zu, %zu), but passed tensor has shape (%zu, %zu, %zu).
BNNS Multihead Attention Apply: 4D add_to_attention must have shape (batch_size, num_heads, target_length, source_length) = (%zu, %zu, %zu, %zu), but passed tensor has shape (%zu, %zu, %zu, %zu).
BNNS Multihead Attention Apply: unsupported layout for argument add_to_attention.
BNNS Multihead Attention Apply: add_to_attention must have type BNNSDataTypeBoolean or BNNSDataTypeFloat32.
BNNS Multihead Attention Apply: If backprop_cache is non-NULL, backprop_cache_size must also be non-NULL
BNNS Multihead Attention Apply: If workspace is non-NULL, workspace_size must also be non-NULL
BNNS Multihead Attention Apply: Supplied workspace is too small (size %ld, but required %ld)
Multihead Attention Backward: Only support for a full-size backprop_cache has been implemented.
Multihead Attention Backward: If workspace is non-NULL, workspace_size must also be non-NULL
Multihead Attention Backward: Insufficient workspace supplied (%zu bytes). Require %zu bytes.
value_delta_copy_out
value_shadow_delta
value_delta
key_delta_copy_out
key_shadow_delta
key_delta
query_delta_copy_out
query_shadow_delta
query_delta
layer_params->query.target_desc
layer_params->key.target_desc
layer_params->query.weights
layer_params->key.weights
layer_params->value.weights
(d_model, d_key, num_heads)
(source_length, k_dim)
(k_dim, d_key, num_heads)
layer_params->value.target_desc
(source_length, v_dim)
(v_dim, d_value, num_heads)
layer_params->output.target_desc
(target_length, d_model)
layer_params->output.weights
(num_heads*d_value, d_model)
BNNS Multihead Attention Create: layer_params->key_attn_bias.data and layer_params->value.attn_bias.data must both be NULL or must both be not NULL
layer_params->key_attn_bias
(d_key, num_heads)
layer_params->value_attn_bias
(d_value, num_heads)
BNNS Multihead Attention Create: fp16 and bf16 may not be mixed or unsupported data type
layer_params->query.bias
layer_params->key.bias
layer_params->value.bias
layer_params->output.bias
(d_model)
BNNS MHA: layer_params->query.weights.layout=%u is not compatible with low memory flag.
BNNS MHA: layer_params->key.weights.layout=%u is not compatible with low memory flag.
BNNS MHA: layer_params->value.weights.layout=%u is not compatible with low memory flag.
BNNS Multihead Attention Create: Unsupported layout for query.weights
BNNS Multihead Attention Create: Unsupported layout for key.weights
BNNS Multihead Attention Create: Unsupported layout for value.weights
BNNS MULTIHEAD ATTENTION GetPointer: Layer was created with dropout=0.0, cannot return pointer to dropout value.
BNNS MULTIHEAD ATTENTION GetPointer: Unsupported target %d
BNNS Multihead Attention Create: Expected %s to have exactly %zu dimensions, but it has %zu
BNNS Multihead Attention Create: Expected %s to have shape %s = (
) but has actual shape (
 = reshape(from=
BNNS SPECIALIZED CONVOLUTION Create: failed to alloacted memory
BNNS Permute Filter: layer_params is NULL
BNNS Permute Filter: illegal input descriptor
BNNS Permute Filter: illegal output descriptor
BNNS Permute Filter: input and output descriptor layout dimension does not match. input dimm = %zu output dimm = %zu
BNNS Permute Filter: permutation array index %zu has illegal value of %zu
BNNS Permute Filter: permutation array is missing axis %zu
BNNS Permute Filter: permutation array axis %zu appears %zu times, each axis should appear exactly once
0123456789abcdefghijklmnopqrstuvwxyz
 -> out_
BNNS Shuffle: input and output data type do not match
BNNS Shuffle: input or output data is NULL
BNNS Shuffle: input and output dimensions should be 4
BNNS Shuffle: input and output sizes are not valid
BNNS Shuffle: input and output sizes do not match
BNNS Pooling Filter: filter is NULL
BNNS Pooling Filter: input is NULL
BNNS Pooling Filter: output is NULL
BNNS Pooling Filter Warning: Batch size is 0, nothing to do
BNNS Pooling: input batch stride doesn't make sense (%zu < %zu x %zu)
BNNS Pooling: dilation supported only in BNNSPoolingFunctionMax/UnMax
BNNS Pooling UnMax: indices array is NULL
Pooling layer filter running slow path: stride=%zu,%zu kernel=%zu,%zu
BNNS Pooling: case not implemented
BNNS Pooling Apply: allocation of work buffer failed
BNNS Pooling Backward: cannot run backward without output delta
BNNS Pooling Backward: only float32 bfloat16 output delta are supported
BNNS Pooling Backward: only float32 bfloat16 input delta are supported
BNNS Pooling Backward: only float32 bfloat16 bias delta are supported
BNNS Pooling Backward: unsupported pooling function
BNNS Pooling Backward: invalid argument
BNNS Pooling Backward: Fusion of compute with activation is unsupported for given activation function
BNNS Pooling Backward: failed to allocate memory
BNNS Pooling Backward: failed to apply activation backward
BNNS Pooling: layer parameters is NULL
BNNS Pooling: input must be a 3D array
BNNS Pooling: input descriptor is illegal
BNNS Pooling: output must be a 3D array
BNNS Pooling: output descriptor is illegal
BNNS Pooling: input/output channel counts do not match
BNNS Pooling: input/output types do not match
BNNS Pooling: invalid kernel dimensions, should be greater than 0
BNNS Pooling: optimized code supports kernel width/height up to 16
BNNS Pooling: dilation only supported for BNNSPoolingFunctionMax/UnMax
BNNS_POOLING: input size computation wraparound input (%zu x %zu), output (%zu x %zu), stride (%zu x %zu), padding (left %zu right %zu up %zu down %zu)
BNNS_POOLING: input size computation wraparound input (%zu x %zu), output (%zu x %zu), stride (%zu x %zu), padding (%zu x %zu)
BNNS_POOLING: output(%zu x %zu) is larger than input(%zu x %zu) with padding(left %zu right %zu up %zu down %zu).
 stride (%zu x %zu)
BNNS_POOLING: output(%zu x %zu) is larger than input(%zu x %zu) with padding(%zu x %zu).
 stride (%zu x %zu)
BNNS_POOLING: invalid pooling function
BNNS_POOLING: supported input/output data types: float32 float16 bfloat16
BNNS_POOLING: slow path: stride not in {1,2}
BNNS_POOLING: slow path: kernel size not in {2,3,4}
BNNS Pooling: Internal Memory size %zu doesn't match expected %d
BNNS Pooling: claiming pre allocated memory failed
BNNS Pooling: failed to allocate context
BNNS Pooling Backward: only float32 bfloat16 delta are supported
BNNS Pooling Backward: wrong pooling function called
 = stack(values=[
BNNS Reduction Create: Invalid reduction function %u
BNNS Reduction Create: weight data type does not match input
BNNS Reduction Create: i_desc and o_desc must have the same number of dimensions (%zu vs %zu)
BNNS Reduction Create: i_desc and w_desc must have the same number of dimensions (%zu vs %zu)
BNNS Reduction Create: i_desc and w_desc dimensions must have the same size (dimension %zu: %zu vs %zu)
BNNS Reduction Create: i_desc and o_desc dimensions must have the same size, or o_desc.size[d] must be 1 to indicate reduction (dimension %zu: %zu vs %zu)
BNNS Reduction Create: ArgMin/ArgMax reduction is only supported on a single axis
BNNS Reduction Apply: Both input and output must non-NULL
BNNS Reduction Backward: input_delta must have the same number of dimensions as i_desc
BNNS Reduction Backward: input_delta must have the same shape as i_desc
BNNS Reduction Backward: output_delta must have the same number of dimensions as o_desc
BNNS Reduction Backward: output_delta must have the same shape as o_desc
BNNS Reduction Backward: output_delta must not be NULL
BNNS Reduction Backward: weights_delta must have the same number of dimensions as w_desc
BNNS Reduction Backward: weights_delta must have the same shape as w_desc
BNNS Reduction Direct Apply: Size of the dimensions of the output can not be greater than the size of the input
BNNS Reduction Direct Apply: ArgMin/ArgMax reduction is only supported on a single axis
BNNS Reduction: Unsupported data type combination for %u
BNNS Reduce Apply Generic: Size mismatch in dimension %zu: input %zu, output %zu
Backward usage is not supported for reduction type %u
BNNS Reduction Backward: Reduce function %u requires input to be supplied for backward computation
BNNS Reduction Backward: Reduce function %u requires output to be supplied for backward computation
BNNS Reduction Backward: Calculation of weights_delta requires input to be supplied
Size mismatch in dimension %zu: input %zu, output %zu
BNNS Sparse Fully Connected: out of scratch memory to encode
BNNS Fully Conneceted Sparsify: Sparse data is greater than dense prune, developer should use dense prune
BNNS Sparse Fully Connected: failed to apply activation
BNNS Arithmetic Filter Internal: unsupported arithmetic with epsilon function
BNNS Arithmetic Filter: layer_params is NULL
BNNS Arithmetic Filter: unsupported arithmetic function
BNNS Arithmetic Filter: memory allocation failed
BNNS Arithmetic Filter: Failed to init filter
BNNS Arithmetic Filter: filter is NULL
BNNS Arithmetic Filter: wrong filter type, this function should only be used for a filter created with BNNSFilterCreateLayerArithmetic
BNNS Arithmetic Filter: in pointer is NULL
BNNS Arithmetic Filter: batch_size>1 and in_stride is 0
BNNS Arithmetic Filter Internal: expected number of inputs call failed
BNNS Arithmetic Filter: wrong number_of_inputs, number_of_inputs=%zu, expecting %zu
BNNS Arithmetic Filter: in[%zu] is NULL
BNNS Arithmetic Filter: in_stride[%zu] is 0
BNNS Arithmetic Filter: output pointer is NULL
BNNS Arithmetic Filter: batch_size>1 and out_stride is 0
BNNS Arithmetic Filter: only BNNSDataTypeFloat32/BFloat16/Float16 is supported
BNNS Arithmetic Filter: only BNNSDataTypeFloat32/BNNSDataTypeBFloat16 is supported
BNNS Arithmetic Filter Internal: unsupported arithmetic function
BNNS Arithmetic Filter: out delta is not valid
BNNS Arithmetic Filter: in_delta is not valid
BNNS Arithmetic Filter: in is NULL, but it is required for backward compute
BNNS Arithmetic Filter: in_delta descriptor %zu is not valid
BNNS Arithmetic Filter: cannot compute activation backward, output is NULL
BNNS Arithmetic Filter: Fusion of arithmetic with activation is unsupported for given activation function
BNNS Arithmetic Filter Backward Internal: unsupported arithmetic function
BNNS Arithmetic Filter Backward: failed to apply activation backward
BNNS Arithmetic Filter: input type is BNNSConstant, no gradient to compute
BNNS Arithmetic Filter: in_delta[0] must be NULL for BNNSArithmeticSelect
BNNS Arithmetic Filter: arithmetic_function_fields is NULL
BNNS Arithmetic Filter: malloc failed
BNNS Arithmetic Filter: unsupported out_type/in_type
BNNS Arithmetic Filter: in, out descriptor check failed
BNNS Arithmetic Filter: in1,in2,out descriptor check failed
BNNS Arithmetic Filter: in1,in2,in3,out descriptor check failed
BNNS Arithmetic: Activation creation failed
BNNS Arithmetic Filter: input size[%zu]=%zu does not equal max input,output size of %zu or 1
BNNS Arithmetic Filter: input%zu and input%zu have the same data pointer, but descriptors are different
BNNS Arithmetic Filter: input%zu and input%zu have the same data pointer, but different BNNSDescriptorType
BNNS Arithmetic Filter: descriptors for input that is being processed in-place and output must exactly match
BNNS Arithmetic Filter: input is being processed in-place but input and output descriptor types do not match
BNNS Arithmetic Filter: output dimension smaller than input is not supported. batch size>1, input type is BNNSSample but out_type is not BNNSSample
BNNS Arithmetic Filter: backward compute for chosen arithmetic function does not allow inplace gradient. out delta data must not be the same as input delta data
BNNS Arithmetic Filter: in_delta%zu is NULL but in%zu type is not BNNSConstant
BNNS Arithmetic Filter: in_delta%zu->data is NULL but in%zu type is not BNNSConstant
BNNS Arithmetic Filter: in%zu type is BNNSSample but in_delta_stride[%zu] is 0
BNNS Arithmetic Filter: arithmetic input_delta and input descriptors must have the same sizes, strides and data types
BNNS Arithmetic Filter: input%zu delta data pointer is not NULL. cannot compute gradient for BNNSConstant
BNNS Arithmetic Filter: arithmetic output_delta and output descriptors must have the same sizes, strides and data types
BNNS Arithmetic Filter: input%zu delta and input%zu delta has the same data pointer, but descriptors are different
BNNS Arithmetic Filter: input%zu delta and input%zu delta has the same data pointer, but different data types
BNNS Arithmetic Filter: input%zu delta and input%zu delta has the same data pointer, but different in_delta_stride
BNNS Arithmetic Filter: forward input pointer points to the same array but input gradient pointers point to different arrays
BNNS Arithmetic Filter: descriptors for out_delta that is being processed in-place and in1_delta must exactly match
BNNS Arithmetic Filter: input1 delta is being processed in-place but input type and output type are different
BNNS Arithmetic Filter: input1 delta and output delta has the same data pointer (inplace), but different in_delta_stride
BNNS Arithmetic Filter: input1 delta and output delta has the same data pointer (inplace), but in1_delta.flags indicates to accumulate result
BNNS Arithmetic Filter: output dimension smaller than input is not supported. batch size>1, an input type is BNNSSample but out_type is not BNNSSample
BNNS Fully Connected Sparsify: failed to allocate scratch buffer
BNNS Crop Resize: data types do not match
BNNS Crop Resize: dimension of input, roi, or output is not correct
BNNS Crop Resize: size of roi is not [N, 4] or [N, 5]
BNNS Crop Resize: size of output is not [N, B, C, OH, OW] or [N, 1, C, OH, OW]
BNNS Crop Resize: only linear method is supported now
BNNS Crop Resize: Unsupported data type
BNNS Crop Resize: sampling mode not supported
BNNS Embedding Create: Only 8, 16, 32 and 64 bit integer types may be used as input data type.
BNNS Embedding Create: Only 16 and 32 bit floating point and 8, 16 and 32 bit integer types may be used as dictionary data type.
BNNS Embedding Create: o_desc dimension must be dim(i_desc) + dim(dictionary) - 1.
BNNS Embedding Create: First dim(dictionary)-1 dimensions of o_desc and dictionary must have consistent shapes.
BNNS Embedding Create: Last dim(i_desc) dimensions of o_desc and i_desc must have consistent shapes.
internal embedding copy
internal embedding copy back
BNNS Embedding Apply: Input value %zu is out of range [0, %zu)
BNNS Embedding ApplyBackward: weights_delta must have the same shape as layer_params->dictionary
BNNS Embedding ApplyBackward: out_delta must have the same shape as layer_params->o_desc
BNNS Embedding ApplyBackward: Input value %zu out of range
BNNS Sparse Embedding Create: Only 8, 16, 32 and 64 bit integer types may be used as input data type.
BNNS Sparse Embedding Create: Only 16 and 32 bit floating point and 8, 16 and 32 bit integer types may be used as dictionary data type.
BNNS Sparse Embedding Create: o_desc dimension must be dim(i_desc) + dim(dictionary) - 1.
BNNS Sparse Embedding Create: First dim(dictionary)-1 dimensions of o_desc and dictionary must have consistent shapes.
BNNS Sparse Embedding Create: Last dim(i_desc) dimensions of o_desc and i_desc must have consistent shapes.
BNNS Sparse Embedding Create: Unsupported optimization function
BNNS Sparse Embedding Create: Dictionary must be of type BNNSDataTypeFloat32
BNNS Sparse Embedding Apply: Filter is no longer valid
BNNS Sparse Embedding Apply: Input value %zu is out of range [0, %zu)
BNNS Sparse Embedding Apply Backwards: Filter is no longer valid
BNNS Embedding Backwards Sparse: Accumulation of sparse weights gradient is not supported.
BNNS Embedding Backwards Sparse: out_delta must have the same shape as layer_params->o_desc
BNNS Embedding Backwards Sparse: dictionary_delta->indices must have a single dimension
BNNS Embedding Backwards Sparse: dictionary_delta->indices.size[0] must be 1
BNNS Embedding Backwards Sparse: dictionary_delta->sparse_dimension_size[0] must be num_embeddings
BNNS Embedding Backwards Sparse: dictionary_delta->values shape must match dictionary item shape
BNNS Embedding Backwards Sparse: For in-place indices, dictionary_delta->count must equal the total size of layer_params->i_desc * batch_size
BNNS Embedding Backwards Sparse: For in-place indices, input must be contiguous
BNNS Embedding Backwards Sparse: For in-place values, dictionary_delta->count must equal the total size of out_delta * batch_size
BNNS Embedding Backwards Sparse: For in-place values, out_delta must be contiguous in its indexing dimensions
BNNS Sparse Embedding Get Representation: Filter is no longer valid
BNNS Sparse Embedding Get Representation: Unexpected value for num_accumulators=%zu (expected %zu)
BNNS Sparse Embedding Get Representation: num_opt_field_changes!=NULL but opt_field_changes_indices is NULL
BNNS Sparse Embedding Get Representation: num_opt_field_changes!=NULL but opt_field_changes is NULL
BNNS Sparse Embedding Get Representation: num_opt_field_changes=NULL, but optimization fields of filter have previosuly been updated
BNNS Sparse Embedding Set Representation: Filter is no longer valid
BNNS Embedding Set Sparse Optimizer Context: Expected %zu accumulators
BNNS Embedding Set Sparse Optimizer Context: values[0]->indices must have dimension 1
BNNS Embedding Set Sparse Optimizer Context: values[0]->indices.size[0] must be 1
BNNS Embedding Set Sparse Optimizer Context: Expected values indices descriptors to be identical (that is point to the same data)
BNNS Embedding Set Sparse Optimizer Context: Expected values indices descriptors to be identical
dest
BNNS Sparse Embedding Optimizer Step: Filter is no longer valid
BNNS Embedding Sparse Optimizer Step: Expected dictionary_delta->indices to have a single dimension
BNNS Embedding Sparse Optimizer Step: Expected dictionary_delta->values to match shape of dictionary
BNNS Embedding Sparse Optimizer Step: Dictionary gradient items must be contiguous
BNNS Embedding Sparse Optimizer Step: Dictionary items must be contiguous
BNNS Embedding Sparse Optimizer Step: grad.data_type must be BNNSDataTypeFloat32
BNNS Sparse Embedding Get Dense: Filter is no longer valid
BNNS Sparse Embedding Get Dense: Unexpected value for num_accumulators=%zu (expected %zu)
BNNS Sparse Embedding Get Dense: Unexpected number of dimensions for accumulator[%zu]
BNNS Sparse Embedding Get Dense: Unsupported data type for accumulator[%zu]
BNNS Sparse Embedding Get Dense: Unexpected shape for accumulator[%zu]
BNNS Sparse Embedding Get Dense: accumulator[%zu] must be contiguous in dictionary item dimensions
BNNS batch norm forward: malloc for mean failed
BNNS batch norm forward: malloc for var failed
BNNS batch norm forward: malloc for isqrtvar failed
BNNS Batchnorm Backward Apply: delta allocation failed
BNNS Batchnorm Backward Apply: activation_grad allocation failed
BNNS instance norm backward: malloc for dbeta failed
BNNS instance norm backward: malloc for dgamma failed
BNNS Layer Norm Apply Backward: malloc activation_grad failed
BNNS Layer Norm Apply Backward: dx hat allocation failed
BNNS Group Norm Apply Backward: failed to allocate memory
BNNS Group Norm Apply Backward: failed to allocate activation grad memory
BNNSActivationFunctionIntegerLinearSaturate isn't supported in apply_bias_and_activation
BNNSActivationFunctionIntegerLinearSaturatePerChannel isn't supported in apply_bias_and_activation
BNNSActivationFunctionIntegerLinearSaturate isn't supported in apply_bias_and_activation_fp16
BNNSActivationFunctionIntegerLinearSaturatePerChannel isn't supported in apply_bias_and_activation_fp16
BNNSActivationFunctionIntegerLinearSaturate isn't supported in apply_bias_and_activation_bf16
BNNSActivationFunctionIntegerLinearSaturatePerChannel isn't supported in apply_bias_and_activation_bf16
allocation failed, size=%zu
reallocation failed, size=%zu
allocation failed, size=%zu, align=%zu
BNNS: unexpected data type, failing
BNNS: Data type not supprted, fail
BNNS: layout not supported
BNNS: active dimension must be greater than 0
BNNS: dimension %zu stride %zu is lower then previous dimension actual size %zu * %zu (size*stride)
BNNS: Opaque data structure doesn't match descriptor information
BNNS: Opaque data structure has incorrect information
BNNS: memory usage exceeded capacity
BNNS PreAllocated Memory: memory usage exceeded capacity
BNNS PreAllocated Memory: failed to claim scratch memory
BNNS Affine Resize: layer parameter is not supported
BNNS Affine Resize: input data type or size is not supported
BNNS Affine Resize: affine data type or size is not supported
BNNS Affine Resize: output data type or size is not supported
BNNS Create Convolution Winograd: malloc failed
BNNS Convolutions Winograd: weights allocation failed
BNNS Convolutions Winograd: bias allocation failed
BNNS Apply Convolution Winograd: memory allocation failed
weight packing must be at least 32 and a power of 2
repacked(
, for=
conv_input_slice(
 = conv(x=
, weights=
, bias=
<none>
, strides=[
], pad=[
], dilation=[
], groups=
UNKNOWN
OPAQUE
STRING
: BNNS Graph only supports FP16 and FP32 operands for this function
Rejected constexpr elimination of op 
 as it increased data footprint by 
 bytes
Constexpr elimination of op 
 would decrease weight footprint, but is not yet supported
Unsupported repack operation from layout=%u to layout=%u
BNNSCopy: Unsupported repack operation from layout=%u to layout=%u
BNNSCopy: src->size[%zu]=%zu is not equal to dest->size[%zu]=%zu
BNNSCopy: Cannot perform type conversion as part of opaque repack
temp_desc
hw.cpufamily
hw.cpusubfamily
hw.physicalcpu
hw.perflevel0.l2cachesize
hw.perflevel1.l2cachesize
BNNS Convolution ARM64: only stride 1x1 and 2x2 are supported
 = slice_by_index(x=
, begin=[
], end=[
], stride=[
BNNS Normalization Apply Backward: input gradient data type not supported
BNNS Normalization Apply Backward: malloc activation_scratch failed
BNNS instance norm backward: malloc for dbeta and dgamma failed
' is not a tensor
' has a non-constant dimension
BNNS Graph Compile: Unsupported data type
BNNS Graph Compile: Attempted to parse unsupported op '%s' (reversed name '%s'
BNNS Convolution Apply: input batch stride doesn't make sense (%zu < %zu x %zu)
BNNS Convolution Apply: output batch stride doesn't make sense (%zu < %zu x %zu)
BNNS Convolution Apply: failed to allocate weights in apply
BNNS Convolution Backward: BNNSNDArrayFlagBackpropAccumulate has not yet been implemented for weights_delta or bias_delta.
BNNS Convolution Backward: cannot run backward without output delta
BNNS Convolution Backward: [out delta/in delta/weight delta/bias delta] - data type check failed. unsupported data type
BNNS Convolution Backward: internal error, incorrect wrapper
BNNS Convolution forward output data type does not match output delta data type
BNNS Convolution forward input data type does not match input delta data type
BNNS Convolution forward weights data type does not match weights delta data type
BNNS Convolution weight packing is not supported backward
BNNS Convolution Backward: BNNSFlagsUseClientPtr must be enabled during training
BNNS Convolution Backward: invalid argument
BNNS Convolution Backward: Fusion of compute with activation is unsupported for given activation function
BNNS Convolution Backward: failed to allocate memory
BNNS Convolution Backward: failed to apply activation backward
BNNS Conv: Unsupported weight format for Input delta compute
BNNS Convolution Apply Backward: could not create transposed convolution filter
BNNS Convolution Apply Backward: Convolution input delta computation failed
BNNS Transposed Convlution Apply: failed to allocate weights buffer
BNNS Transposed Convlution Apply: failed to allocate backward weights buffer
BNNS Transposed Convlution Apply: failed to allocate out delta buffer
BNNS Transposed Convlution Apply: failed to manipulate output delta
BNNS Transposed Convlution Apply: could not create convolution filter
BNNS Convolution Apply Backward: allocation failed
BNNS Convolution Apply Backward: Transposed Convolution Backward Compute failed
transposed convolution parameter error
transposed convolution backward failed
Convolution bias delta failed
BNNS Create Layer Convolution: fused padd convolution allocation failed
BNNS Create Layer Convolution: padding layer creation failed
BNNS Create Layer Convolution: padding descriptor creation failed
BNNS Create Layer Convolution: input channels must be divisible by groups
BNNS Create Layer Convolution: output channels must be divisible by groups
BNNS Create Layer Convolution: bias must match output channels
BNNS Create Layer Convolution: grouped convolution weight layout must be BNNSDataLayoutConvolutionWeightsOIHW
BNNS Create Layer Convolution: failed to allocate memory to copy weights and bias
BNNS Create Layer Convolution: failed to create forward convolution filter
BNNS Create Layer Convolution: failed to create wrapper filter
BNNS Convolution Create: incompatible numbers of channels between images and convolution parameters
BNNS Convolution Create: unsupported weight format
BNNS Convolution Create: input must be a 3D array
BNNS Convolution Create: input descriptor is illegal
BNNS Conv: output must be a 3D array
BNNS Convolution Create: output descriptor is illegal
BNNS Convolution Create:  weights descriptor is illegal
BNNS Convolution Create: not allowed to delay allocation to apply if weights ptr isn't maintained by Client
BNNS Convolution Create: failed to create dilated convolution
BNNS Convolution Create: failed to create packed weights convolution
BNNS Convolution Create: failed to create grouped convolution
BNNS Convolution Create: failed to allocate weights buffer
BNNS Convolution Create: failed to allocate bias buffer
BNNS Convolution Create: input data type is not supported
BNNS Convolution Create: output data type is not supported
BNNS Convolution Create: weight data type is not supported
BNNS Convolution Create: int8/uint8 output supported only with int8/uint8 inputs and weights
BNNS Convolution Create: int16/uint16 output is not supported
BNNS Convolution Create: uint32 output is not supported
BNNS Convolution Create: convolution doesn't support indexed weights
BNNS Convolution Create: failed to allocate memory
BNNS Convolution Create: failed to allocate weights descriptor
BNNS Convolution Create: convert weights
BNNS Convolution Create: failed to prepare blocks
BNNS Convolution Create: failed to create compute blocks
BNNS Grouped Convolution apply: groups should not be used if groups <= 1
BNNS Grouped Convolution apply: weights are null 
BNNS Grouped Convolution apply: failed to create context
BNNS Grouped Convolution apply: unsupported convolution filter type 
BNNS Convolution Apply Backward:  filter is NULL
BNNS Convolution Apply Backward: out_delta is NULL
BNNS Convolution weight delta: malloc failed
BNNS Convolutions Apply: unexpected output data type
BNNS Convolution Backward: out_delta descriptor memory layout is not contiguous
BNNS Convolution bias delta: malloc failed
BNNS Convolution bias delta: upconverting tensors failed
BNNS Convolution: malloc activation_grad failed
Convolution backward: in_delta, weights_delta, bias_delta descriptor pointers are all NULL
BNNS Convolution Backward: in_delta descriptor memory layout is not contiguous
BNNS Convolution Backward: weights_delta descriptor memory layout is not contiguous
BNNS Convolution Backward: bias_delta descriptor memory layout is not contiguous
BNNS Convolution Backward: out_stride and out_delta_stride do not match
BNNS Convolution Backward: out_delta_stride [%zu] does not match out_delta descriptor memory size [%zu]
BNNS Convolution Backward: in_stride and in_delta_stride do not match
BNNS Convolution Backward: in_delta_stride [%zu] does not match in_delta descriptor memory size [%zu]
BNNS gradient computation for Transposed Convolution with groups is not supported
BNNS: unsupported resize forward data type
BNNS Resize: input_delta has %zu dimensions but must match input that has %zu
BNNS Resize: output_delta has %zu dimensions but must match output that has %zu
BNNS Resize: input_delta size[%zu]=%zu but must match input size[%zu]=%zu
BNNS Resize: output_delta size[%zu]=%zu but must match output size[%zu]=%zu
BNNS: unsupported resize backward data type
BNNS Resize: Unsupported data type
BNNS Resize: Input and output have different number of dimensions
BNNS Resize: resize must be in same direction for all direction (request %d downsample and %d upsample)
BNNS Resize: Linear interpolation requires resize in at most two dimensions, but %d dimensions are resized
BNNS Resize: Unsupported interpolation method %d
Downsampling not supported for dimension > 2.
BNNS Resize: downsampling has no support for 3 or more dimensions.
BNNS Resize: downsampling has no support for 3 or higher dimension.
BNNS CONVOLUTIONS CUSTOMIZED: layer param is NULL
BNNS CONVOLUTIONS CUSTOMIZED: malloc failed
BNNS CONVOLUTIONS CUSTOMIZED: malloc failed
BNNS CONVOLUTIONS CUSTOMIZED: failed to allocate bias memory
BNNS CONVOLUTIONS CUSTOMIZED: failed to allocate input repack memory
%{name=layer}s %{name=direction}s
BNNSDirectApply
Filter
Pooling
%{name=direction}s
Quantizer
ClipByValue
ClipByGlobalNorm
ComputeNorm
SparseEmbedding
SparseEmbeddingOptimzierStep
RandomFill
MultiheadAttention
Loss
Arithmetic
Permute
Normalization
FusedFilter
BNNS Signposts Enabled=%c
T_:
T`b:
Tpl:
TX+;
T\e@
