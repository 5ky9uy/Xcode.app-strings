INTERNAL: bwd
^9giP9giP9giP9giP9
????????????????
_h2m
?fff
U?DH
y4#@y4#@y4#@y4#@
r1?V
B6"9B6"9B6"9B6"9N
!rr{>
!rr{>
}76R
YVpA
YVpA
YVpA
<7p7M}
<7p7M}
<7p7M}
:I~D
:I~D
:I~D
:I~D
?G/d?
*?9s
)>e\
^9giP9
BNNS Dropout: Error layer_params is NULL
BNNS Dropout: rate must be in the range [0.0, 1.0].
BNNS Dropout: Unsupported input data type
BNNS Dropout: Input and Output data types must be the same
BNNS Dropout: Error memory allocation failed
BNNS Dropout: Error input descriptor is invalid
BNNS Dropout: Error output descriptor is invalid
BNNS Dropout: Input and output descriptors must have the same shape.
BNNS Dropout: Tensors with dimension greater than %u are not supported.
BNNS Dropout: Error filter is NULL
BNNS Dropout: Error wrong filter type, filter is not Dropout
BNNS Dropout: input pointer is NULL
BNNS Dropout: batch_size>1 and in_stride is 0
BNNS Dropout: output pointer is NULL
BNNS Dropout: batch_size>1 and out_stride is 0
BNNS BatchNorm: Error filter is NULL
BNNS Dropout: input_delta layout, shape and stride must match layer_params->i_desc
BNNS Dropout: output_delta layout, shape and stride must match layer_params->o_desc
Unsupported target %d
v16@?0Q8
v24@?0Q8^v16
Shouldn't call get_data_size with Indexded2 or Indexed4, switch to get_data_bits
BNNS Tensor Contraction: inputB pointer must not be NULL unless B is a weight or the operation is quadratic.
BNNS Tensor Contraction: After adding batch dimension, inputA tensor has too many indices
BNNS Tensor Contraction: After adding batch dimension, inputB tensor has too many indices
BNNS Tensor Contraction: After adding batch dimension, output tensor has too many indices
BNNS Tensor Contraction: inB_delta calculation is non-sensical for a contraction of a tensor with itself.
BNNS Tensor Contraction: inA_delta calculation requires inA to be non-NULL
BNNS Tensor Contraction: inA_delta calculation requires out_delta (and out_delta->data) to be non-NULL
inA_delta
BNNS Tensor Contraction: After adding batch dimension, out_delta tensor has too many indices
BNNS Tensor Contraction: After adding batch dimension, inputA_delta tensor has too many indices
BNNS Tensor Contraction: inA_delta calculation requires inB to be non-NULL
BNNS Tensor Contraction: inB_delta calculation requires inA to be non-NULL
BNNS Tensor Contraction: inB_delta calculation requires out_delta (and out_delta->data) to be non-NULL
inB_delta
BNNS Tensor Contraction: After adding batch dimension, inputB_delta tensor has too many indices
BNNS Tensor Contraction: beta must be 0.0 or 1.0 (beta=%.2f)
BNNS Tensor Contraction: inputA descriptor is illegal "%s"
BNNS Tensor Contraction: ouput descriptor is illegal "%s"
BNNS Tensor Contraction: invalid op string "%s"
BNNS Tensor Contraction: inputB descriptor is illegal "%s"
BNNS Tensor Contraction: both inputA and inputB cannot be weights "%s"
BNNS Tensor Contraction: Wildcard index '*' must appear on both sides of operation or neither: "%s"
BNNS Tensor Contraction: Wildcard index '*' must appear at most once per index set: "%s"
BNNS Tensor Contraction: Wildcard index '*' must appear consistently only at start or end of index sets: "%s"
BNNS Tensor Contraction: number of indices from operation (%td) does not match dimension of input A descriptor (%zu) "%s"
BNNS Tensor Contraction: number of indices from operation (%td) does not match dimension of input B descriptor (%zu) "%s"
BNNS Tensor Contraction: number of indices from operation (%td) does not match dimension of output descriptor (%zu) "%s"
BNNS Tensor Contraction: '%c', index %zu of inputA has size %zu and index %td of output has size %zu, but sizes are required to match
BNNS Tensor Contraction: '%c', index %d of inputA has size %zu and index %td of inputB has size %zu, but sizes are required to match
BNNS Tensor Contraction: index '%c' of input A does not match any index of inputB or output "%s"
BNNS Tensor Contraction: '%c', index %d of inputB has size %zu and index %td of output has size %zu, but sizes are required to match
BNNS Tensor Contraction: index '%c' of input B does not match any index of inputA or output "%s"
BNNS Tensor Contraction: index '%c' of output does not match any index of inputA or inputB "%s"
Data type combination not supported
%c_*ki, %c_*jk -> c_*ij
%c_*ki, %c_*kj -> c_*ij
%c_*ik, %c_*jk -> c_*ij
%c_*ik, %c_*kj -> c_*ij
BNNS Tensor Contraction: %s shape does not match inputA shape in dimension %ld (%zu vs %zu)
BNNS Tensor Contraction: out_delta shape does not match output shape in dimension %ld (%zu vs %zu)
BNNS Tensor Contraction: Wildcard index '*' can only appear as first or last index of "%s_%s"
BNNS Tensor Contraction: repeated indices "%s_%s"
v16@?0^v8
v24@?0Q8Q16
BNNS SoftMax: sum result shouldn't be 0, something is wrong
BNNS SoftMax BF16: sum result shouldn't be 0, something is wrong
malloc activation_grad failed
apply_specialized_convolution_PKT_0: o_height 1 not supported
apply_specialized_convolution_PKT_1: o_height less then 4 isn't supported
apply_specialized_convolution_PKT_2: o_height 1 not supported
apply_specialized_convolution_PKT_3: o_height 1 not supported
apply_specialized_convolution_PKT_3: o_height 2 not supported
BNNS Copy Filter: in and out must not be NULL.
BNNS Copysum Filter: unsupported data type conversion
internal bwd copysum
BNNS Tensor Contraction: Input must not be NULL
BNNS Tensor Contraction: Output must not be NULL
BNNS Tensor Contraction: index '%c' appears multiple times on right-hand side, but with different sizes
BNNS Tensor Contraction: '%c', index %d of input has size %zu and index %td of output has size %zu, but sizes are required to match
BNNS Tensor Contraction: index '%c' appears multiple times on left-hand side, but with different sizes
BNNS Tensor Contraction: Unsupported data type conversion %sfrom %s to %s
(in summation) 
(in copy) 
BNNSTranspose: src and dest have a different number of dimensions.
BNNSTranspose: specified axes must be within range for supplied tensor (axes: %zu, %zu) dimension of tensor is %zu
internal_transpose
internal_copy
int1
int2
int4
int8
int16
int32
int64
uint1
uint2
uint4
uint8
uint16
uint32
uint64
bf16
fp16
fp32
indexed8
indexed4
indexed2
indexed1
bool
unknown
BNNS CONVOLUTIONS VERSION2: Error minimum float w_pack weight packing value is 32
BNNS Dequantize: input and output can't be null
BNNS Dequantize: in place conversion not supported
BNNS Dequantize: only __fp16 output is supported
BNNS Dequantize: lut is null and needed for Indexed input type
BNNS Dequantize: input type not supported
BNNS Layer Norm Apply: failed to allocate scratch memory
BNNS Batchnorm Apply Backward: activation allocation failed
BNNS Batchnorm Apply Backward: failed to apply activation backward
BNNS Instance Norm Apply: input stride (%zu) and output stride (%zu) aren't set correctly
BNNS Instance Norm Apply: failed to allocate scratch memory
BNNS Instance Norm Apply Backward: activation allocation failed
BNNS Instance Norm Apply Backward: failed to apply activation backward
BNNS Layer Norm Apply: input stride (%zu) and output stride (%zu) aren't set correctly
BNNS Layer Norm Apply Backward: activation allocation failed
BNNS Layer Norm Apply Backward: failed to apply activation backward
BNNS Group Norm Apply: failed to allocate scratch memory
BNNS Group Norm Apply Backward: activation allocation failed
BNNS Group Norm Apply Backward: failed to apply activation backward
BNNS Fully Connected Tiny Apply: unexpecetd weights layout
BNNS Fully Connected Apply: allocation of work buffer failed
BNNS Fully Connected: Unexpected result, should be at least 1
BNNS Fully Connected: Unexpected result, at least 1 batch should remain
BNNS Fully Connected: Unexpected result, at least 1 output should remain
BNNS Fully Connected: Failed to fit in memory limit, using generic code
BNNS FUlly: allocation failed
BNNS Fully Connected: weights conversion buffer isn't allocated.
BNNS Fully Connected: inputs conversion buffer isn't allocated.
BNNS Fully Connected Backward: output delta is NULL
BNNS Fully Connected Backward: only float32 or bfloat16 output delta are supported
BNNS Fully Connected Backward: input delta only support float32 or bfloat16
BNNS Fully Connected Backward: weights delta only support float32 or bfloat16
BNNS Fully Connected Backward: bias delta only support float32 or bfloat16
BNNS Fully Connected Backward: output delta BNNSDataLayoutImageCHW isn't supported
BNNS Fully Connected Backward: failed to allocate output delta scratch buffer
BNNS Fully Connected Backward: apply activation backward failed
BNNS Fully Connected Backward: input delta BNNSDataLayoutImageCHW isn't supported
BNNS Fully Connected Backward: failed to allocate input delta scratch buffer
BNNS Fully Connected Backward: failed to compute input delta
BNNS Fully Connected Backward: user requested weights delta but didn't supply original inputs
BNNS Fully Connected Backward: weights sizes doesn't match weights delta sizes
BNNS Fully Connected Backward: unsupported weights delta layout
BNNS Fully Connected Backward: no bias, bias delta ignored
BNNS Fully Connected: Weight type not supported
BNNS Fully Connected Create: failed to allocated memory (%zu bytes))
BNNS_FULLY: input doesn't support Indexed data type
BNNS_FULLY: output only support fp32 data type
BNNS Fully Connected: using generic code to convert data 1 element each time
BNNS Fully Connected: output data type not supported for conversion
BNNS Fully Connected: input must be a 1D array
BNNS Fully Connected: input descriptor is illegal
BNNS Fully Connected: output must be a 1D array
BNNS Fully Connected: output descriptor is illegal
BNNS Fully Connected: weights row size should match input vector size
BNNS Fully Connected: weights number of rows should match output vector size
BNNS Fully Connected: weights must be a 2D array
BNNS Fully Connected: weights descriptor is illegal
BNNS Fully Connected: Pointer to weight data must be non-NULL
BNNS Fully Connected: Output data types supported in this version: Float32, Float16, BFloat16
BNNS Fully Connected: Bias data types supported in this version: Float32
BNNS Fully Connected: Float16 output isn't supported with bias or activation function
BNNS Fully Connected: Input/Weight data types supported in this version: Float32, Float16, BFloat16, Int8, Int16
BNNS Fully Connected: Data type of weights and input should match
BNNS Fully Connected: size computation wraparound -I %zu -O %zu
BNNS Fully Connected: bias size computation wraparound -O %zu bias data type %x
BNNS InTopK: invalid axis value %zu (input tensor has %zu dimensions)
BNNS InTopK: Unsupported input data type for this operation (supported data types are: fp32)
BNNS TopK: Unsupported best_indices data type for this operation (supported data types are: int32)
BNNS InTopK: input tensor with axis removed is not congruent with test_indices tensor
BNNS InTopK: input tensor with axis removed is not congruent with output tensor
BNNS TopK: input descriptor is illegal
BNNS TopK: best_values descriptor is illegal
BNNS TopK: best_indices descriptor is illegal
BNNS TopK: Unsupported input data type for this operation (supported data types are: fp32)
BNNS TopK: Unsupported best_values data type for this operation (supported data types are: fp32)
BNNS TopK: invalid axis value %zu (input tensor has %zu dimensions)
BNNS TopK: best_indices tensor is not consistent with input tensor
BNNS TopK: best_values tensor is not consistent with input tensor
BNNS TopK: best_indices tensor size[%zu]=%zu should be K=%zu
BNNS TopK: best_values tensor size[%zu]=%zu should be K=%zu
malloc
BNNS unsupported sgd variant
BNNS Optimizer Adam Apply: Adam beta1 and beta2 must be positive
BNNS Optimizer Adam Apply: Adam beta1 and beta2 must be strictly less than 1
BNNS Optimizer RMSProp Apply: RMSProp alpha must be in (0,1)
BNNS Optimizer Apply: Error unsupported optimizer function
BNNS Optimizer Apply: Error OptimizerAlgFields is NULL
BNNS Optimizer Apply: parameter array pointer is NULL
BNNS Optimizer Apply: gradient array pointer is NULL
BNNS Optimizer Apply: gradient pointer number %zu is NULL
BNNS Optimizer Apply: Error in parameter %zu: parameter descriptor must be contiguous such that stride[0]=1 and for every N>0 stride[N]=stride[N-1]*size[N-1]
BNNS Optimizer Apply: accumulator array pointer is NULL
BNNS Optimizer Apply: parameter pointer number %zu is NULL
BNNS Optimizer Apply: accumulator pointer number %zu is NULL
BNNS Optimizer Apply: Error in parameter %zu: Parameter and Gradient descriptors must have the same sizes and strides
BNNS Optimizer Apply: Error in parameter %zu: Parameter and Accumulator descriptors must have the same sizes and strides
BNNS Optimizer Apply: Error in Parameter %zu: Parameter data type is not Float32 / Float16 / Bfloat16
BNNS Optimizer Apply: Error in Gradient %zu: Gradient data type is not Float32 / Float16 / Bfloat16
BNNS Optimizer Apply: Error in Accumulator %zu: Accumulator data type is not Float32 / Float16 / BFloat16
BNNS Optimizer Apply: Error time_step is not valid for Adam optimizer. minimal time step value is 1
BNNS Optimizer Apply: Adam optimizer require accumulators pointer to be valid
BNNS Optimizer Apply: accumulator1 pointer number %zu is NULL
BNNS Optimizer Apply: accumulator2 pointer number %zu is NULL
BNNS Optimizer Apply: accumulator3 pointer number %zu is NULL
BNNS Optimizer Apply: Error in parameter %zu: Parameter and Accumulator1 descriptors must have the same sizes and strides
BNNS Optimizer Apply: Error in parameter %zu: Parameter and Accumulator2 descriptors must have the same sizes and strides
BNNS Optimizer Apply: Error in parameter %zu: Parameter and Accumulator3 descriptors must have the same sizes and strides
BNNS Optimizer Apply: Error in parameter %zu: Parameter data type is not BNNSDataTypeFloat32
BNNS Optimizer Apply: Error in gradient %zu: Gradient data type is not BNNSDataTypeFloat32
BNNS Optimizer Apply: Error in accumulator1 %zu: Accumulator data type is not BNNSDataTypeFloat32
BNNS Optimizer Apply: Error in accumulator2 %zu: Accumulator data type is not BNNSDataTypeFloat32
BNNS Optimizer Apply: Error in accumulator3 %zu: Accumulator data type is not BNNSDataTypeFloat32
BNNS Optimizer Apply: RMSProp optimizer require accumulators pointer to be valid
BNNS Optimizer Apply: accumulator_n pointer number %zu is NULL
BNNS Optimizer Apply: accumulator_g pointer number %zu is NULL
BNNS Optimizer Apply: momentum pointer number %zu is NULL
BNNS Optimizer Apply: Error in accumulator_n %zu: Accumulator data type is not BNNSDataTypeFloat32
BNNS Optimizer Apply: Error in accumulator_g %zu: Accumulator data type is not BNNSDataTypeFloat32
BNNS Optimizer Apply: Error in momentum %zu: Accumulator data type is not BNNSDataTypeFloat32
BNNS Optimizer: unsupported optimizer function
BNNS ClipByValue: Error dimensions of input tensor (%zu) and output tensor (%zu) must match
BNNS ClipByValue: Error only supports fp32
BNNS ClipByValue: Error sizes and strides of input tensor and output tensor must match
BNNS ClipByNorm: Error dimensions of input tensor (%zu) and output tensor (%zu) must match
BNNS ClipByNorm: Error only supports fp32
BNNS ClipByNorm: Error sizes and strides of input tensor and output tensor must match
BNNS ClipByGlobalNorm: src[%zu] and dest[%zu] have a different number of dimensions.
BNNS ClipByGlobalNorm: src[%zu] and dest[%zu] must contain fp32 data.
BNNS ClipByGlobalNorm: src[%zu] and dest[%zu] have different sizes or strides.
BNNS ComputeNorm: Error only supports BNNSL2Norm
BNNS ComputeNorm: Error only supports fp32
BNNS ComputeNorm: Error non-axis dimensions of input tensor (%zu) and output tensor (%zu) must match
BNNS ComputeNorm: Error non-axis sizes of input tensor and output tensor must match
BNNS ComputeNorm Backward: Error only supports BNNSL2Norm
BNNS ComputeNorm Backward: Error only supports fp32
BNNS ComputeNorm Backward: Error non-axis dimensions of input tensor (%zu) and output tensor (%zu) must match
BNNS ComputeNorm Backward: Error non-axis sizes of input tensor and output tensor must match
optimizer_private_bnns_init_NDArray: Unsupported layout
BNNS Sparse Fully Connected: input must be a 1D array
BNNS Sparse Fully Connected: input descriptor is illegal
BNNS Sparse Fully Connected: output must be a 1D array
BNNS Sparse Fully Connected: output descriptor is illegal
BNNS Sparse Fully Connected: invalid matrix layout
BNNS Sparse Fully Connected: invalid block row count: %u
BNNS Sparse Fully Connected: invalid block col count: %u
BNNS Sparse Fully Connected: input data types supported in this version: Float16
BNNS Sparse Fully Connected: output data types supported in this version: Float32, Float16
BNNS Sparse Fully Connected: weights data types supported in this version: Float16
BNNS Sparse Fully Connected: unsupported matrix block size %u x %u
BNNS Sparse Fully Connected: Allocation of the setup structure failed
BNNS Convolution: malloc failed
BNNS Compare: in0 is NULL
BNNS Compare: in1 is NULL
BNNS Compare: out is NULL
BNNS Compare: Unsupported I/O tensor data types.
BNNS Compare: Mismatch in input tensor data types.
BNNS Compare: Invalid operation %d for data type %s.
BNNS Compare: Broadcast failed.
BNNS Compare: I/O tensor dimension %lu mismatch.
BNNS Compare: Partially overlapping input and output tensors are not supported.
BNNS Compare: Unsupported I/O tensor layout.
BNNS Compare: I/O tensor layout mismatch.
BNNS Compare: input size[%zu]=%zu does not equal output size of %zu or 1
BNNS Compare: Unsupported operator.
BNNS Fully Connected Choose: inputs, weights, outputs and bias must be contigeous in memory (stride[0] <= 1)
BNNS Fully Connected Init: missing data table for indexed data type
BNNS Fully Connected Apply: Ilegal compute block
BNNS Fully Connected Create: failed to create layer context
BNNS Fully Connected Create: allocation failed
BNNS Fully Connected Direct Apply: input pointer is NULL
BNNS Fully Connected Direct Apply: output pointer is NULL
BNNS Fully Connected Direct Apply: failed to create layer context
BNNS Fully Connected Init: failed to create context
BNNS Fully Connected Init: input descriptor CHW layout isn't set properly
BNNS Fully Connected Init: input descriptor isn't set properly
BNNS Fully Connected Init: input descriptor failed validation
BNNS Fully Connected Init: output descriptor CHW layout isn't set properly
BNNS Fully Connected Init: output descriptor isn't set properly
BNNS Fully Connected Init: output descriptor failed validation
BNNS Fully Connected Init: weights descriptor row major size[0] doesn't match input vector size
BNNS Fully Connected Init: weights descriptor row major size[1] doesn't match output vector size
BNNS Fully Connected Init: weights descriptor column major size[0] doesn't match output vector size
BNNS Fully Connected Init: weights descriptor column major size[1] doesn't match input vector size
BNNS Fully Connected Init: weights descriptor isn't set properly
BNNS Fully Connected Init: weights descriptor failed validation
BNNS Fully Connected Init BNNSNDArrayDescriptor: stride[%zu] is too small
BNNS Fully Connected Apply: failed to apply filter
BNNS Random Generator Create: Unsupported method %u.
BNNS Random Generator Create: Failed to allocate memory
BNNS Random Fill Uniform Float: Invalid descriptor
BNNS Random Fill Uniform Float: Range (%g, %g) is empty
BNNS Random Fill Uniform Float: Range bounds are not exactly representable as bf16
BNNS Random Fill Uniform Float: Range bounds are not exactly representable as fp16
BNNS Random Fill Uniform Float: Unsupported data type
BNNS Random Fill Uniform Integer: Invalid descriptor
BNNS Random Fill Uniform Integer:%s range (%d, %d) is empty
 (clipped)
BNNS Random Fill Uniform Integer: range (%lld, %lld) is empty
BNNS Random Fill Uniform Integer:%s range (%llu, %lld) is empty
BNNS Random Fill Uniform Integer: Unsupported data type
BNNS Random: CCCryptorCreateWithMode() failed
BNNS Random Fill Uniform: Invalid generator
Failed to create random data
BNNS Loss: Error unsupported loss function
BNNS Loss: Error Input width is 0
BNNS Loss: Error Input must be BNNSDataTypeFloat32 type
BNNS Loss Error: Input must be contiguous. stride0 must be 1 or 0
BNNS Loss Error: Input tensors must be contiguous.
BNNS Loss: Error unknown reduction function
BNNS Loss: Error output data type must be BNNSDataTypeFloat32
BNNS Loss: Error output size>1 is only allowed with reduction BNNSLossReductionNone.
BNNS Yolo loss: Error out width (size[0]) must be 1 as yolo loss is always reduced
BNNS Loss: Error anchors is NULL
BNNS Loss: Error number of grid rows is 0
BNNS Loss: Error number of grid columns is 0
BNNS Loss: Error number of anchor boxes is 0
BNNS Loss: Error anchor box size <= 5
BNNS Loss: Error object minimum iou is negative
BNNS Loss: Error no object maximum iou is negative
BNNS Loss: Error input descriptor width (%zu) different from expected width of grid_size*num_anchors*(5+num_+classes) (%zu)
BNNS Loss: memory allocation failed
BNNS Loss: Error filter is NULL
BNNS Loss: Error batch_size is 0
BNNS Loss: Error in is NULL
BNNS Loss Error: in_stride is smaller than i_desc.size[0] or Tensor size
BNNS Loss: Error labels is NULL
BNNS Loss Error: labels_stride is smaller than i_desc.size[0] or Tensor size
BNNS Loss: Error weights_size>0 but weights pointer is NULL
BNNS Loss Warning: weight_size==0 but weight pointer is not NULL. Weight pointer is ignored.
BNNS Loss Error: weight_size value must be 0,1 batch_size, or batch_size*input_width for softmax/sigmoid/categorical cross entropy, mse, mae, log, hinge, and huber
BNNS Loss: Error yolo weight_size value must be 0, use yolo specific weight factors during filter create
BNNS Loss: Error out is NULL
BNNS Loss: Error in_delta descriptor is ilegal
BNNS Loss: Error in_delta must be contiguous, such that stride[0] is 0 or 1
BNNS Yolo loss: Error weight size must be 0
BNNS Loss: unsupported loss function
BNNS Loss Backward: Error filter is NULL
BNNS Loss Backward: Error batch_size is 0
BNNS Loss Backward: Error in is NULL
BNNS Loss Backward: Error in_stride is smaller than i_desc.size[0]
BNNS Loss Backward: Error labels is NULL
BNNS Loss Backward: Error labels_stride is smaller than i_desc.size[0]
BNNS Loss Backward: Error weights_size>0 but weights pointer is NULL
BNNS Loss Backward Error: weight_size value must be 0,1 batch_size, or batch_size*input_width for softmax/sigmoid/categorical cross entropy, mse, mae, log, hinge, and huber
BNNS Loss Backward: Error in_delta descriptor is illegal
BNNS Loss Backward: Error in_delta must be contiguous, such that stride[0] is 0 or 1
BNNS Loss Backward: Error in_delta is NULL
BNNS Loss Backward: Error out_delta descriptor is illegal
BNNS Loss Backward: Error out_delta must be contiguous, such that stride[0] is 0 or 1
BNNS Loss Backward: Error out_delta is NULL
BNNS Loss: unsupported loss backward function
BNNS softmax cross entropy loss error: in pointer is NULL
BNNS softmax cross entropy loss error: in stride cannot be smaller than input width
BNNS softmax cross entropy loss: Error out pointer is NULL
BNNS softmax cross entropy loss: malloc failed
BNNS softmax cross entropy loss: Error weight size must be equal 0,1, batch size, or batch_size*tensor_size 
BNNS softmax cross entropy loss backward : alloc failed
BNNS Loss Warning: reduction BNNSLossReductionWeightedMean sum of weights is zero
BNNS Loss: Error reduction BNNSLossReductionNonZeroWeightMean all weights are zero
BNNS sigmoid cross entropy loss error: in pointer is NULL
BNNS sigmoid cross entropy loss error: in stride cannot be smaller than input width
BNNS sigmoid cross entropy loss: Error out pointer is NULL
BNNS sigmoid cross entropy loss error: must be 0,1 or batch_size*input_width
BNNS sigmoid cross entropy loss: malloc failed
BNNS MSE loss error: in stride cannot be smaller than input width
BNNS MSE loss: malloc failed
BNNS MSE loss error: must be 0,1 or batch_size*input_width
BNNS Huber loss error: in pointer is NULL
BNNS Huber loss error: in stride cannot be smaller than input width
BNNS Huber loss: Error out pointer is NULL
BNNS Huber loss error: must be 0,1 or batch_size*input_width
BNNS Huber loss: malloc failed
BNNS Yolo loss: Error filter is NULL
BNNS Yolo loss: Error in pointer is NULL
BNNS Yolo loss: Error input stride is smaller than input width
BNNS Yolo loss: Error labels (ground truth) is NULL
BNNS Yolo loss: Error ground truth labels stride is smaller than input width
BNNS Yolo loss: Error input delta stride is smaller than input width
BNNS Yolo loss: Error out pointer is NULL
BNNS Yolo loss: Error yolo redution type must be BNNSLossReductionSum
BNNS Yolo loss: Error input descriptor width does not match grid_rows*grid_columns*anchors*(5+classes)
BNNS Yolo loss: alloc failed
BNNS log loss error: in pointer is NULL
BNNS log loss error: in stride cannot be smaller than input width
BNNS log loss: Error out pointer is NULL
BNNS log loss error: must be 0,1 or batch_size*input_width
BNNS cosine distance loss error: in pointer is NULL
BNNS cosine distance loss error: in stride cannot be smaller than input width
BNNS cosine distance loss: Error out pointer is NULL
BNNS cosine distance loss: Error weight size must be equal to 0, 1, batch_size
BNNS Loss: Error in_delta descriptor is illegal
BNNS Hinge loss error: in pointer is NULL
BNNS Hinge loss error: in stride cannot be smaller than input width
BNNS Hinge loss: Error out pointer is NULL
BNNS Hinge loss error: must be 0,1 or batch_size*input_width
BNNS MAE loss error: in pointer is NULL
BNNS MAE loss error: in stride cannot be smaller than input width
BNNS MAE loss: Error out pointer is NULL
BNNS MAE loss error: must be 0,1 or batch_size*input_width
BNNS categorical cross entropy loss error: in pointer is NULL
BNNS categorical cross entropy loss error: in stride cannot be smaller than input width
BNNS categorical cross entropy loss: Error out pointer is NULL
BNNS categorical cross entropy loss error: must be 0,1 or batch_size*input_width
BNNS softmax cross entropy loss backward error: in stride cannot be smaller than input width
BNNS softmax cross entropy loss backward error: out_delta.size[0] is not 1
BNNS softmax cross entropy loss backward : malloc failed
BNNS softmax cross entropy loss: Error weight size must be equal 0,1 or batch size
BNNS sigmoid cross entropy loss backward error: in stride cannot be smaller than input width
BNNS sigmoid cross entropy loss backward error: must be 0,1 or batch_size*input_width
BNNS MSE loss backward error: must be 0,1 or batch_size*input_width
BNNS Log loss backward error: in stride cannot be smaller than input width
BNNS Log loss backward error: must be 0,1 or batch_size*input_width
BNNS cosine distance loss backward error: in stride cannot be smaller than input width
BNNS cosine distance loss backward error: must be 0,1 or batch_size
BNNS Hinge loss backward error: in stride cannot be smaller than input width
BNNS Hinge loss backward error: must be 0,1 or batch_size*input_width
BNNS MAE loss backward error: in stride cannot be smaller than input width
BNNS MAE loss backward error: must be 0,1 or batch_size*input_width
BNNS Categorical CE loss backward error: in stride cannot be smaller than input width
BNNS Categorical CE loss backward error: must be 0,1 or batch_size*input_width
BNNS Create Depthwise Convolution: incompatible numbers of channels between images and convolution parameters
BNNS Create Depthwise Convolution: unsupported weight format
BNNS Create Depthwise Convolution: input must be BNNSDataLayoutImageCHW layout 
BNNS Create Depthwise Convolution: input descriptor is illegal
BNNS Create Depthwise Convolution: output must be BNNSDataLayoutImageCHW layout 
BNNS Create Depthwise Convolution: output descriptor is illegal
BNNS Depthwise Convolution Create: weights descriptor is illegal
BNNS Depthwise Convolution Create: does not support asymmetric padding. left and right pad must match, up and down pad must match
BNNS Create Depthwise Convolution: only float32 is supported
BNNS Create Depthwise Convolution: channel multiplier mismatch
BNNS Create Depthwise Convolution: malloc failed
BNNS Create Depthwise Convolution: weights malloc failed 
invalid argument
BNNS DEPTHWISE CONV: input batch stride doesn't make sense (%zu < %zu x %zu)
BNNS DEPTHWISE CONV: output batch stride doesn't make sense (%zu < %zu x %zu)
BNNS LSTM Direct Apply: filter initialization failed
BNNS LSTM Direct Apply: training cache capacity isn't sufficent
BNNS LSTM Direct Apply: Dropout is only supported in the presence of a training cache
BNNS LSTM Direct Apply: failed to allocate scratch buffer
BNNS LSTM Direct Apply: failed to thread workspace buffer
BNNS LSTM Direct Apply Backward: BNNSNDArrayFlagBackpropAccumulate is only supported on layer_delta->input_descriptor.data_desc.
BNNS LSTM Direct Apply Backward: layer_params initialization failed
BNNS LSTM Direct Apply Backward: layer_delta initialization failed
BNNS LSTM APPLY BACKWARD: Use of dropout without training cache is not supported.
BNNS LSTM APPLY BACKWARD: forward pass intermediate results weren't cached, recomputing forward pass
BNNS LSTM Direct Apply Backward: failed to allocate training cache buffer
BNNS LSTM Direct Apply Backward: failed to compute forward intermediate results
BNNS LSTM Direct Apply Backward: training cache capacity isn't sufficent
BNNS LSTM Direct Apply Backward: failed to allocate scratch buffer
BNNS LSTM init: hidden_size must be greater than zero
BNNS LSTM init: input descriptor.data isn't set correctly (size don't match seq_len/batch_size/input_size)
BNNS LSTM init: input descriptor.data isn't set correctly (size don't match batch_size/seq_len/input_size)
BNNS LSTM init: input descriptor.data isn't set correctly (size don't match input_size/batch_size/seq_len)
BNNS LSTM init: input descriptor.hidden isn't set correctly (size don't match hidden_size/batch_size/num_directions/num_layers)
BNNS LSTM init: input descriptor.cell_state isn't set correctly (size don't match hidden_size/batch_size/num_directions/num_layers)
BNNS LSTM init: output descriptor.data isn't set correctly (size don't match seq_len/batch_size/hidden_size*num_directions)
BNNS LSTM init: output descriptor.data isn't set correctly (size don't match batch_size/seq_len/hidden_size*num_directions)
BNNS LSTM init: output descriptor.data isn't set correctly (size don't match hidden_size/num_directions/batch_size/seq_len)
BNNS LSTM init: output descriptor.hidden isn't set correctly (size don't match hidden_size/batch_size/num_directions/num_layers)
BNNS LSTM init: output descriptor.cell_state isn't set correctly (size don't match hidden_size/batch_size/num_directions/num_layers)
BNNS LSTM init: forget_gate isn't set correctly (size don't match hidden_size/batch_size/num_directions/num_layers)
BNNS LSTM init: input_gate isn't set correctly (size don't match hidden_size/batch_size/num_directions/num_layers)
BNNS LSTM init: candidate_gate isn't set correctly (size don't match hidden_size/batch_size/num_directions/num_layers)
BNNS LSTM init: output_gate isn't set correctly (size don't match hidden_size/batch_size/num_directions/num_layers)
BNNS LSTM: Data type not supported, fail
BNNS Convolution Create: Client needs to support weights ptr with low_mem
BNNS Convolution Create: malloc
BNNS Convolution Create: failed to create convolution context
BNNS Convolution Create: failed to divide work between convolution contexts to fit in memory
BNNS Convolution Create: failed to create single generic convolution
BNNS Convolution Apply: invalid argument
BNNS Convolution Apply: failed to allocate input auxiliary buffer
BNNS Convolution: Winograd weights memory doesn't fit in memory
BNNS Convolution: unable to create Winograd that fit in memory
BNNS Convolution: allocation of contexts failed
BNNS LOW MEM CONVOLUTION: failed to create winograd convolution
BNNS LOW MEM CONVOLUTION: malloc failed
BNNS LOW MEM CONVOLUTION: incompatible BNNS_low_memory_context id
BNNS LOW MEM CONVOLUTION: convolution failed
BNNS LOW MEM CONVOLUTION: input aux buffer wasn't allocated
BNNS LOW MEM CONVOLUTION:: context type not supported
BNNS Convolutions Apply: unexpected input data type
BNNS Convolution Create: apply convolution failed
BNNS Quantization Filter: layer_params is NULL
BNNS Quantization: input layout and output layout must match
BNNS Quantization: unsupported input/output layouts
BNNS Quantization: input descriptor error
BNNS Quantization: input descriptor data is NULL
BNNS Quantization: output descriptor error
BNNS Quantization: output descriptor data is NULL
BNNS Quantization: invalid quantizer function
BNNS Quantization: BNNSQuantize function supported for the following input descriptor data types: BNNSDataTypeFloat32, BNNSDataTypeFloat16, BNNSDataTypeBFloat16, BNNSDataTypeInt32
BNNS Quantization: BNNSQuantize function supported for the following output descriptor data types: BNNSDataTypeInt8, BNNSDataTypeUInt8, BNNSDataTypeInt16, BNNSDataTypeUInt16, BNNSDataTypeInt32, BNNSDataTypeUInt32
BNNS Quantization: BNNSDeQuantize function supported for the following input descriptor data types: BNNSDataTypeInt8, BNNSDataTypeUInt8, BNNSDataTypeInt16, BNNSDataTypeUInt16, BNNSDataTypeInt32, BNNSDataTypeUInt32
BNNS Quantization: BNNSDeQuantize function supported for the following output descriptor data types: BNNSDataTypeFloat32, BNNSDataTypeFloat16, BNNSDataTypeBFloat16, BNNSDataTypeInt32
BNNS Quantization: invalid axis_mask, number of mask set bits must be lower or equal to 1
BNNS Quantization: axis_mask bits are set beyond batch dimension
BNNS Quantization: Error dimensions of input tensor (%zu) and output tensor (%zu) must match
BNNS Quantization: Error shape of input tensor and output tensor must match
BNNS Quantization Filter: scale layout must be BNNSDataLayoutVector
BNNS Quantization: scale descriptor error
BNNS Quantization Filter: bias layout must be BNNSDataLayoutVector
BNNS Quantization: bias descriptor error
BNNS Quantization: bias vector size does not match axis mask, bias size[0]=%zu, expected %zu
BNNS Quantization: scale vector size does not match axis mask, scale size[0]=%zu, expected %zu
BNNS Fused Convolution and Normalization: Error convolution_layer_params is NULL
BNNS Fused Convolution and Quantization: Error quantization_layer_params is NULL
BNNS Fused Compute and Quantization: Error Convolution output layout and Quantization input layout must match
BNNS Fused Compute and Quantization: Error Convolution output data type and Quantization input data type must match
BNNS Fused Compute and Quantization: Error Convolution output descriptor sizes and Quantization input descriptor sizes must match
BNNS Fused Compute and Quantization: Error Convolution output descriptor strides and Quantization input descriptor strides must match
BNNS Fused Compute and Quantization: Error memory allocation failed
BNNS Fused Compute and Quantization create filter failed: compute filter type error
BNNS Fused Compute and Quantization create filter failed
BNNS Quantization filter failed
BNNS Fused Fully Connected and Quantization: Error fully_layer_params is NULL
BNNS Fused Fully Connected and Quantization: Error quantization_layer_params is NULL
BNNS Fused Compute and Quantization: Error Fully Connected output layout and Quantization input layout must match
BNNS Fused Compute and Quantization: Error Fully Connected output data type and Quantization input data type must match
BNNS Fused Compute and Quantization: Error Fully Connected output descriptor sizes and Quantization input descriptor sizes must match
BNNS Fused Compute and Quantization: Error Fully Connected output descriptor strides and Quantization input descriptor strides must match
BNNS Quantization: unsupported data type
BNNS Quantization: unsupported quantization input data type
BNNS Quantization: unsupported quantization output data type
BNNS Quantization: unsupported dequantization input data type
BNNS Quantization: unsupported dequantization output data type
BNNS Transposed Convolution Create: input data type isn't supported
BNNS Transposed Convolution Create: weight data isn't supported
BNNS Transposed Convolution Create: output data type isn't supported
BNNS Transposed Convolution Apply: Filter is NULL
BNNS Transposed Convolution Apply: failed to allocate memory to manipulate input
BNNS Transposed Convolution Apply: unknown weight layout
BNNS Transposed Convolution Create: layer_params is NULL
BNNS Transposed Convolution Create: groups aren't supported
BNNS Transposed Convolution Create: low-mem isn't supported
BNNS Transposed Convolution Create: packed weights aren't supported
BNNS Transposed Convolution Create: failed to create forward path
BNNS Transposed Convolution Create: failed to allocate wrapper memory
BNNS Transposed Convolution Create Forward: transposed convolution failed
BNNS Transposed Convolution Create: weights allocation failed
BNNS Transpsoed Vector Convolution Apply: activation gradient auxilary allocation failed
BNNS Transposed Convolution Create: malloc failed
BNNS Transposed Convolution: weights allocation failed
BNNS Transposed Convolution: bias allocation failed
BNNS Transposed Convolution Reorder Weights: NULL weight object
BNNS Transposed Convolution Reorder Weights: dst weight size too small
BNNS Transposed Convolution Reorder Weights: NULL weight array
BNNS Transposed Convolution Rotate Weights: NULL weight object
BNNS Transposed Convolution Rotate Weights: dst weight size too small
BNNS Transposed Convolution Rotate Weights: NULL weight array
BNNS Transposed Convolution Reorder and Rotate Weights: NULL weight object
BNNS Transposed Convolution Reorder and Rotate Weights: dst weight size too small
BNNS Transposed Convolution Reorder and Rotate Weights: NULL weight array
transposed convolution input manipulation failed
transposed convolution input manipulation failed - nothing to do, should not have allocated dst_input buffer
BNNS: unsupported activation gradient data type
BNNS Transposed Convlution Backward: dy padding failed
BNNS Transposed Convlution Backward: no padding is needed, dy padding failed
BNNS Create Convolution: fp32 weights allocation failed
BNNS Create Convolution: int16 weights allocation failed
BNNS Create Convolution: failed to allocate memory to copy the weights
BNNS Create Convolution: single descriptors allocation failed
BNNS Convolution Create: allocation failed
failed to upconvert or copy weights
failed to allocate none generic format
BNNS Convolution Create: forward pass check (swapping input and output)  - input size computation wraparound input (%zu x %zu), output (%zu x %zu), stride (%zu x %zu), asymmetric padding (%zu,%zu,%zu,%zu)
BNNS Convolution Create: forward pass check (swapping input and output)  - input size computation wraparound input (%zu x %zu), output (%zu x %zu), stride (%zu x %zu), padding (%zu x %zu)
BNNS Convolution Create: forward pass check (swapping input and output) - output(%zu x %zu x %zu) is larger than input(%zu x %zu x %zu) with padding(%zu x %zu).
 kernel (%zu x %zu) and stride (%zu x %zu)
BNNS Normalization: layer_params is NULL
BNNS Normalization: filter memory allocation failed
BNNS Fused Convolution and Normalization: Error normalization_layer_params is NULL
BNNS Fused Convolution and Normalization: Error convolution output descriptor and normalization input descriptor must have the same sizes and strides
BNNS Fused Convolution and Normalization: Error memory allocation failed
BNNS Fused Convolution and Normalization create filter failed: Convolution type error
BNNS Fused Convolution and Normalization create filter failed
BNNS Fused Fully Connected and Normalization: Error fully_layer_params is NULL
BNNS Fused Fully Connected and Normalization: Error batch_normalization_layer_params is NULL
BNNS Fused Fully Connected and Normalization: Error fully connected output descriptor and normalization input descriptor must have the same sizes and strides
BNNS Fused Fully Connected and Normalization: Error memory allocation failed
BNNS Fused Fully Connected and Normalization create filter failed
BNNS Fused Arithmetic and Normalization: Error arithmetic_layer_params is NULL
BNNS Fused Arithmetic and Normalization: Error normalization_layer_params is NULL
BNNS Fused Arithmetic and Normalization: unsupported arithmetic function
BNNS Fused Arithmetic and Normalization: Error arithmetic output descriptor and normalization input descriptor must have the same sizes and strides
BNNS Fused Arithmetic and Normalization: Error memory allocation failed
BNNS Fused Arithmetic and Normalization create filter failed
BNNS Normalization: input pointer is NULL
BNNS Normalization: batch_size>1 and in_stride is 0
BNNS Normalization: output pointer is NULL
BNNS Normalization: batch_size>1 and out_stride is 0
BNNS Normalization: batch_size too large. Backprop cache size limits batch_size <= %zu.
BNNS Normalization: x_hat allocation failed
BNNS Normalization: filter id not supported
BNNS Normalization: inverse variance allocation failed
BNNS Normalization Apply Backward: filter is NULL
BNNS Normalization Apply Backward: Normalization beta delta pointer is NULL. backward computation of all active parameters must be done in a single function call.
BNNS Normalization Apply Backward: Normalization gamma delta pointer is NULL. backward computation of all active parameters must be done in a single function call.
BNNS Normalization Apply Backward: make sure to run normalization forward with training flag enabled before running normalization backward
BNNS Normalization Apply Backward: cannot compute activation backward, output is NULL
BNNS Normalization Apply Backward: Fusion of normalization with activation is unsupported for given activation function
BNNS Normalization Apply Backward: cannot compute input delta because input delta data pointer is NULL
BNNS Normalization Apply Backward: Error normalization input delta and input must have the same sizes and strides
BNNS Normalization Apply Backward: in delta shuold be contiguous and same size as input size
BNNS Normalization Apply Backward: cannot compute beta delta because beta delta data pointer is NULL
BNNS Normalization Apply Backward: BNNSNDArrayFlagBackpropAccumulate is no supported for beta_delta.
BNNS Normalization Apply Backward: Error beta_delta descriptor must match the shape of input up to the normalizaton axis
BNNS Normalization Apply Backward: beta delta shuold be contiguous and same size as number of input channels
BNNS Normalization Apply Backward: cannot compute gamma delta because gamma delta data pointer is NULL
BNNS Normalization Apply Backward: BNNSNDArrayFlagBackpropAccumulate is no supported for gamma_delta.
BNNS Layer Normalization: Error gamma_delta descriptor must match the shape of input up to the normalizaton axis
BNNS Normalization Apply Backward: gamma delta shuold be contiguous and same size as number of input channels
BNNS Normalization Apply Backward: out delta data is NULL
BNNS Normalization Apply Backward: Error Normalization output delta and output must have the same sizes and strides
BNNS Normalization Apply Backward: out delta shuold be contiguous and same size as input size
BNNS Normalization Apply Backward: normalization type not supported
BNNS Normalization Set State: Backprop cache must be in BNNSDataLayoutVector
BNNS Normalization Set State: Backprop cache must be in BNNSDataTypeFloat32
BNNS Normalization Set State: Backprop cache must be in contiguous
BNNS Normalization Set State: Backprop cache pointer is NULL
BNNS Normalization Set State: Backprop cache too small, must be >= %zu fp32 to allow max_batch_size >= 1
BNNS Normalization: Moving Mean isn't set properly
BNNS Normalization: Moving Variance isn't set properly
BNNS Normalization Create: normalization type not supported
BNNS Normalization: input descriptor isn't set properly
BNNS Normalization: Input must be in BNNSDataLayoutImageCHW layout
BNNS Normalization: output descriptor isn't set properly
BNNS Normalization: Output must be in BNNSDataLayoutImageCHW layout
BNNS Normalization: Gamma descriptor isn't set properly
BNNS Normalization: Beta descriptor isn't set properly
BNNS Normalization: Error input descriptor must be contiguous such that stride[0]=1 and for every N>0 stride[N]=stride[N-1]*size[N-1]
BNNS Normalization: Error output descriptor must be contiguous such that stride[0]=1 and for every N>0 stride[N]=stride[N-1]*size[N-1]
BNNS Normalization: Error input size (%zu) different from output size (%zu)
BNNS Layer Normalization: Gamma descriptor must match the shape of input up to the normalizaton axis
BNNS Normalization: Beta descriptor must match the shape of input up to the normalizaton axis
BNNS Normalization: Gamma descriptor size[0] must be the same as number of input channels
BNNS Normalization: Beta descriptor size[0] must be the same as number of input channels
BNNS Normalization: Moving Mean descriptor size[0] must be the same as number of input channels
BNNS Normalization: Moving Variance descriptor size[0] must be the same as number of input channels
BNNS Normalization: Error momentum must be between 0 and 1
BNNS Normalization Warning: epsilon is zero, it may cause division by zero
BNNS Normalization Warning: momentum is zero
BNNS CONVOLUTIONS VERSION2: layer param is NULL
BNNS CONVOLUTIONS VERSION2: unsupported input data type
BNNS CONVOLUTIONS VERSION2: unsupported output data type
BNNS CONVOLUTIONS VERSION2: unsupported weight data type
BNNS CONVOLUTIONS VERSION2: unsupported bias data type
BNNS CONVOLUTIONS VERSION2: int/out/weight/bias descriptor element stride (stride[0]) must be 1
BNNS CONVOLUTIONS VERSION2: 2D conv doesn't fit in memory limit
BNNS CONVOLUTIONS VERSION2: 0 width/height/channel
BNNS CONVOLUTIONS VERSION2: unsupported activation
BNNS CONVOLUTIONS VERSION2: output bias or scale is not supported
BNNS CONVOLUTIONS VERSION2: malloc failed
BNNS CONVOLUTIONS VERSION2: malloc failed
BNNS CONVOLUTIONS VERSION2: failed to allocate bias conversion memory
BNNS CONVOLUTIONS VERSION2: failed to allocate scratch memory
BNNS CONVOLUTIONS VERSION2: invalid argument
BNNS CONVOLUTIONS VERSION2: input batch stride doesn't make sense (%zu < %zu x %zu)
BNNS CONVOLUTIONS VERSION2: output batch stride doesn't make sense (%zu < %zu x %zu)
BNNS CONVOLUTIONS VERSION2: unsupported weight packing
BNNS CONVOLUTIONS VERSION2: failed to allocated memory to convert bias
BNNS CONVOLUTIONS VERSION2: input padding memory allocation failed
BNNS CONVOLUTIONS VERSION2: output repack malloc failed
BNNS CONVOLUTIONS VERSION2: weight repack malloc failed
BNNS Padding Create: layer_params is NULL
BNNS Padding Create: Padding is not supported beyond 4-D tensors.
BNNS Padding Create: Undefined padding mode.
BNNS Padding Create: Unsupported data layout.
BNNS Padding Create: input and output desciptors have differing numbers of dimensions.
BNNS Padding Create: Input dimension is too small for the padding size.
BNNS Padding Create: Input size + padding sizes doesn't match output size in dimension %zu.
BNNS Padding Create: input descriptor is illegal
BNNS Padding Create: output descriptor is illegal
BNNS Padding Create: Unsupported data type.
BNNS Padding Create: I/O data type mismatch.
BNNS Padding Create: memory allocation failed
BNNS Padding Apply: filter is NULL
BNNS Padding Apply: wrong filter type, filter is not Padding.
BNNS Padding Apply: input pointer is NULL
BNNS Padding Apply: batch_size > 1 and in_stride is 0
BNNS Padding Apply: output pointer is NULL
BNNS Padding Apply: batch_size > 1 and out_stride is 0
BNNS Padding Apply Backward: filter is NULL
BNNS Padding Apply Backward: wrong filter type, filter is not padding
BNNS Padding Apply Backward: input descriptor is NULL
BNNS Padding Apply Backward: input data pointer is NULL
BNNS Padding Apply Backward: batch_size>1 and in_stride is 0
BNNS Padding Apply Backward: output delta descriptor is NULL
BNNS Padding Apply Backward: output delta data pointer is NULL
BNNS Padding Apply Backward: batch_size>1 and out_delta_stride is 0
BNNSFilterCreateLayerSparseEmbedding: layer_params must not be NULL
BNNSSparseGetRepresentation: filter must not be NULL
BNNSSparseGetRepresentation: Invalid filter
BNNSSparseSetRepresentation: filter must not be NULL
BNNSSparseSetRepresentation: Invalid filter
BNNSSparseEmbeddingGetDense: filter must not be NULL
BNNSSparseEmbeddingGetDense: Invalid filter
BNNS Fused Filter Multi-Input: Error filter is NULL
BNNS Fused Filter Multi-Input Warning: Batch size is 0, nothing to do
BNNS Fused Filter Multi-Input: Error - only fused arithmetic and normalization is supported.
BNNS Fused Filter Multi-Input: Error arithmetic filter apply failed
BNNS Fused Filter Backward Multi-Input: Error filter is NULL
BNNS Fused Filter Backward Multi-Input Warning: Batch size is 0, nothing to do
BNNS Fused Filter Backward Multi-Input: output delta is NULL
BNNS Fused Filter Backward Multi-Input: Error - only fused arithmetic and normalization is supported.
BNNS Fused Filter Backward Multi-Input: Error normalization beta delta pointer is NULL. backward computation of all active parameters must be done in a single function call. 
BNNS Fused Filter Backward Multi-Input: Error normalization gamma delta pointer is NULL. backward computation of all active parameters must be done in a single function call. 
BNNS Arithmetic Backward Multi-Input: failed to allocate memory
BNNS Fused Filter Backward Multi-Input: Error normalization backward failed
x_ijf*, x_ijc* -> G_fc*
BNNSFilterApplyTwoInputBatch: invalid argument
Tensor Contraction filter only expects one input
invalid filter or incorrect number of inputs
BNNSFilterApplyBackwardTwoInputBatch: invalid argument
Two-input Tensor Contraction filter has no weights
Tensor Contraction filter has no bias
Unsupported filter
BNNS GetStateSize : filter is NULL
BNNS SetState: Error filter is NULL
BNNS SetState: Source filter state is NULL
BNNS SetState: Target filter is NOT Dropout/Normalization filter
BNNS GetState: Error filter is NULL
BNNS GetState: Target filter state is NULL
BNNS GetState: Source filter is NOT Dropout filter
BNNS LayerNorm: Error Normalization axis must be 0, 1, or 2.
BNNS GroupNorm: Error The number of input channels must be divisible by the number of groups.
BNNS Normalization: Error Normalization axis must be 0, 1, or 2.
BNNS Normalization: Error The number of input channels must be divisible by the number of groups.
BNNS Fused Filter: Error number_of_fused_filters is not 2. currently supporting only 2 fused filters
BNNS Fused Filter: Error filter_type is NULL
BNNS Fused Filter: Error layer_params is NULL
BNNS Fused Filter: Error currently supporting first filter of BNNSConvolution, BNNSFullyConnected, BNNSTransposedConvolution, or BNNSArithmetic type only
BNNS Fused Filter: Error currently supporting second filter of BNNSBatchNorm, BNNSInstanceNorm, BNNSLayerNorm, BNNSGroupNorm and BNNSQuantization types only
BNNS Fused Filter: Error layer_params[0] is NULL
BNNS Fused Filter: Error layer_params[1] is NULL
BNNS Fused Filter: Error unknown fused filter type
BNNS Normalization: Error filter is NULL
BNNS Normalization: Error wrong filter type, filter is not normalization
BNNS BatchNorm: Error wrong filter type, filter is not batch norm
BNNS InstanceNorm: Error filter is NULL
BNNS InstanceNorm: Error wrong filter type, filter is not instance norm
BNNS LayerNorm: Error filter is NULL
BNNS LayerNorm: Error wrong filter type, filter is not layer norm
BNNS GroupNorm: Error filter is NULL
BNNS GroupNorm: Error wrong filter type, filter is not group norm
BNNS Fused Filter: Error filter is NULL
BNNS Fused Filter Warning: Batch size is 0, nothing to do
BNNS Fused Filter: Error wrong filter, this function should only be used for a filter created with BNNSFilterCreateFusedLayer
BNNS Fused Filter: Error first filter apply failed
BNNS Fused Filter: Error malloc failed
BNNS Arithmetic Filter: Error filter is NULL
BNNS Arithmetic Filter Warning: Batch size is 0, nothing to do
BNNS Arithmetic Filter: Error wrong filter, this function should only be used for a filter created with BNNSFilterCreateLayerArithmetic
BNNS Pooling: Error wrong filter type, filter is not pooling
BNNS Permute Filter: filter is NULL
BNNS Permute Filter Warning: Batch size is 0, nothing to do
BNNS Permute Filter: input delta is NULL
BNNS Permute Filter: output delta is NULL
BNNS Permute Filter: input delta data pointer is NULL
BNNS Permute Filter: output delta data pointer is NULL
BNNS Permute Filter: inplace gradient is not supported
out_delta is NULL
out_delta->data is NULL
Tensor Contraction filter has no bias: bias_delta must be NULL.
Tensor Contraction filter expects more than one input
Resize filter has no bias: bias_delta must be NULL.
Resize filter has no weights: weights_delta must be NULL.
Resize filter: backward pass requires in_delta and in_delta->data to be non-NULL.
BNNS Copysum filter has no bias: bias_delta must be NULL.
BNNS Copysum filter has no weights: weights_delta must be NULL.
BNNS Copysum filter: backward pass requires in_delta and in_delta->data to be non-NULL.
BNNS Reduction filter has no bias: bias_delta must be NULL.
BNNS Padding: backward pass requires in_delta and in_delta->data to be non-NULL.
BNNS Padding filter has no bias: bias_delta must be NULL.
BNNS Embedding: in must not be NULL.
BNNS Embedding: in_delta must be NULL.
BNNS Embedding: backward pass requires weights_delta and weights_delta->data to be non-NULL.
BNNS Embedding: bias_delta must be NULL.
BNNS Normalization Backward: Error filter is NULL
BNNS Normalization Backward Warning: Batch size is 0, nothing to do
BNNS Normalization Backward: Error wrong filter, this function should only be used for a filter created with BNNSFilterCreateLayerNormalization
BNNS BatchNorm Backward: Error filter is NULL
BNNS BatchNorm Backward Warning: Batch size is 0, nothing to do
BNNS BatchNorm Backward: Error wrong filter, this function should only be used for a filter created with BNNSFilterCreateLayerNormalization
BNNS Instance Norm Backward: Error filter is NULL
BNNS Instance Norm Backward Warning: Batch size is 0, nothing to do
BNNS Instance Norm Backward: Error wrong filter, this function should only be used for a filter created with BNNSFilterCreateLayerNormalization
BNNS Layer Norm Backward: Error filter is NULL
BNNS Layer Norm Backward Warning: Batch size is 0, nothing to do
BNNS Layer Norm Backward: Error wrong filter, this function should only be used for a filter created with BNNSFilterCreateLayerNormalization
BNNS Group Norm Backward: Error filter is NULL
BNNS Group Norm Backward Warning: Batch size is 0, nothing to do
BNNS Group Norm Backward: Error wrong filter, this function should only be used for a filter created with BNNSFilterCreateLayerNormalization
BNNS Fused Filter Backward: Error filter is NULL
BNNS Fused Filter Backward Warning: Batch size is 0, nothing to do
BNNS Fused Filter Backward: output delta is NULL
BNNS Fused Filter Backward: Error delta_parameters is NULL
BNNS Fused Filter Backward: Error wrong filter, this function should only be used for a filter created with BNNSFilterCreateFusedLayer
BNNS Fused Filter Backward: Error - fused compute and quantization gradient is not support 
BNNS Fused Filter Backward: Error Weight delta pointer is NULL. backward computation of all active parameters must be done in a single function call. 
BNNS Fused Filter Backward: Error bias delta pointer is NULL. backward computation of all active parameters must be done in a single function call. 
BNNS Fused Filter Backward: Error normalization beta delta pointer is NULL. backward computation of all active parameters must be done in a single function call. 
BNNS Fused Filter Backward: Error normalization gamma delta pointer is NULL. backward computation of all active parameters must be done in a single function call. 
BNNS Arithmetic Backward: failed to allocate memory
BNNS Fused Filter Backward: Error normalization backward failed
BNNSDirectApplyConvolutionBatch not supported 
BNNSDirectApplyTransposedConvolutionBatch not supported 
BNNSDirectApplyPoolingBatch not supported 
BNNSDirectApplyLossBatch not supported
BNNS Apply: filter or output can't be null
BNNS Apply: Transposed convolution does not support groups
BNNS Apply: Tensor Contraction filter expects more than one input
BNNS Apply: Loss filter apply must be called with BNNSLossFilterApplyBatch
BNNS Apply: Batchnorm filter apply must be called with BNNSNormalizationFilterApplyBatch
BNNS Apply: Instance Norm filter apply must be called with BNNSNormalizationFilterApplyBatch
BNNS Apply: Layer Norm filter apply must be called with BNNSNormalizationFilterApplyBatch
BNNS Apply: Group Norm filter apply must be called with BNNSNormalizationFilterApplyBatch
BNNS Apply: Fused filter apply must be called with BNNSFusedFilterApplyBatch
BNNS Apply: invalid filter
BNNS DESTROY: invalid filter
BNNS Padding + Convolution Apply Backward: input is null
BNNS Padding + Convolution Apply Backward: allocation failed
BNNS Padding + Convolution Apply Backward: Padding failed
BNNS Padding + Convolution Apply Backward: Convolution backward failed
BNNS Padding + Convolution Apply Backward: Padding backward failed
malloc failed
Convolution failed
BNNS Convolution Variant: input type %s not supported!!!
BNNS Convolution Variant: compute type %s not supported!!!
BNNS GRU Fused Gates: Output pointer is null
BNNS GRU Fused Gates: Input pointer is null
BNNS GRU Fused Gates: Recurrent pointer is null
BNNS GRU Fused Gates: Hidden Input pointer is null
BNNS GRU Fused Gates: Input vector size %zu foesn't match output vectro size %zu
BNNS GRU Fused Gates: Recurrent vector size %zu foesn't match output vectro size %zu
BNNS GRU Fused Gates: Hidden Input vector size %zu foesn't match output vectro size %zu
BNNS GRU Fused Gates: number of gates for Input %zu and Recurrent %zu doesn't match
BNNS GRU Fused Gates: Input stride %zu is smaller than number of elements %zu
BNNS GRU Fused Gates: Recurrent stride %zu is smaller than number of elements %zu
BNNS GRU Fused Gates: Output supported data types are fp32 or fp16
BNNS GRU Fused Gates: Input data type doesn't match output data type
BNNS GRU Fused Gates: Recurrent data type doesn't match output data type
BNNS GRU Fused Gates: Hidden Input data type doesn't match output data typen
BNNS GRU Fused Gates: GRU fused gates doesn't support %zu gates
BNNS GRU Fused Gates: Hidden Input batch size %zu doesn't match output batch size %zu
BNNS GRU Fused Gates: Input batch size %zu doesn't match output batch size %zu
BNNS GRU Fused Gates: Hidden Output batch stride %zu is small then a single vector stride
BNNS GRU Fused Gates: Hidden Input batch stride %zu is small then a single vector stride
BNNS GRU Fused Gates: Input batch stride %zu is small then a single vector stride
BNNS GRU Fused Gates: Recurrent batch stride %zu is small then a single vector stride
BNNS CONVOLUTIONS VERSION2: unsupported src type to pack
BNNS Convolution Compute Weights delta: failed to compute
BNNS Convolution Apply Backward: activation_grad allocation failed
weight delta backward failed
BNNS Convolution: int32 repack allocation failed
BNNS Convolution: Float32 repack allocation failed
BNNS Activation Apply: Error input/output/activation pointers must be non-NULL
BNNS Activation Apply: Input and output tensor deosn't match
BNNS Activation Apply: unable to allocate memory to compute activation
BNNS Activation Apply: failed to allocate memory
BNNS Apply Activation Backward: BNNSActivationFunctionIntegerLinearSaturate isn't supported
BNNS Apply Activation Backward: BNNSActivationFunctionIntegerLinearSaturatePerChannel isn't supported
BNNS Apply Activation Backward: failed to allocate temporary y
BNNS Apply Activation Backward: unsupported activation backward
BNNS Activation backward: malloc failed
BNNS Activation: number of memory descriptor is lower than expected
BNNS Activation: claiming pre allocated memory failed
BNNS Activation: memory allocation failed
BNNS Activation: Failed to init filter
BNNS Activation Apply Backward: At least one of in or out must be non-NULL
BNNS Activation Apply Backward: invalid filter
BNNS Activation Apply Backward: out_delta must not be NULL
BNNS Activation Backward: preallocated memory isn't supported
BNNS Activation Apply Backward: Input is required
BNNS Activation Backward: Weights Delta not supported
BNNS Activation Apply Backward: Bias Delta not supported
BNNS Activation Apply Backward: in place operation for none matching data sizes isn't supported
BNNS Activation Apply Backward: Input and output tensor deosn't match
BNNS Activation Apply Backward: In Delta tensor deosn't match
BNNS Activation Apply Backward: Out Delta tensor deosn't match
BNNS Activation Apply Backward: Weights Delta tensor (%zu x %zu) deosn't make sense
BNNS Activation Apply Backward: unable to allocate memory to compute backward with missing original output
BNNS Activation Apply Backward: unable to allocate memory to compute activation, using slower compute path
BNNS Activation Apply Backward: failed to allocate memory
BNNS Activation Apply Backward: failed to init index counter
BNNS Activation: invalid argument
BNNS Activation: in-place activation layer is allowed only for output types with the same or smaller storage size
BNNS Activation: unsupported types for conversion
BNNS Activation: Apply failed
BNNS Activation: val doesn't make sense
BNNS Activation Apply: activation function not supported
BNNS Activation Init: filter is null
BNNS Activation Init: input descriptor is illegal
BNNS Activation Init: output descriptor is illegal
BNNS Activation Init: layout doesn't match
BNNS Activation Init: only 3D conversions are supported
BNNS Activation Init: memory allocation failed
BNNS Activation Init: dim %zu input size %zu != %zu output size
BNNS Activation Init: alpha can't be +/-inf or Nan for Gumbel or Gumbel Max
BNNS Activation Init: beta can't be +/-inf or Nan or zero or negative for Gumbel or Gumbel Max
BNNS Activation Init: BNNSActivationFunctionPReLUPerChannel is only valid with data layout BNNSDataLayoutImageCHW
BNNS Activation Init: invalid activation function
BNNS SoftMax: sum result shouldn't be 0
BNNS Activation Apply Backward: Softmax backward require original output
BNNS Activation Apply Backward: original inputs or original outputs must be available
BNNS Activation Apply Backward: activation function not supported
BNNS Activation: Error NULL input/output pointer
BNNS Activation: Error number of input channels must be equal to number of output channels
BNNS Multihead Attention Apply: key_mask must be a 1D tensor.
BNNS Multihead Attention Apply: key_mask must have type BNNSDataTypeBoolean.
BNNS Multihead Attention Apply: key_mask must have size exactly source_length (key_mask.size[0] = %zu, source_length = %zu).
BNNS Multihead Attention Apply: 2D add_to_attention must have shape (target_length, source_length) = (%zu, %zu), but passed tensor has shape (%zu, %zu).
BNNS Multihead Attention Apply: 3D add_to_attention must have shape (num_heads, target_length, source_length) = (%zu, %zu, %zu), but passed tensor has shape (%zu, %zu, %zu).
BNNS Multihead Attention Apply: 4D add_to_attention must have shape (batch_size, num_heads, target_length, source_length) = (%zu, %zu, %zu, %zu), but passed tensor has shape (%zu, %zu, %zu, %zu).
BNNS Multihead Attention Apply: unsupported layout for argument add_to_attention.
BNNS Multihead Attention Apply: add_to_attention must have type BNNSDataTypeBoolean or BNNSDataTypeFloat32.
BNNS Multihead Attention Apply: If backprop_cache is non-NULL, backprop_cache_size must also be non-NULL
BNNS Multihead Attention Apply: If workspace is non-NULL, workspace_size must also be non-NULL
BNNS Multihead Attention Apply: Supplied workspace is too small (size %ld, but required %ld)
Multihead Attention Backward: Only support for a full-size backprop_cache has been implemented.
Multihead Attention Backward: If workspace is non-NULL, workspace_size must also be non-NULL
Multihead Attention Backward: Insufficient workspace supplied (%zu bytes). Require %zu bytes.
value_delta_copy_out
value_shadow_delta
value_delta
key_delta_copy_out
key_shadow_delta
key_delta
query_delta_copy_out
query_shadow_delta
query_delta
layer_params->query.target_desc
layer_params->key.target_desc
layer_params->query.weights
layer_params->key.weights
layer_params->value.weights
(d_model, d_key, num_heads)
(source_length, k_dim)
(k_dim, d_key, num_heads)
layer_params->value.target_desc
(source_length, v_dim)
(v_dim, d_value, num_heads)
layer_params->output.target_desc
(target_length, d_model)
layer_params->output.weights
(num_heads*d_value, d_model)
BNNS Multihead Attention Create: layer_params->key_attn_bias.data and layer_params->value.attn_bias.data must both be NULL or must both be not NULL
layer_params->key_attn_bias
(d_key, num_heads)
layer_params->value_attn_bias
(d_value, num_heads)
BNNS Multihead Attention Create: fp16 and bf16 may not be mixed or unsupported data type
layer_params->query.bias
layer_params->key.bias
layer_params->value.bias
layer_params->output.bias
(d_model)
BNNS Multihead Attention Create: Unsupported layout for query.weights
BNNS Multihead Attention Create: Unsupported layout for key.weights
BNNS Multihead Attention Create: Unsupported layout for value.weights
BNNS MULTIHEAD ATTENTION GetPointer: Layer was created with dropout=0.0, cannot return pointer to dropout value.
BNNS MULTIHEAD ATTENTION GetPointer: Unsupported target %d
BNNS Multihead Attention Create: Expected %s to have exactly %zu dimensions, but it has %zu
BNNS Multihead Attention Create: Expected %s to have shape %s = (
%s%zu
) but has actual shape (
BNNS SPECIALIZED CONVOLUTION Create: failed to alloacted memory
%s: 
BNNS Permute Filter: layer_params is NULL
BNNS Permute Filter: illegal input descriptor
BNNS Permute Filter: illegal output descriptor
BNNS Permute Filter: input and output descriptor layout dimension does not match. input dimm = %zu output dimm = %zu
BNNS Permute Filter: permutation array index %zu has illegal value of %zu
BNNS Permute Filter: permutation array is missing axis %zu
BNNS Permute Filter: permutation array axis %zu appears %zu times, each axis should appear exactly once
0123456789abcdefghijklmnopqrstuvwxyz
BNNS Pooling Filter: filter is NULL
BNNS Pooling Filter: input is NULL
BNNS Pooling Filter: output is NULL
BNNS Pooling Filter Warning: Batch size is 0, nothing to do
BNNS Pooling: input batch stride doesn't make sense (%zu < %zu x %zu)
BNNS Pooling: dilation supported only in BNNSPoolingFunctionMax/UnMax
BNNS Pooling: dilation supported only for BNNSDataTypeFloat32 data type
Pooling layer filter running slow path: stride=%zu,%zu kernel=%zu,%zu
BNNS Pooling UnMax: indices array is NULL
BNNS Pooling: case not implemented
BNNS Pooling Apply: allocation of work buffer failed
BNNS Pooling Backward: cannot run backward without output delta
BNNS Pooling Backward: only float32 output delta are supported
BNNS Pooling Backward: only float32 input delta are supported
BNNS Pooling Backward: only float32 bias delta are supported
BNNS Pooling Backward: unsupported pooling function
BNNS Pooling Backward: invalid argument
BNNS Pooling Backward: Fusion of compute with activation is unsupported for given activation function
BNNS Pooling Backward: failed to allocate memory
BNNS Pooling Backward: failed to apply activation backward
BNNS Pooling: layer parameters is NULL
BNNS Pooling: input must be a 3D array
BNNS Pooling: input descriptor is illegal
BNNS Pooling: output must be a 3D array
BNNS Pooling: output descriptor is illegal
BNNS Pooling: input/output channel counts do not match
BNNS Pooling: input/output types do not match
BNNS Pooling: invalid kernel dimensions, should be greater than 0
BNNS Pooling: optimized code supports kernel width/height up to 16
BNNS Pooling: dilation only supported for BNNSPoolingFunctionMax/UnMax
BNNS Pooling: dilation only supported for BNNSDataTypeFloat32 input/output data types
BNNS_POOLING: input size computation wraparound input (%zu x %zu), output (%zu x %zu), stride (%zu x %zu), padding (left %zu right %zu up %zu down %zu)
BNNS_POOLING: input size computation wraparound input (%zu x %zu), output (%zu x %zu), stride (%zu x %zu), padding (%zu x %zu)
BNNS_POOLING: output(%zu x %zu) is larger than input(%zu x %zu) with padding(left %zu right %zu up %zu down %zu).
 stride (%zu x %zu)
BNNS_POOLING: output(%zu x %zu) is larger than input(%zu x %zu) with padding(%zu x %zu).
 stride (%zu x %zu)
BNNS_POOLING: invalid pooling function
BNNS_POOLING: supported input/output data types: float32 float16
BNNS_POOLING: slow path: stride not in {1,2}
BNNS_POOLING: slow path: kernel size not in {2,3,4}
BNNS Pooling: Internal Memory size %zu doesn't match expected %d
BNNS Pooling: claiming pre allocated memory failed
BNNS Pooling: failed to allocate context
BNNS Pooling Backward: only float32 delta are supported
BNNS Pooling Backward: wrong pooling function called
BNNS Reduction Create: Only FP32 input data is supported for reduction function %u
BNNS Reduction Create: Only FP32 output data is supported for reduction function %u
BNNS Reduction Create: Only FP32 or bool input data is supported for reduction function %u
BNNS Reduction Create: Only FP32 or bool output data is supported for reduction function %u
BNNS Reduction Create: Invalid reduction function %u
BNNS Reduction Create: Only FP32 weight data is supported
BNNS Reduction Create: i_desc and o_desc must have the same number of dimensions (%zu vs %zu)
BNNS Reduction Create: i_desc and w_desc must have the same number of dimensions (%zu vs %zu)
BNNS Reduction Create: i_desc and o_desc dimensions must have the same size, or o_desc.size[d] must be 1 to indicate reduction (dimension %zu: %zu vs %zu)
BNNS Reduction Create: i_desc and w_desc dimensions must have the same size (dimension %zu: %zu vs %zu)
BNNS Reduction Create: ArgMin/ArgMax reduction is only supported on a single axis
BNNS Reduction Apply: Both input and output must non-NULL
BNNS Reduction Backward: input_delta must have the same number of dimensions as i_desc
BNNS Reduction Backward: input_delta must have the same shape as i_desc
BNNS Reduction Backward: output_delta must have the same number of dimensions as o_desc
BNNS Reduction Backward: output_delta must have the same shape as o_desc
BNNS Reduction Backward: output_delta must not be NULL
BNNS Reduction Backward: weights_delta must have the same number of dimensions as w_desc
BNNS Reduction Backward: weights_delta must have the same shape as w_desc
BNNS Reduction Direct Apply: Size of the dimensions of the output can not be greater than the size of the input
BNNS Reduction Direct Apply: ArgMin/ArgMax reduction is only supported on a single axis
BNNS Reduction Direct Apply: Unsupported data type combination
Backward usage is not supported for reduction type %u
BNNS Reduction Backward: Reduce function %u requires input to be supplied for backward computation
BNNS Reduction Backward: Reduce function %u requires output to be supplied for backward computation
BNNS Reduction Backward: Calculation of weights_delta requires input to be supplied
Size mismatch in dimension %zu: input %zu, output %zu
BNNS Reduce Apply Generic: Size mismatch in dimension %zu: input %zu, output %zu
BNNS Arithmetic Filter: layer_params is NULL
BNNS Arithmetic Filter: unsupported arithmetic function
BNNS Arithmetic Filter: memory allocation failed
BNNS Arithmetic Filter: Failed to init filter
BNNS Arithmetic Filter: filter is NULL
BNNS Arithmetic Filter: wrong filter type, this function should only be used for a filter created with BNNSFilterCreateLayerArithmetic
BNNS Arithmetic Filter: in pointer is NULL
BNNS Arithmetic Filter: batch_size>1 and in_stride is 0
BNNS Arithmetic Filter Internal: expected number of inputs call failed
BNNS Arithmetic Filter: wrong number_of_inputs, number_of_inputs=%zu, expecting %zu
BNNS Arithmetic Filter: in[%zu] is NULL
BNNS Arithmetic Filter: in_stride[%zu] is 0
BNNS Arithmetic Filter: output pointer is NULL
BNNS Arithmetic Filter: batch_size>1 and out_stride is 0
BNNS Arithmetic Filter: descriptors for input that is being processed in-place and output must exactly match
BNNS Arithmetic Filter: input is being processed in-place but input and output descriptor types do not match
BNNS Arithmetic Filter: output dimension smaller than input is not supported. batch size>1, input type is BNNSSample but out_type is not BNNSSample
BNNS Arithmetic Filter: only BNNSDataTypeFloat32 is supported
BNNS Arithmetic Filter: input1 and input2 have the same data pointer, but descriptors are different
BNNS Arithmetic Filter: input1 and input2 have the same data pointer, but different BNNSDescriptorType
BNNS Arithmetic Filter: descriptors for input1 that is being processed in-place and output must exactly match
BNNS Arithmetic Filter: input1 is being processed in-place but input1 and output descriptor types do not match
BNNS Arithmetic Filter: descriptors for input2 that is being processed in-place and output must exactly match
BNNS Arithmetic Filter: input2 is being processed in-place but input2 and output descriptor types do not match
BNNS Arithmetic Filter: output dimension smaller than input is not supported. batch size>1, an input type is BNNSSample but out_type is not BNNSSample
BNNS Arithmetic Filter: input1 and input3 have the same data pointer, but descriptors are different
BNNS Arithmetic Filter: input1 and input3 have the same data pointer, but different BNNSDescriptorType
BNNS Arithmetic Filter: input2 and input3 have the same data pointer, but descriptors are different
BNNS Arithmetic Filter: input2 and input3 have the same data pointer, but different BNNSDescriptorType
BNNS Arithmetic Filter: descriptors for input3 that is being processed in-place and output must exactly match
BNNS Arithmetic Filter: input3 is being processed in-place but input3 and output descriptor types do not match
BNNS Arithmetic Filter Internal: unsupported arithmetic function
BNNS Arithmetic Filter: out delta descriptor pointer is NULL
BNNS Arithmetic Filter: out delta descriptor data pointer is NULL
BNNS Arithmetic Filter: out_delta_stride is 0
BNNS Arithmetic Filter: out delta descriptor validation failed
BNNS Arithmetic Filter: in_delta is NULL
BNNS Arithmetic Filter: in_delta_stride is NULL
BNNS Arithmetic Filter: in is NULL, but it is required for backward compute
BNNS Arithmetic Filter: in_delta descriptor pointer %zu is NULL
BNNS Arithmetic Filter: cannot compute activation backward, output is NULL
BNNS Arithmetic Filter: Fusion of arithmetic with activation is unsupported for given activation function
BNNS Arithmetic Filter Backward Internal: unsupported arithmetic function
BNNS Arithmetic Filter Backward: failed to apply activation backward
BNNS Arithmetic Filter: input type is BNNSConstant, no gradient to compute
BNNS Arithmetic Filter: in_delta[0] is NULL
BNNS Arithmetic Filter: in_delta[0]->data is NULL
BNNS Arithmetic Filter: in_delta_stride[0] is 0
BNNS Arithmetic Filter: in delta descriptor validation failed
BNNS Arithmetic Filter: arithmetic input_delta and input descriptors must have the same sizes, strides and data types
BNNS Arithmetic Filter: arithmetic output_delta and output descriptors must have the same sizes, strides and data types
BNNS Arithmetic Filter: descriptors for out_delta that is being processed in-place and in_delta must exactly match
BNNS Arithmetic Filter: input delta is being processed in-place but input type and output type are different
BNNS Arithmetic Filter: input delta and output delta has the same data pointer (inplace), but different in_delta_stride
BNNS Arithmetic Filter: input delta and output delta has the same data pointer (inplace), but in_delta.flags indicates to accumulate result
BNNS Arithmetic Filter: backward compute for chosen arithmetic function does not allow inplace gradient. out delta data must not be the same as input delta data
BNNS Arithmetic Filter: inputs are both BNNSConstant, no gradient to compute
BNNS Arithmetic Filter: in_delta[0] is NULL but in1 type is not BNNSConstant
BNNS Arithmetic Filter: in_delta[0]->data is NULL but in1 type is not BNNSConstant
BNNS Arithmetic Filter: in1_type is BNNSSample but in_delta_stride[0] is 0
BNNS Arithmetic Filter: input1 delta data pointer is not NULL. cannot compute gradient for BNNSConstant
BNNS Arithmetic Filter: in1 delta descriptor validation failed
BNNS Arithmetic Filter: in_delta[1] is NULL but in2 type is not BNNSConstant
BNNS Arithmetic Filter: in_delta[1]->data is NULL but in2 type is not BNNSConstant
BNNS Arithmetic Filter: in2_type is BNNSSample, but in_delta_stride[1] is 0
BNNS Arithmetic Filter: input2 delta data pointer is not NULL. cannot compute gradient for BNNSConstant
BNNS Arithmetic Filter: in2 delta descriptor validation failed
BNNS Arithmetic Filter: input1 delta and input2 delta has the same data pointer, but descriptors are different
BNNS Arithmetic Filter: input1 delta and input2 delta has the same data pointer, but different data types
BNNS Arithmetic Filter: input1 delta and input2 delta has the same data pointer, but different in_delta_stride
BNNS Arithmetic Filter: forward input pointer points to the same array but input gradient pointers point to different arrays
BNNS Arithmetic Filter: descriptors for out_delta that is being processed in-place and in1_delta must exactly match
BNNS Arithmetic Filter: input1 delta is being processed in-place but input type and output type are different
BNNS Arithmetic Filter: input1 delta and output delta has the same data pointer (inplace), but different in_delta_stride
BNNS Arithmetic Filter: input1 delta and output delta has the same data pointer (inplace), but in1_delta.flags indicates to accumulate result
BNNS Arithmetic Filter: descriptors for out_delta that is being processed in-place and in2_delta must exactly match
BNNS Arithmetic Filter: input2 delta is being processed in-place but input2 type and output type are different
BNNS Arithmetic Filter: input2 delta and output delta has the same data pointer (inplace), but different in_delta_stride
BNNS Arithmetic Filter: input2 delta and output delta has the same data pointer (inplace), but in2_delta.flags indicates to accumulate result
BNNS Arithmetic Filter: backward compute for chosen arithmetic function does not allow inplace gradient. out delta data must not be the same as either of the input delta data
BNNS Arithmetic Filter: inputs are all BNNSConstant, no gradient to compute
BNNS Arithmetic Filter: in_delta[2] is NULL but in3 type is not BNNSConstant
BNNS Arithmetic Filter: in_delta[2]->data is NULL but in3 type is not BNNSConstant
BNNS Arithmetic Filter: in3_type is BNNSSample, but in_delta_stride[2] is 0
BNNS Arithmetic Filter: input3 delta data pointer is not NULL. cannot compute gradient for BNNSConstant
BNNS Arithmetic Filter: in3 delta descriptor validation failed
BNNS Arithmetic Filter: input1 delta and input3 delta has the same data pointer, but descriptors are different
BNNS Arithmetic Filter: input1 delta and input3 delta has the same data pointer, but different data types
BNNS Arithmetic Filter: input1 delta and input3 delta has the same data pointer, but different in_delta_stride
BNNS Arithmetic Filter: input2 delta and input3 delta has the same data pointer, but descriptors are different
BNNS Arithmetic Filter: input2 delta and input3 delta has the same data pointer, but different data types
BNNS Arithmetic Filter: input2 delta and input3 delta has the same data pointer, but different in_delta_stride
BNNS Arithmetic Filter: descriptors for out_delta that is being processed in-place and in3_delta must exactly match
BNNS Arithmetic Filter: input3 delta is being processed in-place but input3 type and output type are different
BNNS Arithmetic Filter: input3 delta and output delta has the same data pointer (inplace), but different in_delta_stride
BNNS Arithmetic Filter: input3 delta and output delta has the same data pointer (inplace), but in3_delta.flags indicates to accumulate result
BNNS Arithmetic Filter: in_delta[0] must be NULL for BNNSArithmeticSelect
BNNS Arithmetic Filter: backward compute for BNNSArithmeticSelect does not allow inplace gradient. out delta data must not be the same as either of the input delta data
BNNS Arithmetic Filter: arithmetic_function_fields is NULL
BNNS Arithmetic Filter: malloc failed
BNNS Arithmetic Filter: input descriptor validation failed
BNNS Arithmetic Filter: input1 descriptor validation failed
BNNS Arithmetic Filter: unsupported out_type
BNNS Arithmetic Filter: unsupported in_type
BNNS Arithmetic Filter: in, out descriptor check failed
BNNS Arithmetic Filter: input2 descriptor validation failed
BNNS Arithmetic Filter: unsupported in1_type
BNNS Arithmetic Filter: unsupported in2_type
BNNS Arithmetic Filter: in1,in2,out descriptor check failed
BNNS Arithmetic Filter: input3 descriptor validation failed
BNNS Arithmetic Filter: unsupported in3_type
BNNS Arithmetic Filter: in1,in2,in3,out descriptor check failed
BNNS Arithmetic: Activation creation failed
BNNS Arithmetic Filter: input and output data types do not match
BNNS Arithmetic Filter: input size[%zu]=%zu does not equal max input,output size of %zu or 1
BNNS Arithmetic Filter: output size[%zu]=%zu does not equal max input,output size of %zu
BNNS Arithmetic Filter: input1 and out data types do not match
BNNS Arithmetic Filter: input2 and out data types do not match
BNNS Arithmetic Filter: input1 size[%zu]=%zu does not equal max input1,input2,out size of %zu or 1
BNNS Arithmetic Filter: input2 size[%zu]=%zu does not equal max input1,input2,out size of %zu  or 1
BNNS Arithmetic Filter: output size[%zu]=%zu does not equal max input1,input2,out size of %zu
BNNS Arithmetic Filter: input3 and out data types do not match
BNNS Arithmetic Filter: input1 size[%zu]=%zu does not equal max input1,input2,input3,out size of %zu or 1
BNNS Arithmetic Filter: input2 size[%zu]=%zu does not equal max input1,input2,input3,out size of %zu  or 1
BNNS Arithmetic Filter: input3 size[%zu]=%zu does not equal max input1,input2,input3,out size of %zu  or 1
BNNS Arithmetic Filter: output size[%zu]=%zu does not equal max input1,input2,input3,out size of %zu
BNNS Arithmetic Filter:  illegal descriptor
BNNS Arithmetic Filter: BNNSArithmeticSelect expects BNNSDataTypeBoolean
BNNS Arithmetic Filter: data layout unsupported
BNNS Arithmetic Filter: descriptor memory non contiguous
BNNS Arithmetic Filter Internal: descriptors must point to contiguous memory
BNNS Arithmetic Filter: in delta descriptor memory non contiguous
BNNS Arithmetic Filter: out delta descriptor memory non contiguous
BNNS Arithmetic Filter: in1 delta descriptor memory non contiguous
BNNS Arithmetic Filter: in2 delta descriptor memory non contiguous
BNNS Arithmetic Filter: output delta descriptor memory non contiguous
BNNS Arithmetic Filter: in3 delta descriptor memory non contiguous
BNNS Embedding Create: Only 8, 16, 32 and 64 bit integer types may be used as input data type.
BNNS Embedding Create: Only 16 and 32 bit floating point and 8, 16 and 32 bit integer types may be used as dictionary data type.
BNNS Embedding Create: o_desc dimension must be dim(i_desc) + dim(dictionary) - 1.
BNNS Embedding Create: First dim(dictionary)-1 dimensions of o_desc and dictionary must have consistent shapes.
BNNS Embedding Create: Last dim(i_desc) dimensions of o_desc and i_desc must have consistent shapes.
internal embedding copy
dict-item
o-item
internal embedding copy back
BNNS Embedding Apply: Input value %zu is out of range [0, %zu)
BNNS Embedding ApplyBackward: weights_delta must have the same shape as layer_params->dictionary
BNNS Embedding ApplyBackward: out_delta must have the same shape as layer_params->o_desc
BNNS Embedding ApplyBackward: Input value %zu out of range
BNNS Sparse Embedding Create: Only 8, 16, 32 and 64 bit integer types may be used as input data type.
BNNS Sparse Embedding Create: Only 16 and 32 bit floating point and 8, 16 and 32 bit integer types may be used as dictionary data type.
BNNS Sparse Embedding Create: o_desc dimension must be dim(i_desc) + dim(dictionary) - 1.
BNNS Sparse Embedding Create: First dim(dictionary)-1 dimensions of o_desc and dictionary must have consistent shapes.
BNNS Sparse Embedding Create: Last dim(i_desc) dimensions of o_desc and i_desc must have consistent shapes.
BNNS Sparse Embedding Create: Unsupported optimization function
BNNS Sparse Embedding Create: Dictionary must be of type BNNSDataTypeFloat32
BNNS Sparse Embedding Apply: Filter is no longer valid
BNNS Sparse Embedding Apply: Input value %zu is out of range [0, %zu)
BNNS Sparse Embedding Apply Backwards: Filter is no longer valid
BNNS Embedding Backwards Sparse: Accumulation of sparse weights gradient is not supported.
BNNS Embedding Backwards Sparse: out_delta must have the same shape as layer_params->o_desc
BNNS Embedding Backwards Sparse: dictionary_delta->indices must have a single dimension
BNNS Embedding Backwards Sparse: dictionary_delta->indices.size[0] must be 1
BNNS Embedding Backwards Sparse: dictionary_delta->sparse_dimension_size[0] must be num_embeddings
BNNS Embedding Backwards Sparse: dictionary_delta->values shape must match dictionary item shape
BNNS Embedding Backwards Sparse: For in-place indices, dictionary_delta->count must equal the total size of layer_params->i_desc * batch_size
BNNS Embedding Backwards Sparse: For in-place indices, input must be contiguous
BNNS Embedding Backwards Sparse: For in-place values, dictionary_delta->count must equal the total size of out_delta * batch_size
BNNS Embedding Backwards Sparse: For in-place values, out_delta must be contiguous in its indexing dimensions
BNNS Sparse Embedding Get Representation: Filter is no longer valid
BNNS Sparse Embedding Get Representation: Unexpected value for num_accumulators=%zu (expected %zu)
BNNS Sparse Embedding Get Representation: num_opt_field_changes!=NULL but opt_field_changes_indices is NULL
BNNS Sparse Embedding Get Representation: num_opt_field_changes!=NULL but opt_field_changes is NULL
BNNS Sparse Embedding Get Representation: num_opt_field_changes=NULL, but optimization fields of filter have previosuly been updated
BNNS Sparse Embedding Set Representation: Filter is no longer valid
BNNS Embedding Set Sparse Optimizer Context: Expected %zu accumulators
BNNS Embedding Set Sparse Optimizer Context: values[0]->indices must have dimension 1
BNNS Embedding Set Sparse Optimizer Context: values[0]->indices.size[0] must be 1
BNNS Embedding Set Sparse Optimizer Context: Expected values indices descriptors to be identical (that is point to the same data)
BNNS Embedding Set Sparse Optimizer Context: Expected values indices descriptors to be identical
dest
BNNS Sparse Embedding Optimizer Step: Filter is no longer valid
BNNS Embedding Sparse Optimizer Step: Expected dictionary_delta->indices to have a single dimension
BNNS Embedding Sparse Optimizer Step: Expected dictionary_delta->values to match shape of dictionary
BNNS Embedding Sparse Optimizer Step: Dictionary gradient items must be contiguous
BNNS Embedding Sparse Optimizer Step: Dictionary items must be contiguous
BNNS Embedding Sparse Optimizer Step: grad.data_type must be BNNSDataTypeFloat32
BNNS Sparse Embedding Get Dense: Filter is no longer valid
BNNS Sparse Embedding Get Dense: Unexpected value for num_accumulators=%zu (expected %zu)
BNNS Sparse Embedding Get Dense: Unexpected number of dimensions for accumulator[%zu]
BNNS Sparse Embedding Get Dense: Unsupported data type for accumulator[%zu]
BNNS Sparse Embedding Get Dense: Unexpected shape for accumulator[%zu]
BNNS Sparse Embedding Get Dense: accumulator[%zu] must be contiguous in dictionary item dimensions
v8@?0
hw.physicalcpu
BNNSActivationFunctionIntegerLinearSaturate isn't supported in apply_bias_and_activation
BNNSActivationFunctionIntegerLinearSaturatePerChannel isn't supported in apply_bias_and_activation
BNNSActivationFunctionIntegerLinearSaturate isn't supported in apply_bias_and_activation_bf16
BNNSActivationFunctionIntegerLinearSaturatePerChannel isn't supported in apply_bias_and_activation_bf16
allocation failed, size=%zu
reallocation failed, size=%zu
allocation failed, size=%zu, align=%zu
BNNS: unexpected data type, failing
BNNS: Data type not supprted, fail
BNNS: layout not supported
BNNS: active dimension must be greater than 0
BNNS: dimension %zu stride %zu is lower then previous dimension actual size %zu * %zu (size*stride)
Unsupported layout: %d
BNNS: memory usage exceeded capacity
BNNS PreAllocated Memory: memory usage exceeded capacity
BNNS PreAllocated Memory: failed to claim scratch memory
BNNS Create Convolution Winograd: malloc failed
BNNS Convolutions Winograd: weights allocation failed
BNNS Convolutions Winograd: bias allocation failed
BNNS Apply Convolution Winograd: memory allocation failed
weight packing must be at least 32 and a power of 2
BNNS Dequantize: shouldn't have reached fp16 convert path
BNNS batch norm forward: malloc for mean failed
BNNS batch norm forward: malloc for var failed
BNNS batch norm forward: malloc for isqrtvar failed
BNNS Batchnorm Backward Apply: delta allocation failed
BNNS Batchnorm Backward Apply: activation_grad allocation failed
BNNS instance norm backward: malloc for dbeta failed
BNNS instance norm backward: malloc for dgamma failed
BNNS Layer Norm Apply Backward: malloc activation_grad failed
BNNS Layer Norm Apply Backward: dx hat allocation failed
BNNS Group Norm Apply Backward: failed to allocate memory
BNNS Group Norm Apply Backward: failed to allocate activation grad memory
BNNS Convolution Apply: input batch stride doesn't make sense (%zu < %zu x %zu)
BNNS Convolution Apply: output batch stride doesn't make sense (%zu < %zu x %zu)
BNNS Convolution Apply: failed to allocate weights in apply
BNNS Convolution Backward: BNNSNDArrayFlagBackpropAccumulate has not yet been implemented for weights_delta or bias_delta.
BNNS Convolution Backward: cannot run backward without output delta
BNNS Convolution Backward: [out delta/in delta/weight delta/bias delta] - data type check failed. unsupported data type
BNNS Convolution Backward: internal error, incorrect wrapper
BNNS Convolution forward output data type does not match output delta data type
BNNS Convolution forward input data type does not match input delta data type
BNNS Convolution forward weights data type does not match weights delta data type
BNNS Convolution weight packing is not supported backward
BNNS Convolution Backward: BNNSFlagsUseClientPtr must be enabled during training
BNNS Convolution Backward: invalid argument
BNNS Convolution Backward: Fusion of compute with activation is unsupported for given activation function
BNNS Convolution Backward: failed to allocate memory
BNNS Convolution Backward: failed to apply activation backward
BNNS Conv: Unsupported weight format for Input delta compute
BNNS Convolution Apply Backward: could not create transposed convolution filter
BNNS Convolution Apply Backward: Convolution input delta computation failed
BNNS Transposed Convlution Apply: failed to allocate weights buffer
BNNS Transposed Convlution Apply: failed to allocate backward weights buffer
BNNS Transposed Convlution Apply: failed to allocate out delta buffer
BNNS Transposed Convlution Apply: failed to manipulate output delta
BNNS Transposed Convlution Apply: could not create convolution filter
BNNS Convolution Apply Backward: allocation failed
BNNS Convolution Apply Backward: Transposed Convolution Backward Compute failed
transposed convolution parameter error
transposed convolution backward failed
Convolution bias delta failed
BNNS Create Layer Convolution: fused padd convolution allocation failed
BNNS Create Layer Convolution: padding layer creation failed
BNNS Create Layer Convolution: padding descriptor creation failed
BNNS Create Layer Convolution: input channels must be divisible by groups
BNNS Create Layer Convolution: output channels must be divisible by groups
BNNS Create Layer Convolution: bias must match output channels
BNNS Create Layer Convolution: grouped convolution weight layout must be BNNSDataLayoutConvolutionWeightsOIHW
BNNS Create Layer Convolution: failed to allocate memory to copy weights and bias
BNNS Create Layer Convolution: failed to create forward convolution filter
BNNS Create Layer Convolution: failed to create wrapper filter
BNNS Convolution Create: incompatible numbers of channels between images and convolution parameters
BNNS Convolution Create: unsupported weight format
BNNS Convolution Create: input must be a 3D array
BNNS Convolution Create: input descriptor is illegal
BNNS Conv: output must be a 3D array
BNNS Convolution Create: output descriptor is illegal
BNNS Convolution Create:  weights descriptor is illegal
BNNS Convolution Create: not allowed to delay allocation to apply if weights ptr isn't maintained by Client
BNNS Convolution Create: failed to create dilated convolution
BNNS Convolution Create: failed to create packed weights convolution
BNNS Convolution Create: failed to create grouped convolution
BNNS Convolution Create: failed to allocate weights buffer
BNNS Convolution Create: failed to allocate bias buffer
BNNS Convolution Create: input data type is not supported
BNNS Convolution Create: output data type is not supported
BNNS Convolution Create: weight data type is not supported
BNNS Convolution Create: int8/uint8 output supported only with int8/uint8 inputs and weights
BNNS Convolution Create: int16/uint16 output is not supported
BNNS Convolution Create: uint32 output is not supported
BNNS Convolution Create: convolution doesn't support indexed weights
BNNS Convolution Create: failed to allocate memory
BNNS Convolution Create: failed to allocate weights descriptor
BNNS Convolution Create: convert weights
BNNS Convolution Create: failed to prepare blocks
BNNS Convolution Create: failed to create compute blocks
BNNS Grouped Convolution apply: groups should not be used if groups <= 1
BNNS Grouped Convolution apply: descriptor check failed
BNNS Grouped Convolution apply: weights are null 
BNNS Grouped Convolution apply: failed to create context
BNNS Grouped Convolution apply: unsupported convolution filter type 
BNNS Convolution Apply Backward:  filter is NULL
BNNS Convolution Apply Backward: out_delta is NULL
BNNS Convolution weight delta: malloc failed
BNNS Convolutions Apply: unexpected output data type
BNNS Convolution Backward: out_delta descriptor memory layout is not contiguous
BNNS Convolution bias delta: upconverting tensors failed
BNNS Convolution: malloc activation_grad failed
BNNS Grouped Convolution: input descriptor does not represent contiguous memory
BNNS Grouped Convolution: in_stride [%zu] and input memory size [%zu] do not match
BNNS Grouped Convolution: output descriptor does not represent contiguous memory
BNNS Grouped Convolution: out_stride [%zu] and output memory size [%zu] do not match
BNNS Grouped Convolution: weight descriptor does not represent contiguous memory
BNNS Grouped Convolution: bias descriptor does not represent contiguous memory
Convolution backward: in_delta, weights_delta, bias_delta descriptor pointers are all NULL
BNNS Convolution Backward: in_delta descriptor memory layout is not contiguous
BNNS Convolution Backward: weights_delta descriptor memory layout is not contiguous
BNNS Convolution Backward: bias_delta descriptor memory layout is not contiguous
BNNS Convolution Backward: out_stride and out_delta_stride do not match
BNNS Convolution Backward: out_delta_stride [%zu] does not match out_delta descriptor memory size [%zu]
BNNS Convolution Backward: in_stride and in_delta_stride do not match
BNNS Convolution Backward: in_delta_stride [%zu] does not match in_delta descriptor memory size [%zu]
Downsampling not supported for dimension > 2.
BNNS Resize: downsampling has no support for 3 or more dimensions.
BNNS Resize: downsampling has no support for 3 or higher dimension.
BNNS Resize: input_delta has %zu dimensions but must match input that has %zu
BNNS Resize: output_delta has %zu dimensions but must match output that has %zu
BNNS Resize: input_delta size[%zu]=%zu but must match input size[%zu]=%zu
BNNS Resize: output_delta size[%zu]=%zu but must match output size[%zu]=%zu
BNNS Resize: Unsupported data type
BNNS Resize: Input and output have different number of dimensions
BNNS Resize: resize must be in same direction for all direction (request %d downsample and %d upsample)
BNNS Resize: Linear interpolation requires resize in at most two dimensions, but %d dimensions are resized
BNNS Resize: Unsupported interpolation method %d
cBuGhnt
"%)-
INTERNAL: bwd
m[mmm^
D_G___J
 ',4>
!,=Qr
*1<ERb
-?Qcu
HT\h
"'000,
?fff
????>
00000J____^
jjjjj
+@+++C
hekeeen
bVeVVVh
359;>AFHKNSVZ^
"%)-
248:=@EGJMRUY]
!'-CHNTpv}
"%)-
"$'),/358;?BFJO
@EJY^dj
WZ_bglsv{
UX]`ejqty~
"',37=C
(3:EP]hw
"',37=C
!rr{>
!rr{>
}76R
YVpA
YVpA
YVpA
<7p7M}
<7p7M}
<7p7M}
:I~D
:I~D
:I~D
:I~D
*?9s
?G/d?
)>e\
?#.'.

%s: 
BNNS Dropout: Error layer_params is NULL
BNNS Dropout: rate must be in the range [0.0, 1.0].
BNNS Dropout: Unsupported input data type
BNNS Dropout: Input and Output data types must be the same
BNNS Dropout: Error memory allocation failed
BNNS Dropout: Error input descriptor is invalid
BNNS Dropout: Error output descriptor is invalid
BNNS Dropout: Input and output descriptors must have the same shape.
BNNS Dropout: Tensors with dimension greater than %u are not supported.
BNNS Dropout: Error filter is NULL
BNNS Dropout: Error wrong filter type, filter is not Dropout
BNNS Dropout: input pointer is NULL
BNNS Dropout: batch_size>1 and in_stride is 0
BNNS Dropout: output pointer is NULL
BNNS Dropout: batch_size>1 and out_stride is 0
BNNS BatchNorm: Error filter is NULL
BNNS Dropout: input_delta layout, shape and stride must match layer_params->i_desc
BNNS Dropout: output_delta layout, shape and stride must match layer_params->o_desc
Unsupported target %d
v16@?0Q8
v24@?0Q8^v16
Shouldn't call get_data_size with Indexded2 or Indexed4, switch to get_data_bits
BNNS Tensor Contraction: inputB pointer must not be NULL unless B is a weight or the operation is quadratic.
BNNS Tensor Contraction: After adding batch dimension, inputA tensor has too many indices
BNNS Tensor Contraction: After adding batch dimension, inputB tensor has too many indices
BNNS Tensor Contraction: After adding batch dimension, output tensor has too many indices
BNNS Tensor Contraction: inB_delta calculation is non-sensical for a contraction of a tensor with itself.
BNNS Tensor Contraction: inA_delta calculation requires inA to be non-NULL
BNNS Tensor Contraction: inA_delta calculation requires out_delta (and out_delta->data) to be non-NULL
inA_delta
BNNS Tensor Contraction: After adding batch dimension, out_delta tensor has too many indices
BNNS Tensor Contraction: After adding batch dimension, inputA_delta tensor has too many indices
BNNS Tensor Contraction: inA_delta calculation requires inB to be non-NULL
BNNS Tensor Contraction: inB_delta calculation requires inA to be non-NULL
BNNS Tensor Contraction: inB_delta calculation requires out_delta (and out_delta->data) to be non-NULL
inB_delta
BNNS Tensor Contraction: After adding batch dimension, inputB_delta tensor has too many indices
BNNS Tensor Contraction: beta must be 0.0 or 1.0 (beta=%.2f)
BNNS Tensor Contraction: inputA descriptor is illegal "%s"
BNNS Tensor Contraction: ouput descriptor is illegal "%s"
BNNS Tensor Contraction: invalid op string "%s"
BNNS Tensor Contraction: inputB descriptor is illegal "%s"
BNNS Tensor Contraction: both inputA and inputB cannot be weights "%s"
BNNS Tensor Contraction: Wildcard index '*' must appear on both sides of operation or neither: "%s"
BNNS Tensor Contraction: Wildcard index '*' must appear at most once per index set: "%s"
BNNS Tensor Contraction: Wildcard index '*' must appear consistently only at start or end of index sets: "%s"
BNNS Tensor Contraction: number of indices from operation (%td) does not match dimension of input A descriptor (%zu) "%s"
BNNS Tensor Contraction: number of indices from operation (%td) does not match dimension of input B descriptor (%zu) "%s"
BNNS Tensor Contraction: number of indices from operation (%td) does not match dimension of output descriptor (%zu) "%s"
BNNS Tensor Contraction: '%c', index %zu of inputA has size %zu and index %td of output has size %zu, but sizes are required to match
BNNS Tensor Contraction: '%c', index %d of inputA has size %zu and index %td of inputB has size %zu, but sizes are required to match
BNNS Tensor Contraction: index '%c' of input A does not match any index of inputB or output "%s"
BNNS Tensor Contraction: '%c', index %d of inputB has size %zu and index %td of output has size %zu, but sizes are required to match
BNNS Tensor Contraction: index '%c' of input B does not match any index of inputA or output "%s"
BNNS Tensor Contraction: index '%c' of output does not match any index of inputA or inputB "%s"
Data type combination not supported
%c_*ki, %c_*jk -> c_*ij
%c_*ki, %c_*kj -> c_*ij
%c_*ik, %c_*jk -> c_*ij
%c_*ik, %c_*kj -> c_*ij
BNNS Tensor Contraction: %s shape does not match inputA shape in dimension %ld (%zu vs %zu)
BNNS Tensor Contraction: out_delta shape does not match output shape in dimension %ld (%zu vs %zu)
BNNS Tensor Contraction: Wildcard index '*' can only appear as first or last index of "%s_%s"
BNNS Tensor Contraction: repeated indices "%s_%s"
v16@?0^v8
v24@?0Q8Q16
BNNS SoftMax: sum result shouldn't be 0, something is wrong
BNNS SoftMax BF16: sum result shouldn't be 0, something is wrong
malloc activation_grad failed
apply_specialized_convolution_PKT_0: o_height 1 not supported
apply_specialized_convolution_PKT_1: o_height less then 4 isn't supported
apply_specialized_convolution_PKT_2: o_height 1 not supported
apply_specialized_convolution_PKT_3: o_height 1 not supported
apply_specialized_convolution_PKT_3: o_height 2 not supported
BNNS Copy Filter: in and out must not be NULL.
BNNS Copysum Filter: unsupported data type conversion
internal bwd copysum
BNNS Tensor Contraction: Input must not be NULL
BNNS Tensor Contraction: Output must not be NULL
BNNS Tensor Contraction: index '%c' appears multiple times on right-hand side, but with different sizes
BNNS Tensor Contraction: '%c', index %d of input has size %zu and index %td of output has size %zu, but sizes are required to match
BNNS Tensor Contraction: index '%c' appears multiple times on left-hand side, but with different sizes
BNNS Tensor Contraction: Unsupported data type conversion %sfrom %s to %s
(in summation) 
(in copy) 
BNNSTranspose: src and dest have a different number of dimensions.
BNNSTranspose: specified axes must be within range for supplied tensor (axes: %zu, %zu) dimension of tensor is %zu
ijklmnpq
internal_transpose
internal_copy
int1
int2
int4
int8
int16
int32
int64
uint1
uint2
uint4
uint8
uint16
uint32
uint64
bf16
fp16
fp32
indexed8
indexed4
indexed2
indexed1
bool
unknown
BNNS CONVOLUTIONS VERSION2: Error minimum float w_pack weight packing value is 32
BNNS Dequantize: input and output can't be null
BNNS Dequantize: in place conversion not supported
BNNS Dequantize: only __fp16 output is supported
BNNS Dequantize: lut is null and needed for Indexed input type
BNNS Dequantize: input type not supported
BNNS Dequantize: shouldn't have reached fp16 convert path
BNNS Layer Norm Apply: failed to allocate scratch memory
BNNS Batchnorm Apply Backward: activation allocation failed
BNNS Batchnorm Apply Backward: failed to apply activation backward
BNNS Instance Norm Apply: input stride (%zu) and output stride (%zu) aren't set correctly
BNNS Instance Norm Apply: failed to allocate scratch memory
BNNS Instance Norm Apply Backward: activation allocation failed
BNNS Instance Norm Apply Backward: failed to apply activation backward
BNNS Layer Norm Apply: input stride (%zu) and output stride (%zu) aren't set correctly
BNNS Layer Norm Apply Backward: activation allocation failed
BNNS Layer Norm Apply Backward: failed to apply activation backward
BNNS Group Norm Apply: failed to allocate scratch memory
BNNS Group Norm Apply Backward: activation allocation failed
BNNS Group Norm Apply Backward: failed to apply activation backward
BNNS Fully Connected Tiny Apply: unexpecetd weights layout
BNNS Fully Connected Apply: allocation of work buffer failed
BNNS Fully Connected: Unexpected result, should be at least 1
BNNS Fully Connected: Unexpected result, at least 1 batch should remain
BNNS Fully Connected: Unexpected result, at least 1 output should remain
BNNS Fully Connected: Failed to fit in memory limit, using generic code
BNNS FUlly: allocation failed
BNNS Fully Connected: weights conversion buffer isn't allocated.
BNNS Fully Connected: inputs conversion buffer isn't allocated.
BNNS Fully Connected Backward: output delta is NULL
BNNS Fully Connected Backward: only float32 or bfloat16 output delta are supported
BNNS Fully Connected Backward: input delta only support float32 or bfloat16
BNNS Fully Connected Backward: weights delta only support float32 or bfloat16
BNNS Fully Connected Backward: bias delta only support float32 or bfloat16
BNNS Fully Connected Backward: output delta BNNSDataLayoutImageCHW isn't supported
BNNS Fully Connected Backward: failed to allocate output delta scratch buffer
BNNS Fully Connected Backward: apply activation backward failed
BNNS Fully Connected Backward: input delta BNNSDataLayoutImageCHW isn't supported
BNNS Fully Connected Backward: failed to allocate input delta scratch buffer
BNNS Fully Connected Backward: failed to compute input delta
BNNS Fully Connected Backward: user requested weights delta but didn't supply original inputs
BNNS Fully Connected Backward: weights sizes doesn't match weights delta sizes
BNNS Fully Connected Backward: unsupported weights delta layout
BNNS Fully Connected Backward: no bias, bias delta ignored
BNNS Fully Connected: Weight type not supported
BNNS Fully Connected Create: failed to allocated memory (%zu bytes))
BNNS_FULLY: input doesn't support Indexed data type
BNNS_FULLY: output only support fp32 data type
BNNS Fully Connected: using generic code to convert data 1 element each time
BNNS Fully Connected: output data type not supported for conversion
BNNS Fully Connected: input must be a 1D array
BNNS Fully Connected: input descriptor is illegal
BNNS Fully Connected: output must be a 1D array
BNNS Fully Connected: output descriptor is illegal
BNNS Fully Connected: weights row size should match input vector size
BNNS Fully Connected: weights number of rows should match output vector size
BNNS Fully Connected: weights must be a 2D array
BNNS Fully Connected: weights descriptor is illegal
BNNS Fully Connected: Pointer to weight data must be non-NULL
BNNS Fully Connected: Output data types supported in this version: Float32, Float16, BFloat16
BNNS Fully Connected: Bias data types supported in this version: Float32
BNNS Fully Connected: Float16 output isn't supported with bias or activation function
BNNS Fully Connected: Input/Weight data types supported in this version: Float32, Float16, BFloat16, Int8, Int16
BNNS Fully Connected: Data type of weights and input should match
BNNS Fully Connected: size computation wraparound -I %zu -O %zu
BNNS Fully Connected: bias size computation wraparound -O %zu bias data type %x
BNNS InTopK: invalid axis value %zu (input tensor has %zu dimensions)
BNNS InTopK: Unsupported input data type for this operation (supported data types are: fp32)
BNNS TopK: Unsupported best_indices data type for this operation (supported data types are: int32)
BNNS InTopK: input tensor with axis removed is not congruent with test_indices tensor
BNNS InTopK: input tensor with axis removed is not congruent with output tensor
BNNS TopK: input descriptor is illegal
BNNS TopK: best_values descriptor is illegal
BNNS TopK: best_indices descriptor is illegal
BNNS TopK: Unsupported input data type for this operation (supported data types are: fp32)
BNNS TopK: Unsupported best_values data type for this operation (supported data types are: fp32)
BNNS TopK: invalid axis value %zu (input tensor has %zu dimensions)
BNNS TopK: best_indices tensor is not consistent with input tensor
BNNS TopK: best_values tensor is not consistent with input tensor
BNNS TopK: best_indices tensor size[%zu]=%zu should be K=%zu
BNNS TopK: best_values tensor size[%zu]=%zu should be K=%zu
malloc
BNNS unsupported sgd variant
BNNS Optimizer Adam Apply: Adam beta1 and beta2 must be positive
BNNS Optimizer Adam Apply: Adam beta1 and beta2 must be strictly less than 1
BNNS Optimizer RMSProp Apply: RMSProp alpha must be in (0,1)
BNNS Optimizer Apply: Error unsupported optimizer function
BNNS Optimizer Apply: Error OptimizerAlgFields is NULL
BNNS Optimizer Apply: parameter array pointer is NULL
BNNS Optimizer Apply: gradient array pointer is NULL
BNNS Optimizer Apply: gradient pointer number %zu is NULL
BNNS Optimizer Apply: Error in parameter %zu: parameter descriptor must be contiguous such that stride[0]=1 and for every N>0 stride[N]=stride[N-1]*size[N-1]
BNNS Optimizer Apply: accumulator array pointer is NULL
BNNS Optimizer Apply: parameter pointer number %zu is NULL
BNNS Optimizer Apply: accumulator pointer number %zu is NULL
BNNS Optimizer Apply: Error in parameter %zu: Parameter and Gradient descriptors must have the same sizes and strides
BNNS Optimizer Apply: Error in parameter %zu: Parameter and Accumulator descriptors must have the same sizes and strides
BNNS Optimizer Apply: Error in Parameter %zu: Parameter data type is not Float32 / Float16 / Bfloat16
BNNS Optimizer Apply: Error in Gradient %zu: Gradient data type is not Float32 / Float16 / Bfloat16
BNNS Optimizer Apply: Error in Accumulator %zu: Accumulator data type is not Float32 / Float16 / BFloat16
BNNS Optimizer Apply: Error time_step is not valid for Adam optimizer. minimal time step value is 1
BNNS Optimizer Apply: Adam optimizer require accumulators pointer to be valid
BNNS Optimizer Apply: accumulator1 pointer number %zu is NULL
BNNS Optimizer Apply: accumulator2 pointer number %zu is NULL
BNNS Optimizer Apply: accumulator3 pointer number %zu is NULL
BNNS Optimizer Apply: Error in parameter %zu: Parameter and Accumulator1 descriptors must have the same sizes and strides
BNNS Optimizer Apply: Error in parameter %zu: Parameter and Accumulator2 descriptors must have the same sizes and strides
BNNS Optimizer Apply: Error in parameter %zu: Parameter and Accumulator3 descriptors must have the same sizes and strides
BNNS Optimizer Apply: Error in parameter %zu: Parameter data type is not BNNSDataTypeFloat32
BNNS Optimizer Apply: Error in gradient %zu: Gradient data type is not BNNSDataTypeFloat32
BNNS Optimizer Apply: Error in accumulator1 %zu: Accumulator data type is not BNNSDataTypeFloat32
BNNS Optimizer Apply: Error in accumulator2 %zu: Accumulator data type is not BNNSDataTypeFloat32
BNNS Optimizer Apply: Error in accumulator3 %zu: Accumulator data type is not BNNSDataTypeFloat32
BNNS Optimizer Apply: RMSProp optimizer require accumulators pointer to be valid
BNNS Optimizer Apply: accumulator_n pointer number %zu is NULL
BNNS Optimizer Apply: accumulator_g pointer number %zu is NULL
BNNS Optimizer Apply: momentum pointer number %zu is NULL
BNNS Optimizer Apply: Error in accumulator_n %zu: Accumulator data type is not BNNSDataTypeFloat32
BNNS Optimizer Apply: Error in accumulator_g %zu: Accumulator data type is not BNNSDataTypeFloat32
BNNS Optimizer Apply: Error in momentum %zu: Accumulator data type is not BNNSDataTypeFloat32
BNNS Optimizer: unsupported optimizer function
BNNS ClipByValue: Error dimensions of input tensor (%zu) and output tensor (%zu) must match
BNNS ClipByValue: Error only supports fp32
BNNS ClipByValue: Error sizes and strides of input tensor and output tensor must match
BNNS ClipByNorm: Error dimensions of input tensor (%zu) and output tensor (%zu) must match
BNNS ClipByNorm: Error only supports fp32
BNNS ClipByNorm: Error sizes and strides of input tensor and output tensor must match
BNNS ClipByGlobalNorm: src[%zu] and dest[%zu] have a different number of dimensions.
BNNS ClipByGlobalNorm: src[%zu] and dest[%zu] must contain fp32 data.
BNNS ClipByGlobalNorm: src[%zu] and dest[%zu] have different sizes or strides.
BNNS ComputeNorm: Error only supports BNNSL2Norm
BNNS ComputeNorm: Error only supports fp32
BNNS ComputeNorm: Error non-axis dimensions of input tensor (%zu) and output tensor (%zu) must match
BNNS ComputeNorm: Error non-axis sizes of input tensor and output tensor must match
BNNS ComputeNorm Backward: Error only supports BNNSL2Norm
BNNS ComputeNorm Backward: Error only supports fp32
BNNS ComputeNorm Backward: Error non-axis dimensions of input tensor (%zu) and output tensor (%zu) must match
BNNS ComputeNorm Backward: Error non-axis sizes of input tensor and output tensor must match
optimizer_private_bnns_init_NDArray: Unsupported layout
BNNS Sparse Fully Connected: input must be a 1D array
BNNS Sparse Fully Connected: input descriptor is illegal
BNNS Sparse Fully Connected: output must be a 1D array
BNNS Sparse Fully Connected: output descriptor is illegal
BNNS Sparse Fully Connected: invalid matrix layout
BNNS Sparse Fully Connected: invalid block row count: %u
BNNS Sparse Fully Connected: invalid block col count: %u
BNNS Sparse Fully Connected: input data types supported in this version: Float16
BNNS Sparse Fully Connected: output data types supported in this version: Float32, Float16
BNNS Sparse Fully Connected: weights data types supported in this version: Float16
BNNS Sparse Fully Connected: unsupported matrix block size %u x %u
BNNS Sparse Fully Connected: Allocation of the setup structure failed
BNNS Convolution: malloc failed
BNNS Compare: in0 is NULL
BNNS Compare: in1 is NULL
BNNS Compare: out is NULL
BNNS Compare: Unsupported I/O tensor data types.
BNNS Compare: Mismatch in input tensor data types.
BNNS Compare: Invalid operation %d for data type %s.
BNNS Compare: Broadcast failed.
BNNS Compare: I/O tensor dimension %lu mismatch.
BNNS Compare: Partially overlapping input and output tensors are not supported.
BNNS Compare: Unsupported I/O tensor layout.
BNNS Compare: I/O tensor layout mismatch.
BNNS Compare: input size[%zu]=%zu does not equal output size of %zu or 1
BNNS Compare: Unsupported operator.
BNNS Fully Connected Choose: inputs, weights, outputs and bias must be contigeous in memory (stride[0] <= 1)
BNNS Fully Connected Apply: failed to allocated scratch memory
BNNS Fully Connected Init: missing data table for indexed data type
BNNS Fully Connected Apply: Ilegal compute block
BNNS Fully Connected Create: failed to create layer context
BNNS Fully Connected Create: allocation failed
BNNS Fully Connected Direct Apply: input pointer is NULL
BNNS Fully Connected Direct Apply: output pointer is NULL
BNNS Fully Connected Direct Apply: failed to create layer context
BNNS Fully Connected Init: failed to create context
BNNS Fully Connected Init: input descriptor CHW layout isn't set properly
BNNS Fully Connected Init: input descriptor isn't set properly
BNNS Fully Connected Init: input descriptor failed validation
BNNS Fully Connected Init: output descriptor CHW layout isn't set properly
BNNS Fully Connected Init: output descriptor isn't set properly
BNNS Fully Connected Init: output descriptor failed validation
BNNS Fully Connected Init: weights descriptor row major size[0] doesn't match input vector size
BNNS Fully Connected Init: weights descriptor row major size[1] doesn't match output vector size
BNNS Fully Connected Init: weights descriptor column major size[0] doesn't match output vector size
BNNS Fully Connected Init: weights descriptor column major size[1] doesn't match input vector size
BNNS Fully Connected Init: weights descriptor isn't set properly
BNNS Fully Connected Init: weights descriptor failed validation
BNNS Fully Connected Init BNNSNDArrayDescriptor: stride[%zu] is too small
BNNS Fully Connected Apply: failed to apply filter
BNNS Random Generator Create: Unsupported method %u.
BNNS Random Generator Create: Failed to allocate memory
BNNS Random Fill Uniform Float: Invalid descriptor
BNNS Random Fill Uniform Float: Range (%g, %g) is empty
BNNS Random Fill Uniform Float: Range bounds are not exactly representable as bf16
BNNS Random Fill Uniform Float: Range bounds are not exactly representable as fp16
BNNS Random Fill Uniform Float: Unsupported data type
BNNS Random Fill Uniform Integer: Invalid descriptor
BNNS Random Fill Uniform Integer:%s range (%d, %d) is empty
 (clipped)
BNNS Random Fill Uniform Integer: range (%lld, %lld) is empty
BNNS Random Fill Uniform Integer:%s range (%llu, %lld) is empty
BNNS Random Fill Uniform Integer: Unsupported data type
BNNS Random: CCCryptorCreateWithMode() failed
BNNS Random Fill Uniform: Invalid generator
Failed to create random data
BNNS Loss: Error unsupported loss function
BNNS Loss: Error Input width is 0
BNNS Loss: Error Input must be BNNSDataTypeFloat32 type
BNNS Loss Error: Input must be contiguous. stride0 must be 1 or 0
BNNS Loss Error: Input tensors must be contiguous.
BNNS Loss: Error unknown reduction function
BNNS Loss: Error output data type must be BNNSDataTypeFloat32
BNNS Loss: Error output size>1 is only allowed with reduction BNNSLossReductionNone.
BNNS Yolo loss: Error out width (size[0]) must be 1 as yolo loss is always reduced
BNNS Loss: Error anchors is NULL
BNNS Loss: Error number of grid rows is 0
BNNS Loss: Error number of grid columns is 0
BNNS Loss: Error number of anchor boxes is 0
BNNS Loss: Error anchor box size <= 5
BNNS Loss: Error object minimum iou is negative
BNNS Loss: Error no object maximum iou is negative
BNNS Loss: Error input descriptor width (%zu) different from expected width of grid_size*num_anchors*(5+num_+classes) (%zu)
BNNS Loss: memory allocation failed
BNNS Loss: Error filter is NULL
BNNS Loss: Error batch_size is 0
BNNS Loss: Error in is NULL
BNNS Loss Error: in_stride is smaller than i_desc.size[0] or Tensor size
BNNS Loss: Error labels is NULL
BNNS Loss Error: labels_stride is smaller than i_desc.size[0] or Tensor size
BNNS Loss: Error weights_size>0 but weights pointer is NULL
BNNS Loss Warning: weight_size==0 but weight pointer is not NULL. Weight pointer is ignored.
BNNS Loss Error: weight_size value must be 0,1 batch_size, or batch_size*input_width for softmax/sigmoid/categorical cross entropy, mse, mae, log, hinge, and huber
BNNS Loss: Error yolo weight_size value must be 0, use yolo specific weight factors during filter create
BNNS Loss: Error out is NULL
BNNS Loss: Error in_delta descriptor is ilegal
BNNS Loss: Error in_delta must be contiguous, such that stride[0] is 0 or 1
BNNS Yolo loss: Error weight size must be 0
BNNS Loss: unsupported loss function
BNNS Loss Backward: Error filter is NULL
BNNS Loss Backward: Error batch_size is 0
BNNS Loss Backward: Error in is NULL
BNNS Loss Backward: Error in_stride is smaller than i_desc.size[0]
BNNS Loss Backward: Error labels is NULL
BNNS Loss Backward: Error labels_stride is smaller than i_desc.size[0]
BNNS Loss Backward: Error weights_size>0 but weights pointer is NULL
BNNS Loss Backward Error: weight_size value must be 0,1 batch_size, or batch_size*input_width for softmax/sigmoid/categorical cross entropy, mse, mae, log, hinge, and huber
BNNS Loss Backward: Error in_delta descriptor is illegal
BNNS Loss Backward: Error in_delta must be contiguous, such that stride[0] is 0 or 1
BNNS Loss Backward: Error in_delta is NULL
BNNS Loss Backward: Error out_delta descriptor is illegal
BNNS Loss Backward: Error out_delta must be contiguous, such that stride[0] is 0 or 1
BNNS Loss Backward: Error out_delta is NULL
BNNS Loss: unsupported loss backward function
BNNS softmax cross entropy loss error: in pointer is NULL
BNNS softmax cross entropy loss error: in stride cannot be smaller than input width
BNNS softmax cross entropy loss: Error out pointer is NULL
BNNS softmax cross entropy loss: malloc failed
BNNS softmax cross entropy loss: Error weight size must be equal 0,1, batch size, or batch_size*tensor_size 
BNNS softmax cross entropy loss backward : alloc failed
BNNS Loss Warning: reduction BNNSLossReductionWeightedMean sum of weights is zero
BNNS Loss: Error reduction BNNSLossReductionNonZeroWeightMean all weights are zero
BNNS sigmoid cross entropy loss error: in pointer is NULL
BNNS sigmoid cross entropy loss error: in stride cannot be smaller than input width
BNNS sigmoid cross entropy loss: Error out pointer is NULL
BNNS sigmoid cross entropy loss error: must be 0,1 or batch_size*input_width
BNNS sigmoid cross entropy loss: malloc failed
BNNS MSE loss error: in stride cannot be smaller than input width
BNNS MSE loss: malloc failed
BNNS MSE loss error: must be 0,1 or batch_size*input_width
BNNS Huber loss error: in pointer is NULL
BNNS Huber loss error: in stride cannot be smaller than input width
BNNS Huber loss: Error out pointer is NULL
BNNS Huber loss error: must be 0,1 or batch_size*input_width
BNNS Huber loss: malloc failed
BNNS Yolo loss: Error filter is NULL
BNNS Yolo loss: Error in pointer is NULL
BNNS Yolo loss: Error input stride is smaller than input width
BNNS Yolo loss: Error labels (ground truth) is NULL
BNNS Yolo loss: Error ground truth labels stride is smaller than input width
BNNS Yolo loss: Error input delta stride is smaller than input width
BNNS Yolo loss: Error out pointer is NULL
BNNS Yolo loss: Error yolo redution type must be BNNSLossReductionSum
BNNS Yolo loss: Error input descriptor width does not match grid_rows*grid_columns*anchors*(5+classes)
BNNS Yolo loss: alloc failed
BNNS log loss error: in pointer is NULL
BNNS log loss error: in stride cannot be smaller than input width
BNNS log loss: Error out pointer is NULL
BNNS log loss error: must be 0,1 or batch_size*input_width
BNNS cosine distance loss error: in pointer is NULL
BNNS cosine distance loss error: in stride cannot be smaller than input width
BNNS cosine distance loss: Error out pointer is NULL
BNNS cosine distance loss: Error weight size must be equal to 0, 1, batch_size
BNNS Loss: Error in_delta descriptor is illegal
BNNS Hinge loss error: in pointer is NULL
BNNS Hinge loss error: in stride cannot be smaller than input width
BNNS Hinge loss: Error out pointer is NULL
BNNS Hinge loss error: must be 0,1 or batch_size*input_width
BNNS MAE loss error: in pointer is NULL
BNNS MAE loss error: in stride cannot be smaller than input width
BNNS MAE loss: Error out pointer is NULL
BNNS MAE loss error: must be 0,1 or batch_size*input_width
BNNS categorical cross entropy loss error: in pointer is NULL
BNNS categorical cross entropy loss error: in stride cannot be smaller than input width
BNNS categorical cross entropy loss: Error out pointer is NULL
BNNS categorical cross entropy loss error: must be 0,1 or batch_size*input_width
BNNS softmax cross entropy loss backward error: in stride cannot be smaller than input width
BNNS softmax cross entropy loss backward error: out_delta.size[0] is not 1
BNNS softmax cross entropy loss backward : malloc failed
BNNS softmax cross entropy loss: Error weight size must be equal 0,1 or batch size
BNNS sigmoid cross entropy loss backward error: in stride cannot be smaller than input width
BNNS sigmoid cross entropy loss backward error: must be 0,1 or batch_size*input_width
BNNS MSE loss backward error: must be 0,1 or batch_size*input_width
BNNS Log loss backward error: in stride cannot be smaller than input width
BNNS Log loss backward error: must be 0,1 or batch_size*input_width
BNNS cosine distance loss backward error: in stride cannot be smaller than input width
BNNS cosine distance loss backward error: must be 0,1 or batch_size
BNNS Hinge loss backward error: in stride cannot be smaller than input width
BNNS Hinge loss backward error: must be 0,1 or batch_size*input_width
BNNS MAE loss backward error: in stride cannot be smaller than input width
BNNS MAE loss backward error: must be 0,1 or batch_size*input_width
BNNS Categorical CE loss backward error: in stride cannot be smaller than input width
BNNS Categorical CE loss backward error: must be 0,1 or batch_size*input_width
BNNS Create Depthwise Convolution: incompatible numbers of channels between images and convolution parameters
BNNS Create Depthwise Convolution: unsupported weight format
BNNS Create Depthwise Convolution: input must be BNNSDataLayoutImageCHW layout 
BNNS Create Depthwise Convolution: input descriptor is illegal
BNNS Create Depthwise Convolution: output must be BNNSDataLayoutImageCHW layout 
BNNS Create Depthwise Convolution: output descriptor is illegal
BNNS Depthwise Convolution Create: weights descriptor is illegal
BNNS Depthwise Convolution Create: does not support asymmetric padding. left and right pad must match, up and down pad must match
BNNS Create Depthwise Convolution: only float32 is supported
BNNS Create Depthwise Convolution: channel multiplier mismatch
BNNS Create Depthwise Convolution: malloc failed
BNNS Create Depthwise Convolution: weights malloc failed 
invalid argument
BNNS DEPTHWISE CONV: input batch stride doesn't make sense (%zu < %zu x %zu)
BNNS DEPTHWISE CONV: output batch stride doesn't make sense (%zu < %zu x %zu)
BNNS LSTM Direct Apply: filter initialization failed
BNNS LSTM Direct Apply: training cache capacity isn't sufficent
BNNS LSTM Direct Apply: Dropout is only supported in the presence of a training cache
BNNS LSTM Direct Apply: failed to allocate scratch buffer
BNNS LSTM Direct Apply: failed to thread workspace buffer
BNNS LSTM Direct Apply Backward: BNNSNDArrayFlagBackpropAccumulate is only supported on layer_delta->input_descriptor.data_desc.
BNNS LSTM Direct Apply Backward: layer_params initialization failed
BNNS LSTM Direct Apply Backward: layer_delta initialization failed
BNNS LSTM APPLY BACKWARD: Use of dropout without training cache is not supported.
BNNS LSTM APPLY BACKWARD: forward pass intermediate results weren't cached, recomputing forward pass
BNNS LSTM Direct Apply Backward: failed to allocate training cache buffer
BNNS LSTM Direct Apply Backward: failed to compute forward intermediate results
BNNS LSTM Direct Apply Backward: training cache capacity isn't sufficent
BNNS LSTM Direct Apply Backward: failed to allocate scratch buffer
BNNS LSTM init: hidden_size must be greater than zero
BNNS LSTM init: input descriptor.data isn't set correctly (size don't match seq_len/batch_size/input_size)
BNNS LSTM init: input descriptor.data isn't set correctly (size don't match batch_size/seq_len/input_size)
BNNS LSTM init: input descriptor.data isn't set correctly (size don't match input_size/batch_size/seq_len)
BNNS LSTM init: input descriptor.hidden isn't set correctly (size don't match hidden_size/batch_size/num_directions/num_layers)
BNNS LSTM init: input descriptor.cell_state isn't set correctly (size don't match hidden_size/batch_size/num_directions/num_layers)
BNNS LSTM init: output descriptor.data isn't set correctly (size don't match seq_len/batch_size/hidden_size*num_directions)
BNNS LSTM init: output descriptor.data isn't set correctly (size don't match batch_size/seq_len/hidden_size*num_directions)
BNNS LSTM init: output descriptor.data isn't set correctly (size don't match hidden_size/num_directions/batch_size/seq_len)
BNNS LSTM init: output descriptor.hidden isn't set correctly (size don't match hidden_size/batch_size/num_directions/num_layers)
BNNS LSTM init: output descriptor.cell_state isn't set correctly (size don't match hidden_size/batch_size/num_directions/num_layers)
BNNS LSTM init: forget_gate isn't set correctly (size don't match hidden_size/batch_size/num_directions/num_layers)
BNNS LSTM init: input_gate isn't set correctly (size don't match hidden_size/batch_size/num_directions/num_layers)
BNNS LSTM init: candidate_gate isn't set correctly (size don't match hidden_size/batch_size/num_directions/num_layers)
BNNS LSTM init: output_gate isn't set correctly (size don't match hidden_size/batch_size/num_directions/num_layers)
BNNS LSTM: Data type not supported, fail
BNNS Convolution Create: Client needs to support weights ptr with low_mem
BNNS Convolution Create: malloc
BNNS Convolution Create: failed to create convolution context
BNNS Convolution Create: failed to divide work between convolution contexts to fit in memory
BNNS Convolution Create: failed to create single generic convolution
BNNS Convolution Apply: invalid argument
BNNS Convolution Apply: failed to allocate input auxiliary buffer
BNNS Convolution: Winograd weights memory doesn't fit in memory
BNNS Convolution: unable to create Winograd that fit in memory
BNNS Convolution: allocation of contexts failed
BNNS LOW MEM CONVOLUTION: failed to create winograd convolution
BNNS LOW MEM CONVOLUTION: malloc failed
BNNS LOW MEM CONVOLUTION: incompatible BNNS_low_memory_context id
BNNS LOW MEM CONVOLUTION: convolution failed
BNNS LOW MEM CONVOLUTION: input aux buffer wasn't allocated
BNNS LOW MEM CONVOLUTION:: context type not supported
BNNS Convolutions Apply: unexpected input data type
BNNS Convolution Create: apply convolution failed
BNNS Quantization Filter: layer_params is NULL
BNNS Quantization: input layout and output layout must match
BNNS Quantization: unsupported input/output layouts
BNNS Quantization: input descriptor error
BNNS Quantization: input descriptor data is NULL
BNNS Quantization: output descriptor error
BNNS Quantization: output descriptor data is NULL
BNNS Quantization: invalid quantizer function
BNNS Quantization: BNNSQuantize function supported for the following input descriptor data types: BNNSDataTypeFloat32, BNNSDataTypeFloat16, BNNSDataTypeBFloat16, BNNSDataTypeInt32
BNNS Quantization: BNNSQuantize function supported for the following output descriptor data types: BNNSDataTypeInt8, BNNSDataTypeUInt8, BNNSDataTypeInt16, BNNSDataTypeUInt16, BNNSDataTypeInt32, BNNSDataTypeUInt32
BNNS Quantization: BNNSDeQuantize function supported for the following input descriptor data types: BNNSDataTypeInt8, BNNSDataTypeUInt8, BNNSDataTypeInt16, BNNSDataTypeUInt16, BNNSDataTypeInt32, BNNSDataTypeUInt32
BNNS Quantization: BNNSDeQuantize function supported for the following output descriptor data types: BNNSDataTypeFloat32, BNNSDataTypeFloat16, BNNSDataTypeBFloat16, BNNSDataTypeInt32
BNNS Quantization: invalid axis_mask, number of mask set bits must be lower or equal to 1
BNNS Quantization: axis_mask bits are set beyond batch dimension
BNNS Quantization: Error dimensions of input tensor (%zu) and output tensor (%zu) must match
BNNS Quantization: Error shape of input tensor and output tensor must match
BNNS Quantization Filter: scale layout must be BNNSDataLayoutVector
BNNS Quantization: scale descriptor error
BNNS Quantization Filter: bias layout must be BNNSDataLayoutVector
BNNS Quantization: bias descriptor error
BNNS Quantization: bias vector size does not match axis mask, bias size[0]=%zu, expected %zu
BNNS Quantization: scale vector size does not match axis mask, scale size[0]=%zu, expected %zu
BNNS Fused Convolution and Normalization: Error convolution_layer_params is NULL
BNNS Fused Convolution and Quantization: Error quantization_layer_params is NULL
BNNS Fused Compute and Quantization: Error Convolution output layout and Quantization input layout must match
BNNS Fused Compute and Quantization: Error Convolution output data type and Quantization input data type must match
BNNS Fused Compute and Quantization: Error Convolution output descriptor sizes and Quantization input descriptor sizes must match
BNNS Fused Compute and Quantization: Error Convolution output descriptor strides and Quantization input descriptor strides must match
BNNS Fused Compute and Quantization: Error memory allocation failed
BNNS Fused Compute and Quantization create filter failed: compute filter type error
BNNS Fused Compute and Quantization create filter failed
BNNS Quantization filter failed
BNNS Fused Fully Connected and Quantization: Error fully_layer_params is NULL
BNNS Fused Fully Connected and Quantization: Error quantization_layer_params is NULL
BNNS Fused Compute and Quantization: Error Fully Connected output layout and Quantization input layout must match
BNNS Fused Compute and Quantization: Error Fully Connected output data type and Quantization input data type must match
BNNS Fused Compute and Quantization: Error Fully Connected output descriptor sizes and Quantization input descriptor sizes must match
BNNS Fused Compute and Quantization: Error Fully Connected output descriptor strides and Quantization input descriptor strides must match
BNNS Quantization: unsupported data type
BNNS Quantization: unsupported quantization input data type
BNNS Quantization: unsupported quantization output data type
BNNS Quantization: unsupported dequantization input data type
BNNS Quantization: unsupported dequantization output data type
BNNS Transposed Convolution Create: input data type isn't supported
BNNS Transposed Convolution Create: weight data isn't supported
BNNS Transposed Convolution Create: output data type isn't supported
BNNS Transposed Convolution Apply: Filter is NULL
BNNS Transposed Convolution Apply: failed to allocate memory to manipulate input
BNNS Transposed Convolution Apply: unknown weight layout
BNNS Transposed Convolution Create: layer_params is NULL
BNNS Transposed Convolution Create: groups aren't supported
BNNS Transposed Convolution Create: low-mem isn't supported
BNNS Transposed Convolution Create: packed weights aren't supported
BNNS Transposed Convolution Create: failed to create forward path
BNNS Transposed Convolution Create: failed to allocate wrapper memory
BNNS Transposed Convolution Create Forward: transposed convolution failed
BNNS Transposed Convolution Create: weights allocation failed
BNNS Transpsoed Vector Convolution Apply: activation gradient auxilary allocation failed
BNNS Transposed Convolution Create: malloc failed
BNNS Transposed Convolution: weights allocation failed
BNNS Transposed Convolution: bias allocation failed
BNNS Transposed Convolution Reorder Weights: NULL weight object
BNNS Transposed Convolution Reorder Weights: dst weight size too small
BNNS Transposed Convolution Reorder Weights: NULL weight array
BNNS Transposed Convolution Rotate Weights: NULL weight object
BNNS Transposed Convolution Rotate Weights: dst weight size too small
BNNS Transposed Convolution Rotate Weights: NULL weight array
BNNS Transposed Convolution Reorder and Rotate Weights: NULL weight object
BNNS Transposed Convolution Reorder and Rotate Weights: dst weight size too small
BNNS Transposed Convolution Reorder and Rotate Weights: NULL weight array
transposed convolution input manipulation failed
transposed convolution input manipulation failed - nothing to do, should not have allocated dst_input buffer
BNNS: unsupported activation gradient data type
BNNS Transposed Convlution Backward: dy padding failed
BNNS Transposed Convlution Backward: no padding is needed, dy padding failed
BNNS Create Convolution: fp32 weights allocation failed
BNNS Create Convolution: int16 weights allocation failed
BNNS Create Convolution: failed to allocate memory to copy the weights
BNNS Create Convolution: single descriptors allocation failed
BNNS Convolution Create: allocation failed
failed to upconvert or copy weights
failed to allocate none generic format
BNNS Convolution Create: forward pass check (swapping input and output)  - input size computation wraparound input (%zu x %zu), output (%zu x %zu), stride (%zu x %zu), asymmetric padding (%zu,%zu,%zu,%zu)
BNNS Convolution Create: forward pass check (swapping input and output)  - input size computation wraparound input (%zu x %zu), output (%zu x %zu), stride (%zu x %zu), padding (%zu x %zu)
BNNS Convolution Create: forward pass check (swapping input and output) - output(%zu x %zu x %zu) is larger than input(%zu x %zu x %zu) with padding(%zu x %zu).
 kernel (%zu x %zu) and stride (%zu x %zu)
BNNS Normalization: layer_params is NULL
BNNS Normalization: filter memory allocation failed
BNNS Fused Convolution and Normalization: Error normalization_layer_params is NULL
BNNS Fused Convolution and Normalization: Error convolution output descriptor and normalization input descriptor must have the same sizes and strides
BNNS Fused Convolution and Normalization: Error memory allocation failed
BNNS Fused Convolution and Normalization create filter failed: Convolution type error
BNNS Fused Convolution and Normalization create filter failed
BNNS Fused Fully Connected and Normalization: Error fully_layer_params is NULL
BNNS Fused Fully Connected and Normalization: Error batch_normalization_layer_params is NULL
BNNS Fused Fully Connected and Normalization: Error fully connected output descriptor and normalization input descriptor must have the same sizes and strides
BNNS Fused Fully Connected and Normalization: Error memory allocation failed
BNNS Fused Fully Connected and Normalization create filter failed
BNNS Fused Arithmetic and Normalization: Error arithmetic_layer_params is NULL
BNNS Fused Arithmetic and Normalization: Error normalization_layer_params is NULL
BNNS Fused Arithmetic and Normalization: unsupported arithmetic function
BNNS Fused Arithmetic and Normalization: Error arithmetic output descriptor and normalization input descriptor must have the same sizes and strides
BNNS Fused Arithmetic and Normalization: Error memory allocation failed
BNNS Fused Arithmetic and Normalization create filter failed
BNNS Normalization: input pointer is NULL
BNNS Normalization: batch_size>1 and in_stride is 0
BNNS Normalization: output pointer is NULL
BNNS Normalization: batch_size>1 and out_stride is 0
BNNS Normalization: batch_size too large. Backprop cache size limits batch_size <= %zu.
BNNS Normalization: x_hat allocation failed
BNNS Normalization: filter id not supported
BNNS Normalization: inverse variance allocation failed
BNNS Normalization Apply Backward: filter is NULL
BNNS Normalization Apply Backward: Normalization beta delta pointer is NULL. backward computation of all active parameters must be done in a single function call.
BNNS Normalization Apply Backward: Normalization gamma delta pointer is NULL. backward computation of all active parameters must be done in a single function call.
BNNS Normalization Apply Backward: make sure to run normalization forward with training flag enabled before running normalization backward
BNNS Normalization Apply Backward: cannot compute activation backward, output is NULL
BNNS Normalization Apply Backward: Fusion of normalization with activation is unsupported for given activation function
BNNS Normalization Apply Backward: cannot compute input delta because input delta data pointer is NULL
BNNS Normalization Apply Backward: Error normalization input delta and input must have the same sizes and strides
BNNS Normalization Apply Backward: in delta shuold be contiguous and same size as input size
BNNS Normalization Apply Backward: cannot compute beta delta because beta delta data pointer is NULL
BNNS Normalization Apply Backward: BNNSNDArrayFlagBackpropAccumulate is no supported for beta_delta.
BNNS Normalization Apply Backward: Error beta_delta descriptor must match the shape of input up to the normalizaton axis
BNNS Normalization Apply Backward: beta delta shuold be contiguous and same size as number of input channels
BNNS Normalization Apply Backward: cannot compute gamma delta because gamma delta data pointer is NULL
BNNS Normalization Apply Backward: BNNSNDArrayFlagBackpropAccumulate is no supported for gamma_delta.
BNNS Layer Normalization: Error gamma_delta descriptor must match the shape of input up to the normalizaton axis
BNNS Normalization Apply Backward: gamma delta shuold be contiguous and same size as number of input channels
BNNS Normalization Apply Backward: out delta data is NULL
BNNS Normalization Apply Backward: Error Normalization output delta and output must have the same sizes and strides
BNNS Normalization Apply Backward: out delta shuold be contiguous and same size as input size
BNNS Normalization Apply Backward: normalization type not supported
BNNS Normalization Set State: Backprop cache must be in BNNSDataLayoutVector
BNNS Normalization Set State: Backprop cache must be in BNNSDataTypeFloat32
BNNS Normalization Set State: Backprop cache must be in contiguous
BNNS Normalization Set State: Backprop cache pointer is NULL
BNNS Normalization Set State: Backprop cache too small, must be >= %zu fp32 to allow max_batch_size >= 1
BNNS Normalization: Moving Mean isn't set properly
BNNS Normalization: Moving Variance isn't set properly
BNNS Normalization Create: normalization type not supported
BNNS Normalization: input descriptor isn't set properly
BNNS Normalization: Input must be in BNNSDataLayoutImageCHW layout
BNNS Normalization: output descriptor isn't set properly
BNNS Normalization: Output must be in BNNSDataLayoutImageCHW layout
BNNS Normalization: Gamma descriptor isn't set properly
BNNS Normalization: Beta descriptor isn't set properly
BNNS Normalization: Error input descriptor must be contiguous such that stride[0]=1 and for every N>0 stride[N]=stride[N-1]*size[N-1]
BNNS Normalization: Error output descriptor must be contiguous such that stride[0]=1 and for every N>0 stride[N]=stride[N-1]*size[N-1]
BNNS Normalization: Error input size (%zu) different from output size (%zu)
BNNS Layer Normalization: Gamma descriptor must match the shape of input up to the normalizaton axis
BNNS Normalization: Beta descriptor must match the shape of input up to the normalizaton axis
BNNS Normalization: Gamma descriptor size[0] must be the same as number of input channels
BNNS Normalization: Beta descriptor size[0] must be the same as number of input channels
BNNS Normalization: Moving Mean descriptor size[0] must be the same as number of input channels
BNNS Normalization: Moving Variance descriptor size[0] must be the same as number of input channels
BNNS Normalization: Error momentum must be between 0 and 1
BNNS Normalization Warning: epsilon is zero, it may cause division by zero
BNNS Normalization Warning: momentum is zero
BNNS CONVOLUTIONS VERSION2: layer param is NULL
BNNS CONVOLUTIONS VERSION2: unsupported input data type
BNNS CONVOLUTIONS VERSION2: unsupported output data type
BNNS CONVOLUTIONS VERSION2: unsupported weight data type
BNNS CONVOLUTIONS VERSION2: unsupported bias data type
BNNS CONVOLUTIONS VERSION2: int/out/weight/bias descriptor element stride (stride[0]) must be 1
BNNS CONVOLUTIONS VERSION2: 2D conv doesn't fit in memory limit
BNNS CONVOLUTIONS VERSION2: 0 width/height/channel
BNNS CONVOLUTIONS VERSION2: unsupported activation
BNNS CONVOLUTIONS VERSION2: output bias or scale is not supported
BNNS CONVOLUTIONS VERSION2: malloc failed
BNNS CONVOLUTIONS VERSION2: malloc failed
BNNS CONVOLUTIONS VERSION2: failed to allocate bias conversion memory
BNNS CONVOLUTIONS VERSION2: failed to allocate scratch memory
BNNS CONVOLUTIONS VERSION2: invalid argument
BNNS CONVOLUTIONS VERSION2: input batch stride doesn't make sense (%zu < %zu x %zu)
BNNS CONVOLUTIONS VERSION2: output batch stride doesn't make sense (%zu < %zu x %zu)
BNNS CONVOLUTIONS VERSION2: unsupported weight packing
BNNS CONVOLUTIONS VERSION2: failed to allocated memory to convert bias
BNNS CONVOLUTIONS VERSION2: input padding memory allocation failed
BNNS CONVOLUTIONS VERSION2: output repack malloc failed
BNNS CONVOLUTIONS VERSION2: weight repack malloc failed
BNNS Padding Create: layer_params is NULL
BNNS Padding Create: Padding is not supported beyond 4-D tensors.
BNNS Padding Create: Undefined padding mode.
BNNS Padding Create: Unsupported data layout.
BNNS Padding Create: input and output desciptors have differing numbers of dimensions.
BNNS Padding Create: Input dimension is too small for the padding size.
BNNS Padding Create: Input size + padding sizes doesn't match output size in dimension %zu.
BNNS Padding Create: input descriptor is illegal
BNNS Padding Create: output descriptor is illegal
BNNS Padding Create: Unsupported data type.
BNNS Padding Create: I/O data type mismatch.
BNNS Padding Create: memory allocation failed
BNNS Padding Apply: filter is NULL
BNNS Padding Apply: wrong filter type, filter is not Padding.
BNNS Padding Apply: input pointer is NULL
BNNS Padding Apply: batch_size > 1 and in_stride is 0
BNNS Padding Apply: output pointer is NULL
BNNS Padding Apply: batch_size > 1 and out_stride is 0
BNNS Padding Apply Backward: filter is NULL
BNNS Padding Apply Backward: wrong filter type, filter is not padding
BNNS Padding Apply Backward: input descriptor is NULL
BNNS Padding Apply Backward: input data pointer is NULL
BNNS Padding Apply Backward: batch_size>1 and in_stride is 0
BNNS Padding Apply Backward: output delta descriptor is NULL
BNNS Padding Apply Backward: output delta data pointer is NULL
BNNS Padding Apply Backward: batch_size>1 and out_delta_stride is 0
BNNSFilterCreateLayerSparseEmbedding: layer_params must not be NULL
BNNSSparseGetRepresentation: filter must not be NULL
BNNSSparseGetRepresentation: Invalid filter
BNNSSparseSetRepresentation: filter must not be NULL
BNNSSparseSetRepresentation: Invalid filter
BNNSSparseEmbeddingGetDense: filter must not be NULL
BNNSSparseEmbeddingGetDense: Invalid filter
BNNS Fused Filter Multi-Input: Error filter is NULL
BNNS Fused Filter Multi-Input Warning: Batch size is 0, nothing to do
BNNS Fused Filter Multi-Input: Error - only fused arithmetic and normalization is supported.
BNNS Fused Filter Multi-Input: Error arithmetic filter apply failed
BNNS Fused Filter Backward Multi-Input: Error filter is NULL
BNNS Fused Filter Backward Multi-Input Warning: Batch size is 0, nothing to do
BNNS Fused Filter Backward Multi-Input: output delta is NULL
BNNS Fused Filter Backward Multi-Input: Error - only fused arithmetic and normalization is supported.
BNNS Fused Filter Backward Multi-Input: Error normalization beta delta pointer is NULL. backward computation of all active parameters must be done in a single function call. 
BNNS Fused Filter Backward Multi-Input: Error normalization gamma delta pointer is NULL. backward computation of all active parameters must be done in a single function call. 
BNNS Arithmetic Backward Multi-Input: failed to allocate memory
BNNS Fused Filter Backward Multi-Input: Error normalization backward failed
x_ijf*, x_ijc* -> G_fc*
BNNSFilterApplyTwoInputBatch: invalid argument
Tensor Contraction filter only expects one input
invalid filter or incorrect number of inputs
BNNSFilterApplyBackwardTwoInputBatch: invalid argument
Two-input Tensor Contraction filter has no weights
Tensor Contraction filter has no bias
Invalid filter
Unsupported filter
BNNS GetStateSize : filter is NULL
BNNS SetState: Error filter is NULL
BNNS SetState: Source filter state is NULL
BNNS SetState: Target filter is NOT Dropout/Normalization filter
BNNS GetState: Error filter is NULL
BNNS GetState: Target filter state is NULL
BNNS GetState: Source filter is NOT Dropout filter
BNNS LayerNorm: Error Normalization axis must be 0, 1, or 2.
BNNS GroupNorm: Error The number of input channels must be divisible by the number of groups.
BNNS Normalization: Error Normalization axis must be 0, 1, or 2.
BNNS Normalization: Error The number of input channels must be divisible by the number of groups.
BNNS Fused Filter: Error number_of_fused_filters is not 2. currently supporting only 2 fused filters
BNNS Fused Filter: Error filter_type is NULL
BNNS Fused Filter: Error layer_params is NULL
BNNS Fused Filter: Error currently supporting first filter of BNNSConvolution, BNNSFullyConnected, BNNSTransposedConvolution, or BNNSArithmetic type only
BNNS Fused Filter: Error currently supporting second filter of BNNSBatchNorm, BNNSInstanceNorm, BNNSLayerNorm, BNNSGroupNorm and BNNSQuantization types only
BNNS Fused Filter: Error layer_params[0] is NULL
BNNS Fused Filter: Error layer_params[1] is NULL
BNNS Fused Filter: Error unknown fused filter type
BNNS Normalization: Error filter is NULL
BNNS Normalization: Error wrong filter type, filter is not normalization
BNNS BatchNorm: Error wrong filter type, filter is not batch norm
BNNS InstanceNorm: Error filter is NULL
BNNS InstanceNorm: Error wrong filter type, filter is not instance norm
BNNS LayerNorm: Error filter is NULL
BNNS LayerNorm: Error wrong filter type, filter is not layer norm
BNNS GroupNorm: Error filter is NULL
BNNS GroupNorm: Error wrong filter type, filter is not group norm
BNNS Fused Filter: Error filter is NULL
BNNS Fused Filter Warning: Batch size is 0, nothing to do
BNNS Fused Filter: Error wrong filter, this function should only be used for a filter created with BNNSFilterCreateFusedLayer
BNNS Fused Filter: Error first filter apply failed
BNNS Fused Filter: Error malloc failed
BNNS Arithmetic Filter: Error filter is NULL
BNNS Arithmetic Filter Warning: Batch size is 0, nothing to do
BNNS Arithmetic Filter: Error wrong filter, this function should only be used for a filter created with BNNSFilterCreateLayerArithmetic
BNNS Pooling: Error wrong filter type, filter is not pooling
BNNS Permute Filter: filter is NULL
BNNS Permute Filter Warning: Batch size is 0, nothing to do
BNNS Permute Filter: input delta is NULL
BNNS Permute Filter: output delta is NULL
BNNS Permute Filter: input delta data pointer is NULL
BNNS Permute Filter: output delta data pointer is NULL
BNNS Permute Filter: inplace gradient is not supported
filter is NULL
out_delta is NULL
out_delta->data is NULL
Tensor Contraction filter has no bias: bias_delta must be NULL.
Tensor Contraction filter expects more than one input
Resize filter has no bias: bias_delta must be NULL.
Resize filter has no weights: weights_delta must be NULL.
Resize filter: backward pass requires in_delta and in_delta->data to be non-NULL.
BNNS Copysum filter has no bias: bias_delta must be NULL.
BNNS Copysum filter has no weights: weights_delta must be NULL.
BNNS Copysum filter: backward pass requires in_delta and in_delta->data to be non-NULL.
BNNS Reduction filter has no bias: bias_delta must be NULL.
BNNS Padding: backward pass requires in_delta and in_delta->data to be non-NULL.
BNNS Padding filter has no bias: bias_delta must be NULL.
BNNS Embedding: in must not be NULL.
BNNS Embedding: in_delta must be NULL.
BNNS Embedding: backward pass requires weights_delta and weights_delta->data to be non-NULL.
BNNS Embedding: bias_delta must be NULL.
BNNS Normalization Backward: Error filter is NULL
BNNS Normalization Backward Warning: Batch size is 0, nothing to do
BNNS Normalization Backward: Error wrong filter, this function should only be used for a filter created with BNNSFilterCreateLayerNormalization
BNNS BatchNorm Backward: Error filter is NULL
BNNS BatchNorm Backward Warning: Batch size is 0, nothing to do
BNNS BatchNorm Backward: Error wrong filter, this function should only be used for a filter created with BNNSFilterCreateLayerNormalization
BNNS Instance Norm Backward: Error filter is NULL
BNNS Instance Norm Backward Warning: Batch size is 0, nothing to do
BNNS Instance Norm Backward: Error wrong filter, this function should only be used for a filter created with BNNSFilterCreateLayerNormalization
BNNS Layer Norm Backward: Error filter is NULL
BNNS Layer Norm Backward Warning: Batch size is 0, nothing to do
BNNS Layer Norm Backward: Error wrong filter, this function should only be used for a filter created with BNNSFilterCreateLayerNormalization
BNNS Group Norm Backward: Error filter is NULL
BNNS Group Norm Backward Warning: Batch size is 0, nothing to do
BNNS Group Norm Backward: Error wrong filter, this function should only be used for a filter created with BNNSFilterCreateLayerNormalization
BNNS Fused Filter Backward: Error filter is NULL
BNNS Fused Filter Backward Warning: Batch size is 0, nothing to do
BNNS Fused Filter Backward: output delta is NULL
BNNS Fused Filter Backward: Error delta_parameters is NULL
BNNS Fused Filter Backward: Error wrong filter, this function should only be used for a filter created with BNNSFilterCreateFusedLayer
BNNS Fused Filter Backward: Error - fused compute and quantization gradient is not support 
BNNS Fused Filter Backward: Error Weight delta pointer is NULL. backward computation of all active parameters must be done in a single function call. 
BNNS Fused Filter Backward: Error bias delta pointer is NULL. backward computation of all active parameters must be done in a single function call. 
BNNS Fused Filter Backward: Error normalization beta delta pointer is NULL. backward computation of all active parameters must be done in a single function call. 
BNNS Fused Filter Backward: Error normalization gamma delta pointer is NULL. backward computation of all active parameters must be done in a single function call. 
BNNS Arithmetic Backward: failed to allocate memory
BNNS Fused Filter Backward: Error normalization backward failed
BNNSDirectApplyConvolutionBatch not supported 
BNNSDirectApplyTransposedConvolutionBatch not supported 
BNNSDirectApplyPoolingBatch not supported 
BNNSDirectApplyLossBatch not supported
BNNS Apply: filter or output can't be null
BNNS Apply: Transposed convolution does not support groups
BNNS Apply: Tensor Contraction filter expects more than one input
BNNS Apply: Loss filter apply must be called with BNNSLossFilterApplyBatch
BNNS Apply: Batchnorm filter apply must be called with BNNSNormalizationFilterApplyBatch
BNNS Apply: Instance Norm filter apply must be called with BNNSNormalizationFilterApplyBatch
BNNS Apply: Layer Norm filter apply must be called with BNNSNormalizationFilterApplyBatch
BNNS Apply: Group Norm filter apply must be called with BNNSNormalizationFilterApplyBatch
BNNS Apply: Fused filter apply must be called with BNNSFusedFilterApplyBatch
BNNS Apply: invalid filter
BNNS DESTROY: invalid filter
BNNS Padding + Convolution Apply Backward: input is null
BNNS Padding + Convolution Apply Backward: allocation failed
BNNS Padding + Convolution Apply Backward: Padding failed
BNNS Padding + Convolution Apply Backward: Convolution backward failed
BNNS Padding + Convolution Apply Backward: Padding backward failed
malloc failed
Padding failed
Convolution failed
BNNS Convolution Variant: input type %s not supported!!!
BNNS Convolution Variant: compute type %s not supported!!!
BNNS GRU Fused Gates: Output pointer is null
BNNS GRU Fused Gates: Input pointer is null
BNNS GRU Fused Gates: Recurrent pointer is null
BNNS GRU Fused Gates: Hidden Input pointer is null
BNNS GRU Fused Gates: Input vector size %zu foesn't match output vectro size %zu
BNNS GRU Fused Gates: Recurrent vector size %zu foesn't match output vectro size %zu
BNNS GRU Fused Gates: Hidden Input vector size %zu foesn't match output vectro size %zu
BNNS GRU Fused Gates: number of gates for Input %zu and Recurrent %zu doesn't match
BNNS GRU Fused Gates: Input stride %zu is smaller than number of elements %zu
BNNS GRU Fused Gates: Recurrent stride %zu is smaller than number of elements %zu
BNNS GRU Fused Gates: Output supported data types are fp32 or fp16
BNNS GRU Fused Gates: Input data type doesn't match output data type
BNNS GRU Fused Gates: Recurrent data type doesn't match output data type
BNNS GRU Fused Gates: Hidden Input data type doesn't match output data typen
BNNS GRU Fused Gates: GRU fused gates doesn't support %zu gates
BNNS GRU Fused Gates: Hidden Input batch size %zu doesn't match output batch size %zu
BNNS GRU Fused Gates: Input batch size %zu doesn't match output batch size %zu
BNNS GRU Fused Gates: Hidden Output batch stride %zu is small then a single vector stride
BNNS GRU Fused Gates: Hidden Input batch stride %zu is small then a single vector stride
BNNS GRU Fused Gates: Input batch stride %zu is small then a single vector stride
BNNS GRU Fused Gates: Recurrent batch stride %zu is small then a single vector stride
BNNS CONVOLUTIONS VERSION2: unsupported src type to pack
BNNS Convolution Compute Weights delta: failed to compute
BNNS Convolution Apply Backward: activation_grad allocation failed
weight delta backward failed
BNNS Convolution: int32 repack allocation failed
BNNS Convolution: Float16 repack allocation failed
BNNS Convolution: Float32 repack allocation failed
BNNS Activation Apply: Error input/output/activation pointers must be non-NULL
BNNS Activation Apply: Input and output tensor deosn't match
BNNS Activation Apply: unable to allocate memory to compute activation
BNNS Activation Apply: failed to allocate memory
BNNS Apply Activation Backward: BNNSActivationFunctionIntegerLinearSaturate isn't supported
BNNS Apply Activation Backward: BNNSActivationFunctionIntegerLinearSaturatePerChannel isn't supported
BNNS Apply Activation Backward: failed to allocate temporary y
BNNS Apply Activation Backward: unsupported activation backward
BNNS Activation backward: malloc failed
BNNS Activation: number of memory descriptor is lower than expected
BNNS Activation: claiming pre allocated memory failed
BNNS Activation: memory allocation failed
BNNS Activation: Failed to init filter
BNNS Activation Apply Backward: At least one of in or out must be non-NULL
BNNS Activation Apply Backward: invalid filter
BNNS Activation Apply Backward: out_delta must not be NULL
BNNS Activation Backward: preallocated memory isn't supported
BNNS Activation Apply Backward: Input is required
BNNS Activation Backward: Weights Delta not supported
BNNS Activation Apply Backward: Bias Delta not supported
BNNS Activation Apply Backward: in place operation for none matching data sizes isn't supported
BNNS Activation Apply Backward: Input and output tensor deosn't match
BNNS Activation Apply Backward: In Delta tensor deosn't match
BNNS Activation Apply Backward: Out Delta tensor deosn't match
BNNS Activation Apply Backward: Weights Delta tensor (%zu x %zu) deosn't make sense
BNNS Activation Apply Backward: unable to allocate memory to compute backward with missing original output
BNNS Activation Apply Backward: unable to allocate memory to compute activation, using slower compute path
BNNS Activation Apply Backward: failed to allocate memory
BNNS Activation Apply Backward: failed to init index counter
BNNS Activation: invalid argument
BNNS Activation: in-place activation layer is allowed only for output types with the same or smaller storage size
BNNS Activation: unsupported types for conversion
BNNS Activation: Apply failed
BNNS Activation: val doesn't make sense
BNNS Activation Apply: activation function not supported
BNNS Activation Init: filter is null
BNNS Activation Init: input descriptor is illegal
BNNS Activation Init: output descriptor is illegal
BNNS Activation Init: layout doesn't match
BNNS Activation Init: only 3D conversions are supported
BNNS Activation Init: memory allocation failed
BNNS Activation Init: dim %zu input size %zu != %zu output size
BNNS Activation Init: alpha can't be +/-inf or Nan for Gumbel or Gumbel Max
BNNS Activation Init: beta can't be +/-inf or Nan or zero or negative for Gumbel or Gumbel Max
BNNS Activation Init: BNNSActivationFunctionPReLUPerChannel is only valid with data layout BNNSDataLayoutImageCHW
BNNS Activation Init: invalid activation function
BNNS SoftMax: sum result shouldn't be 0
BNNS Activation Apply Backward: Softmax backward require original output
BNNS Activation Apply Backward: original inputs or original outputs must be available
BNNS Activation Apply Backward: activation function not supported
BNNS Activation: Error NULL input/output pointer
BNNS Activation: Error number of input channels must be equal to number of output channels
BNNS Multihead Attention Apply: key_mask must be a 1D tensor.
BNNS Multihead Attention Apply: key_mask must have type BNNSDataTypeBoolean.
BNNS Multihead Attention Apply: key_mask must have size exactly source_length (key_mask.size[0] = %zu, source_length = %zu).
BNNS Multihead Attention Apply: 2D add_to_attention must have shape (target_length, source_length) = (%zu, %zu), but passed tensor has shape (%zu, %zu).
BNNS Multihead Attention Apply: 3D add_to_attention must have shape (num_heads, target_length, source_length) = (%zu, %zu, %zu), but passed tensor has shape (%zu, %zu, %zu).
BNNS Multihead Attention Apply: 4D add_to_attention must have shape (batch_size, num_heads, target_length, source_length) = (%zu, %zu, %zu, %zu), but passed tensor has shape (%zu, %zu, %zu, %zu).
BNNS Multihead Attention Apply: unsupported layout for argument add_to_attention.
BNNS Multihead Attention Apply: add_to_attention must have type BNNSDataTypeBoolean or BNNSDataTypeFloat32.
BNNS Multihead Attention Apply: If backprop_cache is non-NULL, backprop_cache_size must also be non-NULL
BNNS Multihead Attention Apply: If workspace is non-NULL, workspace_size must also be non-NULL
BNNS Multihead Attention Apply: Supplied workspace is too small (size %ld, but required %ld)
Multihead Attention Backward: Only support for a full-size backprop_cache has been implemented.
Multihead Attention Backward: If workspace is non-NULL, workspace_size must also be non-NULL
Multihead Attention Backward: Insufficient workspace supplied (%zu bytes). Require %zu bytes.
value_delta_copy_out
value_shadow_delta
value_delta
key_delta_copy_out
key_shadow_delta
key_delta
query_delta_copy_out
query_shadow_delta
query_delta
layer_params->query.target_desc
layer_params->key.target_desc
layer_params->query.weights
layer_params->key.weights
layer_params->value.weights
(d_model, d_key, num_heads)
(source_length, k_dim)
(k_dim, d_key, num_heads)
layer_params->value.target_desc
(source_length, v_dim)
(v_dim, d_value, num_heads)
layer_params->output.target_desc
(target_length, d_model)
layer_params->output.weights
(num_heads*d_value, d_model)
BNNS Multihead Attention Create: layer_params->key_attn_bias.data and layer_params->value.attn_bias.data must both be NULL or must both be not NULL
layer_params->key_attn_bias
(d_key, num_heads)
layer_params->value_attn_bias
(d_value, num_heads)
BNNS Multihead Attention Create: fp16 and bf16 may not be mixed or unsupported data type
layer_params->query.bias
layer_params->key.bias
layer_params->value.bias
layer_params->output.bias
(d_model)
BNNS Multihead Attention Create: Unsupported layout for query.weights
BNNS Multihead Attention Create: Unsupported layout for key.weights
BNNS Multihead Attention Create: Unsupported layout for value.weights
BNNS MULTIHEAD ATTENTION GetPointer: Layer was created with dropout=0.0, cannot return pointer to dropout value.
BNNS MULTIHEAD ATTENTION GetPointer: Unsupported target %d
BNNS Multihead Attention Create: Expected %s to have exactly %zu dimensions, but it has %zu
BNNS Multihead Attention Create: Expected %s to have shape %s = (
%s%zu
) but has actual shape (
BNNS SPECIALIZED CONVOLUTION Create: failed to alloacted memory
BNNS Permute Filter: layer_params is NULL
BNNS Permute Filter: illegal input descriptor
BNNS Permute Filter: illegal output descriptor
BNNS Permute Filter: input and output descriptor layout dimension does not match. input dimm = %zu output dimm = %zu
BNNS Permute Filter: permutation array index %zu has illegal value of %zu
BNNS Permute Filter: permutation array is missing axis %zu
BNNS Permute Filter: permutation array axis %zu appears %zu times, each axis should appear exactly once
0123456789abcdefghijklmnopqrstuvwxyz
 -> out_
BNNS Pooling Filter: filter is NULL
BNNS Pooling Filter: input is NULL
BNNS Pooling Filter: output is NULL
BNNS Pooling Filter Warning: Batch size is 0, nothing to do
BNNS Pooling: input batch stride doesn't make sense (%zu < %zu x %zu)
BNNS Pooling: dilation supported only in BNNSPoolingFunctionMax/UnMax
BNNS Pooling: dilation supported only for BNNSDataTypeFloat32 data type
Pooling layer filter running slow path: stride=%zu,%zu kernel=%zu,%zu
BNNS Pooling UnMax: indices array is NULL
BNNS Pooling: case not implemented
BNNS Pooling Apply: allocation of work buffer failed
BNNS Pooling Backward: cannot run backward without output delta
BNNS Pooling Backward: only float32 output delta are supported
BNNS Pooling Backward: only float32 input delta are supported
BNNS Pooling Backward: only float32 bias delta are supported
BNNS Pooling Backward: unsupported pooling function
BNNS Pooling Backward: invalid argument
BNNS Pooling Backward: Fusion of compute with activation is unsupported for given activation function
BNNS Pooling Backward: failed to allocate memory
BNNS Pooling Backward: failed to apply activation backward
BNNS Pooling: layer parameters is NULL
BNNS Pooling: input must be a 3D array
BNNS Pooling: input descriptor is illegal
BNNS Pooling: output must be a 3D array
BNNS Pooling: output descriptor is illegal
BNNS Pooling: input/output channel counts do not match
BNNS Pooling: input/output types do not match
BNNS Pooling: invalid kernel dimensions, should be greater than 0
BNNS Pooling: optimized code supports kernel width/height up to 16
BNNS Pooling: dilation only supported for BNNSPoolingFunctionMax/UnMax
BNNS Pooling: dilation only supported for BNNSDataTypeFloat32 input/output data types
BNNS_POOLING: input size computation wraparound input (%zu x %zu), output (%zu x %zu), stride (%zu x %zu), padding (left %zu right %zu up %zu down %zu)
BNNS_POOLING: input size computation wraparound input (%zu x %zu), output (%zu x %zu), stride (%zu x %zu), padding (%zu x %zu)
BNNS_POOLING: output(%zu x %zu) is larger than input(%zu x %zu) with padding(left %zu right %zu up %zu down %zu).
 stride (%zu x %zu)
BNNS_POOLING: output(%zu x %zu) is larger than input(%zu x %zu) with padding(%zu x %zu).
 stride (%zu x %zu)
BNNS_POOLING: invalid pooling function
BNNS_POOLING: supported input/output data types: float32 float16
BNNS_POOLING: slow path: stride not in {1,2}
BNNS_POOLING: slow path: kernel size not in {2,3,4}
BNNS Pooling: Internal Memory size %zu doesn't match expected %d
BNNS Pooling: claiming pre allocated memory failed
BNNS Pooling: failed to allocate context
BNNS Pooling Backward: only float32 delta are supported
BNNS Pooling Backward: wrong pooling function called
BNNS Reduction Create: Only FP32 input data is supported for reduction function %u
BNNS Reduction Create: Only FP32 output data is supported for reduction function %u
BNNS Reduction Create: Only FP32 or bool input data is supported for reduction function %u
BNNS Reduction Create: Only FP32 or bool output data is supported for reduction function %u
BNNS Reduction Create: Invalid reduction function %u
BNNS Reduction Create: Only FP32 weight data is supported
BNNS Reduction Create: i_desc and o_desc must have the same number of dimensions (%zu vs %zu)
BNNS Reduction Create: i_desc and w_desc must have the same number of dimensions (%zu vs %zu)
BNNS Reduction Create: i_desc and o_desc dimensions must have the same size, or o_desc.size[d] must be 1 to indicate reduction (dimension %zu: %zu vs %zu)
BNNS Reduction Create: i_desc and w_desc dimensions must have the same size (dimension %zu: %zu vs %zu)
BNNS Reduction Create: ArgMin/ArgMax reduction is only supported on a single axis
BNNS Reduction Apply: Both input and output must non-NULL
BNNS Reduction Backward: input_delta must have the same number of dimensions as i_desc
BNNS Reduction Backward: input_delta must have the same shape as i_desc
BNNS Reduction Backward: output_delta must have the same number of dimensions as o_desc
BNNS Reduction Backward: output_delta must have the same shape as o_desc
BNNS Reduction Backward: output_delta must not be NULL
BNNS Reduction Backward: weights_delta must have the same number of dimensions as w_desc
BNNS Reduction Backward: weights_delta must have the same shape as w_desc
BNNS Reduction Direct Apply: Size of the dimensions of the output can not be greater than the size of the input
BNNS Reduction Direct Apply: ArgMin/ArgMax reduction is only supported on a single axis
BNNS Reduction Direct Apply: Unsupported data type combination
Backward usage is not supported for reduction type %u
BNNS Reduction Backward: Reduce function %u requires input to be supplied for backward computation
BNNS Reduction Backward: Reduce function %u requires output to be supplied for backward computation
BNNS Reduction Backward: Calculation of weights_delta requires input to be supplied
Size mismatch in dimension %zu: input %zu, output %zu
BNNS Reduce Apply Generic: Size mismatch in dimension %zu: input %zu, output %zu
BNNS Arithmetic Filter: layer_params is NULL
BNNS Arithmetic Filter: unsupported arithmetic function
BNNS Arithmetic Filter: memory allocation failed
BNNS Arithmetic Filter: Failed to init filter
BNNS Arithmetic Filter: filter is NULL
BNNS Arithmetic Filter: wrong filter type, this function should only be used for a filter created with BNNSFilterCreateLayerArithmetic
BNNS Arithmetic Filter: in pointer is NULL
BNNS Arithmetic Filter: batch_size>1 and in_stride is 0
BNNS Arithmetic Filter Internal: expected number of inputs call failed
BNNS Arithmetic Filter: wrong number_of_inputs, number_of_inputs=%zu, expecting %zu
BNNS Arithmetic Filter: in[%zu] is NULL
BNNS Arithmetic Filter: in_stride[%zu] is 0
BNNS Arithmetic Filter: output pointer is NULL
BNNS Arithmetic Filter: batch_size>1 and out_stride is 0
BNNS Arithmetic Filter: descriptors for input that is being processed in-place and output must exactly match
BNNS Arithmetic Filter: input is being processed in-place but input and output descriptor types do not match
BNNS Arithmetic Filter: output dimension smaller than input is not supported. batch size>1, input type is BNNSSample but out_type is not BNNSSample
BNNS Arithmetic Filter: only BNNSDataTypeFloat32 is supported
BNNS Arithmetic Filter: input1 and input2 have the same data pointer, but descriptors are different
BNNS Arithmetic Filter: input1 and input2 have the same data pointer, but different BNNSDescriptorType
BNNS Arithmetic Filter: descriptors for input1 that is being processed in-place and output must exactly match
BNNS Arithmetic Filter: input1 is being processed in-place but input1 and output descriptor types do not match
BNNS Arithmetic Filter: descriptors for input2 that is being processed in-place and output must exactly match
BNNS Arithmetic Filter: input2 is being processed in-place but input2 and output descriptor types do not match
BNNS Arithmetic Filter: output dimension smaller than input is not supported. batch size>1, an input type is BNNSSample but out_type is not BNNSSample
BNNS Arithmetic Filter: input1 and input3 have the same data pointer, but descriptors are different
BNNS Arithmetic Filter: input1 and input3 have the same data pointer, but different BNNSDescriptorType
BNNS Arithmetic Filter: input2 and input3 have the same data pointer, but descriptors are different
BNNS Arithmetic Filter: input2 and input3 have the same data pointer, but different BNNSDescriptorType
BNNS Arithmetic Filter: descriptors for input3 that is being processed in-place and output must exactly match
BNNS Arithmetic Filter: input3 is being processed in-place but input3 and output descriptor types do not match
BNNS Arithmetic Filter Internal: unsupported arithmetic function
BNNS Arithmetic Filter: out delta descriptor pointer is NULL
BNNS Arithmetic Filter: out delta descriptor data pointer is NULL
BNNS Arithmetic Filter: out_delta_stride is 0
BNNS Arithmetic Filter: out delta descriptor validation failed
BNNS Arithmetic Filter: in_delta is NULL
BNNS Arithmetic Filter: in_delta_stride is NULL
BNNS Arithmetic Filter: in is NULL, but it is required for backward compute
BNNS Arithmetic Filter: in_delta descriptor pointer %zu is NULL
BNNS Arithmetic Filter: cannot compute activation backward, output is NULL
BNNS Arithmetic Filter: Fusion of arithmetic with activation is unsupported for given activation function
BNNS Arithmetic Filter Backward Internal: unsupported arithmetic function
BNNS Arithmetic Filter Backward: failed to apply activation backward
BNNS Arithmetic Filter: input type is BNNSConstant, no gradient to compute
BNNS Arithmetic Filter: in_delta[0] is NULL
BNNS Arithmetic Filter: in_delta[0]->data is NULL
BNNS Arithmetic Filter: in_delta_stride[0] is 0
BNNS Arithmetic Filter: in delta descriptor validation failed
BNNS Arithmetic Filter: arithmetic input_delta and input descriptors must have the same sizes, strides and data types
BNNS Arithmetic Filter: arithmetic output_delta and output descriptors must have the same sizes, strides and data types
BNNS Arithmetic Filter: descriptors for out_delta that is being processed in-place and in_delta must exactly match
BNNS Arithmetic Filter: input delta is being processed in-place but input type and output type are different
BNNS Arithmetic Filter: input delta and output delta has the same data pointer (inplace), but different in_delta_stride
BNNS Arithmetic Filter: input delta and output delta has the same data pointer (inplace), but in_delta.flags indicates to accumulate result
BNNS Arithmetic Filter: backward compute for chosen arithmetic function does not allow inplace gradient. out delta data must not be the same as input delta data
BNNS Arithmetic Filter: inputs are both BNNSConstant, no gradient to compute
BNNS Arithmetic Filter: in_delta[0] is NULL but in1 type is not BNNSConstant
BNNS Arithmetic Filter: in_delta[0]->data is NULL but in1 type is not BNNSConstant
BNNS Arithmetic Filter: in1_type is BNNSSample but in_delta_stride[0] is 0
BNNS Arithmetic Filter: input1 delta data pointer is not NULL. cannot compute gradient for BNNSConstant
BNNS Arithmetic Filter: in1 delta descriptor validation failed
BNNS Arithmetic Filter: in_delta[1] is NULL but in2 type is not BNNSConstant
BNNS Arithmetic Filter: in_delta[1]->data is NULL but in2 type is not BNNSConstant
BNNS Arithmetic Filter: in2_type is BNNSSample, but in_delta_stride[1] is 0
BNNS Arithmetic Filter: input2 delta data pointer is not NULL. cannot compute gradient for BNNSConstant
BNNS Arithmetic Filter: in2 delta descriptor validation failed
BNNS Arithmetic Filter: input1 delta and input2 delta has the same data pointer, but descriptors are different
BNNS Arithmetic Filter: input1 delta and input2 delta has the same data pointer, but different data types
BNNS Arithmetic Filter: input1 delta and input2 delta has the same data pointer, but different in_delta_stride
BNNS Arithmetic Filter: forward input pointer points to the same array but input gradient pointers point to different arrays
BNNS Arithmetic Filter: descriptors for out_delta that is being processed in-place and in1_delta must exactly match
BNNS Arithmetic Filter: input1 delta is being processed in-place but input type and output type are different
BNNS Arithmetic Filter: input1 delta and output delta has the same data pointer (inplace), but different in_delta_stride
BNNS Arithmetic Filter: input1 delta and output delta has the same data pointer (inplace), but in1_delta.flags indicates to accumulate result
BNNS Arithmetic Filter: descriptors for out_delta that is being processed in-place and in2_delta must exactly match
BNNS Arithmetic Filter: input2 delta is being processed in-place but input2 type and output type are different
BNNS Arithmetic Filter: input2 delta and output delta has the same data pointer (inplace), but different in_delta_stride
BNNS Arithmetic Filter: input2 delta and output delta has the same data pointer (inplace), but in2_delta.flags indicates to accumulate result
BNNS Arithmetic Filter: backward compute for chosen arithmetic function does not allow inplace gradient. out delta data must not be the same as either of the input delta data
BNNS Arithmetic Filter: inputs are all BNNSConstant, no gradient to compute
BNNS Arithmetic Filter: in_delta[2] is NULL but in3 type is not BNNSConstant
BNNS Arithmetic Filter: in_delta[2]->data is NULL but in3 type is not BNNSConstant
BNNS Arithmetic Filter: in3_type is BNNSSample, but in_delta_stride[2] is 0
BNNS Arithmetic Filter: input3 delta data pointer is not NULL. cannot compute gradient for BNNSConstant
BNNS Arithmetic Filter: in3 delta descriptor validation failed
BNNS Arithmetic Filter: input1 delta and input3 delta has the same data pointer, but descriptors are different
BNNS Arithmetic Filter: input1 delta and input3 delta has the same data pointer, but different data types
BNNS Arithmetic Filter: input1 delta and input3 delta has the same data pointer, but different in_delta_stride
BNNS Arithmetic Filter: input2 delta and input3 delta has the same data pointer, but descriptors are different
BNNS Arithmetic Filter: input2 delta and input3 delta has the same data pointer, but different data types
BNNS Arithmetic Filter: input2 delta and input3 delta has the same data pointer, but different in_delta_stride
BNNS Arithmetic Filter: descriptors for out_delta that is being processed in-place and in3_delta must exactly match
BNNS Arithmetic Filter: input3 delta is being processed in-place but input3 type and output type are different
BNNS Arithmetic Filter: input3 delta and output delta has the same data pointer (inplace), but different in_delta_stride
BNNS Arithmetic Filter: input3 delta and output delta has the same data pointer (inplace), but in3_delta.flags indicates to accumulate result
BNNS Arithmetic Filter: in_delta[0] must be NULL for BNNSArithmeticSelect
BNNS Arithmetic Filter: backward compute for BNNSArithmeticSelect does not allow inplace gradient. out delta data must not be the same as either of the input delta data
BNNS Arithmetic Filter: arithmetic_function_fields is NULL
BNNS Arithmetic Filter: malloc failed
BNNS Arithmetic Filter: input descriptor validation failed
BNNS Arithmetic Filter: input1 descriptor validation failed
BNNS Arithmetic Filter: unsupported out_type
BNNS Arithmetic Filter: unsupported in_type
BNNS Arithmetic Filter: in, out descriptor check failed
BNNS Arithmetic Filter: input2 descriptor validation failed
BNNS Arithmetic Filter: unsupported in1_type
BNNS Arithmetic Filter: unsupported in2_type
BNNS Arithmetic Filter: in1,in2,out descriptor check failed
BNNS Arithmetic Filter: input3 descriptor validation failed
BNNS Arithmetic Filter: unsupported in3_type
BNNS Arithmetic Filter: in1,in2,in3,out descriptor check failed
BNNS Arithmetic: Activation creation failed
BNNS Arithmetic Filter: input and output data types do not match
BNNS Arithmetic Filter: input size[%zu]=%zu does not equal max input,output size of %zu or 1
BNNS Arithmetic Filter: output size[%zu]=%zu does not equal max input,output size of %zu
BNNS Arithmetic Filter: input1 and out data types do not match
BNNS Arithmetic Filter: input2 and out data types do not match
BNNS Arithmetic Filter: input1 size[%zu]=%zu does not equal max input1,input2,out size of %zu or 1
BNNS Arithmetic Filter: input2 size[%zu]=%zu does not equal max input1,input2,out size of %zu  or 1
BNNS Arithmetic Filter: output size[%zu]=%zu does not equal max input1,input2,out size of %zu
BNNS Arithmetic Filter: input3 and out data types do not match
BNNS Arithmetic Filter: input1 size[%zu]=%zu does not equal max input1,input2,input3,out size of %zu or 1
BNNS Arithmetic Filter: input2 size[%zu]=%zu does not equal max input1,input2,input3,out size of %zu  or 1
BNNS Arithmetic Filter: input3 size[%zu]=%zu does not equal max input1,input2,input3,out size of %zu  or 1
BNNS Arithmetic Filter: output size[%zu]=%zu does not equal max input1,input2,input3,out size of %zu
BNNS Arithmetic Filter:  illegal descriptor
BNNS Arithmetic Filter: BNNSArithmeticSelect expects BNNSDataTypeBoolean
BNNS Arithmetic Filter: data layout unsupported
BNNS Arithmetic Filter: descriptor memory non contiguous
BNNS Arithmetic Filter Internal: descriptors must point to contiguous memory
BNNS Arithmetic Filter: in delta descriptor memory non contiguous
BNNS Arithmetic Filter: out delta descriptor memory non contiguous
BNNS Arithmetic Filter: in1 delta descriptor memory non contiguous
BNNS Arithmetic Filter: in2 delta descriptor memory non contiguous
BNNS Arithmetic Filter: output delta descriptor memory non contiguous
BNNS Arithmetic Filter: in3 delta descriptor memory non contiguous
BNNS Embedding Create: Only 8, 16, 32 and 64 bit integer types may be used as input data type.
BNNS Embedding Create: Only 16 and 32 bit floating point and 8, 16 and 32 bit integer types may be used as dictionary data type.
BNNS Embedding Create: o_desc dimension must be dim(i_desc) + dim(dictionary) - 1.
BNNS Embedding Create: First dim(dictionary)-1 dimensions of o_desc and dictionary must have consistent shapes.
BNNS Embedding Create: Last dim(i_desc) dimensions of o_desc and i_desc must have consistent shapes.
internal embedding copy
dict-item
o-item
internal embedding copy back
BNNS Embedding Apply: Input value %zu is out of range [0, %zu)
BNNS Embedding ApplyBackward: weights_delta must have the same shape as layer_params->dictionary
BNNS Embedding ApplyBackward: out_delta must have the same shape as layer_params->o_desc
BNNS Embedding ApplyBackward: Input value %zu out of range
BNNS Sparse Embedding Create: Only 8, 16, 32 and 64 bit integer types may be used as input data type.
BNNS Sparse Embedding Create: Only 16 and 32 bit floating point and 8, 16 and 32 bit integer types may be used as dictionary data type.
BNNS Sparse Embedding Create: o_desc dimension must be dim(i_desc) + dim(dictionary) - 1.
BNNS Sparse Embedding Create: First dim(dictionary)-1 dimensions of o_desc and dictionary must have consistent shapes.
BNNS Sparse Embedding Create: Last dim(i_desc) dimensions of o_desc and i_desc must have consistent shapes.
BNNS Sparse Embedding Create: Unsupported optimization function
BNNS Sparse Embedding Create: Dictionary must be of type BNNSDataTypeFloat32
BNNS Sparse Embedding Apply: Filter is no longer valid
BNNS Sparse Embedding Apply: Input value %zu is out of range [0, %zu)
BNNS Sparse Embedding Apply Backwards: Filter is no longer valid
BNNS Embedding Backwards Sparse: Accumulation of sparse weights gradient is not supported.
BNNS Embedding Backwards Sparse: out_delta must have the same shape as layer_params->o_desc
BNNS Embedding Backwards Sparse: dictionary_delta->indices must have a single dimension
BNNS Embedding Backwards Sparse: dictionary_delta->indices.size[0] must be 1
BNNS Embedding Backwards Sparse: dictionary_delta->sparse_dimension_size[0] must be num_embeddings
BNNS Embedding Backwards Sparse: dictionary_delta->values shape must match dictionary item shape
BNNS Embedding Backwards Sparse: For in-place indices, dictionary_delta->count must equal the total size of layer_params->i_desc * batch_size
BNNS Embedding Backwards Sparse: For in-place indices, input must be contiguous
BNNS Embedding Backwards Sparse: For in-place values, dictionary_delta->count must equal the total size of out_delta * batch_size
BNNS Embedding Backwards Sparse: For in-place values, out_delta must be contiguous in its indexing dimensions
BNNS Sparse Embedding Get Representation: Filter is no longer valid
BNNS Sparse Embedding Get Representation: Unexpected value for num_accumulators=%zu (expected %zu)
BNNS Sparse Embedding Get Representation: num_opt_field_changes!=NULL but opt_field_changes_indices is NULL
BNNS Sparse Embedding Get Representation: num_opt_field_changes!=NULL but opt_field_changes is NULL
BNNS Sparse Embedding Get Representation: num_opt_field_changes=NULL, but optimization fields of filter have previosuly been updated
BNNS Sparse Embedding Set Representation: Filter is no longer valid
BNNS Embedding Set Sparse Optimizer Context: Expected %zu accumulators
BNNS Embedding Set Sparse Optimizer Context: values[0]->indices must have dimension 1
BNNS Embedding Set Sparse Optimizer Context: values[0]->indices.size[0] must be 1
BNNS Embedding Set Sparse Optimizer Context: Expected values indices descriptors to be identical (that is point to the same data)
BNNS Embedding Set Sparse Optimizer Context: Expected values indices descriptors to be identical
dest
BNNS Sparse Embedding Optimizer Step: Filter is no longer valid
BNNS Embedding Sparse Optimizer Step: Expected dictionary_delta->indices to have a single dimension
BNNS Embedding Sparse Optimizer Step: Expected dictionary_delta->values to match shape of dictionary
BNNS Embedding Sparse Optimizer Step: Dictionary gradient items must be contiguous
BNNS Embedding Sparse Optimizer Step: Dictionary items must be contiguous
BNNS Embedding Sparse Optimizer Step: grad.data_type must be BNNSDataTypeFloat32
BNNS Sparse Embedding Get Dense: Filter is no longer valid
BNNS Sparse Embedding Get Dense: Unexpected value for num_accumulators=%zu (expected %zu)
BNNS Sparse Embedding Get Dense: Unexpected number of dimensions for accumulator[%zu]
BNNS Sparse Embedding Get Dense: Unsupported data type for accumulator[%zu]
BNNS Sparse Embedding Get Dense: Unexpected shape for accumulator[%zu]
BNNS Sparse Embedding Get Dense: accumulator[%zu] must be contiguous in dictionary item dimensions
BNNS batch norm forward: malloc for mean failed
BNNS batch norm forward: malloc for var failed
BNNS batch norm forward: malloc for isqrtvar failed
BNNS Batchnorm Backward Apply: delta allocation failed
BNNS Batchnorm Backward Apply: activation_grad allocation failed
BNNS instance norm backward: malloc for dbeta failed
BNNS instance norm backward: malloc for dgamma failed
BNNS Layer Norm Apply Backward: malloc activation_grad failed
BNNS Layer Norm Apply Backward: dx hat allocation failed
BNNS Group Norm Apply Backward: failed to allocate memory
BNNS Group Norm Apply Backward: failed to allocate activation grad memory
v8@?0
hw.physicalcpu
BNNSActivationFunctionIntegerLinearSaturate isn't supported in apply_bias_and_activation
BNNSActivationFunctionIntegerLinearSaturatePerChannel isn't supported in apply_bias_and_activation
BNNSActivationFunctionIntegerLinearSaturate isn't supported in apply_bias_and_activation_bf16
BNNSActivationFunctionIntegerLinearSaturatePerChannel isn't supported in apply_bias_and_activation_bf16
allocation failed, size=%zu
reallocation failed, size=%zu
allocation failed, size=%zu, align=%zu
BNNS: unexpected data type, failing
BNNS: Data type not supprted, fail
BNNS: layout not supported
BNNS: active dimension must be greater than 0
BNNS: dimension %zu stride %zu is lower then previous dimension actual size %zu * %zu (size*stride)
Unsupported layout: %d
BNNS: memory usage exceeded capacity
BNNS PreAllocated Memory: memory usage exceeded capacity
BNNS PreAllocated Memory: failed to claim scratch memory
BNNS Create Convolution Winograd: malloc failed
BNNS Convolutions Winograd: weights allocation failed
BNNS Convolutions Winograd: bias allocation failed
BNNS Apply Convolution Winograd: memory allocation failed
weight packing must be at least 32 and a power of 2
BNNS Convolution ARM64: only stride 1x1 and 2x2 are supported
BNNS Convolution Apply: input batch stride doesn't make sense (%zu < %zu x %zu)
BNNS Convolution Apply: output batch stride doesn't make sense (%zu < %zu x %zu)
BNNS Convolution Apply: failed to allocate weights in apply
BNNS Convolution Backward: BNNSNDArrayFlagBackpropAccumulate has not yet been implemented for weights_delta or bias_delta.
BNNS Convolution Backward: cannot run backward without output delta
BNNS Convolution Backward: [out delta/in delta/weight delta/bias delta] - data type check failed. unsupported data type
BNNS Convolution Backward: internal error, incorrect wrapper
BNNS Convolution forward output data type does not match output delta data type
BNNS Convolution forward input data type does not match input delta data type
BNNS Convolution forward weights data type does not match weights delta data type
BNNS Convolution weight packing is not supported backward
BNNS Convolution Backward: BNNSFlagsUseClientPtr must be enabled during training
BNNS Convolution Backward: invalid argument
BNNS Convolution Backward: Fusion of compute with activation is unsupported for given activation function
BNNS Convolution Backward: failed to allocate memory
BNNS Convolution Backward: failed to apply activation backward
BNNS Conv: Unsupported weight format for Input delta compute
BNNS Convolution Apply Backward: could not create transposed convolution filter
BNNS Convolution Apply Backward: Convolution input delta computation failed
BNNS Transposed Convlution Apply: failed to allocate weights buffer
BNNS Transposed Convlution Apply: failed to allocate backward weights buffer
BNNS Transposed Convlution Apply: failed to allocate out delta buffer
BNNS Transposed Convlution Apply: failed to manipulate output delta
BNNS Transposed Convlution Apply: could not create convolution filter
BNNS Convolution Apply Backward: allocation failed
BNNS Convolution Apply Backward: Transposed Convolution Backward Compute failed
transposed convolution parameter error
transposed convolution backward failed
Convolution bias delta failed
BNNS Create Layer Convolution: fused padd convolution allocation failed
BNNS Create Layer Convolution: padding layer creation failed
BNNS Create Layer Convolution: padding descriptor creation failed
BNNS Create Layer Convolution: input channels must be divisible by groups
BNNS Create Layer Convolution: output channels must be divisible by groups
BNNS Create Layer Convolution: bias must match output channels
BNNS Create Layer Convolution: grouped convolution weight layout must be BNNSDataLayoutConvolutionWeightsOIHW
BNNS Create Layer Convolution: failed to allocate memory to copy weights and bias
BNNS Create Layer Convolution: failed to create forward convolution filter
BNNS Create Layer Convolution: failed to create wrapper filter
BNNS Convolution Create: incompatible numbers of channels between images and convolution parameters
BNNS Convolution Create: unsupported weight format
BNNS Convolution Create: input must be a 3D array
BNNS Convolution Create: input descriptor is illegal
BNNS Conv: output must be a 3D array
BNNS Convolution Create: output descriptor is illegal
BNNS Convolution Create:  weights descriptor is illegal
BNNS Convolution Create: not allowed to delay allocation to apply if weights ptr isn't maintained by Client
BNNS Convolution Create: failed to create dilated convolution
BNNS Convolution Create: failed to create packed weights convolution
BNNS Convolution Create: failed to create grouped convolution
BNNS Convolution Create: failed to allocate weights buffer
BNNS Convolution Create: failed to allocate bias buffer
BNNS Convolution Create: input data type is not supported
BNNS Convolution Create: output data type is not supported
BNNS Convolution Create: weight data type is not supported
BNNS Convolution Create: int8/uint8 output supported only with int8/uint8 inputs and weights
BNNS Convolution Create: int16/uint16 output is not supported
BNNS Convolution Create: uint32 output is not supported
BNNS Convolution Create: convolution doesn't support indexed weights
BNNS Convolution Create: failed to allocate memory
BNNS Convolution Create: failed to allocate weights descriptor
BNNS Convolution Create: convert weights
BNNS Convolution Create: failed to prepare blocks
BNNS Convolution Create: failed to create compute blocks
BNNS Grouped Convolution apply: groups should not be used if groups <= 1
BNNS Grouped Convolution apply: descriptor check failed
BNNS Grouped Convolution apply: weights are null 
BNNS Grouped Convolution apply: failed to create context
BNNS Grouped Convolution apply: unsupported convolution filter type 
BNNS Convolution Apply Backward:  filter is NULL
BNNS Convolution Apply Backward: out_delta is NULL
BNNS Convolution weight delta: malloc failed
BNNS Convolutions Apply: unexpected output data type
BNNS Convolution Backward: out_delta descriptor memory layout is not contiguous
BNNS Convolution bias delta: upconverting tensors failed
BNNS Convolution: malloc activation_grad failed
BNNS Grouped Convolution: input descriptor does not represent contiguous memory
BNNS Grouped Convolution: in_stride [%zu] and input memory size [%zu] do not match
BNNS Grouped Convolution: output descriptor does not represent contiguous memory
BNNS Grouped Convolution: out_stride [%zu] and output memory size [%zu] do not match
BNNS Grouped Convolution: weight descriptor does not represent contiguous memory
BNNS Grouped Convolution: bias descriptor does not represent contiguous memory
Convolution backward: in_delta, weights_delta, bias_delta descriptor pointers are all NULL
BNNS Convolution Backward: in_delta descriptor memory layout is not contiguous
BNNS Convolution Backward: weights_delta descriptor memory layout is not contiguous
BNNS Convolution Backward: bias_delta descriptor memory layout is not contiguous
BNNS Convolution Backward: out_stride and out_delta_stride do not match
BNNS Convolution Backward: out_delta_stride [%zu] does not match out_delta descriptor memory size [%zu]
BNNS Convolution Backward: in_stride and in_delta_stride do not match
BNNS Convolution Backward: in_delta_stride [%zu] does not match in_delta descriptor memory size [%zu]
Downsampling not supported for dimension > 2.
BNNS Resize: downsampling has no support for 3 or more dimensions.
BNNS Resize: downsampling has no support for 3 or higher dimension.
BNNS Resize: input_delta has %zu dimensions but must match input that has %zu
BNNS Resize: output_delta has %zu dimensions but must match output that has %zu
BNNS Resize: input_delta size[%zu]=%zu but must match input size[%zu]=%zu
BNNS Resize: output_delta size[%zu]=%zu but must match output size[%zu]=%zu
BNNS Resize: Unsupported data type
BNNS Resize: Input and output have different number of dimensions
BNNS Resize: resize must be in same direction for all direction (request %d downsample and %d upsample)
BNNS Resize: Linear interpolation requires resize in at most two dimensions, but %d dimensions are resized
BNNS Resize: Unsupported interpolation method %d
D;3
tT7
v((|(
h}((
?)_DD)^
M)\DS)[LY)Z_)Y
e)Xhl)W
z)T 
*@<%*
4f*=
k*<|p*
