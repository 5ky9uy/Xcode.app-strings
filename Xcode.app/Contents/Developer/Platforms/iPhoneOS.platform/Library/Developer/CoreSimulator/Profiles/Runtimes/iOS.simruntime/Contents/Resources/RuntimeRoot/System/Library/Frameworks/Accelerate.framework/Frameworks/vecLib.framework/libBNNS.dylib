giP9giP9giP9giP9
giP9
BNNS SoftMax: sum result shouldn't be 0, something is wrong
BNNS BroadcastMatMul: Only supports Float32 data types
BNNS BroadcastMatMul: Unsupported layout inputA->layout=%x
BNNS BroadcastMatMul: Unsupported layout inputB->layout=%x
BNNS BroadcastMatMul: Unsupported layout output->layout=%x
BNNS BroadcastMatMul: Inconsistent tensor dimensions. A has dimension %u, B has dimension %u, C has dimension %u
BNNS BroadcastMatMul: inputA->stride[0] = %zu, but must be >= 1.
BNNS BroadcastMatMul: inputA->stride[%u] = %zu, but must be >= inputA->size[%u] * inputA->stride[%u] = %zu * %zu = %zu.
BNNS BroadcastMatMul: inputB->stride[0] = %zu, but must be >= 1.
BNNS BroadcastMatMul: inputB->stride[%u] = %zu, but must be >= inputB->size[%u] * inputB->stride[%u] = %zu * %zu = %zu.
BNNS BroadcastMatMul:output->stride[0] = %zu, but must be >= 1.
BNNS BroadcastMatMul:output->stride[%u] = %zu, but must be >= output->size[%u] * output->stride[%u] = %zu * %zu = %zu.
BNNS BroadcastMatMul: Require one matrix multiplication dimension in inputA to have stride 1. inputA->stride[%u]=%zu, inputA->stride[%u]=%zu
BNNS BroadcastMatMul: Require one matrix multiplication dimension in inputB to have stride 1. inputB->stride[%u]=%zu, inputB->stride[%u]=%zu
BNNS BroadcastMatMul: Require one matrix result dimension in output to have stride 1. output->stride[%u]=%zu, output->stride[%u]=%zu
BNNS BroadcastMatMul: inconsistent final dimensions A%s is %zux%zu, B%s is %zux%zu, C is %zux%zu
BNNS BroadcastMatMul: broadcasting dimensions do not match.inputA->size[%d]=%zu, inputB->size[%d]=%zu, output->size[%d]=%zu.
BNNS BroadcastMatMul: Requires workspace of size %ld, exceeds limit of size %d.
v16@?0Q8
%s: 
BNNS Convolution version2 error: minimum float w_pack weight packing value is 32
BNNS Dequantize: input and output can't be null
BNNS Dequantize: in place conversion not supported
BNNS Dequantize: only __fp16 output is supported
BNNS Dequantize: lut is null and needed for Indexed input type
BNNS Dequantize: input type not supported
BNNS Fully Connected Apply: allocation of work buffer failed
BNNS Fully Connected: Unexpected result, should be at least 1
BNNS Fully Connected: Unexpected result, at least 1 batch should remain
BNNS Fully Connected: Unexpected result, at least 1 output should remain
BNNS Fully Connected: Failed to fit in memory limit, using generic code
BNNS FUlly: allocation failed
BNNS Fully Connected: weights conversion buffer isn't allocated.
BNNS Fully Connected: inputs conversion buffer isn't allocated.
BNNS Fully Connected Backward: output delta is NULL
BNNS Fully Connected Backward: only float32 delta are supported
BNNS Fully Connected Backward: output delta layout is BNNSDataLayoutImageCHW
BNNS Fully Connected Backward: input delta layout is BNNSDataLayoutImageCHW
BNNS Fully Connected Backward: bias delta layout is BNNSDataLayoutImageCHW
BNNS Fully Connected Backward: invalid argument
BNNS Fully Connected Backward: Fusion of compute with activation is unsupported for given activation function
BNNS Fully Connected Backward: failed to allocate memory
BNNS Fully Connected Backward: failed to apply activation backward
BNNS Fully Connected Backward: user requested weights delta but didn't supply original inputs
BNNS Fully Connected Backward: only fp32 inputs/weights/outputs are supported for backward
BNNS Fully Connected Backward: weights sizes doesn't match weights delta sizes
BNNS Fully Connected Backward: no bias, bias delta is set to 0
BNNS Fully Connected Backward: only float32 bias are supported
BNNS Fully Connected Create: failed to allocated memory (%zu bytes))
Shouldn't call get_data_size with Indexded2 or Indexed4, switch to get_data_bits
BNNS Fully Connected: compute data type not supported
BNNS_FULLY: input doesn't support Indexed data type
BNNS_FULLY: output only support fp32 data type
BNNS Fully Connected: using generic code to convert data 1 element each time
BNNS Fully Connected: output data type not supported for conversion
BNNS Fully Connected: input must be a 1D array
BNNS Fully Connected: input descriptor is illegal
BNNS Fully Connected: output must be a 1D array
BNNS Fully Connected: output descriptor is illegal
BNNS Fully Connected: weights row size should match input vector size
BNNS Fully Connected: weights number of rows should match output vector size
BNNS Fully Connected: weights must be a 2D array
BNNS Fully Connected: weights descriptor is ilegal
BNNS Fully Connected: Pointer to weight data must be non-NULL
BNNS Fully Connected: Output data types supported in this version: Float32, Float16
BNNS Fully Connected: Bias data types supported in this version: Float32
BNNS Fully Connected: Float16 output isn't supported with bias or activation function
BNNS Fully Connected: Input/Weight data types supported in this version: Float32, Int8, Int16
BNNS Fully Connected: Data type of weights and input should match
BNNS Fully Connected: size computation wraparound -I %zu -O %zu
BNNS Fully Connected: bias size computation wraparound -O %zu bias data type %x
BNNS Fully Connected: Weight type not supported
malloc
BNNS Optimizer Error: layer_params is NULL
BNNS Optimizer Error: unsupported optimizer function
BNNS Optimizer Error: memory allocation failed
BNNS Optimizer: Failed to init filter
BNNS Loss Error: filter is NULL
BNNS Optimizer: parameter array pointer is NULL
BNNS Optimizer: gradient array pointer is NULL
BNNS Optimizer: accumulator array pointer is NULL
BNNS Optimizer: parameter pointer number %zu is NULL
BNNS Optimizer: gradient pointer number %zu is NULL
BNNS Optimizer: accumulator pointer number %zu is NULL
BNNS Optimizer Error in parameter %zu: Parameter and Gradient descriptors must have the same sizes and strides
BNNS Optimizer Error in parameter %zu: Parameter and Accumulator descriptors must have the same sizes and strides
BNNS Optimizer Error in parameter %zu: Parameter data type is not BNNSDataTypeFloat32
BNNS Optimizer Error in gradient %zu: Gradient data type is not BNNSDataTypeFloat32
BNNS Optimizer Error in accumulator %zu: Accumulator data type is not BNNSDataTypeFloat32
BNNS Optimizer Error in parameter %zu: parameter descriptor must be dense such that stride[0]=1 and for every N>0 stride[N]=stride[N-1]*size[N-1]
BNNS Optimizer: unsupported optimizer function
BNNS Optimizer Error: OptimizerAlgFields is NULL
BNNS unsupported sgd variant
BNNS Sparse Fully Connected: input must be a 1D array
BNNS Sparse Fully Connected: input descriptor is illegal
BNNS Sparse Fully Connected: output must be a 1D array
BNNS Sparse Fully Connected: output descriptor is illegal
BNNS Sparse Fully Connected: invalid matrix layout
BNNS Sparse Fully Connected: invalid block row count: %u
BNNS Sparse Fully Connected: invalid block col count: %u
BNNS Sparse Fully Connected: input data types supported in this version: Float16
BNNS Sparse Fully Connected: output data types supported in this version: Float32, Float16
BNNS Sparse Fully Connected: weights data types supported in this version: Float16
BNNS Sparse Fully Connected: unsupported matrix block size %u x %u
BNNS Sparse Fully Connected: Allocation of the setup structure failed
BNNS Loss Error: unsupported loss function
BNNS Loss Error: Input width is 0
BNNS Loss Error: Input must be BNNSDataTypeFloat32 type
BNNS Loss Error: Input must be of BNNSDataLayoutVector layout
BNNS Loss Error: Input must be dense. stride0 must be 1 or 0
BNNS Loss Error: unknown reduction function
BNNS Loss Error: output data type must be BNNSDataTypeFloat32
BNNS Loss Error: output size>1 is only allowed with reduction BNNSLossReductionNone.
BNNS Loss Error: o_desc.size[0]!=1 ,softmax cross entropy loss has a single output per batch sample when unreduced, therefore o_desc.size[0] must be 1, Notice o_desc.data must be large enough to contain batch_size results
BNNS Loss Error: o_desc.size[0]!=i_desc.size[0] ,sigmoid cross entropy loss, mse loss, huber loss and Yolo loss output width must match input width when output loss unreduced. Notice o_desc.data must be large enough to contain batch_size*input_width consecutive results, where input_width is i_desc.size[0]
BNNS Loss Error: anchors is NULL
BNNS Loss Error: number of grid rows is 0
BNNS Loss Error: number of grid columns is 0
BNNS Loss Error: number of anchor boxes is 0
BNNS Loss Error: anchor box size <= 5
BNNS Loss Error: object minimum iou is negative
BNNS Loss Error: no object maximum iou is negative
BNNS Loss Error: input descriptor width (%zu) different from expected width of grid_size*num_anchors*(5+num_+classes) (%zu)
BNNS Loss Error: label smoothing only supported for the following loss functions: BNNSLossFunctionSigmoidCrossEntropy, BNNSLossFunctionSoftmaxCrossEntropy
BNNS Loss: memory allocation failed
BNNS Loss Error: batch_size is 0
BNNS Loss Error: in is NULL
BNNS Loss Error: in_stride is smaller than i_desc.size[0]
BNNS Loss Error: labels is NULL
BNNS Loss Error: labels_stride is smaller than i_desc.size[0]
BNNS Loss Error: weights_size>0 but weights pointer is NULL
BNNS Loss Warning: weight_size==0 but weight pointer is not NULL. Weight pointer is ignored.
BNNS Loss Error: weight_size value must be 0,1 or batch_size for softmax cross entropy loss
BNNS Loss Error: weight_size value must be 0,1 or batch_size*input_width for sigmoid cross entropy, mse and huber
BNNS Loss Error: yolo weight_size value must be 0, use yolo specific weight factors during filter create
BNNS Loss Error: out is NULL
BNNS Loss Error: in_delta descriptor is ilegal
BNNS Loss Error: in_delta must be dense, such that stride[0] is 0 or 1
BNNS Yolo loss error: weight size must be 0
BNNS Loss: unsupported loss function
BNNS softmax cross entropy loss error: in pointer is NULL
BNNS softmax cross entropy loss error: in stride cannot be smaller than input width
BNNS softmax cross entropy loss error: out pointer is NULL
BNNS softmax cross entropy loss error: batch_size is 0
BNNS softmax cross entropy loss: malloc failed
BNNS softmax cross entropy loss error: weight size must be equal 0,1 or batch size
BNNS Loss Warning: reduction BNNSLossReductionWeightedMean sum of weights is zero
BNNS Loss Error: reduction BNNSLossReductionNonZeroWeightMean all weights are zero
BNNS sigmoid cross entropy loss error: in stride cannot be smaller than input width
BNNS sigmoid cross entropy loss: malloc failed
BNNS sigmoid cross entropy loss error: must be 0,1 or batch_size*input_width
BNNS MSE loss error: in stride cannot be smaller than input width
BNNS MSE loss: malloc failed
BNNS MSE loss error: must be 0,1 or batch_size*input_width
BNNS Huber loss error: in stride cannot be smaller than input width
BNNS Huber loss error: must be 0,1 or batch_size*input_width
BNNS Huber loss: malloc failed
BNNS Yolo loss error: filter is NULL
BNNS Yolo loss error: batch_size is 0
BNNS Yolo loss error: in pointer is NULL
BNNS Yolo loss error: input stride is smaller than input width
BNNS Yolo loss error: labels (ground truth) is NULL
BNNS Yolo loss error: ground truth labels stride is smaller than input width
BNNS Yolo loss error: input delta stride is smaller than input width
BNNS Yolo loss error: out pointer is NULL
BNNS Yolo loss error: out width (size[0]) must be 1 as yolo loss is always reduced
BNNS Yolo loss error: yolo redution type must be BNNSLossReductionSum
BNNS Yolo loss error: input descriptor width does not match grid_rows*grid_columns*anchors*(5+classes)
BNNS LOW MEM CONVOLUTION: Client needs to support weights ptr with low_mem
BNNS LOW MEM CONVOLUTION: malloc
BNNS LOW MEM CONVOLUTION: failed to create convolution context
BNNS LOW MEM CONVOLUTION: failed to divide work between convolution contexts to fit in memory
BNNS LOW MEM CONVOLUTION: failed to create single generic convolution
BNNS LOW MEM CONVOLUTION: invalid argument
BNNS LOW MEM CONVOLUTION: malloc
BNNS LOW MEM CONVOLUTION: Winograd weights memory doesn't fit in memory
BNNS LOW MEM CONVOLUTION: unable to create Winograd that fit in memory
BNNS LOW MEM CONVOLUTION: malloc failed
BNNS LOW MEM CONVOLUTION: failed to create winograd convolution
BNNS LOW MEM CONVOLUTION: incompatible BNNS_low_memory_context id
BNNS LOW MEM CONVOLUTION: convolution failed
BNNS LOW MEM CONVOLUTION: input aux buffer wasn't allocated
BNNS LOW MEM CONVOLUTION:: context type not supported
BNNS LOW MEM CONVOLUTION: apply convolution failed
failed to upconvert or copy weights
failed to allocate none generic format
BNNS BatchNorm Error: layer_params is NULL
BNNS BatchNorm Error: beta descriptor is illegal
BNNS BatchNorm Error: gamma descriptor is illegal
BNNS BatchNorm Error: input descriptor is illegal
BNNS BatchNorm Error: input descriptor must be dense such that stride[0]=1 and for every N>0 stride[N]=stride[N-1]*size[N-1]
BNNS BatchNorm Error: output descriptor is illegal
BNNS BatchNorm Error: output descriptor must be dense such that stride[0]=1 and for every N>0 stride[N]=stride[N-1]*size[N-1]
BNNS BatchNorm Error: input size (%zu) different from output size (%zu)
BNNS BatchNorm Error: gamma descriptor size[0] must be the same as number of input channels
BNNS BatchNorm Error: beta descriptor size[0] must be the same as number of input channels
BNNS BatchNorm Error: moving mean descriptor is illegal
BNNS BatchNorm Error: moving mean descriptor size[0] must be the same as number of input channels
BNNS BatchNorm Error: moving variance descriptor is illegal
BNNS BatchNorm Error: moving variance descriptor size[0] must be the same as number of input channels
BNNS BatchNorm Error: Input must be BNNSDataTypeFloat32 type
BNNS BatchNorm Error: Output must be BNNSDataTypeFloat32 type
BNNS BatchNorm Error: beta must be BNNSDataTypeFloat32 type
BNNS BatchNorm Error: gamma must be BNNSDataTypeFloat32 type
BNNS BatchNorm Error: moving mean must be BNNSDataTypeFloat32 type
BNNS BatchNorm Error: moving variance must be BNNSDataTypeFloat32 type
BNNS BatchNorm Warning: epsilon is zero, it may cause division by zero
BNNS BatchNorm Warning: momentum is zero
BNNS BatchNorm Error: momentum must be between 0 and 1
BNNS BatchNorm Error: memory allocation failed
BNNS BatchNorm: Failed to init filter
BNNS Fused Convolution and BatchNorm Error: convolution_layer_params is NULL
BNNS Fused Convolution and BatchNorm Error: batch_normalization_layer_params is NULL
BNNS Fused Convolution and BatchNorm Error: convolution output descriptor and batchnorm input descriptor must have the same sizes, strides and data types
BNNS Fused Convolution and BatchNorm Error: memory allocation failed
BNNS Fused Convolution and BatchNorm create filter failed
BNNS Fused Fully Connected and BatchNorm Error: fully_layer_params is NULL
BNNS Fused Fully Connected and BatchNorm Error: batch_normalization_layer_params is NULL
BNNS Fused Fully Connected and BatchNorm Error: fully connected output descriptor and batchnorm input descriptor must have the same sizes, strides and data types
BNNS Fused Fully Connected and BatchNorm Error: memory allocation failed
BNNS Fused Fully Connected and BatchNorm create filter failed
BNNS BatchNorm Error: filter is NULL
BNNS BatchNorm Error: wrong filter type, filter is not batch norm
BNNS BatchNorm: input pointer is NULL
BNNS BatchNorm: batch_size>1 and in_stride is 0
BNNS BatchNorm: output pointer is NULL
BNNS BatchNorm: batch_size>1 and out_stride is 0
BNNS BatchNorm: inference mode and moving_mean pointer is NULL
BNNS BatchNorm: malloc failed
BNNS BatchNorm Backward Error: filter is NULL
BNNS BatchNorm Error: batchnorm input delta and batchnorm input must have the same sizes, strides and data types
BNNS BatchNorm Error: batchnorm beta delta and batchnorm beta must have the same sizes, strides and data types
BNNS BatchNorm Error: batchnorm gamma delta and batchnorm gamma must have the same sizes, strides and data types
BNNS BatchNorm Error: batchnorm output delta and batchnorm output must have the same sizes, strides and data types
BNNS BatchNorm Backward Error: wrong filter type, filter is not batch norm
BNNS BatchNorm Backward Error: batchnorm beta delta pointer is NULL. backward computation of all active parameters must be done in a single function call. 
BNNS BatchNorm Backward Error: batchnorm gamma delta pointer is NULL. backward computation of all active parameters must be done in a single function call. 
BNNS BatchNorm Backward Error: out delta descriptor pointer is NULL
BNNS BatchNorm Backward Error: out delta descriptor data pointer is NULL
BNNS BatchNorm Backward Warning: in delta, beta delta and gamma delta pointers are NULL, nothing to do
BNNS BatchNorm Backward Error: cannot compute input delta because input delta data pointer is NULL
BNNS BatchNorm Backward Error: cannot compute beta delta because beta delta data pointer is NULL
BNNS BatchNorm Backward Error: cannot compute gamma delta because gamma delta data pointer is NULL
BNNS BatchNorm Backward Error: beta delta descriptor must be dense such that stride[0]=1 and for every N>0 stride[N]=stride[N-1]*size[N-1], and size[0] must be equal to number of input channels
BNNS BatchNorm Backward Error: gamma delta descriptor must be dense such that stride[0]=1 and for every N>0 stride[N]=stride[N-1]*size[N-1], and size[0] must be equal to number of input channels
BNNS BatchNorm Backward Error: output delta descriptor must be dense such that stride[0]=1 and for every N>0 stride[N]=stride[N-1]*size[N-1]
BNNS BatchNorm Backward Error: output delta size must be the same size of input
BNNS BatchNorm Backward Error: input delta descriptor must be dense such that stride[0]=1 and for every N>0 stride[N]=stride[N-1]*size[N-1]
BNNS BatchNorm Backward Error: input delta size must be the same size of input
BNNS BatchNorm Backward Error: input delta descriptor data type must be BNNSDataTypeFloat32
BNNS BatchNorm Backward Error: beta delta descriptor data type must be BNNSDataTypeFloat32
BNNS BatchNorm Backward Error: no internal storage of x_hat, make sure to run batchnorm forward with training flag enabled before running batchnorm backward
BNNS BatchNorm Backward Error: no internal storage of inverse variance, make sure to run batchnorm forward with training flag enabled before running batchnorm backward
BNNS BatchNorm Backward Error: cannot compute activation backward, output is NULL
BNNS BatchNorm Backward Error: Fusion of batchnorm with activation is unsupported for given activation function
BNNS Batchnorm Backward: failed to apply activation backward
BNNS CONVOLUTIONS VERSION2: only dense memory laout is supported
BNNS CONVOLUTIONS VERSION2: output bias or scale is not supported
BNNS CONVOLUTIONS VERSION2: integer engine does not support weight bias and scale
BNNS CONVOLUTIONS VERSION2: malloc failed
BNNS CONVOLUTIONS VERSION2: malloc failed
invalid argument
BNNS CONV: input batch stride doesn't make sense (%zu < %zu x %zu)
BNNS CONV: output batch stride doesn't make sense (%zu < %zu x %zu)
BNNS CONVOLUTIONS: malloc failed
BNNS Convolution: malloc failed
Tranposed Convolution currently does not support BNNSInternalFlagsUseLowMemConvolutions
Tranposed Convolution currently does not support dilation
Tranposed Convolution currently does not support weight packing
BNNS malloc failed
malloc failed
BNNS Fused Filter Error: number_of_fused_filters is not 2. currently supporting only 2 fused filters
BNNS Fused Filter Error: filter_type is NULL
BNNS Fused Filter Error: layer_params is NULL
BNNS Fused Filter Error: currently supporting first filter of BNNSConvolution type or BNNSFullyConnected type only
BNNS Fused Filter Error: currently supporting second filter of BNNSBatchNorm type only
BNNS Fused Filter Error: layer_params[0] is NULL
BNNS Fused Filter Error: layer_params[1] is NULL
BNNS Fused Filter Error: convolution activation must be BNNSActivationFunctionIdentity
BNNS Fused Filter Error: fully connected activation must be BNNSActivationFunctionIdentity
BNNS Fused Filter Error: unknown fused filter type
BNNS Fused Filter Error: filter is NULL
BNNS Fused Filter Warning: Batch size is 0, nothing to do
BNNS Fused Filter Error: wrong filter, this function should only be used for a filter created with BNNSFilterCreateFusedLayer
BNNS Fused Filter Error: first filter apply failed
Invalid filter
BNNS BatchNorm Backward Warning: Batch size is 0, nothing to do
BNNS BatchNorm Backward Error: wrong filter, this function should only be used for a filter created with BNNSFilterCreateLayerBatchNorm
BNNS Fused Filter Backward Error: filter is NULL
BNNS Fused Filter Backward Warning: Batch size is 0, nothing to do
BNNS Fused Filter Backward Error: delta_parameters is NULL
BNNS Fused Filter Backward Error: wrong filter, this function should only be used for a filter created with BNNSFilterCreateFusedLayer
BNNS Fused Filter Backward Error: Weight delta pointer is NULL. backward computation of all active parameters must be done in a single function call. 
BNNS Fused Filter Backward Error: bias delta pointer is NULL. backward computation of all active parameters must be done in a single function call. 
BNNS Fused Filter Backward Error: batchnorm beta delta pointer is NULL. backward computation of all active parameters must be done in a single function call. 
BNNS Fused Filter Backward Error: batchnorm gamma delta pointer is NULL. backward computation of all active parameters must be done in a single function call. 
BNNS Fused Filter Backward Error: batchnorm backward failed
BNNSDirectApplyConvolutionBatch not supported 
BNNSDirectApplyTransposedConvolutionBatch not supported 
BNNSDirectApplyFullyConnectedBatch not supported 
BNNSDirectApplyPoolingBatch not supported 
BNNSDirectApplyLossBatch not supported 
Loss filter apply must be called with BNNSLossFilterApplyBatch
Optimizer filter apply must be called with BNNSOptimizerFilterApply
Batchnorm filter apply must be called with BNNSBatchNormFilterApplyBatch
Fused filter apply must be called with BNNSFusedFilterApplyBatch
invalid filter
BNNS DESTROY: invalid filter
input type %s not supported!!!
compute type %s not supported!!!
int8
int16
int32
uint8
uint16
uint32
indexed8
indexed4
indexed2
BNNS GRU Fused Gates: only arm64 version is supported
BNNS SoftMax: sum result shouldn't be 0
BNNSActivationFunctionIntegerLinearSaturate isn't supported in apply_activation_backward
BNNSActivationFunctionIntegerLinearSaturatePerChannel isn't supported in apply_activation_backward
unsupported activation backward
BNNS Activation: number of memory descriptor is lower than expected
BNNS Activation: claiming pre allocated memory failed
BNNS Activation: memory allocation failed
BNNS Activation: Failed to init filter
BNNS Activation Backward: cannot compute backward if both forward pass output and forward pass input are NULL
BNNS Activation Backward: invalid argument
BNNS Activation Backward: output delta data type isn't fp32
BNNS Activation Backward: preallocated memory isn't supported
BNNS Activation Backward: Weights Delta not supported
BNNS Activation Backward: Bias Delta not supported
BNNS Activation Backward: Input Delta type isn't fp32
BNNS Activation: in-place activation layer is allowed only for output types with the same or smaller storage size
BNNS Activation backward: softmax backward does not support stride0!=1
BNNS Activation: invalid argument
BNNS Activation: unsupported types for conversion
BNNS Activation: Apply failed
BNNS Activation Init: input descriptor is ilegal
BNNS Activation Init: output descriptor is ilegal
BNNS Activation Init: layout doesn't match
BNNS Activation Sofmax: only 1D input is supported
BNNS Activation Init: only 3D conversoins are supported
BNNS Activation Init: dim %zu input size %zu != %zu output size
BNNS Activation Init: alpha can't be +/-inf or Nan for Gumbel or Gumbel Max
BNNS Activation Init: beta can't be +/-inf or Nan or zero or negative for Gumbel or Gumbel Max
BNNS Activation Init: invalid activation function
BNNS Activation Init: supported  input data types: float32
BNNS Activation Init: supported output data types: float32
BNNS Activation Init: unsupported types for conversion
BNNS Activation error: NULL input/output pointer
BNNS Activation error: number of input channels must be equal to number of output channels
BNNS Activation error: input/output/activation pointers cannot be NULL
BNNS Activation error: SoftMax activation require input and output images to be dense such that stride0=1, stride1=width, stride2=width*height
BNNS POOLING: input batch stride doesn't make sense (%zu < %zu x %zu)
Pooling layer filter running slow path: stride=%zu,%zu kernel=%zu,%zu
case not implemented
BNNS Pooling Apply: allocation of work buffer failed
BNNS Pooling Backward: cannot run backward without output delta
BNNS Pooling Backward: only float32 output delta are supported
BNNS Pooling Backward: only float32 input delta are supported
BNNS Pooling Backward: only float32 bias delta are supported
BNNS Pooling Backward: unsupported pooling function
BNNS Pooling Backward: invalid argument
BNNS Pooling Backward: Fusion of compute with activation is unsupported for given activation function
BNNS Pooling Backward: failed to allocate memory
BNNS Pooling Backward: failed to apply activation backward
BNNS POOLING: input must be a 3D array
BNNS POOLING: input descriptor is ilegal
BNNS POOLING: output must be a 3D array
BNNS POOLING: output descriptor is ilegal
BNNS POOLING: input/output channel counts do not match
BNNS POOLING: input/output types do not match
BNNS POOLING: invalid kernel dimensions, should be greater than 0
BNNS POOLING: optimized code supports kernel width/height up to 16
BNNS_POOLING: input size computation wraparound input (%zu x %zu), output (%zu x %zu), stride (%zu x %zu), padding (%zu x %zu)
BNNS_POOLING: output(%zu x %zu) is larger than input(%zu x %zu) with padding(%zu x %zu).
 stride (%zu x %zu)
BNNS_POOLING: invalid pooling function
BNNS_POOLING: supported input/output data types: float32 float16
BNNS_POOLING: slow path: stride not in {1,2}
BNNS_POOLING: slow path: kernel size not in {2,3,4}
BNNS POOLING: Internal Memory size %zu doesn't match expected %d
BNNS Pooling: claiming pre allocated memory failed
memory allocation
BNNS Pooling Backward: only float32 delta are supported
BNNS Pooling Backward: wrong pooling function called
BNNS CONV: bias is not 0, activation function is not identity
BNNS CONV: convolution kernel is too large
BNNS CONV: invalid stride/padding
BNNS CONV: float32 only
v8@?0
hw.physicalcpu
BNNSActivationFunctionIntegerLinearSaturate isn't supported in apply_bias_and_activation
BNNSActivationFunctionIntegerLinearSaturatePerChannel isn't supported in apply_bias_and_activation
allocation failed, size=%zu
allocation failed, size=%zu, align=%zu
BNNS Fully Connected: unexpected data type, failing
BNNS Fully Connected: Data type not supprted, fail
invalid layer data type: %u
BNNS: layout not supported
BNNS: active dimension must be greater than 0
BNNS: dimension %zu stride %zu is lower then previous dimension actual size %zu * %zu (size*stride)
BNNS POOLING: memory usage exceeded capacity
BNNS PreAllocated Memory: memory usage exceeded capacity
BNNS PreAllocated Memory: failed to claim scratch memory
BNNS Create Convolution Winograd: malloc failed
memory allocation failed
weight packing must be at least 32 and a power of 2
BNNS Convolution version2 error: minimum int32_t w_pack weight packing value is 32
BNNS Dequantize: shouldn't have reached fp16 convert path
Transposed convolution currently only supports BNNSDataTypeFloat32 data type
BNNS_TRANSPOSE_CONV: forward pass check (swapping input and output)  - input size computation wraparound input (%zu x %zu), output (%zu x %zu), stride (%zu x %zu), padding (%zu x %zu)
BNNS_TRANSPOSE_CONV: forward pass check (swapping input and output) - output(%zu x %zu x %zu) is larger than input(%zu x %zu x %zu) with padding(%zu x %zu).
 kernel (%zu x %zu) and stride (%zu x %zu)
Filter is NULL
transposed convolution apply failed
failed to allocate weights in apply
NULL weight object
dst weight size too small
NULL weight array
BNNS Convolution Backward: cannot run backward without output delta
BNNS Convolution Backward: only float32 delta are supported
BNNS Convolution dilation is not supported backward
BNNS Convolution weight packing is not supported backward
BNNS Convolution Backward: BNNSFlagsUseClientPtr must be enabled during training
BNNS Convolution Backward: invalid argument
BNNS Convolution Backward: Fusion of compute with activation is unsupported for given activation function
BNNS Convolution Backward: failed to apply activation backward
BNNS Conv: Unsupported weight format for Input delta compute
BNNS Convolution Backward: could not create transposed convolution filter
BNNS Convolution Backward: could not create convolution filter
transposed convolution input delta backward failed
transposed convolution parameter error
transposed convolution backward failed
BNNS Create Layer Convolution: failed
BNNS Create Layer Convolution: failed to create wrapper filter
BNNS CONV: incompatible numbers of channels between images and convolution parameters
BNNS CONV: unsupported weight format
BNNS Conv: input must be a 3D array
BNNS Conv: input descriptor is ilegal
BNNS Conv: output must be a 3D array
BNNS Conv: output descriptor is ilegal
BNNS Conv: weights descriptor is ilegal
BNNS CONV: not allowed to delay allocation to apply if weights ptr isn't maintained by Client
BNNS_CONV: input size computation wraparound input (%zu x %zu), output (%zu x %zu), stride (%zu x %zu), padding (%zu x %zu)
BNNS_CONV: output(%zu x %zu x %zu) is larger than input(%zu x %zu x %zu) with padding(%zu x %zu).
 kernel (%zu x %zu) and stride (%zu x %zu)
BNNS CONV: failled to allocate weights buffer
BNNS CONV: input stack data type is not Float32 or Float16 or int8 or uint8
BNNS CONV: output stack data type is not Float32 or Float16 or int8 or uint8
BNNS CONV: int8 input stack only supported with int8 output stack and int8 weights
BNNS CONV: uint8 input stack only supported with uint8 output stack and int8 weights
int8 convolution can't have bias or activation other than BNNSActivationFunctionIntegerLinearSaturate
BNNS CONV: convolution doesn't support indexed weights
BNNS Convolution Create: failed to allocate memory
failed to allocate weights descriptor
convert weights
BNNS Convolution Create: failed to prepare blocks
BNNS Convolution Create: failed to create compute blocks
transposed convolution input manipulation failed
transposed convolution input manipulation failed - nothing to do, should not have allocated dst_input buffer
transposed convolution backward dy padding failed
