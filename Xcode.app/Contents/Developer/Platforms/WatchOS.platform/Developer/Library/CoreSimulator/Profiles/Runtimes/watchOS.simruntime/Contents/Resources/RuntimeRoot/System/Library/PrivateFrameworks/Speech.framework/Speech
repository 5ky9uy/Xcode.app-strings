MbP?
@mcpl
@(#)PROGRAM:Speech  PROJECT:SpeechRecognition-1
init
statusCode
intValue
headers
enumerateKeysAndObjectsUsingBlock:
_responseWithCFURLResponse:
result
searchType
isEqualToString:
JSONObjectWithData:options:error:
description
alloc
dataWithJSONObject:options:error:
initWithData:encoding:
stringByAppendingString:
initWithVoiceSearchResult:
.cxx_destruct
response
data
_response
_data
_searchType
copy
UUID
UUIDString
taskHint
_startedConnectionWithLanguageCode:delegate:taskHint:requestIdentifier:
stopSpeechWithOptions:
cancelSpeech
peakPower
averagePower
addRecordedSpeechSampleData:
array
string
countByEnumeratingWithState:objects:count:
removeSpaceBefore
removeSpaceAfter
appendString:
text
length
startTime
silenceStartTime
endTime
confidenceScore
_initWithSubstring:range:timestamp:duration:confidence:alternativeSubstrings:
addObject:
_initWithSegments:formattedString:
recognition
phrases
rawRecognition
_initWithBestTranscription:rawTranscription:final:
transcriptionsWithTokens:
recognizedResultFromPackage:
isEqual:
class
self
performSelector:
performSelector:withObject:
performSelector:withObject:withObject:
isProxy
isKindOfClass:
isMemberOfClass:
conformsToProtocol:
respondsToSelector:
retain
release
autorelease
retainCount
zone
hash
superclass
debugDescription
dictationConnectionSpeechRecordingWillBegin:
dictationConnectionSpeechRecordingDidBegin:
dictationConnectionSpeechRecordingDidEnd:
dictationConnectionSpeechRecordingDidCancel:
dictationConnection:speechRecordingDidFail:
dictationConnection:speechRecognitionDidFail:
dictationConnection:didRecognizePhrases:languageModel:correctionIdentifier:
dictationConnection:didRecognizeTokens:languageModel:
dictationConnection:didProcessAudioDuration:
dictationConnectionSpeechRecognitionDidSucceed:
dictationConnection:didRecognizeTranscriptionObjects:languageModel:
dictationConnnectionDidChangeAvailability:
dictationConnection:didFinishWritingAudioFile:error:
dictationConnection:didReceiveSearchResults:recognizedText:stable:final:
dictationConnection:didRecognizePackage:
stopSpeech
_initWithRequest:queue:languageCode:taskHint:
state
finish
cancel
_taskHint
isFinishing
isCancelled
error
requestIdentifier
_dictationConnection
_externalQueue
_languageCode
_request
_internalQueue
_completed
_running
_finishing
_cancelled
_error
_requestIdentifier
isFinal
addOperationWithBlock:
errorWithDomain:code:userInfo:
_fireResultHandlerWithResult:error:
_finalizeResultHandler
shouldReportPartialResults
_initWithRequest:queue:languageCode:taskHint:resultHandler:
_resultHandler
_hasFiredFinalResult
_searchRequest
speechRecognitionTaskWasCancelled:
speechRecognitionTask:didFinishSuccessfully:
_tellDelegateDidFinishSuccessfully:
speechRecognitionTaskFinishedReadingAudio:
speechRecognitionTask:didReceiveSearchResults:recognizedText:stable:final:
bestTranscription
formattedString
speechRecognitionTask:didFinishRecognition:
speechRecognitionTask:didHypothesizeTranscription:
speechRecognitionTask:didProcessAudioDuration:
numberWithInteger:
setObject:forKey:
_initWithRequest:queue:languageCode:taskHint:delegate:
_delegate
_recognitionResultToReportAfterFinalSearchResults
_selfReference
_waitForVoiceSearchResult
_hasSentRealSearchResults
interpretations
firstObject
tokens
enumerateObjectsUsingBlock:
stringByAppendingFormat:
arrayWithObjects:count:
setWithArray:
decodeObjectOfClasses:forKey:
decodeBoolForKey:
encodeObject:forKey:
encodeBool:forKey:
segments
alternativeSubstrings
mutableCopy
substring
insertObject:atIndex:
subarrayWithRange:
addObjectsFromArray:
substringRange
removeObjectAtIndex:
timestamp
duration
confidence
count
stringByReplacingCharactersInRange:withString:
expandTranscription:
supportsSecureCoding
copyWithZone:
encodeWithCoder:
initWithCoder:
_initWithBestTranscription:final:
transcriptions
rawTranscriptions
rawTranscription
_transcriptions
_rawTranscriptions
_final
_bestTranscription
_rawTranscription
setWithCapacity:
localeWithLocaleIdentifier:
currentLocale
initWithLocale:
localeIdentifier
stringByReplacingOccurrencesOfString:withString:
containsObject:
objectForKey:
stringWithFormat:
mainQueue
setDelegate:
beginAvailabilityMonitoring
defaultCenter
_informDelegateOfPreferencesChange
addObserverForName:object:queue:usingBlock:
setDelegate:queue:
endSession
cancelAvailabilityMonitoring
removeObserver:
dealloc
dictationIsEnabled
dictationIsAvailableForLanguage:
forcedOfflineDictationIsAvailableForLanguage:
requestOfflineDictationSupportForLanguage:completion:
getForcedOfflineDictationSupportedLanguagesWithCompletion:
raise:format:
sendEngagementFeedback:voiceQueryIdentifier:
_informDelegateOfAvailabilityChange
speechRecognizer:availabilityDidChange:
isAvailable
initialize
supportedLocales
authorizationStatus
requestAuthorization:
_fetchSupportedForcedOfflineLocalesWithCompletion:
callObserver:callChanged:
_isAvailableForForcedOfflineRecognition
_requestOfflineDictationSupportWithCompletion:
_isInternalTaskHint:
recognitionTaskWithRequest:resultHandler:
recognitionTaskWithRequest:delegate:
_sendEngagementFeedback:requestIdentifier:
setQueue:
locale
delegate
defaultTaskHint
setDefaultTaskHint:
queue
_callObserver
_facetimeObserver
_foregroundObserver
_preferencesObserver
_locale
_defaultTaskHint
_queue
numberWithDouble:
numberWithFloat:
encodeInteger:forKey:
encodeDouble:forKey:
decodeObjectOfClass:forKey:
decodeIntegerForKey:
decodeDoubleForKey:
_substring
_confidence
_alternativeSubstrings
_substringRange
_timestamp
_duration
searchTypes
setSearchTypes:
headerFields
setHeaderFields:
queryParameters
setQueryParameters:
_searchTypes
_headerFields
_queryParameters
mainBundle
bundleIdentifier
setApplicationName:
infoDictionary
setApplicationVersion:
setInlineItemList:
setRequestIdentifier:
setVoiceTriggerEventInfo:
setMaximumRecognitionDuration:
setDetectUtterances:
setVoiceSearchTypeOptions:
setVoiceSearchQueryParameters:
setVoiceSearchHeaderFields:
setKeyboardType:
setTaskHint:
setInteractionIdentifier:
setForceOfflineRecognition:
setRecognitionOverrides:
setModelOverrideURL:
initWithActivationEvent:
automaticallyEndpoint
setUseAutomaticEndpointing:
setUseStreamingDictation:
processInfo
systemUptime
setActivationEventTime:
_setSearchRequests:
_searchRequests
_powerMeteringAvailable
_dictationOptionsWithTaskHint:requestIdentifier:
_speechRequestOptions
_maximumRecognitionDuration
_setMaximumRecognitionDuration:
_forceOfflineRecognition
_setForceOfflineRecognition:
_setSearchRequest:
_voiceTriggerEventInfo
_setVoiceTriggerEventInfo:
_recognitionOverrides
_setRecognitionOverrides:
_modelOverrideURL
_setModelOverrideURL:
setShouldReportPartialResults:
contextualStrings
setContextualStrings:
interactionIdentifier
detectMultipleUtterances
setDetectMultipleUtterances:
_shouldReportPartialResults
_detectMultipleUtterances
_contextualStrings
_interactionIdentifier
_maxiumRecognitionDuration
setFieldLabel:
assetWithURL:
caseInsensitiveCompare:
setKeyboardIdentifier:
setOriginalAudioFileURL:
tracksWithMediaType:
formatDescriptions
startRecordedAudioDictationWithOptions:forLanguage:narrowband:
assetReaderWithAsset:error:
numberWithUnsignedLong:
numberWithUnsignedInteger:
numberWithInt:
numberWithBool:
dictionaryWithObjects:forKeys:count:
assetReaderAudioMixOutputWithAudioTracks:audioSettings:
canAddOutput:
addOutput:
startReading
copyNextSampleBuffer
initWithURL:
_URL
initWithStreamDescription:
stringWithUTF8String:
currentHandler
handleFailureInMethod:object:file:lineNumber:description:
startRecordedAudioDictationWithOptions:forLanguage:
_appendAudioPCMBuffer:
_appendAudioSampleBuffer:
_endAudio
nativeAudioFormat
format
_drainAndClearAudioConverter
int16ChannelData
frameLength
dataWithBytes:length:
_convertAndFeedPCMBuffer:
initWithPCMFormat:frameCapacity:
setFrameLength:
convertToBuffer:error:withInputFromBlock:
inputFormat
initFromFormat:toFormat:
setSampleRateConverterQuality:
mutableAudioBufferList
appendAudioPCMBuffer:
appendAudioSampleBuffer:
endAudio
_bufferDelegate
_queuedBuffers
_converter
_audioEnded
initWithLength:
mutableBytes
_formattedString
_segments
v16@?0@"NSString"4@"NSString"8^B12
response
T@"NSHTTPURLResponse",R,N,V_response
data
T@"NSData",R,N,V_data
searchType
Ti,R,N,V_searchType
com.apple.Speech.Task.Internal
v4@?0
hash
TI,R
superclass
T#,R
description
T@"NSString",R,C
debugDescription
_taskHint
Ti,R,N,V_taskHint
requestIdentifier
T@"NSString",R,C,N,V_requestIdentifier
state
Ti,R,N
finishing
TB,R,N,GisFinishing,V_finishing
cancelled
TB,R,N,GisCancelled,V_cancelled
error
T@"NSError",R,C,N,V_error
v12@?0@"SFSpeechRecognitionResult"4@"NSError"8
v16@?0@"AFSpeechToken"4I8^B12
v16@?0@"AFSpeechInterpretation"4I8^B12
_bestTranscription
_rawTranscription
_final
 final=%d, bestTranscription=%@
v16@?0@"NSString"4I8^B12
v16@?0@"SFTranscriptionSegment"4I8^B12
supportsSecureCoding
TB,R
rawTranscription
T@"SFTranscription",R,C,N,V_rawTranscription
rawTranscriptions
T@"NSArray",R,C,N
bestTranscription
T@"SFTranscription",R,C,N,V_bestTranscription
transcriptions
final
TB,R,N,GisFinal,V_final
hi-IN
Cannot make recognizer for %@. Supported locale identifiers are %@
%@-%@
v8@?0@"NSNotification"4
v8@?0@"NSArray"4
Result handler must be non-null
%@ queue must not be nil
_availableForForcedOfflineRecognition
TB,R,N,G_isAvailableForForcedOfflineRecognition
available
TB,R,N,GisAvailable
locale
T@"NSLocale",R,C,N,V_locale
delegate
T@"<SFSpeechRecognizerDelegate>",W,N,V_delegate
defaultTaskHint
Ti,N,V_defaultTaskHint
queue
T@"NSOperationQueue",&,N,V_queue
, substringRange=%@, timestamp=%@, duration=%@, confidence=%@, substring=%@, alternativeSubstrings=%@
_substring
_substringRange.location
_substringRange.length
_timestamp
_duration
_confidence
_alternativeSubstrings
substring
T@"NSString",R,C,N,V_substring
substringRange
T{_NSRange=II},R,N,V_substringRange
timestamp
Td,R,N,V_timestamp
duration
Td,R,N,V_duration
confidence
Tf,R,N,V_confidence
alternativeSubstrings
T@"NSArray",R,N,V_alternativeSubstrings
SFSpeechPreecordedRequest
hi-IN-translit
Translit
searchTypes
Ti,N,V_searchTypes
headerFields
T@"NSDictionary",C,N,V_headerFields
queryParameters
T@"NSDictionary",C,N,V_queryParameters
_searchRequest
T@"_SFSearchRequest",&,N,G_searchRequest,S_setSearchRequest:,V_searchRequest
detectMultipleUtterances
TB,N,V_detectMultipleUtterances
_forceOfflineRecognition
TB,N,G_forceOfflineRecognition,S_setForceOfflineRecognition:,V_forceOfflineRecognition
_voiceTriggerEventInfo
T@"NSDictionary",&,N,G_voiceTriggerEventInfo,S_setVoiceTriggerEventInfo:,V_voiceTriggerEventInfo
_maxiumRecognitionDuration
Td,N,G_maximumRecognitionDuration,S_setMaximumRecognitionDuration:,V_maxiumRecognitionDuration
_recognitionOverrides
T@"NSDictionary",&,N,G_recognitionOverrides,S_setRecognitionOverrides:,V_recognitionOverrides
_modelOverrideURL
T@"NSURL",&,N,G_modelOverrideURL,S_setModelOverrideURL:,V_modelOverrideURL
taskHint
Ti,N,V_taskHint
shouldReportPartialResults
TB,N,V_shouldReportPartialResults
contextualStrings
T@"NSArray",C,N,V_contextualStrings
interactionIdentifier
T@"NSString",C,N,V_interactionIdentifier
Use -[SFSpeechURLRecognitionRequest initWithURL:]
Could not add output for %@
B4@?0
T@"NSURL",R,C,N,V_URL
com.apple.SFSpeechAudioBufferRecognitionRequest
/BuildRoot/Library/Caches/com.apple.xbs/Sources/SpeechFramework_Sim/SpeechFramework-67/SpeechRecognition/SFSpeechRecognitionRequest.m
<Unknown File>
%@ cannot be re-used
Invalid audio format
@"AVAudioBuffer"12@?0I4^i8
Could not drain converter %@
Could not run audio converter %@
nativeAudioFormat
T@"AVAudioFormat",R,N
CMBlockBufferCopyDataBytes could not copy data: %d
, formattedString=%@, segments=%@
_segments
_formattedString
formattedString
T@"NSString",R,C,N,V_formattedString
segments
T@"NSArray",R,C,N,V_segments
_SFSearchResult
SFSpeechRecognitionTask
AFDictationDelegate
NSObject
SFSpeechRecognitionBufferDelegate
_SFSpeechRecognitionBlockTask
_SFSpeechRecognitionDelegateTask
SFSpeechRecognitionResult
NSCopying
NSSecureCoding
NSCoding
SFSpeechRecognizer
CXCallObserverDelegate
SFTranscriptionSegment
_SFSearchRequest
SFSpeechRecognitionRequest
SFSpeechURLRecognitionRequest
SFSpeechAudioBufferRecognitionRequest
SFTranscription
@12@0:4@8
@8@0:4
v8@0:4
i8@0:4
@"NSHTTPURLResponse"
@"NSData"
B12@0:4@8
#8@0:4
@12@0:4:8
@16@0:4:8@12
@20@0:4:8@12@16
B8@0:4
B12@0:4#8
B12@0:4:8
Vv8@0:4
I8@0:4
^{_NSZone=}8@0:4
B12@0:4@"Protocol"8
@"NSString"8@0:4
v12@0:4@8
v16@0:4@8@12
v24@0:4@8@12@16@20
v20@0:4@8@12@16
v20@0:4@8d12
v28@0:4@8@12@16B20B24
v12@0:4@"AFDictationConnection"8
v16@0:4@"AFDictationConnection"8@"NSError"12
v24@0:4@"AFDictationConnection"8@"NSArray"12@"NSString"16@20
v20@0:4@"AFDictationConnection"8@"NSArray"12@"NSString"16
v20@0:4@"AFDictationConnection"8d12
v20@0:4@"AFDictationConnection"8@"NSFileHandle"12@"NSError"16
v28@0:4@"AFDictationConnection"8@"NSArray"12@"NSString"16B20B24
v16@0:4@"AFDictationConnection"8@"AFSpeechPackage"12
v12@0:4@"NSData"8
@24@0:4@8@12@16i20
f8@0:4
@"AFDictationConnection"
@"NSOperationQueue"
@"NSString"
@"SFSpeechRecognitionRequest"
@"NSObject<OS_dispatch_queue>"
@"NSError"
@28@0:4@8@12@16i20@?24
@28@0:4@8@12@16i20@24
v12@0:4B8
@"<_SFSpeechRecognitionTaskDelegatePrivate>"
@"SFSpeechRecognitionResult"
@"_SFSpeechRecognitionDelegateTask"
@12@0:4^{_NSZone=}8
v12@0:4@"NSCoder"8
@12@0:4@"NSCoder"8
@16@0:4@8B12
@20@0:4@8@12B16
@"NSArray"
@"SFTranscription"
v12@0:4@?8
v16@0:4@"CXCallObserver"8@"CXCall"12
B12@0:4i8
@16@0:4@8@?12
@16@0:4@8@12
v16@0:4i8@12
v12@0:4i8
@"CXCallObserver"
@"<NSObject>"
@"NSLocale"
@"<SFSpeechRecognizerDelegate>"
@44@0:4@8{_NSRange=II}12d20d28f36@40
{_NSRange=II}8@0:4
d8@0:4
{_NSRange="location"I"length"I}
@"NSDictionary"
@24@0:4@8@12i16@20
@16@0:4i8@12
v16@0:4d8
@"_SFSearchRequest"
@"NSURL"
v12@0:4^{opaqueCMSampleBuffer=}8
@"<SFSpeechRecognitionBufferDelegate>"
@"NSMutableArray"
@"AVAudioConverter"
zPLR
zPLR
zPLR
zPLR
