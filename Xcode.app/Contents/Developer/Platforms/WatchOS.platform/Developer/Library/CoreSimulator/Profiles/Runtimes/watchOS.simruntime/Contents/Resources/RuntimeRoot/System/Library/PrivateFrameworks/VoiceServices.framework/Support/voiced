?.dat
@mcpl
error setting SRC quality: %d
%s: error %d %s level metering
Error AudioUnitSetProperty _floatConverter %d
Error AudioUnitSetProperty _integerConverter %d
Error AudioComponentInstanceNew _voiceBoostUnit %d
Error AudioUnitSetProperty _voiceBoostUnit, kAudioUnitProperty_MaximumFramesPerSlice %d
Error AudioUnitSetProperty _voiceBoostUnit, kAudioUnitProperty_StreamFormat, kAudioUnitScope_Input %d
Error AudioUnitSetProperty _voiceBoostUnit, kAudioUnitProperty_StreamFormat, kAudioUnitScope_Output,  %d
Error AudioUnitInitialize _voiceBoostUnit %d
Error AudioUnitSetParameter %d
Error AudioConverterConvertComplexBuffer _floatConverter %d
Error AudioUnitProcess _voiceBoostUnit %d
Error AudioConverterConvertComplexBuffer _integerConverter %d
Failed ve_ttsGetLipSyncInfo 0x%x
Error ve_ttsInitialize: 0x%x
Error ve_ttsGetLanguageList: 0x%x
Error ve_ttsGetVoiceList: 0x%x
Error ve_ttsOpen 0x%x
Error ve_ttsSetParamList 0x%x
Error ve_ttsClose 0x%x
Error ve_ttsUnInitialize 0x%x
Error ve_ttsResourceLoad 0x%x, path: '%@', type: '%@'
Success ve_ttsResourceLoad, path: '%@', type: '%@'
Error ve_ttsSetOutDevice 0x%x
Error ve_ttsProcessText2Speech 0x%x
Error ve_ttsStop 0x%x
Error getting components in voice path, %@
Engine preheating latency: %.3f
Unable to find broker file
%s: error converting disambiguation context
%s: allowing recognition start
%s: recognition requested when busy
%s: releasing active client to begin
%s: client requested cancellation of active recognition
%s: client requested cancellation of queued recognition
cancelling recognition
released from holding.. beginning now.
Running with background thread priority
couldn't create instance for client port - cancelling
couldn't open audio input file for reading
%s: sample rate change (now %d Hz); invalidating queue
%s: no valid models could be created
%s: client died - cancelling recognition
couldn't add listener for queue running state (%d)
couldn't create audio queue
setting recognition thread priority to %d
%s: sleeping for %g s
%s: finished starting queue in %g s
couldn't start audio queue for recognition (%d)
%s: starting recognition
%s: starting recognition from file
couldn't get file format description.
Recognition results:
--------------------
%s: error posting client completion notification
%s: error posting client error notification
Cached task %p with hash %@ in memory, caching to disk now.
Caching is disabled. Skipping caching.
Hash %@ is now cached in disk
Error converting audio during caching. %@
Can't add audio cache, error: %@
In-memory cached synthesis %@ is found.
Preinstalled cached synthesis %@ is found.
On-disk cached synthesis %@ is found.
Short-term cached synthesis is found for text '%@'
decoderStreamDescription formatID: %@, sample rate: %@
Unknown server audio format ID: %d
Invalid chunk size: %d at offset %d, bytes count = %d
Unable to convert OPUS to PCM. %@
Reset MobileAsset query cache and retry selecting voice
No voice available
Compact voice is explicitly disabled.
Voice is deleted at path '%@'
forceServerTTS is enabled by user default.
forceServerTTS is enabled by speech request.
Received AceObject: %@
Voice count for %@_%@: %@
Voice: %@:%@:%@:%@:%@:%@
Error: %@
Sent AceObject: %@
#AFClientLite Request %p received AceObject: %@, %s
Server TTS word timing info size: %d
Server TTS word timing info, offset: %ld, length: %ld, word: %@, sampleIndex: %@, timestamp: %.2f
Error: %@, %s
Synthesis completed, total packet number: %d, %s
#AFClientLite error: %@, %s
#AFClientLite Request %p gets completion call. %s
%s: posted %s to client
caching model <%s> class <%s> ...
no valid cache found; recaching everything.
cache for model <%s> is valid; skipping recache request.
... finished caching model in %g s with error %d <%s> class <%s>
cache for model <%s> is valid; skipping recache.
recache for model <%s> done in %g s
error caching model <%s>
%s: couldn't write keyword index for cache
%s: couldn't create manifest for cache
%s: error setting info dict on temp cache
%s: couldn't move temp cache into place... deleting
%s: couldn't save cache; no base dir exists or couldn't create temp cache
error writing model configuration cache Info.plist:
beginning plugin registry rebuild...
finished.
%s: error examining plugins directory (%ld)
%s: error writing plugin registry cache:
Preference changed for disableNewBackend, deleted cached engine
Prewarming: Invoked with request: '%@'
Unable to prewarm, error: %@
Can't prewarm engine with path '%@'
Engine is previously prewarmed with path '%@'
Prewarm finished. Latency: %.3f
Prewarming: Completed with request: '%@'
Can't create engine with path '%@'
Voice specific resources found.
Specified resource file '%@' does not exist at: '%@'
Using timestamp inside voiced
Speaking pre-synthesized audio: %@
Can't create VSAudioPlaybackService
Error, %@
Pre-synthesized audio request stopped
Finished speaking pre-synthesized audio: %@
Task is cancelled by user: %@
data provider does not implement value method
%s: plugin class does not conform to appropriate protocol
%s: plugin class not found
%s: error loading plugin:
Enqueuing cached audio
playbackService is initialized already.
Starting AudioQueue
Task %p is cancelled by user. text: '%@'
Finished task %p with error, text: '%@', error: %@
Device task %p: Finished %@ utterance: '%@', voice: %{public}@:%@:%{public}@:%@:%@:%@:%{public}@:%@, voice resource: %{public}@:%{public}@, rate:%.2f, pitch:%.2f, volume:%.2f, isEager:%@
Ignore unspeakable task, type: %@
Start spinNextTask
Dispatch speaking task %p
Starting task %p
%p wait another speaking task %p
%p interrupt task %p
Speak task %p is attached to eager task %p
Dispatch synthesis task %p
Finish spinNextTask
Cancel current task: %p
Task %p reported word time info
Device SpeakTask %p: Instrument metric: %@
Task %p started speaking
Device EagerTask %p: Instrument metric: %@
Task %p reported finish, error: %@
err %d copying manifest
Error %s, %@
Cache type name too long %@
%@ is not TTS language, falling back to %@
Error in reading audio data from file: %@ error:%@
Can't remove cache file '%@', error: %@
Time to cleaning cache: %f
Error getting cache path: %@, error: %@
Unable to get URL: %@, URL resources: %@
#AudioSession session interrupted
#AudioSession mediaserverd died
#AudioSession : Setting up audio session
#AudioSession error setting HW sample rate: %ld
#AudioSession : category = %d
#AudioSession error %ld setting audio category
#AudioSession error %ld setting bluetooth allowability
#AudioSession : Bluetooth %sabled
#AudioSession active count went negative for input!
#AudioSession active count went negative for output!
#AudioSession active count went negative!
#AudioSession : activity %d --> %s
#AudioSession : Active --> FALSE
#AudioSession : Active --> TRUE
#AudioSession error %ld activating or deactivating session for activity %ld
#AudioSession could not stop queue (%d)
Error AudioQueueNewOutputWithAudioSession %d
VSAudioPlaybackService init latency: %.3f
Error AudioQueueDispose %d
mediaserverd reset
Error AudioQueueStart %d
Success AudioQueueStart
Error AudioQueueAllocateBuffer %d
Detected AudioQueue stall, enqueueing %.2f samples of silence
Error AudioQueueEnqueueBuffer %d
Enqueued audio buffer at sample time: %.2f, size: %ld
Error AudioQueueFlush %d
Error AudioQueueStop %d
Error AudioQueuePause %d
Success AudioQueuePause
Error AudioQueueReset %d
Error AudioQueueAddPropertyListener %d
Error AudioQueueRemovePropertyListener %d
Detected stall of audio queue after all samples are enqueued.
Error AudioQueueGetProperty isRunning %d
Unable to enable kAudioQueueProperty_EnableLevelMetering, err: %d
Unable to disable kAudioQueueProperty_EnableLevelMetering, err: %d
Error: %s, errno: %d
Played audio buffer at sample time: %f, size: %ld
Error AudioQueueFreeBuffer %d
Looking for data %s
Found data at path %s
-----------> Preheat: Preheating %u bytes of uselect starting at %u on same thread %s .....
-----------> done preheating %u bytes (%g seconds)
Reading cache %@ error: %@
%@ is not TTS language, fallback to %@
Update with connection identifier: %{public}@
Created server speak task: %p, with request: 0x%0lx
Created speak task: %p, with request: 0x%0lx
Created presynthesized task: %p
Created server synthesis task: %p, with request: 0x%0lx
Created synthesis task: %p, with request: 0x%0lx
set auto download voice assets:%@
%s override voice info for server TTS platform, %@
Using requestCreatedTimestamp inside voiced
Received server TTS response. Use Server TTS.
Received device synthesis previously, ignore server TTS.
Server network error: %@
Device fallback synthesis is disabled.
Start playing compact synthsis instead.
Received device synthesis. Use device synthesis since it's not compact voice.
Received device synthesis with compact voice. Wait for server response.
Wait for network response, timeout value: %.3f
One-shot server TTS is previously cached.
Server task %p: Instrument metric: %@
Server task %p is cancelled by user. text: '%@'
Error in server task %p, error: %@
Server task %p: Finished speaking utterance: '%@', isEager:%@, isOneShot:%@, isTimeOut:%@, useDeviceTTS:%@, deviceVoice:'%@'
Starting speech task: %p
Device task %p: Instrument metric: %@
#CacheDelete purgeable urgency: %d, info: %@
#CacheDelete purge urgency: %d, info: %@
#CacheDelete periodic urgency: %d, info: %@
Error %s: Audio dump directory not created as a file with that name exists at path: %@
Error %s: Audio dump directory not created: %@
Error in creating audio dump directory: %@
Generating audio to file
Generated audio to file: %@
Error in writing bytes to audio file: %d
Error in creating audio file: %d
Unable to madvise file '%@' MADV_DONTNEED, error: %s
Unable to munmap file '%@', error: %s
Unable to open file '%s', error: %s
Unable to get size of file '%s', error: %s
Unable to mmap '%s', error: %s
madvise failed for '%@', error: %s
Added short term cache:%@ for key:'%@'
Removed short term cache for key:'%@'
voiced starting up...
Cannot register CacheDelete service.
Receive notification %s
com.apple.MobileAsset.VoiceServices.VoiceResources.new-asset-installed: checking for a voice update
Perform force asset update
Skipping voice download: %@
Updating VoiceResource...
xpc_activity state must be RUN, got: %ld
running xpc_activity
Error cleanUnusedAssets in scheduled background task: %@
XPC connection invalidated, identifier: %{public}@
_VSAudioQueueSetLevelMeteringPropertyValue
enabling
disabling
com.apple.voiceservices.notification.voice-update
com.apple.voiceservices.request
v4@?0
%@.%@.%@
client
%@.%@
voice
locale
gender
footprint
loggingPrefix
T@"NSString",&,N,V_loggingPrefix
asbd
T{AudioStreamBasicDescription=dIIIIIIII},N,V_asbd
pcmBufferSize
TI,N,V_pcmBufferSize
floatConverter
T^{OpaqueAudioConverter=},N,V_floatConverter
integerConverter
T^{OpaqueAudioConverter=},N,V_integerConverter
voiceBoostUnit
T^{OpaqueAudioComponentInstance=},N,V_voiceBoostUnit
audioTimeStamp
T{AudioTimeStamp=dQdQ{SMPTETime=ssILLssss}LI},N,V_audioTimeStamp
voiceBoostGainDecibels
Tf,N,V_voiceBoostGainDecibels
audioData
T@"NSData",&,N,V_audioData
packetCount
Ti,N,V_packetCount
packetDescriptions
T@"NSData",&,N,V_packetDescriptions
broker.hdr
broker.hdr.asset
VSVocalizerEngine
@unionOfObjects.startTime
text
T@"NSString",&,N,V_text
utf8WordTimingInfos
T@"NSMutableArray",&,N,V_utf8WordTimingInfos
markerCount
TI,N,V_markerCount
numOfPromptsTriggered
TI,N,V_numOfPromptsTriggered
mutablePCMData
T@"NSMutableData",&,N,V_mutablePCMData
stopMark
Tl,N,V_stopMark
callback
T@?,C,N,V_callback
state
Ti,N,V_state
error
T@"NSError",&,N,V_error
engine
T@"VSSpeechEngine",&,N,V_engine
nil path or nil mimeType
path
mimeType
ve_ttsResourceLoad
ve_ttsSetOutDevice
ve_ttsProcessText2Speech
indx
mdnf
rnnf
clc_
<COMPONENT>monogryph/featextract</COMPONENT>
voicePath
T@"NSString",&,N,V_voicePath
vocalizerSpeech
T{_VE_HSAFE=^vI},N,V_vocalizerSpeech
vocalizerInstance
T{_VE_HSAFE=^vI},N,V_vocalizerInstance
vocalizerDataClass
T^v,N,V_vocalizerDataClass
language
T{_VE_LANGUAGE=[128c][4c][128c]S},N,V_language
voiceInfo
T{?=[128c][128c][128c][128c][128c]S},N,V_voiceInfo
currentCallbackResult
T@"VSSpeechSynthesisCallbackResult",&,N,V_currentCallbackResult
memoryMaps
T@"NSMutableArray",&,N,V_memoryMaps
T{AudioStreamBasicDescription=dIIIIIIII},R,N,V_asbd
rate
Tf,N,V_rate
pitch
Tf,N,V_pitch
volume
Tf,N,V_volume
_default
server_VSRecognitionPrepareOrBegin
server_VSRecognitionBegin
server_VSRecognitionCancel
com.apple.voiceservices.recognition
Error %d at %s:%d
/BuildRoot/Library/Caches/com.apple.xbs/Sources/VoiceServicesDaemons_Sim/VoiceServices-471/Daemon/VSRecognitionServer.c
_CreateEngineIfNecessary
_InitializeEngine
_RecognitionClientInvalidationCallback
Error %d at %s:%d (%s)
couldn't start recognition
recording.wav
_BeginRecognition
_SendChoices
com.apple.voiced.cachingQueue
@"AVAudioBuffer"12@?0I4^i8
threadLock
T@"NSLock",&,N,V_threadLock
inMemoryCaches
T@"NSMutableArray",&,N,V_inMemoryCaches
cacheStore
T@"VSSpeechCache",&,N,V_cacheStore
shortTermCache
T@"VSShortTermCache",&,N,V_shortTermCache
cachingQueue
T@"NSObject<OS_dispatch_queue>",&,N,V_cachingQueue
audioDataFromFile:error:
AudioFileOpenURL
AudioFileGetProperty kAudioFilePropertyDataFormat
AudioFileGetProperty kAudioFilePropertyAudioDataByteCount
AudioFileGetProperty kAudioFilePropertyAudioDataPacketCount
com.apple.voiceservices.trigger.asset-force-update
VoiceServicesErrorDomain
ServerTTSErrorDomain
%@ %@ %@ %.2f %.2f %.2f %@
@"NSError"8@?0@"VSSpeechSynthesisCallbackResult"4
No voice available
Compact voice is explicitly disabled.
Voice is deleted already.
Can't create VSSpeechEngine
cachingService
T@"VSCachingService",&,N,V_cachingService
selectedVoice
T@"VSVoiceAssetSelection",&,N,V_selectedVoice
selectedVoiceResource
T@"VSVoiceResourceAsset",&,N,V_selectedVoiceResource
voiceBooster
T@"VSVoiceBooster",&,N,V_voiceBooster
request
T@"VSSpeechRequest",R,N,V_request
delegate
T@"<VSSpeechSynthesisCoreDelegate>",W,N,V_delegate
instrumentMetrics
T@"VSInstrumentMetrics",W,N,V_instrumentMetrics
v8@?0@"SABaseCommand"4
v12@?0B4@"NSError"8
-[VSSiriTTSClient startSynthesisRequest:responseHandler:completion:]_block_invoke
@unionOfObjects.timestamp
Unable to parse audio data
afClient
T@"AFClientLite",&,N,V_afClient
ServerTTSTimeout
ServerTTSPrefer
knowledgeStore
T@"CKKnowledgeStore",&,N,V_knowledgeStore
timeout
Td,R,N
prefer
TB,R,N
VSClientPostNotification
VSLocaleIdentifier
VSVersion
VSPluginVersions
KeywordIndex.plist
%@%@
.vscache
Info.plist
temp.vscache.XXXXX
_SaveEngineToCache
VSEngineIdentifier
temp.vscache.
PluginPath
PluginRegistry.plist
_CreateRegistryAndSaveToCache
modelid-desc
pluginid-vers
pluginpath-moddate
/System/Library/VoiceServices/PlugIns
com.apple.voiced.prewarmQueue
disableNewBackend
Prewarm textify emoji
/BuildRoot/Library/Caches/com.apple.xbs/Sources/VoiceServicesDaemons_Sim/VoiceServices-471/Daemon/VSPrewarmService.m
<Unknown File>
Invalid parameter not satisfying: %@
voiceSelection != nil
VoiceServices/config
cachedEngine
T@"VSSpeechEngine",&,N,V_cachedEngine
loadedResources
T@"VSVoiceResourceAsset",&,N,V_loadedResources
prewarmQueue
T@"NSObject<OS_dispatch_queue>",&,N,V_prewarmQueue
Library
Caches
VoiceServices
task: inprogress %@, request: %@
Can't create VSAudioPlaybackService
Can't decode audio data
hash
TI,R
superclass
T#,R
description
T@"NSString",R,C
debugDescription
T@"VSPresynthesizedAudioRequest",R,N,V_request
T@"<VSSpeechServiceDelegate>",W,N,V_delegate
playbackService
T@"VSAudioPlaybackService",&,N,V_playbackService
T@"VSInstrumentMetrics",&,N,V_instrumentMetrics
T@"NSMutableData",&,N,V_audioData
VSRecognitionVersion
VSRecognitionModels
VSRecognitionModelIdentifier
VSRecognitionModelFileName
VSRecognitionModelIsTopLevel
VSRecognitionModelWeight
VSRecognitionModelIsCancelModel
VSRecognitionModelRequiredCapabilities
VSRecognitionModelDataProvider
VSRecognitionResultValidator
VSRecognitionResultHandler
vsplugin
lang
VSRecognitionModelDefinition
.plist
VSPlugin
<VSPlugin %p: %@>
VSRecognitionClasses
VSRecognitionSequences
VSRecognitionKeywords
VSRecognitionClassIdentifier
VSRecognitionClassRequiredCapabilities
VSRecognitionClassElements
VSRecognitionClassElementValues
VSRecognitionClassSequences
VSRecognitionClassType
VSRecognitionClassWeight
VSRecognitionClassContainsKeywords
VSRecognitionClassTypeCommand
VSRecognitionClassTypePersonName
VSRecognitionClassTypeStreetName
VSRecognitionClassTypeCityName
VSRecognitionClassTypeSongTitle
VSRecognitionClassTypeArtistName
VSRecognitionClassTypeAlbumName
VSRecognitionSequenceElements
VSRecognitionSequenceDisambiguationTag
_InstantiatePluginClassWithRecognitionModelKeyedName
_LoadPluginIfNecessary
isvalid
VSRecognitionClassDataProvider
/BuildRoot/Library/Caches/com.apple.xbs/Sources/VoiceServicesDaemons_Sim/VoiceServices-471/Daemon/VSSpeechSpeakTask+Utilities.m
self.voiceSelection != nil
[self.speechCache isKindOfClass:[VSSpeechCacheAudio class]]
speaking
synthesizing
previousProvider
T@"<AFAudioPowerProviding>",W,N,V_previousProvider
dflt%ld
VSRecognitionModel
com.apple.voiced.speakingQueue
eagerTasks
T@"NSMutableArray",&,N,V_eagerTasks
speakTasks
T@"NSMutableArray",&,N,V_speakTasks
currentTask
T@"<VSSpeechTaskProtocol>",&,N,V_currentTask
threadMutex
T{_opaque_pthread_mutex_t=l[40c]},N,V_threadMutex
threadMutexAttr
T{_opaque_pthread_mutexattr_t=l[8c]},N,V_threadMutexAttr
speakingQueue
T@"NSObject<OS_dispatch_queue>",&,N,V_speakingQueue
lastSynthesisRequest
T@"VSSpeechRequest",&,N,V_lastSynthesisRequest
lastSynthesisRequestCreatedTimeStamp
TQ,N,V_lastSynthesisRequestCreatedTimeStamp
VSRecognitionClass
VSRecognitionSequence
<%@>
Synthesis is cancelled/interrupted.
speakTask
T@"VSSpeechSpeakTask",&,N,V_speakTask
readyForEagerTask
TB,N,V_readyForEagerTask
Manifest.sqlitedb
SELECT model_id, validity FROM Model;
 WHERE model_id = ?;
INSERT OR REPLACE INTO ValueTranslation (model_id, class_id, original_value, translated_value) VALUES (?, ?, ?, ?);
SELECT original_value FROM ValueTranslation WHERE model_id = ? AND class_id = ? AND translated_value = ?;
DELETE FROM ValueTranslation
 model_id = ?
 class_id = ?
 WHERE
 AND
VSRecognitionConfigurationCacheManifest
Model
model_id
validity
last_update
SELECT %s FROM %s WHERE ROWID = ?
UPDATE %s SET %s = ? WHERE ROWID = ?
ValueTranslation
CREATE TABLE Model (ROWID INTEGER PRIMARY KEY AUTOINCREMENT, model_id TEXT, validity TEXT, last_update INTEGER, UNIQUE(model_id));
CREATE INDEX ModelIdIndex on Model(model_id);
CREATE TABLE ValueTranslation (ROWID INTEGER PRIMARY KEY AUTOINCREMENT, model_id TEXT, class_id TEXT, original_value TEXT, translated_value TEXT, UNIQUE(model_id, class_id, original_value));
CREATE INDEX ValueTranslationModelIdClassIdIndex on ValueTranslation(model_id, class_id, translated_value);
/var/mobile/Library/VoiceServices/SpeechCache
TTSResources/PreinstallCache/
VSSpeechCacheErrorDomain
com.apple.voiceservices
-[VSSpeechCache initWithStorePath:]
Cache type name too long
-[VSSpeechCache addCache:]
male
female
%@_%@
i12@?0@"NSURL"4@"NSURL"8
dirPath
T@"NSString",&,N,V_dirPath
preinstalledCacheDir
T@"NSString",&,N,V_preinstalledCacheDir
VSAudioSessionQueue
ACTIVE
INACTIVE
LatencyFudgeFactor
Error AudioQueueStart
-[VSAudioPlaybackService getAveragePower:andPeakPower:]
audioQueue
T^{OpaqueAudioQueue=},N,V_audioQueue
waitForStateChangeMutex
T{_opaque_pthread_mutex_t=l[40c]},N,V_waitForStateChangeMutex
stateChangeCondition
T{_opaque_pthread_cond_t=l[24c]},N,V_stateChangeCondition
enqueuedSampleCount
Td,N,V_enqueuedSampleCount
initialSampleTime
Td,N,V_initialSampleTime
lastPlayedSampleTime
Td,N,V_lastPlayedSampleTime
lastCheckedSampleTime
Td,N,V_lastCheckedSampleTime
sessionID
TI,N,V_sessionID
_VSVocalizerDataMappingImplMap
/BuildRoot/Library/Caches/com.apple.xbs/Sources/VoiceServicesDaemons_Sim/VoiceServices-471/Daemon/VSVocalizerPlatform.c
info->mappedData == NULL
uselect
T@"NSString",&,N,V_key
magicVersion
Ti,R,N,V_magicVersion
timingInfos
T@"NSArray",R,N,V_timingInfos
audio
T@"VSAudioData",R,N,V_audio
VSAudioPowerUpdateQueue
-[VSSpeechXPCHandler beginAudioPowerUpdateWithReply:]
v8@?0@"AFXPCWrapper"4
-[VSSpeechXPCHandler endAudioPowerUpdate]
-[VSSpeechXPCHandler getVoiceInfoForLanguageCode:footprint:gender:type:reply:]
connectionIdentifier
T@"NSString",&,N,V_connectionIdentifier
audioPowerUpdateQueue
T@"NSObject<OS_dispatch_queue>",&,N,V_audioPowerUpdateQueue
audioPowerUpdater
T@"AFAudioPowerUpdater",&,N,V_audioPowerUpdater
%@:%@:server
Unable to create playback service
v12@?0@"VSAudioData"4@"NSArray"8
v8@?0@"NSError"4
Server TTS timeout
Task is cancelled.
shouldSpeak
TB,N,V_shouldSpeak
T@"VSSpeechRequest",&,N,V_request
wordTimingInfo
T@"NSArray",&,N,V_wordTimingInfo
timeoutCondition
T{_opaque_pthread_cond_t=l[24c]},N,V_timeoutCondition
T@"VSSpeechServerTask",&,N,V_speakTask
synthesisCore
T@"VSSpeechSynthesisCore",&,N,V_synthesisCore
useServerResponse
TB,N,V_useServerResponse
useDeviceSynthesis
TB,N,V_useDeviceSynthesis
speechStartReported
TB,N,V_speechStartReported
racingMutex
T{_opaque_pthread_mutex_t=l[40c]},N,V_racingMutex
compactAudioPCM
T@"NSMutableData",&,N,V_compactAudioPCM
compactAudioData
T@"VSAudioData",&,N,V_compactAudioData
compactWordTimingInfo
T@"NSArray",&,N,V_compactWordTimingInfo
\audio=
\audio="%@"\
\mrk=%@=%@\
.wav
%@ %@ %@ %@ %.2f %.2f %.2f %@
Speech is cancelled/interrupted.
T@"NSArray",&,N,V_timingInfos
voiceSelection
T@"VSVoiceAssetSelection",&,N,V_voiceSelection
voiceResource
T@"VSVoiceResourceAsset",&,N,V_voiceResource
speechCache
T@"<VSSpeechCacheItem>",&,N,V_speechCache
phonemes
T@"NSArray",&,N,V_phonemes
utterance
T@"NSString",&,N,V_utterance
T@"VSAudioData",&,N,V_audio
taskAuxiliaryQueue
T@"NSObject<OS_dispatch_queue>",&,N,V_taskAuxiliaryQueue
CACHE_DELETE_AMOUNT
CACHE_DELETE_VOLUME
com.apple.voiced.CacheDelete
r^{__CFDictionary=}12@?0i4r^{__CFDictionary=}8
/var/mobile/Library/VoiceServices/AudioDump
Wrong file exists at path
-[VSAudioDump createAudioDumpDirectoryIfNeeded]
yyyy-MM-dd_HH-mm-ss
%@/%@_%@.wav
audioDumpPath
T@"NSString",&,N,V_audioDumpPath
filePath
T@"NSString",R,N,V_filePath
fileSize
TL,R,N,V_fileSize
mappedData
T^v,R,N,V_mappedData
cache
T@"NSCache",&,N,V_cache
cacheTimer
T@"NSCache",&,N,V_cacheTimer
com.apple.notifyd.matching
v8@?0@"NSObject<OS_xpc_object>"4
com.apple.MobileAsset.VoiceServices.CustomVoice.cached-metadata-updated
com.apple.MobileAsset.VoiceServicesVocalizerVoice.cached-metadata-updated
com.apple.MobileAsset.VoiceServices.CustomVoice.new-asset-installed
com.apple.MobileAsset.VoiceServices.GryphonVoice.new-asset-installed
com.apple.MobileAsset.VoiceServicesVocalizerVoice.new-asset-installed
com.apple.MobileAsset.VoiceServices.VoiceResources.new-asset-installed
LastAttemptedDownloadTimeForNewResource
com.apple.voiced
KeepAliveManager
com.apple.voiceservices.keepalive
connectionCount
TI,N,V_connectionCount
runloopSourceRef
T^{__CFRunLoopSource=},N,V_runloopSourceRef
listener
T@"NSXPCListener",&,N,V_listener
init
alloc
initWithLoggingPrefix:
stringWithFormat:
class
isMemberOfClass:
speakTask
request
clientBundleIdentifier
recordCategory:value:
voiceSelection
voiceData
masteredVersion
name
languages
firstObject
gender
genderStringFromGender:
footprint
footprintStringFromFootprint:
defaultService
tallyTask:
.cxx_destruct
loggingPrefix
setLoggingPrefix:
_loggingPrefix
uninitialize
dealloc
initialize
voiceBoostGainDecibels
length
mutableBytes
dataWithLength:
errorWithDomain:code:userInfo:
initWithStreamDescription:pcmBufferSize:
setVoiceBoostGainDecibels:
processData:
asbd
setAsbd:
pcmBufferSize
setPcmBufferSize:
floatConverter
setFloatConverter:
integerConverter
setIntegerConverter:
voiceBoostUnit
setVoiceBoostUnit:
audioTimeStamp
setAudioTimeStamp:
_voiceBoostGainDecibels
_pcmBufferSize
_floatConverter
_integerConverter
_voiceBoostUnit
_asbd
_audioTimeStamp
audioData
packetCount
allocWithZone:
copy
setAudioData:
setPacketCount:
packetDescriptions
setPacketDescriptions:
dataWithData:
appendData:
copyWithZone:
duration
concatenateWithAudio:
_audioData
_packetCount
_packetDescriptions
array
setLength:
engine
setStartTime:
setTextRange:
addObject:
valueForKeyPath:
wordTimingInfoFrom:timestamps:
arrayWithCapacity:
vocalizerInstance
stringWithUTF8String:
initWithCallback:pcmBufferSize:
resetPCMBuffer
pcmData
markerBuffer
markerBufferSize
processMarkerBuffer
wordTimingInfos
phonemes
numOfPromptsTriggered
state
setState:
error
setError:
text
setText:
utf8WordTimingInfos
setUtf8WordTimingInfos:
markerCount
setMarkerCount:
setNumOfPromptsTriggered:
mutablePCMData
setMutablePCMData:
stopMark
setStopMark:
callback
setCallback:
setEngine:
_markerBuffer
_state
_error
_text
_utf8WordTimingInfos
_markerCount
_numOfPromptsTriggered
_mutablePCMData
_stopMark
_callback
_engine
brokerInfoData
bytes
isGryphonVoice:
standardInstance
disableNewBackend
UTF8String
dictionaryWithObjects:forKeys:count:
dataWithContentsOfFile:
currentCallbackResult
lengthOfBytesUsingEncoding:
defaultManager
voicePath
contentsOfDirectoryAtPath:error:
countByEnumeratingWithState:objects:count:
shouldPreheatComponent:
stringByAppendingPathComponent:
mmapAndScanFile:
code
domain
isEqualToString:
arrayWithObjects:count:
setWithArray:
containsObject:
hasPrefix:
initWithFilePath:
memoryMaps
madvise
appendBytes:length:
initWithData:encoding:
containsString:
isUserCancelError:
initWithVoicePath:
setVolume:
setRate:
setPitch:
loadResourceAtPath:mimeType:
synthesizeText:callback:
stopAtMarker:
preheat
reloadMappedFiles
setVoicePath:
rate
pitch
volume
vocalizerSpeech
setVocalizerSpeech:
setVocalizerInstance:
vocalizerDataClass
setVocalizerDataClass:
language
setLanguage:
voiceInfo
setVoiceInfo:
setCurrentCallbackResult:
setMemoryMaps:
_voicePath
_rate
_pitch
_volume
_vocalizerDataClass
_currentCallbackResult
_memoryMaps
_vocalizerSpeech
_vocalizerInstance
_language
_voiceInfo
defaultCacheStore
sharedInstance
initWithCache:shortTermMemory:
md5hash
audio
timingInfos
enqueueCacheWithHash:audio:timingInfo:completion:
disableCache
initWithKey:audio:wordTimingInfo:
threadLock
lock
inMemoryCaches
unlock
cachingQueue
cacheWithHash:audio:timingInfo:
removeObject:
initWithStreamDescription:
initFromFormat:toFormat:
initWithPCMFormat:frameCapacity:
setFrameLength:
mutableAudioBufferList
data
maximumOutputPacketSize
initWithFormat:packetCapacity:maximumPacketSize:
convertToBuffer:error:withInputFromBlock:
audioBufferList
cacheStore
addCache:
onDiskCacheForHash:
cacheDataForKey:
speechCache
inMemoryCacheForHash:
setSpeechCache:
instrumentMetrics
setIsCacheHitFromMemory:
languageCode
preinstalledCacheForText:language:gender:
setIsCacheHitFromDisk:
shortTermCacheForHash:
isKindOfClass:
setTimingInfos:
setAudioDuration:
shortTermCache
setObject:forKey:timeToLive:
objectForKey:
standardService
enqueueCacheSpeakTask:completion:
onDiskCacheForSimilarTask:
fetchCacheForTask:
enqueueShortTermCacheWithHash:audio:timingInfo:completion:
setThreadLock:
setInMemoryCaches:
setCacheStore:
setShortTermCache:
setCachingQueue:
_threadLock
_inMemoryCaches
_cacheStore
_shortTermCache
_cachingQueue
fileURLWithPath:
decoderStreamDescription
formatID
sampleRate
unsignedIntegerValue
doubleValue
formatFlags
unsignedIntValue
bytesPerPacket
framesPerPacket
bytesPerFrame
channelsPerFrame
bitsPerChannel
reserved
audioBuffer
populateWithPCMData:
populateWithOpusData:
playerStreamDescription
populatePCMDataWithSiriOpusSData:withOpusASBD:
beginChunkDecoderForStreamDescription:
dataWithBytes:length:
decodeChunk:outError:
endChunkDecoding
audioDataFromFile:error:
audioDataFromSAUIAudioData:
audioDataFromPresynthesisRequest:
selectedVoice
selectedVoiceResource
contextInfo
voiceSelectionWithRequest:error:
isCancelled
getCacheForHash:
reportWordTimingInfo:
reportAudio:
sharedService
waitUntilPrewarmFinish
prepareForSynthesis
insertContextInfo:
substituteAudioWithLocalPath
textifyEmojiWithLanguage:
precomposedStringWithCanonicalMapping
sleepForTimeInterval:
voiceBooster
setPromptCount:
adjustWordTimingInfo:
cachingService
cancel
sharedManager
selectVoiceResourceAssetForLanguage:
voiceSelection_noRetry_WithRequest:error:
resetCache
voiceType
selectVoiceForLang:type:gender:footprint:
disableCompactVoiceFallback
fileExistsAtPath:
cachedEngineForVoice:resources:
setIsWarmStart:
prewarmEngineForVoice:resources:
setVoiceBooster:
gainDecibelWithVolume:
stringByReplacingOccurrencesOfString:withString:
markerStringForContext:
textRange
delegate
synthesisCore:didReceiveWordTimingInfo:
respondsToSelector:
synthesisCore:didReceiveAudio:
initWithRequest:
main
setDelegate:
setInstrumentMetrics:
setCachingService:
setSelectedVoice:
setSelectedVoiceResource:
_request
_delegate
_instrumentMetrics
_cachingService
_selectedVoice
_selectedVoiceResource
_voiceBooster
forceServerTTS
isPreinstalledCacheAvailableForRequest:
canUseServerTTS
fetchSpeechSynthesisVoiceRequest
count
setLanguageCode:
genderStringFromVSGender:
setGender:
speechSynthesisVoice
setFilteredVoiceKey:
voiceKeyList
numberWithUnsignedInteger:
contentVersion
quality
keyString
setLanguages:
genderFromString:
setName:
integerValue
numberWithInteger:
setContentVersion:
footprintFromString:
setFootprint:
localizedDescription
handleCommand:afterCurrentRequest:commandHandler:completion:
startSpeechSynthesisRequest
setAudioType:
setStreaming:
setEnableAudioInfo:
errorCode
reason
reasonDescription
aceAudioData
aceAudioInfo
wordTimingInfoList
floatValue
numberWithFloat:
offset
word
sampleIndex
timestamp
totalPacketNumber
defaultClient
shouldUseServerTTSForRequest:
queryVoices:reply:
startSynthesisRequest:responseHandler:completion:
afClient
setAfClient:
_afClient
userDefaultsKnowledgeStore
setKnowledgeStore:
knowledgeStore
valueForKey:
boolValue
defaultConfig
timeout
prefer
_knowledgeStore
addObserver:forKeyPath:options:context:
defaultCenter
removeObserver:
fetchVoiceAsset
fetchVoiceResource
voiceResource
cachedEngine
loadedResources
currentHandler
handleFailureInMethod:object:file:lineNumber:description:
prewarmQueue
setCachedEngine:
loadEngine:withVoiceResources:
fileURLWithPathComponents:
path
setSearchPathURL:
setLoadedResources:
searchPathURL
resourceList
resourceMimeTypes
objectForKeyedSubscript:
observeValueForKeyPath:ofObject:change:context:
prewarmWithRequest:
setPrewarmQueue:
_cachedEngine
_loadedResources
_prewarmQueue
requestCreatedTimestamp
setRequestCreatedTimestamp:
isExecuting
numberWithBool:
setSynthesisBeginTimestamp:
audioSessionID
initWithAudioSessionID:asbd:
setPlaybackService:
playbackService
presynthesizedAudioRequestSuccessWithInstrumentMetrics:error:
presynthesizedAudioRequestDidStopAtEnd:error:
presynthesizedAudioRequestDidStart
start
setAudioStartTimestampDiffs:
enqueue:packetCount:packetDescriptions:
flushAndStop
setSpeechEndTimestamp:
stop
pause
audioPowerProvider
isEqual:
self
performSelector:
performSelector:withObject:
performSelector:withObject:withObject:
isProxy
conformsToProtocol:
retain
release
autorelease
retainCount
zone
hash
superclass
description
debugDescription
suspend
resume
reportFinish
isSpeaking
reportInstrumentMetrics
reportSpeechStart
reportTimingInfo
_clockFactor
_playbackService
getValue:weight:atIndex:forClassWithIdentifier:inModelWithIdentifier:
phoneticValueAtIndex:forClassWithIdentifier:inModelWithIdentifier:
valueCountForClassWithIdentifier:inModelWithIdentifier:
valueAtIndex:forClassWithIdentifier:inModelWithIdentifier:
cacheValidityIdentifier
isCacheValidityIdentifierValid:
beginReportingChanges
stopReportingChanges
validRecognitionResultFromRecognitionResult:
validRecognitionResultFromRecognitionResult:knownDisambiguationValues:
setUtterance:
synthesisBeginTimestamp
setSynthesisEndTimestamp:
startPlaybackServiceWithAudioSessionID:
taskAuxiliaryQueue
audioStartTimestampDiffs
setVoiceResource:
_fetchVoiceAsset_NoRetry
setVoiceSelection:
setVoiceAssetKey:
type
typeStringFromType:
isBuiltInVoice
eagerRequestCreatedTimeStampDiffs
speakCachedAudio
waitUntilAudioFinished
pausePlayback
resumePlayback
adjustWordTimingInfo
logFinish
sharedQueue
currentTask
didEndAccessPower
getCurrentAudioPowerProvider
willBeginAccessPower
getAveragePower:andPeakPower:
sharedServices
previousProvider
setPreviousProvider:
_previousProvider
isAudioAccessory
spinNextTask
removeObjectAtIndex:
isSimilarTo:
setEagerRequestCreatedTimeStampDiffs:
readyForEagerTask
setSpeakTask:
addTask:
cancelCurrentTask
suspendCurrentTask
resumeCurrentTask
eagerTasks
setEagerTasks:
speakTasks
setSpeakTasks:
setCurrentTask:
threadMutex
setThreadMutex:
threadMutexAttr
setThreadMutexAttr:
speakingQueue
setSpeakingQueue:
lastSynthesisRequest
setLastSynthesisRequest:
lastSynthesisRequestCreatedTimeStamp
setLastSynthesisRequestCreatedTimeStamp:
_eagerTasks
_speakTasks
_currentTask
_speakingQueue
_lastSynthesisRequest
_lastSynthesisRequestCreatedTimeStamp
_threadMutexAttr
_threadMutex
synthesize
speechBeginTimestamp
setReadyForEagerTask:
outputPath
defaultAudioDumpStore
saveAudioToFileIfNeeded:
shouldCache
utterance
synthesisRequest:didReceiveTimingInfo:
speechRequestDidReceiveTimingInfo:
speechRequestSuccessWithInstrumentMetrics:
voiceAssetKey
synthesisEndTimestamp
setSpeechBeginTimestamp:
speechEndTimestamp
audioDuration
isWarmStart
speechRequestDidStart
speechRequestDidStopWithSuccess:phonemesSpoken:error:
componentsJoinedByString:
synthesisRequest:didFinishWithInstrumentMetrics:error:
_readyForEagerTask
_speakTask
bundleWithIdentifier:
bundlePath
createDirectoryAtPath:withIntermediateDirectories:attributes:error:
initWithStorePath:
dataUsingEncoding:
serializedData
dataWithCapacity:
dirPath
writeToFile:options:error:
availableLanguages
fallbackLanguageForLanguage:
preinstalledAudioHashForLanguage:gender:
preinstalledCacheDir
stringByAppendingPathExtension:
isReadableFileAtPath:
subdataWithRange:
initWithKey:data:
contentsOfDirectoryAtURL:includingPropertiesForKeys:options:error:
getResourceValue:forKey:error:
compare:
sortedArrayUsingComparator:
subarrayWithRange:
removeItemAtURL:error:
fileURLWithPath:isDirectory:
resourceValuesForKeys:error:
cleanCache
totalCacheSize
deleteCache
setDirPath:
setPreinstalledCacheDir:
_dirPath
_preinstalledCacheDir
_audioSessionInterrupted:
addObserver:selector:name:object:
_mediaServicesWereReset:
userInfo
_setupAudioSession
setPreferredSampleRate:error:
setCategory:error:
categoryOptions
setCategory:withOptions:error:
_setCategoryForActivity:
_nextActivityForActive:activity:serverActivity:
setActive:error:
category
_safeSetupAudioSession
_safeServerGeneration
_safeSetCategoryForActivity:
_safeSetActive:withActivity:
_safeSetBluetoothInputAllowed:
_queue
_audioSessionIsSetUp
_desiredState
_cachedState
_bluetoothAllowed
_activityBag
_serverGeneration
outputLatency
inputLatency
IOBufferDuration
currentRoute
inputs
objectAtIndex:
portType
opaqueSessionID
handleMediaServerReset
waitForAudioQueueStop
signalQueueRunningStateChange
isAudioQueueRunning
isAudioQueueStalled
durationOfAudioDataLength:withAudioDescription:
bytesOfDuration:withAudioDescription:
reset
audioQueue
setAudioQueue:
sessionID
setSessionID:
waitForStateChangeMutex
setWaitForStateChangeMutex:
stateChangeCondition
setStateChangeCondition:
enqueuedSampleCount
setEnqueuedSampleCount:
initialSampleTime
setInitialSampleTime:
lastPlayedSampleTime
setLastPlayedSampleTime:
lastCheckedSampleTime
setLastCheckedSampleTime:
_audioQueue
_sessionID
_enqueuedSampleCount
_initialSampleTime
_lastPlayedSampleTime
_lastCheckedSampleTime
_stateChangeCondition
_waitForStateChangeMutex
archivedDataWithRootObject:requiringSecureCoding:error:
getBytes:range:
setWithObjects:
unarchivedObjectOfClasses:fromData:error:
magicVersion
setKey:
_magicVersion
_timingInfos
_audio
_key
invalidate
performLanguageFallBackIfNeededWithRequest:
initWithRequest:shouldSpeak:
setCompletionBlock:
pointer
audioPowerUpdateQueue
initWithProvider:queue:frequency:delegate:
audioPowerUpdater
createNewXPCWrapperWithCompletion:
beginUpdate
endUpdate
setAudioPowerUpdater:
installedAssetsForType:voicename:language:gender:footprint:
remoteObjectProxy
speechRequestDidPause
speechRequestDidContinue
speechRequestMark:didStartForRange:
cleanOldVoiceResources
cleanUnusedVoiceAssets
installedVoiceResources
initWithDictionaryRepresentation:
dictionaryRepresentation
isServerTTSPlatform
sharedCurrentSpeakingTaskQueue
updateWithConnectionIdentifier:
prewarmIfNeededWithRequest:
startSpeechRequest:
startSynthesisRequest:
pauseSpeechRequestAtMark:
continueSpeechRequest
stopSpeechRequestAtMark:
startPresynthesizedAudioRequest:
cachePresynthesizedAudioRequest:
stopPresynthesizedAudioRequest
getVoiceNamesForLanguage:reply:
getFootprintsForVoiceName:languageCode:reply:
getSpeechIsActiveReply:
getSpeechIsActiveForConnectionReply:
beginAudioPowerUpdateWithReply:
endAudioPowerUpdate
cleanUnusedAssets:
getLocalVoicesReply:
getLocalVoiceResourcesReply:
setAutoDownloadedVoiceAssets:
getAutoDownloadedVoiceAssets:
getVoiceResourceForLanguage:reply:
getVoiceInfoForLanguageCode:footprint:gender:type:reply:
setLogToFile:
getLogToFile:
getTTSServerVoicesWithFilter:reply:
initWithConnection:
connectionIdentifier
setConnectionIdentifier:
setAudioPowerUpdateQueue:
_connection
_connectionIdentifier
_audioPowerUpdateQueue
_audioPowerUpdater
setIsServerTTS:
setIsSpeechRequest:
setCanUseServerTTS:
useDeviceSynthesis
synthesisCore
setUseServerResponse:
setWordTimingInfo:
playAudioData:
setIsServerTimeout:
disableServerTimeoutFallback
compactAudioData
compactWordTimingInfo
useServerResponse
setUseDeviceSynthesis:
setCompactAudioData:
setCompactWordTimingInfo:
shouldSpeak
speechStartReported
setSpeechStartReported:
serverTTSTimeout
proceedWithSpeechCache:
proceedWithServerTTS
amendVoiceWithDefaultSettings:
handleServerResponse:timingInfo:
setIsServerTTSRacing:
waitForServerResponse
handleServerNetworkError
wordTimingInfo
isServerTimeout
numberWithInt:
isSynthesisCached
isServerTTS
descriptiveKey
handleDeviceSynthesis:timingInfo:
setShouldSpeak:
setRequest:
timeoutCondition
setTimeoutCondition:
setSynthesisCore:
racingMutex
setRacingMutex:
compactAudioPCM
setCompactAudioPCM:
_shouldSpeak
_useServerResponse
_useDeviceSynthesis
_speechStartReported
_wordTimingInfo
_synthesisCore
_compactAudioPCM
_compactAudioData
_compactWordTimingInfo
_timeoutCondition
_racingMutex
stringWithString:
rangeOfString:options:range:
substringWithRange:
stringByAppendingString:
replaceCharactersInRange:withString:
string
appendString:
synthesizeAndSpeak
addObjectsFromArray:
setPhonemes:
setAudio:
setTaskAuxiliaryQueue:
_voiceSelection
_voiceResource
_speechCache
_phonemes
_utterance
_taskAuxiliaryQueue
longLongValue
activeVoiceAssets
inactiveVoiceAssets
totalSizeOfAssets:
size
purgeableVoiceAssetsWithInfo:urgency:
totalAudioDumpSize
numberWithLongLong:
purgeAsset:
deleteAudioDump
purgeable:urgency:
purge:urgency:
periodic:urgency:
registerCacheDelete
enableAudioDump
fileExistsAtPath:isDirectory:
audioDumpPath
cleanDirectory:
totalDirectorySize:
deleteFilesInDirectory:
createAudioDumpDirectoryIfNeeded
setDateFormat:
date
stringFromDate:
setOutputPath:
decodeChunks:streamDescription:outError:
absoluteString
cleanAudioDump
setAudioDumpPath:
_audioDumpPath
mmap
filePath
fileSize
mappedData
_filePath
_fileSize
_mappedData
timeToLiveTimerFired:
timerWithTimeInterval:target:selector:userInfo:repeats:
cache
setObject:forKey:
cacheTimer
mainRunLoop
addTimer:forMode:
removeObjectForKey:
setCache:
setCacheTimer:
_cache
_cacheTimer
timeIntervalSinceNow
numberWithDouble:
initWithMachServiceName:
maintainWithAudioType:keepAudioSessionActive:
interfaceWithProtocol:
setExportedInterface:
setExportedObject:
setInvalidationHandler:
listener:shouldAcceptNewConnection:
_keepAliveListener
_keepAliveManager
hasActiveKeepAlives
maintainKeepAlive:
cancelKeepAlive:
_activeKeepAlives
_shouldChangeAudioSession
setManager:
_manager
_isActive
_activity
_keepSessionActive
_transaction
_registryRunLoopSource
downloadVoiceResource:useBattery:completion:
cancelDownload:completion:
scheduleBackgroundTask
setRemoteObjectInterface:
setClasses:forSelector:argumentIndex:ofReply:
connectionCount
setConnectionCount:
runloopSourceRef
setRunloopSourceRef:
listener
setListener:
_connectionCount
_runloopSourceRef
_listener
VSAggdService
VSVoiceBooster
VSAudioData
NSCopying
VSSpeechSynthesisCallbackResult
VSSpeechEngine
VSCachingService
SAUIAudioData
VSSpeechSynthesisCore
VSSiriTTSClient
VSServerTTSConfig
VSPrewarmService
VSSpeechPresynthesizedTask
VSSpeechSpeakableProtocol
VSSpeechTaskProtocol
NSObject
VSRecognitionModelDataProvider
VSRecognitionResultValidator
Utilities
VSSpeechAudioPowerService
AFAudioPowerProviding
VSSpeechEagerProtocol
VSSpeechTaskQueue
VSSpeechSynthesisTask
VSSpeechCache
VSAudioSession
VSAudioPlaybackService
VSSpeechCacheAudio
VSSpeechCacheItem
VSSpeechXPCHandler
VSSpeechXPCServiceProtocol
VSSpeechServiceDelegate
VSSpeechServerTask
VSSpeechSynthesisCoreDelegate
VSSpeechService
VSSpeechSpeakTask
VSCacheDeleteService
VSAudioDump
VSMemoryMap
VSShortTermCache
VSRemoteKeepAlive
VSKeepAliveServer
NSXPCListenerDelegate
VSServerKeepAliveManager
VSKeepAliveClient
VSSpeechServer
SpeechService
@8@0:4
@12@0:4@8
v16@0:4@8@12
v12@0:4@8
v8@0:4
@"NSString"
@52@0:4{AudioStreamBasicDescription=dIIIIIIII}8I48
B8@0:4
v12@0:4f8
f8@0:4
{AudioStreamBasicDescription=dIIIIIIII}8@0:4
v48@0:4{AudioStreamBasicDescription=dIIIIIIII}8
I8@0:4
v12@0:4I8
^{OpaqueAudioConverter=}8@0:4
v12@0:4^{OpaqueAudioConverter=}8
^{OpaqueAudioComponentInstance=}8@0:4
v12@0:4^{OpaqueAudioComponentInstance=}8
{AudioTimeStamp=dQdQ{SMPTETime=ssILLssss}LI}8@0:4
v72@0:4{AudioTimeStamp=dQdQ{SMPTETime=ssILLssss}LI}8
^{OpaqueAudioConverter=}
^{OpaqueAudioComponentInstance=}
{AudioStreamBasicDescription="mSampleRate"d"mFormatID"I"mFormatFlags"I"mBytesPerPacket"I"mFramesPerPacket"I"mBytesPerFrame"I"mChannelsPerFrame"I"mBitsPerChannel"I"mReserved"I}
{AudioTimeStamp="mSampleTime"d"mHostTime"Q"mRateScalar"d"mWordClockTime"Q"mSMPTETime"{SMPTETime="mSubframes"s"mSubframeDivisor"s"mCounter"I"mType"L"mFlags"L"mHours"s"mMinutes"s"mSeconds"s"mFrames"s}"mFlags"L"mReserved"I}
@12@0:4^{_NSZone=}8
d8@0:4
i8@0:4
v12@0:4i8
@"NSData"
@16@0:4@?8I12
^{?=IiIIIISII*}8@0:4
l8@0:4
v12@0:4l8
@?8@0:4
v12@0:4@?8
[64{?="ulMrkInfo"I"eMrkType"i"ulSrcPos"I"ulSrcTextLen"I"ulDestPos"I"ulDestLen"I"usPhoneme"S"ulMrkId"I"ulParam"I"szPromptID"*}]
@"NSError"
@"NSMutableArray"
@"NSMutableData"
@"VSSpeechEngine"
B12@0:4@8
@16@0:4@8@12
@16@0:4@8@?12
{_VE_HSAFE=^vI}8@0:4
v16@0:4{_VE_HSAFE=^vI}8
^v8@0:4
v12@0:4^v8
{_VE_LANGUAGE=[128c][4c][128c]S}8@0:4
v270@0:4{_VE_LANGUAGE=[128c][4c][128c]S}8
{?=[128c][128c][128c][128c][128c]S}8@0:4
v650@0:4{?=[128c][128c][128c][128c][128c]S}8
@"VSSpeechSynthesisCallbackResult"
{_VE_HSAFE="pHandleData"^v"u32Check"I}
{_VE_LANGUAGE="szLanguage"[128c]"szLanguageTLW"[4c]"szVersion"[128c]"u16LangId"S}
{?="szVersion"[128c]"szLanguage"[128c]"szVoiceName"[128c]"szVoiceAge"[128c]"szVoiceType"[128c]"u16LangId"S}
v16@0:4@8@?12
v24@0:4@8@12@16@?20
@20@0:4@8@12@16
@"NSLock"
@"VSSpeechCache"
@"VSShortTermCache"
@"NSObject<OS_dispatch_queue>"
B52@0:4@8{AudioStreamBasicDescription=dIIIIIIII}12
@16@0:4@8^@12
@"VSSpeechRequest"
@"<VSSpeechSynthesisCoreDelegate>"
@"VSInstrumentMetrics"
@"VSCachingService"
@"VSVoiceAssetSelection"
@"VSVoiceResourceAsset"
@"VSVoiceBooster"
@12@0:4l8
v20@0:4@8@?12@?16
@"AFClientLite"
@"CKKnowledgeStore"
v24@0:4@8@12@16^v20
#8@0:4
@12@0:4:8
@16@0:4:8@12
@20@0:4:8@12@16
B12@0:4#8
B12@0:4:8
Vv8@0:4
^{_NSZone=}8@0:4
B12@0:4@"Protocol"8
@"NSString"8@0:4
@"VSSpeechRequest"8@0:4
@"VSInstrumentMetrics"8@0:4
@"<AFAudioPowerProviding>"8@0:4
@"VSPresynthesizedAudioRequest"
@"<VSSpeechServiceDelegate>"
@"VSAudioPlaybackService"
i16@0:4@8@12
@20@0:4i8@12@16
B28@0:4^@8^i12i16@20@24
i16@0:4@"NSString"8@"NSString"12
@"NSString"20@0:4i8@"NSString"12@"NSString"16
B28@0:4^@8^i12i16@"NSString"20@"NSString"24
@"NSDictionary"8@0:4
B12@0:4@"NSDictionary"8
@"VSRecognitionResult"12@0:4@"VSRecognitionResult"8
@"VSRecognitionResult"16@0:4@"VSRecognitionResult"8@"NSDictionary"12
B16@0:4^f8^f12
@"<AFAudioPowerProviding>"
v12@0:4@"<VSSpeechSpeakableProtocol>"8
{_opaque_pthread_mutex_t=l[40c]}8@0:4
v52@0:4{_opaque_pthread_mutex_t=l[40c]}8
{_opaque_pthread_mutexattr_t=l[8c]}8@0:4
v20@0:4{_opaque_pthread_mutexattr_t=l[8c]}8
Q8@0:4
v16@0:4Q8
@"<VSSpeechTaskProtocol>"
{_opaque_pthread_mutexattr_t="__sig"l"__opaque"[8c]}
{_opaque_pthread_mutex_t="__sig"l"__opaque"[40c]}
v12@0:4B8
@"VSSpeechSpeakTask"
@20@0:4@8@12l16
l20@0:4B8l12l16
v16@0:4B8l12
{?="category"i"activity"l}
^{__CFBag=}
d52@0:4I8{AudioStreamBasicDescription=dIIIIIIII}12
I56@0:4d8{AudioStreamBasicDescription=dIIIIIIII}16
@52@0:4I8{AudioStreamBasicDescription=dIIIIIIII}12
@20@0:4@8i12@16
^{OpaqueAudioQueue=}8@0:4
v12@0:4^{OpaqueAudioQueue=}8
{_opaque_pthread_cond_t=l[24c]}8@0:4
v36@0:4{_opaque_pthread_cond_t=l[24c]}8
v16@0:4d8
^{OpaqueAudioQueue=}
{_opaque_pthread_cond_t="__sig"l"__opaque"[24c]}
@"NSData"8@0:4
@16@0:4@"NSString"8@"NSData"12
@"NSArray"
@"VSAudioData"
Vv12@0:4@8
Vv12@0:4l8
Vv16@0:4@8@?12
Vv20@0:4@8@12@?16
Vv12@0:4@?8
Vv28@0:4@8l12l16l20@?24
Vv12@0:4B8
Vv12@0:4@"NSString"8
Vv12@0:4@"VSSpeechRequest"8
Vv12@0:4@"VSPresynthesizedAudioRequest"8
Vv16@0:4@"NSString"8@?<v@?@"NSArray">12
Vv20@0:4@"NSString"8@"NSString"12@?<v@?@"NSArray">16
Vv12@0:4@?<v@?B>8
Vv12@0:4@?<v@?@"AFXPCWrapper">8
Vv12@0:4@?<v@?@"NSError">8
Vv12@0:4@?<v@?@"NSArray"@"NSError">8
Vv12@0:4@"NSArray"8
Vv12@0:4@?<v@?@"NSArray">8
Vv16@0:4@"NSString"8@?<v@?@"VSVoiceResourceAsset">12
Vv28@0:4@"NSString"8l12l16l20@?<v@?@"VSVoiceAsset">24
Vv16@0:4@"VSVoiceAsset"8@?<v@?@"NSArray"@"NSError">12
Vv20@0:4l8{_NSRange=II}12
Vv20@0:4B8@12@16
Vv16@0:4@8@12
Vv20@0:4@8@12@16
Vv16@0:4B8@12
Vv20@0:4B8@"NSString"12@"NSError"16
Vv12@0:4@"VSInstrumentMetrics"8
Vv16@0:4@"VSSpeechRequest"8@"NSArray"12
Vv20@0:4@"VSSpeechRequest"8@"VSInstrumentMetrics"12@"NSError"16
Vv16@0:4B8@"NSError"12
Vv16@0:4@"VSInstrumentMetrics"8@"NSError"12
@"NSXPCConnection"
@"AFAudioPowerUpdater"
v16@0:4@"VSSpeechSynthesisCore"8@"VSAudioData"12
v16@0:4@"VSSpeechSynthesisCore"8@"NSArray"12
@16@0:4@8B12
@"VSSpeechServerTask"
@"VSSpeechSynthesisCore"
@"<VSSpeechCacheItem>"
@16@0:4@8i12
q12@0:4@8
I12@0:4@8
L8@0:4
v24@0:4@8@12d16
@"NSCache"
Vv16@0:4i8B12
B16@0:4@8@12
B16@0:4@"NSXPCListener"8@"NSXPCConnection"12
@"NSXPCListener"
@"VSServerKeepAliveManager"
@"NSMutableSet"
@"NSObject<OS_os_transaction>"
^{__CFRunLoopSource=}
^{__CFRunLoopSource=}8@0:4
v12@0:4^{__CFRunLoopSource=}8
f16@0:4d8
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
<key>BuildMachineOSBuild</key>
<string>17A405001</string>
<key>CFBundleDevelopmentRegion</key>
<string>English</string>
<key>CFBundleExecutable</key>
<string>voiced</string>
<key>CFBundleIdentifier</key>
<string>com.apple.voiced</string>
<key>CFBundleInfoDictionaryVersion</key>
<string>6.0</string>
<key>CFBundleName</key>
<string>voiced</string>
<key>CFBundlePackageType</key>
<string>FMWK</string>
<key>CFBundleShortVersionString</key>
<string>1.0.0</string>
<key>CFBundleSignature</key>
<string>????</string>
<key>CFBundleSupportedPlatforms</key>
<array>
<string>WatchSimulator</string>
</array>
<key>CFBundleVersion</key>
<string>1.0</string>
<key>DTCompiler</key>
<string>com.apple.compilers.llvm.clang.1_0</string>
<key>DTPlatformBuild</key>
<string></string>
<key>DTPlatformName</key>
<string>watchsimulator</string>
<key>DTPlatformVersion</key>
<string>5.0</string>
<key>DTSDKBuild</key>
<string>16R5283d</string>
<key>DTSDKName</key>
<string>watchsimulator5.0</string>
<key>DTXcode</key>
<string>1000</string>
<key>DTXcodeBuild</key>
<string>10N162a</string>
<key>MinimumOSVersion</key>
<string>5.0</string>
<key>UIDeviceFamily</key>
<array>
<integer>1</integer>
<integer>2</integer>
</array>
</dict>
</plist>
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
<key>com.apple.Contacts.database-allow</key>
<true/>
<key>com.apple.coreaudio.allow-opus-codec</key>
<true/>
<key>com.apple.coreaudio.register-internal-aus</key>
<true/>
<key>com.apple.private.assets.accessible-asset-types</key>
<array>
<string>com.apple.MobileAsset.VoiceServices.CustomVoice</string>
<string>com.apple.MobileAsset.VoiceServices.VoiceResources</string>
<string>com.apple.MobileAsset.VoiceServicesVocalizerVoice</string>
<string>com.apple.MobileAsset.VoiceServices.GryphonVoice</string>
</array>
<key>com.apple.private.assets.change-daemon-config</key>
<true/>
<key>com.apple.private.coreaudio.borrowaudiosession.allow</key>
<true/>
<key>com.apple.private.tcc.allow</key>
<array>
<string>kTCCServiceAddressBook</string>
<string>kTCCServiceMicrophone</string>
<string>kTCCServiceMediaLibrary</string>
</array>
<key>com.apple.siri.client_lite</key>
<true/>
</dict>
</plist>
zPLR
zPLR
zPLR
zPLR
zPLR
