mcpl
supo
?ffffff
NSt3__114basic_ifstreamIcNS_11char_traitsIcEEEE
NSt3__113basic_filebufIcNS_11char_traitsIcEEEE
@mcpl
NSt3__118basic_stringstreamIcNS_11char_traitsIcEENS_9allocatorIcEEEE
NSt3__115basic_stringbufIcNS_11char_traitsIcEENS_9allocatorIcEEEE
NSt3__119basic_ostringstreamIcNS_11char_traitsIcEENS_9allocatorIcEEEE
xfua2vpelppa
init
zeroFilterWindowSizeInMs
bytes
initWithToken:sampleRate:numChannels:
getZeroStatisticsFromBuffer:entireSamples:
stopReportZeroStatistics
.cxx_destruct
_methodToken
_continuousZeroCounter
_zeroCounterWinSz
_numChannels
_sampleRate
numberWithUnsignedInteger:
getNumberForKey:category:default:
unsignedIntegerValue
numberWithUnsignedInt:
unsignedIntValue
numberWithFloat:
floatValue
SSVNoiseLevelChannelBitset
SSVLKFSChannelBitset
SSVNoiseLowerPercentile
SSVNoiseUpperPercentile
SSVLKFSLowerPercentile
SSVLKFSUpperPercentile
SSVNoiseTimeConstant
SSVNoiseMicSensitivityOffset
SSVLKFSTimeConstant
SSVLKFSMicSensitivityOffset
SSVNoiseTTSMappingInputRangeLow
SSVNoiseTTSMappingInputRangeHigh
SSVNoiseTTSMappingOutputRangeLow
SSVNoiseTTSMappingOutputRangeHigh
SSVLKFSTTSMappingInputRangeLow
SSVLKFSTTSMappingInputRangeHigh
SSVLKFSTTSMappingOutputRangeLow
SSVLKFSTTSMappingOutputRangeHigh
SSVUserOffsetInputRangeLow
SSVUserOffsetInputRangeHigh
SSVUserOffsetOutputRangeLow
SSVUserOffsetOutputRangeHigh
SSVTTSVolumeLowerLimitDB
SSVTTSVolumeUpperLimitDB
dictionaryWithObjects:forKeys:count:
SSVParameterDirectionary
alloc
localeWithLocaleIdentifier:
setLocale:
setDateFormat:
stringFromDate:
stringWithFormat:
_removeOldLoggingFilesIfNeededAtDirectory:
URLWithString:
initWithURL:inputFormat:outputFormat:
daysBeforeRemovingLogFiles
removeLogFilesInDirectory:matchingPattern:beforeDays:
createAudioFileWriterWithLoggingDir:inputFormat:outputFormat:
dateWithTimeIntervalSinceNow:
countByEnumeratingWithState:objects:count:
distantFuture
getResourceValue:forKey:error:
localizedDescription
compare:
removeItemAtURL:error:
_URLsInDirectory:matchingPattern:completion:
_contentsOfDirectoryAtURL:matchingPattern:includingPropertiesForKeys:error:
regularExpressionWithPattern:options:error:
contentsOfDirectoryAtURL:includingPropertiesForKeys:options:error:
lastPathComponent
length
numberOfMatchesInString:options:range:
predicateWithBlock:
filteredArrayUsingPredicate:
class
description
UTF8String
_getSystemVolumeDB:
_setDefaultParameters
_setAsset:
_convertDB2Mag:
getNumElementInBitset:
_reset
_resetVoiceTriggerInfo
_resetStartAnalyzeTime
inputRecordingSampleByteDepth
convertToFloatLPCMBufFromShortLPCMBuf:
startSampleCount
_setStartAnalyzeTime:
dataForChannel:
iterateBitset:block:
_prepareSoundLevelBufferFromSamples:soundType:
numSamples
subChunkFrom:numSamples:
_processAudioChunk:soundType:
objectForKeyedSubscript:
intValue
_estimatedTTSVolume:lowerLimit:upperLimit:TTSmappingInputRangeLow:TTSmappingInputRangeHigh:TTSmappingOutputRangeLow:TTSmappingOutputRangeHigh:
_combineResultsWithOptimalFromNoise:andOptimalFromLkfs:withUserOffset:
scaleInputWithInRangeOutRange:minIn:maxIn:minOut:maxOut:
isEqual:
self
performSelector:
performSelector:withObject:
performSelector:withObject:withObject:
isProxy
isKindOfClass:
isMemberOfClass:
conformsToProtocol:
respondsToSelector:
retain
release
autorelease
retainCount
zone
hash
superclass
debugDescription
speechManagerRecordBufferAvailable:buffer:
speechManagerLPCMRecordBufferAvailable:chunk:
speechManagerDidStartForwarding:successfully:error:
speechManagerDidStopForwarding:forReason:
speechManagerRecordingContext
speechManagerRecordHardwareConfigurationDidChange:toConfiguration:
speechManagerDetectedSystemVolumeChange:withVolume:
speechManagerBeginRecordInterruption:
speechManagerBeginRecordInterruption:withContext:
speechManagerEndRecordInterruption:
voiceTriggerDidDetectKeyword:
voiceTriggerDidDetectNearMiss:
voiceTriggerDidDetectSpeakerReject:
voiceTriggerDidDetectTwoShotAtTime:
keywordDetectorDidDetectKeyword
initWithSamplingRate:asset:systemVolume:
setAsset:
prepareSoundLevelBufferFromSamples:soundType:firedVoiceTriggerEvent:triggerStartTimeSampleOffset:triggerEndTimeSampleOffset:
estimateSoundLevelbySoundType:
pauseLKFSProcessing
resumeLKFSProcessing
reset
estimatedTTSVolumeForNoiseLevelAndLKFS:LKFS:
.cxx_construct
_queue
_smartSiriVolumeNoiseLevel
_smartSiriVolumeLKFS
_floatBuffer
_defaults
_startAnalyzeSampleCount
_samplesFed
_isStartSampleCountMarked
_firedTriggerEvent
_triggerEndTimeSampleOffset
_triggerStartTimeSampleOffset
_shouldPauseLKFSProcess
_currentAsset
_systemVolumeDB
_noiseLevelChannelBitset
_LKFSChannelBitset
_noiseLowerPercentile
_noiseUpperPercentile
_LKFSLowerPercentile
_LKFSUpperPercentile
_noiseTimeConstant
_noiseMicSensitivityOffset
_LKFSTimeConstant
_LKFSMicSensitivityOffset
_noiseTTSMappingInputRangeLow
_noiseTTSMappingInputRangeHigh
_noiseTTSMappingOutputRangeLow
_noiseTTSMappingOutputRangeHigh
_LKFSTTSMappingInputRangeLow
_LKFSTTSMappingInputRangeHigh
_LKFSTTSMappingOutputRangeLow
_LKFSTTSMappingOutputRangeHigh
_userOffsetInputRangeLow
_userOffsetInputRangeHigh
_userOffsetOutputRangeLow
_userOffsetOutputRangeHigh
_TTSVolumeLowerLimitDB
_TTSVolumeUpperLimitDB
stringByAppendingFormat:
copy
close
dealloc
dataWithBytes:length:
initWithURL:
loadAllSamples
readSamplesFromChannelIdx:
numChannels
_fFile
_deinterleavedData
_frameSize
_sampleByteDepth
setActivePhraseId:
channelForProcessedInput
setActiveChannel:
CVTConfigPathNDAPI
resourcePath
initWithConfigPath:resourcePath:
setDelegate:
CVTThreshold
inputRecordingSampleRate
CVTTwoShotDecisionWaitTime
CVTTwoShotThreshold
doubleValue
processAudioChunk:
_shotAnalyzerNDAPI:hasResultAvailable:forChannel:
_keywordAnalyzerNDAPI:hasResultAvailable:forChannel:
supportCSTwoShotDecision
numberWithBool:
keywordAnalyzerNDAPI:hasResultAvailable:forChannel:
initWithManager:asset:
start
startDetectTwoShot:
delegate
speechManager
setSpeechManager:
queue
setQueue:
currentAsset
setCurrentAsset:
keywordAnalyzer
setKeywordAnalyzer:
keywordThreshold
setKeywordThreshold:
mode
setMode:
analyzedSampleCount
setAnalyzedSampleCount:
triggerEndSampleCount
setTriggerEndSampleCount:
twoShotDecisionWaitSamples
setTwoShotDecisionWaitSamples:
twoShotThreshold
setTwoShotThreshold:
activeChannel
_delegate
_speechManager
_keywordAnalyzer
_keywordThreshold
_mode
_analyzedSampleCount
_triggerEndSampleCount
_twoShotDecisionWaitSamples
_twoShotThreshold
_activeChannel
lpcmNarrowBandASBD
lpcmASBD
initWithInASBD:outASBD:
_createSampleRateConverterWithInASBD:outASBD:
dataWithLength:
mutableBytes
stringWithUTF8String:
currentHandler
handleFailureInMethod:object:file:lineNumber:description:
setLength:
upsampler
downsampler
convertSampleRateOfBuffer:
_sampleRateConverter
_outBufferScaleFactor
_inASBD
_outASBD
initWithUTF8String:
initWithXPCObject:
xpcObject
getSiriLanguageWithFallback:
initWithLength:
convertToShortLPCMBufFromFloatLPCMBuf:
modelDirectory
_createDirectoryIfNotExist:
utteranceDirectory
defaultManager
fileExistsAtPath:isDirectory:
removeItemAtPath:error:
createDirectoryAtPath:withIntermediateDirectories:attributes:error:
sharedPreferences
CSSATBasePath
stringByAppendingPathComponent:
_satPath
array
contentsOfDirectoryAtPath:error:
predicateWithFormat:
caseInsensitiveCompare:
sortedArrayUsingSelector:
addObject:
count
modelPath
fileExistsAtPath:
_isDirectoryEmpty:
initWithSpeakerModelFileName:languageCode:
enrollmentUtterance
needsRetrain
discard
isValid
_modelFileName
_languageCode
_modelPath
_utteranceDirectory
retrainSpeakerModel:forVoiceTriggerAsset:
voiceTriggerEnabledPolicy
_transitVoiceTriggerStatus:
setCallback:
isEnabled
VTFirstPassThreshold
VTFirstPassDelaySecondsForChannelSelection
VTFirstPassMasterChannelScoreBoost
VTFirstPassProcessingChunkSeconds
VTFirstPassProcessingChannelsBitset
removeAllObjects
VTFirstPassConfigPathNDAPI
_startVoiceTrigger
_stopVoiceTrigger
startRecordingWithSetting:event:error:
stopRecordingWithEvent:
_reportVoiceTriggerFirstPassFire
mutableCopy
setObject:forKey:
resetBest
voiceTriggerFirstPass:didDetectKeyword:
voiceTriggerStartPolicy
setVoiceTriggerStartPolicy:
voiceTriggerEnabled
setVoiceTriggerEnabled:
keywordAnalyzersNDAPI
setKeywordAnalyzersNDAPI:
hasTriggerPending
setHasTriggerPending:
firstPassThreshold
setFirstPassThreshold:
bestScore
setBestScore:
bestChannel
setBestChannel:
onsetResult
setOnsetResult:
onsetChannel
setOnsetChannel:
channelSelectionDelay
setChannelSelectionDelay:
delayInSamplesRequiredForChannelSelection
setDelayInSamplesRequiredForChannelSelection:
masterChannelScoreBoost
setMasterChannelScoreBoost:
channelSelectionScores
setChannelSelectionScores:
processingChunkSamples
setProcessingChunkSamples:
processingChannelsBitset
setProcessingChannelsBitset:
_voiceTriggerEnabled
_hasTriggerPending
_voiceTriggerStartPolicy
_keywordAnalyzersNDAPI
_firstPassThreshold
_bestScore
_bestChannel
_onsetResult
_onsetChannel
_channelSelectionDelay
_delayInSamplesRequiredForChannelSelection
_masterChannelScoreBoost
_channelSelectionScores
_processingChunkSamples
_processingChannelsBitset
initWithMachServiceName:options:
setRemoteObjectInterface:
getCoreSpeechServiceConnection
setInvalidationHandler:
resume
remoteObjectProxy
setDelayInterstitialSounds:level:completion:
getTriggerCount:
clearTriggerCount:
setDelayInterstitialSounds:level:
getTriggerCount
clearTriggerCount
inputRecordingFramesPerPacket
inputRecordingSampleRateNarrowBand
inputRecordingBytesPerFrame
inputRecordingBytesPerPacket
inputRecordingNumberOfChannels
inputRecordingDurationInSecs
inputRecordingSampleBitDepth
inputRecordingEncoderAudioQuality
inputRecordingSampleRateConverterAlgorithm
inputRecordingBufferDuration
audioConverterBitrate
channelForOutputReference
zeroFilterApproxAbsSpeechThreshold
csAudioProcessingQueuePriority
weakObjectsHashTable
_setupStateMachine
_setupCircularBuffer
_createListenPollingTimer
supportAlwaysListening
_setupSmartSiriVolume
_setupVoiceTrigger
defaultCenter
removeObserver:
_startForwardingToSmartSiriVolume
_notifyEvent:
mediaserverdDidRestart
addObserver:selector:name:object:
sharedInstance
addObserver:
willDestroy
supportContinuousVoiceTrigger
shouldRunVTOnCS
sharedManager
assetForCurrentLanguageOfType:
addObserver:forAssetType:
defaultFallBackAssetForVoiceTrigger:
_getVoiceTriggerAsset
isEqualToString:
initWithAudioBuffer:
registerObserver:
supportSelfTriggerSuppression
supportKeywordDetector
initWithNumChannels:recordingDuration:samplingRate:
_volumeFromAVSystemController
_getSmartSiriVolumeAsset
initWithInitialState:
addTransitionFrom:to:for:
currentState
_eventName:
performTransitionForEvent:
audioRecorder
initWithContext:error:
setAudioRecorder:
isRecording
setCurrentContext:error:
_getClientRecordContext
isRecordContextVoiceTrigger:
unsignedLongLongValue
hostTimeToTimeInterval:
_scheduleSetRecordModeToRecordingWithDelay:forReason:validator:completion:
_setRecordMode:error:
prepareRecordWithSettings:error:
prewarmAudioSession
recordRoute
recordSettings
isNarrowBand
errorWithDomain:code:userInfo:
_createRecorderWithContextIfNeeded:error:
_prepareRecorderWithSettings:error:
startRecordingWithSettings:error:
_cancelPendingSetRecordModeToRecordingForReason:
releaseClientAudioSession:
_releaseAudioSessionForListening:error:
_releaseClientAudioSession:
releaseAudioSession:
sampleCount
_startRecordingWithSettings:error:
_reinitializeVoiceTriggerIfNeeded
_startRecordingForClient:error:
notifyEvent:
supportOpportunisticZLL
sampleCountFromHostTime:
stopRecording
_stateName:
_stopForwardingToFirstPassVoiceTrigger
_stopForwardingToSelfTriggerDetector
_stopForwardingToSecondPassVoiceTrigger
_stopForwardingToClient
_stopForwardingToContinuousVoiceTrigger
_stopForwardingToKeywordDetector
_startForwardingToFirstPassVoiceTrigger
_startForwardingToSelfTriggerDetector
_startForwardingToSecondPassVoiceTrigger
_startForwardingToClient
_startForwardingToContinuousVoiceTrigger
_startForwardingToKeywordDetector
_startListenPolling
_destroyAudioRecorderIfNeeded
voiceTriggerRecordContext
lpcmRecordSettings
_prepareListenWithSettings:error:
_startListening:
_stopListenPolling
removeObject:
containsObject:
setIsContinuousRunningMode:
voiceTriggerInfo
startDetectKeyword:
initWithData:numChannels:numSamples:sampleByteDepth:startSampleCount:hostTime:
_performPendingSetRecordModeToRecordingForReason:
hostTimeFromSampleCount:
_reinitializeVoiceTriggerWithAsset:
isEqualAsset:
_reinitializeSmartSiriVolumeWithAsset:
numberWithInteger:
audioRecorderBufferAvailable:buffer:atTime:
audioRecorderBufferAvailable:buffer:
audioRecorderDidStartRecording:successfully:error:
audioRecorderDidStopRecording:forReason:
audioRecorderRecordHardwareConfigurationDidChange:toConfiguration:
audioRecorderBeginRecordInterruption:
audioRecorderBeginRecordInterruption:withContext:
audioRecorderEndRecordInterruption:
voiceTriggerDetectedOnAOP:
audioRecorderDisconnected:
audioRecorderLostMediaserverd:
didTransitFrom:to:by:
didIgnoreEvent:from:
CSAssetManagerDidDownloadNewAsset:
CSLanguageCodeUpdateMonitor:didReceiveLanguageCodeChanged:
initWithVoiceTriggerFirstPass:voicetriggerSecondPass:voicetriggerEventNotifier:audioRecorder:
startManager
registerSpeechController:
getCurrentState
isClientRecording
setClientContext:error:
prepareRecordingForClient:error:
_setCurrentContext:error:
releaseClientAudioSession
_systemVolumeDidChange:
getEstimatedTTSVolume
assetQueryQueue
setAssetQueryQueue:
stateMachine
setStateMachine:
audioBuffer
setAudioBuffer:
currentVoiceTriggerAsset
setCurrentVoiceTriggerAsset:
voiceTriggerFirstPass
setVoiceTriggerFirstPass:
voiceTriggerSecondPass
setVoiceTriggerSecondPass:
clientController
setClientController:
voiceTriggerEventNotifier
setVoiceTriggerEventNotifier:
voiceTriggerFileLogger
setVoiceTriggerFileLogger:
selfTriggerDetector
setSelfTriggerDetector:
continuousVoiceTrigger
setContinuousVoiceTrigger:
keywordDetector
setKeywordDetector:
smartSiriVolume
setSmartSiriVolume:
myriad
setMyriad:
voiceTriggerFidesClient
setVoiceTriggerFidesClient:
activeAudioProcessors
setActiveAudioProcessors:
continuousAudioProcessors
setContinuousAudioProcessors:
lastForwardedSampleCount
setLastForwardedSampleCount:
secondPassStartSampleCount
setSecondPassStartSampleCount:
clientStartSampleCount
setClientStartSampleCount:
recordingPendingTimeout
setRecordingPendingTimeout:
lastVoiceTriggerEventInfo
setLastVoiceTriggerEventInfo:
listenPollingTimer
setListenPollingTimer:
pendingSetRecordModeToRecordingToken
setPendingSetRecordModeToRecordingToken:
pendingSetRecordModeToRecordingCompletion
setPendingSetRecordModeToRecordingCompletion:
systemVolumeValue
setSystemVolumeValue:
_audioRecorder
_assetQueryQueue
_stateMachine
_audioBuffer
_currentVoiceTriggerAsset
_voiceTriggerFirstPass
_voiceTriggerSecondPass
_clientController
_voiceTriggerEventNotifier
_voiceTriggerFileLogger
_selfTriggerDetector
_continuousVoiceTrigger
_keywordDetector
_smartSiriVolume
_myriad
_voiceTriggerFidesClient
_activeAudioProcessors
_continuousAudioProcessors
_lastForwardedSampleCount
_secondPassStartSampleCount
_clientStartSampleCount
_recordingPendingTimeout
_lastVoiceTriggerEventInfo
_listenPollingTimer
_pendingSetRecordModeToRecordingToken
_pendingSetRecordModeToRecordingCompletion
_systemVolumeValue
lowercaseString
hasPrefix:
substringFromIndex:
replaceMatchesInString:options:range:withTemplate:
_stringByStrippingLeadingNoise:
_stringByStrippingTrailingNoise:
rangeOfString:options:
stringValue
_firstMatchesForRegularExpression:
firstMatchInString:options:range:
numberOfRanges
rangeAtIndex:
substringWithRange:
_stringByFixingNamePattern:
_stringByStrippingNoiseLeadingNoise:TrailingNoise:
_hasSubstring:
_matchesRegularExpression:
_caseInsensitiveHasMatchInEnumeration:
_firstMatchesForRegularExpressions:
getVoiceTriggerAssetTypeString
getEndpointAssetTypeString
dictionary
assetManagerEnabledPolicy
_fetchRemoteMetaData
assetOfType:language:
installedAssetOfType:language:
_isReadyToUse
predicateForAssetType:language:
installedAssetOfType:withPredicate:
_fetchRemoteAssetOfType:withPredicate:
_installedAssetOfType:withPredicate:
getCSAssetOfType:
_assetQueryForAssetType:withPredicate:localOnly:
runQueryAndReturnError:
predicate
_findLatestInstalledAsset:
state
isLatestCompareTo:
initWithAssetType:
setPredicate:
setQueriesLocalAssetInformationOnly:
startQuery:
isSpringboardStarted
isFirstUnlocked
supportHybridEndpointer
predicateForfetchRemoteMetadataForAssetType:
_runAssetQuery:completion:
_updateFromRemoteToLocalAssets:forAssetType:
isInstalled
isDownloading
cancelDownloadAndReturnError:
path
purgeAndReturnError:
_downloadAsset:withComplete:
_startDownloadingAsset:progress:completion:
objectForKey:
setProgressHandler:
requiredDiskSpaceIsAvailable:error:
_defaultDownloadOptions
beginDownloadWithOptions:
resumeDownload:
adjustDownloadOptions:completion:
setObject:forKeyedSubscript:
CSVoiceTriggerAssetMetaUpdateMonitor:didReceiveNewVoiceTriggerAssetMetaData:
CSSpeechEndpointAssetMetaUpdateMonitor:didReceiveNewSpeechEndpointAssetMetaData:
installedAssetForCurrentLanguageOfType:
currentLanguageCode
removeObserver:forAssetType:
_csAssetsDictionary
_enablePolicy
_currentLanguageCode
_observers
getVoiceTriggerAssetCurrentCompatibilityVersion
getEndpointAssetCurrentCompatibilityVersion
supportPremiumAssets
componentsJoinedByString:
predicateWithFormat:argumentArray:
getStringForKey:category:default:
setupPhraseSpotter
startMasterTimerWithTimeout:
resultAlreadyReported
stopMasterTimer
closeSessionWithStatus:successfully:complete:
closeSessionWithCompletion:
updateMeterAndForward
pushAudioInputIntoPCMBuffer:
requestTriggeredUtterance:
sharedTrainer
trainUtterance:languageCode:
setupSpeechRecognitionTaskWithVoiceTriggerEventInfo:
trimBeginingOfPCMBufferWithVoiceTriggerEventInfo:
computeRequiredTrailingSamples
feedSpeechRecognitionWithPCMBuffer
closeSessionWithStatus:successfully:
handleAudioBufferForVTWithAudioInput:withDetectedBlock:
finishSpeechRecognitionTask
feedSpeechRecognitionTrailingSamplesWithCompletedBlock:
triggeredUtterance:
updateMeters
averagePower
CSVTUITrainingSessionRMSAvailable:
analyze:
boolValue
numSamplesInPCMBuffer
objectAtIndex:
appendAudioPCMBuffer:
removeObjectAtIndex:
frameLength
initWithCommonFormat:sampleRate:channels:interleaved:
initWithPCMFormat:frameCapacity:
mutableAudioBufferList
setFrameLength:
replaceObjectAtIndex:withObject:
removeObjectsInRange:
createAVAudioPCMBufferWithNSData:
handleAudioInput:
sharedGrammars
getLMEforLocale:
setContextualStrings:
setTaskHint:
_setVoiceTriggerEventInfo:
recognitionTaskWithRequest:delegate:
endAudio
finish
handleMasterTimeout:
scheduledTimerWithTimeInterval:target:selector:userInfo:repeats:
invalidate
formattedString
whitespaceAndNewlineCharacterSet
stringByTrimmingCharactersInSet:
speechRecognitionDidDetectSpeech:
speechRecognitionTask:didHypothesizeTranscription:
speechRecognitionTask:didFinishRecognition:
speechRecognitionTaskFinishedReadingAudio:
speechRecognitionTaskWasCancelled:
speechRecognitionTask:didFinishSuccessfully:
audioSessionDidStartRecording:error:
audioSessionDidStopRecording:
audioSessionRecordBufferAvailable:
audioSessionUnsupportedAudioRoute
audioSessionErrorDidOccur:
didDetectBeginOfSpeech
didDetectEndOfSpeech:
initWithUtteranceId:sessionNumber:Locale:audioSession:keywordDetector:speechRecognizer:speechRecognitionRequest:sessionDelegate:sessionDispatchQueue:completion:
startTraining
suspendTraining
resumeTraining
_status
_utteranceId
_sessionNumber
_locale
_audioSession
_speechRecognizer
_speechRecognitionRequest
_speechRecognitionTask
_masterTimer
_pcmBufArray
_resultReported
_sessionProcess
_sessionSuspended
_ASRErrorOccured
_sessionDelegate
_trainingCompletion
_numRequiredTrailingSamples
_numTrailingSamples
initWithManager:
startController
getFixedHighPrioritySerialQueueWithLabel:
twoShotNotificationEnabled
_currentAudioRecorderSampleRate
_contextToString:
_getRecordSettings
_setupDownsamplerIfNeeded
_setupAudioConverter:
numberWithUnsignedLong:
numberWithInt:
duckOthersOption
setDuckOthersOption:
_isVoiceTriggered
resetForNewRequestWithSampleRate:
fileLoggingIsEnabled
assistantAudioFileLogDirectory
lpcmNonInterleavedASBD
lpcmInterleavedASBD
sampleByteDepth
hostTime
processAudioSamplesAsynchronously:
addSamples:timestamp:
speechControllerLPCMRecordBufferAvailable:buffer:
data
addSamples:len:
channels
packetDescriptionCount
bytesDataSize
initWithCapacity:
packetDescriptions
initWithBytes:length:
timeStamp
speechControllerRecordBufferAvailable:buffers:recordedAt:
speechControllerDidStartRecording:successfully:error:
recordingStoppedForReason:
flush
speechControllerDidStopRecording:forReason:
_deviceAudioLogging
speechControllerRecordHardwareConfigurationDidChange:toConfiguration:
speechControllerDidUpdateSmartSiriVolume:forReason:
speechControllerBeginRecordInterruption:
speechControllerBeginRecordInterruption:withContext:
speechControllerEndRecordInterruption:
narrowBandOpusConverter
opusConverter
setAlertSoundFromURL:forType:
playAlertSoundForType:
alertStartTime
playRecordStartingAlertAndResetEndpointer
setMeteringEnabled:
peakPowerForChannel:
averagePowerForChannel:
passThruVoiceTriggerInfo
resetForVoiceTriggerTwoShotWithSampleRate:
speechControllerDidDetectVoiceTriggerTwoShot:atTime:
speechControllerRequestsOperation:forReason:
metrics
setEndpointerDelegate:
processServerEndpointFeatures:
allKeys
initWithData:encoding:
lastEndOfVoiceActivityTime
endpointerModelVersion
updateEndpointerThreshold:
updateEndpointerDelayedTrigger:
shouldAcceptEagerResultForDuration:resultsCompletionHandler:
sharedController
audioConverterDidConvertPackets:packets:timestamp:
initializeRecordSessionWithContext:
preheat
resetAudioSession
releaseAudioSession
getLPCMAudioStreamBasicDescription
setSynchronousCallbackEnabled:
setRecordBufferDuration:
getRecordBufferDuration
startRecording:
isVoiceTriggered
peakPowerForOutputReference
averagePowerForOutputReference
outputReferenceChannel
endpointAnalyzer
setEndpointAnalyzerDelegate:
resetEndpointer
_getSpeechIdentifier
getSmartSiriVolume
endpointerProxy
setEndpointerProxy:
avvcContext
setAvvcContext:
isOpus
setIsOpus:
isActivated
setIsActivated:
setIsNarrowBand:
audioFileWriter
setAudioFileWriter:
setTwoShotNotificationEnabled:
_opusAudioConverter
_narrowBandOpusConverter
_audioConverter
_downsampler
_requestedRecordSettings
_lastVoiceTriggerInfo
_isOpus
_isActivated
_isNarrowBand
_twoShotNotificationEnabled
_endpointerProxy
_avvcContext
_audioFileWriter
setFileLoggingLevel:
fileLoggingLevel
baseDir
assistantLogDirectory
arrayWithObjects:count:
enumeratorAtURL:includingPropertiesForKeys:options:errorHandler:
getUserVoiceProfileUploadPathWithEnrolledLanguageList:
_CSSATUploadPath
_getEnrolledLanguageList
enumeratorAtPath:
_isDirectory:
pathExtension
copyItemAtPath:toPath:error:
_CSSATUpdatePath
isCurrentDeviceCompatibleWithVoiceProfileAt:
_markSATEnrollmentSuccessForLanguageCode:
_markSATEnrollmentMigratedForLanguageCode:
_markSATEnrollmentWithMarker:forLanguage:
createFileAtPath:contents:attributes:
deviceProductType
_deviceCategoryMap
dataWithContentsOfURL:
JSONObjectWithData:options:error:
interstitialRelativeDirForLevel:
voiceTriggerInCoreSpeech
_storeModeEnabled
setFileLoggingIsEnabled:
voiceTriggerAudioLogDirectory
getUserVoiceProfileFileList
getUserVoiceProfileUploadPath
notifyUserVoiceProfileUploadComplete
getUserVoiceProfileUpdateDirectory
notifyUserVoiceProfileUpdateReady
remoteVoiceTriggerDelayTime
remoteVoiceTriggerEndpointTimeoutWithDefault:
interstitialAbsoluteDirForLevel:
myriadFileLoggingEnabled
integerValue
initialState
setInitialState:
transitions
setTransitions:
_currentState
_initialState
_transitions
_stopMonitoring
_startMonitoringWithQueue:
enumerateObservers:
CSEventMonitorDidReceiveEvent:
enumerateObserversInQueue:
notifyObserver:
utteranceFileASBD
_closeAudioFile
fileURLWithPath:isDirectory:
startRecording
appendAudioData:
_audioFile
_asbd
_url
_audioLength
date
_audioLogDirectory
_timeStampString
dataWithJSONObject:options:error:
writeToFile:atomically:
_metaFilenameWithPrefix:
stringByReplacingOccurrencesOfString:withString:
saveRecordingBufferFrom:to:toURL:
_writeDictionary:toPath:
_removeOldLoggingFilesIfNeededWithPattern:
_addAssetManagerEnabledConditions
addConditions:
VTSecondPassThreshold
VTSecondPass2ndChanceThreshold
VTSecondPassLoggingThreshold
VTSecondPassPreTriggerAudioTime
VTSecondPassAnalyzerPrependingAudioTime
VTSecondPassAnalyzerTrailingAudioTime
VTSecondPassConfigPathNDAPI
VTSecondPassConfigPathRecognizer
VTSecondPassUseKeywordSpotting
VTSecondPassRecognizerThresholdOffset
VTSecondPassRecognizerScoreScaleFactor
VTSecondPassRecognizerToken
VTSecondPassRecognizerWaitTime
_addVoiceTriggerEnabledConditions
_subscribeEventMonitors
subscribeEventMonitor:
_didReceiveNewSpeechEndpointAssetMetaData
_notifyObserver:
_notifyToken
opusRecordSettings
alertMuteSettings
copySamplesFrom:to:
pHash:length:
signalEstimate
dataWithCapacity:
appendBytes:length:
_generateMyriadInfo:score:channel:absoluteTime:
lastHash
setSignalEstimate:
_signalEstimate
_logDESRecordWithType:result:
_lastTriggerDataWithResult:
numberWithUnsignedLongLong:
_fidesRecordInfo
matchWithString:TrailingStr:LeadingStr:Pattern:
createGrammars
bundleForClass:
bundlePath
dataWithContentsOfFile:
_getTrailingPatternsWithGrammars:withLocale:
_getLeadingPatternsWithGrammars:withLocale:
_getRegexPatternsWithGrammars:withUtt:withLocale:
_getLMEWithGrammar:withLocale:
URLSession:didBecomeInvalidWithError:
URLSession:didReceiveChallenge:completionHandler:
URLSessionDidFinishEventsForBackgroundURLSession:
getTrailingPatternsForUtt:Locale:
getLeadingPatternsForUtt:Locale:
getRegexPatternsForUtt:Locale:
_grammar
initWithLocaleIdentifier:withAudioSession:
setLocaleIdentifier:
createKeywordDetector
initWithLanguageCode:
initWithLocale:
_stopAudioSession
destroySpeakerTrainer
_destroyAudioSession
_setupAudioSession
closeSessionBeforeStartWithStatus:successfully:withCompletion:
_createAudioAnalyzer
_shouldShowHeadsetDisconnectionMessage
_startAudioSession
createSpeechRecognizer
sharedtrainingSessionQueue
_audioSource
audioSource
prepareRecord
setEndpointStyle:
setStartWaitTime:
setEndWaitTime:
setInterspeechWaitTime:
hasCorrectAudioRoute
VTUITrainingManagerFeedLevel:
VTUITrainingManagerStopListening
trainingManagerWithLocaleID:
CSVTUITrainingSessionStopListen
endpointer:didDetectStartpointAtTime:
endpointer:didDetectHardEndpointAtTime:withMetrics:
_beginOfSpeechDetected
_endOfSpeechDetected
cleanupWithCompletion:
trainUtterance:shouldUseASR:completion:
cancelTrainingForID:
suspendAudio
setSuspendAudio:
startRMS
stopRMS
shouldPerformRMS
didDetectForceEndPoint
VTUITrainingSessionStopListen
setRms:
speechRecognizerAvailable
_performRMS
_audioAnalyzer
_trainingSessions
_currentTrainingSession
_suspendAudio
_cleanupCompletion
_speechRecognizerAvailable
_rms
hasRemoteCoreSpeech
rootQueueWithFixedPriority:
getFixedPrioritySerialQueueWithLabel:fixedPriority:
analyzeWavData:numSamples:
getAnalyzedResultForPhraseId:
sampleFed
getAnalyzedResult
initWithResult:
bestStart
setBestStart:
bestEnd
setBestEnd:
getSuperVectorWithEndPoint:
activePhraseId
_novDetector
_lastSampleFed
_sampleFedWrapAroundOffset
_activePhraseId
initWithTotalAudioRecorded:featuresAtEndpoint:endpointerType:serverFeatureLatencyDistribution:additionalMetrics:
totalAudioRecorded
setTotalAudioRecorded:
featuresAtEndpoint
setFeaturesAtEndpoint:
endpointerType
setEndpointerType:
serverFeatureLatencyDistribution
setServerFeatureLatencyDistribution:
additionalMetrics
setAdditionalMetrics:
_featuresAtEndpoint
_endpointerType
_serverFeatureLatencyDistribution
_additionalMetrics
_totalAudioRecorded
waitingForConnection:error:
isConnected
localURL
string
_compatibilityVersion
appendString:
_version
appendFormat:
_footprint
assetForAssetType:resourcePath:configVersion:
attributes
isPremium
lpcmInt16ASBD
lpcmInt16NarrowBandASBD
opusASBD
opusNarrowBandASBD
aiffFileASBD
_checkVoiceTriggerEnabled
_didReceiveVoiceTriggerSettingChanged:
_notifyObserver:withEnabled:
CSVoiceTriggerEnabledMonitor:didReceiveEnabled:
_didReceiveVoiceTriggerSettingChangedInQueue:
_isVoiceTriggerEnabled
startRecordingWithOptions:error:
stopRecording:
didPlayEndpointBeep
voiceTriggerEventInfo
hasPendingTwoShotBeep
keywordDetectorThreshold
keywordDetectorConfigPathRecognizer
keywordDetectorWaitTimeSinceVT
hashFromResourcePath
initWithAsset:speakerModel:
subdataWithRange:
analyzeWavForEnrollment:numSamples:
addLastTriggerToProfile
_saveUtterance:meta:to:
stringByAppendingString:
convertStopReason:
resetEndPointer
hasAudioRoute
beepCancellerDidCancelSamples:buffer:timestamp:
cancelBeepFromSamples:timestamp:
willBeep
_beepCanceller
_beepFloatVec
_shortBuffer
_numTotalInputSamples
_numTotalOutputSamples
triggerConfidence
initWithConfigPath:triggerTokens:useKeywordSpotting:
_notifySecondPassReject
runRecognition
configVersion
numberWithDouble:
processSuperVector:withResult:
_analyzeForKeywordDetection:result:forChannel:forceMaximized:
speakerDetector:didDetectSpeaker:
speakerDetector:didDetectSpeakerReject:
keywordAnalyzerQuasar:hasResultAvailable:forChannel:
keywordAnalyzerNDAPI
setKeywordAnalyzerNDAPI:
keywordAnalyzerQuasar
setKeywordAnalyzerQuasar:
speakerDetector
setSpeakerDetector:
speakerModel
setSpeakerModel:
secondPassTimeout
setSecondPassTimeout:
numProcessedSamples
setNumProcessedSamples:
keywordLoggingThreshold
setKeywordLoggingThreshold:
lastScore
setLastScore:
extraSamplesAtStart
setExtraSamplesAtStart:
analyzerPrependingSamples
setAnalyzerPrependingSamples:
analyzerTrailingSamples
setAnalyzerTrailingSamples:
useSAT
setUseSAT:
nearMissDelayTimeout
setNearMissDelayTimeout:
nearMissCandidateDetectedSamples
setNearMissCandidateDetectedSamples:
hasPendingNearMiss
setHasPendingNearMiss:
lastAnalyzerResult
setLastAnalyzerResult:
recognizerScore
setRecognizerScore:
isRunningRecognizer
setIsRunningRecognizer:
recognizerResultPending
setRecognizerResultPending:
recognizerScoreScaleFactor
setRecognizerScoreScaleFactor:
recognizerWaitSamples
setRecognizerWaitSamples:
earlyDetectFiredMachTime
setEarlyDetectFiredMachTime:
lastResult
setLastResult:
processedSampleCountsInPending
setProcessedSampleCountsInPending:
firstPassTriggerStartSampleCount
setFirstPassTriggerStartSampleCount:
firstPassTriggerFireSampleCount
setFirstPassTriggerFireSampleCount:
firstPassChannelSelectionScores
setFirstPassChannelSelectionScores:
firstPassChannelSelectionDelaySeconds
setFirstPassChannelSelectionDelaySeconds:
firstPassMasterChannelScoreBoost
setFirstPassMasterChannelScoreBoost:
firstPassOnsetScore
setFirstPassOnsetScore:
firstPassOnsetChannel
setFirstPassOnsetChannel:
_useSAT
_hasPendingNearMiss
_isRunningRecognizer
_recognizerResultPending
_keywordAnalyzerNDAPI
_keywordAnalyzerQuasar
_speakerDetector
_speakerModel
_secondPassTimeout
_numProcessedSamples
_keywordLoggingThreshold
_lastScore
_extraSamplesAtStart
_analyzerPrependingSamples
_analyzerTrailingSamples
_nearMissDelayTimeout
_nearMissCandidateDetectedSamples
_lastAnalyzerResult
_recognizerScore
_recognizerScoreScaleFactor
_recognizerWaitSamples
_lastResult
_processedSampleCountsInPending
_firstPassTriggerStartSampleCount
_firstPassTriggerFireSampleCount
_firstPassChannelSelectionScores
_firstPassChannelSelectionDelaySeconds
_firstPassMasterChannelScoreBoost
_firstPassOnsetScore
_firstPassOnsetChannel
_earlyDetectFiredMachTime
_notifyTriggerEvent:
_notifyNearMissEvent:
_notifySpeakerReject:
_notifyTwoShotDetectionAt:
_notifyKeywordDetect
_createVoiceTriggerEventInfoString:
unregisterObserver:
isContinuousRunningMode
_isContinuousRunningMode
initWithZeroWindowSize:approxAbsSpeechThreshold:numHostTicksPerAudioSample:
filterZerosInAudioPacket:atBufferHostTime:filteredPacket:
endAudioAndFetchAnyTrailingZerosPacket:
_audioZeroFilterImpl
readAudioChunksFrom:block:
getTestResponse:
interfaceWithProtocol:
setWithArray:
setClasses:forSelector:argumentIndex:ofReply:
addSamples:numSamples:atHostTime:
sampleCountFromHostTime:anchorHostTime:anchorSampleCount:
hostTimeFromSampleCount:anchorHostTime:anchorSampleCount:
stringWithCString:encoding:
createAudioCircularBufferWithDefaultSettings
addSamples:numSamples:
copySamplesFromHostTime:
copyBufferWithNumSamplesCopiedIn:
bufferLength
setBufferLength:
_csAudioCircularBufferImpl
_anchorSampleCount
_anchorHostTime
_bufferLength
_asssetMetaUpdatedKey
_didReceiveNewVoiceTriggerAssetMetaData
speakerDetectorNDAPIConfigPath
speakerDetectorThreshold
speakerDetectorRetrainTriggerThreshold
initWithWordCount:trailingSilenceDuration:eosLikelihood:pauseCounts:silencePosterior:taskName:processedAudioDurationInMilliseconds:
initWithWordCount:trailingSilenceFrames:endOfSilenceLikelihood:pauseCounts:silencePosterior:taskName:
wordCount
setWordCount:
trailingSilenceDuration
setTrailingSilenceDuration:
eosLikelihood
setEosLikelihood:
pauseCounts
setPauseCounts:
silencePosterior
setSilencePosterior:
processedAudioDurationInMilliseconds
setProcessedAudioDurationInMilliseconds:
taskName
setTaskName:
_wordCount
_trailingSilenceDuration
_pauseCounts
_processedAudioDurationInMilliseconds
_taskName
_eosLikelihood
_silencePosterior
_configureWithASBD:andFrameRate:
_resetWithSampleRate:
_configureWithSampleRate:andFrameRate:
subChunkFrom:numSamples:forChannel:
_processAudioSamples:
_detectVoiceActivityInSamples:numSamples:
replaceBytesInRange:withBytes:length:
_getEndpointMetricsForAudioTimestamp:
endpointStyle
delay
setDelay:
startWaitTime
automaticEndpointingSuspensionEndTime
setAutomaticEndpointingSuspensionEndTime:
minimumDurationForEndpointer
setMinimumDurationForEndpointer:
lastStartOfVoiceActivityTime
bypassSamples
setBypassSamples:
endpointMode
setEndpointMode:
interspeechWaitTime
endWaitTime
saveSamplesSeenInReset
setSaveSamplesSeenInReset:
canProcessCurrentRequest
handleVoiceTriggerWithActivationInfo:
sampleRate
setSampleRate:
frameRate
setFrameRate:
detectedOneShotStartpoint
setDetectedOneShotStartpoint:
detectedRecurrentStartpoint
setDetectedRecurrentStartpoint:
communicatedStartPointDetection
setCommunicatedStartPointDetection:
detectedOneShotEndpoint
setDetectedOneShotEndpoint:
detectedRecurrentEndpoint
setDetectedRecurrentEndpoint:
communicatedEndpointDetection
setCommunicatedEndpointDetection:
samplesSeen
setSamplesSeen:
numSamplesProcessed
setNumSamplesProcessed:
lastOneShotStartpoint
setLastOneShotStartpoint:
lastOneShotEndpoint
setLastOneShotEndpoint:
lastRecurrentStartpoint
setLastRecurrentStartpoint:
lastRecurrentEndpoint
setLastRecurrentEndpoint:
floatSampleBuffer
setFloatSampleBuffer:
topLevelParameterDict
setTopLevelParameterDict:
modelDictPath
setModelDictPath:
isConfigured
setIsConfigured:
previousSamplesSeen
setPreviousSamplesSeen:
apQueue
setApQueue:
recordingDidStop
setRecordingDidStop:
vtEndInSampleCount
setVtEndInSampleCount:
_audioUnitEPVAD2
_saveSamplesSeenInReset
_detectedOneShotStartpoint
_detectedRecurrentStartpoint
_communicatedStartPointDetection
_detectedOneShotEndpoint
_detectedRecurrentEndpoint
_communicatedEndpointDetection
_isConfigured
_recordingDidStop
_endpointStyle
_endpointMode
_frameRate
_floatSampleBuffer
_topLevelParameterDict
_modelDictPath
_apQueue
_vtEndInSampleCount
_interspeechWaitTime
_startWaitTime
_endWaitTime
_automaticEndpointingSuspensionEndTime
_minimumDurationForEndpointer
_bypassSamples
_delay
_samplesSeen
_numSamplesProcessed
_lastOneShotStartpoint
_lastOneShotEndpoint
_lastRecurrentStartpoint
_lastRecurrentEndpoint
_previousSamplesSeen
dictionaryWithContentsOfFile:
initWithArray:
enumerateObjectsUsingBlock:
appendData:
_data
_numSamples
_startSampleCount
_hostTime
bestPhrase
earlyWarning
setSampleFed:
setBestPhrase:
setEarlyWarning:
isRescoring
setIsRescoring:
_earlyWarning
_isRescoring
_sampleFed
_bestPhrase
_bestStart
_bestEnd
isAvailable
_firedVoiceTriggerTimeout
shouldHandleSession
shouldMatchPayload
_firedEndPointTimeout
_registerVoiceTriggerTimeout
_reportStopListening
_registerEndPointTimeout
matchRecognitionResult:withMatchedBlock:withNonMatchedBlock:
bestTranscription
_detectBOS
_ASRResultReceived
_reportedStopListening
_updateAssetWithLanguage:
_updateAssetWithCurrentLanguage
serverFeaturesLatencyDistributionDictionary
_getCSHybridEndpointerConfigForAsset:
serverFeaturesQueue
setServerFeaturesQueue:
lastKnownServerEPFeatures
setLastKnownServerEPFeatures:
serverFeatureLatencies
setServerFeatureLatencies:
serverFeaturesWarmupLatency
setServerFeaturesWarmupLatency:
lastServerFeatureTimestamp
setLastServerFeatureTimestamp:
didReceiveServerFeatures
setDidReceiveServerFeatures:
clientLagThresholdMs
setClientLagThresholdMs:
clampedSFLatencyMsForClientLag
setClampedSFLatencyMsForClientLag:
useDefaultServerFeaturesOnClientLag
setUseDefaultServerFeaturesOnClientLag:
hybridClassifierQueue
setHybridClassifierQueue:
lastReportedEndpointTimeMs
setLastReportedEndpointTimeMs:
stateSerialQueue
setStateSerialQueue:
didCommunicateEndpoint
setDidCommunicateEndpoint:
currentRequestSampleRate
setCurrentRequestSampleRate:
vtExtraAudioAtStartInMs
setVtExtraAudioAtStartInMs:
hepAudioOriginInMs
setHepAudioOriginInMs:
firstAudioPacketTimestamp
setFirstAudioPacketTimestamp:
didTimestampFirstAudioPacket
setDidTimestampFirstAudioPacket:
silencePosteriorGeneratorQueue
setSilencePosteriorGeneratorQueue:
_canProcessCurrentRequest
_didReceiveServerFeatures
_useDefaultServerFeaturesOnClientLag
_didCommunicateEndpoint
_didTimestampFirstAudioPacket
_serverFeaturesQueue
_lastKnownServerEPFeatures
_serverFeatureLatencies
_lastServerFeatureTimestamp
_hybridClassifierQueue
_stateSerialQueue
_currentRequestSampleRate
_firstAudioPacketTimestamp
_silencePosteriorGeneratorQueue
_serverFeaturesWarmupLatency
_clientLagThresholdMs
_clampedSFLatencyMsForClientLag
_lastReportedEndpointTimeMs
_vtExtraAudioAtStartInMs
_hepAudioOriginInMs
getHostClockFrequency
secondsToHostTime:
hostTimeToSeconds:
_shouldEnterTwoShotAtEndPointTime:
_shouldUseVAD2ForTwoShot
endpointerDelegate
hybridEndpointer
setHybridEndpointer:
vad2Endpointer
setVad2Endpointer:
activeEndpointer
setActiveEndpointer:
didEnterTwoshot
setDidEnterTwoshot:
_didEnterTwoshot
_endpointerDelegate
_hybridEndpointer
_vad2Endpointer
_activeEndpointer
increaseTriggerCount
triggerCount
_triggerCount
initWithBool:
initWithDouble:
initWithLongLong:
initWithUnsignedLongLong:
objCType
longLongValue
_checkFirstUnlocked
_notifyObserver:withUnlocked:
CSFirstUnlockMonitor:didReceiveFirstUnlock:
_didReceiveFirstUnlockInQueue:
_didReceiveFirstUnlock:
_firstUnlocked
_scaleDecayConstants:
_savePeaks:averagePower:maxSample:
_linearToDB:
_ampToDB:
initWithSampleRate:
process:stride:inFrameToProcess:
getPeakPowerDB
getAveragePowerDB
_averagePowerI
_instantaneousMode
_peak
_maxPeak
_decay
_peakDecay
_averagePowerPeak
_peakHoldCount
_previousBlockSize
_decay1
_peakDecay1
_checkSpringBoardStarted
_didReceiveSpringboardStarted:
_notifyObserver:withStarted:
CSSpringboardStartMonitor:didReceiveStarted:
_didReceiveSpringboardStartedInQueue:
_isSpringBoardStarted
_recognizeWavData:length:
_previousUtteranceTokens
_triggerTokenList
_useKeywordSpotting
_triggerConfidence
hybridEndpointerAssetFilename
initWithResourcePath:configFile:configVersion:
fallBackAssetResourcePath
_decodeJson:
assetHashInResourcePath:
defaultFallBackAssetForSmartSiriVolume:
getBoolForKey:category:default:
_decodedInfo
_path
_resourcePath
_configVersion
_configureAudioConverter:
_convertBufferedLPCM:allowPartial:timestamp:
_opusConverter
_bufferedLPCM
_recordBasePacketsPerSecond
_opusOutASBD
_convertPacketCount
_convertAudioCapacity
_lastTimestamp
_notifyObserver:withLanguageCode:
_didReceiveLanguageCodeUpdate
_checkAllConditionsEnabled
notifyCallback:
_monitors
_conditions
_callback
outputAudioChannel
setOutputAudioChannel:
_outputAudioChannel
initWithDictionary:
enumerateKeysAndObjectsUsingBlock:
_initializeSAT:
_computeSATScore:
_initializeNDAPI:resourcePath:
_threshold
_spkModel
_sampleLengthFrom:To:
_lastKeywordScore
_voiceControllerWithContext:error:
_recordingSampleRate
_destroyVoiceController
setRecordDelegate:
setPlaybackDelegate:
_shouldUseRemoteRecordForContext:
_createDeInterleaverIfNeeded
_createSampleRateConverterIfNeeded
_shouldRunZeroFilter
_resetZeroFilter
playbackRoute
_deinterleaveBufferIfNeeded:
_samplingRateConvertIfNeeded:
_processAudioChainWithZeroFiltering:atTime:
_processAudioChain:atTime:
sharedAnalytics
logEventWithType:context:
_audioRecorderDidStartRecordingSuccessfully:error:
_audioRecorderDidStopRecordingForReason:
voiceControllerDidStartRecording:successfully:
voiceControllerDidStartRecording:successfully:error:
voiceControllerDidStopRecording:forReason:
voiceControllerDidDetectStartpoint:
voiceControllerDidDetectEndpoint:ofType:
voiceControllerDidDetectEndpoint:ofType:atTime:
voiceControllerEncoderErrorDidOccur:error:
voiceControllerLPCMRecordBufferAvailable:buffer:
voiceControllerRecordHardwareConfigurationDidChange:toConfiguration:
voiceControllerBeginRecordInterruption:
voiceControllerBeginRecordInterruption:withContext:
voiceControllerEndRecordInterruption:
voiceControllerMediaServicesWereLost:
voiceControllerMediaServicesWereReset:
voiceControllerRecordBufferAvailable:buffer:
voiceControllerPlaybackBufferAvailable:buffer:
voiceControllerDidStartPlaying:successfully:
voiceControllerDidStopPlaying:forReason:
voiceControllerDecoderErrorDidOccur:error:
voiceControllerPlaybackHardwareConfigurationDidChange:toConfiguration:
voiceControllerBeginPlaybackInterruption:
voiceControllerEndPlaybackInterruption:
enableVoiceTriggerOnAOP:
updateVoiceTriggerAOPModel:
voiceTriggerOccuredNotification:
_voiceController
_zeroFilter
_deinterleaver
_interleavedABL
_pNonInterleavedABL
_needSampleRateConversion
_remoteRecordClient
_latestContext
_shouldUseRemoteRecord
decisionWaitSampleCount
setDecisionWaitSampleCount:
_decisionWaitSampleCount
fileURL
isWriting
fFile
inASBD
outASBD
_fileURL
%s In %@: Continuous digital zero detected, lasting %{public}u samples per channel
%s In %@: Continuous digital zero in this audio chunk detected, lasting %{public}u samples per channel
%s Logging audio file into : %{public}@
%s CS logging files under %{public}@ created before %{public}@ will be removed.
%s Couldn't get creation date: %@
%s Could not remove %@: %@
%s SmartSiriVolume init value for noise estimation %{public}f
%s SmartSiriVolume init value for LKFS estimation %{public}f
%s asset is nil, use default parameters(this should not happen).
%s SmartSiriVolume configure: %{public}@
%s SmartSiriVolume: estimated noise level %{public}f
%s SmartSiriVolume: estimated LKFS %{public}f
%s SmartSiriVolume: pause LKFS calculation.
%s SmartSiriVolume: resume LKFS calculation.
%s SmartSiriVolume: TTS volume in dB from noise %{public}f, from LKFS %{public}f, with user offset %{public}f
%s Couldn't find keychain value %@ for account %@ %{public}d
%s ::: Error reading file %{public}@, err: %{public}d
%s ::: Error getting data format from audio file : %{public}d
%s ::: Error getting data length from audio file : %{public}d
%s ::: Error reading data from audio file : %{public}d
%s asset is nil, stop initialization
%s Setting two shot decision mode triggerEndSampleCount = %{public}tu (%{public}.3f), twoShotThreshold = %{public}.3f, twoShotDecisionWaitSamples = %{public}tu (%{public}.3f)
%s Setting active channel of continuous voice trigger to %{public}tu according to VTEI
%s Could not find Assets. Cannot process Audio
%s Shot: best score = %{public}f for channel = %{public}tu
%s analyzedSampleCount: %{public}tu, checkTime: %{public}tu
%s Entering two shot at %{public}.2f with [score: %{public}.3f > threshold: %{public}.3f]
%s Not entering two shot: [score: %{public}.3f < threshold: %{public}.3f]
%s NDAPI continuous voicetrigger best score = %{public}f for channel = %{public}tu
%s %{public}@
%s Cannot create SampleRateConverter using AudioConverterNew : %{public}d
%s Cannot set Quality property to audioConverter
%s Cannot set Complexity property to audioConverter
%s Audio resampling done : %tu
%s AudioConverter is sad: 0x%{public}xd
%s xpc object string return nil
%s xpc object should be XPC_TYPE_STRING
%s Siri language is nil, falling back to %@
%s Cannot create directory since directory is nil
%s same name of file exists, this will be removed
%s Creating Directory : %{public}@
%s Creating Directory failed : %{public}@
%s Cannot remove model directory(%@) : %@
%s Cannot remove utterance directory(%@) : %@
%s VoiceTrigger start policy changed : %{public}@
%s Initializing first pass Corealis for channel : %{public}tu
%s %tu first pass Corealis were created
%s _voiceTriggerEnabled = %{public}@
%s NDAPI first pass best score = %{public}.3f for channel = %{public}tu, heartbeat = %{public}lld
%s NDAPI in channel: %{public}tu passed threshold with score %{public}.3f at sample %{public}tu, setting up decision delay in samples: %{public}tu
%s Set to use the alignment of channel %{public}tu that first crossed the threshold: %{public}@
%s Update to use the alignment of channel %{public}tu: %{public}@
%s NDAPI first pass best score for channel selection = %{public}.3f for channel = %{public}tu at sample %{public}tu
%s Boosting master channel (ch0) score to %{public}.3f by %{public}.3f for stream selection
%s setting delay interstitial %{public}d sounds with level : %{public}d
%s CSCoreSpeechServices Invalidated
%s Cannot register interstitial sounds : %{public}@
%s Successfully register interstitial
%s XPC Connection is not exist?
%s Celestial is not available on this platform.
%s CSVoiceTriggerAsset found: %{public}@
%s Cannot find voicetrigger asset from asset manager, let's fallback to asset in the framework
%s speechController = %{public}p
%s event : %{public}@
%s context = %{public}@
%s Creating new CSAudioRecorder with context : %{public}@
%s Cannot create audio recorder : %{public}@
%s It cannot change context because it is recording
%s Cannot change context : %{public}@
%s recordingSettings = %{public}@
%s AVVC already recording.
%s setRecordModeToRecordingDelay = %{public}f
%s timeIntervalSinceLastTriggerEnd = %{public}f
%s Failed SetRecordModeToRecording with %{public}f seconds delay from prepareRecorder due to error %{public}@.
%s Finished SetRecordModeToRecording with %{public}f seconds delay from prepareRecorder.
%s Cancelled SetRecordModeToRecording with %{public}f seconds delay from prepareRecorder.
%s Scheduled SetRecordModeToRecording with %{public}f seconds delay in prepareRecorder.
%s AVVC already recording, change record mode to recording here.
%s AVVC already recording, This shouldn't be called
%s AVVC Prepare recording failed : %{public}@
%s recordingSettings : %{public}@
%s AVVC already recording, nothing to prepare
%s Not supported in this platform
%s context : %{public}@
%s Cannot set context since mediaserverd is recovering from crash
%s Cannot prepare since mediaserverd is recovering from crash
%s Cannot prepare since audio recorder was not initialized
%s recordingSettings from CS : %{public}@
%s settings : %{public}@
%s startRecording failed : %{public}@
%s mode : %{public}ld
%s Delayed SetRecordModeToRecording: Consumed token %{public}@ with %{public}f seconds delay for reason %{public}@.
%s Delayed SetRecordModeToRecording: Setting record mode to recording for reason %{public}@.
%s Delayed SetRecordModeToRecording: Failed to set record mode to recording for reason %{public}@ due to error %@.
%s Delayed SetRecordModeToRecording: Successfully set record mode to recording for reason %{public}@.
%s Delayed SetRecordModeToRecording: Ignored set record mode to recording for reason %{public}@ because the validator rejected.
%s Delayed SetRecordModeToRecording: Ignored set record mode to recording for reason %{public}@ because the scheduled token %{public}@ does not match the current token %{public}@.
%s Delayed SetRecordModeToRecording: Scheduled new token %{public}@ with %{public}f seconds delay for reason %{public}@.
%s Delayed SetRecordModeToRecording: Cancelled token %{public}@ for reason %{public}@.
%s Delayed SetRecordModeToRecording: Consumed token %{public}@ in advance for reason %{public}@.
%s Activate Context = %{public}@
%s setCurrentContext failed : %{public}@
%s sessionOptions : %{public}tu
%s startRecordingBy %{public}@
%s start recording since mediaserverd is recovering from crash
%s _lastForwardedSampleCounts = %{public}tu, audioBufferSampleCount = %{public}tu
%s Failed SetRecordModeToRecording with %{public}f seconds delay from startRecording due to error %{public}@.
%s Finished SetRecordModeToRecording with %{public}f seconds delay from startRecording.
%s Cancelled SetRecordModeToRecording with %{public}f seconds delay from startRecording.
%s Scheduled SetRecordModeToRecording with %{public}f seconds delay in startRecording.
%s Try start recording under PollingListening state.
%s Try start recording under Stopping state.
%s Requested startHostTime = %{public}llu, _clientStartSampleCount = %{public}tu
%s Cannot use opportunisticZLL since _clientStartSampleCount is newer than audioBuffer sample count
%s Opportunistic ZLL will be used starting from : %{public}tu
%s Opportunistic ZLL will not be used : %{public}tu
%s Generating fake didStartRecording delegate
%s Generating fake didStopRecording delegate
%s AudioRecorder lost mediaserverd connection
%s Mediaserverd recovered from crash
%s from:%{public}@ to:%{public}@ by:%{public}@
%s Ignore event(%{public}@) from(%{public}@) since we don't have transition
%s Trying to startListening
%s _createRecorderWithContextIfNeeded failed, it will try again %{public}f seconds later
%s _prepareListenWithSettings failed, it will try again %{public}f seconds later
%s startListening succeed
%s startListening failed, it will try again %{public}f seconds later
%s Still recording, let's do not destroy.
%s ClientSpeechController is nil
%s %{public}@, sucessfully:%{public}@ error:%{public}@
%s forward to speechManagerDidStartForwarding
%s ignore audioRecorderDidStartRecording
%s forReason : %{public}ld
%s forward to speechManagerDidStopForwarding
%s ignore audioRecorderDidStopRecording
%s toConfiguration: %{public}ld
%s context: %{public}@
%s new VoiceTrigger asset downloaded
%s Language Code Changed : %{public}@
%s new asset available, change to new model
%s given asset is same as existing, don't need to reinitialize VoiceTrigger
%s SmartSiriVolume: final estimated TTS volume %{public}f with system volume %{public}f
%s Wrongly called: SmartSiriVolume is not supported on this device type.
%s init-_currentLanguageCode: %{public}@
%s Not able to fetch remote meta now, registering for callback
%s CSAssetManager cannot query for nil language
%s Error running asset-query for assetType:%{public}lu, query: %{public}@, predicate: %{public}@, error: %{public}@
%s ::: found %{public}lu assets for assetType=%{public}lu, matching query: %{public}@
%s ::: %{public}s
%s ::: predicate: %{public}@
%s ::: %{public}s; query: %{public}@
%s Error running asset query: error %{public}@
%s ::: Request Fetching RemoteMetaData
%s ::: Request fetching remote asset
%s ::: Fetching remote asset
%s ::: Purging installed asset : %{public}@
%s ::: Request downloading remote asset
%s ::: Start downloading asset
%s ::: download progress: %{public}3.0f%%
%s ::: Error downloading; %{public}@
%s ::: download completed successfully.
%s Attempting to download asset %{public}@
%s Failure resuming paused voice asset %{public}@
%s Asset doesn't need downloading, invoking completion
%s _currentLanguageCode changed: %{public}@
%s ERR: Unknown AssetType: %{public}lu
%s Called with status : %{public}d
%s returning status to UI : %{public}d
%s Already reported status or no callback
%s %{public}s Called
%s Will suspend training
%s Will resume training
%s %{public}s called
%s Decide to delay ending ASR: [%{public}ld] samples
%s Triggered! Event info: %{public}@
%{public}9lld %{public}9lld %{public}9lld
%s analyzing.... score so far: %{public}5.3f
%s feeding tailing: [%{public}ld] samples
%s correctSampleSize:    [%{public}ld]
%s accumSampleSize:      [%{public}ld]
%s startBufferIndex:     [%{public}ld]
%s startBufferSampleSize:[%{public}ld]
%s samplesToBeDeleted:   [%{public}ld]
%s Total Number of buffs:[%{public}ld]
%s Adjusting the start buffer
%s Adjusting the array elements
%s Unsupported
%s Begin of speech detected
%s End of speech detected
%s %{public}s CALLED
%s Master Timeout Fired
%s recognized text = %{public}@
%s Context : %{public}@
%s Resetting CoreSpeech frameworks
%s Ask start recording from: %{public}tu
%s SpeechController to receive data from channel %{public}tu
%s Ask delay audio session active by %{public}f seconds
%s SpeechManager still forwarding audio after didStopForwarding, we shouldn't have this
%s AVVCAudioBuffer contains %{public}d packet descriptions, size %{public}d, channels %{public}d. Ignoring
%s packetCount %{public}d
%s Bad packet length %{public}d. Skipping rest of record buffer.
%s SpeechController is trying to forward encoded audio after didStopForwarding, we shouldn't have this
%s Not available
%s Two shot is detected at time %{public}.3f, should notify? [%{public}@]
%s Requesting QuickStop operation upon detecting keyword
%s Couldn't create speech log directory at path %{public}@ %{public}@
%s Cannot delete existing SATUpload Diretory : %{public}@
%s Cannot create SAT Upload Directory : %{public}@
%s Cannot create directory(%{public}@)
%s Cannot copy file from %{public}@ to %{public}@ : %{public}@
%s PHS update directory already exists, remove before we move forward
%s Failed to delete PHS update directory
%s Failed to create PHS update directory
%s We need SAT directory, deleting the file with same name first
%s Failed to get device hash list %{public}@
%s Processing sync data from device hash: %{public}@
%s Error to copy profile from %{public}@ to %{public}@, error: %{public}@
%s language: %{public}@, enableVTAfterSyncLanguage: %{public}@, currSiriLanguage: %{public}@
%s Enabling VoiceTrigger Upon VoiceProfile sync for language: %{public}@
%s VoiceTrigger does not exist for this platform, not setting VoiceTriggerEnabled
%s Not enabling VoiceTrigger Upon VoiceProfile sync for language: %{public}@
%s Sucessfully migrated language %{public}@
%s Migrated language %{public}@ but failed to mark SAT enrollment
%s Sucessfully marked as migrated for language : %{public}@
%s Failed to mark migrated for language : %{public}@
%s Failed to remove update path [%{public}@] upon migration completion, error: %{public}@
%s Coudn't mark SAT enrollment %{public}@ at path %{public}@
%s Marked SAT enrollment %{public}@ at path %{public}@
%s We can't mark SAT {public}%@ when there is no audio directory
%s ERR: Unknown device. returning false: %{public}@
%s Malformed audio-dir URL for string <%{public}@>:url
%s ERR: reading contents of audioDir: %{public}@
%s No jsonFiles found in %{public}@: jsonFiles.count=%{public}lu
%s Unexpected: empty JSON data for file: %{public}@
%s Error reading metaDict at path: %{public}@
%s metaProductType: %{public}@
%s CurrentDevice{%{public}@:%{public}@} matched with: {%{public}@:%{public}@}
%s vtProfile{%{public}@:%{public}@} does NOT matcch with currDevice{%{public}@:%{public}@}
%s Could not find productType in VT-Meta file, trying next one
%s No compatible VT profile found for CurrDevice: %{public}@: %{public}@
%s Failure disposing audio file %{public}d
%s Audio file already configured, closing first
%s Creating audio file at URL %{public}@
%s Failed creating audio file at url %{public}@ %{public}d
%s Error setting input format %{public}d
%s No audio file to append data
%s Failed writing audio file %{public}d
%s Closing file at URL %{public}@, audio size: %{public}u
%s Couldn't create voice trigger audio logging directory at path %{public}@ %{public}@
%s Error writing out event info meta: %{public}@
%s Start monitoring : speech endpoint asset meta update
%s Stop monitoring : speech endpoint asset meta update
%s New speech endpoint asset is available
%s could not allocate %{public}d bytes for %{public}@
%s begin zerocross vad, lentotal = %{public}d
%s ran out of buffer, no voice activity
%s sigsum = %{public}f sigNorm= %{public}d
%s vad offset = %{public}d, lentotal = %{public}d
%s vad could not find a start offset %{public}d > %{public}d - %{public}d
%s BTLE AudioPayload ringBuffer startpoint: %{public}lld samplesAvail: %{public}lu, activeChannel: %{public}tu
%s BTLE raw audio size = %{public}ld
%s BTLE padded %{public}ld samples to fill out buffer
%s Advert data: %{public}@
%s advert data write failed
%s Posted siri audio hash notification
%s Invalid active channel in VTEI: %{public}tu, defaulting to master channel: %{public}tu
%s Fides trigger (trigger): %{public}@
%s Fides trigger (near-miss): %{public}@
%s Locale: [%{public}@]
%s No locale set when creating phrase spotter.
%s Creation of Keyword Detector failed.
%s %{public}s async called
%s Called before completion called
%s BEGIN num:%{public}ld use:%{public}d
%s AudioSession setup failed
%s Has wrong audio routing, ask user to unplug headset
%s Start Audio Session failed
%s _sessionNumber [%{public}ld]
%s %{public}s Canceling Training
%s AudioSession StartRecording Failed
%s Setting suspendAudio:[%{public}d]
%s Resume training
%s Suspend training
%s Stop Listening
%s NDAPI initialization failed
%s set StartAnalyzeSampleCount = %{public}lld
%s Couldn't create CoreSpeech log directory at path %{public}@ %{public}@
%s Dealloc of CSRemoteControlClient, it should close connection
%s Start monitring : VoiceTrigger setting switch
%s Cannot start monitoring VoiceTrigger setting switch because it was already started
%s Stop monitring : VoiceTrigger setting switch
%s VoiceTrigger enabled = %{public}@
%s Creating new CSAudioRecorder with context : %@
%s AudioRecorder creation failed : %@
%s Cannot prepare since audio recorder does not exist
%s AudioRecorder is already recording, do not prepare anymore
%s Cannot prepareRecordWithSettings : %@
%s BeepCanceller asset file loading from : %{public}@
%s beepVector Size = %{public}lu
%s Cannot initialize beep canceller
%s Beep canceller initialized with maxNumSamples = %{public}d
%s It will beep now
%s Reset beep cancellation
%s Sending early detect notification upon first pass trigger
%s Received first pass triggered in channel: %{public}tu with trigger start: %{public}tu
%s Second pass timeout (%{public}.2fs) should not exceed the ring buffer size, set to ring buffer size
%s Second pass set to analyze %{public}tu samples (%{public}.2fs) from %{public}tu to %{public}tu
%s Stop feeding audio at sampleCount: %{public}tu
%s Set to bypass %{public}.3f more audio until recognizer comes back
%s Notify second pass reject at: %{public}tu with best score up to: %{public}.3f
%s Skipping %{public}tu samples while waiting for recognizer
%s NDAPI second pass best score = %{public}f for channel = %{public}tu with analyzed samples: %{public}tu
%s Trigger detected with %{public}tu analyzed samples in NDAPI
%s Detected near miss!!!, %{public}@
%s Waiting for logging near miss until timeout %{public}tu samples
%s Detected near miss candidate at %{public}tu, let's wait %{public}tu samples to log
%s recognizerScore updated from %{public}.3f to: %{public}.3f for channel %tu
%s force detection analysis: %{public}tu samples processed while pending recognition results
%s EventNotifier received VoiceTrigger event
%s Notifying VoiceTrigger Trigger!!!!
%s Reporting VoiceTrigger two shot detection at time : %{public}lf
%s Error reading audio file: %{public}d, skipping...
%s numChannels: %{public}lu, recordingDuration: %{public}f, sampleRate: %{public}f
%s Cannot copy samples since this is empty
%s Could NOT copyFrom: %{public}lu to: %{public}lu, retSampleCount: %{public}lu
%s copyBuffer: oldestSample: %{public}lu latestSample: %{public}lu, numSamplesCopied: %{public}lu
%s CSAudioCircularBuffer.reset
%s saveRecordingBufferFrom: %{public}lu to: %{public}lu toURL: %{public}@
%s csrb: %{public}@
%s Invalid request: (%{public}lu, %{public}lu): noting to write to file
%s Invalid request: reqStartSample=%{public}lu, reqEndSample=%{public}lu, oldestSampleInBuffer: %{public}lu, latestSampleInBuffer=%{public}lu
%s Start monitoring : VoiceTrigger Asset meta update
%s Stop monitoring : VoiceTrigger Asset meta update
%s New VoiceTrigger asset metadata is available
%s VAD2 preheat...
%s CSVAD2EndpointAnalyzer: resetForNewRequestWithSampleRate
%s ERR: Deprecated VAD2 reset called
%s %{public}@ Resetting with style %{public}ld, _samplesSeen: %{public}f, newSampleRate: %{public}tu, _sampleRate: %{public}f
%s _audioUnitEPVAD2=%{public}p, auNeedsReset: %{public}d
%s Failed to reset EPVAD2: %{public}d
%s vtEndInSecs: %{public}f, _vtEndInSampleCount: %{public}lu, voiceTriggerInfo: %{public}@,
%s Empty samplesBuffer!
%s Received audio buffer with 8 frames of zeroes
%s Not configured
%s done: %{public}d, _detectedOneShotStartpoint: %{public}d, _communicatedEndpointDetection: %{public}d, _startWaitTime: %{public}f_samplesSeen: %{public}f, _delay: %{public}f, _sampleRate: %{public}f(_startWaitTime + _delay) * _sampleRate): %{public}f, (_samplesSeen / _sampleRate): %{public}f, _automaticEndpointingSuspensionEndTime: %{public}f
%s No startpoint detected after %{public}f, timing out, _samplesSeen: %f, _samplesSeen(ms): %f, _lastOneShotEndpoint: %f
%s Ignoring recurrent endpoint at %{public}f becuase it's too early (< %{public}f)
%s Fell back to recurrent endpoint (%{public}f) because one-shot is too early (%{public}f < %{public}f)
%s Fell back to recurrent endpoint, _samplesSeen: %f, _samplesSeen(ms): %f, _lastOneShotEndpoint: %f
%s Reporting one-shot ep:  _samplesSeen: %f, _samplesSeen(ms): %f, _lastOneShotEndpoint: %f
%s sampleRate: %{public}lf frameRate: %{public}d
%s Skipping re-initialization of EPVAD2; no audio consumed yet
%s EPVAD2 reset with existing parameters
%s Could not find endpointer audio unit component
%s AU instantiation error: %{public}d
%s No model available for mode: %{public}d
%s Error reading plist endpoint model at %{public}@
%s Could not set kAUEndpointVADProperty_ViterbiModelData: %{public}d
%s Could not set %{public}@ to %{public}@: %{public}d
%s Could not initialize audio unit: %{public}d
%s CSVAD2Endpointer is configured
%s Unexpected block size of %{public}u, not %{public}u. Skipping this block of audio.
%s Could not process audio via endpointer: %{public}d
%s tuningLibraryPath: %{public}@
%s VAD2-epModelPath: %{public}@
%s Could not read kAUEndpointVAD2Property_LatestEndpointerEventTimeSeconds: %{public}d
%s Found one shot startpoint at %{public}.3f seconds
%s Found one shot endpoint at %{public}.3f seconds
%s Could not read kAUEndpointVAD2Property_LatestRecurrentVADEventTimeSeconds: %{public}d
%s Found recurrent startpoint at %{public}.3f seconds
%s Found recurrent endpoint at %{public}.3f seconds
%s xpc object should be XPC_TYPE_ARRAY
%s xpcObject value is NULL
%s Cannot decode non-plist types of XPC object
%s Cannot encode non-plist types into XPC object : %{public}@
%s Cannot generate subChunk since channel(%{public}tu) is larger than number of channels(%{public}tu)
%s Cannot generate subChunk if it reuqest more than it has : %{public}tu %{public}tu %{public}tu
%s Fired VoiceTrigger Timeout
%s Stop right now since ASR has issue
%s EOS Timeout Fired
%s AudioSession Started
%s AudioSession Stopped
%s unknown endpoint type
%s Non Final: [%{public}@]
%s NON Final Matching
%s Final: [%{public}@]
%s Final Matching
%s Final Not Matching
%s SPEECH RECOGNITION TASK FINISH UNSUCCESSFULLY
%s %{public}@ %{public}@ %{public}ld %{public}@
%s Recog Result: [%{public}ld]
%s HEP::RecordingDidStop: Ignoring processAudioSamplesAsynchronously: Not queueing
%s triggerEndSeconds: %{public}f, _vtEndInSampleCount: %{public}lu, _vtExtraAudioAtStartInMs: %{public}lu,  _hepAudioOriginInMs: %{public}f, voiceTriggerInfo: %{public}@,
%s CSHybridEndpointAnalyzer recordingStoppedForReason: %{public}lu
%s language changed to: %{public}@: CSHybridEndpointer new asset: %{public}@
%s new hybrid endpoint asset downloaded, CSHybridEndpointer asset : %{public}@
%s %{public}@ doesnt exist
%s Could not read: %{public}@
%s Could not decode contents of: %{public}@: err: %{public}@
%s Delta is larger than anchorHostTime
%s Delta is larger than anchorSampleCount
%s CSHybridEndpointer canProcessCurrentRequest
%s CSHybridEndpointer can-NOT-ProcessCurrentRequest, fallback  to VAD2
%s _activeEndpointer=%{public}@
%s 2-shot: DONT reset any endpointers
%s endpointer: %{public}@: didDetectStartpointAtTime: %{public}f
%s CSEndpointerProxy didDetectHardEndpoint using for 1-2 shot at Time: %{public}f
%s %{public}@: Endpointer didDetectHardEndpointAtTime:withMetrics: %{public}f, CallingDelegate: %{public}@
%s CSEndpointerProxy: didDetectHardEndpoint: ep-time: %{public}f, triggerEnd: %{public}f, vad2EndWaitTime: %{public}f, delta: %{public}f, legacyTwoShotThreshold: %{public}f, enterTwoShot: %{public}d
%s WARN: endpointerModelVersion called when CSHybridEndpointer is not available
%s Cannot create NSNumber if xpcObject is NULL
%s XPC object type should be BOOL, DOUBLE, INT64, or UINT64
%s Cannot create xpcObject if objcType is NULL
%s Cannot create xpcObject since there is no matching type
%s Stop monitoring : First unlock
%s Start monitoring : Springboard start
%s Cannot start monitoring Springboard start because it was already started
%s Stop monitoring : Springboard start
%s SpringBoard started = %{public}@
%s Fallback asset resource path : %{public}@
%s Cannot find corespeech asset from resourcePath : %{public}@
%s Configuration file is not exists : %{public}@
%s Cannot read configuration file : %{public}@
%s Cannot decode configuration json file : %{public}@
%s Configuration json file is not expected format
%s Cannot access to %{public}@ %{public}@ using default value
%s There is not audio buffer to convert. Skip this.
%s Resetting AudioConverter buffer
%s createAudioConverter : initial frames per buffer = dur %{public}.2f * sr %{public}.2f = %{public}u
%s _configureAudioConverter: encoded audio needs minimum of %{public}u bytes per output buffer
%s _configureAudioConverter: AudioConverterGetProperty(kAudioConverterPropertyMinimumOutputBufferSize) returned status %{public}d
%s _configureAudioConverter: final framesPerBuffer: %{public}u
%s _configureAudioConverter: _convertPacketCount: %{public}u
%s _configureAudioConverter: AudioConverterGetProperty(MaximumOutputPacketSize): returned status %{public}d
%s createAudioConverter: outputSizePerPacket: %{public}u
%s _configureAudioConverter: _convertAudioCapacity %{public}u bytes
%s Cannot create AudioConverter using AudioConverterNew : %{public}u
%s Cannot set encoder bit rate : %{public}u
%s Cannot set codecQuality : %{public}u
%s Start monitoring : Siri language code
%s Stop monitoring : Siri language code
%s Siri language changed to : %{public}@
%s Ignore notifying change of language code, since it is nil
%s Output NDAPI second pass best score = %{public}f for channel = %{public}tu
%s Notifying self trigger detected
%s xpc object should be XPC_TYPE_DICTIONARY
%s xpcObject key or value is NULL
%s Cannot encode key into xpcobject since the key is not NSString class type
%s SAT successfully initialized : %{public}@
%s SAT Score = %{public}f, threshold = %{public}f
%s Cannot create CSVTUIKeywordDetector since there is no asset available
%s Cannot create CSVTUIKeywordDetector since we cannot initialize NDAPI
%s AVVC initialization failed
%s Successfully create AVVC : %{public}p
%s Trying to set record buffer duration : %{public}lf
%s Failed setting record buffer duration. Duration is %{public}lf
%s Creating beep canceller...
%s Calling AVVC prepareRecordWithSettings : %{public}@
%s Creating SampleRateConverter
%s Calling AVVC setCurrentContext : %{public}@
%s Calling AVVC prewarmAudioSession
%s Calling AVVC releaseAudioSession : %{public}tu
%s zeroFilterWinSz: %{public}tu, numHostTicksPerAudioSample: %{public}f
%s _vtEndInSampleCount:%{public}ld, _numSamplesProcessed: %{public}ld, vtInfo: %{public}@
%s Resetting ZeroFilter
%s Calling AVVC startRecordingWithSettings : %{public}@
%s Calling AVVC startRecording
%s Calling AVVC stopRecording
%s Sampling rate = %{public}f
%s AVVC sampling rate = %{public}f
%s AVVC doesn't return sampleRate, assume it is default sample rate
%s Calling AVVC playAlertSoundsForType : %{public}ld
%s Zero Filter Metrics: %@
%s Beep Canceller Metrics : %@
%s successfully : %{public}d, error : %{public}@
%s toConfiguration : %{public}d
%s withContext : %{public}@
%s AVVC lost mediaserverd connection
%s AVVC informed mediaserverd reset, no further action required
%s Failed to deinterleave the data: %{public}d
%s Cannot create de-interleaver using AudioConverterNew: %{public}d
%s Created de-interleaver
%s Created narrowBandToWidBandConverter
%s Cannot create NSData with size 0
%s xpc object should be XPC_TYPE_DATA
%s Setting decisionWaitSampleCount at %{public}tu (%{public}.3f) given vtEndSampleCount at %{public}tu (%{public}.3f)
%s Keyword detected at %{public}tu with %{public}.3f confidence
%s Keyword NOT detected at %tu with %{public}.3f confidence
%s Failed to create regular expression : %{public}@
%s ::: Error creating output file %{public}@, err: %{public}d
%s ::: Error writing to output wave file. : %{public}ld
-[CSAudioZeroCounter getZeroStatisticsFromBuffer:entireSamples:]
-[CSAudioZeroCounter stopReportZeroStatistics]
smartSiriVolume
noiseLevelChannelBitset
LKFSChannelBitset
noiseLowerPercentile
noiseUpperPercentile
LKFSLowerPercentile
LKFSUpperPercentile
noiseTimeConstant
noiseMicSensitivityOffset
LKFSTimeConstant
LKFSMicSensitivityOffset
noiseTTSMappingInputRangeLow
noiseTTSMappingInputRangeHigh
noiseTTSMappingOutputRangeLow
noiseTTSMappingOutputRangeHigh
LKFSTTSMappingInputRangeLow
LKFSTTSMappingInputRangeHigh
LKFSTTSMappingOutputRangeLow
LKFSTTSMappingOutputRangeHigh
userOffsetInputRangeLow
userOffsetInputRangeHigh
userOffsetOutputRangeLow
userOffsetOutputRangeHigh
TTSVolumeLowerLimitDB
TTSVolumeUpperLimitDB
SSVNoiseLevelChannelBitset
TI,R,N
SSVLKFSChannelBitset
SSVNoiseLowerPercentile
SSVNoiseUpperPercentile
SSVLKFSLowerPercentile
SSVLKFSUpperPercentile
SSVNoiseTimeConstant
Tf,R,N
SSVNoiseMicSensitivityOffset
SSVLKFSTimeConstant
SSVLKFSMicSensitivityOffset
SSVNoiseTTSMappingInputRangeLow
SSVNoiseTTSMappingInputRangeHigh
SSVNoiseTTSMappingOutputRangeLow
SSVNoiseTTSMappingOutputRangeHigh
SSVLKFSTTSMappingInputRangeLow
SSVLKFSTTSMappingInputRangeHigh
SSVLKFSTTSMappingOutputRangeLow
SSVLKFSTTSMappingOutputRangeHigh
SSVUserOffsetInputRangeLow
SSVUserOffsetInputRangeHigh
SSVUserOffsetOutputRangeLow
SSVUserOffsetOutputRangeHigh
SSVTTSVolumeLowerLimitDB
SSVTTSVolumeUpperLimitDB
SSVParameterDirectionary
T@"NSDictionary",R,N
en_US_POSIX
yyyy_MM_dd-HHmmss.SSS
%@/%@%@.wav
+[CSAudioFileManager createAudioFileWriterWithLoggingDir:inputFormat:outputFormat:]
^%@*
-[CSUtils(Directory) removeLogFilesInDirectory:matchingPattern:beforeDays:]_block_invoke
-[CSUtils(Directory) removeLogFilesInDirectory:matchingPattern:beforeDays:]_block_invoke_2
v12@?0@"NSArray"4@"NSError"8
v4@?0
B12@?0@"NSURL"4@"NSDictionary"8
triggerStartSampleCount
triggerEndSampleCount
-[CSSmartSiriVolume initWithSamplingRate:asset:systemVolume:]
-[CSSmartSiriVolume _setAsset:]
v8@?0I4
-[CSSmartSiriVolume estimateSoundLevelbySoundType:]_block_invoke
-[CSSmartSiriVolume pauseLKFSProcessing]
-[CSSmartSiriVolume resumeLKFSProcessing]
-[CSSmartSiriVolume _combineResultsWithOptimalFromNoise:andOptimalFromLkfs:withUserOffset:]
hash
TI,R
superclass
T#,R
description
T@"NSString",R,C
debugDescription
allocator<T>::allocate(size_t n) 'n' exceeds maximum supported size
com.apple.assistant
Siri Global
 - %@
CSKeychainValueForAccountAndKey
-[CSAudioFileReader initWithURL:]
-[CSAudioFileReader loadAllSamples]
numChannels
TL,R,N,V_numChannels
triggerEndSeconds
triggerScore
isContinuous
activeChannel
CSContinuousVoiceTrigger Queue
-[CSContinuousVoiceTrigger _setAsset:]
-[CSContinuousVoiceTrigger startDetectTwoShot:]
-[CSContinuousVoiceTrigger startDetectTwoShot:]_block_invoke
-[CSContinuousVoiceTrigger speechManagerLPCMRecordBufferAvailable:chunk:]_block_invoke
-[CSContinuousVoiceTrigger speechManagerDidStartForwarding:successfully:error:]
-[CSContinuousVoiceTrigger speechManagerDidStopForwarding:forReason:]
best_score
-[CSContinuousVoiceTrigger _shotAnalyzerNDAPI:hasResultAvailable:forChannel:]
-[CSContinuousVoiceTrigger _shotAnalyzerNDAPI:hasResultAvailable:forChannel:]_block_invoke
-[CSContinuousVoiceTrigger _keywordAnalyzerNDAPI:hasResultAvailable:forChannel:]
best_start
best_end
speechManager
T@"CSSpeechManager",W,N,V_speechManager
queue
T@"NSObject<OS_dispatch_queue>",&,N,V_queue
currentAsset
T@"CSAsset",&,N,V_currentAsset
keywordAnalyzer
T@"CSKeywordAnalyzerNDAPI",&,N,V_keywordAnalyzer
keywordThreshold
Tf,N,V_keywordThreshold
mode
Ti,N,V_mode
analyzedSampleCount
TI,N,V_analyzedSampleCount
TI,N,V_triggerEndSampleCount
twoShotDecisionWaitSamples
TI,N,V_twoShotDecisionWaitSamples
twoShotThreshold
Tf,N,V_twoShotThreshold
TI,N,V_activeChannel
delegate
T@"<CSVoiceTriggerDelegate>",W,N,V_delegate
-[CSAudioSampleRateConverter _createSampleRateConverterWithInASBD:outASBD:]
/BuildRoot/Library/Caches/com.apple.xbs/Sources/CoreSpeech_Sim/CoreSpeech-135.3/CoreSpeech/CSAudioSampleRateConverter.m
<Unknown File>
Too many buffers
l16@?0^I4^{AudioBufferList=I[1{AudioBuffer=II^v}]}8^^{AudioStreamPacketDescription}12
-[CSAudioSampleRateConverter convertSampleRateOfBuffer:]
-[NSString(XPCObject) initWithXPCObject:]
+[CSUtils(LanguageCode) getSiriLanguageWithFallback:]
-[CSSpeakerModel _createDirectoryIfNotExist:]
xx_XX
model
audio
self ENDSWITH '.wav'
-[CSSpeakerModel discard]
modelPath
T@"NSString",R,N
utteranceDirectory
enrollmentUtterance
T@"NSArray",R,N
isValid
TB,R,N
needsRetrain
firstPassDetectedChannel
firstPassScore
firstPassOnsetChannel
firstPassOnsetScore
firstPassChannelSelectionScores
firstPassChannelSelectionDelaySeconds
firstPassMasterChannelScoreBoost
firstPassStartSampleCount
firstPassEndSampleCount
firstPassFireSampleCount
VoiceTrigger First Pass Queue
-[CSVoiceTriggerFirstPass start]
-[CSVoiceTriggerFirstPass start]_block_invoke
RUNNING
STOPPED
v8@?0B4
-[CSVoiceTriggerFirstPass setAsset:]
-[CSVoiceTriggerFirstPass _setAsset:]
-[CSVoiceTriggerFirstPass _setAsset:]_block_invoke
-[CSVoiceTriggerFirstPass _transitVoiceTriggerStatus:]_block_invoke
-[CSVoiceTriggerFirstPass speechManagerLPCMRecordBufferAvailable:chunk:]_block_invoke
-[CSVoiceTriggerFirstPass speechManagerDidStartForwarding:successfully:error:]
-[CSVoiceTriggerFirstPass speechManagerDidStopForwarding:forReason:]
-[CSVoiceTriggerFirstPass _keywordAnalyzerNDAPI:hasResultAvailable:forChannel:]
samples_at_fire
ch%tu
-[CSVoiceTriggerFirstPass _reportVoiceTriggerFirstPassFire]
voiceTriggerStartPolicy
T@"CSPolicy",&,N,V_voiceTriggerStartPolicy
voiceTriggerEnabled
TB,N,V_voiceTriggerEnabled
keywordAnalyzersNDAPI
T@"NSMutableArray",&,N,V_keywordAnalyzersNDAPI
hasTriggerPending
TB,N,V_hasTriggerPending
firstPassThreshold
Tf,N,V_firstPassThreshold
bestScore
Tf,N,V_bestScore
bestChannel
TI,N,V_bestChannel
onsetResult
T@"NSDictionary",&,N,V_onsetResult
onsetChannel
TI,N,V_onsetChannel
channelSelectionDelay
TI,N,V_channelSelectionDelay
delayInSamplesRequiredForChannelSelection
TI,N,V_delayInSamplesRequiredForChannelSelection
masterChannelScoreBoost
Tf,N,V_masterChannelScoreBoost
channelSelectionScores
T@"NSDictionary",&,N,V_channelSelectionScores
processingChunkSamples
TI,N,V_processingChunkSamples
processingChannelsBitset
TI,N,V_processingChannelsBitset
T@"<CSVoiceTriggerFirstPassDelegate>",W,N,V_delegate
com.apple.corespeech.corespeechservices
+[CSCoreSpeechServices setDelayInterstitialSounds:level:]
+[CSCoreSpeechServices setDelayInterstitialSounds:level:]_block_invoke
v8@?0@"NSError"4
+[CSCoreSpeechServices getTriggerCount]_block_invoke
+[CSCoreSpeechServices clearTriggerCount]_block_invoke
speechManagerStartSampleCount
speechManagerSetRecordModeToRecordingDelay
triggerEndMachTime
CSSpeechManager Asset Query Queue
-[CSSpeechManager startManager]
-[CSSpeechManager _reset]
-[CSSpeechManager _getVoiceTriggerAsset]
en-US
-[CSSpeechManager _setupVoiceTrigger]
-[CSSpeechManager registerSpeechController:]
-[CSSpeechManager _notifyEvent:]
-[CSSpeechManager _createRecorderWithContextIfNeeded:error:]
-[CSSpeechManager _prepareRecorderWithSettings:error:]
B4@?0
-[CSSpeechManager _prepareRecorderWithSettings:error:]_block_invoke
v12@?0B4@"NSError"8
-[CSSpeechManager _prepareListenWithSettings:error:]
-[CSSpeechManager prewarmAudioSession]
-[CSSpeechManager recordRoute]
-[CSSpeechManager recordSettings]
-[CSSpeechManager setClientContext:error:]
-[CSSpeechManager setClientContext:error:]_block_invoke
reason
Mediaserverd is recovering from crash
-[CSSpeechManager prepareRecordingForClient:error:]
-[CSSpeechManager prepareRecordingForClient:error:]_block_invoke
Cannot prepare since audio recorder was not initialized
-[CSSpeechManager _startRecordingWithSettings:error:]
-[CSSpeechManager _startListening:]
-[CSSpeechManager _setRecordMode:error:]
-[CSSpeechManager _scheduleSetRecordModeToRecordingWithDelay:forReason:validator:completion:]_block_invoke
-[CSSpeechManager _scheduleSetRecordModeToRecordingWithDelay:forReason:validator:completion:]
-[CSSpeechManager _cancelPendingSetRecordModeToRecordingForReason:]
-[CSSpeechManager _performPendingSetRecordModeToRecordingForReason:]
-[CSSpeechManager _setCurrentContext:error:]
-[CSSpeechManager _releaseClientAudioSession:]
-[CSSpeechManager _releaseAudioSessionForListening:error:]
-[CSSpeechManager startRecordingWithSetting:event:error:]
-[CSSpeechManager startRecordingWithSetting:event:error:]_block_invoke
-[CSSpeechManager startRecordingWithSetting:event:error:]_block_invoke_2
Fail to start recording under PollingListening state.
Fail to start recording when awaiting to stop.
-[CSSpeechManager _startRecordingForClient:error:]
-[CSSpeechManager _startRecordingForClient:error:]_block_invoke
-[CSSpeechManager stopRecordingWithEvent:]_block_invoke_2
-[CSSpeechManager audioRecorderLostMediaserverd:]
-[CSSpeechManager mediaserverdDidRestart]
-[CSSpeechManager didTransitFrom:to:by:]_block_invoke
-[CSSpeechManager didIgnoreEvent:from:]
-[CSSpeechManager _createListenPollingTimer]
-[CSSpeechManager _createListenPollingTimer]_block_invoke
-[CSSpeechManager _startListenPolling]
-[CSSpeechManager _stopListenPolling]
-[CSSpeechManager _destroyAudioRecorderIfNeeded]
-[CSSpeechManager _startForwardingToClient]
-[CSSpeechManager _stopForwardingToClient]
-[CSSpeechManager _volumeFromAVSystemController]
-[CSSpeechManager audioRecorderDidStartRecording:successfully:error:]_block_invoke
-[CSSpeechManager audioRecorderDidStopRecording:forReason:]_block_invoke
-[CSSpeechManager audioRecorderRecordHardwareConfigurationDidChange:toConfiguration:]_block_invoke
-[CSSpeechManager audioRecorderBeginRecordInterruption:]_block_invoke
-[CSSpeechManager audioRecorderBeginRecordInterruption:withContext:]_block_invoke
-[CSSpeechManager audioRecorderEndRecordInterruption:]_block_invoke
-[CSSpeechManager audioRecorderDisconnected:]_block_invoke
-[CSSpeechManager CSAssetManagerDidDownloadNewAsset:]
-[CSSpeechManager CSLanguageCodeUpdateMonitor:didReceiveLanguageCodeChanged:]
-[CSSpeechManager _reinitializeVoiceTriggerWithAsset:]
Init
Stop
FirstPass
SecondPass
RecordPending
RecordWithVTRunning
RecordWithVTStopped
PollingListeningWithVTRunning
PollingListeningWithVTStopped
Listeninng
StoppingWithVTRunning
StoppingWithVTStopped
MediaserverdRecoveringWithVTRunning
MediaserverdRecoveringWithVTStopped
unknown(%tu)
ClientPrepare
AudioRecorderRelease
VoiceTriggerRunning
VoiceTriggerStopped
FirstPassTriggered
SecondPassTriggered
SecondPassRejected
SelfTriggerDetected
RecordPendingTimeout
ClientStartRecording
ClientStopRecording
ClientReleaseRecordSession
RecordingDidStop
ListeningSucceed
MediaserverdCrashed
MediaserverdRestarted
kDidStartFailed
RecorderDisconnected
StartListeningForAudioAnalysis
-[CSSpeechManager getEstimatedTTSVolume]
assetQueryQueue
T@"NSObject<OS_dispatch_queue>",&,N,V_assetQueryQueue
stateMachine
T@"CSStateMachine",&,N,V_stateMachine
audioBuffer
T@"CSAudioCircularBuffer",&,N,V_audioBuffer
currentVoiceTriggerAsset
T@"CSAsset",&,N,V_currentVoiceTriggerAsset
voiceTriggerFirstPass
T@"CSVoiceTriggerFirstPass",&,N,V_voiceTriggerFirstPass
voiceTriggerSecondPass
T@"CSVoiceTriggerSecondPass",&,N,V_voiceTriggerSecondPass
clientController
T@"<CSSpeechManagerDelegate>",W,N,V_clientController
voiceTriggerEventNotifier
T@"CSVoiceTriggerEventNotifier",&,N,V_voiceTriggerEventNotifier
voiceTriggerFileLogger
T@"CSVoiceTriggerFileLogger",&,N,V_voiceTriggerFileLogger
selfTriggerDetector
T@"CSSelfTriggerDetector",&,N,V_selfTriggerDetector
continuousVoiceTrigger
T@"CSContinuousVoiceTrigger",&,N,V_continuousVoiceTrigger
keywordDetector
T@"CSKeywordDetector",&,N,V_keywordDetector
T@"CSSmartSiriVolume",&,N,V_smartSiriVolume
myriad
T@"CSMyriadPHash",&,N,V_myriad
voiceTriggerFidesClient
T@"CSVoiceTriggerFidesClient",&,N,V_voiceTriggerFidesClient
activeAudioProcessors
T@"NSHashTable",&,N,V_activeAudioProcessors
continuousAudioProcessors
T@"NSHashTable",&,N,V_continuousAudioProcessors
lastForwardedSampleCount
TI,N,V_lastForwardedSampleCount
secondPassStartSampleCount
TI,N,V_secondPassStartSampleCount
clientStartSampleCount
TI,N,V_clientStartSampleCount
recordingPendingTimeout
Ti,N,V_recordingPendingTimeout
lastVoiceTriggerEventInfo
T@"NSDictionary",&,N,V_lastVoiceTriggerEventInfo
listenPollingTimer
T@"NSObject<OS_dispatch_source>",&,N,V_listenPollingTimer
pendingSetRecordModeToRecordingToken
T@"NSUUID",&,N,V_pendingSetRecordModeToRecordingToken
pendingSetRecordModeToRecordingCompletion
T@?,C,N,V_pendingSetRecordModeToRecordingCompletion
systemVolumeValue
Tf,N,V_systemVolumeValue
audioRecorder
T@"CSAudioRecorder",&,N,V_audioRecorder
com.apple.MobileAsset.VoiceTriggerAssets
com.apple.MobileAsset.VoiceTriggerAssetsWatch
com.apple.MobileAsset.VoiceTriggerAssetsMarsh
com.apple.MobileAsset.VoiceTriggerAssetsMac
com.apple.MobileAsset.SpeechEndpointAssets
-[CSAssetManager init]
Serial CSAssetManager queue
-[CSAssetManager assetOfType:language:]
-[CSAssetManager installedAssetOfType:language:]
-[CSAssetManager _installedAssetOfType:withPredicate:]
-[CSAssetManager _assetQueryForAssetType:withPredicate:localOnly:]
-[CSAssetManager _runAssetQuery:completion:]
-[CSAssetManager _runAssetQuery:completion:]_block_invoke
-[CSAssetManager _runAssetQuery:completion:]_block_invoke_2
-[CSAssetManager _fetchRemoteMetaData]
-[CSAssetManager _fetchRemoteAssetOfType:withPredicate:]
-[CSAssetManager _updateFromRemoteToLocalAssets:forAssetType:]
-[CSAssetManager _defaultDownloadOptions]
-[CSAssetManager _downloadAsset:withComplete:]
-[CSAssetManager _downloadAsset:withComplete:]_block_invoke
v12@?0d4
-[CSAssetManager _downloadAsset:withComplete:]_block_invoke_2
-[CSAssetManager _startDownloadingAsset:progress:completion:]
v12@?0@"NSDictionary"4@"NSError"8
-[CSAssetManager _startDownloadingAsset:progress:completion:]_block_invoke
-[CSAssetManager CSLanguageCodeUpdateMonitor:didReceiveLanguageCodeChanged:]
currentLanguageCode
+[CSAssetManager(Utils) predicateForAssetType:language:]
(%@ == %K)
(%@ IN %K)
((%K == nil) OR (%K != %@))
 && 
configFileNDAPI
threshold
delaySecondsForChannelSelection
processingChunkSeconds
config.txt
VTFirstPassConfigPathNDAPI
VTFirstPassThreshold
VTFirstPassDelaySecondsForChannelSelection
VTFirstPassMasterChannelScoreBoost
VTFirstPassProcessingChunkSeconds
VTFirstPassProcessingChannelsBitset
isTriggerEvent
totalSampleCount
-[CSVTUITrainingSession closeSessionWithStatus:successfully:]
-[CSVTUITrainingSession closeSessionWithStatus:successfully:complete:]_block_invoke
-[CSVTUITrainingSession suspendTraining]
-[CSVTUITrainingSession suspendTraining]_block_invoke
-[CSVTUITrainingSession resumeTraining]
-[CSVTUITrainingSession resumeTraining]_block_invoke
-[CSVTUITrainingSession setupPhraseSpotter]
-[CSVTUITrainingSession handleAudioInput:]_block_invoke_2
v8@?0@"NSDictionary"4
-[CSVTUITrainingSession handleAudioBufferForVTWithAudioInput:withDetectedBlock:]
-[CSVTUITrainingSession feedSpeechRecognitionTrailingSamplesWithCompletedBlock:]
-[CSVTUITrainingSession trimBeginingOfPCMBufferWithVoiceTriggerEventInfo:]
-[CSVTUITrainingSession audioSessionUnsupportedAudioRoute]
-[CSVTUITrainingSession didDetectBeginOfSpeech]
-[CSVTUITrainingSession didDetectEndOfSpeech:]
PHS explicit training utterance
[%ld] VTUISession Number:[%ld]
-[CSVTUITrainingSession startMasterTimerWithTimeout:]
-[CSVTUITrainingSession handleMasterTimeout:]
-[CSVTUITrainingSession stopMasterTimer]
-[CSVTUITrainingSession speechRecognitionTask:didHypothesizeTranscription:]
CSSpeechRecordSettingsKey_AudioSessionActiveDelay
-[CSSpeechController initializeRecordSessionWithContext:]
-[CSSpeechController prepareRecordWithSettings:error:]
-[CSSpeechController setCurrentContext:error:]
-[CSSpeechController preheat]
-[CSSpeechController prewarmAudioSession]
-[CSSpeechController resetAudioSession]
-[CSSpeechController reset]
-[CSSpeechController releaseAudioSession]
-[CSSpeechController releaseAudioSession:]
-[CSSpeechController startRecordingWithSettings:error:]
-[CSSpeechController startRecordingWithSettings:error:]_block_invoke
-[CSSpeechController stopRecording]
-[CSSpeechController recordRoute]
-[CSSpeechController speechManagerLPCMRecordBufferAvailable:chunk:]_block_invoke
-[CSSpeechController speechManagerRecordBufferAvailable:buffer:]_block_invoke
-[CSSpeechController speechManagerDidStartForwarding:successfully:error:]
-[CSSpeechController speechManagerDidStartForwarding:successfully:error:]_block_invoke
-[CSSpeechController speechManagerDidStopForwarding:forReason:]
-[CSSpeechController speechManagerDidStopForwarding:forReason:]_block_invoke
-[CSSpeechController speechManagerRecordHardwareConfigurationDidChange:toConfiguration:]
-[CSSpeechController speechManagerRecordHardwareConfigurationDidChange:toConfiguration:]_block_invoke
-[CSSpeechController speechManagerDetectedSystemVolumeChange:withVolume:]
-[CSSpeechController speechManagerDetectedSystemVolumeChange:withVolume:]_block_invoke
-[CSSpeechController speechManagerBeginRecordInterruption:]
-[CSSpeechController speechManagerBeginRecordInterruption:]_block_invoke
-[CSSpeechController speechManagerBeginRecordInterruption:withContext:]
-[CSSpeechController speechManagerBeginRecordInterruption:withContext:]_block_invoke
-[CSSpeechController speechManagerEndRecordInterruption:]
-[CSSpeechController speechManagerEndRecordInterruption:]_block_invoke
-[CSSpeechController audioConverterDidConvertPackets:packets:timestamp:]
-[CSSpeechController setAlertSoundFromURL:forType:]
-[CSSpeechController playAlertSoundForType:]
-[CSSpeechController playRecordStartingAlertAndResetEndpointer]
-[CSSpeechController setMeteringEnabled:]
-[CSSpeechController outputReferenceChannel]
-[CSSpeechController voiceTriggerInfo]
-[CSSpeechController voiceTriggerDidDetectTwoShotAtTime:]_block_invoke
-[CSSpeechController keywordDetectorDidDetectKeyword]_block_invoke
Accounts
Speech Identifier
%c%c%c%c
none
endpointerProxy
T@"CSEndpointerProxy",&,N,V_endpointerProxy
avvcContext
T@"NSDictionary",&,N,V_avvcContext
isOpus
TB,N,V_isOpus
isActivated
TB,N,V_isActivated
isNarrowBand
TB,N,V_isNarrowBand
audioFileWriter
T@"CSAudioFileWriter",&,N,V_audioFileWriter
twoShotNotificationEnabled
TB,N,V_twoShotNotificationEnabled
T@"<CSSpeechControllerDelegate>",W,N,V_delegate
duckOthersOption
TB,N
endpointAnalyzer
T@"<CSEndpointAnalyzer>",R,N
com.apple.voicetrigger
com.apple.voicetrigger.notbackedup
kDeviceCategory01
kDeviceCategory02
productType
VoiceTrigger CoreSpeech Enabled
Enable Two Shot Notification
com.apple.demo-settings
StoreDemoMode
File Logging Level
Library
Logs/CrashReporter/VoiceTrigger/audio/
/Logs/CrashReporter/Assistant/
SpeechLogs
-[CSPreferences assistantAudioFileLogDirectory]
VoiceTrigger/SAT
Caches/VoiceTrigger/SATUpdate
Caches/VoiceTrigger/SATUpload
-[CSPreferences getUserVoiceProfileUploadPathWithEnrolledLanguageList:]
json
-[CSPreferences notifyUserVoiceProfileUploadComplete]
-[CSPreferences getUserVoiceProfileUpdateDirectory]
-[CSPreferences notifyUserVoiceProfileUpdateReady]
Enable VoiceTrigger Upon VoiceProfile Sync For Language
enrollment_completed
enrollment_migrated
-[CSPreferences _markSATEnrollmentWithMarker:forLanguage:]
-[CSPreferences isCurrentDeviceCompatibleWithVoiceProfileAt:]
pathExtension='json'
iPad6,3
iPad6,4
iPad7,1
iPad7,2
iPad7,3
iPad7,4
iPhone8,1
iPhone8,2
iPhone8,4
iPhone9,1
iPhone9,2
iPhone9,3
iPhone9,4
iPhone10,1
iPhone10,2
iPhone10,3
iPhone10,4
iPhone10,5
iPhone10,6
iPhone10,7
iPhone10,8
iPad3,4
iPad3,5
iPad3,6
iPad4,1
iPad4,2
iPad4,3
iPad4,4
iPad4,5
iPad4,6
iPad4,7
iPad4,8
iPad4,9
iPad5,1
iPad5,2
iPad5,3
iPad5,4
iPad6,7
iPad6,8
iPad6,11
iPad6,12
iPad7,5
iPad7,6
iPhone5,1
iPhone5,2
iPhone5,3
iPhone5,4
iPhone6,1
iPhone6,2
iPhone7,1
iPhone7,2
Remote VoiceTrigger Delay
Remote VoiceTrigger Endpoint Timeout
VoiceTrigger/interstitial
Myriad File Logging Enabled
initialState
Ti,N,V_initialState
transitions
T@"NSMutableDictionary",&,N,V_transitions
T@"<CSStateMachineDelegate>",W,N,V_delegate
currentState
Ti,R,N,V_currentState
Serial CSEventMonitor queue
/BuildRoot/Library/Caches/com.apple.xbs/Sources/CoreSpeech_Sim/CoreSpeech-135.3/CoreSpeech/CSEventMonitor.m
Subclasses need to overwrite this method
-[CSAudioFileLog _closeAudioFile]
-[CSAudioFileLog startRecording]_block_invoke
-input.wav
-[CSAudioFileLog appendAudioData:]_block_invoke
-[CSAudioFileLog stopRecording]_block_invoke
-triggered
-almost
-rejected
VoiceTrigger audio logging queue
-[CSVoiceTriggerFileLogger _audioLogDirectory]
/tmp
yyyyMMdd-HHmmss
%@%@%@
.json
-[CSVoiceTriggerFileLogger _writeDictionary:toPath:]
.wav
%@.(?:wav|json)$
T@"CSAudioCircularBuffer",W,N,V_audioBuffer
2ndChanceThreshold
loggingThreshold
preTriggerAudioTime
analyzerPrependingAudioTime
analyzerTrailingAudioTime
configFileRecognizer
useKeywordSpotting
recognizerThresholdOffset
recognizerScoreScaleFactor
recognizerToken
recognizerWaitTime
config_marsh.txt
recognizer.json
hey_Siri
VTSecondPassThreshold
VTSecondPass2ndChanceThreshold
VTSecondPassLoggingThreshold
VTSecondPassPreTriggerAudioTime
VTSecondPassAnalyzerPrependingAudioTime
VTSecondPassAnalyzerTrailingAudioTime
VTSecondPassConfigPathNDAPI
VTSecondPassConfigPathRecognizer
VTSecondPassUseKeywordSpotting
VTSecondPassRecognizerThresholdOffset
VTSecondPassRecognizerScoreScaleFactor
VTSecondPassRecognizerToken
VTSecondPassRecognizerWaitTime
v8@?0i4
-[CSSpeechEndpointAssetMetaUpdateMonitor _startMonitoringWithQueue:]
-[CSSpeechEndpointAssetMetaUpdateMonitor _stopMonitoring]
-[CSSpeechEndpointAssetMetaUpdateMonitor _didReceiveNewSpeechEndpointAssetMetaData]
v8@?0@4
com.apple.MobileAsset.SpeechEndpointAssets.cached-metadata-updated
createFloatArray
sqrt
complex part zero vec
fft magnitudes array
normalized fft magnitudes
zeroCrossingVAD
HammingWindow
myriad_audio_analysis
windowed array for signal estimation
-[CSMyriadPHash pHash:length:]
last energy value set
current energy value set
-[CSMyriadPHash _generateMyriadInfo:score:channel:absoluteTime:]
/private/var/tmp/siriBC
com.apple.siri.myriad.apayload
-[CSMyriadPHash _audioLogDirectory]
-[CSMyriadPHash voiceTriggerDidDetectKeyword:]_block_invoke
Myriad-
signalEstimate
Ts,N,V_signalEstimate
com.apple.fides.borealis.record-creation
-[CSVoiceTriggerFidesClient voiceTriggerDidDetectKeyword:]_block_invoke
-[CSVoiceTriggerFidesClient voiceTriggerDidDetectNearMiss:]_block_invoke
near-miss
trigger
speaker-reject
unknown
numSamples
sampleByteDepth
startSampleCount
hostTime
Testing [%@] against regex.
regex.json
Cannot parse to JSON
Cannot find the file
trailing_garbage
leading_garbage
regex
com.apple.VoiceTriggerUI.TrainingSessionQueue
com.apple.VoiceTriggerUI.TrainingManager
-[CSVTUITrainingManager setLocaleIdentifier:]
-[CSVTUITrainingManager createKeywordDetector]
-[CSVTUITrainingManager cleanupWithCompletion:]
-[CSVTUITrainingManager cleanupWithCompletion:]_block_invoke
-[CSVTUITrainingManager cleanupWithCompletion:]_block_invoke_2
-[CSVTUITrainingManager trainUtterance:shouldUseASR:completion:]
-[CSVTUITrainingManager trainUtterance:shouldUseASR:completion:]_block_invoke_2
-[CSVTUITrainingManager cancelTrainingForID:]
-[CSVTUITrainingManager closeSessionBeforeStartWithStatus:successfully:withCompletion:]
-[CSVTUITrainingManager _shouldShowHeadsetDisconnectionMessage]
-[CSVTUITrainingManager _startAudioSession]
-[CSVTUITrainingManager setSuspendAudio:]
-[CSVTUITrainingManager setSuspendAudio:]_block_invoke
-[CSVTUITrainingManager VTUITrainingSessionStopListen]
Tf,V_rms
T@"<CSVTUITrainingManagerDelegate>",W,N,V_delegate
speechRecognizerAvailable
TB,R,V_speechRecognizerAvailable
audioSource
suspendAudio
com.apple.corespeech
InternalBuild
-[CSKeywordAnalyzerNDAPI initWithConfigPath:resourcePath:]
-[CSKeywordAnalyzerNDAPI _setStartAnalyzeTime:]
activePhraseId
TI,N,V_activePhraseId
T@"<CSKeywordAnalyzerNDAPIScoreDelegate>",W,N,V_delegate
Framework
Logs/CrashReporter/CoreSpeech/
Logs/CrashReporter/CoreSpeech/audio/
%@/%@%@%@
_CSGetOrCreateAudioLogDirectory
totalAudioRecorded
Td,N,V_totalAudioRecorded
featuresAtEndpoint
T@"NSArray",&,N,V_featuresAtEndpoint
endpointerType
Ti,N,V_endpointerType
serverFeatureLatencyDistribution
T@"NSDictionary",&,N,V_serverFeatureLatencyDistribution
additionalMetrics
T@"NSDictionary",&,N,V_additionalMetrics
CSRemoteControlClient
-[CSRemoteControlClient dealloc]
T@"<CSRemoteControlClientDelegate>",W,N,V_delegate
Languages
Footprint
Premium
com.apple.coreaudio.BorealisToggled
-[CSVoiceTriggerEnabledMonitor _startMonitoringWithQueue:]
-[CSVoiceTriggerEnabledMonitor _stopMonitoring]
-[CSVoiceTriggerEnabledMonitor _checkVoiceTriggerEnabled]
CSRemoteRecordClient Queue
T@"<CSRemoteRecordClientDelegate>",W,N,V_delegate
waitTimeSinceVT
keyword_detector.json
keywordDetectorConfigPathRecognizer
keywordDetectorThreshold
keywordDetectorWaitTimeSinceVT
isMaximized
-[CSVoiceTriggerSpeakerTrainer trainUtterance:languageCode:]
samples_fed
Dictionary to JSON conversion failed : %@
Borealis Input
com.apple.VoiceTriggerUI.RemoteRecordSessionQueue
-[CSVTUIAudioSessionRemote _audioRecorder]
-[CSVTUIAudioSessionRemote prepareRecord]
T@"<CSVTUIAudioSessionDelegate>",W,N,V_delegate
beepLocation
statsComputed
beepPower
signalPower
originalPower
absMaxVal
above95pcOfMax
totalInputSamples
totalOutputSamples
jbl_begin.bin
-[CSBeepCanceller init]
-[CSBeepCanceller willBeep]
-[CSBeepCanceller reset]
T@"<CSBeepCancellerDelegate>",W,N,V_delegate
metrics
triggerFireSampleCount
triggerStartSeconds
triggerFireSeconds
extraSamplesAtStart
analyzerPrependingSamples
analyzerTrailingSamples
effectiveThreshold
recognizerScore
recognizerScaleFactor
earlyDetectFiredMachTime
triggerStartMachTime
triggerFireMachTime
hardwareSamplerate
configVersion
VoiceTrigger Second Pass Queue
-[CSVoiceTriggerSecondPass _setAsset:]
-[CSVoiceTriggerSecondPass voiceTriggerFirstPass:didDetectKeyword:]
com.apple.voicetrigger.EarlyDetect
-[CSVoiceTriggerSecondPass voiceTriggerFirstPass:didDetectKeyword:]_block_invoke
-[CSVoiceTriggerSecondPass speechManagerLPCMRecordBufferAvailable:chunk:]_block_invoke
-[CSVoiceTriggerSecondPass speechManagerDidStartForwarding:successfully:error:]
-[CSVoiceTriggerSecondPass speechManagerDidStopForwarding:forReason:]
-[CSVoiceTriggerSecondPass _analyzeForKeywordDetection:result:forChannel:forceMaximized:]
-[CSVoiceTriggerSecondPass keywordAnalyzerQuasar:hasResultAvailable:forChannel:]
-[CSVoiceTriggerSecondPass keywordAnalyzerQuasar:hasResultAvailable:forChannel:]_block_invoke
keywordAnalyzerNDAPI
T@"CSKeywordAnalyzerNDAPI",&,N,V_keywordAnalyzerNDAPI
keywordAnalyzerQuasar
T@"CSKeywordAnalyzerQuasar",&,N,V_keywordAnalyzerQuasar
speakerDetector
T@"CSSpeakerDetectorNDAPI",&,N,V_speakerDetector
speakerModel
T@"CSSpeakerModel",&,N,V_speakerModel
secondPassTimeout
TI,N,V_secondPassTimeout
numProcessedSamples
TI,N,V_numProcessedSamples
keywordLoggingThreshold
Tf,N,V_keywordLoggingThreshold
lastScore
Tf,N,V_lastScore
TI,N,V_extraSamplesAtStart
TI,N,V_analyzerPrependingSamples
TI,N,V_analyzerTrailingSamples
useSAT
TB,N,V_useSAT
nearMissDelayTimeout
TI,N,V_nearMissDelayTimeout
nearMissCandidateDetectedSamples
TI,N,V_nearMissCandidateDetectedSamples
hasPendingNearMiss
TB,N,V_hasPendingNearMiss
lastAnalyzerResult
T@"NSDictionary",&,N,V_lastAnalyzerResult
Tf,N,V_recognizerScore
isRunningRecognizer
TB,N,V_isRunningRecognizer
recognizerResultPending
TB,N,V_recognizerResultPending
Tf,N,V_recognizerScoreScaleFactor
recognizerWaitSamples
TI,N,V_recognizerWaitSamples
TQ,N,V_earlyDetectFiredMachTime
lastResult
T@"NSDictionary",&,N,V_lastResult
processedSampleCountsInPending
TI,N,V_processedSampleCountsInPending
firstPassTriggerStartSampleCount
TI,N,V_firstPassTriggerStartSampleCount
firstPassTriggerFireSampleCount
TI,N,V_firstPassTriggerFireSampleCount
T@"NSDictionary",&,N,V_firstPassChannelSelectionScores
Tf,N,V_firstPassChannelSelectionDelaySeconds
Tf,N,V_firstPassMasterChannelScoreBoost
Tf,N,V_firstPassOnsetScore
TI,N,V_firstPassOnsetChannel
VoiceTriggerEventNotifier queue
voiceTriggerEventInfo: {
%@: %@
-[CSVoiceTriggerEventNotifier _notifyTriggerEvent:]
-[CSVoiceTriggerEventNotifier _notifyTriggerEvent:]_block_invoke
com.apple.coreaudio.borealisTrigger
com.apple.voicetrigger.nearMiss
-[CSVoiceTriggerEventNotifier _notifyTwoShotDetectionAt:]
isContinuousRunningMode
TB,N,V_isContinuousRunningMode
CSInitialContinousZeros
CSMaxContinousZeros
CSMidSegmentContinousZeros
start
+[CSUtils(AudioFile) readAudioChunksFrom:block:]
-[CSAudioCircularBuffer initWithNumChannels:recordingDuration:samplingRate:]
-[CSAudioCircularBuffer copySamplesFromHostTime:]
-[CSAudioCircularBuffer copySamplesFrom:to:]
-[CSAudioCircularBuffer copyBufferWithNumSamplesCopiedIn:]
-[CSAudioCircularBuffer reset]
-[CSAudioCircularBuffer saveRecordingBufferFrom:to:toURL:]
bufferLength
TI,N,V_bufferLength
copySamples
  mNumChannels: 
  mRecordingDurationInSecs: 
  mSampleRate: 
  mBytesPerSample: 
  mBufferLengthInSamples: 
  mNextWritePos: 
  mSamplesCount: 
  mMemoryPool(
): [
    chan-
: sz=
: mem-sz: 
-[CSVoiceTriggerAssetMetaUpdateMonitor _startMonitoringWithQueue:]
-[CSVoiceTriggerAssetMetaUpdateMonitor _stopMonitoring]
-[CSVoiceTriggerAssetMetaUpdateMonitor _didReceiveNewVoiceTriggerAssetMetaData]
com.apple.MobileAsset.VoiceTriggerAssets.cached-metadata-updated
com.apple.MobileAsset.VoiceTriggerAssetsWatch.cached-metadata-updated
com.apple.MobileAsset.VoiceTriggerAssetsMarsh.cached-metadata-updated
com.apple.MobileAsset.VoiceTriggerAssetsMac.cached-metadata-updated
speakerDetectorNDAPI
configPath
retrainTriggerThreshold
speakerDetectorNDAPIConfigPath
speakerDetectorThreshold
speakerDetectorRetrainTriggerThreshold
{wordCount: %ld, trailingSilDuration: %ld, eosLikelihood: %f, pauseCounts: %@, silencePosterior: %f, taskName: %@, processedAudioDurationInMilliseconds: %ld}
wordCount
Ti,N,V_wordCount
trailingSilenceDuration
Ti,N,V_trailingSilenceDuration
eosLikelihood
Td,N,V_eosLikelihood
pauseCounts
T@"NSArray",C,N,V_pauseCounts
silencePosterior
Td,N,V_silencePosterior
processedAudioDurationInMilliseconds
Ti,N,V_processedAudioDurationInMilliseconds
taskName
T@"NSString",C,N,V_taskName
com.apple.cs.%@.apQueue
-[CSVAD2EndpointAnalyzer preheat]
-[CSVAD2EndpointAnalyzer resetForNewRequestWithSampleRate:]
-[CSVAD2EndpointAnalyzer reset]
-[CSVAD2EndpointAnalyzer _resetWithSampleRate:]
-[CSVAD2EndpointAnalyzer handleVoiceTriggerWithActivationInfo:]_block_invoke
-[CSVAD2EndpointAnalyzer _processAudioSamples:]
endpointStyle
Ti,N
delay
Td,N
startWaitTime
automaticEndpointingSuspensionEndTime
minimumDurationForEndpointer
lastEndOfVoiceActivityTime
Td,R,N
lastStartOfVoiceActivityTime
bypassSamples
endpointMode
interspeechWaitTime
endWaitTime
saveSamplesSeenInReset
T@"<CSEndpointAnalyzerDelegate>",W,N
canProcessCurrentRequest
TI,N
endpointerModelVersion
sampleRate
Td,N,V_sampleRate
frameRate
TL,N,V_frameRate
detectedOneShotStartpoint
TB,N,V_detectedOneShotStartpoint
detectedRecurrentStartpoint
TB,N,V_detectedRecurrentStartpoint
communicatedStartPointDetection
TB,N,V_communicatedStartPointDetection
detectedOneShotEndpoint
TB,N,V_detectedOneShotEndpoint
detectedRecurrentEndpoint
TB,N,V_detectedRecurrentEndpoint
communicatedEndpointDetection
TB,N,V_communicatedEndpointDetection
samplesSeen
Td,N,V_samplesSeen
numSamplesProcessed
Td,N,V_numSamplesProcessed
lastOneShotStartpoint
Td,N,V_lastOneShotStartpoint
lastOneShotEndpoint
Td,N,V_lastOneShotEndpoint
lastRecurrentStartpoint
Td,N,V_lastRecurrentStartpoint
lastRecurrentEndpoint
Td,N,V_lastRecurrentEndpoint
floatSampleBuffer
T@"NSMutableData",&,N,V_floatSampleBuffer
topLevelParameterDict
T@"NSDictionary",&,N,V_topLevelParameterDict
modelDictPath
T@"NSString",&,N,V_modelDictPath
isConfigured
TB,N,V_isConfigured
previousSamplesSeen
Td,N,V_previousSamplesSeen
apQueue
T@"NSObject<OS_dispatch_queue>",&,N,V_apQueue
recordingDidStop
TB,N,V_recordingDidStop
vtEndInSampleCount
TI,N,V_vtEndInSampleCount
Ti,N,V_endpointStyle
Td,N,V_delay
Td,N,V_startWaitTime
Td,N,V_automaticEndpointingSuspensionEndTime
Td,N,V_minimumDurationForEndpointer
Td,N,V_bypassSamples
Ti,N,V_endpointMode
Td,N,V_interspeechWaitTime
Td,N,V_endWaitTime
TB,N,V_saveSamplesSeenInReset
T@"<CSEndpointAnalyzerDelegate>",W,N,V_delegate
-[CSVAD2EndpointAnalyzer(private) _configureWithSampleRate:andFrameRate:]
kAUEndpointVAD2Property_EDLStartWaitTimeSec
kAUEndpointVAD2Property_EDLInterspeechWaitTimeSec
kAUEndpointVAD2Property_EDLSpeechStartAdjustSec
kAUEndpointVAD2Property_EDLSpeechEndAdjustSec
kAUEndpointVAD2Property_EDLWindowLengthSeconds
kAUEndpointVAD2Property_EDLSpeechFraction
kAUEndpointVAD2Property_EDLNonspeechFraction
kAUEndpointVAD2Property_IsRealtimeOperationMode
kAUEndpointVAD2Property_DecoderLatencySeconds
kAudioUnitProperty_MaximumFramesPerSlice
-[CSVAD2EndpointAnalyzer(private) _detectVoiceActivityInSamples:numSamples:]
Library/Audio/Tunings/Generic/AU/
EndpointerModelPathForStyle
aufx-epv2-bluetooth8khz-appl.plist
aufx-epv2-appl.plist
EndpointerSpeechBeginListener
EndpointerSpeechEndListener
RecurrentVADSpeechBeginListener
RecurrentVADSpeechEndListener
-[NSArray(XPCObject) initWithXPCObject:]
-[NSArray(XPCObject) initWithXPCObject:]_block_invoke
B12@?0L4@"NSObject<OS_xpc_object>"8
-[NSArray(XPCObject) xpcObject]_block_invoke
v16@?0@4I8^B12
-[CSAudioChunk subChunkFrom:numSamples:forChannel:]
-[CSAudioChunk subChunkFrom:numSamples:]
data
T@"NSData",R,N,V_data
TI,R,N,V_numChannels
TI,R,N,V_numSamples
TI,R,N,V_sampleByteDepth
TQ,R,N,V_startSampleCount
TQ,R,N,V_hostTime
best_phrase
early_warning
is_rescoring
sampleFed
TI,N,V_sampleFed
bestPhrase
TI,N,V_bestPhrase
bestStart
TI,N,V_bestStart
bestEnd
TI,N,V_bestEnd
earlyWarning
TB,N,V_earlyWarning
isRescoring
TB,N,V_isRescoring
dictionary
-[CSVTUITrainingSessionWithPayload _firedVoiceTriggerTimeout]
-[CSVTUITrainingSessionWithPayload _firedEndPointTimeout]
-[CSVTUITrainingSessionWithPayload audioSessionDidStartRecording:error:]
-[CSVTUITrainingSessionWithPayload audioSessionDidStopRecording:]
-[CSVTUITrainingSessionWithPayload didDetectBeginOfSpeech]
-[CSVTUITrainingSessionWithPayload didDetectEndOfSpeech:]
-[CSVTUITrainingSessionWithPayload speechRecognitionTask:didHypothesizeTranscription:]_block_invoke
-[CSVTUITrainingSessionWithPayload speechRecognitionTask:didHypothesizeTranscription:]_block_invoke_2
-[CSVTUITrainingSessionWithPayload speechRecognitionTask:didFinishRecognition:]_block_invoke
-[CSVTUITrainingSessionWithPayload speechRecognitionTask:didFinishRecognition:]_block_invoke_2
-[CSVTUITrainingSessionWithPayload speechRecognitionTask:didFinishSuccessfully:]_block_invoke
-[CSVTUITrainingSessionWithPayload matchRecognitionResult:withMatchedBlock:withNonMatchedBlock:]
-[CSHybridEndpointAnalyzer processAudioSamplesAsynchronously:]
-[CSHybridEndpointAnalyzer handleVoiceTriggerWithActivationInfo:]_block_invoke
-[CSHybridEndpointAnalyzer recordingStoppedForReason:]
/BuildRoot/Library/Caches/com.apple.xbs/Sources/CoreSpeech_Sim/CoreSpeech-135.3/CoreSpeech/CSHybridEndpointAnalyzer.m
CSHybridEndpointAnalyzer reset called
-[CSHybridEndpointAnalyzer CSLanguageCodeUpdateMonitor:didReceiveLanguageCodeChanged:]
-[CSHybridEndpointAnalyzer CSAssetManagerDidDownloadNewAsset:]
cs_hep_marsh.json
cs_hep.json
-[CSHybridEndpointAnalyzer _getCSHybridEndpointerConfigForAsset:]
serverFeaturesQueue
T@"NSObject<OS_dispatch_queue>",&,N,V_serverFeaturesQueue
lastKnownServerEPFeatures
T@"CSServerEndpointFeatures",&,N,V_lastKnownServerEPFeatures
serverFeatureLatencies
T@"NSMutableArray",&,N,V_serverFeatureLatencies
serverFeaturesWarmupLatency
Td,N,V_serverFeaturesWarmupLatency
lastServerFeatureTimestamp
T@"NSDate",&,N,V_lastServerFeatureTimestamp
didReceiveServerFeatures
TB,N,V_didReceiveServerFeatures
clientLagThresholdMs
Td,N,V_clientLagThresholdMs
clampedSFLatencyMsForClientLag
Td,N,V_clampedSFLatencyMsForClientLag
useDefaultServerFeaturesOnClientLag
TB,N,V_useDefaultServerFeaturesOnClientLag
hybridClassifierQueue
T@"NSObject<OS_dispatch_queue>",&,N,V_hybridClassifierQueue
lastReportedEndpointTimeMs
Td,N,V_lastReportedEndpointTimeMs
stateSerialQueue
T@"NSObject<OS_dispatch_queue>",&,N,V_stateSerialQueue
didCommunicateEndpoint
TB,N,V_didCommunicateEndpoint
currentRequestSampleRate
TI,N,V_currentRequestSampleRate
vtExtraAudioAtStartInMs
Td,N,V_vtExtraAudioAtStartInMs
hepAudioOriginInMs
Td,N,V_hepAudioOriginInMs
firstAudioPacketTimestamp
T@"NSDate",&,N,V_firstAudioPacketTimestamp
didTimestampFirstAudioPacket
TB,N,V_didTimestampFirstAudioPacket
silencePosteriorGeneratorQueue
T@"NSObject<OS_dispatch_queue>",&,N,V_silencePosteriorGeneratorQueue
TB,R,N,V_canProcessCurrentRequest
+[CSUtils(Time) hostTimeFromSampleCount:anchorHostTime:anchorSampleCount:]
+[CSUtils(Time) sampleCountFromHostTime:anchorHostTime:anchorSampleCount:]
-[CSEndpointerProxy resetForNewRequestWithSampleRate:]
-[CSEndpointerProxy resetForVoiceTriggerTwoShotWithSampleRate:]
-[CSEndpointerProxy endpointer:didDetectStartpointAtTime:]
-[CSEndpointerProxy endpointer:didDetectHardEndpointAtTime:withMetrics:]
-[CSEndpointerProxy _shouldEnterTwoShotAtEndPointTime:]
-[CSEndpointerProxy endpointerModelVersion]
hybridEndpointer
T@"<CSEndpointAnalyzerImpl>",&,N,V_hybridEndpointer
vad2Endpointer
T@"<CSEndpointAnalyzerImpl>",&,N,V_vad2Endpointer
activeEndpointer
T@"<CSEndpointAnalyzerImpl>",W,N,V_activeEndpointer
didEnterTwoshot
TB,N,V_didEnterTwoshot
endpointerDelegate
T@"<CSEndpointAnalyzerDelegate>",W,N,V_endpointerDelegate
CSVoiceTriggerStatistics queue
-[NSNumber(XPCObject) initWithXPCObject:]
-[NSNumber(XPCObject) xpcObject]
-[CSFirstUnlockMonitor _stopMonitoring]
-[CSSpringboardStartMonitor _startMonitoringWithQueue:]
-[CSSpringboardStartMonitor _stopMonitoring]
-[CSSpringboardStartMonitor _checkSpringBoardStarted]
com.apple.springboard.finishedstartup
-[CSKeywordAnalyzerQuasar runRecognition]
-[CSKeywordAnalyzerQuasar endAudio]
triggerConfidence
Td,R,N,V_triggerConfidence
T@"<CSKeywordAnalyzerQuasarScoreDelegate>",W,N,V_delegate
corespeech.json
hybridendpointer.json
hybridendpointer_marsh.json
/BuildRoot/Library/Caches/com.apple.xbs/Sources/CoreSpeech_Sim/CoreSpeech-135.3/CoreSpeech/CSAsset.m
ERR: Unknown assetType: %lu
+[CSAsset fallBackAssetResourcePath]
defaultFallback
en-IN
en_IN
fr-CA
fr_CA
en-IE
en_IE
en-ZA
it-IT
it_IT
it-CH
ar-AE
ar_AE
ar-SA
th-TH
th_TH
sv-SE
sv_SE
ja-JP
ja_JP
es-ES
es_ES
es-US
es-CL
nl-NL
nl_NL
tr-TR
tr_TR
de-AT
de_AT
da-DK
da_DK
ms-MY
ms_MY
ru-RU
ru_RU
en-AU
en_US
en-CA
en-GB
en-SG
fr-BE
fr_BE
en-NZ
en_NZ
fr-FR
fr_FR
fr-CH
nb-NO
nb_NO
fi-FI
fi_FI
nl-BE
nl_BE
zh-HK
zh_HK
yue-CN
ko-KR
ko_KR
es-MX
es_MX
zh-CN
zh_CN
zh-TW
de-DE
de_DE
de-CH
pt-BR
pt_BR
he-IL
he_IL
-[CSAsset initWithResourcePath:configFile:configVersion:]
-[CSAsset _decodeJson:]
-[CSAsset getNumberForKey:category:default:]
-[CSAsset getStringForKey:category:default:]
configVersion:%@ resourcePath:%@ path:%@
path
T@"NSString",R,N,V_path
resourcePath
T@"NSString",R,N,V_resourcePath
hashFromResourcePath
T@"NSString",R,N,V_configVersion
-[CSAudioConverter _convertBufferedLPCM:allowPartial:timestamp:]
/BuildRoot/Library/Caches/com.apple.xbs/Sources/CoreSpeech_Sim/CoreSpeech-135.3/CoreSpeech/CSAudioConverter.m
Cannot produce ASPD for PCM
-[CSAudioConverter reset]
-[CSAudioConverter _configureAudioConverter:]
T@"<CSAudioConverterDelegate>",W,V_delegate
CreateAudioConverter
-[CSLanguageCodeUpdateMonitor _startMonitoringWithQueue:]
-[CSLanguageCodeUpdateMonitor _stopMonitoring]
-[CSLanguageCodeUpdateMonitor _didReceiveLanguageCodeUpdate]
Serial CSPolicy queue
CSSelfVoiceTriggerDetector Queue
-[CSSelfTriggerDetector _setAsset:]
-[CSSelfTriggerDetector speechManagerLPCMRecordBufferAvailable:chunk:]_block_invoke
-[CSSelfTriggerDetector speechManagerDidStartForwarding:successfully:error:]
-[CSSelfTriggerDetector speechManagerDidStopForwarding:forReason:]
-[CSSelfTriggerDetector keywordAnalyzerNDAPI:hasResultAvailable:forChannel:]
com.apple.siri.corespeech.selftrigger
outputAudioChannel
TI,N,V_outputAudioChannel
-[NSDictionary(XPCObject) initWithXPCObject:]
-[NSDictionary(XPCObject) initWithXPCObject:]_block_invoke
B12@?0r*4@"NSObject<OS_xpc_object>"8
-[NSDictionary(XPCObject) xpcObject]_block_invoke
v16@?0@4@8^B12
-[CSSpeakerDetectorNDAPI _initializeSAT:]
-[CSSpeakerDetectorNDAPI processSuperVector:withResult:]
T@"<CSSpeakerDetectorNDAPIDelegate>",W,N,V_delegate
-[CSVTUIKeywordDetector initWithLanguageCode:]
-[CSAudioRecorder willDestroy]
-[CSAudioRecorder _destroyVoiceController]
-[CSAudioRecorder _voiceControllerWithContext:error:]
-[CSAudioRecorder _beepCanceller]
-[CSAudioRecorder prepareRecordWithSettings:error:]
-[CSAudioRecorder setCurrentContext:error:]
-[CSAudioRecorder prewarmAudioSession]
-[CSAudioRecorder releaseAudioSession:]
-[CSAudioRecorder _resetZeroFilter]
-[CSAudioRecorder startRecordingWithSettings:error:]
-[CSAudioRecorder startRecording:]
context
-[CSAudioRecorder startRecording]
-[CSAudioRecorder stopRecording]
-[CSAudioRecorder _recordingSampleRate]
Builtin Microphone
-[CSAudioRecorder playAlertSoundForType:]
ZeroFilterMetrics
-[CSAudioRecorder _audioRecorderDidStopRecordingForReason:]
BeepCancellerMetrics
-[CSAudioRecorder voiceControllerDidStartRecording:successfully:error:]
-[CSAudioRecorder voiceControllerDidStopRecording:forReason:]
-[CSAudioRecorder voiceControllerRecordHardwareConfigurationDidChange:toConfiguration:]
-[CSAudioRecorder voiceControllerBeginRecordInterruption:]
-[CSAudioRecorder voiceControllerBeginRecordInterruption:withContext:]
-[CSAudioRecorder voiceControllerEndRecordInterruption:]
-[CSAudioRecorder voiceControllerMediaServicesWereLost:]
-[CSAudioRecorder voiceControllerMediaServicesWereReset:]
-[CSAudioRecorder _deinterleaveBufferIfNeeded:]
-[CSAudioRecorder _createDeInterleaverIfNeeded]
-[CSAudioRecorder _createSampleRateConverterIfNeeded]
T@"<CSAudioRecorderDelegate>",W,N,V_delegate
-[NSData(XPCObject) initWithXPCObject:]
twoShotDecisionWaitTime
CVTConfigPathNDAPI
CVTThreshold
CVTTwoShotThreshold
CVTTwoShotDecisionWaitTime
CSKeywordDetector Queue
-[CSKeywordDetector startDetectKeyword:]
-[CSKeywordDetector startDetectKeyword:]_block_invoke
-[CSKeywordDetector _setAsset:]
-[CSKeywordDetector speechManagerLPCMRecordBufferAvailable:chunk:]_block_invoke
-[CSKeywordDetector speechManagerDidStartForwarding:successfully:error:]
-[CSKeywordDetector speechManagerDidStopForwarding:forReason:]
-[CSKeywordDetector keywordAnalyzerQuasar:hasResultAvailable:forChannel:]
T@"CSKeywordAnalyzerQuasar",&,N,V_keywordAnalyzer
decisionWaitSampleCount
TI,N,V_decisionWaitSampleCount
nohash
((?:[a-z]|[0-9])*)\.asset
+[CSUtils(ResourcePathHash) assetHashInResourcePath:]
-[CSAudioFileWriter initWithURL:inputFormat:outputFormat:]
-[CSAudioFileWriter addSamples:len:]
fileURL
T@"NSURL",R,N,V_fileURL
CSAudioZeroCounter
SmartSiriVolume
CSAudioFileManager
Directory
CSSmartSiriVolume
CSSpeechManagerDelegate
NSObject
CSVoiceTriggerDelegate
CSAudioFileReader
CSContinuousVoiceTrigger
CSKeywordAnalyzerNDAPIScoreDelegate
CSAudioSampleRateConverter
XPCObject
LanguageCode
LPCMTypeConversion
CSSpeakerModel
CSSpeakerModelRetrainer
CSVoiceTriggerFirstPass
CSCoreSpeechServices
CSConfig
CSSpeechManager
CSAudioRecorderDelegate
CSStateMachineDelegate
CSAssetManagerDelegate
CSLanguageCodeUpdateMonitorDelegate
CSVTUIEditDistance
CSAssetManager
CSVoiceTriggerAssetMetaUpdateMonitorDelegate
CSSpeechEndpointAssetMetaUpdateMonitorDelegate
Utils
VoiceTriggerFirstPass
CSVTUITrainingSession
SFSpeechRecognitionTaskDelegate
CSVTUIAudioSessionDelegate
CSVTUIEndPointDelegate
CSSpeechController
CSAudioConverterDelegate
CSPreferences
CSStateMachine
Meter
CSEventMonitor
CSAudioFileLog
Alert
CSVoiceTriggerFileLogger
Metrics
CSAssetManagerEnablePolicyMac
VoiceTriggerSecondPass
VoiceTriggerPassThru
CSVoiceTriggerEnabledPolicyNonAOP
CSSpeechEndpointAssetMetaUpdateMonitor
VoiceTriggerRecord
CSMyriadPHash
CSVoiceTriggerFidesClient
FidesRecordInfoHelper
CSVTUIRegularExpressionMatcher
CSVTUIASRGrammars
NSURLSessionDelegate
CSVTUITrainingManager
CSVTUITrainingSessionDelegate
CSEndpointAnalyzerDelegate
CSUtils
CSKeywordAnalyzerNDAPI
CSEndpointerMetrics
CSRemoteControlClient
CSAsset
AudioStreamBasicDescription
CSVoiceTriggerEnabledMonitor
CSRemoteRecordClient
KeywordDetector
CSVoiceTriggerSpeakerTrainer
CSVTUIAudioSessionRemote
CSVTUIAudioSession
CSBeepCanceller
CSVoiceTriggerSecondPass
CSSpeakerDetectorNDAPIDelegate
CSKeywordAnalyzerQuasarScoreDelegate
CSVoiceTriggerFirstPassDelegate
CSVoiceTriggerEventNotifier
CSAudioZeroFilter
AudioFile
CSAssetManagerEnablePolicy
CSCoreSpeechServiceListenerDelegate
CSAudioCircularBuffer
CSVoiceTriggerAssetMetaUpdateMonitor
SpeakerDetectorNDAPI
CSServerEndpointFeatures
CSVAD2EndpointAnalyzer
CSEndpointAnalyzerImpl
CSEndpointAnalyzer
private
CSAudioChunk
CSNovDetectorResult
CSNovDetector
Bitset
CSVTUITrainingSessionWithPayload
CSHybridEndpointAnalyzer
Time
CSEndpointerProxy
CSVoiceTriggerStatistics
CSFirstUnlockMonitor
CSAudioPowerMeter
CSSpringboardStartMonitor
CSKeywordAnalyzerQuasar
CSAudioConverter
CSLanguageCodeUpdateMonitor
CSPolicy
CSEventMonitorDelegate
CSSelfTriggerDetector
RecordContext
CSSpeakerDetectorNDAPI
CSVTUIKeywordDetector
CSAudioRecorder
AVVoiceControllerRecordDelegate
AVVoiceControllerPlaybackDelegate
CSBeepCancellerDelegate
ContinuousVoiceTrigger
DuckOption
CSKeywordDetector
ResourcePathHash
CSAudioFileWriter
@20@0:4@8f12I16
v16@0:4@8I12
v8@0:4
@"NSString"
I8@0:4
f8@0:4
@8@0:4
@92@0:4@8{AudioStreamBasicDescription=dIIIIIIII}12{AudioStreamBasicDescription=dIIIIIIII}52
v12@0:4@8
v20@0:4@8@12f16
v20@0:4@8@12@?16
@24@0:4@8@12@16^@20
B12@0:4@8
#8@0:4
@12@0:4:8
@16@0:4:8@12
@20@0:4:8@12@16
B8@0:4
B12@0:4#8
B12@0:4:8
Vv8@0:4
^{_NSZone=}8@0:4
B12@0:4@"Protocol"8
@"NSString"8@0:4
v16@0:4@8@12
v20@0:4@8B12@16
v16@0:4@8i12
v16@0:4@8f12
v16@0:4@"CSSpeechManager"8@"AVVCAudioBuffer"12
v16@0:4@"CSSpeechManager"8@"CSAudioChunk"12
v20@0:4@"CSSpeechManager"8B12@"NSError"16
v16@0:4@"CSSpeechManager"8i12
@"NSDictionary"8@0:4
v16@0:4@"CSSpeechManager"8f12
v12@0:4@"CSSpeechManager"8
v16@0:4@"CSSpeechManager"8@"NSDictionary"12
v16@0:4d8
v12@0:4@"NSDictionary"8
@20@0:4f8@12f16
v16@0:4I8i12
v36@0:4@8i12B16Q20Q28
f12@0:4i8
f16@0:4f8f12
f28@0:4f8f12f16f20f24
f36@0:4f8f12f16f20f24f28f32
f20@0:4f8f12f16
v16@0:4Q8
f12@0:4f8
@"NSObject<OS_dispatch_queue>"
{unique_ptr<SmartSiriVolume, std::__1::default_delete<SmartSiriVolume> >="__ptr_"{__compressed_pair<SmartSiriVolume *, std::__1::default_delete<SmartSiriVolume> >="__first_"^{SmartSiriVolume}}}
{vector<float, std::__1::allocator<float> >="__begin_"^f"__end_"^f"__end_cap_"{__compressed_pair<float *, std::__1::allocator<float> >="__first_"^f}}
@"NSUserDefaults"
@"CSAsset"
@12@0:4@8
@12@0:4L8
L8@0:4
^{OpaqueExtAudioFile=}
v20@0:4@8@12I16
v20@0:4@"CSKeywordAnalyzerNDAPI"8@"NSDictionary"12I16
@16@0:4@8@12
v12@0:4f8
i8@0:4
v12@0:4i8
v12@0:4I8
@"<CSVoiceTriggerDelegate>"
@"CSSpeechManager"
@"CSKeywordAnalyzerNDAPI"
@88@0:4{AudioStreamBasicDescription=dIIIIIIII}8{AudioStreamBasicDescription=dIIIIIIII}48
^{OpaqueAudioConverter=}88@0:4{AudioStreamBasicDescription=dIIIIIIII}8{AudioStreamBasicDescription=dIIIIIIII}48
^{OpaqueAudioConverter=}
{AudioStreamBasicDescription="mSampleRate"d"mFormatID"I"mFormatFlags"I"mBytesPerPacket"I"mFramesPerPacket"I"mBytesPerFrame"I"mChannelsPerFrame"I"mBitsPerChannel"I"mReserved"I}
B16@0:4@8@12
v12@0:4B8
@"<CSVoiceTriggerFirstPassDelegate>"
@"CSPolicy"
@"NSMutableArray"
@"NSDictionary"
d8@0:4
S8@0:4
v24@0:4@8@12Q16
v24@0:4@"CSAudioRecorder"8@"NSData"12Q16
v16@0:4@"CSAudioRecorder"8@"AVVCAudioBuffer"12
v20@0:4@"CSAudioRecorder"8B12@"NSError"16
v16@0:4@"CSAudioRecorder"8i12
v12@0:4@"CSAudioRecorder"8
v16@0:4@"CSAudioRecorder"8@"NSDictionary"12
v20@0:4i8i12i16
v16@0:4i8i12
v12@0:4@"CSAssetManager"8
v16@0:4@8@"NSString"12
@24@0:4@8@12@16@20
B16@0:4@8^@12
B12@0:4^@8
B16@0:4i8^@12
v28@0:4d8@16@?20@?24
B16@0:4I8^@12
B20@0:4@8I12^@16
v16@0:4@8^@12
Q16@0:4Q8
@12@0:4i8
@12@0:4I8
@?8@0:4
v12@0:4@?8
@"CSAudioRecorder"
@"CSStateMachine"
@"CSAudioCircularBuffer"
@"CSVoiceTriggerFirstPass"
@"CSVoiceTriggerSecondPass"
@"<CSSpeechManagerDelegate>"
@"CSVoiceTriggerEventNotifier"
@"CSVoiceTriggerFileLogger"
@"CSSelfTriggerDetector"
@"CSContinuousVoiceTrigger"
@"CSKeywordDetector"
@"CSSmartSiriVolume"
@"CSMyriadPHash"
@"CSVoiceTriggerFidesClient"
@"NSHashTable"
@"NSObject<OS_dispatch_source>"
@"NSUUID"
v16@0:4@8B12
@16@0:4I8@12
@20@0:4I8@12B16
v16@0:4@8@?12
v16@0:4I8@12
v20@0:4@8@?12@?16
@"NSMutableDictionary"
v12@0:4@"SFSpeechRecognitionTask"8
v16@0:4@"SFSpeechRecognitionTask"8@"SFTranscription"12
v16@0:4@"SFSpeechRecognitionTask"8@"SFSpeechRecognitionResult"12
v16@0:4@"SFSpeechRecognitionTask"8B12
v16@0:4B8@12
v16@0:4B8@"NSError"12
v12@0:4@"NSData"8
v12@0:4@"NSError"8
@48@0:4i8i12@16@20@24@28@32@36@40@?44
v16@0:4i8B12
v20@0:4i8B12@?16
@"CSVTUIKeywordDetector"
@"<CSVTUIAudioSession>"
@"SFSpeechRecognizer"
@"SFSpeechAudioBufferRecognitionRequest"
@"SFSpeechRecognitionTask"
@"NSTimer"
@"<CSVTUITrainingSessionDelegate>"
v24@0:4@"CSAudioConverter"8@"NSArray"12Q16
{AudioStreamBasicDescription=dIIIIIIII}8@0:4
B16@0:4d8
B12@0:4B8
B16@0:4@8i12
B12@0:4i8
Q8@0:4
f12@0:4I8
v20@0:4d8@?16
@"CSAudioConverter"
@"CSAudioSampleRateConverter"
@"CSAudioZeroCounter"
@"<CSSpeechControllerDelegate>"
@"CSEndpointerProxy"
@"CSAudioFileWriter"
@12@0:4^@8
d16@0:4d8
@"<CSStateMachineDelegate>"
@"NSURL"
S16@0:4^f8i12
@28@0:4I8f12I16Q20
s8@0:4
v12@0:4s8
v16@0:4i8@12
i24@0:4@8@12@16@20
v16@0:4@"NSURLSession"8@"NSError"12
v20@0:4@"NSURLSession"8@"NSURLAuthenticationChallenge"12@?<v@?i@"NSURLCredential">16
v12@0:4@"NSURLSession"8
@16@0:4i8@12
@20@0:4@8i12@16
v20@0:4@8d12
v24@0:4@8d12@20
v20@0:4@"<CSEndpointAnalyzer>"8d12
v24@0:4@"<CSEndpointAnalyzer>"8d12@"CSEndpointerMetrics"20
@12@0:4@?8
i20@0:4i8B12@?16
@"CSVAD2EndpointAnalyzer"
@"CSVTUITrainingSession"
@"<CSVTUITrainingManagerDelegate>"
@16@0:4@8i12
@"CSNovDetector"
@"<CSKeywordAnalyzerNDAPIScoreDelegate>"
@32@0:4d8@16i20@24@28
@"NSArray"
B20@0:4d8^@16
@"<CSRemoteControlClientDelegate>"
@"<CSRemoteRecordClientDelegate>"
B20@0:4@8@12@16
v12@0:4@"<CSVTUIAudioSessionDelegate>"8
v12@0:4@"<Endpointer>"8
i12@0:4i8
@"<CSVTUIAudioSessionDelegate>"
v20@0:4@8Q12
{unique_ptr<BatchBeepCanceller, std::__1::default_delete<BatchBeepCanceller> >="__ptr_"{__compressed_pair<BatchBeepCanceller *, std::__1::default_delete<BatchBeepCanceller> >="__first_"^{BatchBeepCanceller}}}
{vector<short, std::__1::allocator<short> >="__begin_"^s"__end_"^s"__end_cap_"{__compressed_pair<short *, std::__1::allocator<short> >="__first_"^s}}
@"<CSBeepCancellerDelegate>"
v16@0:4@"CSSpeakerDetectorNDAPI"8@"NSDictionary"12
v20@0:4@"CSKeywordAnalyzerQuasar"8@"NSDictionary"12I16
v16@0:4@"CSVoiceTriggerFirstPass"8@"NSDictionary"12
v24@0:4@8@12I16B20
@"CSKeywordAnalyzerQuasar"
@"CSSpeakerDetectorNDAPI"
@"CSSpeakerModel"
@24@0:4I8S12d16
Q24@0:4@8Q12^@20
Q12@0:4^@8
{unique_ptr<CSAudioZeroFilterImpl<unsigned short>, std::__1::default_delete<CSAudioZeroFilterImpl<unsigned short> > >="__ptr_"{__compressed_pair<CSAudioZeroFilterImpl<unsigned short> *, std::__1::default_delete<CSAudioZeroFilterImpl<unsigned short> > >="__first_"^{CSAudioZeroFilterImpl<unsigned short>}}}
B16@0:4@8@?12
Vv12@0:4@?8
Vv20@0:4@8i12@?16
Vv12@0:4@?<v@?@"NSString">8
Vv20@0:4@"NSArray"8i12@?<v@?@"NSError">16
Vv12@0:4@?<v@?I>8
Vv12@0:4@?<v@?>8
@20@0:4I8f12f16
v16@0:4r^v8I12
v24@0:4r^v8I12Q16
@16@0:4Q8
@16@0:4I8I12
@12@0:4^I8
v20@0:4I8I12@16
{unique_ptr<corespeech::CSAudioCircularBufferImpl<unsigned short>, std::__1::default_delete<corespeech::CSAudioCircularBufferImpl<unsigned short> > >="__ptr_"{__compressed_pair<corespeech::CSAudioCircularBufferImpl<unsigned short> *, std::__1::default_delete<corespeech::CSAudioCircularBufferImpl<unsigned short> > >="__first_"^{CSAudioCircularBufferImpl<unsigned short>}}}
r*8@0:4
@44@0:4i8i12d16@24d28@36i40
@40@0:4i8i12d16@24d28@36
v12@0:4@"CSAudioChunk"8
@"<CSEndpointAnalyzerDelegate>"8@0:4
v12@0:4@"<CSEndpointAnalyzerDelegate>"8
v12@0:4@"CSServerEndpointFeatures"8
v20@0:4d8@?<v@?B@"NSArray">16
v12@0:4L8
^{OpaqueAudioComponentInstance=}
@"<CSEndpointAnalyzerDelegate>"
@"NSMutableData"
@16@0:4d8
B16@0:4^{AudioStreamBasicDescription=dIIIIIIII}8L12
v20@0:4d8L16
v16@0:4^f8L12
@40@0:4@8I12I16I20Q24Q32
@20@0:4I8I12I16
@"NSData"
I12@0:4I8
v16@0:4I8@?12
@"CSServerEndpointFeatures"
@"NSDate"
Q12@0:4f8
f16@0:4Q8
d16@0:4Q8
Q32@0:4Q8Q16Q24
@"<CSEndpointAnalyzerImpl>"
@12@0:4f8
v20@0:4r^s8i12i16
@20@0:4@8@12B16
v16@0:4r^s8i12
@"<CSKeywordAnalyzerQuasarScoreDelegate>"
@20@0:4I8@12@16
@20@0:4@8@12@16
B20@0:4@8@12B16
v24@0:4@8B12Q16
v12@0:4^{OpaqueAudioConverter=}8
@"<CSAudioConverterDelegate>"
f12@0:4@8
@16@0:4@8I12
@"<CSSpeakerDetectorNDAPIDelegate>"
I16@0:4I8I12
v24@0:4@8i12d16
v16@0:4@"AVVoiceController"8B12
v20@0:4@"AVVoiceController"8B12@"NSError"16
v16@0:4@"AVVoiceController"8i12
v12@0:4@"AVVoiceController"8
v24@0:4@"AVVoiceController"8i12d16
v16@0:4@"AVVoiceController"8@"NSError"12
v16@0:4@"AVVoiceController"8@"AVVCAudioBuffer"12
v16@0:4@"AVVoiceController"8@"NSDictionary"12
v24@0:4@"CSBeepCanceller"8@"NSData"12Q16
@16@0:4@8^@12
@"AVVoiceController"
@"CSAudioZeroFilter"
@"CSBeepCanceller"
{AudioBufferList="mNumberBuffers"I"mBuffers"[1{AudioBuffer="mNumberChannels"I"mDataByteSize"I"mData"^v}]}
^{AudioBufferList=I[1{AudioBuffer=II^v}]}
@"CSRemoteRecordClient"
@"<CSAudioRecorderDelegate>"
s16@0:4r^v8l12
zPLR
zPLR
zPLR
zPLR
zPLR
zPLR
zPLR
zPLR
zPLR
zPLR
zPLR
zPLR
zPLR
