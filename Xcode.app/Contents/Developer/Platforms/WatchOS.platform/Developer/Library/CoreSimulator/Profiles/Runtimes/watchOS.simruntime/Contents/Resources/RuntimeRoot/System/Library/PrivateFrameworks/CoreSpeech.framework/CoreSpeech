com.apple.siri.SiriDebug.SpeakerVoiceGradingTrigger
com.apple.siri.SiriDebug.RemoteNearMissGradingTrigger
com.apple.siri.SiriDebug
+[CSSiriDebugConnection launchSiriDebugAppWithMessage:]_block_invoke
v4@?0
v12@?0@"AFSiriResponse"4@"NSError"8
CSMediaPlayingMonitor queue
-[CSMediaPlayingMonitor initializeMediaPlayingState]_block_invoke
v8@?0@4
v8@?0I4
-[CSMediaPlayingMonitor _startMonitoringWithQueue:]
-[CSMediaPlayingMonitor _stopMonitoring]
-[CSMediaPlayingMonitor _notePossiblePlayPausedStateChange:]
PLAYING
NOT PLAYING
Audio/Video
Alarm
CSVolumeMonitor queue
-[CSVolumeMonitor _startMonitoringWithQueue:]
-[CSVolumeMonitor fetchVolumeFromAVSystemControllerForAudioCategory:]_block_invoke
-[CSVolumeMonitor systemControllerDied:]
-[CSVolumeMonitor startObservingSystemVolumes]
-[CSVolumeMonitor _startObservingSystemControllerLifecycle]
CSTimerMonitor queue
-[CSTimerMonitor _startMonitoringWithQueue:]
-[CSTimerMonitor _stopMonitoring]
CSAlarmMonitor queue
-[CSAlarmMonitor _startMonitoringWithQueue:]
-[CSAlarmMonitor _stopMonitoring]
-[CSAudioZeroCounter getZeroStatisticsFromBuffer:entireSamples:]
-[CSAudioZeroCounter stopReportZeroStatistics]
smartSiriVolume
noiseLevelChannelBitset
LKFSChannelBitset
energyBufferSize
noiseLowerPercentile
noiseUpperPercentile
LKFSLowerPercentile
LKFSUpperPercentile
noiseTimeConstant
noiseMicSensitivityOffset
LKFSTimeConstant
LKFSMicSensitivityOffset
noiseTTSMappingInputRangeLow
noiseTTSMappingInputRangeHigh
noiseTTSMappingOutputRangeLow
noiseTTSMappingOutputRangeHigh
LKFSTTSMappingInputRangeLow
LKFSTTSMappingInputRangeHigh
LKFSTTSMappingOutputRangeLow
LKFSTTSMappingOutputRangeHigh
userOffsetInputRangeLow
userOffsetInputRangeHigh
userOffsetOutputRangeLow
userOffsetOutputRangeHigh
TTSVolumeLowerLimitDB
TTSVolumeUpperLimitDB
noiseWeight
SSVNoiseLevelChannelBitset
TI,R,N
SSVLKFSChannelBitset
SSVEnergyBufferSize
SSVNoiseLowerPercentile
SSVNoiseUpperPercentile
SSVLKFSLowerPercentile
SSVLKFSUpperPercentile
SSVNoiseTimeConstant
Tf,R,N
SSVNoiseMicSensitivityOffset
SSVLKFSTimeConstant
SSVLKFSMicSensitivityOffset
SSVNoiseTTSMappingInputRangeLow
SSVNoiseTTSMappingInputRangeHigh
SSVNoiseTTSMappingOutputRangeLow
SSVNoiseTTSMappingOutputRangeHigh
SSVLKFSTTSMappingInputRangeLow
SSVLKFSTTSMappingInputRangeHigh
SSVLKFSTTSMappingOutputRangeLow
SSVLKFSTTSMappingOutputRangeHigh
SSVUserOffsetInputRangeLow
SSVUserOffsetInputRangeHigh
SSVUserOffsetOutputRangeLow
SSVUserOffsetOutputRangeHigh
SSVTTSVolumeLowerLimitDB
SSVTTSVolumeUpperLimitDB
SSVNoiseWeight
SSVParameterDirectionary
T@"NSDictionary",R,N
PCM-
OPUS_
com.apple.CoreSpeech.AudioLogging
+[CSAudioFileManager generateDeviceAudioLogging:numChannels:speechId:]_block_invoke
+[CSAudioFileManager _readDataFromFileHandle:toFileHandle:]
%@%@.wav
%@/%@%@.wav
+[CSAudioFileManager _createAudioFileWriterWithLoggingDir:inputFormat:outputFormat:]
en_US_POSIX
yyyy_MM_dd-HHmmss.SSS
^%@*
#SpkrRejected#
FinalScores
SpIdScoreThreshold
productType
productVersion
buildVersion
speakerDetected
.json
-detected.json
-rejected.json
-[NSDictionary(SpIdMetadataLogging) logSpeakerIdMetadataAtFilepath:additionalMetadata:]
/BuildRoot/Library/Caches/com.apple.xbs/Sources/CoreSpeech_Sim/CoreSpeech-140.13.76/CoreSpeech/NSDictionary+SpIdMetadataLogging.m
<Unknown File>
Error creating uttMetaJsonData: %@
v8@?0i4
-[CSVoiceTriggerAssetDownloadMonitor _startMonitoringWithQueue:]
-[CSVoiceTriggerAssetDownloadMonitor _stopMonitoring]
-[CSVoiceTriggerAssetDownloadMonitor _didInstalledNewVoiceTriggerAsset]
com.apple.MobileAsset.VoiceTriggerAssets.new-asset-installed
com.apple.MobileAsset.VoiceTriggerAssetsWatch.new-asset-installed
com.apple.MobileAsset.VoiceTriggerAssetsMarsh.new-asset-installed
com.apple.MobileAsset.VoiceTriggerAssetsMac.new-asset-installed
+[CSUtils(Directory) removeLogFilesInDirectory:matchingPattern:beforeDays:]_block_invoke
v12@?0@"NSArray"4^@8
+[CSUtils(Directory) clearLogFilesInDirectory:matchingPattern:exceedNumber:]_block_invoke_2
+[CSUtils(Directory) clearLogFilesInDirectory:matchingPattern:exceedNumber:]_block_invoke
Unable to get %@ for file at %@: %@
i12@?0@"NSURL"4@"NSURL"8
B12@?0@"NSURL"4@"NSDictionary"8
+[CSUtils(Directory) _contentsOfDirectoryAtURL:matchingPattern:includingPropertiesForKeys:error:]
triggerStartSampleCount
noiseLevelDB
musicLevelDB
musicPlaybackVolumeDB
alarmVolume
finalTTSVolume
isMediaPlaying
isAlarmPlaying
isTimerPlaying
isLKFSProcessPaused
removeVoiceTriggerSamples
-[CSSmartSiriVolume initWithSamplingRate:asset:]
-[CSSmartSiriVolume startSmartSiriVolume]_block_invoke
RUNNING
PAUSED
v8@?0B4
-[CSSmartSiriVolume initializeMediaPlayingState]_block_invoke
playing
NOT playing
-[CSSmartSiriVolume initializeMediaPlayingState]
-[CSSmartSiriVolume initializeAlarmState]_block_invoke
firing
NOT firing
-[CSSmartSiriVolume initializeTimerState]_block_invoke
-[CSSmartSiriVolume _setAsset:]
-[CSSmartSiriVolume _processAudioChunk:soundType:]
-[CSSmartSiriVolume estimateSoundLevelbySoundType:]_block_invoke
-[CSSmartSiriVolume _pauseSSVProcessing]_block_invoke
-[CSSmartSiriVolume _resumeSSVProcessing]_block_invoke
-[CSSmartSiriVolume voiceTriggerDidDetectKeyword:]
-[CSSmartSiriVolume voiceTriggerDidDetectKeyword:]_block_invoke
-[CSSmartSiriVolume estimatedTTSVolumeForNoiseLevelAndLKFS:LKFS:]_block_invoke
-[CSSmartSiriVolume _combineResultsWithOptimalFromNoise:andOptimalFromLkfs:withUserOffset:]
-[CSSmartSiriVolume CSMediaPlayingMonitor:didReceiveMediaPlayingChanged:]_block_invoke
-[CSSmartSiriVolume CSAlarmMonitor:didReceiveAlarmChanged:]_block_invoke
-[CSSmartSiriVolume CSTimerMonitor:didReceiveTimerChanged:]_block_invoke
-[CSSmartSiriVolume _setStartAnalyzeTime:]
hash
TI,R
superclass
T#,R
description
T@"NSString",R,C
debugDescription
delegate
T@"<CSSmartSiriVolumeDelegate>",W,N,V_delegate
allocator<T>::allocate(size_t n) 'n' exceeds maximum supported size
com.apple.assistant
Siri Global
 - %@
CSKeychainValueForAccountAndKey
CSAudioFileReader Queue
-[CSAudioFileReader initWithURL:]
-[CSAudioFileReader prepareRecording:]
-[CSAudioFileReader startRecording]
-[CSAudioFileReader _readAudioBufferAndFeed]
-[CSAudioFileReader stopRecording]
T@"<CSAudioFileReaderDelegate>",W,N,V_delegate
-[CSAudioSampleRateConverter _createSampleRateConverterWithInASBD:outASBD:]
/BuildRoot/Library/Caches/com.apple.xbs/Sources/CoreSpeech_Sim/CoreSpeech-140.13.76/CoreSpeech/CSAudioSampleRateConverter.m
Too many buffers
l16@?0^I4^{AudioBufferList=I[1{AudioBuffer=II^v}]}8^^{AudioStreamPacketDescription}12
-[CSAudioSampleRateConverter convertSampleRateOfBuffer:]
-[NSString(XPCObject) _cs_initWithXPCObject:]
sysConfigFilepath
T@"NSString",R,N,V_sysConfigFilepath
spIdType
satScoreThreshold
+[CSUtils(LanguageCode) getSiriLanguageWithFallback:]
com.apple.VoiceTriggerUI.TrainingSessionQueue
com.apple.VoiceTriggerUI.TrainingManager
-[CSVTUITrainingManager setLocaleIdentifier:]
-[CSVTUITrainingManager createKeywordDetector]
-[CSVTUITrainingManager cleanupWithCompletion:]
-[CSVTUITrainingManager cleanupWithCompletion:]_block_invoke
-[CSVTUITrainingManager trainUtterance:shouldUseASR:completion:]
-[CSVTUITrainingManager trainUtterance:shouldUseASR:completion:]_block_invoke_2
-[CSVTUITrainingManager trainUtterance:shouldUseASR:completion:]_block_invoke
-[CSVTUITrainingManager cancelTrainingForID:]
-[CSVTUITrainingManager closeSessionBeforeStartWithStatus:successfully:withCompletion:]
-[CSVTUITrainingManager _shouldShowHeadsetDisconnectionMessage]
-[CSVTUITrainingManager _startAudioSession]
-[CSVTUITrainingManager setSuspendAudio:]
-[CSVTUITrainingManager setSuspendAudio:]_block_invoke
-[CSVTUITrainingManager CSVTUITrainingSession:hasTrainUtterance:languageCode:]
-[CSVTUITrainingManager VTUITrainingSessionStopListen]
Tf,V_rms
T@"<CSVTUITrainingManagerDelegate>",W,N,V_delegate
speechRecognizerAvailable
TB,R,V_speechRecognizerAvailable
audioSource
suspendAudio
-[CSSpeakerModel _createDirectoryIfNotExist:]
xx_XX
model
audio
self ENDSWITH '.wav'
-[CSSpeakerModel discard]
modelPath
T@"NSString",R,N
utteranceDirectory
enrollmentUtterance
T@"NSArray",R,N
isValid
TB,R,N
needsRetrain
lastSpeakerIdInfo
audioSessionState
TI,N,GgetAudioSessionState,V_audioSessionState
com.apple.corespeech.corespeechservices
com.apple.corespeech.xpc
+[CSCoreSpeechServices installedVoiceTriggerAssetForLanguageCode:completion:]_block_invoke
reason
CoreSpeechXPC service invalidated
+[CSCoreSpeechServices fetchRemoteVoiceTriggerAssetForLanguageCode:completion:]_block_invoke
+[CSCoreSpeechServices voiceTriggerRTModelForVersion:minorVersion:downloadedModels:preinstalledModels:completion:]_block_invoke
+[CSCoreSpeechServices voiceTriggerRTModelForVersion:minorVersion:downloadedModels:preinstalledModels:completion:]
+[CSCoreSpeechServices requestUpdatedSATAudio]_block_invoke
+[CSCoreSpeechServices getFirstPassRunningMode]_block_invoke
fileUrl
T@"NSURL",&,N,V_fileUrl
aesKey
T@"NSData",&,N,V_aesKey
readBuffer
T@"NSData",&,N,V_readBuffer
sampleByteDepth
TI,N,V_sampleByteDepth
speechManagerStartSampleCount
speechManagerSetRecordModeToRecordingDelay
speechManagerDuckOthers
triggerEndMachTime
CSSpeechManager Asset Query Queue
-[CSSpeechManager startManager]
-[CSSpeechManager startManager]_block_invoke
-[CSSpeechManager _reset]
-[CSSpeechManager _getVoiceTriggerAssetForMac:]_block_invoke
v12@?0@"CSAsset"4@"NSError"8
v8@?0@"CSAsset"4
-[CSSpeechManager registerSpeechController:]
-[CSSpeechManager _notifyEvent:]
-[CSSpeechManager _createRecorderWithContextIfNeeded:error:]
-[CSSpeechManager _prepareRecorderWithSettings:error:]
B4@?0
-[CSSpeechManager _prepareRecorderWithSettings:error:]_block_invoke
v12@?0B4@"NSError"8
-[CSSpeechManager _prepareListenWithSettings:error:]
-[CSSpeechManager prewarmAudioSession]
-[CSSpeechManager recordRoute]
-[CSSpeechManager recordDeviceInfo]
-[CSSpeechManager recordSettings]
-[CSSpeechManager setClientContext:error:]
-[CSSpeechManager setClientContext:error:]_block_invoke
Mediaserverd is recovering from crash
-[CSSpeechManager prepareRecordingForClient:error:]
-[CSSpeechManager prepareRecordingForClient:error:]_block_invoke
Cannot prepare since audio recorder was not initialized
-[CSSpeechManager _startRecordingWithSettings:error:]
-[CSSpeechManager _startListening:]
-[CSSpeechManager _setRecordMode:error:]
-[CSSpeechManager _setRecordMode:withDelay:error:]
-[CSSpeechManager _scheduleSetRecordModeToRecordingWithDelay:forReason:validator:completion:]_block_invoke
-[CSSpeechManager _scheduleSetRecordModeToRecordingWithDelay:forReason:validator:completion:]
-[CSSpeechManager _cancelPendingSetRecordModeToRecordingForReason:]
-[CSSpeechManager _performPendingSetRecordModeToRecordingForReason:]
-[CSSpeechManager _setCurrentContext:error:]
-[CSSpeechManager _releaseClientAudioSession:]
-[CSSpeechManager _releaseAudioSessionForListening:error:]
-[CSSpeechManager _handleVoiceTriggerSwitchAOP2APEvent:settings:error:]
-[CSSpeechManager startRecordingAsyncWithSetting:event:completion:]
-[CSSpeechManager startRecordingWithSetting:event:error:]
-[CSSpeechManager _startRecordingWithSettings:event:error:]
-[CSSpeechManager _startRecordingWithSettings:event:error:]_block_invoke
Fail to start recording under PollingListening state.
Fail to start recording when awaiting to stop.
-[CSSpeechManager _startRecordingForAOPFirstPassTriggerWithSettings:error:]
-[CSSpeechManager _startRecordingForClient:error:]
-[CSSpeechManager _startRecordingForClient:error:]_block_invoke
-[CSSpeechManager stopRecordingWithEvent:]_block_invoke
-[CSSpeechManager _stopRecordingWithEvent:]_block_invoke
-[CSSpeechManager audioRecorderLostMediaserverd:]
-[CSSpeechManager mediaserverdDidRestart]
-[CSSpeechManager didTransitFrom:to:by:]_block_invoke
-[CSSpeechManager didIgnoreEvent:from:]
-[CSSpeechManager _createListenPollingTimer]
-[CSSpeechManager _createListenPollingTimer]_block_invoke
-[CSSpeechManager _startListenPolling]
-[CSSpeechManager _stopListenPolling]
-[CSSpeechManager _destroyAudioRecorderIfNeeded]
-[CSSpeechManager _startForwardingToClient]
-[CSSpeechManager _stopForwardingToClient]
-[CSSpeechManager audioRecorderBufferAvailable:buffer:atTime:]_block_invoke
-[CSSpeechManager audioRecorderDidStartRecording:successfully:error:]_block_invoke
-[CSSpeechManager audioRecorderDidStopRecording:forReason:]_block_invoke
-[CSSpeechManager audioRecorderRecordHardwareConfigurationDidChange:toConfiguration:]_block_invoke
-[CSSpeechManager audioRecorderDidFinishAlertPlayback:ofType:error:]_block_invoke
-[CSSpeechManager audioRecorderBeginRecordInterruption:]_block_invoke
-[CSSpeechManager audioRecorderBeginRecordInterruption:withContext:]_block_invoke
-[CSSpeechManager audioRecorderEndRecordInterruption:]_block_invoke
-[CSSpeechManager audioRecorder:willSetAudioSessionActive:]_block_invoke
-[CSSpeechManager audioRecorder:didSetAudioSessionActive:]_block_invoke
-[CSSpeechManager audioRecorderDisconnected:]_block_invoke
-[CSSpeechManager CSLanguageCodeUpdateMonitor:didReceiveLanguageCodeChanged:]
Init
Stop
FirstPassAP
SecondPassAP
SecondPassAOP
RecordPendingAP
RecordPendingAOP
RecordWithVTRunningAP
Record
PollingListeningWithVTRunningAP
PollingListening
Listeninng
StoppingWithVTRunningAP
Stopping
MediaserverdRecoveringWithVTRunningAP
MediaserverdRecovering
unknown(%tu)
ClientPrepare
AudioRecorderRelease
VoiceTriggerRunning
VoiceTriggerStopped
VoiceTriggerSwitchAP2AOP
VoiceTriggerSwitchAOP2AP
APFirstPassTriggered
AOPFirstPassTriggered
SecondPassTriggered
SecondPassRejected
SelfTriggerDetected
RecordPendingTimeout
ClientStartRecording
ClientStopRecording
ClientReleaseRecordSession
RecordingDidStop
ListeningSucceed
MediaserverdCrashed
MediaserverdRestarted
kDidStartFailed
RecorderDisconnected
SiriEnabled
SiriDisabled
-[CSSpeechManager getEstimatedTTSVolume]
-[CSSpeechManager CSSiriEnabledMonitor:didReceiveEnabled:]_block_invoke
-[CSSpeechManager _createClearLoggingFileTimer]
-[CSSpeechManager _createClearLoggingFileTimer]_block_invoke
-[CSSpeechManager _startClearLoggingFilesTimer]
assetQueryQueue
T@"NSObject<OS_dispatch_queue>",&,N,V_assetQueryQueue
stateMachine
T@"CSStateMachine",&,N,V_stateMachine
audioBuffer
T@"CSAudioCircularBuffer",&,N,V_audioBuffer
clientController
T@"<CSSpeechManagerDelegate>",W,N,V_clientController
secondPassStartSampleCount
TI,N,V_secondPassStartSampleCount
lastVoiceTriggerEventInfo
T@"NSDictionary",&,N,V_lastVoiceTriggerEventInfo
T@"CSSmartSiriVolume",&,N,V_smartSiriVolume
activeAudioProcessors
T@"NSHashTable",&,N,V_activeAudioProcessors
continuousAudioProcessors
T@"NSHashTable",&,N,V_continuousAudioProcessors
lastForwardedSampleCount
TI,N,V_lastForwardedSampleCount
clientStartSampleCount
TI,N,V_clientStartSampleCount
listenPollingTimer
T@"NSObject<OS_dispatch_source>",&,N,V_listenPollingTimer
clearLoggingFileTimer
T@"NSObject<OS_dispatch_source>",&,N,V_clearLoggingFileTimer
listenPollingTimerCount
Ti,N,V_listenPollingTimerCount
clearLoggingFileTimerCount
Ti,N,V_clearLoggingFileTimerCount
pendingSetRecordModeToRecordingToken
T@"NSUUID",&,N,V_pendingSetRecordModeToRecordingToken
pendingSetRecordModeToRecordingCompletion
T@?,C,N,V_pendingSetRecordModeToRecordingCompletion
isSiriEnabled
TB,N,V_isSiriEnabled
deviceRoleIsStereo
TB,N,V_deviceRoleIsStereo
isAudioSessionActive
TB,N,V_isAudioSessionActive
audioSessionActivationDelay
Td,N,V_audioSessionActivationDelay
audioRecorder
T@"CSAudioRecorder",&,N,V_audioRecorder
queue
T@"NSObject<OS_dispatch_queue>",&,N,V_queue
Serial CSAssetManager queue
en-US
-[CSAssetManager init]
-[CSAssetManager init]_block_invoke
-[CSAssetManager CSLanguageCodeUpdateMonitor:didReceiveLanguageCodeChanged:]
currentLanguageCode
deque
triggerEndSampleCount
isTriggerEvent
totalSampleCount
triggerScore
-[CSVTUITrainingSession closeSessionWithStatus:successfully:]
-[CSVTUITrainingSession closeSessionWithStatus:successfully:complete:]_block_invoke
-[CSVTUITrainingSession suspendTraining]
-[CSVTUITrainingSession suspendTraining]_block_invoke
-[CSVTUITrainingSession resumeTraining]
-[CSVTUITrainingSession resumeTraining]_block_invoke
-[CSVTUITrainingSession setupPhraseSpotter]
-[CSVTUITrainingSession handleAudioInput:]_block_invoke_2
v8@?0@"NSDictionary"4
-[CSVTUITrainingSession handleAudioBufferForVTWithAudioInput:withDetectedBlock:]
-[CSVTUITrainingSession feedSpeechRecognitionTrailingSamplesWithCompletedBlock:]
-[CSVTUITrainingSession trimBeginingOfPCMBufferWithVoiceTriggerEventInfo:]
-[CSVTUITrainingSession audioSessionUnsupportedAudioRoute]
-[CSVTUITrainingSession didDetectBeginOfSpeech]
-[CSVTUITrainingSession didDetectEndOfSpeech:]
PHS explicit training utterance
[%ld] VTUISession Number:[%ld]
-[CSVTUITrainingSession startMasterTimerWithTimeout:]
-[CSVTUITrainingSession handleMasterTimeout:]
-[CSVTUITrainingSession stopMasterTimer]
-[CSVTUITrainingSession speechRecognitionTask:didHypothesizeTranscription:]
activeChannel
TI,N,V_activeChannel
T@"<CSKeywordAnalyzerNDEAPIScoreDelegate>",W,N,V_delegate
twoShotAudibleFeedbackDelay
CSSpeechRecordSettingsKey_AudioSessionActiveDelay
CSSpeechRecordSettingsKey_AudioSessionActiveReason
com.apple.corespeech.twoShotAudibleFeedback
-[CSSpeechController initializeRecordSessionWithContext:]
-[CSSpeechController prepareRecordWithSettings:error:]
-[CSSpeechController setCurrentContext:error:]
-[CSSpeechController preheat]
-[CSSpeechController prewarmAudioSession]
-[CSSpeechController resetAudioSession]
-[CSSpeechController reset]
-[CSSpeechController releaseAudioSession]
-[CSSpeechController releaseAudioSession:]
-[CSSpeechController startRecordingWithSettings:error:]
-[CSSpeechController startRecordingWithSettings:error:]_block_invoke
-[CSSpeechController stopRecording]
-[CSSpeechController recordRoute]
-[CSSpeechController recordDeviceInfo]
-[CSSpeechController speechManagerLPCMRecordBufferAvailable:chunk:]_block_invoke
-[CSSpeechController speechManagerRecordBufferAvailable:buffer:]_block_invoke
-[CSSpeechController speechManagerDidStartForwarding:successfully:error:]
-[CSSpeechController speechManagerDidStartForwarding:successfully:error:]_block_invoke
-[CSSpeechController speechManagerDidStopForwarding:forReason:]
-[CSSpeechController speechManagerDidStopForwarding:forReason:]_block_invoke
-[CSSpeechController speechManagerRecordHardwareConfigurationDidChange:toConfiguration:]
-[CSSpeechController speechManagerRecordHardwareConfigurationDidChange:toConfiguration:]_block_invoke
-[CSSpeechController speechManagerDetectedSystemVolumeChange:withVolume:forReason:]
-[CSSpeechController speechManagerDetectedSystemVolumeChange:withVolume:forReason:]_block_invoke
-[CSSpeechController speechManagerDidFinishAlertPlayback:ofType:error:]
-[CSSpeechController speechManagerDidFinishAlertPlayback:ofType:error:]_block_invoke
-[CSSpeechController speechManagerBeginRecordInterruption:]
-[CSSpeechController speechManagerBeginRecordInterruption:]_block_invoke
-[CSSpeechController speechManagerBeginRecordInterruption:withContext:]
-[CSSpeechController speechManagerBeginRecordInterruption:withContext:]_block_invoke
-[CSSpeechController speechManagerEndRecordInterruption:]
-[CSSpeechController speechManagerEndRecordInterruption:]_block_invoke
-[CSSpeechController speechManager:willSetAudioSessionActive:]
-[CSSpeechController speechManager:willSetAudioSessionActive:]_block_invoke
-[CSSpeechController speechManager:didSetAudioSessionActive:]
-[CSSpeechController speechManager:didSetAudioSessionActive:]_block_invoke
-[CSSpeechController audioConverterDidConvertPackets:packets:timestamp:]
-[CSSpeechController setAlertSoundFromURL:forType:]
-[CSSpeechController playAlertSoundForType:]
-[CSSpeechController playRecordStartingAlertAndResetEndpointer]
-[CSSpeechController setMeteringEnabled:]
-[CSSpeechController outputReferenceChannel]
-[CSSpeechController voiceTriggerInfo]
-[CSSpeechController voiceTriggerDidDetectTwoShotAtTime:]_block_invoke
-[CSSpeechController keywordDetectorDidDetectKeyword]_block_invoke
Accounts
Speech Identifier
%c%c%c%c
none
-[CSSpeechController beginWaitingForMyriad]
-[CSSpeechController endWaitingForMyriadWithDecision:]
-[CSSpeechController CSMediaPlayingMonitor:didReceiveMediaPlayingChanged:]_block_invoke
-[CSSpeechController CSAlarmMonitor:didReceiveAlarmChanged:]_block_invoke
-[CSSpeechController CSTimerMonitor:didReceiveTimerChanged:]_block_invoke
-[CSSpeechController _setSoundPlayingState]
 NOT
-[CSSpeechController speakerRecognizer:hasSpeakerIdInfo:]
-[CSSpeechController speakerRecognizerFinishedProcessing:withFinalSpeakerIdInfo:]
endpointerProxy
T@"CSEndpointerProxy",&,N,V_endpointerProxy
speechManager
T@"CSSpeechManager",W,N,V_speechManager
avvcContext
T@"NSDictionary",&,N,V_avvcContext
isOpus
TB,N,V_isOpus
isActivated
TB,N,V_isActivated
isNarrowBand
TB,N,V_isNarrowBand
audioFileWriter
T@"CSPlainAudioFileWriter",&,N,V_audioFileWriter
spIdFactory
T@"CSSpeakerIdRecognizerFactory",&,N,V_spIdFactory
spIdRecognizer
T@"<CSSpIdSpeakerRecognizer>",&,N,V_spIdRecognizer
spIdUserScores
T@"NSDictionary",&,N,V_spIdUserScores
twoShotNotificationEnabled
TB,N,V_twoShotNotificationEnabled
TB,N,V_isMediaPlaying
TB,N,V_isAlarmPlaying
TB,N,V_isTimerPlaying
isSoundPlaying
TB,N,V_isSoundPlaying
myriadPreventingTwoShotFeedback
TB,N,V_myriadPreventingTwoShotFeedback
T@"<CSSpeechControllerDelegate>",W,N,V_delegate
duckOthersOption
TB,N
endpointAnalyzer
T@"<CSEndpointAnalyzer>",R,N
-[CSAsset(RTModel) RTModel]
%02x
-[CSVTUITrainingSessionWithPayload _firedVoiceTriggerTimeout]
-[CSVTUITrainingSessionWithPayload _firedEndPointTimeout]
-[CSVTUITrainingSessionWithPayload audioSessionDidStartRecording:error:]
-[CSVTUITrainingSessionWithPayload audioSessionDidStopRecording:]
-[CSVTUITrainingSessionWithPayload didDetectBeginOfSpeech]
-[CSVTUITrainingSessionWithPayload didDetectEndOfSpeech:]
-[CSVTUITrainingSessionWithPayload speechRecognitionTask:didHypothesizeTranscription:]_block_invoke
-[CSVTUITrainingSessionWithPayload speechRecognitionTask:didFinishRecognition:]_block_invoke
-[CSVTUITrainingSessionWithPayload speechRecognitionTask:didFinishSuccessfully:]_block_invoke
-[CSVTUITrainingSessionWithPayload matchRecognitionResult:withMatchedBlock:withNonMatchedBlock:]
com.apple.voicetrigger
com.apple.voicetrigger.notbackedup
VoiceTrigger CoreSpeech Enabled
Enable Two Shot Notification
com.apple.demo-settings
StoreDemoMode
File Logging Level
Library
Logs/CrashReporter/VoiceTrigger/audio/
/Logs/CrashReporter/Assistant/
SpeechLogs
-[CSPreferences assistantAudioFileLogDirectory]
Second Pass Audio Logging Enabled
VoiceTrigger/SAT
Caches/VoiceTrigger/SATUpdate
Caches/VoiceTrigger/SATUpload
-[CSPreferences getUserVoiceProfileUploadPathWithEnrolledLanguageList:]
json
-[CSPreferences notifyUserVoiceProfileUploadComplete]
-[CSPreferences getUserVoiceProfileUpdateDirectory]
-[CSPreferences notifyUserVoiceProfileUpdateReady]
Enable VoiceTrigger Upon VoiceProfile Sync For Language
enrollment_completed
enrollment_migrated
-[CSPreferences _markSATEnrollmentWithMarker:forLanguage:]
+[CSPreferences isCurrentDeviceCompatibleWithVoiceProfileAt:]
pathExtension='json'
kCSDeviceCategory_Unknown
kCSDeviceCategory_iOS_NonAop
kCSDeviceCategory_iOS_Aop
kCSDeviceCategory_macOS
iPad3,4
iPad3,5
iPad3,6
iPad4,1
iPad4,2
iPad4,3
iPad4,4
iPad4,5
iPad4,6
iPad4,7
iPad4,8
iPad4,9
iPad5,1
iPad5,2
iPad5,3
iPad5,4
iPad6,7
iPad6,8
iPad6,11
iPad6,12
iPhone5,1
iPhone5,2
iPhone5,3
iPhone5,4
iPhone6,1
iPhone6,2
iPhone7,1
iPhone7,2
iPad
iPhone
+[CSPreferences _deviceCategoryForDeviceProductType:]
Remote VoiceTrigger Delay
Remote VoiceTrigger Endpoint Timeout
VoiceTrigger/interstitial
Myriad File Logging Enabled
-[CSPreferences enableAudioInjection:]
Audio Injection Enabled
-[CSPreferences setAudioInjectionFilePath:]
Audio Injection File Path
-[CSPreferences audioInjectionFilePath]
-[CSPreferences audioInjectionFilePath]_block_invoke
v16@?0@4I8^B12
SpeakerId Enabled
SmartSiriVolume SoftVolume Enabled
Audio Session Activation Delay
Max Number Logging Files
Enable SiriActivation HomePod
CSSampleCountHostTimeConverter
anchorSampleCount
TQ,N,V_anchorSampleCount
anchorHostTime
TQ,N,V_anchorHostTime
initialState
Ti,N,V_initialState
transitions
T@"NSMutableDictionary",&,N,V_transitions
T@"<CSStateMachineDelegate>",W,N,V_delegate
currentState
Ti,R,N,V_currentState
-[CSPlainAudioFileWriter initWithURL:inputFormat:outputFormat:]
-[CSPlainAudioFileWriter addSamples:numSamples:]
fileURL
T@"NSURL",R,N,V_fileURL
Serial CSEventMonitor queue
/BuildRoot/Library/Caches/com.apple.xbs/Sources/CoreSpeech_Sim/CoreSpeech-140.13.76/CoreSpeech/CSEventMonitor.m
Subclasses need to overwrite this method
-[CSAudioFileLog _closeAudioFile]
-[CSAudioFileLog startRecording]_block_invoke
-input.wav
-[CSAudioFileLog appendAudioData:]_block_invoke
-[CSAudioFileLog stopRecording]_block_invoke
-[CSSpeechManager(Alert) setAlertSoundFromURL:forType:]
-[CSSpeechManager(Alert) playAlertSoundForType:]
-[CSSpeechManager(Alert) playRecordStartingAlertAndResetEndpointer]
[%@]
[%llu]
[%f]
BuiltInAOPVoiceTrigger
TestSetAPMode
TestSetAOPMode
Unknown
UUID
T@"NSString",R,N,V_UUID
type
TI,N,V_type
deviceId
T@"NSString",&,N,V_deviceId
activationInfo
T@"NSDictionary",&,N,V_activationInfo
hosttime
TQ,N,V_hosttime
vadScore
Tf,N,V_vadScore
localizedDescription
com.apple.MobileAsset.VoiceTriggerAssets
com.apple.MobileAsset.VoiceTriggerAssetsWatch
com.apple.MobileAsset.VoiceTriggerAssetsMarsh
com.apple.MobileAsset.VoiceTriggerAssetsMac
com.apple.MobileAsset.SpeechEndpointAssets
Serial CSAssetController queue
-[CSAssetController assetOfType:language:]
-[CSAssetController assetOfType:language:completion:]
-[CSAssetController installedAssetOfType:language:]
-[CSAssetController installedAssetOfType:language:completion:]
v12@?0@"ASAsset"4@"NSError"8
-[CSAssetController _installedAssetOfType:withPredicate:]
-[CSAssetController _installedAssetOfType:withPredicate:completion:]_block_invoke
v12@?0@"NSArray"4@"NSError"8
-[CSAssetController _assetQueryForAssetType:withPredicate:localOnly:]
-[CSAssetController _runAssetQuery:completion:]
-[CSAssetController _runAssetQuery:completion:]_block_invoke
-[CSAssetController fetchRemoteMetaOfType:]
-[CSAssetController _fetchRemoteAssetOfType:withPredicate:localOnly:completion:]
-[CSAssetController _updateFromRemoteToLocalAssets:forAssetType:completion:]
-[CSAssetController _defaultDownloadOptions]
-[CSAssetController _downloadAsset:withComplete:]
v12@?0d4
-[CSAssetController _downloadAsset:withComplete:]_block_invoke
v8@?0@"NSError"4
-[CSAssetController _startDownloadingAsset:progress:completion:]
v12@?0@"NSDictionary"4@"NSError"8
-[CSAssetController _startDownloadingAsset:progress:completion:]_block_invoke
+[CSAssetController(Utils) predicateForAssetType:language:]
(%@ == %K)
(%@ IN %K)
((%K == nil) OR (%K != %@))
 && 
Unknown InvocationStyle: %lu
tdti
Unknown CSSpIdType: %ld
+[CSUtils(SpeakerId) spIdTypeForString:]
EnrollmentRunMode
DetectionRunMode
RetrainingMode
/BuildRoot/Library/Caches/com.apple.xbs/Sources/CoreSpeech_Sim/CoreSpeech-140.13.76/CoreSpeech/CSUtils+SpeakerId.m
Unknown RunMode: %ld
config_td_spid.txt
config_ti_spid.txt
config_tdti_spid.txt
+[CSUtils(SpeakerId) spIdSATDirForLocale:]
%@/spid
spid-imported
+[CSUtils(SpeakerId) createDirectoryIfDoesNotExist:]
Logs/CoreSpeech/spid/
grading
+[CSUtils(SpeakerId) spIdAudioLogsCountLimitReached]
SiriDebugVT
SpeakerIDToGradeData
VoiceProfiles
trained_users.json
VoiceProfileCache
AudioFileOpenURL Failed : %@
EARTests
AudioFileOpenURL failed: %@
ExtAudioFileWrapAudioFileID failed: %@
Error reading audio-file: %d
EOF. Num bytes read: %lu
+[CSUtils(SpeakerId) isSpidAssetsAvailable]
T@"<CSAudioDecoderDelegate>",W,V_delegate
CSAudioRouteChangeMonitor queue
-[CSAudioRouteChangeMonitor _stopMonitoring]
CSSmartSiriVolumeEnablePolicy queue
-[CSSmartSiriVolumeEnablePolicy _addSmartSiriVolumeEnabledConditions]_block_invoke
VoiceTrigger Asset Change Monitor
T@"<CSVoiceTriggerAssetChangeDelegate>",W,N,V_delegate
com.apple.corespeech.voicetriggerassetchange
-[CSSpeechEndpointAssetMetaUpdateMonitor _startMonitoringWithQueue:]
-[CSSpeechEndpointAssetMetaUpdateMonitor _stopMonitoring]
-[CSSpeechEndpointAssetMetaUpdateMonitor _didReceiveNewSpeechEndpointAssetMetaData]
com.apple.MobileAsset.SpeechEndpointAssets.cached-metadata-updated
Testing [%@] against regex.
-[CSEncryptedAudioFileWriter endAudio]
writeBuffer
T@"NSMutableData",&,N,V_writeBuffer
-[CSDispatchGroup leave]
regex.json
Cannot parse to JSON
Cannot find the file
trailing_garbage
leading_garbage
regex
CSP2P_CommandType_Key
CSP2P_CommandDict_Key
corespeech
com.apple.siridebug.request.generic
com.apple.siridebug.command.remote.heysiri
com.apple.siridebug.command.parallel.recording
com.apple.siridebug.command.transfer.voiceprofile
com.apple.siridebug.command.fetch.parallelrecording
com.apple.siridebug.command.transfer.parallelrecording
com.apple.siridebug.command.fetch.voicegradingdata
com.apple.siridebug.command.transfer.voicegradingdata
com.apple.siridebug.command.delete.voiceprofile
CSP2P_RemoteHeySiriEnable_Key
CSP2P_RemoteHeySiriStatus_Key
CSP2P_RemoteRecordingStart_Key
CSP2P_RemoteRecordingStatus_Key
CSP2P_VoiceProfileData_Key
CSP2P_VoiceProfileFileName_Key
CSP2P_VoiceProfileSpeakerName_Key
CSP2P_VoiceProfileLocale_Key
CSP2P_VoiceProfileDataType_Key
CSP2P_VoiceProfileSegment_Key
CSP2P_VoiceProfileTotalSegments_Key
CSP2P_VoiceProfileStatus_Key
CSP2P_VoiceProfileProfileId_Key
CSP2P_VoiceProfileRecordedData_Key
CSP2P_VoiceProfileRemoteFileName_Key
CSP2P_VoiceDataToBeGraded_Key
CSP2P_VoiceFileNameToBeGraded_Key
CSP2P_GradingDataTransferStatus_Key
CSP2P_PeerIdentifier_Key
CSP2P_VoiceProfilePeerName_Key
CSP2P_IsDataCompressed_Key
com.apple.corespeech.p2psvc
-[CSP2PService processRemoteCommandWithPayload:fromPeer:withReply:]_block_invoke
CoreSpeech
-[CSP2PService sendInfo1ToNearbyPeer]_block_invoke
-[CSP2PService sendInfo2ToNearbyPeer]_block_invoke
-[CSP2PService _processRemoteHeySiriCommandWithRequest:fromSenderID:withReply:]
-[CSP2PService _processParallelRecordingCommandWithRequest:fromSenderID:withReply:]
-[CSP2PService _compressFilesInDirectory:matchingPredicate:compressedFileAvailable:]
/BuildRoot/Library/Caches/com.apple.xbs/Sources/CoreSpeech_Sim/CoreSpeech-140.13.76/CoreSpeech/CSP2PService.m
Invalid parameter not satisfying: %@
peerId
-[CSP2PService _sendParallelRecordingsToPeerId:voiceProfileRequestInfo:withReply:]
-[CSP2PService _sendParallelRecordingsToPeerId:voiceProfileRequestInfo:withReply:]_block_invoke
v24@?0@"NSString"4@"NSData"8L12B16@"NSError"20
-synced
-almost
-[CSP2PService _sendData2ToPeerId:]_block_invoke_2
.wav
-detected.wav
-[CSP2PService _sendData1ToPeerId:]_block_invoke_2
-rejected.wav
-[CSP2PService _sendData1ToPeerId:]_block_invoke
-[CSP2PService _sendData1File:withFileName:toPeerId:withCompressedFlag:]
fileData
fileName
unknown
-[CSP2PService _sendData1File:withFileName:toPeerId:withCompressedFlag:]_block_invoke
-[CSP2PService _receiveParallelRecordingFromPeerId:recordingInfo:withReply:]
remote
%@_%@
Ignoring sync of existing file %@ from %@
-[CSP2PService _receiveData1FromPeerId:requestInfo:withReply:]
%@.%@
-[CSP2PService _receiveVoiceProfileFromPeerId:voiceProfileInfo:withReply:]
CoreSpeechCache
-[CSP2PService _receiveVoiceProfileFromPeerId:voiceProfileInfo:withReply:]_block_invoke
-[CSP2PService _processVoiceProfileDeleteCommandWithRequest:fromSenderID:withReply:]_block_invoke
-[CSP2PService _processDataFetchCommandWithRequest:fromSenderID:withReply:]
lastCommunicatedPeer
T@"NSString",&,N,V_lastCommunicatedPeer
adCompanionServiceProvider
T@"<CSADCompanionServiceProvider>",W,N,V_adCompanionServiceProvider
com.apple.corespeech
InternalBuild
CSSafeSetOutErrorWithNSError
4D8XW4YwJI7QvyPhv1TEdw
-[CSKeywordAnalyzerNDAPI initWithConfigPath:resourcePath:]
-[CSKeywordAnalyzerNDAPI _setStartAnalyzeTime:]
samples_at_fire
threshold_normal
-[CSKeywordAnalyzerNDAPI getThreshold]
threshold_logging
-[CSKeywordAnalyzerNDAPI getLoggingThreshold]
activePhraseId
TI,N,V_activePhraseId
T@"<CSKeywordAnalyzerNDAPIScoreDelegate>",W,N,V_delegate
Framework
Logs/CrashReporter/CoreSpeech/
Logs/CrashReporter/CoreSpeech/audio/
%@/%@%@%@
_CSGetOrCreateAudioLogDirectory
/tmp
totalAudioRecorded
Td,N,V_totalAudioRecorded
featuresAtEndpoint
T@"NSArray",&,N,V_featuresAtEndpoint
endpointerType
Ti,N,V_endpointerType
serverFeatureLatencyDistribution
T@"NSDictionary",&,N,V_serverFeatureLatencyDistribution
additionalMetrics
T@"NSDictionary",&,N,V_additionalMetrics
CSSACInfoMonitor queue
-[CSSACInfoMonitor _startMonitoringWithQueue:]
-[CSSACInfoMonitor _stopMonitoring]
-[CSSACInfoMonitor isDeviceRoleStereo]
CSRemoteControlClient
-[CSRemoteControlClient dealloc]
T@"<CSRemoteControlClientDelegate>",W,N,V_delegate
Languages
Footprint
Premium
RTModelData
RTModelHash
RTModelLocale
RTModelDigest
RTModelSignature
RTModelCertificate
RT Model for 
 from asset 
CorealisRTModel
CorealisRTModelVersion
dataSize(%d), hash(%@), locale(%@), digest(%@), cert(%@), signature(%@)
supportsSecureCoding
TB,R
modelData
T@"NSData",R,N,V_modelData
modelLocale
T@"NSString",R,N,V_modelLocale
modelHash
T@"NSString",R,N,V_modelHash
digest
T@"NSData",R,N,V_digest
signature
T@"NSData",R,N,V_signature
certificate
T@"NSData",R,N,V_certificate
com.apple.coreaudio.BorealisToggled
-[CSVoiceTriggerEnabledMonitor _startMonitoringWithQueue:]_block_invoke
-[CSVoiceTriggerEnabledMonitor _startMonitoringWithQueue:]
-[CSVoiceTriggerEnabledMonitor _stopMonitoring]
-[CSVoiceTriggerEnabledMonitor _checkVoiceTriggerEnabled]
CSRemoteRecordClient Queue
T@"<CSRemoteRecordClientDelegate>",W,N,V_delegate
Borealis Input
com.apple.VoiceTriggerUI.RemoteRecordSessionQueue
-[CSVTUIAudioSessionRemote _audioRecorder]
-[CSVTUIAudioSessionRemote prepareRecord]
T@"<CSVTUIAudioSessionDelegate>",W,N,V_delegate
-[CSSiriEnabledMonitor _startMonitoringWithQueue:]
Enabled
Disabled
-[CSSiriEnabledMonitor _stopMonitoring]
_AssistantPrefsChangedNotification
beepLocation
statsComputed
beepPower
signalPower
originalPower
absMaxVal
above95pcOfMax
totalInputSamples
totalOutputSamples
jbl_begin.bin
-[CSBeepCanceller init]
-[CSBeepCanceller willBeep]
-[CSBeepCanceller reset]
T@"<CSBeepCancellerDelegate>",W,N,V_delegate
metrics
[UniqueUttTag: %@, InvocationStyle:(%lu)%@, Asset: %@, vtEventInfo: %@]
%@_%@_%@.wav
%@_%@_%@.json
invocationStyle
TI,N,V_invocationStyle
asset
T@"CSAsset",&,N,V_asset
vtEventInfo
T@"NSDictionary",&,N,V_vtEventInfo
uniqueUttTag
T@"NSString",&,N,V_uniqueUttTag
locale
T@"NSString",R,N,V_locale
Failed to create rootless dir at path: %@, status: %d, errno: %d, err: %s
CoreSpeechRootless
-[NSFileManager(Rootless) convertToRootlessDirectoryAtPath:error:]
Failed to convert path: %@ to rootless, status: %d, errno: %d, err: %s
CSActivationEventNotifier
-[CSActivationEventNotifier notifyActivationEvent:completion:]_block_invoke
-[CSActivationEventNotifier notifyActivationEvent:deviceId:activationInfo:completion:]_block_invoke
-[CSActivationEventNotifier setDelegate:for:]_block_invoke
-[CSActivationEventNotifier _didReceiveAOPFirstPassTrigger:completion:]
-[CSActivationEventNotifier _didReceiveAOPFirstPassTrigger:completion:]_block_invoke
-[CSActivationEventNotifier receiveTestNotificationAPMode]
-[CSActivationEventNotifier receiveTestNotificationAOPMode]
notifyToken
Ti,N,V_notifyToken
delegates
T@"NSMapTable",&,N,V_delegates
pendingActivationEvent
T@"CSActivationEvent",&,N,V_pendingActivationEvent
pendingCompletion
T@?,C,N,V_pendingCompletion
CSInitialContinousZeros
CSMaxContinousZeros
CSMidSegmentContinousZeros
start
+[CSUtils(AudioFile) readAudioChunksFrom:block:]
-[CSAssetManagerEnablePolicy _addAssetManagerEnabledConditions]_block_invoke
-[CSAudioCircularBuffer initWithNumChannels:recordingDuration:samplingRate:]
-[CSAudioCircularBuffer copySamplesFromHostTime:]
-[CSAudioCircularBuffer copySamplesFrom:to:]
-[CSAudioCircularBuffer copyBufferWithNumSamplesCopiedIn:]
-[CSAudioCircularBuffer reset]
-[CSAudioCircularBuffer saveRecordingBufferFrom:to:toURL:]
bufferLength
TI,N,V_bufferLength
copySamples
  mNumChannels: 
  mRecordingDurationInSecs: 
  mSampleRate: 
  mBytesPerSample: 
  mBufferLengthInSamples: 
  mNextWritePos: 
  mSamplesCount: 
  mMemoryPool(
): [
    chan-
: sz=
: mem-sz: 
-[CSVoiceTriggerAssetMetaUpdateMonitor _startMonitoringWithQueue:]
-[CSVoiceTriggerAssetMetaUpdateMonitor _stopMonitoring]
-[CSVoiceTriggerAssetMetaUpdateMonitor _didReceiveNewVoiceTriggerAssetMetaData]
com.apple.MobileAsset.VoiceTriggerAssets.cached-metadata-updated
com.apple.MobileAsset.VoiceTriggerAssetsWatch.cached-metadata-updated
com.apple.MobileAsset.VoiceTriggerAssetsMarsh.cached-metadata-updated
com.apple.MobileAsset.VoiceTriggerAssetsMac.cached-metadata-updated
{wordCount: %ld, trailingSilDuration: %ld, eosLikelihood: %f, pauseCounts: %@, silencePosterior: %f, taskName: %@, processedAudioDurationInMilliseconds: %ld}
wordCount
Ti,N,V_wordCount
trailingSilenceDuration
Ti,N,V_trailingSilenceDuration
eosLikelihood
Td,N,V_eosLikelihood
pauseCounts
T@"NSArray",C,N,V_pauseCounts
silencePosterior
Td,N,V_silencePosterior
processedAudioDurationInMilliseconds
Ti,N,V_processedAudioDurationInMilliseconds
taskName
T@"NSString",C,N,V_taskName
com.apple.cs.%@.apQueue
-[CSVAD2EndpointAnalyzer preheat]
-[CSVAD2EndpointAnalyzer resetForNewRequestWithSampleRate:]
-[CSVAD2EndpointAnalyzer reset]
-[CSVAD2EndpointAnalyzer _resetWithSampleRate:]
triggerEndSeconds
-[CSVAD2EndpointAnalyzer handleVoiceTriggerWithActivationInfo:]_block_invoke
-[CSVAD2EndpointAnalyzer _processAudioSamples:]
endpointStyle
Ti,N
delay
Td,N
startWaitTime
automaticEndpointingSuspensionEndTime
minimumDurationForEndpointer
lastEndOfVoiceActivityTime
Td,R,N
lastStartOfVoiceActivityTime
bypassSamples
endpointMode
interspeechWaitTime
endWaitTime
saveSamplesSeenInReset
T@"<CSEndpointAnalyzerDelegate>",W,N
canProcessCurrentRequest
TI,N
endpointerModelVersion
elapsedTimeWithNoSpeech
sampleRate
Td,N,V_sampleRate
frameRate
TL,N,V_frameRate
detectedOneShotStartpoint
TB,N,V_detectedOneShotStartpoint
detectedRecurrentStartpoint
TB,N,V_detectedRecurrentStartpoint
communicatedStartPointDetection
TB,N,V_communicatedStartPointDetection
detectedOneShotEndpoint
TB,N,V_detectedOneShotEndpoint
detectedRecurrentEndpoint
TB,N,V_detectedRecurrentEndpoint
communicatedEndpointDetection
TB,N,V_communicatedEndpointDetection
samplesSeen
Td,N,V_samplesSeen
numSamplesProcessed
Td,N,V_numSamplesProcessed
lastOneShotStartpoint
Td,N,V_lastOneShotStartpoint
lastOneShotEndpoint
Td,N,V_lastOneShotEndpoint
lastRecurrentStartpoint
Td,N,V_lastRecurrentStartpoint
lastRecurrentEndpoint
Td,N,V_lastRecurrentEndpoint
floatSampleBuffer
T@"NSMutableData",&,N,V_floatSampleBuffer
topLevelParameterDict
T@"NSDictionary",&,N,V_topLevelParameterDict
modelDictPath
T@"NSString",&,N,V_modelDictPath
isConfigured
TB,N,V_isConfigured
previousSamplesSeen
Td,N,V_previousSamplesSeen
apQueue
T@"NSObject<OS_dispatch_queue>",&,N,V_apQueue
recordingDidStop
TB,N,V_recordingDidStop
vtEndInSampleCount
TI,N,V_vtEndInSampleCount
Ti,N,V_endpointStyle
Td,N,V_delay
Td,N,V_startWaitTime
Td,N,V_automaticEndpointingSuspensionEndTime
Td,N,V_minimumDurationForEndpointer
Td,N,V_bypassSamples
Ti,N,V_endpointMode
Td,N,V_interspeechWaitTime
Td,N,V_endWaitTime
TB,N,V_saveSamplesSeenInReset
T@"<CSEndpointAnalyzerDelegate>",W,N,V_delegate
-[CSVAD2EndpointAnalyzer(private) _configureWithSampleRate:andFrameRate:]
kAUEndpointVAD2Property_EDLStartWaitTimeSec
kAUEndpointVAD2Property_EDLInterspeechWaitTimeSec
kAUEndpointVAD2Property_EDLSpeechStartAdjustSec
kAUEndpointVAD2Property_EDLSpeechEndAdjustSec
kAUEndpointVAD2Property_EDLWindowLengthSeconds
kAUEndpointVAD2Property_EDLSpeechFraction
kAUEndpointVAD2Property_EDLNonspeechFraction
kAUEndpointVAD2Property_IsRealtimeOperationMode
kAUEndpointVAD2Property_DecoderLatencySeconds
kAudioUnitProperty_MaximumFramesPerSlice
-[CSVAD2EndpointAnalyzer(private) _detectVoiceActivityInSamples:numSamples:]
Library/Audio/Tunings/Generic/AU/
EndpointerModelPathForStyle
aufx-epv2-bluetooth8khz-appl.plist
aufx-epv2-appl.plist
EndpointerSpeechBeginListener
EndpointerSpeechEndListener
RecurrentVADSpeechBeginListener
RecurrentVADSpeechEndListener
-[NSArray(XPCObject) _cs_initWithXPCObject:]
-[NSArray(XPCObject) _cs_initWithXPCObject:]_block_invoke
B12@?0L4@"NSObject<OS_xpc_object>"8
-[NSArray(XPCObject) _cs_xpcObject]_block_invoke
-[CSAudioChunk subChunkFrom:numSamples:forChannel:]
-[CSAudioChunk subChunkFrom:numSamples:]
-[CSAudioChunk skipSamplesAtStartSuchThatNumSamplesReceivedSoFar:reachesACountOf:completionHandler:]
-[CSAudioChunk splitAudioChunkSuchThatNumSamplesReceivedSoFar:reachesACountOf:completionHandler:]
data
T@"NSData",R,N,V_data
numChannels
TI,R,N,V_numChannels
numSamples
TI,R,N,V_numSamples
TI,R,N,V_sampleByteDepth
startSampleCount
TQ,R,N,V_startSampleCount
hostTime
TQ,R,N,V_hostTime
samples_fed
best_phrase
best_start
best_end
best_score
early_warning
is_rescoring
sampleFed
TI,N,V_sampleFed
bestPhrase
TI,N,V_bestPhrase
bestStart
TI,N,V_bestStart
bestEnd
TI,N,V_bestEnd
bestScore
Tf,N,V_bestScore
earlyWarning
TB,N,V_earlyWarning
isRescoring
TB,N,V_isRescoring
dictionary
-[CSHybridEndpointAnalyzer processAudioSamplesAsynchronously:]
extraSamplesAtStart
-[CSHybridEndpointAnalyzer handleVoiceTriggerWithActivationInfo:]_block_invoke
-[CSHybridEndpointAnalyzer recordingStoppedForReason:]
/BuildRoot/Library/Caches/com.apple.xbs/Sources/CoreSpeech_Sim/CoreSpeech-140.13.76/CoreSpeech/CSHybridEndpointAnalyzer.m
CSHybridEndpointAnalyzer reset called
-[CSHybridEndpointAnalyzer CSLanguageCodeUpdateMonitor:didReceiveLanguageCodeChanged:]
-[CSHybridEndpointAnalyzer CSAssetManagerDidDownloadNewAsset:]
cs_hep_marsh.json
cs_hep.json
-[CSHybridEndpointAnalyzer _getCSHybridEndpointerConfigForAsset:]
currentAsset
T@"CSAsset",&,N,V_currentAsset
serverFeaturesQueue
T@"NSObject<OS_dispatch_queue>",&,N,V_serverFeaturesQueue
lastKnownServerEPFeatures
T@"CSServerEndpointFeatures",&,N,V_lastKnownServerEPFeatures
serverFeatureLatencies
T@"NSMutableArray",&,N,V_serverFeatureLatencies
serverFeaturesWarmupLatency
Td,N,V_serverFeaturesWarmupLatency
lastServerFeatureTimestamp
T@"NSDate",&,N,V_lastServerFeatureTimestamp
didReceiveServerFeatures
TB,N,V_didReceiveServerFeatures
clientLagThresholdMs
Td,N,V_clientLagThresholdMs
clampedSFLatencyMsForClientLag
Td,N,V_clampedSFLatencyMsForClientLag
useDefaultServerFeaturesOnClientLag
TB,N,V_useDefaultServerFeaturesOnClientLag
hybridClassifierQueue
T@"NSObject<OS_dispatch_queue>",&,N,V_hybridClassifierQueue
lastReportedEndpointTimeMs
Td,N,V_lastReportedEndpointTimeMs
stateSerialQueue
T@"NSObject<OS_dispatch_queue>",&,N,V_stateSerialQueue
didCommunicateEndpoint
TB,N,V_didCommunicateEndpoint
currentRequestSampleRate
TI,N,V_currentRequestSampleRate
vtExtraAudioAtStartInMs
Td,N,V_vtExtraAudioAtStartInMs
hepAudioOriginInMs
Td,N,V_hepAudioOriginInMs
firstAudioPacketTimestamp
T@"NSDate",&,N,V_firstAudioPacketTimestamp
didTimestampFirstAudioPacket
TB,N,V_didTimestampFirstAudioPacket
silencePosteriorGeneratorQueue
T@"NSObject<OS_dispatch_queue>",&,N,V_silencePosteriorGeneratorQueue
didDetectSpeech
TB,N,V_didDetectSpeech
Td,N,V_elapsedTimeWithNoSpeech
TB,R,N,V_canProcessCurrentRequest
+[CSUtils(Time) hostTimeFromSampleCount:anchorHostTime:anchorSampleCount:]
+[CSUtils(Time) sampleCountFromHostTime:anchorHostTime:anchorSampleCount:]
+[CSUtils(Time) macHostTimeFromBridgeHostTime:]
trainingType
explicit
implicit
utteranceWav
+[CSVoiceTriggerEnrollmentDataManager saveRawUtteranceAndMetadata:to:isExplicitEnrollment:]
yyyyMMdd-HHmmss
+[CSVoiceTriggerEnrollmentDataManager saveUtteranceAndMetadata:atDirectory:isExplicitEnrollment:]
+[CSVoiceTriggerEnrollmentDataManager saveUtterance:utteranceAudioPath:numSamplesToWrite:isExplicitEnrollment:]
+[CSVoiceTriggerEnrollmentDataManager saveMetadata:isExplicitEnrollment:]
+[CSVoiceTriggerEnrollmentDataManager _getBaseMetaDictionaryForUtterancePath:]
+[CSVoiceTriggerEnrollmentDataManager writeMetaDict:atMetaPath:]
-[CSEndpointerProxy _setupVAD2Endpointer]
-[CSEndpointerProxy resetForNewRequestWithSampleRate:]
-[CSEndpointerProxy resetForVoiceTriggerTwoShotWithSampleRate:]
-[CSEndpointerProxy endpointer:didDetectStartpointAtTime:]
-[CSEndpointerProxy endpointer:didDetectHardEndpointAtTime:withMetrics:]
-[CSEndpointerProxy _shouldEnterTwoShotAtEndPointTime:]
-[CSEndpointerProxy endpointerModelVersion]
hybridEndpointer
T@"<CSEndpointAnalyzerImpl>",&,N,V_hybridEndpointer
vad2Endpointer
T@"<CSEndpointAnalyzerImpl>",&,N,V_vad2Endpointer
activeEndpointer
T@"<CSEndpointAnalyzerImpl>",W,N,V_activeEndpointer
didEnterTwoshot
TB,N,V_didEnterTwoshot
vad2EndpointStyle
Ti,N,V_vad2EndpointStyle
vad2EndpointtMode
Ti,N,V_vad2EndpointtMode
vad2StartWaitTime
Td,N,V_vad2StartWaitTime
vad2EndWaitTime
Td,N,V_vad2EndWaitTime
vad2InterspeechWaitTime
Td,N,V_vad2InterspeechWaitTime
vad2Delay
Td,N,V_vad2Delay
vad2AutomaticEndpointingSuspensionEndTime
Td,N,V_vad2AutomaticEndpointingSuspensionEndTime
vad2MinimumDurationForEndpointer
Td,N,V_vad2MinimumDurationForEndpointer
vad2SaveSamplesSeenInReset
TB,N,V_vad2SaveSamplesSeenInReset
endpointerDelegate
T@"<CSEndpointAnalyzerDelegate>",W,N,V_endpointerDelegate
-[NSNumber(XPCObject) _cs_initWithXPCObject:]
-[NSNumber(XPCObject) _cs_xpcObject]
-[CSFirstUnlockMonitor _stopMonitoring]
userName
sysConfigRoot
satModelDir
satAudioDir
-[CSSpringboardStartMonitor _startMonitoringWithQueue:]
-[CSSpringboardStartMonitor _stopMonitoring]
-[CSSpringboardStartMonitor _checkSpringBoardStarted]
com.apple.springboard.finishedstartup
-[CSKeywordAnalyzerQuasar runRecognition]
-[CSKeywordAnalyzerQuasar endAudio]
triggerConfidence
Td,R,N,V_triggerConfidence
T@"<CSKeywordAnalyzerQuasarScoreDelegate>",W,N,V_delegate
-[CSNetworkAvailabilityMonitor _startMonitoringWithQueue:]
-[CSNetworkAvailabilityMonitor _stopMonitoring]
-[CSNetworkAvailabilityMonitor _availabilityChanged]
%@ {route = %@, isRemoteDevice = %d, remoteDeviceUID = %@, remoteDeviceProductIdentifier = %@}
route
isRemoteDevice
remoteDeviceUID
remoteDeviceProductIdentifier
T@"NSString",R,C,N,V_route
TB,R,N,V_isRemoteDevice
T@"NSUUID",R,C,N,V_remoteDeviceUID
T@"NSString",R,C,N,V_remoteDeviceProductIdentifier
corespeech.json
hybridendpointer.json
hybridendpointer_marsh.json
/BuildRoot/Library/Caches/com.apple.xbs/Sources/CoreSpeech_Sim/CoreSpeech-140.13.76/CoreSpeech/CSAsset.m
ERR: Unknown assetType: %lu
/System/Library/PrivateFrameworks/CoreSpeech.framework
+[CSAsset fallBackAssetResourcePath]
-[CSAsset initWithResourcePath:configFile:configVersion:]
-[CSAsset _decodeJson:]
-[CSAsset getNumberForKey:category:default:]
-[CSAsset getStringForKey:category:default:]
configVersion:%@ resourcePath:%@ path:%@
path
T@"NSString",R,N,V_path
resourcePath
T@"NSString",R,N,V_resourcePath
hashFromResourcePath
configVersion
T@"NSString",R,N,V_configVersion
-[CSAudioConverter _convertBufferedLPCM:allowPartial:timestamp:]
/BuildRoot/Library/Caches/com.apple.xbs/Sources/CoreSpeech_Sim/CoreSpeech-140.13.76/CoreSpeech/CSAudioConverter.m
Cannot produce ASPD for PCM
-[CSAudioConverter reset]
-[CSAudioConverter _configureAudioConverter:]
T@"<CSAudioConverterDelegate>",W,V_delegate
CreateAudioConverter
-[CSLanguageCodeUpdateMonitor _startMonitoringWithQueue:]
-[CSLanguageCodeUpdateMonitor _stopMonitoring]
-[CSLanguageCodeUpdateMonitor _didReceiveLanguageCodeUpdate]
Serial CSPolicy queue
-[NSDictionary(XPCObject) _cs_initWithXPCObject:]
-[NSDictionary(XPCObject) _cs_initWithXPCObject:]_block_invoke
B12@?0r*4@"NSObject<OS_xpc_object>"8
-[NSDictionary(XPCObject) _cs_xpcObject]_block_invoke
v16@?0@4@8^B12
-[CSSpeakerDetectorNDAPI _initializeSAT:]
-[CSSpeakerDetectorNDAPI processSuperVector:withResult:]
T@"<CSSpeakerDetectorNDAPIDelegate>",W,N,V_delegate
isMaximized
-[CSVTUIKeywordDetector initWithLanguageCode:]
config.txt
-[CSAudioRecorder willDestroy]
-[CSAudioRecorder _destroyVoiceController]
-[CSAudioRecorder _voiceControllerWithContext:error:]
-[CSAudioRecorder _beepCanceller]
-[CSAudioRecorder prepareRecordWithSettings:error:]
-[CSAudioRecorder setCurrentContext:error:]
-[CSAudioRecorder prewarmAudioSession]
-[CSAudioRecorder releaseAudioSession:]
-[CSAudioRecorder setDuckOthersOption:]
-[CSAudioRecorder enableMiniDucking:]
Enable
Disable
-[CSAudioRecorder _resetZeroFilter]
-[CSAudioRecorder _startRecordingForAudioInjection]
-[CSAudioRecorder startRecordingWithSettings:error:]
-[CSAudioRecorder startRecording:]
context
-[CSAudioRecorder startRecording]
-[CSAudioRecorder stopRecording]
-[CSAudioRecorder setRecordMode:error:]
-[CSAudioRecorder _recordingSampleRate]
Builtin Microphone
-[CSAudioRecorder voiceControllerRecordBufferAvailable:buffer:]
-[CSAudioRecorder playAlertSoundForType:]
ZeroFilterMetrics
-[CSAudioRecorder _audioRecorderDidStopRecordingForReason:]
BeepCancellerMetrics
-[CSAudioRecorder voiceControllerDidStartRecording:successfully:error:]
-[CSAudioRecorder voiceControllerDidStopRecording:forReason:]
-[CSAudioRecorder voiceControllerRecordHardwareConfigurationDidChange:toConfiguration:]
-[CSAudioRecorder voiceControllerDidFinishAlertPlayback:ofType:error:]
-[CSAudioRecorder voiceControllerBeginRecordInterruption:]
-[CSAudioRecorder voiceControllerBeginRecordInterruption:withContext:]
-[CSAudioRecorder voiceControllerEndRecordInterruption:]
-[CSAudioRecorder voiceControllerWillSetAudioSessionActive:willActivate:]
-[CSAudioRecorder voiceControllerDidSetAudioSessionActive:isActivated:]
-[CSAudioRecorder voiceControllerMediaServicesWereLost:]
-[CSAudioRecorder voiceControllerMediaServicesWereReset:]
-[CSAudioRecorder _deinterleaveBufferIfNeeded:]
-[CSAudioRecorder _createDeInterleaverIfNeeded]
-[CSAudioRecorder _createSampleRateConverterIfNeeded]
-[CSAudioRecorder _createAudioPowerMeterIfNeeded]
T@"<CSAudioRecorderDelegate>",W,N,V_delegate
kCCParamError
kCCBufferTooSmall
kCCMemoryFailure
kCCAlignmentError
kCCDecodeError
kCCUnimplemented
kCCOverflow
kCCRNGFailure
kCCUnspecifiedError
kCCCallSequenceError
kCCKeySizeError
Unexpected: %ld
randomBytesWithLength:%lu failed. err=%d
CSNSDataEncryptDecrypt
-[NSData(Encryption) saveEncryptedDataUsingAESKey:atFilepath:]_block_invoke
ivtag
v20@?0@"NSData"4@"NSData"8@"NSData"12@"NSError"16
+[NSData(Encryption) decryptedDataUsingAESKey:atFilepath:error:]
aesKey is nil
/BuildRoot/Library/Caches/com.apple.xbs/Sources/CoreSpeech_Sim/CoreSpeech-140.13.76/CoreSpeech/NSData+Encryption.m
AESKey(%lu) != %lu
CoreSpeechNSDataEncryption
CoreSpeechNSDataDecryption
-[NSData(XPCObject) _cs_initWithXPCObject:]
nohash
((?:[a-z]|[0-9])*)\.asset
+[CSUtils(ResourcePathHash) assetHashInResourcePath:]
alloc
initWithAppBundleIdentifier:
initWithTaskDeliverer:
initWithMessage:makeAppFrontmost:
handleSiriRequest:deliveryHandler:completionHandler:
launchSiriDebugAppWithMessage:
init
_notifyObserver:mediaIsPlayingState:
enumerateObserversInQueue:
_stopMonitoring
defaultCenter
_notePossiblePlayPausedStateChange:
addObserver:selector:name:object:
removeObserver:name:object:
userInfo
objectForKey:
boolValue
notifyObserver:
CSMediaPlayingMonitor:didReceiveMediaPlayingChanged:
respondsToSelector:
sharedInstance
initializeMediaPlayingState
_startMonitoringWithQueue:
mediaPlayingState
.cxx_destruct
_mediaIsPlaying
_queue
fetchVolumeFromAVSystemControllerForAudioCategory:
_startObservingSystemControllerLifecycle
startObservingSystemVolumes
removeObserver:
dealloc
isEqualToString:
musicVolume
alarmVolume
systemVolumeDidChange:
systemControllerDied:
_musicVolumeLevel
_alarmVolumeLevel
initializeTimerState
timerState
_timerFiringState
initializeAlarmState
alarmState
_alarmFiringState
assetManagerEnabledPolicy
zeroFilterWindowSizeInMs
shouldDeinterleaveAudioOnCS
bytes
initWithToken:sampleRate:numChannels:
getZeroStatisticsFromBuffer:entireSamples:
stopReportZeroStatistics
_methodToken
_continuousZeroCounter
_zeroCounterWinSz
_numChannels
_analyzeStep
_sampleRate
_shouldDeinterleaveAudio
numberWithUnsignedInteger:
getNumberForKey:category:default:
unsignedIntegerValue
numberWithUnsignedInt:
unsignedIntValue
numberWithFloat:
floatValue
SSVNoiseLevelChannelBitset
SSVLKFSChannelBitset
SSVEnergyBufferSize
SSVNoiseLowerPercentile
SSVNoiseUpperPercentile
SSVLKFSLowerPercentile
SSVLKFSUpperPercentile
SSVNoiseTimeConstant
SSVNoiseMicSensitivityOffset
SSVLKFSTimeConstant
SSVLKFSMicSensitivityOffset
SSVNoiseTTSMappingInputRangeLow
SSVNoiseTTSMappingInputRangeHigh
SSVNoiseTTSMappingOutputRangeLow
SSVNoiseTTSMappingOutputRangeHigh
SSVLKFSTTSMappingInputRangeLow
SSVLKFSTTSMappingInputRangeHigh
SSVLKFSTTSMappingOutputRangeLow
SSVLKFSTTSMappingOutputRangeHigh
SSVUserOffsetInputRangeLow
SSVUserOffsetInputRangeHigh
SSVUserOffsetOutputRangeLow
SSVUserOffsetOutputRangeHigh
SSVTTSVolumeLowerLimitDB
SSVTTSVolumeUpperLimitDB
SSVNoiseWeight
dictionaryWithObjects:forKeys:count:
SSVParameterDirectionary
_sharedAudioLoggingQueue
fileURL
URLByDeletingLastPathComponent
path
sharedPreferences
assistantAudioFileLogDirectory
containsString:
defaultManager
removeItemAtURL:error:
seekToEndOfFile
seekToFileOffset:
readDataOfLength:
length
writeData:
fileLoggingIsEnabled
_createAudioFileWriterWithLoggingDir:inputFormat:outputFormat:
_createTempAudioFileWriterWithInputFormat:outputFormat:
_getDateLabel
stringWithFormat:
stringByAppendingPathComponent:
fileURLWithPath:
initWithURL:inputFormat:outputFormat:
maxNumLoggingFiles
pruneNumberOfLogFilesTo:
URLWithString:
localeWithLocaleIdentifier:
setLocale:
setDateFormat:
stringFromDate:
removeLogFilesInDirectory:matchingPattern:beforeDays:
arrayWithObjects:
countByEnumeratingWithState:objects:count:
clearLogFilesInDirectory:matchingPattern:exceedNumber:
generateDeviceAudioLogging:numChannels:speechId:
_readDataFromFileHandle:toFileHandle:
createAudioFileWriterFromWithInputFormat:outputFormat:
removeLogFilesOlderThanNDays:
dictionaryWithDictionary:
objectForKeyedSubscript:
deviceProductType
deviceProductVersion
deviceBuildVersion
addEntriesFromDictionary:
stringByReplacingOccurrencesOfString:withString:
dataWithJSONObject:options:error:
stringWithUTF8String:
currentHandler
handleFailureInMethod:object:file:lineNumber:description:
writeToFile:atomically:
logSpeakerIdMetadataAtFilepath:additionalMetadata:
_notificationKey
_didInstalledNewVoiceTriggerAsset
_notifyObserver:
enumerateObservers:
CSVoiceTriggerAssetDownloadMonitor:didInstallNewAsset:
_notifyToken
dateWithTimeIntervalSinceNow:
distantFuture
getResourceValue:forKey:error:
localizedDescription
compare:
URLsInDirectory:matchingPattern:completion:
count
objectAtIndex:
_sortedURLsInDirectory:matchingPattern:completion:
_contentsOfDirectoryAtURL:matchingPattern:includingPropertiesForKeys:error:
arrayWithObjects:count:
sortedArrayUsingComparator:
regularExpressionWithPattern:options:error:
contentsOfDirectoryAtURL:includingPropertiesForKeys:options:error:
lastPathComponent
numberOfMatchesInString:options:range:
predicateWithBlock:
filteredArrayUsingPredicate:
class
description
UTF8String
_setDefaultParameters
_setAsset:
_convertDB2Mag:
getNumElementInBitset:
_reset
fetchInitSystemVolumes
_resumeSSVProcessing
_pauseSSVProcessing
setCallback:
isEnabled
addObserver:
_getMusicVolumeDB:
_resetStartAnalyzeTime
inputRecordingSampleByteDepth
convertToFloatLPCMBufFromShortLPCMBuf:
dataForChannel:
iterateBitset:block:
_prepareSoundLevelBufferFromSamples:soundType:
startSampleCount
_setStartAnalyzeTime:
numSamples
subChunkFrom:numSamples:
_processAudioChunk:soundType:
_estimatedTTSVolume:lowerLimit:upperLimit:TTSmappingInputRangeLow:TTSmappingInputRangeHigh:TTSmappingOutputRangeLow:TTSmappingOutputRangeHigh:
_combineResultsWithOptimalFromNoise:andOptimalFromLkfs:withUserOffset:
_scaleInputWithInRangeOutRange:minIn:maxIn:minOut:maxOut:
numberWithBool:
sharedAnalytics
logEventWithType:context:
smartSiriVolumeSoftVolumeEnabled
CSSmartSiriVolumeDidReceiveAlarmChanged:
CSSmartSiriVolumeDidReceiveTimerChanged:
CSSmartSiriVolumeDidReceiveMusicVolumeChanged:
isEqual:
self
performSelector:
performSelector:withObject:
performSelector:withObject:withObject:
isProxy
isKindOfClass:
isMemberOfClass:
conformsToProtocol:
retain
release
autorelease
retainCount
zone
hash
superclass
debugDescription
CSAlarmMonitor:didReceiveAlarmChanged:
CSTimerMonitor:didReceiveTimerChanged:
speechManagerRecordBufferAvailable:buffer:
speechManagerLPCMRecordBufferAvailable:chunk:
speechManagerDidStartForwarding:successfully:error:
speechManagerDidStopForwarding:forReason:
speechManagerRecordingContext
speechManagerRecordHardwareConfigurationDidChange:toConfiguration:
speechManagerDetectedSystemVolumeChange:withVolume:forReason:
speechManagerDidFinishAlertPlayback:ofType:error:
speechManagerBeginRecordInterruption:
speechManagerBeginRecordInterruption:withContext:
speechManagerEndRecordInterruption:
speechManager:willSetAudioSessionActive:
speechManager:didSetAudioSessionActive:
voiceTriggerDidDetectKeyword:
voiceTriggerDidDetectNearMiss:
voiceTriggerDidDetectSpeakerReject:
voiceTriggerDidDetectTwoShotAtTime:
keywordDetectorDidDetectKeyword
initWithSamplingRate:asset:
startSmartSiriVolume
setAsset:
prepareSoundLevelBufferFromSamples:soundType:firedVoiceTriggerEvent:triggerStartTimeSampleOffset:triggerEndTimeSampleOffset:
estimateSoundLevelbySoundType:
reset
estimatedTTSVolumeForNoiseLevelAndLKFS:LKFS:
CSVolumeMonitor:didReceiveMusicVolumeChanged:
CSVolumeMonitor:didReceiveAlarmVolumeChanged:
.cxx_construct
delegate
setDelegate:
_smartSiriVolumeNoiseLevel
_smartSiriVolumeLKFS
_floatBuffer
_defaults
_ssvEnablePolicy
_startAnalyzeSampleCount
_samplesFed
_processedSampleCount
_isStartSampleCountMarked
_shouldPauseSSVProcess
_shouldPauseLKFSProcess
_alarmSoundIsFiring
_timerSoundIsFiring
_currentAsset
_musicVolumeDB
_alarmVolume
_noiseLevelChannelBitset
_LKFSChannelBitset
_energyBufferSize
_noiseLowerPercentile
_noiseUpperPercentile
_LKFSLowerPercentile
_LKFSUpperPercentile
_noiseTimeConstant
_noiseMicSensitivityOffset
_LKFSTimeConstant
_LKFSMicSensitivityOffset
_noiseTTSMappingInputRangeLow
_noiseTTSMappingInputRangeHigh
_noiseTTSMappingOutputRangeLow
_noiseTTSMappingOutputRangeHigh
_LKFSTTSMappingInputRangeLow
_LKFSTTSMappingInputRangeHigh
_LKFSTTSMappingOutputRangeLow
_LKFSTTSMappingOutputRangeHigh
_userOffsetInputRangeLow
_userOffsetInputRangeHigh
_userOffsetOutputRangeLow
_userOffsetOutputRangeHigh
_TTSVolumeLowerLimitDB
_TTSVolumeUpperLimitDB
_noiseWeight
_delegate
stringByAppendingFormat:
copy
inputRecordingSampleRate
_readAudioBufferAndFeed
audioFileReaderDidStartRecording:successfully:error:
dataWithLength:
audioFileReaderBufferAvailable:buffer:atTime:
audioFileReaderDidStopRecording:forReason:
close
initWithURL:
setRecordBufferDuration:
prepareRecording:
startRecording
stopRecording
readSamplesFromChannelIdx:
_fFile
_audioFeedTimer
_bufferDuration
_outASBD
lpcmNarrowBandASBD
lpcmASBD
initWithInASBD:outASBD:
_createSampleRateConverterWithInASBD:outASBD:
mutableBytes
setLength:
upsampler
downsampler
convertSampleRateOfBuffer:
_sampleRateConverter
_outBufferScaleFactor
_inASBD
initWithUTF8String:
_cs_initWithXPCObject:
_cs_xpcObject
initWithCSSpIdType:delegate:
spIdType
satScoreThreshold
processAudioData:
endProcessing
updateModelWithBestScoreUser:
rejectUtterance
logUtteranceUnderDirectory:withScores:withWinner:
sysConfigFilepath
_sysConfigFilepath
getSiriLanguageWithFallback:
initWithLocaleIdentifier:withAudioSession:
setLocaleIdentifier:
createKeywordDetector
initWithLanguageCode:
initWithLocale:
isRecording
releaseAudioSession
_stopAudioSession
destroySpeakerTrainer
_destroyAudioSession
_setupAudioSession
closeSessionBeforeStartWithStatus:successfully:withCompletion:
_createAudioAnalyzer
_shouldShowHeadsetDisconnectionMessage
_startAudioSession
createSpeechRecognizer
sharedtrainingSessionQueue
initWithUtteranceId:sessionNumber:Locale:audioSession:keywordDetector:speechRecognizer:speechRecognitionRequest:sessionDelegate:sessionDispatchQueue:completion:
addObject:
startTraining
suspendTraining
closeSessionWithStatus:successfully:complete:
_audioSource
audioSource
prepareRecord
setEndpointStyle:
setStartWaitTime:
setEndWaitTime:
setInterspeechWaitTime:
preheat
resetForNewRequestWithSampleRate:
hasCorrectAudioRoute
resumeTraining
VTUITrainingManagerFeedLevel:
VTUITrainingManagerHasTrainUtterance:languageCode:
sharedTrainer
trainUtterance:languageCode:
audioSessionDidStartRecording:error:
audioSessionDidStopRecording:
initWithData:numChannels:numSamples:sampleByteDepth:startSampleCount:hostTime:
processAudioSamplesAsynchronously:
audioSessionRecordBufferAvailable:
audioSessionErrorDidOccur:
audioSessionUnsupportedAudioRoute
didDetectBeginOfSpeech
didDetectEndOfSpeech:
VTUITrainingManagerStopListening
trainingManagerWithLocaleID:
CSVTUITrainingSessionRMSAvailable:
CSVTUITrainingSessionStopListen
CSVTUITrainingSession:hasTrainUtterance:languageCode:
endpointer:didDetectStartpointAtTime:
endpointer:didDetectHardEndpointAtTime:withMetrics:
_beginOfSpeechDetected
_endOfSpeechDetected
cleanupWithCompletion:
trainUtterance:shouldUseASR:completion:
cancelTrainingForID:
suspendAudio
setSuspendAudio:
startRMS
stopRMS
shouldPerformRMS
didDetectForceEndPoint
VTUITrainingSessionStopListen
setRms:
speechRecognizerAvailable
_performRMS
_locale
_audioSession
_audioAnalyzer
_keywordDetector
_trainingSessions
_currentTrainingSession
_sessionNumber
_suspendAudio
_cleanupCompletion
_speechRecognizer
_speechRecognizerAvailable
_rms
initWithLength:
convertToShortLPCMBufFromFloatLPCMBuf:
modelDirectory
_createDirectoryIfNotExist:
utteranceDirectory
fileExistsAtPath:isDirectory:
removeItemAtPath:error:
createDirectoryAtPath:withIntermediateDirectories:attributes:error:
CSSATBasePath
_satPath
array
contentsOfDirectoryAtPath:error:
predicateWithFormat:
caseInsensitiveCompare:
sortedArrayUsingSelector:
modelPath
fileExistsAtPath:
_isDirectoryEmpty:
initWithSpeakerModelFileName:languageCode:
enrollmentUtterance
needsRetrain
discard
isValid
_modelFileName
_languageCode
_modelPath
_utteranceDirectory
initWithContext:delegate:
processAudioChunk:
recordingStoppedForReason:
processMyriadDecision:
lastSpeakerIdInfo
getAudioSessionState
notifyAduioSessionStateChange:
setAudioSessionState:
_audioSessionState
initWithMachServiceName:options:
setRemoteObjectInterface:
initWithServiceName:
getCoreSpeechXPCConnection
errorWithDomain:code:userInfo:
setInvalidationHandler:
resume
remoteObjectProxy
installedVoiceTriggerAssetForLanguageCode:completion:
fetchRemoteVoiceTriggerAssetForLanguageCode:completion:
voiceTriggerRTModelForVersion:minorVersion:downloadedModels:preinstalledModels:completion:
getCoreSpeechServiceConnection
requestUpdatedSATAudio:
getFirstPassRunningMode:
requestUpdatedSATAudio
getFirstPassRunningMode
inputRecordingFramesPerPacket
inputRecordingSampleRateNarrowBand
inputRecordingBytesPerFrame
inputRecordingBytesPerPacket
inputRecordingNumberOfChannels
inputRecordingDurationInSecs
inputRecordingSampleBitDepth
EncryptionAudioSampleByteDepth
inputRecordingEncoderAudioQuality
inputRecordingSampleRateConverterAlgorithm
inputRecordingBufferDuration
audioConverterBitrate
channelForOutputReference
channelForProcessedInput
zeroFilterApproxAbsSpeechThreshold
csAudioProcessingQueuePriority
daysBeforeRemovingLogFiles
decryptedDataUsingAESKey:atFilepath:error:
initWithFileUrl:aesKey:sampleByteDepth:
readAudioChunksWithCallback:
fileUrl
setFileUrl:
aesKey
setAesKey:
readBuffer
setReadBuffer:
sampleByteDepth
setSampleByteDepth:
_fileUrl
_aesKey
_readBuffer
_sampleByteDepth
weakObjectsHashTable
_setupStateMachine
_setupCircularBuffer
_createListenPollingTimer
_createClearLoggingFileTimer
audioSessionActivationDelay
_notifyEvent:
mediaserverdDidRestart
_startClearLoggingFilesTimer
willDestroy
sharedManager
assetForCurrentLanguageOfType:completion:
supportContinuousVoiceTrigger
shouldRunVTOnCS
_getVoiceTriggerAssetForMac:
supportCircularBuffer
_createCircularBuffer
initWithNumChannels:recordingDuration:samplingRate:
getSpeechManagerStateMachine
currentState
_eventName:
performTransitionForEvent:
audioRecorder
initWithContext:error:
setAudioRecorder:
setCurrentContext:error:
isDeviceRoleStereo
_getClientRecordContext
isRecordContextVoiceTrigger:
isRecordingContextHDVC:
doubleValue
unsignedLongLongValue
hostTimeToTimeInterval:
_scheduleSetRecordModeToRecordingWithDelay:forReason:validator:completion:
_setRecordMode:error:
prepareRecordWithSettings:error:
prepareListenWithSettings:error:
prewarmAudioSession
recordRoute
recordDeviceInfo
recordSettings
isNarrowBand
_createRecorderWithContextIfNeeded:error:
_enableMiniDucking:
_prepareRecorderWithSettings:error:
startRecordingWithSettings:error:
startListening:
_cancelPendingSetRecordModeToRecordingForReason:
setRecordMode:error:
_setRecordMode:withDelay:error:
releaseClientAudioSession:
_releaseAudioSessionForListening:error:
supportPacketDecoding
_releaseClientAudioSession:
releaseAudioSession:
enableMiniDucking:
sampleCount
_stateName:
_startRecordingWithSettings:event:error:
_handleAOPFirstPassTriggerEvent:settings:error:
_handleVoiceTriggerSwitchAOP2APEvent:settings:error:
_startRecordingWithSettings:error:
_startRecordingForClient:error:
notifyEvent:
voiceTriggerRecordContext
lpcmRecordSettings
supportOpportunisticZLL
sampleCountFromHostTime:
_stopRecordingWithEvent:
handleLostServerConnection
handleServerDidRestart
_stopForwardingToClient
_startForwardingToClient
_startListenPolling
_destroyAudioRecorderIfNeeded
_prepareListenWithSettings:error:
_startListening:
_stopListenPolling
removeObject:
containsObject:
addSamples:numSamples:atHostTime:
processSampleCount:hostTime:
bufferLength
copySamplesFrom:to:
_performPendingSetRecordModeToRecordingForReason:
numberWithInteger:
supportSmartVolume
audioRecorderBufferAvailable:buffer:atTime:
audioRecorderBufferAvailable:buffer:
audioRecorderDidStartRecording:successfully:error:
audioRecorderDidStopRecording:forReason:
audioRecorderRecordHardwareConfigurationDidChange:toConfiguration:
audioRecorderDidFinishAlertPlayback:ofType:error:
audioRecorderBeginRecordInterruption:
audioRecorderBeginRecordInterruption:withContext:
audioRecorderEndRecordInterruption:
audioRecorder:willSetAudioSessionActive:
audioRecorder:didSetAudioSessionActive:
voiceTriggerDetectedOnAOP:
audioRecorderDisconnected:
audioRecorderLostMediaserverd:
didTransitFrom:to:by:
didIgnoreEvent:from:
CSSiriEnabledMonitor:didReceiveEnabled:
CSAudioServerCrashMonitorDidReceiveServerCrash:
CSAudioServerCrashMonitorDidReceiveServerRestart:
CSLanguageCodeUpdateMonitor:didReceiveLanguageCodeChanged:
initWithVoiceTriggerFirstPass:voicetriggerSecondPass:voicetriggerEventNotifier:audioRecorder:
startManager
_getVoiceTriggerAsset:
_destroyCircularBuffer
registerSpeechController:
getCurrentState
isClientRecording
setClientContext:error:
prepareRecordingForClient:error:
_setCurrentContext:error:
releaseClientAudioSession
startRecordingAsyncWithSetting:event:completion:
startRecordingWithSetting:event:error:
_startRecordingForAOPFirstPassTriggerWithSettings:error:
stopRecordingWithEvent:
_startForwardingToSmartSiriVolume
_reinitializeSmartSiriVolumeWithAsset:
getEstimatedTTSVolume
queue
setQueue:
assetQueryQueue
setAssetQueryQueue:
stateMachine
setStateMachine:
audioBuffer
setAudioBuffer:
clientController
setClientController:
secondPassStartSampleCount
setSecondPassStartSampleCount:
lastVoiceTriggerEventInfo
setLastVoiceTriggerEventInfo:
smartSiriVolume
setSmartSiriVolume:
activeAudioProcessors
setActiveAudioProcessors:
continuousAudioProcessors
setContinuousAudioProcessors:
lastForwardedSampleCount
setLastForwardedSampleCount:
clientStartSampleCount
setClientStartSampleCount:
listenPollingTimer
setListenPollingTimer:
clearLoggingFileTimer
setClearLoggingFileTimer:
listenPollingTimerCount
setListenPollingTimerCount:
clearLoggingFileTimerCount
setClearLoggingFileTimerCount:
pendingSetRecordModeToRecordingToken
setPendingSetRecordModeToRecordingToken:
pendingSetRecordModeToRecordingCompletion
setPendingSetRecordModeToRecordingCompletion:
isSiriEnabled
setIsSiriEnabled:
deviceRoleIsStereo
setDeviceRoleIsStereo:
isAudioSessionActive
setIsAudioSessionActive:
setAudioSessionActivationDelay:
_isSiriEnabled
_deviceRoleIsStereo
_isAudioSessionActive
_audioRecorder
_assetQueryQueue
_stateMachine
_audioBuffer
_clientController
_secondPassStartSampleCount
_lastVoiceTriggerEventInfo
_smartSiriVolume
_activeAudioProcessors
_continuousAudioProcessors
_lastForwardedSampleCount
_clientStartSampleCount
_listenPollingTimer
_clearLoggingFileTimer
_listenPollingTimerCount
_clearLoggingFileTimerCount
_pendingSetRecordModeToRecordingToken
_pendingSetRecordModeToRecordingCompletion
_audioSessionActivationDelay
_notifyObserver:withClamshellState:
CSClamshellStateMonitor:didReceiveClamshellStateChange:
isClamshellClosed
_didReceiveClamshellStateChangeNotification:
lowercaseString
hasPrefix:
substringFromIndex:
mutableCopy
replaceMatchesInString:options:range:withTemplate:
_stringByStrippingLeadingNoise:
_stringByStrippingTrailingNoise:
rangeOfString:options:
stringValue
_firstMatchesForRegularExpression:
firstMatchInString:options:range:
numberOfRanges
rangeAtIndex:
substringWithRange:
_stringByFixingNamePattern:
_stringByStrippingNoiseLeadingNoise:TrailingNoise:
_hasSubstring:
_matchesRegularExpression:
_caseInsensitiveHasMatchInEnumeration:
_firstMatchesForRegularExpressions:
dictionary
sharedController
addObserver:forAssetType:
_fetchRemoteMetaData
_canFetchRemoteAsset:
assetOfType:language:
installedAssetOfType:language:
assetOfType:language:completion:
installedAssetOfType:language:completion:
fetchRemoteMetaOfType:
supportHybridEndpointer
setObject:forKeyedSubscript:
CSAssetManagerDidDownloadNewAsset:
CSVoiceTriggerAssetMetaUpdateMonitor:didReceiveNewVoiceTriggerAssetMetaData:
CSSpeechEndpointAssetMetaUpdateMonitor:didReceiveNewSpeechEndpointAssetMetaData:
CSAssetController:didDownloadNewAssetForType:
setDaemonRunningMode:
assetForCurrentLanguageOfType:
installedAssetForCurrentLanguageOfType:
installedAssetForCurrentLanguageOfType:completion:
currentLanguageCode
removeObserver:forAssetType:
_enablePolicy
_currentLanguageCode
_observers
_daemonRunningMode
setupPhraseSpotter
startMasterTimerWithTimeout:
resultAlreadyReported
stopMasterTimer
closeSessionWithCompletion:
removeAllObjects
updateMeterAndForward
pushAudioInputIntoPCMBuffer:
requestTriggeredUtterance:
setupSpeechRecognitionTaskWithVoiceTriggerEventInfo:
trimBeginingOfPCMBufferWithVoiceTriggerEventInfo:
computeRequiredTrailingSamples
feedSpeechRecognitionWithPCMBuffer
closeSessionWithStatus:successfully:
handleAudioBufferForVTWithAudioInput:withDetectedBlock:
finishSpeechRecognitionTask
feedSpeechRecognitionTrailingSamplesWithCompletedBlock:
triggeredUtterance:
updateMeters
averagePower
analyze:
numSamplesInPCMBuffer
appendAudioPCMBuffer:
removeObjectAtIndex:
frameLength
initWithCommonFormat:sampleRate:channels:interleaved:
initWithPCMFormat:frameCapacity:
mutableAudioBufferList
setFrameLength:
replaceObjectAtIndex:withObject:
removeObjectsInRange:
createAVAudioPCMBufferWithNSData:
handleAudioInput:
sharedGrammars
getLMEforLocale:
setContextualStrings:
setTaskHint:
setObject:forKey:
_setVoiceTriggerEventInfo:
recognitionTaskWithRequest:delegate:
endAudio
finish
handleMasterTimeout:
scheduledTimerWithTimeInterval:target:selector:userInfo:repeats:
invalidate
formattedString
whitespaceAndNewlineCharacterSet
stringByTrimmingCharactersInSet:
speechRecognitionDidDetectSpeech:
speechRecognitionTask:didHypothesizeTranscription:
speechRecognitionTask:didFinishRecognition:
speechRecognitionTaskFinishedReadingAudio:
speechRecognitionTaskWasCancelled:
speechRecognitionTask:didFinishSuccessfully:
_status
_utteranceId
_speechRecognitionRequest
_speechRecognitionTask
_masterTimer
_pcmBufArray
_resultReported
_sessionProcess
_sessionSuspended
_ASRErrorOccured
_sessionDelegate
_trainingCompletion
_numRequiredTrailingSamples
_numTrailingSamples
initWithBlob:
activeChannel
setActiveChannel:
_currentBlob
_activeChannel
initWithManager:
startController
getFixedHighPrioritySerialQueueWithLabel:
twoShotNotificationEnabled
_currentAudioRecorderSampleRate
_initializeMediaPlayingState
_initializeAlarmState
_initializeTimerState
_setSoundPlayingState
_contextToString:
_getRecordSettings
integerValue
_private_PacketDecodingUsed
_private_PacketEncodingUsed
_setupDownsamplerIfNeeded
_setupAudioConverter:
numberWithUnsignedLong:
numberWithInt:
duckOthersOption
setDuckOthersOption:
speakerIdEnabled
_isVoiceTriggered
voiceTriggerInfo
initWithSpIdInvocationStyle:asset:locale:vtEventInfo:
_isSpeakerIdTrainingTriggered
speakerIdRecognizerWithContext:delegate:
_shouldSetStartSampleCount
_isAutoPrompted
alertMuteBehaviorDict
_setupSpeakerId
elapsedTimeWithNoSpeech
speechControllerDidDetectVoiceTriggerTwoShot:atTime:
lpcmNonInterleavedASBD
lpcmInterleavedASBD
isRecordContextRaiseToSpeak:
isRecordContextAutoPrompt:
isRecordContextSpeakerIdTrainingTrigger:
isRecordingContextBTDT:
hostTime
addSamples:timestamp:
speechControllerLPCMRecordBufferAvailable:buffer:
data
addSamples:numSamples:
channels
packetDescriptionCount
bytesDataSize
initWithCapacity:
packetDescriptions
initWithBytes:length:
timeStamp
speechControllerRecordBufferAvailable:buffers:recordedAt:
speechControllerDidStartRecording:successfully:error:
flush
speechControllerDidStopRecording:forReason:
_deviceAudioLogging
speechControllerRecordHardwareConfigurationDidChange:toConfiguration:
speechControllerDidUpdateSmartSiriVolume:forReason:
speechControllerDidFinishAlertPlayback:ofType:error:
speechControllerBeginRecordInterruption:
speechControllerBeginRecordInterruption:withContext:
speechControllerEndRecordInterruption:
speechController:willSetAudioSessionActive:
speechController:didSetAudioSessionActive:
narrowBandOpusConverter
opusConverter
setAlertSoundFromURL:forType:
playAlertSoundForType:
alertStartTime
playRecordStartingAlertAndResetEndpointer
setMeteringEnabled:
peakPowerForChannel:
averagePowerForChannel:
passThruVoiceTriggerInfo
resetForVoiceTriggerTwoShotWithSampleRate:
PacketDecodingUsed
speechControllerRequestsOperation:forReason:
metrics
setEndpointerDelegate:
processServerEndpointFeatures:
allKeys
initWithData:encoding:
_getSpeechIdentifier
speexRecordSettings
opusRecordSettings
lastEndOfVoiceActivityTime
endpointerModelVersion
updateEndpointerThreshold:
updateEndpointerDelayedTrigger:
shouldAcceptEagerResultForDuration:resultsCompletionHandler:
audioConverterDidConvertPackets:packets:timestamp:
speakerRecognizer:hasSpeakerIdInfo:
speakerRecognizerFinishedProcessing:withFinalSpeakerIdInfo:
initializeRecordSessionWithContext:
resetAudioSession
getLPCMAudioStreamBasicDescription
setSynchronousCallbackEnabled:
getRecordBufferDuration
startRecording:
isVoiceTriggered
isRTSTriggered
peakPowerForOutputReference
averagePowerForOutputReference
outputReferenceChannel
endpointAnalyzer
setEndpointAnalyzerDelegate:
resetEndpointer
isSmartSiriVolumeAvailable
getSmartSiriVolume
beginWaitingForMyriad
endWaitingForMyriadWithDecision:
endpointerProxy
setEndpointerProxy:
speechManager
setSpeechManager:
avvcContext
setAvvcContext:
isOpus
setIsOpus:
isActivated
setIsActivated:
setIsNarrowBand:
audioFileWriter
setAudioFileWriter:
spIdFactory
setSpIdFactory:
spIdRecognizer
setSpIdRecognizer:
spIdUserScores
setSpIdUserScores:
setTwoShotNotificationEnabled:
isMediaPlaying
setIsMediaPlaying:
isAlarmPlaying
setIsAlarmPlaying:
isTimerPlaying
setIsTimerPlaying:
isSoundPlaying
setIsSoundPlaying:
myriadPreventingTwoShotFeedback
setMyriadPreventingTwoShotFeedback:
_opusAudioConverter
_narrowBandOpusConverter
_audioConverter
_downsampler
_requestedRecordSettings
_lastVoiceTriggerInfo
_twoShotAudibleFeedbackQueue
_twoShotAudibleFeedbackDecisionGroup
_isOpus
_isActivated
_isNarrowBand
_twoShotNotificationEnabled
_isMediaPlaying
_isAlarmPlaying
_isTimerPlaying
_isSoundPlaying
_myriadPreventingTwoShotFeedback
_endpointerProxy
_speechManager
_avvcContext
_audioFileWriter
_spIdFactory
_spIdRecognizer
_spIdUserScores
stringWithCapacity:
appendFormat:
RTModel
_sha1:
_sha256:
isAvailable
_firedVoiceTriggerTimeout
shouldHandleSession
shouldMatchPayload
_firedEndPointTimeout
_registerVoiceTriggerTimeout
_reportStopListening
_registerEndPointTimeout
matchRecognitionResult:withMatchedBlock:withNonMatchedBlock:
bestTranscription
getTrailingPatternsForUtt:Locale:
getLeadingPatternsForUtt:Locale:
getRegexPatternsForUtt:Locale:
matchWithString:TrailingStr:LeadingStr:Pattern:
_detectBOS
_ASRResultReceived
_reportedStopListening
setFileLoggingLevel:
fileLoggingLevel
intValue
baseDir
assistantLogDirectory
enumeratorAtURL:includingPropertiesForKeys:options:errorHandler:
getUserVoiceProfileUploadPathWithEnrolledLanguageList:
_CSSATUploadPath
_getEnrolledLanguageList
enumeratorAtPath:
_isDirectory:
pathExtension
copyItemAtPath:toPath:error:
_CSSATUpdatePath
isCurrentDeviceCompatibleWithVoiceProfileAt:
_markSATEnrollmentSuccessForLanguageCode:
_markSATEnrollmentMigratedForLanguageCode:
_markSATEnrollmentWithMarker:forLanguage:
createFileAtPath:contents:attributes:
_deviceCategoryForDeviceProductType:
dataWithContentsOfURL:
JSONObjectWithData:options:error:
deviceCategoryStringRepresentationForCategoryType:
setWithObjects:
interstitialRelativeDirForLevel:
enumerateObjectsUsingBlock:
voiceTriggerEnabled
voiceTriggerInCoreSpeech
_storeModeEnabled
setFileLoggingIsEnabled:
voiceTriggerAudioLogDirectory
secondPassAudioLoggingEnabled
getUserVoiceProfileFileList
getUserVoiceProfileUploadPath
notifyUserVoiceProfileUploadComplete
getUserVoiceProfileUpdateDirectory
notifyUserVoiceProfileUpdateReady
remoteVoiceTriggerDelayTime
remoteVoiceTriggerEndpointTimeoutWithDefault:
interstitialAbsoluteDirForLevel:
myriadFileLoggingEnabled
enableAudioInjection:
audioInjectionEnabled
setAudioInjectionFilePath:
audioInjectionFilePath
useSiriActivationSPIForHomePod
hostTimeFromSampleCount:anchorHostTime:anchorSampleCount:
sampleCountFromHostTime:anchorHostTime:anchorSampleCount:
hostTimeFromSampleCount:
anchorSampleCount
setAnchorSampleCount:
anchorHostTime
setAnchorHostTime:
_anchorSampleCount
_anchorHostTime
initWithInitialState:
addTransitionFrom:to:for:
initialState
setInitialState:
transitions
setTransitions:
_currentState
_initialState
_transitions
utteranceFileASBD
initWithFilepath:
isWriting
fFile
inASBD
outASBD
_fileURL
CSEventMonitorDidReceiveEvent:
_closeAudioFile
fileURLWithPath:isDirectory:
appendAudioData:
_audioFile
_asbd
_url
_audioLength
initWithCSspIdType:withSysConfigFile:sysConfigRoot:delegate:
UUID
UUIDString
initWithType:deviceId:activationInfo:vadScore:hosttime:
_activationTypeString
builtInMicVoiceTriggerEvent:hostTime:
initWithType:deviceId:activationInfo:hosttime:
type
setType:
deviceId
setDeviceId:
activationInfo
setActivationInfo:
hosttime
setHosttime:
vadScore
setVadScore:
_UUID
_type
_deviceId
_activationInfo
_vadScore
_hosttime
isScreenLocked
getVoiceTriggerAssetTypeString
getEndpointAssetTypeString
_isReadyToUse
predicateForAssetType:language:
installedAssetOfType:withPredicate:
_fetchRemoteAssetOfType:withPredicate:localOnly:completion:
installedAssetOfType:withPredicate:completion:
_installedAssetOfType:withPredicate:
getCSAssetOfType:
_installedAssetOfType:withPredicate:completion:
_assetQueryForAssetType:withPredicate:localOnly:
runQueryAndReturnError:
predicate
_findLatestInstalledAsset:
stopQuery
startQuery:
state
isLatestCompareTo:
initWithAssetType:
setPredicate:
setQueriesLocalAssetInformationOnly:
isSpringboardStarted
isFirstUnlocked
predicateForfetchRemoteMetadataForAssetType:
_runAssetQuery:completion:
_updateFromRemoteToLocalAssets:forAssetType:completion:
isInstalled
isDownloading
cancelDownloadAndReturnError:
purgeAndReturnError:
_downloadAsset:withComplete:
_startDownloadingAsset:progress:completion:
setProgressHandler:
requiredDiskSpaceIsAvailable:error:
_defaultDownloadOptions
beginDownloadWithOptions:
resumeDownload:
adjustDownloadOptions:completion:
_csAssetsDictionary
getVoiceTriggerAssetCurrentCompatibilityVersion
getEndpointAssetCurrentCompatibilityVersion
supportPremiumAssets
componentsJoinedByString:
predicateWithFormat:argumentArray:
spIdSATDirForLocale:
spIdSATDirForLocale:userName:
stringForCSSpIdType:
spIdSATDirForLocale:userName:spidType:
URLsForDirectory:inDomains:
lastObject
createDirectoryIfDoesNotExist:
spIdAudioLogsDir
spIdAudioLogsDir2
spIdSiriDebugVTDataDirectory
spIdSiriDebugVoiceProfileStoreRootDirectory
spIdSiriDebugVoiceProfileStoreRootDirectoryForLocale:
URLByAppendingPathComponent:
dataWithCapacity:
satConfigFileNameForCSSpIdType:
resourcePath
stringForInvocationStyle:
spIdTypeForString:
stringForCSSATRunMode:
spIdSATAudioDirForLocale:userName:spidType:
spIdSATModelDirForLocale:userName:spidType:
spIdVoiceProfileImportRootDir
spIdAudioLogsCountLimitReached
spIdDataRootDirectory
spIdSiriDebugTrainedUsersFilePathForLocale:
spIdSiriDebugVoiceProfileRootDirectoryForProfile:locale:
spidAudioTrainUtterancesDir
streamAudioFromFileUrl:audioStreamBasicDescriptor:samplesPerStreamChunk:audioDataAvailableHandler:
isSpidAssetsAvailable
opusASBD
lpcmInt16ASBD
objectAtIndexedSubscript:
dataWithBytes:length:
audioDecoderDidDecodePackets:buffer:timestamp:
opusDecoder
addPackets:timestamp:
_decoder
preferredExternalRouteDidChange:
_addSmartSiriVolumeEnabledConditions
_subscribeEventMonitors
subscribeEventMonitor:
addConditions:
assetChangeMonitorDidDetectAssetChange:
sharedMonitor
startMonitoring
notifyVoiceTriggerAssetChanged
_didReceiveNewSpeechEndpointAssetMetaData
alertMuteSettings
appendBytes:length:
generateIfNecessaryVoiceTriggerProfilesAESKey
saveEncryptedDataUsingAESKey:atFilepath:
initWithFileUrl:sampleByteDepth:
writeBuffer
setWriteBuffer:
_writeBuffer
enter
leave
waitWithTimeout:
_dispatchGroup
_dispatchGroupCounter
createGrammars
bundleForClass:
bundlePath
dataWithContentsOfFile:
_getTrailingPatternsWithGrammars:withLocale:
_getLeadingPatternsWithGrammars:withLocale:
_getRegexPatternsWithGrammars:withUtt:withLocale:
_getLMEWithGrammar:withLocale:
URLSession:didBecomeInvalidWithError:
URLSession:didReceiveChallenge:completionHandler:
URLSessionDidFinishEventsForBackgroundURLSession:
_grammar
_processRemoteHeySiriCommandWithRequest:fromSenderID:withReply:
_processParallelRecordingCommandWithRequest:fromSenderID:withReply:
_sendParallelRecordingsToPeerId:voiceProfileRequestInfo:withReply:
_receiveParallelRecordingFromPeerId:recordingInfo:withReply:
_receiveVoiceProfileFromPeerId:voiceProfileInfo:withReply:
_processDataFetchCommandWithRequest:fromSenderID:withReply:
_processVoiceProfileDeleteCommandWithRequest:fromSenderID:withReply:
_receiveData1FromPeerId:requestInfo:withReply:
_sendData1ToPeerId:
_sendData2ToPeerId:
sendMessageWithPayload:toPeer:withReply:
_compressFilesInDirectory:matchingPredicate:compressedFileAvailable:
_sendData1File:withFileName:toPeerId:withCompressedFlag:
URLByDeletingPathExtension
stringByAppendingString:
moveItemAtPath:toPath:error:
writeToFile:options:error:
temporaryDirectory
writeToURL:atomically:
importSpIdProfileFromDir:forLocale:completion:
deleteSpeakerIdProfileWithIdentifier:forLocale:completion:
processRemoteCommandWithPayload:fromPeer:withReply:
sendInfo1ToNearbyPeer
sendInfo2ToNearbyPeer
adCompanionServiceProvider
setAdCompanionServiceProvider:
lastCommunicatedPeer
setLastCommunicatedPeer:
_adCompanionServiceProvider
_lastCommunicatedPeer
supportRaiseToSpeak
rootQueueWithFixedPriority:
supportKeywordDetector
supportSelfTriggerSuppression
supportCSTwoShotDecision
supportImplicitTraining
supportSAT
supportPremiumModel
hasRemoteCoreSpeech
hasRemoteBuiltInMic
getFixedPrioritySerialQueueWithLabel:fixedPriority:
systemUpTime
deviceUserAssignedName
initWithConfigPath:resourcePath:
resetBest
analyzeWavData:numSamples:
getAnalyzedResultForPhraseId:
sampleFed
getAnalyzedResult
keywordAnalyzerNDAPI:hasResultAvailable:forChannel:
initWithResult:
bestStart
setBestStart:
bestEnd
setBestEnd:
getSuperVectorWithEndPoint:
getOptionValue:
getThreshold
getLoggingThreshold
activePhraseId
setActivePhraseId:
_novDetector
_lastSampleFed
_sampleFedWrapAroundOffset
_activePhraseId
date
initWithTotalAudioRecorded:featuresAtEndpoint:endpointerType:serverFeatureLatencyDistribution:additionalMetrics:
totalAudioRecorded
setTotalAudioRecorded:
featuresAtEndpoint
setFeaturesAtEndpoint:
endpointerType
setEndpointerType:
serverFeatureLatencyDistribution
setServerFeatureLatencyDistribution:
additionalMetrics
setAdditionalMetrics:
_featuresAtEndpoint
_endpointerType
_serverFeatureLatencyDistribution
_additionalMetrics
_totalAudioRecorded
_isDeviceRoleStereo
waitingForConnection:error:
isConnected
localURL
string
_compatibilityVersion
appendString:
_version
_footprint
assetForAssetType:resourcePath:configVersion:
attributes
isPremium
containsValueForKey:
decodeObjectForKey:
encodeObject:forKey:
base64EncodedStringWithOptions:
substringToIndex:
supportsSecureCoding
encodeWithCoder:
initWithCoder:
initWithData:hash:locale:digest:signature:certificate:
initWithHash:locale:
initWithData:hash:locale:
builtInRTModelDictionary
modelData
modelLocale
modelHash
digest
signature
certificate
_modelData
_modelLocale
_modelHash
_digest
_signature
_certificate
lpcmInt16NarrowBandASBD
opusNarrowBandASBD
aiffFileASBD
_checkVoiceTriggerEnabled
_didReceiveVoiceTriggerSettingChanged:
_notifyObserver:withEnabled:
CSVoiceTriggerEnabledMonitor:didReceiveEnabled:
_didReceiveVoiceTriggerSettingChangedInQueue:
_isVoiceTriggerEnabled
startRecordingWithOptions:error:
stopRecording:
didPlayEndpointBeep
voiceTriggerEventInfo
hasPendingTwoShotBeep
retrainSpIdProfilesForLocale:completion:
convertStopReason:
resetEndPointer
hasAudioRoute
_didReceiveSiriSettingChanged:
dataWithContentsOfFile:options:error:
beepCancellerDidCancelSamples:buffer:timestamp:
numberWithUnsignedLongLong:
cancelBeepFromSamples:timestamp:
willBeep
_beepCanceller
_beepFloatVec
_shortBuffer
_numTotalInputSamples
_numTotalOutputSamples
utteranceAudioFilepathForSpIdType:
utteranceMetadataFilePathForSpIdType:
uniqueUttTag
setUniqueUttTag:
asset
locale
vtEventInfo
setVtEventInfo:
invocationStyle
setInvocationStyle:
_uniqueUttTag
_asset
_vtEventInfo
_invocationStyle
fileSystemRepresentation
makeRootlessDirectoryAtPath:error:
convertToRootlessDirectoryAtPath:error:
interfaceWithProtocol:
setWithArray:
setClasses:forSelector:argumentIndex:ofReply:
strongToWeakObjectsMapTable
_startMonitoring
_isVoiceTriggerEvent:
activationEventNotifier:event:completion:
notifyActivationEvent:completion:
_hasPendingActivationForType:
secondsToHostTime:
receiveTestNotificationAPMode
receiveTestNotificationAOPMode
sharedNotifier
start
stop
notifyActivationEvent:deviceId:activationInfo:completion:
setDelegate:for:
_didReceiveAOPFirstPassTrigger:completion:
_setupTestNotification
notifyToken
setNotifyToken:
delegates
setDelegates:
pendingActivationEvent
setPendingActivationEvent:
pendingCompletion
setPendingCompletion:
_delegates
_pendingActivationEvent
_pendingCompletion
initWithZeroWindowSize:approxAbsSpeechThreshold:numHostTicksPerAudioSample:
filterZerosInAudioPacket:atBufferHostTime:filteredPacket:
endAudioAndFetchAnyTrailingZerosPacket:
_audioZeroFilterImpl
readAudioChunksFrom:block:
_addAssetManagerEnabledConditions
getTestResponse:
setDelayInterstitialSounds:level:completion:
getTriggerCount:
clearTriggerCount:
stringWithCString:encoding:
createAudioCircularBufferWithDefaultSettings
copySamplesFromHostTime:
copyBufferWithNumSamplesCopiedIn:
saveRecordingBufferFrom:to:toURL:
setBufferLength:
_csAudioCircularBufferImpl
_bufferLength
_asssetMetaUpdatedKey
_didReceiveNewVoiceTriggerAssetMetaData
getSpeechManagerStateMachineForMac
getSpeechManagerStateMachineAOPBridgeOS
getSpeechManagerStateMachineDefault
initWithWordCount:trailingSilenceDuration:eosLikelihood:pauseCounts:silencePosterior:taskName:processedAudioDurationInMilliseconds:
initWithWordCount:trailingSilenceFrames:endOfSilenceLikelihood:pauseCounts:silencePosterior:taskName:
wordCount
setWordCount:
trailingSilenceDuration
setTrailingSilenceDuration:
eosLikelihood
setEosLikelihood:
pauseCounts
setPauseCounts:
silencePosterior
setSilencePosterior:
processedAudioDurationInMilliseconds
setProcessedAudioDurationInMilliseconds:
taskName
setTaskName:
_wordCount
_trailingSilenceDuration
_pauseCounts
_processedAudioDurationInMilliseconds
_taskName
_eosLikelihood
_silencePosterior
_configureWithASBD:andFrameRate:
_resetWithSampleRate:
_configureWithSampleRate:andFrameRate:
subChunkFrom:numSamples:forChannel:
_processAudioSamples:
_detectVoiceActivityInSamples:numSamples:
replaceBytesInRange:withBytes:length:
_getEndpointMetricsForAudioTimestamp:
endpointStyle
delay
setDelay:
startWaitTime
automaticEndpointingSuspensionEndTime
setAutomaticEndpointingSuspensionEndTime:
minimumDurationForEndpointer
setMinimumDurationForEndpointer:
lastStartOfVoiceActivityTime
bypassSamples
setBypassSamples:
endpointMode
setEndpointMode:
interspeechWaitTime
endWaitTime
saveSamplesSeenInReset
setSaveSamplesSeenInReset:
canProcessCurrentRequest
handleVoiceTriggerWithActivationInfo:
sampleRate
setSampleRate:
frameRate
setFrameRate:
detectedOneShotStartpoint
setDetectedOneShotStartpoint:
detectedRecurrentStartpoint
setDetectedRecurrentStartpoint:
communicatedStartPointDetection
setCommunicatedStartPointDetection:
detectedOneShotEndpoint
setDetectedOneShotEndpoint:
detectedRecurrentEndpoint
setDetectedRecurrentEndpoint:
communicatedEndpointDetection
setCommunicatedEndpointDetection:
samplesSeen
setSamplesSeen:
numSamplesProcessed
setNumSamplesProcessed:
lastOneShotStartpoint
setLastOneShotStartpoint:
lastOneShotEndpoint
setLastOneShotEndpoint:
lastRecurrentStartpoint
setLastRecurrentStartpoint:
lastRecurrentEndpoint
setLastRecurrentEndpoint:
floatSampleBuffer
setFloatSampleBuffer:
topLevelParameterDict
setTopLevelParameterDict:
modelDictPath
setModelDictPath:
isConfigured
setIsConfigured:
previousSamplesSeen
setPreviousSamplesSeen:
apQueue
setApQueue:
recordingDidStop
setRecordingDidStop:
vtEndInSampleCount
setVtEndInSampleCount:
_audioUnitEPVAD2
_saveSamplesSeenInReset
_detectedOneShotStartpoint
_detectedRecurrentStartpoint
_communicatedStartPointDetection
_detectedOneShotEndpoint
_detectedRecurrentEndpoint
_communicatedEndpointDetection
_isConfigured
_recordingDidStop
_endpointStyle
_endpointMode
_frameRate
_floatSampleBuffer
_topLevelParameterDict
_modelDictPath
_apQueue
_vtEndInSampleCount
_interspeechWaitTime
_startWaitTime
_endWaitTime
_automaticEndpointingSuspensionEndTime
_minimumDurationForEndpointer
_bypassSamples
_delay
_samplesSeen
_numSamplesProcessed
_lastOneShotStartpoint
_lastOneShotEndpoint
_lastRecurrentStartpoint
_lastRecurrentEndpoint
_previousSamplesSeen
dictionaryWithContentsOfFile:
initWithArray:
subdataWithRange:
appendData:
skipSamplesAtStartSuchThatNumSamplesReceivedSoFar:reachesACountOf:completionHandler:
splitAudioChunkSuchThatNumSamplesReceivedSoFar:reachesACountOf:completionHandler:
numChannels
_data
_numSamples
_startSampleCount
_hostTime
bestPhrase
bestScore
earlyWarning
setSampleFed:
setBestPhrase:
setBestScore:
setEarlyWarning:
isRescoring
setIsRescoring:
_earlyWarning
_isRescoring
_sampleFed
_bestPhrase
_bestStart
_bestEnd
_bestScore
_updateAssetWithLanguage:
_updateAssetWithCurrentLanguage
serverFeaturesLatencyDistributionDictionary
_readClientLagParametersFromHEPAsset:
_getCSHybridEndpointerConfigForAsset:
currentAsset
setCurrentAsset:
serverFeaturesQueue
setServerFeaturesQueue:
lastKnownServerEPFeatures
setLastKnownServerEPFeatures:
serverFeatureLatencies
setServerFeatureLatencies:
serverFeaturesWarmupLatency
setServerFeaturesWarmupLatency:
lastServerFeatureTimestamp
setLastServerFeatureTimestamp:
didReceiveServerFeatures
setDidReceiveServerFeatures:
clientLagThresholdMs
setClientLagThresholdMs:
clampedSFLatencyMsForClientLag
setClampedSFLatencyMsForClientLag:
useDefaultServerFeaturesOnClientLag
setUseDefaultServerFeaturesOnClientLag:
hybridClassifierQueue
setHybridClassifierQueue:
lastReportedEndpointTimeMs
setLastReportedEndpointTimeMs:
stateSerialQueue
setStateSerialQueue:
didCommunicateEndpoint
setDidCommunicateEndpoint:
currentRequestSampleRate
setCurrentRequestSampleRate:
vtExtraAudioAtStartInMs
setVtExtraAudioAtStartInMs:
hepAudioOriginInMs
setHepAudioOriginInMs:
firstAudioPacketTimestamp
setFirstAudioPacketTimestamp:
didTimestampFirstAudioPacket
setDidTimestampFirstAudioPacket:
silencePosteriorGeneratorQueue
setSilencePosteriorGeneratorQueue:
didDetectSpeech
setDidDetectSpeech:
setElapsedTimeWithNoSpeech:
_canProcessCurrentRequest
_didReceiveServerFeatures
_useDefaultServerFeaturesOnClientLag
_didCommunicateEndpoint
_didTimestampFirstAudioPacket
_didDetectSpeech
_serverFeaturesQueue
_lastKnownServerEPFeatures
_serverFeatureLatencies
_lastServerFeatureTimestamp
_hybridClassifierQueue
_stateSerialQueue
_currentRequestSampleRate
_firstAudioPacketTimestamp
_silencePosteriorGeneratorQueue
_serverFeaturesWarmupLatency
_clientLagThresholdMs
_clampedSFLatencyMsForClientLag
_lastReportedEndpointTimeMs
_vtExtraAudioAtStartInMs
_hepAudioOriginInMs
_elapsedTimeWithNoSpeech
getHostClockFrequency
hostTimeToSeconds:
macHostTimeFromBridgeHostTime:
saveMetadata:isExplicitEnrollment:
saveUtterance:utteranceAudioPath:numSamplesToWrite:isExplicitEnrollment:
_getBaseMetaDictionaryForUtterancePath:
writeMetaDict:atMetaPath:
saveRawUtteranceAndMetadata:to:isExplicitEnrollment:
saveUtteranceAndMetadata:atDirectory:isExplicitEnrollment:
_setupVAD2Endpointer
_shouldEnterTwoShotAtEndPointTime:
_shouldUseVAD2ForTwoShot
endpointerDelegate
hybridEndpointer
setHybridEndpointer:
vad2Endpointer
setVad2Endpointer:
activeEndpointer
setActiveEndpointer:
didEnterTwoshot
setDidEnterTwoshot:
vad2EndpointStyle
setVad2EndpointStyle:
vad2EndpointtMode
setVad2EndpointtMode:
vad2StartWaitTime
setVad2StartWaitTime:
vad2EndWaitTime
setVad2EndWaitTime:
vad2InterspeechWaitTime
setVad2InterspeechWaitTime:
vad2Delay
setVad2Delay:
vad2AutomaticEndpointingSuspensionEndTime
setVad2AutomaticEndpointingSuspensionEndTime:
vad2MinimumDurationForEndpointer
setVad2MinimumDurationForEndpointer:
vad2SaveSamplesSeenInReset
setVad2SaveSamplesSeenInReset:
_didEnterTwoshot
_vad2SaveSamplesSeenInReset
_endpointerDelegate
_hybridEndpointer
_vad2Endpointer
_activeEndpointer
_vad2EndpointStyle
_vad2EndpointtMode
_vad2StartWaitTime
_vad2EndWaitTime
_vad2InterspeechWaitTime
_vad2Delay
_vad2AutomaticEndpointingSuspensionEndTime
_vad2MinimumDurationForEndpointer
initWithBool:
initWithDouble:
initWithLongLong:
initWithUnsignedLongLong:
objCType
longLongValue
_checkFirstUnlocked
_notifyObserver:withUnlocked:
CSFirstUnlockMonitor:didReceiveFirstUnlock:
_didReceiveFirstUnlockInQueue:
_didReceiveFirstUnlock:
_firstUnlocked
_scaleDecayConstants:
_savePeaks:averagePower:maxSample:
_linearToDB:
_ampToDB:
initWithSampleRate:
process:stride:inFrameToProcess:
getPeakPowerDB
getAveragePowerDB
_averagePowerI
_instantaneousMode
_peak
_maxPeak
_decay
_peakDecay
_averagePowerPeak
_peakHoldCount
_previousBlockSize
_decay1
_peakDecay1
initWithCSspIdType:userName:assetResourcePath:satDirectory:assetHash:
userName
sysConfigFile
sysConfigRoot
satModelDir
satAudioDir
analyzeSpeakerVector:numElements:
updateSAT
_checkSpringBoardStarted
_didReceiveSpringboardStarted:
_notifyObserver:withStarted:
CSSpringboardStartMonitor:didReceiveStarted:
_didReceiveSpringboardStartedInQueue:
_isSpringBoardStarted
initWithConfigPath:triggerTokens:useKeywordSpotting:
runRecognition
_recognizeWavData:length:
triggerConfidence
_previousUtteranceTokens
_triggerTokenList
_useKeywordSpotting
_triggerConfidence
_availabilityChanged
_didReceivedNetworkAvailabilityChangedNotification:
_notifyObserver:withNetworkAvailability:
CSNetworkAvailabilityMonitor:didReceiveNetworkAvailabilityChanged:
initWithFormat:
decodeObjectOfClass:forKey:
initWithRoute:isRemoteDevice:remoteDeviceUID:remoteDeviceProductIdentifier:
isRemoteDevice
remoteDeviceUID
remoteProductIdentifier
copyWithZone:
initWithAVVCRecordDeviceInfo:
route
remoteDeviceProductIdentifier
_isRemoteDevice
_route
_remoteDeviceUID
_remoteDeviceProductIdentifier
hybridEndpointerAssetFilename
initWithResourcePath:configFile:configVersion:
_decodeJson:
assetHashInResourcePath:
fallBackAssetResourcePath
defaultFallBackAssetForSmartSiriVolume
getBoolForKey:category:default:
getStringForKey:category:default:
containsKey:category:
hashFromResourcePath
isEqualAsset:
configVersion
_decodedInfo
_path
_resourcePath
_configVersion
_configureAudioConverter:
_convertBufferedLPCM:allowPartial:timestamp:
_opusConverter
_bufferedLPCM
_recordBasePacketsPerSecond
_opusOutASBD
_convertPacketCount
_convertAudioCapacity
_lastTimestamp
_notifyObserver:withLanguageCode:
_didReceiveLanguageCodeUpdate
_checkAllConditionsEnabled
notifyCallback:
_monitors
_conditions
_callback
initWithDictionary:
enumerateKeysAndObjectsUsingBlock:
speakerDetectorThreshold
maxSpeakerVectorsToPersist
_initializeSAT:
_computeSATScore:
speakerDetector:didDetectSpeaker:
speakerDetector:didDetectSpeakerReject:
addLastTriggerToProfileWithSuperVector:
initWithAsset:speakerModel:
_initializeNDAPI:resourcePath:
processSuperVector:withResult:
analyzeWavForEnrollment:numSamples:
addLastTriggerToProfile
getSATVectorCount
getMaxSpeakerVectorsToPersist
_threshold
_maxSpeakerVectorsToPersist
_spkModel
CVTThreshold
VTSecondPassPreTriggerAudioTime
_sampleLengthFrom:To:
_keywordAnalyzer
_lastKeywordScore
_keywordThreshold
_extraSamplesAtStart
_voiceControllerWithContext:error:
_recordingSampleRate
_destroyVoiceController
_audioRecorderDidStartRecordingSuccessfully:error:
setRecordDelegate:
setPlaybackDelegate:
_shouldUseRemoteRecordForContext:
_createDeInterleaverIfNeeded
_createSampleRateConverterIfNeeded
_createAudioPowerMeterIfNeeded
_shouldInjectAudio
_needResetAudioInjectionIndex:
_shouldRunZeroFilter
_resetZeroFilter
_startRecordingForAudioInjection
currentRecordDeviceInfo
playbackRoute
streamDescription
_deinterleaveBufferIfNeeded:
_samplingRateConvertIfNeeded:
_processAudioChainWithZeroFiltering:atTime:
_processAudioChain:atTime:
_audioRecorderDidStopRecordingForReason:
voiceControllerDidStartRecording:successfully:
voiceControllerDidStartRecording:successfully:error:
voiceControllerDidStopRecording:forReason:
voiceControllerDidDetectStartpoint:
voiceControllerDidDetectEndpoint:ofType:
voiceControllerDidDetectEndpoint:ofType:atTime:
voiceControllerEncoderErrorDidOccur:error:
voiceControllerDidFinishAlertPlayback:ofType:error:
voiceControllerRecordHardwareConfigurationDidChange:toConfiguration:
voiceControllerBeginRecordInterruption:
voiceControllerBeginRecordInterruption:withContext:
voiceControllerEndRecordInterruption:
voiceControllerMediaServicesWereLost:
voiceControllerMediaServicesWereReset:
voiceControllerWillSetAudioSessionActive:willActivate:
voiceControllerDidSetAudioSessionActive:isActivated:
voiceControllerRecordBufferAvailable:buffer:
voiceControllerLPCMRecordBufferAvailable:buffer:
voiceControllerPlaybackBufferAvailable:buffer:
voiceControllerDidStartPlaying:successfully:
voiceControllerDidStopPlaying:forReason:
voiceControllerDecoderErrorDidOccur:error:
voiceControllerPlaybackHardwareConfigurationDidChange:toConfiguration:
voiceControllerBeginPlaybackInterruption:
voiceControllerEndPlaybackInterruption:
_voiceController
_zeroFilter
_deinterleaver
_interleavedABL
_pNonInterleavedABL
_needSampleRateConversion
_remoteRecordClient
_powerMeter
_shouldUsePowerMeter
_latestContext
_shouldUseRemoteRecord
_opusDecoder
_audioFileReader
_audioFilePathIndex
_waitingForDidStart
dataWithPropertyList:format:options:error:
stringByAppendingPathExtension:
encryptedDataWithAESGCMKey:completion:
propertyListWithData:options:format:error:
decryptedDataWithAESGCMKey:ivData:tagData:error:
randomBytesWithLength:error:
errorMessageForCCErrorCode:
getVoiceTriggerProfilesAESKey
generateIfNecessaryAESKeyWithKeySizeInBits:applicationTag:keyLabel:shouldGenerateIfNecessary:
generateAESKeyWithKeySizeInBits:
storeAESKeyInKeychain:applicationTag:keyLabel:
getAESKeyFromKeychainWithApplicationTag:keyLabel:
deleteAESKeyWithApplicationTag:keyLabel:
getKeychainAttributesForAESKeyWithApplicationTag:keyLabel:
isSiriRestrictedOnLockScreen
%s %@ task delivered.
%s %@ completed with response %@ and error %@.
%s Get initial state from MediaRemote: media is on playing state %{public}ld.
%s Start monitoring MediaRemote: media playback
%s Stop monitoring MediaRemote: media playback
%s MediaRemote reported the now playing app playback state changed to %s (state %d)
%s Celestial is not available on this platform.
%s notification = %{public}@
%s MobileTimer is not available on this platform.
%s In %@: Continuous digital zero detected, lasting %{public}u samples per channel
%s In %@: Continuous digital zero in this audio chunk detected, lasting %{public}u samples per channel
%s Plan removing the temp file %{public}@
%s Failed to remove temp file %{public}@ reason: %{public}@
%s Start copying %{public}u bytes of data to crashreporter
%s Failed to read data from %{public}@
%s Finished copying data to crashreporter.
%s Logging audio file into : %{public}@
%s SpkrId:: Error creating uttMetaJsonData: %@
%s SpkrId:: Failed to create UttMeta...
%s Start monitoring : VoiceTrigger Asset Download
%s Stop monitoring : VoiceTrigger Asset Download
%s New VoiceTrigger is now installed
%s CS logging files under %{public}@ created before %{public}@ will be removed.
%s Couldn't get creation date: %{public}@
%s Could not remove %{public}@: %{public}@
%s CS logging files number %{public}lu with pattern %{public}@ under %{public}@ exceeding limit, only keep the latest %{public}lu ones
%s Regular expression is nil!
%s SmartSiriVolume: deleted %{public}u elements in energy buffer.
%s SmartSiriVolume: number of elements to delete exceeds energy buffer size, ignore.
%s SmartSiriVolume init value for noise estimation %{public}f
%s SmartSiriVolume init value for LKFS estimation %{public}f
%s SmartSiriVolume enable policy changed : %{public}@
%s SmartSiriVolume received MediaRemote initial state as %{public}@
%s SmartSiriVolume haven't got MediaRemote callback yet, let's assume media is playing.
%s SmartSiriVolume received alarm initial state as %{public}@
%s SmartSiriVolume received timer initial state as %{public}@
%s asset is nil, use default parameters(this should not happen).
%s SmartSiriVolume configure: %{public}@
%s SmartSiriVolume heartbeat = %{public}lld
%s SmartSiriVolume: estimated noise level %{public}f
%s SmartSiriVolume: estimated LKFS %{public}f
%s SmartSiriVolume: pause SSV calculation.
%s SmartSiriVolume: resume SSV calculation.
%s SmartSiriVolume received VT event!
%s SmartSiriVolume remove samples from VT utterances by %{public}llu, with startAnalyzeSampleCount = %{public}llu, samplesFed = %{public}llu, triggerStartSampleCount = %{public}llu
%s SmartSiriVolume trying to delete too many VT samples, set triggerDurationToDelete to be limited max: %{public}llu
%s SmartSiriVolume got empty VT event!
%s SmartSiriVolume dismiss alarm firing as VoiceTrigger detected.
%s SmartSiriVolume dismiss timer firing as VoiceTrigger detected.
%s SmartSiriVolume: final estimated TTS volume in dB %{public}f
%s SmartSiriVolume: adjust TTS volume since alarm/timer is firing.
%s SmartSiriVolume: TTS volume in dB from noise %{public}f, from LKFS %{public}f, with user offset %{public}f
%s SmartSiriVolume: soft volume algorithm in use
%s SmartSiriVolume: pause LKFS calculation according to MediaRemote notification.
%s SmartSiriVolume: resume LKFS calculation according to MediaRemote notification.
%s SmartSiriVolume received unknown media playing state, let's assume media is playing.
%s SmartSiriVolume received unknown alarm state, let's reset alarm state.
%s SmartSiriVolume: alarm firing status = %@ according to MobileTimer notification.
%s SmartSiriVolume received unknown timer state, let's reset timer state.
%s SmartSiriVolume: timer firing status = %@ according to MobileTimer notification.
%s SmartSiriVolume: set StartAnalyzeSampleCount = %{public}lld
%s Couldn't find keychain value %@ for account %@ %{public}d
%s ::: Error reading file %@, err: %d
%s CSAudioFileReader requires prepare recording settings to feed audio
%s CSAudioFileReader only support LinearPCM to feed
%s Setting ExtAudioFileSetProperty failed : %d
%s Starting audio file feed timer, bufferDuration = %f sampleRate = %f, bytesPerFrame = %d, channelsPerFrame = %d
%s ::: Error reading data from audio file : %d
%s Reach to EOF, chunkSize = %d
%s Stopping audio file feed timer
%s Cannot create SampleRateConverter using AudioConverterNew : %{public}d
%s Cannot set Quality property to audioConverter
%s Cannot set Complexity property to audioConverter
%s Audio resampling done : %lu
%s AudioConverter is sad: 0x%{public}xd
%s xpc object string return nil
%s xpc object should be XPC_TYPE_STRING
%s Siri language is nil, falling back to %@
%s Locale: [%{public}@]
%s No locale set when creating phrase spotter.
%s Creation of Keyword Detector failed.
%s %{public}s Called
%s %{public}s async called
%s Called before completion called
%s BEGIN num:%{public}ld use:%{public}d
%s AudioSession setup failed
%s Has wrong audio routing, ask user to unplug headset
%s Start Audio Session failed
%s _sessionNumber [%{public}ld]
%s %{public}s Canceling Training
%s Called with status : %{public}d
%s %{public}s called
%s AudioSession StartRecording Failed
%s Setting suspendAudio:[%{public}d]
%s Resume training
%s Suspend training
%s Sending Training utterance to delegate
%s Training utterance was not saved by MBAgent, save locally now
%s Stop Listening
%s Cannot create directory since directory is nil
%s same name of file exists, this will be removed
%s Creating Directory : %{public}@
%s Creating Directory failed : %{public}@
%s Cannot remove model directory(%@) : %@
%s Cannot remove utterance directory(%@) : %@
%s getCoreSpeechXPCConnection Invalidated
%s Asking current VoiceTrigger aset for %{public}d.%{public}d
%s CSCoreSpeechServices Invalidated
%s Request updated SAT audio succeed.
%s Request updated SAT audio failed.
%s Siri is enabled, let start!!
%s Siri is not enabled yet we are keep waiting
%s Cannot get a VoiceTrigger asset : %{public}@
%s CSVoiceTriggerAsset found: %{public}@
%s speechController = %{public}p
%s event : %{public}@
%s context = %{public}@
%s Creating new CSAudioRecorder with context : %{public}@
%s Cannot create audio recorder : %{public}@
%s It cannot change context because it is recording
%s Cannot change context : %{public}@
%s recordingSettings = %{public}@, %{public}d
%s AVVC already recording.
%s setRecordModeToRecordingDelay = %{public}f
%s timeIntervalSinceLastTriggerEnd = %{public}f
%s Failed SetRecordModeToRecording with %{public}f seconds delay from prepareRecorder due to error %{public}@.
%s Finished SetRecordModeToRecording with %{public}f seconds delay from prepareRecorder.
%s Cancelled SetRecordModeToRecording with %{public}f seconds delay from prepareRecorder.
%s Scheduled SetRecordModeToRecording with %{public}f seconds delay in prepareRecorder.
%s AVVC already recording, change record mode to recording here.
%s AVVC already recording, nothing to prepare
%s AVVC Prepare recording failed : %{public}@
%s recordingSettings : %{public}@
%s AVVC Prepare Listening failed : %{public}@
%s context : %{public}@
%s Cannot set context since mediaserverd is recovering from crash
%s Cannot prepare since mediaserverd is recovering from crash
%s Cannot prepare since audio recorder was not initialized
%s recordingSettings from CS : %{public}@
%s settings : %{public}@
%s startRecording failed : %{public}@
%s startListening failed : %{public}@
%s mode : %{public}ld
%s Creating fake session activation notification for recording mode
%s setRecordMode failed : %{public}@
%s mode : %{public}ld, delay : %{public}.3f
%s Not supported in this platform
%s Delayed SetRecordModeToRecording: Consumed token %{public}@ with %{public}f seconds delay for reason %{public}@.
%s Delayed SetRecordModeToRecording: Setting record mode to recording for reason %{public}@.
%s Delayed SetRecordModeToRecording: Failed to set record mode to recording for reason %{public}@ due to error %@.
%s Delayed SetRecordModeToRecording: Successfully set record mode to recording for reason %{public}@.
%s Delayed SetRecordModeToRecording: Ignored set record mode to recording for reason %{public}@ because the validator rejected.
%s Delayed SetRecordModeToRecording: Ignored set record mode to recording for reason %{public}@ because the scheduled token %{public}@ does not match the current token %{public}@.
%s Delayed SetRecordModeToRecording: Scheduled new token %{public}@ with %{public}f seconds delay for reason %{public}@.
%s Delayed SetRecordModeToRecording: Cancelled token %{public}@ for reason %{public}@.
%s Delayed SetRecordModeToRecording: Consumed token %{public}@ in advance for reason %{public}@.
%s Activate Context = %{public}@
%s setCurrentContext failed : %{public}@
%s sessionOptions : %{public}tu
%s Expect switch AOP to AP event
%s Cannot handle switch AOP to AP event in current state : %{public}@
%s startRecordingBy %{public}@, currentState: %{public}@, settings: %{public}@
%s start recording since mediaserverd is recovering from crash
%s _lastForwardedSampleCount = %{public}tu, audioBufferSampleCount = %{public}tu
%s _lastForwardedSampleCounts = %{public}tu, audioBufferSampleCount = %{public}tu
%s Failed SetRecordModeToRecording with %{public}f seconds delay from startRecording due to error %{public}@.
%s Finished SetRecordModeToRecording with %{public}f seconds delay from startRecording.
%s Cancelled SetRecordModeToRecording with %{public}f seconds delay from startRecording.
%s Scheduled SetRecordModeToRecording with %{public}f seconds delay in startRecording.
%s Try start recording under PollingListening state.
%s Try start recording under Stopping state.
%s Failed to set current context %{public}@.
%s Failed to set prepare recorder %{public}@.
%s Failed to start recording %{public}@.
%s Requested startHostTime = %{public}llu, _clientStartSampleCount = %{public}tu
%s Cannot use opportunisticZLL since _clientStartSampleCount is newer than audioBuffer sample count
%s Opportunistic ZLL will be used starting from : %{public}tu
%s Opportunistic ZLL will not be used : %{public}tu
%s Generating fake didStartRecording delegate
%s stopRecording by %{public}@, currentState: %{public}@
%s Generating fake didStopRecording delegate
%s AudioRecorder lost mediaserverd connection
%s Mediaserverd recovered from crash
%s from:%{public}@ to:%{public}@ by:%{public}@
%s We do nothing for transition between %{public}@ and %{public}@
%s Ignore event(%{public}@) from(%{public}@) since we don't have transition
%s Trying to startListening
%s _createRecorderWithContextIfNeeded failed, it will try again %{public}f seconds later
%s _prepareListenWithSettings failed, it will try again %{public}f seconds later
%s startListening failed, it will try again %{public}f seconds later
%s Listen polling is already started, ignore startListenPolling request.
%s No listen polling timer is on, ignore stopListenPolling request.
%s Still recording, let's do not destroy.
%s ClientSpeechController is nil
%s ignore because lastForwardedSampleCount:%{public}lu, theMostRecentSampleCount:%{public}lu
%s Buffer underrun!!!!, lastForwardedSampleTime:%{public}lu, oldestSampleTimeInBuffer:%{public}lu
%s %{public}@, sucessfully:%{public}@ error:%{public}@
%s forward to speechManagerDidStartForwarding
%s listen succeed under %{public}@, going to stop recording
%s startListening succeed
%s didStart failed, it will try again %{public}f seconds later
%s ignore audioRecorderDidStartRecording
%s forReason : %{public}ld
%s forward to speechManagerDidStopForwarding
%s ignore audioRecorderDidStopRecording
%s toConfiguration: %{public}ld
%s type: %{public}d, error: %{public}@
%s context: %{public}@
%s active : %{public}d
%s Ignore session active notification
%s Language Code Changed : %{public}@
%s SmartSiriVolume: final estimated TTS volume %{public}f with music volume %{public}f
%s Wrongly called: SmartSiriVolume is not supported on this device type.
%s We stop listen polling since we anyway going to stop
%s Trying to start clear logging files
%s Clear logging file timer is already started, ignore startClearLoggingFilesTimer request.
%s init-_currentLanguageCode: %{public}@
%s Not able to fetch remote meta now, registering for callback
%s Asset Manager Policy has been enabled, try to fetch remote meta now
%s _currentLanguageCode changed: %{public}@
%s returning status to UI : %{public}d
%s Already reported status or no callback
%s Will suspend training
%s Will resume training
%s Decide to delay ending ASR: [%{public}ld] samples
%s Triggered! Event info: %{public}@
%{public}9lld %{public}9lld %{public}9lld
%s analyzing.... score so far: %{public}5.3f
%s feeding tailing: [%{public}ld] samples
%s correctSampleSize:    [%{public}ld]
%s accumSampleSize:      [%{public}ld]
%s startBufferIndex:     [%{public}ld]
%s startBufferSampleSize:[%{public}ld]
%s samplesToBeDeleted:   [%{public}ld]
%s Total Number of buffs:[%{public}ld]
%s Adjusting the start buffer
%s Adjusting the array elements
%s Unsupported
%s Begin of speech detected
%s End of speech detected
%s %{public}s CALLED
%s Master Timeout Fired
%s recognized text = %{public}@
%s Context : %{public}@
%s Resetting CoreSpeech frameworks
%s Ask start recording from: %{public}tu
%s Voice trigger to use the current voice triggered channel: %{public}tu
%s Auto prompt to use the last voice triggered channel: %{public}tu
%s Make startRecording/stopRecording mute for BTDT
%s SpeechController to receive data from default channel
%s SpeechController to receive data from channel %{public}tu
%s Start recording invoked too late, override scheduledCheckTime: %{public}llu to currentTime: %{public}llu
%s Scheduled audible feedback decision after %{public}.3fseconds (vtEndMachTime: %{public}llu currentMachTime: %{public}llu)
%s Two shot audible feedback decision timed out while waiting for Myriad decision
%s Two shot audible feedback decision not needed since we already stopped recording
%s Two shot audible feedback decision (%{public}.3fs later than the scheduled time), elapsedTimeWithNoSpeech: %{public}.3f
%s Two shot audible feedback is needed, should notify? [%{public}@]
%s Ask delay audio session active by %{public}f seconds
%s %{public}@
%s SpeechManager still forwarding audio after didStopForwarding, we shouldn't have this
%s Mute endpointing as SpeakerId parallel recording triggered
%s AVVCAudioBuffer contains %{public}d packet descriptions, size %{public}d, channels %{public}d. Ignoring
%s packetCount %{public}d
%s Bad packet length %{public}d. Skipping rest of record buffer.
%s SmartSiriVolume update reason: %lu
%s SpeechController is trying to forward encoded audio after didStopForwarding, we shouldn't have this
%s Not available
%s Two shot is detected at time %{public}.3f, should notify? [%{public}@]
%s Not playing two shot feedback since sound was playing when triggered
%s Requesting QuickStop operation upon detecting keyword
%s Received Myriad started
%s Received Myriad finished with decision: %tu
%s Received unknown media playing state, ignoring
%s Received unknown alarm playing state, ignoring
%s Received unknown timer playing state, ignoring
%s Detected sound is%{public}@ playing: media(%d) alarm(%d) timer(%d)
%s SpkrId:: SpeakerIdInfo from incorrect SpeakerRecognizer: expected: %@, spkrRecognizer: %@
%s CS doesn't have ndblobbuilder!
%s Fired VoiceTrigger Timeout
%s Stop right now since ASR has issue
%s EOS Timeout Fired
%s AudioSession Started
%s AudioSession Stopped
%s unknown endpoint type
%s Non Final: [%{public}@]
%s NON Final Matching
%s Final: [%{public}@]
%s Final Matching
%s Final Not Matching
%s SPEECH RECOGNITION TASK FINISH UNSUCCESSFULLY
%s %{public}@ %{public}@ %{public}ld %{public}@
%s Recog Result: [%{public}ld]
%s Couldn't create speech log directory at path %{public}@ %{public}@
%s Cannot delete existing SATUpload Diretory : %{public}@
%s Cannot create SAT Upload Directory : %{public}@
%s Cannot create directory(%{public}@)
%s Cannot copy file: %{public}@ to %{public}@
%s PHS update directory already exists, remove before we move forward
%s Failed to delete PHS update directory
%s Failed to create PHS update directory
%s We need SAT directory, deleting the file with same name first
%s Failed to get device hash list %{public}@
%s Processing sync data from device hash: %{public}@
%s Error to copy profile from %{public}@ to %{public}@, error: %{public}@
%s language: %{public}@, enableVTAfterSyncLanguage: %{public}@, currSiriLanguage: %{public}@
%s Enabling VoiceTrigger Upon VoiceProfile sync for language: %{public}@ and enrolled language: %{public}@
%s VoiceTrigger does not exist for this platform, not setting VoiceTriggerEnabled
%s Not enabling VoiceTrigger Upon VoiceProfile sync for language: %{public}@
%s Sucessfully migrated language %{public}@
%s Migrated language %{public}@ but failed to mark SAT enrollment
%s Sucessfully marked as migrated for language : %{public}@
%s Failed to mark migrated for language : %{public}@
%s Failed to remove update path [%{public}@] upon migration completion, error: %{public}@
%s Coudn't mark SAT enrollment %{public}@ at path %{public}@
%s Marked SAT enrollment %{public}@ at path %{public}@
%s We can't mark SAT {public}%@ when there is no audio directory
%s ERR: Unknown device. returning false: %{public}@
%s Malformed audio-dir URL for string <%{public}@>:url
%s ERR: reading contents of audioDir: %{public}@
%s No jsonFiles found in %{public}@: jsonFiles.count=%{public}lu
%s Unexpected: empty JSON data for file: %{public}@
%s Error reading metaDict at path: %{public}@
%s metaProductType: %{public}@
%s vtprofile: currDevice=[%{public}@:%{public}@] ; vpDirDevice=[%{public}@:%{public}@]
%s VoiceProfile MATCH
%s VoiceProfile MIS-MATCH
%s Could not find productType in VT-Meta file, trying next one
%s No compatible VT profile found for CurrDevice: %{public}@
%s Unknown Device category for deviceProduceType: %@
%s enableAudioInection: is only available on internal builds
%s setAudioInjectionFilePath: is only available on internal builds
%s kCSAudioInjectionFilePathKey is not array type
%s kCSAudioInjectionFilePathKey array size = %d
%s kCSAudioInjectionFilePathKey doesn't have NSString as an array entry
%s ::: Error creating output file %{public}@, err: %{public}d
%s ::: Error writing to output wave file. : %{public}ld
%s Failure disposing audio file %{public}d
%s Audio file already configured, closing first
%s Creating audio file at URL %{public}@
%s Failed creating audio file at url %{public}@ %{public}d
%s Error setting input format %{public}d
%s No audio file to append data
%s Failed writing audio file %{public}d
%s Closing file at URL %{public}@, audio size: %{public}u
%s Cannot setAlertSoundFromURL since mediaserverd is recovering
%s Cannot playAlertSoundForType since mediaserverd is recovering
%s Cannot playRecordStartingAlertAndResetEndpointer since mediaserverd is recovering
%s CSAssetController cannot query for nil language
%s Error running asset-query for assetType:%{public}lu, query: %{public}@, predicate: %{public}@, error: %{public}@
%s ::: found %{public}lu assets for assetType=%{public}lu, matching query: %{public}@
%s ::: %{public}s
%s ::: predicate: %{public}@
%s ::: %{public}s; query: %{public}@
%s Error running asset query: error %{public}@
%s ::: Request Fetching RemoteMetaData : assetType : %{public}d
%s ::: Request fetching remote asset
%s ::: Fetching remote asset
%s ::: Purging installed asset : %{public}@
%s ::: Request downloading remote asset
%s ::: Start downloading asset
%s ::: download progress: %{public}3.0f%%
%s ::: Error downloading; %{public}@
%s ::: download completed successfully.
%s Attempting to download asset %{public}@
%s Failure resuming paused voice asset %{public}@
%s Asset doesn't need downloading, invoking completion
%s ERR: Unknown AssetType: %{public}lu
%s SpkrId:: Unknown CSSpIdType string: %@
%s SpkrId:: ERR: spIdRootDirForLocale called with locale=%@
%s SpkrId:: Direntry with same name exists, this will be removed: %@
%s SpkrId:: Creating Directory : %@
%s SpkrId:: Creating Directory failed : %@
%s SpkrId:: Exceeded privacy limit for grading utterances : %ld (%d)
%s SpkrId:: SpeakerId Assets missing at %@
%s Stop monitoring : AudioRouteChangeMonitor
%s SmartSiriVolume cannot be resumed since Siri is speaking
%s Start monitoring : speech endpoint asset meta update
%s Stop monitoring : speech endpoint asset meta update
%s New speech endpoint asset is available
%s Failed to write to: %{public}@. err: %{public}@
%s unbalanced dispatch_group_enter and leave : ignore we are ignore dispatch_group_leave
%s Non internal build, Ignoring command %@ from peerId %@ - Bailing out!
%s Received Malformed command %@ from peerId %@ - Bailing out!
%s Command %@ received from peerId %@
%s Unknown Command: (%@) - Ignoring
%s Triggering sync with peer - %@
%s Triggering nearmiss sync with peer - %@
%s CSP2P_RemoteHeySiriCmd: ENABLE HeySiri: Not Implemented Yet: 
%s CSP2P_RemoteHeySiriCmd: DISABLE HeySiri: Not Implemented Yet: 
%s CSP2P_ParallelRecordingCmd: received malformed command -                         CSP2P_VoiceProfileProfileId_Key - %@                                              CSP2P_VoiceProfileSpeakerName_Key - %@ 
%s CSP2P_ParallelRecordingCmd: received command when speakerId is disabled - Bailing out
%s CSP2P_ParallelRecordingCmd: SpId Assets are not available - Bailing out
%s CSP2P_ParallelRecordingCmd: START RECORDING
%s CSP2P_ParallelRecordingCmd: Failed to set the recording context
%s CSP2P_ParallelRecordingCmd: Failed to prepare recording
%s CSP2P_ParallelRecordingCmd: Failed to start recording
%s CSP2P_ParallelRecordingCmd: STOP RECORDING
%s Cannot read contents of directory: %@, err: %@
%s Could not determine if [%@] is a directory or not. Err=%@
%s Found dir: %@. Skipping compression
%s _compressFilesInDirectory: Malloc failed for file %@ (%lu) - Discarding
%s _compressFilesInDirectory: Compression failed for file %@ (%lu) - Sending Uncompressed
%s _compressFilesInDirectory: File %@ compressed from %ld to %ld 
%s CSP2P_VoiceProfileParallelRecordingsFetchCmd: Cannot send VoiceProfile when _adCompanionServiceProvider is nil - returning
%s CSP2P_VoiceProfileParallelRecordingsFetchCmd: Message incomplete - Bailing out %@
%s CSP2P_VoiceProfileParallelRecordingsFetchCmd: File %@ isCompressed: %d, compressedSize: %ld, err: %@ 
%s CSP2P_VoiceProfileParallelRecordingsFetchCmd: Failed VoiceProfileTransfer: %@, error %@
%s Failed to remove the file %@
%s transfering file %@ isCompressed: %d, compressedSize: %ld, err:%@
%s Failed moving file from %@ to %@ with error %@
%s %@ is nil - Bailing out
%s Failed in transporting Voice file %@ with reponse: %@, error %@
%s Failed to remove the file %@ with error %@
%s CSP2P_VoiceProfileParallelRecordingTransferCmd: received malformed command - %@ %@ %@
%s CSP2P_VoiceProfileParallelRecordingTransferCmd: unknown IDS peer with passed Identifier %@, %@ %@
%s CSP2P_VoiceProfileParallelRecordingTransferCmd: received malformed command - %@
%s CSP2P_VoiceProfileParallelRecordingTransferCmd: Creating directory failed with error %@
%s Syncing parallel recorded audio file - %@ from %@
%s Uncompressed file %@ sent by peer %@
%s Writing to file(%@) failed!. Err=%@
%s received malformed command - %@ %@ %@
%s unknown IDS peer with passed Identifier %@, %@ %@
%s received malformed command - %@
%s Ignoring sync of existing file %@ from %@
%s Syncing audio file - %@ from %@
%s CSP2P_VoiceProfileTransferCmd: received malformed command - %@ %@ %@
%s CSP2P_VoiceProfileTransferCmd: received malformed command: CSP2P_VoiceProfileData_Key: %@CSP2P_VoiceProfileFileName_Key: %@CSP2P_VoiceProfileSpeakerName_Key: %@CSP2P_VoiceProfileLocale_Key: %@CSP2P_VoiceProfileDataType_Key: %@CSP2P_VoiceProfileTotalSegments_Key: %@CSP2P_VoiceProfileSegment_Key: %@
%s CSP2P_VoiceProfileTransferCmd: received command when speakerId is disabled - Bailing out
%s CSP2P_VoiceProfileTransferCmd: SpId Assets are not available - Bailing out
%s CSP2P_VoiceProfileTransferCmd: Received VoiceProfile Segment (%@/%@) from peerId %@
%s CSP2P_VoiceProfileTransferCmd: Failed to delete the directory %@ with error %@
%s CSP2P_VoiceProfileTransferCmd: received VoiceProfileSegment %@, expected %d
%s CSP2P_VoiceProfileTransferCmd: Creating directory failed with error %@
%s CSP2P_VoiceProfileTransferCmd: Writing to file failed!!!
%s Trigger Voice training with import Dir %@
%s Import finished with result: %d, err: %@
%s Received request to delete VoiceProfile %@ from peerId %@
%s Delete finished with result: %d, err: %@
%s Cannot send data across when _adCompanionServiceProvider is nil - returning
%s ERR: %{public}@
%s NDAPI initialization failed
%s set StartAnalyzeSampleCount = %{public}lld
%s NDAPI config doesn't contain threshold_normal
%s NDAPI config doesn't contain threshold_logging
%s Couldn't create CoreSpeech log directory at path %{public}@ %{public}@
%s Start monitoring : SACInfo
%s Stop monitoring : SACInfo
%s Device is in stereo mode : %{public}@
%s Dealloc of CSRemoteControlClient, it should close connection
%s VoiceTrigger is already %{public}@, received duplicated notification!
%s Start monitring : VoiceTrigger setting switch
%s Cannot start monitoring VoiceTrigger setting switch because it was already started
%s Stop monitring : VoiceTrigger setting switch
%s VoiceTrigger enabled = %{public}@
%s Creating new CSAudioRecorder with context : %@
%s AudioRecorder creation failed : %@
%s Cannot prepare since audio recorder does not exist
%s AudioRecorder is already recording, do not prepare anymore
%s Cannot prepareRecordWithSettings : %@
%s Start monitoring : Siri setting switch, Siri is %{public}@
%s Stop monitoring : Siri setting switch
%s Siri Enabled = %{public}@
%s BeepCanceller asset file loading from : %{public}@
%s Could not read beep file: %@
%s beepVector Size = %{public}lu
%s Cannot initialize beep canceller
%s Beep canceller initialized with maxNumSamples = %{public}d
%s It will beep now
%s Reset beep cancellation
%s Vault already exists and is NOT secure. Attempting to secure: %{public}@
%s Success setting up DataVault at: %{public}@
%s Received Activation Event : %{public}@
%s Returning error for already existing pending activation event : %{public}@
%s No delegate registered : Postpone activation event handling until we have delegate registered
%s Pending Timeout fired for %{public}@ returning error for timeout
%s There is no pending activation event to timeout
%s Cannot handle activation event : %{public}@
%s Found pending activation : %{public}@, handle pending activation immediately
%s AOP First Pass trigger detected
%s Error reading audio file: %{public}d, skipping...
%s AssetManager cannot be turned on since springBoard is not started
%s AssetManager cannot be turned on since isFirstUnlocked is NO
%s AssetManager cannot be turned on since network is not available
%s numChannels: %{public}lu, recordingDuration: %{public}f, sampleRate: %{public}f
%s Cannot copy samples since this is empty
%s Could NOT copyFrom: %{public}lu to: %{public}lu, retSampleCount: %{public}lu
%s copyBuffer: oldestSample: %{public}lu latestSample: %{public}lu, numSamplesCopied: %{public}lu
%s CSAudioCircularBuffer.reset
%s saveRecordingBufferFrom: %{public}lu to: %{public}lu toURL: %{public}@
%s csrb: %{public}@
%s Invalid request: (%{public}lu, %{public}lu): noting to write to file
%s Invalid request: reqStartSample=%{public}lu, reqEndSample=%{public}lu, oldestSampleInBuffer: %{public}lu, latestSampleInBuffer=%{public}lu
%s Start monitoring : VoiceTrigger Asset meta update
%s Stop monitoring : VoiceTrigger Asset meta update
%s New VoiceTrigger asset metadata is available
%s VAD2 preheat...
%s CSVAD2EndpointAnalyzer: resetForNewRequestWithSampleRate
%s ERR: Deprecated VAD2 reset called
%s %{public}@ Resetting with style %{public}ld, _samplesSeen: %{public}f, newSampleRate: %{public}tu, _sampleRate: %{public}f
%s _audioUnitEPVAD2=%{public}p, auNeedsReset: %{public}d
%s Failed to reset EPVAD2: %{public}d
%s vtEndInSecs: %{public}f, _vtEndInSampleCount: %{public}lu, voiceTriggerInfo: %{public}@,
%s Empty samplesBuffer!
%s Received audio buffer with 8 frames of zeroes
%s Not configured
%s done: %{public}d, _detectedOneShotStartpoint: %{public}d, _communicatedEndpointDetection: %{public}d, _startWaitTime: %{public}f_samplesSeen: %{public}f, _delay: %{public}f, _sampleRate: %{public}f(_startWaitTime + _delay) * _sampleRate): %{public}f, (_samplesSeen / _sampleRate): %{public}f, _automaticEndpointingSuspensionEndTime: %{public}f
%s No startpoint detected after %{public}f, timing out, _samplesSeen: %f, _samplesSeen(ms): %f, _lastOneShotEndpoint: %f
%s Ignoring recurrent endpoint at %{public}f becuase it's too early (< %{public}f)
%s Fell back to recurrent endpoint (%{public}f) because one-shot is too early (%{public}f < %{public}f)
%s Fell back to recurrent endpoint, _samplesSeen: %f, _samplesSeen(ms): %f, _lastOneShotEndpoint: %f
%s Reporting one-shot ep:  _samplesSeen: %f, _samplesSeen(ms): %f, _lastOneShotEndpoint: %f
%s sampleRate: %{public}lf frameRate: %{public}d
%s Skipping re-initialization of EPVAD2; no audio consumed yet
%s EPVAD2 reset with existing parameters
%s Could not find endpointer audio unit component
%s AU instantiation error: %{public}d
%s No model available for mode: %{public}d
%s Error reading plist endpoint model at %{public}@
%s Could not set kAUEndpointVADProperty_ViterbiModelData: %{public}d
%s Could not set %{public}@ to %{public}@: %{public}d
%s Could not initialize audio unit: %{public}d
%s CSVAD2Endpointer is configured
%s Unexpected block size of %{public}u, not %{public}u. Skipping this block of audio.
%s Could not process audio via endpointer: %{public}d
%s tuningLibraryPath: %{public}@
%s VAD2-epModelPath: %{public}@
%s Could not read kAUEndpointVAD2Property_LatestEndpointerEventTimeSeconds: %{public}d
%s Found one shot startpoint at %{public}.3f seconds
%s Found one shot endpoint at %{public}.3f seconds
%s Could not read kAUEndpointVAD2Property_LatestRecurrentVADEventTimeSeconds: %{public}d
%s Found recurrent startpoint at %{public}.3f seconds
%s Found recurrent endpoint at %{public}.3f seconds
%s xpc object should be XPC_TYPE_ARRAY
%s xpcObject value is NULL
%s Cannot decode non-plist types of XPC object
%s Cannot encode non-plist types into XPC object : %{public}@
%s Cannot generate subChunk since channel(%{public}tu) is larger than number of channels(%{public}tu)
%s Cannot generate subChunk if it reuqest more than it has : %{public}tu %{public}tu %{public}tu
%s SpkrId:: totalNumSamplesReceived(%lu) > maxSamplesToSkip(%lu), Not Skipping any samples
%s SpkrId:: Processing ended at: numSamplesProcessed=%lu, totalSampleCountToReach=%lu
%s HEP::RecordingDidStop: Ignoring processAudioSamplesAsynchronously: Not queueing
%s triggerEndSeconds: %{public}f, _vtEndInSampleCount: %{public}lu, _vtExtraAudioAtStartInMs: %{public}lu,  _hepAudioOriginInMs: %{public}f, voiceTriggerInfo: %{public}@,
%s CSHybridEndpointAnalyzer recordingStoppedForReason: %{public}lu
%s language changed to: %{public}@: CSHybridEndpointer new asset: %{public}@
%s new hybrid endpoint asset downloaded, CSHybridEndpointer asset : %{public}@
%s %{public}@ doesnt exist
%s Could not read: %{public}@
%s Could not decode contents of: %{public}@: err: %{public}@
%s Delta is larger than anchorHostTime
%s Delta is larger than anchorSampleCount
%s Not supported on this platform
%s Saving utterance and meta as %{public}@ training.
%s Failed to write utterance into %{public}@
%s numSamplesToWrite %{public}lu
%s Failed to get CSAudioFileWriter:
%s Failed to addSamples to CSAudioFileWriter: %{public}@
%s Failed to endAudio on CSAudioFileWriter: %{public}@
%s ERR: called with nil metaPath
%s ERR: called with nil uttPath
%s ERR: called with nil uttMeta
%s Cannot create json file : %{public}@
%s Creating new VAD2-EP
%s CSHybridEndpointer canProcessCurrentRequest
%s CSHybridEndpointer can-NOT-ProcessCurrentRequest, fallback  to VAD2
%s _activeEndpointer=%{public}@
%s shouldUseCVT2ShotDecision: %d, isWatchRTSTriggered=%d
%s CVT-2shot: Resetting VAD2 ep
%s CVT-2shot: NOT resetting vad2 ep
%s CVT-2shot: Ignoring non-VAD2 ep
%s endpointer: %{public}@: didDetectStartpointAtTime: %{public}f
%s CSEndpointerProxy didDetectHardEndpoint using for 1-2 shot at Time: %{public}f
%s %{public}@: Endpointer didDetectHardEndpointAtTime:withMetrics: %{public}f, CallingDelegate: %{public}@
%s CSEndpointerProxy: didDetectHardEndpoint: ep-time: %{public}f, triggerEnd: %{public}f, vad2EndWaitTime: %{public}f, delta: %{public}f, legacyTwoShotThreshold: %{public}f, enterTwoShot: %{public}d
%s WARN: endpointerModelVersion called when CSHybridEndpointer is not available
%s Cannot create NSNumber if xpcObject is NULL
%s XPC object type should be BOOL, DOUBLE, INT64, or UINT64
%s Cannot create xpcObject if objcType is NULL
%s Cannot create xpcObject since there is no matching type
%s Stop monitoring : First unlock
%s Start monitoring : Springboard start
%s Cannot start monitoring Springboard start because it was already started
%s Stop monitoring : Springboard start
%s SpringBoard started = %{public}@
%s network state notify key : %s
%s Start monitoring : network availability
%s Stop monitoring : network availability
%s Network availability changed
%s Fallback asset resource path : %{public}@
%s Cannot find corespeech asset from resourcePath : %{public}@
%s Configuration file is not exists : %{public}@
%s Cannot read configuration file : %{public}@
%s Cannot decode configuration json file : %{public}@
%s Configuration json file is not expected format
%s Cannot access to %{public}@ %{public}@ using default value
%s There is not audio buffer to convert. Skip this.
%s Resetting AudioConverter buffer
%s createAudioConverter : initial frames per buffer = dur %{public}.2f * sr %{public}.2f = %{public}u
%s _configureAudioConverter: encoded audio needs minimum of %{public}u bytes per output buffer
%s _configureAudioConverter: AudioConverterGetProperty(kAudioConverterPropertyMinimumOutputBufferSize) returned status %{public}d
%s _configureAudioConverter: final framesPerBuffer: %{public}u
%s _configureAudioConverter: _convertPacketCount: %{public}u
%s _configureAudioConverter: AudioConverterGetProperty(MaximumOutputPacketSize): returned status %{public}d
%s createAudioConverter: outputSizePerPacket: %{public}u
%s _configureAudioConverter: _convertAudioCapacity %{public}u bytes
%s Cannot create AudioConverter using AudioConverterNew : %{public}u
%s Cannot set encoder bit rate : %{public}u
%s Cannot set codecQuality : %{public}u
%s Start monitoring : Siri language code
%s Stop monitoring : Siri language code
%s Siri language changed to : %{public}@
%s Ignore notifying change of language code, since it is nil
%s xpc object should be XPC_TYPE_DICTIONARY
%s xpcObject key or value is NULL
%s Cannot encode key into xpcobject since the key is not NSString class type
%s SAT successfully initialized : %{public}@
%s SAT Score = %{public}f, threshold = %{public}f
%s Cannot create CSVTUIKeywordDetector since there is no asset available
%s Cannot create CSVTUIKeywordDetector since we cannot initialize NDAPI
%s AVVC initialization failed
%s Successfully create AVVC : %{public}p
%s Trying to set record buffer duration : %{public}lf
%s Failed setting record buffer duration. Duration is %{public}lf
%s Creating beep canceller...
%s Calling AVVC prepareRecordWithSettings : %{public}@
%s Creating SampleRateConverter
%s Calling AVVC setCurrentContext : %{public}@
%s Calling AVVC prewarmAudioSession
%s Calling AVVC releaseAudioSession : %{public}tu
%s Should not call setDuckOthersOptions with NO in B238
%s %{public}@ miniDucking now
%s zeroFilterWinSz: %{public}tu, numHostTicksPerAudioSample: %{public}f
%s _vtEndInSampleCount:%{public}ld, _numSamplesProcessed: %{public}ld, vtInfo: %{public}@
%s ::: CSAudioRecord will inject audio file instead of recording
%s Resetting AudioFilePathIndex
%s Increase AudioFilePathIndex = %d
%s AudioFilePathIndex is out-of-boundary _audioFilePathIndex:%d injectAudioFilePaths:%d
%s AudioFilePathIndex:%d accessing:%@
%s Unable to find injectAudioFilePath = %@
%s Resetting ZeroFilter
%s Calling AVVC startRecordingWithSettings : %{public}@
%s Calling AVVC startRecording
%s Calling AVVC stopRecording
%s no operation : setRecordMode is not implemented in AVVC
%s Sampling rate = %{public}f
%s AVVC sampling rate = %{public}f
%s AVVC doesn't return sampleRate, assume it is default sample rate
%s Cannot handle audio buffer : unexpected format(%{public}u)
%s Calling AVVC playAlertSoundsForType : %{public}ld
%s Zero Filter Metrics: %@
%s Beep Canceller Metrics : %@
%s successfully : %{public}d, error : %{public}@
%s toConfiguration : %{public}d
%s type : %{public}d, error : %{public}@
%s withContext : %{public}@
%s activate : %{public}d
%s AVVC lost mediaserverd connection
%s AVVC informed mediaserverd reset, no further action required
%s Failed to deinterleave the data: %{public}d
%s Cannot create de-interleaver using AudioConverterNew: %{public}d
%s Created de-interleaver
%s Created narrowBandToWidBandConverter
%s Creating Audio Power Meter
%s We don't need Audio Power Meter
%s Could not encrypt data. Err=%{public}@
%s Failed to create ivtag-plist from dict: %{public}@, err=%{public}@
%s Failed to write ivData=%{public}@ at filepath=%{public}@, err=%{public}@
%s Failed to write encryptedData to file at: %{public}@, err=%{public}@
%s Failed to delete ivtag file when saving encryptedFile failed!: err=%{public}@
%s Could not read ivtag file: %{public}@, err: %{public}@
%s Could not create ivtagDict from ivtag-plist. Err=%@
%s Could not read encryptedData from file: %@, err: %@
%s Cannot create NSData with size 0
%s xpc object should be XPC_TYPE_DATA
%s Failed to create regular expression : %{public}@
CSSiriDebugConnection
CSMediaPlayingMonitor
CSVolumeMonitor
CSTimerMonitor
CSAlarmMonitor
CSAssetManagerEnablePolicyFactory
CSAudioZeroCounter
SmartSiriVolume
CSAudioFileManager
SpIdMetadataLogging
CSVoiceTriggerAssetDownloadMonitor
Directory
CSSmartSiriVolume
CSMediaPlayingMonitorDelegate
NSObject
CSAlarmMonitorDelegate
CSTimerMonitorDelegate
CSSpeechManagerDelegate
CSVoiceTriggerDelegate
CSAudioFileReader
CSAudioSampleRateConverter
XPCObject
CSSpIdProcessor
LanguageCode
CSVTUITrainingManager
CSVTUITrainingSessionDelegate
CSVTUIAudioSessionDelegate
CSEndpointAnalyzerDelegate
LPCMTypeConversion
CSSpeakerModel
CSSpIdVTSpeakerRecognizer
CSSpIdSpeakerRecognizer
CSAudioSessionMonitor
CSCoreSpeechServices
CSConfig
CSEncryptedAudioFileReader
CSSpeechManager
CSAudioRecorderDelegate
CSStateMachineDelegate
CSSiriEnabledMonitorDelegate
CSAudioServerCrashMonitorGibraltarDelegate
CSSmartSiriVolumeDelegate
CSAudioRouteChangeMonitorDelegate
CSVoiceTriggerAssetDownloadMonitorDelegate
CSLanguageCodeUpdateMonitorDelegate
CSClamshellStateMonitor
CSVTUIEditDistance
CSAssetManager
CSVoiceTriggerAssetMetaUpdateMonitorDelegate
CSSpeechEndpointAssetMetaUpdateMonitorDelegate
CSAssetControllerDelegate
CSVTUITrainingSession
SFSpeechRecognitionTaskDelegate
CSVTUIEndPointDelegate
CSKeywordAnalyzerNDEAPI
CSSpeechController
CSAudioConverterDelegate
CSSpIdSpeakerRecognizerDelegate
RTModel
CSVTUITrainingSessionWithPayload
CSSpIdTrainingParallelRecorder
CSPreferences
CSAudioTimeConverter
CSStateMachine
Meter
CSPlainAudioFileWriter
CSAudioFileWriter
CSEventMonitor
CSAudioFileLog
Alert
Metrics
CSSpIdSpeakerVectorGenerator
CSActivationEvent
CSSpIdTIOnlySpeakerRecognizer
CSScreenLockMonitor
CSAssetController
CSEventMonitorDelegate
Utils
SpeakerId
CSAudioDecoder
CSAudioRouteChangeMonitor
VoiceTriggerPassThru
CSSmartSiriVolumeEnablePolicy
CSVoiceTriggerAssetChangeMonitor
CSSpeechEndpointAssetMetaUpdateMonitor
VoiceTriggerRecord
CSVTUIRegularExpressionMatcher
CSEncryptedAudioFileWriter
CSDispatchGroup
CSVTUIASRGrammars
NSURLSessionDelegate
CSP2PService
CSUtils
CSKeywordAnalyzerNDAPI
CSEndpointerMetrics
CSSACInfoMonitor
CSRemoteControlClient
CSAsset
CSVoiceTriggerRTModel
NSSecureCoding
NSCoding
AudioStreamBasicDescription
CSVoiceTriggerEnabledMonitor
CSRemoteRecordClient
CSSpIdProfilesManager
CSVTUIAudioSessionRemote
CSVTUIAudioSession
CSSiriEnabledMonitor
CSBeepCanceller
CSSpIdContext
Rootless
CoreSpeechXPCProtocol
CSActivationEventNotifier
CSAudioZeroFilter
AudioFile
CSAssetManagerEnablePolicy
CSCoreSpeechServiceListenerDelegate
CSAudioCircularBuffer
CSVoiceTriggerAssetMetaUpdateMonitor
CSSpeakerIdRecognizerFactory
CSAssetManagerEnablePolicyMac
CSSpeechManagerStateMachineFactory
CSServerEndpointFeatures
CSVoiceTriggerSpeakerTrainer
CSVAD2EndpointAnalyzer
CSEndpointAnalyzerImpl
CSEndpointAnalyzer
private
CSAudioChunk
CSNovDetectorResult
CSNovDetector
Bitset
CSHybridEndpointAnalyzer
CSAssetManagerDelegate
Time
CSVoiceTriggerEnrollmentDataManager
CSEndpointerProxy
CSFirstUnlockMonitor
CSAudioPowerMeter
CSSpIdSATAnalyzer
CSSpringboardStartMonitor
CSKeywordAnalyzerQuasar
CSNetworkAvailabilityMonitor
CSAudioRecordDeviceInfo
NSCopying
CSAudioConverter
CSLanguageCodeUpdateMonitor
CSPolicy
RecordContext
CSSpeakerDetectorNDAPI
CSVTUIKeywordDetector
CSAudioRecorder
AVVoiceControllerRecordDelegate
AVVoiceControllerPlaybackDelegate
CSBeepCancellerDelegate
CSAudioDecoderDelegate
CSAudioFileReaderDelegate
Encryption
CSAESKeyManager
DuckOption
ResourcePathHash
CSSRFUserSettingMonitor
v12@0:4@8
@8@0:4
v8@0:4
v16@0:4@8i12
i8@0:4
@"NSObject<OS_dispatch_queue>"
f8@0:4
@20@0:4@8f12I16
v16@0:4@8I12
@"NSString"
I8@0:4
v20@0:4@8L12@16
v16@0:4@8@12
@88@0:4{AudioStreamBasicDescription=dIIIIIIII}8{AudioStreamBasicDescription=dIIIIIIII}48
@92@0:4@8{AudioStreamBasicDescription=dIIIIIIII}12{AudioStreamBasicDescription=dIIIIIIII}52
v12@0:4f8
v12@0:4I8
r*8@0:4
v20@0:4@8@12f16
v20@0:4@8@12I16
v20@0:4@8@12@?16
@24@0:4@8@12@16^@20
B12@0:4@8
#8@0:4
@12@0:4:8
@16@0:4:8@12
@20@0:4:8@12@16
B8@0:4
B12@0:4#8
B12@0:4:8
Vv8@0:4
^{_NSZone=}8@0:4
B12@0:4@"Protocol"8
@"NSString"8@0:4
v16@0:4@"CSMediaPlayingMonitor"8i12
v16@0:4@"CSAlarmMonitor"8i12
v16@0:4@"CSTimerMonitor"8i12
v20@0:4@8B12@16
v20@0:4@8f12I16
v20@0:4@8i12@16
v16@0:4@8B12
v16@0:4@"CSSpeechManager"8@"AVVCAudioBuffer"12
v16@0:4@"CSSpeechManager"8@"CSAudioChunk"12
v20@0:4@"CSSpeechManager"8B12@"NSError"16
v16@0:4@"CSSpeechManager"8i12
@"NSDictionary"8@0:4
v20@0:4@"CSSpeechManager"8f12I16
v20@0:4@"CSSpeechManager"8i12@"NSError"16
v12@0:4@"CSSpeechManager"8
v16@0:4@"CSSpeechManager"8@"NSDictionary"12
v16@0:4@"CSSpeechManager"8B12
v16@0:4d8
v12@0:4@"NSDictionary"8
@16@0:4f8@12
v16@0:4I8i12
v36@0:4@8i12B16Q20Q28
f12@0:4i8
f16@0:4f8f12
f28@0:4f8f12f16f20f24
f36@0:4f8f12f16f20f24f28f32
f20@0:4f8f12f16
v16@0:4@8f12
v16@0:4Q8
f12@0:4f8
{unique_ptr<SmartSiriVolume, std::__1::default_delete<SmartSiriVolume> >="__ptr_"{__compressed_pair<SmartSiriVolume *, std::__1::default_delete<SmartSiriVolume> >="__value_"^{SmartSiriVolume}}}
{vector<float, std::__1::allocator<float> >="__begin_"^f"__end_"^f"__end_cap_"{__compressed_pair<float *, std::__1::allocator<float> >="__value_"^f}}
@"NSUserDefaults"
@"CSSmartSiriVolumeEnablePolicy"
@"CSAsset"
@"<CSSmartSiriVolumeDelegate>"
@12@0:4@8
B16@0:4d8
@12@0:4L8
^{OpaqueExtAudioFile=}
@"NSObject<OS_dispatch_source>"
{AudioStreamBasicDescription="mSampleRate"d"mFormatID"I"mFormatFlags"I"mBytesPerPacket"I"mFramesPerPacket"I"mBytesPerFrame"I"mChannelsPerFrame"I"mBitsPerChannel"I"mReserved"I}
@"<CSAudioFileReaderDelegate>"
^{OpaqueAudioConverter=}88@0:4{AudioStreamBasicDescription=dIIIIIIII}8{AudioStreamBasicDescription=dIIIIIIII}48
^{OpaqueAudioConverter=}
@16@0:4I8@12
v20@0:4@8@12@16
v20@0:4@"CSVTUITrainingSession"8@"NSData"12@"NSString"16
v16@0:4B8@12
v12@0:4i8
v16@0:4B8@"NSError"12
v12@0:4@"NSData"8
v12@0:4@"NSError"8
v20@0:4@8d12
v24@0:4@8d12@20
v20@0:4@"<CSEndpointAnalyzer>"8d12
v24@0:4@"<CSEndpointAnalyzer>"8d12@"CSEndpointerMetrics"20
@16@0:4@8@12
@12@0:4@?8
i20@0:4i8B12@?16
B12@0:4i8
v20@0:4i8B12@?16
v12@0:4B8
@"<CSVTUIAudioSession>"
@"CSVAD2EndpointAnalyzer"
@"CSVTUIKeywordDetector"
@"NSMutableArray"
@"CSVTUITrainingSession"
@"SFSpeechRecognizer"
@"<CSVTUITrainingManagerDelegate>"
@16@0:4@"CSSpIdContext"8@"<CSSpIdSpeakerRecognizerDelegate>"12
v12@0:4@"CSAudioChunk"8
v16@0:4@8@?12
v28@0:4I8I12@16@20@?24
L8@0:4
d8@0:4
S8@0:4
@20@0:4@8@12I16
B12@0:4@?8
@"NSURL"
@"NSData"
v24@0:4@8@12Q16
v24@0:4@"CSAudioRecorder"8@"NSData"12Q16
v16@0:4@"CSAudioRecorder"8@"AVVCAudioBuffer"12
v20@0:4@"CSAudioRecorder"8B12@"NSError"16
v16@0:4@"CSAudioRecorder"8i12
v20@0:4@"CSAudioRecorder"8i12@"NSError"16
v12@0:4@"CSAudioRecorder"8
v16@0:4@"CSAudioRecorder"8@"NSDictionary"12
v16@0:4@"CSAudioRecorder"8B12
v20@0:4i8i12i16
v16@0:4i8i12
v16@0:4@"CSSiriEnabledMonitor"8B12
v12@0:4@"CSAudioServerCrashMonitorGibraltar"8
v16@0:4@8@"NSString"12
@24@0:4@8@12@16@20
v12@0:4@?8
B16@0:4@8^@12
B12@0:4^@8
B16@0:4i8^@12
B24@0:4i8d12^@20
v28@0:4d8@16@?20@?24
B16@0:4I8^@12
B20@0:4I8@12^@16
v20@0:4@8I12@?16
B20@0:4@8I12^@16
v16@0:4@8^@12
@12@0:4i8
@12@0:4I8
@?8@0:4
@"CSAudioRecorder"
@"CSStateMachine"
@"CSAudioCircularBuffer"
@"<CSSpeechManagerDelegate>"
@"NSDictionary"
@"CSSmartSiriVolume"
@"NSHashTable"
@"NSUUID"
v16@0:4@"CSAssetController"8I12
v16@0:4I8@?12
v20@0:4I8@12@?16
B12@0:4I8
@"CSPolicy"
@"NSMutableDictionary"
v12@0:4@"SFSpeechRecognitionTask"8
v16@0:4@"SFSpeechRecognitionTask"8@"SFTranscription"12
v16@0:4@"SFSpeechRecognitionTask"8@"SFSpeechRecognitionResult"12
v16@0:4@"SFSpeechRecognitionTask"8B12
@48@0:4i8i12@16@20@24@28@32@36@40@?44
v16@0:4i8B12
@"SFSpeechAudioBufferRecognitionRequest"
@"SFSpeechRecognitionTask"
@"NSTimer"
@"<CSVTUITrainingSessionDelegate>"
@"NSMutableData"
@"<CSKeywordAnalyzerNDEAPIScoreDelegate>"
v24@0:4@"CSAudioConverter"8@"NSArray"12Q16
v16@0:4@"<CSSpIdSpeakerRecognizer>"8@"NSDictionary"12
{AudioStreamBasicDescription=dIIIIIIII}8@0:4
B12@0:4B8
B16@0:4@8i12
Q8@0:4
f12@0:4I8
v20@0:4d8@?16
@"CSAudioConverter"
@"CSAudioSampleRateConverter"
@"CSAudioZeroCounter"
@"NSObject<OS_dispatch_group>"
@"<CSSpeechControllerDelegate>"
@"CSEndpointerProxy"
@"CSSpeechManager"
@"CSPlainAudioFileWriter"
@"CSSpeakerIdRecognizerFactory"
@"<CSSpIdSpeakerRecognizer>"
v20@0:4@8@?12@?16
I12@0:4@8
@12@0:4^@8
B16@0:4@8@12
d16@0:4d8
v24@0:4Q8Q16
Q16@0:4Q8
@"<CSStateMachineDelegate>"
B16@0:4r^v8I12
B16@0:4r^v8l12
@24@0:4I8@12@16@20
@20@0:4@8Q12
@32@0:4I8@12@16f20Q24
@28@0:4I8@12@16Q20
@20@0:4I8@12B16
v24@0:4I8@12B16@?20
v60@0:4@8{AudioStreamBasicDescription=dIIIIIIII}12I52@?56
v20@0:4@8Q12
@"<CSAudioDecoderDelegate>"
@"<CSVoiceTriggerAssetChangeDelegate>"
i24@0:4@8@12@16@20
@16@0:4@8I12
l16@0:4Q8
v16@0:4@"NSURLSession"8@"NSError"12
v20@0:4@"NSURLSession"8@"NSURLAuthenticationChallenge"12@?<v@?i@"NSURLCredential">16
v12@0:4@"NSURLSession"8
@16@0:4i8@12
@20@0:4@8i12@16
v24@0:4@8@12@16B20
@"<CSADCompanionServiceProvider>"
@16@0:4@8i12
@"CSNovDetector"
@"<CSKeywordAnalyzerNDAPIScoreDelegate>"
@32@0:4d8@16i20@24@28
@"NSArray"
B20@0:4d8^@16
@"<CSRemoteControlClientDelegate>"
v12@0:4@"NSCoder"8
@12@0:4@"NSCoder"8
@32@0:4@8@12@16@20@24@28
@20@0:4@8@12@16
@"<CSRemoteRecordClientDelegate>"
v12@0:4@"<CSVTUIAudioSessionDelegate>"8
v12@0:4@"<Endpointer>"8
i12@0:4i8
@"<CSVTUIAudioSessionDelegate>"
{unique_ptr<BatchBeepCanceller, std::__1::default_delete<BatchBeepCanceller> >="__ptr_"{__compressed_pair<BatchBeepCanceller *, std::__1::default_delete<BatchBeepCanceller> >="__value_"^{BatchBeepCanceller}}}
{vector<short, std::__1::allocator<short> >="__begin_"^s"__end_"^s"__end_cap_"{__compressed_pair<short *, std::__1::allocator<short> >="__value_"^s}}
@"<CSBeepCancellerDelegate>"
v16@0:4@"NSString"8@?<v@?@"NSString"@"NSString"@"NSError">12
v28@0:4I8I12@"NSArray"16@"NSArray"20@?<v@?@"CSVoiceTriggerRTModel"@"CSVoiceTriggerRTModel"@"NSError">24
v24@0:4I8@12@16@?20
@"NSMapTable"
@"CSActivationEvent"
@24@0:4I8S12d16
Q24@0:4@8Q12^@20
Q12@0:4^@8
{unique_ptr<CSAudioZeroFilterImpl<unsigned short>, std::__1::default_delete<CSAudioZeroFilterImpl<unsigned short> > >="__ptr_"{__compressed_pair<CSAudioZeroFilterImpl<unsigned short> *, std::__1::default_delete<CSAudioZeroFilterImpl<unsigned short> > >="__value_"^{CSAudioZeroFilterImpl<unsigned short>}}}
B16@0:4@8@?12
Vv12@0:4@?8
Vv20@0:4@8i12@?16
Vv12@0:4@?<v@?@"NSString">8
Vv20@0:4@"NSArray"8i12@?<v@?@"NSError">16
Vv12@0:4@?<v@?I>8
Vv12@0:4@?<v@?>8
Vv12@0:4@?<v@?B>8
Vv12@0:4@?<v@?i>8
@20@0:4I8f12f16
v16@0:4r^v8I12
v24@0:4r^v8I12Q16
@16@0:4Q8
@16@0:4I8I12
@12@0:4^I8
v20@0:4I8I12@16
{unique_ptr<corespeech::CSAudioCircularBufferImpl<unsigned short>, std::__1::default_delete<corespeech::CSAudioCircularBufferImpl<unsigned short> > >="__ptr_"{__compressed_pair<corespeech::CSAudioCircularBufferImpl<unsigned short> *, std::__1::default_delete<corespeech::CSAudioCircularBufferImpl<unsigned short> > >="__value_"^{CSAudioCircularBufferImpl<unsigned short>}}}
@44@0:4i8i12d16@24d28@36i40
@40@0:4i8i12d16@24d28@36
@"<CSEndpointAnalyzerDelegate>"8@0:4
v12@0:4@"<CSEndpointAnalyzerDelegate>"8
v12@0:4@"CSServerEndpointFeatures"8
v20@0:4d8@?<v@?B@"NSArray">16
v12@0:4L8
^{OpaqueAudioComponentInstance=}
@"<CSEndpointAnalyzerDelegate>"
@16@0:4d8
B16@0:4^{AudioStreamBasicDescription=dIIIIIIII}8L12
v20@0:4d8L16
v16@0:4^f8L12
@40@0:4@8I12I16I20Q24Q32
@20@0:4I8I12I16
v20@0:4I8I12@?16
I12@0:4I8
v12@0:4@"CSAssetManager"8
@"CSServerEndpointFeatures"
@"NSDate"
Q12@0:4f8
f16@0:4Q8
d16@0:4Q8
Q32@0:4Q8Q16Q24
v20@0:4@8@12B16
B20@0:4@8@12B16
B24@0:4@8@12I16B20
B16@0:4@8B12
@"<CSEndpointAnalyzerImpl>"
@12@0:4f8
v20@0:4r^s8i12i16
@28@0:4I8@12@16@20@24
f16@0:4@8I12
@20@0:4@8@12B16
v16@0:4r^s8i12
@"<CSKeywordAnalyzerQuasarScoreDelegate>"
@12@0:4^{_NSZone=}8
@24@0:4@8B12@16@20
@20@0:4I8@12@16
v24@0:4@8B12Q16
v12@0:4^{OpaqueAudioConverter=}8
@"<CSAudioConverterDelegate>"
f12@0:4@8
@"CSSpeakerModel"
@"<CSSpeakerDetectorNDAPIDelegate>"
I16@0:4I8I12
@"CSKeywordAnalyzerNDAPI"
v24@0:4@8i12d16
v16@0:4@"AVVoiceController"8B12
v20@0:4@"AVVoiceController"8B12@"NSError"16
v16@0:4@"AVVoiceController"8i12
v12@0:4@"AVVoiceController"8
v24@0:4@"AVVoiceController"8i12d16
v16@0:4@"AVVoiceController"8@"NSError"12
v20@0:4@"AVVoiceController"8i12@"NSError"16
v16@0:4@"AVVoiceController"8@"NSDictionary"12
v16@0:4@"AVVoiceController"8@"AVVCAudioBuffer"12
v24@0:4@"CSBeepCanceller"8@"NSData"12Q16
v24@0:4@"CSAudioDecoder"8@"NSData"12Q16
v24@0:4@"CSAudioFileReader"8@"NSData"12Q16
v20@0:4@"CSAudioFileReader"8B12@"NSError"16
v16@0:4@"CSAudioFileReader"8i12
@16@0:4@8^@12
@"AVVoiceController"
@"CSAudioZeroFilter"
@"CSBeepCanceller"
{AudioBufferList="mNumberBuffers"I"mBuffers"[1{AudioBuffer="mNumberChannels"I"mDataByteSize"I"mData"^v}]}
^{AudioBufferList=I[1{AudioBuffer=II^v}]}
@"CSRemoteRecordClient"
@"CSAudioPowerMeter"
@"CSAudioDecoder"
@"CSAudioFileReader"
@"<CSAudioRecorderDelegate>"
@16@0:4I8^@12
@20@0:4@8@12^@16
@24@0:4I8@12@16B20
B20@0:4@8@12@16
fff?
mcpl
supo
?ffffff
@mcpl
NSt3__118basic_stringstreamIcNS_11char_traitsIcEENS_9allocatorIcEEEE
NSt3__115basic_stringbufIcNS_11char_traitsIcEENS_9allocatorIcEEEE
NSt3__119basic_ostringstreamIcNS_11char_traitsIcEENS_9allocatorIcEEEE
xfua2vpelppa
zPLR
zPLR
zPLR
zPLR
zPLR
zPLR
zPLR
zPLR
zPLR
zPLR
zPLR
zPLR
zPLR
zPLR
zPLR
zPLR
zPLR
zPLR
zPLR
zPLR
