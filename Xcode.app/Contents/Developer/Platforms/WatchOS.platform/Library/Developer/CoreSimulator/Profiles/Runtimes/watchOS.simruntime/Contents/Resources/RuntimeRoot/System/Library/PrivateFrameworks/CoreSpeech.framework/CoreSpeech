pbhw
pbtb
pbiu
otua
ciov
bhev
eltb
siar
tdtb
cvdh
cvpc
tcid
tsop
rtsh
tvps
fff?
?(knN33
mcpl
supo
ffffff
@mcpl
NSt3__118basic_stringstreamIcNS_11char_traitsIcEENS_9allocatorIcEEEE
NSt3__115basic_stringbufIcNS_11char_traitsIcEENS_9allocatorIcEEEE
NSt3__119basic_ostringstreamIcNS_11char_traitsIcEENS_9allocatorIcEEEE
mcplsupoxeps 
v8@?0
-[CSGestureMonitor isTriggerHandheld]
wakeGestureTimestamp
TQ,N,V_wakeGestureTimestamp
dismissalTimestamp
TQ,N,V_dismissalTimestamp
[SplitterEnabled(%d)]
[Device%d(%@) DoAP(%d)]
[SplitterState:%d]
splitterEnabled
TB,N,V_splitterEnabled
shouldDisableSpeakerVerificationInSplitterMode
TB,R,N
-[NviDataLogger logData:]
-[NviDataLogger stream:handleEvent:]
hash
TQ,R
superclass
T#,R
description
T@"NSString",R,C
debugDescription
oStream
T@"NSOutputStream",&,N,V_oStream
meta_version.json
enrollment_version.json
enrollment_completed
enrollment_migrated
meta_version
trainingType
explicit
implicit
implicitBaseProfile
handheld
near-field
far-field
utteranceWav
productVersion
productType
triggerSource
audioInputSource
otherSourceProfileMatch
containsPayload
grainedDate
buildVersion
+[CSUtteranceMetadataManager saveUtteranceMetadataForUtterance:enrollmentType:isHandheldEnrollment:triggerSource:audioInput:otherBiometricResult:containsPayload:]
+[CSUtteranceMetadataManager _getBaseMetaDictionaryForUtterancePath:]
+[CSUtteranceMetadataManager _writeMetaDict:forUtterancePath:]
.wav
.json
+[CSUtteranceMetadataManager upgradeMetaFilesIfNecessaaryAtSATRoot:]
+[CSUtteranceMetadataManager _saveMetaVersionFileAtPath:]
+[CSUtteranceMetadataManager _upgradeLocaleDirectoryIfNecessary:]
json
+[CSUtteranceMetadataManager _audioDirectoryNeedsUpgrade:]
+[CSUtteranceMetadataManager _upgradeUtteranceMeta:]
+[CSUtteranceMetadataManager isUtteranceImplicitlyTrained:]
+[CSUtteranceMetadataManager getUtteranceEnrollmentType:]
+[CSUtteranceMetadataManager recordedTimeStampOfFile:]
en_US_POSIX
yyyyMMdd
CSVoiceTriggerAssetHandler
-[CSVoiceTriggerAssetHandler getVoiceTriggerAsset:]
/BuildRoot/Library/Caches/com.apple.xbs/Sources/CoreSpeech_Sim/CoreSpeech-353.1.3/CoreSpeech/CSVoiceTriggerAssetHandler.m
observers
T@"NSHashTable",&,N,V_observers
queue
T@"NSObject<OS_dispatch_queue>",&,N,V_queue
CSAudioSessionController Queue
-[CSAudioSessionController dealloc]
-[CSAudioSessionController audioSessionInfoProvider:didReceiveAudioSessionInterruptionNotificationWithUserInfo:]_block_invoke
-[CSAudioSessionController audioSessionInfoProvider:didReceiveAudioSessionRouteChangeNotificationWithUserInfo:]_block_invoke
-[CSAudioSessionController audioSessionInfoProvider:didReceiveAudioSessionMediaServicesWereLostNotificationWithUserInfo:]_block_invoke
-[CSAudioSessionController audioSessionInfoProvider:didReceiveAudioSessionMediaServicesWereResetNotificationWithUserInfo:]_block_invoke
-[CSAudioSessionController _createXPCClientConnectionIfNeeded]
-[CSAudioSessionController _getLocalAudioSessionID]
-[CSAudioSessionController _startMonitoring]
-[CSAudioSessionController _stopMonitoring]
-[CSAudioSessionController _registerInterruptionNotification]
-[CSAudioSessionController _registerAudioRouteChangeNotification]
-[CSAudioSessionController _handleInterruption:]_block_invoke
-[CSAudioSessionController _mediaServicesWereLost:]_block_invoke
-[CSAudioSessionController _mediaServicesWereReset:]_block_invoke
-[CSAudioSessionController _audioRouteChanged:]_block_invoke
-[CSAudioSessionController _teardownXPCClientIfNeeded]
-[CSAudioSessionController CSXPCClient:didDisconnect:]_block_invoke
sessionInfoProvider
T@"<CSAudioSessionInfoProviding>",&,N,V_sessionInfoProvider
xpcClient
T@"CSXPCClient",&,N,V_xpcClient
shouldKeepConnection
TB,V_shouldKeepConnection
com.apple.siri.SiriDebug.SpeakerVoiceGradingTrigger
com.apple.siri.SiriDebug.RemoteNearMissGradingTrigger
com.apple.siri.SiriDebug.VoiceProfileAddedTrigger
com.apple.siri.SiriDebug.VoiceProfileSyncTrigger
com.apple.siri.SiriDebug
+[CSSiriDebugConnection launchSiriDebugAppWithMessage:]_block_invoke
v24@?0@"AFSiriResponse"8@"NSError"16
CSMediaPlayingMonitor queue
-[CSMediaPlayingMonitor initializeMediaPlayingState]_block_invoke_2
v16@?0@8
v12@?0I8
-[CSMediaPlayingMonitor _startMonitoringWithQueue:]
-[CSMediaPlayingMonitor _stopMonitoring]
-[CSMediaPlayingMonitor _notePossiblePlayPausedStateChange:]
PLAYING
NOT PLAYING
Audio/Video
Alarm
CSVolumeMonitor queue
-[CSVolumeMonitor _startMonitoringWithQueue:]
-[CSVolumeMonitor fetchVolumeFromAVSystemControllerForAudioCategory:]_block_invoke
-[CSVolumeMonitor systemControllerDied:]
-[CSVolumeMonitor startObservingSystemVolumes]
-[CSVolumeMonitor _startObservingSystemControllerLifecycle]
com.apple.cs.spid-nvi-additional-signals
spIdNviAssetsTaskId
triggerStartSampleCount
signalControllerNvi
T@"NviSignalProvidersController",&,N,V_signalControllerNvi
resultRxNvi
T@"CSSpeakerIdNviSignalReceiver",&,N,V_resultRxNvi
nviQueue
T@"NSObject<OS_dispatch_queue>",&,N,V_nviQueue
spIdCtx
T@"CSSpIdContext",&,N,V_spIdCtx
CSTimerMonitor queue
-[CSTimerMonitor _startMonitoringWithQueue:]
-[CSTimerMonitor _stopMonitoring]
CSAlarmMonitor queue
-[CSAlarmMonitor _startMonitoringWithQueue:]
-[CSAlarmMonitor _stopMonitoring]
-[CSAudioStreamHolding dealloc]
name
T@"NSString",&,N,V_name
delegate
T@"<CSBiometricMatchMonitorDelegate>",W,N,V_delegate
-[CSAudioZeroCounter getZeroStatisticsFromBuffer:entireSamples:]
-[CSAudioZeroCounter stopReportZeroStatistics]
smartSiriVolume
noiseLevelChannelBitset
LKFSChannelBitset
energyBufferSize
noiseLowerPercentile
noiseUpperPercentile
LKFSLowerPercentile
LKFSUpperPercentile
noiseTimeConstant
noiseMicSensitivityOffset
LKFSTimeConstant
LKFSMicSensitivityOffset
noiseTTSMappingInputRangeLow
noiseTTSMappingInputRangeHigh
noiseTTSMappingOutputRangeLow
noiseTTSMappingOutputRangeHigh
LKFSTTSMappingInputRangeLow
LKFSTTSMappingInputRangeHigh
LKFSTTSMappingOutputRangeLow
LKFSTTSMappingOutputRangeHigh
userOffsetInputRangeLow
userOffsetInputRangeHigh
userOffsetOutputRangeLow
userOffsetOutputRangeHigh
TTSVolumeLowerLimitDB
TTSVolumeUpperLimitDB
noiseWeight
SSVNoiseLevelChannelBitset
TQ,R,N
SSVLKFSChannelBitset
SSVEnergyBufferSize
TI,R,N
SSVNoiseLowerPercentile
SSVNoiseUpperPercentile
SSVLKFSLowerPercentile
SSVLKFSUpperPercentile
SSVNoiseTimeConstant
Tf,R,N
SSVNoiseMicSensitivityOffset
SSVLKFSTimeConstant
SSVLKFSMicSensitivityOffset
SSVNoiseTTSMappingInputRangeLow
SSVNoiseTTSMappingInputRangeHigh
SSVNoiseTTSMappingOutputRangeLow
SSVNoiseTTSMappingOutputRangeHigh
SSVLKFSTTSMappingInputRangeLow
SSVLKFSTTSMappingInputRangeHigh
SSVLKFSTTSMappingOutputRangeLow
SSVLKFSTTSMappingOutputRangeHigh
SSVUserOffsetInputRangeLow
SSVUserOffsetInputRangeHigh
SSVUserOffsetOutputRangeLow
SSVUserOffsetOutputRangeHigh
SSVTTSVolumeLowerLimitDB
SSVTTSVolumeUpperLimitDB
SSVNoiseWeight
SSVParameterDirectionary
T@"NSDictionary",R,N
CSVoiceTriggerXPCService Queue
-[CSVoiceTriggerXPCService enableVoiceTriggerForCoreSpeechDaemon:withAssertion:timestamp:]
voicetrigger assertion queue
-[CSVoiceTriggerXPCService enableVoiceTriggerForCoreSpeechDaemon:withAssertion:timestamp:]_block_invoke
Enabled
Disabled
-[CSVoiceTriggerXPCService setPhraseSpotterBypassingForCoreSpeechDaemon:timeout:]
bypassed
NOT bypassed
phrasespotter assertion queue
-[CSVoiceTriggerXPCService setPhraseSpotterBypassingForCoreSpeechDaemon:timeout:]_block_invoke_2
-[CSVoiceTriggerXPCService notifyVoiceTriggeredSiriSessionCancelledForCoreSpeechDaemon]
-[CSVoiceTriggerXPCService notifyServiceConnectionLostForCoreSpeechDaemon]
-[CSVoiceTriggerXPCService _createXPCClientConnectionIfNeeded]
-[CSVoiceTriggerXPCService voiceTriggerXPCClient:didDisconnect:]_block_invoke
-[CSVoiceTriggerXPCService _teardownXPCClientIfNeeded]
T@"<CSVoiceTriggerXPCServiceDelegate>",W,N,V_delegate
activationAssertions
T@"NSMutableSet",&,N,V_activationAssertions
isPhraseSpotterBypassed
TB,N,V_isPhraseSpotterBypassed
T@"CSVoiceTriggerXPCClient",&,N,V_xpcClient
OPP-
PCM-
OPUS_
-synced
com.apple.CoreSpeech.AudioLogging
+[CSAudioFileManager generateDeviceAudioLogging:speechId:]_block_invoke
FLLR
+[CSAudioFileManager _readDataFromFileHandle:toFileHandle:]
%@%@.wav
%@/%@%@.wav
+[CSAudioFileManager _createAudioFileWriterForOpportuneSpeakListenerWithLoggingDir:inputFormat:outputFormat:]
+[CSAudioFileManager _createAudioFileWriterWithLoggingDir:inputFormat:outputFormat:]
yyyy_MM_dd-HHmmss.SSS
^%@*
attsiri
+[CSAudioFileManager audioFileWriterForAttentiveSiri]
%@.wav
SpIdScoreThreshold
#SpkrRejected#
-[NSDictionary(SpIdMetadataLogging) logSpeakerIdMetadataAtFilepath:additionalMetadata:]
speakerDetected
-detected.json
-rejected.json
/BuildRoot/Library/Caches/com.apple.xbs/Sources/CoreSpeech_Sim/CoreSpeech-353.1.3/CoreSpeech/NSDictionary+SpIdMetadataLogging.m
<Unknown File>
Error creating uttMetaJsonData: %@
v12@?0i8
-[CSVoiceTriggerAssetDownloadMonitor _startMonitoringWithQueue:]
-[CSVoiceTriggerAssetDownloadMonitor _stopMonitoring]
-[CSVoiceTriggerAssetDownloadMonitor _didInstalledNewVoiceTriggerAsset]
com.apple.MobileAsset.VoiceTriggerAssets.ma.new-asset-installed
com.apple.MobileAsset.VoiceTriggerAssetsWatch.ma.new-asset-installed
com.apple.MobileAsset.VoiceTriggerAssetsMarsh.ma.new-asset-installed
com.apple.MobileAsset.VoiceTriggerAssetsMac.ma.new-asset-installed
+[CSUtils(Directory) removeLogFilesInDirectory:matchingPattern:beforeDays:]_block_invoke
v24@?0@"NSArray"8^@16
+[CSUtils(Directory) clearLogFilesInDirectory:matchingPattern:exceedNumber:]_block_invoke_2
+[CSUtils(Directory) clearLogFilesInDirectory:matchingPattern:exceedNumber:]_block_invoke
Unable to get %@ for file at %@: %@
q24@?0@"NSURL"8@"NSURL"16
B24@?0@"NSURL"8@"NSDictionary"16
+[CSUtils(Directory) _contentsOfDirectoryAtURL:matchingPattern:includingPropertiesForKeys:error:]
Languages
Footprint
Premium
isMediaPlaying
noiseLevelDB
musicLevelDB
musicPlaybackVolumeDB
alarmVolume
finalTTSVolume
isAlarmPlaying
isTimerPlaying
isLKFSProcessPaused
removeVoiceTriggerSamples
-[CSSmartSiriVolume initWithSamplingRate:asset:]
-[CSSmartSiriVolume startSmartSiriVolume]_block_invoke
RUNNING
PAUSED
v12@?0B8
v20@?0B8@"NSError"12
-[CSSmartSiriVolume _startListenPolling]
-[CSSmartSiriVolume _startListenPollingWithInterval:completion:]
-[CSSmartSiriVolume _startListenPollingWithInterval:completion:]_block_invoke
-[CSSmartSiriVolume _startListenWithCompletion:]_block_invoke_2
-[CSSmartSiriVolume _startListenWithCompletion:]_block_invoke
-[CSSmartSiriVolume _startListenWithCompletion:]
-[CSSmartSiriVolume _stopListening]
-[CSSmartSiriVolume _stopListening]_block_invoke
-[CSSmartSiriVolume initializeMediaPlayingState]_block_invoke
playing
NOT playing
-[CSSmartSiriVolume initializeMediaPlayingState]
-[CSSmartSiriVolume initializeAlarmState]_block_invoke
firing
NOT firing
-[CSSmartSiriVolume initializeTimerState]_block_invoke
-[CSSmartSiriVolume _setAsset:]
-[CSSmartSiriVolume _processAudioChunk:soundType:]
v16@?0Q8
-[CSSmartSiriVolume estimateSoundLevelbySoundType:]_block_invoke
-[CSSmartSiriVolume _pauseSSVProcessing]_block_invoke
-[CSSmartSiriVolume _resumeSSVProcessing]_block_invoke
-[CSSmartSiriVolume audioStreamProvider:audioBufferAvailable:]_block_invoke
-[CSSmartSiriVolume audioStreamProvider:didStopStreamUnexpectly:]
-[CSSmartSiriVolume voiceTriggerDidDetectKeyword:deviceId:]
-[CSSmartSiriVolume voiceTriggerDidDetectKeyword:deviceId:]_block_invoke
-[CSSmartSiriVolume estimatedTTSVolumeForNoiseLevelAndLKFS:LKFS:]_block_invoke
-[CSSmartSiriVolume _combineResultsWithOptimalFromNoise:andOptimalFromLkfs:withUserOffset:]
-[CSSmartSiriVolume CSMediaPlayingMonitor:didReceiveMediaPlayingChanged:]_block_invoke
-[CSSmartSiriVolume CSAlarmMonitor:didReceiveAlarmChanged:]_block_invoke
-[CSSmartSiriVolume CSTimerMonitor:didReceiveTimerChanged:]_block_invoke
-[CSSmartSiriVolume CSSiriEnabledMonitor:didReceiveEnabled:]
-[CSSmartSiriVolume CSAudioServerCrashMonitorDidReceiveServerRestart:]
-[CSSmartSiriVolume siriClientBehaviorMonitor:didStartStreamWithContext:successfully:option:]_block_invoke
-[CSSmartSiriVolume _setStartAnalyzeTime:]
audioStream
T@"CSAudioStream",&,N,V_audioStream
listenPollingTimer
T@"NSObject<OS_dispatch_source>",&,N,V_listenPollingTimer
listenPollingTimerCount
Tq,N,V_listenPollingTimerCount
T@"<CSSmartSiriVolumeDelegate>",W,N,V_delegate
allocator<T>::allocate(size_t n) 'n' exceeds maximum supported size
-[CSAudioPreprocessor resetWithSampleRate:containsVoiceTrigger:voiceTriggerInfo:]
-[CSAudioPreprocessor flush]
sampleRate
Tf,N,V_sampleRate
upsampler
T@"CSAudioSampleRateConverter",&,N,V_upsampler
zeroFilter
T@"CSVoiceTriggerAwareZeroFilter",&,N,V_zeroFilter
beepCanceller
T@"CSBeepCanceller",&,N,V_beepCanceller
zeroCounter
T@"CSAudioZeroCounter",&,N,V_zeroCounter
T@"<CSAudioPreprocessorDelegate>",W,N,V_delegate
allowVoiceTriggerAssetDownloading
TB,N,V_allowVoiceTriggerAssetDownloading
allowEndpointAssetDownloading
TB,N,V_allowEndpointAssetDownloading
allowLanguageDetectorAssetDownloading
TB,N,V_allowLanguageDetectorAssetDownloading
com.apple.assistant
Siri Global
 - %@
CSKeychainValueForAccountAndKey
tdPsrCanProcessRequest
TB,R,N,V_tdPsrCanProcessRequest
tdSATModelFilePath
T@"NSString",R,N,V_tdSATModelFilePath
getSATVectorCount
Tq,R,N,V_getSATVectorCount
lastRequestSatScore
Tf,R,N,V_lastRequestSatScore
speakerVector
T@"NSData",&,N,V_speakerVector
CSBluetoothManager Queue
-[CSBluetoothManager getBTLocalDeviceWithCompletion:]
-[CSBluetoothManager getBTLocalDeviceWithCompletion:]_block_invoke
v16@?0^{BTLocalDeviceImpl=}8
-[CSBluetoothManager _getWirelessSplitterInfoFromLocalDevice:]
-[CSBluetoothManager _detachBluetoothSession]
-[CSBluetoothManager _attachBluetoothSession]
CSBluetoothManager
-[CSBluetoothManager _sessionAttached:result:]
-[CSBluetoothManager _sessionDetached:]
-[CSBluetoothManager _sessionTerminated:]
-[CSBluetoothManager _setUpLocalDevice]
bluetoothSession
T^{BTSessionImpl=},N,V_bluetoothSession
isAttachingBluetoothSession
TB,N,V_isAttachingBluetoothSession
localDevice
T^{BTLocalDeviceImpl=},N,V_localDevice
pairedDeviceAddresses
T@"NSArray",&,N,V_pairedDeviceAddresses
connectedDeviceAddresses
T@"NSArray",&,N,V_connectedDeviceAddresses
bluetoothSessionSetupGroup
T@"NSObject<OS_dispatch_group>",&,N,V_bluetoothSessionSetupGroup
numberOfBaseProfileUtterancesToUpload
TQ,R,N,V_numberOfBaseProfileUtterancesToUpload
kVTSiriAssertionEnabledDarwinNotification
kVTSiriAssertionDisabledDarwinNotification
CSSiriAssertionMonitor queue
-[CSSiriAssertionMonitor init]
-[CSSiriAssertionMonitor _stopMonitoring]
-[CSSiriAssertionMonitor enableAssertionReceived]_block_invoke
-[CSSiriAssertionMonitor disableAssertionReceived]_block_invoke
type
-[CSSiriAssertionMonitor handleXPCMessage:messageBody:client:]
CSAudioFileReader Queue
-[CSAudioFileReader initWithURL:]
-[CSAudioFileReader prepareRecording:]
-[CSAudioFileReader startRecording]
-[CSAudioFileReader _readAudioBufferAndFeed]
-[CSAudioFileReader stopRecording]
T@"<CSAudioFileReaderDelegate>",W,N,V_delegate
nohash
((?:[a-z]|[0-9])*)\.asset
+[CSUtils(ResourcePathHash) assetHashInResourcePath:]
CSAudioRouteChangeMonitorImplWatch queue
-[CSAudioRouteChangeMonitorImplWatch activeAudioRouteDidChange:]_block_invoke
-[CSAudioRouteChangeMonitorImplWatch _stopMonitoring]
-[CSAudioRouteChangeMonitorImplWatch _notifyHearstConnectionState:]
-[CSAudioSampleRateConverter _createSampleRateConverterWithInASBD:outASBD:]
/BuildRoot/Library/Caches/com.apple.xbs/Sources/CoreSpeech_Sim/CoreSpeech-353.1.3/CoreSpeech/CSAudioSampleRateConverter.m
Too many buffers
i32@?0^I8^{AudioBufferList=I[1{AudioBuffer=II^v}]}16^^{AudioStreamPacketDescription}24
-[CSAudioSampleRateConverter convertSampleRateOfBuffer:]
CSLanguageDetectorAssetMonitor
v24@?0@"NSArray"8@"NSError"16
-[CSLanguageDetectorAssetMonitor startMonitor]_block_invoke
en-US
-[CSLanguageDetectorAssetMonitor _supportedLocale:]_block_invoke
v24@?0@"CSAsset"8@"NSError"16
notifyToken
Ti,N,V_notifyToken
T@"<CSLanguageDetectorAssetMonitorDelegate>",W,N,V_delegate
com.apple.MobileAsset.LanguageDetectorAssets.ma.new-asset-installed
-[NSString(XPCObject) _cs_initWithXPCObject:]
RemoteVAD Align Queue
-[CSOpportuneSpeakListener startListenWithOption:completion:]
-[CSOpportuneSpeakListener _startRequestWithCompletion:]_block_invoke
-[CSOpportuneSpeakListener _startRequestWithCompletion:]
-[CSOpportuneSpeakListener stopListenWithStateReset:completion:]_block_invoke
-[CSOpportuneSpeakListener stopListenWithStateReset:completion:]
-[CSOpportuneSpeakListener audioStreamProvider:audioBufferAvailable:]
-[CSOpportuneSpeakListener spgEndpointAnalyzer:hasSilenceScoreEstimate:]
spgEndpointAnalyzer
T@"CSSPGEndpointAnalyzer",&,N,V_spgEndpointAnalyzer
remoteVADSPGRatio
Ti,N,V_remoteVADSPGRatio
audioStreamProvider
T@"<CSAudioStreamProviding>",&,N,V_audioStreamProvider
audioSessionProvider
T@"<CSAudioSessionProviding>",&,N,V_audioSessionProvider
latestContext
T@"CSAudioRecordContext",&,N,V_latestContext
isMediaPlayingNow
TB,V_isMediaPlayingNow
remoteVADAlignBuffer
T@"NSMutableArray",&,N,V_remoteVADAlignBuffer
remoteVADAlignCount
TQ,N,V_remoteVADAlignCount
alignBufferQueue
T@"NSObject<OS_dispatch_queue>",&,N,V_alignBufferQueue
audioFileWriter
T@"CSPlainAudioFileWriter",&,N,V_audioFileWriter
T@"<CSOpportuneSpeakListenerDelegate>",W,N,V_delegate
com.apple.corespeech.corespeechd.voiceid.xpc
v16@?0@"NSObject<OS_xpc_object>"8
-[CSVoiceIdXPCClient _handleListenerEvent:]
-[CSVoiceIdXPCClient _handleListenerError:]
utterencePath
audioDevice
audioRecord
voiceTriggerEventInfo
otherCtxt
body
result
resultErrorDomain
resultErrorCode
xpcConnection
T@"NSObject<OS_xpc_object>",&,N,V_xpcConnection
sysConfigFilepath
T@"NSString",R,N,V_sysConfigFilepath
spIdType
satScoreThreshold
NviError
-[CSNNVADEndpointAnalyzer init]
-[CSNNVADEndpointAnalyzer preheat]
-[CSNNVADEndpointAnalyzer reset]
-[CSNNVADEndpointAnalyzer processAudioSamplesAsynchronously:]
-[CSNNVADEndpointAnalyzer recordingStoppedForReason:]
-[CSNNVADEndpointAnalyzer stopEndpointer]
-[CSNNVADEndpointAnalyzer resetForNewRequestWithSampleRate:recordContext:recordSettings:]
endpointStyle
Tq,N
delay
Td,N
startWaitTime
automaticEndpointingSuspensionEndTime
minimumDurationForEndpointer
lastEndOfVoiceActivityTime
Td,R,N
lastStartOfVoiceActivityTime
bypassSamples
endpointMode
interspeechWaitTime
endWaitTime
saveSamplesSeenInReset
TB,N
T@"<CSEndpointAnalyzerDelegate>",W,N
implDelegate
T@"<CSEndpointAnalyzerImplDelegate>",W,N
canProcessCurrentRequest
activeChannel
TQ,N
endpointerModelVersion
T@"NSString",R,N
elapsedTimeWithNoSpeech
Tq,N,VendpointStyle
Td,N,Vdelay
Td,N,VstartWaitTime
Td,N,VautomaticEndpointingSuspensionEndTime
Td,N,VminimumDurationForEndpointer
Td,R,N,VlastEndOfVoiceActivityTime
Td,R,N,VlastStartOfVoiceActivityTime
T@"<CSEndpointAnalyzerDelegate>",W,N,Vdelegate
T@"<CSEndpointAnalyzerImplDelegate>",W,N,VimplDelegate
TB,R,N,VcanProcessCurrentRequest
TQ,N,VactiveChannel
com.apple.corespeech.corespeechd.activation.xpc
-[CSActivationXPCClient dealloc]
-[CSActivationXPCClient _handleListenerEvent:]
-[CSActivationXPCClient _handleListenerError:]
-[CSActivationXPCClient notifyActivationEvent:completion:]
event
T@"<CSLanguageDetectorDelegate>",W,N,V_delegate
+[CSAudioRecordContext defaultContext]
CSAudioRecordTypeUnspecified
CSAudioRecordTypeHomePress
CSAudioRecordTypeWiredHeadsetButtonPress
CSAudioRecordTypeBluetoothHeadSetButtonPress
CSAudioRecordTypeUIButtonPress
CSAudioRecordTypeServerInvoke
CSAudioRecordTypeVoiceTrigger
CSAudioRecordTypeStark
CSAudioRecordTypeTVRemote
CSAudioRecordTypeRaiseToSpeak
CSAudioRecordTypeHearstDoubleTap
CSAudioRecordTypeHearstVoice
CSAudioRecordTypeJarvis
CSAudioRecordTypePost
CSAudioRecordTypeDictation
CSAudioRecordTypeVoiceTriggerTraining
CSAudioRecordTypeOpportuneSpeaker
CSAudioRecordTypeUnknown
recordType[%@] deviceId[%@] alwaysUseBuiltInMic[%d]
Tq,N,V_type
T@"NSString",&,N,V_deviceId
TB,N,V_alwaysUseRemoteBuiltInMic
xpcObject
T@"NSObject<OS_xpc_object>",R,N
alwaysUseRemoteBuiltInMic
deviceId
+[CSUtils(LanguageCode) getSiriLanguageWithFallback:]
CSStopRecordingReasonDefault
CSStopRecordingForClientEndpoint
CSStopRecordingForServerEndpoint
CSStopRecordingForReleaseAudioSession
CSStopRecordingForRequestCancellation
stopRecordingReason
TQ,N,V_stopRecordingReason
baseProfileConfidenceScoreThreshold
TQ,R,N,V_baseProfileConfidenceScoreThreshold
implicitConfidenceScoreThreshold
TQ,R,N,V_implicitConfidenceScoreThreshold
implicitDeltaConfidenceScoreThreshold
TQ,R,N,V_implicitDeltaConfidenceScoreThreshold
maxNumberOfBaseProfileUtterances
TQ,R,N,V_maxNumberOfBaseProfileUtterances
satVTImplicitThreshold
Tf,R,N,V_satVTImplicitThreshold
triggerEndMachTime
-[CSMyriadSelfTriggerCoordinator selfTriggerDetector:didDetectSelfTrigger:]
com.apple.siri.corespeech.selftrigger
T@"<CSMyriadSelfTriggerCoordinatorDelegate>",W,N,V_delegate
com.apple.VoiceTriggerUI.TrainingSessionQueue
com.apple.VoiceTriggerUI.TrainingManager
-[CSVTUITrainingManager setLocaleIdentifier:]
-[CSVTUITrainingManager createKeywordDetector]
-[CSVTUITrainingManager prepareWithCompletion:]_block_invoke
-[CSVTUITrainingManager cleanupWithCompletion:]
-[CSVTUITrainingManager cleanupWithCompletion:]_block_invoke
-[CSVTUITrainingManager trainUtterance:shouldUseASR:completion:]
-[CSVTUITrainingManager trainUtterance:shouldUseASR:completion:]_block_invoke_2
-[CSVTUITrainingManager trainUtterance:shouldUseASR:completion:]_block_invoke
-[CSVTUITrainingManager cancelTrainingForID:]
-[CSVTUITrainingManager closeSessionBeforeStartWithStatus:successfully:withCompletion:]
-[CSVTUITrainingManager _shouldShowHeadsetDisconnectionMessage]
-[CSVTUITrainingManager _startAudioSession]
-[CSVTUITrainingManager setSuspendAudio:]
-[CSVTUITrainingManager setSuspendAudio:]_block_invoke
-[CSVTUITrainingManager CSVTUITrainingSessionStopListen]
-[CSVTUITrainingManager CSVTUITrainingSession:hasTrainUtterance:languageCode:payload:]
Tf,V_rms
T@"<CSVTUITrainingManagerDelegate>",W,N,V_delegate
speechRecognizerAvailable
TB,R,V_speechRecognizerAvailable
audioSource
suspendAudio
profileUpdateFailedExplicitUttScore
profileUpdateDiscardImplicitUttScore
profileUpdateExplicitUttScore
profileUpdateImplicitUttScore
profileUpdateNumPrunedUttsPHS
profileUpdateNumDiscardedUttsPHS
profileUpdateNumRetainedUttsPHS
xx_XX
unknown
com.apple.corespeech.aggregator
uptimeSeconds
-[CSAggregator _logUptime]
downtimeSeconds
TdPsrSATDetectionTimedOut
TdPsrSATDetectionTimeMs
TdPsrFailedDuringSATDetection
TdPsrFailedDuringSATRetraining
profileUpdateFailCode
profileUpdateScoreMSE
TdPsrSATRetrainingTimedOut
TdPsrSATRetrainingWaitTimeMs
%@.%@
com.apple.voicetrigger
%@.%@.%@
-[CSAggregator _addValueForScalarKey:withValue:]
-[CSAggregator _pushValueForDistributionKey:withValue:]
ADClientAddValueForScalarKey
/System/Library/PrivateFrameworks/AggregateDictionary.framework/AggregateDictionary
ADClientPushValueForDistributionKey
self ENDSWITH '.wav'
-[CSSpeakerModel discard]
modelPath
utteranceDirectory
tiModelPath
tiUtteranceDirectory
tdtiModelPath
tdtiUtteranceDirectory
enrollmentUtterance
T@"NSArray",R,N
isValid
needsRetrain
-[CSCommandControlStreamEventMonitor isStreaming]
/BuildRoot/Library/Caches/com.apple.xbs/Sources/CoreSpeech_Sim/CoreSpeech-353.1.3/CoreSpeech/CSSpeechControllerMonitor.m
Subclasses need to overwrite this method (_startMonitoringWithQueue)
Subclasses need to overwrite this method (_stopMonitoring)
Subclasses need to overwrite this method (notifySpeechControllerRecordStateChange:withEventUUID:
recordState
TQ,N,V_recordState
lastSpeakerIdInfo
-[CSSelectiveChannelAudioFileWriter initWithURL:inputFormat:outputFormat:channelBitset:]
-[CSSelectiveChannelAudioFileWriter addSamples:numSamples:]
fileURL
T@"NSURL",R,N,V_fileURL
numberOfChannels
TI,R,N,V_numberOfChannels
audioSessionState
TQ,N,GgetAudioSessionState,V_audioSessionState
com.apple.corespeech.corespeechservices
com.apple.corespeech.xpc
+[CSCoreSpeechServices installedVoiceTriggerAssetForLanguageCode:completion:]_block_invoke
reason
CoreSpeechXPC service invalidated
+[CSCoreSpeechServices fetchRemoteVoiceTriggerAssetForLanguageCode:completion:]_block_invoke
+[CSCoreSpeechServices voiceTriggerRTModelForVersion:minorVersion:downloadedModels:preinstalledModels:completion:]
+[CSCoreSpeechServices voiceTriggerRTModelForVersion:minorVersion:downloadedModels:preinstalledModels:completion:]_block_invoke
+[CSCoreSpeechServices voiceTriggerJarvisLanguageList:jarvisSelectedLanguage:completion:]_block_invoke
+[CSCoreSpeechServices voiceTriggerJarvisLanguageList:jarvisSelectedLanguage:completion:]
+[CSCoreSpeechServices requestUpdatedSATAudio]_block_invoke
+[CSCoreSpeechServices getFirstPassRunningMode]_block_invoke
v16@?0q8
T@"NSString",C,N,V_deviceId
fileUrl
T@"NSURL",&,N,V_fileUrl
aesKey
T@"NSData",&,N,V_aesKey
readBuffer
T@"NSData",&,N,V_readBuffer
sampleByteDepth
TQ,N,V_sampleByteDepth
CSSpeechManager Asset Query Queue
-[CSSpeechManager startManager]
-[CSSpeechManager registerSpeechController:]
-[CSSpeechManager registerSiriClientProxy:]
-[CSSpeechManager registerVolumeController:]
v32@?0@"NSNumber"8@"CSAudioProvider"16^B24
-[CSSpeechManager audioProviderWithContext:error:]
-[CSSpeechManager audioProviderWithContext:error:]_block_invoke
-[CSSpeechManager audioProviderWithStreamID:]_block_invoke
-[CSSpeechManager _getAudioRecorderWithError:]
-[CSSpeechManager audioProviderInvalidated:streamHandleId:]_block_invoke
-[CSSpeechManager audioRecorderWillBeDestroyed:]_block_invoke
-[CSSpeechManager voiceTriggerAssetHandler:didChangeCachedAsset:]
-[CSSpeechManager _createClearLoggingFileTimer]
-[CSSpeechManager _createClearLoggingFileTimer]_block_invoke
-[CSSpeechManager _startClearLoggingFilesTimer]
assetQueryQueue
T@"NSObject<OS_dispatch_queue>",&,N,V_assetQueryQueue
audioRecorder
T@"CSAudioRecorder",&,N,V_audioRecorder
audioProviders
T@"NSMutableDictionary",&,N,V_audioProviders
fallbackAudioSessionReleaseProvider
T@"CSFallbackAudioSessionReleaseProvider",&,N,V_fallbackAudioSessionReleaseProvider
clientController
T@"<CSSpeechManagerDelegate>",W,N,V_clientController
volumeClientController
T@"<CSSmartSiriVolumeDelegate>",W,N,V_volumeClientController
voiceTriggerImplicitTraining
T@"CSSpIdImplicitTraining",&,N,V_voiceTriggerImplicitTraining
clearLoggingFileTimer
T@"NSObject<OS_dispatch_source>",&,N,V_clearLoggingFileTimer
clearLoggingFileTimerCount
Tq,N,V_clearLoggingFileTimerCount
opportuneSpeakListnerTestService
T@"CSOpportuneSpeakListnerTestService",&,N,V_opportuneSpeakListnerTestService
T@"CSSmartSiriVolume",R,N,V_smartSiriVolume
-[CSSpeechEndHostTimeEstimator notifyTrailingSilenceDurationAtEndpoint:]
-[CSSpeechEndHostTimeEstimator estimatedSpeechEndHostTime]
numAudioSampleForwarded
TQ,N,V_numAudioSampleForwarded
lastAudioChunkHostTime
TQ,N,V_lastAudioChunkHostTime
endPointNotified
TB,N,V_endPointNotified
trailingSilenceDurationAtEndpoint
Td,N,V_trailingSilenceDurationAtEndpoint
hybridendpointer_searchfield_dictation.json
CSCommandControlListener
-[CSCommandControlListener startListenWithOption:completion:]
-[CSCommandControlListener _startRequestWithCompletion:]_block_invoke
-[CSCommandControlListener _startRequestWithCompletion:]
-[CSCommandControlListener stopListenWithCompletion:]
-[CSCommandControlListener stopListenWithCompletion:]_block_invoke
-[CSCommandControlListener audioStreamProvider:didStopStreamUnexpectly:]_block_invoke
-[CSCommandControlListener CSXPCClient:didDisconnect:]_block_invoke
T@"<CSCommandControlListenerDelegate>",W,N,V_delegate
satTriggered
firstPassTriggerSource
ApplicationProcessor
com.apple.voicetrigger.PHSProfileDownloadTrigger
VoiceProfileAvailabilityMetaBlobVersion
com.apple.cs.profileManager
Library
VoiceTrigger/SAT
-[CSVoiceProfileManager notifyImplicitTrainingUtteranceAvailable:forVoiceProfileId:withRecordDeviceInfo:withRecordCtxt:withVoiceTriggerCtxt:withOtherCtxt:withCompletion:]
-[CSVoiceProfileManager notifyUserVoiceProfileDownloadReadyForUser:getData:completion:]_block_invoke
Missing downloadTriggerBlock - Bailing out
Unknown device category for device type %@ - Bailing out
-[CSVoiceProfileManager notifyUserVoiceProfileUpdateReady]_block_invoke
v32@?0@"NSString"8Q16^B24
Enable VoiceTrigger Upon VoiceProfile Sync For Language
-[CSVoiceProfileManager _downloadAndEnrollVoiceProfileForProfileId:withDownloadTriggerBlock:]_block_invoke_2
-[CSVoiceProfileManager _downloadAndEnrollVoiceProfileForProfileId:withDownloadTriggerBlock:]_block_invoke
v24@?0@"NSError"8@"NSString"16
@"NSError"16@?0Q8
-[CSVoiceProfileManager _downloadAndEnrollVoiceProfileForProfileId:withDownloadTriggerBlock:]
SAT download path is nil - Bailing out
-[CSVoiceProfileManager _downloadVoiceProfileForProfileId:forDeviceCategory:withDownloadTriggerBlock:withCompletion:]
Download for %@ failed with %@
Failed to get contents of %@ with error %@
-[CSVoiceProfileManager _enrollVoiceProfileForSiriProfileId:fromCacheDirectoryPath:withCategoryType:]
Skipping profile Update for %@ in %@
Failed to enroll user - %@
SourcePath (%@) or DestinationPath (%@) is nil - Bailing out
Source directory %@ doesnt exist
-[CSVoiceProfileManager _copyVoiceProfileFromSrcDir:toDestDir:]
Profile %@ with mismatch product category - Skipping
Error to copy profile from %@ to %@, error: %@
Migrated language %@ for %@ but failed to mark SAT enrollment
-[CSVoiceProfileManager _markVoiceProfileMigrationCompleteForSiriProfileId:forLanguageCode:]
Failed to mark migrated for %@ in language %@
-[CSVoiceProfileManager _enableVoiceTriggerIfLanguageMatches:]
Caches/VoiceTrigger/SATUpdate
Caches/VoiceTrigger/SATUpdateNewerZone
_%d_%d
-[CSVoiceProfileManager _getUserVoiceProfileDownloadCacheDirectoryWithUpdatePath:]
-[CSVoiceProfileManager notifyUserVoiceProfileUploadCompleteForSiriProfileId:withError:]_block_invoke
-[CSVoiceProfileManager uploadUserVoiceProfileForSiriProfileId:withUploadTrigger:completion:]_block_invoke
@"NSError"24@?0@"CSVoiceProfileContext"8@"NSString"16
-[CSVoiceProfileManager notifyUserVoiceProfileUploadComplete]_block_invoke
audio
v32@?0Q8Q16Q24
-[CSVoiceProfileManager _stageVoiceProfileWithSiriProfileId:fromPath:withForceUpload:]
-[CSVoiceProfileManager _getVoiceProfilePathsToBeUploadedForSiriProfileId:]
-[CSVoiceProfileManager _copySATFilesFromPath:toPath:withCompletion:]
audiocache
Failed to copy to SATUpload Diretory : %@
v24@?0@"NSError"8Q16
-[CSVoiceProfileManager _copyVoiceProfileAtPath:toPath:]
ERR: Number of training utterances copied from %@ to %@ is too less %ld
Cannot delete existing SATUpload Diretory : %@
-[CSVoiceProfileManager _prepareVoiceProfileWithSiriProfileId:withUploadBlock:]
Cannot create SAT Upload Directory : %@
Failed to upload %@ with error %@ - Bailing out
-[CSVoiceProfileManager _markVoiceProfileTrainingSyncForLanguage:]
Enable VoiceProfile Training Sync For Language
-[CSVoiceProfileManager _isMarkedForVoiceProfileTrainingSyncForLanguage:]
-[CSVoiceProfileManager hasVoiceProfileIniCloudForLanguageCode:withBackupMetaBlob:]
ERR: Unknown product type. Returning false, language: %@
-[CSVoiceProfileManager isVoiceProfileUploadedToiCloudForLanguageCode:withCompletionBlock:]
ERR: Unknown device-category for device: %@, languageCode: %@
-[CSVoiceProfileManager isVoiceProfileUploadedToiCloudForLanguageCode:withCompletionBlock:]_block_invoke
v24@?0@"NSDictionary"8@"NSError"16
-[CSVoiceProfileManager hasVoiceProfileIniCloudForLanguageCode:]
-[CSVoiceProfileManager enableVoiceTriggerUponVoiceProfileSyncForLanguage:]
-[CSVoiceProfileManager devicesWithVoiceProfileIniCloudForLanguage:]
-[CSVoiceProfileManager devicesWithVoiceProfileIniCloudForLanguage:]_block_invoke
v16@?0@"NSArray"8
Caches/VoiceTrigger/SATLegacyUpload
-[CSVoiceProfileManager markSATEnrollmentSuccessForSiriProfileId:forLanguageCode:]
-[CSVoiceProfileManager _markSATEnrollmentWithMarker:forLanguage:forSiriProfileId:]
Library/Caches/VoiceTrigger
Caches/VoiceTrigger/SATUpload
Caches/VoiceTrigger/SATUploadStaging
-[CSVoiceProfileManager _createAndSendImplicitUtterenceXPCMessage:withRecordDeviceInfo:withRecordCtxt:withVoiceTriggerCtxt:withOtherCtxt:withCompletion:]
%lld
-[CSVoiceProfileManager _createAndSendImplicitUtterenceXPCMessage:withRecordDeviceInfo:withRecordCtxt:withVoiceTriggerCtxt:withOtherCtxt:withCompletion:]_block_invoke
currentDeviceCategory
TQ,N,V_currentDeviceCategory
T@"CSVoiceIdXPCClient",&,N,V_xpcClient
Serial CSAssetManager queue
-[CSAssetManager initWithDownloadOption:]
-[CSAssetManager initWithDownloadOption:]_block_invoke
ENABLED
DISABLED
-[CSAssetManager setAssetDownloadingOption:]_block_invoke
-[CSAssetManager _fetchRemoteMetaData]
-[CSAssetManager _canFetchRemoteAsset:]
-[CSAssetManager CSLanguageCodeUpdateMonitor:didReceiveLanguageCodeChanged:]
-[CSAssetManager _createPeriodicalDownloadTimer]
-[CSAssetManager _createPeriodicalDownloadTimer]_block_invoke
-[CSAssetManager _startPeriodicalDownload]
-[CSAssetManager _stopPeriodicalDownload]
currentLanguageCode
deque
triggerEndSampleCount
isTriggerEvent
totalSampleCount
triggerScore
-[CSVTUITrainingSession closeSessionWithStatus:successfully:]
-[CSVTUITrainingSession closeSessionWithStatus:successfully:complete:]_block_invoke
-[CSVTUITrainingSession suspendTraining]
-[CSVTUITrainingSession suspendTraining]_block_invoke
-[CSVTUITrainingSession resumeTraining]
-[CSVTUITrainingSession resumeTraining]_block_invoke
-[CSVTUITrainingSession setupPhraseSpotter]
-[CSVTUITrainingSession handleAudioInput:]_block_invoke_2
v16@?0@"NSDictionary"8
-[CSVTUITrainingSession handleAudioBufferForVTWithAudioInput:withDetectedBlock:]
-[CSVTUITrainingSession feedSpeechRecognitionTrailingSamplesWithCompletedBlock:]
-[CSVTUITrainingSession trimBeginingOfPCMBufferWithVoiceTriggerEventInfo:]
-[CSVTUITrainingSession audioSessionUnsupportedAudioRoute]
-[CSVTUITrainingSession didDetectBeginOfSpeech]
-[CSVTUITrainingSession didDetectEndOfSpeech:]
PHS explicit training utterance
[%ld] VTUISession Number:[%ld]
-[CSVTUITrainingSession startMasterTimerWithTimeout:]
-[CSVTUITrainingSession handleMasterTimeout:]
-[CSVTUITrainingSession stopMasterTimer]
-[CSVTUITrainingSession speechRecognitionTask:didHypothesizeTranscription:]
-[NviDirectionalitySignalProvider initWithDataSource:assetsProvider:]
-[NviDirectionalitySignalProvider addDelegate:]
-[NviDirectionalitySignalProvider removeDelegate:]
-[NviDirectionalitySignalProvider startWithNviContext:didStartHandler:]
-[NviDirectionalitySignalProvider reset]
-[NviDirectionalitySignalProvider stopWithDidStopHandler:]
sigType
TQ,N,V_activeChannel
T@"<CSKeywordAnalyzerNDEAPIScoreDelegate>",W,N,V_delegate
clientStartSampleCount
triggerFireMachTime
twoShotAudibleFeedbackDelay
BuiltInAlwaysOnProcessor
CSSpeechRecordSettingsKey_AudioSessionActiveDelay
CSSpeechRecordSettingsKey_AudioSessionActiveReason
CSSpeechRecordSettingsKey_LanguageDetectorLocales
CSSpeechRecordSettingsKey_LanguageDetectorDictationLanguages
CSSpeechRecordSettingsKey_LanguageDetectorCurrentKeyboard
CSSpeechRecordSettingsKey_LanguageDetectorWasLanguageToggled
CSSpeechRecordSettingsKey_LanguageDetectorMultilingualKeyboardLanguages
CSSpeechRecordSettingsKey_LanguageDetectorKeyboardConvoLanguagePriors
CSSpeechRecordSettingsKey_LanguageDetectorKeyboardGlobalLanguagePriors
CSSpeechRecordSettingsKey_LanguageDetectorPreviousMessageLanguage
CSSpeechRecordSettingsKey_LanguageDetectorGlobalLastKeyboardUsed
CSSpeechRecordSettingsKey_LanguageDetectorDictationLanguagePriors
CSSpeechRecordSettingsKey_LanguageDetectorConversationalMessages
CSSpeechRecordSettingsKey_disableEndpointer
CSSpeechRecordSettingsKey_DictationRequestOrigin
CSSpeechRecordSettingsKey_isDucking
CSSpeechController ContextReset
com.apple.corespeech.twoShotAudibleFeedback
-[CSSpeechController initializeRecordSessionWithContext:]
-[CSSpeechController prepareRecordWithSettings:error:]
-[CSSpeechController _fetchLastTriggerInfo]
-[CSSpeechController _fetchLastTriggerInfo]_block_invoke_2
v24@?0@"NSDictionary"8@"NSDictionary"16
-[CSSpeechController _activateAudioSessionWithDelay:error:]_block_invoke
-[CSSpeechController _activateAudioSessionWithDelay:error:]
B8@?0
-[CSSpeechController _scheduleActivateAudioSessionWithDelay:forReason:validator:completion:]_block_invoke
-[CSSpeechController _scheduleActivateAudioSessionWithDelay:forReason:validator:completion:]
-[CSSpeechController _cancelPendingAudioSessionActivateForReason:]
-[CSSpeechController _performPendingAudioSessionActivateForReason:]
-[CSSpeechController _activateAudioSession:forRetry:]
-[CSSpeechController setCurrentContext:error:]
-[CSSpeechController preheat]
-[CSSpeechController prewarmAudioSession]
-[CSSpeechController resetAudioSession]
-[CSSpeechController reset]
-[CSSpeechController releaseAudioSession]
-[CSSpeechController releaseAudioSession:]
-[CSSpeechController _setupSpeakerId]
v32@?0@8#16@?<v@?>24
-[CSSpeechController startRecordingWithSettings:error:]
-[CSSpeechController startRecordingWithSettings:error:]_block_invoke
-[CSSpeechController startRecordingWithSettings:error:]_block_invoke_2
-[CSSpeechController _startPhaticDecision]
-[CSSpeechController _startPhaticDecision]_block_invoke
v16@?0@"NSError"8
-[CSSpeechController stopRecording]
-[CSSpeechController stopRecordingWithOptions:]
-[CSSpeechController recordRoute]
-[CSSpeechController recordDeviceInfo]
-[CSSpeechController playbackRoute]
-[CSSpeechController _didStopForReason:]
-[CSSpeechController audioStreamProvider:didStopStreamUnexpectly:]
-[CSSpeechController audioStreamProvider:audioBufferAvailable:]_block_invoke
-[CSSpeechController audioStreamProvider:audioChunkForTVAvailable:]_block_invoke
-[CSSpeechController audioStreamProvider:didHardwareConfigurationChange:]
-[CSSpeechController audioStreamProvider:didHardwareConfigurationChange:]_block_invoke
-[CSSpeechController audioSessionProvider:providerInvalidated:]_block_invoke_2
-[CSSpeechController audioSessionProvider:didChangeContext:]
-[CSSpeechController audioSessionController:didReceiveAudioSessionMediaServicesWereResetNotificationWithUserInfo:]
-[CSSpeechController audioAlertProvidingDidFinishAlertPlayback:ofType:error:]
-[CSSpeechController audioAlertProvidingDidFinishAlertPlayback:ofType:error:]_block_invoke
-[CSSpeechController audioSessionProviderBeginInterruption:]
-[CSSpeechController audioSessionProviderBeginInterruption:]_block_invoke
-[CSSpeechController audioSessionProviderBeginInterruption:withContext:]
-[CSSpeechController audioSessionProviderBeginInterruption:withContext:]_block_invoke
-[CSSpeechController audioSessionProviderEndInterruption:]
-[CSSpeechController audioSessionProviderEndInterruption:]_block_invoke
-[CSSpeechController audioSessionProvider:willSetAudioSessionActive:]
-[CSSpeechController audioSessionProvider:willSetAudioSessionActive:]_block_invoke
-[CSSpeechController audioSessionProvider:didSetAudioSessionActive:]
-[CSSpeechController audioSessionProvider:didSetAudioSessionActive:]_block_invoke
-[CSSpeechController smartSiriVolumeControllerDetectedSystemVolumeChange:withVolume:forReason:]
-[CSSpeechController smartSiriVolumeControllerDetectedSystemVolumeChange:withVolume:forReason:]_block_invoke
-[CSSpeechController audioConverterDidConvertPackets:packets:durationInSec:timestamp:]
-[CSSpeechController setAlertSoundFromURL:forType:]
-[CSSpeechController playAlertSoundForType:]
-[CSSpeechController playRecordStartingAlertAndResetEndpointer]
-[CSSpeechController stopEndpointer]
-[CSSpeechController setMeteringEnabled:]
-[CSSpeechController _createAudioPowerMeterIfNeeded]
-[CSSpeechController outputReferenceChannel]
-[CSSpeechController voiceTriggerInfo]
-[CSSpeechController voiceTriggerDidDetectKeyword:deviceId:]_block_invoke
-[CSSpeechController keywordDetectorDidDetectKeyword]_block_invoke
-[CSSpeechController _fetchAudioProviderWithContext:]
-[CSSpeechController _createAudioProviderFromXPCWithContext:]
Accounts
Speech Identifier
%c%c%c%c
none
-[CSSpeechController cancelCurrentLanguageDetectorRequest]_block_invoke
-[CSSpeechController beginWaitingForMyriad]
-[CSSpeechController endWaitingForMyriadWithDecision:]
-[CSSpeechController CSMediaPlayingMonitor:didReceiveMediaPlayingChanged:]_block_invoke
-[CSSpeechController CSAlarmMonitor:didReceiveAlarmChanged:]_block_invoke
-[CSSpeechController CSTimerMonitor:didReceiveTimerChanged:]_block_invoke
-[CSSpeechController _setSoundPlayingState]
 NOT
-[CSSpeechController speakerRecognizer:hasSpeakerIdInfo:]_block_invoke
-[CSSpeechController speakerRecognizerFinishedProcessing:withFinalSpeakerIdInfo:]_block_invoke
-[CSSpeechController CSXPCClient:didDisconnect:]_block_invoke
-[CSSpeechController _teardownAudioProviderIfNeeded]
-[CSSpeechController _tearDownBargeInModeProviderIfNeeded]
endpointerProxy
T@"CSEndpointerProxy",&,N,V_endpointerProxy
audioRecordContext
T@"CSAudioRecordContext",&,N,V_audioRecordContext
streamProvider
T@"<CSAudioStreamProviding>",&,N,V_streamProvider
sessionProvider
T@"<CSAudioSessionProviding>",&,N,V_sessionProvider
alertProvider
T@"<CSAudioAlertProviding>",&,N,V_alertProvider
audioMeterProvider
T@"<CSAudioMeterProviding>",&,N,V_audioMeterProvider
audioMetricProvider
T@"<CSAudioMetricProviding>",&,N,V_audioMetricProvider
bargeInModeProvider
T@"<CSBargeInModeProviding>",&,N,V_bargeInModeProvider
isOpus
TB,N,V_isOpus
isActivated
TB,N,V_isActivated
isNarrowBand
TB,N,V_isNarrowBand
serverLoggingWriter
T@"CSSelectiveChannelAudioFileWriter",&,N,V_serverLoggingWriter
volumeController
T@"CSSmartSiriVolumeController",&,N,V_volumeController
spIdFactory
T@"CSSpeakerIdRecognizerFactory",&,N,V_spIdFactory
spIdRecognizer
T@"<CSSpIdSpeakerRecognizer>",&,N,V_spIdRecognizer
spIdUserScores
T@"NSDictionary",&,N,V_spIdUserScores
voiceProfileStore
T@"CSUserVoiceProfileStore",&,N,V_voiceProfileStore
twoShotNotificationEnabled
TB,N,V_twoShotNotificationEnabled
TB,N,V_isMediaPlaying
TB,N,V_isAlarmPlaying
TB,N,V_isTimerPlaying
isSoundPlaying
TB,N,V_isSoundPlaying
isRemoteVADAvailableStream
TB,N,V_isRemoteVADAvailableStream
myriadPreventingTwoShotFeedback
TB,N,V_myriadPreventingTwoShotFeedback
needsPostGain
TB,N,V_needsPostGain
speechEndHostTimeEstimator
T@"CSSpeechEndHostTimeEstimator",&,N,V_speechEndHostTimeEstimator
languageDetector
T@"CSLanguageDetector",&,N,V_languageDetector
shouldUseLanguageDetectorForCurrentRequest
TB,N,V_shouldUseLanguageDetectorForCurrentRequest
pendingAudioSessionActivationToken
T@"NSUUID",&,N,V_pendingAudioSessionActivationToken
pendingAudioSessionActivationCompletion
T@?,C,N,V_pendingAudioSessionActivationCompletion
audioSessionActivationDelay
Td,N,V_audioSessionActivationDelay
bargeInModeXPCClient
T@"CSXPCClient",&,N,V_bargeInModeXPCClient
cachedAvgPower
Tf,N,V_cachedAvgPower
cachedPeakPower
Tf,N,V_cachedPeakPower
powerMeter
T@"CSAudioPowerMeter",&,N,V_powerMeter
didDeliverLastBuffer
TB,N,V_didDeliverLastBuffer
T@"<CSSpeechControllerDelegate>",W,N,V_delegate
speakerIdDelegate
T@"<CSSpeakerIdentificationDelegate>",W,N,V_speakerIdDelegate
languageDetectorDelegate
T@"<CSLanguageDetectorDelegate>",W,N,V_languageDetectorDelegate
duckOthersOption
endpointAnalyzer
T@"<CSEndpointAnalyzer>",R,N
rtblobs
blob
majorVersion
minorVersion
signature
cert
rtlocalemap
-[CSAsset(RTModel) createRTModelWithLocale:]
-[CSAsset(RTModel) latestHearstRTModelForLocale:]
-[CSAsset(RTModel) hearstRTModelWithMajorVersion:minorVersion:locale:]
-[CSAsset(RTModel) hearstRTModelLocaleMap]
%02x
-[CSVTUITrainingSessionWithPayload _firedVoiceTriggerTimeout]
-[CSVTUITrainingSessionWithPayload _firedEndPointTimeout]
-[CSVTUITrainingSessionWithPayload handleAudioInput:]_block_invoke
-[CSVTUITrainingSessionWithPayload audioSessionDidStartRecording:error:]
-[CSVTUITrainingSessionWithPayload audioSessionDidStopRecording:]
-[CSVTUITrainingSessionWithPayload didDetectBeginOfSpeech]
-[CSVTUITrainingSessionWithPayload didDetectEndOfSpeech:]
-[CSVTUITrainingSessionWithPayload closeSessionWithStatus:successfully:]
-[CSVTUITrainingSessionWithPayload speechRecognitionTask:didHypothesizeTranscription:]_block_invoke
-[CSVTUITrainingSessionWithPayload speechRecognitionTask:didFinishRecognition:]_block_invoke
-[CSVTUITrainingSessionWithPayload speechRecognitionTask:didFinishSuccessfully:]_block_invoke
-[CSVTUITrainingSessionWithPayload matchRecognitionResult:withMatchedBlock:withNonMatchedBlock:]
T@"NSDictionary",&,N,V_voiceTriggerEventInfo
com.apple.voicetrigger.notbackedup
kCSPreferencesJarvisTriggerModeDidChangeDarwinNotification
Phrase Detector Enabled
AttentiveSiri Enabled
AttentiveSiri AudioLogging Enabled
VoiceTrigger CoreSpeech Enabled
-[CSPreferences voiceTriggerInCoreSpeech]_block_invoke
CoreSpeech Daemon Enabled
-[CSPreferences corespeechDaemonEnabled]_block_invoke
Enable Two Shot Notification
com.apple.demo-settings
StoreDemoMode
File Logging Level
Logs/CrashReporter/VoiceTrigger/audio/
/Logs/CrashReporter/Assistant/
SpeechLogs
-[CSPreferences assistantAudioFileLogDirectory]
VoiceTrigger
siriBC
Second Pass Audio Logging Enabled
Jarvis Audio Logging Enabled
Jarvis Trigger Mode
Enable SoS Audio Logging
mobile
Logs/CrashReporter/CoreSpeech/sos/
-[CSPreferences getStartOfSpeechAudioLogFilePath]
/tmp
yyyyMMdd_HHmmss.SSS
%@/%@
Remote VoiceTrigger Delay
Remote VoiceTrigger Endpoint Timeout
VoiceTrigger/interstitial
Myriad File Logging Enabled
-[CSPreferences enableAudioInjection:]
Audio Injection Enabled
-[CSPreferences setAudioInjectionFilePath:]
Audio Injection File Path
-[CSPreferences audioInjectionFilePath]
-[CSPreferences audioInjectionFilePath]_block_invoke
v32@?0@8Q16^B24
SpeakerId Enabled
iTunes Account Signin Enabled
SpeakerId Score Type
SmartSiriVolume SoftVolume Enabled
Audio Session Activation Delay
Max Number Logging Files
Enable SiriActivation HomePod
Enable SiriActivation watchOS
IOS Support Barge-in
-[CSPreferences iOSBargeInSupportEnabled]_block_invoke
enabled
disabled
-[CSPreferences iOSBargeInSupportEnabled]
Overwrite Remote VAD Score
Hearst First Pass Model Version
Hearst Second Pass Model Version
Hearst Fake Model Path
VoiceTrigger Companion Sync Enabled
Enable OpportuneSpeakListener Bypass
CSSampleCountHostTimeConverter
anchorSampleCount
TQ,N,V_anchorSampleCount
anchorHostTime
TQ,N,V_anchorHostTime
-[CSAudioStartStreamOption(AVVC) avvcStartRecordSettingsWithAudioStreamHandleId:]
-[NviSignalProvidersController initWithAssetsProvider:dataSourceMap:signalProviderToDataSourceMap:]
-[NviSignalProvidersController dealloc]
/BuildRoot/Library/Caches/com.apple.xbs/Sources/CoreSpeech_Sim/CoreSpeech-353.1.3/Nvi/SignalProviders/NviSignalProvidersController.m
No DataSource found for SignalType: %@
-[NviSignalProvidersController _setupSignalProviders:]
-[NviSignalProvidersController _startDataSourcesWithContext:]
-[NviSignalProvidersController _startDataSourcesWithContext:]_block_invoke
-[NviSignalProvidersController _startSignalProvidersWithContext:]
-[NviSignalProvidersController _startSignalProvidersWithContext:]_block_invoke
-[NviSignalProvidersController _stopDataSources]_block_invoke
-[NviSignalProvidersController _stopDataSources]
-[NviSignalProvidersController _stopCurrentlyRunningSignalProviders]_block_invoke
-[NviSignalProvidersController _stopCurrentlyRunningSignalProviders]
-[NviSignalProvidersController _iterateSignalMask:withHandler:]
v16@?0@"<NviSignalProvider>"8
assetsProvider
T@"<NviAssetsProvider>",&,N,V_assetsProvider
dataSrcMap
T@"NSDictionary",&,N,V_dataSrcMap
sigProvidersMap
T@"NSMapTable",&,N,V_sigProvidersMap
currActiveSigProvTypes
T@"NSHashTable",&,N,V_currActiveSigProvTypes
currActiveDataSourceTypes
T@"NSHashTable",&,N,V_currActiveDataSourceTypes
com.apple.corespeech.corespeechd.xpc
-[CSXPCClient sendXPCClientType]
xpcClientType
-[CSXPCClient prepareAudioProviderWithContext:clientType:error:]
context
clientType
activateReason
deactivateOption
setDuckOthersOption
enableMiniDucking
alertType
soundPath
alertStartTime
-[CSXPCClient alertStartTime]
alertBehavior
setMeterEnable
channelNumber
power
-[CSXPCClient peakPowerForChannel:]
-[CSXPCClient averagePowerForChannel:]
-[CSXPCClient audioMetric]
audioMetric
audioStreamRequest
-[CSXPCClient audioStreamWithRequest:streamName:error:]
v24@?0@"CSAudioStream"8@"NSError"16
-[CSXPCClient audioStreamWithRequest:streamName:completion:]
-[CSXPCClient prepareAudioStreamSync:request:error:]
-[CSXPCClient prepareAudioStream:request:completion:]
startAudioStreamOption
-[CSXPCClient triggerInfoForContext:completion:]
voiceTriggerInfo
rtsTriggerInfo
-[CSXPCClient enableVoiceTrigger:withAssertion:]
-[CSXPCClient enableVoiceTrigger:withAssertion:]_block_invoke
recordRoute
recordDeviceInfo
recordSettings
playbackRoute
-[CSXPCClient audioChunkFrom:to:]
-[CSXPCClient audioChunkToEndFrom:]
-[CSXPCClient saveRecordingBufferFrom:to:toURL:]
-[CSXPCClient saveRecordingBufferToEndFrom:toURL:]
-[CSXPCClient holdAudioStreamWithDescription:timeout:]
-[CSXPCClient cancelAudioStreamHold:]
-[CSXPCClient isRecording]
sessionID
-[CSXPCClient audioSessionID]
volume
-[CSXPCClient getEstimatedTTSVolume]
alarmState
timerState
sampleCount
replyHostTime
-[CSXPCClient hostTimeFromSampleCount:]
hostTime
replySampleCount
-[CSXPCClient sampleCountFromHostTime:]
option
-[CSXPCClient enableBargeInMode:completion:]
-[CSXPCClient _handleListenerEvent:]
-[CSXPCClient _handleListenerMessage:]
-[CSXPCClient _handleListenerError:]
-[CSXPCClient _handleAlertProvidingDelegateMessageBody:]
didFinishAlertPlayback
errorDomain
errorCode
-[CSXPCClient _handleSessionProvidingDelegateMessageBody:]
interruptionContext
-[CSXPCClient _handleSessionProvidingDelegateBeginInterruptionWithContext:]
willSetAudioSessionActive
didSetAudioSessionActive
streamHandleIdInvalidationflag
didChangeContextFlag
-[CSXPCClient _handleSessionInfoProvidingDelegateMessageBody:]
notificationInfo
-[CSXPCClient _handleSessionInfoProvidingDelegateInterruptionNotification:]
-[CSXPCClient _handleSessionInfoProvidingDelegateRouteChangeNotification:]
-[CSXPCClient _handleSessionInfoProvidingDelegateMediaServicesWereLostNotification:]
-[CSXPCClient _handleSessionInfoProvidingDelegateMediaServicesWereResetNotification:]
-[CSXPCClient _handleStreamProvidingDelegateMessageBody:]
stopReason
chunk
hardwareConfig
audioSessionInfoObservers
T@"NSHashTable",&,N,V_audioSessionInfoObservers
TQ,N,V_xpcClientType
audioSessionProvidingDelegate
T@"<CSAudioSessionProvidingDelegate>",W,N,V_audioSessionProvidingDelegate
audioStreamProvidingDelegate
T@"<CSAudioStreamProvidingDelegate>",W,N,V_audioStreamProvidingDelegate
audioAlertProvidingDelegate
T@"<CSAudioAlertProvidingDelegate>",W,N,V_audioAlertProvidingDelegate
T@"<CSXPCClientDelegate>",W,N,V_delegate
initialState
Tq,N,V_initialState
transitions
T@"NSMutableDictionary",&,N,V_transitions
T@"<CSStateMachineDelegate>",W,N,V_delegate
currentState
Tq,R,N,V_currentState
-[CSPlainAudioFileWriter initWithURL:inputFormat:outputFormat:]
-[CSPlainAudioFileWriter addSamples:numSamples:]
Serial CSEventMonitor queue
-[CSEventMonitor _startMonitoringWithQueue:]
/BuildRoot/Library/Caches/com.apple.xbs/Sources/CoreSpeech_Sim/CoreSpeech-353.1.3/CoreSpeech/CSEventMonitor.m
-[CSEventMonitor _stopMonitoring]
-[CSAudioChunkForTV initWithXPCObject:]
T@"NSArray",&,N,V_packets
Tf,N,V_avgPower
Tf,N,V_peakPower
TQ,N,V_timeStamp
avgPower
peakPower
timeStamp
packets
-[CSAudioFileLog _closeAudioFile]
-[CSAudioFileLog startRecording]_block_invoke
-input.wav
-[CSAudioFileLog appendAudioData:]_block_invoke
-[CSAudioFileLog stopRecording]_block_invoke
+[CSAudioStreamRequest defaultRequestWithContext:]
+[CSAudioStreamRequest requestForLpcmRecordSettingsWithContext:]
+[CSAudioStreamRequest requestForOpusRecordSettingsWithContext:]
+[CSAudioStreamRequest requestForSpeexRecordSettingsWithContext:]
T@"CSAudioRecordContext",&,N,V_recordContext
TB,N,V_requiresHistoricalBuffer
TB,N,V_useCustomizedRecordSettings
Tq,N,V_audioFormat
Td,N,V_sampleRate
TI,N,V_lpcmBitDepth
TB,N,V_lpcmIsFloat
TI,N,V_numberOfChannels
TI,N,V_encoderBitRate
TB,N,V_isSiri
requiresHistoricalBuffer
useCustomizedRecordSettings
audioFormat
lpcmBitDepth
lpcmIsFloat
NumberOfChannels
encoderBitRate
isSiri
recordContext
TQ,R,N,V_spIdType
[%@]
[%llu]
[%f]
BuiltInAOPVoiceTrigger
RemoteMicVoiceTrigger
RemoteMicVAD
TestSetAPMode
TestSetAOPMode
JarvisVoiceTrigger
mediaserverdLaunched
Unknown
UUID
T@"NSString",R,N,V_UUID
TQ,N,V_type
T@"NSDictionary",&,N,V_activationInfo
TQ,N,V_hosttime
Tf,N,V_vadScore
localizedDescription
uuid
activationInfo
vadScore
hosttime
com.apple.MobileAsset.VoiceTriggerAssets
com.apple.MobileAsset.VoiceTriggerAssetsWatch
com.apple.MobileAsset.VoiceTriggerAssetsMarsh
com.apple.MobileAsset.VoiceTriggerAssetsMac
com.apple.MobileAsset.SpeechEndpointAssetsWatch
com.apple.MobileAsset.LanguageDetectorAssets
-[CSAssetController init]
Serial CSAssetController queue
V1 Assets Clean-up queue
-[CSAssetController _cleanUpMobileAssetV1Directory]
-[CSAssetController assetOfType:language:]
-[CSAssetController allInstalledAssetsOfType:language:]
q24@?0@"MAAsset"8@"MAAsset"16
v32@?0@"MAAsset"8Q16^B24
-[CSAssetController assetOfType:language:completion:]
-[CSAssetController installedAssetOfType:language:]
-[CSAssetController installedAssetOfType:language:completion:]
v24@?0@"MAAsset"8@"NSError"16
-[CSAssetController _installedAssetOfType:withLanguage:]
-[CSAssetController _installedAssetOfType:withLanguage:completion:]_block_invoke
-[CSAssetController _findLatestInstalledAsset:]
-[CSAssetController _assetQueryForAssetType:]
-[CSAssetController _runAssetQuery:completion:]
-[CSAssetController _runAssetQuery:completion:]_block_invoke
-[CSAssetController fetchRemoteMetaOfType:]
-[CSAssetController _fetchRemoteAssetOfType:withLanguage:completion:]
-[CSAssetController _fetchRemoteAssetOfType:withLanguage:completion:]_block_invoke_2
-[CSAssetController _fetchRemoteAssetOfType:withLanguage:completion:]_block_invoke
-[CSAssetController _downloadAssetCatalogForAssetType:complete:]_block_invoke
-[CSAssetController _updateFromRemoteToLocalAssets:forAssetType:completion:]
-[CSAssetController _downloadAsset:withComplete:]
v16@?0d8
-[CSAssetController _downloadAsset:withComplete:]_block_invoke
-[CSAssetController _startDownloadingAsset:progress:completion:]
v16@?0@"MAProgressNotification"8
assetsMigrationQueue
T@"NSObject<OS_dispatch_queue>",&,N,V_assetsMigrationQueue
csAssetsDictionary
T@"NSDictionary",&,N,V_csAssetsDictionary
T@"NSMutableDictionary",&,N,V_observers
+[CSAssetController(Utils) addKeyValuePairForQuery:assetType:]
com.apple.corespeech.monitor queue
-[CSSpeechControllerMonitorImpl _startMonitoringWithQueue:]
-[CSSpeechControllerMonitorImpl _notifyObserver:withSpeechControllerRecordState:withEventUUID:]
-[CSSpeechControllerMonitorImpl _notifyObserver:withSpeechControllerRecordState:withEventUUID:]_block_invoke
-[CSSpeechControllerMonitorImpl _notifyObserver:withSpeechControllerRecordState:]
VoiceProfileCompatabiltyVersion
VoiceProfileProductType
VoiceProfileSWVersion
VoiceProfileCategoryType
VoiceProfileIdentifier
spIdAudioProcessedDuration
spIdUnknownUserScore
spIdKnownUserScores
spIdKnownUserRawScores
spIdUserScoresVersion
spIdKnownUserProfileVersions
spIdScoreThresholdingType
spIdVTInvocationScoreThresholdingType
spIdNonVTInvocationScoreThresholdingType
spIdAssetVersion
spIdDirectionSigsArr
shouldRecordUserAudio
shouldRecordPayload
Unknown InvocationStyle: %lu
tdti
tdtiexplicit
tdexplicit
Unknown CSSpIdType: %lu
+[CSUtils(SpeakerId) spIdTypeForString:]
ImplicitAdditionRunMode
ModelQueryRunMode
RetrainingMode
+[CSUtils(SpeakerId) stringForCSSATRunMode:]
LSTM
Unknown CSSpIdModelType: %lu
config_td_spid.txt
config_ti_spid.txt
config_tdti_spid.txt
+[CSUtils(SpeakerId) satConfigFileNameForCSSpIdType:forModelType:]
config_sr_sat.txt
config.txt
trained_users.json
+[CSUtils(SpeakerId) spIdSATDirForLocale:]
spid
+[CSUtils(SpeakerId) spIdSATAudioDirForLocale:spidType:]
td-sr-model
model
+[CSUtils(SpeakerId) spIdSATModelDirForLocale:spidType:modelType:]
spid-imported
+[CSUtils(SpeakerId) createDirectoryIfDoesNotExist:]
Logs/CoreSpeech/spid/
grading
+[CSUtils(SpeakerId) spIdAudioLogsCountLimitReached]
+[CSUtils(SpeakerId) cleanupOrphanedVoiceIdGradingFiles]
v32@?0@"NSString"8@"NSURL"16^B24
+[CSUtils(SpeakerId) cleanupOrphanedVoiceIdGradingFiles]_block_invoke
VoiceProfileCache
AudioFileOpenURL Failed : %@
EARTests
AudioFileOpenURL failed: %@
ExtAudioFileWrapAudioFileID failed: %@
Error reading audio-file: %d
EOF. Num bytes read: %lu
+[CSUtils(SpeakerId) isSpidSupportedInCurrentLanguage]
+[CSUtils(SpeakerId) getVoiceProfileVersionFromVersionFilePath:]
+[CSUtils(SpeakerId) getVoiceProfileIdentityFromVersionFilePath:]
+[CSUtils(SpeakerId) getVoiceProfileProductCategoryFromVersionFilePath:]
+[CSUtils(SpeakerId) checkIfMigrationNecessaryForCompatibilityVersion:forLanguageCode:]
+[CSUtils(SpeakerId) migrateVoiceProfileToVersion:forLanguageCode:]
+[CSUtils(SpeakerId) updateVoiceProfileVersionFileForProfileId:forLanguageCode:]
kCSDeviceCategory_Unknown
kCSDeviceCategory_iOS_NonAop
kCSDeviceCategory_iOS_Aop
kCSDeviceCategory_macOS
kCSDeviceCategory_audioAccessory
kCSDeviceCategory_iOS_Aop_Explicit
iPad3,4
iPad3,5
iPad3,6
iPad4,1
iPad4,2
iPad4,3
iPad4,4
iPad4,5
iPad4,6
iPad4,7
iPad4,8
iPad4,9
iPad5,1
iPad5,2
iPad5,3
iPad5,4
iPad6,7
iPad6,8
iPad6,11
iPad6,12
iPhone5,1
iPhone5,2
iPhone5,3
iPhone5,4
iPhone6,1
iPhone6,2
iPhone7,1
iPhone7,2
iPad
iPhone
Accessory
+[CSUtils(SpeakerId) deviceCategoryForDeviceProductType:]
+[CSUtils(SpeakerId) isCurrentDeviceCompatibleWithNewerVoiceProfileAt:]
+[CSUtils(SpeakerId) isCurrentDeviceCompatibleWithVoiceProfileAt:]
pathExtension='json'
Caches/VoiceTrigger/ImplicitUtterences
+[CSUtils(SpeakerId) getNumberOfAudioFilesInDirectory:]
+[CSUtils(SpeakerId) dumpFilesInDirectory:]
+[CSUtils(SpeakerId) markUploadForVoiceProfile:]
ScheduledUserVoiceProfilesUpload
+[CSUtils(SpeakerId) markVoiceProfileUploaded:]
+[CSUtils(SpeakerId) getContentsOfDirectory:]
+[CSUtils(SpeakerId) getHomeUserIdForVoiceProfile:withCompletion:]
+[CSUtils(SpeakerId) getHomeUserIdForVoiceProfile:withCompletion:]_block_invoke
v24@?0@"NSString"8@"NSError"16
homeUserId query for siriProfileId %@ timedout !
SiriDebugVT
SpeakerIDToGradeData
VoiceProfiles
Caches
+[CSUtils(SpeakerId) mapRawScores:toScoresOfType:withRawScoreOffset:withRawScoreScale:withLogitCeil:withLogitFloor:withSATThreshold:]
+[CSUtils(SpeakerId) spIdMapScoresToSharedSiriID:]
+[CSUtils(SpeakerId) spIdMapIdentifiersToSiriDebugID:]
+[CSUtils(SpeakerId) spIdComposeProfileVersionsFor:]
%d.%d.%d
+[CSUtils(SpeakerId) getEnrollmentUtterancesFromDirectory:]
self.absoluteString ENDSWITH '.wav'
+[CSUtils(SpeakerId) getExplicitEnrollmentUtterancesFromDirectory:]_block_invoke
v32@?0@"NSURL"8Q16^B24
+[CSUtils(SpeakerId) getExplicitOnlyEnrollmentUtterancesFromDirectory:]_block_invoke
+[CSUtils(SpeakerId) getBaseProfileOnlyEnrollmentUtterancesFromDirectory:]_block_invoke
+[CSUtils(SpeakerId) getImplicitEnrollmentUtterancesFromDirectory:]_block_invoke
+[CSUtils(SpeakerId) getImplicitEnrollmentUtterancesPriorTo:forType:forLanguageCode:forProfileID:]
+[CSUtils(SpeakerId) getImplicitEnrollmentUtterancesPriorTo:forType:forLanguageCode:forProfileID:]_block_invoke
+[CSUtils(SpeakerId) removeItemAtPath:]
+[CSUtils(SpeakerId) deleteExistingSATModelForLanguageCode:]
T@"<CSAudioDecoderDelegate>",W,V_delegate
-[CSAudioRouteChangeMonitor _startMonitoringWithQueue:]
/BuildRoot/Library/Caches/com.apple.xbs/Sources/CoreSpeech_Sim/CoreSpeech-353.1.3/CoreSpeech/CSAudioRouteChangeMonitor.m
-[CSAudioRouteChangeMonitor _stopMonitoring]
-[CSAudioRouteChangeMonitor getHearstConnected:]
-[CSAudioRouteChangeMonitor hearstConnected]
-[CSAudioRouteChangeMonitor getJarvisConnected:]
-[CSAudioRouteChangeMonitor jarvisConnected]
CSSmartSiriVolumeEnablePolicy queue
-[CSSmartSiriVolumeEnablePolicy _addSmartSiriVolumeEnabledConditions]_block_invoke
+[CSUtils(AudioHardware) supportIOSBargeIn]_block_invoke
VoiceTrigger Asset Change Monitor
T@"<CSVoiceTriggerAssetChangeDelegate>",W,N,V_delegate
com.apple.corespeech.voicetriggerassetchange
-[NviAudioFileWriter initWithURL:inputFormat:outputFormat:]
-[NviAudioFileWriter addSamples:numSamples:]
-[CSVoiceTriggerEnabledPolicyNonAOP _addVoiceTriggerEnabledConditions]_block_invoke
CSSiriClientBehaviorMonitor
-[CSSiriClientBehaviorMonitor notifyWillStartStreamWithContext:option:]_block_invoke
-[CSSiriClientBehaviorMonitor notifyDidStartStreamWithContext:successfully:option:]_block_invoke
-[CSSiriClientBehaviorMonitor notifyWillStopStream:]_block_invoke
-[CSSiriClientBehaviorMonitor notifyDidStopStream:]_block_invoke
isStreaming
TB,N,V_isStreaming
-[CSSpeechEndpointAssetMetaUpdateMonitor _startMonitoringWithQueue:]
-[CSSpeechEndpointAssetMetaUpdateMonitor _stopMonitoring]
-[CSSpeechEndpointAssetMetaUpdateMonitor _didReceiveNewSpeechEndpointAssetMetaData]
com.apple.MobileAsset.SpeechEndpointAssetsWatch.ma.cached-metadata-updated
-[CSVoiceTriggerAssetHandlerMac _getVoiceTriggerAssetFromAssetManager:]_block_invoke
-[CSVoiceTriggerAssetHandlerMac _checkNewAssetAvailablity]_block_invoke
-[CSVoiceTriggerAssetHandlerMac CSVoiceTriggerAssetDownloadMonitor:didInstallNewAsset:]
-[CSVoiceTriggerAssetHandlerMac CSLanguageCodeUpdateMonitor:didReceiveLanguageCodeChanged:]
-[CSVoiceTriggerAssetHandlerMac CSFirstUnlockMonitor:didReceiveFirstUnlock:]
cachedAsset
T@"CSAsset",&,V_cachedAsset
-[CSVoiceTriggerAOPModeEnabledPolicyIOS _addConditionsForIOSBargeIn]_block_invoke
-[CSVoiceTriggerAOPModeEnabledPolicyIOS _addConditionsForIOSAOP]_block_invoke
-[CSVoiceTriggerAOPModeEnabledPolicyIOS _isSpeechDetectionDevicePresent]
start
stop
-[CSVoiceTriggerStatAggregator logFalseWakeUp:]
numFalseWakeUp
TQ,N,V_numFalseWakeUp
lastAggTimeFalseWakeUp
TQ,N,V_lastAggTimeFalseWakeUp
Library/nvi
T@"NSDictionary",C,N,V_voiceTriggerInfo
T@"NSDictionary",C,N,V_rtsTriggerInfo
triggerEndSeconds
com.apple.nvi
IsNviEnabled
InternalBuild
NviVADSignalType
NviKwdSignalType
NviDirectionalitySignalType
NviAsdAnchorSignalType
NviAsdPayloadSignalType
+[NviUtils strRepForNviSignalType:]
/BuildRoot/Library/Caches/com.apple.xbs/Sources/CoreSpeech_Sim/CoreSpeech-353.1.3/Nvi/Utils/NviUtils.m
Unknown NviSignalTypeString: <%@>
NviAudioDataSrcType
+[NviUtils strRepForNviDataSourceType:]
NviDataSource_END_MARKER
+[NviUtils nviDataSourceTypeForStr:]
+[NviUtils _createDirAtPath:]
Unexpected!! Received dir for NviConfig: %@
+[NviUtils readJsonDictionaryAt:]
+[NviUtils getValueFromDictionaryOfDictionaries:keypath:]
+[NviUtils createDirAtPath:]
+[CSVTUIRegularExpressionMatcher matchWithString:TrailingStr:LeadingStr:Pattern:]
-[CSEncryptedAudioFileWriter endAudio]
writeBuffer
T@"NSMutableData",&,N,V_writeBuffer
[requestHistoricalAudioDataWithHostTime = %@]
[requestHistoricalAudioDataSampleCount = %@]
[useOpportunisticZLL = %@]
[startRecordingHostTime = %llu]
[startRecordingSampleCount = %llu]
[alertBehavior = %llu %llu %llu]
TB,N,V_requestHistoricalAudioDataWithHostTime
TB,N,V_requestHistoricalAudioDataSampleCount
TQ,N,V_startRecordingHostTime
TQ,N,V_startRecordingSampleCount
TB,N,V_useOpportunisticZLL
Tq,N,V_startAlertBehavior
Tq,N,V_stopAlertBehavior
Tq,N,V_errorAlertBehavior
requestHistoricalAudioDataWithHostTime
requestHistoricalAudioDataSampleCount
startRecordingHostTime
startRecordingSampleCount
useOpportunisticZLL
startAlertBehavior
stopAlertBehavior
errorAlertBehavior
-[CSDispatchGroup leave]
-[CSSmartSiriVolumeController getEstimatedTTSVolume]_block_invoke
-[CSSmartSiriVolumeController _createXPCClientConnectionIfNeeded]
T@"<CSSmartSiriVolumeControllerDelegate>",W,N,V_delegate
address
T@"NSString",C,N,V_address
supportDoAP
TB,N,V_supportDoAP
isTemporaryPairedNotInContacts
TB,N,V_isTemporaryPairedNotInContacts
regex.json
Cannot parse to JSON
Cannot find the file
trailing_garbage
leading_garbage
regex
CSP2P_CommandType_Key
CSP2P_CommandDict_Key
corespeech
com.apple.siridebug.request.generic
com.apple.siridebug.command.remote.heysiri
com.apple.siridebug.command.parallel.recording
com.apple.siridebug.command.transfer.voiceprofile
com.apple.siridebug.command.query.voiceprofile
com.apple.siridebug.command.reverse.transfer.voiceprofile
com.apple.siridebug.command.fetch.voiceprofile
com.apple.siridebug.command.voiceprofile.update.trigger
com.apple.siridebug.command.fetch.parallelrecording
com.apple.siridebug.command.transfer.parallelrecording
com.apple.siridebug.command.fetch.voicegradingdata
com.apple.siridebug.command.transfer.voicegradingdata
com.apple.siridebug.command.delete.voiceprofile
com.apple.siridebug.command.musicaccount.signin
CSP2P_RemoteHeySiriEnable_Key
CSP2P_RemoteHeySiriStatus_Key
CSP2P_RemoteRecordingStart_Key
CSP2P_RemoteRecordingStatus_Key
CSP2P_VoiceProfileData_Key
CSP2P_VoiceProfileFileName_Key
CSP2P_VoiceProfileSpeakerName_Key
CSP2P_VoiceProfileLocale_Key
CSP2P_VoiceProfileDataType_Key
CSP2P_VoiceProfileSegment_Key
CSP2P_VoiceProfileTotalSegments_Key
CSP2P_VoiceProfileStatus_Key
CSP2P_VoiceProfileProfileId_Key
CSP2P_VoiceProfileHomeId_Key
CSP2P_VoiceProfileRelativeFilePath_Key
CSP2P_VoiceProfileBaseVersion_Key
CSP2P_VoiceProfileImplicitVersion_Key
CSP2P_VoiceProfileSiriProfileId_Key
CSP2P_VoiceProfileOnboardType_Key
CSP2P_VoiceProfileOnboardTimeStamp_Key
CSP2P_VoiceProfileTransferCompleted_Key
CSP2P_VoiceProfileRecordedData_Key
CSP2P_VoiceProfileRemoteFileName_Key
CSP2P_VoiceDataToBeGraded_Key
CSP2P_VoiceFileNameToBeGraded_Key
CSP2P_GradingDataTransferStatus_Key
CSP2P_PeerIdentifier_Key
CSP2P_VoiceProfilePeerName_Key
CSP2P_IsDataCompressed_Key
CSP2P_UncompressedDataSize_Key
CSP2P_GradingBatchTransferID_Key
CSP2P_VoiceProfileiTunesUserID_Key
CSP2P_VoiceProfileiTunesPassword_Key
CSP2P_MusicSigninMultiUserToken_Key
CSP2P_MusicSigninDsid_Key
CSP2P_MusicSigninAltDsid_Key
CSP2P_MusicSigninHomeUserId_Key
CSP2P_MusicSigninHomeId_Key
CSP2P_MusicSigninSiriDebugProfileId_Key
remote
-triggered
-almost
-rejected
-activation
com.apple.corespeech.p2psvc
-[CSP2PService processRemoteCommandWithPayload:fromPeer:withReply:]_block_invoke
CoreSpeech
-[CSP2PService sendInfo1ToNearbyPeer]_block_invoke
-[CSP2PService sendVoiceTriggerGradingDataToCompanion]_block_invoke
-[CSP2PService sendVoiceProfileUpdatedMessageToNearbyPeerForLocale:]_block_invoke
-[CSP2PService _processRemoteHeySiriCommandWithRequest:fromSenderID:withReply:]
-[CSP2PService _compressFilesInDirectory:matchingPredicate:compressedFileAvailable:]
/BuildRoot/Library/Caches/com.apple.xbs/Sources/CoreSpeech_Sim/CoreSpeech-353.1.3/CoreSpeech/CSP2PService.m
Invalid parameter not satisfying: %@
peerId
-[CSP2PService _sendParallelRecordingsToPeerId:voiceProfileRequestInfo:withReply:]
-[CSP2PService _sendParallelRecordingsToPeerId:voiceProfileRequestInfo:withReply:]_block_invoke
v52@?0@"NSString"8@"NSData"16Q24Q32B40@"NSError"44
-[CSP2PService _sendVoiceTriggerGradingDataToPeerId:]_block_invoke_2
-[CSP2PService _sendVoiceTriggerGradingDataToPeerId:]_block_invoke
-[CSP2PService _sendData1ToPeerId:]_block_invoke_2
-detected.wav
-[CSP2PService _sendData1ToPeerId:]_block_invoke
-rejected.wav
-[CSP2PService _sendData1File:withFileName:toPeerId:withCompressedFlag:withUncompressedDataSize:withBatchId:withRetainFileFlag:]
fileData
fileName
-[CSP2PService _sendData1File:withFileName:toPeerId:withCompressedFlag:withUncompressedDataSize:withBatchId:withRetainFileFlag:]_block_invoke
-[CSP2PService _receiveParallelRecordingFromPeerId:recordingInfo:withReply:]
%@_%@
-[CSP2PService _receiveData1FromPeerId:requestInfo:withReply:]
suppressnotification
-[CSP2PService _processMusicAccountSignInCommandFromPeerId:requestInfo:withReply:]
-[CSP2PService _receiveVoiceProfileFromPeerId:voiceProfileInfo:withReply:]
CoreSpeechCache
com.apple.siri.corespeech.voiceprofilelist.change
-[CSP2PService _processVoiceProfileDeleteCommandWithRequest:fromSenderID:withReply:]_block_invoke
-[CSP2PService _processDataFetchCommandWithRequest:fromSenderID:withReply:]
-[CSP2PService _processVoiceProfileListQueryCommandFromPeerId:requestInfo:withReply:]
voiceprofiles
-[CSP2PService _processFetchVoiceProfileCommandFromPeerId:requestInfo:withReply:]
_%d_%d_%@
-[CSP2PService _sendVoiceProfile:toPeerId:]
td/audio
tdti/audio
ti/audio
-[CSP2PService _sendVoiceProfile:toPeerId:]_block_invoke
-[CSP2PService _processReverseTransferVoiceProfileCommandFromPeerId:requestInfo:withReply:]
-[CSP2PService _processVoiceProfileUpdateTriggerFromPeerId:requestInfo:withReply:]
-[CSP2PService _sendVoiceProfileUpdateTriggerToPeerId:forLocale:]_block_invoke
lastCommunicatedPeer
T@"NSString",&,N,V_lastCommunicatedPeer
voiceTriggerBatchId
T@"NSString",&,N,V_voiceTriggerBatchId
voiceIdentificationBatchId
T@"NSString",&,N,V_voiceIdentificationBatchId
adCompanionServiceProvider
T@"<CSADCompanionServiceProvider>",W,N,V_adCompanionServiceProvider
com.apple.corespeech
-[CSVoiceTriggerAwareZeroFilter resetWithSampleRate:containsVoiceTrigger:voiceTriggerInfo:]
vtEndInSampleCount
TQ,N,V_vtEndInSampleCount
numSamplesProcessed
TQ,N,V_numSamplesProcessed
T@"CSAudioZeroFilter",&,N,V_zeroFilter
T@"<CSVoiceTriggerAwareZeroFilterDelegate>",W,N,V_delegate
com.apple.da
liveOnHomePod
CSSafeSetOutErrorWithNSError
+N9mZUAHooNvMiQnjeTJ8g
+[CSUtils supportHybridEndpointer]
PTQ+ABwag03BwO/CKvIK/A
4D8XW4YwJI7QvyPhv1TEdw
VoiceId
satinitfailed
satmodelfilefailed
satvectorfailed
tdsrfailed
tdsrtimeout
retrainsatfailed
explicituttrejected
toolessaudiofiles
unrecognizedmetadata
delayedscores
missinghomeidforclouduser
voiceidstaleprofiledetected
-[CSKeywordAnalyzerNDAPI initWithConfigPath:resourcePath:]
-[CSKeywordAnalyzerNDAPI _setStartAnalyzeTime:]
samples_at_fire
start_sample_count
threshold_normal
-[CSKeywordAnalyzerNDAPI getThreshold]
threshold_logging
-[CSKeywordAnalyzerNDAPI getLoggingThreshold]
threshold_reject_logging
-[CSKeywordAnalyzerNDAPI getRejectLoggingThreshold]
activePhraseId
TI,N,V_activePhraseId
T@"<CSKeywordAnalyzerNDAPIScoreDelegate>",W,N,V_delegate
Framework
Logs/CrashReporter/CoreSpeech/
Logs/CrashReporter/CoreSpeech/audio/
%@/%@%@%@
::: Initializing CoreSpeech logging...
yyyyMMdd-HHmmss
CSLogInitIfNeeded_block_invoke
gitrelno_unavailable
_CSGetOrCreateAudioLogDirectory
SignalTs, ProcessedAudioMs, StartSample, EndSample, Azimuth, EmaAzimuth, Confidence, SpatialSpreadSpectrum
%llu,%f,%lu,%lu,%f,%f,%f,
{%@, {start=%lu, end=%lu, conf=%f, az=%f, estAz=%fdist=%@}
,%d, 
%f, 
startSample
TQ,N,V_startSample
endSample
TQ,N,V_endSample
confidence
Tf,N,V_confidence
azimuth
Tf,N,V_azimuth
estimatedAzimuth
Tf,N,V_estimatedAzimuth
processedAudioDurMs
Td,N,V_processedAudioDurMs
spatialSpectrumData
T@"NSArray",&,N,V_spatialSpectrumData
azDistribution
T@"NSDictionary",&,N,V_azDistribution
mostSampledAzimuth
totalAudioRecorded
Td,N,V_totalAudioRecorded
featuresAtEndpoint
T@"NSArray",&,N,V_featuresAtEndpoint
endpointerType
Tq,N,V_endpointerType
serverFeatureLatencyDistribution
T@"NSDictionary",&,N,V_serverFeatureLatencyDistribution
additionalMetrics
T@"NSDictionary",&,N,V_additionalMetrics
CSSACInfoMonitor queue
-[CSSACInfoMonitor _startMonitoringWithQueue:]
-[CSSACInfoMonitor _stopMonitoring]
-[CSSACInfoMonitor isDeviceRoleStereo]
CSRemoteControlClient
-[CSRemoteControlClient dealloc]
T@"<CSRemoteControlClientDelegate>",W,N,V_delegate
RTModelData
RTModelHash
RTModelLocale
RTModelDigest
RTModelSignature
RTModelCertificate
RT Model for 
 from asset 
CorealisRTModel
CorealisRTModelVersion
dataSize(%d), hash(%@), locale(%@), digest(%@), cert(%@), signature(%@)
supportsSecureCoding
TB,R
modelData
T@"NSData",R,N,V_modelData
modelLocale
T@"NSString",R,N,V_modelLocale
modelHash
T@"NSString",R,N,V_modelHash
digest
T@"NSData",R,N,V_digest
T@"NSData",R,N,V_signature
certificate
T@"NSData",R,N,V_certificate
::: Initializing NVI logging...
InitNviLogging_block_invoke
CSAudioRouteChangeMonitorImpl queue
-[CSAudioRouteChangeMonitorImpl preferredExternalRouteDidChange:]_block_invoke
-[CSAudioRouteChangeMonitorImpl carPlayAudioRouteDidChange:]_block_invoke
-[CSAudioRouteChangeMonitorImpl _stopMonitoring]
-[CSAudioRouteChangeMonitorImpl _notifyHearstConnectionState:]
-[CSAudioRouteChangeMonitorImpl _notifyJarvisConnectionState:]
UserVoiceProfileDateTrained
UserVoiceProfilePath
UserVoiceProfileID
UserSharedSiriID
UserSharedHomeID
UserSharedSiriDebugID
UserVoiceProfileUserName
UserVoiceProfileOnboardType
UserVoiceProfileExpSatVecCount
UserVoiceProfileBaseProfileSatVecCount
UserVoiceProfileImplicitSatVecCount
Creating CSUserVoiceProfile with no UserName: vpDict: %@
locale
T@"NSString",&,N,V_locale
dateAdded
T@"NSDate",&,N,V_dateAdded
voiceProfileFilePath
T@"NSString",&,N,V_voiceProfileFilePath
profileID
T@"NSString",&,N,V_profileID
siriProfileId
T@"NSString",&,N,V_siriProfileId
onboardedType
TQ,N,V_onboardedType
numberOfExplicitSatVectors
Tq,N,V_numberOfExplicitSatVectors
numberOfBaseProfileSatVectors
Tq,N,V_numberOfBaseProfileSatVectors
numberOfImplicitProfileSatVectors
Tq,N,V_numberOfImplicitProfileSatVectors
sharedHomeID
T@"NSString",&,N,V_sharedHomeID
siriDebugID
T@"NSString",&,N,V_siriDebugID
userName
T@"NSString",&,N,V_userName
com.apple.coreaudio.BorealisToggled
-[CSVoiceTriggerEnabledMonitor _startMonitoringWithQueue:]_block_invoke
-[CSVoiceTriggerEnabledMonitor _startMonitoringWithQueue:]
-[CSVoiceTriggerEnabledMonitor _stopMonitoring]
-[CSVoiceTriggerEnabledMonitor _checkVoiceTriggerEnabled]
CSRemoteRecordClient Queue
T@"<CSRemoteRecordClientDelegate>",W,N,V_delegate
[profileId: %@, language: %@, product: %@, version: %@, homeId: %@, name: %@]
profileId
T@"NSString",&,N,V_profileId
languageCode
T@"NSString",&,N,V_languageCode
productCategory
T@"NSString",&,N,V_productCategory
version
T@"NSNumber",&,N,V_version
onboardType
TQ,N,V_onboardType
homeId
T@"NSString",&,N,V_homeId
siriDebugProfileId
T@"NSString",&,N,V_siriDebugProfileId
Borealis Input
com.apple.VoiceTriggerUI.RemoteRecordSessionQueue
-[CSVTUIAudioSessionRemote init]
-[CSVTUIAudioSessionRemote _audioRecorder]
-[CSVTUIAudioSessionRemote prepareRecord]
-[CSVTUIAudioSessionRemote startRecording]
-[CSVTUIAudioSessionRemote stopRecording]
audioStreamHandleId
TQ,N,V_audioStreamHandleId
T@"<CSVTUIAudioSessionDelegate>",W,N,V_delegate
-[CSSiriEnabledMonitor _startMonitoringWithQueue:]
-[CSSiriEnabledMonitor _stopMonitoring]
_AssistantPrefsChangedNotification
com.apple.nvi.csaudiosrc
-[NviCSAudioDataSource startWithNviContext:didStartHandler:]_block_invoke_2
-[NviCSAudioDataSource stopWithDidStopHandler:]_block_invoke_2
-[NviCSAudioDataSource _createAudioStreamWithCurrentNviContext]
-[NviCSAudioDataSource audioStreamProvider:avBufferAvailable:]
-[NviCSAudioDataSource audioStreamProvider:didStopStreamUnexpectly:]
-[NviCSAudioDataSource audioStreamProvider:audioChunkForTVAvailable:]
numBytesPerSample
nviCtx
T@"NviContext",&,N,V_nviCtx
receivers
T@"NSHashTable",&,N,V_receivers
beepLocation
statsComputed
beepPower
signalPower
originalPower
absMaxVal
above95pcOfMax
totalInputSamples
totalOutputSamples
jbl_begin.bin
-[CSBeepCanceller init]
-[CSBeepCanceller willBeep]
-[CSBeepCanceller reset]
T@"<CSBeepCancellerDelegate>",W,N,V_delegate
metrics
[UniqueUttTag: %@, InvocationStyle:(%lu)%@, Asset: %@, fallbackAsset: %@,vtEventInfo: %@]
%@_%@_%@.wav
%@_%@_%@.json
invocationStyle
TQ,N,V_invocationStyle
asset
T@"CSAsset",&,N,V_asset
vtEventInfo
T@"NSDictionary",&,N,V_vtEventInfo
uniqueUttTag
T@"NSString",&,N,V_uniqueUttTag
fallbackAsset
T@"CSAsset",R,N,V_fallbackAsset
T@"NSString",R,N,V_locale
Failed to create rootless dir at path: %@, status: %d, errno: %d, err: %s
CoreSpeechRootless
-[NSFileManager(Rootless) convertToRootlessDirectoryAtPath:error:]
Failed to convert path: %@ to rootless, status: %d, errno: %d, err: %s
CSActivationEventNotifier
-[CSActivationEventNotifier notifyActivationEvent:completion:]_block_invoke
-[CSActivationEventNotifier notifyActivationEventForCoreSpeechDaemon:completion:]_block_invoke
-[CSActivationEventNotifier _notifyActivationEvent:completion:]
-[CSActivationEventNotifier _notifyActivationEvent:completion:]_block_invoke
-[CSActivationEventNotifier notifyActivationEvent:deviceId:activationInfo:completion:]_block_invoke
-[CSActivationEventNotifier setDelegate:for:]_block_invoke
-[CSActivationEventNotifier _didReceiveAOPFirstPassTrigger:completion:]
-[CSActivationEventNotifier _didReceiveAOPFirstPassTrigger:completion:]_block_invoke
-[CSActivationEventNotifier receiveTestNotificationAPMode]
-[CSActivationEventNotifier receiveTestNotificationAOPMode]
-[CSActivationEventNotifier _createXPCClientConnection]
delegates
T@"NSMapTable",&,N,V_delegates
pendingActivationEvent
T@"CSActivationEvent",&,N,V_pendingActivationEvent
pendingCompletion
T@?,C,N,V_pendingCompletion
-[CSGestureMonitorWatch _startMonitoringWithQueue:]
-[CSGestureMonitorWatch wakeGestureRecognized:]
CSLSWakeGestureMonitor
Unable to find class %s
/System/Library/PrivateFrameworks/CarouselServices.framework/CarouselServices
CSInitialContinousZeros
CSMaxContinousZeros
CSMidSegmentContinousZeros
+[CSUtils(AudioFile) readAudioChunksFrom:block:]
-[CSSoftwareUpdateCheckingMonitor _startMonitoringWithQueue:]
-[CSSoftwareUpdateCheckingMonitor _stopMonitoring]
-[CSSoftwareUpdateCheckingMonitor _checkSoftwareUpdateCheckingState]
com.apple.duetscheduler.restartCheckNotification
-[CSAssetManagerEnablePolicy _addAssetManagerEnabledConditions]_block_invoke
-[CSAudioCircularBuffer initWithNumChannels:recordingDuration:samplingRate:]
-[CSAudioCircularBuffer copySamplesFromHostTime:]
-[CSAudioCircularBuffer copySamplesFrom:to:]
-[CSAudioCircularBuffer copyBufferWithNumSamplesCopiedIn:]
-[CSAudioCircularBuffer reset]
-[CSAudioCircularBuffer saveRecordingBufferFrom:to:toURL:]
bufferLength
TQ,N,V_bufferLength
copySamples
  mNumChannels: 
  mRecordingDurationInSecs: 
  mSampleRate: 
  mBytesPerSample: 
  mBufferLengthInSamples: 
  mNextWritePos: 
  mSamplesCount: 
  mMemoryPool(
): [
    chan-
: sz=
: mem-sz: 
-[CSVoiceTriggerAssetMetaUpdateMonitor _startMonitoringWithQueue:]
-[CSVoiceTriggerAssetMetaUpdateMonitor _stopMonitoring]
-[CSVoiceTriggerAssetMetaUpdateMonitor _didReceiveNewVoiceTriggerAssetMetaData]
com.apple.MobileAsset.VoiceTriggerAssets.ma.cached-metadata-updated
com.apple.MobileAsset.VoiceTriggerAssetsWatch.ma.cached-metadata-updated
com.apple.MobileAsset.VoiceTriggerAssetsMarsh.ma.cached-metadata-updated
com.apple.MobileAsset.VoiceTriggerAssetsMac.ma.cached-metadata-updated
-[CSRemoteVADCircularBuffer copySamplesFrom:to:]
capacity
TQ,R,N,V_capacity
size
TQ,R,N,V_size
beginSampleCount
TQ,R,N,V_beginSampleCount
-[CSAudioStream initWithAudioStreamProvider:streamName:streamRequest:]
-[CSAudioStream dealloc]
-[CSAudioStream startAudioStreamWithOption:completion:]
-[CSAudioStream isStreaming]
-[CSAudioStream audioStreamProvider:didStopStreamUnexpectly:]
-[CSAudioStream audioStreamProvider:didHardwareConfigurationChange:]
T@"<CSAudioStreamProviding>",W,N,V_streamProvider
streaming
TB,V_streaming
streamingUUID
T@"NSUUID",&,V_streamingUUID
T@"<CSAudioStreamProvidingDelegate>",W,N,V_delegate
lastForwardedSampleCount
TQ,N,V_lastForwardedSampleCount
scheduledFutureSample
TB,N,V_scheduledFutureSample
streamRequest
T@"CSAudioStreamRequest",&,N,V_streamRequest
startStreamOption
T@"CSAudioStartStreamOption",&,N,V_startStreamOption
{wordCount: %ld, trailingSilDuration: %ld, eosLikelihood: %f, pauseCounts: (%@), silencePosterior: %f, taskName: %@, processedAudioDurationInMilliseconds: %ld}
wordCount
Tq,N,V_wordCount
trailingSilenceDuration
Tq,N,V_trailingSilenceDuration
eosLikelihood
Td,N,V_eosLikelihood
pauseCounts
T@"NSArray",C,N,V_pauseCounts
silencePosterior
Td,N,V_silencePosterior
processedAudioDurationInMilliseconds
Tq,N,V_processedAudioDurationInMilliseconds
taskName
T@"NSString",C,N,V_taskName
VoiceTriggerEventInfo
-[CSSiriRestrictionOnLockScreenMonitor _startMonitoringWithQueue:]
-[CSSiriRestrictionOnLockScreenMonitor _stopMonitoring]
-[CSSiriRestrictionOnLockScreenMonitor _checkSiriRestrictedOnLockScreen]
-[CSSpringboardStartMonitor _startMonitoringWithQueue:]
-[CSSpringboardStartMonitor _stopMonitoring]
-[CSSpringboardStartMonitor _checkSpringBoardStarted]
com.apple.springboard.finishedstartup
CSAudioProvider
-[CSAudioProvider setStreamState:]
-[CSAudioProvider setAudioRecorder:]_block_invoke
-[CSAudioProvider setCurrentContext:error:]
-[CSAudioProvider setCurrentContext:error:]_block_invoke
-[CSAudioProvider _audioStreamWithRequest:streamName:error:]
-[CSAudioProvider _prepareAudioStreamSync:request:error:]
-[CSAudioProvider _createCircularBufferIfNeeded]
-[CSAudioProvider _tearDownCircularBufferIfNeeded]
-[CSAudioProvider startAudioStream:option:completion:]
-[CSAudioProvider startAudioStream:option:completion:]_block_invoke
-[CSAudioProvider prepareAudioStreamSync:request:error:]
-[CSAudioProvider prepareAudioStream:request:completion:]
-[CSAudioProvider _startAudioStream:option:completion:]
-[CSAudioProvider _handleDidStartAudioStreamWithResult:error:]
-[CSAudioProvider _handleDidStopAudioStreamWithReason:]
-[CSAudioProvider _stopAudioStream:option:completion:]
-[CSAudioProvider _stopAudioStream:option:completion:]_block_invoke
-[CSAudioProvider _stopAudioStream:option:completion:]_block_invoke_2
/BuildRoot/Library/Caches/com.apple.xbs/Sources/CoreSpeech_Sim/CoreSpeech-353.1.3/CoreSpeech/CSAudioProvider.m
-[CSAudioProvider holdAudioStreamWithDescription:timeout:]
-[CSAudioProvider holdAudioStreamWithDescription:timeout:]_block_invoke_2
-[CSAudioProvider holdAudioStreamWithDescription:timeout:]_block_invoke
-[CSAudioProvider cancelAudioStreamHold:]
-[CSAudioProvider cancelAudioStreamHold:]_block_invoke
-[CSAudioProvider prewarmAudioSessionWithError:]
-[CSAudioProvider activateAudioSessionWithReason:error:]
-[CSAudioProvider _activateAudioSessionWithReason:error:]
-[CSAudioProvider deactivateAudioSession:error:]
-[CSAudioProvider _deactivateAudioSession:error:]
-[CSAudioProvider setAlertSoundFromURL:forType:]
-[CSAudioProvider playAlertSoundForType:]
-[CSAudioProvider playRecordStartingAlertAndResetEndpointer]
-[CSAudioProvider alertStartTime]
-[CSAudioProvider triggerInfoForContext:completion:]_block_invoke
-[CSAudioProvider _shouldStopRecording]
-[CSAudioProvider audioRecorderStreamHandleIdInvalidated:]
-[CSAudioProvider audioRecorderWillBeDestroyed:]_block_invoke
-[CSAudioProvider _fetchHistoricalAudioAndForwardToStream:remoteVAD:]
-[CSAudioProvider _forwardAudioChunk:remoteVAD:atTime:toStream:]
-[CSAudioProvider _scheduleAlertFinishTimeout:]
-[CSAudioProvider _scheduleAlertFinishTimeout:]_block_invoke
-[CSAudioProvider _didReceiveFinishStartAlertPlaybackAt:]
-[CSAudioProvider audioRecorderBuiltInAudioStreamInvalidated:error:]_block_invoke
-[CSAudioProvider audioRecorderDisconnected:]
-[CSAudioProvider CSAudioServerCrashMonitorDidReceiveServerCrash:]
-[CSAudioProvider CSAudioServerCrashMonitorDidReceiveServerRestart:]
-[CSAudioProvider _handleAudioSystemFailure]
StreamInit
StreamPrepared
StreamStarting
StreamSteaming
StreamStopping
StreamStoppingWithScheduledStart
unknown(%tu)
Recording transaction
-[CSAudioProvider _releaseRecordingTransactionIfNeeded]
-[CSAudioProvider _scheduleDidStartRecordingDelegateWatchDog]
-[CSAudioProvider _schduleDidStartRecordingDelegateWatchDogWithToken:]
-[CSAudioProvider _clearDidStartRecordingDelegateWatchDog]
-[CSAudioProvider _scheduleDidStopRecordingDelegateWatchDog]
-[CSAudioProvider _scheduleDidStopRecordingDelegateWatchDog:]
-[CSAudioProvider _clearDidStopRecordingDelegateWatchDog]
recordQueue
T@"NSObject<OS_dispatch_queue>",&,N,V_recordQueue
streamState
TQ,N,V_streamState
startPendingStreams
T@"NSHashTable",&,N,V_startPendingStreams
startPendingOnStoppingStreams
T@"NSHashTable",&,N,V_startPendingOnStoppingStreams
alertPlaybackFinishWaitingStreams
T@"NSHashTable",&,N,V_alertPlaybackFinishWaitingStreams
streams
T@"NSHashTable",&,N,V_streams
stopPendingStreams
T@"NSHashTable",&,N,V_stopPendingStreams
pendingStartCompletions
T@"NSMutableArray",&,N,V_pendingStartCompletions
alertPlaybackFinishWaitingCompletions
T@"NSMutableArray",&,N,V_alertPlaybackFinishWaitingCompletions
pendingStopCompletions
T@"NSMutableArray",&,N,V_pendingStopCompletions
startPendingOnStoppingStreamToCompletionDict
T@"NSMutableDictionary",&,N,V_startPendingOnStoppingStreamToCompletionDict
providerDelegate
T@"<CSAudioProviderDelegate>",W,N,V_providerDelegate
sessionDelegate
T@"<CSAudioSessionProvidingDelegate>",W,N,V_sessionDelegate
streamHolders
T@"NSMutableArray",&,N,V_streamHolders
historicalBufferRequestStreams
T@"NSHashTable",&,N,V_historicalBufferRequestStreams
circularBuffer
T@"CSAudioCircularBuffer",&,N,V_circularBuffer
alertDelegate
T@"<CSAudioAlertProvidingDelegate>",W,N,V_alertDelegate
lastAudioRecorderContext
T@"CSAudioRecordContext",&,N,V_lastAudioRecorderContext
audioSystemRecovering
TB,N,V_audioSystemRecovering
audioPreprocessor
T@"CSAudioPreprocessor",&,N,V_audioPreprocessor
recordingTransaction
T@"CSOSTransaction",&,N,V_recordingTransaction
recordingWillStartGroup
T@"NSObject<OS_dispatch_group>",&,N,V_recordingWillStartGroup
waitingForAlertFinish
TB,N,V_waitingForAlertFinish
alertPlaybackFinishTimeoutToken
T@"NSUUID",&,N,V_alertPlaybackFinishTimeoutToken
startRecordingWatchDogToken
T@"NSUUID",&,N,V_startRecordingWatchDogToken
stopRecordingWatchDogToken
T@"NSUUID",&,N,V_stopRecordingWatchDogToken
circularBufferStartHostTime
TQ,N,V_circularBufferStartHostTime
circularBufferStartSampleCount
TQ,N,V_circularBufferStartSampleCount
-[CSListeningEnabledPolicyWatch _addListeningEnabledConditions]_block_invoke
-[NSArray(XPCObject) _cs_initWithXPCObject:]
-[NSArray(XPCObject) _cs_initWithXPCObject:]_block_invoke
B24@?0Q8@"NSObject<OS_xpc_object>"16
-[NSArray(XPCObject) _cs_xpcObject]_block_invoke
-[CSAlwaysOnProcessorStateMonitor _didReceiveAOPListeningStateChange:]
-[CSAudioChunk subChunkFrom:numSamples:forChannel:]
-[CSAudioChunk subChunkFrom:numSamples:]
-[CSAudioChunk splitAudioChunkSuchThatNumSamplesReceivedSoFar:reachesACountOf:completionHandler:]
T@"NSData",R,N,V_data
TQ,R,N,V_numChannels
TQ,R,N,V_numSamples
TQ,R,N,V_sampleByteDepth
TQ,R,N,V_startSampleCount
TQ,R,N,V_hostTime
remoteVADAvailable
T@"NSData",&,N,V_remoteVAD
numChannels
numSamples
startSampleCount
data
remoteVAD
samples_fed
best_phrase
best_start
best_end
best_score
early_warning
is_rescoring
sampleFed
TQ,N,V_sampleFed
bestPhrase
TQ,N,V_bestPhrase
bestStart
TQ,N,V_bestStart
bestEnd
TQ,N,V_bestEnd
bestScore
Tf,N,V_bestScore
earlyWarning
TB,N,V_earlyWarning
isRescoring
TB,N,V_isRescoring
dictionary
T@"NSString",&,N,V_endpointerModelVersion
TB,R,N,V_canProcessCurrentRequest
+[CSUtils(Time) hostTimeFromSampleCount:anchorHostTime:anchorSampleCount:]
+[CSUtils(Time) sampleCountFromHostTime:anchorHostTime:anchorSampleCount:]
+[CSUtils(Time) macHostTimeFromBridgeHostTime:]
CSAudioSessionInfoProvider
-[CSAudioSessionInfoProvider CSAudioServerCrashMonitorDidReceiveServerCrash:]_block_invoke
-[CSAudioSessionInfoProvider CSAudioServerCrashMonitorDidReceiveServerRestart:]_block_invoke
-[CSAudioSessionInfoProvider _registerInterruptionNotification]
-[CSAudioSessionInfoProvider _registerAudioRouteChangeNotification]
-[CSAudioSessionInfoProvider _handleInterruption:]_block_invoke
-[CSAudioSessionInfoProvider _audioRouteChanged:]_block_invoke
-[CSAudioSessionInfoProvider _deregisterAudioSessionNotifications]
sessionInfoQueue
T@"NSObject<OS_dispatch_queue>",&,N,V_sessionInfoQueue
+[CSVoiceTriggerEnrollmentDataManager saveRawUtteranceAndMetadata:to:isExplicitEnrollment:]
+[CSVoiceTriggerEnrollmentDataManager saveUtteranceAndMetadata:atDirectory:isExplicitEnrollment:]
+[CSVoiceTriggerEnrollmentDataManager saveUtterance:utteranceAudioPath:numSamplesToWrite:isExplicitEnrollment:]
+[CSVoiceTriggerEnrollmentDataManager saveMetadata:isExplicitEnrollment:]
+[CSVoiceTriggerEnrollmentDataManager _getBaseMetaDictionaryForUtterancePath:]
+[CSVoiceTriggerEnrollmentDataManager writeMetaDict:atMetaPath:]
-[CSEndpointerProxy resetForNewRequestWithSampleRate:recordContext:recordSettings:]
-[CSEndpointerProxy resetForVoiceTriggerTwoShotWithSampleRate:]
-[CSEndpointerProxy preheat]
-[CSEndpointerProxy endpointer:didDetectStartpointAtTime:]
-[CSEndpointerProxy endpointer:didDetectHardEndpointAtTime:withMetrics:]
-[CSEndpointerProxy endpointer:detectedTwoShotAtTime:]
-[CSEndpointerProxy endpointerModelVersion]
hybridEndpointer
T@"<CSEndpointAnalyzerImpl>",&,N,V_hybridEndpointer
nnvadEndpointer
T@"<CSEndpointAnalyzerImpl>",&,N,V_nnvadEndpointer
activeEndpointer
T@"<CSEndpointAnalyzerImpl>",W,N,V_activeEndpointer
recordingDidStop
TB,N,V_recordingDidStop
endpointerDelegate
T@"<CSEndpointAnalyzerDelegate>",W,N,V_endpointerDelegate
-[NSNumber(XPCObject) _cs_initWithXPCObject:]
-[NSNumber(XPCObject) _cs_xpcObject]
-[CSFirstUnlockMonitor _stopMonitoring]
kVTPreferencesPhraseSpotterEnabledDidChangeDarwinNotification
-[CSPhraseSpotterEnabledMonitor _checkPhraseSpotterEnabled]
-[CSPhraseSpotterEnabledMonitor _phraseSpotterEnabledDidChange]
corespeechd speaker xpc connection client queue
-[CSVoiceIdXPCConnection _handleClientEvent:]
-[CSVoiceIdXPCConnection _handleClientMessage:client:]
-[CSVoiceIdXPCConnection _handleClientMessage:client:]_block_invoke
-[CSVoiceIdXPCConnection _handleClientError:client:]
connection
T@"NSObject<OS_xpc_object>",&,N,V_connection
T@"NSString",R,N,V_profileID
sysConfigRoot
T@"NSString",R,N,V_sysConfigRoot
satModelDir
T@"NSString",R,N,V_satModelDir
satAudioDir
T@"NSString",R,N,V_satAudioDir
satModelFilePath
T@"NSString",R,N,V_satModelFilePath
tdSrSysConfigFile
T@"NSString",R,N,V_tdSrSysConfigFile
tdSrSysConfigRoot
T@"NSString",R,N,V_tdSrSysConfigRoot
satModelAvailable
TB,R,N,V_satModelAvailable
Tf,R,N,V_satScoreThreshold
satScoreVTScale
Tf,R,N,V_satScoreVTScale
satScoreVTOffset
Tf,R,N,V_satScoreVTOffset
satScoreNonVTScale
Tf,R,N,V_satScoreNonVTScale
satScoreNonVTOffset
Tf,R,N,V_satScoreNonVTOffset
combinationWeight
Tf,R,N,V_combinationWeight
satLogitCeilScore
Tf,R,N,V_satLogitCeilScore
satLogitFloorScore
Tf,R,N,V_satLogitFloorScore
satImplicitThreshold
Tf,R,N,V_satImplicitThreshold
satImplicitBaseProfileThreshold
TQ,R,N,V_satImplicitBaseProfileThreshold
satImplicitProfileThreshold
TQ,R,N,V_satImplicitProfileThreshold
satImplicitProfileDeltaThreshold
TQ,R,N,V_satImplicitProfileDeltaThreshold
retrainThresholdTrigger
Tf,R,N,V_retrainThresholdTrigger
retrainExplicitUttThresholdSAT
Tf,R,N,V_retrainExplicitUttThresholdSAT
retrainExplicitUttThresholdTDSR
Tf,R,N,V_retrainExplicitUttThresholdTDSR
retrainThresholdSAT
Tf,R,N,V_retrainThresholdSAT
retrainThresholdTDSR
Tf,R,N,V_retrainThresholdTDSR
pruningNumRetentionUtterance
Ti,R,N,V_pruningNumRetentionUtterance
maximumSpeakerVectors
Ti,R,N,V_maximumSpeakerVectors
maxAllowedImplicitUtterances
Ti,R,N,V_maxAllowedImplicitUtterances
maxAllowedBaseProfileUtterances
Ti,R,N,V_maxAllowedBaseProfileUtterances
voiceProfilePruningCookie
T@"NSString",R,N,V_voiceProfilePruningCookie
CSFallbackAudioSessionReleaseProvider
-[CSFallbackAudioSessionReleaseProvider fallbackDeactivateAudioSession:error:]_block_invoke
-[CSFallbackAudioSessionReleaseProvider fallbackDeactivateAudioSession:error:]
T@"<CSSPGEndpointAnalyzerDelegate>",W,N,V_delegate
samplingRate
Tf,N,V_samplingRate
dictationLanguages
T@"NSSet",&,N,V_dictationLanguages
currentKeyboard
T@"NSString",&,N,V_currentKeyboard
wasLanguageToggled
TB,N,V_wasLanguageToggled
multilingualKeyboardLanguages
T@"NSArray",&,N,V_multilingualKeyboardLanguages
keyboardConvoLanguagePriors
T@"NSDictionary",&,N,V_keyboardConvoLanguagePriors
keyboardGlobalLanguagePriors
T@"NSDictionary",&,N,V_keyboardGlobalLanguagePriors
previousMessageLanguage
T@"NSString",&,N,V_previousMessageLanguage
globalLastKeyboardUsed
T@"NSString",&,N,V_globalLastKeyboardUsed
dictationLanguagePriors
T@"NSDictionary",&,N,V_dictationLanguagePriors
conversationalMessages
T@"NSArray",&,N,V_conversationalMessages
-[CSStartOfSpeechDetector initWithConfig:samplingRate:minSpeechFrames:numLeadingFrames:delegate:]
T@"<CSStartOfSpeechDetectorDelegate>",W,N,V_delegate
-[CSAudioServerCrashMonitor _startMonitoringWithQueue:]
serverState
TQ,N,V_serverState
com.apple.corespeech.voicetriggerservice
-[CSVoiceTriggerXPCClient dealloc]
-[CSVoiceTriggerXPCClient _handleListenerEvent:]
-[CSVoiceTriggerXPCClient _handleListenerError:]
enable
assertion
timestamp
phraseSpotterBypass
phraseSpotterBypassTimeout
T@"<CSVoiceTriggerXPCClientDelegate>",W,N,V_delegate
-[CSCoreSpeechDaemonStateMonitor notifyDaemonStateChanged:]
com.apple.corespeech.corespeechd.launch
-[CSCoreSpeechDaemonStateMonitor _startMonitoringWithQueue:]
-[CSCoreSpeechDaemonStateMonitor _stopMonitoring]
-[CSCoreSpeechDaemonStateMonitor _didReceiveDaemonStateChanged:]
-[CSKeywordAnalyzerQuasar dealloc]
-[CSKeywordAnalyzerQuasar runRecognition]
-[CSKeywordAnalyzerQuasar endAudio]
triggerConfidence
Td,R,N,V_triggerConfidence
T@"<CSKeywordAnalyzerQuasarScoreDelegate>",W,N,V_delegate
-[CSNetworkAvailabilityMonitor _startMonitoringWithQueue:]
-[CSNetworkAvailabilityMonitor _stopMonitoring]
-[CSNetworkAvailabilityMonitor _availabilityChanged]
signalBuffer
T@"NSMutableArray",&,N,V_signalBuffer
route
isRemoteDevice
remoteDeviceUID
remoteDeviceProductIdentifier
%@ {route = %@, isRemoteDevice = %d, remoteDeviceUID = %@, remoteDeviceProductIdentifier = %@}
T@"NSString",R,C,N,V_route
TB,R,N,V_isRemoteDevice
T@"NSUUID",R,C,N,V_remoteDeviceUID
T@"NSString",R,C,N,V_remoteDeviceProductIdentifier
CSOpportuneSpeakListnerTestService
com.apple.corespeech.opportunelistner.start
com.apple.corespeech.opportunelistner.stop
A945B95D-69F6-FC77-4FAE-91F50A039CD8
-[CSOpportuneSpeakListnerTestService receiveOpportuneSpeakListenerStart]_block_invoke
-[CSOpportuneSpeakListnerTestService receiveOpportuneSpeakListenerStop]_block_invoke
-[CSOpportuneSpeakListnerTestService opportuneSpeakListener:hasRemoteVADAvailable:]
-[CSOpportuneSpeakListnerTestService opportuneSpeakListener:hasVADAvailable:]
-[CSOpportuneSpeakListnerTestService opportuneSpeakListener:didStopUnexpectly:]
corespeech.json
assets.json
hybridendpointer.json
hybridendpointer_marsh.json
/BuildRoot/Library/Caches/com.apple.xbs/Sources/CoreSpeech_Sim/CoreSpeech-353.1.3/CoreSpeech/CSAsset.m
ERR: Unknown assetType: %lu
/System/Library/PrivateFrameworks/CoreSpeech.framework
+[CSAsset fallBackAssetResourcePath]
defaultFallbackHearst
-[CSAsset initWithResourcePath:configFile:configVersion:]
-[CSAsset _decodeJson:]
-[CSAsset getNumberForKey:category:default:]
-[CSAsset getStringForKey:category:default:]
configVersion:%@ resourcePath:%@ path:%@
path
T@"NSString",R,N,V_path
resourcePath
T@"NSString",R,N,V_resourcePath
hashFromResourcePath
configVersion
T@"NSString",R,N,V_configVersion
-[CSAudioConverter _convertBufferedLPCM:allowPartial:timestamp:]
/BuildRoot/Library/Caches/com.apple.xbs/Sources/CoreSpeech_Sim/CoreSpeech-353.1.3/CoreSpeech/CSAudioConverter.m
Cannot produce ASPD for PCM
-[CSAudioConverter reset]
-[CSAudioConverter _configureAudioConverter:]
T@"<CSAudioConverterDelegate>",W,V_delegate
CreateAudioConverter
-[CSLanguageCodeUpdateMonitor _startMonitoringWithQueue:]
-[CSLanguageCodeUpdateMonitor _stopMonitoring]
-[CSLanguageCodeUpdateMonitor _didReceiveLanguageCodeUpdate]
Serial CSPolicy queue
-[NSData(Nvi) splitAudioDataToReachSampleCount:currSampleCount:numBytesPerSample:completionHandler:]
nviConfigRoot
hepConfigFilepath
voic
carplay
hearst
raisetospeak
auto
-[NSDictionary(XPCObject) _cs_initWithXPCObject:]
-[NSDictionary(XPCObject) _cs_initWithXPCObject:]_block_invoke
B24@?0r*8@"NSObject<OS_xpc_object>"16
-[NSDictionary(XPCObject) _cs_xpcObject]_block_invoke
v32@?0@8@16^B24
-[CSSpeakerDetectorNDAPI _initializeSAT:]
T@"<CSSpeakerDetectorNDAPIDelegate>",W,N,V_delegate
detector-config
supported-locales
detector.json
sos-options.json
SPG.json
languageDetectorSupportedLocale
languageDetectorConfigFile
startOfSpeechDetectorConfigFile
spgConfigFile
isMaximized
-[CSVTUIKeywordDetector initWithLanguageCode:]
-[CSAudioRecorder initWithQueue:error:]
-[CSAudioRecorder willDestroy]
-[CSAudioRecorder dealloc]
-[CSAudioRecorder _destroyVoiceController]
-[CSAudioRecorder _voiceControllerWithError:]
-[CSAudioRecorder setContext:error:]
-[CSAudioRecorder setCurrentContext:streamHandleId:error:]
-[CSAudioRecorder prepareAudioStreamRecord:streamHandleId:error:]
-[CSAudioRecorder _startAudioStreamForAudioInjection]
-[CSAudioRecorder startAudioStreamWithOption:streamHandleId:error:]
-[CSAudioRecorder stopAudioStreamWithStreamHandleId:error:]
-[CSAudioRecorder isSessionCurrentlyActivated]
Builtin Microphone
-[CSAudioRecorder recordingSampleRateWithStreamHandleId:]
-[CSAudioRecorder isNarrowBandWithStreamHandleId:]
-[CSAudioRecorder prewarmAudioSessionWithStreamHandleId:error:]
-[CSAudioRecorder activateAudioSessionWithReason:streamHandleId:error:]
-[CSAudioRecorder deactivateAudioSession:error:]
+[CSAudioRecorder createSharedAudioSession]
-[CSAudioRecorder setDuckOthersOption:]
-[CSAudioRecorder enableMiniDucking:]
Enable
Disable
-[CSAudioRecorder configureAlertBehavior:audioStreamHandleId:]
-[CSAudioRecorder voiceTriggerInfo]
useRemoteBuiltInMic
-[CSAudioRecorder _processAudioBuffer:audioStreamHandleId:]
-[CSAudioRecorder _compensateChannelDataIfNeeded:receivedNumChannels:]
-[CSAudioRecorder playAlertSoundForType:]
-[CSAudioRecorder voiceControllerDidStartRecording:forStream:successfully:error:]
-[CSAudioRecorder voiceControllerAudioCallback:forStream:buffer:]
-[CSAudioRecorder voiceControllerDidStopRecording:forStream:forReason:]
-[CSAudioRecorder voiceControllerStreamInvalidated:forStream:]
-[CSAudioRecorder voiceControllerRecordHardwareConfigurationDidChange:toConfiguration:]
-[CSAudioRecorder voiceControllerDidFinishAlertPlayback:ofType:error:]
-[CSAudioRecorder voiceControllerEncoderErrorDidOccur:error:]
-[CSAudioRecorder voiceControllerBeginRecordInterruption:]
-[CSAudioRecorder voiceControllerBeginRecordInterruption:withContext:]
-[CSAudioRecorder voiceControllerEndRecordInterruption:]
-[CSAudioRecorder voiceControllerWillSetAudioSessionActive:willActivate:]
-[CSAudioRecorder voiceControllerDidSetAudioSessionActive:isActivated:]
-[CSAudioRecorder voiceControllerMediaServicesWereLost:]
-[CSAudioRecorder voiceControllerMediaServicesWereReset:]
-[CSAudioRecorder _deinterleaveBufferIfNeeded:]
-[CSAudioRecorder _createDeInterleaverIfNeeded]
-[CSAudioRecorder _getRecordSettingsWithRequest:]
CSCommandControlBehaviorMonitor
-[CSCommandControlBehaviorMonitor notifyWillStartStreamWithContext:option:]_block_invoke
-[CSCommandControlBehaviorMonitor notifyDidStartStreamWithContext:successfully:option:]_block_invoke
-[CSCommandControlBehaviorMonitor notifyWillStopStream:]_block_invoke
-[CSCommandControlBehaviorMonitor notifyDidStopStream:]_block_invoke
kCCParamError
kCCBufferTooSmall
kCCMemoryFailure
kCCAlignmentError
kCCDecodeError
kCCUnimplemented
kCCOverflow
kCCRNGFailure
kCCUnspecifiedError
kCCCallSequenceError
kCCKeySizeError
Unexpected: %ld
randomBytesWithLength:%lu failed. err=%d
CSNSDataEncryptDecrypt
-[NSData(Encryption) saveEncryptedDataUsingAESKey:atFilepath:]_block_invoke
ivtag
v40@?0@"NSData"8@"NSData"16@"NSData"24@"NSError"32
+[NSData(Encryption) decryptedDataUsingAESKey:atFilepath:error:]
aesKey is nil
/BuildRoot/Library/Caches/com.apple.xbs/Sources/CoreSpeech_Sim/CoreSpeech-353.1.3/CoreSpeech/NSData+Encryption.m
AESKey(%lu) != %lu
CoreSpeechNSDataEncryption
CoreSpeechNSDataDecryption
{isVT=%d, requestHistoricalAudio=%d, reqStartAudioSampleId=%lu, reqStartMachAbsTime=%llu}
T@"NSDictionary",&,N,V_voiceTriggerInfo
requestHistoricalAudio
TB,N,V_requestHistoricalAudio
reqStartAudioSampleId
TQ,N,V_reqStartAudioSampleId
reqStartMachAbsTime
TQ,N,V_reqStartMachAbsTime
shouldLogRawSensorData
TB,N,V_shouldLogRawSensorData
rootLogDir
T@"NSString",&,N,V_rootLogDir
-[NSData(XPCObject) _cs_initWithXPCObject:]
%@-%@
-[CSOSTransaction initWithDescription:]
-[CSOSTransaction dealloc]
+[NviSignalData headerString]
-[NviSignalData stringForLogging]
{%@:ts=%lld}
TQ,N,V_sigType
sigGenTs
TQ,N,V_sigGenTs
%s Tagging as handheld as user interacted in last %f secs
%s Tagging as farfield as last user interaction %f secs back
%s Tagging as FarField as user dismissed
%s Unable to write to o/p stream ! 
%s Got event! %lu
%s Got unhandled evt code %lu 
%s ERR: uttPath is nil -  Bailing out
%s ERR: uttPath is nil - Bailing out
%s ERR: uttMeta is nil - Bailing out
%s ::: Error creating json Metadata: %{public}@
%s Error reading contents of SAT root: %{public}@: err: %{public}@
%s Error determining if file is dir-entry: url=%{public}@, err=%{public}@
%s ERROR creating meta-version json-data from dict: ERR: %{public}@
%s Error reading contents of audioDir: %{public}@, err: %{public}@
%s Missing meta-file: Creating new Meta file for audio file: %{public}@
%s ERR: Unexpected. metaVersionFileData is empty while the file exists at: %{public}@
%s Json-Err reading metaVersionFile: %{public}@: err: %{public}@
%s ERR: uttMetaURL is nil
%s ERR: Unexpected. metaData is nil while the uttMetaPath exists at: %{public}@
%s error reading meta-file: %{public}@
%s ERR: uttMetaURL is nil, defaulting to NO
%s ERR: metaData is nil, defaulting to NO for %{public}@
%s ERR: read metafile %{public}@ failed with %{public}@ - defaulting to NO
%s ERR: missing %{public}@ key in %{public}@ - defaulting to NO
%s ERR: uttMetaURL is nil - Bailing out
%s ERR: metaData is nil for %{public}@ - Bailing out
%s ERR: read metafile %{public}@ failed with %{public}@ - Bailing out
%s ERR: metaDict from file %{public}@ isnt a dictionary - %{public}@
%s ERR: missing %{public}@ key in %{public}@ - Bailing out
%s ERR: %{public}@ is not present
%s %{public}@ deallocated
%s audioProvider not exist
%s CoreSpeechDaemon not enabled, no need to create xpc client
%s Fetched audio session ID %lu
%s Start Monitoring : AudioSession notification from corespeechd
%s Start monitoring : AudioSessionMediaServicesWereLost, AudioSessionMediaServicesWereReset
%s Stop Monitoring : AudioSession notification from corespeechd
%s Start monitoring : AudioSessionInterruption
%s Start monitoring : AudioSessionRouteChangeNotification
%s reset sessionInfoProvider since xpcClient disconnected
%s %@ task delivered.
%s %@ completed with response %@ and error %@.
%s Get initial state from MediaRemote: media is on playing state %{public}ld.
%s Start monitoring MediaRemote: media playback
%s Stop monitoring MediaRemote: media playback
%s MediaRemote reported the now playing app playback state changed to %s (state %d)
%s Celestial is not available on this platform.
%s notification = %{public}@
%s MobileTimer is not available on this platform.
%s Dealloc audioStreamHolding : %{public}@
%s In %@: Continuous digital zero detected, lasting %{public}u samples per channel
%s In %@: Continuous digital zero in this audio chunk detected, lasting %{public}u samples per channel
%s ::: %{public}s enable: %{public}d reason: %{public}@ timestamp : %{public}lf
%s Ignoring request to enable/disable voice trigger with nil reason.
%s ::: Asserting that VoiceTrigger should be %{public}@ with reason: %{public}@. Existing assertions (%{public}lu): %{public}@; times: %{public}@ vs %{public}f
%s Ignoring request to enable/disable voice trigger - time order violation.
%s ::: Ignore request as phraseSpotter already %{public}@
%s ::: Asserting that PhraseSpotter should be %{public}@, timeout: %{public}f
%s ::: Timeout!! PhraseSpotter should be NOT bypassed
%s HandleDisconnect
%s voiceTriggerXPC client not exist
%s CoreSpeechDaemon not enabled, no need to create voiceTriggerXPC
%s reset xpcClient since it disconnected
%s Plan removing the temp file %{public}@
%s Failed to remove temp file %{public}@ reason: %{public}@
%s Start copying %{public}u bytes of data to crashreporter with wav file header offset %{public}llu
%s Failed to read data from %{public}@
%s Finished copying data to crashreporter.
%s Logging audio file into : %{public}@
%s Removing non-dir at AttentiveSiri AudioLog dir: %@
%s Error removing %@: err: %@
%s Failed to create AudioLogging directory for AttentiveSiri: %@
%s Created AudioLogging dir for AttentiveSiri at: %@
%s Voice Profile for profileID %@ not found
%s SpkrId:: Error creating uttMetaJsonData: %@
%s SpkrId:: Failed to create UttMeta...
%s Start monitoring : VoiceTrigger Asset Download
%s Stop monitoring : VoiceTrigger Asset Download
%s New VoiceTrigger is now installed
%s CS logging files under %{public}@ created before %{public}@ will be removed.
%s Couldn't get creation date: %{public}@
%s Could not remove %{public}@: %{public}@
%s CS logging files number %{public}lu with pattern %{public}@ under %{public}@ exceeding limit, only keep the latest %{public}lu ones
%s Regular expression is nil!
%s SmartSiriVolume: deleted %{public}u elements in energy buffer.
%s SmartSiriVolume: number of elements to delete exceeds energy buffer size, ignore.
%s SmartSiriVolume init value for noise estimation %{public}f
%s SmartSiriVolume init value for LKFS estimation %{public}f
%s SmartSiriVolume enable policy changed : %{public}@
%s Already started listen polling, skip
%s listen polling has failed : %{public}@
%s Skip listen polling since audio is streaming / Siri disabled
%s Start audio stream successfully ? %{public}@, error : %{public}@
%s Received didStartRecording when Siri is off
%s Failed in requesting audio stream : %{public}@
%s Failed to stop audio stream : %{public}@
%s No audio stream to stop, we shouldn't hit this
%s SmartSiriVolume received MediaRemote initial state as %{public}@
%s SmartSiriVolume haven't got MediaRemote callback yet, let's assume media is playing.
%s SmartSiriVolume received alarm initial state as %{public}@
%s SmartSiriVolume received timer initial state as %{public}@
%s asset is nil, use default parameters(this should not happen).
%s SmartSiriVolume configure: %{public}@
%s SmartSiriVolume heartbeat = %{public}lld
%s SmartSiriVolume: estimated noise level %{public}f
%s SmartSiriVolume: estimated LKFS %{public}f
%s SmartSiriVolume: pause SSV calculation.
%s SmartSiriVolume: resume SSV calculation.
%s Siri is disabled, we shouldn't receive audio here, heartbeat = %{public}lld
%s stream stopped unexpectedly : %{public}ld
%s SmartSiriVolume received VT event!
%s SmartSiriVolume remove samples from VT utterances by %{public}llu, with startAnalyzeSampleCount = %{public}llu, samplesFed = %{public}llu, triggerStartSampleCount = %{public}llu
%s SmartSiriVolume trying to delete too many VT samples, set triggerDurationToDelete to be limited max: %{public}llu
%s SmartSiriVolume got empty VT event!
%s SmartSiriVolume dismiss alarm firing as VoiceTrigger detected.
%s SmartSiriVolume dismiss timer firing as VoiceTrigger detected.
%s SmartSiriVolume: final estimated TTS volume in dB %{public}f
%s SmartSiriVolume: adjust TTS volume since alarm/timer is firing.
%s SmartSiriVolume: TTS volume in dB from noise %{public}f, from LKFS %{public}f, with user offset %{public}f
%s SmartSiriVolume: soft volume algorithm in use
%s SmartSiriVolume: pause LKFS calculation according to MediaRemote notification.
%s SmartSiriVolume: resume LKFS calculation according to MediaRemote notification.
%s SmartSiriVolume received unknown media playing state, let's assume media is playing.
%s SmartSiriVolume received unknown alarm state, let's reset alarm state.
%s SmartSiriVolume: alarm firing status = %@ according to MobileTimer notification.
%s SmartSiriVolume received unknown timer state, let's reset timer state.
%s SmartSiriVolume: timer firing status = %@ according to MobileTimer notification.
%s Siri enabled : %{public}d
%s Mediaserverd/bridgeaudiod recovered from crash
%s SmartSiriVolume dismiss alarm firing as Siri client is recording.
%s SmartSiriVolume dismiss timer firing as Siri client is recording.
%s SmartSiriVolume: set StartAnalyzeSampleCount = %{public}lld
%s Resetting audio preprocessor : %{public}f, containsVoiceTrigger:%{public}d
%s Flushing audio preprocessor
%s Couldn't find keychain value %@ for account %@ %{public}d
%s Trying to access BTLocalDevice
%s Accessing BTLocalDevice
%s BTLocalDevice %{public}p
%s Failed to get sharing enable flag : %d
%s Failed getting sharing device addresses %d
%s Failed converting address from BTDeviceAddress (result = %d).
%s Device is temporary paired and not in contacts
%s Detaching bluetooth session : %{public}p
%s Already Attaching Bluetooth Session
%s Start attaching bluetooth session
%s session = %{public}p, result = %{public}d
%s session = %p
%s detached session is different from currently attached session, ignore
%s terminated session is different from currently attached session, ignore
%s Bluetooth Session is NULL
%s Failed to get default local device from session %{public}p, (result = %{public}d)
%s Failed to add local device callback from session %{public}p, (result = %{public}d
%s Start monitoring: siri assertion enable/disable
%s Stop monitoring: siri assertion enable/disable
%s did receive enable assertion
%s did receive disable assertion
%s Unknown body type : %{public}lld
%s ::: Error reading file %@, err: %d
%s CSAudioFileReader requires prepare recording settings to feed audio
%s CSAudioFileReader only support LinearPCM to feed
%s Setting ExtAudioFileSetProperty failed : %d
%s Starting audio file feed timer, bufferDuration = %f sampleRate = %f, bytesPerFrame = %d, channelsPerFrame = %d
%s ::: Error reading data from audio file : %d
%s Reach to EOF, chunkSize = %d
%s Stopping audio file feed timer
%s Failed to create regular expression : %{public}@
%s Received active route change notification
%s Stop monitoring : AudioRouteChangeMonitor
%s Notifying Hearst Connection State : %{public}d
%s Cannot create SampleRateConverter using AudioConverterNew : %{public}d
%s Cannot set Quality property to audioConverter
%s Cannot set Complexity property to audioConverter
%s Audio resampling done : %lu
%s AudioConverter is sad: 0x%{public}xd
%s Cannot start monitoring language detector asset, since we already registered
%s LanguageDetector supported locale is nil : %{public}@
%s xpc object string return nil
%s xpc object should be XPC_TYPE_STRING
%s Start Listening request with deviceId : %{public}@
%s CSOpportuneSpeakListener received didStart : %{public}d, %{public}@
%s remoteVADDuration = %{public}d, spgDuration = %{public}d, _remoteVADSPGRatio = %{public}d
%s AudioStreamRequest has failed : %{public}@
%s CSOpportuneSpeakListener received didStop : %{public}d, %{public}@
%s Request stop CSOpportuneSpeakListener
%s Audio coming from DoAP should contains RemoteVAD
%s boronScore : %{public}d, reportBoron : %{public}d, slienceScore : %{public}lf
%s cannot handle nil event 
%s ignore unknown types of message: %{public}@
%s cannot handle nil error
%s Listener connection disconnected
%s connection error: %{public}s
%s Not supported on this platform
%s disconnect activationXPCClient
%s cannot handle event : event = %{public}p
%s ignore unknown types of message 
%s cannot handle error : error = %{public}p
%s default to recordContext : %{public}@
%s Siri language is nil, falling back to %@
%s Advert data: %{public}@
%s advert data write failed
%s CSVoiceTriggerAsset found: %{public}@
%s Locale: [%{public}@]
%s No locale set when creating phrase spotter.
%s Creation of Keyword Detector failed.
%s %s async called
%s %{public}s Called
%s %{public}s async called
%s Called before completion called
%s BEGIN num:%{public}ld use:%{public}d
%s AudioSession setup failed
%s Has wrong audio routing, ask user to unplug headset
%s Start Audio Session failed
%s _sessionNumber [%{public}ld]
%s %{public}s Canceling Training
%s Called with status : %{public}d
%s %{public}s called
%s AudioSession StartRecording Failed
%s Setting suspendAudio:[%{public}d]
%s Resume training
%s Suspend training
%s Stop Listening
%s ERR: Failed to save explicit utterance
%s ::: VoiceTrigger has been ACTIVE for an interval of %{public}5.3f seconds.
%s ::: VoiceTrigger has been INACTIVE for an interval of %{public}5.3f seconds.
%s Adding value for scalar key %{public}@ : %{public}lld
%s Pushing value for distribution key %{public}@ : %{public}lf
%s Cannot remove model directory(%@) : %@
%s Cannot remove utterance directory(%@) : %@
%s Fetching CommandControl Listening State: %d
%s ::: Error creating output file %{public}@, err: %{public}d
%s ::: Error writing to output wave file. : %{public}ld
%s getCoreSpeechXPCConnection Invalidated
%s Asking current VoiceTrigger aset for %{public}d.%{public}d
%s Asking keyword language given Jarvis language list %{public}@, jarvis-selected language: %{public}@
%s CSCoreSpeechServices Invalidated
%s Request updated SAT audio succeed.
%s Request updated SAT audio failed.
%s speechController = %{public}p
%s xpcListener = %{public}p
%s volumeController = %{public}p
%s context = %{public}@
%s Failed to create audio recorder : %{public}@
%s For Context : %{public}@, audioStreamId(%llu) has allocated
%s Failed to get audio stream handle ID : %{publid}@
%s has match with audio stream handle id : %llu
%s does not match with audio stream handle id(%llu), creating new audio provider
%s have matched audioProvider with stream handle id : %llu
%s don't have matched audioProvider with stream handle id : %llu, need to create one later
%s audioProvider[%{public}@] invalidated with streamHandleId : %{public}llu
%s No matched audioProvider found for streamHandleId : %{public}llu
%s Received VoiceTrigger cached asset change notification, let's reinitialize VoiceTrigger
%s Trying to start clear logging files
%s Clear logging file timer is already started, ignore startClearLoggingFilesTimer request.
%s SpeechEndEstimation: trailingSilenceDuration = %{public}f
%s SpeechEndEstimation: TrailingSilenceDuration at endpointer(%{public}f) is longer than threshold(%{public}f), force to make 0
%s SpeechEndEstimation: _lastAudioChunkHostTime = %{public}llu, estimatedSpeechEndHostTime = %{public}llu
%s Start Listening for Command Control
%s Calling didStart of CSCommandControlListener
%s Stopping stopListenWithCompletion
%s Calling didStop of CSCommandControlListener
%s Calling didStopUnexpectly
%s Received xpc disconnection, audioStream is streaming = %{public}d
%s ERR: FilePath is nil - Bailing out
%s ERR: Training utterance doesnt exist at %@ - Bailing out
%s ERR: Training utterance is marked as directory at %@ - Bailing out
%s sharedSiriId is nil - Bailing out
%s Enrolling voice profile of user - %{public}@ 
%s Failed enrolling %{public}@ with error %{public}@
%s Failed to get device hash list %{public}@
%s Processing sync data from device hash: %{public}@
%s Skipping language [%{public}@] since we already have enrollment data for this language
%s Error to copy profile from %{public}@ to %{public}@, error: %{public}@
%s Copied %@ to %@
%s Skipping marking enrollment success for language %{public}@ with error %{public}@
%s language: %{public}@, enableVTAfterSyncLanguage: %{public}@, currSiriLanguage: %{public}@
%s Enabling VoiceTrigger Upon VoiceProfile sync for language: %{public}@ and enrolled language: %{public}@
%s VoiceTrigger does not exist for this platform, not setting VoiceTriggerEnabled
%s Not enabling VoiceTrigger Upon VoiceProfile sync for language: %{public}@
%s Sucessfully migrated language %{public}@
%s Migrated language %{public}@ but failed to mark SAT enrollment
%s Sucessfully marked as migrated for language : %{public}@
%s Failed to mark migrated for language : %{public}@
%s Failed to remove update path [%{public}@] upon migration completion, error: %{public}@
%s Failed to download voice profile with error %{public}@ and category %{public}@ 
%s Successfully enrolled voice profile %{public}@ with %{public}@ profile
%s Voice profile %{public}@ is upto date - category %{public}@profile skipped
%s ERR: Failed in enrolling Voice profile %{public}@ with category %{public}@ profile
%s Failed to enroll siriProfileId %{public}@ with %{public}@
%s Failed to download voice profiles with error %{public}@
%s Enrolled voice profiles with error %{public}@
%s Language %{public}@ not supported in %{public}@ - Skipping
%s Skipping profile download for %{public}@ - %{public}@ not matching current %{public}@
%s Skipping profile download for %{public}@ - voiceId not supported in %{public}@
%s Failed migrating Voice profile with ID %{public}@ for language %{public}@ with error %{public}@
%s Failed marking Voice profile with ID %{public}@ for language %{public}@ with error %{public}@
%s Sucessfully enrolled %{public}@ for language %{public}@
%s PHS update directory already exists, remove before we move forward
%s Failed to delete PHS update directory
%s Failed to create PHS update directory
%s Upload of Voice Profile for %{public}@ completed with error %{public}@
%s Cannot delete existing SATUpload Diretory : %{public}@
%s Upload trigger of voice profile of user - %{public}@ 
%s Base voice profile is full for %{public}@
%s ERR: Base voice profile utterances not found for %{public}@ - Bailing out
%s ERR: Failed to stage voice profile %{public}@
%s ERR: Cannot copy file %{public}@ to %{public}@ with error %{public}@
%s ERR: Cannot move file %{public}@ to %{public}@ with error %{public}@
%s ERR: No base utterances added - Removing staging dir
%s %{public}d (%{public}d, %{public}d) base utterances added to staged profile at %{public}@
%s Unsupported languagecode %{public}@ in %{public}@ - Skipping
%s Skipping uploading %{public}@ legacy version (%lu) of voice profile, current version %lu
%s Skipping audiocache file %@
%s Cannot copy file %{public}@ to %{public}@ with error %{public}@
%s ERR: Failed to copy from %{public}@ to %{public}@ with error %{public}@
%s Cannot create %{public}@ with error %{public}@ - Skipping language
%s Cannot copy voice profile from %{public}@ to %{public}@ with error %{public}@
%s Triggering upload of voice profile %{public}@
%s Upload of voice profile at %{public}@ with category %{public}@ completed successfully
%s Triggering upload of explicit voice profile %{public}@
%s Upload of explicit voice profile at %{public}@ completed successfully
%s Setting VoiceProfile Training Sync for language: %{public}@
%s ERR: %{public}s: Bailing out as language is nil!
%s VoiceProfile training sync language: %{public}@, VoiceProfile language: %{public}@
%s ERR: metaBlob is nil. Returning false, language: %{public}@
%s ERR: Unknown device. Returning false, language: %{public}@
%s ERR: Unknown device-category for device: %{public}@, languageCode: %{public}@
%s ERR: Failed to deserialize metaBlob with error %{public}@
%s Looking VoiceProfile for CurrDevice: %{public}@{%{public}@} in metablob %{public}@
%s currDevice=[%{public}@ : {%{public}@}] ; syncedDevice=[%{public}@ : {%{public}@}]
%s VoiceProfile MATCH
%s VoiceProfile MIS-MATCH
%s CurrDevice: [%{public}@ : {%{public}@}] DOES NOT have VoiceProfile synced in iCloud
%s Querying VoiceProfile upload state on %{public}@
%s ERR: Fetching cached devices resulted in error %{public}@
%s Looking VoiceProfile for CurrDevice: %{public}@{%{public}@} in devices %{public}@
%s VoiceProfile available locally for %{public}@, not uploaded yet
%s Searching for synced-VoiceProfile for CurrDevice: %{public}@{%{public}@}
%s Will Enable VoiceTrigger after VoiceProfile sync for language: %{public}@
languageCode: %{public}@
%s Devices with VoiceProfile in iCloud for language: %{public}@:%{public}@
%s Timedout waiting for AFSettingsConnection:getDevicesWithAvailablePHSAssetsForLanguage: %{public}ld, waitedFor: %{public}d, Returning nil
%s timeToRet(AFSettingsConnection:getDevicesWithAvailablePHSAssetsForLanguage:): %{public}fms
%s ERR: Failed in marking Enrollment as Successful
%s Coudn't mark SAT enrollment %{public}@ at path %{public}@
%s Marked SAT enrollment %{public}@ at path %{public}@
%s We can't mark SAT %{public}@ when there is no audio directory
%s ERR: SAT did not trigger!!! - Bailing out
%s Rejecting barge-in utterance from adding to voice profile
%s CSSpIdImplicitTraining not available
%s Unable to copy implicit utterence to %{public}@ with error %{public}@
%s Received xpc completion with result %d
%s init-_currentLanguageCode: %{public}@
%s Asset Manager Policy has been %{public}@
%s Asset Manager Policy has been enabled, start fetching meta data now
%s set option : allowVoiceTriggerAssetsDownload ? %{public}@;               allowEndpointAssetDownload ? %{public}@;               allowLanguageDetectorAssetDownload ? %{public}@
%s Need to fetch remote meta now, since we have new asset need to be downloaded
%s Does not need to fetch remote meta now
%s Cannot fetch VoiceTrigger asset meta data
%s Undefined assetType : %{public}u
%s _currentLanguageCode changed: %{public}@
%s Trying to start download meta data
%s Periodical downloading is already scheduled, ignore request.
%s No periodical downloading is scheduled, ignore request.
%s returning status to UI : %{public}d
%s Already reported status or no callback
%s Will suspend training
%s Will resume training
%s Decide to delay ending ASR: [%{public}ld] samples
%s Triggered! Event info: %{public}@
%{public}9lld %{public}9lld %{public}9lld
%s analyzing.... score so far: %{public}5.3f
%s feeding tailing: [%{public}ld] samples
%s correctSampleSize:    [%{public}ld]
%s accumSampleSize:      [%{public}ld]
%s startBufferIndex:     [%{public}ld]
%s startBufferSampleSize:[%{public}ld]
%s samplesToBeDeleted:   [%{public}ld]
%s Total Number of buffs:[%{public}ld]
%s Adjusting the start buffer
%s Adjusting the array elements
%s Unsupported
%s Begin of speech detected
%s End of speech detected with endpoint type: %{public}ld
%s %{public}s CALLED
%s Master Timeout Fired
%s recognized text = %{public}@
%s Not supported on this platform.
%s Context : %{public}@
%s settings : %{public}@
%s Session Provider does not exist
%s Received speicial error code that corespeech needs to setContext and activate audio session again
%s CSSpeechController is already streaming audio.., we don't need to create another audio stream here
%s Prepare audio stream succeeded ? %{public}@, error - %{public}@
%s audioStreamWithRequest succeeded ? %{public}@, error - %{public}@
%s Failed to get audioStream : %{public}@
%s AudioStreamProvider is not existing?
%s We need to apply 12dB post gain for this Siri audio session
%s We don't need to apply post gain
%s Done prepareRecord with result: %{public}@.
%s xpcClient not existing
%s received lastVoiceTriggerInfo %{public}@, lastRTSTriggerInfo %{public}@
%s Activating Audio Session Now.
%s StreamProvider is already recording
%s duckingDelayedTime = %{public}f, timeIntervalSinceLastTriggerEnd = %{public}lf
%s Failed activate audio session with %{public}f seconds delay from prepareRecordWithSettings due to error %{public}@.
%s Finished activate audio session with %{public}f seconds delay from prepareRecordWithSettings.
%s Cancelled activate audio session with %{public}f seconds delay from prepareRecordWithSettings.
%s Scheduled SetRecordModeToRecording with %{public}f seconds delay in prepareRecordWithSettings.
%s Delayed active audio session: Consumed token %{public}@ with %{public}f seconds delay for reason %{public}@.
%s Delayed active audio session: Activating audio session for reason %{public}@.
%s Delayed active audio session: Failed to activate audio session for reason %{public}@ due to error %@.
%s Delayed active audio session: Successfully activate audio session for reason %{public}@.
%s Delayed active audio session: Ignored activating audio session for reason %{public}@ because the validator rejected.
%s Delayed active audio session: Ignored activate audio session for reason %{public}@ because the scheduled token %{public}@ does not match the current token %{public}@.
%s Delayed SetRecordModeToRecording: Scheduled new token %{public}@ with %{public}f seconds delay for reason %{public}@.
%s Delayed SetRecordModeToRecording: Cancelled token %{public}@ for reason %{public}@.
%s Delayed audio session activate: Consumed token %{public}@ in advance for reason %{public}@.
%s Delayed SetRecordModeToRecording: Setting record mode to recording for reason %{public}@.
%s Delayed SetRecordModeToRecording: Failed to set record mode to recording for reason %{public}@ due to error %@.
%s Delayed SetRecordModeToRecording: Successfully set record mode to recording for reason %{public}@.
%s AudioSession activated successfully ? %{public}@
%s AudioSession Provider not available
%s context : %{public}@
%s Resetting CoreSpeech frameworks
%s ERR: Last known assets are nil - Bailing out
%s Ask start recording from: %{public}tu
%s Voice trigger to use the current voice triggered channel: %{public}tu
%s Auto prompt to use the last voice triggered channel: %{public}tu
%s SpeechController to receive data from default channel
%s SpeechController to receive data from channel %{public}tu
%s Ask delay audio session active by %{public}f seconds
%s Sending client speechControllerDidStartRecording successfully? %{pubic}@
%s audioStream not existing
%s _activateAudoiSessionWithDelay has failed. startRecordWithSettings has failed
%s Start recording invoked too late (%{public}.3f seconds), override scheduledCheckTime: %{public}llu to currentTime: %{public}llu
%s Scheduled audible feedback decision after %{public}.3fseconds (vtEndMachTime: %{public}llu currentMachTime: %{public}llu)
%s Two shot audible feedback decision not needed since we already stopped recording
%s Two shot audible feedback decision (%{public}.3fs later than the scheduled time), elapsedTimeWithNoSpeech: %{public}.3f
%s Two shot audible feedback is needed, should notify? [%{public}@]
%s Two shot audible feedback decision timed out while waiting for Myriad decision
%s Phatic not needed since we already stopped recording
%s Phatic decision elapsedTimeWithNoSpeech: %{public}.3f
%s Notifying scheduled phatic playback...
%s Failed to playback phatic, error: %{public}@
%s Asking stopRecording when audio stream is not existing
%s Options: %{public}@ at: %{public}llu
%s Reporting didDeliverLastPacket at: %{public}llu
%s SpeechEndEstimation: %{public}llu
%s Reason : %{public}ld
%s SpeechEndEstimation: Should Estimate SpeechEndHostTime
%s Sending client speechControllerDidStopRecording for reason : %{public}d, at: %{public}llu
%s SpeechEndEstimation: Reporting didStop with estimated speech end : %{public}llu, at: %{public}llu
%s _didDeliverLastPacket=%d. Dropping Audio packets of size=%lu
%s SpeechManager still forwarding audio after didStopForwarding, we shouldn't have this
%s AudioProvider is invalidated, teardown connection to audioprovider
%s SmartSiriVolume update reason: %lu
%s SpeechController is trying to forward encoded audio after didStopForwarding, we shouldn't have this
%s Setting Alert Sounds From : %{public}@ for AlertType : %{public}d
%s Creating Audio Power Meter with record route %{public}@
%s We don't need Audio Power Meter with record route %{public}@
%s Not available
%s received VoiceTriggerInfo %{public}@
%s Requesting QuickStop operation upon detecting keyword
%s AudioProvider has been quired : %{public}p
%s Unable to prepareAudioProvider in _xpcClient, teardown XPC connection again
%s Cancelling current language detector request !
%s Received Myriad started
%s Received Myriad finished with decision: %tu
%s Received unknown media playing state, ignoring
%s Received unknown alarm playing state, ignoring
%s Received unknown timer playing state, ignoring
%s Detected sound is%{public}@ playing: media(%d) alarm(%d) timer(%d)
%s SpkrId:: %@
%s SpkrId:: Discarded speakerId scores for {%@, %d} activation
%s SpkrId:: SpeakerIdInfo from incorrect SpeakerRecognizer: expected: %{public}@, spkrRecognizer: %{public}@
%s XPCConnection disconnected
%s reset audioProvider since xpcClient disconnected
%s reset bargeInModeProvider since _bargeInModeProvider disconnected
%s CS doesn't have ndblobbuilder!
%s latestMajorVersion = %d, LatestMinorVersion = %d
%s corespeech.json doesn't contains rtblobs
%s blob file name is not exists
%s blob file is not exists at %{public}@
%s Reading blob from : %{public}@
%s Blob is nil : %{public}@
%s RTLocaleMap is not available on asset
%s Fired VoiceTrigger Timeout
%s Stop right now since ASR has issue
%s EOS Timeout Fired
%s Force endpoint fired
%s Discarding surplus audio of %{public}lu samples, audio sample available %{public}lu (%{public}lu, %{public}lu, %{public}lu)
%s AudioSession Started
%s AudioSession Stopped
%s unknown endpoint type
%s Called with status : %{public}d and success : %{public}d utteranceStored : %{public}d
%s ERR: Failed to store utterance, overiding status
%s Non Final: [%{public}@]
%s NON Final Matching
%s Final: [%{public}@]
%s Final Matching
%s Final Not Matching
%s SPEECH RECOGNITION TASK FINISH UNSUCCESSFULLY
%s %{public}@ %{public}@ %{public}ld %{public}@
%s Recog Result: [%{public}ld]
%s value = %{public}d
%s Couldn't create speech log directory at path %{public}@ %{public}@
%s Couldn't create SoS log directory at path %{public}@ %{public}@
%s enableAudioInection: is only available on internal builds
%s setAudioInjectionFilePath: is only available on internal builds
%s kCSAudioInjectionFilePathKey is not array type
%s kCSAudioInjectionFilePathKey array size = %d
%s kCSAudioInjectionFilePathKey doesn't have NSString as an array entry
%s Override iOS barge-in support key to: %{public}@
%s Shouldn't be called on non-iOS platform
%s Start Recording Host Time = %{public}llu
%s %p created
%s %p dealloced
%s sp=%p
%s %@ not supported yet.
%s Failed to create: %@
%s SigPrvdrs: %@
%s Starting datasrc: %@
%s Failed to start %@. Err=%@
%s >>> All DataSources Started within timeout of 2secs: timeTaken=%f ms
%s WARN: DataSources Start timedout. timeout=2secs
%s Starting signal provider: %@
%s Failed to start %@: Err=%@
%s >>> All SignalProviders didStart within timeout of 2secs: timeTaken=%f ms
%s WARN: SignalProviders timedout didStart. timeout=2secs
%s >>> All DataSources Stopped within timeout of 2secs: timeTaken=%f ms
%s WARN: DataSources timedout stopping. timeout=2secs
%s Failed to stop %@: Err=%@
%s >>> All SignalProviders didStop within timeout of 2secs: timeTaken=%f ms
%s WARN: SignalProviders timedout didStop. timeout=2secs
%s WARN: Cannot find SignalProvider for %@. Skipping
%s Sending XPCClientType : %{public}d
%s Prepare Audio Provider with Context : %{public}@
%s Failed to get reply result correctly
%s Received alertStartTime = %{public}llu
%s Received peakPower = %{public}f
%s Received averagePower = %{public}f
%s Sending audioMetric request
%s Failed to get audioMetric reply
%s audioMetric : %{public}@
%s Received invalid audioMetric
%s Error creating message
%s audioStreamWithRequest for stream %{public}@
%s Invalid message: stream is nil or request is nil
%s PrepareAudioStream %{public}@
%s Sending VoiceTriggerInfo request
%s Failed to get VoiceTriggerInfo request
%s Received VoiceTriggerInfo %{public}@
%s Failed to parse VoiceTriggerInfo from raw data
%s Received rtsTriggerInfo %{public}@
%s Failed to parse rtsTriggerInfo from raw data
%s Message not valid
%s ::: %{public}s enable: %{public}d reason: %{public}@
%s assertionType = %{public}lld
%s Not implemented
%s No message!!
%s NO reply!!!
%s No reply for hostTimeFromSampleCount request with sampleCount %{public}llu
%s No message for hostTimeFromSampleCount request with sampleCount %{public}llu
%s No reply for sampleCountFromHostTime request with hostTime %{public}llu
%s No message for sampleCountFromHostTime request with hostTime %{public}llu
%s No message for barge-in mode request
%s Cannot handle nil message
%s Unexpected message type : %{public}lld
%s AlertProvidingDelegate messageType : %{public}lld
%s Unexpected type : %{public}lld
%s SessionProvidingDelegate messageType : %{public}lld
%s invalid context
%s SessionInfoProvidingDelegate messageType : %{public}lld
%s Received notificationInfo %{public}@
%s Failed to parse notificationInfo from raw data
%s invalid packets
%s Failure disposing audio file %{public}d
%s Audio file already configured, closing first
%s Creating audio file at URL %{public}@
%s Failed creating audio file at url %{public}@ %{public}d
%s Error setting input format %{public}d
%s No audio file to append data
%s Failed writing audio file %{public}d
%s Closing file at URL %{public}@, audio size: %{public}u
%s _csAssetsDictionary = %{public}@
%s CSAssetController cannot query for nil language
%s ::: found %{public}lu installed assets for assetType=%{public}lu, matching query: %{public}@
%s Error running asset-query for assetType:%{public}lu, query: %{public}@, error: %{public}lu
%s ::: found %{public}lu assets for assetType=%{public}lu, matching query: %{public}@
%s Asset state : %{public}ld
%s ::: %{public}s
%s ::: %{public}s; query: %{public}@
%s Found %{public}lu assets
%s Error running asset query: error %{public}lu, or result is empty
%s ::: Request Fetching RemoteMetaData : assetType : %{public}d
%s ::: Request fetching remote asset
%s ::: found %{public}lu assets for assetType %{public}lu
%s Failed to finish query for assetType %{public}lu with error %{public}lu
%s Meta data downloaded successfully for assetType %{public}lu
%s Failed to download meta data for assetType %{public}lu with error %{public}lu
%s ::: Fetching remote asset
%s ::: Purging installed asset : %{public}@
%s ::: Request downloading remote asset for assetType %{public}lu
%s ::: Start downloading asset
%s ::: download progress: %{public}3.0f%%
%s ::: Error downloading from %{public}@ with error %{public}@
%s ::: download completed successfully from %{public}@.
%s Attempting to download asset %{public}@, asset state : %{public}ld
%s ERR: Unknown AssetType: %{public}lu
%s Start monitoring : SpeechControllerGibraltar event
%s Will notify speechController record state change to %{public}lu in %{public}f seconds
%s There is no pending event to timeout
%s Received duplicated recording state %{public}lu -> %{public}lu
%s Notifying speechController record state change to %{public}lu
%s SpkrId:: Unknown CSSpIdType string: %@
%s ERR: Unknown RunMode: %lu
%s ERR: Unknown CSSpIdType: %lu
%s Unknown CSSpIdType: %lu
%s SpkrId:: ERR: spIdRootDirForLocale called with locale=nil
%s SpkrId:: Incorrect usage of API - Bailing out
%s SpkrId:: path is nil - Bailing out
%s SpkrId:: Direntry with same name exists, this will be removed: %@
%s SpkrId:: Creating Directory : %@
%s SpkrId:: Creating Directory failed : %@
%s SpkrId:: Exceeded privacy limit for grading utterances : %ld (%d)
%s ERR: reading contents of gradingDir: %{public}@ with error %{public}@
%s Deleting orphaned grading file %{public}@
%s SpkrId:: VoiceId not supported in language %{public}@
%s Unable to read data from file: %@
%s SpkrId:: Could not read existing %@ file: err: %@
%s SpkrId:: ERR: Could not read existing %@ file: err: %@
%s SpkrId:: ERR: %@ is a directory
%s SpkrId:: ERR: Could not find version file - %@
%s ERR: Migration API called on HomePod - Bailing out
%s ERR: Failed to move %{public}@ to %{public}@ with error %{public}@
%s ERR: Failed to delete %{public}@ with error %{public}@
%s Successfully moved %{public}@ to %{public}@
%s Skipping moving of file %{public}@
%s Coudn't fetch the list of files at path: %{public}@ %{public}@
%s Migrating Voice Profile for %{public}@ from %lu to %lu not supported
%s Could not read existing %@ file: err: %@
%s ERR: error creating updatedVoiceProfileJsonData from: %@, err: %@
%s ERR: error removing voice profile version file at: %@, err: %@
%s ERR: Error writing voice profile version file at: %@, err:%@
%s Unknown Device category for deviceProduceType: %@
%s ERR: Unknown device. returning false: %{public}@
%s ERR: satLanguagePath is nil. returning false
%s Voice Profile Mismatch - CurrentDeviceCategory %@ VoiceProfileCategory %@
%s Malformed audio-dir URL for string <%{public}@>:url
%s ERR: reading contents of audioDir: %{public}@
%s No jsonFiles found in %{public}@: jsonFiles.count=%{public}lu
%s Unexpected: empty JSON data for file: %{public}@
%s Error reading metaDict at path: %{public}@
%s metaProductType: %{public}@
%s vtprofile: currDevice=[%{public}@:%{public}@] ; vpDirDevice=[%{public}@:%{public}@]
%s Could not find productType in VT-Meta file, trying next one
%s No compatible VT profile found for CurrDevice: %{public}@
%s ERR: fetching contents of %{public}@ failed with error %{public}@
%s ERR: Directory is nil - Bailing out
%s ERR: %{public}@ doesnt exist - Bailing out
%s Dump content of %{public}@ - %{public}@
%s %{public}@ is already in marked list - Skipping update
%s ERR: marked profiles to upload is nil!
%s ERR: %{public}@ is not in marked list - %{public}@
%s Error reading directory at %@: err: %@
%s %@ is empty
%s Fetching homeUserId for siriProfileId %{public}@
%s siriProfileId %{public}@ maps to homeUserId %{public}@
%s ERR: Home User Id erred %{public}@ for Siri Profile Id %{private}@
%s ERR: %@
%s Incorrect logitCeil %f and logitFloor %f - defaulting them to %f and %f
%s satAudioDirectory is nil - Bailing out
%s metaVersionFile %{public}@ doesnt exist - Skipping
%s ERR: date is nil - Bailing out
%s Filtered %@ with enrolled date %@
%s ERR: trying to remove %@ directory, bailing out
%s ERR: satTDModelDirector is nil for LanguageCode %@
%s Couldn't delete invalidated speaker model at path %{public}@ %{public}@
%s SmartSiriVolume cannot be resumed since Siri is speaking
%s Device supporting barge-in ? %{public}@
%s VoiceTrigger cannot be turned on since SpeechDetectionVAD is not present
%s VoiceTrigger cannot be turned on since voiceTriggerInCoreSpeech is NO
%s VoiceTrigger cannot be turned on since VoiceTrigger is disabled
%s VoiceTrigger cannot be turned on since Siri is disabled
%s VoiceTrigger cannot be turned on since SpringBoard is not started yet
%s VoiceTrigger cannot be turned on since device is not unlocked after restart
%s VoiceTrigger cannot be turned on since device is on battery
%s VoiceTrigger cannot be turned on since Siri is restricted on lock screen
%s VoiceTrigger cannot be turned on since Software Update Checking is running
%s Start monitoring : speech endpoint asset meta update
%s Stop monitoring : speech endpoint asset meta update
%s New speech endpoint asset is available
%s Cannot get a VoiceTrigger asset : %{public}@
%s Asset Query failed : %{public}@
%s cached asset:%{public}@, new asset:%{public}@
%s New asset is same as cached asset, ignore notification
%s New asset is different from cached one. Updating cached asset
%s new VoiceTrigger asset downloaded
%s Language Code Changed : %{public}@
%s First unlock notification received : %{public}d
%s CommandControl Streaming = %{public}d
%s Turn on AP mode since command control is streaming
%s VoiceTrigger AOP mode cannot be turned on since builtIn speaker is active
%s VoiceTrigger AOP mode cannot be turned on since Siri client is recording
%s AOP Listening is disabled
%s Speech Detection VAD is not available, we will still running in AOP mode
%s PowerLog : Trying to log falseWakeUp : %{public}d
%s ::: incrementing false wakeup to %{public}llu
%s ::: accumulated false wakeup count is %{public}llu so far, not reporting yet because it has been only %{public}.2f seconds since last report
%s WARN: Invalid sigType: %lu
%s Unknown DataSrc Type: %{public}lu
%s Unknown DataSrcTypeStr(%{public}@)
%s Failed to create dir at: %{public}@
%s Json file doesnt exist at: %{public}@
%s Could not read Json file at: %{public}@, err: %{public}@
%s Failed to parse json at: %{public}@, err: %{public}@
%s Could not find <%{public}@> in Keypath=%{public}@
%s Testing [%@] against regex.
%s Failed to write to: %{public}@. err: %{public}@
%s unbalanced dispatch_group_enter and leave : ignore we are ignore dispatch_group_leave
%s Estimated TTS volume : %{public}f
%s SmartSiriVolume: final estimated TTS volume %{public}f with music volume %{public}f
%s Wrongly called: SmartSiriVolume is not supported on this device type. smartSiriVolume : %{public}p
%s SmartSiriVolume not available
%s Non internal build, Ignoring command %@ from peerId %@ - Bailing out!
%s Received Malformed command %@ from peerId %@ - Bailing out!
%s Command %@ received from peerId %@
%s Unknown Command: (%@) - Ignoring
%s Triggering sync with peer - %@
%s Triggering nearmiss sync with peer - %@
%s Triggering voice profile sync with peer - %@
%s CSP2P_RemoteHeySiriCmd: ENABLE HeySiri: Not Implemented Yet: 
%s CSP2P_RemoteHeySiriCmd: DISABLE HeySiri: Not Implemented Yet: 
%s Cannot read contents of directory: %@, err: %@
%s Could not determine if [%@] is a directory or not. Err=%@
%s Found dir: %@. Skipping compression
%s _compressFilesInDirectory: Malloc failed for file %@ (%lu) - Discarding
%s _compressFilesInDirectory: Compression failed for file %@ (%lu) - Sending Uncompressed
%s _compressFilesInDirectory: File %@ compressed from %ld to %ld 
%s CSP2P_VoiceProfileParallelRecordingsFetchCmd: Cannot send VoiceProfile when _adCompanionServiceProvider is nil - returning
%s CSP2P_VoiceProfileParallelRecordingsFetchCmd: Message incomplete - Bailing out %@
%s CSP2P_VoiceProfileParallelRecordingsFetchCmd: File %@ isCompressed: %d, compressedSize: %ld, err: %@ 
%s CSP2P_VoiceProfileParallelRecordingsFetchCmd: Failed VoiceProfileTransfer: %@, error %@
%s Failed to remove the file %@
%s Failed in compressing %{public}@ with errror %{public}@ - Bailing out
%s Transfering NearMiss file %@ withCompression %{public}@ and size %ld in batch %{public}@
%s Failed moving file from %@ to %@ with error %@
%s Transfering VoiceId file %@ withCompression %{public}@ and size %ld in batch %{public}@
%s %@ is nil - Bailing out
%s Failed in transporting Voice file %@ with reponse: %@, error %@
%s Failed to remove the file %@ with error %@
%s Failed to move the file %@ to %@ with error %@
%s CSP2P_VoiceProfileParallelRecordingTransferCmd: received malformed command - %@ %@ %@
%s CSP2P_VoiceProfileParallelRecordingTransferCmd: unknown IDS peer with passed Identifier %@, %@ %@
%s CSP2P_VoiceProfileParallelRecordingTransferCmd: received malformed command - %@
%s CSP2P_VoiceProfileParallelRecordingTransferCmd: Creating directory failed with error %@
%s Ignoring sync of existing file %@ from %@
%s Syncing parallel recorded audio file - %@ from %@
%s Uncompressed file %@ sent by peer %@
%s ERR: Failed to allocate buffer of size %zu, bailing out
%s Writing to file(%@) failed!. Err=%@
%s received malformed command - %@ %@ %@
%s unknown IDS peer with passed Identifier %@, %@ %@
%s received malformed command - %@
%s Syncing audio file - %@ from %@
%s received Music account credentials before voice profile landed for %@
%s CSP2P_VoiceProfileTransferCmd: received malformed command - %@ %@ %@
%s CSP2P_VoiceProfileTransferCmd: received malformed command: CSP2P_VoiceProfileData_Key: %@CSP2P_VoiceProfileFileName_Key: %@CSP2P_VoiceProfileSpeakerName_Key: %@CSP2P_VoiceProfileLocale_Key: %@CSP2P_VoiceProfileDataType_Key: %@CSP2P_VoiceProfileTotalSegments_Key: %@CSP2P_VoiceProfileSegment_Key: %@
%s CSP2P_VoiceProfileTransferCmd: received command when speakerId is disabled - Bailing out
%s CSP2P_VoiceProfileTransferCmd: SpId Assets are not available - Bailing out
%s CSP2P_VoiceProfileTransferCmd: Received VoiceProfile Segment (%@/%@) from peerId %@
%s CSP2P_VoiceProfileTransferCmd: Failed to delete the directory %@ with error %@
%s CSP2P_VoiceProfileTransferCmd: received VoiceProfileSegment %@, expected %d
%s CSP2P_VoiceProfileTransferCmd: Creating directory failed with error %@
%s CSP2P_VoiceProfileTransferCmd: Writing to file failed!!!
%s Received request to delete VoiceProfile %@ from peerId %@
%s Cannot send data across when _adCompanionServiceProvider is nil - returning
%s ERR: Rejecting command %@ sent to non Horseman device
%s ERR: received malformed command - %@ %@
%s ERR: unknown IDS peer with passed Identifier %@, %@ %@
%s ERR: received malformed command with locale nil - %@
%s ERR: received malformed command with profileId nil - %@
%s ERR: Failed to find voice profile with identifier - %@
%s CSP2P_VoiceProfileFetchCmd: Transferring voice profile %{public}@
%s CSP2P_VoiceProfileFetchCmd: File %@ isCompressed: %d, compressedSize: %ld, err: %@
%s CSP2P_VoiceProfileReverseTransferCmd: Failed VoiceProfileTransfer: %@, error %@
%s CSP2P_VoiceProfileReverseTransferCmd: Successfully transferred %@
%s CSP2P_VoiceProfileReverseTransferCmd: Failed transferring voice profile %@ with error %@
%s CSP2P_VoiceProfileReverseTransferCmd: Successfully transferred voice profile %@
%s ERR: Rejecting command %@ sent to Horseman device
%s ERR: received malformed command with relative path nil - %@
%s Failed in sending trigger for Voice profile update to peer %@ with error %@
%s zeroFilterWinSz: %{public}tu, numHostTicksPerAudioSample: %{public}f
%s _vtEndInSampleCount:%{public}ld, _numSamplesProcessed: %{public}ld, voiceTriggerInfo: %{public}@
%s ERR: %{public}@
%s HEP enabled=%d
%s NDAPI initialization failed
%s set StartAnalyzeSampleCount = %{public}lld
%s NDAPI config doesn't contain threshold_normal
%s NDAPI config doesn't contain threshold_logging
%s NDAPI config doesn't contain threshold_reject_logging
%s ::: CoreSpeech logging initialized (%s)
%s Couldn't create CoreSpeech log directory at path %{public}@ %{public}@
%s Start monitoring : SACInfo
%s Stop monitoring : SACInfo
%s Device is in stereo mode : %{public}@
%s Dealloc of CSRemoteControlClient, it should close connection
%s ::: NVI logging initialized
%s Received external route change notification
%s Received CarPlay route change notification
%s Notifying Jarvis Connection State : %{public}d
%s VoiceTrigger is already %{public}@, received duplicated notification!
%s Start monitring : VoiceTrigger setting switch
%s Cannot start monitoring VoiceTrigger setting switch because it was already started
%s Stop monitring : VoiceTrigger setting switch
%s VoiceTrigger enabled = %{public}@
%s Creating audioRecorder has failed
%s AudioRecorder creation failed : %@
%s AudioStream Handle ID is invalid : %{public}@
%s Cannot prepare since audio recorder does not exist
%s AudioRecorder is already recording, do not prepare anymore
%s Failed to prepareAudioStreamRecord : %{public}@
%s Failed to startAudioStream : %{public}@
%s failed to stopRecording : %{public}@
%s Start monitoring : Siri setting switch, Siri is %{public}@
%s Stop monitoring : Siri setting switch
%s Siri Enabled = %{public}@
%s Start audio stream successfully ? %{public}@, error : %{public}@, startRecordingSampleCount=%lu
%s Stopped audioStream with result=%d, err=%@
%s audioProvider == nil, error : %{public}@
%s provider: %{public}@, unexpectedStop: %{public}ld
%s BeepCanceller asset file loading from : %{public}@
%s Could not read beep file: %@
%s beepVector Size = %{public}lu
%s Cannot initialize beep canceller
%s Beep canceller initialized with maxNumSamples = %{public}d
%s It will beep now
%s Reset beep cancellation
%s Vault already exists and is NOT secure. Attempting to secure: %{public}@, notSecure: %{public}d
%s Success setting up DataVault at: %{public}@
%s Received Activation Event : %{public}@
%s Received Activation Event in CoreSpeechDaemon: %{public}@
%s Returning error for already existing pending activation event : %{public}@
%s No delegate registered : Postpone activation event handling until we have delegate registered
%s Pending Timeout fired for %{public}@ returning error for timeout
%s There is no pending activation event to timeout
%s corespeechd received mediaserverd launched event
%s Cannot handle activation event : %{public}@
%s Found pending activation : %{public}@, handle pending activation immediately
%s AOP First Pass trigger detected
%s activation client not exist
%s Start monitoring: wake Gesture
%s ::: Received Gesture: %@
%s Wake Gesture: currDate=%@, wakeGestureTimeStamp=%@, 
currMachAbsTime=%llu, timeDelta=%f, 
timeDeltaInTicks=%llu, possWakeGesture=%llu
%s Error reading audio file: %{public}d, skipping...
%s Start monitoring : Software update checking state
%s Cannot start monitoring software update checking state because it was already started
%s Stop monitoring : Software update checking state
%s Software update checking running : %{public}@
%s AssetManager cannot be turned on since isFirstUnlocked is NO
%s AssetManager cannot be turned on since network is not available
%s numChannels: %{public}lu, recordingDuration: %{public}f, sampleRate: %{public}f
%s Cannot copy samples since this is empty
%s Could NOT copyFrom: %{public}lu to: %{public}lu, retSampleCount: %{public}lu
%s copyBuffer: oldestSample: %{public}lu latestSample: %{public}lu, numSamplesCopied: %{public}lu
%s CSAudioCircularBuffer.reset
%s saveRecordingBufferFrom: %{public}lu to: %{public}lu toURL: %{public}@
%s csrb: %{public}@
%s Invalid request: (%{public}lu, %{public}lu): noting to write to file
%s Invalid request: reqStartSample=%{public}lu, reqEndSample=%{public}lu, oldestSampleInBuffer: %{public}lu, latestSampleInBuffer=%{public}lu
%s Start monitoring : VoiceTrigger Asset meta update
%s Stop monitoring : VoiceTrigger Asset meta update
%s New VoiceTrigger asset metadata is available
%s stream %{public}@ initialized
%s stream %{public}@ is deallocated
%s Creating UUID for start audio stream request : %{public}@
%s AudioStream<%{public}@> is streaming : %{public}d
%s AudioStream<%{public}@> has received didStopStreamUnexpectly
%s AudioStream<%{public}@> has received didHardwareConfigurationChange
%s Start monitoring : Setting preference change
%s Stop monitoring : Setting preference change
%s Siri restricted on lock screen : %{public}@
%s Start monitoring : Springboard start
%s Cannot start monitoring Springboard start because it was already started
%s Stop monitoring : Springboard start
%s SpringBoard started = %{public}@
%s CSAudioProvider[%{public}@]:StreamState changed from : %{public}@ to : %{public}@
%s CSAudioProvider[%{public}@]:Setting audioRecorder : %{public}p
%s CSAudioProvider[%{public}@]:setCurrentContext : %{public}@
%s CSAudioProvider[%{public}@]:Cannot change context since audio recorder is currently recording
%s CSAudioProvider[%{public}@]:audioStreamWithRequest for stream <%{public}@>
%s Failed to _prepareAudioStreamSync : %{public}@
%s CSAudioProvider[%{public}@]:Prepare audio stream reuqested while state is %{public}@
%s CSAudioProvider[%{public}@]:Cannot prepare, audio system is recovering
%s CSAudioProvider[%{public}@]:Asking AudioRecorder prepareAudioStreamRecord
%s CSAudioProvider[%{public}@]:prepareAudioStreamRecord failed : %{public}@
%s CSAudioProvider[%{public}@]:Create circular buffer
%s CSAudioProvider[%{public}@]:Tear down circular buffer
%s CSAudioProvider[%{public}@]:startAudioStream with stream : %{public}@ with stream state : %{public}@, option : %{public}@, streamId : %{public}llu
%s CSAudioProvider[%{public}@]:state was %{public}@, prepareAudioStream first
%s CSAudioProvider[%{public}@]:prepareAudioStreamSync with stream : %{public}@ with stream state : %{public}@
%s CSAudioProvider[%{public}@]:prepareAudioStream with stream : %{public}@ with stream state : %{public}@
%s CSAudioProvider[%{public}@]:Cannot handle start audio stream on : %{public}@
%s CSAudioProvider[%{public}@]:Cannot startAudioStream, audio system is recovering
%s CSAudioProvider[%{public}@]:Requested startHostTime = %{public}llu, _clientStartSampleCount = %{public}tu
%s CSAudioProvider[%{public}@]:%{public}@ is requesting earlier audio than asked, we can't deliver earlier audio
%s CSAudioProvider[%{public}@]:Set circularBufferStartHostTime = %{public}llu, circularBufferStartSampleCount = %{public}lu
%s CSAudioProvider[%{public}@]:Entering dispatch group for recordingWillStartGroup
%s CSAudioProvider[%{public}@]:Failed to fetch historical audio since _clientStartSampleCount is newer than audioBuffer sample count(%{public}llu)
%s CSAudioProvider[%{public}@]:Leaving dispatch group for recordingWillStartGroup
%s CSAudioProvider[%{public}@]:Received didStartRecording while %{public}@
%s CSAudioProvider[%{public}@]:Received didStopRecording reason : %{public}d, streamState : %{public}@
%s CSAudioProvider[%{public}@]:Received didStopRecording while %{public}@
%s CSAudioProvider[%{public}@]:Waiting for recordingWillStartGroup before scheduling stopAudioStream
%s CSAudioProvider[%{public}@]:Scheduled stopAudioStream after waiting for recordingWillStartGroup - stopAudioStream %{public}@ with streamState : %{public}@
%s CSAudioProvider[%{public}@]:requested stop audio stream while stoppingWithScheduledStart, take out audio stream from schedule
%s CSAudioProvider[%{public}@]:Stream %{public}@ is not streaming on stream state : %{public}@, ignore the stopAudioStream request
%s CSAudioProvider[%{public}@]:Cannot handle stop audio stream on : %{public}@
%s CSAudioProvider[%{public}@]:requested stop audio stream while stopping, adding audio stream into stop pending
%s CSAudioProvider[%{public}@]:Stop all recordings, moving stream state to %{public}@
%s CSAudioProvider[%{public}@]:Failed to stop audioStream : %{public}@
%s CSAudioProvider[%{public}@]:%{public}@ ask for audio hold stream for %{public}f
%s CSAudioProvider[%{public}@]:Timeout for %{public}@ has fired
%s CSAudioProvider[%{public}@]:Removing %{public}@ from stream holders
%s CSAudioProvider[%{public}@]:%{public}@ stream holder was already removed from stream holders
%s CSAudioProvider[%{public}@]:%{public}@ ask for cancel hold stream
%s Failed to prewarmAudioSessionWithError : %{public}@
%s Failed to activateAudioSessionWithReason: %{public}@
%s CSAudioProvider[%{public}@]:Activating Audio Session under : %{public}@
%s Failed to activateAudioSession : %{public}@
%s Failed to deactivate audio session : %{public}@
%s CSAudioProvider[%{public}@]:Deactivating Audio Session under : %{public}@
%s Failed to deactivateAudioSession : %{public}@
%s CSAudioProvider[%{public}@]:AVVC is recovering, ignore command...
%s Fetching voiceTriggerInfo locally for context type : %{public}lld
%s Fetching voiceTriggerInfo from audioRecorder
%s CSAudioProvider[%{public}@]:Cannot stopRecording as there are %{public}tu streamHolders
%s CSAudioProvider[%{public}@]:Shouldn't stop AVVC recording as there are %{public}tu streams
%s CSAudioProvider[%{public}@]:
%s CSAudioProvider[%{public}@]:Buffer underrun!!!!, lastForwardedSampleTime:%{public}lu, oldestSampleTimeInBuffer:%{public}lu, stream:%{public}@
%s CSAudioProvider[%{public}@]:Ignore forwarding stream %{public}@                                    the audio packets until sampleCount == %{public}lu (theMostRecentSampleCount:%{public}lu)
%s CSAudioProvider[%{public}@]:Buffer overrun!!! lastForwardedSampleTime:%{public}lu,                                    theMostRecentSampleCount:%{public}lu, stream:%{public}@
%s ScheduleAlertFinishTimeout : %{public}@
%s ScheduleAlertFinishTimeout will be ignored : %{public}@, %{public}@
%s Received finishStartAlertPlaybackAt:%{public}llu streamState : %{public}@
%s CSAudioProvider[%{public}@]:Requested alertFinishHostTime = %{public}llu, _clientStartSampleCount = %{public}tu, circularBufferSampleCount = %{public}tu
%s Audio Streaming already stopped
%s Will invalidate current builtIn audio stream : %{public}@
%s failed to stopAudioStream : %{public}@
%s CSAudioProvider[%{public}@]:Audio Recorder Disconnected
%s CSAudioProvider[%{public}@]:Mediaserverd/bridgeaudiod crashed
%s CSAudioProvider[%{public}@]:Mediaserverd/bridgeaudiod recovered from crash
%s CSAudioProvider[%{public}@]:AudioRecorder will be destroyed
%s CSAudioProvider[%{public}@]:recordingTransaction already released
%s CSAudioProvider[%{public}@]:Release recording transaction at streamState : %{public}@
%s Schedule didStart WDT %{public}@ for %{public}lf seconds
%s startRecordingWatchDogDidFire : %{public}@, currentToken : %{public}@
%s startRecordingWatchDogToken doesn't match. Ignore this WDT fire
%s Clearing didStartRecordingDelegate WatchDogTimer : %{public}@
%s Schedule didStop WDT %{public}@ for %{public}lf seconds
%s stopRecordingWatchDogDidFire : %{public}@, currentToken : %{public}@
%s stopRecordingWatchDogToken doesn't match. Ignore this WDT fire
%s Clearing didStopRecordingDelegate WatchDogTimer : %{public}@
%s Listening on watch cannot be turned on since speech detection VAD is disabled
%s Listening on watch cannot be turned on since Siri is disabled
%s Listening on watch cannot be turned on since SpringBoard is not started
%s Listening on watch cannot be turned on since device is not unlocked after restart
%s Listening on watch cannot be turned on since Siri assertion is disabled
%s Listening on watch cannot be turned on since audioInjection is enabled
%s xpc object should be XPC_TYPE_ARRAY
%s xpcObject value is NULL
%s Cannot decode non-plist types of XPC object
%s Cannot encode non-plist types into XPC object : %{public}@
%s Received AOP Listening state change notification : %{public}d
%s Cannot generate subChunk since channel(%{public}tu) is larger than number of channels(%{public}tu)
%s Cannot generate subChunk if it reuqest more than it has : %{public}tu %{public}tu %{public}tu
%s SpkrId:: Processing ended at: numSamplesProcessed=%lu, totalSampleCountToReach=%lu
%s Delta is larger than anchorHostTime: anchorSampleCount = %{public}lld, sampleTime = %{public}lld, anchorHostTime = %{public}lld
%s Delta is larger than anchorSampleCount
%s Mediaserverd/bridgeaudiod crashed
%s Stop monitoring AudioSession activities
%s Saving utterance and meta as %{public}@ training.
%s Failed to write utterance into %{public}@
%s Saving %{public}@ at %{public}@ as %{public}@ training.
%s numSamplesToWrite %{public}lu
%s Failed to get CSAudioFileWriter:
%s Failed to addSamples to CSAudioFileWriter: %{public}@
%s Failed to endAudio on CSAudioFileWriter: %{public}@
%s ERR: called with nil metaPath
%s ERR: called with nil uttPath
%s ERR: called with nil uttMeta
%s Cannot create json file : %{public}@
%s Endpointer is disabled in RecordSettings: %@
%s CSHybridEndpointer canProcessCurrentRequest
%s CSHybridEndpointer can-NOT-ProcessCurrentRequest, fallback  to NNVAD
%s _activeEndpointer=%{public}@
%s shouldUseCVT2ShotDecision: %{public}d, isWatchRTSTriggered=%{public}d
%s preheat
%s endpointer: %{public}@: didDetectStartpointAtTime: %{public}f
%s %{public}@: Endpointer didDetectHardEndpointAtTime:withMetrics: %{public}f, CallingDelegate: %{public}@
%s Reported 2-shot at: %{public}f secs
%s speechcontroller.delegate doesnt respond to speechControllerDidDetectVoiceTriggerTwoShot
%s WARN: endpointerModelVersion called when CSHybridEndpointer is not available
%s Cannot create NSNumber if xpcObject is NULL
%s XPC object type should be BOOL, DOUBLE, INT64, or UINT64
%s Cannot create xpcObject if objcType is NULL
%s Cannot create xpcObject since there is no matching type
%s Stop monitoring : First unlock
%s PhraseSpotter enabled = %{public}@
%s PhraseSpotter is already %{public}@, received duplicated notification!
%s event = %{public}p client = %{public}p cannot handle event
%s ignore unknown types of message
%s message = %{public}p, client = %{public}p, cannot handle message
%s MessageType = %{public}lld
%s Received msg of type %{public}lld for utt %{public}@
%s Implicit utterence processing done with error %{public}@
%s Utt: %{public}@ removed from cache with error %{public}@
%s Received error %{public}@ from client %{public}@
%s Client %{public}p connection disconnected, noticing xpc listener
%s Cannot deactivateAudioSession since audio recorder doesn't exist
%s Cannot deactivateAudioSession with %{public}@
%s Start Of Speech Detector not supported !
%s Start monitoring : Mediaserverd crash / recover event
%s disconnect VoiceTriggerXPCClient
%s Notifying CoreSpeechDaemon launched
%s Start monitoring : corespeechd state
%s Cannot start monitoring corespeechd state because it was already started
%s Stop monitoring : corespeechd state
%s CoreSpeechDaemon state changed to %{public}u
%s network state notify key : %s
%s Start monitoring : network availability
%s Stop monitoring : network availability
%s Network availability changed
%s startListenWithOption : %{public}d, %{public}@
%s stopListenWithCompletion : %{public}d, %{public}@
%s hasRemoteVADAvailable : %d
%s hasVADAvailable : %d
%s didStopUnexpectly : %d
%s Fallback asset resource path : %{public}@
%s Cannot find corespeech asset from resourcePath : %{public}@
%s Configuration file is not exists : %{public}@
%s Cannot read configuration file : %{public}@
%s Cannot decode configuration json file : %{public}@
%s Configuration json file is not expected format
%s Cannot access to %{public}@ %{public}@ using default value
%s There is not audio buffer to convert. Skip this.
%s Resetting AudioConverter buffer
%s createAudioConverter : initial frames per buffer = dur %{public}.2f * sr %{public}.2f = %{public}u
%s Failed to get audioConverter property (kAudioConverterCurrentOutputStreamDescription) : %{public}d
%s _configureAudioConverter: encoded audio needs minimum of %{public}u bytes per output buffer
%s _configureAudioConverter: AudioConverterGetProperty(kAudioConverterPropertyMinimumOutputBufferSize) returned status %{public}d
%s _configureAudioConverter: final framesPerBuffer: %{public}u
%s _configureAudioConverter: _convertPacketCount: %{public}u
%s _configureAudioConverter: AudioConverterGetProperty(MaximumOutputPacketSize): returned status %{public}d
%s createAudioConverter: outputSizePerPacket: %{public}u
%s _configureAudioConverter: _convertAudioCapacity %{public}u bytes
%s Cannot create AudioConverter using AudioConverterNew : %{public}u
%s Cannot set encoder bit rate : %{public}u
%s Cannot set codecQuality : %{public}u
%s Start monitoring : Siri language code
%s Stop monitoring : Siri language code
%s Siri language changed to : %{public}@
%s Ignore notifying change of language code, since it is nil
%s RequiredSampleCount reached: currSampleCount=%{public}lu, endingSampleCount=%{public}lu
%s xpc object should be XPC_TYPE_DICTIONARY
%s xpcObject key or value is NULL
%s Cannot encode key into xpcobject since the key is not NSString class type
%s SAT successfully initialized : %{public}@
%s Cannot create CSVTUIKeywordDetector since there is no asset available
%s Cannot create CSVTUIKeywordDetector since we cannot initialize NDAPI
%s Failed to create AVVC : %{public}@
%s Create new CSAudioRecorder = %{public}p
%s CSAudioRecorder %{public}p deallocated
%s AVVC initialization failed
%s Successfully create AVVC : %{public}p
%s Calling AVVC setContext
%s Failed to get handle id : %{public}@
%s setContext elapsed time = %{public}lf
%s Calling AVVC setContextForStream : %{public}@
%s setCurrentContext elapsed time = %{public}lf
%s Calling AVVC prepareRecordForStream(%{public}llu) : %{public}@
%s AVVC prepareRecordForStream failed : %{public}@
%s prepareRecordForStream elapsed time = %{public}lf
%s ::: CSAudioRecord will inject audio file instead of recording
%s Resetting AudioFilePathIndex
%s Increase AudioFilePathIndex = %d
%s AudioFilePathIndex is out-of-boundary _audioFilePathIndex:%d injectAudioFilePaths:%d
%s AudioFilePathIndex:%d accessing:%@
%s Unable to find injectAudioFilePath = %@
%s Calling AVVC startRecordForStream : %{public}@
%s startRecordForStream failed : %{public}@
%s startRecordForStream elapsed time = %{public}lf
%s Call AVVC stopRecordForStream
%s Failed to stopRecordForStream : %{public}@
%s stopRecordForStream elapsed time = %{public}lf
%s Session State = %d
%s AudioSessionState = YES
%s AudioSessionState = NO
%s AVVC sampling rate = %{public}f
%s AVVC doesn't return sampleRate, assume it is default sample rate
%s isNarrowBand = YES for streamHandleId = %{public}lu
%s isNarrowBand = NO for streamHandleId = %{public}lu
%s Calling AVVC setSessionActive for Prewarm
%s Prewarm AudioSession has failed : %{public}@
%s Calling AVVC setSessionActivate for activation
%s AVVC setSessionActivate has failed : %{public}@
%s AVVC successfully activated audioSession
%s setSessionActivate elapsed time = %{public}lf
%s Calling AVVC setSessionActivate for deactivation : %{public}tu
%s Creating audio session with allow mixable audio while recording to YES
%s Failed to setAllowMixableAudioWhileRecording : %{public}@
%s Should not call setDuckOthersOptions with NO in B238
%s %{public}@ miniDucking now
%s enableMiniDucking elapsed time = %{public}lf
%s %{public}@
%s configureAlertBehavior elapsed time = %{public}lf
%s VoiceTriggerInfo is nil from AVVC
%s AVVCAudioBuffer contains %{public}d packet descriptions, size %{public}d, channels %{public}d. Ignoring
%s Bad packet length %{public}d. Skipping rest of record buffer.
%s Peak : %f, Avg : %f
%s packetCount %{public}d
%s Cannot handle audio buffer : unexpected format(%{public}u)
%s Compensating %{public}u channel(s), heartbeat = %{public}lld
%s Calling AVVC playAlertSoundsForType : %{public}ld
%s Received didStartRecording : %{public}p, forStream:%{public}llu, successfully:%{public}d, error:%{public}@
%s Received audio buffer %{public}d, heartbeat = %{public}llu, streamID (%{public}lu)
%s Received didStopRecording : %{public}p forStream:%{public}llu forReason:%{public}ld
%s Received Stream Invalidated : %{public}llu
%s toConfiguration : %{public}d
%s type : %{public}d, error : %{public}@
%s Encoder error : %{public}@
%s withContext : %{public}@
%s activate : %{public}d
%s AVVC lost mediaserverd connection
%s AVVC informed mediaserverd reset, no further action required
%s Failed to deinterleave the data: %{public}d
%s Cannot create de-interleaver using AudioConverterNew: %{public}d
%s Created de-interleaver
%s Unsupported audio format!
%s Could not encrypt data. Err=%{public}@
%s Failed to create ivtag-plist from dict: %{public}@, err=%{public}@
%s Failed to write ivData=%{public}@ at filepath=%{public}@, err=%{public}@
%s Failed to write encryptedData to file at: %{public}@, err=%{public}@
%s Failed to delete ivtag file when saving encryptedFile failed!: err=%{public}@
%s Could not read ivtag file: %{public}@, err: %{public}@
%s Could not create ivtagDict from ivtag-plist. Err=%@
%s Could not read encryptedData from file: %@, err: %@
%s Cannot create NSData with size 0
%s xpc object should be XPC_TYPE_DATA
%s Creating OS Transaction for %{public}@
%s Release OS Transaction for %{public}@
%s Abstract Impl. Returning nil
zPLR
zPLR
zPLR
zPLR
zPLR
zPLR
zPLR
zPLR
zPLR
zPLR
zPLR
zPLR
zPLR
zPLR
zPLR
zPLR
zPLR
zPLR
zPLR
zPLR
zPLR
zPLR
zPLR
zPLR
zPLR
zPLR
zPLR
zPLR
zPLR
zPLR
CSGestureMonitor
CSBluetoothWirelessSplitterInfo
NviDataLogger
NSStreamDelegate
NSObject
CSUtteranceMetadataManager
CSVoiceTriggerAssetHandler
CSAudioSessionController
CSAudioSessionInfoProvidingDelegate
CSXPCClientDelegate
CSSiriDebugConnection
CSCommandControlListenerOption
CSMediaPlayingMonitor
CSVolumeMonitor
CSSpeakerIdNviOrchestrator
CSTimerMonitor
CSAlarmMonitor
CSAudioStreamHolding
CSAssetManagerEnablePolicyFactory
CSBiometricMatchMonitor
AVVC
CSAudioZeroCounter
SmartSiriVolume
CSVoiceTriggerXPCService
CSVoiceTriggerXPCClientDelegate
CSAudioFileManager
SpIdMetadataLogging
CSVoiceTriggerAssetDownloadMonitor
CSInvalidSATEntriesCleaner
Directory
CSAsset
CSSmartSiriVolume
CSMediaPlayingMonitorDelegate
CSAudioStreamProvidingDelegate
CSSiriEnabledMonitorDelegate
CSAudioServerCrashMonitorDelegate
CSSiriClientBehaviorMonitorDelegate
CSVoiceTriggerDelegate
CSAlarmMonitorDelegate
CSTimerMonitorDelegate
CSVolumeMonitorDelegate
CSAudioPreprocessor
CSVoiceTriggerAwareZeroFilterDelegate
CSBeepCancellerDelegate
CSAssetDownloadingOption
CSSpIdVTTextDependentSpeakerRecognizer
CSBluetoothManager
CSUserVoiceProfileStore
CSSiriAssertionMonitor
CSXPCConnectionDelegate
CSAudioFileReader
ResourcePathHash
CSAudioRouteChangeMonitorImplWatch
CSAudioSampleRateConverter
CSLanguageDetectorAssetMonitor
XPCObject
CSOpportuneSpeakListener
CSSPGEndpointAnalyzerDelegate
CSVoiceIdXPCClient
CSSpIdProcessor
CSNNVADEndpointAnalyzer
CSEndpointAnalyzerImpl
CSEndpointAnalyzer
CSActivationXPCClient
CSLanguageDetector
CSAudioRecordContext
NSCopying
LanguageCode
CSStopRecordingOptions
CSVoiceProfileTrainer
CSMyriadSelfTriggerCoordinator
CSSelfTriggerDetectorDelegate
CSVTUITrainingManager
CSVTUITrainingSessionDelegate
CSVTUIAudioSessionDelegate
CSEndpointAnalyzerDelegate
LPCMTypeConversion
CSAggregator
CSSpeakerModel
CSCommandControlStreamEventMonitor
CSCommandControlBehaviorMonitorDelegate
CSSpeechControllerMonitor
CSSpIdVTSpeakerRecognizer
CSSpIdSpeakerRecognizer
CSSelectiveChannelAudioFileWriter
CSAudioFileWriter
CSAudioSessionMonitor
Indexing
CSCoreSpeechServices
CSOpportuneSpeakListenerDeviceManager
CSAVVoiceTriggerClientManager
CSConfig
CSEncryptedAudioFileReader
CSSpeechManager
CSVoiceTriggerAssetHandlerDelegate
CSActivationEventNotifierDelegate
CSAudioRecorderDelegate
CSVoiceTriggerXPCServiceDelegate
CSAudioProviderDelegate
CSSpeechEndHostTimeEstimator
CSClamshellStateMonitor
HybridEndpointer
CSSpIdImplicitTraining
CSVTUIEditDistance
CSCommandControlListener
CSVoiceProfileManager
CSAssetManager
CSVoiceTriggerAssetMetaUpdateMonitorDelegate
CSSpeechEndpointAssetMetaUpdateMonitorDelegate
CSAssetControllerDelegate
CSLanguageCodeUpdateMonitorDelegate
CSVTUITrainingSession
SFSpeechRecognitionTaskDelegate
CSVTUIEndPointDelegate
NviDirectionalitySignalProvider
NviSignalProvider
CSKeywordAnalyzerNDEAPI
CSSpeechController
CSAudioConverterDelegate
CSSpIdSpeakerRecognizerDelegate
CSSmartSiriVolumeControllerDelegate
CSAudioSessionProvidingDelegate
CSAudioAlertProvidingDelegate
CSAudioSessionControllerDelegate
CSSpeechManagerDelegate
CSContinuousVoiceTriggerDelegate
RTModel
CSVTUITrainingSessionWithPayload
CSSpIdTrainingParallelRecorder
CSPreferences
CSAudioTimeConverter
NviSignalProvidersController
CSXPCClient
CSAudioSessionProviding
CSFallbackAudioSessionReleaseProviding
CSAudioStreamProviding
CSAudioAlertProviding
CSAudioSessionInfoProviding
CSAudioMeterProviding
CSAudioMetricProviding
CSSmartSiriVolumeProviding
CSAudioTimeConversionProviding
CSTriggerInfoProviding
CSBargeInModeProviding
CSStateMachine
CSPlainAudioFileWriter
CSEventMonitor
CSAudioChunkForTV
CSVoiceTriggerDataCollector
CSAudioFileLog
CSAudioStreamRequest
CSSpIdSpeakerVectorGenerator
CSActivationEvent
CSSpIdTIOnlySpeakerRecognizer
CSScreenLockMonitor
CSAssetController
CSEventMonitorDelegate
Utils
CSSpeechControllerMonitorImpl
SpeakerId
CSAudioDecoder
CSAudioRouteChangeMonitor
CSSmartSiriVolumeEnablePolicy
AudioHardware
CSVoiceTriggerAssetChangeMonitor
NviAudioFileWriter
CSVoiceTriggerEnabledPolicyNonAOP
CSSiriClientBehaviorMonitor
CSSpeechEndpointAssetMetaUpdateMonitor
VoiceTriggerRecord
CSVoiceTriggerAssetHandlerMac
CSVoiceTriggerAssetDownloadMonitorDelegate
CSFirstUnlockMonitorDelegate
CSVoiceTriggerAOPModeEnabledPolicyIOS
CSVoiceTriggerStatAggregator
NviConstants
CSVoiceTriggerEventInfoProvider
NviUtils
CSVTUIRegularExpressionMatcher
CSEncryptedAudioFileWriter
CSAudioStartStreamOption
CSDispatchGroup
CSSmartSiriVolumeController
CSSmartSiriVolumeDelegate
CSBluetoothDeviceInfo
CSVTUIASRGrammars
NSURLSessionDelegate
CSP2PService
CSVoiceTriggerAwareZeroFilter
CSUtils
CSDiagnosticReporter
CSKeywordAnalyzerNDAPI
CSBuiltinSpeakerStateMonitor
NviDirectionalitySignalData
CSEndpointerMetrics
CSSACInfoMonitor
CSRemoteControlClient
CSVoiceTriggerRTModel
NSSecureCoding
NSCoding
CSAudioRouteChangeMonitorImpl
AudioStreamBasicDescription
CSUserVoiceProfile
CSiTunesAccountManager
CSVoiceTriggerEnabledMonitor
CSRemoteRecordClient
CSVoiceProfileContext
CSVTUIAudioSessionRemote
CSVTUIAudioSession
CSSiriEnabledMonitor
CSVoiceProfileRetrainManager
NviCSAudioDataSource
NviAudioDataSource
NviDataSource
CSAlertBehaviorPredictor
CSBeepCanceller
CSSpIdContext
Rootless
CoreSpeechXPCProtocol
CSActivationEventNotifier
CSGestureMonitorWatch
CSLSWakeGestureObserver
CSAudioZeroFilter
AudioFile
CSSoftwareUpdateCheckingMonitor
CSAssetManagerEnablePolicy
CSCoreSpeechServiceListenerDelegate
CSVoiceTriggerAOPModeEnabledPolicyFactory
CSBatteryMonitor
CSVoiceProfilePruner
CSAudioCircularBuffer
CSVoiceTriggerAssetMetaUpdateMonitor
CSSpeakerIdRecognizerFactory
CSAssetManagerEnablePolicyMac
CSRemoteVADCircularBuffer
CSAudioStream
CSServerEndpointFeatures
CSVTUIAudioSessionAVVC
AVVoiceControllerRecordDelegate
AVVoiceControllerPlaybackDelegate
CSVoiceTriggerSpeakerTrainer
CSSiriRestrictionOnLockScreenMonitor
MCProfileConnectionObserver
CSSpringboardStartMonitor
CSAudioProvider
CSAudioPreprocessorDelegate
CSListeningEnabledPolicyWatch
CSAlwaysOnProcessorStateMonitor
CSAudioChunk
CSNovDetectorResult
CSNovDetector
Bitset
CSHybridEndpointAnalyzer
CSOpportuneSpeakListenerOption
Time
CSAudioSessionInfoProvider
CSVoiceTriggerEnrollmentDataManager
CSEndpointerProxy
CSEndpointAnalyzerImplDelegate
CSFirstUnlockMonitor
CSAudioPowerMeter
CSPhraseSpotterEnabledMonitor
CSVoiceIdXPCConnection
CSSpIdSATAnalyzer
CSFallbackAudioSessionReleaseProvider
CSSPGEndpointAnalyzer
CSLanguageDetectorOption
CSStartOfSpeechDetector
CSAudioServerCrashMonitor
CSVoiceTriggerXPCClient
CSCoreSpeechDaemonStateMonitor
CSKeywordAnalyzerQuasar
CSNetworkAvailabilityMonitor
CSSpeechDetectionDevicePresentMonitor
CSSpeakerIdNviSignalReceiver
NviSignalProviderDelegate
CSAudioRecordDeviceInfo
CSOpportuneSpeakListnerTestService
CSOpportuneSpeakListenerDelegate
CSAudioConverter
CSLanguageCodeUpdateMonitor
CSPolicy
CSSpeakerIdNviAssetsProvider
NviAssetsProvider
RecordContext
CSSpeakerDetectorNDAPI
LanguageDetector
CSVTUIKeywordDetector
remoteVoiceActivityVADBuffer
CSAudioRecorder
CSAudioDecoderDelegate
CSAudioFileReaderDelegate
CSCommandControlBehaviorMonitor
Encryption
NviContext
CSAESKeyManager
CSJarvisTriggerModeMonitor
CSOSTransaction
NviSignalData
CSSRFUserSettingMonitor
init
sharedInstance
wakeGestureTimestamp
dismissalTimestamp
hostTimeToSeconds:
_startMonitoringWithQueue:
_stopMonitoring
isTriggerHandheld
setWakeGestureTimestamp:
setDismissalTimestamp:
_wakeGestureTimestamp
_dismissalTimestamp
array
string
appendFormat:
countByEnumeratingWithState:objects:count:
address
supportDoAP
splitterState
copy
addObject:
_hasDeviceTemporaryPairedNotInContacts
isTemporaryPairedNotInContacts
description
splitterDeviceList
addDeviceIntoSplitterDeviceList:
shouldDisableSpeakerVerificationInSplitterMode
.cxx_destruct
splitterEnabled
setSplitterEnabled:
_splitterDeviceList
_splitterEnabled
outputStreamToFileAtPath:append:
setDelegate:
currentRunLoop
scheduleInRunLoop:forMode:
open
hasSpaceAvailable
stringWithFormat:
lengthOfBytesUsingEncoding:
cStringUsingEncoding:
write:maxLength:
close
removeFromRunLoop:forMode:
isEqual:
class
self
performSelector:
performSelector:withObject:
performSelector:withObject:withObject:
isProxy
isKindOfClass:
isMemberOfClass:
conformsToProtocol:
respondsToSelector:
retain
release
autorelease
retainCount
zone
hash
superclass
debugDescription
stream:handleEvent:
initWithFilePath:appendHdr:
logData:
endRequest
oStream
setOStream:
_oStream
_getBaseMetaDictionaryForUtterancePath:
dictionaryWithDictionary:
setObject:forKeyedSubscript:
numberWithUnsignedInteger:
timeStampWithSaltGrain
numberWithBool:
_writeMetaDict:forUtterancePath:
deviceProductType
deviceProductVersion
deviceBuildVersion
dictionaryWithObjects:forKeys:count:
dataWithJSONObject:options:error:
localizedDescription
stringByReplacingOccurrencesOfString:withString:
writeToFile:atomically:
URLByAppendingPathComponent:
_saveMetaVersionFileAtPath:
defaultManager
arrayWithObjects:count:
contentsOfDirectoryAtURL:includingPropertiesForKeys:options:error:
getResourceValue:forKey:error:
boolValue
_upgradeLocaleDirectoryIfNecessary:
writeToURL:atomically:
lastPathComponent
spIdSATAudioDirForLocale:spidType:
fileURLWithPath:
_audioDirectoryNeedsUpgrade:
absoluteString
isEqualToString:
path
pathExtension
fileExistsAtPath:
_upgradeUtteranceMeta:
dataWithContentsOfURL:
JSONObjectWithData:options:error:
objectForKeyedSubscript:
unsignedIntegerValue
dictionary
dataWithContentsOfFile:
URLByDeletingPathExtension
URLByAppendingPathExtension:
submitDiagnosticReportWithType:withSubType:withContext:
stringByDeletingPathExtension
stringByAppendingPathExtension:
localeWithLocaleIdentifier:
setLocale:
setDateFormat:
dateFromString:
saveUtteranceMetadataForUtterance:enrollmentType:isHandheldEnrollment:triggerSource:audioInput:otherBiometricResult:containsPayload:
saveMetaVersionFileAtSATAudioDirectory:
upgradeMetaFilesIfNecessaaryAtSATRoot:
isUtteranceImplicitlyTrained:
getUtteranceEnrollmentType:
recordedTimeStampOfFile:
weakObjectsHashTable
removeObject:
voiceTriggerAssetHandler:didChangeCachedAsset:
sharedHandler
getVoiceTriggerAsset:
defaultFallbackModelIfNil:
registerObserver:
unregisterObserver:
notifyObservers:
queue
setQueue:
observers
setObservers:
_queue
_observers
dealloc
count
_startMonitoring
_getAudioSessionID
sharedPreferences
corespeechDaemonEnabled
_createXPCClientConnectionIfNeeded
sessionInfoProvider
audioSessionID
_getLocalAudioSessionID
audioSessionController:didReceiveAudioSessionInterruptionNotificationWithUserInfo:
audioSessionController:didReceiveAudioSessionRouteChangeNotificationWithUserInfo:
audioSessionController:didReceiveAudioSessionMediaServicesWereLostNotificationWithUserInfo:
audioSessionController:didReceiveAudioSessionMediaServicesWereResetNotificationWithUserInfo:
initWithType:
setSessionInfoProvider:
connect
opaqueSessionID
setShouldKeepConnection:
defaultCenter
_mediaServicesWereLost:
addObserver:selector:name:object:
_mediaServicesWereReset:
_registerInterruptionNotification
_registerAudioRouteChangeNotification
removeObserver:
removeObserver:name:object:
_handleInterruption:
_audioRouteChanged:
userInfo
audioSessionController:didReceiveAudioSessionOwnerLostNotification:
_teardownXPCClientIfNeeded
audioSessionInfoProvider:didReceiveAudioSessionInterruptionNotificationWithUserInfo:
audioSessionInfoProvider:didReceiveAudioSessionRouteChangeNotificationWithUserInfo:
audioSessionInfoProvider:didReceiveAudioSessionMediaServicesWereLostNotificationWithUserInfo:
audioSessionInfoProvider:didReceiveAudioSessionMediaServicesWereResetNotificationWithUserInfo:
CSXPCClient:didDisconnect:
getAudioSessionIDWithCompletion:
getAudioSessionID
xpcClient
setXpcClient:
shouldKeepConnection
_shouldKeepConnection
_sessionInfoProvider
_xpcClient
initWithAppBundleIdentifier:
initWithTaskDeliverer:
initWithMessage:makeAppFrontmost:
handleSiriRequest:deliveryHandler:completionHandler:
launchSiriDebugAppWithMessage:
defaultOption
_notifyObserver:mediaIsPlayingState:
enumerateObserversInQueue:
_notePossiblePlayPausedStateChange:
objectForKey:
notifyObserver:
CSMediaPlayingMonitor:didReceiveMediaPlayingChanged:
initializeMediaPlayingState
mediaPlayingState
mediaPlayingStateWithCompletion:
_mediaIsPlaying
fetchVolumeFromAVSystemControllerForAudioCategory:
_startObservingSystemControllerLifecycle
startObservingSystemVolumes
musicVolume
musicVolumeWithCompletion:
alarmVolume
systemVolumeDidChange:
systemControllerDied:
_musicVolumeLevel
_alarmVolumeLevel
_startSpIdNviSignalProcessing
_endSpIdNviSignalProcessing
_uploadDirectionVectorToAFAnalyticsOnQueue
initWithTaskIdentifier:
initWithAssetsProvider:dataSourceMap:signalProviderToDataSourceMap:
initWithQueue:
registerSignalProviderDelegateForAllSignalTypes:
vtEventInfo
setVoiceTriggerInfo:
setRequestHistoricalAudio:
voiceTriggerInfo
setReqStartAudioSampleId:
startWithNviContext:
stop
unregisterSignalProviderDelegateForAllSignalTypes:
signalBuffer
initWithArray:copyItems:
setObject:forKey:
sharedAnalytics
logEventWithType:context:
initWithSpeakerIdContext:
start
uploadDirectionVectorToAFAnalytics
signalControllerNvi
setSignalControllerNvi:
resultRxNvi
setResultRxNvi:
nviQueue
setNviQueue:
spIdCtx
setSpIdCtx:
_signalControllerNvi
_resultRxNvi
_nviQueue
_spIdCtx
initializeTimerState
timerState
_timerFiringState
initializeAlarmState
alarmState
_alarmFiringState
name
setName:
_name
assetManagerEnabledPolicy
startObserving
getLastBiometricMatchEvent:atTime:
getBiometricMatchResultForTriggerTimeStamp:
delegate
_delegate
avvcContext
initWithMode:deviceUID:
avvcContextSettings
zeroFilterWindowSizeInMs
shouldDeinterleaveAudioOnCS
bytes
initWithToken:sampleRate:numChannels:
resetWithSampleRate:
getZeroStatisticsFromBuffer:entireSamples:
stopReportZeroStatistics
_methodToken
_continuousZeroCounter
_zeroCounterWinSz
_numChannels
_analyzeStep
_sampleRate
_shouldDeinterleaveAudio
numberWithUnsignedLongLong:
getNumberForKey:category:default:
unsignedLongLongValue
numberWithUnsignedInt:
unsignedIntValue
numberWithFloat:
floatValue
SSVNoiseLevelChannelBitset
SSVLKFSChannelBitset
SSVEnergyBufferSize
SSVNoiseLowerPercentile
SSVNoiseUpperPercentile
SSVLKFSLowerPercentile
SSVLKFSUpperPercentile
SSVNoiseTimeConstant
SSVNoiseMicSensitivityOffset
SSVLKFSTimeConstant
SSVLKFSMicSensitivityOffset
SSVNoiseTTSMappingInputRangeLow
SSVNoiseTTSMappingInputRangeHigh
SSVNoiseTTSMappingOutputRangeLow
SSVNoiseTTSMappingOutputRangeHigh
SSVLKFSTTSMappingInputRangeLow
SSVLKFSTTSMappingInputRangeHigh
SSVLKFSTTSMappingOutputRangeLow
SSVLKFSTTSMappingOutputRangeHigh
SSVUserOffsetInputRangeLow
SSVUserOffsetInputRangeHigh
SSVUserOffsetOutputRangeLow
SSVUserOffsetOutputRangeHigh
SSVTTSVolumeLowerLimitDB
SSVTTSVolumeUpperLimitDB
SSVNoiseWeight
SSVParameterDirectionary
processInfo
systemUptime
enableVoiceTrigger:withAssertion:timestamp:
doubleValue
numberWithDouble:
enableAssertionReceived
disableAssertionReceived
setPhraseSpotterBypassing:timeout:
voiceTriggerXPCService:setPhraseSpotterBypassing:
notifyVoiceTriggeredSiriSessionCancelled
enableVoiceTriggerForCoreSpeechDaemon:withAssertion:timestamp:
setPhraseSpotterBypassingForCoreSpeechDaemon:timeout:
sharedService
sharedServiceForCoreSpeechDaemon
voiceTriggerXPCClient:didDisconnect:
enableVoiceTrigger:withAssertion:
notifyVoiceTriggeredSiriSessionCancelledForCoreSpeechDaemon
notifyServiceConnectionLostForCoreSpeechDaemon
activationAssertions
setActivationAssertions:
isPhraseSpotterBypassed
setIsPhraseSpotterBypassed:
_isPhraseSpotterBypassed
_activationAssertions
_sharedAudioLoggingQueue
fileURL
URLByDeletingLastPathComponent
assistantAudioFileLogDirectory
containsString:
removeItemAtURL:error:
inputRecordingNumberOfChannels
inputRecordingSampleRate
inputRecordingSampleByteDepth
seekToEndOfFile
seekToFileOffset:
readDataOfLength:
getBytes:length:
initWithData:encoding:
offsetInFile
length
writeData:
fileLoggingIsEnabled
_createAudioFileWriterForOpportuneSpeakListenerWithLoggingDir:inputFormat:outputFormat:
_createAudioFileWriterWithLoggingDir:inputFormat:outputFormat:
_getDateLabel
stringByAppendingPathComponent:
getNumElementInBitset:
lpcmNonInterleavedASBDWithSampleRate:numberOfChannels:
lpcmInterleavedASBDWithSampleRate:numberOfChannels:
initWithURL:inputFormat:outputFormat:channelBitset:
maxNumLoggingFiles
pruneNumberOfLogFilesTo:
URLWithString:
initWithURL:inputFormat:outputFormat:
stringFromDate:
removeLogFilesInDirectory:matchingPattern:beforeDays:
arrayWithObjects:
clearLogFilesInDirectory:matchingPattern:exceedNumber:
isAttentiveSiriAudioLoggingEnabled
assistantLogDirectory
fileExistsAtPath:isDirectory:
removeItemAtPath:error:
createDirectoryAtPath:withIntermediateDirectories:attributes:error:
utteranceFileASBD
generateDeviceAudioLogging:speechId:
_readDataFromFileHandle:toFileHandle:
createAudioFileWriterForOpportuneSpeakListenerWithInputFormat:outputFormat:
createAudioFileWriterForRemoteVADWithInputFormat:outputFormat:
createAudioFileWriterFromWithInputFormat:outputFormat:
createSelectiveChannelAudioFileWriterWithChannelBitset:
removeLogFilesOlderThanNDays:
audioFileWriterForAttentiveSiri
speakerIdDetected
spIdMapIdentifiersToSiriDebugID:
spIdComposeProfileVersionsFor:
userVoiceProfileForVoiceProfileID:
siriDebugID
siriProfileId
profileID
addEntriesFromDictionary:
stringWithUTF8String:
currentHandler
handleFailureInMethod:object:file:lineNumber:description:
logSpeakerIdMetadataAtFilepath:additionalMetadata:
_notificationKey
_didInstalledNewVoiceTriggerAsset
_notifyObserver:
enumerateObservers:
CSVoiceTriggerAssetDownloadMonitor:didInstallNewAsset:
_notifyToken
sanitizeSATFilesWithAsset:
cleanupInvalidSATEntriesAtSATRoot:payloadUtteranceLifeTimeInDays:dryRun:
dateWithTimeIntervalSinceNow:
distantFuture
compare:
URLsInDirectory:matchingPattern:completion:
objectAtIndex:
_sortedURLsInDirectory:matchingPattern:completion:
_contentsOfDirectoryAtURL:matchingPattern:includingPropertiesForKeys:error:
sortedArrayUsingComparator:
regularExpressionWithPattern:options:error:
numberOfMatchesInString:options:range:
predicateWithBlock:
filteredArrayUsingPredicate:
getLocalUrl
_compatibilityVersion
stringValue
appendString:
_version
_footprint
assetForAssetType:resourcePath:configVersion:
attributes
state
integerValue
isPremium
getCSAssetOfType:
isDownloading
isCSAssetInstalled
canBePurged
isLatestCompareTo:
UTF8String
_setDefaultParameters
_setAsset:
_convertDB2Mag:
_reset
fetchInitSystemVolumes
_resumeSSVProcessing
_pauseSSVProcessing
setCallback:
isEnabled
addObserver:
_startListenPolling
_startListenPollingWithInterval:completion:
audioStream
isStreaming
_startListenWithCompletion:
contextForBuiltInVoiceTrigger
sharedManager
audioProviderWithContext:error:
sharedManagerForCoreSpeechDaemon
defaultRequestWithContext:
audioStreamWithRequest:streamName:error:
setAudioStream:
mutedOption
_stopListening
startAudioStreamWithOption:completion:
stopAudioStreamWithOption:completion:
_getMusicVolumeDB:
_resetStartAnalyzeTime
convertToFloatLPCMBufFromShortLPCMBuf:
dataForChannel:
iterateBitset:block:
_prepareSoundLevelBufferFromSamples:soundType:
startSampleCount
_setStartAnalyzeTime:
numSamples
subChunkFrom:numSamples:
_processAudioChunk:soundType:
_estimatedTTSVolume:lowerLimit:upperLimit:TTSmappingInputRangeLow:TTSmappingInputRangeHigh:TTSmappingOutputRangeLow:TTSmappingOutputRangeHigh:
_combineResultsWithOptimalFromNoise:andOptimalFromLkfs:withUserOffset:
_scaleInputWithInRangeOutRange:minIn:maxIn:minOut:maxOut:
smartSiriVolumeSoftVolumeEnabled
CSSmartSiriVolumeDidReceiveAlarmChanged:
CSSmartSiriVolumeDidReceiveTimerChanged:
CSSmartSiriVolumeDidReceiveMusicVolumeChanged:
audioStreamProvider:didStopStreamUnexpectly:
audioStreamProvider:audioBufferAvailable:
audioStreamProvider:audioChunkForTVAvailable:
audioStreamProvider:didHardwareConfigurationChange:
CSSiriEnabledMonitor:didReceiveEnabled:
CSAudioServerCrashMonitorDidReceiveServerCrash:
CSAudioServerCrashMonitorDidReceiveServerRestart:
siriClientBehaviorMonitor:willStartStreamWithContext:option:
siriClientBehaviorMonitor:didStartStreamWithContext:successfully:option:
siriClientBehaviorMonitor:willStopStream:
siriClientBehaviorMonitor:didStopStream:
voiceTriggerDidDetectKeyword:deviceId:
voiceTriggerDidDetectNearMiss:
voiceTriggerDidDetectSpeakerReject:
keywordDetectorDidDetectKeyword
voiceTriggerGotSuperVector:
raiseToSpeakDetected:
CSAlarmMonitor:didReceiveAlarmChanged:
CSTimerMonitor:didReceiveTimerChanged:
CSVolumeMonitor:didReceiveMusicVolumeChanged:
CSVolumeMonitor:didReceiveAlarmVolumeChanged:
initWithSamplingRate:asset:
startSmartSiriVolume
setAsset:
prepareSoundLevelBufferFromSamples:soundType:firedVoiceTriggerEvent:triggerStartTimeSampleOffset:triggerEndTimeSampleOffset:
estimateSoundLevelbySoundType:
reset
estimatedTTSVolumeForNoiseLevelAndLKFS:LKFS:
.cxx_construct
listenPollingTimer
setListenPollingTimer:
listenPollingTimerCount
setListenPollingTimerCount:
_smartSiriVolumeNoiseLevel
_smartSiriVolumeLKFS
_floatBuffer
_defaults
_ssvEnablePolicy
_startAnalyzeSampleCount
_samplesFed
_processedSampleCount
_isStartSampleCountMarked
_isListenPollingStarting
_shouldPauseSSVProcess
_shouldPauseLKFSProcess
_alarmSoundIsFiring
_timerSoundIsFiring
_currentAsset
_musicVolumeDB
_alarmVolume
_noiseLevelChannelBitset
_LKFSChannelBitset
_energyBufferSize
_noiseLowerPercentile
_noiseUpperPercentile
_LKFSLowerPercentile
_LKFSUpperPercentile
_noiseTimeConstant
_noiseMicSensitivityOffset
_LKFSTimeConstant
_LKFSMicSensitivityOffset
_noiseTTSMappingInputRangeLow
_noiseTTSMappingInputRangeHigh
_noiseTTSMappingOutputRangeLow
_noiseTTSMappingOutputRangeHigh
_LKFSTTSMappingInputRangeLow
_LKFSTTSMappingInputRangeHigh
_LKFSTTSMappingOutputRangeLow
_LKFSTTSMappingOutputRangeHigh
_userOffsetInputRangeLow
_userOffsetInputRangeHigh
_userOffsetOutputRangeLow
_userOffsetOutputRangeHigh
_TTSVolumeLowerLimitDB
_TTSVolumeUpperLimitDB
_noiseWeight
_audioStream
_listenPollingTimer
_listenPollingTimerCount
supportZeroFilter
supportBeepCanceller
resetWithSampleRate:containsVoiceTrigger:voiceTriggerInfo:
setSampleRate:
_isNarrowBand:
upsampler
setUpsampler:
zeroFilter
beepCanceller
convertSampleRateOfBuffer:
processBuffer:atTime:
cancelBeepFromSamples:timestamp:
audioPreprocessor:hasAvailableBuffer:atTime:
flush
_reportMetrics
willBeep
inputRecordingSampleRateNarrowBand
zeroFilter:zeroFilteredBufferAvailable:atHostTime:
beepCancellerDidCancelSamples:buffer:timestamp:
initWithSampleRate:
sampleRate
setZeroFilter:
setBeepCanceller:
zeroCounter
setZeroCounter:
_upsampler
_zeroFilter
_beepCanceller
_zeroCounter
allowVoiceTriggerAssetDownloading
setAllowVoiceTriggerAssetDownloading:
allowEndpointAssetDownloading
setAllowEndpointAssetDownloading:
allowLanguageDetectorAssetDownloading
setAllowLanguageDetectorAssetDownloading:
_allowVoiceTriggerAssetDownloading
_allowEndpointAssetDownloading
_allowLanguageDetectorAssetDownloading
stringByAppendingFormat:
psrTdAssetExistsAtResourcePath:
initWithResourcePath:satDirectory:assetHash:shouldCreateModelDir:delegate:
getThresholdSAT
getCombinationWeight
getRejectLoggingThreshold
resetForNewRequest
processAudio:numSamples:
endAudio
satScore
updateSAT
deleteExistingSATModel
deleteVectorAtIndex:
logWithAudioFilepath:
scoreSpeakerVector:withDimensions:
tdPsrCanProcessRequest
tdSATModelFilePath
getSATVectorCount
lastRequestSatScore
speakerVector
setSpeakerVector:
_tdPsrCanProcessRequest
_lastRequestSatScore
_tdSATModelFilePath
_getSATVectorCount
_speakerVector
_attachBluetoothSession
_getWirelessSplitterInfoFromLocalDevice:
getBTLocalDeviceWithCompletion:
initWithCapacity:
initWithUTF8String:
setAddress:
setSupportDoAP:
setIsTemporaryPairedNotInContacts:
_tearDownLocalDevice
_detachBluetoothSession
_setUpLocalDevice
getWirelessSplitterInfoWithCompletion:
device:serviceMask:serviceEventType:serviceSpecificEvent:result:
_sessionAttached:result:
_sessionDetached:
_sessionTerminated:
localDevice:event:result:
bluetoothSession
setBluetoothSession:
isAttachingBluetoothSession
setIsAttachingBluetoothSession:
localDevice
setLocalDevice:
pairedDeviceAddresses
setPairedDeviceAddresses:
connectedDeviceAddresses
setConnectedDeviceAddresses:
bluetoothSessionSetupGroup
setBluetoothSessionSetupGroup:
_isAttachingBluetoothSession
_bluetoothSession
_localDevice
_pairedDeviceAddresses
_connectedDeviceAddresses
_bluetoothSessionSetupGroup
addUserVoiceProfile:fromUtteranceCache:withCompletion:
deleteUserVoiceProfile:
checkIfVoiceProfile:needsUpdatedWith:withCategory:
retrainVoiceProfilesForLanguage:withForceRetrain:withCompletion:
addImplicitUtterance:toVoiceProfile:withTriggerSource:withAudioInput:withCompletion:
userVoiceProfilesForLocale:
userVoiceProfileForSharedSiriDebugID:
userVoiceProfileForSiriProfileId:
resyncVoiceProfilesOnboardedThroughCloud
addiTunesAccountForVoiceProfile:withMultiUserToken:withDsid:withAltDsid:withHomeId:withHomeUserId:withCompletionBlock:
cleanupInvalidCloudOnBoardedProfilesWithDryRun:
_logVoiceProfileConfusionForAsset:withCleanup:
numberOfBaseProfileUtterancesToUpload
_numberOfBaseProfileUtterancesToUpload
CSSiriAssertionMonitor:didReceiveEnabled:
handleXPCMessage:messageBody:client:
CSXPCConnectionReceivedClientError:clientError:client:
_assertionState
_readAudioBufferAndFeed
audioFileReaderDidStartRecording:successfully:error:
dataWithLength:
audioFileReaderBufferAvailable:buffer:atTime:
audioFileReaderDidStopRecording:forReason:
initWithURL:
setRecordBufferDuration:
prepareRecording:
startRecording
stopRecording
readSamplesFromChannelIdx:
_fFile
_audioFeedTimer
_bufferDuration
_outASBD
firstMatchInString:options:range:
numberOfRanges
rangeAtIndex:
substringWithRange:
assetHashInResourcePath:
_fetchHearstConnectionState
_notifyHearstConnectionState:
CSAudioRouteChangeMonitor:didReceiveAudioRouteChangeEvent:
getHearstConnected:
hearstConnected
getJarvisConnected:
jarvisConnected
activeAudioRouteDidChange:
_isHearstConnected
lpcmNarrowBandASBD
lpcmASBD
initWithInASBD:outASBD:
_createSampleRateConverterWithInASBD:outASBD:
mutableBytes
setLength:
downsampler
_sampleRateConverter
_outBufferScaleFactor
_inASBD
languageDetectorAssetMonitor:didReceiveNewAssetWithSupportLocale:
_supportedLocale:
supportLanguageDetector
setAssetDownloadingOption:
languageDetectorSupportedLocale
assetOfType:language:completion:
errorWithDomain:code:userInfo:
startMonitor
supportedLocale:
notifyToken
setNotifyToken:
_cs_initWithXPCObject:
_cs_xpcObject
initWithAnalyzeMode
deviceId
lpcmNonInterleavedWithRemoteVADASBD
lpcmInterleavedWithRemoteVADASBD
setDeviceId:
contextForHearstVoiceTriggerWithDeviceId:
contextForOpportuneSpeakerListener
_resetAlignBuffer
prepareAudioProviderWithContext:clientType:error:
_startRequestWithCompletion:
removeAllObjects
setRequiresHistoricalBuffer:
getFrameDurationMs
remoteVADDuration
stopListenWithStateReset:completion:
opportuneSpeakListener:didStopUnexpectly:
data
apply12dBGain:
channelForProcessedInput
addAudio:numSamples:
remoteVAD
opportuneSpeakListenerBypassEnabled
_addRemoteVADSignal:
dataWithRemoteVADWithScaleFactor:numAudioSamplesPerRemoteVAD:
addSamples:numSamples:
removeObjectAtIndex:
_shouldReportBoron
_popRemoteVADSignal
opportuneSpeakListener:hasRemoteVADAvailable:
opportuneSpeakListener:hasVADAvailable:
startListenWithOption:completion:
stopListenWithCompletion:
spgEndpointAnalyzer:hasSilenceScoreEstimate:
spgEndpointAnalyzer
setSpgEndpointAnalyzer:
remoteVADSPGRatio
setRemoteVADSPGRatio:
audioStreamProvider
setAudioStreamProvider:
audioSessionProvider
setAudioSessionProvider:
latestContext
setLatestContext:
isMediaPlayingNow
setIsMediaPlayingNow:
remoteVADAlignBuffer
setRemoteVADAlignBuffer:
remoteVADAlignCount
setRemoteVADAlignCount:
alignBufferQueue
setAlignBufferQueue:
audioFileWriter
setAudioFileWriter:
_isMediaPlayingNow
_remoteVADSPGRatio
_spgEndpointAnalyzer
_audioStreamProvider
_audioSessionProvider
_latestContext
_remoteVADAlignBuffer
_remoteVADAlignCount
_alignBufferQueue
_audioFileWriter
_handleListenerEvent:
disconnect
_handleListenerError:
initWithDictionary:
_sendMessage:connection:completion:
_decodeError:
notifyImplicitUtterance:audioDeviceType:audioRecordType:voiceTriggerEventInfo:otherCtxt:completion:
xpcConnection
setXpcConnection:
_xpcConnection
initWithSpIdContext:forSpIdType:delegate:
spIdType
satScoreThreshold
processAudioData:
endProcessing
setCVTTriggerPhraseDetected
updateModelWithBestScoreUser:
rejectUtterance
logUtteranceUnderDirectory:withScores:withWinner:
sysConfigFilepath
_sysConfigFilepath
preheat
endpointStyle
setEndpointStyle:
delay
setDelay:
startWaitTime
setStartWaitTime:
automaticEndpointingSuspensionEndTime
setAutomaticEndpointingSuspensionEndTime:
minimumDurationForEndpointer
setMinimumDurationForEndpointer:
lastEndOfVoiceActivityTime
lastStartOfVoiceActivityTime
bypassSamples
setBypassSamples:
endpointMode
setEndpointMode:
interspeechWaitTime
setInterspeechWaitTime:
endWaitTime
setEndWaitTime:
saveSamplesSeenInReset
setSaveSamplesSeenInReset:
resetForNewRequestWithSampleRate:recordContext:recordSettings:
processAudioSamplesAsynchronously:
stopEndpointer
recordingStoppedForReason:
trailingSilenceDurationAtEndpoint
implDelegate
setImplDelegate:
canProcessCurrentRequest
activeChannel
setActiveChannel:
processServerEndpointFeatures:
updateEndpointerThreshold:
updateEndpointerDelayedTrigger:
shouldAcceptEagerResultForDuration:resultsCompletionHandler:
handleVoiceTriggerWithActivationInfo:
endpointerModelVersion
elapsedTimeWithNoSpeech
xpcObject
notifyActivationEvent:completion:
recordRecognitionLanguage:
setMostRecentRecognitionLanguage:
setInteractionIDforCurrentRequest:
cancelCurrentRequest
resetForNewRequest:
initWithRecordType:deviceId:
contextForServerInvoke
recordTypeFromAVVCActivationMode:
setType:
setAlwaysUseRemoteBuiltInMic:
copyWithZone:
_createAVVCContextWithType:deviceId:
avvcActivationMode:
numberWithInteger:
type
isBuiltInVoiceTriggered
isHearstVoiceTriggered
isJarvisVoiceTriggered
isHearstDoubleTapTriggered
recordTypeString:
contextForJarvisWithDeviceId:
contextForBTLE
contextForVoiceTriggerTraining
defaultContext
initWithXPCObject:
initWithAVVCContext:
isVoiceTriggered
isTriggeredFromHearst
isRTSTriggered
isServerInvoked
isStarkTriggered
alwaysUseRemoteBuiltInMic
_alwaysUseRemoteBuiltInMic
_type
_deviceId
getSiriLanguageWithFallback:
stopRecordingReason
setStopRecordingReason:
_stopRecordingReason
initWithVoiceProfile:spIdType:satRunMode:languageCode:asset:
addUtterances:withCompletion:
processUtterance:ofSpIdType:withUpdatePolicyBlock:withCompletionBlock:
checkIfUpdateNecessaryForAudioFileCount:
checkIfRetrainingIsRequired
checkIfImplicitTrainingRequired
checkIfImplicitSATPossibleWithBaseProfileVectorCount:
checkIfProfileNeedsUploadForBaseProfileVectorCount:
baseProfileConfidenceScoreThreshold
implicitConfidenceScoreThreshold
implicitDeltaConfidenceScoreThreshold
maxNumberOfBaseProfileUtterances
satVTImplicitThreshold
_satVTImplicitThreshold
_baseProfileConfidenceScoreThreshold
_implicitConfidenceScoreThreshold
_implicitDeltaConfidenceScoreThreshold
_maxNumberOfBaseProfileUtterances
dataWithCapacity:
appendBytes:length:
myriadHashFilePath
CSMyriadSelfTriggerCoordinator:didGenerateMyriadPHashForSelfTrigger:
selfTriggerDetector:didDetectSelfTrigger:
initWithLocaleIdentifier:withAudioSession:
setLocaleIdentifier:
createKeywordDetector
initWithLanguageCode:
initWithLocale:
prepareRecord
isRecording
releaseAudioSession
_stopAudioSession
destroySpeakerTrainer
_destroyAudioSession
_setupAudioSession
closeSessionBeforeStartWithStatus:successfully:withCompletion:
_createAudioAnalyzer
_shouldShowHeadsetDisconnectionMessage
_startAudioSession
createSpeechRecognizer
sharedtrainingSessionQueue
initWithUtteranceId:sessionNumber:Locale:audioSession:keywordDetector:speechRecognizer:speechRecognitionRequest:sessionDelegate:sessionDispatchQueue:completion:
startTraining
suspendTraining
closeSessionWithStatus:successfully:complete:
_audioSource
audioSource
hasCorrectAudioRoute
resumeTraining
VTUITrainingManagerFeedLevel:
VTUITrainingManagerStopListening
sharedTrainer
trainUtterance:languageCode:withAsset:
audioSessionDidStartRecording:error:
audioSessionDidStopRecording:
initWithData:numChannels:numSamples:sampleByteDepth:startSampleCount:hostTime:remoteVAD:
audioSessionRecordBufferAvailable:
audioSessionErrorDidOccur:
audioSessionUnsupportedAudioRoute
didDetectBeginOfSpeech
didDetectEndOfSpeech:
trainingManagerWithLocaleID:
CSVTUITrainingSessionRMSAvailable:
CSVTUITrainingSessionStopListen
CSVTUITrainingSession:hasTrainUtterance:languageCode:payload:
endpointer:didDetectStartpointAtTime:
endpointer:didDetectHardEndpointAtTime:withMetrics:
prepareWithCompletion:
_beginOfSpeechDetected
_endOfSpeechDetected
cleanupWithCompletion:
trainUtterance:shouldUseASR:completion:
cancelTrainingForID:
suspendAudio
setSuspendAudio:
startRMS
stopRMS
shouldPerformRMS
didDetectForceEndPoint
setRms:
speechRecognizerAvailable
_performRMS
_locale
_audioSession
_audioAnalyzer
_keywordDetector
_trainingSessions
_currentTrainingSession
_sessionNumber
_suspendAudio
_cleanupCompletion
_speechRecognizer
_speechRecognizerAvailable
_rms
initWithLength:
convertToShortLPCMBufFromFloatLPCMBuf:
setLanguageCode:
_logUptime
systemUpTime
_makeKeyWithLanguageAndAssetString:
_pushValueForDistributionKey:withValue:
_addValueForScalarKey:withValue:
_makeKey:
sharedAggregator
cumulativeUptime:cumulativeDowntime:reset:
setAssetString:
logTdPsrSATDetectionTimedOut
logTdPsrSATDetectionWaitTimeMs:
logTdPsrFailedDuringSATDetection
logTdPsrFailedDuringSATRetraining
logVoiceProfilePruningFailureWithReasonCode:
logProfileUpdateScoreMSE:
logTdPsrSATRetrainingTimedOut
logTdPsrSATRetrainingWaitTimeMs:
logProfileUpdateNumPrunedUttsPHS:
logProfileUpdateNumDiscardedUttsPHS:
logProfileUpdateNumRetainedUttsPHS:
logProfileUpdateUtt:withScore:
_currentState
_lastAggTime
_cumulativeUptime
_cumulativeDowntime
_lastAggTimeFalseWakeUp
_numFalseWakeUp
_timer
_languageCode
_assetString
spIdSATModelDirForLocale:spidType:modelType:
createDirectoryIfDoesNotExist:
spIdSATModelDirForLocale:profileId:spidType:modelType:
utteranceDirectory
contentsOfDirectoryAtPath:error:
predicateWithFormat:
caseInsensitiveCompare:
sortedArrayUsingSelector:
modelPath
_isDirectoryEmpty:
initWithSpeakerModelFileName:languageCode:
tiModelPath
tdtiModelPath
tdtiUtteranceDirectory
tiUtteranceDirectory
enrollmentUtterance
needsRetrain
discard
isValid
_modelFileName
_modelPath
_utteranceDirectory
_tdtiModelPath
_tdtiUtteranceDirectory
_tiModelPath
_tiUtteranceDirectory
_notifyStopCommandControl
commandControlBehaviorMonitor:willStartStreamWithContext:option:
commandControlBehaviorMonitor:didStartStreamWithContext:successfully:option:
commandControlBehaviorMonitor:willStopStream:
commandControlBehaviorMonitor:didStopStream:
_isCommandControlStreaming
notifySpeechControllerRecordStateChange:withEventUUID:
recordState
setRecordState:
_recordState
initWithContext:delegate:
processAudioChunk:
processMyriadDecision:
lastSpeakerIdInfo
mutableCopy
numberOfChannels
isWriting
fFile
inASBD
outASBD
selectedChannelList
_numberOfChannels
_fileURL
notifyAduioSessionStateChange:
getAudioSessionState
setAudioSessionState:
_audioSessionState
removeObjectForKey:
enumerateObjects:
initWithMachServiceName:options:
setRemoteObjectInterface:
initWithServiceName:
getCoreSpeechXPCConnection
setInvalidationHandler:
resume
remoteObjectProxy
installedVoiceTriggerAssetForLanguageCode:completion:
fetchRemoteVoiceTriggerAssetForLanguageCode:completion:
voiceTriggerRTModelForVersion:minorVersion:locale:downloadedModels:preinstalledModels:completion:
voiceTriggerJarvisLanguageList:jarvisSelectedLanguage:completion:
getCoreSpeechServiceConnection
requestUpdatedSATAudio:
getFirstPassRunningMode:
voiceTriggerRTModelForVersion:minorVersion:downloadedModels:preinstalledModels:completion:
requestUpdatedSATAudio
getFirstPassRunningMode
sharedVoiceTriggerClient
inputRecordingFramesPerPacket
shouldRunVTOnCS
supportIOSBargeIn
inputRecordingBytesPerFrame
inputRecordingBytesPerPacket
inputRecordingDurationInSecs
inputRecordingSampleBitDepth
EncryptionAudioSampleByteDepth
inputRecordingEncoderAudioQuality
inputRecordingSampleRateConverterAlgorithm
inputRecordingBufferDuration
audioConverterBitrate
channelForOutputReference
zeroFilterApproxAbsSpeechThreshold
csAudioProcessingQueuePriority
daysBeforeRemovingLogFiles
serverLoggingChannelBitset
decryptedDataUsingAESKey:atFilepath:error:
initWithFileUrl:aesKey:sampleByteDepth:
readAudioChunksWithCallback:
fileUrl
setFileUrl:
aesKey
setAesKey:
readBuffer
setReadBuffer:
sampleByteDepth
setSampleByteDepth:
_fileUrl
_aesKey
_readBuffer
_sampleByteDepth
startManager
_createClearLoggingFileTimer
_startClearLoggingFilesTimer
supportHearstVoiceTrigger
sharedNotifierForCoreSpeechDaemon
setDelegate:for:
sharedNotifier
supportJarvisVoiceTrigger
createSharedAudioSession
supportBluetoothDeviceVoiceTrigger
UUID
enumerateKeysAndObjectsUsingBlock:
_getAudioRecorderWithError:
setContext:error:
audioProviders
initWithAudioStreamHandleId:audioRecorder:
setAudioProviderDelegate:
initWithAudioRecorder:
initWithQueue:error:
setAudioRecorder:
activationEventNotifier:event:completion:
audioRecorderBufferAvailable:audioStreamHandleId:buffer:remoteVAD:atTime:
audioRecorderBufferAvailable:audioStreamHandleId:buffer:
audioRecorderDidStartRecord:audioStreamHandleId:successfully:error:
audioRecorderDidStopRecord:audioStreamHandleId:reason:
audioRecorderRecordHardwareConfigurationDidChange:toConfiguration:
audioRecorderDidFinishAlertPlayback:ofType:error:
audioRecorderBeginRecordInterruption:
audioRecorderBeginRecordInterruption:withContext:
audioRecorderEndRecordInterruption:
audioRecorder:willSetAudioSessionActive:
audioRecorder:didSetAudioSessionActive:
voiceTriggerDetectedOnAOP:
audioRecorderDisconnected:
audioRecorderLostMediaserverd:
audioRecorderWillBeDestroyed:
audioRecorderBuiltInAudioStreamInvalidated:error:
audioRecorderStreamHandleIdInvalidated:
audioProviderInvalidated:streamHandleId:
_getVoiceTriggerAssetIfNeeded:
registerSpeechController:
registerSiriClientProxy:
registerVolumeController:
audioProviderWithUUID:
audioProviderWithStreamID:
fetchFallbackAudioSessionReleaseProvider
_reinitializeSmartSiriVolumeWithAsset:
smartSiriVolume
assetQueryQueue
setAssetQueryQueue:
audioRecorder
setAudioProviders:
fallbackAudioSessionReleaseProvider
setFallbackAudioSessionReleaseProvider:
clientController
setClientController:
volumeClientController
setVolumeClientController:
voiceTriggerImplicitTraining
setVoiceTriggerImplicitTraining:
clearLoggingFileTimer
setClearLoggingFileTimer:
clearLoggingFileTimerCount
setClearLoggingFileTimerCount:
opportuneSpeakListnerTestService
setOpportuneSpeakListnerTestService:
_smartSiriVolume
_assetQueryQueue
_audioRecorder
_audioProviders
_fallbackAudioSessionReleaseProvider
_clientController
_volumeClientController
_voiceTriggerImplicitTraining
_clearLoggingFileTimer
_clearLoggingFileTimerCount
_opportuneSpeakListnerTestService
secondsToHostTime:
addNumSamples:hostTime:
notifyTrailingSilenceDurationAtEndpoint:
estimatedSpeechEndHostTime
numAudioSampleForwarded
setNumAudioSampleForwarded:
lastAudioChunkHostTime
setLastAudioChunkHostTime:
endPointNotified
setEndPointNotified:
setTrailingSilenceDurationAtEndpoint:
_endPointNotified
_numAudioSampleForwarded
_lastAudioChunkHostTime
_trailingSilenceDurationAtEndpoint
_notifyObserver:withClamshellState:
CSClamshellStateMonitor:didReceiveClamshellStateChange:
isClamshellClosed
_didReceiveClamshellStateChangeNotification:
resourcePath
configFilepathForDictationOrigin:
setRecordingContext:
processImplicitTrainingUtterance:forVoiceProfileId:withRecordDeviceInfo:withRecordCtxt:withVoiceTriggerCtxt:withOtherCtxt:withCompletion:
processImplicitTrainingUtteranceWithVoiceTriggerEventInfo:
processSingleUserImplicitTrainingUtterance:audioDeviceType:audioRecordType:withVoiceTriggerCtxt:withOtherCtxt:withCompletion:
lowercaseString
hasPrefix:
substringFromIndex:
replaceMatchesInString:options:range:withTemplate:
_stringByStrippingLeadingNoise:
_stringByStrippingTrailingNoise:
rangeOfString:options:
_firstMatchesForRegularExpression:
_stringByFixingNamePattern:
_stringByStrippingNoiseLeadingNoise:TrailingNoise:
_hasSubstring:
_matchesRegularExpression:
_caseInsensitiveHasMatchInEnumeration:
_firstMatchesForRegularExpressions:
commandControlListener:didStopUnexpectly:
commandControlListener:hasLPCMBufferAvailable:
deviceCategoryForDeviceProductType:
setCurrentDeviceCategory:
baseDir
_createAndSendImplicitUtterenceXPCMessage:withRecordDeviceInfo:withRecordCtxt:withVoiceTriggerCtxt:withOtherCtxt:withCompletion:
_CSSATDownloadPath
_getUserVoiceProfileDownloadCacheDirectoryWithUpdatePath:
_downloadAndEnrollVoiceProfileForProfileId:withDownloadTriggerBlock:
CSSATBasePath
isSATEnrolledForLanguageCode:
isCurrentDeviceCompatibleWithVoiceProfileAt:
spIdSATDirForLocale:profileId:spidType:
copyItemAtPath:toPath:error:
enumerateObjectsUsingBlock:
_markSATEnrollmentSuccessForSiriProfileId:forLanguageCode:
_markSATEnrollmentMigratedForSiriProfileId:forLanguageCode:
deviceCategoryStringRepresentationForCategoryType:
_enrollVoiceProfileForSiriProfileId:fromCacheDirectoryPath:withCategoryType:
code
_stageVoiceProfileWithSiriProfileId:fromPath:withForceUpload:
_downloadVoiceProfileForProfileId:forDeviceCategory:withDownloadTriggerBlock:withCompletion:
currentLanguageCode
initWithSharedSiriId:languageCode:productCategory:version:sharedHomeId:userName:
_getUserVoiceProfileDownloadCacheDirectoryForProfileId:forDeviceCategory:forVoiceProfileVersion:
removeItemAtPath:
containsObject:
isSpidSupportedInCurrentLanguage
_enrollVoiceProfileForSiriProfileId:forLanguage:fromSourceDir:
_copyVoiceProfileFromSrcDir:toDestDir:
_markVoiceProfileMigrationCompleteForSiriProfileId:forLanguageCode:
_enableVoiceTriggerIfLanguageMatches:
isCurrentDeviceCompatibleWithNewerVoiceProfileAt:
stringByAppendingString:
_CSSATUploadPathForSiriProfileId:
_prepareVoiceProfileWithSiriProfileId:withUploadBlock:
_CSSATLegacyUploadPath
getVoiceProfilesMarkedForUpload
locale
stringForCSSpIdType:
getEnrollmentUtterancesCountFromDirectory:withCountBlock:
markVoiceProfileUploaded:
spIdSATImplicitAudioCacheDirForLocale:profileId:
getBaseProfileOnlyEnrollmentUtterancesFromDirectory:
_CSSATUploadStagingPathForSiriProfileId:
_copyVoiceProfileAtPath:toPath:
copyItemAtURL:toURL:error:
enumeratorAtURL:includingPropertiesForKeys:options:errorHandler:
getCurrentVoiceProfileVersionForProfileId:forLanguageCode:
spIdSATDirForLocale:profileId:
enumeratorAtPath:
_isDirectory:
stringByDeletingLastPathComponent
getContentsOfDirectory:
_copySATFilesFromPath:toPath:withCompletion:
_getVoiceProfilePathsToBeUploadedForSiriProfileId:
languageCode
productCategory
_isMarkedForVoiceProfileTrainingSyncForLanguage:
setProductCategory:
getDevicesWithAvailablePHSAssetsOnDeviceCheck:
devicesWithVoiceProfileIniCloudForLanguage:
date
getDevicesWithAvailablePHSAssetsForLanguage:completion:
timeIntervalSinceDate:
markSATEnrollmentSuccessForSiriProfileId:forLanguageCode:
spIdSATDirForLocale:
_markVoiceProfileTrainingSyncForLanguage:
_isSATMarkedForSiriProfileId:withMarker:forLanguageCode:
isSATEnrolledForSiriProfileId:forLanguageCode:
isSATEnrollmentMigratedForSiriProfileId:forLanguageCode:
updateVoiceProfileVersionFileForProfileId:forLanguageCode:
_markSATEnrollmentWithMarker:forLanguage:forSiriProfileId:
createFileAtPath:contents:attributes:
route
getImplicitUtteranceCacheDirectory
getSATEnrollmentPath
isSATAvailable
notifyImplicitTrainingUtteranceAvailable:forVoiceProfileId:withRecordDeviceInfo:withRecordCtxt:withVoiceTriggerCtxt:withOtherCtxt:withCompletion:
getUserVoiceProfileUpdateDirectory
notifyUserVoiceProfileDownloadReadyForUser:getData:completion:
notifyUserVoiceProfileUpdateReady
notifyUserVoiceProfileUploadCompleteForSiriProfileId:withError:
uploadUserVoiceProfileForSiriProfileId:withUploadTrigger:completion:
getUserVoiceProfileUploadPathWithEnrolledLanguageList:
notifyUserVoiceProfileUploadComplete
_getEnrolledLanguageList
getCachedVoiceProfileAvailabilityMetaBlob
hasVoiceProfileIniCloudForLanguageCode:withBackupMetaBlob:
isVoiceProfileUploadedToiCloudForLanguageCode:withCompletionBlock:
hasVoiceProfileIniCloudForLanguageCode:
enableVoiceTriggerUponVoiceProfileSyncForLanguage:
provisionedVoiceProfilesForLocale:
markSATEnrollmentSuccessForLanguageCode:
isSATEnrollmentMigratedForLanguageCode:
_isRemoteVoiceTriggerAvailable
discardSATEnrollmentForLanguageCode:
discardSATEnrollmentForProfileId:forLanguageCode:
discardAllSATEnrollment
_getSATEnrollmentAudioPathForLanguageCodeForLegacyVoiceProfile:
_CSSATCachePath
currentDeviceCategory
_currentDeviceCategory
initWithDownloadOption:
shouldDownloadVTAssetsOnDaemon
sharedController
addObserver:forAssetType:
_createPeriodicalDownloadTimer
_startPeriodicalDownload
_stopPeriodicalDownload
_fetchRemoteMetaData
_canFetchRemoteAsset:
assetOfType:language:
installedAssetOfType:language:
allInstalledAssetsOfType:language:
installedAssetOfType:language:completion:
fetchRemoteMetaOfType:
supportHybridEndpointer
assetForCurrentLanguageOfType:completion:
CSAssetManagerDidDownloadNewAsset:
CSVoiceTriggerAssetMetaUpdateMonitor:didReceiveNewVoiceTriggerAssetMetaData:
CSSpeechEndpointAssetMetaUpdateMonitor:didReceiveNewSpeechEndpointAssetMetaData:
CSAssetController:didDownloadNewAssetForType:
CSLanguageCodeUpdateMonitor:didReceiveLanguageCodeChanged:
assetForCurrentLanguageOfType:
installedAssetForCurrentLanguageOfType:
installedAssetForCurrentLanguageOfType:completion:
removeObserver:forAssetType:
_enablePolicy
_currentLanguageCode
_downloadingOption
_downloadTimer
_downloadTimerCount
setupPhraseSpotter
startMasterTimerWithTimeout:
resultAlreadyReported
stopMasterTimer
closeSessionWithCompletion:
updateMeterAndForward
pushAudioInputIntoPCMBuffer:
requestTriggeredUtterance:
setupSpeechRecognitionTaskWithVoiceTriggerEventInfo:
trimBeginingOfPCMBufferWithVoiceTriggerEventInfo:
computeRequiredTrailingSamples
feedSpeechRecognitionWithPCMBuffer
closeSessionWithStatus:successfully:
handleAudioBufferForVTWithAudioInput:withDetectedBlock:
finishSpeechRecognitionTask
feedSpeechRecognitionTrailingSamplesWithCompletedBlock:
triggeredUtterance:
updateMeters
averagePower
analyze:
numSamplesInPCMBuffer
appendAudioPCMBuffer:
frameLength
initWithCommonFormat:sampleRate:channels:interleaved:
initWithPCMFormat:frameCapacity:
mutableAudioBufferList
setFrameLength:
replaceObjectAtIndex:withObject:
removeObjectsInRange:
createAVAudioPCMBufferWithNSData:
handleAudioInput:
_registerEndPointTimeout
sharedGrammars
getLMEforLocale:
setContextualStrings:
setTaskHint:
_setVoiceTriggerEventInfo:
recognitionTaskWithRequest:delegate:
finish
handleMasterTimeout:
scheduledTimerWithTimeInterval:target:selector:userInfo:repeats:
invalidate
formattedString
whitespaceAndNewlineCharacterSet
stringByTrimmingCharactersInSet:
speechRecognitionDidDetectSpeech:
speechRecognitionTask:didHypothesizeTranscription:
speechRecognitionTask:didFinishRecognition:
speechRecognitionTaskFinishedReadingAudio:
speechRecognitionTaskWasCancelled:
speechRecognitionTask:didFinishSuccessfully:
_status
_utteranceId
_speechRecognitionRequest
_speechRecognitionTask
_masterTimer
_pcmBufArray
_resultReported
_sessionProcess
_sessionSuspended
_ASRErrorOccured
_sessionDelegate
_trainingCompletion
_numRequiredTrailingSamples
_numTrailingSamples
initWithDataSource:assetsProvider:
addDelegate:
removeDelegate:
startWithNviContext:didStartHandler:
stopWithDidStopHandler:
sigType
initWithBlob:
_currentBlob
_activeChannel
startController
getFixedHighPrioritySerialQueueWithLabel:
twoShotNotificationEnabled
_currentAudioRecorderSampleRate
supportPhatic
_initializeMediaPlayingState
_initializeAlarmState
_initializeTimerState
_setSoundPlayingState
supportSmartVolume
_contextToString:
setCurrentContext:error:
supportSessionActivateDelay
_shouldResetContextAtPrepare
audioRecordContext
_fetchAudioProviderWithContext:
sessionProvider
enableMiniDucking:
_shouldFetchVoiceTriggerInfo
_shouldFetchRaiseToSpeakInfo
_fetchLastTriggerInfo
_activateAudioSessionWithDelay:error:
domain
_activateAudioSession:forRetry:
setIsSiri:
prepareAudioStreamSyncWithRequest:error:
streamProvider
isNarrowBand
_setupDownsamplerIfNeeded
_setupAudioConverter:isNarrowBand:
recordRoute
_createAudioPowerMeterIfNeeded
triggerInfoForContext:completion:
_isDelayedDuckingSupportedContext
hostTimeToTimeInterval:
_scheduleActivateAudioSessionWithDelay:forReason:validator:completion:
_cancelPendingAudioSessionActivateForReason:
_enableBargeInMode:
activateAudioSessionWithReason:error:
setAudioRecordContext:
prewarmAudioSessionWithError:
_teardownAudioProviderIfNeeded
_tearDownBargeInModeProviderIfNeeded
deactivateAudioSession:error:
_fetchFallbackAudioSessionReleaseProviding
fallbackDeactivateAudioSession:error:
recordSettings
numberWithInt:
duckOthersOption
setDuckOthersOption:
speakerIdEnabled
objectAtIndexedSubscript:
initWithSpIdInvocationStyle:asset:fallbackAsset:locale:vtEventInfo:
speakerIdRecognizerWithContext:delegate:
setDictationLanguages:
setCurrentKeyboard:
setWasLanguageToggled:
setMultilingualKeyboardLanguages:
setKeyboardConvoLanguagePriors:
setKeyboardGlobalLanguagePriors:
setPreviousMessageLanguage:
setGlobalLastKeyboardUsed:
setDictationLanguagePriors:
setConversationalMessages:
setAVVCAlertBehavior:
supportOpportunisticZLL
setUseOpportunisticZLL:
setStartRecordingHostTime:
setRequestHistoricalAudioDataWithHostTime:
_shouldSetStartSampleCount
setRequestHistoricalAudioDataSampleCount:
setStartRecordingSampleCount:
_shouldSetStartSampleCountForRTS
_setupSpeakerId
_startPhaticDecision
lpcmNonInterleavedASBD
lpcmInterleavedASBD
_shouldUseLanguageDetector:
_createLanguageDetectorIfNeeded
_languageDetectorOptionFromSettings:
setSamplingRate:
languageDetectorDelegate
notifyWillStartStreamWithContext:option:
speechControllerDidStartRecording:successfully:error:
notifyDidStartStreamWithContext:successfully:option:
_canPlayPhaticDuringMediaPlayback
speechControllerDidDetectVoiceTriggerTwoShot:atTime:
shouldDelayPhaticForMyriadDecision
_shouldSchedulePhaticAtStartRecording
_scheduledPhaticDelay
speechControllerRequestsOperation:forReason:completion:
_phaticPlaybackReason
startRecordingWithSettings:error:
notifyWillStopStream:
_didStopForReason:
speechControllerDidDeliverLastBuffer:forReason:estimatedSpeechEndHostTime:
recordDeviceInfo
playbackRoute
speechControllerDidStopRecording:forReason:
speechControllerDidStopRecording:forReason:estimatedSpeechEndHostTime:
_deviceAudioLoggingWithFileWriter:
notifyDidStopStream:
hostTime
process:stride:inFrameToProcess:
addSamples:timestamp:
speechControllerLPCMRecordBufferAvailable:buffer:
avgPower
setCachedAvgPower:
peakPower
setCachedPeakPower:
speechControllerRecordBufferAvailable:buffers:recordedAt:
packets
timeStamp
speechControllerRecordBufferAvailable:buffers:durationInSec:recordedAt:
speechControllerRecordHardwareConfigurationDidChange:toConfiguration:
speechControllerDidFinishAlertPlayback:ofType:error:
speechControllerBeginRecordInterruption:
speechControllerBeginRecordInterruption:withContext:
speechControllerEndRecordInterruption:
speechController:willSetAudioSessionActive:
speechController:didSetAudioSessionActive:
speechControllerDidUpdateSmartSiriVolume:forReason:
narrowBandOpusConverter
opusConverter
alertProvider
setAlertSoundFromURL:forType:
playAlertSoundForType:
alertStartTime
playRecordStartingAlertAndResetEndpointer
audioMeterProvider
setMeteringEnabled:
getPeakPowerDB
cachedPeakPower
getAveragePowerDB
cachedAvgPower
speechControllerRequestsOperation:forReason:
audioMetricProvider
audioMetric
setEndpointerDelegate:
_createAudioProviderFromXPCWithContext:
setStreamProvider:
setSessionProvider:
setAlertProvider:
setAudioMeterProvider:
setAudioMetricProvider:
setAudioSessionDelegate:
setAudioAlertDelegate:
setBargeInModeProvider:
allKeys
_getSpeechIdentifier
getEstimatedTTSVolume
spIdMapScoresToSharedSiriID:
speakerIdentificationDidDetectSpeakerWithScores:
isSmartSiriVolumeAvailable
audioConverterDidConvertPackets:packets:durationInSec:timestamp:
speakerRecognizer:hasSpeakerIdInfo:
speakerRecognizerFinishedProcessing:withFinalSpeakerIdInfo:
smartSiriVolumeControllerDetectedSystemVolumeChange:withVolume:forReason:
audioSessionProviderBeginInterruption:
audioSessionProviderBeginInterruption:withContext:
audioSessionProviderEndInterruption:
audioSessionProvider:willSetAudioSessionActive:
audioSessionProvider:didSetAudioSessionActive:
audioSessionProvider:providerInvalidated:
audioSessionProvider:didChangeContext:
audioAlertProvidingDidFinishAlertPlayback:ofType:error:
continuousVoiceTrigger:detectedVoiceTriggerResult:
continuousVoiceTrigger:detectedSilenceAfterVoiceTriggerAt:
initializeRecordSessionWithContext:
prepareRecordWithSettings:error:
_performPendingAudioSessionActivateForReason:
prewarmAudioSession
resetAudioSession
releaseAudioSession:
getLPCMAudioStreamBasicDescription
setSynchronousCallbackEnabled:
getRecordBufferDuration
startRecording:
stopRecordingWithOptions:
peakPowerForChannel:
averagePowerForChannel:
peakPowerForOutputReference
averagePowerForOutputReference
outputReferenceChannel
metrics
endpointAnalyzer
setEndpointAnalyzerDelegate:
resetEndpointer
_createBargeInModeProviderFromXPCIfNeeded
getSmartSiriVolume
languageDetectorSetMostRecentRecognitionLanguage:
cancelCurrentLanguageDetectorRequest
setLanguageDetectorInteractionID:
beginWaitingForMyriad
endWaitingForMyriadWithDecision:
speakerIdDelegate
setSpeakerIdDelegate:
setLanguageDetectorDelegate:
endpointerProxy
setEndpointerProxy:
bargeInModeProvider
isOpus
setIsOpus:
isActivated
setIsActivated:
setIsNarrowBand:
serverLoggingWriter
setServerLoggingWriter:
volumeController
setVolumeController:
spIdFactory
setSpIdFactory:
spIdRecognizer
setSpIdRecognizer:
spIdUserScores
setSpIdUserScores:
voiceProfileStore
setVoiceProfileStore:
setTwoShotNotificationEnabled:
isMediaPlaying
setIsMediaPlaying:
isAlarmPlaying
setIsAlarmPlaying:
isTimerPlaying
setIsTimerPlaying:
isSoundPlaying
setIsSoundPlaying:
isRemoteVADAvailableStream
setIsRemoteVADAvailableStream:
myriadPreventingTwoShotFeedback
setMyriadPreventingTwoShotFeedback:
needsPostGain
setNeedsPostGain:
speechEndHostTimeEstimator
setSpeechEndHostTimeEstimator:
languageDetector
setLanguageDetector:
shouldUseLanguageDetectorForCurrentRequest
setShouldUseLanguageDetectorForCurrentRequest:
pendingAudioSessionActivationToken
setPendingAudioSessionActivationToken:
pendingAudioSessionActivationCompletion
setPendingAudioSessionActivationCompletion:
audioSessionActivationDelay
setAudioSessionActivationDelay:
bargeInModeXPCClient
setBargeInModeXPCClient:
powerMeter
setPowerMeter:
didDeliverLastBuffer
setDidDeliverLastBuffer:
_contextResetQueue
_opusAudioConverter
_narrowBandOpusConverter
_audioConverter
_downsampler
_requestedRecordSettings
_lastVoiceTriggerInfo
_lastRTSTriggerInfo
_audibleFeedbackQueue
_twoShotAudibleFeedbackDecisionGroup
_isOpus
_isActivated
_isNarrowBand
_twoShotNotificationEnabled
_isMediaPlaying
_isAlarmPlaying
_isTimerPlaying
_isSoundPlaying
_isRemoteVADAvailableStream
_myriadPreventingTwoShotFeedback
_needsPostGain
_shouldUseLanguageDetectorForCurrentRequest
_didDeliverLastBuffer
_cachedAvgPower
_cachedPeakPower
_speakerIdDelegate
_languageDetectorDelegate
_endpointerProxy
_audioRecordContext
_streamProvider
_sessionProvider
_alertProvider
_audioMeterProvider
_audioMetricProvider
_bargeInModeProvider
_serverLoggingWriter
_volumeController
_spIdFactory
_spIdRecognizer
_spIdUserScores
_voiceProfileStore
_speechEndHostTimeEstimator
_languageDetector
_pendingAudioSessionActivationToken
_pendingAudioSessionActivationCompletion
_audioSessionActivationDelay
_bargeInModeXPCClient
_powerMeter
createRTModelWithLocale:
hearstRTModelWithMajorVersion:minorVersion:locale:
_sha1:
_sha256:
initWithData:hash:locale:digest:signature:certificate:
stringWithCapacity:
RTModelWithFallbackLanguage:
latestHearstRTModelForLocale:
hearstRTModelLocaleMap
isAvailable
_firedVoiceTriggerTimeout
shouldHandleSession
shouldMatchPayload
_firedEndPointTimeout
_registerVoiceTriggerTimeout
_reportStopListening
_registerForceEndPointTimeout
matchRecognitionResult:withMatchedBlock:withNonMatchedBlock:
bestTranscription
getTrailingPatternsForUtt:Locale:
getLeadingPatternsForUtt:Locale:
getRegexPatternsForUtt:Locale:
matchWithString:TrailingStr:LeadingStr:Pattern:
voiceTriggerEventInfo
setVoiceTriggerEventInfo:
_detectBOS
_ASRResultReceived
_reportedStopListening
_utteranceStored
_numSamplesFed
_bestTriggerSampleStart
_extraSamplesAtStart
_voiceTriggerEventInfo
_storeModeEnabled
setFileLoggingLevel:
fileLoggingLevel
intValue
myriadHashDirectory
interstitialRelativeDirForLevel:
voiceTriggerEnabled
phraseSpotterEnabled
isAttentiveSiriEnabled
voiceTriggerInCoreSpeech
setFileLoggingIsEnabled:
voiceTriggerAudioLogDirectory
secondPassAudioLoggingEnabled
jarvisAudioLoggingEnabled
setJarvisTriggerMode:
getJarvisTriggerMode
startOfSpeechAudioLoggingEnabled
getStartOfSpeechAudioLogFilePath
remoteVoiceTriggerDelayTime
remoteVoiceTriggerEndpointTimeoutWithDefault:
interstitialAbsoluteDirForLevel:
myriadFileLoggingEnabled
enableAudioInjection:
audioInjectionEnabled
setAudioInjectionFilePath:
audioInjectionFilePath
speakerIdiTunesAccountSigninEnabled
speakerIdScoreReportingType
useSiriActivationSPIForHomePod
useSiriActivationSPIForwatchOS
iOSBargeInSupportEnabled
shouldOverwriteRemoteVADScore
overwritingRemoteVADScore
setHearstFirstPassModelVersion:
setHearstSecondPassModelVersion:
fakeHearstModelPath
companionSyncVoiceTriggerUtterancesEnabled
hostTimeFromSampleCount:anchorHostTime:anchorSampleCount:
sampleCountFromHostTime:anchorHostTime:anchorSampleCount:
processSampleCount:hostTime:
hostTimeFromSampleCount:
sampleCountFromHostTime:
anchorSampleCount
setAnchorSampleCount:
anchorHostTime
setAnchorHostTime:
_anchorSampleCount
_anchorHostTime
startRecordingHostTime
initWithStreamID:atStartHostTime:
avvcAlertBehavior
setStartAlert:
setStopAlert:
setStopOnErrorAlert:
requestHistoricalAudioDataWithHostTime
_alertBehaviorTypeFromAVVCOberrideType:
setStartAlertBehavior:
setStopAlertBehavior:
setErrorAlertBehavior:
startAlertBehavior
_avvcAlertOverrideType:
stopAlertBehavior
errorAlertBehavior
avvcStartRecordSettingsWithAudioStreamHandleId:
avvcSettings
_setupSignalProviders:
mapTableWithKeyOptions:valueOptions:
strRepForNviSignalType:
strRepForNviDataSourceType:
signalProvidersMapForContext:
hashTableWithOptions:
_startSignalProvidersWithContext:
_startDataSourcesWithContext:
_stopDataSources
_stopCurrentlyRunningSignalProviders
_iterateSignalMask:withHandler:
initialize
registerSignalProviderDelegate:forSignalTypes:
unregisterSignalProviderDelegate:forSignalType:
assetsProvider
setAssetsProvider:
dataSrcMap
setDataSrcMap:
sigProvidersMap
setSigProvidersMap:
currActiveSigProvTypes
setCurrActiveSigProvTypes:
currActiveDataSourceTypes
setCurrActiveDataSourceTypes:
_assetsProvider
_dataSrcMap
_sigProvidersMap
_currActiveSigProvTypes
_currActiveDataSourceTypes
sendXPCClientType
_sendMessageAndReplySync:connection:error:
setAudioSessionProvidingDelegate:
setAudioAlertProvidingDelegate:
initWithAudioStreamProvider:streamName:streamRequest:
_handleListenerMessage:
_handleSessionProvidingDelegateMessageBody:
_handleStreamProvidingDelegateMessageBody:
_handleAlertProvidingDelegateMessageBody:
_handleSessionInfoProvidingDelegateMessageBody:
_handleListenerDisconnectedError:
_handleAlertProvidingDelegateDidFinishAlertPlayback:
audioAlertProvidingDelegate
_handleSessionProvidingDelegateBeginInterruption:
_handleSessionProvidingDelegateBeginInterruptionWithContext:
_handleSessionProvidingDelegateEndInterruption:
_handleSessionProvidingDelegateWillSetAudioSession:
_handleSessionProvidingDelegateDidSetAudioSession:
_handleSessionProvidingDelegateStreamHandleIdInvalidation:
_handleSessionProvidingDelegateDidChangeContext:
audioSessionProvidingDelegate
_handleSessionInfoProvidingDelegateInterruptionNotification:
_handleSessionInfoProvidingDelegateRouteChangeNotification:
_handleSessionInfoProvidingDelegateMediaServicesWereLostNotification:
_handleSessionInfoProvidingDelegateMediaServicesWereResetNotification:
_handleStreamProvidingDelegateDidStopUnexpectly:
_handleStreamProvidingDelegateChunkAvailable:
_handleStreamProvidingDelegateChunkForTVAvailable:
_handleStreamProvidingDelegateHardwareConfigChange:
audioStreamWithRequest:streamName:completion:
prepareAudioStreamSync:request:error:
prepareAudioStream:request:completion:
startAudioStream:option:completion:
stopAudioStream:option:completion:
audioChunkFrom:to:
audioChunkToEndFrom:
saveRecordingBufferFrom:to:toURL:
saveRecordingBufferToEndFrom:toURL:
holdAudioStreamWithDescription:timeout:
cancelAudioStreamHold:
configureAlertBehavior:
updateMusicVolume:
updateAlarmVolume:
updateAlarmState:
updateTimerState:
enableBargeInMode:completion:
pingpong:
audioStreamProvidingDelegate
setAudioStreamProvidingDelegate:
audioSessionInfoObservers
setAudioSessionInfoObservers:
xpcClientType
setXpcClientType:
_audioSessionProvidingDelegate
_audioStreamProvidingDelegate
_audioAlertProvidingDelegate
_audioSessionInfoObservers
_xpcClientType
didTransitFrom:to:by:
didIgnoreEvent:from:
initWithInitialState:
addTransitionFrom:to:for:
performTransitionForEvent:
currentState
initialState
setInitialState:
transitions
setTransitions:
_initialState
_transitions
initWithFilepath:
CSEventMonitorDidReceiveEvent:
setPackets:
setAvgPower:
setPeakPower:
setTimeStamp:
_avgPower
_peakPower
_packets
_timeStamp
addPHSRejectEntry:
addPHSAcceptEntryAndSubmit:
_closeAudioFile
fileURLWithPath:isDirectory:
appendAudioData:
_audioFile
_asbd
_url
_audioLength
setAudioFormat:
setEncoderBitRate:
setNumberOfChannels:
setLpcmBitDepth:
setLpcmIsFloat:
setRecordContext:
setUseCustomizedRecordSettings:
requestForLpcmRecordSettingsWithContext:
requestForOpusRecordSettingsWithContext:
requestForSpeexRecordSettingsWithContext:
recordContext
requiresHistoricalBuffer
useCustomizedRecordSettings
audioFormat
lpcmBitDepth
lpcmIsFloat
encoderBitRate
isSiri
_requiresHistoricalBuffer
_useCustomizedRecordSettings
_lpcmIsFloat
_isSiri
_lpcmBitDepth
_encoderBitRate
_recordContext
_audioFormat
initWithCSSpIdType:withSysConfigFilepath:sysConfigRoot:delegate:queue:
_spIdType
UUIDString
initWithType:deviceId:activationInfo:vadScore:hosttime:
initWithType:deviceId:activationInfo:hosttime:
_activationTypeString
remoteMicVoiceTriggerEvent:activationInfo:hostTime:
remoteMicVADEvent:vadScore:hostTime:
builtInMicVoiceTriggerEvent:hostTime:
jarvisVoiceTriggerEvent:activationInfo:hostTime:
mediaserverdLaunchedEvent:
activationInfo
setActivationInfo:
hosttime
setHosttime:
vadScore
setVadScore:
_vadScore
_UUID
_activationInfo
_hosttime
componentsSeparatedByString:
firstObject
shortFormForUUID
isScreenLocked
getVoiceTriggerAssetTypeString
getEndpointAssetTypeString
getLanguageDetectorAssetTypeString
_cleanUpMobileAssetV1Directory
_isReadyToUse
installedAssetOfType:withLanguage:
_fetchRemoteAssetOfType:withLanguage:completion:
_assetQueryForAssetType:
returnTypes:
queryMetaDataSync
results
filteredAssetsForAssets:assetType:language:
queryParams
installedAssetOfType:withLanguage:completion:
_installedAssetOfType:withLanguage:
_installedAssetOfType:withLanguage:completion:
_findLatestInstalledAsset:
queryMetaData:
addKeyValuePairForQuery:assetType:
isFirstUnlocked
_runAssetQuery:completion:
_downloadAssetCatalogForAssetType:complete:
_updateFromRemoteToLocalAssets:forAssetType:completion:
_defaultDownloadOptions
startCatalogDownload:options:then:
cancelDownloadSync
purgeSync
_downloadAsset:withComplete:
setAllowsCellularAccess:
setCanUseLocalCacheServer:
setDiscretionary:
assetServerUrl
_startDownloadingAsset:progress:completion:
expectedTimeRemaining
totalWritten
totalExpected
attachProgressCallBack:
spaceCheck:
startDownload:then:
assetsMigrationQueue
setAssetsMigrationQueue:
csAssetsDictionary
setCsAssetsDictionary:
_assetsMigrationQueue
_csAssetsDictionary
valueForKey:
supportPremiumAssets
getVoiceTriggerAssetCurrentCompatibilityVersion
getEndpointAssetCurrentCompatibilityVersion
getLanguageDetectorCurrentCompatibilityVersion
addKeyValuePair:with:
filteredAssetsForFetchRemoteMetaDataForAssets:assetType:
_notifyObserver:withSpeechControllerRecordState:withEventUUID:
_notifyObserver:withSpeechControllerRecordState:
CSSpeechControllerMonitorDidChangeRecordState:recordState:
_pendingRecordingStopUUID
spIdSATAudioDirForLocale:profileId:spidType:
URLsForDirectory:inDomains:
lastObject
spIdAudioLogsDir
spIdAudioLogsDir2
satConfigFileNameForCSSpIdType:forModelType:
getProfileVersionFilePathForProfileId:forLanguageCode:
getVoiceProfileVersionFromVersionFilePath:
getVoiceProfileIdentityFromVersionFilePath:
getVoiceProfileProductCategoryFromVersionFilePath:
moveItemAtPath:toPath:error:
setValue:forKey:
writeToFile:options:error:
setWithObjects:
arrayWithArray:
enter
leave
getHomeUserIdForSharedUserId:completion:
waitWithTimeout:
spIdSiriDebugVTDataDirectory
spIdSiriDebugVoiceProfileStoreRootDirectory
spIdSiriDebugVoiceProfileStoreRootDirectoryForLocale:
dictionaryWithCapacity:
numberOfExplicitSatVectors
numberOfBaseProfileSatVectors
numberOfImplicitProfileSatVectors
getExplicitEnrollmentUtterancesFromDirectory:
getEnrollmentUtterancesFromDirectory:
insertObject:atIndex:
getImplicitEnrollmentUtterancesFromDirectory:
compare:options:
stringForInvocationStyle:
spIdTypeForString:
stringForCSSATRunMode:
stringForCSSpIdModelType:
spIdTrainedUsersFilePathForLocale:
spIdVoiceProfileImportRootDir
spIdAudioLogsCountLimitReached
cleanupOrphanedVoiceIdGradingFiles
spidAudioTrainUtterancesDir
streamAudioFromFileUrl:audioStreamBasicDescriptor:samplesPerStreamChunk:audioDataAvailableHandler:
getCurrentVoiceProfileIdentityForProfileId:forLanguageCode:
getCurrentVoiceProfileProductCategoryForLanguageCode:
checkIfMigrationNecessaryForCompatibilityVersion:forLanguageCode:
migrateVoiceProfileToVersion:forLanguageCode:
getNumberOfAudioFilesInDirectory:
dumpFilesInDirectory:
markUploadForVoiceProfile:
getHomeUserIdForVoiceProfile:withCompletion:
spIdDataRootDirectory
spIdSiriDebugTrainedUsersFilePathForLocale:
spIdSiriDebugVoiceProfileRootDirectoryForProfile:locale:
spIdSiriDebugVoiceProfileCacheDirectoryForProfile:locale:
mapRawScores:toScoresOfType:withRawScoreOffset:withRawScoreScale:withLogitCeil:withLogitFloor:withSATThreshold:
getEnrollmentUtterancesForModelType:forLanguageCode:
getExplicitOnlyEnrollmentUtterancesFromDirectory:
getExplicitEnrollmentUtterancesForType:forLanguageCode:forProfileID:
getSortedImplicitEnrollmentUtterancesForType:forLanguageCode:forProfileID:
getImplicitEnrollmentUtterancesPriorTo:forType:forLanguageCode:forProfileID:
deleteExistingSATModelForLanguageCode:
opusASBD
lpcmInt16ASBD
dataWithBytes:length:
audioDecoderDidDecodePackets:audioStreamHandleId:buffer:remoteVAD:timestamp:receivedNumChannels:
opusDecoder
addPackets:audioStreamHandleId:remoteVAD:timestamp:receivedNumChannels:
_decoder
_addSmartSiriVolumeEnabledConditions
_subscribeEventMonitors
subscribeEventMonitor:
addConditions:
hasRemoteBuiltInMic
assetChangeMonitorDidDetectAssetChange:
sharedMonitor
startMonitoring
notifyVoiceTriggerAssetChanged
_addVoiceTriggerEnabledConditions
isPresent
isSpringboardStarted
batteryState
isRestricted
isSoftwareUpdateCheckingRunning
setIsStreaming:
_isStreaming
_didReceiveNewSpeechEndpointAssetMetaData
alertMuteBehaviorDict
voiceTriggerRecordContext
hearstVoiceTriggerRecordContext:
jarvisVoiceTriggerRecordContext:
lpcmRecordSettings
opusRecordSettings
speexRecordSettings
alertMuteSettings
setCachedAsset:
cachedAsset
_getVoiceTriggerAssetFromAssetManager:
isEqualAsset:
_checkNewAssetAvailablity
CSFirstUnlockMonitor:didReceiveFirstUnlock:
_cachedAsset
_addVoiceTriggerAOPModeEnabledConditions
_addConditionsForIOSBargeIn
_addConditionsForIOSAOP
_isSpeechDetectionDevicePresent
currentBuiltinSpeakerState
_borealisPowerlog:
powerLogVoiceTriggerStart
powerLogVoiceTriggerStop
logFalseWakeUp:
numFalseWakeUp
setNumFalseWakeUp:
lastAggTimeFalseWakeUp
setLastAggTimeFalseWakeUp:
numChannelsForNviDirectionality
nviDirectionalityStartingChannelId
nviDirectionalityEndingChannelId
monoChannelLpcmASBD
allChannelsLpcmInterleavedASBD
allChannelsLpcmNonInterleavedASBD
nviDirectionalityLpcmNonInterleavedASBD
nviDirectionalityLpcmInterleavedASBD
nviLogsRootDir
rtsTriggerInfo
setRtsTriggerInfo:
_voiceTriggerInfo
_rtsTriggerInfo
dataWithContentsOfFile:options:error:
isNviEnabled
strRepForNviSignalMask:
nviSignalTypeForStr:
nviDataSourceTypeForStr:
_createDirAtPath:
timeStampString
getVoiceTriggerEndSampleCountFromVTEI:
getVoiceTriggerEndSecsFromVTEI:
readJsonDictionaryAt:
getValueFromDictionaryOfDictionaries:keypath:
createDirAtPath:
generateIfNecessaryVoiceTriggerProfilesAESKey
saveEncryptedDataUsingAESKey:atFilepath:
initWithFileUrl:sampleByteDepth:
writeBuffer
setWriteBuffer:
_writeBuffer
requestHistoricalAudioDataSampleCount
startRecordingSampleCount
useOpportunisticZLL
_requestHistoricalAudioDataWithHostTime
_requestHistoricalAudioDataSampleCount
_useOpportunisticZLL
_startRecordingHostTime
_startRecordingSampleCount
_startAlertBehavior
_stopAlertBehavior
_errorAlertBehavior
_dispatchGroup
_dispatchGroupCounter
_supportDoAP
_isTemporaryPairedNotInContacts
_address
createGrammars
bundleForClass:
bundlePath
_getTrailingPatternsWithGrammars:withLocale:
_getLeadingPatternsWithGrammars:withLocale:
_getRegexPatternsWithGrammars:withUtt:withLocale:
_getLMEWithGrammar:withLocale:
URLSession:didBecomeInvalidWithError:
URLSession:didReceiveChallenge:completionHandler:
URLSessionDidFinishEventsForBackgroundURLSession:
_grammar
_processRemoteHeySiriCommandWithRequest:fromSenderID:withReply:
_processParallelRecordingCommandWithRequest:fromSenderID:withReply:
_sendParallelRecordingsToPeerId:voiceProfileRequestInfo:withReply:
_receiveParallelRecordingFromPeerId:recordingInfo:withReply:
_receiveVoiceProfileFromPeerId:voiceProfileInfo:withReply:
_processDataFetchCommandWithRequest:fromSenderID:withReply:
_processVoiceProfileDeleteCommandWithRequest:fromSenderID:withReply:
_receiveData1FromPeerId:requestInfo:withReply:
_processMusicAccountSignInCommandFromPeerId:requestInfo:withReply:
_processVoiceProfileListQueryCommandFromPeerId:requestInfo:withReply:
_processFetchVoiceProfileCommandFromPeerId:requestInfo:withReply:
_processReverseTransferVoiceProfileCommandFromPeerId:requestInfo:withReply:
_processVoiceProfileUpdateTriggerFromPeerId:requestInfo:withReply:
_sendData1ToPeerId:
_sendVoiceTriggerGradingDataToPeerId:
_sendVoiceProfileUpdateTriggerToPeerId:forLocale:
numberWithUnsignedLong:
sendMessageWithPayload:toPeer:withReply:
_compressFilesInDirectory:matchingPredicate:compressedFileAvailable:
_sendData1File:withFileName:toPeerId:withCompressedFlag:withUncompressedDataSize:withBatchId:withRetainFileFlag:
temporaryDirectory
initNewVoiceProfileWithLocale:
setSiriDebugID:
setSharedHomeID:
setUserName:
setOnboardedType:
sharedHomeID
onboardedType
userName
dateAdded
initWithObjectsAndKeys:
_sendVoiceProfile:toPeerId:
processRemoteCommandWithPayload:fromPeer:withReply:
sendInfo1ToNearbyPeer
sendVoiceTriggerGradingDataToCompanion
sendVoiceProfileUpdatedMessageToNearbyPeerForLocale:
adCompanionServiceProvider
setAdCompanionServiceProvider:
lastCommunicatedPeer
setLastCommunicatedPeer:
voiceTriggerBatchId
setVoiceTriggerBatchId:
voiceIdentificationBatchId
setVoiceIdentificationBatchId:
_adCompanionServiceProvider
_lastCommunicatedPeer
_voiceTriggerBatchId
_voiceIdentificationBatchId
getHostClockFrequency
initWithZeroWindowSize:approxAbsSpeechThreshold:numHostTicksPerAudioSample:
filterZerosInAudioPacket:atBufferHostTime:filteredPacket:
endAudioAndFetchAnyTrailingZerosPacket:
vtEndInSampleCount
setVtEndInSampleCount:
numSamplesProcessed
setNumSamplesProcessed:
_vtEndInSampleCount
_numSamplesProcessed
supportRaiseToSpeak
supportPremiumWatchAssets
supportTTS
rootQueueWithFixedPriority:
supportContinuousVoiceTrigger
supportKeywordDetector
supportSelfTriggerSuppression
supportCSTwoShotDecision
supportSAT
supportCompactPlus
supportPremiumModel
supportTdsrOnCS
hasRemoteCoreSpeech
supportCircularBuffer
getFixedPrioritySerialQueueWithLabel:fixedPriority:
deviceUserAssignedName
initWithConfigPath:resourcePath:
resetBest
analyzeWavData:numSamples:
getAnalyzedResultForPhraseId:
sampleFed
getAnalyzedResult
keywordAnalyzerNDAPI:hasResultAvailable:forChannel:
initWithResult:
bestStart
setBestStart:
bestEnd
setBestEnd:
getSuperVectorWithEndPoint:
getOptionValue:
getThreshold
getLoggingThreshold
activePhraseId
setActivePhraseId:
_novDetector
_lastSampleFed
_sampleFedWrapAroundOffset
_activePhraseId
isBuiltinSpeakerMuted
initWithSignalType:timestamp:
sigGenTs
headerString
initWithStartSample:endSample:confidence:azimuth:estimatedAzimuth:
mostSampledAzimuth
stringForLogging
_spatialSpectrumLogStr
startSample
setStartSample:
endSample
setEndSample:
confidence
setConfidence:
azimuth
setAzimuth:
estimatedAzimuth
setEstimatedAzimuth:
processedAudioDurMs
setProcessedAudioDurMs:
spatialSpectrumData
setSpatialSpectrumData:
azDistribution
setAzDistribution:
_confidence
_azimuth
_estimatedAzimuth
_startSample
_endSample
_processedAudioDurMs
_spatialSpectrumData
_azDistribution
initWithTotalAudioRecorded:featuresAtEndpoint:endpointerType:serverFeatureLatencyDistribution:additionalMetrics:
totalAudioRecorded
setTotalAudioRecorded:
featuresAtEndpoint
setFeaturesAtEndpoint:
endpointerType
setEndpointerType:
serverFeatureLatencyDistribution
setServerFeatureLatencyDistribution:
additionalMetrics
setAdditionalMetrics:
_totalAudioRecorded
_featuresAtEndpoint
_endpointerType
_serverFeatureLatencyDistribution
_additionalMetrics
isDeviceRoleStereo
_isDeviceRoleStereo
waitingForConnection:error:
isConnected
containsValueForKey:
decodeObjectForKey:
encodeObject:forKey:
base64EncodedStringWithOptions:
substringToIndex:
supportsSecureCoding
encodeWithCoder:
initWithCoder:
initWithHash:locale:
initWithData:hash:locale:
builtInRTModelDictionary
modelData
modelLocale
modelHash
digest
signature
certificate
_modelData
_modelLocale
_modelHash
_digest
_signature
_certificate
_fetchJarvisConnectionState
_notifyJarvisConnectionState:
preferredExternalRouteDidChange:
carPlayAudioRouteDidChange:
_isJarvisConnected
lpcmInt16NarrowBandASBD
opusNarrowBandASBD
aiffFileASBD
dateWithTimeIntervalSince1970:
timeIntervalSince1970
dictionaryWithObjectsAndKeys:
dictionaryRepresentation
setDateAdded:
voiceProfileFilePath
setVoiceProfileFilePath:
setProfileID:
setSiriProfileId:
setNumberOfExplicitSatVectors:
setNumberOfBaseProfileSatVectors:
setNumberOfImplicitProfileSatVectors:
_dateAdded
_voiceProfileFilePath
_profileID
_siriProfileId
_onboardedType
_numberOfExplicitSatVectors
_numberOfBaseProfileSatVectors
_numberOfImplicitProfileSatVectors
_sharedHomeID
_siriDebugID
_userName
addiTunesAccountWithMultiUserToken:withDsid:withAltDsid:withHomeId:withHomeUserId:forProfileId:
_checkVoiceTriggerEnabled
_didReceiveVoiceTriggerSettingChanged:
_notifyObserver:withEnabled:
CSVoiceTriggerEnabledMonitor:didReceiveEnabled:
_didReceiveVoiceTriggerSettingChangedInQueue:
_isVoiceTriggerEnabled
startRecordingWithOptions:error:
stopRecording:
didPlayEndpointBeep
hasPendingTwoShotBeep
profileId
setProfileId:
version
setVersion:
onboardType
setOnboardType:
homeId
setHomeId:
siriDebugProfileId
setSiriDebugProfileId:
_profileId
_productCategory
_onboardType
_homeId
_siriDebugProfileId
isRecordingWithStreamHandleId:
prepareAudioStreamRecord:streamHandleId:error:
startAudioStreamWithOption:streamHandleId:error:
stopAudioStreamWithStreamHandleId:error:
convertStopReason:
_handleDidStopWithReason:
resetEndPointer
hasAudioRoute
audioStreamHandleId
setAudioStreamHandleId:
_audioStreamHandleId
_didReceiveSiriSettingChanged:
fetchIsEnabled
_isSiriEnabled
analyzeImplicitUtterance:withVoiceTriggerContext:withCompletion:
triggerVoiceProfileRetrainingWithAsset:
_createAudioStreamWithCurrentNviContext
requestHistoricalAudio
reqStartAudioSampleId
receiveOnlyProcessedChannelData
audioChunkAvailable:numChannels:numSamplesPerChannel:startSampleId:atAbsMachTimestamp:
numChannels
addReceiver:
removeReceiver:
numBytesPerSample
audioStreamProvider:avBufferAvailable:
nviCtx
setNviCtx:
receivers
setReceivers:
_nviCtx
_receivers
_beepFloatVec
_shortBuffer
_numTotalInputSamples
_numTotalOutputSamples
utteranceAudioFilepathForSpIdType:
utteranceMetadataFilePathForSpIdType:
uniqueUttTag
setUniqueUttTag:
asset
fallbackAsset
setVtEventInfo:
invocationStyle
setInvocationStyle:
_uniqueUttTag
_asset
_fallbackAsset
_vtEventInfo
_invocationStyle
fileSystemRepresentation
makeRootlessDirectoryAtPath:error:
convertToRootlessDirectoryAtPath:error:
interfaceWithProtocol:
setWithArray:
setClasses:forSelector:argumentIndex:ofReply:
strongToWeakObjectsMapTable
_createXPCClientConnection
_notifyActivationEvent:completion:
_isVoiceTriggerEvent:
_hasPendingActivationForType:
receiveTestNotificationAPMode
receiveTestNotificationAOPMode
notifyActivationEventForCoreSpeechDaemon:completion:
notifyActivationEvent:deviceId:activationInfo:completion:
_didReceiveAOPFirstPassTrigger:completion:
_setupTestNotification
delegates
setDelegates:
pendingActivationEvent
setPendingActivationEvent:
pendingCompletion
setPendingCompletion:
_delegates
_pendingActivationEvent
_pendingCompletion
addObserver:observeFilteredGestures:includingWhenScreenOff:
gestureType
timestamp
_didReceiveWakeGesture
gestureMonitorDidReceiveWakeGesture:
wakeGestureRecognized:
wakeGestureRecognized:date:
_gestureMonitor
_audioZeroFilterImpl
readAudioChunksFrom:block:
_checkSoftwareUpdateCheckingState
_didReceiveSoftwareUpdateCheckingStateChanged:
_softwareUpdateCheckingState
_notifyObserver:withSoftwareUpdateCheckingRunning:
CSSoftwareUpdateCheckingMonitor:didReceiveStateChanged:
_didReceiveSoftwareUpdateCheckingStateChangedInQueue:
_isSoftwareUpdateCheckingRunning
_addAssetManagerEnabledConditions
_shouldCheckNetworkAvailability
getTestResponse:
getCurrentVoiceTriggerLocale:
setDelayInterstitialSounds:level:completion:
getTriggerCount:
clearTriggerCount:
voiceTriggerAOPModeEnabledPolicy
initWithVoiceProfile:spIdType:languageCode:asset:
pruneVoiceProfileIfNecessary
initWithNumChannels:recordingDuration:samplingRate:
addSamples:numSamples:atHostTime:
sampleCount
copySamplesFrom:to:
stringWithCString:encoding:
createAudioCircularBufferWithDefaultSettings
copySamplesFromHostTime:
copyBufferWithNumSamplesCopiedIn:
bufferLength
setBufferLength:
_csAudioCircularBufferImpl
_bufferLength
_asssetMetaUpdatedKey
_didReceiveNewVoiceTriggerAssetMetaData
initWithRecordingDuration:audioSamplesPerRemoteVAD:audioSampleRate:
remoteVADSampleCount
copySamplesFromAudioSampleCount:toAudioSampleCount:
capacity
size
beginSampleCount
_remoteVADCircularBufferImpl
_audioSamplesPerRemoteVAD
_capacity
_size
_beginSampleCount
setStreaming:
setStreamRequest:
setStreamingUUID:
setStartStreamOption:
streamingUUID
streaming
prepareAudioStreamWithRequest:completion:
lastForwardedSampleCount
setLastForwardedSampleCount:
scheduledFutureSample
setScheduledFutureSample:
streamRequest
startStreamOption
_scheduledFutureSample
_streaming
_lastForwardedSampleCount
_streamRequest
_startStreamOption
_streamingUUID
initWithWordCount:trailingSilenceDuration:eosLikelihood:pauseCounts:silencePosterior:taskName:processedAudioDurationInMilliseconds:
componentsJoinedByString:
initWithWordCount:trailingSilenceFrames:endOfSilenceLikelihood:pauseCounts:silencePosterior:taskName:
wordCount
setWordCount:
trailingSilenceDuration
setTrailingSilenceDuration:
eosLikelihood
setEosLikelihood:
pauseCounts
setPauseCounts:
silencePosterior
setSilencePosterior:
processedAudioDurationInMilliseconds
setProcessedAudioDurationInMilliseconds:
taskName
setTaskName:
_wordCount
_trailingSilenceDuration
_eosLikelihood
_pauseCounts
_silencePosterior
_processedAudioDurationInMilliseconds
_taskName
voiceControllerDidStartRecording:successfully:
voiceControllerDidStartRecording:successfully:error:
voiceControllerDidStopRecording:forReason:
voiceControllerDidDetectStartpoint:
voiceControllerDidDetectEndpoint:ofType:
voiceControllerDidDetectEndpoint:ofType:atTime:
voiceControllerEncoderErrorDidOccur:error:
voiceControllerDidFinishAlertPlayback:ofType:error:
voiceControllerDidFinishAlertPlayback:withSettings:error:
voiceControllerRecordHardwareConfigurationDidChange:toConfiguration:
voiceControllerBeginRecordInterruption:
voiceControllerBeginRecordInterruption:withContext:
voiceControllerEndRecordInterruption:
voiceControllerMediaServicesWereLost:
voiceControllerMediaServicesWereReset:
voiceControllerWillSetAudioSessionActive:willActivate:
voiceControllerDidSetAudioSessionActive:isActivated:
voiceControllerRecordBufferAvailable:buffer:
voiceControllerLPCMRecordBufferAvailable:buffer:
voiceControllerWirelessSplitterRouteAvailable:devices:
voiceControllerDidStartRecording:forStream:successfully:error:
voiceControllerDidStopRecording:forStream:forReason:
voiceControllerAudioCallback:forStream:buffer:
voiceControllerStreamInvalidated:forStream:
voiceControllerPlaybackBufferAvailable:buffer:
voiceControllerDidStartPlaying:successfully:
voiceControllerDidStopPlaying:forReason:
voiceControllerDecoderErrorDidOccur:error:
voiceControllerPlaybackHardwareConfigurationDidChange:toConfiguration:
voiceControllerBeginPlaybackInterruption:
voiceControllerEndPlaybackInterruption:
_voiceController
trainUtterance:languageCode:
sharedConnection
_checkSiriRestrictedOnLockScreen
effectiveBoolValueForSetting:
_notifyObserver:withRestricted:
CSSiriRestrictionOnLockScreenMonitor:didReceiveRestrictionChanged:
_didReceiveRestrictionChanged:
profileConnectionDidReceiveRestrictionChangedNotification:userInfo:
profileConnectionDidReceivePasscodeChangedNotification:userInfo:
profileConnectionDidReceivePasscodePolicyChangedNotification:userInfo:
profileConnectionDidReceiveProfileListChangedNotification:userInfo:
profileConnectionDidReceiveEffectiveSettingsChangedNotification:userInfo:
profileConnectionDidReceiveDefaultsChangedNotification:userInfo:
profileConnectionDidReceiveAppWhitelistChangedNotification:userInfo:
_didReceiveRestrictionChangedInQueue:
_isRestricted
_checkSpringBoardStarted
_didReceiveSpringboardStarted:
_notifyObserver:withStarted:
CSSpringboardStartMonitor:didReceiveStarted:
_didReceiveSpringboardStartedInQueue:
_isSpringBoardStarted
setStreamState:
_streamStateName:
setProviderDelegate:
setCurrentContext:streamHandleId:error:
_prepareAudioStreamSync:request:error:
historicalBufferRequestStreams
_createCircularBufferIfNeeded
_audioStreamWithRequest:streamName:error:
_handleAudioSystemFailure
_startAudioStream:option:completion:
_prepareAudioStream:request:completion:
startPendingOnStoppingStreams
startPendingOnStoppingStreamToCompletionDict
_didPlayStartAlertSoundForSiri:audioStream:
alertPlaybackFinishWaitingStreams
alertPlaybackFinishWaitingCompletions
_scheduleAlertFinishTimeout:
circularBufferStartHostTime
circularBufferStartSampleCount
startPendingStreams
pendingStartCompletions
_holdRecordingTransactionIfNeeded
recordingSampleRateWithStreamHandleId:
_scheduleDidStartRecordingDelegateWatchDog
_resetCircularBufferStartTime
setCircularBufferStartHostTime:
setCircularBufferStartSampleCount:
streams
_clearDidStartRecordingDelegateWatchDog
_releaseRecordingTransactionIfNeeded
_clearDidStopRecordingDelegateWatchDog
_preEpilogueAudioStream
stopPendingStreams
pendingStopCompletions
_postEpilogueAudioStream
_shouldHandleStartPendingOnStopping:withStopReason:
objectEnumerator
_stopAudioStream:option:completion:
_cs_isHashTableEmpty
_shouldStopRecording
_scheduleDidStopRecordingDelegateWatchDog
_audioChunkFrom:to:
_saveRecordingBufferFrom:to:toURL:
streamHolders
recordRouteWithStreamHandleId:
recordDeviceInfoWithStreamHandleId:
recordSettingsWithStreamHandleId:
setSessionDelegate:
prewarmAudioSessionWithStreamHandleId:error:
_activateAudioSessionWithReason:error:
activateAudioSessionWithReason:streamHandleId:error:
_deactivateAudioSession:error:
setAlertDelegate:
playRecordStartingAlertAndResetEndpointerFromStream:
_isVoiceTriggerInfoAvailableLocally:
_processAudioBuffer:remoteVAD:atTime:
_handleDidStartAudioStreamWithResult:error:
_handleDidStopAudioStreamWithReason:
sessionDelegate
providerDelegate
_fetchHistoricalAudioAndForwardToStream:remoteVAD:
setRemoteVAD:
_processAudioBufferForWatch:remoteVAD:atTime:
_forwardAudioChunk:remoteVAD:atTime:toStream:
isNarrowBandWithStreamHandleId:
_didReceiveFinishStartAlertPlaybackAt:
alertDelegate
willDestroy
initWithDescription:
isSessionCurrentlyActivated
_schduleDidStartRecordingDelegateWatchDogWithToken:
_scheduleDidStopRecordingDelegateWatchDog:
_tearDownCircularBufferIfNeeded
notifyProviderContextChanged
recordQueue
setRecordQueue:
streamState
setStartPendingStreams:
setStartPendingOnStoppingStreams:
setAlertPlaybackFinishWaitingStreams:
setStreams:
setStopPendingStreams:
setPendingStartCompletions:
setAlertPlaybackFinishWaitingCompletions:
setPendingStopCompletions:
setStartPendingOnStoppingStreamToCompletionDict:
setStreamHolders:
setHistoricalBufferRequestStreams:
circularBuffer
setCircularBuffer:
lastAudioRecorderContext
setLastAudioRecorderContext:
audioSystemRecovering
setAudioSystemRecovering:
audioPreprocessor
setAudioPreprocessor:
recordingTransaction
setRecordingTransaction:
recordingWillStartGroup
setRecordingWillStartGroup:
waitingForAlertFinish
setWaitingForAlertFinish:
alertPlaybackFinishTimeoutToken
setAlertPlaybackFinishTimeoutToken:
startRecordingWatchDogToken
setStartRecordingWatchDogToken:
stopRecordingWatchDogToken
setStopRecordingWatchDogToken:
_audioSystemRecovering
_waitingForAlertFinish
_recordQueue
_streamState
_startPendingStreams
_startPendingOnStoppingStreams
_alertPlaybackFinishWaitingStreams
_streams
_stopPendingStreams
_pendingStartCompletions
_alertPlaybackFinishWaitingCompletions
_pendingStopCompletions
_startPendingOnStoppingStreamToCompletionDict
_providerDelegate
_streamHolders
_historicalBufferRequestStreams
_circularBuffer
_alertDelegate
_lastAudioRecorderContext
_audioPreprocessor
_recordingTransaction
_recordingWillStartGroup
_alertPlaybackFinishTimeoutToken
_startRecordingWatchDogToken
_stopRecordingWatchDogToken
_circularBufferStartHostTime
_circularBufferStartSampleCount
_addListeningEnabledConditions
initWithArray:
CSAlwaysOnProcessorStateMonitor:didReceiveStateChange:
_didReceiveAOPListeningStateChange:
_isListeningEnabled
initWithBytes:length:
subdataWithRange:
appendData:
remoteVADAvailable
subChunkFrom:numSamples:forChannel:
skipSamplesAtStartSuchThatNumSamplesReceivedSoFar:reachesACountOf:completionHandler:
splitAudioChunkSuchThatNumSamplesReceivedSoFar:reachesACountOf:completionHandler:
_data
_numSamples
_startSampleCount
_hostTime
_remoteVAD
bestPhrase
bestScore
earlyWarning
setSampleFed:
setBestPhrase:
setBestScore:
setEarlyWarning:
isRescoring
setIsRescoring:
_earlyWarning
_isRescoring
_bestScore
_sampleFed
_bestPhrase
_bestStart
_bestEnd
setEndpointerModelVersion:
_canProcessCurrentRequest
_endpointerModelVersion
macHostTimeFromBridgeHostTime:
_deregisterAudioSessionNotifications
_registerAudioSessionNotifications
sessionInfoQueue
setSessionInfoQueue:
_sessionInfoQueue
saveMetadata:isExplicitEnrollment:
saveUtterance:utteranceAudioPath:numSamplesToWrite:isExplicitEnrollment:
writeMetaDict:atMetaPath:
saveRawUtteranceAndMetadata:to:isExplicitEnrollment:
saveUtteranceAndMetadata:atDirectory:isExplicitEnrollment:
_setupNNVADEndpointer
isRecordContextVoiceTrigger:
isRecordContextHearstVoiceTrigger:
isRecordContextJarvisVoiceTrigger:
endpointer:detectedTwoShotAtTime:
resetForVoiceTriggerTwoShotWithSampleRate:
endPointAnalyzerType
endpointerDelegate
hybridEndpointer
setHybridEndpointer:
nnvadEndpointer
setNnvadEndpointer:
activeEndpointer
setActiveEndpointer:
recordingDidStop
setRecordingDidStop:
_recordingDidStop
_endpointerDelegate
_hybridEndpointer
_nnvadEndpointer
_activeEndpointer
initWithBool:
initWithDouble:
initWithLongLong:
initWithUnsignedLongLong:
objCType
longLongValue
_checkFirstUnlocked
_didReceiveFirstUnlock:
_notifyObserver:withUnlocked:
_firstUnlockNotified
_didReceiveFirstUnlockInQueue:
_firstUnlocked
_scaleDecayConstants:
_savePeaks:averagePower:maxSample:
_linearToDB:
_ampToDB:
_averagePowerI
_instantaneousMode
_peak
_maxPeak
_decay
_peakDecay
_averagePowerPeak
_peakHoldCount
_previousBlockSize
_decay1
_peakDecay1
_checkPhraseSpotterEnabled
CSPhraseSpotterEnabledMonitor:didReceiveEnabled:
_didReceivePhraseSpotterSettingChangedInQueue:
_phraseSpotterEnabledDidChange
_isPhraseSpotterEnabled
_handleClientEvent:
_handleClientMessage:client:
_handleClientError:client:
_sendReplyMessageWithResult:error:event:client:
initWithConnection:
activateConnection
connection
setConnection:
_connection
createSATAnalyzersForCSSpIdType:withModel:withAsset:withFallbackAsset:
initWithCSSpIdType:modelType:profile:locale:assetResourcePath:assetHash:
analyzeSuperVector:withDimensions:
initializeSATWithModelPath:
getSpeakerVectorAtIndex:
sysConfigRoot
satModelDir
satAudioDir
satModelFilePath
tdSrSysConfigFile
tdSrSysConfigRoot
satModelAvailable
satScoreVTScale
satScoreVTOffset
satScoreNonVTScale
satScoreNonVTOffset
combinationWeight
satLogitCeilScore
satLogitFloorScore
satImplicitThreshold
satImplicitBaseProfileThreshold
satImplicitProfileThreshold
satImplicitProfileDeltaThreshold
retrainThresholdTrigger
retrainExplicitUttThresholdSAT
retrainExplicitUttThresholdTDSR
retrainThresholdSAT
retrainThresholdTDSR
pruningNumRetentionUtterance
maximumSpeakerVectors
maxAllowedImplicitUtterances
maxAllowedBaseProfileUtterances
voiceProfilePruningCookie
_satModelAvailable
_satScoreThreshold
_satScoreVTScale
_satScoreVTOffset
_satScoreNonVTScale
_satScoreNonVTOffset
_combinationWeight
_satLogitCeilScore
_satLogitFloorScore
_satImplicitThreshold
_retrainThresholdTrigger
_retrainExplicitUttThresholdSAT
_retrainExplicitUttThresholdTDSR
_retrainThresholdSAT
_retrainThresholdTDSR
_pruningNumRetentionUtterance
_maximumSpeakerVectors
_maxAllowedImplicitUtterances
_maxAllowedBaseProfileUtterances
_sysConfigRoot
_satModelDir
_satAudioDir
_satModelFilePath
_tdSrSysConfigFile
_tdSrSysConfigRoot
_satImplicitBaseProfileThreshold
_satImplicitProfileThreshold
_satImplicitProfileDeltaThreshold
_voiceProfilePruningCookie
initWithEndpointThreshold:
samplingRate
dictationLanguages
currentKeyboard
wasLanguageToggled
multilingualKeyboardLanguages
keyboardConvoLanguagePriors
keyboardGlobalLanguagePriors
previousMessageLanguage
globalLastKeyboardUsed
dictationLanguagePriors
conversationalMessages
_wasLanguageToggled
_samplingRate
_dictationLanguages
_currentKeyboard
_multilingualKeyboardLanguages
_keyboardConvoLanguagePriors
_keyboardGlobalLanguagePriors
_previousMessageLanguage
_globalLastKeyboardUsed
_dictationLanguagePriors
_conversationalMessages
initWithConfig:samplingRate:minSpeechFrames:numLeadingFrames:delegate:
_didReceiveMediaserverNotification:
_notifyObserver:withMediaserverState:
notifyAudioServerCrash
_mediaserverdDidRestart
serverState
setServerState:
_serverState
_didReceiveDaemonStateChanged:
_notifyObserver:withDaemonState:
coreSpeechDaemonStateMonitor:didReceiveStateChanged:
notifyDaemonStateChanged:
initWithConfigPath:triggerTokens:useKeywordSpotting:
runRecognition
_recognizeWavData:length:
triggerConfidence
_previousUtteranceTokens
_triggerTokenList
_useKeywordSpotting
_triggerConfidence
_availabilityChanged
_didReceivedNetworkAvailabilityChangedNotification:
_notifyObserver:withNetworkAvailability:
CSNetworkAvailabilityMonitor:didReceiveNetworkAvailabilityChanged:
handleSpeechDetectionVADPresentChange:
signalProvider:hasSignalData:
setSignalBuffer:
_signalBuffer
initWithUUIDString:
initWithRoute:isRemoteDevice:remoteDeviceUID:remoteDeviceProductIdentifier:
initWithFormat:
decodeObjectOfClass:forKey:
isRemoteDevice
remoteDeviceUID
remoteProductIdentifier
initWithAVVCRecordDeviceInfo:
remoteDeviceProductIdentifier
_isRemoteDevice
_route
_remoteDeviceUID
_remoteDeviceProductIdentifier
receiveOpportuneSpeakListenerStart
receiveOpportuneSpeakListenerStop
listener
hybridEndpointerAssetFilename
initWithResourcePath:configFile:configVersion:
fallBackAssetResourcePath
_decodeJson:
defaultFallBackAssetForSmartSiriVolume
defaultFallBackAssetForHearst
getBoolForKey:category:default:
getStringForKey:category:default:
containsKey:category:
containsCategory:
hashFromResourcePath
configVersion
_decodedInfo
_path
_resourcePath
_configVersion
_configureAudioConverter:
_convertBufferedLPCM:allowPartial:timestamp:
replaceBytesInRange:withBytes:length:
_opusConverter
_bufferedLPCM
_recordBasePacketsPerSecond
_opusOutASBD
_convertPacketCount
_convertAudioCapacity
_lastTimestamp
_outPacketSizeInSec
_notifyObserver:withLanguageCode:
_didReceiveLanguageCodeUpdate
_checkAllConditionsEnabled
notifyCallback:
notifyCallback
_monitors
_conditions
_callback
splitAudioDataToReachSampleCount:currSampleCount:numBytesPerSample:completionHandler:
rawMicChannelsDataWithNumSamplesPerChannel:
strRepForFloatData
signalProviderToDatasourceMap
isSignalProviderDisabledInCFPrefs:
isDirectionalityLoggingEnabled
vadSoSThresholdInMs
vadSoSConvergenceInMs
kwdConfigPath
kwdThresholdsMap
isDirectionalityAvailable
dirAzimuthEMAParam
dirAzimuthMatchThreshold
isAsdAvailable
anchorAsdConfigFile
anchorAsdConfigRoot
payloadAsdFrontendConfigFile
payloadAsdFrontendConfigRoot
payloadAsdModelPath
nviConfigRoot
hepConfigFilepath
isRecordContextHearstDoubleTap:
isRecordContextRaiseToSpeak:
isRecordContextAutoPrompt:
isRecordContextHomeButtonPress:
isRecordContextSpeakerIdTrainingTrigger:
isRecordContextJarvisButtonPress:
recordContextString:
speakerDetectorThreshold
maxSpeakerVectorsToPersist
_initializeSAT:
addLastTriggerToProfileWithSuperVector:
initWithAsset:speakerModel:
_initializeNDAPI:resourcePath:
getSatThreshold
computeSATScore:
analyzeWavForEnrollment:numSamples:
addLastTriggerToProfile
getMaxSpeakerVectorsToPersist
_threshold
_maxSpeakerVectorsToPersist
_spkModel
languageDetectorConfigFile
startOfSpeechDetectorConfigFile
spgConfigFile
CVTThreshold
VTSecondPassCategoryForFirstPassSource:
VTSecondPassPreTriggerAudioTimeFrom:
_sampleLengthFrom:To:
_keywordAnalyzer
_lastKeywordScore
_keywordThreshold
_audioBuffer
remoteVoiceActivityAvailable
remoteVoiceActivityVAD
remoteVoiceActivityVADBuffer
_voiceControllerWithError:
_destroyVoiceController
_audioRecorderDidStartRecordingSuccessfully:streamHandleID:error:
setRecordDelegate:
initWithError:
setContextForStream:forStream:error:
_getRecordSettingsWithRequest:
_createDeInterleaverIfNeeded
initWithStreamID:settings:bufferDuration:
prepareRecordForStream:error:
_shouldInjectAudio
_needResetAudioInjectionIndex:
_startAudioStreamForAudioInjection
startRecordForStream:error:
stopRecordForStream:error:
getCurrentSessionState
getCurrentStreamState:
getRecordDeviceInfoForStream:
getRecordSettingsForStream:
activateAudioSessionForStream:isPrewarm:error:
_convertDeactivateOption:
deactivateAudioSessionWithOptions:
setAllowMixableAudioWhileRecording:error:
channels
packetDescriptionCount
bytesDataSize
packetDescriptions
updateMeterForStream:
getPeakPowerForStream:forChannel:
getAveragePowerForStream:forChannel:
streamDescription
_deinterleaveBufferIfNeeded:
_compensateChannelDataIfNeeded:receivedNumChannels:
_processAudioChain:audioStreamHandleId:remoteVAD:atTime:
replaceBytesInRange:withBytes:
playAlertSoundForType:overrideMode:
_processAudioBuffer:audioStreamHandleId:
_audioRecorderDidStopRecordingForReason:streamHandleID:
configureAlertBehavior:audioStreamHandleId:
_shouldUseRemoteRecordForContext:
_shouldUseRemoteBuiltInMic:
_deinterleaver
_interleavedABL
_pNonInterleavedABL
_remoteRecordClient
_shouldUseRemoteRecord
_opusDecoders
_audioFileReader
_audioFilePathIndex
_waitingForDidStart
dataWithPropertyList:format:options:error:
encryptedDataWithAESGCMKey:completion:
propertyListWithData:options:format:error:
decryptedDataWithAESGCMKey:ivData:tagData:error:
randomBytesWithLength:error:
errorMessageForCCErrorCode:
reqStartMachAbsTime
setReqStartMachAbsTime:
shouldLogRawSensorData
setShouldLogRawSensorData:
rootLogDir
setRootLogDir:
_requestHistoricalAudio
_shouldLogRawSensorData
_reqStartAudioSampleId
_reqStartMachAbsTime
_rootLogDir
getVoiceTriggerProfilesAESKey
generateIfNecessaryAESKeyWithKeySizeInBits:applicationTag:keyLabel:shouldGenerateIfNecessary:
generateAESKeyWithKeySizeInBits:
storeAESKeyInKeychain:applicationTag:keyLabel:
getAESKeyFromKeychainWithApplicationTag:keyLabel:
deleteAESKeyWithApplicationTag:keyLabel:
getKeychainAttributesForAESKeyWithApplicationTag:keyLabel:
setTriggerMode:
getTriggerMode
_transaction
_description
decodeIntegerForKey:
decodeInt64ForKey:
encodeInteger:forKey:
encodeInt64:forKey:
setSigType:
setSigGenTs:
_sigType
_sigGenTs
isSiriRestrictedOnLockScreen
@16@0:8
v24@0:8@16
v16@0:8
B16@0:8
Q16@0:8
v24@0:8Q16
v20@0:8B16
@"NSMutableArray"
B24@0:8@16
#16@0:8
@24@0:8:16
@32@0:8:16@24
@40@0:8:16@24@32
B24@0:8#16
B24@0:8:16
Vv16@0:8
^{_NSZone=}16@0:8
B24@0:8@"Protocol"16
@"NSString"16@0:8
v32@0:8@16Q24
v32@0:8@"NSStream"16Q24
@32@0:8@16@24
@"NSOutputStream"
v64@0:8@16@24B32@36@44Q52B60
@24@0:8@16
v32@0:8@16@24
v24@0:8@?16
@"NSObject<OS_dispatch_queue>"
@"NSHashTable"
v32@0:8@"<CSAudioSessionInfoProviding>"16@"NSDictionary"24
v28@0:8@16B24
v28@0:8@"CSXPCClient"16B24
I16@0:8
@"<CSAudioSessionInfoProviding>"
@"CSXPCClient"
v32@0:8@16q24
q16@0:8
f16@0:8
@"NviSignalProvidersController"
@"CSSpeakerIdNviSignalReceiver"
@"CSSpIdContext"
@"NSString"
B32@0:8^B16^Q24
Q24@0:8Q16
@"<CSBiometricMatchMonitorDelegate>"
@32@0:8@16f24I28
v20@0:8f16
v28@0:8@16I24
v28@0:8@"CSVoiceTriggerXPCClient"16B24
v28@0:8B16@20
v36@0:8B16@20d28
v28@0:8B16d20
@"<CSVoiceTriggerXPCServiceDelegate>"
@"NSMutableSet"
@"CSVoiceTriggerXPCClient"
@96@0:8{AudioStreamBasicDescription=dIIIIIIII}16{AudioStreamBasicDescription=dIIIIIIII}56
@24@0:8Q16
@104@0:8@16{AudioStreamBasicDescription=dIIIIIIII}24{AudioStreamBasicDescription=dIIIIIIII}64
r*16@0:8
@36@0:8@16q24B32
v36@0:8@16@24f32
v40@0:8@16@24Q32
v40@0:8@16@24@?32
@48@0:8@16@24@32^@40
v32@0:8@"CSMediaPlayingMonitor"16q24
v32@0:8@"<CSAudioStreamProviding>"16q24
v32@0:8@"<CSAudioStreamProviding>"16@"CSAudioChunk"24
v32@0:8@"<CSAudioStreamProviding>"16@"CSAudioChunkForTV"24
v28@0:8@"CSSiriEnabledMonitor"16B24
v24@0:8@"CSAudioServerCrashMonitor"16
v40@0:8@16@24@32
v44@0:8@16@24B32@36
v40@0:8@"CSSiriClientBehaviorMonitor"16@"CSAudioRecordContext"24@"CSAudioStartStreamOption"32
v44@0:8@"CSSiriClientBehaviorMonitor"16@"CSAudioRecordContext"24B32@"CSAudioStartStreamOption"36
v32@0:8@"CSSiriClientBehaviorMonitor"16@"CSAudioStopStreamOption"24
v32@0:8@"NSDictionary"16@"NSString"24
v24@0:8@"NSDictionary"16
v24@0:8@"NSData"16
v32@0:8@"CSAlarmMonitor"16q24
v32@0:8@"CSTimerMonitor"16q24
v28@0:8@16f24
v28@0:8@"CSVolumeMonitor"16f24
@28@0:8f16@20
v32@0:8d16@?24
v28@0:8I16q20
v52@0:8@16q24B32Q36Q44
f24@0:8q16
f24@0:8f16f20
f36@0:8f16f20f24f28f32
f44@0:8f16f20f24f28f32f36f40
f28@0:8f16f20f24
f20@0:8f16
v24@0:8q16
{unique_ptr<SmartSiriVolume, std::__1::default_delete<SmartSiriVolume> >="__ptr_"{__compressed_pair<SmartSiriVolume *, std::__1::default_delete<SmartSiriVolume> >="__value_"^{SmartSiriVolume}}}
{vector<float, std::__1::allocator<float> >="__begin_"^f"__end_"^f"__end_cap_"{__compressed_pair<float *, std::__1::allocator<float> >="__value_"^f}}
@"NSUserDefaults"
@"CSSmartSiriVolumeEnablePolicy"
@"CSAsset"
@"<CSSmartSiriVolumeDelegate>"
@"CSAudioStream"
@"NSObject<OS_dispatch_source>"
v40@0:8@"CSVoiceTriggerAwareZeroFilter"16@"NSData"24Q32
v40@0:8@"CSBeepCanceller"16@"NSData"24Q32
@20@0:8f16
v32@0:8f16B20@24
B20@0:8f16
@"<CSAudioPreprocessorDelegate>"
@"CSAudioSampleRateConverter"
@"CSVoiceTriggerAwareZeroFilter"
@"CSBeepCanceller"
@"CSAudioZeroCounter"
@52@0:8@16@24@32B40@44
d16@0:8
v20@0:8i16
f32@0:8@16Q24
@"NSData"
@24@0:8^{BTLocalDeviceImpl=}16
v40@0:8^{BTDeviceImpl=}16I24i28I32i36
v28@0:8^{BTSessionImpl=}16i24
v24@0:8^{BTSessionImpl=}16
v32@0:8^{BTLocalDeviceImpl=}16i24i28
^{BTSessionImpl=}16@0:8
^{BTLocalDeviceImpl=}16@0:8
v24@0:8^{BTLocalDeviceImpl=}16
^{BTSessionImpl=}
^{BTLocalDeviceImpl=}
@"NSArray"
@"NSObject<OS_dispatch_group>"
B40@0:8@16@24Q32
v36@0:8@16B24@?28
v56@0:8@16@24@32@40@?48
v72@0:8@16@24@32@40@48@56@?64
@20@0:8B16
v40@0:8@"NSObject<OS_xpc_object>"16@"NSObject<OS_xpc_object>"24@"NSObject<OS_xpc_object>"32
v40@0:8@"CSXPCConnection"16@"NSObject<OS_xpc_object>"24@"NSObject<OS_xpc_object>"32
B24@0:8d16
@20@0:8I16
^{OpaqueExtAudioFile=}
{AudioStreamBasicDescription="mSampleRate"d"mFormatID"I"mFormatFlags"I"mBytesPerPacket"I"mFramesPerPacket"I"mBytesPerFrame"I"mChannelsPerFrame"I"mBitsPerChannel"I"mReserved"I}
@"<CSAudioFileReaderDelegate>"
^{OpaqueAudioConverter=}96@0:8{AudioStreamBasicDescription=dIIIIIIII}16{AudioStreamBasicDescription=dIIIIIIII}56
^{OpaqueAudioConverter=}
i16@0:8
@"<CSLanguageDetectorAssetMonitorDelegate>"
v32@0:8@16@?24
v28@0:8B16@?20
v32@0:8@16d24
@"<CSOpportuneSpeakListenerDelegate>"
@"CSSPGEndpointAnalyzer"
@"<CSAudioStreamProviding>"
@"<CSAudioSessionProviding>"
@"CSAudioRecordContext"
@"CSPlainAudioFileWriter"
v64@0:8@16@24@32@40@48@?56
@"NSObject<OS_xpc_object>"
@40@0:8@16Q24@32
v24@0:8d16
v40@0:8Q16@24@32
v40@0:8Q16@"NSDictionary"24@"NSDictionary"32
v24@0:8@"CSAudioChunk"16
@"<CSEndpointAnalyzerDelegate>"16@0:8
v24@0:8@"<CSEndpointAnalyzerDelegate>"16
@"<CSEndpointAnalyzerImplDelegate>"16@0:8
v24@0:8@"<CSEndpointAnalyzerImplDelegate>"16
v24@0:8@"CSServerEndpointFeatures"16
v32@0:8d16@?<v@?B@"NSArray">24
@"<CSEndpointAnalyzerDelegate>"
@"<CSEndpointAnalyzerImplDelegate>"
@"<CSLanguageDetectorDelegate>"
@24@0:8q16
@24@0:8^{_NSZone=}16
@32@0:8q16@24
q24@0:8q16
@56@0:8@16Q24Q32@40@48
v48@0:8@16Q24@?32@?40
B24@0:8Q16
v32@0:8@"CSSelfTriggerDetector"16@"NSDictionary"24
@"<CSMyriadSelfTriggerCoordinatorDelegate>"
B44@0:8@16@24@32B40
B44@0:8@"CSVTUITrainingSession"16@"NSData"24@"NSString"32B40
v28@0:8B16@"NSError"20
v24@0:8@"NSError"16
v40@0:8@16d24@32
v32@0:8@"<CSEndpointAnalyzer>"16d24
v40@0:8@"<CSEndpointAnalyzer>"16d24@"CSEndpointerMetrics"32
@24@0:8@?16
q36@0:8q16B24@?28
B24@0:8q16
v32@0:8i16B20@?24
@"<CSVTUIAudioSession>"
@"CSNNVADEndpointAnalyzer"
@"CSVTUIKeywordDetector"
@"CSVTUITrainingSession"
@"SFSpeechRecognizer"
@"<CSVTUITrainingManagerDelegate>"
v36@0:8^@16^@24B32
v40@0:8@"CSCommandControlBehaviorMonitor"16@"CSAudioRecordContext"24@"CSAudioStartStreamOption"32
v44@0:8@"CSCommandControlBehaviorMonitor"16@"CSAudioRecordContext"24B32@"CSAudioStartStreamOption"36
v32@0:8@"CSCommandControlBehaviorMonitor"16@"CSAudioStopStreamOption"24
v32@0:8Q16@24
@32@0:8@"CSSpIdContext"16@"<CSSpIdSpeakerRecognizerDelegate>"24
@"NSDictionary"16@0:8
B32@0:8r^v16Q24
@112@0:8@16{AudioStreamBasicDescription=dIIIIIIII}24{AudioStreamBasicDescription=dIIIIIIII}64Q104
B32@0:8r^v16q24
@"NSURL"
v56@0:8Q16Q24@32@40@?48
S16@0:8
@40@0:8@16@24Q32
B24@0:8@?16
v32@0:8@"CSVoiceTriggerAssetHandler"16@"CSAsset"24
v40@0:8@"CSActivationEventNotifier"16@"CSActivationEvent"24@?<v@?B@"NSError">32
v56@0:8@16Q24@32@40Q48
v40@0:8@16Q24@32
v44@0:8@16Q24B32@36
v40@0:8@16Q24q32
v40@0:8@16q24@32
v56@0:8@"CSAudioRecorder"16Q24@"NSData"32@"NSData"40Q48
v40@0:8@"CSAudioRecorder"16Q24@"CSAudioChunkForTV"32
v44@0:8@"CSAudioRecorder"16Q24B32@"NSError"36
v40@0:8@"CSAudioRecorder"16Q24q32
v32@0:8@"CSAudioRecorder"16q24
v40@0:8@"CSAudioRecorder"16q24@"NSError"32
v24@0:8@"CSAudioRecorder"16
v32@0:8@"CSAudioRecorder"16@"NSDictionary"24
v28@0:8@"CSAudioRecorder"16B24
v32@0:8@"CSAudioRecorder"16@"NSError"24
v28@0:8@"CSVoiceTriggerXPCService"16B24
v32@0:8@"CSAudioProvider"16Q24
@32@0:8@16^@24
@24@0:8^@16
@"CSSmartSiriVolume"
@"CSAudioRecorder"
@"NSMutableDictionary"
@"CSFallbackAudioSessionReleaseProvider"
@"<CSSpeechManagerDelegate>"
@"CSSpIdImplicitTraining"
@"CSOpportuneSpeakListnerTestService"
v32@0:8Q16Q24
@"<CSCommandControlListenerDelegate>"
v40@0:8@16@?24@?32
@32@0:8@16@?24
@40@0:8@16@24@32
@40@0:8@16Q24Q32
v36@0:8@16@24B32
B32@0:8@16@24
B40@0:8@16@24@32
@"CSVoiceIdXPCClient"
v32@0:8@"CSAssetController"16Q24
v32@0:8@16@"NSString"24
@32@0:8Q16@24
v32@0:8Q16@?24
v40@0:8Q16@24@?32
@"CSPolicy"
@"CSAssetDownloadingOption"
v24@0:8@"SFSpeechRecognitionTask"16
v32@0:8@"SFSpeechRecognitionTask"16@"SFTranscription"24
v32@0:8@"SFSpeechRecognitionTask"16@"SFSpeechRecognitionResult"24
v28@0:8@"SFSpeechRecognitionTask"16B24
@96@0:8q16q24@32@40@48@56@64@72@80@?88
v24@0:8i16B20
@"SFSpeechAudioBufferRecognitionRequest"
@"SFSpeechRecognitionTask"
@"NSTimer"
@"<CSVTUITrainingSessionDelegate>"
@32@0:8@"<NviDataSource>"16@"<NviAssetsProvider>"24
v24@0:8@"<NviSignalProviderDelegate>"16
v32@0:8@"NviContext"16@?<v@?B@"NSError">24
v24@0:8@?<v@?B@"NSError">16
@"NSMutableData"
@"<CSKeywordAnalyzerNDEAPIScoreDelegate>"
v44@0:8@16@24f32Q36
v44@0:8@"CSAudioConverter"16@"NSArray"24f32Q36
v32@0:8@"<CSSpIdSpeakerRecognizer>"16@"NSDictionary"24
v36@0:8@16f24Q28
v36@0:8@"CSSmartSiriVolumeController"16f24Q28
v24@0:8@"<CSAudioSessionProviding>"16
v32@0:8@"<CSAudioSessionProviding>"16@"NSDictionary"24
v28@0:8@"<CSAudioSessionProviding>"16B24
v40@0:8@"<CSAudioAlertProviding>"16q24@"NSError"32
v32@0:8@"CSAudioSessionController"16@"NSDictionary"24
v32@0:8@"CSContinuousVoiceTrigger"16@"NSDictionary"24
v32@0:8@"CSContinuousVoiceTrigger"16d24
B32@0:8@16^@24
B32@0:8d16^@24
v48@0:8d16@24@?32@?40
B28@0:8^@16B24
{AudioStreamBasicDescription=dIIIIIIII}16@0:8
B24@0:8^@16
B24@0:8B16B20
B32@0:8@16q24
f24@0:8Q16
@?16@0:8
@"CSAudioConverter"
@"NSDictionary"
@"<CSSpeechControllerDelegate>"
@"<CSSpeakerIdentificationDelegate>"
@"CSEndpointerProxy"
@"<CSAudioAlertProviding>"
@"<CSAudioMeterProviding>"
@"<CSAudioMetricProviding>"
@"<CSBargeInModeProviding>"
@"CSSelectiveChannelAudioFileWriter"
@"CSSmartSiriVolumeController"
@"CSSpeakerIdRecognizerFactory"
@"<CSSpIdSpeakerRecognizer>"
@"CSUserVoiceProfileStore"
@"CSSpeechEndHostTimeEstimator"
@"CSLanguageDetector"
@"NSUUID"
@"CSAudioPowerMeter"
@40@0:8Q16Q24@32
d24@0:8d16
B20@0:8B16
@"<NviAssetsProvider>"
@"NSMapTable"
B32@0:8Q16^@24
v24@0:8@"<CSAudioSessionProvidingDelegate>"16
@40@0:8@16@24^@32
B40@0:8@16@24^@32
@32@0:8Q16Q24
v40@0:8Q16Q24@32
@32@0:8@16d24
B32@0:8@"CSAudioRecordContext"16^@24
@"CSAudioStream"40@0:8@"CSAudioStreamRequest"16@"NSString"24^@32
v40@0:8@"CSAudioStreamRequest"16@"NSString"24@?<v@?@"CSAudioStream"@"NSError">32
B40@0:8@"CSAudioStream"16@"CSAudioStreamRequest"24^@32
v40@0:8@"CSAudioStream"16@"CSAudioStreamRequest"24@?<v@?B@"NSError">32
v40@0:8@"CSAudioStream"16@"CSAudioStartStreamOption"24@?<v@?B@"NSError">32
v40@0:8@"CSAudioStream"16@"CSAudioStopStreamOption"24@?<v@?B@"NSError">32
@"CSAudioChunk"32@0:8Q16Q24
@"CSAudioChunk"24@0:8Q16
v40@0:8Q16Q24@"NSURL"32
v32@0:8Q16@"NSURL"24
@"CSAudioStreamHolding"32@0:8@"NSString"16d24
v24@0:8@"CSAudioStreamHolding"16
@"CSAudioRecordDeviceInfo"16@0:8
v24@0:8@"<CSAudioAlertProvidingDelegate>"16
B32@0:8@"NSURL"16q24
v24@0:8@"<CSAudioSessionInfoProvidingDelegate>"16
v32@0:8@"CSAudioRecordContext"16@?<v@?@"NSDictionary"@"NSDictionary">24
v28@0:8B16@?<v@?@"NSError">20
B40@0:8@16Q24^@32
@"<CSAudioSessionProvidingDelegate>"
@"<CSAudioStreamProvidingDelegate>"
@"<CSAudioAlertProvidingDelegate>"
@"<CSXPCClientDelegate>"
v40@0:8q16q24q32
@"<CSStateMachineDelegate>"
v20@0:8I16
@56@0:8Q16@24@32@40@48
@36@0:8@16f24Q28
@32@0:8@16Q24
@52@0:8Q16@24@32f40Q44
@48@0:8Q16@24@32Q40
v40@0:8@16Q24@?32
v32@0:8^@16Q24
Q24@0:8@16
@48@0:8@16@24Q32Q40
v80@0:8@16{AudioStreamBasicDescription=dIIIIIIII}24Q64@?72
Q32@0:8@16@24
B32@0:8Q16@24
q24@0:8@16
@52@0:8@16Q24f32f36f40f44f48
@40@0:8Q16@24@32
@48@0:8@16Q24@32@40
v52@0:8@16Q24@32Q40I48
@"<CSAudioDecoderDelegate>"
@"<CSVoiceTriggerAssetChangeDelegate>"
v32@0:8r^v16q24
v36@0:8@16B24@28
v28@0:8@"CSFirstUnlockMonitor"16B24
d24@0:8@16
q48@0:8@16@24@32@40
q24@0:8Q16
@"<CSSmartSiriVolumeControllerDelegate>"
v32@0:8@"NSURLSession"16@"NSError"24
v40@0:8@"NSURLSession"16@"NSURLAuthenticationChallenge"24@?<v@?q@"NSURLCredential">32
v24@0:8@"NSURLSession"16
@40@0:8@16q24@32
v64@0:8@16@24@32B40Q44@52B60
@"<CSADCompanionServiceProvider>"
@"<CSVoiceTriggerAwareZeroFilterDelegate>"
@"CSAudioZeroFilter"
@20@0:8i16
@28@0:8@16i24
@"CSNovDetector"
@"<CSKeywordAnalyzerNDAPIScoreDelegate>"
@44@0:8Q16Q24f32f36f40
@56@0:8d16@24q32@40@48
@"<CSRemoteControlClientDelegate>"
v24@0:8@"NSCoder"16
@24@0:8@"NSCoder"16
@64@0:8@16@24@32@40@48@56
{AudioStreamBasicDescription=dIIIIIIII}24@0:8f16I20
@"NSDate"
@"<CSRemoteRecordClientDelegate>"
@"NSNumber"
v24@0:8@"<CSVTUIAudioSessionDelegate>"16
v24@0:8@"<Endpointer>"16
@"<CSVTUIAudioSessionDelegate>"
v24@0:8@"<NviDataReceiver>"16
@"NviContext"
{unique_ptr<BatchBeepCanceller, std::__1::default_delete<BatchBeepCanceller> >="__ptr_"{__compressed_pair<BatchBeepCanceller *, std::__1::default_delete<BatchBeepCanceller> >="__value_"^{BatchBeepCanceller}}}
{vector<short, std::__1::allocator<short> >="__begin_"^s"__end_"^s"__end_cap_"{__compressed_pair<short *, std::__1::allocator<short> >="__value_"^s}}
@"<CSBeepCancellerDelegate>"
v64@0:8Q16Q24@32@40@48@?56
v32@0:8@"NSString"16@?<v@?@"NSString"@"NSString"@"NSError">24
v64@0:8Q16Q24@"NSString"32@"NSArray"40@"NSArray"48@?<v@?@"CSVoiceTriggerRTModel"@"CSVoiceTriggerRTModel"@"NSError">56
v40@0:8@"NSArray"16@"NSString"24@?<v@?@"NSString"@"NSError">32
v48@0:8Q16@24@32@?40
@"CSActivationEvent"
v24@0:8@"CSLSWakeGesture"16
v32@0:8Q16@"NSDate"24
@"CSLSWakeGestureMonitor"
@36@0:8Q16S24d28
Q40@0:8@16Q24^@32
Q24@0:8^@16
{unique_ptr<CSAudioZeroFilterImpl<unsigned short>, std::__1::default_delete<CSAudioZeroFilterImpl<unsigned short> > >="__ptr_"{__compressed_pair<CSAudioZeroFilterImpl<unsigned short> *, std::__1::default_delete<CSAudioZeroFilterImpl<unsigned short> > >="__value_"^{CSAudioZeroFilterImpl<unsigned short>}}}
B32@0:8@16@?24
C16@0:8
Vv24@0:8@?16
Vv40@0:8@16q24@?32
Vv24@0:8@?<v@?@"NSString">16
Vv40@0:8@"NSArray"16q24@?<v@?@"NSError">32
Vv24@0:8@?<v@?Q>16
Vv24@0:8@?<v@?>16
Vv24@0:8@?<v@?B>16
Vv24@0:8@?<v@?q>16
@32@0:8Q16f24f28
v32@0:8r^v16Q24
v40@0:8r^v16Q24Q32
@24@0:8^Q16
{unique_ptr<corespeech::CSAudioCircularBufferImpl<unsigned short>, std::__1::default_delete<corespeech::CSAudioCircularBufferImpl<unsigned short> > >="__ptr_"{__compressed_pair<corespeech::CSAudioCircularBufferImpl<unsigned short> *, std::__1::default_delete<corespeech::CSAudioCircularBufferImpl<unsigned short> > >="__value_"^{CSAudioCircularBufferImpl<unsigned short>}}}
@28@0:8f16i20f24
{unique_ptr<corespeech::CSAudioCircularBufferImpl<unsigned char>, std::__1::default_delete<corespeech::CSAudioCircularBufferImpl<unsigned char> > >="__ptr_"{__compressed_pair<corespeech::CSAudioCircularBufferImpl<unsigned char> *, std::__1::default_delete<corespeech::CSAudioCircularBufferImpl<unsigned char> > >="__value_"^{CSAudioCircularBufferImpl<unsigned char>}}}
@"CSAudioStreamRequest"
@"CSAudioStartStreamOption"
@72@0:8q16q24d32@40d48@56q64
@64@0:8q16q24d32@40d48@56
v28@0:8@16i24
v36@0:8@16i24d28
v36@0:8@16i24@28
v28@0:8@"AVVoiceController"16B24
v36@0:8@"AVVoiceController"16B24@"NSError"28
v32@0:8@"AVVoiceController"16q24
v24@0:8@"AVVoiceController"16
v28@0:8@"AVVoiceController"16i24
v36@0:8@"AVVoiceController"16i24d28
v32@0:8@"AVVoiceController"16@"NSError"24
v36@0:8@"AVVoiceController"16i24@"NSError"28
v40@0:8@"AVVoiceController"16@"AVVCAlertInformation"24@"NSError"32
v32@0:8@"AVVoiceController"16@"NSDictionary"24
v32@0:8@"AVVoiceController"16@"AVVCAudioBuffer"24
v28@0:8B16@"NSArray"20
v44@0:8@"AVVoiceController"16Q24B32@"NSError"36
v40@0:8@"AVVoiceController"16Q24q32
v40@0:8@"AVVoiceController"16Q24@"AVVCAudioBuffer"32
v32@0:8@"AVVoiceController"16Q24
@"AVVoiceController"
v32@0:8@"MCProfileConnection"16@"NSDictionary"24
v40@0:8@"CSAudioPreprocessor"16@"NSData"24Q32
v48@0:8@16@24Q32@40
B32@0:8Q16q24
@"<CSAudioProviderDelegate>"
@"CSAudioCircularBuffer"
@"CSAudioPreprocessor"
@"CSOSTransaction"
@72@0:8@16Q24Q32Q40Q48Q56@64
@28@0:8f16Q20
@40@0:8Q16Q24Q32
v40@0:8Q16Q24@?32
I24@0:8Q16
Q20@0:8f16
d24@0:8Q16
Q40@0:8Q16Q24Q32
B36@0:8@16@24B32
B44@0:8@16@24Q32B40
B28@0:8@16B24
v32@0:8@"<CSEndpointAnalyzerImpl>"16d24
@"<CSEndpointAnalyzerImpl>"
v32@0:8r^s16i24i28
v28@0:8i16i20i24
v44@0:8B16@20@28@36
@48@0:8Q16Q24@32@40
@64@0:8Q16Q24@32@40@48@56
@"<CSSPGEndpointAnalyzerDelegate>"
@"NSSet"
@56@0:8@16Q24Q32Q40@48
@"<CSStartOfSpeechDetectorDelegate>"
@"<CSVoiceTriggerXPCClientDelegate>"
@36@0:8@16@24B32
v28@0:8r^s16i24
@"<CSKeywordAnalyzerQuasarScoreDelegate>"
v32@0:8@"<NviSignalProvider>"16@"NviSignalData"24
@44@0:8@16B24@28@36
v28@0:8@"CSOpportuneSpeakListener"16B24
@"CSOpportuneSpeakListener"
v36@0:8@16B24Q28
v24@0:8^{OpaqueAudioConverter=}16
@"<CSAudioConverterDelegate>"
v48@0:8Q16Q24Q32@?40
@24@0:8@"NSString"16
@"NSDictionary"24@0:8@"NviContext"16
f24@0:8@16
@"CSSpeakerModel"
@"<CSSpeakerDetectorNDAPIDelegate>"
Q24@0:8I16I20
@"CSKeywordAnalyzerNDAPI"
v60@0:8@16Q24@32@40Q48I56
v60@0:8@"CSAudioDecoder"16Q24@"NSData"32@"NSData"40Q48I56
v40@0:8@"CSAudioFileReader"16@"NSData"24Q32
v36@0:8@"CSAudioFileReader"16B24@"NSError"28
v32@0:8@"CSAudioFileReader"16q24
Q32@0:8@16^@24
B40@0:8Q16Q24^@32
v48@0:8@16Q24@32Q40
@28@0:8@16I24
v36@0:8B16Q20@28
v32@0:8q16Q24
{AudioBufferList="mNumberBuffers"I"mBuffers"[1{AudioBuffer="mNumberChannels"I"mDataByteSize"I"mData"^v}]}
^{AudioBufferList=I[1{AudioBuffer=II^v}]}
@"CSRemoteRecordClient"
@"CSAudioFileReader"
@32@0:8Q16^@24
@44@0:8Q16@24@32B40
@"NSObject<OS_os_transaction>"
