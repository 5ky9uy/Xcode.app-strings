%s ::: Error creating output file %{public}@, err: %{public}d
%s ::: Error writing to output wave file. : %{public}ld
%s metaInfo passed is nil - Bailing out
%s Unable to read data from file: %@
%s Could not read existing %@ file: err: %@
%s ERR: Failed to create json %{public}@ with %{public}@
%s saveAudioChunk toURL: %{public}@
%s Invalid request: nothing to write to file
%s Setting mixable to yes as we are in an active call
%s SpkrId:: VAD not supported!!!
%s Plan removing the temp file %{public}@
%s Failed to remove temp file %{public}@ reason: %{public}@
%s Start copying %{public}u bytes of data to crashreporter with wav file header offset %{public}llu
%s Failed to read data from %{public}@
%s Finished copying data to crashreporter.
%s Logging audio file into : %{public}@
%s ERR: reading contents of gradingDir: %{public}@ with error %{public}@
%s Deleting orphaned grading file %{public}@
%s ERR: Failed to delete %{public}@ with error %{public}@
%s Removing non-dir at AttentiveSiri AudioLog dir: %@
%s Error removing %@: err: %@
%s Failed to create AudioLogging directory for AttentiveSiri: %@
%s Created AudioLogging dir for AttentiveSiri at: %@
%s CS logging files under %{public}@ created before %{public}@ will be removed.
%s Couldn't get creation date: %{public}@
%s Could not remove %{public}@: %{public}@
%s CS logging files number %{public}lu with pattern %{public}@ under %{public}@ exceeding limit, only keep the latest %{public}lu ones
%s Regular expression is nil!
%s SpkrId:: Failed to create SSRSpeakerRecognizerPSR
%s SpkrId:: %@::uniqueUttTag: %@, extraSamplesAtStart: %lu, _tdEndInSampleCount: %lu(%f ms)
%s SpkrId:: CSSpIdVTSpeakerRecognizer dealloc
%s SpkrId:: Discarded ScoreCard for mismatch session - {%{public}@, %{public}@}
%s SSROrch[%{public}@]:: Failed to create PSR Recognizer
%s SSROrch[%{public}@]:: Failed to create SAT Recognizer
%s SSROrch[%{public}@]:: Successfully initialized with {%{public}@, %{public}@}
%s SpkrId:: Released OS transaction for %{public}@
%s SSROrch[%{public}@]:: Ignoring addAudio, endAudio: %d procSamples: %lu maxProcSamples: %lu
%s SSROrch[%{public}@]:: Released OS transaction for %{public}@
%s SSROrch[%{public}@]:: Creating OS transaction for %{public}@
%s SSROrch[%{public}@]:: Scorecard %{public}@ with delay:%{public}ldms, processed:%{public}ldms, await:%{public}ldms
%s ERR: Posting diagnostic report for abnormal score delay - %ldms
%s SSROrch[%{public}@]:: Sync score report with %{public}f delay - with known user scores %{public}@
%s SSROrch[%{public}@]:: ERR: VoiceInfo is nil from recognizer %{public}@
%s SSROrch[%{public}@]:: Discarded speaker scores for session - %{public}@
%s SSROrch[%{public}@]:: EndAudioCalled is false, returning for recognizer %{public}@
%s SSROrch[%{public}@]:: PSR Analyzer finished the session with %{public}@
%s SSROrch[%{public}@]:: SAT Analyzer finished the session with %{public}@
%s SSROrch[%{public}@]:: Wait for %{public}@ analyzer to complete the session - %{public}@
%s SSROrch[%{public}@]:: Finished the session with known user scores %{public}@
%s SSROrch[%{public}@]:: Discarded speaker scores for session
%s SSROrch[%{public}@]:: Speech started at - %ldms
%s SSROrch[%{public}@]:: Starting a new segment of speech - %ldms
%s CSVoiceTriggerAsset found: %{public}@
%s Locale: [%{public}@]
%s No locale set when creating phrase spotter.
%s Creation of Keyword Detector failed.
%s %s async called
%s %{public}s async called
%s Waiting for didStop
%s Done waiting for didStop
%s Called before completion called
%s %{public}s Called
%s BEGIN num:%{public}ld use:%{public}d
%s AudioSession setup failed
%s Has wrong audio routing, ask user to unplug headset
%s Start Audio Session failed
%s _sessionNumber [%{public}ld]
%s %{public}s Canceling Training
%s Called with status : %{public}d
%s %{public}s called
%s AudioSession StartRecording Failed
%s Setting suspendAudio:[%{public}d]
%s Resume training
%s Suspend training
%s Stop Listening
%s ERR: Failed to save explicit utterance
%s audioSessionDidStopRecording
%s ERR: SpeakerRecognition is not available on this platform
%s Creating audioRecorder has failed
%s AudioRecorder creation failed : %@
%s AudioStream Handle ID is invalid : %{public}@
%s audioRecorder %p created
%s Cannot prepare since audio recorder does not exist
%s Failed to activate audio session, error : %{public}@
%s AudioRecorder is already recording, do not prepare anymore
%s Failed to prepareAudioStreamRecord : %{public}@
%s Failed to startAudioStream : %{public}@
%s failed to stopRecording : %{public}@
%s audioInput:[%@]
%s audioOutput:[%@]
%s ERR: voiceProfile is nil - Bailing out
%s ERR: Failed to create TriggerPhraseDetector in %{public}@ with %{public}@
%s ERR: Failed in trigger processing %{public}@ with %{public}@
%s Trigger Score %{public}f not satisfied implicit VT threshold %f
%s Using recognizer scale factor: %f for phrase detector
%s ERR: Failed to create trigger phrase detectors
%s Processing %{public}@ for trigger word detection
%s ERR: Failed to read file: %@
%s ERR: Failed processing %{public}@ with error %{public}@
%s ERR: %{public}@
%s Best trigger score for %{public}@ is %{public}f (%{public}f, %{public}f)
%s SpkrId:: Unknown CSSpIdType string: %@
%s ERR: Unknown CSSpIdType: %lu
%s Unknown CSSpIdType: %lu
%s SpkrId:: path is nil - Bailing out
%s SpkrId:: Direntry with same name exists, this will be removed: %@
%s SpkrId:: Creating Directory : %@
%s SpkrId:: Creating Directory failed : %@
%s SpkrId:: Exceeded privacy limit for grading utterances : %ld (%d)
%s SpkrId:: VoiceId not supported in language %{public}@
%s SpkrId:: ERR: filePath passed as nil - Bailing out
%s SpkrId:: ERR: Could not read existing %@ file: err: %@
%s SpkrId:: ERR: %@ is a directory
%s SpkrId:: ERR: file do not exist - %@
%s Unknown Device category for deviceProduceType: %@
%s ERR: Unknown device. returning false: %{public}@
%s ERR: satLanguagePath is nil. returning false
%s Voice Profile Mismatch - CurrentDeviceCategory %@ VoiceProfileCategory %@
%s Malformed audio-dir URL for string <%{public}@>:url
%s ERR: reading contents of audioDir: %{public}@
%s No jsonFiles found in %{public}@: jsonFiles.count=%{public}lu
%s Unexpected: empty JSON data for file: %{public}@
%s Error reading metaDict at path: %{public}@
%s metaProductType: %{public}@
%s vtprofile: currDevice=[%{public}@:%{public}@] ; vpDirDevice=[%{public}@:%{public}@]
%s VoiceProfile MATCH
%s VoiceProfile MIS-MATCH
%s Could not find productType in VT-Meta file, trying next one
%s No compatible VT profile found for CurrDevice: %{public}@
%s ERR: fetching contents of %{public}@ failed with error %{public}@
%s ERR: Directory is nil - Bailing out
%s ERR: %{public}@ doesnt exist - Bailing out
%s Dump content of %{public}@ - %{public}@
%s Error reading directory at %@: err: %@
%s %@ is empty
%s Fetching homeUserId for siriProfileId %{public}@
%s siriProfileId %{public}@ maps to homeUserId %{public}@
%s ERR: Home User Id erred %{public}@ for Siri Profile Id %{private}@
%s ERR: Seeing more than one voice profiles for Siri App Domain
%s SpkrId:: Error creating uttMetaJsonData: %@
%s SpkrId:: Failed to create UttMeta...
%s Setting payloadstartSample %lu for trigger duration of %fsecs
%s ERR: Setting max payloadstartSample %lu for trigger duration of %fsecs
%s Failed to read file: %@
%s %@
%s Deleted file %{public}@
%s EOF: utteranceLength: %lums, tdlength: %lums tdtiLength: %lums tdtiDiscardedLength: %lums
%s satAudioDirectory is nil - Bailing out
%s metaVersionFile %{public}@ doesnt exist
%s Found %{public}d ambiguous explicit utterances
%s metaVersionFile %{public}@ doesnt exist - Skipping
%s ERR: Fetching contents of %{public}@ failed with error - %{public}@
%s ERR: Scores for profileId %{public}@ not present in %{public}@ - Skipping
%s Siri language is nil, falling back to %@
%s endpointId not provided, fallback to legacy query
%s Failed to query language code with endpointId %@, trying legacy query
%s Cannot create CSVTUIKeywordDetector since there is no asset available
%s Cannot create CSVTUIKeywordDetector since we cannot initialize NDAPI
%s Sending Analytics Event - %{public}@
%s Processing onboarded Siri user: %{public}@
%s Detected matching %{public}d users: %{public}@
%s Valid profile not found %{public}@ and %{public}@ - defaulting to %{public}@
%s Adding invalid user for deletion - %{public}@
%s Skipping retaining user %{public}@
%s Detected invalid user: %{public}@
%s File path doesnt exist - %{public}@
%s ERR: Failed reading contents of SAT root %{public}@ with %{public}@
%s App domains in use - %{public}@
%s ERR: Failed determining if file is dir-entry url=%{public}@ with %{public}@
%s Deleting invalid file %{public}@
%s Deleting invalid domain %{public}@ not part of domains %{public}@
%s Processing domain - %{public}@
%s ERR: Failed reading AppDomain %{public}@ at %{public}@ with %{public}@
%s Processing locale - %{public}@
%s Deleting invalid locale %{public}@ not supported in set %{public}@ and current language %{public}@
%s Processing profile - %{public}@
%s Deleting invalid profile %{public}@
%s Removing Implicit utterance cache directory at %{public}@
%s Processing profile %{public}@ with version %{public}d and identity %{public}@
%s Found legacy voice profile - Skipping
%s Deleting invalid SAT entry: %{public}@
%s ERR: Failed to get atrributes of file %{public}@, err %{public}@, size %{public}llu
%s Deleting invalid SAT entry: %{public}@ %{public}@
%s Found non-meta file: %{public}@
%s Deleting invalid SAT entry: %{public}@ : <%{public}@>
%s Processed %{public}@ with %{public}d explicit and %{public}d implicit utterances
%s ObsoleteCutOffDate is nil - Bailing out
%s Checking payload utterances prior to %{public}@ for profile %{public}@ and modelType %d
%s Deleting lifetimeexpired SAT entry %{public}@
%s Deleted lifetimeexpired metafile %{public}@
%s Deleting model file %{public}@ with err %{public}@
%s SpkrId:: ERR: missing arguments to create voice profile - Bailing out
%s Importing %{public}@ of %{public}@ from import Dir %{public}@
%s Successfully imported %{publice}@ %{public}lu(%{public}lu) utterances to profile %{public}@
%s Failed in importing %{public}@ of %{public}@ from import Dir %{public}@
%s Copied %{public}@ to %{public}@ with error %{public}@
%s Copied TD audio files %{public}lu to TDTI which now has %{public}lu(%{public}lu) utterances
%s Error to copy utterance from %{public}@ to %{public}@, error: %{public}@
%s Copied Utterance from %{public}@ to %{public}@
%s Error to copy jsonFile from %{public}@ to %{public}@, error: %{public}@
%s Successfully copied %{public}lu(%{public}lu) utterances to profile %{public}@
%s ERR: date is nil - Bailing out
%s Filtered %@ with enrolled date %@
%s Couldn't delete invalidated speaker model at path %{public}@ %{public}@
%s ERR: Removing %{public}@ as explicit utterances %{public}d from audio dir - %{public}@
%s SAT path doesnt exist - %@
%s Coudn't mark SAT enrollment %{public}@ at path %{public}@
%s Marked SAT enrollment %{public}@ at path %{public}@
%s We can't mark SAT %{public}@ when there is no audio directory
%s ERR: Unknown device-category for device: %{public}@
%s ERR: error creating updatedVoiceProfileJsonData from: %@, err: %@
%s ERR: error removing voice profile version file at: %@, err: %@
%s ERR: Error writing voice profile version file at: %@, err:%@
%s ERR: Profile dict is nil - Bailing out
%s ERR: error updating updatedVoiceProfileJsonData from: %@, err: %@
%s %{public}@
%s Skipping SAT Model {%{public}@, %{public}@} for %{public}@
%s Skipping model {%{public}@, %{public}@} for %{public}@
%s Added model context {%{public}@, %{public}@} for %{public}@
%s ERR: uttPath is nil -  Bailing out
%s ERR: uttPath is nil - Bailing out
%s ERR: uttMeta is nil - Bailing out
%s ::: Error creating json Metadata: %{public}@
%s ERR: uttMetaURL is nil, defaulting to NO
%s ERR: metaData is nil, defaulting to NO for %{public}@
%s ERR: read metafile %{public}@ failed with %{public}@ - defaulting to NO
%s ERR: uttMetaURL is nil - Bailing out
%s ERR: metaData is nil for %{public}@ - Bailing out
%s ERR: read metafile %{public}@ failed with %{public}@ - Bailing out
%s ERR: metaDict from file %{public}@ isnt a dictionary - %{public}@
%s ERR: %{public}@ is not present
%s ERR: Unexpected. metaVersionFileData is empty while the file exists at: %{public}@
%s Json-Err reading metaVersionFile: %{public}@: err: %{public}@
%s ERR: filePath is nil!
%s Testing [%@] against regex.
%s Fired VoiceTrigger Timeout
%s Stop right now since ASR has issue
%s EOS Timeout Fired
%s Force endpoint fired
%s Discarding surplus audio of %{public}lu samples, audio sample available %{public}lu (%{public}lu, %{public}lu, %{public}lu)
%s AudioSession Started
%s AudioSession Stopped
%s Begin of speech detected
%s End of speech detected with endpoint type: %{public}ld
%s unknown endpoint type
%s Called with status : %{public}d and success : %{public}d utteranceStored : %{public}d
%s ERR: Failed to store utterance, overiding status
%s Non Final: [%{public}@]
%s NON Final Matching
%s Final: [%{public}@]
%s Final Matching
%s Final Not Matching
%s SPEECH RECOGNITION TASK FINISH UNSUCCESSFULLY
%s %{public}@ %{public}@ %{public}ld %{public}@
%s Recog Result: [%{public}ld]
%s returning status to UI : %{public}d
%s Already reported status or no callback
%s Will suspend training
%s Will resume training
%s Decide to delay ending ASR: [%{public}ld] samples
%s Triggered! Event info: %{public}@
%{public}9lld %{public}9lld %{public}9lld
%s analyzing.... score so far: %{public}5.3f (%{public}ld)
%s feeding tailing: [%{public}ld] samples
%s correctSampleSize:    [%{public}ld]
%s accumSampleSize:      [%{public}ld]
%s startBufferIndex:     [%{public}ld]
%s startBufferSampleSize:[%{public}ld]
%s samplesToBeDeleted:   [%{public}ld]
%s Total Number of buffs:[%{public}ld]
%s Adjusting the start buffer
%s Adjusting the array elements
%s Unsupported
%s %{public}s CALLED
%s Master Timeout Fired
%s recognized text = %{public}@
%s ERR: Profile not available for %{public}@ & %{public}@ - Bailing out
%s ERR: No configured Siri Profiles
%s ERR: More than one Siri Voice Profiles - %{public}@
%s ERR: Failed to add profile into the store with error %{public}@
%s ERR: Finished implicit training for %@ with error %{public}@
%s Finished implicit training for %@
%s Received implicit utterance for %{public}@ from %{public}@ with context %d
%s ERR: FilePath is nil - Bailing out
%s ERR: Training utterance doesnt exist at %@ - Bailing out
%s ERR: Training utterance is marked as directory at %@ - Bailing out
%s VoiceTriggerEventInfo is nil - Bailing out
%s kVTEILanguageCode is nil - Bailing out
%s ERR: trigger score not found in VTEI - Bailing out
%s ERR: SAT did not trigger!!! - Bailing out
%s Implicit training not enabled for %{public}@
%s sharedSiriId is nil - Bailing out
%s Rejecting barge-in utterance from adding to voice profile
%s Privacy disallowed implicit utterance %{public}@ - skipping
%s ERR: Failed to segment %{public}@ with %{public}@ - Bailing out
%s Processed implicit utterance %{public}@ successfully
%s Voice Profile is full - Ignoring
%s Implicit Policy not satisfied - Ignoring
%s ERR: Failed to process implicit utterance %{public}@ with error %{public}@
%s Enrolling voice profile of %@ 
%s Failed enrolling %@ with error %@
%s Failed to get device hash list %{public}@
%s Processing sync data from device hash: %{public}@
%s Skipping language [%{public}@] since we already have enrollment data for this language
%s Skipping language [%{public}@] as file path doesnt exist - %{public}@
%s Skipping language [%{public}@] as voice profile not compatible
%s Error to copy profile from %{public}@ to %{public}@, error: %{public}@
%s Adding voiceprofile for %{public}@ in language %{public}@ completed with error %{public}@
%s ERR: Failed migrating Voice profile for language %{public}@ with error %{public}@
%s Successfully added %{public}@ in %.2fms
%s language: %{public}@, enableVTAfterSyncLanguage: %{public}@, currSiriLanguage: %{public}@
%s Enabling VoiceTrigger Upon VoiceProfile sync for language: %{public}@ and enrolled language: %{public}@
%s VoiceTrigger does not exist for this platform, not setting VoiceTriggerEnabled
%s Not enabling VoiceTrigger Upon VoiceProfile sync for language: %{public}@
%s Sucessfully migrated language %{public}@
%s Migrated language %{public}@ but failed to mark SAT enrollment
%s Sucessfully marked as migrated for language : %{public}@
%s Failed to mark migrated for language : %{public}@
%s Failed to remove update path [%{public}@] upon migration completion, error: %{public}@
%s Failed to download voice profile with error %{public}@ and category %{public}@ 
%s Successfully enrolled voice profile %@ with %{public}@ profile
%s Triggering profile sync check
%s Skipped enrolling voice profile %@ with %{public}@ profile
%s ERR: Failed in enrolling Voice profile %@ with category %{public}@ profile
%s Failed to enroll siriProfileId %@ with %{public}@
%s Language %{public}@ not supported in %{public}@ - Skipping
%s Skipping profile download for %{public}@ - %{public}@ not matching current %{public}@
%s Skipping profile download for %{public}@ - voiceId not supported in %{public}@
%s ERR: %@
%s ERR: Failed migrating Voice profile with ID %{public}@ for language %{public}@ with error %{public}@
%s Sucessfully enrolled %{public}@ for language %{public}@
%s PHS update directory already exists, remove before we move forward
%s Failed to delete PHS update directory
%s Failed to create PHS update directory
%s Upload of Voice Profile for %@ completed with error %{public}@
%s Upload of Voice Profile for %@ completed successfully
%s ERR: Failed to delete existing SATUpload diretory : %{public}@
%s Upload trigger of voice profile of %@ 
%s Upload not supported on %{public}@
%s Legacy upload API called on Horseman - Bailing out
%s Cannot delete existing SATUpload Diretory : %{public}@
%s Cannot create SAT Upload Directory : %{public}@
%s Cannot create %{public}@ with error %{public}@ - Skipping language
%s Cannot create directory(%{public}@)
%s Cannot copy file: %{public}@ to %{public}@
%s ERR: siriProfileId is nil - Bailing out
%s Unsupported languagecode %{public}@ in %{public}@ - Skipping
%s Skipping uploading %{public}@ legacy version (%lu) of voice profile, current version %lu
%s Skipping uploading %{public}@ voice profile for profileId %@
%s Skipping audiocache file %@
%s Cannot copy file %{public}@ to %{public}@ with error %{public}@
%s ERR: Failed to copy from %{public}@ to %{public}@ with error %{public}@
%s ERR: Cannot copy file %{public}@ to %{public}@ with error %{public}@
%s Successfully copied {%{public}d,%{public}d} utterances from %@ to %@
%s Cannot copy voice profile from %@ to %@ with error %{public}@
%s Triggering upload of voice profile %@
%s Upload of voice profile at %@ with category %{public}@ completed successfully
%s Triggering upload of explicit voice profile %@
%s Upload of explicit voice profile at %@ completed successfully
%s Setting VoiceProfile Training Sync for language: %{public}@
%s ERR: %{public}s: Bailing out as language is nil!
%s VoiceProfile training sync language: %{public}@, VoiceProfile language: %{public}@
%s ERR: metaBlob is nil. Returning false, language: %{public}@
%s ERR: Unknown device. Returning false, language: %{public}@
%s ERR: Unknown device-category for device: %{public}@, languageCode: %{public}@
%s ERR: Failed to deserialize metaBlob with error %{public}@
%s Looking VoiceProfile for CurrDevice: %{public}@{%{public}@} in metablob %{public}@
%s currDevice=[%{public}@ : {%{public}@}] ; syncedDevice=[%{public}@ : {%{public}@}]
%s CurrDevice: [%{public}@ : {%{public}@}] DOES NOT have VoiceProfile synced in iCloud
%s Triggering VoiceProfile upload for %{public}@
%s Querying VoiceProfile upload state on %{public}@
%s ERR: Fetching cached devices resulted in error %{public}@
%s Looking VoiceProfile for CurrDevice: %{public}@{%{public}@} in devices %{public}@
%s VoiceProfile available locally for %{public}@, not uploaded yet
%s Searching for synced-VoiceProfile for CurrDevice: %{public}@{%{public}@}
%s Will Enable VoiceTrigger after VoiceProfile sync for language: %{public}@
languageCode: %{public}@
%s Devices with VoiceProfile in iCloud for language: %{public}@:%{public}@
%s Timedout waiting for AFSettingsConnection:getDevicesWithAvailablePHSAssetsForLanguage: %{public}ld, waitedFor: %{public}d, Returning nil
%s timeToRet(AFSettingsConnection:getDevicesWithAvailablePHSAssetsForLanguage:): %{public}fms
%s voiceProfileArray is nil for %{public}@ and %{public}@!
%s voiceProfileArray is nil!
%s Profiles already migrated, check for enrollment on %{public}@ on profile %{public}@
%s ERR: Failed to delete Voice Profile %{public}@ with error %{public}@
%s Couldn't delete SAT directory at path %@ %@
%s Couldn't delete SAT cache directory at path %@ %@
%s No Audio file exists when enrollment marker is set, remove marker
%s Contents of audio dir - %{public}@
%s Language Code is nil!
%s Not supported on this platform
%s default to recordContext : %{public}@
%s Failed to create AVVC : %{public}@
%s Create new CSAudioRecorder = %{public}p
%s CSAudioRecorder %{public}p deallocated
%s AVVC initialization failed
%s Successfully create AVVC : %{public}p
%s Calling AVVC setContext : %@
%s Failed to get handle id : %{public}@
%s setContext elapsed time = %{public}lf
%s Calling AVVC setContextForStream : %{public}@
%s setCurrentContext elapsed time = %{public}lf
%s Calling AVVC prepareRecordForStream(%{public}llu) : %{public}@
%s AVVC prepareRecordForStream failed : %{public}@
%s prepareRecordForStream elapsed time = %{public}lf
%s Calling AVVC startRecordForStream : %{public}@
%s startRecordForStream failed : %{public}@
%s startRecordForStream elapsed time = %{public}lf
%s Calling AVVC stopRecordForStream
%s Failed to stopRecordForStream : %{public}@
%s stopRecordForStream elapsed time = %{public}lf
%s Calling AVVC setSessionActivate for activation
%s AVVC setSessionActivate has failed : %{public}@
%s AVVC successfully activated audioSession
%s setSessionActivate elapsed time = %{public}lf
%s Calling AVVC setSessionActivate for deactivation : %{public}tu
%s Cannot handle audio buffer : unexpected format(%{public}u)
%s Compensating %{public}u channel(s), heartbeat = %{public}lld
%s Received didStartRecording : %{public}p, forStream:%{public}llu, successfully:%{public}d, error:%{public}@
%s Received audio buffer %{public}d, heartbeat = %{public}llu, streamID (%{public}lu)
%s Received didStopRecording : %{public}p forStream:%{public}llu forReason:%{public}ld
%s toConfiguration : %{public}d
%s withContext : %{public}@
%s activate : %{public}d
%s AVVC lost mediaserverd connection
%s AVVC informed mediaserverd reset, no further action required
%s Failed to deinterleave the data: %{public}d
%s Cannot create de-interleaver using AudioConverterNew: %{public}d
%s Created de-interleaver
%s ERR: Failed to create asset providers - Bailing out
%s parsing provider: %@ name: %@
%s ERR: got nil assets from provider: %@
%s Got asset with version: %@ from provider: %@
%s ERR: Asset not available from provider: %@
%s SpkrId:: Scorecard for {%{public}d, %{public}.2fsec %dms} - %{public}@
%s SpkrId:: %@
%s SpkrId:: Discarded speaker scores for session - %{public}@
%s SpkrId:: Final - %@
%s ::: found %{public}lu installed assets for matching query: %{public}@
%s ERR: Failed to asset for %{public}@
%s Error running query: %{public}@, error: %{public}lu
%s Failed to get assetString for assetType %{public}d
%s ::: found %{public}lu assets matching query: %{public}@
%s Error running asset-query: %{public}@, error: %{public}lu
%s Asset state : %{public}ld
%s Chosen Asset %{public}@
%s SpkrId:: ERR: appDomain passed as nil
%s SpkrId:: ERR: locale passed as nil
%s Profiles already migrated - Bailing out
%s Migration of voice profile is triggered...
%s Sat directory doesnt exist %@
%s Language %{public}@ not supported in %{public}@ - Deleting
%s Migrating voice profiles in languages - %{public}@
%s Voice profile migration for language - %{public}@
%s Skipped migrating non-siri landed profile - %{public}@, %{public}@
%s voice profile created is nil!!! - Skipping %{public}@
%s Moving contents from %{public}@ to %{public}@ failed with error %{public}@
%s Completed migrating voiceprofile for %{public}@ in language %{public}@
%s Triggered cleanup duplicated profiles
%s ERR: Deleted duplicated voiceprofile(%lu): %{public}@
%s Found %{public}d duplicated profiles
%s Triggering voice profiles download
%s ERR: Failed retraining LiveOn onboarded users with error %{public}@
%s Successfully retrained LiveOn onboarded users
%s Cleanup model files with assets %{public}@
%s Synchronize voiceprofiles with Assistant...
%s ERR: Deleted stale voiceprofile(%lu): %@
%s Missing user models - Triggering voice profiles download
%s Needs retraining - Triggering voice profiles download
%s ERR: Failed in adding %{public}@ to %{public}@ with error %{public}@
%s Added Implicit SAT vector from %{public}@ to profile %{public}@
%s Locale is nil - Bail out
%s assetForLocale is nil - Bail out
%s ERR: Failed purging profile %{public}@ with error - %{public}@
%s ERR: Failed to get top scorer in %{public}@ - Bailing out
%s ERR: Utterance scored %{public}f (%{public}@) with next top score %{public}f (%{public}@) for profileId %{public}@
%s Utterance scored %{public}f (%{public}@) with next top score %{public}f (%{public}@) and thresholds (%{public}f, %{public}f)
%s Utterance scored %{public}f for %{public}@ and thresholds (%{public}f, %{public}f)
%s Skipping retraining for language %{public}@, current %{public}@
%s Detected mean pitch for explicit utterances = %{public}f Hz
%s Deleting VoiceProfile %{public}@
%s Err: %@
%s Needs Retraining Profile %{public}@ - existing {%{public}@} downloaded {%{public}@, %{public}d}
%s Needs Retraining storage for Profile %{public}@ - existing {%{public}@} downloaded {%{public}@, %{public}d}
%s Skipping Profile %{public}@ - existing {%{public}@} downloaded {%{public}@, %{public}d}
%s Needs Retraining %{public}@ model update for profile %{public}@ 
%s Retraining %{public}@ for locale %{public}@
%s Needs retraining %{public}@ - Triggering voice profiles download
%s Retraining successfully finished for %{public}@ in %{public}fms
%s Adding User Voice Profile %@
%s Updating User Voice Profile to %@ from %@
%s Deleting User Voice Profile %@
%s User Voice Profile not found %@ - Bailing out
%s ERR: UserVoiceProfile Action undefined %ld - Bailing out
%s Updating profile %{public}@ with userName %{public}@
%s Retraining for locale %{public}@ with force %d
%s ERR: Retraining failed for %{public}@ with error %{public}@ in %{public}fms
%s Retraining finished for %@ with error %@ in %fms
%s Creating OS Transaction %p for %{public}@
%s No Implicit audio - ignoring filterToVoiceTriggerUtterances
%s ERR: ignoring filtering option as %{public}@ or %{public}@ is nil
%s ERR: ignoring filtering option as VTAssets not found on %{public}@
%s ERR: Failed training %{public}@ of %{public}@ with error %{public}@
%s Failed to create %{public}@ model with error %{public}@ for profile %{public}@
%s Releasing OS Transaction %{public}@
%s Skipping retraining for %{public}@ - for {%{public}@, %{public}@}
%s ERR: Failed resetting for %{public}@ - for {%{public}@, %{public}@}
%s ERR: Failed in retraining %{public}@ with error %{public}@
%s ERR: Failed to delete the model at %{public}@
%s Added utterance %{public}@ to {%{public}@, %{public}@, %{public}@, %{public}@} with score %{public}@
%s Rejected utterance %{public}@ with error %{public}@
%s Saving utterance and meta as %{public}@ training.
%s Failed to write utterance into %{public}@
%s Saving %{public}@ at %{public}@ as %{public}@ training.
%s numSamplesToWrite %{public}lu
%s Failed to get CSAudioFileWriter:
%s Failed to addSamples to CSAudioFileWriter: %{public}@
%s Failed to endAudio on CSAudioFileWriter: %{public}@
%s ERR: called with nil metaPath
%s ERR: called with nil uttPath
%s ERR: called with nil uttMeta
%s Cannot create json file : %{public}@
%s Skipping Model {%{public}@, %{public}@} for %{public}@
%s Skipping Model {%{public}@, %{public}@} as file doesnt exist at %{public}@
%s ERR: triggering profile retrain for asset %{publiic}@
-[CSPlainAudioFileWriter initWithURL:inputFormat:outputFormat:]
-[CSPlainAudioFileWriter addSamples:numSamples:]
json
en_US_POSIX
yyyy_MM_dd-HHmmss.SSS
productType
productVersion
buildVersion
timeStamp
-[CSPlainAudioFileWriter addContextKey:withContext:]
-SL.json
-[CSPlainAudioFileWriter createAcousticMetaFileForContext:withContext:]
-[CSPlainAudioFileWriter addContextKey:fromMetaFile:]
+[CSPlainAudioFileWriter saveAudioChunck:toURL:]
-[CSAudioRecordContext(AVVC) avvcContextSettings]
-[SSRVoiceActivityDetector initWithContext:delegate:]
OPP-
PCM-
OPUS_
Ads-
PHS-
-synced
v8@?0
com.apple.CoreSpeech.AudioLogging
+[CSAudioFileManager generateDeviceAudioLogging:speechId:]_block_invoke
FLLR
+[CSAudioFileManager _readDataFromFileHandle:toFileHandle:]
%@%@.wav
%@/%@%@.wav
+[CSAudioFileManager _createAudioFileWriterForAdBlockerWithLoggingDir:inputFormat:outputFormat:]
+[CSAudioFileManager _createAudioFileWriterForOpportuneSpeakListenerWithLoggingDir:inputFormat:outputFormat:]
+[CSAudioFileManager _createAudioFileWriterWithLoggingDir:withLoggingUUID:inputFormat:outputFormat:]
+[CSAudioFileManager _createAudioFileWriterForPHSTrainingWithLoggingDir:inputFormat:outputFormat:]
^%@*
%@(%@)?.wav$
+[CSAudioFileManager cleanupOrphanedGradingFiles]
+[CSAudioFileManager cleanupOrphanedGradingFiles]_block_invoke
v32@?0@"NSString"8@"NSURL"16^B24
attsiri
+[CSAudioFileManager audioFileWriterForAttentiveSiri]
%@.wav
Dispose Log Queue
+[CSUtils(Directory) removeLogFilesInDirectory:matchingPattern:beforeDays:]_block_invoke
v24@?0@"NSArray"8^@16
+[CSUtils(Directory) clearLogFilesInDirectory:matchingPattern:exceedNumber:]_block_invoke_2
+[CSUtils(Directory) clearLogFilesInDirectory:matchingPattern:exceedNumber:]_block_invoke
Unable to get %@ for file at %@: %@
q24@?0@"NSURL"8@"NSURL"16
B24@?0@"NSURL"8@"NSDictionary"16
+[CSUtils(Directory) _contentsOfDirectoryAtURL:matchingPattern:includingPropertiesForKeys:error:]
SSRSpeakerRecognizerPSR.m
Incorrect ctx for VTSpeakerRecognizer: %@
com.apple.ssr.psrq
-[SSRSpeakerRecognizerPSR initWithContext:delegate:]
extraSamplesAtStart
triggerEndSeconds
-[SSRSpeakerRecognizerPSR dealloc]
-[SSRSpeakerRecognizerPSR voiceRecognitionPSRAnalyzer:hasVoiceRecognitionInfo:]
extraAudioAtStartInMs
tdEndInMs
-[SSRSpeakerRecognizerPSR voiceRecognitionPSRAnalyzerFinishedProcessing:withVoiceRecognitionInfo:]
com.apple.speakerrecognition
recognition
-[SSRSpeakerRecognitionOrchestrator initWithContext:withDelegate:error:]
ERR: Failed to init PSR and SAT recognizers - Bailing out
reason
ERR: Failed to init VAD - Bailing out
com.apple.ssr.orchestratorq
SAT+PSR
-[SSRSpeakerRecognitionOrchestrator dealloc]
-[SSRSpeakerRecognitionOrchestrator processAudio:numSamples:]_block_invoke
-[SSRSpeakerRecognitionOrchestrator resetWithContext:]_block_invoke
SSRSpeakerRecognitionOrchestrator-%@
-[SSRSpeakerRecognitionOrchestrator _logSpeakerIdProcessorScoreDelayWithScoreInfo:hasFinished:]
finished
reported
%@_%d
-[SSRSpeakerRecognitionOrchestrator getLatestVoiceRecognitionInfo]
-[SSRSpeakerRecognitionOrchestrator speakerRecognizer:hasSpeakerIdInfo:]_block_invoke
-[SSRSpeakerRecognitionOrchestrator speakerRecognizerFinishedProcessing:withFinalSpeakerIdInfo:]_block_invoke
-[SSRSpeakerRecognitionOrchestrator SSRVoiceActivityDetector:didDetectStartPointAt:]_block_invoke
-[SSRSpeakerRecognitionOrchestrator SSRVoiceActivityDetector:didDetectEndPointAt:]_block_invoke
com.apple.VoiceTriggerUI.TrainingSessionQueue
com.apple.VoiceTriggerUI.TrainingManager
-[SSRVTUITrainingManager setLocaleIdentifier:]
-[SSRVTUITrainingManager createKeywordDetector]
-[SSRVTUITrainingManager prepareWithCompletion:]_block_invoke
-[SSRVTUITrainingManager cleanupWithCompletion:]
-[SSRVTUITrainingManager cleanupWithCompletion:]_block_invoke
-[SSRVTUITrainingManager trainUtterance:shouldUseASR:completion:]
-[SSRVTUITrainingManager trainUtterance:shouldUseASR:completion:]_block_invoke_2
-[SSRVTUITrainingManager trainUtterance:shouldUseASR:completion:]_block_invoke
-[SSRVTUITrainingManager cancelTrainingForID:]
-[SSRVTUITrainingManager closeSessionBeforeStartWithStatus:successfully:withCompletion:]
-[SSRVTUITrainingManager _shouldShowHeadsetDisconnectionMessage]
-[SSRVTUITrainingManager _startAudioSession]
-[SSRVTUITrainingManager setSuspendAudio:]
-[SSRVTUITrainingManager setSuspendAudio:]_block_invoke
-[SSRVTUITrainingManager CSVTUITrainingSessionStopListen]
-[SSRVTUITrainingManager CSVTUITrainingSession:hasTrainUtterance:languageCode:payload:]
-[SSRVTUITrainingManager audioSessionDidStopRecording:]
-[SSRVoiceProfileRetrainerFactory init]
Borealis Input
com.apple.VoiceTriggerUI.RecordSessionQueue
-[CSVTUIAudioSessionRecorder init]
-[CSVTUIAudioSessionRecorder _audioRecorder]
-[CSVTUIAudioSessionRecorder prepareRecord]
-[CSVTUIAudioSessionRecorder startRecording]
-[CSVTUIAudioSessionRecorder stopRecording]
-[CSVTUIAudioSessionRecorder _hasCorrectInputAudioRoute]
-[CSVTUIAudioSessionRecorder _hasCorrectOutputAudioRoute]
-[SSRVoiceProfileMetaContext initWithVoiceProfile:]
[profileId: %@, language: %@, product: %@, version: %@, homeId: %@, name: %@, pitch:%@ Hz]
voiceTriggerSecondPass
+[SSRTriggerPhraseDetector filterVTAudioFiles:withLocale:withAsset:]
v20@?0@"NSError"8f16
-[SSRTriggerPhraseDetector initWithLocale:asset:]
-[SSRTriggerPhraseDetector computeTriggerConfidenceForAudio:withCompletion:]
-[SSRTriggerPhraseDetector computeTriggerConfidenceForAudio:withCompletion:]_block_invoke
v44@?0{AudioBufferList=I[1{AudioBuffer=II^v}]}8B32@"NSError"36
NDAPI: missing best_score for %@
best_score
Quasar: missing best_score for %@
com.apple.corespeech
Languages
Footprint
Premium
totalSamplesAtEndOfCapture
spIdAudioProcessedDuration
spIdUnknownUserScore
spIdKnownUserScores
spIdUserScoresVersion
spIdScoreThresholdingType
spIdVTInvocationScoreThresholdingType
spIdNonVTInvocationScoreThresholdingType
SpIdScoreThreshold
spIdAssetVersion
spIdDirectionSigsArr
shouldRecordUserAudio
shouldRecordPayload
bestVoiceTriggerScore
sessionId
segmentStartTime
segmentCounter
myriad
ssrMeta
voiceTriggerRequestUID
numEnrollmentUtt
combinationWeight
numSpeakerVectors
numExplicitUtt
numImplicitUtt
profileID
psrContext
spIdKnownUserPSRScores
spIdKnownUserPSRExpScores
satContext
spIdKnownUserSATScores
spIdKnownUserSATExpScores
Unknown InvocationStyle: %lu
tdti
tdtiexplicit
tdexplicit
Unknown CSSpIdType: %lu
+[SSRUtils spIdTypeForString:]
Unknown SpeakerRecognizerType: %lu
Unknown VoiceProfileRetrainerType: %lu
config.txt
config_td_spid.txt
config_sr_sat.txt
+[SSRUtils satConfigFileNameForCSSpIdType:]
config_tdti_spid.txt
+[SSRUtils psrConfigFileNameForCSSpIdType:]
config_ti_spid.txt
+[SSRUtils satConfigFileNameForCSSpIdType:forModelType:forAssetType:]
spid-imported
+[SSRUtils createDirectoryIfDoesNotExist:]
Library/Logs/CrashReporter/ssr
self ENDSWITH '.wav'
+[SSRUtils ssrAudioLogsCountWithinPrivacyLimit]
+[SSRUtils cleanupOrphanedVoiceIdGradingFiles]
+[SSRUtils cleanupOrphanedVoiceIdGradingFiles]_block_invoke
VoiceProfileCache
AudioFileOpenURL Failed : %@
EARTests
AudioFileOpenURL failed: %@
ExtAudioFileWrapAudioFileID failed: %@
Error reading audio-file: %d
EOF. Num bytes read: %lu
+[SSRUtils isSpeakerRecognitionSupportedInLocale:]
+[SSRUtils readJsonFileAtPath:]
kCSDeviceCategory_Unknown
kCSDeviceCategory_iOS_NonAop
kCSDeviceCategory_iOS_Aop
kCSDeviceCategory_macOS
kCSDeviceCategory_audioAccessory
kCSDeviceCategory_iOS_Aop_Explicit
iPad3,4
iPad3,5
iPad3,6
iPad4,1
iPad4,2
iPad4,3
iPad4,4
iPad4,5
iPad4,6
iPad4,7
iPad4,8
iPad4,9
iPad5,1
iPad5,2
iPad5,3
iPad5,4
iPad6,7
iPad6,8
iPad6,11
iPad6,12
iPhone5,1
iPhone5,2
iPhone5,3
iPhone5,4
iPhone6,1
iPhone6,2
iPhone7,1
iPhone7,2
iPod
iPad
iPhone
Accessory
+[SSRUtils deviceCategoryForDeviceProductType:]
+[SSRUtils isCurrentDeviceCompatibleWithNewerVoiceProfileAt:]
+[SSRUtils isCurrentDeviceCompatibleWithVoiceProfileAt:]
audio
pathExtension='json'
Caches/VoiceTrigger/ImplicitUtterences
+[SSRUtils getNumberOfAudioFilesInDirectory:]
v32@?0@"NSString"8Q16^B24
+[SSRUtils dumpFilesInDirectory:]
+[SSRUtils getContentsOfDirectory:]
+[SSRUtils getHomeUserIdForVoiceProfile:withCompletion:]
+[SSRUtils getHomeUserIdForVoiceProfile:withCompletion:]_block_invoke
v24@?0@"NSString"8@"NSError"16
homeUserId query for siriProfileId %@ timedout !
+[SSRUtils getVoiceProfileForSiriProfileId:forLanguageCode:]
+[SSRUtils logSpeakerRecognitionGradingMetadataAtFilepath:withScoreInfo:]
.wav
ERR: Audio path is nil - Bailing out
+[SSRUtils segmentVoiceTriggerFromAudioFile:withVTEventInfo:withStorePayloadPortion:withCompletion:]
ERR: Failed initializing loggers at %@ and %@
+[SSRUtils segmentVoiceTriggerFromAudioFile:withVTEventInfo:withStorePayloadPortion:withCompletion:]_block_invoke
ERR: Failed to read file: %@
+[SSRUtils getEnrollmentUtterancesFromDirectory:]
+[SSRUtils getExplicitEnrollmentUtterancesFromDirectory:]_block_invoke
v32@?0@"NSURL"8Q16^B24
+[SSRUtils getExplicitEnrollmentUtterancesFromDirectory:]
+[SSRUtils getExplicitMarkedEnrollmentUtterancesFromDirectory:]_block_invoke
+[SSRUtils getImplicitEnrollmentUtterancesFromDirectory:]_block_invoke
+[SSRUtils _getUtterancesFromDirectory:]
self.absoluteString ENDSWITH '.wav'
+[SSRUtils removeItemAtPath:]
Failed to get contents of %@ with error %@
+[SSRUtils moveContentsOfSrcDirectory:toDestDirectory:]
Failed to move %@ to %@ with error %@
+[SSRUtils combineScoreFromPSR:fromSAT:withCombinedWt:]
+[CSUtils(LanguageCode) getSiriLanguageWithFallback:]
+[CSUtils(LanguageCode) getSiriLanguageWithEndpointId:fallbackLanguage:]
triggerStartSampleCount
triggerEndSampleCount
isTriggerEvent
totalSampleCount
triggerScore
isMaximized
-[CSVTUIKeywordDetector initWithAsset:]
CSRemoteRecordClient Queue
VoiceTriggerEventInfo
com.apple.ssr
locale
asset
profileUpdateFailedExplicitUttScore
profileUpdateDiscardImplicitUttScore
profileUpdateExplicitUttScore
profileUpdateImplicitUttScore
profileUpdateNumPrunedUttsPHS
profileUpdateNumDiscardedUttsPHS
profileUpdateNumRetainedUttsPHS
profileUpdateScoreMSE
profileUpdateFailCode
speakerRecognitionWaitTimeMs
speakerRecognitionProcessingStatus
retrainingWaitTimeMs
retrainingStatusCode
TdPsrSATRetrainingTimedOut
TdPsrExtraAudioSamplesProcessed
TdPsrFailedDuringSATDetection
xx_XX
unknown
@"NSDictionary"8@?0
%@.%d
-[SSRLoggingAggregator pushAnalytics]
voic
carplay
hearst
raisetospeak
auto
B24@?0@"SSRVoiceProfile"8@"NSDictionary"16
-[SSRVoiceProfileStoreCleaner filterDuplicatedSiriProfilesFrom:]
Primary
-[SSRVoiceProfileStoreCleaner filterInvalidSiriProfilesFrom:]
kAFAssistantErrorDomain
-[SSRVoiceProfileStoreCleaner filterInvalidSiriProfilesFrom:]_block_invoke
-[SSRVoiceProfileStoreCleaner cleanupProfileStore]
ERR: Failed to get appdomain for profile %@
-[SSRVoiceProfileStoreCleaner cleanupProfileStore]_block_invoke
v32@?0@"SSRVoiceProfile"8Q16^B24
-[SSRVoiceProfileStoreCleaner _cleanupAppDomain:]
-[SSRVoiceProfileStoreCleaner _cleanuplanguageCodePath:forAppDomain:]
-[SSRVoiceProfileStoreCleaner _cleanupImplicitUtteranceCacheForProfile:]
-[SSRVoiceProfileStoreCleaner _cleanupOrphanedMetafilesForProfile:payloadUtteranceLifeTimeInDays:]
-[SSRVoiceProfileStoreCleaner _cleanupContentsOfSatFolder:]
-[SSRVoiceProfileStoreCleaner _cleanupInvalidAudioFiles:]
Failed reading contents of audioDir: %@, err: %@
-[SSRVoiceProfileStoreCleaner _cleanupOrphanedMetafilesAtURL:]
enrollment_completed
-[SSRVoiceProfileStoreCleaner _cleanupPayloadUtterancesFromProfile:forModelType:exceedingLifeTimeInDays:]
obsoleteCutOffDate is nil - Bailing out
-[SSRVoiceProfileStoreCleaner _cleanupPayloadUtterancesFromProfile:forModelType:exceedingLifeTimeInDays:]_block_invoke
Error reading contents of modelDir: %@, err: %@
-[SSRVoiceProfileStoreCleaner _cleanupModelFilesAtDir:forAssetArray:]
UserVoiceProfileDateTrained
UserVoiceProfileLocale
UserVoiceProfileAppDomain
UserVoiceProfilePitch
UserVoiceProfilePath
UserVoiceProfileID
UserSharedSiriID
UserVoiceProfileUserName
VoiceProfileIdentifier
VoiceProfileCompatabiltyVersion
VoiceProfileProductType
VoiceProfileSWVersion
VoiceProfileCategoryType
UserVoiceProfileOnboardType
UserVoiceProfileExpSatVecCount
VoiceProfilePruningCookie
-[SSRVoiceProfile initNewVoiceProfileWithLocale:withAppDomain:]
Creating SSRVoiceProfile with no profileId vpDict: %@
-[SSRVoiceProfile importVoiceProfileAtPath:]_block_invoke
Q24@?0@"NSURL"8Q16
-[SSRVoiceProfile importVoiceProfileAtPath:]
ERR: Too less audio files (%ld) for onboarding
utterances passed is nil!
-[SSRVoiceProfile addUtterances:spIdType:]
Failed to copy utterances with error %@
-[SSRVoiceProfile getImplicitEnrollmentUtterancesPriorTo:forType:]
-[SSRVoiceProfile getImplicitEnrollmentUtterancesPriorTo:forType:]_block_invoke
audiocache
td-sr-model
model
-[SSRVoiceProfile deleteModelForSpidType:recognizerType:]
enrollment_migrated
-[SSRVoiceProfile _isSATMarkedWithMarker:]
-[SSRVoiceProfile _markSATEnrollmentWithMarker:]
-[SSRVoiceProfile _updateVoiceProfileVersionFile]
-[SSRVoiceProfile updatePruningCookie:]
{wordCount: %ld, trailingSilDuration: %ld, eosLikelihood: %f, pauseCounts: (%@), silencePosterior: %f, taskName: %@, processedAudioDurationInMilliseconds: %ld}
WordCount
TrailingSilDuration
EOSLikelihood
PauseCounts
SilencePosterior
ProcessedAudioDurationInMilliseconds
SSRVoiceRetrainingVoiceProfile
SSRVoiceRetrainingCompareVoiceProfiles
SSRVoiceRetrainingCompareVoiceProfilesSpIdType
SSRVoiceRetrainingAsset
SSRVoiceRetrainingSpIdType
SSRVoiceRetrainingFilterToVoiceTriggerUtterances
SSRVoiceRetrainingForce
SSRVoiceRetrainingPayloadProfile
retraining
ERR: VoiceProfile is invalid - Bailing out
-[SSRVoiceProfileRetrainingContext initWithVoiceRetrainingContext:error:]
ERR: Last known assets are nil - Bailing out
ERR: _modelsContext is nil - Bailing out
[SessionId: %@, Asset: %@, ProfileID: %@]
meta_version.json
enrollment_version.json
meta_version
trainingType
explicit
implicit
handheld
near-field
far-field
utteranceWav
triggerSource
audioInputSource
otherSourceProfileMatch
containsPayload
grainedDate
+[SSRVoiceProfileMetadataManager saveUtteranceMetadataForUtterance:enrollmentType:isHandheldEnrollment:triggerSource:audioInput:otherBiometricResult:containsPayload:]
+[SSRVoiceProfileMetadataManager _getBaseMetaDictionaryForUtterancePath:]
+[SSRVoiceProfileMetadataManager _writeMetaDict:forUtterancePath:]
.json
+[SSRVoiceProfileMetadataManager isUtteranceImplicitlyTrained:]
+[SSRVoiceProfileMetadataManager getUtteranceEnrollmentType:]
+[SSRVoiceProfileMetadataManager recordedTimeStampOfFile:]
yyyyMMdd
+[SSRVoiceProfileMetadataManager recordedTimeStampFromFileName:]
yyyyMMdd-HHmmss
+[CSVTUIRegularExpressionMatcher matchWithString:TrailingStr:LeadingStr:Pattern:]
-[CSVTUITrainingSessionWithPayload _firedVoiceTriggerTimeout]
-[CSVTUITrainingSessionWithPayload _firedEndPointTimeout]
v16@?0@"NSDictionary"8
-[CSVTUITrainingSessionWithPayload handleAudioInput:]_block_invoke
-[CSVTUITrainingSessionWithPayload audioSessionDidStartRecording:error:]
-[CSVTUITrainingSessionWithPayload audioSessionDidStopRecording:]
-[CSVTUITrainingSessionWithPayload didDetectBeginOfSpeech]
-[CSVTUITrainingSessionWithPayload didDetectEndOfSpeech:]
-[CSVTUITrainingSessionWithPayload closeSessionWithStatus:successfully:]
-[CSVTUITrainingSessionWithPayload speechRecognitionTask:didHypothesizeTranscription:]_block_invoke
-[CSVTUITrainingSessionWithPayload speechRecognitionTask:didFinishRecognition:]_block_invoke
-[CSVTUITrainingSessionWithPayload speechRecognitionTask:didFinishSuccessfully:]_block_invoke
-[CSVTUITrainingSessionWithPayload matchRecognitionResult:withMatchedBlock:withNonMatchedBlock:]
-[CSVTUITrainingSession closeSessionWithStatus:successfully:]
-[CSVTUITrainingSession closeSessionWithStatus:successfully:complete:]_block_invoke
-[CSVTUITrainingSession suspendTraining]
-[CSVTUITrainingSession suspendTraining]_block_invoke
-[CSVTUITrainingSession resumeTraining]
-[CSVTUITrainingSession resumeTraining]_block_invoke
-[CSVTUITrainingSession setupPhraseSpotter]
-[CSVTUITrainingSession handleAudioInput:]_block_invoke_2
-[CSVTUITrainingSession handleAudioBufferForVTWithAudioInput:withDetectedBlock:]
-[CSVTUITrainingSession feedSpeechRecognitionTrailingSamplesWithCompletedBlock:]
-[CSVTUITrainingSession trimBeginingOfPCMBufferWithVoiceTriggerEventInfo:]
-[CSVTUITrainingSession audioSessionUnsupportedAudioRoute]
-[CSVTUITrainingSession didDetectBeginOfSpeech]
-[CSVTUITrainingSession didDetectEndOfSpeech:]
PHS explicit training utterance
[%ld] VTUISession Number:[%ld]
-[CSVTUITrainingSession startMasterTimerWithTimeout:]
-[CSVTUITrainingSession handleMasterTimeout:]
-[CSVTUITrainingSession stopMasterTimer]
-[CSVTUITrainingSession speechRecognitionTask:didHypothesizeTranscription:]
triggerFireMachTime
satTriggered
firstPassTriggerSource
deviceHandHeld
languageCode
ApplicationProcessor
com.apple.voicetrigger.PHSProfileDownloadTrigger
com.apple.voicetrigger.speakermodelUpdated
com.apple.voicetrigger.retrainRequired
com.apple.voicetrigger.voiceprofilesync
VoiceProfileAvailabilityMetaBlobVersion
com.apple.cs.profileManager
Library
VoiceTrigger/SAT
-[SSRVoiceProfileManager discardSiriEnrollmentForProfileId:forLanguageCode:]
-[SSRVoiceProfileManager _getVoiceProfilesForSiriProfileId:withLanguageCode:]
ERR: profile is nil - Bailing out
-[SSRVoiceProfileManager addUtterances:toProfile:withContext:withCompletion:]
ERR: Context is nil - Bailing out
ERR: Failed to copy %@ to %@, error: %@
-[SSRVoiceProfileManager addUtterances:toProfile:withContext:withCompletion:]_block_invoke
ERR: Failed in marking Enrollment as Successful for profile %@
v20@?0B8@"NSError"12
ImplicitTraining
-[SSRVoiceProfileManager notifyImplicitTrainingUtteranceAvailable:forVoiceProfileId:withRecordDeviceInfo:withRecordCtxt:withVoiceTriggerCtxt:withOtherCtxt:withCompletion:]_block_invoke
primary
v16@?0@"NSError"8
-[SSRVoiceProfileManager notifyImplicitTrainingUtteranceAvailable:forVoiceProfileId:withRecordDeviceInfo:withRecordCtxt:withVoiceTriggerCtxt:withOtherCtxt:withCompletion:]
Failed to get asset for locale %@
%lld
ERR: Voice Profile not found for %@ - Bailing out
ERR: Voice Profile locale %@ not matching with %@ - Bailing out
v32@?0@"NSError"8@"NSURL"16@"NSURL"24
-[SSRVoiceProfileManager notifyUserVoiceProfileDownloadReadyForUser:getData:completion:]_block_invoke
Primary User
Missing downloadTriggerBlock - Bailing out
Unknown device category for device type %@ - Bailing out
-[SSRVoiceProfileManager notifyUserVoiceProfileUpdateReady]_block_invoke
userAddition timedout after %fms
Enable VoiceTrigger Upon VoiceProfile Sync For Language
-[SSRVoiceProfileManager _downloadAndEnrollVoiceProfileForProfileId:withDownloadTriggerBlock:]_block_invoke_2
-[SSRVoiceProfileManager _downloadAndEnrollVoiceProfileForProfileId:withDownloadTriggerBlock:]_block_invoke
v24@?0@"NSError"8@"NSString"16
@"NSError"16@?0Q8
-[SSRVoiceProfileManager _downloadAndEnrollVoiceProfileForProfileId:withDownloadTriggerBlock:]
SAT download path is nil - Bailing out
-[SSRVoiceProfileManager _downloadVoiceProfileForProfileId:forDeviceCategory:withDownloadTriggerBlock:withCompletion:]
Download for %@ failed with %@
-[SSRVoiceProfileManager _enrollVoiceProfileForSiriProfileId:fromCacheDirectoryPath:withCategoryType:]
Skipping profile Update for %@ in %@
ERR: Failed to import profile %@ for %@
ERR: Migrated language %@ for %@ but failed to mark SAT enrollment
ERR: Failed to mark migrated for %@ in language %@
Failed to init retrainCtxt for profileID %@ with error %@
-[SSRVoiceProfileManager _enrollVoiceProfileForSiriProfileId:fromCacheDirectoryPath:withCategoryType:]_block_invoke
userAddition timedout for siriProfileId %@ after %fms
Failed to enroll user - %@
SourcePath (%@) or DestinationPath (%@) is nil - Bailing out
-[SSRVoiceProfileManager _enableVoiceTriggerIfLanguageMatches:]
Caches/VoiceTrigger/SATUpdate
Caches/VoiceTrigger/SATUpdateNewerZone
_%d_%d
-[SSRVoiceProfileManager _getUserVoiceProfileDownloadCacheDirectoryWithUpdatePath:]
-[SSRVoiceProfileManager notifyUserVoiceProfileUploadCompleteForSiriProfileId:withError:]_block_invoke
-[SSRVoiceProfileManager uploadUserVoiceProfileForSiriProfileId:withUploadTrigger:completion:]_block_invoke
@"NSError"24@?0@"SSRVoiceProfileMetaContext"8@"NSString"16
-[SSRVoiceProfileManager getUserVoiceProfileUploadPathWithEnrolledLanguageList:]
-[SSRVoiceProfileManager getUserVoiceProfileUploadPathWithEnrolledLanguageList:]_block_invoke
-[SSRVoiceProfileManager notifyUserVoiceProfileUploadComplete]_block_invoke
-[SSRVoiceProfileManager _getVoiceProfilePathsToBeUploadedForSiriProfileId:]
-[SSRVoiceProfileManager _copyExplicitEnrollmentFilesFromPath:toPath:withCompletion:]
Failed to copy to SATUpload Diretory : %@
v24@?0@"NSError"8Q16
-[SSRVoiceProfileManager _copyVoiceProfileAtPath:toPath:]
ERR: Number of training utterances copied from %@ to %@ is too less %ld
Cannot delete existing SATUpload Diretory : %@
-[SSRVoiceProfileManager _prepareVoiceProfileWithSiriProfileId:withUploadBlock:]
Cannot create SAT Upload Directory : %@
Failed to upload %@ with error %@ - Bailing out
-[SSRVoiceProfileManager _markVoiceProfileTrainingSyncForLanguage:]
Enable VoiceProfile Training Sync For Language
-[SSRVoiceProfileManager _isMarkedForVoiceProfileTrainingSyncForLanguage:]
-[SSRVoiceProfileManager hasVoiceProfileIniCloudForLanguageCode:withBackupMetaBlob:]
ERR: Unknown product type. Returning false, language: %@
-[SSRVoiceProfileManager isVoiceProfileUploadedToiCloudForLanguageCode:withCompletionBlock:]
ERR: Unknown device-category for device: %@, languageCode: %@
ERR: Improper VoiceProfile detected: %@, languageCode: %@
-[SSRVoiceProfileManager isVoiceProfileUploadedToiCloudForLanguageCode:withCompletionBlock:]_block_invoke
v24@?0@"NSDictionary"8@"NSError"16
-[SSRVoiceProfileManager hasVoiceProfileIniCloudForLanguageCode:]
-[SSRVoiceProfileManager enableVoiceTriggerUponVoiceProfileSyncForLanguage:]
-[SSRVoiceProfileManager devicesWithVoiceProfileIniCloudForLanguage:]
-[SSRVoiceProfileManager devicesWithVoiceProfileIniCloudForLanguage:]_block_invoke
v16@?0@"NSArray"8
Caches/VoiceTrigger/SATLegacyUpload
-[SSRVoiceProfileManager provisionedVoiceProfilesForAppDomain:withLocale:]
q24@?0@"SSRVoiceProfile"8@"SSRVoiceProfile"16
-[SSRVoiceProfileManager provisionedVoiceProfilesForLocale:]
-[SSRVoiceProfileManager getVoiceProfileAnalyticsForAppDomain:withLocale:]
ERR: Voice Profile sent as nil - Bailing out
-[SSRVoiceProfileManager triggerRetrainingVoiceProfile:withContext:withCompletion:]
ERR: Voice Profile not found for Id %@ - Bailing out
-[SSRVoiceProfileManager markSATEnrollmentSuccessForVoiceProfile:]
-[SSRVoiceProfileManager markSATEnrollmentSuccessForVoiceProfile:]_block_invoke
-[SSRVoiceProfileManager isSATEnrolledForSiriProfileId:forLanguageCode:]
ERR: Voice Profile passed is nil - Bailing out
-[SSRVoiceProfileManager deleteUserVoiceProfile:]
-[SSRVoiceProfileManager deleteAllVoiceProfilesForAppDomain:]
Library/Caches/VoiceTrigger
Caches/VoiceTrigger/SATUpload
td/audio
-[SSRVoiceProfileManager _isLegacyEnrollmentMarkedWith:forLanguageCode:]
SSRVoiceProfileStore
Class getSSRVoiceProfileStoreClass(void)_block_invoke
SSRVoiceProfileManager.m
Unable to find class %s
void *CoreSpeechLibrary(void)
/System/Library/PrivateFrameworks/CoreSpeech.framework/CoreSpeech
/System/Library/PrivateFrameworks/CoreSpeech.framework/Contents/MacOS/CoreSpeech
-[CSNNVADEndpointAnalyzer init]
-[CSNNVADEndpointAnalyzer preheat]
-[CSNNVADEndpointAnalyzer reset]
-[CSNNVADEndpointAnalyzer processAudioSamplesAsynchronously:]
-[CSNNVADEndpointAnalyzer recordingStoppedForReason:]
-[CSNNVADEndpointAnalyzer stopEndpointer]
-[CSNNVADEndpointAnalyzer resetForNewRequestWithSampleRate:recordContext:recordSettings:]
+[CSAudioRecordContext defaultContext]
CSAudioRecordTypeUnspecified
CSAudioRecordTypeHomePress
CSAudioRecordTypeWiredHeadsetButtonPress
CSAudioRecordTypeBluetoothHeadSetButtonPress
CSAudioRecordTypeUIButtonPress
CSAudioRecordTypeServerInvoke
CSAudioRecordTypeVoiceTrigger
CSAudioRecordTypeStark
CSAudioRecordTypeTVRemote
CSAudioRecordTypeRaiseToSpeak
CSAudioRecordTypeHearstDoubleTap
CSAudioRecordTypeHearstVoice
CSAudioRecordTypeJarvis
CSAudioRecordTypePost
CSAudioRecordTypeDictation
CSAudioRecordTypeVoiceTriggerTraining
CSAudioRecordTypeOpportuneSpeaker
CSAudioRecordTypeOpportuneSpeakerListenerWithCall
CSAudioRecordTypeUnknown
recordType[%@] deviceId[%@] alwaysUseBuiltInMic[%d] isRequestDuringActiveCall[%d]
type
alwaysUseRemoteBuiltInMic
deviceId
isRequestDuringActiveCall
[Context = %ld]
[DeviceId = %@]
[Announced = %d]
[streamHandleId = %d]
[startHostTime = %llu]
[startAlert = %d]
[stopAlert = %d]
[stopOnErrorAlert = %d]
[skipAlert = %@]
VoiceControllerCreationQueue
-[CSVTUIAudioRecorder initWithQueue:error:]
-[CSVTUIAudioRecorder dealloc]
-[CSVTUIAudioRecorder _destroyVoiceController]
-[CSVTUIAudioRecorder _voiceControllerWithError:]_block_invoke
-[CSVTUIAudioRecorder _voiceControllerWithError:]
-[CSVTUIAudioRecorder setContext:error:]
-[CSVTUIAudioRecorder setCurrentContext:streamHandleId:error:]
-[CSVTUIAudioRecorder prepareAudioStreamRecordWithStreamHandleId:error:]
-[CSVTUIAudioRecorder startAudioStreamWithStreamHandleId:error:]
-[CSVTUIAudioRecorder stopAudioStreamWithStreamHandleId:error:]
Builtin Microphone
-[CSVTUIAudioRecorder activateAudioSessionWithReason:streamHandleId:error:]
-[CSVTUIAudioRecorder deactivateAudioSession:error:]
useRemoteBuiltInMic
-[CSVTUIAudioRecorder _processAudioBuffer:audioStreamHandleId:arrivalTimestampToAudioRecorder:]
-[CSVTUIAudioRecorder _compensateChannelDataIfNeeded:receivedNumChannels:]
-[CSVTUIAudioRecorder voiceControllerDidStartRecording:forStream:successfully:error:]
-[CSVTUIAudioRecorder voiceControllerAudioCallback:forStream:buffer:]
-[CSVTUIAudioRecorder voiceControllerDidStopRecording:forStream:forReason:]
-[CSVTUIAudioRecorder voiceControllerRecordHardwareConfigurationDidChange:toConfiguration:]
-[CSVTUIAudioRecorder voiceControllerBeginRecordInterruption:]
-[CSVTUIAudioRecorder voiceControllerBeginRecordInterruption:withContext:]
-[CSVTUIAudioRecorder voiceControllerEndRecordInterruption:]
-[CSVTUIAudioRecorder voiceControllerWillSetAudioSessionActive:willActivate:]
-[CSVTUIAudioRecorder voiceControllerDidSetAudioSessionActive:isActivated:]
-[CSVTUIAudioRecorder voiceControllerMediaServicesWereLost:]
-[CSVTUIAudioRecorder voiceControllerMediaServicesWereReset:]
-[CSVTUIAudioRecorder _deinterleaveBufferIfNeeded:force:]
-[CSVTUIAudioRecorder _createDeInterleaverIfNeeded]
Serial SSRAssetManager queue
en-US
-[SSRAssetManager init]
Trial
-[SSRAssetManager allInstalledAssetsOfType:forLanguage:]_block_invoke_2
-[SSRAssetManager allInstalledAssetsOfType:forLanguage:]_block_invoke
v32@?0@"<SSRAssetProviding>"8Q16^B24
-[SSRAssetManager _latestVersionedAssetOfType:fromProviders:forLocale:]_block_invoke
-[SSRSpeakerRecognitionController voiceRecognitionOrchestrator:hasVoiceRecognitionInfo:]
ERR: Scorecard not available in score dictionary - %@
-[SSRSpeakerRecognitionController voiceRecognitionOrchestratorFinishedProcessing:withFinalVoiceRecognitionInfo:]
-[SSRMobileAssetProvider allInstalledAssetsOfType:forLanguage:]
q24@?0@"MAAsset"8@"MAAsset"16
-[SSRMobileAssetProvider allInstalledAssetsOfType:forLanguage:]_block_invoke_2
v32@?0@"MAAsset"8Q16^B24
com.apple.MobileAsset.VoiceTriggerAssets
com.apple.MobileAsset.VoiceTriggerAssetsIPad
com.apple.MobileAsset.VoiceTriggerAssetsWatch
com.apple.MobileAsset.VoiceTriggerAssetsMarsh
com.apple.MobileAsset.VoiceTriggerAssetsMac
com.apple.MobileAsset.SpeakerRecognitionAssets
com.apple.MobileAsset.SpeechEndpointAssetsWatch
-[SSRMobileAssetProvider _buildAssetQueryForAssetType:]
-[SSRMobileAssetProvider _installedMobileAssetOfType:forLanguage:]
-[SSRMobileAssetProvider _findLatestInstalledAsset:]
com.apple.corespeech.voiceprofilestore
-[SSRVoiceProfileStore userVoiceProfilesForAppDomain:]
-[SSRVoiceProfileStore userVoiceProfilesForLocale:]
-[SSRVoiceProfileStore migrateVoiceProfilesIfNeededWithCompletionBlock:]_block_invoke
Filtered languages is nil - %@
spid
trained_users.json
Could not read existing %@ file: err: %@
-[SSRVoiceProfileStore cleanupDuplicatedProfiles]_block_invoke
-[SSRVoiceProfileStore cleanupVoiceProfileStore:]_block_invoke_2
-[SSRVoiceProfileStore cleanupVoiceProfileStore:]_block_invoke
-[SSRVoiceProfileStore cleanupVoiceProfileModelFilesForLocale:]_block_invoke
-[SSRVoiceProfileStore _synchronizeSiriVoiceProfilesWithAssistant]_block_invoke
com.apple.siri.corespeech.voiceprofilelist.change
-[SSRVoiceProfileStore addImplicitUtterance:toVoiceProfile:withAsset:withTriggerSource:withAudioInput:withCompletion:]_block_invoke
Profile %@ is full - Ignoring
Utterance %@ in profile %@ not satisfied the implicit VT policy
Rejecting Implicit utterance %@ for profile %@
@"NSError"24@?0@"NSURL"8@"NSDictionary"16
Utterance %@ rejected for profile %@
v32@?0@"NSError"8@"NSDictionary"16@"NSDictionary"24
Utterance %@ in profile %@ not satisfied the implicit policy
-[SSRVoiceProfileStore _logVoiceProfileConfusionWithCleanup:]
B16@?0@"NSDictionary"8
-[SSRVoiceProfileStore _logVoiceProfileConfusionWithCleanup:]_block_invoke
v32@?0@"<SSRVoiceProfileRetrainer>"8Q16^B24
-[SSRVoiceProfileStore evaluateImplicitAdditionPolicyWithScores:forProfile:withImplicitThreshold:withDeltaThreshold:]
v32@?0@"NSString"8@"NSNumber"16^B24
Profile is nil!
-[SSRVoiceProfileStore addUserVoiceProfile:withContext:withCompletion:]_block_invoke
-[SSRVoiceProfileStore _deleteUserVoiceProfile:]
Deleting profile data at %@ failed with error %@
Profile path is nil!
-[SSRVoiceProfileStore checkIfVoiceProfile:needsUpdatedWith:withCategory:]
-[SSRVoiceProfileStore _checkIfRetrainingRequiredForProfile:]_block_invoke
Failed to init retrainers for profileID %@ with ctxt %@
B16@?0Q8
-[SSRVoiceProfileStore retrainVoiceProfile:withContext:withCompletion:]_block_invoke
-[SSRVoiceProfileStore _updateTrainedUsersWithAction:UserVoiceProfile:]
Voice Profile not found for profileId: %@ - Bailing out
-[SSRVoiceProfileStore updateVoiceProfile:withUserName:]
-[SSRVoiceProfileStore _retrainLiveOnOnboardedProfilesForLanguage:withForceRetrain:withCompletion:]
VoiceProfile is nil - Bailing out
-[SSRVoiceProfileStore _retrainVoiceProfile:withContext:]
context is nil - Bailing out
Invalid spIdType %d - Bailing out
SSRVoiceProfileStore retrainer - %@
Too less (%d) audio files in %@ 
-[SSRVoiceProfileStore _retrainVoiceProfile:withContext:withUtterances:]
-[SSRVoiceProfileStore _retrainVoiceProfile:withContext:withUtterances:]_block_invoke
Failed to copy %@ to %@ with error %@
-[SSRVoiceProfileStore copyAudioFiles:toProfile:forModelType:]
-[CSSelectiveChannelAudioFileWriter initWithURL:inputFormat:outputFormat:channelBitset:]
v16@?0Q8
-[CSSelectiveChannelAudioFileWriter addSamples:numSamples:]
+[SSREnrollmentDataManager saveRawUtteranceAndMetadata:to:isExplicitEnrollment:]
+[SSREnrollmentDataManager saveUtteranceAndMetadata:atDirectory:isExplicitEnrollment:]
+[SSREnrollmentDataManager saveUtterance:utteranceAudioPath:numSamplesToWrite:isExplicitEnrollment:]
+[SSREnrollmentDataManager saveMetadata:isExplicitEnrollment:]
+[SSREnrollmentDataManager _getBaseMetaDictionaryForUtterancePath:]
+[SSREnrollmentDataManager writeMetaDict:atMetaPath:]
Known User Voice Profiles
Voice Profile Store Version
mobile
SSRSpeakerRecognitionStyle
SSRSpeakerRecognitionAsset
SSRSpeakerRecognitionAssetArray
SSRSpeakerRecognitionVADAssetPath
SSRSpeakerRecognitionLocale
SSRSpeakerRecognitionVTEventInfo
SSRSpeakerRecognitionProfileArray
SSRSpeakerRecognitionUsePayloadProfile
SSRSpeakerRecognitionMaxAudioSecs
SSRSpeakerRecognitionOSTransactionReqd
com.apple.siri
com.apple.siridebug
ERR: SpeakerRecognition not enabled - Bailing out
-[SSRSpeakerRecognitionContext initWithVoiceRecognitionContext:error:]
ERR: Invalid Speaker Recognition style - Bailing out
ERR: Asset not picked - Bailing out
ERR: Endpointer Asset not picked - Bailing out
v24@?0@"NSDictionary"8@"NSDictionary"16
ERR: ModelsContext is nil for locale %@ - Bailing out
%@_%@_%@
[SessionId: %@, RecognitionStyle:(%lu)%@, Asset: %@, vtEventInfo: %@]
-[SSRSpeakerRecognitionContext composeModelContextsForProfiles:forSpIdType:forAsset:completion:]
-[SSRSpeakerRecognitionContext pickAssetForProfiles:forSpIdType:withAssetArray:]
-[SSRSpeakerRecognitionContext dealloc]
regex.json
Cannot parse to JSON
Cannot find the file
trailing_garbage
leading_garbage
regex
speakerRecognition
satThreshold
implicitProfileThreshold
implicitProfileDeltaThreshold
implicitVTThreshold
pruningExplicitSATThreshold
pruningExplicitPSRThreshold
pruningSATThreshold
pruningPSRThreshold
numPruningRetentionUtt
maxEnrollmentUtterances
pruningCookie
configFileRecognizer
configFileNDAPI
voiceTriggerSecondPassAOP
implicit_training_enabled
multiUserHighScoreThreshold
multiUserLowScoreThreshold
multiUserConfidentScoreThreshold
multiUserDeltaScoreThreshold
recognizer.json
@pbhw
pbtb
pbiu
otua
ciov
bhev
eltb
siar
tdtb
cvdh
cvpc
tcid
tsop
rtsh
tvps
emoh
emoh
ciov
333333
softlink:r:path:/System/Library/PrivateFrameworks/CoreAnalytics.framework/CoreAnalytics
softlink:r:path:/System/Library/PrivateFrameworks/CoreSpeech.framework/CoreSpeech
zPLR
zPLR
zPLR
zPLR
zPLR
zPLR
zPLR
zPLR
zPLR
zPLR
zPLR
zPLR
CSPlainAudioFileWriter
CSAudioFileWriter
NSObject
AVVC
SSRVoiceActivityDetector
CSAudioFileManager
SSRSpeakerAnalyzerPSR
Directory
SSRSpeakerRecognizerPSR
SSRSpeakerAnalyzerPSRDelegate
SSRSpeakerRecognizer
SSRTriggerPhraseDetectorNDAPI
SSRSpeakerRecognitionOrchestrator
SSRSpeakerRecognizerDelegate
SSRVoiceActivityDetectorDelegate
SSRVoiceProfileRetrainerSAT
SSRVoiceProfileRetrainer
SSRDESRecordWriter
AudioStreamBasicDescription
SSRSpeakerRecognitionScorer
SSRVTUITrainingManager
CSVTUITrainingSessionDelegate
CSVTUIAudioSessionDelegate
CSEndpointAnalyzerDelegate
SSRVoiceProfileRetrainerFactory
CSVTUIAudioSessionRecorder
CSVTUIAudioRecorderDelegate
CSVTUIAudioSession
SSRVoiceProfileMetaContext
SSRTriggerPhraseDetector
CSAsset
SSRUtils
LanguageCode
SSRTrialAssetProvider
SSRAssetProviding
CSVTUIKeywordDetector
CSRemoteRecordClient
SSRVoiceProfileComposer
SSRVoiceProfileRetrainerPSR
SSRLoggingAggregator
RecordContext
SSRSpeakerRecognizerSAT
SSRVoiceProfileStoreCleaner
SSRVoiceProfile
NSSecureCoding
NSCoding
SSRTriggerPhraseDetectorQuasar
CSServerEndpointFeatures
CSVTUIEditDistance
SSRVoiceProfilePruner
SSRVoiceProfileRetrainingContext
SSRVoiceProfileModelContext
SSRVoiceProfileMetadataManager
CSVTUIRegularExpressionMatcher
CSVTUITrainingSessionWithPayload
SFSpeechRecognitionTaskDelegate
CSVTUIEndPointDelegate
CSVTUITrainingSession
SSRVoiceProfileManager
CSNNVADEndpointAnalyzer
CSEndpointAnalyzerImpl
CSEndpointAnalyzer
CSAudioRecordContext
NSCopying
debugDescription
CSVTUIAudioRecorder
AVVoiceControllerRecordDelegate
CSAudioServerCrashEventProviding
CSAudioSessionEventProviding
SSRAssetManager
SSRSpeakerRecognitionController
SSRSpeakerRecognitionOrchestratorDelegate
SSRMobileAssetProvider
SSRVoiceProfileStore
CSSelectiveChannelAudioFileWriter
SSRBiometricMatch
SSRSpeakerAnalyzerSAT
SSREnrollmentDataManager
SSRVoiceProfileStorePrefs
SSRSpeakerRecognitionContext
SSRSpeakerRecognitionModelContext
CSVTUIASRGrammars
NSURLSessionDelegate
SSRPitchExtractor
SSRAESKeyManager
SpeakerRecognition
AudioHardware
utteranceFileASBD
lpcmInt16ASBD
initWithURL:inputFormat:outputFormat:
fileURLWithPath:
init
endAudio
dealloc
URLByDeletingPathExtension
URLByAppendingPathExtension:
defaultManager
path
fileExistsAtPath:
localeWithLocaleIdentifier:
setLocale:
setDateFormat:
stringFromDate:
deviceProductType
deviceProductVersion
deviceBuildVersion
numberWithBool:
dictionaryWithObjects:forKeys:count:
dictionaryWithDictionary:
setObject:forKey:
dataWithJSONObject:options:error:
writeToFile:atomically:
dataWithContentsOfFile:
JSONObjectWithData:options:error:
mutableCopy
lastPathComponent
stringByDeletingPathExtension
stringByAppendingString:
URLByDeletingLastPathComponent
stringByAppendingPathComponent:
addContextKey:withContext:
inputRecordingSampleRate
numChannels
lpcmNonInterleavedASBDWithSampleRate:numberOfChannels:
lpcmInterleavedASBDWithSampleRate:numberOfChannels:
data
bytes
numSamples
addSamples:numSamples:
saveAudioChunck:toURL:
isEqual:
class
self
performSelector:
performSelector:withObject:
performSelector:withObject:withObject:
isProxy
isKindOfClass:
isMemberOfClass:
conformsToProtocol:
respondsToSelector:
retain
release
autorelease
retainCount
zone
hash
superclass
description
debugDescription
TQ,R
T#,R
T@"NSString",R,C
initWithURL:
initWithFilepath:
createAcousticMetaFileForContext:withContext:
addContextKey:fromMetaFile:
fileURL
.cxx_destruct
isWriting
fFile
inASBD
outASBD
_fileURL
T@"NSURL",R,N,V_fileURL
avvcContext
objectForKeyedSubscript:
unsignedIntegerValue
initWithMode:deviceUID:
supportHandsFree
isRequestDuringActiveCall
setActivationMode:
setAnnounceCallsEnabled:
avvcContextSettings
initWithContext:delegate:
processAudioData:numSamples:
resetWithContext:
_sharedAudioLoggingQueue
sharedPreferences
assistantAudioFileLogDirectory
containsString:
removeItemAtURL:error:
inputRecordingNumberOfChannels
inputRecordingSampleByteDepth
seekToEndOfFile
seekToFileOffset:
readDataOfLength:
getBytes:length:
initWithData:encoding:
isEqualToString:
offsetInFile
length
writeData:
fileLoggingIsEnabled
_createAudioFileWriterForOpportuneSpeakListenerWithLoggingDir:inputFormat:outputFormat:
_createAudioFileWriterForPHSTrainingWithLoggingDir:inputFormat:outputFormat:
_createAudioFileWriterWithLoggingDir:withLoggingUUID:inputFormat:outputFormat:
_getDateLabel
stringWithFormat:
getNumElementInBitset:
initWithURL:inputFormat:outputFormat:channelBitset:
isAdBlockerAudioLoggingEnabled
voiceTriggerAudioLogDirectory
_createAudioFileWriterForAdBlockerWithLoggingDir:inputFormat:outputFormat:
pruneLogFiles
URLWithString:
removeLogFilesInDirectory:matchingPattern:beforeDays:
maxNumGradingFiles
pruneNumberOfGradingFilesTo:
maxNumLoggingFiles
pruneNumberOfLogFilesTo:
arrayWithObjects:
countByEnumeratingWithState:objects:count:
clearLogFilesInDirectory:matchingPattern:exceedNumber:
cleanupOrphanedGradingFiles
arrayWithObjects:count:
contentsOfDirectoryAtURL:includingPropertiesForKeys:options:error:
dictionary
absoluteString
setObject:forKeyedSubscript:
removeObjectForKey:
fileExistsAtPath:isDirectory:
removeItemAtPath:error:
enumerateKeysAndObjectsUsingBlock:
isAttentiveSiriAudioLoggingEnabled
assistantLogDirectory
createDirectoryAtPath:withIntermediateDirectories:attributes:error:
generateDeviceAudioLogging:speechId:
_readDataFromFileHandle:toFileHandle:
createAudioFileWriterForOpportuneSpeakListenerWithInputFormat:outputFormat:
createAudioFileWriterForPHSTrainingWithInputFormat:outputFormat:
createAudioFileWriterForRemoteVADWithInputFormat:outputFormat:withLoggingUUID:
createAudioFileWriterWithInputFormat:outputFormat:withLoggingUUID:
createSelectiveChannelAudioFileWriterWithChannelBitset:
createAudioFileWriterForAdBlockerWithInputFormat:outputFormat:
removeLogFilesOlderThanNDays:
audioFileWriterForAttentiveSiri
initWithVoiceRecognitionContext:delegate:queue:
processAudioData:
resetForNewRequest
getVoiceRecognizerResults
_sharedDisposeLoggingQueue
dateWithTimeIntervalSinceNow:
distantFuture
getResourceValue:forKey:error:
localizedDescription
compare:
URLsInDirectory:matchingPattern:completion:
count
objectAtIndex:
_sortedURLsInDirectory:matchingPattern:completion:
_contentsOfDirectoryAtURL:matchingPattern:includingPropertiesForKeys:error:
sortedArrayUsingComparator:
regularExpressionWithPattern:options:error:
numberOfMatchesInString:options:range:
predicateWithBlock:
filteredArrayUsingPredicate:
recognitionStyle
currentHandler
handleFailureInMethod:object:file:lineNumber:description:
_initializeWithContext:
csAudioProcessingQueuePriority
getFixedHighPrioritySerialQueueWithLabel:priority:
stringForInvocationStyle:
sessionId
vtEventInfo
floatValue
numberWithFloat:
speakerRecognizer:hasSpeakerIdInfo:
speakerRecognizerFinishedProcessing:withFinalSpeakerIdInfo:
voiceRecognitionPSRAnalyzer:hasVoiceRecognitionInfo:
voiceRecognitionPSRAnalyzerFinishedProcessing:withVoiceRecognitionInfo:
lastScoreCard
T@"NSDictionary",R,N
spIdCtx
setSpIdCtx:
setSessionId:
lastSpeakerInfo
setLastSpeakerInfo:
queue
setQueue:
delegate
setDelegate:
invocationStyleStr
setInvocationStyleStr:
extraSamplesAtStart
setExtraSamplesAtStart:
vtEndInSampleCount
setVtEndInSampleCount:
endInSampleCount
setEndInSampleCount:
numSamplesProcessed
setNumSamplesProcessed:
processingEnded
setProcessingEnded:
totalNumSamplesReceived
setTotalNumSamplesReceived:
psrAnalyzer
setPsrAnalyzer:
_processingEnded
_spIdCtx
_sessionId
_lastSpeakerInfo
_queue
_delegate
_invocationStyleStr
_extraSamplesAtStart
_vtEndInSampleCount
_endInSampleCount
_numSamplesProcessed
_totalNumSamplesReceived
_psrAnalyzer
T@"SSRSpeakerRecognitionContext",&,N,V_spIdCtx
T@"NSString",&,N,V_sessionId
T@"NSDictionary",&,N,V_lastSpeakerInfo
T@"NSObject<OS_dispatch_queue>",&,N,V_queue
T@"<SSRSpeakerRecognizerDelegate>",W,N,V_delegate
T@"NSString",&,N,V_invocationStyleStr
TQ,N,V_extraSamplesAtStart
TQ,N,V_vtEndInSampleCount
TQ,N,V_endInSampleCount
TQ,N,V_numSamplesProcessed
TB,N,V_processingEnded
TQ,N,V_totalNumSamplesReceived
T@"SSRSpeakerAnalyzerPSR",&,N,V_psrAnalyzer
initWithConfigPath:resourcePath:phraseId:
analyzeWavData:numSamples:
reset
getSuperVectorWithEndPoint:
errorWithDomain:code:userInfo:
logAggregator
setSpeakerRecognitionProcessingStatus:
debugUtteranceAudioFile
debugUtteranceMetaFile
updateDebugFilePathsForSegment:
speakerRecognitionAudioLoggingEnabled
ssrAudioLogsDir
createDirectoryIfDoesNotExist:
ssrAudioLogsCountWithinPrivacyLimit
date
timeIntervalSinceReferenceDate
maxAllowedAudioSamples
osTransactionReqd
UUID
UUIDString
UTF8String
_resetWithContext:
integerValue
sharedInstance
submitVoiceIdIssueReport:
combinationWeight
combineScoreFromPSR:fromSAT:withCombinedWt:
numEnrollmentUtterances
scoreType
numberWithUnsignedInteger:
configVersion
numberWithDouble:
copy
logSpeakerRecognitionGradingMetadataAtFilepath:withScoreInfo:
stringByDeletingLastPathComponent
stringByAppendingPathExtension:
orchestratorScoresWithPSRScores:withSATScores:withSegmentStartTime:
timeIntervalSinceDate:
setSpeakerRecognitionWaitTime:
pushAnalytics
voiceRecognitionOrchestrator:hasVoiceRecognitionInfo:
_logSpeakerIdProcessorScoreDelayWithScoreInfo:hasFinished:
voiceRecognitionOrchestratorFinishedProcessing:withFinalVoiceRecognitionInfo:
SSRVoiceActivityDetector:didDetectStartPointAt:
SSRVoiceActivityDetector:didDetectEndPointAt:
initWithContext:withDelegate:error:
processAudio:numSamples:
getLatestVoiceRecognitionInfo
context
setContext:
ssrUttLogger
setSsrUttLogger:
myriadResult
setMyriadResult:
psrRecognizer
setPsrRecognizer:
satRecognizer
setSatRecognizer:
setVad:
psrLastSpeakerInfo
setPsrLastSpeakerInfo:
satLastSpeakerInfo
setSatLastSpeakerInfo:
combinedScores
setCombinedScores:
psrFinalSpeakerInfo
setPsrFinalSpeakerInfo:
satFinalSpeakerInfo
setSatFinalSpeakerInfo:
debugUtteranceAudioFilePath
setDebugUtteranceAudioFilePath:
debugUtteranceJsonFilePath
setDebugUtteranceJsonFilePath:
transaction
setTransaction:
transDesc
setTransDesc:
_lastScoreReportTimeStamp
_lastSegmentStartTime
_segmentCounter
_numSamplesAddedToSpeakerRecognizers
_endAudioCalled
_startPointReported
_context
_ssrUttLogger
_myriadResult
_psrRecognizer
_satRecognizer
_vad
_psrLastSpeakerInfo
_satLastSpeakerInfo
_combinedScores
_psrFinalSpeakerInfo
_satFinalSpeakerInfo
_debugUtteranceAudioFilePath
_debugUtteranceJsonFilePath
_transaction
_transDesc
T@"SSRSpeakerRecognitionContext",&,N,V_context
T@"<SSRSpeakerRecognitionOrchestratorDelegate>",W,N,V_delegate
T@"<CSAudioFileWriter>",&,N,V_ssrUttLogger
TQ,N,V_myriadResult
T@"<SSRSpeakerRecognizer>",&,N,V_psrRecognizer
T@"<SSRSpeakerRecognizer>",&,N,V_satRecognizer
T@"SSRVoiceActivityDetector",&,N,V_vad
T@"NSDictionary",&,N,V_psrLastSpeakerInfo
T@"NSDictionary",&,N,V_satLastSpeakerInfo
T@"NSDictionary",&,N,V_combinedScores
T@"NSDictionary",&,N,V_psrFinalSpeakerInfo
T@"NSDictionary",&,N,V_satFinalSpeakerInfo
T@"NSString",&,N,V_debugUtteranceAudioFilePath
T@"NSString",&,N,V_debugUtteranceJsonFilePath
T@"NSObject<OS_os_transaction>",&,N,V_transaction
T@"NSString",&,N,V_transDesc
initWithVoiceRetrainingContext:
resetModelForRetraining
addUtterances:withPolicy:withCompletion:
needsRetrainingWithAudioFiles:
purgeLastSpeakerEmbedding
purgeConfusionInformationWithPolicy:
modelFilePath
implicitTrainingRequired
retrainerType
T@"NSURL",R,N
TB,R,N
TQ,R,N
T@"NSURL",R,N,VmodelFilePath
TB,R,N,VimplicitTrainingRequired
TQ,R,N,VretrainerType
createDESRecordWithSuperVector:withMetaInfo:
inputRecordingSampleRateNarrowBand
inputRecordingBytesPerPacket
inputRecordingFramesPerPacket
inputRecordingBytesPerFrame
inputRecordingSampleBitDepth
inputRecordingIsFloat
lpcmInt16NarrowBandASBD
lpcmFloatASBD
opusASBD
opusNarrowBandASBD
speexASBD
lpcmInterleavedASBD
lpcmMonoInterleavedASBD
lpcmInterleavedWithRemoteVADASBD
lpcmMonoInterleavedWithRemoteVADASBD
lpcmNonInterleavedASBD
lpcmMonoNonInterleavedASBD
lpcmNonInterleavedWithRemoteVADASBD
lpcmMonoNonInterleavedWithRemoteVADASBD
lpcmASBD
lpcmNarrowBandASBD
aiffFileASBD
createVoiceScorersWithVoiceProfiles:withConfigFile:withResourceFile:withOffsetsType:
initWithProfileID:withModelFile:withConfigFile:withResourceFile:withOffsetsType:
resetScorerWithModelFilePath:
analyzeSpeakerVector:withDimensions:withThresholdType:
scoreSpeakerVector:withDimensions:withThresholdType:
analyzeSuperVector:withDimensions:withThresholdType:
updateSAT
getSATVectorCount
getSpeakerVectorAtIndex:
deleteVectorAtIndex:
profileID
sysConfigRoot
psrConfigFilePath
psrConfigRoot
satModelAvailable
_satModelAvailable
_profileID
_sysConfigRoot
_psrConfigFilePath
_psrConfigRoot
T@"NSString",R,N,V_profileID
T@"NSString",R,N,V_sysConfigRoot
T@"NSString",R,N,V_psrConfigFilePath
T@"NSString",R,N,V_psrConfigRoot
TB,R,N,V_satModelAvailable
initWithLocaleIdentifier:withAudioSession:withAppDomain:
setLocaleIdentifier:
initNewVoiceProfileWithLocale:withAppDomain:
createKeywordDetector
initWithAsset:
initWithLocale:
prepareRecord
isRecording
stopRecording
releaseAudioSession
enter
_stopAudioSession
leave
destroySpeakerTrainer
waitWithTimeout:
_destroyAudioSession
_setupAudioSession
closeSessionBeforeStartWithStatus:successfully:withCompletion:
_createAudioAnalyzer
_shouldShowHeadsetDisconnectionMessage
_startAudioSession
createSpeechRecognizer
sharedtrainingSessionQueue
initWithUtteranceId:sessionNumber:Locale:audioSession:keywordDetector:speechRecognizer:speechRecognitionRequest:sessionDelegate:sessionDispatchQueue:completion:
addObject:
startTraining
suspendTraining
closeSessionWithStatus:successfully:complete:
_audioSource
audioSource
setEndpointStyle:
setStartWaitTime:
setEndWaitTime:
setInterspeechWaitTime:
preheat
resetForNewRequestWithSampleRate:recordContext:recordSettings:
hasCorrectAudioRoute
startRecording
resumeTraining
VTUITrainingManagerFeedLevel:
VTUITrainingManagerStopListening
sharedTrainer
addUtterance:toProfile:withAsset:
audioSessionDidStartRecording:error:
audioSessionDidStopRecording:
initWithData:numChannels:numSamples:sampleByteDepth:startSampleCount:hostTime:remoteVAD:
processAudioSamplesAsynchronously:
audioSessionRecordBufferAvailable:
audioSessionErrorDidOccur:
audioSessionUnsupportedAudioRoute
didDetectBeginOfSpeech
didDetectEndOfSpeech:
trainingManagerWithLocaleID:withAppDomain:
CSVTUITrainingSessionRMSAvailable:
CSVTUITrainingSessionStopListen
CSVTUITrainingSession:hasTrainUtterance:languageCode:payload:
endpointer:didDetectStartpointAtTime:
endpointer:didDetectHardEndpointAtTime:withMetrics:
voiceProfile
prepareWithCompletion:
_beginOfSpeechDetected
_endOfSpeechDetected
cleanupWithCompletion:
trainUtterance:shouldUseASR:completion:
cancelTrainingForID:
suspendAudio
setSuspendAudio:
startRMS
stopRMS
shouldPerformRMS
didDetectForceEndPoint
setRms:
speechRecognizerAvailable
audioFileWriter
setAudioFileWriter:
_performRMS
_locale
_audioSession
_audioAnalyzer
_keywordDetector
_trainingSessions
_currentTrainingSession
_sessionNumber
_suspendAudio
_cleanupCompletion
_speechRecognizer
_currentAsset
_profile
_didStopWaitingGroup
_speechRecognizerAvailable
_rms
_audioFileWriter
T@"CSPlainAudioFileWriter",&,N,V_audioFileWriter
T@"SSRVoiceProfile",R
Tf,V_rms
T@"<SSRVTUITrainingManagerDelegate>",W,N,V_delegate
TB,R,V_speechRecognizerAvailable
isSpeakerRecognitionAvailable
voiceRetrainersWithContext:
_audioRecorder
initWithSampleRate:
initWithQueue:error:
contextForVoiceTriggerTraining
setContext:error:
registerObserver:
activateAudioSessionWithReason:streamHandleId:error:
isRecordingWithStreamHandleId:
prepareAudioStreamRecordWithStreamHandleId:error:
startAudioStreamWithStreamHandleId:error:
stopAudioStreamWithStreamHandleId:error:
deactivateAudioSession:error:
unregisterObserver:
getAveragePowerDB
_hasCorrectInputAudioRoute
_hasCorrectOutputAudioRoute
recordRouteWithStreamHandleId:
playbackRoute
subdataWithRange:
processFloatBuffer:stride:inFrameToProcess:
processShortBuffer:stride:inFrameToProcess:
convertStopReason:
_handleDidStopWithReason:
audioRecorderBufferAvailable:audioStreamHandleId:buffer:remoteVAD:atTime:arrivalTimestampToAudioRecorder:numberOfChannels:
audioRecorderDidStartRecord:audioStreamHandleId:successfully:error:
audioRecorderDidStopRecord:audioStreamHandleId:reason:
audioRecorderRecordHardwareConfigurationDidChange:toConfiguration:
audioRecorderDisconnected:
setEndpointerDelegate:
resetEndPointer
hasAudioRoute
updateMeters
averagePower
powerMeter
setPowerMeter:
audioStreamHandleId
setAudioStreamHandleId:
_powerMeter
_audioStreamHandleId
T@"CSAudioPowerMeter",&,N,V_powerMeter
TQ,N,V_audioStreamHandleId
T@"<CSVTUIAudioSessionDelegate>",W,N,V_delegate
appDomain
siriProfileId
locale
userName
dateAdded
profilePitch
initWithVoiceProfile:
initWithSharedSiriId:languageCode:productCategory:version:
setAppDomain:
profileId
setProfileId:
languageCode
setLanguageCode:
productCategory
setProductCategory:
version
setVersion:
setDateAdded:
pitch
setPitch:
sharedSiriId
setSharedSiriId:
homeId
setHomeId:
setUserName:
_appDomain
_profileId
_languageCode
_productCategory
_version
_dateAdded
_pitch
_sharedSiriId
_homeId
_userName
T@"NSString",&,N,V_appDomain
T@"NSString",&,N,V_profileId
T@"NSString",&,N,V_languageCode
T@"NSString",&,N,V_productCategory
T@"NSNumber",&,N,V_version
T@"NSDate",&,N,V_dateAdded
T@"NSNumber",&,N,V_pitch
T@"NSString",&,N,V_sharedSiriId
T@"NSString",&,N,V_homeId
T@"NSString",&,N,V_userName
initWithLocale:asset:
computeTriggerConfidenceForAudio:withCompletion:
removeObject:
satVTImplicitThreshold
resourcePath
keywordDetectorNDAPIConfigFilePath
supportPremiumModel
VTSecondPassConfigPathRecognizerExistFrom:
keywordDetectorQuasarConfigFilePath
initWithLocale:configPath:resourcePath:
VTSecondPassRecognizerScoreScaleFactorFrom:
dataWithBytes:length:
streamAudioFromFileUrl:audioStreamBasicDescriptor:samplesPerStreamChunk:audioDataAvailableHandler:
bestScore
filterVTAudioFiles:withLocale:withAsset:
detectorNDAPI
setDetectorNDAPI:
detectorQuasar
setDetectorQuasar:
recognizerScoreScaleFactor
setRecognizerScoreScaleFactor:
_recognizerScoreScaleFactor
_detectorNDAPI
_detectorQuasar
T@"SSRTriggerPhraseDetectorNDAPI",&,N,V_detectorNDAPI
T@"SSRTriggerPhraseDetectorQuasar",&,N,V_detectorQuasar
Tf,N,V_recognizerScoreScaleFactor
getLocalUrl
string
_compatibilityVersion
stringValue
appendString:
appendFormat:
_footprint
assetForAssetType:resourcePath:configVersion:
attributes
objectForKey:
state
isPremium
getCSAssetOfType:
isDownloading
isCSAssetInstalled
canBePurged
isLatestCompareTo:
URLsForDirectory:inDomains:
lastObject
contentsOfDirectoryAtPath:error:
predicateWithFormat:
removeItemAtPath:
URLByAppendingPathComponent:
dataWithCapacity:
mutableBytes
sharedManager
installedAssetOfType:forLanguage:
assetProvider
satConfigFileNameForCSSpIdType:forModelType:forAssetType:
readJsonFileAtPath:
intValue
setWithObjects:
containsObject:
deviceCategoryForDeviceProductType:
getVoiceProfileProductCategoryFromVersionFilePath:
deviceCategoryStringRepresentationForCategoryType:
dataWithContentsOfURL:
baseDir
pathExtension
enumerateObjectsUsingBlock:
enumeratorAtPath:
getHomeUserIdForSharedUserId:completion:
userVoiceProfilesForAppDomain:
userVoiceProfilesForAppDomain:forLocale:
objectAtIndexedSubscript:
getExplicitEnrollmentUtterancesFromDirectory:
getImplicitEnrollmentUtterancesFromDirectory:
arrayByAddingObjectsFromArray:
_getUtterancesFromDirectory:
isUtteranceImplicitlyTrained:
getUtteranceEnrollmentType:
insertObject:atIndex:
addObjectsFromArray:
recordedTimeStampFromFileName:
compare:options:
recordedTimeStampOfFile:
moveItemAtPath:toPath:error:
addEntriesFromDictionary:
stringForCSSpIdType:
explicitSpIdTypeForSpId:
spIdTypeForString:
stringForSpeakerRecognizerType:
stringForVoiceProfileRetrainerType:
satConfigFileNameForCSSpIdType:
psrConfigFileNameForCSSpIdType:
spIdVoiceProfileImportRootDir
cleanupOrphanedVoiceIdGradingFiles
spidAudioTrainUtterancesDir
isSpeakerRecognitionSupportedInLocale:
getVoiceProfileIdentityFromVersionFilePath:
isCurrentDeviceCompatibleWithNewerVoiceProfileAt:
isCurrentDeviceCompatibleWithVoiceProfileAt:
getImplicitUtteranceCacheDirectory
getNumberOfAudioFilesInDirectory:
dumpFilesInDirectory:
getContentsOfDirectory:
getHomeUserIdForVoiceProfile:withCompletion:
getVoiceProfilesForSiriProfileId:
getVoiceProfileForSiriProfileId:forLanguageCode:
segmentVoiceTriggerFromAudioFile:withVTEventInfo:withStorePayloadPortion:withCompletion:
getEnrollmentUtterancesFromDirectory:
getExplicitMarkedEnrollmentUtterancesFromDirectory:
getEnrollmentUtterancesCountFromDirectory:withCountBlock:
moveContentsOfSrcDirectory:toDestDirectory:
encryptFileAt:andSaveTo:error:
getSiriLanguageWithFallback:
getSiriLanguageWithEndpointId:fallbackLanguage:
getAssetProviderType
installedAssetOfType:forLanguageCode:
allInstalledAssetsOfType:forLanguage:
reloadForLocale:
CVTThreshold
VTSecondPassCategoryForFirstPassSource:
VTSecondPassPreTriggerAudioTimeFrom:
inputRecordingDurationInSecs
initWithNumChannels:recordingDuration:samplingRate:
bestStart
bestEnd
samplesFed
unsignedIntValue
sampleCount
_sampleLengthFrom:To:
copySamplesFrom:to:
channelForProcessedInput
dataForChannel:
analyze:
triggeredUtterance:
_keywordAnalyzer
_lastKeywordScore
_keywordThreshold
_audioBuffer
initWithDeviceId:
waitingForConnection:error:
isConnected
startRecordingWithOptions:error:
stopRecording:
didPlayEndpointBeep
voiceTriggerEventInfo
hasPendingTwoShotBeep
T@"<CSRemoteRecordClientDelegate>",W,N,V_delegate
addUtterance:toProfile:
stringByAppendingFormat:
numberWithInt:
initWithEvent:locale:configVersion:
pushAnalyticsWithLazyBlock:
setVoiceProfilePruningFailureReasonCode:
setVoiceProfileUpdateScoreMSE:
setVoiceProfileDiscardedUtteranceCount:
setvoiceProfilePrunedUtteranceCount:
setVoiceProfileRetainedUtteranceCount:
appendVoiceProfileExplicitUtteranceScoreWith:
appendVoiceProfileImplicitUtteranceScoreWith:
appendVoiceProfileDiscardedImplicitUtteranceScoreWith:
appendVoiceProfileFailedExplicitUtteranceScoreWith:
setVoiceProfileRetrainingFailureReasonCode:
setRetrainingWaitTime:
voiceProfilePruningFailureReasonCode
voiceProfileUpdateScoreMSE
voiceProfileDiscardedUtteranceCount
voiceProfilePrunedUtteranceCount
setVoiceProfilePrunedUtteranceCount:
voiceProfileRetainedUtteranceCount
voiceProfileRetrainingFailureReasonCode
retrainingWaitTime
speakerRecognitionProcessingStatus
speakerRecognitionWaitTime
speakerRecognitionPSRProcessingStatus
setSpeakerRecognitionPSRProcessingStatus:
speakerRecognitionSATProcessingStatus
setSpeakerRecognitionSATProcessingStatus:
_eventString
_eventContext
explicitUtteranceIndex
explicitFailedUtteranceIndex
implicitUtteranceIndex
implicitDiscardedUtteranceIndex
_voiceProfileUpdateScoreMSE
_voiceProfilePruningFailureReasonCode
_voiceProfileDiscardedUtteranceCount
_voiceProfilePrunedUtteranceCount
_voiceProfileRetainedUtteranceCount
_voiceProfileRetrainingFailureReasonCode
_retrainingWaitTime
_speakerRecognitionProcessingStatus
_speakerRecognitionWaitTime
_speakerRecognitionPSRProcessingStatus
_speakerRecognitionSATProcessingStatus
TQ,N,V_voiceProfilePruningFailureReasonCode
Tf,N,V_voiceProfileUpdateScoreMSE
TQ,N,V_voiceProfileDiscardedUtteranceCount
TQ,N,V_voiceProfilePrunedUtteranceCount
TQ,N,V_voiceProfileRetainedUtteranceCount
TQ,N,V_voiceProfileRetrainingFailureReasonCode
Td,N,V_retrainingWaitTime
TQ,N,V_speakerRecognitionProcessingStatus
Td,N,V_speakerRecognitionWaitTime
TQ,N,V_speakerRecognitionPSRProcessingStatus
TQ,N,V_speakerRecognitionSATProcessingStatus
isRecordContextVoiceTrigger:
isRecordContextJarvisVoiceTrigger:
isRecordContextHearstDoubleTap:
isRecordContextRaiseToSpeak:
isRecordContextAutoPrompt:
isRecordContextHomeButtonPress:
isRecordContextSpeakerIdTrainingTrigger:
isRecordContextHearstVoiceTrigger:
isRecordContextJarvisButtonPress:
isValidRecordContext:
recordContextString:
isMarkedSATEnrolled
getExplicitEnrollmentUtterancesForType:
reverseObjectEnumerator
domain
code
provisionedVoiceProfilesForAppDomain:withLocale:
SSRSpeakerProfilesBasePath
initWithCapacity:
_cleanupAppDomain:
SSRBasePathForAppDomain:
_cleanuplanguageCodePath:forAppDomain:
voiceProfileForId:
_cleanupOrphanedMetafilesForProfile:payloadUtteranceLifeTimeInDays:
_cleanupImplicitUtteranceCacheForProfile:
voiceProfileImplicitCacheDirPath
voiceProfileIdentity
voiceProfileVersion
voiceProfileBasePath
_cleanupContentsOfSatFolder:
voiceProfileAudioDirPathForSpidType:
_cleanupOrphanedMetafilesAtURL:
_cleanupPayloadUtterancesFromProfile:forModelType:exceedingLifeTimeInDays:
attributesOfItemAtPath:error:
fileSize
_cleanupInvalidAudioFiles:
getImplicitEnrollmentUtterancesPriorTo:forType:
voiceProfileModelDirForSpidType:recognizerType:
_cleanupModelFilesAtDir:forAssetArray:
arrayWithCapacity:
hashFromResourcePath
filterDuplicatedSiriProfilesFrom:
filterInvalidSiriProfilesFrom:
cleanupProfileStore
cleanupInvalidModelsForProfile:withAssetArray:
decodeObjectOfClass:forKey:
encodeObject:forKey:
doubleValue
dateWithTimeIntervalSince1970:
timeIntervalSince1970
dictionaryWithObjectsAndKeys:
dictionaryRepresentation
addUtterances:spIdType:
copyItemAtURL:toURL:error:
getEnrollmentUtterancesForModelType:
_voiceProfilePathForSpidType:
_getProfileVersionFilePath
_updateVoiceProfileVersionFile
_markSATEnrollmentWithMarker:
_isSATMarkedWithMarker:
createFileAtPath:contents:attributes:
setValue:forKey:
writeToFile:options:error:
supportsSecureCoding
encodeWithCoder:
initWithCoder:
TB,R
initWithDictionary:
setSharedSiriProfileId:
voiceProfileModelFilePathForRecognizerType:spIdType:
importVoiceProfileAtPath:
getExplicitMarkedEnrollmentUtterancesForType:
getImplicitEnrollmentUtterancesForType:
profileLocallyAvailable
deleteModelForSpidType:recognizerType:
markSATEnrollmentSuccess
markSATEnrollmentMigrated
isMarkedSATMigrated
pruningCookie
updatePruningCookie:
profileBasePath
setProfileBasePath:
setProfilePitch:
_siriProfileId
_profileBasePath
_profilePitch
T@"NSString",&,N,V_profileBasePath
T@"NSString",R,N
T@"NSNumber",&,N,V_profilePitch
T@"NSString",R,N,V_locale
T@"NSString",R,N,V_appDomain
T@"NSDate",R,N,V_dateAdded
T@"NSString",R,N,V_siriProfileId
initWithWordCount:trailingSilenceDuration:eosLikelihood:pauseCounts:silencePosterior:taskName:processedAudioDurationInMilliseconds:
componentsJoinedByString:
numberWithInteger:
initWithWordCount:trailingSilenceFrames:endOfSilenceLikelihood:pauseCounts:silencePosterior:taskName:
wordCount
setWordCount:
trailingSilenceDuration
setTrailingSilenceDuration:
eosLikelihood
setEosLikelihood:
pauseCounts
setPauseCounts:
silencePosterior
setSilencePosterior:
processedAudioDurationInMilliseconds
setProcessedAudioDurationInMilliseconds:
taskName
setTaskName:
_wordCount
_trailingSilenceDuration
_eosLikelihood
_pauseCounts
_silencePosterior
_processedAudioDurationInMilliseconds
_taskName
Tq,N,V_wordCount
Tq,N,V_trailingSilenceDuration
Td,N,V_eosLikelihood
T@"NSArray",C,N,V_pauseCounts
Td,N,V_silencePosterior
Tq,N,V_processedAudioDurationInMilliseconds
T@"NSString",C,N,V_taskName
lowercaseString
hasPrefix:
substringFromIndex:
replaceMatchesInString:options:range:withTemplate:
_stringByStrippingLeadingNoise:
_stringByStrippingTrailingNoise:
rangeOfString:options:
_firstMatchesForRegularExpression:
firstMatchInString:options:range:
array
numberOfRanges
rangeAtIndex:
substringWithRange:
_stringByFixingNamePattern:
_stringByStrippingNoiseLeadingNoise:TrailingNoise:
_hasSubstring:
_matchesRegularExpression:
_caseInsensitiveHasMatchInEnumeration:
_firstMatchesForRegularExpressions:
pruneVoiceProfile:forSpIdType:withAsset:
psrCombinationWeight
boolValue
initWithConfigFilePath:withModelPath:withCompareModelFilePaths:
maxAllowedEnrollmentUtterances
initWithVoiceRetrainingContext:error:
compareVoiceProfileArray
setCompareVoiceProfileArray:
setVoiceProfile:
spIdType
resourceFilePath
filterToVoiceTriggerUtterances
forceRetrain
maxAllowedSpeakerVectors
modelsContext
asset
setAsset:
setLogAggregator:
_filterToVoiceTriggerUtterances
_forceRetrain
_combinationWeight
_compareVoiceProfileArray
_voiceProfile
_spIdType
_resourceFilePath
_configVersion
_maxAllowedSpeakerVectors
_modelsContext
_asset
_logAggregator
T@"NSArray",&,N,V_compareVoiceProfileArray
T@"SSRVoiceProfile",&,N,V_voiceProfile
TQ,R,N,V_spIdType
T@"NSURL",R,N,V_resourceFilePath
T@"NSString",R,N,V_configVersion
TB,R,N,V_filterToVoiceTriggerUtterances
TB,R,N,V_forceRetrain
TQ,R,N,V_maxAllowedSpeakerVectors
T@"NSDictionary",R,N,V_modelsContext
Tf,R,N,V_combinationWeight
T@"CSAsset",&,N,V_asset
T@"SSRLoggingAggregator",&,N,V_logAggregator
T@"NSString",R,N,V_sessionId
configFilePath
voiceProfileModelFilePath
compareModelFilePaths
_configFilePath
_voiceProfileModelFilePath
_compareModelFilePaths
T@"NSURL",R,N,V_configFilePath
T@"NSURL",R,N,V_voiceProfileModelFilePath
T@"NSDictionary",R,N,V_compareModelFilePaths
_getBaseMetaDictionaryForUtterancePath:
timeStampWithSaltGrain
_writeMetaDict:forUtterancePath:
stringByReplacingOccurrencesOfString:withString:
dateFromString:
saveUtteranceMetadataForUtterance:enrollmentType:isHandheldEnrollment:triggerSource:audioInput:otherBiometricResult:containsPayload:
matchWithString:TrailingStr:LeadingStr:Pattern:
isAvailable
_firedVoiceTriggerTimeout
shouldHandleSession
shouldMatchPayload
finishSpeechRecognitionTask
closeSessionWithStatus:successfully:
_firedEndPointTimeout
updateMeterAndForward
pushAudioInputIntoPCMBuffer:
setupSpeechRecognitionTaskWithVoiceTriggerEventInfo:
feedSpeechRecognitionWithPCMBuffer
_registerVoiceTriggerTimeout
handleAudioBufferForVTWithAudioInput:withDetectedBlock:
handleAudioInput:
_reportStopListening
_registerEndPointTimeout
_registerForceEndPointTimeout
requestTriggeredUtterance:
formattedString
whitespaceAndNewlineCharacterSet
stringByTrimmingCharactersInSet:
matchRecognitionResult:withMatchedBlock:withNonMatchedBlock:
bestTranscription
sharedGrammars
getTrailingPatternsForUtt:Locale:
getLeadingPatternsForUtt:Locale:
getRegexPatternsForUtt:Locale:
speechRecognitionDidDetectSpeech:
speechRecognitionTask:didHypothesizeTranscription:
speechRecognitionTask:didFinishRecognition:
speechRecognitionTaskFinishedReadingAudio:
speechRecognitionTaskWasCancelled:
speechRecognitionTask:didFinishSuccessfully:
setVoiceTriggerEventInfo:
_detectBOS
_ASRResultReceived
_reportedStopListening
_utteranceStored
_numSamplesFed
_bestTriggerSampleStart
_voiceTriggerEventInfo
T@"NSDictionary",&,N,V_voiceTriggerEventInfo
setupPhraseSpotter
startMasterTimerWithTimeout:
resultAlreadyReported
stopMasterTimer
closeSessionWithCompletion:
removeAllObjects
trimBeginingOfPCMBufferWithVoiceTriggerEventInfo:
computeRequiredTrailingSamples
feedSpeechRecognitionTrailingSamplesWithCompletedBlock:
numSamplesInPCMBuffer
appendAudioPCMBuffer:
removeObjectAtIndex:
frameLength
initWithCommonFormat:sampleRate:channels:interleaved:
initWithPCMFormat:frameCapacity:
mutableAudioBufferList
setFrameLength:
replaceObjectAtIndex:withObject:
removeObjectsInRange:
createAVAudioPCMBufferWithNSData:
getLMEforLocale:
setContextualStrings:
setTaskHint:
_setVoiceTriggerEventInfo:
recognitionTaskWithRequest:delegate:
finish
handleMasterTimeout:
scheduledTimerWithTimeInterval:target:selector:userInfo:repeats:
invalidate
_status
_utteranceId
_speechRecognitionRequest
_speechRecognitionTask
_masterTimer
_pcmBufArray
_resultReported
_sessionProcess
_sessionSuspended
_ASRErrorOccured
_sessionDelegate
_trainingCompletion
_numRequiredTrailingSamples
_numTrailingSamples
setCurrentDeviceCategory:
discardSiriEnrollmentForProfileId:forLanguageCode:
_getVoiceProfilesForSiriProfileId:withLanguageCode:
deleteUserVoiceProfile:
updateVoiceProfile:withUserName:
_markVoiceProfileTrainingSyncForLanguage:
addUserVoiceProfile:withContext:withCompletion:
initWithDescription:
route
type
satImplicitTrainingEnabled
isIOSDeviceSupportingBargeIn
fileURLWithPathComponents:
unsignedLongLongValue
getLastBiometricMatchForVoiceTriggerTimeStamp:
defaultCenter
postNotificationName:object:
addImplicitUtterance:toVoiceProfile:withAsset:withTriggerSource:withAudioInput:withCompletion:
_CSSATDownloadPath
_getUserVoiceProfileDownloadCacheDirectoryWithUpdatePath:
_downloadAndEnrollVoiceProfileForProfileId:withDownloadTriggerBlock:
_enrollVoiceProfileForSiriProfileId:fromCacheDirectoryPath:withCategoryType:
_downloadVoiceProfileForProfileId:forDeviceCategory:withDownloadTriggerBlock:withCompletion:
_getUserVoiceProfileDownloadCacheDirectoryForProfileId:forDeviceCategory:forVoiceProfileVersion:
checkIfVoiceProfile:needsUpdatedWith:withCategory:
_enableVoiceTriggerIfLanguageMatches:
_CSSATUploadPathForSiriProfileId:
_prepareVoiceProfileWithSiriProfileId:withUploadBlock:
_CSSATLegacyUploadPath
_getVoiceProfilePathsToBeUploadedForSiriProfileId:
_isDirectory:
copyItemAtPath:toPath:error:
_copyExplicitEnrollmentFilesFromPath:toPath:withCompletion:
_copyVoiceProfileAtPath:toPath:
_isMarkedForVoiceProfileTrainingSyncForLanguage:
getDevicesWithAvailablePHSAssetsOnDeviceCheck:
devicesWithVoiceProfileIniCloudForLanguage:
getDevicesWithAvailablePHSAssetsForLanguage:completion:
userVoiceProfilesForLocale:
userVoiceProfileForVoiceProfileID:
migrateVoiceProfilesIfNeededWithCompletionBlock:
cleanupDuplicatedProfiles
cleanupVoiceProfileStore:
cleanupVoiceProfileModelFilesForLocale:
retrainVoiceProfile:withContext:withCompletion:
sharedStorePrefs
getVoiceProfileStoreVersion
_isLegacyEnrollmentMarkedWith:forLanguageCode:
_CSSATCachePath
getSATEnrollmentPath
modelDirectoryPathForProfile:
discardSiriEnrollmentForLanguageCode:
newVoiceProfileWithLocale:withAppDomain:
addUtterances:toProfile:withContext:withCompletion:
notifyImplicitTrainingUtteranceAvailable:forVoiceProfileId:withRecordDeviceInfo:withRecordCtxt:withVoiceTriggerCtxt:withOtherCtxt:withCompletion:
getUserVoiceProfileUpdateDirectory
notifyUserVoiceProfileDownloadReadyForUser:getData:completion:
notifyUserVoiceProfileUpdateReady
notifyUserVoiceProfileUploadCompleteForSiriProfileId:withError:
uploadUserVoiceProfileForSiriProfileId:withUploadTrigger:completion:
getUserVoiceProfileUploadPathWithEnrolledLanguageList:
notifyUserVoiceProfileUploadComplete
getCachedVoiceProfileAvailabilityMetaBlob
hasVoiceProfileIniCloudForLanguageCode:withBackupMetaBlob:
isVoiceProfileUploadedToiCloudForLanguageCode:withCompletionBlock:
hasVoiceProfileIniCloudForLanguageCode:
enableVoiceTriggerUponVoiceProfileSyncForLanguage:
provisionedVoiceProfilesForLocale:
getVoiceProfileAnalyticsForAppDomain:withLocale:
triggerVoiceProfileMigrationWithCompletion:
triggerVoiceProfileDuplicatesCleanup
triggerVoiceProfileCleanupWithCompletion:
pruneImplicitUtterancesOfProfile:withAsset:
triggerRetrainingVoiceProfile:withContext:withCompletion:
markSATEnrollmentSuccessForVoiceProfile:
isSATEnrolledForSiriProfileId:forLanguageCode:
isSATEnrollmentMigratedForSiriProfileId:forLanguageCode:
_isRemoteVoiceTriggerAvailable
deleteAllVoiceProfilesForAppDomain:
currentDeviceCategory
_currentDeviceCategory
TQ,N,V_currentDeviceCategory
stringWithUTF8String:
handleFailureInFunction:file:lineNumber:description:
endpointStyle
delay
setDelay:
startWaitTime
automaticEndpointingSuspensionEndTime
setAutomaticEndpointingSuspensionEndTime:
minimumDurationForEndpointer
setMinimumDurationForEndpointer:
lastEndOfVoiceActivityTime
lastStartOfVoiceActivityTime
bypassSamples
setBypassSamples:
endpointMode
setEndpointMode:
interspeechWaitTime
endWaitTime
saveSamplesSeenInReset
setSaveSamplesSeenInReset:
Tq,N
Td,N
Td,R,N
TB,N
stopEndpointer
recordingStoppedForReason:
trailingSilenceDurationAtEndpoint
implDelegate
setImplDelegate:
canProcessCurrentRequest
activeChannel
setActiveChannel:
processServerEndpointFeatures:
updateEndpointerThreshold:
updateEndpointerDelayedTrigger:
shouldAcceptEagerResultForDuration:resultsCompletionHandler:
logFeaturesWithEvent:locale:
handleVoiceTriggerWithActivationInfo:
endpointerModelVersion
elapsedTimeWithNoSpeech
T@"<CSEndpointAnalyzerDelegate>",W,N
T@"<CSEndpointAnalyzerImplDelegate>",W,N
TQ,N
T@"<CSEndpointAnalyzerDelegate>",W,N,Vdelegate
T@"<CSEndpointAnalyzerImplDelegate>",W,N,VimplDelegate
TB,R,N,VcanProcessCurrentRequest
TQ,N,VactiveChannel
Tq,N,VendpointStyle
Td,N,Vdelay
Td,N,VstartWaitTime
Td,N,VautomaticEndpointingSuspensionEndTime
Td,N,VminimumDurationForEndpointer
Td,R,N,VlastEndOfVoiceActivityTime
Td,R,N,VlastStartOfVoiceActivityTime
initWithRecordType:deviceId:
setAlwaysUseRemoteBuiltInMic:
contextForServerInvoke
recordTypeFromAVVCActivationMode:
setType:
copyWithZone:
setDeviceId:
setIsRequestDuringActiveCall:
_createAVVCContextWithType:deviceId:
avvcActivationMode:
deviceId
isBuiltInVoiceTriggered
isHearstVoiceTriggered
isJarvisVoiceTriggered
isHearstDoubleTapTriggered
recordTypeString:
contextForHearstVoiceTriggerWithDeviceId:
contextForOpportuneSpeakerListener
contextForOpportuneSpeakerListenerWithCall
contextForBuiltInVoiceTrigger
contextForJarvisWithDeviceId:
contextForBTLEWithDeviceId:
contextForHomeButton
defaultContext
initWithXPCObject:
initWithAVVCContext:
isVoiceTriggered
isTriggeredFromHearst
isRTSTriggered
isHomePressed
isServerInvoked
isStarkTriggered
isDictation
recordSourceTypeFromAudioRecordType
xpcObject
alwaysUseRemoteBuiltInMic
_alwaysUseRemoteBuiltInMic
_isRequestDuringActiveCall
_type
_deviceId
Tq,N,V_type
T@"NSString",&,N,V_deviceId
TB,N,V_alwaysUseRemoteBuiltInMic
TB,N,V_isRequestDuringActiveCall
T@"NSObject<OS_xpc_object>",R,N
activationMode
activationDeviceUID
announceCallsEnabled
streamID
startHostTime
startAlert
stopAlert
stopOnErrorAlert
skipAlert
_voiceControllerWithError:
weakObjectsHashTable
_destroyVoiceController
setRecordDelegate:
initWithError:
setSynchronousCallbackEnabled:
setContextForStream:forStream:error:
_getRecordSettingsWithRequest
_createDeInterleaverIfNeeded
inputRecordingBufferDuration
initWithStreamID:settings:bufferDuration:
setMeteringEnabled:
prepareRecordForStream:error:
initWithStreamID:atStartHostTime:
setSkipAlert:
setStartAlert:
setStopAlert:
setStopOnErrorAlert:
startRecordForStream:error:
stopRecordForStream:error:
hasRemoteBuiltInMic
getCurrentStreamState:
getRecordDeviceInfoForStream:
recordRoute
activateAudioSessionForStream:isPrewarm:recordMode:error:
_convertDeactivateOption:
deactivateAudioSessionWithOptions:
streamDescription
_deinterleaveBufferIfNeeded:force:
_compensateChannelDataIfNeeded:receivedNumChannels:
timeStamp
_processAudioChain:audioStreamHandleId:remoteVAD:atTime:arrivalTimestampToAudioRecorder:numberOfChannels:
initWithLength:
replaceBytesInRange:withBytes:
_audioRecorderDidStartRecordingSuccessfully:streamHandleID:error:
bytesDataSize
_processAudioBuffer:audioStreamHandleId:arrivalTimestampToAudioRecorder:
_audioRecorderDidStopRecordingForReason:streamHandleID:
channels
initWithBytes:length:
shouldDeinterleaveAudioOnCS
numberWithUnsignedInt:
voiceControllerDidStartRecording:successfully:
voiceControllerDidStartRecording:successfully:error:
voiceControllerDidStopRecording:forReason:
voiceControllerDidDetectStartpoint:
voiceControllerDidDetectEndpoint:ofType:
voiceControllerDidDetectEndpoint:ofType:atTime:
voiceControllerEncoderErrorDidOccur:error:
voiceControllerDidFinishAlertPlayback:ofType:error:
voiceControllerDidFinishAlertPlayback:withSettings:error:
voiceControllerRecordHardwareConfigurationDidChange:toConfiguration:
voiceControllerBeginRecordInterruption:
voiceControllerBeginRecordInterruption:withContext:
voiceControllerEndRecordInterruption:
voiceControllerMediaServicesWereLost:
voiceControllerMediaServicesWereReset:
voiceControllerWillSetAudioSessionActive:willActivate:
voiceControllerDidSetAudioSessionActive:isActivated:
voiceControllerRecordBufferAvailable:buffer:
voiceControllerLPCMRecordBufferAvailable:buffer:
voiceControllerWirelessSplitterRouteAvailable:devices:
voiceControllerDidStartRecording:forStream:successfully:error:
voiceControllerDidStopRecording:forStream:forReason:
voiceControllerAudioCallback:forStream:buffer:
voiceControllerStreamInvalidated:forStream:
setAudioServerCrashEventDelegate:
setAudioSessionEventDelegate:
setCurrentContext:streamHandleId:error:
_shouldUseRemoteRecordForContext:
_shouldUseRemoteBuiltInMic:
voiceControllerCreationQueue
setVoiceControllerCreationQueue:
observers
setObservers:
_voiceController
_deinterleaver
_interleavedABL
_pNonInterleavedABL
_remoteRecordClient
_latestContext
_shouldUseRemoteRecord
_waitingForDidStart
_voiceControllerCreationQueue
_observers
T@"NSObject<OS_dispatch_queue>",&,N,V_voiceControllerCreationQueue
T@"NSHashTable",&,N,V_observers
_latestVersionedAssetOfType:fromProviders:forLocale:
scannerWithString:
scanFloat:
_convertVersionStringToFloat:
assetProviders
setAssetProviders:
currentLanguageCode
setCurrentLanguageCode:
_assetProviders
_currentLanguageCode
T@"NSArray",&,N,V_assetProviders
T@"NSString",&,N,V_currentLanguageCode
T@"<SSRAssetManagerDelegate>",W,N,V_delegate
speakerRecognitionController:hasSpeakerInfo:
speakerRecognitionFinishedProcessing:withFinalSpeakerInfo:
processAudio:withNumberOfSamples:
getLatestSpeakerInfo
orchestrator
setOrchestrator:
setLastScoreCard:
_orchestrator
_lastScoreCard
T@"<SSRSpeakerRecognitionControllerDelegate>",W,N,V_delegate
T@"SSRSpeakerRecognitionOrchestrator",&,N,V_orchestrator
T@"NSDictionary",&,N,V_lastScoreCard
supportsSpeakerRecognitionAssets
_installedMobileAssetOfType:forLanguage:
_buildAssetQueryForAssetType:
returnTypes:
queryMetaDataSync
results
_filteredAssets:forLanguage:
queryParams
_getSSRAssetTypeString
_getSSRAssetCurrentCompatibilityVersion
_getVoiceTriggerAssetTypeString
_getVoiceTriggerAssetCurrentCompatibilityVersion
_getEndpointAssetTypeString
_getEndpointAssetCurrentCompatibilityVersion
initWithType:
addKeyValuePair:with:
_findLatestInstalledAsset:
valueForKey:
initStore
_loadVoiceProfiles
_updateTrainedUsersWithAction:UserVoiceProfile:
_deleteUserVoiceProfile:
_synchronizeSiriVoiceProfilesWithAssistant
_retrainLiveOnOnboardedProfilesForLanguage:withForceRetrain:withCompletion:
voiceTriggerEnabled
_logVoiceProfileConfusionWithCleanup:
satImplicitProfileThreshold
satImplicitProfileDeltaThreshold
evaluateImplicitAdditionPolicyWithScores:forProfile:withImplicitThreshold:withDeltaThreshold:
_getTopScoringProfileIdFromScores:
_retrainVoiceProfile:withContext:
getPitchForUtteranceAudioFiles:
_enrolledVoiceProfiles
loadKnownUserVoiceProfiles
_saveTrainedUsers:
setVoiceProfileStoreVersion:
saveKnownUserVoiceProfiles:
setWithArray:
minusSet:
allObjects
_retrainVoiceProfile:withContext:withUtterances:
logVoiceProfileConfusionWithCleanup:
_checkIfRetrainingRequiredForProfile:
copyAudioFiles:toProfile:forModelType:
voiceProfileArray
setVoiceProfileArray:
storePrefs
setStorePrefs:
_voiceProfileArray
_storePrefs
T@"NSMutableArray",&,V_voiceProfileArray
T@"SSRVoiceProfileStorePrefs",&,N,V_storePrefs
iterateBitset:block:
numberOfChannels
selectedChannelList
_numberOfChannels
TI,R,N,V_numberOfChannels
saveMetadata:isExplicitEnrollment:
saveUtterance:utteranceAudioPath:numSamplesToWrite:isExplicitEnrollment:
writeMetaDict:atMetaPath:
saveRawUtteranceAndMetadata:to:isExplicitEnrollment:
saveUtteranceAndMetadata:atDirectory:isExplicitEnrollment:
pickAssetForProfiles:forSpIdType:withAssetArray:
pickAssetForProfiles:forSpIdType:
composeModelContextsForProfiles:forSpIdType:forAsset:completion:
speakerIdScoreReportingType
initWithConfigFilePath:withModelFilePaths:
_checkIfModelsPresentForProfiles:forSpIdType:forAsset:
initWithVoiceRecognitionContext:error:
setSpIdType:
vadResourcePath
expModelsContext
_osTransactionReqd
_activeChannel
_scoreType
_recognitionStyle
_vtEventInfo
_vadResourcePath
_expModelsContext
_maxAllowedAudioSamples
_debugUtteranceAudioFile
_debugUtteranceMetaFile
T@"NSArray",&,N,V_voiceProfileArray
TQ,N,V_spIdType
T@"NSString",&,N,V_locale
TQ,R,N,V_activeChannel
TQ,R,N,V_scoreType
TQ,R,N,V_recognitionStyle
T@"NSDictionary",R,N,V_vtEventInfo
T@"NSURL",R,N,V_vadResourcePath
T@"NSDictionary",R,N,V_expModelsContext
TQ,R,N,V_maxAllowedAudioSamples
TB,R,N,V_osTransactionReqd
T@"NSString",R,N,V_debugUtteranceAudioFile
T@"NSString",R,N,V_debugUtteranceMetaFile
voiceProfilesModelFilePaths
_voiceProfilesModelFilePaths
T@"NSDictionary",R,N,V_voiceProfilesModelFilePaths
createGrammars
bundleForClass:
bundlePath
_getTrailingPatternsWithGrammars:withLocale:
_getLeadingPatternsWithGrammars:withLocale:
_getRegexPatternsWithGrammars:withUtt:withLocale:
_getLMEWithGrammar:withLocale:
URLSession:didBecomeInvalidWithError:
URLSession:didReceiveChallenge:completionHandler:
URLSessionDidFinishEventsForBackgroundURLSession:
_grammar
_processAudioFileURL:
_getVoicingProbFromRawData:
_getPitchHzFromRawData:
getVoiceTriggerProfilesAESKey
generateIfNecessaryVoiceTriggerProfilesAESKey
generateIfNecessaryAESKeyWithKeySizeInBits:applicationTag:keyLabel:shouldGenerateIfNecessary:
generateAESKeyWithKeySizeInBits:
storeAESKeyInKeychain:applicationTag:keyLabel:
getAESKeyFromKeychainWithApplicationTag:keyLabel:
deleteAESKeyWithApplicationTag:keyLabel:
getKeychainAttributesForAESKeyWithApplicationTag:keyLabel:
containsCategory:
getNumberForKey:category:default:
satScoreThreshold
getStringForKey:category:default:
getBoolForKey:category:default:
containsSpeakerRecognitionCategory
multiUserLowScoreThreshold
multiUserHighScoreThreshold
multiUserConfidentScoreThreshold
multiUserDeltaScoreThreshold
pruningExplicitUttThresholdSAT
pruningExplicitUttThresholdPSR
pruningThresholdSAT
pruningThresholdPSR
pruningNumRetentionUtterance
voiceProfilePruningCookie
Tf,R,N
Tq,R,N
v32@0:8@16@24
B24@0:8@16
#16@0:8
@16@0:8
@24@0:8:16
@32@0:8:16@24
@40@0:8:16@24@32
B16@0:8
B24@0:8#16
B24@0:8:16
Vv16@0:8
Q16@0:8
^{_NSZone=}16@0:8
B24@0:8@"Protocol"16
@"NSString"16@0:8
B32@0:8r^v16Q24
@24@0:8@16
@104@0:8@16{AudioStreamBasicDescription=dIIIIIIII}24{AudioStreamBasicDescription=dIIIIIIII}64
v16@0:8
B32@0:8r^v16q24
^{OpaqueExtAudioFile=}
{AudioStreamBasicDescription="mSampleRate"d"mFormatID"I"mFormatFlags"I"mBytesPerPacket"I"mFramesPerPacket"I"mBytesPerFrame"I"mChannelsPerFrame"I"mBitsPerChannel"I"mReserved"I}
@"NSURL"
@32@0:8@16@24
v32@0:8@16Q24
v24@0:8@16
@96@0:8{AudioStreamBasicDescription=dIIIIIIII}16{AudioStreamBasicDescription=dIIIIIIII}56
@104@0:8{AudioStreamBasicDescription=dIIIIIIII}16{AudioStreamBasicDescription=dIIIIIIII}56@96
@24@0:8Q16
@112@0:8@16@24{AudioStreamBasicDescription=dIIIIIIII}32{AudioStreamBasicDescription=dIIIIIIII}72
v20@0:8f16
v24@0:8Q16
@40@0:8@16@24@32
v36@0:8@16@24f32
v40@0:8@16@24Q32
v40@0:8@16@24@?32
@48@0:8@16@24@32^@40
v32@0:8@"SSRSpeakerAnalyzerPSR"16@"NSDictionary"24
@32@0:8@"SSRSpeakerRecognitionContext"16@"<SSRSpeakerRecognizerDelegate>"24
v32@0:8@"NSData"16Q24
v24@0:8@"SSRSpeakerRecognitionContext"16
@"NSDictionary"16@0:8
v20@0:8B16
@"SSRSpeakerRecognitionContext"
@"NSString"
@"NSDictionary"
@"NSObject<OS_dispatch_queue>"
@"<SSRSpeakerRecognizerDelegate>"
@"SSRSpeakerAnalyzerPSR"
@40@0:8@16@24Q32
@32@0:8@16Q24
v32@0:8@"<SSRSpeakerRecognizer>"16@"NSDictionary"24
v32@0:8@"SSRVoiceActivityDetector"16Q24
@40@0:8@16@24^@32
v28@0:8@16B24
@40@0:8@16@24d32
@"<SSRSpeakerRecognitionOrchestratorDelegate>"
@"<CSAudioFileWriter>"
@"<SSRSpeakerRecognizer>"
@"SSRVoiceActivityDetector"
@"NSObject<OS_os_transaction>"
v40@0:8@16@?24@?32
@24@0:8@?16
@24@0:8@"SSRVoiceProfileRetrainingContext"16
v40@0:8@"NSArray"16@?<@"NSError"@?@"NSURL"@"NSDictionary">24@?<v@?@"NSError"@"NSDictionary"@"NSDictionary">32
B24@0:8@"NSArray"16
@"NSError"24@0:8@?<B@?@"NSDictionary">16
@"NSURL"16@0:8
{AudioStreamBasicDescription=dIIIIIIII}16@0:8
{AudioStreamBasicDescription=dIIIIIIII}24@0:8f16I20
@48@0:8@16@24@32Q40
@56@0:8@16@24@32@40Q48
f40@0:8@16Q24Q32
v20@0:8i16
B44@0:8@16@24@32B40
B44@0:8@"CSVTUITrainingSession"16@"NSData"24@"NSString"32B40
v28@0:8B16@20
v24@0:8q16
v28@0:8B16@"NSError"20
v24@0:8@"NSData"16
v24@0:8@"NSError"16
v32@0:8@16d24
v40@0:8@16d24@32
v32@0:8@"<CSEndpointAnalyzer>"16d24
v40@0:8@"<CSEndpointAnalyzer>"16d24@"CSEndpointerMetrics"32
v24@0:8@?16
q36@0:8q16B24@?28
B24@0:8q16
v32@0:8i16B20@?24
f16@0:8
@"<CSVTUIAudioSession>"
@"CSNNVADEndpointAnalyzer"
@"CSVTUIKeywordDetector"
@"NSMutableArray"
@"CSVTUITrainingSession"
@"SFSpeechRecognizer"
@"CSAsset"
@"SSRVoiceProfile"
@"CSDispatchGroup"
@"<SSRVTUITrainingManagerDelegate>"
@"CSPlainAudioFileWriter"
v68@0:8@16Q24@32@40Q48Q56i64
v44@0:8@16Q24B32@36
v40@0:8@16Q24q32
v32@0:8@16q24
v68@0:8@"CSVTUIAudioRecorder"16Q24@"NSData"32@"NSData"40Q48Q56i64
v44@0:8@"CSVTUIAudioRecorder"16Q24B32@"NSError"36
v40@0:8@"CSVTUIAudioRecorder"16Q24q32
v32@0:8@"CSVTUIAudioRecorder"16q24
v24@0:8@"CSVTUIAudioRecorder"16
v24@0:8@"<CSVTUIAudioSessionDelegate>"16
v24@0:8@"<Endpointer>"16
q24@0:8q16
@"CSVTUIAudioRecorder"
@"<CSVTUIAudioSessionDelegate>"
@"CSAudioPowerMeter"
@48@0:8@16@24@32@40
@"NSNumber"
@"NSDate"
v32@0:8@16@?24
@"SSRTriggerPhraseDetectorNDAPI"
@"SSRTriggerPhraseDetectorQuasar"
Q24@0:8Q16
Q24@0:8@16
@40@0:8Q16Q24Q32
v80@0:8@16{AudioStreamBasicDescription=dIIIIIIII}24Q64@?72
q24@0:8@16
v44@0:8@16@24B32@?36
B40@0:8@16@24^@32
@36@0:8@16@24f32
@32@0:8Q16@24
@"CSAsset"32@0:8Q16@"NSString"24
@"NSArray"32@0:8Q16@"NSString"24
Q24@0:8I16I20
@"CSAudioCircularBuffer"
B32@0:8d16^@24
B32@0:8@16^@24
B24@0:8^@16
@"<CSRemoteRecordClientDelegate>"
B32@0:8@16@24
B40@0:8@16@24@32
v24@0:8d16
d16@0:8
@"NSMutableDictionary"
@32@0:8@16q24
@40@0:8@16Q24q32
v24@0:8@"NSCoder"16
@24@0:8@"NSCoder"16
@32@0:8Q16Q24
B32@0:8Q16Q24
@72@0:8q16q24d32@40d48@56q64
@64@0:8q16q24d32@40d48@56
q16@0:8
@"NSArray"
v40@0:8@16Q24@32
@32@0:8@16^@24
@"SSRLoggingAggregator"
v64@0:8@16@24B32@36@44Q52B60
q48@0:8@16@24@32@40
v24@0:8@"SFSpeechRecognitionTask"16
v32@0:8@"SFSpeechRecognitionTask"16@"SFTranscription"24
v32@0:8@"SFSpeechRecognitionTask"16@"SFSpeechRecognitionResult"24
v28@0:8@"SFSpeechRecognitionTask"16B24
v24@0:8i16B20
@96@0:8q16q24@32@40@48@56@64@72@80@?88
@"SFSpeechAudioBufferRecognitionRequest"
@"SFSpeechRecognitionTask"
@"NSTimer"
@"<CSVTUITrainingSessionDelegate>"
v48@0:8@16@24@32@?40
v72@0:8@16@24@32@40@48@56@?64
@32@0:8@16@?24
v48@0:8@16Q24@?32@?40
@40@0:8@16Q24Q32
@24@0:8^@16
v40@0:8Q16@24@32
v32@0:8d16@?24
v40@0:8Q16@"CSAudioRecordContext"24@"NSDictionary"32
v24@0:8@"CSAudioChunk"16
@"<CSEndpointAnalyzerDelegate>"16@0:8
v24@0:8@"<CSEndpointAnalyzerDelegate>"16
@"<CSEndpointAnalyzerImplDelegate>"16@0:8
v24@0:8@"<CSEndpointAnalyzerImplDelegate>"16
v24@0:8@"CSServerEndpointFeatures"16
v32@0:8d16@?<v@?B@"NSArray">24
v32@0:8@"NSString"16@"NSString"24
v24@0:8@"NSDictionary"16
@"<CSEndpointAnalyzerDelegate>"
@"<CSEndpointAnalyzerImplDelegate>"
@24@0:8q16
@24@0:8^{_NSZone=}16
@32@0:8q16@24
v36@0:8@16B24@28
v28@0:8@16i24
v36@0:8@16i24d28
v36@0:8@16i24@28
v40@0:8@16@24@32
v28@0:8@"AVVoiceController"16B24
v36@0:8@"AVVoiceController"16B24@"NSError"28
v32@0:8@"AVVoiceController"16q24
v24@0:8@"AVVoiceController"16
v28@0:8@"AVVoiceController"16i24
v36@0:8@"AVVoiceController"16i24d28
v32@0:8@"AVVoiceController"16@"NSError"24
v36@0:8@"AVVoiceController"16i24@"NSError"28
v40@0:8@"AVVoiceController"16@"AVVCAlertInformation"24@"NSError"32
v32@0:8@"AVVoiceController"16@"NSDictionary"24
v32@0:8@"AVVoiceController"16@"AVVCAudioBuffer"24
v28@0:8B16@"NSArray"20
v44@0:8@"AVVoiceController"16Q24B32@"NSError"36
v40@0:8@"AVVoiceController"16Q24q32
v40@0:8@"AVVoiceController"16Q24@"AVVCAudioBuffer"32
v32@0:8@"AVVoiceController"16Q24
v24@0:8@"<CSAudioServerCrashEventProvidingDelegate>"16
v24@0:8@"<CSAudioSessionEventProvidingDelegate>"16
Q32@0:8@16^@24
B40@0:8@16Q24^@32
B32@0:8Q16^@24
B24@0:8Q16
B40@0:8Q16Q24^@32
v60@0:8@16Q24@32Q40Q48i56
v40@0:8@16Q24Q32
@28@0:8@16I24
v36@0:8B16Q20@28
v32@0:8q16Q24
@28@0:8@16B24
@"AVVoiceController"
^{OpaqueAudioConverter=}
{AudioBufferList="mNumberBuffers"I"mBuffers"[1{AudioBuffer="mNumberChannels"I"mDataByteSize"I"mData"^v}]}
^{AudioBufferList=I[1{AudioBuffer=II^v}]}
@"CSRemoteRecordClient"
@"NSHashTable"
f24@0:8@16
@40@0:8Q16@24@32
@"<SSRAssetManagerDelegate>"
v32@0:8@"SSRSpeakerRecognitionOrchestrator"16@"NSDictionary"24
@"<SSRSpeakerRecognitionControllerDelegate>"
@"SSRSpeakerRecognitionOrchestrator"
v64@0:8@16@24@32@40@48@?56
B40@0:8@16@24f32f36
B40@0:8@16@24Q32
v32@0:8Q16@24
v36@0:8@16B24@?28
@"SSRVoiceProfileStorePrefs"
@112@0:8@16{AudioStreamBasicDescription=dIIIIIIII}24{AudioStreamBasicDescription=dIIIIIIII}64Q104
I16@0:8
v36@0:8@16@24B32
B36@0:8@16@24B32
B44@0:8@16@24Q32B40
B28@0:8@16B24
v48@0:8@16Q24@32@?40
B40@0:8@16Q24@32
@40@0:8@16Q24@32
v32@0:8@"NSURLSession"16@"NSError"24
v40@0:8@"NSURLSession"16@"NSURLAuthenticationChallenge"24@?<v@?q@"NSURLCredential">32
v24@0:8@"NSURLSession"16
@40@0:8@16q24@32
f20@0:8f16
@44@0:8Q16@24@32B40
333333
333333
